<html>
<head>
<title>Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP Features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Docker容器中运行Spark NLP，用于命名实体识别和其他NLP特性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-spark-nlp-in-docker-container-for-named-entity-recognition-and-other-nlp-features-8acdb662da5b?source=collection_archive---------28-----------------------#2020-06-06">https://towardsdatascience.com/running-spark-nlp-in-docker-container-for-named-entity-recognition-and-other-nlp-features-8acdb662da5b?source=collection_archive---------28-----------------------#2020-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1688" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Docker环境下使用Spark NLP和Jupyter notebook进行自然语言处理</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ce447dc2cb8262d4caefd97084f4b78e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dydKmnlxYYzQf9o6f-jVQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="58c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如[1]所述，<a class="ae lu" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>是语言学、计算机科学、信息工程、人工智能等许多研究领域共有的一个共同研究子领域。NLP通常关注计算机和人类自然语言之间的交互，特别是如何使用计算机来处理和分析自然语言数据(例如，文本、语音等)。).NLP中的一些主要挑战包括语音识别、自然语言理解(例如，文本理解)和自然语言生成。</p><p id="1f68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">机器学习在文本理解中的早期应用之一是电子邮件和垃圾消息检测[1]。随着深度学习的推进，许多新的高级语言理解方法已经问世，如深度学习方法<a class="ae lu" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>(使用MobileBERT进行问答的例子见[2】)。</p><p id="5857" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NLP中另一个流行的方法是<a class="ae lu" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank">命名实体识别(NER) </a>。NER的主要目的是提取<a class="ae lu" href="https://en.wikipedia.org/wiki/Named_entity" rel="noopener ugc nofollow" target="_blank">命名实体</a>(例如，个人姓名、组织名称、地点名称、产品名称等。)来自<a class="ae lu" href="https://en.wikipedia.org/wiki/Unstructured_data" rel="noopener ugc nofollow" target="_blank">非结构化文本</a>。有许多支持NER的开源NLP库/工具，如NLTK和SpaCy [3]。最近，Spark NLP [4]得到了越来越多的关注，因为它提供了更完整的受支持的NLP特性列表[5][6]。</p><p id="d85b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我看来，Spark NLP [4]的开发是基于Ubuntu Linux和OpenJDK的。因此，由于Colab使用Ubuntu操作系统，所以在Colab 中直接设置Spark NLP的环境(参见<a class="ae lu" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb" rel="noopener ugc nofollow" target="_blank">指令和代码示例</a>)。然而，我注意到很难在Mac上为Spark NLP设置一个本地环境，原因如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/044ef159aeb329c083ea8aad04f833dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VQS8uprtbB36lgNsDsJ4w.png"/></div></div></figure><p id="b875" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免这个问题，本文演示了如何建立一个Docker环境[7]来运行NER的Spark NLP和Docker容器中的其他NLP特性。这样的Docker环境可以作为建立Spark NLP <a class="ae lu" href="https://en.wikipedia.org/wiki/Microservices" rel="noopener ugc nofollow" target="_blank">微服务</a>平台的基础。</p><h1 id="36b7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">1.Docker简介</h1><p id="7abf" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">如[7]中所述，Docker是一种工具，它允许我们在沙箱(称为<em class="mt">容器</em>)中轻松部署应用程序(例如Spark NLP)，以在任何Docker支持的主机操作系统(即Mac)上运行。</p><p id="b30c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Docker的基本概念是:</p><ul class=""><li id="0f46" class="mu mv it la b lb lc le lf lh mw ll mx lp my lt mz na nb nc bi translated">Dockerfile:</li><li id="7825" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">Docker图像</li><li id="a754" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">码头集装箱</li></ul><h2 id="7b93" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">1.1文档文件</h2><p id="d52b" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">一个<a class="ae lu" href="https://docs.docker.com/engine/reference/builder/" rel="noopener ugc nofollow" target="_blank"><em class="mt">Docker file</em></a>【7】是一个简单的文本文件，包含一个用于创建Docker映像的命令列表(类似于Linux命令)。这是一种自动化Docker图像创建过程的方法。</p><h2 id="5660" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">1.2 Docker图像</h2><p id="ab70" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">一个<a class="ae lu" href="https://jfrog.com/knowledge-base/a-beginners-guide-to-understanding-and-building-docker-images/" rel="noopener ugc nofollow" target="_blank"> docker映像</a>【7】是一个只读模板，包含一组用于创建docker容器的指令，该容器可以在Docker平台上运行。它提供了一种打包应用程序和预配置服务器环境的便捷方式。</p><p id="d3f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Docker映像是从Docker文件构建的。</p><h2 id="bc33" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">1.3码头集装箱</h2><p id="4e0e" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">容器是包含代码及其所有依赖项的标准软件包，因此应用程序可以从一个计算环境快速可靠地运行到另一个计算环境。一个<a class="ae lu" href="https://www.docker.com/resources/what-container" rel="noopener ugc nofollow" target="_blank"> Docker容器</a>【7】是一个轻量级的、独立的、可执行的软件包，它包含了应用程序的一切，比如代码、运行时、系统工具、系统库和设置。</p><p id="98a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Docker容器是从Docker映像构建的。</p><h1 id="1ce3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">2.用Jupyter笔记本为Spark NLP设置Docker环境</h1><p id="2da4" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">设置Docker环境以使用Jupyter notebook运行Spark NLP的过程包括以下步骤:</p><ul class=""><li id="2d8c" class="mu mv it la b lb lc le lf lh mw ll mx lp my lt mz na nb nc bi translated">安装Docker</li><li id="afd9" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">在Docker Hub注册</li><li id="a249" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">创建Dockerfile文件</li><li id="41c3" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">建立码头形象</li><li id="b664" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">启动码头集装箱</li><li id="4ea8" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">推送Docker图像</li><li id="fcd6" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">拉动Docker图像</li></ul><h2 id="8b2b" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.1安装Docker</h2><p id="0706" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">不同平台安装Docker的说明网上有:<a class="ae lu" href="https://docs.docker.com/docker-for-mac/install" rel="noopener ugc nofollow" target="_blank"> Mac </a>，<a class="ae lu" href="https://docs.docker.com/install/linux/docker-ce/ubuntu" rel="noopener ugc nofollow" target="_blank"> Linux </a>和<a class="ae lu" href="https://docs.docker.com/docker-for-windows/install" rel="noopener ugc nofollow" target="_blank"> Windows </a>。</p><p id="d16b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦docker安装完成，我们可以使用以下Docker命令和相应的输出来验证安装:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="8cf4" class="ni lx it nv b gy nz oa l ob oc">docker --version<br/>Docker version 19.03.8, build afacb8b</span></pre><h2 id="e945" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.2在Docker Hub注册</h2><p id="0499" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">类似于<a class="ae lu" href="https://github.com/" rel="noopener ugc nofollow" target="_blank"> Github </a>分享源代码文件，<a class="ae lu" href="https://hub.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker Hub </a>就是分享Docker镜像。为了共享本地机器上的docker映像，需要将本地机器上的docker映像推送到Docker Hub服务器，以便其他人可以从Docker Hub获取Docker映像。</p><p id="9f17" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">需要先去<a class="ae lu" href="https://hub.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker Hub </a>注册才能使用Docker Hub服务。</p><h2 id="f3d3" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.3创建Dockerfile</h2><p id="4a13" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">为了构建新的Docker映像，首先需要创建一个Docker文件。</p><p id="bfa0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了简化运行Spark NLP workshop的过程，<a class="ae lu" href="https://www.johnsnowlabs.com/spark-nlp" rel="noopener ugc nofollow" target="_blank"> John Snow LABS </a>提供了一个<a class="ae lu" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/Dockerfile" rel="noopener ugc nofollow" target="_blank">Spark NLP workshop Docker file</a>，用于在Docker容器中运行workshop示例。</p><p id="f362" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了在Docker容器中构建一个新的Docker映像来运行Spark NLP和Jupyter notebook，我基于Spark NLP workshop Dockerfile文件创建了一个新的<a class="ae lu" href="https://github.com/yzzhang/machine-learning/blob/master/deep_learning/nlp/spark-nlp-docker/Dockerfile" rel="noopener ugc nofollow" target="_blank">Docker文件</a>【8】，并做了以下修改:</p><ul class=""><li id="c7b8" class="mu mv it la b lb lc le lf lh mw ll mx lp my lt mz na nb nc bi translated">删除了教程和相关的笔记本和数据文件</li><li id="e84c" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">用Spark NLP 2.5.1替换Spark NLP 2.4.5</li><li id="ed35" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">已调整docker hub用户名</li><li id="b1cd" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">调整了docker容器中的主目录名</li><li id="3f2b" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">添加了命令行<em class="mt">卷</em>选项，将主机上的当前工作目录映射到Docker容器中的主目录</li><li id="52c1" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">移除Jupyter笔记本配置</li></ul><h2 id="ce7a" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.4建立码头工人形象</h2><p id="4bd8" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">使用新的Docker文件[8]，可以按如下方式构建新的Docker映像:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0f1d" class="ni lx it nv b gy nz oa l ob oc">docker build -t zhangyuefeng123/sparknlp:1.0 .</span></pre><p id="3f3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建完成后，应该会显示以下docker命令和Docker图像标记:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/52f91845c8f7740eed27e28242a07966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6S23U31vZgMyMX9RfoeeMg.png"/></div></div></figure><h2 id="1d21" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.5启动码头集装箱</h2><p id="d2f7" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">一旦新的Docker映像准备就绪，就可以使用下面的命令启动一个新的Docker容器，使用Jupyter notebook运行Spark NLP:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="8ce0" class="ni lx it nv b gy nz oa l ob oc">docker run -it --volume $PWD:/home/yuefeng -p 8888:8888 -p 4040:4040 zhangyuefeng123/sparknlp:1.0</span></pre><p id="5797" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果一切顺利，应该会显示以下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/44156e8522b5fa6e8043ec9f1efeb7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yq35XZ5qNlpS3MHVBXbFSQ.png"/></div></div></figure><h2 id="6c23" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.6推送Docker图像</h2><p id="8308" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">为了与他人共享本地主机上的Docker映像(如<em class="mt">zhangyue feng 123/spark NLP:1.0</em>)，需要将映像推送到Docker Hub，如下所示:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a683" class="ni lx it nv b gy nz oa l ob oc">docker push zhangyuefeng123/sparknlp:1.0</span></pre><p id="3d24" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是Docker Hub推送Docker图片的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/ee3f69ea65f2b636aaf7d44bbd0060cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-oHIIzi_ZwQwRzLC-Rugg.png"/></div></div></figure><h2 id="959f" class="ni lx it bd ly nj nk dn mc nl nm dp mg lh nn no mi ll np nq mk lp nr ns mm nt bi translated">2.7拉动Docker图像</h2><p id="4ff1" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">如果具有预期功能的Docker映像(例如，<em class="mt">zhangyue feng 123/spark NLP:1.0</em>)已经存在于Docker Hub中，则它可以被拉到本地主机上以供重用，而不是从Dockerfile构建新的Docker映像，如下所示:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="6252" class="ni lx it nv b gy nz oa l ob oc">docker pull zhangyuefeng123/sparknlp:1.0</span></pre><h1 id="1d36" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">3.使用Docker容器中的Jupyter笔记本运行Spark NLP</h1><p id="7eec" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">一旦一个新的Docker容器开始运行(详见第2.5节)，我们可以复制生成的URL，如下所示，并将其粘贴到Web浏览器中，以启动Jupyter notebook Web界面:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d63d" class="ni lx it nv b gy nz oa l ob oc">http://127.0.0.1:8888/?token=9785e71530db2288bc4edcc70a6133136a39c3f706779554</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/425aa60281e5bae8f310a67901089108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZsEGbBZ_82X1GJs2hLo0rg.png"/></div></div></figure><p id="4f3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦Jupyter笔记本启动，我们就可以像往常一样使用它(详见下一节)。</p><h1 id="b70d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">4.将Spark NLP用于NER和其他NLP功能</h1><p id="1622" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">为了验证Docker container中运行的Jupyter notebook具有相同的预期功能，我创建了一个新的Jupyter notebook<em class="mt">Spark-nlp-Docker-demo . ipynb，并使用它来</em>执行[6]中的主要代码片段，以将Spark NLP应用于ner和其他NLP功能。</p><p id="c330" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，下面的代码导入所需的pyspark和spark NLP库，然后启动一个Spark会话，在Spark上运行Spark NLP:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="ffd5" class="ni lx it nv b gy nz oa l ob oc">from pyspark.sql import SparkSession<br/>from pyspark.ml import Pipeline</span><span id="7d7e" class="ni lx it nv b gy oh oa l ob oc">import sparknlp<br/>from sparknlp.annotator import *<br/>from sparknlp.common import *<br/>from sparknlp.base import *</span><span id="881e" class="ni lx it nv b gy oh oa l ob oc">spark = sparknlp.start()</span></pre><p id="7c41" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">官方<a class="ae lu" href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/resources/conll2003" rel="noopener ugc nofollow" target="_blank"> CoNLL2003数据集</a>的训练和测试数据集被下载用于演示目的；</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b689" class="ni lx it nv b gy nz oa l ob oc">from urllib.request import urlretrieve</span><span id="7925" class="ni lx it nv b gy oh oa l ob oc">urlretrieve('<a class="ae lu" href="https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train'" rel="noopener ugc nofollow" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train'</a>,<br/>           'eng.train')</span><span id="1760" class="ni lx it nv b gy oh oa l ob oc">urlretrieve('<a class="ae lu" href="https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa'" rel="noopener ugc nofollow" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa'</a>,<br/>           'eng.testa')</span></pre><p id="642b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的代码用于读取定型数据集并显示前500条记录。训练数据集遵循训练集中用于训练NER模型的注释的标准格式。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="723a" class="ni lx it nv b gy nz oa l ob oc">with open("eng.train") as f:<br/>    c=f.read()</span><span id="ee79" class="ni lx it nv b gy oh oa l ob oc">print (c[:500])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/1c6becdf99eb85a145d950f8ac4ae18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*lM-DfhdOO-9I3n2cbODJwg.png"/></div></figure><p id="d933" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练数据集可以以更易读的格式加载:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="82fa" class="ni lx it nv b gy nz oa l ob oc">from sparknlp.training import CoNLL</span><span id="0754" class="ni lx it nv b gy oh oa l ob oc">training_data = CoNLL().readDataset(spark, './eng.train')<br/>training_data.show(3)<br/>training_data.count()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/8011232d9dd2776bd6d4b81e1f226640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VgJPm_-BnsQAesMu5Q4dQ.png"/></div></div></figure><p id="cdab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下代码加载预训练的BERT嵌入模型，并使用它将测试数据集转换为BERT嵌入格式(即，将每个单词编码为768维向量)。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c982" class="ni lx it nv b gy nz oa l ob oc">bert_annotator = BertEmbeddings.pretrained('bert_base_cased', 'en') \<br/> .setInputCols(["sentence",'token'])\<br/> .setOutputCol("bert")\<br/> .setCaseSensitive(False)\<br/> .setPoolingLayer(0)</span><span id="2c41" class="ni lx it nv b gy oh oa l ob oc">test_data = CoNLL().readDataset(spark, './eng.testa')test_data = bert_annotator.transform(test_data)<br/>test_data.show(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/e8523665b55d482b1098cd924962904c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BP2INp2FGcnxg-u2PqaEA.png"/></div></div></figure><p id="a514" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的代码显示了句子的标记、相应的BERT嵌入和相应的带标签的NER标签。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="cb50" class="ni lx it nv b gy nz oa l ob oc">test_data.select("bert.result","bert.embeddings",'label.result').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/59f39c4ffcd02c052ac5fb41068fdeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dGuflyyTSRFoGcsefkHiw.png"/></div></div></figure><p id="f762" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下代码首先将测试数据集中的1，000条记录保存到一个Parquet文件中，然后创建一个基于Tensorflow的字符级CNN-DLSTM模型<em class="mt"> NerDLApproach </em>，使用经过训练的BERT嵌入模型<em class="mt"> bert_annotator </em>和NerDLApproach模型形成一个管道，最后使用训练数据集中的1，000条记录和Parquet文件中保存的1，000条测试记录来训练管道。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a2fb" class="ni lx it nv b gy nz oa l ob oc">test_data.limit(1000).write.parquet("test_withEmbeds.parquet")</span><span id="c6e0" class="ni lx it nv b gy oh oa l ob oc">nerTagger = NerDLApproach()\<br/>  .setInputCols(["sentence", "token", "bert"])\<br/>  .setLabelColumn("label")\<br/>  .setOutputCol("ner")\<br/>  .setMaxEpochs(1)\<br/>  .setLr(0.001)\<br/>  .setPo(0.005)\<br/>  .setBatchSize(8)\<br/>  .setRandomSeed(0)\<br/>  .setVerbose(1)\<br/>  .setValidationSplit(0.2)\<br/>  .setEvaluationLogExtended(True) \<br/>  .setEnableOutputLogs(True)\<br/>  .setIncludeConfidence(True)\<br/>  .setTestDataset("test_withEmbeds.parquet")</span><span id="9e73" class="ni lx it nv b gy oh oa l ob oc">pipeline = Pipeline(<br/>    stages = [<br/>    bert_annotator,<br/>    nerTagger<br/>  ])</span><span id="99b6" class="ni lx it nv b gy oh oa l ob oc">ner_model = pipeline.fit(training_data.limit(1000))</span></pre><p id="77ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，经过训练的管道可用于预测测试数据集的NER标签(参见下面的前20行结果):</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="6e0b" class="ni lx it nv b gy nz oa l ob oc">predictions = ner_model.transform(test_data)<br/>predictions.select('token.result','label.result','ner.result').show(truncate=40)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/801c90ff3378ada3106037f563d14ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S16Tk9M80yJiSqAEwlYDww.png"/></div></div></figure><p id="f654" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标记为NER标签的前20行标记和相应的预测NER标签可以以更可读的格式显示:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9a57" class="ni lx it nv b gy nz oa l ob oc">import pyspark.sql.functions as F</span><span id="e0a5" class="ni lx it nv b gy oh oa l ob oc">predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias("cols")) \<br/>.select(F.expr("cols['0']").alias("token"),<br/>        F.expr("cols['1']").alias("ground_truth"),<br/>        F.expr("cols['2']").alias("prediction")).show(truncate=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/fe7acc622040e64aa3682d146367cc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*PfZq8jiqfPN7-Pu5tFvstA.png"/></div></div></figure><p id="8eca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下代码显示了如何使用预训练的管道为给定的句子生成NER标记。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0848" class="ni lx it nv b gy nz oa l ob oc">from sparknlp.pretrained import PretrainedPipeline</span><span id="4132" class="ni lx it nv b gy oh oa l ob oc">pretrained_pipeline = PretrainedPipeline('recognize_entities_dl', lang='en')</span><span id="5d1f" class="ni lx it nv b gy oh oa l ob oc">text = "The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris."</span><span id="5cbd" class="ni lx it nv b gy oh oa l ob oc">result = pretrained_pipeline.annotate(text)</span><span id="20ba" class="ni lx it nv b gy oh oa l ob oc">list(zip(result['token'], result['ner']))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ba772debdd80d8023c640908bcfb0e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*aeuoUupEUafNAA7hcs0B7g.png"/></div></figure><p id="e938" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不同的预训练模型可用于形成新的管道:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b9c7" class="ni lx it nv b gy nz oa l ob oc">import json<br/>import os<br/>from pyspark.ml import Pipeline<br/>from sparknlp.base import *<br/>from sparknlp.annotator import *<br/>import sparknlp</span><span id="eae6" class="ni lx it nv b gy oh oa l ob oc">def get_ann_pipeline ():<br/>    <br/>    document_assembler = DocumentAssembler() \<br/>        .setInputCol("text")\<br/>        .setOutputCol('document')</span><span id="6e9e" class="ni lx it nv b gy oh oa l ob oc">    sentence = SentenceDetector()\<br/>        .setInputCols(['document'])\<br/>        .setOutputCol('sentence')\<br/>        .setCustomBounds(['\n'])</span><span id="f75d" class="ni lx it nv b gy oh oa l ob oc">    tokenizer = Tokenizer() \<br/>        .setInputCols(["sentence"]) \<br/>        .setOutputCol("token")</span><span id="a617" class="ni lx it nv b gy oh oa l ob oc">    pos = PerceptronModel.pretrained() \<br/>          .setInputCols(["sentence", "token"]) \<br/>          .setOutputCol("pos")<br/>    embeddings = WordEmbeddingsModel.pretrained()\<br/>          .setInputCols(["sentence", "token"])\<br/>          .setOutputCol("embeddings")</span><span id="76ae" class="ni lx it nv b gy oh oa l ob oc">    ner_model = NerDLModel.pretrained() \<br/>          .setInputCols(["sentence", "token", "embeddings"]) \<br/>          .setOutputCol("ner")</span><span id="9d90" class="ni lx it nv b gy oh oa l ob oc">    ner_converter = NerConverter()\<br/>          .setInputCols(["sentence", "token", "ner"])\<br/>          .setOutputCol("ner_chunk")</span><span id="5ad0" class="ni lx it nv b gy oh oa l ob oc">    ner_pipeline = Pipeline(<br/>        stages = [<br/>            document_assembler,<br/>            sentence,<br/>            tokenizer,<br/>            pos,<br/>            embeddings,<br/>            ner_model,<br/>            ner_converter<br/>        ]<br/>    )</span><span id="a822" class="ni lx it nv b gy oh oa l ob oc">    empty_data = spark.createDataFrame([[""]]).toDF("text")</span><span id="3791" class="ni lx it nv b gy oh oa l ob oc">    ner_pipelineFit = ner_pipeline.fit(empty_data)</span><span id="8c0f" class="ni lx it nv b gy oh oa l ob oc">    ner_lp_pipeline = LightPipeline(ner_pipelineFit)</span><span id="84d7" class="ni lx it nv b gy oh oa l ob oc">    return ner_lp_pipeline</span></pre><p id="4de1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下代码使用上述函数创建一个新管道，然后使用它为给定的句子生成各种注释/标记:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="ee7a" class="ni lx it nv b gy nz oa l ob oc">conll_pipeline = get_ann_pipeline ()<br/>parsed = conll_pipeline.annotate ("Peter Parker is a nice guy and lives in New York.")<br/>parsed</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/6bb73ed8f5a8356a01c52f709776b7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_2hKZCH-mFyu4haJb9SNbA.png"/></div></div></figure><h1 id="6090" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">5.摘要</h1><p id="7209" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">Spark NLP [4]越来越受欢迎，因为它在一个系统中支持更多的NLP功能。Spark NLP是在Ubuntu Linux系统上用OpenJDK开发的。根据我的经验，我注意到很难为Spark NLP no Mac设置一个本地环境，这是由于一个已知的异常“<em class="mt">异常:Java网关进程在发送其端口号</em>之前退出”。</p><p id="572a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免这个安装问题，在本文中，我演示了如何设置一个Docker环境来运行Spark NLP和Jupyter notebook for NER以及Docker容器中的其他NLP功能。</p><p id="bd9a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用[6]中的代码示例验证了Mac上Spark NLP的Docker环境。</p><p id="69f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Spark NLP的Docker环境有潜力作为建立Spark NLP <a class="ae lu" href="https://en.wikipedia.org/wiki/Microservices" rel="noopener ugc nofollow" target="_blank">微服务</a>平台的基础。</p><p id="60f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Docker文件和Docker的Jupyter笔记本都可以在Github [8]中获得。</p><h1 id="5894" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><ol class=""><li id="ec9a" class="mu mv it la b lb mo le mp lh oq ll or lp os lt ot na nb nc bi translated">Y.张，利用word2vec-keras进行自然语言处理的深度学习</li><li id="e757" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">Y.张，<a class="ae lu" rel="noopener" target="_blank" href="/deep-learning-for-natural-language-processing-on-mobile-devices-3024747a7043">面向移动设备的自然语言处理深度学习</a></li><li id="47de" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">南李，<a class="ae lu" rel="noopener" target="_blank" href="/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da">利用NLTK和SpaCy进行命名实体识别</a></li><li id="bd3f" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated"><a class="ae lu" href="https://nlp.johnsnowlabs.com/" rel="noopener ugc nofollow" target="_blank">火花NLP </a></li><li id="ac13" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">动词 （verb的缩写）科贾曼，<a class="ae lu" rel="noopener" target="_blank" href="/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59">Spark NLP简介:基础和基本组件</a></li><li id="81ec" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">动词 （verb的缩写）科贾曼，<a class="ae lu" rel="noopener" target="_blank" href="/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77">命名实体识别(NER)与BERT在Spark NLP中</a></li><li id="955e" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">页（page的缩写）适合初学者的docker</li><li id="1353" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt ot na nb nc bi translated">Y.张，<a class="ae lu" href="https://github.com/yzzhang/machine-learning/tree/master/deep_learning/nlp/spark-nlp-docker" rel="noopener ugc nofollow" target="_blank">Github中的Dockerfile和Jupyter笔记本</a></li></ol></div></div>    
</body>
</html>