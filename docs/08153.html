<html>
<head>
<title>Activation Functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/activation-functions-in-neural-networks-eb8c1ba565f8?source=collection_archive---------62-----------------------#2020-06-15">https://towardsdatascience.com/activation-functions-in-neural-networks-eb8c1ba565f8?source=collection_archive---------62-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="addd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么我们需要激活函数，是什么让它们如此特别</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/17288ae12f1b8b3e7e007e57c9c2f2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Puv0mgYE_C_-Rip1vbqFeA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼尔·托马斯在 Unsplash 上拍摄的照片</p></figure><p id="3c2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经网络之所以如此特殊，是因为它们能够模拟输入和输出之间高度复杂的关系。我们无法用线性模型实现这些复杂的关系。因此，神经网络需要能够表示<strong class="la iu">非线性</strong>。这就是我们在神经网络中使用激活函数的原因。如果没有激活函数，神经网络可以被认为是一个线性模型包。</p><p id="7f0d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">激活功能增加了神经网络的学习能力。用神经网络解决的许多任务包含非线性，例如图像、文本、声波。因此，我们需要非线性来解决深度学习领域中最常见的任务，如图像和语音识别、自然语言处理等。</p><p id="4f97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">没有激活功能的神经元只是输入和偏置的线性组合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/0c608315ed56c178eef0536a694be8e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*KDLyhodosRaJIb7MSc0I2g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">没有激活功能的神经元</p></figure><p id="8cb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这只是一个神经元。神经网络中典型的隐藏层有许多神经元。然而，没有激活函数，我们有许多不同的输入线性组合。通过添加额外的隐藏层，我们得到输入的线性组合的线性组合，而不会给我们带来非线性的复杂性。然而，当我们向神经元添加激活函数时，我们就摆脱了线性的诅咒，能够模拟复杂的非线性关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/fd67f1b02a227eea4b5fd85a09b4f77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_lLj1VmVXCVN3k2zu43mA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有激活功能的神经元</p></figure><p id="c6c2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多不同的激活功能可用。每一个都有自己的优点和缺点以及具体的特点。我们不仅对激活函数本身感兴趣，而且对它的导数也感兴趣。原因是神经网络实际“学习”的方式。</p><p id="8af0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输入乘以权重并加上偏差。然后应用激活函数得到输出。这个过程被称为<strong class="la iu">正向传播</strong>。将神经网络的输出与实际目标值进行比较，并计算差值(或损失)。关于损失的信息被反馈到神经网络，并且权重被更新，从而减少损失。这个过程被称为<strong class="la iu">反向传播。</strong>使用基于导数的<strong class="la iu">梯度下降算法</strong>更新权重。因此，激活函数的导数也应该携带关于输入值的信息。</p><p id="6b05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多激活功能可用。我们将涵盖神经网络中最常用的非线性激活函数。</p><h1 id="948b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">乙状结肠</strong></h1><p id="fb2d" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们熟悉逻辑回归中的 sigmoid 函数。著名的 S 形函数将输入值转换为 0 到 1 之间的范围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cc187ab0695ed175f73366960a37d09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*ZUoxuoj-0tzno3WOiddH9w.png"/></div></figure><p id="4f4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来画一下。我们首先创建一个 numpy 数组，并对其应用 sigmoid 函数。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="ccaf" class="mz lx it mv b gy na nb l nc nd">import numpy as np<br/>import pandas as pd</span><span id="e68f" class="mz lx it mv b gy ne nb l nc nd">x = np.linspace(-10,10,50)<br/>y = 1 / (1 + np.exp(-x))</span></pre><p id="3a58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后用 matplotlib 绘图。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a4b1" class="mz lx it mv b gy na nb l nc nd">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="126c" class="mz lx it mv b gy ne nb l nc nd">plt.figure(figsize=(8,5))<br/>plt.title("Sigmoid Function", fontsize=15)<br/>plt.grid()<br/>plt.plot(x, y)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/725d7f87df601a0e6d27626e3779fc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*Qg_f0t-vEVNlaYpUTknj0Q.png"/></div></figure><p id="1c3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个神经元的输出在 0 和 1 之间标准化。上图的中间区域显示，x(输入)的微小变化会导致 y(输出)的相对较大变化。Sigmoid 函数擅长检测这些区域中的差异，这使其成为良好的分类器。因此，它通常用于二元分类任务。</p><p id="c3bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，没有什么是完美的。sigmoid 函数有一个缺点。随着我们远离中心，x 值的变化对 y 值的影响很小或没有影响，让我们来看看 sigmoid 函数的导数。我们可以使用 numpy 的梯度函数来计算导数:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a982" class="mz lx it mv b gy na nb l nc nd">x = np.linspace(-10,10,50)<br/>dx = x[1]-x[0]<br/>y = 1 / (1 + np.exp(-x))<br/>dydx = np.gradient(y, dx)</span></pre><p id="d235" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们在同一张图上绘制 y 和 dydx:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="3a30" class="mz lx it mv b gy na nb l nc nd">plt.figure(figsize=(8,5))<br/>plt.title("Sigmoid Function and Its Derivative", fontsize=15)<br/>plt.grid()<br/>plt.plot(x, y)<br/>plt.plot(x, dydx)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b05233479780f1ad0adb66aabb3bb418.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*Xf4bXz56DxpNS-_DksFvGA.png"/></div></figure><p id="b62c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，当我们远离中心时，导数趋向于零。在评论这个图之前，让我们记住神经网络是如何学习的。神经网络的学习意味着更新权重以最小化损失(实际值和预测值之间的差异)。基于梯度更新权重，梯度基本上是函数的导数。如果梯度非常接近零，则以非常小的增量更新权重。这导致神经网络学习速度非常慢，永远无法收敛。这也被称为<strong class="la iu">消失梯度</strong>问题。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="b04f" class="lw lx it bd ly lz nn mb mc md no mf mg jz np ka mi kc nq kd mk kf nr kg mm mn bi translated"><strong class="ak">双曲正切</strong></h1><p id="5feb" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">Tanh 与 sigmoid 函数非常相似，只是它是围绕原点对称的。输出值限制在(-1，+1)的范围内。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="d3b2" class="mz lx it mv b gy na nb l nc nd">x = np.linspace(-5,5,80)<br/>y_tanh =2*(1 / (1 + np.exp(-2*x)))-1</span><span id="314f" class="mz lx it mv b gy ne nb l nc nd">plt.figure(figsize=(8,5))<br/>plt.title("Tanh Function", fontsize=15)<br/>plt.grid()<br/>plt.plot(x, y_tanh)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f084854ac3de3aefabea1adf530b6754.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Abcx2Xkqq1m0xlRgASH-UA.png"/></div></figure><p id="fbf4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Tanh 是以<strong class="la iu">零点为中心的</strong>，这样渐变就不会被限制在一个特定的方向上移动。因此，它比 sigmoid 函数收敛得更快。</p><p id="9ede" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">tanh 的导数类似于 sigmoid 的导数，但是更陡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/77e5d5caea848a14cf167a6fe7bd1871.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*IX89AkjJ1h4nrs32nmklVg.png"/></div></figure><p id="4c43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从导数线可以看出，tanh 也有消失梯度的问题。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="fd32" class="lw lx it bd ly lz nn mb mc md no mf mg jz np ka mi kc nq kd mk kf nr kg mm mn bi translated"><strong class="ak"> ReLU(整流线性单元)</strong></h1><p id="5e19" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">对于大于 0 的输入，relu 函数的输出等于输入值。对于所有其他输入值，输出为 0。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="db0a" class="mz lx it mv b gy na nb l nc nd">x = np.linspace(-10,10,50)<br/>y_relu = np.where(x &lt; 0, 0, x)</span><span id="df30" class="mz lx it mv b gy ne nb l nc nd">plt.figure(figsize=(8,5))<br/>plt.title("ReLU Function", fontsize=15)<br/>plt.grid()<br/>plt.plot(x, y_relu)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7ebe22ce99f2875f8ce87cb93d83ad05.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*b56ABEFJv9_a8-Fb95uTqQ.png"/></div></figure><p id="a207" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Relu 使得仅激活网络中的一些神经元成为可能，这使得计算效率比 tanh 和 sigmoid 更高。所有的神经元都被 tanh 和 sigmoid 激活，这导致了密集的计算。因此，relu 比 tanh 和 sigmoid 收敛得更快。</p><p id="c266" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于大于 0 的输入值，relu 的导数为 1。对于所有其他输入值，导数为 0，这导致一些权重在反向传播期间永远不会更新。因此，神经网络不能学习负输入值。这个问题被称为<strong class="la iu">将死再禄</strong>问题。使用泄漏 relu 函数的一种解决方案。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="a67f" class="lw lx it bd ly lz nn mb mc md no mf mg jz np ka mi kc nq kd mk kf nr kg mm mn bi translated"><strong class="ak">泄漏的 ReLU </strong></h1><p id="d1c0" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">除了正输入值之外，它与 relu 相同。对于负值，leaky relu 输出一个非常小的数字，而 relu 只给出 0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/821a99fb7a9ba10c1b22e89e537333d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*7ExxZVNgEWrewRoXYux6Ew.png"/></div></figure></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="ed0a" class="lw lx it bd ly lz nn mb mc md no mf mg jz np ka mi kc nq kd mk kf nr kg mm mn bi translated"><strong class="ak"> Softmax </strong></h1><p id="d377" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">Softmax 获取实数的输入值，并将其归一化为概率分布。概率与输入值的指数成正比。考虑一下，神经网络的输出层有 10 个神经元。Softmax 函数采用这 10 个输出并创建概率分布。10 个值的概率加起来是 1。</p><p id="e162" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Softmax 激活用于具有多个类别的分类任务。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="40ec" class="lw lx it bd ly lz nn mb mc md no mf mg jz np ka mi kc nq kd mk kf nr kg mm mn bi translated"><strong class="ak">唰</strong></h1><p id="e0cc" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><a class="ae nu" href="https://arxiv.org/abs/1710.05941v1" rel="noopener ugc nofollow" target="_blank"> Swish </a>是一个自门控激活功能，与我们迄今为止讨论过的功能相比相对较新。这是由谷歌的研究人员发现的。在计算效率方面，它与 relu 相似，但在更深的模型上比 relu 表现得更好。正如研究人员所说，“在许多具有挑战性的数据集上，swish 往往比 relu 更好地工作”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e0097d9fb47228c6707ccb5733609b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*0-QhBjpbov67QPBGtOoidQ.png"/></div></figure><p id="3e30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们画出 swish 的曲线图。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="351d" class="mz lx it mv b gy na nb l nc nd">x = np.linspace(-5,5,50)<br/>y_swish = x*(1 / (1 + np.exp(-x)))</span><span id="5662" class="mz lx it mv b gy ne nb l nc nd">plt.figure(figsize=(8,5))<br/>plt.title("Swish Function", fontsize=15)<br/>plt.grid()<br/>plt.plot(x, y_swish)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d531cdf6d85d5a3e2dca0d3ed3635779.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*COdhIqzaoDjYMlKj3GV5wQ.png"/></div></figure><p id="582a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Swish 在 0 处没有锐边，不像 relu 在训练时更容易收敛。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="019e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经讨论了 6 种不同的激活函数。它们在某些方面都各有利弊。激活函数对计算复杂性和模型的收敛性有影响。因此，更好地理解它们是如何表现的，以便我们可以根据任务选择最佳的激活函数。通常，激活函数的期望属性是:</p><ul class=""><li id="c923" class="nx ny it la b lb lc le lf lh nz ll oa lp ob lt oc od oe of bi translated">计算成本低</li><li id="c76b" class="nx ny it la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">零居中</li><li id="c0e5" class="nx ny it la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">可微的。激活函数的导数需要携带关于输入值的信息，因为权重是基于梯度更新的。</li><li id="b7d5" class="nx ny it la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">不会导致消失梯度问题</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="fe16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>