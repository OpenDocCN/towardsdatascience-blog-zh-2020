<html>
<head>
<title>A small timing experiment on the new Tokenizers library — a write-up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于新的 Tokenizers 库的一个小计时实验——一篇文章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6?source=collection_archive---------26-----------------------#2020-01-19">https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6?source=collection_archive---------26-----------------------#2020-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="11ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">剧透:速度惊人🔥</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1eb7e04af2fff6671b75b17f5e2c1d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UCqN5meuMWIxtpQ8eAGxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GitHub 上的<a class="ae ky" href="https://github.com/huggingface/tokenizers" rel="noopener ugc nofollow" target="_blank"> Tokenizers 库的自述文件</a></p></figure><p id="e28f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一周前，可爱的人在<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">抱脸</strong> </a>向公众发布了他们的新<a class="ae ky" href="https://github.com/huggingface/tokenizers" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Tokenizers 库</strong> </a>。tokenizers 用<a class="ae ky" href="https://www.rust-lang.org/" rel="noopener ugc nofollow" target="_blank"> Rust </a>编程语言编写(与 Python 相比，它以性能著称)，提供了“当今最常用的 Tokenizers 的实现，重点关注性能和通用性”。</p><p id="15f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，这篇文章介绍了一些小型计时实验的结果，这些实验对比和比较了吴等人(2016 年)介绍的<strong class="lb iu">分词器</strong>的不同实现，并随着 Devlin 等人(2018 年)发布和出版的<a class="ae ky" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">BERT</strong></a><strong class="lb iu"/>而得到推广。与这些计时实验相关的所有代码都可以在下面的 Jupyter 笔记本中找到:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure><h1 id="4620" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">🤗变形金刚 vs💥标记化者</h1><p id="8a63" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在第一个计时实验中，我比较了在流行的<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">变形金刚库</strong> </a>(也是通过拥抱脸)中实现的 Bert 单词块标记器的性能(在执行时间方面)和新的标记器库的性能。对于这两者，我在 5 次独立运行中标记(编码)了 100 万个英语句子，其结果可以在下面找到:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器与令牌化器定时实验</p></figure><p id="1efd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，平均执行时间仅为 45.6 秒，与 Transformers 库实现(平均执行时间为 6 分 42 秒)相比，Tokenizers 库实现的速度提高了近 9 倍。</p><h1 id="0197" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">🧵多线程性能</h1><p id="fcf4" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">Tokenizers 还提供了一种一次(批量)编码多个句子的方法，由于其多线程实现(在 Rust 中)，这可以显著提高性能。然而，Python 也支持多线程，例如使用<code class="fe mu mv mw mx b"><a class="ae ky" href="https://docs.python.org/3.7/library/concurrent.futures.html" rel="noopener ugc nofollow" target="_blank">concurrent.futures</a></code>。</p><p id="91e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，与第一次计时实验类似，这里我比较了使用<code class="fe mu mv mw mx b">concurrent.futures.ThreadPoolExecutor</code>与<code class="fe mu mv mw mx b">submit</code>和<code class="fe mu mv mw mx b">map</code>的 Bert 单词块标记器的性能，以及标记器的本机<code class="fe mu mv mw mx b">encode_batch</code>，结果如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多线程性能计时实验</p></figure><p id="e551" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，令人惊讶的是，<code class="fe mu mv mw mx b">submit</code>和<code class="fe mu mv mw mx b">map</code>与非多线程令牌化相比，性能(相等)更差。然而，更有趣(也更令人印象深刻)的是，Tokenizers 库<strong class="lb iu">自带的多线程<code class="fe mu mv mw mx b">encode_batch</code>只需要 10.6 秒</strong>就可以标记一百万个句子！</p><h1 id="9b3e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结论</h1><p id="b3bb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">正如宣传的那样，拥抱脸的新 Tokenizers 库<strong class="lb iu">提供了比 Transformers 库快得多(几乎 9 倍)的 BERT 单词块 tokenizer 实现</strong>。然而，当批量对句子进行分词时，性能更加令人印象深刻，因为<strong class="lb iu">只需 10.6 秒就可以对 100 万个句子进行分词</strong>。因此，我想我可以有把握地断定<strong class="lb iu">它快得惊人🔥</strong>！</p><p id="c1cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然新的 Tokenizers 库提供了更多的好处，而不仅仅是其令人印象深刻的性能(例如，在新词汇上训练标记器的能力)，但应该说，这种性能的显著提高不仅<strong class="lb iu">允许对越来越大的数据集进行标记</strong>(在运行中)，而且<strong class="lb iu">允许这些方法和技术</strong>更好的民主化(例如，在更便宜的硬件上部署，如移动电话和 SOC)，允许来自各种背景的有抱负的 NLP 爱好者开始使用最新和最棒的 NLP🤗</p><h1 id="4307" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">参考</h1><p id="cba0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">[1] Y. Wu 等，<a class="ae ky" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank"> Google 的神经机器翻译系统:弥合人与机器翻译的鸿沟</a> (2016)，【arXiv 预印本 arXiv:1609.08144 。</p><p id="bbec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] J. Devlin 等，<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向变换器的预训练</a> (2018)，<em class="my"> arXiv 预印本 arXiv:1810.04805 </em>。</p></div></div>    
</body>
</html>