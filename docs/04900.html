<html>
<head>
<title>Day 119 of #NLP365: NLP Papers Summary — An Argument-Annotated Corpus of Scientific Publications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第119天:NLP论文摘要——科学出版物的论据注释文集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097?source=collection_archive---------48-----------------------#2020-04-28">https://towardsdatascience.com/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097?source=collection_archive---------48-----------------------#2020-04-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="548a" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="f603" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="481f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的262天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="4e9a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz">一个带论据注释的科学出版物语料库</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="e165" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="53ba" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">使用论证组件和关系注释扩展了Dr. Inventor语料库，并进行了注释研究。这里的目标是理解科学文本中的不同论点，以及它们是如何联系在一起的。我们对带注释的论证进行了分析，并探索了存在于科学写作中的论证之间的关系。这些贡献如下:</p><ol class=""><li id="5a92" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">为涵盖不同研究领域的科技文本提出了一个通用的议论文注释方案</li><li id="96de" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">具有论证组件和关系注释的扩展的Dr. Inventor语料库</li><li id="366f" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">对语料库进行了信息论分析</li></ol><h1 id="e4f4" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">注释方案</h1><p id="bb9b" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有许多论证的理论框架，我们最初使用图尔敏模型，因为它简单并且与人工智能和论证挖掘相关。图尔敏模式有6种论证成分:主张、数据、保证、支持、限定和反驳。然而，在最初的注释之后，我们意识到并不是所有的组件都存在。因此，我们将注释方案简化为以下三个论证部分:</p><ol class=""><li id="c242" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated"><em class="lz">自己的主张</em>。与作者作品相关的论证性陈述</li><li id="5e6e" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">背景权利要求</em>。与作者作品相关的论述性陈述</li><li id="ffd3" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">数据组件</em>。支持或反对某一主张的事实。这包括参考资料和带有例子的事实</li></ol><p id="b9eb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了这些论证成分集，我们引入了以下三种关系类型:</p><ol class=""><li id="044d" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated"><em class="lz">支撑</em>。如果一个组件的实际准确性随着另一个组件的增加而增加，则这种关系在两个组件之间成立</li><li id="9a3e" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">与</em>相矛盾。如果一个组件的实际准确性随着另一个组件而降低，则这种关系在两个组件之间成立</li><li id="0df3" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">语义相同</em>。这种关系捕获语义相同的声明或数据组件。这类似于自变量共指和/或事件共指</li></ol><h1 id="5677" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">注释研究</h1><p id="36a6" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们对Dr. Inventor语料库进行了注释研究，并扩展了数据集。Dr. Inventor语料库有四层带有子标签的修辞注释，如下所示:</p><ol class=""><li id="29a9" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">话语角色</li><li id="d0d2" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">引用目的</li><li id="ea5c" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">主观方面</li><li id="c1f3" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">总结相关性</li></ol><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/20c253ae6d0779497160df70c36bf9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*Qgjt5_bkYVnHy2TN.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">4个不同的注释层[1]</p></figure><p id="a59b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注释过程由一名专家和三名非专家注释者组成。注释者在校准阶段接受培训，所有注释者一起注释一个出版物。我们为每次迭代计算注释者间的一致(IAA ),并讨论任何分歧。下图展示了跨5次迭代的IAA分数进展。有严格和软弱两个版本。严格版本要求实体在跨度和类型上完全匹配，关系在组件、方向和关系类型上完全匹配。弱版本要求类型匹配，范围重叠。协议(IAA)如预期的那样随着迭代而增加。此外，关系上的一致程度较低，因为这通常更加主观，更不用说关系上的一致会受到组件上的一致的影响。</p><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6ba13c73024fc09f6e5f4bad91d2a4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/0*I5Zb8xEyW3c8cCYV.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">注释者间协议(IAA) [1]</p></figure><h1 id="dbab" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">语料库分析</h1><h2 id="d382" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">辩论注释分析</h2><p id="24ec" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">表2展示了在Dr. Inventor语料库中每个论证成分和关系的汇总统计。大约有。自有主张的数量是背景主张的2倍，这是意料之中的，因为语料库由原创研究论文组成。此外，数据组件只有索赔的一半多。这可能是因为并非所有索赔都得到支持，或者索赔可以得到其他索赔的支持。自然，有许多支持关系，因为作者倾向于通过用数据组件或其他声明来支持它来加强他们的声明。表3展示了论证部分的长度。自有和背景索赔的长度相似，而数据部分的长度是一半。这可以归因于这样一个事实，即在计算机科学中，解释往往是简短的，而且大多数情况下，作者只会参考表格和数字来支持。</p><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/68249b7cb5c10644882fbff5f71eec1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*uhbFlW1S-CjsEpPa.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">发明家博士语料库统计[1]</p></figure><p id="603d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">科学论文的论证结构遵循有向无环图(DAG ),其中论证部分是节点，边是关系。下面的表4展示了科学论文论证结构的DAG的图形分析。有27件独立索赔和39件无证据的索赔。最大入度显示了节点之间的最大连接数。平均6个告诉我们，有许多索赔提供了强有力的支持证据。我们还运行了PageRank算法来识别最重要的声明，并在表5中列出了一些示例。结果显示，大多数排名最高的索赔来自背景索赔，告诉我们，在计算机图形学论文中，他们倾向于把更多的重点放在他们的工作动机的研究差距，而不是实证结果。</p><div class="ny nz oa ob gt ab cb"><figure class="op iv oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/91b50bf1f7180cc08938610430ba5bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/0*28TSjhP4gxFnMhBn.png"/></div></figure><figure class="op iv oz or os ot ou paragraph-image"><img src="../Images/98541fd57b69c5c2de256238cea6b695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/0*J4D2U4cCpg7IbGr8.png"/><p class="iy iz gj gh gi ja jb bd b be z dk pa di pb pc translated">左图:基于图表的论证结构分析|右图:主张类型的示例以及与这些主张相关的句子[1]</p></figure></div><h2 id="4e67" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">与其他修辞方面的联系</h2><p id="b124" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们的新论证组件与Inventor博士语料库中的现有注释有多好的联系？在下面的表6中，我们展示了归一化互信息(NMI ),它测量五个标注层之间的共享信息量。我们展示了所有注释对的NMI分数:</p><ol class=""><li id="9ec8" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">变元组件(AC)</li><li id="5609" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">话语角色</li><li id="c162" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">主观方面</li><li id="d6d2" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">相关概述</li><li id="985e" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">引用上下文(CC)</li></ol><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/e340fc01f1a9c4f17b1a3503b5a82c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/0*4vTKoItapvNVG242.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">标准化互信息(NMI) [1]</p></figure><p id="1178" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">AC和DR之间有一个很强的NMI分数，这是有意义的，因为背景声明可能会在话语角色背景部分找到。另一个高NMI得分介于AC和CC之间。这是有意义的，因为在背景技术权利要求中经常引用引文。</p><h1 id="88f9" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">结论和未来工作</h1><p id="1328" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们创建了第一个带论证注释的科学论文语料库，并提供了语料库和论证分析的关键摘要统计数据。潜在的未来工作可能涉及扩展其他领域论文的语料库，并进一步开发分析科学写作的模型。</p><h2 id="d31c" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">来源:</h2><p id="e188" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]劳舍尔，a .，格拉瓦什，g .和庞泽托，S.P .，2018年11月。附有论证注释的科学出版物文集。在<em class="lz">关于论点挖掘的第五次研讨会的会议录</em>(第40-46页)。</p><p id="2de9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年4月28日</em><a class="ae jc" href="https://ryanong.co.uk/2020/04/28/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="c64e" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="6ab6" class="ne nf jf lf b lg mz lj na lm pe lq pf lu pg ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="6a4a" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sensation-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="f411" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="d84c" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="e5db" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towardsdatascience . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="079e" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="0cf2" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li></ul><h1 id="66a6" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">总结</h1><ul class=""><li id="9039" class="ne nf jf lf b lg mz lj na lm pe lq pf lu pg ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="560f" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="ccf0" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="bf5c" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="d97b" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61a E6 CDF 32 f</a></li><li id="c276" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li></ul><h1 id="0fc1" class="mh mi jf bd mj mk ns mm mn mo nt mq mr ku nu kv mt kx nv ky mv la nw lb mx my bi translated">其他人</h1><ul class=""><li id="4ab4" class="ne nf jf lf b lg mz lj na lm pe lq pf lu pg ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="044a" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="ab2c" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly ph nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li></ul></div></div>    
</body>
</html>