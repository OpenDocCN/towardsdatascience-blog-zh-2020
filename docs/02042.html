<html>
<head>
<title>A Semi-Supervised Embedding based Fuzzy Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于半监督嵌入的模糊聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-semi-supervised-embedding-based-fuzzy-clustering-b2023c0fde7c?source=collection_archive---------22-----------------------#2020-02-26">https://towardsdatascience.com/a-semi-supervised-embedding-based-fuzzy-clustering-b2023c0fde7c?source=collection_archive---------22-----------------------#2020-02-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0175" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">维基百科名人页面的自动标记</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1c2bddc489029c5c4cf41944aba2afcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oCNkyx2Cu6wEdOis"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/photos/QtFAXP6z0Wk" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="4aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.geeksforgeeks.org/clustering-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">聚类</strong> </a>是将群体或数据点分成若干组，使得同一组中的数据点更加相似的任务。而另一方面，<a class="ae ky" href="https://en.wikipedia.org/wiki/Fuzzy_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">模糊聚类</strong> </a>是一种聚类形式，其中每个数据点可以属于多个聚类。<a class="ae ky" href="https://en.wikipedia.org/wiki/Semi-supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">半监督学习</strong> </a> <strong class="lb iu"> </strong>是一类机器学习，我们通过显示少量已标记数据，让机器学习或决定大量未标记数据。</p><p id="26db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经完成了所有术语的定义，让我们来讨论一些现实生活中的场景，并进入这篇博客的主题。</p><p id="d05d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常情况下，我们对自己想要的集群有一个想法。例如，如果我们对城市进行分组，我们希望看到更多的城市被分组在一起，而不是城市郊区或城镇。类似地，如果我们对人进行聚类，我们希望看到来自相似背景/领域/爱好的人被分组在一起。</p><blockquote class="lv lw lx"><p id="54fc" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">这篇博客的主要思想是通过向我们的算法展示一些我们希望我们的聚类是什么样子的例子，来定义一种创建模糊聚类的新方法。在这个博客中，我们将把维基百科名人分为5个组/群。更多细节，这个名人数据集是如何创建的可以在<a class="ae ky" rel="noopener" target="_blank" href="/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25">这里</a>找到。但是，这并不重要，因为我已经提供了数据的子集和所需的代码，供您尝试和使用这个概念。最重要的是，这种技术非常通用，它可以很容易地应用于许多用例，比如plug &amp; play。</p></blockquote></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="078e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">问题陈述</strong></h1><p id="6b04" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们有一个维基百科名人数据集(包含所有过去或现在有维基页面的名人)。我们想自动将这些人分成5组。</p><ol class=""><li id="ba0d" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu"> <em class="ly">艺术家</em> </strong>:所有演员、音乐家、电影制作人、艺术创作人、画家等</li><li id="8c11" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">运动员:任何运动项目的运动员</li><li id="a670" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu"> <em class="ly">科学家/研究人员</em> </strong>:任何背景的科学家，如物理学家、化学家、生物学家、历史学家、计算机科学家等</li><li id="a64c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu"><em class="ly"/></strong>领导人/政治家:任何国家或地区的领导人/政治家</li><li id="0684" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu"> <em class="ly">其他影响者</em> </strong>:哲学家、教师、医生、企业家、技术人员等</li></ol><p id="1a62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的一点是，有些人可能属于多个类别，正如我们所知，许多运动员后来进入政界或在职业生涯中表演等。因此，我们想要实现的是模糊分组/聚类，而不是硬聚类。</p><h1 id="b6f2" class="mj mk it bd ml mm nu mo mp mq nv ms mt jz nw ka mv kc nx kd mx kf ny kg mz na bi translated">对数据的研究</h1><p id="7514" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在设计策略或算法之前，我们先看一次数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/cfb8622712619853271b5cce546aee25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ll4xd1fCTL6F8CsMe2RM_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集快照</p></figure><p id="5ae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集由两列组成，标题和内容。标题是这个维基百科页面的人的名字，内容是关于他们童年、职业、成就等的页面内容(大文本)。</p><p id="f36f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维基百科大约有598万页面，其中有105万页面是关于人的。所以，我们的数据集由105万个名人的维基百科页面组成。</p><p id="300a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能有许多现在或过去的运动员、艺术家、领导人等等，人们可能不知道。这种算法的美妙之处在于，它能以适当的概率自动为它们找到正确的聚类。</p><h1 id="0966" class="mj mk it bd ml mm nu mo mp mq nv ms mt jz nw ka mv kc nx kd mx kf ny kg mz na bi translated">设计战略</h1><h2 id="5f8c" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">步骤1:创建一个小标签数据</h2><p id="a441" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">第一步将是利用半监督学习的好处，或者简单地说，利用我们拥有的少量信息。<strong class="lb iu"> <em class="ly">我们会列出每个组的一些知名人士，并获取他们的维基百科页面。这个列表可能是你的选择，可能因人而异。让我们看看我列出的不同类别的人。</em></strong></p><blockquote class="lv lw lx"><p id="e5d9" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">艺人</em> </strong>:莱昂纳多·迪卡普里奥，阿米特巴·巴强，盖尔·加朵，艾德·希兰</p><p id="63d7" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">运动员</em> </strong>:安德烈·阿加西、玛丽亚·莎拉波娃、萨钦·滕杜尔卡尔、莱昂内尔·梅西</p><p id="6222" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">科学家/研究人员</em> </strong>:阿尔伯特·爱因斯坦、艾萨克·牛顿、德米特里·门捷列夫、查尔斯·达尔文</p><p id="aea7" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">领导人/政治家</em> </strong>:巴拉克·奥巴马、纳伦德拉·莫迪、弗拉基米尔·普京</p><p id="8fbd" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"><em class="it"/></strong>:史蒂夫·乔布斯、拉里·佩奇、比尔·盖茨、艾伦·格林斯潘、萨尔·汗、杰弗里·辛顿</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ede0486a1d8a1c4a10f5f9ff0bc35e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NzjpEF3BrUQezUGNHgiQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd on">艺人</strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/340770438532a801b646928475910655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YME4SzRNyq9vUxS6rP6Omg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd on">运动员</strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/9df9758002eb10cff7eb1db88fe9344e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*044lv7YduoZdDb4-r_kR1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd on"> <em class="oq">科学家/研究人员</em> </strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/63a29776d3e8eaa17fc07e0edde36ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEKwubv_lbJn6_J19GXE2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd on"> <em class="oq">领导人/政治家</em> </strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/e0464957e0d59bb9e0f70162a7e819ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSwBaQSaIOkZcDvHqFiHzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd on"> <em class="oq">其他影响者</em> </strong></p></figure><p id="affa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个列表就像我们创建的21个人的小标签数据。训练集看起来像</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/57f1b67da26508d9eb319480b848f642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCzdSXCGglnXFRcbmN-U9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">21人的培训组</p></figure><h2 id="3b5e" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">步骤2:为有限的标记数据生成嵌入</h2><p id="bdcd" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们现在有21个维基百科页面的语料库。我们可以把每一页称为语料库的一个文档。我们将生成每个文档的嵌入。一个<a class="ae ky" href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">嵌入</strong> </a>是一个相对低维的空间，你可以将高维向量平移到其中。嵌入使得在大量输入上进行机器学习变得更加容易，比如表示单词的稀疏向量。它可以是任何长度。在这项工作中，我使用了100维嵌入。不同长度绝对可以玩。我使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" rel="noopener ugc nofollow" target="_blank"> NMF </a>(非负矩阵分解)来生成这些嵌入。</p><h2 id="0322" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">非负矩阵分解</h2><p id="530d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" rel="noopener ugc nofollow" target="_blank">非负矩阵因式分解</a> (NMF或NNMF)是一种算法，其中矩阵V被因式分解成(通常)两个矩阵W和H，其性质是所有三个矩阵都没有负元素。</p><p id="3838" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩阵<strong class="lb iu"> V </strong>被分解成两个矩阵<strong class="lb iu"> W </strong>和<strong class="lb iu"> H </strong>的乘积</p><p id="fe03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> V = W.H </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/1e015f963b4f0c2dbabde63a28b49970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6cogqtpvG89Ih-gRR4T_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵V分解成W &amp; H矩阵</p></figure><p id="3dc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，矩阵V的维数是4 * 6。我们可以把它看作一个有4个文档和6个单词的语料库。单元格Aᵢⱼ的每个条目是单词Wⱼ在文档Dᵢ.中出现的次数我们将文档的特征维度从6降低到2。因此，矩阵W的维数为4 * 2，矩阵H的维数为2* 6。</p><p id="6c1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从另一个角度来看，在新的映射中，每个文档由两个潜在特征表示，即矩阵w。矩阵中某个单元的值越高，该文档与潜在特征的关联程度越高。矩阵H定义了每个单词属于这些潜在特征的程度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/5d8f57d7def028a0a328769694adbc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gHalUcBMepALu0KShGeWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">直观矩阵分解</p></figure><p id="060f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用矩阵W作为嵌入或降维矩阵。降维使得对大量输入进行机器学习变得更加容易，比如表示单词的稀疏向量。在python中，我们可以简单地使用sklearn提供的NMF包。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用NMF创建V矩阵并将其分解为W和H</p></figure><p id="a4b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果，我们把Vᵀᴿᴬᴵᴺ分解成了Wᵀᴿᴬᴵᴺ和Hᵀᴿᴬᴵᴺ</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/2e855cb7c1197e3ee99aef5ee8fc91cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqLD_A5FYRvGNj7KM35eGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Wᵀᴿᴬᴵᴺ矩阵</p></figure><h2 id="0a6f" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated"><strong class="ak">步骤3:创建组嵌入</strong></h2><p id="e886" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们有5个群体:艺术家、运动员、科学家、政治家和领导人。我们从每个组中挑选了一些人，并创建了21个包含训练集的记录。我们将使用在Wᵀᴿᴬᴵᴺ为每个组学习的嵌入的平均值，并将其称为组/簇嵌入。</p><p id="7dbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，艺术家嵌入将作为:</p><p id="e24d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ly"> Artistsᵉᵐᵇᵉᵈᵈᶦⁿᵍ =平均水平(莱昂纳多·dicaprioᵉᵐᵇᵉᵈᵈᶦⁿᵍ，阿米塔布·bachchanᵉᵐᵇᵉᵈᵈᶦⁿᵍ，加尔·gadotᵉᵐᵇᵉᵈᵈᶦⁿᵍ，埃德·sheeranᵉᵐᵇᵉᵈᵈᶦⁿᵍ)</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成集群嵌入的代码</p></figure><h2 id="72d2" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">步骤4:为整个语料库生成嵌入</h2><p id="e417" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">测试集包含除了21个名人页面之外的所有剩余的105万个名人页面，我们已经将其作为标记的训练数据。现在，对于每个文档，我们需要生成一个嵌入，并且潜在特征需要与训练集的潜在特征相同。也就是说，我们需要产生Vᵀᴱˢᵀ、Wᵀᴱˢᵀ和Hᵀᴱˢᵀ.</p><p id="4507" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们一个一个地看每个矩阵</p><ol class=""><li id="8f76" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu"> Vᵀᴱˢᵀ </strong>:通过计算单词‘w’在文档中出现的次数，我们可以很容易地生成Vᵀᴱˢᵀ。只是我们需要小心，我们只采用我们在训练数据中看到的单词，并以相同的顺序忽略新单词。</li><li id="0cde" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu"> Hᵀᴱˢᵀ </strong>:矩阵Hᵀᴱˢᵀ代表每个单词属于潜在特征的多少。因此它和Hᵀᴿᴬᴵᴺ.是一样的即Hᵀᴱˢᵀ = Hᵀᴿᴬᴵᴺ</li><li id="4731" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">Wᵀᴱˢᵀ: 这是主要的也是唯一剩下的让我们去寻找的母体。我们将使用矩阵代数来找到它。让我们看看如何-</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/9759dd970c39fcec466918d22a3072b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojjNssQqjKr6Jp55T36NTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算Wᵀᴱˢᵀ的矩阵代数</p></figure><p id="3b5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在代码中，我们不需要接受inverse和all，因为包为我们做了这些，没有隐式地指定它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算Wᵀᴱˢᵀ的代码</p></figure><h2 id="a42b" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">步骤5:分配聚类概率</h2><p id="e01e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">最后一步是分配每个维基百科名人聚类分数，并将其转换为概率。为此，我们使用聚类嵌入对页面嵌入进行余弦相似度计算，然后归一化得分以获得概率。</p><p id="e0ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们必须给演员汤姆·汉克斯分配聚类分数，过程将是:</p><p id="aad0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分数计算</strong></p><p id="44a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">艺术家=余弦_sim(汤姆·汉克斯<em class="ly"> ᵉᵐᵇᵉᵈᵈᶦⁿᵍ </em>，<em class="ly"> Artistsᵉᵐᵇᵉᵈᵈᶦⁿᵍ) = 0.68 </em></p><p id="9a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SportsPerson = cosine_sim(汤姆·汉克斯<em class="ly"> ᵉᵐᵇᵉᵈᵈᶦⁿᵍ </em>，<em class="ly">sportspersonsᵉᵐᵇᵉᵈᵈᶦⁿᵍ)= 0.04</em></p><p id="d4ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">科学家=余弦_sim(汤姆·汉克斯<em class="ly"> ᵉᵐᵇᵉᵈᵈᶦⁿᵍ </em>，<em class="ly">scientistsᵉᵐᵇᵉᵈᵈᶦⁿᵍ)= 0.1</em></p><p id="9049" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">政客=余弦_sim(汤姆·汉克斯<em class="ly"> ᵉᵐᵇᵉᵈᵈᶦⁿᵍ </em>，<em class="ly">politiciansᵉᵐᵇᵉᵈᵈᶦⁿᵍ)= 0.01</em></p><p id="59d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">影响者=余弦_sim(汤姆·汉克斯<em class="ly"> ᵉᵐᵇᵉᵈᵈᶦⁿᵍ </em>，<em class="ly">influencersᵉᵐᵇᵉᵈᵈᶦⁿᵍ)= 0.1</em></p><blockquote class="lv lw lx"><p id="9be4" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">余弦相似度是一个总是在0到1之间的数字。0表示没有相似性，1表示完全相似</p></blockquote><p id="6abb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分数对概率的标准化</strong></p><p id="def3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">艺人概率= 0.68/(0.68+0.04+0.1+0.01+0.1)~ 0.72</p><p id="df3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运动员概率= 0.04/(0.68+0.04+0.1+0.01+0.1)~ 0.05</p><p id="1b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">科学家概率= 0.1/(0.68+0.04+0.1+0.01+0.1)~ 0.11</p><p id="9d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">政治家概率= 0.01/(0.68+0.04+0.1+0.01+0.1)~<em class="ly">0.01</em></p><p id="f70d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">影响者概率= 0.1/(0.68+0.04+0.1+0.01+0.1)~ 0.11</p><blockquote class="lv lw lx"><p id="4d95" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">概率加起来是1。汤姆·汉克斯是一名艺术家，有72%的机会被分配到艺术家群体。我们对所有的名人维基页面都这样做，并得到聚类概率。</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于分配聚类概率的代码</p></figure><p id="2cb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看看一些名人的这些概率。概率被转换成下面的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/185a2dbc22f3d22d3704b11a58d5ab7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Aw_YX_EkfIXa5DgeVKYOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些名人的百分比聚类概率。</p></figure><h1 id="bd92" class="mj mk it bd ml mm nu mo mp mq nv ms mt jz nw ka mv kc nx kd mx kf ny kg mz na bi translated">结束注释</h1><p id="7843" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们设计了一种非常新颖和有效的基于半监督嵌入的模糊聚类技术。这对于许多用例来说是相当普遍的。我们使用先验知识给集群一个形状和一个启动。</p><p id="d3c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以通过以下方式进一步改善结果:</p><ol class=""><li id="f9ac" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">使用提供的初始标记数据。我们可以提供更多高质量的数据来改善最终结果。</li><li id="215c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">播放或交叉验证正确的嵌入长度。</li><li id="ea11" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">设计一个更具创新性的策略来发现集群嵌入，而不仅仅是简单的平均。</li></ol><p id="ea9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果任务是分别标记每个运动项目的运动员，那么你的种子数据和算法会有什么变化？请评论你的想法。所有的代码和一小部分维基百科名人数据集可以在这里找到。</p><p id="493a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ly">我的Youtube频道更多内容:</em> </strong></p><div class="pa pb gp gr pc pd"><a href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener  ugc nofollow" target="_blank"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd iu gy z fp pi fr fs pj fu fw is bi translated">阿布舍克·蒙戈利</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">嗨，伙计们，欢迎来到频道。该频道旨在涵盖各种主题，从机器学习，数据科学…</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">www.youtube.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr ks pd"/></div></div></a></div><blockquote class="lv lw lx"><p id="c027" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">关于作者-: </strong></p><p id="eb2e" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">Abhishek Mungoli是一位经验丰富的数据科学家，拥有ML领域的经验和计算机科学背景，跨越多个领域并具有解决问题的思维方式。擅长各种机器学习和零售业特有的优化问题。热衷于大规模实现机器学习模型，并通过博客、讲座、聚会和论文等方式分享知识。</p><p id="be3a" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">我的动机总是把最困难的事情简化成最简单的版本。我喜欢解决问题、数据科学、产品开发和扩展解决方案。我喜欢在闲暇时间探索新的地方和健身。在<a class="ae ky" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="lb iu">中</strong> </a>、<strong class="lb iu"/><a class="ae ky" href="https://www.linkedin.com/in/abhishek-mungoli-39048355/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Linkedin</strong></a><strong class="lb iu"/>或<strong class="lb iu"/><a class="ae ky" href="https://www.instagram.com/simplyspartanx/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">insta gram</strong></a><strong class="lb iu"/>关注我，查看我<a class="ae ky" href="https://medium.com/@mungoliabhishek81" rel="noopener">以前的帖子</a>。我欢迎反馈和建设性的批评。我的一些博客-</p></blockquote><ul class=""><li id="ab4b" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu ps nm nn no bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/decomposing-a-time-series-in-a-simple-and-intuitive-way-19d3213c420b?source=---------7------------------">以简单&amp;直观的方式分解时间序列</a></li><li id="0e84" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ps nm nn no bi translated"><a class="ae ky" href="https://medium.com/walmartlabs/how-gpu-computing-literally-saved-me-at-work-fc1dc70f48b6" rel="noopener">GPU计算如何在工作中拯救了我？</a></li><li id="7623" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ps nm nn no bi translated">信息论&amp; KL散度<a class="ae ky" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">第一部分</a>和<a class="ae ky" rel="noopener" target="_blank" href="/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d">第二部分</a></li><li id="5f4e" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ps nm nn no bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25">使用Apache Spark处理维基百科，创建热点数据集</a></li><li id="4dda" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ps nm nn no bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/5-mistakes-every-data-scientist-should-avoid-bcc8142d7693">每个数据科学家都应该避免的五个错误</a></li><li id="b5e7" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ps nm nn no bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/compare-which-machine-learning-model-performs-better-4912b2ed597d">比较哪种机器学习模型表现更好</a></li></ul></div></div>    
</body>
</html>