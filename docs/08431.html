<html>
<head>
<title>PEGASUS: Google’s State of the Art Abstractive Summarization Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PEGASUS: Google 最先进的抽象摘要模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce?source=collection_archive---------16-----------------------#2020-06-19">https://towardsdatascience.com/pegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce?source=collection_archive---------16-----------------------#2020-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d3f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌人工智能如何生成人类级别的摘要</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5f819efc26a1038672a51e8626c77dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f1FmAwsoR1HpO1Nr"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">苏丹欧阳在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总结的能力评估一个人对一篇给定的文章或一种语言的理解。</p><blockquote class="lv"><p id="0662" class="lw lx it bd ly lz ma mb mc md me lu dk translated">也许对一个人智力的最好测试是他做总结的能力</p><p id="6e43" class="lw lx it bd ly lz ma mb mc md me lu dk translated">—李顿·斯特雷奇</p></blockquote><p id="49d5" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">因此，摘要在自然语言处理中是一个相当重要的概念。在<a class="ae ky" href="https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453" rel="noopener">这篇文章</a>中，我已经介绍了作为一个整体的摘要和抽象摘要以及它的实现。如果你有兴趣了解这项任务的简要背景，可以考虑读一读；飞马模型是在变形金刚架构上训练的。</p><p id="7500" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论谷歌人工智能最近提出的一篇论文，“<a class="ae ky" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank"> PEGASUS:使用提取的间隙句子进行抽象概括的预训练</a>，该论文预计将在<a class="ae ky" href="https://icml.cc/Conferences/2020" rel="noopener ugc nofollow" target="_blank"> ICML 2020 </a>上发表。</p><h1 id="6237" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">PEGASUS:用提取的间隔句进行抽象摘要的预训练</h1><p id="5f71" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">像任何其他序列转换任务一样，PEGASUS 也实施 seq2seq 体系结构。然而，这种架构的新颖之处在于其自我监督的预训练目标。</p><p id="4c3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自我监督学习是深度学习的新亮点。它基本上消除了数据对标记样本的依赖，并使大量未探索、未标记的数据可用于训练。</p><blockquote class="nh ni nj"><p id="ce0b" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">基于转换器的模型与自我监督的预训练(例如，<a class="ae ky" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae ky" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>、<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet </a>、<a class="ae ky" href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener ugc nofollow" target="_blank">艾伯特</a>、<a class="ae ky" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a>、<a class="ae ky" href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener ugc nofollow" target="_blank">伊莱克特拉</a>)的组合，已经被证明在整体语言建模任务中非常有效。</p></blockquote><h2 id="5a83" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">间断句生成(GSG):自动监督的摘要目标</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f1bd8c98a5ad1f02ace90293d13ce04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*E33QsB9AUmoDd3k2.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谷歌人工智能博客<a class="ae ky" href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html" rel="noopener ugc nofollow" target="_blank">在 PEGASUS 进行的自我监督预培训</a></p></figure><blockquote class="lv"><p id="4eec" class="lw lx it bd ly lz ob oc od oe of lu dk translated">这个目标背后的主要思想是假设预训练自我监督目标越接近最终的下游任务，微调性能越好</p></blockquote><p id="df15" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">因此，在 PEGASUS 中，<strong class="lb iu">完整的句子被从文档中删除</strong>(即它们被“屏蔽”)，并且<strong class="lb iu">模型被训练来预测这些句子</strong>，如图所示。作者承认，这项任务似乎几乎不可能完成，事实上对人类来说也是如此。但是这种训练对于具有原始文档实例的句子的生成引起了更高的理解感；从而支持他们的假设。这项任务被称为<strong class="lb iu">间隙句子生成(GSG)。</strong></p><p id="241a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，作者声称从文档中选择最重要的句子来屏蔽效果最好。这是通过根据称为<a class="ae ky" href="https://www.aclweb.org/anthology/W04-1013/" rel="noopener ugc nofollow" target="_blank"> ROUGE </a>(通常用于评估摘要任务中摘要的质量)的度量来找到与完整文档最相似的句子来完成的。</p><h2 id="e6ab" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">掩蔽语言模型(MLM)</h2><p id="d820" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">虽然飞马座的主要贡献是<strong class="lb iu"> GSG </strong>(上一节讨论过)，但它的基础架构由一个编码器和一个解码器组成；因此，将编码器预先训练为屏蔽语言模型是有意义的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/56799c1f911e68ceb9288597a7f32070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*d8GxaHclWmTx-P3x.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语言建模 v/s Masked 语言建模 by <a class="ae ky" href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener ugc nofollow" target="_blank"> Google AI 博客</a></p></figure><p id="0603" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这项任务中，我们随机屏蔽序列中的单词，并使用序列中的其他单词来预测这些被屏蔽的单词。GSG 任务可以被理解为一个文档级的 MLM，并且就是从这个概念中派生出来的。</p><p id="3a9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，正如<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT 论文</a>中所建议的，序列中 15%的单词被随机屏蔽，模型被训练来预测这些被屏蔽的单词。</p><h2 id="f272" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">综合训练</h2><p id="b28f" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">前面几节中讨论的两种方法都被合并，并且以组合的方式训练转换器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1acbc1126057a61af75c8854088fff94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*yp5xLaVL7vOs6YT9QPO2Ow.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLM(左)+ GSG(右)一起在飞马训练来自<a class="ae ky" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><blockquote class="nh ni nj"><p id="4ba1" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">GSG 和 MLM 作为预训练目标同时应用于此示例。本来有三句话。一个句子用[MASK1]屏蔽，用作目标生成文本(GSG)。其他两个句子保留在输入中，但是一些单词被[MASK2] (MLM)随机屏蔽。</p><p id="1af5" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">— <a class="ae ky" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank">飞马纸业</a></p></blockquote><h1 id="8904" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结果</h1><p id="e414" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">该模型在 12 个公共摘要数据集上进行了微调。在其中 6 个数据集上，它已经超过了之前的技术水平，令人惊讶的是，它只在很少的样本上进行训练。</p><h2 id="d7cd" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">微调</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8f53fcb30b4cccc05e64b71d963a18f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*U3sfReTId6ZA9YAi.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谷歌人工智能博客<a class="ae ky" href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html" rel="noopener ugc nofollow" target="_blank">对选定的 4 个数据集(虚线是未经预训练的完全监督模型的结果)的各种 ROUGE 指标</a></p></figure><p id="c7a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以清楚地看到，在最少 1000 个训练样本的情况下，PEGASUS 在这些数据集上已经超越并达到了最先进的水平。</p><h2 id="421e" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">人的素质总结</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2f26adfe333614f463f7642d655b0aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*28Zm8_XPyZa14YNj.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谷歌人工智能博客进行的人类评级测试</p></figure><p id="7df0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PEGASUS 还在 3 个数据集上取得了人类水平的结果。评估是通过对人工摘要和模型生成的摘要进行评级来完成的，而不知道哪一个是哪一个。</p><blockquote class="nh ni nj"><p id="274c" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">“我们用 3 个不同的数据集进行了实验，结果发现，与我们的模型相比，人类评分者并不总是更喜欢人类摘要”</p></blockquote><h2 id="4d72" class="no ml it bd mm np nq dn mq nr ns dp mu li nt nu mw lm nv nw my lq nx ny na nz bi translated">清点船只</h2><p id="d0fe" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这是飞马座取得的另一个有趣的结果:</p><p id="5185" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自 Xsum 数据集的一篇文章提出了 4 艘船的名字，即。英国皇家海军舰艇坎伯兰号、坎伯敦号、查塔姆号和康沃尔号。该模型正确地将其抽象为<em class="nk">、【四艘皇家海军护卫舰】、<em class="nk">、</em>，尽管这里没有提到样本中的数字“四”。</em></p><p id="68fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这是一种侥幸，作者通过从名单中增加或删除名字来测试这一点。如果有 2-5 个名字，该模型可以正确地提取数字。然而，它将 6 艘船误算为“7 艘”，这表明它只能提取列表中的一小部分名称。</p><p id="b088" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">演示可以在<a class="ae ky" href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><blockquote class="nh ni nj"><p id="09de" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated"><strong class="lb iu">有趣的事实:该模型仅使用 T5 的 5%的参数数量就取得了比 T5 等同类模型更好的结果。</strong></p></blockquote><h1 id="2e1c" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><p id="0273" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们已经讨论了 Google 的抽象摘要模型的工作状态。我们还看到了在一个与下游任务相对相似的任务上进行预训练如何极大地增强了模型在微调上的性能。这为比一般情况更具体地模拟自我监督预训练目标提供了可能性。</p><p id="5e3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码和检查点是开源的，可以在这里找到<a class="ae ky" href="https://github.com/google-research/pegasus" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="067f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">参考</h1><div class="oi oj gp gr ok ol"><a href="https://arxiv.org/abs/1912.08777" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">PEGASUS:用提取的间隔句进行抽象摘要的预训练</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">最近的工作预训练变压器与自我监督的目标对大型文本语料库显示了巨大的成功…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">PEGASUS:抽象文本摘要的最新模型</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">学生的任务通常是阅读一份文件并编写一份摘要(例如，读书报告)来展示…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">ai.googleblog.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">使用变压器的抽象文本摘要</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">对谷歌 Transformer 模型的详尽解释；从理论到实施</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ks ol"/></div></div></a></div></div></div>    
</body>
</html>