<html>
<head>
<title>The Hero Rises: Build Your Own SSD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">英雄崛起:打造自己的SSD</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-hero-rises-build-your-own-ssd-febfbdd3bd03?source=collection_archive---------52-----------------------#2020-04-19">https://towardsdatascience.com/the-hero-rises-build-your-own-ssd-febfbdd3bd03?source=collection_archive---------52-----------------------#2020-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b863" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">现实世界中的数据科学</h2><div class=""/><div class=""><h2 id="b6fb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度学习代码库系列的英雄之旅——IIB部分</h2></div><p id="8eb1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由<a class="ae ln" href="https://www.linkedin.com/in/dan-malowany-78b2b21/" rel="noopener ugc nofollow" target="_blank">丹·马洛万尼</a>和<a class="ae ln" href="https://www.linkedin.com/in/gal-hyams-2146a662/" rel="noopener ugc nofollow" target="_blank">加尔·海姆斯</a><br/>T5】创作的快板艾团队</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/6f11d2ae6f08d5f78fd91440ddb6ac61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2US3ifFa-vXm_FnDGerXBA.jpeg"/></div></div></figure><p id="5fdc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着最先进的模型不断变化，人们需要有效地编写模块化的机器学习代码库，以支持和维持R&amp;D机器和深度学习多年的努力。在本系列的第一篇博客中，我们展示了如何编写一个可读和可维护的代码来训练Torchvision MaskRCNN模型，利用Ignite的框架。在我们的<a class="ae ln" rel="noopener" target="_blank" href="/the-battle-of-speed-vs-23b61eb4225d">第二篇文章(IIA部分)</a>中，我们详细介绍了单触发和双触发探测器之间的根本区别，以及为什么单触发方法是速度/精度权衡的最佳选择。因此，在这篇文章中，我们很自然地收集了如何利用MaskRCNN代码库的模块化特性，并使其能够训练MaskRCNN和SSD模型。由于代码库的模块化性质，只需要对代码进行最小的修改。</p><p id="dc11" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ln" href="https://pytorch.org/docs/stable/torchvision/index.html" rel="noopener ugc nofollow" target="_blank"> Torchvision </a>是一个由流行的数据集、模型架构和计算机视觉的通用图像转换组成的包。除其他外，它还包含一个预先训练好的模型动物园，用于图像分类、对象检测、人物关键点检测、语义分割和实例分割模型，随时可供开箱即用。这使得PyTorch用户的生活变得更加容易，因为它缩短了想法和产品之间的时间。或者一篇研究论文。或者一篇博文。</p><p id="e30b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Torchvision不包含单镜头对象检测模型的实现，例如这个流行的<a class="ae ln" href="https://link.springer.com/chapter/10.1007/978-3-319-46448-0_2" rel="noopener ugc nofollow" target="_blank"> SSD </a> <strong class="kt jd">。因此，我们添加了一个:</strong>基于Torchvision模型的SSD实现，作为特征提取的主干。自发布以来，在最初的SSD上进行了许多改进。然而，为了清晰和简单起见，我们将重点放在了最初的SSD元架构上。让我们深入研究一下实现的逻辑和方法。完整的代码可以在<a class="ae ln" href="https://github.com/allegroai/trains-blogs/tree/master/the_hero_rises" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。</p><h1 id="7ee5" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">常量:从默认值开始</h1><p id="235d" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">首先，我们把手电筒放在代码常量上，它们是<a class="ae ln" href="https://github.com/allegroai/trains-blogs/blob/fd92c462cbbceb8d4027aec2cf9acd5579117fe1/the_hero_rises/SSD/ssd_model.py#L113" rel="noopener ugc nofollow" target="_blank"> SSD类构造函数</a>的默认输入参数。这些是为PASCAL-VOC数据集定制的512×512输入图像的常见值。(在本系列的第三部分中，我们将演示如何根据您自己的数据集调整这些值)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mz"><img src="../Images/851a5708fbabe7b14dee80d452b4837e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ML9eJF-t9U3KVmlzJ6d6DQ.jpeg"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">图1: SSD元架构和多特征图计算— <a class="ae ln" href="https://link.springer.com/chapter/10.1007/978-3-319-46448-0_2" rel="noopener ugc nofollow" target="_blank"> SSD论文</a></p></figure><p id="3adb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些<a class="ae ln" href="https://github.com/allegroai/trains-blogs/blob/fd92c462cbbceb8d4027aec2cf9acd5579117fe1/the_hero_rises/SSD/ssd_model.py#L17" rel="noopener ugc nofollow" target="_blank">列表中的每一个</a>包含7个条目——每个特征图一个条目，从该特征图中进行对象检测(见上面的<em class="ne">图1 </em>)。注意，列表之一BOX_SIZES有8个条目，实际的框尺寸<a class="ae ln" href="https://github.com/allegroai/trains-blogs/blob/fd92c462cbbceb8d4027aec2cf9acd5579117fe1/the_hero_rises/SSD/box_coder.py#L13" rel="noopener ugc nofollow" target="_blank">计算</a>是基于这些值执行的。</p><p id="c41e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如<em class="ne">图2 </em>(上图)所示，矩形和方形锚点平铺特征图。<em class="ne"> aspect_ratio </em>列表包含每个特征图的矩形纵横比列表。这个列表中的每个数字为每个先前的中心定义了两个矩形:一个具有提到的纵横比，另一个具有相反的纵横比。也就是说，对于每个2:1的“躺着的”矩形锚，我们也有一个1:2的“站着的”矩形锚。除了矩形锚之外，两种尺寸的方形锚被设置在每个先前的中心上。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="a37b" class="nk md it ng b gy nl nm l nn no"><em class="ne"># The size ratio between the current layer and the original image.<br/># I.e, how many pixel steps on the original image are equivalent to a single pixel step on the feature map.</em><br/><br/>STEPS = (8, 16, 32, 64, 128, 256, 512)<br/><br/><em class="ne"># Length of the smaller anchor rectangle, for each feature map.</em><br/><br/>BOX_SIZES = (35.84, 76.8, 153.6, 230.4, 307.2, 384.0, 460.8, 537.6)<br/><br/><em class="ne"># Aspect ratio of the rectangular SSD anchors, besides 1:1</em><br/><br/>ASPECT_RATIOS = ((2,), (2, 3), (2, 3), (2, 3), (2, 3), (2,), (2,))<br/><br/><em class="ne"># feature maps sizes.</em><br/><br/>FM_SIZES = (64, 32, 16, 8, 4, 2, 1)<br/><br/><em class="ne"># Amount of anchors for each feature map</em><br/><br/>NUM_ANCHORS = (4, 6, 6, 6, 6, 4, 4)<br/><br/><em class="ne"># Amount of each feature map channels, i.e third dimension.</em><br/><br/>IN_CHANNELS = (512, 1024, 512, 256, 256, 256, 256)</span></pre><h1 id="a8a6" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">SSD类构造函数</h1><p id="9789" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">这个<em class="ne"> SSD </em>类产生一个基于Torchvision特征提取器的SSD对象检测模型，参数如上所述。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="ffcd" class="nk md it ng b gy nl nm l nn no"><strong class="ng jd">class SSD</strong>(nn.Module):<br/>   <strong class="ng jd">def __init__</strong>(<strong class="ng jd">self</strong>, backbone, num_classes, loss_function,<br/>                num_anchors=NUM_ANCHORS,<br/>                in_channels=IN_CHANNELS,<br/>                steps=STEPS,<br/>                box_sizes=BOX_SIZES,<br/>                aspect_ratios=ASPECT_RATIOS,<br/>                fm_sizes=FM_SIZES,<br/>                heads_extractor_class=HeadsExtractor):<br/><br/><strong class="ng jd">super</strong>(SSD, <strong class="ng jd">self</strong>).__init__()<br/>...<br/><strong class="ng jd">self</strong>.extractor = heads_extractor_class(backbone)<br/><strong class="ng jd">self</strong>.criterion = loss_function<br/><strong class="ng jd">self</strong>.box_coder = SSDBoxCoder(self.steps, self.box_sizes, self.aspect_ratios, <strong class="ng jd">self</strong>.fm_sizes)<br/><br/><strong class="ng jd">self</strong>._create_heads()</span></pre><h1 id="3bc7" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">创建分类和本地化负责人</h1><p id="6818" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">下面，我们首先将特征映射的分解从SSD模型中分离出来，使SSD能够轻松适应调整后的特征映射提取器。如果您确实要修改SSD，那么在调用SSD构造函数时，不要忘记修改相关的参数。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="6050" class="nk md it ng b gy nl nm l nn no">class HeadsExtractor(nn.Module):<br/>   def __init__(self, backbone):<br/>       super(HeadsExtractor, self).__init__()<br/><br/>       def split_backbone(net):<br/>           features_extraction = [x for x in net.children()][:-2]<br/>          <br/>           if type(net) == torchvision.models.vgg.VGG:<br/>               features_extraction = [*features_extraction[0]]<br/>               net_till_conv4_3 = features_extraction[:-8]<br/>               rest_of_net = features_extraction[-7:-1]<br/>           elif type(net) == torchvision.models.resnet.ResNet:<br/>               net_till_conv4_3 = features_extraction[:-2]<br/>               rest_of_net = features_extraction[-2]<br/>           else:<br/>               raise ValueError('We only support VGG and ResNet')<br/>           return nn.Sequential(*net_till_conv4_3), nn.Sequential(*rest_of_net)<br/><br/>       self.till_conv4_3, self.till_conv5_3 = split_backbone(backbone)<br/>       self.norm4 = L2Norm(512, 20)<br/><br/>       self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)<br/>       self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)<br/>       self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)<br/><br/>       self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)<br/>       self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)<br/><br/>       self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1)<br/>       self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)<br/><br/>       self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1)<br/>       self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2)<br/><br/>       self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1)<br/>       self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2)<br/><br/>       self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1)<br/>       self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2)<br/><br/>       self.conv12_1 = nn.Conv2d(256, 128, kernel_size=1)<br/>       self.conv12_2 = nn.Conv2d(128, 256, kernel_size=4, padding=1</span></pre><p id="2bca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SSD模型共享所有的分类和定位计算，直到最终的内容分类器和空间回归器。<em class="ne"> create_heads </em>方法在每个特征图的顶部创建SSD分类和定位头，产生每锚预测。对于每个锚，定位头预测向量移位和拉伸(cx，xy，w，h)，而分类头预测每类概率的向量。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="2b86" class="nk md it ng b gy nl nm l nn no"><strong class="ng jd">def _create_heads</strong>(<strong class="ng jd">self</strong>):<br/>       <strong class="ng jd">self</strong>.loc_layers = nn.ModuleList()<br/>       <strong class="ng jd">self</strong>.cls_layers = nn.ModuleList()<br/>       <strong class="ng jd">for</strong> i <strong class="ng jd">in</strong> <strong class="ng jd">range</strong>(<strong class="ng jd">len</strong>(<strong class="ng jd">self</strong>.in_channels)):<br/><strong class="ng jd">       self</strong>.loc_layers += [nn.Conv2d(<strong class="ng jd">self</strong>.in_channels[i], <strong class="ng jd">self</strong>.num_anchors[i] * 4, kernel_size=3, padding=1)]<br/><strong class="ng jd">       self</strong>.cls_layers += [nn.Conv2d(<strong class="ng jd">self</strong>.in_channels[i], <strong class="ng jd">self</strong>.num_anchors[i] * <strong class="ng jd">self</strong>.num_classes, kernel_size=3<strong class="ng jd">, </strong>padding=1)]</span></pre><p id="2615" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SSD模型从最高分辨率到最低分辨率建立了一个特征图的层次结构，并检测每个特征图上的对象。<em class="ne">头部提取器</em>类放置特征地图，并使其可用于检测器。其命名基于VGG-16特征提取器(其中<em class="ne"> conv4_3 </em>是用作SSD模型特征图的最高分辨率层的名称)。</p><p id="848b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">不同的数据集和影像大小最适合调整后的要素地图等级；小图像不需要像大图像那样多不同的特征地图。类似地，没有小对象的数据集可以避免高分辨率的特征地图(加速模型计算时间)。</p><h1 id="4b07" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">定义SSD正向传递</h1><p id="dcf8" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">在下面的方法中，计算SSD模型上的图像批次的正向传递，并返回其结果。</p><p id="7edc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果模型处于评估模式，则正向传递返回输入图像上的模型预测。但是，如果在训练模式下进行正向传递，则只返回损失。这是一种常见的设计，它只返回损失，比返回所有检测的计算效率更高。</p><p id="5243" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用这种方法，<em class="ne"> extracted_batch </em>参数保存图像批次的布局特征图，然后分别计算跨每个特征图的预测。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="5b9b" class="nk md it ng b gy nl nm l nn no"><strong class="ng jd">def forward</strong>(<strong class="ng jd">self</strong>, images, targets=None):<br/>       <strong class="ng jd">if</strong> <strong class="ng jd">self</strong>.training <strong class="ng jd">and</strong> targets <strong class="ng jd">is</strong> None:<br/>           <strong class="ng jd">raise</strong> ValueError("In training mode, targets should be passed")<br/>       loc_preds = []<br/>       cls_preds = []<br/>       input_images = torch.stack(images) <strong class="ng jd">if</strong> <strong class="ng jd">isinstance</strong>(images, <strong class="ng jd">list</strong>) <strong class="ng jd">else</strong> images<br/>       extracted_batch = <strong class="ng jd">self</strong>.extractor(input_images)<br/>       <strong class="ng jd">for</strong> i, x <strong class="ng jd">in</strong> <strong class="ng jd">enumerate</strong>(extracted_batch):<br/>           loc_pred = <strong class="ng jd">self</strong>.loc_layers[i](x)<br/>           loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous()<br/>           loc_preds.append(loc_pred.view(loc_pred.size(0), -1, 4))<br/><br/>           cls_pred = <strong class="ng jd">self</strong>.cls_layers[i](x)<br/>           cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous()<br/>           cls_preds.append(cls_pred.view(cls_pred.size(0), -1, <strong class="ng jd">self</strong>.num_classes))<br/><br/>       loc_preds = torch.cat(loc_preds, 1)<br/>       cls_preds = torch.cat(cls_preds, 1)<br/><br/>      <strong class="ng jd">if</strong> <strong class="ng jd">self</strong>.training:<br/>           encoded_targets = [<strong class="ng jd">self</strong>.box_coder.encode(target['boxes'], target['labels']) <strong class="ng jd">for</strong> target <strong class="ng jd">in</strong> targets]<br/>           loc_targets = torch.stack([encoded_target[0] <strong class="ng jd">for</strong> encoded_target <strong class="ng jd">in</strong> encoded_targets])<br/>           cls_targets = torch.stack([encoded_target[1] <strong class="ng jd">for</strong> encoded_target <strong class="ng jd">in</strong> encoded_targets])<br/>           losses = <strong class="ng jd">self</strong>.criterion(loc_preds, loc_targets, cls_preds, cls_targets)<br/>           <strong class="ng jd">return</strong> losses<br/><br/>       detections = []<br/><br/>       <strong class="ng jd">for</strong> batch, (loc, cls) <strong class="ng jd">in</strong> <strong class="ng jd">enumerate</strong>(<strong class="ng jd">zip</strong>(loc_preds.split(split_size=1, dim=0),<br/><br/>                                              cls_preds.split(split_size=1, dim=0))):<br/><br/>           boxes, labels, scores = <strong class="ng jd">self</strong>.box_coder.decode(loc.squeeze(), F.softmax(cls.squeeze(), dim=1))<br/><br/>           detections.append({'boxes': boxes, 'labels': labels, 'scores': scores})<br/><br/>       <strong class="ng jd">return</strong> detections</span></pre><h1 id="e86c" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">将SSD模型连接到代码库</h1><p id="225d" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">为了调整<a class="ae ln" href="https://github.com/allegroai/trains-blogs/blob/master/once_upon_a_repository/train_model.py" rel="noopener ugc nofollow" target="_blank"> MaskRCNN代码库</a>的训练和评估脚本，并使其能够训练MaskRCNN和SSD模型，我们将以下条目添加到配置数据中(手动或通过<a class="ae ln" href="https://github.com/allegroai/trains-server" rel="noopener ugc nofollow" target="_blank"> Trains Server </a> web app)。</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="5557" class="nk md it ng b gy nl nm l nn no">'model_type': 'ssd', 'ssd_backbone': 'resnet50'</span></pre><p id="8461" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果您查看一下train脚本，您会发现除了上述配置数据更改之外，原始MaskRCNN脚本和新脚本(也支持SSD)之间的唯一区别是模型对象定义部分:</p><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="f168" class="nk md it ng b gy nl nm l nn no"><em class="ne"># Get the relevant model based in task arguments</em><br/><br/>   <strong class="ng jd">if</strong> configuration_data.get('model_type') == 'maskrcnn':<br/>       model = get_model_instance_segmentation(num_classes, configuration_data.get('mask_predictor_hidden_layer'))<br/>   <strong class="ng jd">elif</strong> configuration_data.get('model_type') == 'ssd':<br/>       backbone = get_backbone(configuration_data.get('backbone'))<br/>       model = SSD(backbone=backbone, num_classes=num_classes, loss_function=SSDLoss(num_classes))<br/>       model.dry_run(torch.rand(size=(1, 3, configuration_data.get('image_size'), configuration_data.get('image_size')))*255)<br/>   <strong class="ng jd">else</strong>:<br/>       <strong class="ng jd">raise</strong> ValueError('Only "maskrcnn" and "ssd" are supported as model type')</span></pre><p id="fb20" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这意味着这个代码库中所有剩余的资产都保持不变。从R&amp;D资源的角度来看，这是一个巨大的优势。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi np"><img src="../Images/b780bd409439e5760ecb4dbdbe884a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gs4Zfu0uv7STq51dA9V0Dg.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">本系列第一部分中的训练脚本和本文中的训练脚本的比较，展示了这个代码库的模块化本质。</p></figure><h1 id="c2b4" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">快板火车——坐下来，放松和监控你的实验</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nq"><img src="../Images/577b65fa7aecdcfe9a4f9b284a02fc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ERN1Fb5ewPmn-A_hBlHR9w.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">培训期间Trains web app的快照，显示标量(损失、学习率等。)前进</p></figure><p id="4296" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与<a class="ae ln" href="https://github.com/allegroai/trains-blogs/tree/master/once_upon_a_repository" rel="noopener ugc nofollow" target="_blank">原代码库</a>一样，使用<a class="ae ln" href="https://github.com/allegroai/trains" rel="noopener ugc nofollow" target="_blank"> Allegro Trains </a>，一个开源实验&amp; autoML manager，让我们实时监控训练过程，包括:学习率、损耗、val数据集上的mAP等。此外，Allegro Trains会在训练过程中自动监控CPU、GPU和计算机信息。这一重要工具有助于识别内存泄漏、硬盘空间不足、GPU利用率低等问题。</p><p id="27e9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于包括脚本参数在内的所有这些信息都记录在<a class="ae ln" href="https://github.com/allegroai/trains-server" rel="noopener ugc nofollow" target="_blank"> Trains服务器</a>中，因此可以比较不同的训练课程，并识别出产生优异结果的超参数。</p><h1 id="9df3" class="mc md it bd me mf mg mh mi mj mk ml mm ki mn kj mo kl mp km mq ko mr kp ms mt bi translated">结论</h1><p id="9cf0" class="pw-post-body-paragraph kr ks it kt b ku mu kd kw kx mv kg kz la mw lc ld le mx lg lh li my lk ll lm im bi translated">在<a class="ae ln" href="https://allegro.ai/blog/the-battle-of-speed-accuracy-single-shot-vs-two-shot-detection/" rel="noopener ugc nofollow" target="_blank">上一篇文章(IIA) </a>中，我们深入探讨了单触发探测器相对于双触发探测器的优势。在这里，我们将这些知识编写成代码，并在Torchvision预训练的backbone之上创建一个SSD模型，您可以根据自己的目的使用它。此外，我们展示了遵循本系列指南编写可维护的模块化代码库的优势。</p><p id="8f23" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">完整代码可在<a class="ae ln" href="https://github.com/allegroai/trains-blogs/tree/master/the_hero_rises" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。这里介绍的SSD类的部分是基于<a class="ae ln" href="https://github.com/kuangliu/torchcv/tree/master/examples/ssd" rel="noopener ugc nofollow" target="_blank">这个</a>写得很好的SSD实现。谢谢匡柳😉</p><p id="39db" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一篇文章中，我们将向您展示如何优化SSD模型，并根据您的数据进行调整。敬请期待！</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="ac4d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ne">原载于2020年4月19日</em><a class="ae ln" href="https://allegro.ai/blog/the-hero-rises-build-your-own-ssd/" rel="noopener ugc nofollow" target="_blank"><em class="ne">https://allegro . ai</em></a><em class="ne">。</em></p></div></div>    
</body>
</html>