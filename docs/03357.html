<html>
<head>
<title>Value Function Approximation — Control Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">价值函数逼近—控制方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/value-function-approximation-control-methods-a7a18e46ba29?source=collection_archive---------46-----------------------#2020-03-30">https://towardsdatascience.com/value-function-approximation-control-methods-a7a18e46ba29?source=collection_archive---------46-----------------------#2020-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="df48" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">强化学习之旅</a></h2><div class=""/><div class=""><h2 id="b3af" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">未知、无模型环境中的控制算法</h2></div><p id="e948" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">欢迎回到我的强化学习专栏。今天，我们将继续建立在我的<a class="ae ln" rel="noopener" target="_blank" href="/value-function-approximation-prediction-algorithms-98722818501b">上一篇关于价值函数逼近的文章</a>的基础上。在上一篇文章中，我们展示了价值函数逼近中使用的政策预测方法，这一次，我们将看看控制方法。</p><p id="b948" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我在<a class="ae ln" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">专栏</a>的旧帖子中提到了这些帖子中使用的很多符号。此外，像往常一样，为我提供了对强化学习世界的惊人见解的资源将在帖子底部链接到。事不宜迟，让我们更深入地探讨价值函数逼近。</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="5fa9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这一点上，广义策略迭代对我们来说应该不是新概念。这里的变化将是使用<em class="lv">近似</em>策略评估。我们将从一些参数向量<em class="lv"> w </em>开始，定义一些值函数。然后，我们将贪婪地进行一点探索，关于价值函数，给我们一个新的政策。然后，为了评估这个新策略，我们更新我们的价值函数的参数，不断重复这个过程，直到我们(希望)收敛到一个最优的价值函数。下图说明了这一过程。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/4d4fea9a9793ff0f0c2e7cfe23dd64bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/0*AckGJkENGSZ-hWRb.png"/></div></figure><p id="c8d1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你会注意到，像以前一样，我们不需要一直走到顶线，或者换句话说，浪费时间/经验样本去尝试精确地拟合我们的函数逼近器。在我们的政策稍作调整后，我们立即行动，掌握最新的数据。</p><p id="e480" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上述算法的问题是可能找不到最佳值函数，因为在现实中，我们只是越来越接近近似值函数。</p><p id="a7d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们的第一步将是近似动作值函数，而不是状态值函数。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/06b7a481fab3e62e6d526d2da1e2a85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*mI0S4ZWGnwewhcLZ8zCvjg.png"/></div></figure><p id="5eea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于每一个状态和任何行为，用参数<em class="lv"> w </em>，我们建立一个函数，预测我们期望从那个状态和行为中得到多少回报。</p><p id="d6b8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们最小化近似动作值函数和真实动作值函数之间的均方误差。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/8007c9cef9c19925b797f37dfb1ff6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*b8yhgttW8THzFVESGW67gw.png"/></div></figure><p id="1e0e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用链式法则和随机梯度下降，我们找到一个局部最小值:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/e94eb258b2c8b0114919d559d84c9bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*qPGMVGhD98Oqwy_HtYCe6g.png"/></div></figure><p id="599f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们考虑最简单的情况，使用<em class="lv">线性</em>动作值函数近似。我们构建一个特征向量来表示状态<em class="lv">和</em>动作:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/236c2be93b289e490391b34cf0fe01cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*5UPTsSwdLbwiP0Y2b8b2rw.png"/></div></figure><p id="18fe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些特征解释了整个状态-动作空间。我们通过构建特征的线性组合来做到这一点，但我们也可以使用更复杂的系统，如神经网络。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9d8dd6d81cbe9f71829da8f39efff092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*nVySpt2Pz1t-lFRbEV25sw.png"/></div></figure><p id="f736" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，梯度下降更新收缩为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b7e198ed5ac90dcff9c5b81ded802e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*hkayfDIdDEz4t7HJ2PbgFA.png"/></div></figure></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="f652" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们来看一个例子。下面的例子是强化学习中最广泛使用的问题之一，也是一个有趣的问题。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/37f3c8c471bc06dce607f8eb2c0a7fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*U1IhVYjRBart6K2jNQMAqw.png"/></div></figure><p id="08b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的想法是这辆车陷入了困境。汽车没有足够的动力直接开上山坡到达目的地，所以它必须向上开一点，松手，允许自己向后摆动通过下坡和后面的山坡，获得一些动量，并不断重复这个过程，直到它有足够的动量到达顶部。我们想要实现这个目标，即在不知道世界动力学(如重力、摩擦力等)的情况下，无模型地计算出控制策略。</p><p id="f59e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我们来定义这个问题的状态空间。这里的状态空间是汽车的位置，以及速度，使它成为一个二维的状态空间。</p><p id="87f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面的伪代码显示了一个用于逼近最优值函数的分段 Sarsa TD 控制方法。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/e74d1b809491b1359aa9321c288b6f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PUGniLUYFgQleU5g4vXFWg.png"/></div></div></figure><p id="eb59" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在让我们的山地汽车代理运行 100 集之后，结果显示了不可否认的学习:</p><div class="lx ly lz ma gt ab cb"><figure class="mq mb mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/bdfee5d672eae6bc93ea28d48f440d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*U6C2LKAf2wEybW35Ooz3Mg.png"/></div></figure><figure class="mq mb mw ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/605902ce6a7c17a1acb9edd32b37d7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*PsR8JyTxBx5zQGMY8C8dag.png"/></div></figure></div><div class="ab cb"><figure class="mq mb mx ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/51e07683ce62b282ac5cea6d5789711c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*uFYvfqixYvxLOGAKs-7eDQ.png"/></div></figure><figure class="mq mb my ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/1d86bcef54a34f428173e610bbe3ffb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*VBSmzJQPujEU_UMoI13O0w.png"/></div></figure></div><p id="3afc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢你再次加入我对强化学习知识的探索。从这里开始只会变得更好，因为下一次我们将着眼于<em class="lv">深度 Q 学习</em>，使用深度神经网络作为函数逼近器，建立在这篇文章和上一篇文章的概念之上。期待再次学习！</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><h1 id="1919" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated"><span class="l nr ns nt bm nu nv nw nx ny di"> R </span>资源</h1><p id="e28e" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated"><a class="ae ln" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">强化学习:简介</strong>作者<em class="lv">萨顿和</em>巴尔托</a></p><p id="5e8a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">YouTube 上大卫·西尔弗的 RL 课程</p><p id="6263" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ln" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">强化学习 Github </a>由<em class="lv"> dennybritz </em></p><p id="870b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ln" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> OpenAI </em>旗下<em class="lv">健身房</em>T21】</a></p></div></div>    
</body>
</html>