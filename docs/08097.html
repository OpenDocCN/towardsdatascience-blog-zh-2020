<html>
<head>
<title>BERT NLP — How To Build a Question Answering Bot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BERT NLP——如何构建问答机器人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b?source=collection_archive---------6-----------------------#2020-06-15">https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b?source=collection_archive---------6-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f39" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过亲自动手的PyTorch代码理解直觉，BERT在SQuAD上进行了微调。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e46794246b2bc5ba43fcc0fbc675aac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPoRimadRoXf1_66To7LCg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯蒂夫·约翰森在<a class="ae ky" href="https://unsplash.com/photos/6ZvMJlNF4YU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的抽象画</p></figure><p id="b9bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将介绍基于神经网络创建和编码问答系统的关键思想。实现使用谷歌的语言模型，称为预训练伯特。实践证明PyTorch代码的问题回答与伯特微调和班是在文章的最后提供。</p><h1 id="d9fe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是问答？</h1><p id="61f1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在问答任务中，模型接收关于文本内容的问题，并被要求在文本中标记答案的开始和结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/42c4240de1d37d5bea0022af363de36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQK2KJvQyPE5vpS9qGJ8BQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带答案的文本和相关问题示例</p></figure><p id="c260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，问题<em class="mt">的答案是“除了SCN细胞之外，在哪里还发现了独立的昼夜节律？”</em>位于红色突出显示的位置。问题<em class="mt">“独立时钟的术语是什么？”</em>在蓝色位置回答。</p><p id="e5d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们有一个非常大的这样的文本集，以及样本问题和答案在文本中的位置，我们可以训练一个神经网络来学习上下文、问题和答案之间的关系。给定与训练文本相似的新上下文，由此产生的网络将能够回答看不见的问题。</p><p id="7975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几十年来，机器阅读理解一直吸引着计算机科学家的注意力。最近大规模标记数据集的产生使研究人员能够建立受监督的神经系统，自动回答用自然语言提出的问题。</p><h1 id="39bd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">小队数据集</h1><p id="c404" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><a class="ae ky" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">斯坦福问答数据集(SQuAD) </a>是用于阅读理解的大规模标记数据集的主要例子。Rajpurkar等人开发了SQuAD 2.0，它结合了一组维基百科文章中关于同一段落的10万个可回答问题和5万个无法回答的问题。这些无法回答的问题是由人群工作者以敌对的方式写出来的，看起来与可回答的问题相似。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/9a4a42150b6da4fda80b33a08510e01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZpiYdw6aigSzpby6dVF5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">小队数据集的快照</p></figure><p id="0198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">doc_tokens</code>描述上下文，即我们希望模型理解的文本。</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="2ab9" class="nd lw it my b gy ne nf l ng nh">At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for "Halo", and Song of the Year for "Single Ladies (Put a Ring on It)", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single "Telephone" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga, tying them with Mariah Carey for most number-ones since the Nielsen Top 40 airplay chart launched in 1992. "Telephone" received a Grammy Award nomination for Best Pop Collaboration with Vocals.</span></pre><p id="5081" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">question_text</code>描述应该从上下文中回答的问题。</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="52d0" class="nd lw it my b gy ne nf l ng nh">How many awards was Beyonce nominated for at the 52nd Grammy Awards?</span></pre><p id="5c5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">orig_answer_text</code>代表问题的正确答案。</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="97dd" class="nd lw it my b gy ne nf l ng nh">ten</span></pre><p id="ccc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答案总是从上下文中开始于<code class="fe mv mw mx my b">start_position </code>并结束于<code class="fe mv mw mx my b">end_position</code>的部分。如果该问题在上下文中没有任何答案，<code class="fe mv mw mx my b">is_impossible </code>的值为<code class="fe mv mw mx my b">true</code>。</p><h1 id="a224" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">问答中的迁移学习</h1><p id="728b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">SQuAD数据集提供了15万个问题，这在深度学习世界中并不算多。迁移学习背后的想法是采用一个在非常大的数据集上训练的模型，然后使用SQuAD数据集对该模型进行微调。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/050ab4bdc210f5ba7073aeaf40adeb2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDPed1_pVXQNnyfZf44N5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERT的整体预培训和微调程序。图片由<a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> Jacob Devlin等人@ Google AI Language </a>(来源:Arxiv上的原始BERT论文)</p></figure><p id="40f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT是一个训练有素的<a class="ae ky" rel="noopener" target="_blank" href="/lost-in-translation-found-by-transformer-46a16bf6418f"> Transformer </a>编码器堆栈，基本版有12个，大版有24个。伯特接受了维基百科和图书语料库的培训，图书语料库包含超过10，000本不同流派的书籍。我将在下面的文章中详细介绍Transformer架构。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/lost-in-translation-found-by-transformer-46a16bf6418f"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">伯特解释道。迷失在翻译中。被变形金刚发现。</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">打造下一个聊天机器人？伯特，GPT-2:解决变压器模型的奥秘。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ks nm"/></div></div></a></div><p id="821d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用BERT从小队文本中提取高质量的语言特征，只需在顶部添加一个线性层。线性图层有两个输出，第一个输出用于预测当前子笔画是答案的开始位置的概率，第二个输出用于预测答案的结束位置。</p><p id="3f41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面你可以找到一个模型的总结。为了简单起见，我只显示了第一个和最后一个编码器层。同一层通常会重复12次。我们可以在开头看到<em class="mt">Bert embeddeds</em>层，后面是每个编码器层的Transformer架构:<em class="mt"> BertAttention </em>，<em class="mt"> BertIntermediate </em>，<em class="mt"> BertOutput </em>。最后，我们有两个输出的<em class="mt"> BertPooler </em>和<em class="mt">线性</em>层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="39b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练模型相对简单。底层已经有了很好的英语单词表示，我们只需要训练顶层，在底层进行一些调整来适应我们的问答任务。为此，您可以定制我在下面的文章中提供的代码。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/bert-for-dummies-step-by-step-tutorial-fb90890ffe03"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">伯特为假人-一步一步教程</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">变压器DIY实用指南。经过实践验证的PyTorch代码，用于对BERT进行微调的意图分类。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="od l nx ny nz nv oa ks nm"/></div></div></a></div><p id="eb2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能期望F1分数在74%左右。如果我们查看当前的<a class="ae ky" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> SQuAD 1.0排行榜</a>，我们会看到对测试数据集的评估使我们进入了前100名，鉴于免费GPU上可用的资源有限，这是可以接受的。</p><p id="f441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微调后的模型可用于对我们选择的文本和问题进行推理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7d6708eb590e48944aa428b140dee896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*5n6IKqrBLXxUV8ocr3L3iw.png"/></div></figure><h1 id="310d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="08f8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我解释了如何在SQUaD数据集上微调预训练的BERT模型，以解决任何文本上的问答任务。</p><p id="f087" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以用BERT 改编我的<a class="ae ky" href="https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> PyTorch代码用于NLU来解决你的问答任务。</a></p><p id="2f7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些读者可能会发现这本<a class="ae ky" href="https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">谷歌Colab笔记本</a>中的完整代码更加简单明了。功劳归于<a class="ae ky" href="https://www.linkedin.com/in/prachur-bhargava-83478a4/" rel="noopener ugc nofollow" target="_blank">微软首席数据科学家Prachur Bhargava </a>和<a class="ae ky" href="https://www.linkedin.com/in/himanshu-mohan-869561137/" rel="noopener ugc nofollow" target="_blank">美国运通软件工程师Himanshu Mohan </a>，他们优雅地解决了在Google Colab上训练模型的内存问题。谢谢你，Prachur和Himanshu！</p><p id="1745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在我下面的文章中了解更多关于语言模型的知识。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/representing-text-in-natural-language-processing-1eead30e57d8"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">自然语言处理中的文本表示</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">理解书面单词:温习Word2vec、GloVe、TF-IDF、单词袋、N-grams、1-hot编码…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="of l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/bert-for-dummies-step-by-step-tutorial-fb90890ffe03"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">伯特为假人-一步一步教程</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">变压器DIY实用指南。经过实践验证的PyTorch代码，用于对BERT进行微调的意图分类。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="od l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/topic-modeling-with-latent-dirichlet-allocation-by-example-3b22cd10c835"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">具有潜在狄利克雷分配(LDA)的主题建模教程</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">这是一本实用指南，包含经过实践检验的Python代码。找到人们在推特上谈论的内容。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="og l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/sentiment-analysis-a-benchmark-903279cab44a"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">情感分析:一个基准</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">递归神经网络解释。使用FCNNs、CNN、RNNs和嵌入对客户评论进行分类。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="oh l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">基于序列对序列模型的自然语言理解</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何预测客户询问背后的意图？Seq2Seq型号说明。在ATIS数据集上演示的槽填充…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="oi l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/heres-how-to-build-a-language-translator-in-few-lines-of-code-using-keras-30f7e0b3aa1d"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">假人的神经机器翻译——5分钟指南</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">AI能让濒危语言不消失吗？</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="oj l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/truecasing-in-natural-language-processing-12c4df086c21"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">自然语言处理中的真实大小写</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">恢复推文和短信中的大写字母可以提高可读性。正确的正确大小写对于…至关重要</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="ok l nx ny nz nv oa ks nm"/></div></div></a></div><p id="41a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。</p></div></div>    
</body>
</html>