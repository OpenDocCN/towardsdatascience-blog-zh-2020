<html>
<head>
<title>Ridge Regressions on Easy Mode</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简易模式下的岭回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-regressions-on-easy-mode-9e7353a0e3f9?source=collection_archive---------76-----------------------#2020-05-26">https://towardsdatascience.com/ridge-regressions-on-easy-mode-9e7353a0e3f9?source=collection_archive---------76-----------------------#2020-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f5b2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解岭回归模型背后的理论，如何通过python和scikit-learn对其进行编码和调优。</h2></div><h1 id="c07a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">它们是什么？</h1><p id="1c36" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">岭回归可以被认为是线性回归的一个<strong class="kz ir">步骤。它们也被称为吉洪诺夫正则化；和对于<strong class="kz ir">缓解线性回归</strong>中的多重共线性问题特别有用，该问题通常出现在具有大量参数的模型中。</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/185f868be50f20bfe7fc7404bb21e505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JvUACGoYaIwIhmv9"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">照片由<a class="ae mj" href="https://unsplash.com/@nicolasjleclercq?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尼古拉斯·J·勒克莱尔</a>在<a class="ae mj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="1fbf" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><em class="mp">简单概括线性回归</em>:他们通过最小化模型的<em class="mp">残差平方和(RSS) </em>来估计<em class="mp">线性模型截距</em>和<em class="mp">斜率系数</em>。RSS公式及其生成的线性模型如下所示:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/764ac1ea384529e7c62fafb1c0fa1c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*0nF8TDWg5j-fYmrlUBhdOA.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">点击<a class="ae mj" href="https://medium.com/python-in-plain-english/linear-regressions-with-scikitlearn-a5d54efe898f?source=your_stories_page---------------------------" rel="noopener">这里</a>了解更多关于如何用python编写自己的线性回归代码！</p></figure><p id="d24b" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">虽然简单的线性回归在大多数数据分析情况下通常被认为是有效的，但它们总是产生直线最佳拟合线。这源于关于因变量和自变量之间关系的强有力的先验假设:它们具有线性关系。</p><p id="29c6" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">当我们不得不抛弃这个假设时，问题就来了，我们需要接受缺乏参数来控制模型复杂性的事实。</p><blockquote class="mr ms mt"><p id="c20e" class="kx ky mp kz b la mk jr lc ld ml ju lf mu mm li lj mv mn lm ln mw mo lq lr ls ij bi translated">这就是岭回归的用武之地。他们增加了一个额外的术语，如下图红色所示。</p></blockquote><p id="0cf6" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">这一项被称为<em class="mp"> L2正则化参数</em>，其中<strong class="kz ir"> L1可以产生稀疏模型，而L2不能。</strong>最终结果仍然是一个简单的线性模型，如果使用OLS方法，其格式<strong class="kz ir">与</strong>相似，但系数<strong class="kz ir">可能不同</strong>。(通过下面的示例显示)</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mx"><img src="../Images/ffc2edc331693b854958d1da86073924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWnl18eVwPnmjtmVenjf_w.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">岭回归方程</p></figure><p id="5f21" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">正则项充当<strong class="kz ir">损失函数</strong>，它是在计算岭回归的RSS时对错误预测的<strong class="kz ir">惩罚。它通过<strong class="kz ir">将系数的平方值加到模型的RSS上来起作用。</strong>由于“最佳”模型是具有最小RSS的模型，<strong class="kz ir">具有较大系数的模型根据</strong>被有效过滤。</strong></p><p id="a842" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">正则化的作用是<strong class="kz ir">通过试图限制模型以降低复杂性来减少过拟合</strong>。正则项通过<strong class="kz ir">将系数的平方值添加到模型的RSS中来起作用。</strong>由于“最佳”模型是RSS最小的模型，因此<strong class="kz ir">系数较大的模型会根据</strong>进行有效过滤。</p><p id="a039" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><strong class="kz ir">最终结果是，模型更喜欢系数较小的特征。</strong></p><p id="30a4" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">正则化对于100+变量特别有效。<strong class="kz ir">此外，正则化的量由正则化项中的阿尔法参数控制。</strong>较高的alpha值导致模型招致更多的惩罚，使系数进一步向0收缩。python中的默认值是1。<strong class="kz ir">一个特殊的情况是，当α= 0时，得到的模型相当于OLS简单线性回归。</strong></p><h1 id="4bee" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">代码</h1><p id="37ba" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了举例说明岭回归的编码，我们将看看如何使用<em class="mp">犯罪数据集</em>。</p><h2 id="77cc" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">包装</h2><p id="343c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于包，让我们导入<em class="mp">犯罪数据集、</em>和用于创建训练测试分割、进行MixMaxScaling和创建岭回归的类。</p><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="abe9" class="my kg iq nl b gy np nq l nr ns">import numpy<br/>from adspy_shared_utilities import load_crime_dataset<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.linear_model import Ridge</span></pre><h2 id="08fa" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">加载数据集，并创建75 / 25列车测试数据分割:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="6c6a" class="my kg iq nl b gy np nq l nr ns">(X_crime, y_crime) = load_crime_dataset()<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X_crime,<br/>                                   y_crime, random_state = 0)<br/># take note of the indentation.</span></pre><h2 id="9829" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">缩放训练和测试数据集:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="03a6" class="my kg iq nl b gy np nq l nr ns">scaler = MinMaxScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.transform(X_test)</span></pre><h2 id="b950" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">创建alpha值为20.0的岭回归对象，并在缩放的X训练数据上拟合该对象以及相应的训练数据Y标签:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="1ec1" class="my kg iq nl b gy np nq l nr ns">linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)</span></pre><h2 id="02b4" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">获取模型系数:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="d7a1" class="my kg iq nl b gy np nq l nr ns">linridge.intercept_<br/>linridge.coef_ </span><span id="49cc" class="my kg iq nl b gy nt nq l nr ns"><strong class="nl ir">-3352.423035846206<br/>[  1.95091438e-03   2.19322667e+01   9.56286607e+00  -3.59178973e+01<br/>   6.36465325e+00  -1.96885471e+01  -2.80715856e-03   1.66254486e+00<br/>  -6.61426604e-03  -6.95450680e+00   1.71944731e+01  -5.62819154e+00<br/>   8.83525114e+00   6.79085746e-01  -7.33614221e+00   6.70389803e-03<br/>   9.78505502e-04   5.01202169e-03  -4.89870524e+00  -1.79270062e+01<br/>   9.17572382e+00  -1.24454193e+00   1.21845360e+00   1.03233089e+01<br/>  -3.78037278e+00  -3.73428973e+00   4.74595305e+00   8.42696855e+00<br/>   3.09250005e+01   1.18644167e+01  -2.05183675e+00  -3.82210450e+01<br/>   1.85081589e+01   1.52510829e+00  -2.20086608e+01   2.46283912e+00<br/>   3.29328703e-01   4.02228467e+00  -1.12903533e+01  -4.69567413e-03<br/>   4.27046505e+01  -1.22507167e-03   1.40795790e+00   9.35041855e-01<br/>  -3.00464253e+00   1.12390514e+00  -1.82487653e+01  -1.54653407e+01<br/>   2.41917002e+01  -1.32497562e+01  -4.20113118e-01  -3.59710660e+01<br/>   1.29786751e+01  -2.80765995e+01   4.38513476e+01   3.86590044e+01<br/>  -6.46024046e+01  -1.63714023e+01   2.90397330e+01   4.15472907e+00<br/>   5.34033563e+01   1.98773191e-02  -5.47413979e-01   1.23883518e+01<br/>   1.03526583e+01  -1.57238894e+00   3.15887097e+00   8.77757987e+00<br/>  -2.94724962e+01  -2.32995397e-04   3.13528914e-04  -4.13628414e-04<br/>  -1.79851056e-04  -5.74054527e-01  -5.17742507e-01  -4.20670930e-01<br/>   1.53383594e-01   1.32725423e+00   3.84863158e+00   3.03024594e+00<br/>  -3.77692644e+01   1.37933464e-01   3.07676522e-01   1.57128807e+01<br/>   3.31418306e-01   3.35994414e+00   1.61265911e-01  -2.67619878e+00]</strong></span></pre><h2 id="a5d4" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">获得模型R平方得分</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="9197" class="my kg iq nl b gy np nq l nr ns">linridge.score(X_train_scaled, y_train)<br/>linridge.score(X_test_scaled, y_test)</span><span id="3c4d" class="my kg iq nl b gy nt nq l nr ns"><strong class="nl ir">-31.672<br/>-37.249</strong></span></pre><h2 id="fe6c" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">非零特征的数量:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="c96f" class="my kg iq nl b gy np nq l nr ns">(np.sum(linridge.coef_ != 0))</span><span id="8a39" class="my kg iq nl b gy nt nq l nr ns"><strong class="nl ir">88</strong></span></pre><h2 id="9b3e" class="my kg iq bd kh mz na dn kl nb nc dp kp lg nd ne kr lk nf ng kt lo nh ni kv nj bi translated">调整alpha值以最大化训练和测试分数。我们可以获得非零特征的数量以及如下的训练和测试分数:</h2><pre class="lu lv lw lx gt nk nl nm nn aw no bi"><span id="31ab" class="my kg iq nl b gy np nq l nr ns">for alpha in [0, 1, 10, 20, 50, 100, 1000]:<br/>    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled,y_train) <br/>    r2_train = linridge.score(X_train_scaled, y_train)<br/>    r2_test = linridge.score(X_test_scaled, y_test)<br/>    num_coeff_bigger = np.sum(abs(linridge.coef_) &gt; 1.0)<br/>    print('Alpha = {:.2f}\nnum abs(coeff) &gt; 1.0: {}, \<br/>r-squared training: {:.2f}, r-squared test: {:.2f}\n'<br/>         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))</span><span id="f80b" class="my kg iq nl b gy nt nq l nr ns"><strong class="nl ir">Ridge regression: effect of alpha regularization parameter</strong></span><span id="91ac" class="my kg iq nl b gy nt nq l nr ns"><strong class="nl ir">Alpha = 0.00<br/>num abs(coeff) &gt; 1.0: 88, r-squared training: 0.67, r-squared test: 0.50<br/><br/>Alpha = 1.00<br/>num abs(coeff) &gt; 1.0: 87, r-squared training: 0.66, r-squared test: 0.56<br/><br/>Alpha = 10.00<br/>num abs(coeff) &gt; 1.0: 87, r-squared training: 0.63, r-squared test: 0.59<br/><br/>Alpha = 20.00<br/>num abs(coeff) &gt; 1.0: 88, r-squared training: 0.61, r-squared test: 0.60<br/><br/>Alpha = 50.00<br/>num abs(coeff) &gt; 1.0: 86, r-squared training: 0.58, r-squared test: 0.58<br/><br/>Alpha = 100.00<br/>num abs(coeff) &gt; 1.0: 87, r-squared training: 0.55, r-squared test: 0.55<br/><br/>Alpha = 1000.00<br/>num abs(coeff) &gt; 1.0: 84, r-squared training: 0.31, r-squared test: 0.30</strong></span></pre><h1 id="9a6f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="8d2f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通过这篇文章，我希望您已经<strong class="kz ir">了解了岭回归背后的理论，以及如何通过python和scikit-learn对岭回归模型进行编码和调优。</strong></p><p id="b897" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">我通过由Coursera主办的密歇根大学MOOC“Python中的应用机器学习”了解到了这一点。</p><p id="d7ff" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">如果你有问题或者想讨论后新冠肺炎世界，请随时联系我。</p><p id="01ea" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">我希望我能够以这样或那样的方式帮助您学习数据科学方法！</p><p id="d16d" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">这是另一篇数据科学文章！</p><div class="nu nv gp gr nw nx"><a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-in-6-steps-efbcbebce54d" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">6步k近邻</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">使用scikit-学习python</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol md nx"/></div></div></a></div></div></div>    
</body>
</html>