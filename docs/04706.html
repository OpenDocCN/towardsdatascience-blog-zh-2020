<html>
<head>
<title>GanGAN: How to Grow a Brain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">安钢:如何培养大脑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gangan-how-to-grow-a-brain-44d61c4f8251?source=collection_archive---------57-----------------------#2020-04-25">https://towardsdatascience.com/gangan-how-to-grow-a-brain-44d61c4f8251?source=collection_archive---------57-----------------------#2020-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7413" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">~<em class="kf">gan是类比机器，它得到meta ~ </em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/7c68ec7d21c50939be96a23232d5a859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x9sEG0g5OOOryluGcbPpbg.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@artic_studios?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">丹尼尔·奥伯格</a>在<a class="ae kw" href="https://unsplash.com/s/photos/brain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9914" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">TL；博士</strong>——如果你有两个神经网络，每个用于一个不同的任务，那么你有时可以找到一个GAN，它将一个任务 <strong class="kz ir"> <em class="lt">转换为另一个任务</em></strong><em class="lt">。类比！因此，找到<strong class="kz ir">GAN，它‘GAN’成对的GAN</strong>，你就可以增长层次抽象。使用这些gan在观察到的数据分布</em>之外做出预测<em class="lt">——类比让你零射击新任务。</em></p><p id="2fdc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我应该从一个具体的例子开始:</p><p id="a859" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">扔石头</strong></p><p id="2f42" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设你有一个已经训练好的观看视频的神经网络。它观察了几千个小时的人们拿着、扔着和扔着棒球。你的网络很擅长预测抛物线轨迹。但是，只见过棒球做那种事情。你给它看一个有人扔石头的<em class="lt">单</em>视频，神经网络就丢了。可恶。</p><p id="0217" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是，等等！如果你能训练出一只能拍下扔石头视频每一帧的狗，然后<strong class="kz ir"> <em class="lt">把石头转换成棒球</em> </strong>，那么你就又是金牌了！当然，获得更多摇滚视频可能更容易，但当你试图解决<em class="lt">在线学习</em>时，这个问题变得不可克服，特别是对于<em class="lt">长尾事件</em>，以及更具适应性、更安全的神经网络所必需的<em class="lt">泛化能力</em>。</p><p id="2b00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这里的关键概念是:采用一个众所周知的任务(棒球轨迹)，将其与GAN翻译器(摇滚到棒球)结合，从而通过类比做出合理准确的预测<em class="lt">。(实际上润色视频的每一帧，使摇滚看起来像棒球，可能太笨拙了。你只需要一个GAN来做<em class="lt">任何让rock-translator网络具有预测性的事情；</em>神经网络生成的<em class="lt">训练机制的发展</em>表明，在我们看来，最佳类比转换可能看起来<a class="ae kw" href="https://www.youtube.com/watch?v=jxIkPxkN10U" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"/></a>。)</em></p><p id="3147" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">可以把甘的类比推理能力看作是他自己的一块小石头。因为我们正在用这些东西建造一座大教堂…</p><p id="f8ad" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">甘的甘</strong></p><p id="f885" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设你一直在用这些gan在<em class="lt">数百</em>个不同的任务之间形成类比。<em class="lt">每一个</em>任务对，你都要检查是否有一个<em class="lt">紧凑而准确的</em>‘GANalogy’通常情况下，除非你让你的神经网络变得庞大，以便它能记住所有的东西，否则你无法获得一个像样的GAN。那些是在感觉层面上<em class="lt">实际上不同于</em>的任务。忽略那些；如果GAN看起来像他们中的一员，就不要费心全程训练它了。</p><p id="baf9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然而，即使GAN模拟网络缩减到几百个神经元，仍有一些任务运行得很好。那些甘人是守护者。一旦你把所有你能理解的GAN都弄懂了，你就可以说:“对于这些<em class="lt"/><strong class="kz ir"><em class="lt">两个</em></strong><em class="lt">GAN</em>，有GAN翻译吗？”<em class="lt">这些</em> GanGANs会是像“(摇滚到棒球)作品像(雕塑到人)”和“(电场到磁场)作品像(矩阵到矩阵)”这样的东西。<strong class="kz ir"> <em class="lt">真实、有用的比喻</em> </strong>。包括任务到GAN和低层到高层GAN。<em class="lt">每一个</em>对！</p><p id="a3f3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一层一层，向上到更大的抽象，找到工作的gan。当然，它们中的许多都是多余的——例如，你可能有一个棒球到岩石的GAN、棒球到苗条和苗条到岩石的GAN……<em class="lt">冗余让你</em> <strong class="kz ir"> <em class="lt">检查</em> </strong> <em class="lt">你的工作</em>，类似于专家神经网络模型的混合。更重要的是，如果几个不同的答案路径都给了<em class="lt">相同的结果</em>，它们更有可能是正确的答案，因为<strong class="kz ir">的错误往往会使<em class="lt">与</em>互不相同，而事实是<em class="lt">与</em> </strong>一致。类似地，自我一致性作为唯一的约束最近仅从<a class="ae kw" href="https://www.youtube.com/watch?v=eTYcMB6Yhe8" rel="noopener ugc nofollow" target="_blank">视频</a>中生成了近SOTA无监督深度感知。</p><p id="0722" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">永远记住细节</strong></p><p id="1d3d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我有理由相信，你永远不会想要明确地组合原始的感觉网络，这是一个错误，希望“将众多任务集成到一个单一的端到端神经网络中，将提高性能”。第<em class="lt">号保留</em>所有微小的感觉网络，并保留每一个单独的和多余的干，<em class="lt">只要它实际上起到类比的作用</em>。这样，任何一个<em class="lt">组件</em>任务都有能力独立地与一个<em class="lt">新</em>任务相结合，形成一个<strong class="kz ir">专门且独特的类比</strong>。相比之下，已经<strong class="kz ir">集成到单片端到端网络</strong>中的任务将<em class="lt">从新的GAN模拟形成中</em>模糊，因为单片网络<em class="lt">作为一个整体</em>不是模拟的。</p><p id="980b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">随着安钢的进展，我会继续更新，我还不想过多地谈论细节。简而言之:有许多<em class="lt">更抽象的损失函数</em>可能值得<em class="lt">添加</em>到单个网络的损失函数，以有效地调节这些gan的生成和整合。此外，用于推理阶段的GANs架构显然有很大的作用；我受到了关于<a class="ae kw" rel="noopener" target="_blank" href="/using-meta-neurons-to-learn-facts-from-a-single-training-example-781ca0b7424d">元神经元</a>的工作的启发，这些元神经元用于组合GAN输出的不和谐声音。Hinton的胶囊也是一个值得记住的好模型。请随意尝试——我很慢而且不稳定，所以你可能会在我之前完成。:)</p></div></div>    
</body>
</html>