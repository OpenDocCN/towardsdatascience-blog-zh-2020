<html>
<head>
<title>Concrete Compressive Strength Prediction using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的混凝土抗压强度预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/concrete-compressive-strength-prediction-using-machine-learning-4a531b3c43f3?source=collection_archive---------5-----------------------#2020-03-05">https://towardsdatascience.com/concrete-compressive-strength-prediction-using-machine-learning-4a531b3c43f3?source=collection_archive---------5-----------------------#2020-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/df6130be199d4ffa4ef8dd3ec3446d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQg3g36bTRx0ZNytVWm5nA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">里卡多·戈麦斯·安吉尔在<a class="ae jg" href="https://unsplash.com/s/photos/construction-site?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="ebd6" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">机器学习在土木工程中的应用</h2></div><h1 id="ea93" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">混凝土抗压强度</h1><p id="46cc" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">混凝土的抗压强度决定了混凝土的质量。这通常由混凝土圆柱体上的标准压碎试验来确定。这需要工程师用不同的原材料组合建造小型混凝土圆柱体，并测试这些圆柱体的强度随每种原材料的变化而变化。测试钢瓶的建议等待时间是28天，以确保正确的结果。这消耗了大量的时间，并且需要大量的劳动力来准备不同的原型并测试它们。此外，这种方法容易出现人为错误，一个小错误就可能导致等待时间急剧增加。</p><p id="7c3b" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">减少等待时间和减少尝试组合数量的一种方法是利用数字模拟，我们可以向计算机提供我们所知道的信息，计算机尝试不同的组合来预测抗压强度。这样，我们可以减少物理上可以尝试的组合数量，并减少实验时间。但是，要设计这样的软件，我们必须知道所有原材料之间的关系，以及一种材料如何影响强度。可以推导出数学方程，并基于这些方程进行模拟，但我们不能期望现实世界中的关系是相同的。此外，这些测试已经进行了很多次，我们有足够的现实世界的数据可以用于预测建模。</p><p id="f5eb" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在本文中，我们将分析<a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength" rel="noopener ugc nofollow" target="_blank">混凝土抗压强度</a>数据集，并建立机器学习模型来预测抗压强度。这个<a class="ae jg" href="https://github.com/pranaymodukuru/Concrete-compressive-strength/blob/master/ConcreteCompressiveStrengthPrediction.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>包含了所有的代码，可以并行使用。</p><h1 id="baf2" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">数据集描述</h1><p id="74d3" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据集由1030个实例组成，具有9个属性，没有缺失值。有8个输入变量和1个输出变量。七个输入变量代表原材料的数量(以kg/m计量),一个代表龄期(以天计)。目标变量是混凝土抗压强度，单位为兆帕(MPa —兆帕)。我们将研究这些数据，看看输入特征是如何影响抗压强度的。</p><h1 id="9280" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">探索性数据分析</h1><p id="b69c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据科学项目的第一步是在建模之前理解数据并从数据中获得洞察力。这包括检查任何缺失值、绘制与目标变量相关的特征、观察所有特征的分布等等。让我们导入数据并开始分析。</p><p id="335f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们检查输入特征之间的相关性，这将给出每个变量如何影响所有其他变量的想法。这可以通过计算特征之间的皮尔逊相关来实现，如下面的代码所示。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="f861" class="na kz jj mw b gy nb nc l nd ne">corr = data.corr() sns.heatmap(corr, annot=True, cmap='Blues')</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ab379525ce95c7b2fd821672338a1988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/0*i5FlQRIxhLJQav7h.png"/></div></figure><p id="d6d9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以观察到<strong class="ls jk">抗压强度</strong> (CC_Strength)和<strong class="ls jk">水泥</strong>之间的高度正相关。这是真的，因为混凝土的强度确实随着制备过程中水泥用量的增加而增加。另外，<strong class="ls jk">龄期</strong>和<strong class="ls jk">超塑化剂</strong>是影响抗压强度的另外两个因素。</p><p id="6d4f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些特征之间还有其他强相关性，</p><ul class=""><li id="1907" class="ng nh jj ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><strong class="ls jk">超塑化剂</strong>与<strong class="ls jk">水</strong>呈强负相关。</li><li id="752f" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">超塑化剂</strong>与<strong class="ls jk">粉煤灰</strong>、<strong class="ls jk">细骨料</strong>呈正相关关系。</li></ul><p id="c60a" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些相关性有助于详细理解数据，因为它们给出了一个变量如何影响另一个变量的想法。我们可以进一步使用seaborn中的<strong class="ls jk"> pairplot </strong>来绘制所有特性之间的成对关系以及特性沿对角线的分布。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="dd61" class="na kz jj mw b gy nb nc l nd ne">sns.pairplot(data)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/47c8bc28e1580d23540dcaca691182a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bXTAzRQxNrI2rJLz.png"/></div></div></figure><p id="28fa" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">配对图直观地展示了所有特征之间的相关性。</p><p id="d640" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以在<strong class="ls jk"> CC_Strength </strong>和其他特征之间绘制散点图，以查看更复杂的关系。</p><p id="4530" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">CC _强度vs(水泥、龄期、水)</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="3c3c" class="na kz jj mw b gy nb nc l nd ne">sns.scatterplot(y="CC_Strength", x="Cement", hue="Water",size="Age", data=data, ax=ax, sizes=(50, 300))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6c9ff548d880d9721375b95b7d5d52c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/0*sAelcVaW6FCAnz-I.png"/></div></figure><p id="924a" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从这幅图中我们可以观察到，</p><ul class=""><li id="51b8" class="ng nh jj ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><strong class="ls jk">抗压强度随着水泥量的增加而增加</strong>，当我们在x轴上向右移动时，圆点向上移动。</li><li id="ab2d" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">抗压强度随着年龄的增长而增加</strong>(因为圆点的大小代表年龄)，情况并非总是如此，但在一定程度上是可以的。</li><li id="4a4a" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">时间越短的水泥需要越多的水泥来获得更高的强度</strong>，因为当我们在x轴上向右移动时，较小的点会向上移动。</li><li id="0521" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">水泥越老，需要的水就越多</strong>，可以通过观察圆点的颜色来确认。深色的大点表示年龄大，水多。</li><li id="4613" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">制备混凝土时使用较少的水</strong>混凝土强度增加，因为较低侧(y轴)的点较暗，较高端(y轴)的点较亮。</li></ul><p id="5176" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">CC强度vs(细骨料、超塑化剂、粉煤灰)</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="3808" class="na kz jj mw b gy nb nc l nd ne">sns.scatterplot(y="CC_Strength", x="FineAggregate", hue="FlyAsh",<br/>   size="Superplasticizer", data=data, ax=ax, sizes=(50, 300))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi gj"><img src="../Images/566d3dce13e8423c5d53afb690a471b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xemODnOQBNtW8MKg.png"/></div></div></figure><p id="1ef1" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">观察，</p><ul class=""><li id="416f" class="ng nh jj ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><strong class="ls jk">抗压强度降低飞灰增加</strong>，因为黑点集中在代表低抗压强度的区域。</li><li id="ed12" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls jk">抗压强度随着超塑化剂的增加而增加</strong>，因为点越大，曲线中的点越高。</li></ul><p id="a4a8" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以直观地理解2D、3D和max直到4D图(由颜色和大小表示的特征)如上所示，我们可以进一步使用seaborn的行和列绘图特征来做进一步的分析，但我们仍然缺乏自己跟踪所有这些相关性的能力。出于这个原因，我们可以转向机器学习来捕捉这些关系，并对问题给出更好的见解。</p><h1 id="744d" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">数据预处理</h1><p id="b7fd" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在我们对数据拟合机器学习模型之前，我们需要将数据分割成训练和测试分割。可以重新调整特征的比例，使其均值为零，标准差为1，即所有特征都落在相同的范围内。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="05ff" class="na kz jj mw b gy nb nc l nd ne">X = data.iloc[:,:-1] # Features <br/>y = data.iloc[:,-1] # Target </span><span id="409d" class="na kz jj mw b gy nw nc l nd ne">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2) </span><span id="a65d" class="na kz jj mw b gy nw nc l nd ne">sc = StandardScaler() </span><span id="34b8" class="na kz jj mw b gy nw nc l nd ne">X_train = sc.fit_transform(X_train) <br/>X_test = sc.transform(X_test)</span></pre><h1 id="b723" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">模型结构</h1><p id="08c8" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在准备好数据之后，我们可以在训练数据上拟合不同的模型，并比较它们的性能，以选择性能好的算法。由于这是一个回归问题，我们可以使用RMSE(均方根误差)和$R $分数作为评估指标。</p><h2 id="afe1" class="na kz jj bd la nx ny dn le nz oa dp li lz ob oc lk md od oe lm mh of og lo oh bi translated">1.线性回归</h2><p id="f7c4" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们将从线性回归开始，因为这是任何回归问题的首选算法。该算法试图在输入特征和目标变量之间形成线性关系，即它拟合由下式给出的直线，</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/266c29f77ec378d892192d1cf7cbc7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*_cbFQF4cd5lQsOcc5iB41Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">线性回归</p></figure><p id="411c" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">其中w_i对应于特征x_i的系数。</p><p id="c9c9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">通过对成本函数使用正则化项，可以进一步控制这些系数的大小。将系数的大小相加将导致系数接近于零，这种线性回归的变化称为<strong class="ls jk">拉索</strong>回归。将系数的平方和添加到成本函数将使系数在相同的范围内，这种变化被称为<strong class="ls jk">岭</strong>回归。这两种变化都有助于降低模型的复杂性，从而减少数据过度拟合的机会。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="a965" class="na kz jj mw b gy nb nc l nd ne"># Importing models <br/>from sklearn.linear_model import LinearRegression, Lasso, Ridge </span><span id="bd5b" class="na kz jj mw b gy nw nc l nd ne"># Linear Regression <br/>lr = LinearRegression() </span><span id="3381" class="na kz jj mw b gy nw nc l nd ne"># Lasso Regression <br/>lasso = Lasso() </span><span id="388a" class="na kz jj mw b gy nw nc l nd ne"># Ridge Regression <br/>ridge = Ridge() </span><span id="bddf" class="na kz jj mw b gy nw nc l nd ne"># Fitting models on Training data <br/>lr.fit(X_train, y_train) <br/>lasso.fit(X_train, y_train) <br/>ridge.fit(X_train, y_train) </span><span id="1b71" class="na kz jj mw b gy nw nc l nd ne"># Making predictions on Test data </span><span id="c59a" class="na kz jj mw b gy nw nc l nd ne">y_pred_lr = lr.predict(X_test) <br/>y_pred_lasso = lasso.predict(X_test) <br/>y_pred_ridge = ridge.predict(X_test) </span><span id="aeb9" class="na kz jj mw b gy nw nc l nd ne"><br/>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score </span><span id="6e95" class="na kz jj mw b gy nw nc l nd ne">print("Model\t\t\t RMSE \t\t R2") <br/>print("""LinearRegression \t {:.2f} \t\t{:.2f}""".format(  np.sqrt(mean_squared_error(y_test, y_pred_lr)), r2_score(y_test, y_pred_lr))) </span><span id="ef0a" class="na kz jj mw b gy nw nc l nd ne">print("""LassoRegression \t {:.2f} \t\t{:.2f}""".format( np.sqrt(mean_squared_error(y_test, y_pred_lasso)), r2_score(y_test, y_pred_lasso))) print("""RidgeRegression \t {:.2f} \t\t{:.2f}""".format( np.sqrt(mean_squared_error(y_test, y_pred_ridge)), r2_score(y_test, y_pred_ridge)))</span></pre><p id="2152" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">输出</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c65f8ded45b85d4d018b0e059e2bfa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*v76E6oco7RhKfZtx2ozLmA.png"/></div></figure><p id="b493" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这三种算法的性能没有太大差别，我们可以用下面的代码画出这三种算法为特性分配的系数。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1aa4" class="na kz jj mw b gy nb nc l nd ne">coeff_lr = lr.coef_ <br/>coeff_lasso = lasso.coef_ <br/>coeff_ridge = ridge.coef_ </span><span id="26a1" class="na kz jj mw b gy nw nc l nd ne">labels = req_col_names[:-1] <br/>x = np.arange(len(labels)) <br/>width = 0.3 </span><span id="152b" class="na kz jj mw b gy nw nc l nd ne">fig, ax = plt.subplots(figsize=(10,6)) <br/>rects1 = ax.bar(x - 2*(width/2), coeff_lr, width, label='LR') <br/>rects2 = ax.bar(x, coeff_lasso, width, label='Lasso') <br/>rects3 = ax.bar(x + 2*(width/2), coeff_ridge, width, label='Ridge') </span><span id="6660" class="na kz jj mw b gy nw nc l nd ne">ax.set_ylabel('Coefficient') <br/>ax.set_xlabel('Features') <br/>ax.set_title('Feature Coefficients') <br/>ax.set_xticks(x) <br/>ax.set_xticklabels(labels, rotation=45) <br/>ax.legend() </span><span id="b1c3" class="na kz jj mw b gy nw nc l nd ne">def autolabel(rects): <br/>   """Attach a text label above each bar in *rects*, displaying its height.""" <br/>   for rect in rects: <br/>      height = rect.get_height() <br/>      ax.annotate('{:.2f}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords="offset points", ha='center', va='bottom') </span><span id="bbc3" class="na kz jj mw b gy nw nc l nd ne">autolabel(rects1) <br/>autolabel(rects2) <br/>autolabel(rects3) <br/>fig.tight_layout() <br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/09f7cd381f710e82f36fe90134885522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ro9CFpmtf7JxiSkt.png"/></div></div></figure><p id="017a" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如图所示，Lasso回归将系数推向零，而正常线性回归和岭回归的系数几乎相同。</p><p id="d1cc" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以通过绘制真实值和预测值来进一步了解预测情况，</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="8f55" class="na kz jj mw b gy nb nc l nd ne">fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4)) ax1.scatter(y_pred_lr, y_test, s=20) <br/>ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) <br/>ax1.set_ylabel("True") <br/>ax1.set_xlabel("Predicted") <br/>ax1.set_title("Linear Regression") <br/>ax2.scatter(y_pred_lasso, y_test, s=20) </span><span id="044f" class="na kz jj mw b gy nw nc l nd ne">ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) ax2.set_ylabel("True") <br/>ax2.set_xlabel("Predicted") <br/>ax2.set_title("Lasso Regression") <br/>ax3.scatter(y_pred_ridge, y_test, s=20) </span><span id="8bbf" class="na kz jj mw b gy nw nc l nd ne">ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) <br/>ax3.set_ylabel("True") <br/>ax3.set_xlabel("Predicted") <br/>ax3.set_title("Ridge Regression") <br/>fig.suptitle("True vs Predicted") <br/>fig.tight_layout(rect=[0, 0.03, 1, 0.95])</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/e3cd38e03abd4da663b027ac984034c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3TCebkpmtf04of6q.png"/></div></div></figure><p id="2bd0" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果预测值和目标值相等，那么散点图上的点将位于直线上。正如我们在这里看到的，没有一个模型能正确预测抗压强度。</p><h2 id="6b70" class="na kz jj bd la nx ny dn le nz oa dp li lz ob oc lk md od oe lm mh of og lo oh bi translated">2.决策树</h2><p id="2c0d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">决策树算法用树状结构表示数据，其中每个节点表示对某个功能做出的决策。在这种情况下，这种算法将提供更好的性能，因为我们在一些输入特征中有许多零，如上面的对图中的分布所示。这将有助于决策树基于某些特征条件构建树，从而进一步提高性能。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1d6c" class="na kz jj mw b gy nb nc l nd ne">from sklearn.tree import DecisionTreeRegressor <br/>dtr = DecisionTreeRegressor() <br/>dtr.fit(X_train, y_train) <br/>y_pred_dtr = dtr.predict(X_test) </span><span id="56f0" class="na kz jj mw b gy nw nc l nd ne">print("Model\t\t\t\t RMSE \t\t R2") <br/>print("""Decision Tree Regressor \t {:.2f} \t\t{:.2f}""".format( np.sqrt(mean_squared_error(y_test, y_pred_dtr)), r2_score(y_test, y_pred_dtr))) </span><span id="a9ef" class="na kz jj mw b gy nw nc l nd ne">plt.scatter(y_test, y_pred_dtr) <br/>plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) <br/>plt.xlabel("Predicted") <br/>plt.ylabel("True") <br/>plt.title("Decision Tree Regressor") plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/137752a763bf16bd0a9ef1beaeca695a.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*TvUsPwEpYWIRGOHq.png"/></div></figure><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/aae9d9069ddabdd5967d100c83df260c.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*JERdBxTH5_vZT4Po1QqV_Q.png"/></div></figure><p id="0fa9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">均方根误差(RMSE)从10.29下降到7.31，因此决策树回归器显著提高了性能。这可以在图中观察到，并且更多的点更靠近线。</p><h2 id="f47c" class="na kz jj bd la nx ny dn le nz oa dp li lz ob oc lk md od oe lm mh of og lo oh bi translated">3.随机森林</h2><p id="0b53" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">使用决策树回归器提高了我们的性能，我们可以通过集成更多的树来进一步提高性能。随机森林回归器使用从训练数据中采样的随机数据子集来训练随机初始化的树，这将使我们的模型更加健壮。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="2b3a" class="na kz jj mw b gy nb nc l nd ne">from sklearn.ensemble import RandomForestRegressor </span><span id="4b89" class="na kz jj mw b gy nw nc l nd ne">rfr = RandomForestRegressor(n_estimators=100) <br/>rfr.fit(X_train, y_train) </span><span id="d4ab" class="na kz jj mw b gy nw nc l nd ne">y_pred_rfr = rfr.predict(X_test) </span><span id="618c" class="na kz jj mw b gy nw nc l nd ne">print("Model\t\t\t\t RMSE \t\t R2") print("""Random Forest Regressor \t {:.2f} \t\t{:.2f}""".format( np.sqrt(mean_squared_error(y_test, y_pred_rfr)), r2_score(y_test, y_pred_rfr))) </span><span id="0702" class="na kz jj mw b gy nw nc l nd ne">plt.scatter(y_test, y_pred_rfr) <br/>plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) <br/>plt.xlabel("Predicted") <br/>plt.ylabel("True") <br/>plt.title("Random Forest Regressor") <br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/aa4383bdea7dc6d01efe210047155e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*Zefnsst6Z6gd3uVd.png"/></div></figure><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4bdccd700e5e5b4cad514e3525b999f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*Z_zqsyCX64P-i0-eAJbbwg.png"/></div></figure><p id="9df1" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">通过集合多棵树，RMSE进一步减少。我们可以为基于树的模型绘制特征重要性。特征重要性显示了在进行预测时特征对模型的重要性。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="7637" class="na kz jj mw b gy nb nc l nd ne">feature_dtr = dtr.feature_importances_ <br/>feature_rfr = rfr.feature_importances_ </span><span id="1d02" class="na kz jj mw b gy nw nc l nd ne">labels = req_col_names[:-1] <br/>x = np.arange(len(labels)) <br/>width = 0.3 <br/>fig, ax = plt.subplots(figsize=(10,6)) <br/>rects1 = ax.bar(x-(width/2), feature_dtr, width, label='Decision Tree') <br/>rects2 = ax.bar(x+(width/2), feature_rfr, width, label='Random Forest') <br/>ax.set_ylabel('Importance') <br/>ax.set_xlabel('Features') <br/>ax.set_title('Feature Importance') <br/>ax.set_xticks(x) <br/>ax.set_xticklabels(labels, rotation=45) <br/>ax.legend(loc="upper left", bbox_to_anchor=(1,1)) <br/>autolabel(rects1) <br/>autolabel(rects2) <br/>fig.tight_layout() <br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/c2fdcf77ca7024c219c09e05cf729932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MQmS_Wx72YAgDcmB.png"/></div></div></figure><p id="6913" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">基于树的模型将水泥和年龄视为最重要的特征。在预测混凝土强度时，粉煤灰、粗骨料和细骨料是最不重要的因素。</p><h2 id="0c80" class="na kz jj bd la nx ny dn le nz oa dp li lz ob oc lk md od oe lm mh of og lo oh bi translated">比较</h2><p id="481a" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">最后，让我们比较所有算法的结果。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="316d" class="na kz jj mw b gy nb nc l nd ne">models = [lr, lasso, ridge, dtr, rfr] <br/>names = ["Linear Regression", "Lasso Regression", "Ridge Regression", "Decision Tree Regressor", "Random Forest Regressor"] rmses = [] </span><span id="ac28" class="na kz jj mw b gy nw nc l nd ne">for model in models: <br/>   rmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test)))) </span><span id="b056" class="na kz jj mw b gy nw nc l nd ne">x = np.arange(len(names)) <br/>width = 0.3 <br/>fig, ax = plt.subplots(figsize=(10,7)) <br/>rects = ax.bar(x, rmses, width) <br/>ax.set_ylabel('RMSE') <br/>ax.set_xlabel('Models') <br/>ax.set_title('RMSE with Different Algorithms') <br/>ax.set_xticks(x) <br/>ax.set_xticklabels(names, rotation=45) <br/>autolabel(rects) <br/>fig.tight_layout() <br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/d5ed75f995d8a92f3b3892a2924388f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*se2LUadUz32-oVxo.png"/></div></div></figure><h1 id="a050" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论</h1><p id="e4ce" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们分析了抗压强度数据，并使用机器学习来预测混凝土的抗压强度。我们使用了线性回归及其变体、决策树和随机森林来进行预测，并比较了它们的性能。随机森林回归器具有最低的RMSE，是解决该问题的好选择。此外，我们可以通过执行网格搜索或随机搜索来调整超参数，从而进一步提高算法的性能。</p><h1 id="c37c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">参考</h1><ol class=""><li id="180d" class="ng nh jj ls b lt lu lw lx lz oq md or mh os ml ot nm nn no bi translated">叶一成，“使用人工神经网络对高性能混凝土的强度进行建模”，水泥与混凝土研究，第28卷，第12期，第1797–1808页(1998年)。</li><li id="a0a6" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml ot nm nn no bi translated">Ahsanul Kabir，医学博士Monjurul Hasan，Khasro Miah，“混凝土的<a class="ae jg" href="https://www.researchgate.net/publication/258255660_Strength_Prediction_Model_for_Concrete" rel="noopener ugc nofollow" target="_blank">强度预测模型”，ACEE Int .土木与环境工程学报，第2卷第1期，2013年8月。</a></li><li id="b4da" class="ng nh jj ls b lt np lw nq lz nr md ns mh nt ml ot nm nn no bi translated"><a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Concrete+抗压+强度</a></li></ol></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="8c37" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="pb">原载于2020年3月5日</em><a class="ae jg" href="https://pranaymodukuru.github.io/post/concrete-compressive-strength/" rel="noopener ugc nofollow" target="_blank"><em class="pb">https://pranaymudukuru . github . io</em></a><em class="pb">。</em></p></div></div>    
</body>
</html>