<html>
<head>
<title>Deep Learning for NLP: Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的深度学习:单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-nlp-word-embeddings-4f5c90bcdab5?source=collection_archive---------21-----------------------#2020-06-13">https://towardsdatascience.com/deep-learning-for-nlp-word-embeddings-4f5c90bcdab5?source=collection_archive---------21-----------------------#2020-06-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4011a5f8e3edfccf05842ef62a0f248a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwVL-apx5IpEtuHyz9vLCw.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="4e6a" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">简单直观的单词嵌入</h2></div><h1 id="8bd4" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">介绍</h1><p id="71d5" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jf"> <em class="mh">单词嵌入</em> </strong>已经成为最常用的工具之一，也是需要处理语音或文本等自然语言的人工智能任务取得惊人成就的主要驱动力。</p><p id="889c" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf">在本帖中，我们将揭开它们背后的神奇</strong>，看看它们是什么，为什么它们成为了<strong class="ln jf">自然语言处理</strong> (NLP下文)世界中的标准，它们是如何构建的，并探索一些最常用的单词嵌入算法。</p><p id="64e3" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">一切都将以简单直观的方式进行解释，避免复杂的数学运算，并尽量使帖子的内容易于理解。</p><p id="bdf8" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">这将在以下小节中进行分解:</p><ol class=""><li id="362a" class="mn mo je ln b lo mi lr mj lu mp ly mq mc mr mg ms mt mu mv bi translated"><strong class="ln jf">什么是单词嵌入？</strong></li><li id="47df" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg ms mt mu mv bi translated"><strong class="ln jf">为什么要使用单词嵌入？</strong></li><li id="a293" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg ms mt mu mv bi translated"><strong class="ln jf">单词嵌入是如何构建的？</strong></li><li id="793a" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg ms mt mu mv bi translated"><strong class="ln jf">最流行的单词嵌入有哪些？</strong></li></ol><p id="6874" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">一旦你准备好了，让我们开始看看什么是单词嵌入。</p><h1 id="0266" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">1)什么是单词嵌入？</h1><p id="f7fe" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">计算机把一切都分解成数字。更确切地说是比特(零和一)。当计算机内部的软件(例如机器学习算法)必须操作或处理一个单词时会发生什么？很简单，这个<strong class="ln jf">单词需要作为</strong>交给计算机，作为它唯一能理解的:<strong class="ln jf">数字。</strong></p><p id="522e" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">在NLP中，最简单的方法是通过<strong class="ln jf">创建一个包含大量单词的词汇表</strong>(比如说100，000个单词)<strong class="ln jf">，并为词汇表中的每个单词</strong>分配一个数字。</p><p id="7fc2" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">我们词汇中的第一个词(<em class="mh">苹果</em>也许)将是数字0。第二个字(<em class="mh">香蕉</em>’)将是数字1，依此类推直到数字99.998，前一个到最后一个字(<em class="mh">国王</em>’)和999.999被分配给最后一个字(<em class="mh">王后</em>’)。</p><p id="9898" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">然后我们将<strong class="ln jf">每个单词表示为一个长度为100.000 </strong>的向量，其中每个单项除了其中一项都是零，对应于该单词所关联的数字的索引。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/832175274833e4dfb7ea52efebc5e551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q63oC7lJmGPSgpKexs1YMg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">前面段落中一些例子的矢量表示。</p></figure><p id="6ff0" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">这被称为单词的<strong class="ln jf">一键编码</strong>。</p><p id="ec70" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf"> one-hot编码有各种不同的与效率和上下文相关的问题</strong>，我们马上就会看到。</p><p id="db5c" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">单词嵌入只是另一种形式<strong class="ln jf">通过向量</strong>表示单词，它通过某种方式抽象每个单词的上下文或高级含义<strong class="ln jf">成功地解决了使用一键编码</strong>带来的许多问题。</p><p id="5b6e" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">这里的主要要点是，单词嵌入<strong class="ln jf">是表示单词的向量，所以意思相似的单词有相似的向量。</strong></p><h1 id="03a6" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">2)为什么要使用单词嵌入？</h1><p id="5b6f" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">考虑前面的例子，但是我们的词汇中只有三个词:<em class="mh">‘苹果’，【香蕉】，</em>和<em class="mh">‘国王’。</em>这些单词的<strong class="ln jf">一个热编码</strong>向量<strong class="ln jf">表示</strong>如下。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b6a0d8c10736dc134c6bc2d2149458d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*hz-ATwbJlONkIfjpl9wfmw.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">我们词汇的一个热门向量表示</p></figure><p id="4e23" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">如果我们在三维空间<strong class="ln jf">中绘制这些单词向量，我们将得到如下图所示的表示，其中每个轴代表我们拥有的一个维度，图标代表每个单词向量的末端。</strong></p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/957b7042843c28cd75814b078273463e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oGHCRjkOSZL6GvWjmPm_A.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">三维空间中我们的一个热编码字向量的表示。</p></figure><p id="8129" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">正如我们所看到的，<strong class="ln jf">从任何一个向量</strong>(图标的位置)<strong class="ln jf">到所有其他向量的距离都是相同的</strong>:两个大小为1的步长在不同的方向。如果我们将问题扩展到100.000维，采取更多的步骤，但在所有单词向量之间保持相同的距离，这将是相同的。</p><p id="8a4e" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">理想情况下，我们希望<strong class="ln jf">具有相似含义或代表相似项目的词的向量靠近在一起，</strong>远离那些具有完全不同含义的词:我们希望<em class="mh">苹果</em>靠近<em class="mh">香蕉</em>但远离<em class="mh">国王</em>。</p><p id="9c42" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">此外，一个热编码非常<strong class="ln jf">低效</strong>。仔细想想，它们是巨大的空向量，只有一项的值不为零。它们非常稀疏，会大大降低我们的计算速度。</p><p id="6a81" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf">总之:</strong>一种热编码不考虑单词的上下文或含义，所有单词向量之间的距离相同，效率非常低。</p><p id="5832" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf">单词嵌入通过用一个相当小的(150、300、500维)固定大小的向量表示词汇表中的每个单词来解决这些问题</strong>，这个向量被称为嵌入，它是在训练期间学习的。</p><p id="8699" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">这些向量是以这样一种方式创建的，即出现在相似上下文中或者具有相似含义<strong class="ln jf">的<strong class="ln jf">单词靠得很近</strong>，并且它们不像从一键嵌入中导出的那些向量那样是稀疏向量。</strong></p><p id="d559" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">如果我们有一个<strong class="ln jf">二维单词嵌入</strong>表示我们之前的4个单词，并将其绘制在2D网格上，它看起来会像下图一样。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/cfa682dccf366e7a5c83d374d777b413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOvcH2lZXWyOtmcqwniahQ.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">我们的示例单词的二维单词嵌入表示</p></figure><p id="041e" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">从上图中我们可以清楚地看到，单词'<em class="mh"> apple </em>'和'<em class="mh"> banana </em>'之间的单词嵌入表示比单词'<em class="mh"> king </em>'和'<em class="mh"> queen </em>'之间的单词嵌入表示更接近，这适用于相反的情况:当我们使用单词嵌入时，具有相似含义的<strong class="ln jf">单词更接近。</strong></p><p id="e0fd" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">这个事实也让我们可以做一些非常非常酷的事情。我们可以<strong class="ln jf">操作单词嵌入</strong>，使用单词的表示从一个已知单词到另一个单词。</p><p id="6158" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">下图显示了<strong class="ln jf">如果我们从单词<strong class="ln jf"> ' <em class="mh"> king </em> ' </strong>的嵌入中减去单词<strong class="ln jf"> ' <em class="mh"> royal </em> ' </strong>的嵌入</strong>，我们将到达单词<strong class="ln jf"> ' <em class="mh"> man </em> ' </strong>的嵌入附近。以类似的方式，如果我们从皇后的嵌入中减去'<em class="mh"> royal' </em>的嵌入，我们到达单词<strong class="ln jf"> ' <em class="mh"> woman </em> ' </strong>的嵌入附近的某处。<strong class="ln jf">爽吧？</strong></p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/61c18d0599bab28202277797b5042e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*UQw1pQuMVZKm3gEqTAo5lQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">单词嵌入操作。国王、王后、男人、女人的例子非常流行来说明这一点。</p></figure><p id="792e" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf">最后</strong>，正如我们在单词嵌入向量中看到的，它们通常具有较小的尺寸(在我们的例子中是2，但是大多数时候它们具有150、200、300或500维)并且不是稀疏的，使得使用它们进行计算<strong class="ln jf">比使用单热点向量</strong>更有效。</p><h1 id="e5f9" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">3)单词嵌入是如何构建的？</h1><p id="6a79" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">你可能已经猜到了，像机器学习生态系统中的许多元素一样，单词嵌入是通过学习建立的。从数据中学习。</p><p id="2601" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">有许多算法可以学习单词嵌入，我们将很快看到它们，但总的目标是<strong class="ln jf">构建一个矩阵E </strong>，它可以将表示单词的一个热点向量转换为固定大小的向量，即该单词的嵌入。</p><p id="6709" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">让我们来看一个非常高级的例子。</p><p id="c558" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">想想这句话<strong class="ln jf"><em class="mh">我喜欢喝苹果冰沙</em>’</strong>。如果我<strong class="ln jf">去掉</strong>这个词<strong class="ln jf">’<em class="mh">苹果</em></strong>’我们就剩下下面这个不完整的句子:‘<strong class="ln jf"><em class="mh">我喜欢喝__思慕雪</em></strong>’。如果我接下来给你这个不完整的句子，并让你猜缺失的单词，你可能会说像'<strong class="ln jf"> <em class="mh">香蕉</em> </strong>'、<strong class="ln jf"> ' <em class="mh">草莓</em>'、</strong>或'<strong class="ln jf"> <em class="mh">苹果</em> </strong>'这样的单词，它们都有相似的意思，并且通常出现在相似的上下文中。</p><p id="4b83" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">学习单词嵌入的主要方法之一是通过与此非常相似的<strong class="ln jf">过程:算法通过<strong class="ln jf">在巨大的文本句子语料库中猜测缺失单词</strong>来学习在相似上下文中多次出现的单词的相似单词嵌入。</strong></p><p id="e434" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><strong class="ln jf">嵌入矩阵E </strong>(将一个热嵌入转换成单词嵌入向量的矩阵)<strong class="ln jf">通过训练类似于语言模型(试图预测句子中缺失单词的模型)<strong class="ln jf">的东西来计算</strong>，使用人工神经网络来预测该缺失单词</strong>，其方式类似于如何计算网络的权重和偏差。</p><p id="872c" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">在实践中，你可以避免训练你自己的单词嵌入，因为有从各种语料库构建的<strong class="ln jf">公开可用的单词嵌入(像<a class="ae no" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">维基百科或Twitter </a> GloVe单词嵌入)，节省你的时间和精力。</strong></p><p id="baea" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">最后，让我们简单看看一些最流行的单词嵌入算法。</p><h1 id="b256" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">4)最流行的单词嵌入有哪些？</h1><p id="2317" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">最常用的两种单词嵌入算法是Word2Vec和GloVe。让我们看看它们是如何工作的。</p><ul class=""><li id="4498" class="mn mo je ln b lo mi lr mj lu mp ly mq mc mr mg np mt mu mv bi translated"><strong class="ln jf">word 2 vec:</strong><em class="mh">word 2 vec</em>是一组相关的模型，它们通过使用两层、浅层的人工神经网络来产生单词嵌入，这些人工神经网络试图使用单词的上下文来预测单词(<strong class="ln jf"> <em class="mh">连续的单词包</em> </strong> — CBOW)，或者仅使用一个单词来预测上下文(<strong class="ln jf"> <em class="mh"> Skip-gram </em> </strong>模型)。这是上一节描述的过程。</li></ul><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/207de29b46e06883d77bef5747135305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiGU1FxdKZ2k5_uhNJSkuA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">CBOW和Skipgram Word2Vec型号。在CBOW中，我们试图使用上下文来预测单词apple，而在Skip-gram模型中，我们试图预测单词apple的上下文。</p></figure><ul class=""><li id="dcea" class="mn mo je ln b lo mi lr mj lu mp ly mq mc mr mg np mt mu mv bi translated"><strong class="ln jf"> GloVe: </strong>是<em class="mh">全局向量的缩写，</em><strong class="ln jf"/>GloVe算法通过在单词之间使用<strong class="ln jf">共现矩阵</strong>来计算单词嵌入。这个矩阵是<strong class="ln jf">通过阅读一个巨大的句子语料库</strong>并为它找到的每个独特的单词创建一列和一行而构建的。对于每个单词，它使用特定的窗口大小记录它与其他单词在同一个句子中出现的次数，因此它也可以测量两个单词在一个句子中的接近程度。</li></ul><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a07e9041b1ad28ab11c89c7a6bf0bf9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*JUQU_R0pRhGmvsXKQx49Yw.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">窗口大小为2的句子“我喜欢喝苹果冰沙”的同现矩阵</p></figure><h1 id="be9a" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">结论和额外资源</h1><p id="adce" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">就是这样！一如既往，我希望你喜欢这篇文章，并且我设法帮助你理解了什么是单词嵌入，它们是如何工作的，以及它们为什么如此强大。</p><p id="e630" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">如果您想了解有关该主题的更多信息，您可以在这里找到一些附加资源:</p><ul class=""><li id="7564" class="mn mo je ln b lo mi lr mj lu mp ly mq mc mr mg np mt mu mv bi translated"><a class="ae no" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mh">一种神经概率模型。</em> </a> <a class="ae no" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mh">本吉奥等</em>。</a></li><li id="7bc0" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg np mt mu mv bi translated">吴恩达教授关于学习单词嵌入的讲座。</li><li id="5b0d" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg np mt mu mv bi translated"><a class="ae no" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">机器学习掌握单词嵌入的帖子。</a></li><li id="2d03" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg np mt mu mv bi translated"><a class="ae no" href="https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/" rel="noopener ugc nofollow" target="_blank">单词嵌入及其在语义模型中的应用概述。</a></li></ul><p id="5587" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><em class="mh">如果你喜欢这篇文章，那么请随时关注我的推特@jaimezorno </em>   <em class="mh">。还有，你可以看看我其他关于数据科学和机器学习的帖子</em> <a class="ae no" href="https://medium.com/@jaimezornoza?source=post_page---------------------------" rel="noopener"> <em class="mh">这里</em> </a> <em class="mh">。好好读！</em></p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="7fd7" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated"><em class="mh">如果你想了解更多关于机器学习和人工智能的知识</em> <a class="ae no" href="https://medium.com/@jaimezornoza" rel="noopener"> <strong class="ln jf"> <em class="mh">在Medium上关注我</em> </strong> </a> <em class="mh">，敬请关注我的下一篇帖子！另外，你可以查看</em> <a class="ae no" href="https://howtolearnmachinelearning.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ln jf"> <em class="mh">这个资源库</em> </strong> </a> <em class="mh">获取更多关于机器学习和人工智能的资源！</em></p><p id="826d" class="pw-post-body-paragraph ll lm je ln b lo mi kf lq lr mj ki lt lu mk lw lx ly ml ma mb mc mm me mf mg im bi translated">最后，看看我关于NLP深度学习的其他帖子:</p><div class="is it gp gr iu nu"><a rel="noopener follow" target="_blank" href="/deep-learning-for-nlp-anns-rnns-and-lstms-explained-95866c1db2e4"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd jf gy z fp nz fr fs oa fu fw jd bi translated">NLP的深度学习:ann，RNNs和LSTMs解释！</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">了解人工神经网络，深度学习，循环神经网络和LSTMs前所未有地使用…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi ja nu"/></div></div></a></div><div class="is it gp gr iu nu"><a rel="noopener follow" target="_blank" href="/deep-learning-for-nlp-creating-a-chatbot-with-keras-da5ca051e051"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd jf gy z fp nz fr fs oa fu fw jd bi translated">NLP的深度学习:用Keras创建聊天机器人！</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">了解如何使用Keras构建递归神经网络并创建聊天机器人！谁不喜欢友好的机器人…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oj l of og oh od oi ja nu"/></div></div></a></div><ul class=""><li id="a822" class="mn mo je ln b lo mi lr mj lu mp ly mq mc mr mg np mt mu mv bi translated">封面图片来自<a class="ae no" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ln jf"><em class="mh"/></strong></a>。</li><li id="41f6" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg np mt mu mv bi translated">图标来自<a class="ae no" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ln jf"> <em class="mh">平面图标</em> </strong> </a>。</li><li id="eb84" class="mn mo je ln b lo mw lr mx lu my ly mz mc na mg np mt mu mv bi translated">所有其他图像都是自己制作的。</li></ul></div></div>    
</body>
</html>