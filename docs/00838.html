<html>
<head>
<title>Key Concepts of Modern Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">现代强化学习的关键概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/key-concepts-of-modern-reinforcement-learning-f420f6603045?source=collection_archive---------32-----------------------#2020-01-23">https://towardsdatascience.com/key-concepts-of-modern-reinforcement-learning-f420f6603045?source=collection_archive---------32-----------------------#2020-01-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="79b6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习导论</h2></div><p id="c268" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习设置的基本层次包括一个在反馈回路中与环境交互的主体。代理基于在时间<code class="fe le lf lg lh b">s_{t-1}</code>从处于前一状态的环境接收的响应，在时间<code class="fe le lf lg lh b">s_t</code>为环境的每个状态选择动作。从这个基本设置中，我们已经可以确定强化学习设置中的两个主要组件，即<strong class="kk iu">代理</strong>和<strong class="kk iu">环境</strong>。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/1449537055c538dc53aca2aeff0e6ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KHKqdHBhzpd0xsPa.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated"><strong class="bd ly">代理-环境接口的递归表示。</strong>在时间步骤<code class="fe le lf lg lh b">t</code>，代理接收环境状态的实例<code class="fe le lf lg lh b">s_t</code>。然后，代理从状态<code class="fe le lf lg lh b">s_t</code>的可用动作集中选择一个动作<code class="fe le lf lg lh b">a_t</code>。在下一次迭代中，代理收到一个新的状态实例<code class="fe le lf lg lh b">s_{t+1}</code>和一个基于在前一个状态<code class="fe le lf lg lh b">s_t</code>中采取的动作<code class="fe le lf lg lh b">a_t</code>的即时奖励<code class="fe le lf lg lh b">r_{t+1}</code>。</p></figure><p id="c9cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当代理与环境交互时，它学习一个<strong class="kk iu">策略</strong>。策略是一种“习得的策略”,它控制着代理在环境的特定时间<code class="fe le lf lg lh b">t</code>选择动作的行为。策略可以被视为从环境的状态到在那些状态中采取的动作的映射。</p><p id="1145" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化代理的目标是在反馈配置中与环境交互时最大化其长期回报。代理从每个状态-动作循环中得到的响应(代理在环境的每个状态下从一组动作中选择一个动作)被称为奖励函数。<strong class="kk iu">奖励函数</strong>(或简称奖励)是基于代理所做的动作的该状态的合意性的信号。</p><p id="f918" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“有利的”奖励可以指示代理的良好的即时事件(即，状态-动作对)。另一方面,“不利的”奖励可能对代理人来说是一个坏事件。奖励函数对于强化代理所面临的问题是唯一的，并且影响代理所做的最优策略的选择。奖励函数在很大程度上定义了强化学习任务。</p><p id="0e99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个关键的部分是一个<strong class="kk iu">值函数</strong>(或者简单的值)的概念。当代理在环境的特定状态下采取行动时，奖励函数向代理传达该状态的直接和内在的合意性。然而，结果可能是，一个有即时高回报的状态可能会导致其他非常不受欢迎的状态。这并不好，因为RL代理的目标是最大化长期回报。状态的<strong class="kk iu">价值函数</strong>是通过考虑可能的未来状态和它们的回报函数而得到的当前状态的预期长期合意性。</p><p id="a7a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">归根结底，虽然RL代理的目标是最大化价值，但奖励是代理在与环境交互时收到的主要信号。评估价值的想法是在代理人与环境相互作用的每个状态下提高奖励的质量。因此，当代理在一个状态中采取行动时，它基于价值估计来这样做，以便它可以转移到具有高价值的新状态，从而导致长期回报。</p><p id="9def" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奖励很容易获得，因为它们本质上是直接从环境中获得的反馈。另一方面，当主体反复与环境交互并收集更多信息时，必须不断地评估值。寻找一种有效的技术来估计值的任务是设计现代强化学习算法的核心。</p><p id="c707" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，重要的是要注意，虽然估计值函数已经影响了现代RL文献中的许多想法，但强化学习问题仍然可以在不估计值的情况下解决。但是当然，这种方法的效率、适用性和可伸缩性是另外一个讨论。</p><p id="cb86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们需要一个环境模型来学习强化学习代理的最优策略。环境模型必须以某种方式表示环境的随机性质，并在采取行动时向代理返回下一个状态和响应。拥有一个环境模型在计划中是有用的，在计划中，代理在采取行动之前考虑可能的未来结果。在任何情况下，强化学习系统也可以是初步的试错学习器，正如在学习自动机理论中看到的那样。通过反复试验学习的代理也可以学习环境的模型，并在以后使用它进行计算规划。</p><h1 id="420f" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">文献学</h1><ul class=""><li id="0239" class="mr ms it kk b kl mt ko mu kr mv kv mw kz mx ld my mz na nb bi translated">纳伦德拉，K. S .，&amp; Thathachar，硕士(2012)。学习自动机:导论。快递公司。</li><li id="9e61" class="mr ms it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">萨顿和巴尔托(1998年)。强化学习:导论。麻省理工出版社。</li></ul></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="b498" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">【https://ekababisong.org】最初发表于<a class="ae np" href="https://ekababisong.org/key-concepts-of-modern-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"><em class="no"/></a><em class="no">。</em></p></div></div>    
</body>
</html>