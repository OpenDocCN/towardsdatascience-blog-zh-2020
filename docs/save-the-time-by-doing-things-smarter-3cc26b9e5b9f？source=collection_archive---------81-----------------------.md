# 通过更聪明地做事来节省时间

> 原文：<https://towardsdatascience.com/save-the-time-by-doing-things-smarter-3cc26b9e5b9f?source=collection_archive---------81----------------------->

## 提高工作效率的 5 个简单技巧

![](img/6d927f3b67f7f1f95e846fb174306871.png)

西蒙·米加吉在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

时间是每个任务、每个项目的主要限制因素。如果你有无限的时间，你基本上可以做任何事情，取得任何成就。因此，明智的做法是优化你处理日常任务的方式，让它们花费更少的时间。当然，我们希望完成的工作量和质量保持不变。如果你在一个地方节省了一些额外的时间，你可以把它花在其他事情上。

在本文中，我将分享 5 个关于如何优化工作方式的技巧，并在处理数据科学项目时节省一些额外的空闲时间。实际上，这些技巧中的大部分都非常通用，不仅适用于数据科学，也适用于其他领域。

# 1.使用脚本自动化小型重复性任务

我相信这个想法对你来说并不新鲜。但是在现实中，你有多经常遵循这个建议呢？我见过许多人一次又一次地输入相同的行，每天在容易避免的地方重复完全相同的代码、文本和命令。

*不要写两遍相同的代码。*

我打赌你听说过这样一句话，甚至可能在为一些开源或商业产品编写代码时遵循这条规则。但是这条规则不仅仅对源代码有效。你的日常小任务呢？我将提供一个小例子让你更好地理解这个想法。

对于我的数据科学项目，我通常需要相当多的工具。像朱庇特笔记本，有的。txt 文件，用于存储临时笔记和链接，控制台终端打开我的项目的主目录，文件浏览器，互联网浏览器，等等。在我的互联网浏览器中，我通常喜欢打开一些预定义的标签，如我的电子邮件、我的日历、Kaggle、Matplotlib 函数参考等等。

因此，当我启动电脑时，我首先要做的是启动许多程序和应用程序。我已经测量过了——我可以设法在大约 2 分钟内开始。没有那么多，因为我一天只需要做一次，对吗？但后来我计算了一下，需要多长时间。每个工作日 2 分钟等于每周 10 分钟。总计每月 40 分钟。总计每年 480 分钟。顺便说一下，这正好是 8 小时或 1 个工作日。所以基本上，每年我都要花一整天的时间打开电脑。

然后有一天我写了一个大约 20 个命令的小脚本，它启动 Jupyter notebook，打开终端，打开浏览器，以及我需要的所有其他东西。除此之外，它甚至检查我的互联网连接，因为有时我的电脑无法捕捉无线网络，我需要重启网络接口几次才能让它工作。我的脚本替我做了。

现在我可以把这 2 分钟花在不同的任务上。例如，我可以在电脑启动时用手机查看电子邮件，或者做一些其他的小事情。我知道，那只是每天 2 分钟。但是事情总结起来。

> 如果你每个工作日只花 2 分钟做一些重复性的工作，那么到年底，你将会每天工作 8 小时。

当你在电脑上工作时，有很多事情可以实现自动化。甚至你在浏览器中的操作也可以自动化，使用一些工具，比如 Selenium，它可以移动鼠标并执行点击。我很确定在你典型的日常工作流程中有几个自动化的机会，你只需要发现它们。

# 2.开发时避免大量数据

这个我再举个例子解释一下。

您对新数据集进行探索性数据分析(称为 EDA)。这不是一个巨大的数据集，但却是一个相当大的数据集，比如说，大约有 1000 万行。在如此大的数据上，一些操作可能会非常慢。在 EDA 的过程中，你需要一次又一次地运行相同的代码片段——修复一些 bug 和错别字，调优一些参数，检查一些想法。如果每次运行只需要几秒钟，那么整个过程会非常慢。

您可以通过在开始时对整个数据集的某个随机子集进行采样来优化这一点。这个子集应该足够小，以允许代码快速运行，但也应该足够大，以代表全部数据。

只是要小心——如果你只抽取了很小一部分数据，那么就要避免对数据下结论。那就在这个阶段写一个无 bug 的代码吧。由于切换到完整版本的数据后，结论可能会发生变化。因此，如果采样数据部分足够大，效果会更好。“足够大”的确切含义可能因情况而异，但例如，大约 30-50 万行足够快，并且在大多数情况下仍然代表表格数据。

如果你使用 Pandas Python 库，那么`sample`函数可以帮你做到这一点。如下例所示，我们将包含完整数据的熊猫数据帧`df_full`采样为仅包含原始数据 20%的较小数据帧`df_used`:

```
df_used = df_full.sample(frac=0.2)
```

当您完成 EDA 并修复了所有错误后，现在可以将代码行从前面的示例切换到下面的示例:

```
#df_used = df_full.sample(frac=0.2)
df_used = df_full
```

现在，重新运行您的笔记本，只需等待一次，就可以在完整的大数据集上执行缓慢的操作。

这不仅适用于 EDA，也适用于涉及大数据规模的大多数任务，如训练模型、执行 SQL 查询等。首先，让它没有错误地工作，然后在完整数据上运行。

# 3.使用复制粘贴模板

也许有人告诉过你，复制粘贴总是不好的？如果是，不要相信他们——有些情况下，复制粘贴是最有效的做事方式。

考虑一个任务，比如绘制一些变量来呈现 EDA 结果。作为一名数据科学家，您很可能已经多次完成这样的任务。我用的是 Matplotlib Python 库。

下面给出了一些用 Matplotlib 在 Jupyter 笔记本中绘制两个数组值的简单示例代码，以供参考。

```
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(df.index, df['value1'], label='Value 1')
plt.plot(df.index, df['value2'], label='Value 2')
plt.title('Results of my EDA')
plt.legend()
```

每次从头开始写所有这些行不是很有效率。即使过一段时间后，您将学会在不查看 Matplotlib 文档的情况下编写那些行和必要的参数，它仍然会很慢，并且会有引入一些错别字的风险。

另一方面，也不值得创建自己的自定义库函数作为 Matplotlib 函数的包装，因为您将需要许多不同的绘图，通常有轻微但重要的变化。因此，这种定制的包装器代码会增长得非常快，以支持所有需要的绘图变量，过一段时间后，您会发现自己要花很多时间来维护该包装器并修复其中的错误。

对于这种情况，有一个更好的解决方案——准备一个基本模板，您可以根据需要对其进行复制粘贴和编辑。包括关于最重要参数的注释，这样您就不需要每次都在文档中查找它们。

> 抄写文本比书写快。

我有一个*。txt* 文件，包含许多常用的代码片段模板(通常每个模板有 5-10 行)。用于绘制折线图、绘制饼图、应用宁滨将连续变量转换为分类变量等的模板。每当我需要一些常用的功能时，这让我节省了一些时间。

# 4.写你自己的 StackOverflow 式的笔记

你遇到过一些不平凡的任务吗？为了完成这项任务，你很可能不得不在 Google 上搜索信息，阅读工具文档，在 YouTube 上观看演示视频，或者做一些其他耗时的事情。当你经过几个小时的奋斗成功完成任务后，你会愉快地忘记它。有时…直到下一次你需要再次做完全相同的事情。

几年前，当我刚刚开始从事数据科学项目时，我必须安装 XGBoost 库，这是当时用于表格数据预测的最先进的库。在那时，在我的 Windows 上安装 XGBoost 确实是一项重要的任务，因为要获得最新版本，我必须从源代码中编译它。为了编译它，我必须先安装 Visual Studio。此外，我必须安装一些依赖项才能让一切正常工作。基本上，我花了整个晚上的时间从头开始设置一切。

在我的 XGBoost 在 Windows 上运行之后，我高兴地开始使用它…直到大约 4 个月后我的硬盘崩溃。然后，我花了几乎完全相同的时间再次安装它，因为我几乎不记得我到底做了什么让它工作。现在，这次我花了额外的 5 分钟来记下关键点、主要信息来源的链接以及最棘手部分的解决方案。

大约 6 个月后，我买了一台新的笔记本电脑，这为我节省了很多时间。然后当我咨询我的朋友在他的机器上安装 XGBoost 时。

每当你遇到一个不简单的棘手任务时，就值得考虑记下一些关于解决方案的简短笔记。因为有可能你将不得不再次做同样的事情。可以用简单的*。txt* 文件，一些应用程序的笔记或任何你喜欢的工具。我建议您保持在线访问，以防您可能需要从其他设备上查看它。

# 5.使长程序步骤独立

考虑一个典型的机器学习流水线。您读取一些输入数据，执行清理，转换数据，计算要素，训练模型，然后进行预测。比方说，你有一个程序，一个接一个地完成所有这些步骤。现在，您获得了一些额外的输入数据，并希望重新计算您的预测。这很简单——你只要运行你的程序，过一段时间它就会给你新的预测。

现在，让我们说，你想检查是否改变你的模型中的一些参数可以给你更好的准确性。你改变程序中的参数，重新运行程序，再次得到新的预测。但是等等——这一次您实际上不需要重新读取输入数据。所有的数据转换和特征计算也不是必须的。你只是想用新的参数重新训练模型。

因此，如果有可能从某个预定义的点重新运行程序，效率会更高。在这个特殊的例子中，只是模型训练和预测。

> 通过不必要的执行代码，你只是在浪费时间和电力。

这就是我非常喜欢使用 Jupyter 笔记本来完成数据科学任务的原因。它们内置了将整个程序分割成小步骤(单元)的支持。每个单元都可以独立执行。要记住的主要技巧是不要修改先前单元格中定义/计算的变量。否则，从中间的某个地方重新运行笔记本时，可能会导致错误的结果。

实际上，使用传统的 Python 脚本也可以实现类似的独立步骤，只需要一些额外的小工作。执行此操作的示例伪代码:

```
step = read_input_argument()
if step==0: #read and clean initial input data data = read_and_clean_data() save_to_disk(data, 'step1.csv')
if step<=1: #transform data
  if step==1:
    data = read_from_disk('step1.csv') data = transform(data) save_to_disk(data, 'step2.csv')
if step<=2: #calculate features
  if step==2:
    data = read_from_disk('step2.csv') data = get_features(data) ...
```

这样的程序接收`step`参数。如果它的值为 0，将执行整个程序。如果该值等于 1，程序将跳过清理输入数据，但将使用一个预先保存的临时文件与已经清理的数据。通过这种方式，您可以指定开始程序执行的步骤，从而节省大量时间，不必花费无数次清理和转换不需要的数据。

另外一个建议是保持足够大的步长。否则，您可能会遇到这样的情况:在每一步之后，写入/读取中间临时文件的成本都要高于在步骤中完成的实际工作的成本。

# 一些最后的话

![](img/54f881eff40743bf830220b537feb2d6.png)

由[约书亚·厄尔](https://unsplash.com/@joshuaearle?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄的照片

不是所有的东西都适合每个人。所以，可能不是所有的建议都像对我一样对你有用。但希望你能得到一些改进工作方式的新想法。

请随意留下你关于节省时间和更有效做事的建议和技巧的评论。

效率高，事半功倍，给自己多留点时间！

感谢阅读！