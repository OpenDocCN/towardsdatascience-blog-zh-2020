<html>
<head>
<title>Dealing with Highly Dimensional Data using Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析(PCA)处理高维数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6?source=collection_archive---------9-----------------------#2020-04-24">https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6?source=collection_archive---------9-----------------------#2020-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad43" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PCA入门指南以及如何用sklearn实现(带代码！)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1b2fbcecd83f258b874713a997ee3e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLjoPzxh0NILH7-aWNacBg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">纳比尔·侯赛因在<a class="ae kv" href="https://unsplash.com/s/photos/3d?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9844" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据科学家在创建算法时的一个常见问题是变量太多。很自然，你会认为添加更多的信息只会让你的模型更好，但是你添加的每一个特性都会带来另一个维度。作为人类，我们只能想象二维或三维的事物。对于数据，此规则不适用！数据可以有无限多的维度，但这就是维度诅咒<strong class="ky ir"> <em class="ls"> </em> </strong>发挥作用的地方。</p><p id="25e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">维数灾难</em> </strong>是数据科学家经常面临的悖论。为了提高机器学习模型的准确性，您希望使用更多的信息，但是添加的功能越多，维数(n)就会增加。随着特征空间的维数增加，配置的数量呈指数增加，并且反过来，观察覆盖的配置的数量减少。</p><p id="6cc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为数据科学家，我们的最终目标是创建能够快速运行且易于解释的简单模型。当我们有大量的特征时，我们的模型变得更加复杂，可解释性降低。为了处理这些复杂的数据集，主成分分析是降低数据维数的理想方法。</p><h2 id="9906" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">什么是主成分分析，用来做什么？</strong></h2><p id="2885" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">主成分分析</em> </strong>，或者更俗称<strong class="ky ir"> <em class="ls"> PCA </em> </strong>，是一种在保持大部分重要信息的同时减少变量数量的方法。它将多个可能相关的变量转化为数量较少的不相关变量，称为<strong class="ky ir"> <em class="ls">主成分</em> </strong>。主成分是原始变量的线性组合，这些变量在特定的正交维度上由它们的方差(或特征值)加权。PCA的主要目的是将模型特征简化为更少的组件，以帮助可视化数据中的模式，并帮助模型更快地运行。使用PCA还可以通过消除高度相关的特征来减少过度拟合模型的机会。</p><p id="ee83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要注意的是，你应该只对<strong class="ky ir"> <em class="ls">连续</em> </strong>变量应用PCA，而不是分类变量。虽然从技术上讲，您可以对一位热编码或二进制数据使用PCA，但它的效果不是很好。这是因为PCA被设计为最小化方差(方差),这在对二元变量执行时没有太大意义。如果您有混合数据，替代方法如<a class="ae kv" href="https://www.researchgate.net/profile/Dominique_Valentin/publication/239542271_Multiple_Correspondence_Analysis/links/54a979900cf256bf8bb95c95.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ls">MCA</em></strong></a><strong class="ky ir"><em class="ls"/></strong>可能会更好。</p><h2 id="6f9d" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">那么，您如何知道您的PCA中保留了多少信息呢？</strong></h2><p id="cf1e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们使用<strong class="ky ir"> <em class="ls">解释方差比率</em> </strong>作为一种度量标准来评估您的主成分的有用性，并选择在您的模型中使用多少成分。解释的差异比率是由每个所选组件引起的差异的百分比。理想情况下，您可以通过添加每个组件的解释方差比率来选择要包括在您的模型中的组件数量，直到您达到大约0.8或80%的总数，以避免过度拟合。</p><p id="639d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，sklearn通过他们的<strong class="ky ir"> <em class="ls">很容易得到解释的方差比。解释_方差_比率_ </em> </strong>参数！我们将在编码示例中使用它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/8a2e149e7f090609081224c95e941bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oqTTgc61bvTx5fHsCnODNA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">美国宇航局在<a class="ae kv" href="https://unsplash.com/s/photos/galaxy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="510e" class="ms lu iq bd lv mt mu mv ly mw mx my mb jw mz jx me jz na ka mh kc nb kd mk nc bi translated">使用Sklearn的PCA示例</h1><ol class=""><li id="e0ab" class="nd ne iq ky b kz mm lc mn lf nf lj ng ln nh lr ni nj nk nl bi translated">首先，让我们为我们的长代码示例加载iris数据集。iris数据集是一个著名的数据集，包含来自三个不同物种的150朵iris花的测量数据。</li></ol><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="3591" class="lt lu iq nn b gy nr ns l nt nu">from sklearn import datasets<br/>import pandas as pd<br/> <br/>iris = datasets.load_iris()<br/>df = pd.DataFrame(iris.data, columns=iris.feature_names)<br/>df['Target'] = iris.get('target')<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3a7207bf8ffce9c2356a842e7651334f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*Q6sIXy4iT1nnkp8JIDyneg.png"/></div></figure><p id="0f66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.接下来，我们将把“features”列表中的所有列分隔成变量“X ”,把“target”变量分隔成变量“y”。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="6183" class="lt lu iq nn b gy nr ns l nt nu"># Create features and target datasets<br/>features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']<br/>X = df[features].values<br/>y = df['Target'].values</span></pre><p id="ebc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.在实现PCA之前，我们必须将数据标准化。这是<strong class="ky ir">绝对必要的</strong>，因为PCA使用我们数据的标准偏差计算我们数据在新轴上的新投影。PCA给予方差较高的变量比方差较低的变量更大的权重，因此在相同的尺度上归一化数据以获得合理的协方差是很重要的。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="85d7" class="lt lu iq nn b gy nr ns l nt nu">from sklearn.preprocessing import StandardScaler<br/><br/># Standardize the features<br/>X = StandardScaler().fit_transform(X)<br/><br/># Preview X<br/>pd.DataFrame(data=X, columns=features).head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3cf89ecbfe31eb5f51b8d9707c02fc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*0-AEV7g1k9YPem5Nme5oFQ.png"/></div></figure><p id="5d80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.现在，我们将使用sklearn导入PCA，并将我们的原始数据(4维)投影到2维。在这一部分中，sklearn创建了一个协方差矩阵来计算特征向量(主分量)及其对应的特征值。<strong class="ky ir"> <em class="ls">特征向量</em> </strong>确定新特征空间的方向，而<strong class="ky ir"> <em class="ls">特征值</em> </strong>确定沿着新特征轴的数据的幅度或方差。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="8ce0" class="lt lu iq nn b gy nr ns l nt nu"># Import PCA from sklearn<br/>from sklearn.decomposition import PCA<br/><br/># Instantiate PCA<br/>pca = PCA(n_components=2)<br/><br/># Fit PCA to features<br/>principalComponents = pca.fit_transform(X)</span></pre><p id="b140" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.为了更好地可视化主成分，让我们将它们与熊猫数据帧中与特定观察相关联的目标(花类型)配对。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="9b22" class="lt lu iq nn b gy nr ns l nt nu"># Create a new dataset from principal components <br/>df = pd.DataFrame(data = principalComponents, <br/>                  columns = ['PC1', 'PC2'])<br/><br/>target = pd.Series(iris['target'], name='target')<br/><br/>result_df = pd.concat([df, target], axis=1)<br/>result_df.head(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/77b2d33e469910380ceca5240162f0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*f1pm2iWvZ4YguqW2bpJqxA.png"/></div></figure><p id="a54f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6.现在，我们可以使用目标数据根据类别分布来可视化主成分。这段代码根据主要成分创建了一个散点图，同时根据每个示例被分类的花类型对示例进行了颜色编码。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="7197" class="lt lu iq nn b gy nr ns l nt nu"># Visualize Principal Components with a scatter plot<br/>fig = plt.figure(figsize = (12,10))<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel('First Principal Component ', fontsize = 15)<br/>ax.set_ylabel('Second Principal Component ', fontsize = 15)<br/>ax.set_title('Principal Component Analysis (2PCs) for Iris Dataset', fontsize = 20)<br/><br/>targets = [0, 1, 2]<br/>colors = ['r', 'g', 'b']<br/>for target, color in zip(targets, colors):<br/>    indicesToKeep = iris['target'] == target<br/>    ax.scatter(result_df.loc[indicesToKeep, 'PC1'], <br/>               result_df.loc[indicesToKeep, 'PC2'], <br/>               c = color, <br/>               s = 50)<br/>ax.legend(targets)<br/>ax.grid()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/76369a5a8c73e045298026fe1cf2f378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*w86VhHDBtwZuRDy-Cb-PRw.png"/></div></figure><p id="6dee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7.我们可以看到，这三个类是截然不同的，也是完全可以分开的。我们可以得出结论，压缩的数据表示对于分类模型来说很可能是足够的。我们可以将整个数据集中的差异与使用。解释_方差_比率_。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="2463" class="lt lu iq nn b gy nr ns l nt nu"># Calculate the variance explained by priciple components<br/>print('Variance of each component:', pca.explained_variance_ratio_)<br/>print('\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c712342f0e4b80a862938216b4400405.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*UWl75tJg0FDViEwn_dg7Ig.png"/></div></figure><p id="8fcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，我们的前两个主成分解释了该数据集中的大部分方差(95.81%)！这是与原始数据相比所表示的总信息的指示。</p><h1 id="99d1" class="ms lu iq bd lv mt mu mv ly mw mx my mb jw mz jx me jz na ka mh kc nb kd mk nc bi translated">摘要</h1><p id="2a67" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这篇文章的要点是:</p><ul class=""><li id="3280" class="nd ne iq ky b kz la lc ld lf oa lj ob ln oc lr od nj nk nl bi translated">PCA是一种常用的<strong class="ky ir">降维</strong>技术。</li><li id="4833" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr od nj nk nl bi translated">PCA的目标是将模型要素简化为更少的不相关要素，以帮助可视化数据中的模式，并帮助它更快地运行。</li><li id="c88c" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr od nj nk nl bi translated">仅对<strong class="ky ir">连续</strong>数据应用PCA。</li><li id="31d1" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr od nj nk nl bi translated">在应用PCA之前，请确保您的数据已经过<strong class="ky ir">规范化</strong>！！</li></ul><p id="5ce1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">***</p><p id="0e86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">感谢阅读！</strong></p><p id="aa06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">如果你喜欢我的博文，可以关注我的媒体</strong> <a class="ae kv" href="https://medium.com/@isalindgren313" rel="noopener"> <strong class="ky ir"> <em class="ls">这里</em> </strong> </a> <strong class="ky ir">支持我。也可以通过我的LinkedIn </strong> <a class="ae kv" href="https://www.linkedin.com/in/isabellalindgren/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ls">这里</em> </strong> </a> <strong class="ky ir">与我联系。</strong></p><p id="5146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">***</p><p id="9f3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献:</strong></p><ol class=""><li id="908c" class="nd ne iq ky b kz la lc ld lf oa lj ob ln oc lr ni nj nk nl bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></li><li id="2782" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality" rel="noopener ugc nofollow" target="_blank">https://deepai . org/machine-learning-glossary-and-terms/curse-of-dimensionality</a></li><li id="9e1e" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://elitedatascience.com/overfitting-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://elitedata science . com/over fitting-in-machine-learning</a></li><li id="6267" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://www.geeksforgeeks.org/principal-component-analysis-with-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/principal-component-analysis-with-python/</a></li><li id="3bb9" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></li><li id="a82e" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://www.researchgate.net/profile/Dominique_Valentin/publication/239542271_Multiple_Correspondence_Analysis/links/54a979900cf256bf8bb95c95.pdf" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/profile/Dominique _ Valentin/publication/239542271 _ Multiple _ communication _ Analysis/links/54a 979900 cf 256 BF 8 bb 95 c 95 . pdf</a></li><li id="a992" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/" rel="noopener ugc nofollow" target="_blank">https://www . vision dummy . com/2014/03/特征值-特征向量/ </a></li><li id="bd1a" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2015 _ PCA _ in _ 3 _ steps . html</a></li><li id="d65e" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated"><a class="ae kv" href="https://etav.github.io/python/scikit_pca.html" rel="noopener ugc nofollow" target="_blank">https://etav.github.io/python/scikit_pca.html</a></li><li id="a2b4" class="nd ne iq ky b kz oe lc of lf og lj oh ln oi lr ni nj nk nl bi translated">熨斗数据科学课程第37节</li></ol></div></div>    
</body>
</html>