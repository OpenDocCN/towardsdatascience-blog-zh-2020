<html>
<head>
<title>Gradient Descent Extensions to Your Deep Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习模型的梯度下降扩展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-extensions-to-your-deep-learning-models-32045ccfa644?source=collection_archive---------45-----------------------#2020-03-22">https://towardsdatascience.com/gradient-descent-extensions-to-your-deep-learning-models-32045ccfa644?source=collection_archive---------45-----------------------#2020-03-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3e70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">了解不同的可用方法，并选择最适合的方法来解决您的问题。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/2925d8afd89bc98db20b9d0936796666.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*pZsQ14NbpAjywfvypGUCag.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来源:<a class="ae la" href="https://pixabay.com/illustrations/artificial-intelligence-brain-think-3382507/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="43c1" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="5256" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">本文的目的是探索不同的梯度下降扩展，如动量，Adagrad，RMSprop…</p><p id="abcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae la" rel="noopener" target="_blank" href="/neural-networks-from-hero-to-zero-afc30205df05">之前的文章</a>中，我们已经研究了在深度学习模型中实现反向传播的三种方法:</p><ul class=""><li id="78ab" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated">梯度下降</li><li id="96f1" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">随机梯度下降</li><li id="1641" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">小批量随机梯度下降</li></ul><p id="529f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在此基础上，我们保留小批量，因为它允许更高的速度，因为它不必计算整个数据集的梯度和误差，并且消除了随机梯度下降中存在的高可变性。</p><p id="4692" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些方法有所改进，比如动量法。此外，还有其他更复杂的算法，如Adam、RMSProp或Adagrad。</p><p id="f62a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看他们！</p><h1 id="c851" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">动力</h1><p id="1e99" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">想象自己又回到了童年，有一个穿上溜冰鞋，爬上最陡的街道，开始沿着它走下去的好主意。你完全是初学者，这是你第二次穿冰鞋。</p><p id="d816" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我不知道你们中有没有人真的这么做过，但是我确实做过，所以让我来解释一下发生了什么:</p><ul class=""><li id="693c" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated">你刚刚起步，速度很慢，你甚至似乎在控制之中，你随时都可能停下来。</li><li id="0348" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">但是你走得越低，你移动得越快:这叫做动量。<br/>所以你走的路越多，你背负的惯性就越大，你走的就越快。</li><li id="8047" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">好吧，对于那些好奇的人来说，故事的结尾是在陡峭街道的尽头有一个栅栏。剩下的你可以想象…</li></ul><p id="3dce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">动量技巧正是这样。当我们沿着损失曲线计算梯度并进行更新时，我们会更加重视梯度最小化方向上的更新，而不太重视其他方向上的更新。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/4aa0ff54a6f8e17a1df927c0dbbd9969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kySelLgxhyvHCuNJsBnwjA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图</p></figure><p id="15f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，结果是加快了网络的训练。</p><p id="db4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，多亏了这一时刻，我们本可以避开路上的小坑洼或小洞(由于速度，我们可以飞越它们)。</p><p id="38d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这篇伟大的文章中了解更多关于这种技术背后的数学基础:<a class="ae la" href="http://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/neural-networks-3/#sgd</a></p><h1 id="f12b" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">内斯特罗夫动量</h1><p id="708c" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">回到之前的例子:我们正在全速前进(因为我们已经建立了很大的势头)，突然我们看到了它的尽头。我们希望能够刹车，减速以避免撞车。这正是内斯特罗夫所做的。</p><p id="198e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">内斯特罗夫计算梯度，但不是在当前点，而是在我们知道我们的时刻将带我们去的点，然后应用一个修正。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2f47b416ae0ea398a242d7288253d219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OE2S_0uZ8otXXUFCsfpnhg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">按作者分列的数字</p></figure><p id="434b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，使用标准矩，我们计算梯度(小橙色向量)，然后在梯度方向上迈出一大步(大橙色向量)。</p><p id="1898" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用内斯特罗夫，我们将首先在我们先前的梯度(绿色向量)的方向上进行一次大的跳跃，测量梯度并进行适当的校正(红色向量)。</p><p id="2e1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在实践中，它比单独使用动量要好得多。这就像计算未来的权重梯度(因为我们加入了之前计算的时刻)。</p><p id="4f09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这篇伟大的文章中了解更多关于这种技术背后的数学基础:<a class="ae la" href="http://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/neural-networks-3/#sgd</a></p><p id="4ced" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">内斯特罗夫动量和标准动量都是SGD的延伸。</p><p id="2d90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在将要看到的方法是基于自适应学习速率的，允许我们加快或减慢更新权重的速度。例如，我们可以在开始时使用高速，在接近最小值时降低速度。</p><h1 id="b9ee" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">自适应梯度</h1><p id="3282" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">它保存计算梯度的历史记录(特别是梯度平方和的历史记录),并对更新的“步骤”进行标准化。</p><p id="caa8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其背后的直觉是，它识别具有非常高梯度的参数，这些参数的权重更新将非常突然，然后给它们分配较低的学习速率以减轻这种突然性。</p><p id="3bf6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同时，具有非常低的梯度的参数将被分配高的学习率。</p><p id="208f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样，我们设法加速了算法的收敛。</p><p id="7b10" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在它的原始论文中了解更多关于这种技术背后的理论:【http://jmlr.org/papers/v12/duchi11a.html<a class="ae la" href="http://jmlr.org/papers/v12/duchi11a.html" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="7671" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">RMSprop</h1><p id="9dc5" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">AdaGrad的问题是，在计算平方梯度的和时，我们使用的是单调递增函数，这会导致学习速率试图补偿直到变为零才停止增长的值，从而停止学习。</p><p id="13c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RMSprop建议使用decay_rate来减少梯度的平方和。</p><p id="da36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">论文尚未发表，但可以在这里阅读更多:<a class="ae la" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . Toronto . edu/~ tij men/CSC 321/slides/lecture _ slides _ le C6 . pdf</a></p><h1 id="7077" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="10e0" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">最后，Adam是最现代的算法之一，它通过向更新规则添加动量来改进RMSprop。它引入了2个新参数，beta1和beta2，建议值为0.9和0.999。</p><p id="d8f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里查看它的论文:<a class="ae la" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6980</a>。</p><h1 id="f87a" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">但是，我们应该使用哪一个呢？</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/d2ed01f3ffaf92a72c4dce37bb9de5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*OgVO3oC5dOXXQ08Jz6pE-A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来源:<a class="ae la" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">原创亚当论文</a></p></figure><p id="f436" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据经验，建议从Adam开始。如果它不能很好地工作，那么你可以尝试和调整其余的技术。但大多数时候，亚当工作得很好。</p><p id="5d2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以查看这些资源，以便更好地理解这些技术，以及如何和何时应用它们:</p><ul class=""><li id="5f01" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated">【https://deepnotes.io/sgd-momentum-adaptive T4】</li><li id="c515" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated"><a class="ae la" href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">http://ruder . io/optimizing-gradient-descent-descent/index . html # gradient descent optimizationalgorithms</a></li></ul><h1 id="b0f1" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">最后的话</h1><p id="d0f9" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">一如既往，我希望你<strong class="js iu"> </strong>喜欢这个职位！</p><p id="8a3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mz">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae la" href="https://medium.com/@rromanss23" rel="noopener"> <em class="mz">这里</em> </a> <em class="mz">。</em></p><p id="704a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mz">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="js iu"> <em class="mz">请在Medium </em> </strong> <em class="mz">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>