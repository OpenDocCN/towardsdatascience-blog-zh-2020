<html>
<head>
<title>Neural Network from Scratch With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 从零开始的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualize-how-a-neural-network-works-from-scratch-3c04918a278?source=collection_archive---------16-----------------------#2020-09-04">https://towardsdatascience.com/visualize-how-a-neural-network-works-from-scratch-3c04918a278?source=collection_archive---------16-----------------------#2020-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e1a8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过可视化每一步的结果，您可以更好地理解简单的神经网络是如何工作的</h2></div><p id="363f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络通常被认为是一种黑盒算法。数据可视化可以帮助我们更好地理解这种算法的原理。由于标准软件包没有给出如何找到参数的所有细节，我们将从头开始编写一个神经网络。为了直观地显示结果，我们选择了一个简单的数据集。</p><h1 id="d6f1" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">简单的数据集和神经网络结构</h1><p id="19d5" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">让我们使用这个只有一个特征 x 的简单数据集。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="08f0" class="mk lf it mg b gy ml mm l mn mo">import numpy as np</span><span id="f8ff" class="mk lf it mg b gy mp mm l mn mo">X=np.array([[-1.51], [-1.29], [-1.18], [-0.64],<br/>[-0.53], [-0.09], [0.13], [0.35],<br/>[0.89], [1.11], [1.33], [1.44]])</span><span id="07de" class="mk lf it mg b gy mp mm l mn mo">y=np.array([[0], [0], [0], [0],<br/>[1], [1], [1], [1],[0], [0], [0], [0]])</span></pre><p id="ae91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">x 是具有 12 个观察值的单列向量，y 也是具有 12 个值的列向量，这些值表示目标。我们可以将这个数据集可视化。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="5cf5" class="mk lf it mg b gy ml mm l mn mo">import matplotlib.pyplot as plt<br/>plt.scatter(X,y)</span></pre><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/cc1e894de048752c7ac45e1228c4aa9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*UPc0mtwYbtH2KJYCHBsRJA.png"/></div></figure><p id="a324" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于那些已经知道神经网络如何工作的人来说，通过看到这个图表，你应该能够找到一个简单的结构。在下一部分中，激活函数将是 sigmoid 函数。</p><p id="7220" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以问题是:<strong class="kk iu">我们需要多少层和多少个神经元来建立一个适合上面数据集的神经网络？</strong></p><p id="3746" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们只使用一个神经元，这与进行逻辑回归是一样的，因为激活函数是 sigmoid 函数。我们知道这是行不通的，因为数据集不是线性可分的，简单的逻辑回归不适用于非线性可分的数据。所以我们必须添加一个隐藏层。隐藏层中的每个神经元将导致线性决策边界。</p><p id="5661" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，逻辑回归创建一个超平面作为决策边界。由于这里我们只有一个特征，那么这个超平面只是一个点。从视觉上，我们可以看到我们需要两个点来区分两个类。它们的值一个是-0.5，另一个是 0.5。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/727515fbc732513d61bc51f31a7e68f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qM2oEwBJ8VRM8VEbQCZzlg.png"/></div></div></figure><p id="4f7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，具有以下结构的神经网络将是我们数据集的良好分类器。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mz"><img src="../Images/c59b399421e232a45e1c64ee851a664c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7LqTXXN0bIJJBEn0-MGHw.png"/></div></div></figure><p id="71ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不清楚，你可以看看这篇文章。</p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/intuitively-how-do-neural-networks-work-d7710b602e51"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">直观来看，神经网络是如何工作的？</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">“神经网络”这个术语可能看起来很神秘，为什么一个算法叫做神经网络？它真的模仿真实的…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr ms nd"/></div></div></a></div><h1 id="5bae" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">使用 scikit 学习 MLPClassifier</h1><p id="29c1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在从头开始构建神经网络之前，让我们首先使用已经构建的算法来确认这样的神经网络是合适的，并可视化结果。</p><p id="ca85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用 scikit learn 中的<strong class="kk iu"> MLPClassifier </strong>。在下面的代码中，我们用参数<strong class="kk iu"> hidden_layer_sizes </strong>指定隐藏层的数量和神经元的数量。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="97ef" class="mk lf it mg b gy ml mm l mn mo">from sklearn.neural_network import MLPClassifier</span><span id="75ae" class="mk lf it mg b gy mp mm l mn mo">clf = MLPClassifier(solver=’lbfgs’,hidden_layer_sizes=(2,), activation=”logistic”,max_iter=1000)</span><span id="cbe4" class="mk lf it mg b gy mp mm l mn mo">clf.fit(X, y)</span></pre><p id="34a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们就可以计算分数了。(您应该得到 1.0，否则，由于局部最小值，您可能必须再次运行代码)。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="9ec5" class="mk lf it mg b gy ml mm l mn mo">clf.score(X,y)</span></pre><p id="9a3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太棒了，怎么才能把算法的结果可视化呢？由于我们知道这个神经网络是由 2+1 逻辑回归构成的，所以我们可以用下面的代码得到参数。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="a255" class="mk lf it mg b gy ml mm l mn mo">clf.coefs_<br/>clf.intercepts_</span></pre><p id="4bcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们如何解释这些结果？</p><p id="be0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<strong class="kk iu"> clf.coefs_ </strong>，你会得到(例如):</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="a570" class="mk lf it mg b gy ml mm l mn mo">[array([[-20.89123833, -8.09121263]]), array([[-20.19430919], [ 17.74430684]])]</span></pre><p id="7e3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且对于<strong class="kk iu"> clf.intercepts_ </strong></p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="374a" class="mk lf it mg b gy ml mm l mn mo">[array([-12.35004862, 4.62846821]), array([-8.19425129])]</span></pre><p id="76de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列表的第一项包含隐藏层的参数，第二项包含输出层的参数。</p><p id="d956" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这些参数，我们可以绘制曲线:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="9ce6" class="mk lf it mg b gy ml mm l mn mo">def sigmoid(x):<br/>    return 1.0/(1+ np.exp(-x))</span><span id="a174" class="mk lf it mg b gy mp mm l mn mo">plt.scatter(X,y)<br/>a1_1=sigmoid(xseq*clf.coefs_[0][0,0]+clf.intercepts_[0][0])<br/>a1_2=sigmoid(xseq*clf.coefs_[0][0,1]+clf.intercepts_[0][1])<br/>output=sigmoid(a1_1*clf.coefs_[1][0]+a1_2*clf.coefs_[1][1]+clf.intercepts_[1])</span><span id="9201" class="mk lf it mg b gy mp mm l mn mo">plt.plot(xseq,a1_1,c=”red”)<br/>plt.plot(xseq,a1_2,c=”blue”)<br/>plt.plot(xseq,output,c=”black”)</span></pre><p id="cb5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以得到下面的图表:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ns"><img src="../Images/85a91f70764781b7e2d23c5760a3d361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMQwXtxbpWN1JfEZHXrEXA.png"/></div></div></figure><ul class=""><li id="01d3" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">红色的是隐藏层的神经元 1 的结果</li><li id="e0c5" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">蓝色的是隐藏层的神经元 2 的结果</li><li id="c2d0" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">黑色的是输出</li></ul><p id="a721" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您运行代码，您可能会得到另一个结果，因为损失函数有几个全局最小值。</p><p id="053b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<strong class="kk iu"> keras </strong>中，当然也可以创建相同的结构:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="b0c7" class="mk lf it mg b gy ml mm l mn mo">from keras.models import Sequential<br/>from keras.layers import Dense<br/>model = Sequential()<br/>model.add(Dense(2, activation=’sigmoid’))<br/>model.add(Dense(1, activation=’sigmoid’))<br/>model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])<br/>model.fit(X_train, y_train, epochs=300)</span></pre><h1 id="341b" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">从头开始编码</h1><p id="0c25" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">现在的问题是这七个参数是怎么找到的？一种方法是使用梯度下降。</p><h2 id="c33a" class="mk lf it bd lg oh oi dn lk oj ok dp lo kr ol om lq kv on oo ls kz op oq lu or bi translated">正向传播</h2><p id="a537" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">首先，让我们做正向传播。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7c526293fa0b8a7a21c69abf1ec672c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*YFmS4CWb6DAaJLLg2oZiSg.png"/></div></figure><p id="6984" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个神经元，我们必须找到权重 w 和偏差 b。让我们尝试一些随机值。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="427e" class="mk lf it mg b gy ml mm l mn mo">plt.scatter(X,y)<br/>plt.plot(xseq,sigmoid(xseq*(-11)-11),c="red")</span></pre><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ot"><img src="../Images/2a80a2324d13aae6b0a0ca5d4f5193ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9LBVioXIlUV1dK2JuTLjQ.png"/></div></div></figure><p id="2e66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于有两个神经元，我们可以通过为参数创建矩阵来进行矩阵乘法:</p><ul class=""><li id="3621" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">权重矩阵应该有两列。(因为这里输入数据有一列，所以权重矩阵应该有一行两列)。我们可以做一个随机的初始化，选择一些值。</li><li id="2370" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">偏置应该具有相同的结构。</li></ul><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="655b" class="mk lf it mg b gy ml mm l mn mo">w1=np.random.rand(X.shape[1],2) # random initialization<br/>w1=np.array([[-1,9]])<br/>b1=np.array([[-1,5]])</span><span id="9535" class="mk lf it mg b gy mp mm l mn mo">z1=np.dot(X, w1)+b1<br/>a1=sigmoid(z1)</span></pre><p id="ef19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您只是在阅读，而不是同时运行笔记本，可以做的一个练习是回答以下问题:</p><ul class=""><li id="cae5" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">np.dot(X，w1)的维数是多少？</li><li id="16e4" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">z1 的尺寸是多少？</li><li id="1680" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">为什么做加法可以？如果 b1 是一个简单的一维数组呢？</li><li id="4d9e" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">a1 的维数是多少？</li></ul><p id="a50b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们用之前创建的 xseq 替换输入 X，我们可以绘制曲线:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c9f7" class="mk lf it mg b gy ml mm l mn mo">a1_seq=sigmoid(np.dot(xseq.reshape(-1,1), w1)+b1)</span><span id="f476" class="mk lf it mg b gy mp mm l mn mo">plt.scatter(X,y)<br/>plt.plot(xseq,a1_seq[:,0],c="red")<br/>plt.plot(xseq,a1_seq[:,1],c="blue")</span></pre><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ot"><img src="../Images/5c9b9027817e8da2314bbbb262d3f89a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qr-fkkG7Z_g7IlzWdaXPQ.png"/></div></div></figure><p id="8d70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在对于输出，这是一个非常相似的计算。</p><ul class=""><li id="9054" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">权重矩阵现在有了行，因为隐藏层产生了两列的矩阵。</li><li id="d570" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">偏差矩阵是一个标量</li></ul><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c631" class="mk lf it mg b gy ml mm l mn mo">w2 = np.random.rand(2,1)<br/>b2=0<br/>output=sigmoid(np.dot(a1,w2)+b2)</span></pre><p id="0038" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以用其他曲线来绘制输出</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="1b99" class="mk lf it mg b gy ml mm l mn mo">output_seq=sigmoid(np.dot(a1_seq,w2)+b2)</span><span id="531c" class="mk lf it mg b gy mp mm l mn mo">plt.scatter(X,y)<br/>plt.plot(xseq,a1_seq[:,0],c=”red”)<br/>plt.plot(xseq,a1_seq[:,1],c=”blue”)<br/>plt.plot(xseq,output_seq,c=”black”)</span></pre><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ot"><img src="../Images/91d3519da032bcb6c2dcb4f1091a9c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qp9X99jcHZp7EKGG4ImAew.png"/></div></div></figure><p id="a230" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，随机选择的参数并不好。</p><p id="a38a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总结一下正向传播:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="b0da" class="mk lf it mg b gy ml mm l mn mo">def feedforward(input,w1,w2,b1,b2):<br/>    a1 = sigmoid(np.dot(input, w1)+b1)<br/>    output = sigmoid(np.dot(a1, w2)+b2)<br/>    return output</span></pre><h2 id="bcc3" class="mk lf it bd lg oh oi dn lk oj ok dp lo kr ol om lq kv on oo ls kz op oq lu or bi translated">成本函数的可视化</h2><p id="d892" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">合适的参数是那些最小化成本函数的参数。我们可以使用交叉熵:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ou"><img src="../Images/2246b32ae2b059bf0b7506e3e9b20c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlixIq-lI04IUMxF09bO_Q.png"/></div></div></figure><p id="a87c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数可以编码如下:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="e103" class="mk lf it mg b gy ml mm l mn mo">def cost(y,output):<br/>    return -np.sum(y*np.log(output)+(1-y)*np.log(1-output))/12</span></pre><p id="ca38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于有 7 个参数，可视化成本函数并不容易。我们就选择其中一个来变化吧。例如 w1 中的第一重量。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="eeb0" class="mk lf it mg b gy ml mm l mn mo">b1=np.array([[16.81,-23.41]])<br/>w2= np.array([28.8,-52.89])<br/>b2=-17.53</span><span id="ed9a" class="mk lf it mg b gy mp mm l mn mo">p = np.linspace(-100,100,10000)</span><span id="1e54" class="mk lf it mg b gy mp mm l mn mo">for i in range(len(p)):<br/>    w1=np.array([[p[i],-37.94]])<br/>    output=feedforward(X,w1,w2,b1,b2)<br/>    cost_seq[i]=cost(y,output)</span></pre><p id="bb9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以看到它一点也不凸。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/6732136dd7353b0728625f3ac3e481ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZuI2lkVGKFfztcxRBaRSQ.png"/></div></div></figure><p id="8841" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也有可能改变两个参数。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ow"><img src="../Images/ffff9c7491688eb96b5c601283b5b22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XArlA4l128cEQhMgNvcljQ.png"/></div></div></figure><p id="f33e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更好地形象化成本函数，我们还可以制作一个动画。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7294abba81d0bd6b33d3842fb5330b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/1*ZFINCTcLsgl0gD2rGgXKWA.gif"/></div></figure><p id="70c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们用梯度下降法找到这个成本函数的一些合适的全局最小值，这叫做反向传播。</p><h2 id="5dab" class="mk lf it bd lg oh oi dn lk oj ok dp lo kr ol om lq kv on oo ls kz op oq lu or bi translated">反向传播</h2><p id="4486" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">偏导数可能会很难看，但幸运的是，有了交叉熵作为损失函数，最终的结果会有一些简化。</p><p id="a340" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这又是成本函数:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ou"><img src="../Images/2246b32ae2b059bf0b7506e3e9b20c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlixIq-lI04IUMxF09bO_Q.png"/></div></div></figure><ul class=""><li id="748b" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">请注意，当您使用成本函数来计算模型的成本时，此函数的输入变量是模型的输出和目标变量的真实值。</li><li id="6096" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">如果我们试图找到模型的最佳参数，那么我们认为这个成本函数的输入变量就是这些参数。我们要计算成本函数对每个参数的偏导数。</li></ul><p id="f5c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 w1 的偏导数，使用链式法则，我们有:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/49a7345de60fb68a0da594e159e55171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*OvlngocBvZfNwvKrUMNn2w.png"/></div></figure><p id="22f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，对于 sigmoid 函数，导数可以写成:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oz"><img src="../Images/93ad1df002a971f7c19f8180b5783ebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*plFu6bL9NPPNVEg-GfP-9w.png"/></div></div></figure><p id="003d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(请注意，代价函数是函数之和，函数之和的偏导数是函数的偏导数之和，所以为了简化记法，我们将去掉和符号，确切地说是均值计算)。</p><p id="0dd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们先按如下计算前两项:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/52dcdd4bd5e21747f531a8d9cf96955c.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*I5O9oQGW2X_l87kjXCdV0g.png"/></div></figure><p id="cf1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而且我们可以注意到，它们可以简化为(yhat-y)。</p><p id="bc2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们得到 w1 的最终结果:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/50afa980d02704690b58e126bf8949e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*qnKVZ_w_CaPCPnkT2VU4Lw.png"/></div></figure><p id="5499" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 b1，表达式非常相似，因为唯一的区别是最后的偏导数:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e390f092261116eb526e69f2acb5736d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*9J5aqsvJuu6BhM2xL81now.png"/></div></figure><p id="b409" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将用矩阵乘法对计算进行编码。在编码之前，我们可以问自己一些问题(并回答它们):</p><ul class=""><li id="0da3" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">残差的维数是多少(yhat — y)？它是一个列向量，行数等于总观察数。</li><li id="369a" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">w2 的维度是多少？这是一个两行一列的矩阵。记住，它是两个隐藏神经元的权重矩阵，用来计算输出。</li><li id="46e0" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">(yhat — y)*w2 的维数是多少？因为维数 w2 是(2，1)，所以我们不能做简单的乘法。我们做什么呢我们可以转置 w1 矩阵。然后(yhat — y)*w2 会给我们一个两列和 12 个观察值的矩阵。这是完美的，因为我们想要计算每一个重量。</li><li id="49c6" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">a1 的维数是多少？这是隐藏层的结果。因为我们有两个神经元，a1 有两列和 12 个观察值。并且与先前矩阵的乘法将是逐元素的乘法。</li><li id="bf11" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">所有这些都是非常一致的，因为最终，我们将得到一个包含 2 列和 12 个观察值的矩阵。第一列与第一个神经元的权重相关联，第二列与隐藏层中第二个神经元的权重相关联。</li></ul><p id="0cf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们做一些编码:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="0bf0" class="mk lf it mg b gy ml mm l mn mo">d_b1_v=np.dot((output-y), w2.T) * a1*(1-a1)</span></pre><p id="92da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个矩阵代表什么？我们可以再次展示对 b1 的偏导数。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/749ac394f0abb12520b46b2b6ab140a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*lOHSrQ2PKK8ttbZDwnVUXA.png"/></div></figure><p id="0a35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">矩阵 d_b1_v 是所有观测值的偏导数。而要得到最终的导数，就要计算那些与所有观测值相关的和(记住，L 是函数的和)，并计算平均值。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="3bbd" class="mk lf it mg b gy ml mm l mn mo">d_b1=np.mean(d_b1_v,axis=0)</span></pre><p id="c8ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 w1，我们必须考虑 x。对于每一个观察值，我们必须用 x 的值乘以以前偏导数得到的值，然后将它们全部相加。这就是一个点积。为了得到平均值，我们必须除以观察次数。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/50afa980d02704690b58e126bf8949e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*qnKVZ_w_CaPCPnkT2VU4Lw.png"/></div></figure><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4145" class="mk lf it mg b gy ml mm l mn mo">d_w1 = np.dot(X.T, d_b1_v)/12</span></pre><p id="4aaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们继续输出层的参数。会简单很多。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/b9c70e0e71c50ba34085a8b368ff156e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*i9c2T7ruOHclFotwQQyo3A.png"/></div></figure><p id="71cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经有了:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d6808551552e86e26e213485bc6c79f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*KEvyW622WDoLjPReMo8mdg.png"/></div></figure><p id="f793" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以 w2 和 b2 的导数是:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/0e753a173e8b83513228d42795c86cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*ZYXWlYCPnqtI1L4aXuBm-A.png"/></div></figure><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/97de0d25e94c220b6b2b3c19ec08af52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*iy0LS-agRH3N93o01HodAA.png"/></div></figure><p id="11fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 b2，我们只需对残差求和</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="5482" class="mk lf it mg b gy ml mm l mn mo">np.sum(output-y)/12</span></pre><p id="5550" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 w2，它是 a1(层 1 的结果)和残差之间的点积。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="39f3" class="mk lf it mg b gy ml mm l mn mo">np.dot(a1.T, (output-y))/12</span></pre><h2 id="d45f" class="mk lf it bd lg oh oi dn lk oj ok dp lo kr ol om lq kv on oo ls kz op oq lu or bi translated">包装中的最终算法</h2><p id="13f0" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">现在我们可以创建一个类来包含向前传播和向后传播这两个步骤。</p><p id="f435" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了基于<a class="ae pd" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">这篇非常受欢迎的文章</a>的 python 代码。你可能已经看过了。不同之处在于</p><ul class=""><li id="9f7a" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">损失函数(交叉熵代替均方误差)</li><li id="5298" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">添加学习率</li></ul><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="f3c3" class="mk lf it mg b gy ml mm l mn mo">class NeuralNetwork:<br/>    def __init__(self, x, y):<br/>        self.input = x<br/>        self.w1 = np.random.rand(self.input.shape[1],2)<br/>        self.w2 = np.random.rand(2,1)<br/>        self.b1 = np.zeros(2)<br/>        self.b2 = 0.0<br/>        self.y = y<br/>        self.output = np.zeros(self.y.shape)</span><span id="b84e" class="mk lf it mg b gy mp mm l mn mo">    def feedforward(self):<br/>        self.a1 = sigmoid(np.dot(self.input, self.w1)+self.b1)<br/>        self.output = sigmoid(np.dot(self.a1, self.w2)+self.b2)</span><span id="8dcc" class="mk lf it mg b gy mp mm l mn mo">    def backprop(self):<br/>        lr=0.1</span><span id="4cfe" class="mk lf it mg b gy mp mm l mn mo">        res=self.output-self.y</span><span id="c4af" class="mk lf it mg b gy mp mm l mn mo">        d_w2 = np.dot(self.a1.T, res)<br/>        d_b2 = np.sum(res)<br/>        d_b1_v=np.dot(res, self.w2.T) * self.a1*(1-self.a1)<br/>        d_b1 = np.sum(d_b1_v,axis=0)<br/>        d_w1 = np.dot(self.input.T, d_b1_v)</span><span id="f3d4" class="mk lf it mg b gy mp mm l mn mo">        self.w1 -= d_w1*lr<br/>        self.w2 -= d_w2*lr<br/>        self.b1 -= d_b1*lr<br/>        self.b2 -= d_b2*lr</span></pre><p id="4a4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后可以在梯度下降过程中存储 7 个参数的中间值，并绘制曲线。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pe"><img src="../Images/1c18513eda5b4d44ff20d43b56f785ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2l5sQCiq84NWbke9aMvSzQ.gif"/></div></div></figure></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><p id="465c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">动画是用 R 代码中的图形制作的。所以如果你对神经网络的 R 代码从零开始感兴趣，请评论。</p><p id="dc31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你的代码对你来说很难理解，我还创建了一个 Excel (Google Sheet)文件来做梯度下降。如果你感兴趣，请在评论中告诉我。是的，你可能会认为在 Excel 中进行机器学习是疯狂的，我同意你的观点，尤其是在完成了所有七个参数的梯度下降的所有步骤之后。但目的是为了更好地理解。为此，Excel 是一个非常好的工具。</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pm"><img src="../Images/1080dac6d63c9665761e79c51b0b9f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q96KkeLLVOZg3aur3YyBGg.png"/></div></div></figure></div></div>    
</body>
</html>