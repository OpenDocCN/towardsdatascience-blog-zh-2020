<html>
<head>
<title>Web Scraping News Articles to Build an NLP Data Pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Web抓取新闻文章以构建NLP数据管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-news-articles-to-build-an-nlp-data-pipeline-92ec6083da2?source=collection_archive---------7-----------------------#2020-02-22">https://towardsdatascience.com/web-scraping-news-articles-to-build-an-nlp-data-pipeline-92ec6083da2?source=collection_archive---------7-----------------------#2020-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Tensorflow 2.0、Scrapy和SpaCy的3个简单步骤！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b2545ad24164e85b88b662c1a2cb219a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KY02c3GIB0seLthj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@simplicity?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Marija Zaric </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dca0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管可以使用现成的数据集进行实验，但是生成NLP数据管道可以进一步提高您的技能，并在项目选择上给予您更多的自由。</p><p id="550c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将从头到尾解释我的NLP数据工作流程。我在下面列出了我在工作流程中使用的三个开源Python框架；</p><ul class=""><li id="58fc" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">刺儿</strong> </a>用于从网页中提取原始文本数据</li><li id="5593" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">空间</strong> </a>用于清理和规范化文本</li><li id="ad37" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">tensor flow 2.0</strong></a>用于构建数据管道</li></ul><p id="9d1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的工作流程将通过3个易于遵循的步骤进行解释。我的<a class="ae ky" href="https://github.com/eisbilen/NLPDataPipeline" rel="noopener ugc nofollow" target="_blank"> GitHub资源库中提供了完整的源代码。</a></p><p id="b7dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从第一步开始。</p><h1 id="2e79" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤1 — Web抓取:从Web中提取原始文本数据</h1><p id="e99f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我决定从<a class="ae ky" href="https://www.trtworld.com" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> TRT World网站</strong> </a>上抓取新闻文章，使用抓取的文本数据来试验几种NLP算法和数据管道概念。</p><p id="e0f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的目标是收集大约2000-3000篇文章，并将它们存储在一个JSON文件中。</p><p id="0736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我创建了一个<strong class="lb iu"> Scrapy </strong>项目并生成了2个蜘蛛；一个用于提取文章链接，另一个用于提取文章标题，以及使用上一步中捕获的链接的正文。</p><p id="4004" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们安装Scrapy并开始我们的Scrapy项目。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c513" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Install the scrapy</strong><br/>$ pip install scrapy</span><span id="3965" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Start web scraping project with scrapys</strong><br/>$ scrapy startproject TRTWorld<br/>$ cd TRTWorld</span><span id="64a7" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu">TRTWorld $</strong> scrapy genspider Articles trtworld.com<br/><strong class="nh iu">TRTWorld $</strong> scrapy genspider ArticleScraper trtworld.com</span></pre><p id="74a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的第一个蜘蛛是<strong class="lb iu"> "Articles.py" </strong>，它将通过访问500个网页来获取文章链接。它将提取每个页面上可用的文章链接的href信息，并将它们存储在一个JSON文件中。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="f09e" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Spider 1 <br/># Articles.py which scrape article links</strong></span><span id="65ae" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># imports</strong><br/>import scrapy<br/>from scrapy.http import Request<br/>from TRTWorld.items import TrtworldItem</span><span id="ff3c" class="nl mk it nh b gy nq nn l no np">class ArticlesSpider(scrapy.Spider):<br/> name = 'Articles'<br/> allowed_domains = ['trtworld.com']<br/> start_urls = ['http://trtworld.com/']</span><span id="638a" class="nl mk it nh b gy nq nn l no np">def start_requests(self):</span><span id="5677" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Hardcoded URL that contains TURKEY related subjects</strong><br/>  url="https://www.trtworld.com/turkey?page={}"</span><span id="1a3a" class="nl mk it nh b gy nq nn l no np">link_urls = [url.format(i) for i in range(0,500)]</span><span id="f8be" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Loops through 500 pages to get the article links</strong><br/>  for link_url in link_urls:</span><span id="b982" class="nl mk it nh b gy nq nn l no np">print(link_url)</span><span id="f79f" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Request to get the HTML content</strong><br/>    request=Request(link_url, cookies={'store_language':'en'}, <br/>    callback=self.parse_main_pages)</span><span id="60b1" class="nl mk it nh b gy nq nn l no np">yield request</span><span id="32b0" class="nl mk it nh b gy nq nn l no np">def parse_main_pages(self,response):</span><span id="55c0" class="nl mk it nh b gy nq nn l no np">item=TrtworldItem()</span><span id="6da3" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Gets HTML content where the article links are stored</strong><br/>  content=response.xpath('//div[@id="items"]//div[@class="article- <br/>  meta"]')</span><span id="a436" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Loops through the each and every article link in HTML 'content'</strong><br/>  for article_link in content.xpath('.//a'):</span><span id="263e" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Extracts the href info of the link to store in scrapy item</strong><br/>   item['article_url'] =    <br/>   article_link.xpath('.//@href').extract_first()</span><span id="d240" class="nl mk it nh b gy nq nn l no np">item['article_url'] =   <br/>   "https://www.trtworld.com"+item['article_url']</span><span id="65a8" class="nl mk it nh b gy nq nn l no np">yield(item)</span><span id="ca17" class="nl mk it nh b gy nq nn l no np">def parse(self, response):<br/>  pass</span></pre><p id="93c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们完成我们的第一个蜘蛛后，我们现在可以用下面的命令运行它来生成“article _ links”JSON文件。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="1769" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu">TRTWorld $ </strong>scrapy crawl -o article_links.json -t json Articles</span></pre><p id="88e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是使用存储在JSON文件中的链接抓取新闻文章。为此，让我们创建我们的第二个蜘蛛是“文章刮刀”。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="9dd6" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Spider 2<br/># ArticleScraper.py which scrape article headlies and bodies</strong></span><span id="d47c" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># imports</strong><br/>import scrapy<br/>from scrapy.http import Request<br/>from TRTWorld.items import TrtworldItem</span><span id="b59b" class="nl mk it nh b gy nq nn l no np">import json</span><span id="d2a0" class="nl mk it nh b gy nq nn l no np">class ArticlescraperSpider(scrapy.Spider):<br/> name = 'ArticleScraper'<br/> allowed_domains = ['trtworld.com']<br/> start_urls = ['<a class="ae ky" href="http://trtworld.com/'" rel="noopener ugc nofollow" target="_blank">http://trtworld.com/'</a>]</span><span id="c63d" class="nl mk it nh b gy nq nn l no np">def start_requests(self):<br/>  <br/>  <strong class="nh iu"># Open the JSON file which contains article links<br/>  </strong>with open('/Users/erdemisbilen/Angular/TRTWorld<br/>  /article_links.json') as json_file:<br/>   <br/>   data = json.load(json_file)<br/>   <br/>   for p in data:<br/>    print('URL: ' + p['article_url'])</span><span id="f87a" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Request to get the HTML content</strong><br/>    request=Request(p['article_url'],<br/>               cookies={'store_language':'en'},<br/>               callback=self.parse_article_page)<br/>    yield request</span><span id="9a3d" class="nl mk it nh b gy nq nn l no np">def parse_article_page(self,response):</span><span id="4465" class="nl mk it nh b gy nq nn l no np">item=TrtworldItem()<br/> a_body=""</span><span id="1390" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Extracts the article_title and stores in scrapy item</strong><br/> item['article_title']=response.xpath('//h1[<a class="ae ky" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>="article-<br/> title"]/text()').extract();</span><span id="3c3e" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Extracts the article_description and stores in scrapy item</strong><br/> item['article_description']=response.xpath('//h3[<a class="ae ky" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>="article-<br/> description "]/text()').extract();</span><span id="0676" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Extracts the article_body in &lt;p&gt; elements</strong><br/> for p in response.xpath('//div[<a class="ae ky" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>="contentBox bg-w <br/> noMedia"]//p/text()').extract():<br/>   <br/>   a_body=a_body+p<br/>   item['article_body']= a_body<br/>   yield(item)</span><span id="4a98" class="nl mk it nh b gy nq nn l no np">def parse(self, response):<br/>  pass</span></pre><p id="b0ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的命令运行<strong class="lb iu">文章抓取器</strong>蜘蛛并生成一个包含3000篇新闻文章的JSON文件。您可以在下面看到蜘蛛生成的JSON文件的内容。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4945" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu">TRTWorld $ </strong>scrapy crawl -o article_body.json -t json ArticleScraper</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/dacaa67471c996163a8f81c9d1091ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wh-Y5MqviCu3Dh5nLD-y2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">' article_body.json '文件，包含3000篇世界新闻文章</p></figure><h1 id="f47d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤2 —文本预处理:归一化和去噪</h1><p id="851b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">既然我们的JSON文件中存储了大约3000篇文章，我们可以开始考虑在我们的实验性NLP研究中使用它们。</p><p id="0d4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要在NLP应用程序中使用任何文本数据，我们必须将文本转换成数字，因为计算机很难理解这些单词。</p><p id="c572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之前，我们应该清理和规范化我们的文本。这一步将我们的文本转换为更简单和结构化的形式，以便机器学习算法可以更有效和更好地执行。</p><p id="289a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，我们有包含结构良好的句子的新闻文章，所以我们可能不需要应用下面列出的所有预处理。请参见下面的示例文章。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/136c2131262f6385d50904e1e066afd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*WWT2duPYtGhAFyqDD2N95Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">任何预处理之前的示例新闻文章</p></figure><p id="10c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从安装<a class="ae ky" href="https://spacy.io/usage" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">空间</strong> </a>和它所需的依赖项开始。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c288" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Install the spaCy</strong><br/>pip install -U spacy</span><span id="c568" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Install the spaCy Lemmatization<br/></strong>pip install -U spacy-lookups-data</span><span id="3b7c" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Install the spaCy English Model</strong><br/>python -m spacy download en_core_web_sm</span></pre><p id="178b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了清理和规范文本，我们将在文本中应用以下流程:</p><ul class=""><li id="82ea" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">句子分割</strong></li></ul><p id="ad0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将把每篇文章分成句子。然后我们将在<a class="ae ky" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> spaCy库的帮助下清理和规范化每个句子。</strong>T11】</a></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a326" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Splitting text into sentences using spaCy</strong><br/>def split_sentences(document):<br/> sentences = [sent.string.strip() for sent in doc.sents]<br/> return sentences</span></pre><ul class=""><li id="5b8b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">删除停用词</strong></li></ul><p id="a008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">停用词是语言中最常用的词，对自然语言处理任务(如情感分析或文本分类)没有帮助。因此，您可以考虑将它们从文本中删除，以提高模型的速度和准确性。</p><p id="71c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，我使用了spaCy的内置停用词。您可以根据特定领域的要求自定义默认停用词。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8e1d" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Removes stopwords from a sentence using spaCy (token.is_stop)</strong><br/>def remove_stopwords(sentence):<br/> sentence = nlp(sentence)<br/> processed_sentence = ' '.join([token.text for token in sentence if token.is_stop != True ])<br/> return processed_sentence</span><span id="4af0" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Removes stopwords from spaCy default stopword list</strong><br/>nlp.Defaults.stop_words -= {"my_stopword_1", "my_stopword_2"}</span><span id="5d20" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Adds custom stopwords into spaCy default stopword list<br/></strong>nlp.Defaults.stop_words |= {"my_stopword_1", "my_stopword_2"}</span><span id="73e1" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Prints spaCy default stopwords</strong><br/>print(nlp.Defaults.stop_words)</span></pre><ul class=""><li id="e032" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">删除标点、引号、括号、货币字符和数字</strong></li></ul><p id="b9d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，我们只需要NLP管道中的单词，这意味着我们必须从句子中删除标点符号和其他特殊字符，包括数字。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="7765" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Removes punctuation and special chars from a sentence using spaCy </strong><br/>def remove_punctuation_special_chars(sentence):<br/> sentence = nlp(sentence)<br/> processed_sentence = ' '.join([token.text for token in sentence <br/> if token.is_punct != True and <br/>     token.is_quote != True and <br/>     token.is_bracket != True and <br/>     token.is_currency != True and <br/>     token.is_digit != True])<br/> return processed_sentence</span><span id="6242" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># spaCy - List of special charecters to be removed<br/>_currency = r"\$ £ € ¥ ฿ US\$ C\$ A\$ ₽ ﷼ ₴"<br/>_punct = (<br/>    r"… …… , : ; \! \? ¿ ؟ ¡ \( \) \[ \] \{ \} &lt; &gt; _ # \* &amp; 。 ？ ！ ， 、 ； ： ～ · । ، ۔ ؛ ٪" )<br/>_quotes = r'\' " ” “ ` ‘ ´ ’ ‚ , „ » « 「 」 『 』 （ ） 〔 〕 【 】 《 》 〈 〉'</strong></span></pre><ul class=""><li id="6fb0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">词汇化</strong></li></ul><p id="97d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词汇化是将单词还原成它的基本形式。这是通过把一个词的屈折和派生的相关形式简化为它的基本形式来实现的。</p><p id="2742" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词汇化旨在减少词汇量并使单词规范化。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6667" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Lemmatization process with spaCy</strong><br/>def lemmatize_text(sentence):<br/>    sentence = nlp(sentence)<br/>    processed_sentence = ' '.join([word.lemma_ for word in <br/>    sentence])<br/>    <br/>    return processed_sentence</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/ddeb1d3d23d573501e1f0187fb306aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Rdk2n-xS7lQS7L4hYiRFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">空间化和降噪处理后的例句</p></figure><p id="3687" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在上面看到在词汇化和停用词删除后句子是如何被修改的。</p><p id="55d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据手头的NLP任务，您可以考虑不应用某些清理和规范化过程，因为在上述每个过程中都会有一定程度的信息丢失。</p><p id="24fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在某些情况下，我们可能希望看到词频来理解文本的内容。<strong class="lb iu">【spaCy】</strong>提供易于应用的工具来实现这一点。下面，您可以看到包含文本预处理和词频计算功能的完整Python脚本。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="df89" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># JSONtoTXT.py</strong></span><span id="0417" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Reads news articles from a JSON file<br/># Splits the content into sentences<br/># Cleans and normalizes the content<br/># Write each processed sentence into a text file</strong></span><span id="9d01" class="nl mk it nh b gy nq nn l no np">import json<br/>import spacy<br/>from spacy.lang.en import English # updated<br/>from spacy.lang.en.stop_words import STOP_WORDS<br/>from collections import Counter<br/>import re</span><span id="ef02" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Loads the spaCy small English language model</strong><br/>nlp = spacy.load('en_core_web_sm')</span><span id="dc29" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Removes stopwords from spaCy default stopword list<br/></strong>nlp.Defaults.stop_words -= {"my_stopword_1", "my_stopword_2"}</span><span id="feb7" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Adds custom stopword into spaCy default stopword list<br/></strong>nlp.Defaults.stop_words |= {"my_stopword_1", "my_stopword_2"}</span><span id="cbf6" class="nl mk it nh b gy nq nn l no np">print(nlp.Defaults)</span><span id="a63e" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Calculates the frequency of words in a document</strong><br/>def word_frequency(my_doc):</span><span id="115b" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># all tokens that arent stop words or punctuations</strong><br/> words = [token.text for token in my_doc if token.is_stop != True <br/> and token.is_punct != True]</span><span id="bc57" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># noun tokens that arent stop words or punctuations</strong><br/> nouns = [token.text for token in my_doc if token.is_stop != True <br/> and token.is_punct != True and token.pos_ == "NOUN"]</span><span id="92e7" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># verb tokens that arent stop words or punctuations</strong><br/> verbs = [token.text for token in my_doc if token.is_stop != True <br/> and token.is_punct != True and token.pos_ == "VERB"]</span><span id="69a1" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># five most common words</strong><br/> word_freq = Counter(words)<br/> common_words = word_freq.most_common(5)<br/> print("---------------------------------------")<br/> print("5 MOST COMMON TOKEN")<br/> print(common_words)<br/> print("---------------------------------------")<br/> print("---------------------------------------")</span><span id="de3e" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># five most common nouns</strong><br/> noun_freq = Counter(nouns)<br/> common_nouns = noun_freq.most_common(5)<br/> print("5 MOST COMMON NOUN")<br/> print(common_nouns)<br/> print("---------------------------------------")<br/> print("---------------------------------------")</span><span id="134e" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># five most common verbs</strong><br/> verb_freq = Counter(verbs)<br/> common_verbs = verb_freq.most_common(5)<br/> print("5 MOST COMMON VERB")<br/> print(common_verbs)<br/> print("---------------------------------------")<br/> print("---------------------------------------")</span><span id="daae" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Removes stopwords from a sentence using spaCy (token.is_stop)</strong><br/>def remove_stopwords(sentence):<br/> sentence = nlp(sentence)<br/> processed_sentence = ' '.join([token.text for token in sentence if <br/> token.is_stop != True ])<br/> return processed_sentence</span><span id="11c4" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Removes punctuation and special chars from a sentence using spaCy<br/></strong>def remove_punctuation_special_chars(sentence):<br/> sentence = nlp(sentence)<br/> processed_sentence = ' '.join([token.text for token in sentence <br/>  if token.is_punct != True and <br/>     token.is_quote != True and <br/>     token.is_bracket != True and <br/>     token.is_currency != True and <br/>     token.is_digit != True])<br/> return processed_sentence</span><span id="e546" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Lemmatization process with spaCy</strong><br/>def lemmatize_text(sentence):<br/>    sentence = nlp(sentence)<br/>    processed_sentence = ' '.join([word.lemma_ for word in <br/>    sentence])<br/>    return processed_sentence</span><span id="4e17" class="nl mk it nh b gy nq nn l no np">def remove_special_chars(text):<br/> bad_chars = ["%", "#", '"', "*"] <br/> for i in bad_chars: <br/>  text = text.replace(i, '')<br/> return text</span><span id="19f5" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Splitting text into sentences using spaCy</strong><br/>def split_sentences(document):<br/> sentences = [sent.string.strip() for sent in doc.sents]<br/> return sentences</span><span id="bcce" class="nl mk it nh b gy nq nn l no np">sentence_index = 0</span><span id="b7e7" class="nl mk it nh b gy nq nn l no np">with open('/Users/erdemisbilen/TFvenv/articles_less.json') as json_file:<br/> data = json.load(json_file)<br/>  <br/> with open("article_all.txt", "w") as text_file:<br/>  for p in data:<br/>   article_body = p['article_body']<br/>   article_body = remove_special_chars(article_body)</span><span id="ca3e" class="nl mk it nh b gy nq nn l no np">doc = nlp(article_body)</span><span id="b2af" class="nl mk it nh b gy nq nn l no np">sentences = split_sentences(doc)<br/>   word_frequency(doc)</span><span id="b2de" class="nl mk it nh b gy nq nn l no np">for sentence in sentences:<br/>    sentence_index +=1<br/>    print("Sentence #" + str(sentence_index) + "-----------------")<br/>    print("Original Sentence               : " + sentence)<br/>    sentence = remove_stopwords(sentence)<br/>    sentence = remove_punctuation_special_chars(sentence)<br/>    print("Stopwors and Punctuation Removal: " + sentence)<br/>    sentence = lemmatize_text(sentence)<br/>    print("Lemmitization Applied           : " + sentence)<br/>    text_file.write(sentence + '\n')<br/>  <br/> text_file.close()</span></pre><p id="ab51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在下面看到，脚本在处理新闻文章后产生的内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/179552fa5df11f12aae160fb2361894c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giz3VxPEcT3WR-Wz2ayScw.png"/></div></div></figure><p id="bd5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经清理并规范化了我们的文本，并将其拆分成句子，现在是时候用<strong class="lb iu"> Tensorflow 2.0 </strong>构建一个数据管道了。</p><h1 id="83e0" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤3 —构建数据管道:tf.data API</h1><p id="2619" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在许多情况下，将文本内容直接输入NLP模型并不是管理数据输入过程的有效方式。</p><p id="9d12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">tensor flow '<a class="ae ky" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">TF . data API</strong></a><strong class="lb iu">'</strong>考虑到灵活性和效率，提供了更好的性能。如果你想了解为什么'<strong class="lb iu">TF . data '</strong>比传统的数据管道更好，我鼓励你观看这个视频。</p><p id="7773" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将使用tensor flow<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset" rel="noopener ugc nofollow" target="_blank">"<strong class="lb iu">TF . data . textline dataset API "</strong></a>来构建我的NLP数据管道。</p><p id="977a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从安装“张量流”和“张量流-数据集”开始。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="9b28" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu"># Install the tensorflow with pip</strong><br/>$ pip install tensorflow</span><span id="af37" class="nl mk it nh b gy nq nn l no np"><strong class="nh iu"># Install the tensorflow-datasets with pip</strong><br/>$ pip install tensorflow-datasets</span></pre><p id="dbe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将主要按照TensorFlow提供的<a class="ae ky" href="https://www.tensorflow.org/tutorials/load_data/text" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">【加载文本】</strong> </a>教程来开发我的数据管道。</p><p id="5dfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将使用<strong class="lb iu">" TF . data . textline dataset "</strong>和我们在前面步骤中生成的" articlesTXT.txt "文件来构建我们的数据集。</p><p id="5f6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们的数据管道中只有一个txt文件，但是下面的代码提供了加载和标记几个txt文件的能力。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a3df" class="nl mk it nh b gy nm nn l no np">parent_dir = "/Users/erdemisbilen/Angular/TRTWorld/articlesTXT"<br/>FILE_NAMES = ['article_all.txt']</span><span id="f504" class="nl mk it nh b gy nq nn l no np">BUFFER_SIZE = 2000<br/>BATCH_SIZE = 128<br/>TAKE_SIZE = 200</span><span id="8041" class="nl mk it nh b gy nq nn l no np">def labeler(example, index):<br/>  return example, tf.cast(index, tf.int64)</span><span id="7913" class="nl mk it nh b gy nq nn l no np">labeled_data_sets = []</span><span id="9252" class="nl mk it nh b gy nq nn l no np">for i, file_name in enumerate(FILE_NAMES):<br/>  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, <br/>  file_name))<br/>  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))<br/>  labeled_data_sets.append(labeled_dataset)</span><span id="f627" class="nl mk it nh b gy nq nn l no np">all_labeled_data = labeled_data_sets[0]</span><span id="ff4c" class="nl mk it nh b gy nq nn l no np">for labeled_dataset in labeled_data_sets[1:]:<br/>  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)<br/>  <br/>all_labeled_data = all_labeled_data.shuffle(<br/>    BUFFER_SIZE, reshuffle_each_iteration=False)</span></pre><p id="0e0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们会使用<strong class="lb iu">" tfds . features . text . tokenizer "</strong>将句子拆分成记号，构建我们的词汇集。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0117" class="nl mk it nh b gy nm nn l no np">tokenizer = tfds.features.text.Tokenizer()</span><span id="2825" class="nl mk it nh b gy nq nn l no np">vocabulary_set = set()</span><span id="79ba" class="nl mk it nh b gy nq nn l no np">for text_tensor, _ in all_labeled_data:<br/>  some_tokens = tokenizer.tokenize(text_tensor.numpy())<br/>  vocabulary_set.update(some_tokens)</span><span id="c560" class="nl mk it nh b gy nq nn l no np">vocab_size = len(vocabulary_set)</span><span id="a322" class="nl mk it nh b gy nq nn l no np">print("Vocabulary size.   :" + str(vocab_size))<br/>print("-------------------------------")<br/>print(vocabulary_set)<br/>print("-------------------------------")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/1324db9c2d92cb4dd7dcad85199871de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCaDiF6qxi4fKhIoRv4v1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">词汇量和我们词汇中的单词</p></figure><p id="7f93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们创建了词汇表，下一步就是通过为词汇表中的每个单词分配一个惟一的整数值来对词汇表中的每个单词进行编码。这是一个基于索引的编码过程，它用唯一的索引号映射每个单词。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="f53d" class="nl mk it nh b gy nm nn l no np">encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)</span><span id="4eaf" class="nl mk it nh b gy nq nn l no np">example_text = next(iter(all_labeled_data))[0].numpy()<br/>print(example_text)</span><span id="eb76" class="nl mk it nh b gy nq nn l no np">encoded_example = encoder.encode(example_text)<br/>print(encoded_example)</span></pre><p id="818b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以使用映射函数对整个数据集进行编码。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="504f" class="nl mk it nh b gy nm nn l no np">def encode(text_tensor, label):<br/>  encoded_text = encoder.encode(text_tensor.numpy())<br/>  return encoded_text, label</span><span id="02f4" class="nl mk it nh b gy nq nn l no np">def encode_map_fn(text, label):<br/>  encoded_text, label = tf.py_function(encode, <br/>                                       inp=[text, label], <br/>                                       Tout=(tf.int64, tf.int64))<br/>  return encoded_text, label</span><span id="527a" class="nl mk it nh b gy nq nn l no np">all_encoded_data = all_labeled_data.map(encode_map_fn)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/71d81f447e35531b397e2af9367ad568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoDVYo12O96tkB9V3V4agQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">句子的编码形式</p></figure><p id="8772" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据集包含不同长度的句子。现在，最后一步是将每个数据集项目填充到特定大小的向量，因为许多NLP模型使用固定长度的向量维度。</p><p id="2cf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时，我们将以128的批处理大小对数据集的内容进行批处理。这将是我们在训练过程的每次迭代中输入到模型中的数据集项目的数量。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2de6" class="nl mk it nh b gy nm nn l no np">train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)<br/>train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([200],()))</span><span id="fcfa" class="nl mk it nh b gy nq nn l no np">test_data = all_encoded_data.take(TAKE_SIZE)<br/>test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([200],()))</span><span id="6cf7" class="nl mk it nh b gy nq nn l no np">sample_text, sample_labels = next(iter(test_data))</span><span id="f091" class="nl mk it nh b gy nq nn l no np">sample_text[0], sample_labels[0]</span></pre><p id="85ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">填充后，我们数据集中的所有句子都表示为一个向量(大小为200)，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/193590c8333fbba4a50ec1ca1cb2f3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*L0AQ6QEZPNwE1ndeEyIqJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">填充后的数据集项</p></figure><p id="9eaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们的数据管道已经准备好了，我们可以开始构建一个LSTM模型来测试我们的数据管道。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="671a" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu">#Training a LSTM model to test the data pipeline</strong><br/>vocab_size += 1</span><span id="c8ca" class="nl mk it nh b gy nq nn l no np">model = tf.keras.Sequential()<br/>model.add(tf.keras.layers.Embedding(vocab_size, 64))<br/>model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))</span><span id="4dc7" class="nl mk it nh b gy nq nn l no np">for units in [64, 64]:<br/>  model.add(tf.keras.layers.Dense(units, activation='relu'))</span><span id="e595" class="nl mk it nh b gy nq nn l no np"># Output layer. The first argument is the number of labels.<br/>model.add(tf.keras.layers.Dense(3, activation='softmax'))</span><span id="d908" class="nl mk it nh b gy nq nn l no np">optimizer = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)</span><span id="1696" class="nl mk it nh b gy nq nn l no np">model.compile(optimizer= optimizer,<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</span><span id="c8c6" class="nl mk it nh b gy nq nn l no np">log_dir="logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span><span id="5c08" class="nl mk it nh b gy nq nn l no np">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)</span><span id="b1ff" class="nl mk it nh b gy nq nn l no np">model.fit(train_data, epochs=10, steps_per_epoch=4, validation_data=test_data, callbacks=[tensorboard_callback])</span><span id="7b0d" class="nl mk it nh b gy nq nn l no np">eval_loss, eval_acc = model.evaluate(test_data)</span><span id="fc9c" class="nl mk it nh b gy nq nn l no np">print('\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/450b849522e27edd543aa351e6cf2d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhNYNixNFg1SRWPTlnjWnA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用Tensorflow训练LSTM模型</p></figure><p id="597c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面可以看到，数据通过我们构建的数据管道成功地输入到模型中。</p><h1 id="d4b4" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">摘要</h1><p id="1974" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在这篇文章中，我试图解释我从零开始构建NLP数据管道的方法。我希望我的文章能帮助你构建你的NLP应用程序。</p></div></div>    
</body>
</html>