<html>
<head>
<title>Intro to Reinforcement Learning: The Explore-Exploit Dilemma</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习导论:探索-利用困境</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intro-to-reinforcement-learning-the-explore-exploit-dilemma-463ceb004989?source=collection_archive---------30-----------------------#2020-08-25">https://towardsdatascience.com/intro-to-reinforcement-learning-the-explore-exploit-dilemma-463ceb004989?source=collection_archive---------30-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/375897f5c65a5b1c2d74264589bf6712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*roW19v1oct0-nBzZ"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Javier Allegue Barros 在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="bb3f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">探索-利用困境</strong></p><p id="5831" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决定和困境是一枚硬币的两面。想象一个学生期待学习数据科学。他在网上搜索数据科学课程，它会返回哈佛、麻省理工、Coursera、Udemy、Udacity 等许多课程。现在，这里有一个难题:在给定所有课程/信息的情况下，他如何在初始阶段找出哪门课程最适合他？对他来说，在一个接一个地看完所有的课程大纲后决定最好的课程可能是理想的解决方案。在强化学习中，这是一种<strong class="ki iu">探索</strong>,人们收集信息以评估可能导致他在未来做出更好决定的情景。探索之后，他可能会决定从某个特定的课程中学习。这在强化学习<strong class="ki iu"> </strong>中被称为<strong class="ki iu">利用</strong>，在强化学习中，人们可以在给定当前获得的知识或信息的情况下做出具有最高可能结果的最优决策。</p><p id="e36a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">勘探是一个必要的步骤，尽管这是一个劳动密集型和耗时的过程。因此，它提出了问题:我们应该探索多久？我们应该什么时候开始开发？我们应该开发多少？研究这些问题为我们提供了我们实际寻求的东西:“在探索足够多的选项的同时，确定要利用的最佳选项”。这就是强化学习中的<strong class="ki iu">探索-利用困境</strong>。</p><p id="d805" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">多武装匪徒问题(MABP) </strong>是探索-利用困境的经典例子，目标是调查一组选项中的最佳选项，然后利用所选选项。在 MABP 有多台机器(强盗),每台强盗都有不同的胜率。了解每台机器的胜率是一项挑战，因为玩多台机器需要花费金钱和时间。我们只想玩或<strong class="ki iu">利用</strong>有最高可能结果的机器，同时，我们应该<strong class="ki iu">探索</strong>所有的机器，找出胜率最高的机器。因此，出现了被称为 MABP 的探索-开发困境。</p><p id="39af" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从商业到我们的日常生活活动，探索-利用困境几乎无处不在。我们应该看网飞或亚马逊上的哪部电影/电视剧——这是一个两难困境的真实例子。当我打开网飞或亚马逊 Prime 时，我通常会开始探索不同的电影/电视节目，以找到一个令人愉快的节目来观看或利用(不幸的是，我经常在长时间探索后失去了观看的兴趣！！！).另一个著名的例子是，我们应该去我们最喜欢的餐馆或尝试一个新的。</p><p id="9749" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"/><strong class="ki iu">探索-剥削困境是如何解决的？</strong></p><p id="3e84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们知道了困境，下一步是——解决实际强化学习算法中的困境。有多种策略可以解决这一难题，即调查具有最高可能结果的最佳解决方案:</p><p id="0bd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">ε-贪婪法</strong></p><p id="b1be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ε-Greedy(ε-Greedy)是最常见和最简单的算法，通过随机选择来平衡探索和利用之间的权衡。假设我们在初始阶段探讨<em class="le"/><strong class="ki iu"><em class="le">n</em></strong><em class="le"/>选项。然后，通过ε-贪婪算法，<em class="le"> (1- </em> ε <em class="le"> ) </em>百分时间我们贪婪地利用<strong class="ki iu"><em class="le"/></strong>n 个选项中的最佳选项<strong class="ki iu"> <em class="le"> k </em> </strong>，在剩余的<strong class="ki iu"> <em class="le"> e </em> </strong>百分时间内随机探索其他选项，以获得比先前最佳选项<em class="le"> k. </em>更好的决策</p><p id="b89f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">ε递减法</strong></p><p id="391e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ε减少类似于ε贪婪方法。ε贪婪方法中的ε保持固定，而在ε递减方法中，ε值随时间逐渐减小。随着ε值的减小，探索新选项的次数逐渐减少，这意味着最佳选项在此过程中变得更加确定。</p><h1 id="2de8" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">阅读默罕默德·马苏姆博士(以及媒体上成千上万的其他作家)的每一个故事。</h1><p id="9610" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">你的会员费将直接支持和激励穆罕默德·马苏姆和你所阅读的成千上万的其他作家。你还可以在 Medium 上看到所有的故事—<a class="ae kf" href="https://masum-math8065.medium.com/membership" rel="noopener"><strong class="ki iu">【https://masum-math8065.medium.com/membership】</strong></a></p><p id="7b82" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">快乐阅读！</strong></p></div></div>    
</body>
</html>