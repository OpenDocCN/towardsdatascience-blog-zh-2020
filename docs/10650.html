<html>
<head>
<title>Why We Use Unsupervised Learning (With K-means Clustering From Scratch)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我们使用无监督学习(从头开始使用 K 均值聚类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-we-use-unsupervised-learning-with-k-means-clustering-from-scratch-1401efdd6fde?source=collection_archive---------34-----------------------#2020-07-25">https://towardsdatascience.com/why-we-use-unsupervised-learning-with-k-means-clustering-from-scratch-1401efdd6fde?source=collection_archive---------34-----------------------#2020-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c442" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么无监督学习是一个伟大的工具，却不能产生定量的结果。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5c58941aa4356bce89b195e3549d444f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdXWb4aj3qKNK7jv_D-jlw.jpeg"/></div></div></figure><p id="1ed5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">在数据科学世界中，监督学习是一个有趣的话题，不从事数据科学的人通常不会强调这一点，此外，这也是许多数据科学家自己经常忽视的一个想法。对此有一个解释，因为对于许多就业机会来说，无监督学习根本不重要。当然，有时它会发挥作用，但在大多数情况下，企业不会投资做他们永远不知道结果的研究。在不知道结果的情况下做研究正是无监督形容词在这种情况下的应用，所以如果我们不知道我们的模型输出了什么，那么我们怎么能在它的预测和发现中找到价值呢？</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="bf1c" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">分析</h1><p id="1455" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">无监督学习算法的主要功能是分析。使用无监督学习算法来探索您的数据可以告诉您许多关于所述数据的某些属性的信息。例如，聚类分析可以显示某些连续值是如何分组的，不管它们是相关还是不相关。您可以使用无监督学习来发现数据中的自然模式，这些模式仅通过统计分析或比较值并不明显。</p><p id="2abe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">无监督学习算法在图像识别和基因组学中也有自己的优势。在基因组学中，它们可以用来对遗传学进行聚类或分析基因组数据序列。无监督学习用于建模概率密度，这对生物信息学学科非常有用。</p><p id="b54a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">无人监管的另一个好处是，不需要任何人工干预就可以非常容易地收集分析数据。典型的机器学习，无论是强化的还是监督的，都需要手动绘制标签，以便正确理解模型的结果。有监督学习和无监督学习的区别如下:</p><h2 id="c344" class="nd mh it bd mi ne nf dn mm ng nh dp mq ld ni nj ms lh nk nl mu ll nm nn mw no bi translated">无监督学习</h2><ul class=""><li id="28a9" class="np nq it kw b kx my la mz ld nr lh ns ll nt lp nu nv nw nx bi translated">不太准确</li><li id="8af2" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">不需要标记数据</li><li id="9ecd" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">最少的人力</li></ul><h2 id="e673" class="nd mh it bd mi ne nf dn mm ng nh dp mq ld ni nj ms lh nk nl mu ll nm nn mw no bi translated">监督学习</h2><ul class=""><li id="e4b4" class="np nq it kw b kx my la mz ld nr lh ns ll nt lp nu nv nw nx bi translated">高度准确</li><li id="ad90" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">有一致的目标</li><li id="453b" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">需要带标签的数据</li><li id="ea09" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">需要人的努力</li></ul></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="4634" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">示例:Kmeans 聚类</h1><p id="3c87" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">聚类是最常用的无监督学习方法。这是因为这通常是可视化探索和发现更多数据的最佳方式之一。有几种不同类型的集群，包括:</p><ul class=""><li id="f65b" class="np nq it kw b kx ky la lb ld od lh oe ll of lp nu nv nw nx bi translated"><strong class="kw iu">层次集群</strong>:通过创建一个集群树来构建集群的多层次结构。</li><li id="4488" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated"><strong class="kw iu"> k 均值聚类</strong>:根据到聚类质心的距离将数据划分为 k 个不同的聚类。</li><li id="401a" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated"><strong class="kw iu">高斯混合模型</strong>:将聚类建模为多元正态密度成分的混合物。</li><li id="08bb" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated"><strong class="kw iu">自组织地图</strong>:使用学习数据拓扑和分布的神经网络。</li><li id="c327" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated"><strong class="kw iu">隐马尔可夫模型</strong>:利用观测数据恢复状态序列。</li></ul><p id="0e88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在今天的例子中，我们将学习 klearn 中提供的 Kmeans 集群。在 K-均值聚类中，对于每个点 x:</p><ul class=""><li id="a14c" class="np nq it kw b kx ky la lb ld od lh oe ll of lp nu nv nw nx bi translated">找到最近的质心 c</li><li id="585c" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">将点 x 分配给群集 j</li></ul><p id="45ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后对于每个聚类 j (=1..k):</p><ul class=""><li id="7802" class="np nq it kw b kx ky la lb ld od lh oe ll of lp nu nv nw nx bi translated">新的质心 c 等于在前一步骤中分配给聚类 j 的所有点 x 的平均值。</li></ul><p id="9305" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然这个模型在 sklearn.cluster 的 KMeans 类下可用，但今天我将编写自己的函数来计算 K 均值聚类。我们将从创建一个能够测量欧几里德长度的函数开始。这就像使用 Numpy 的 linalg 一样简单。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="fcd7" class="nd mh it oh b gy ol om l on oo">import numpy as np<br/>def euclidian(a, b):<br/>    return np.linalg.norm(a-b)</span></pre><p id="14c1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我还导入了 Matplotlib.pyplot 和 Matplotlib.animation，以便将来对我们的集群进行可视化:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="e328" class="nd mh it oh b gy ol om l on oo">import matplotlib.pyplot as plt<br/># animation<br/>import matplotlib.animation as animation</span></pre><p id="1085" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我将使用文本数据，所以我使用 numpy 的这个函数来加载它:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="99b0" class="nd mh it oh b gy ol om l on oo">def load_dataset(name):<br/>    return np.loadtxt(name)</span></pre><p id="e7b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在是时候做我们实际的 K 均值函数了。首先，我们将添加参数 k、ε和距离(在我们的例子中是欧几里得的。)之后，我们将为质心创建一个空列表，并设置距离算法以使用之前的欧几里德函数。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="18ee" class="nd mh it oh b gy ol om l on oo">def kmeans(k, epsilon=0, distance='euclidian'):<br/>    history_centroids = []<br/>    #set the distance calculation type <br/>    if distance == 'euclidian':<br/>        dist_method = euclidian</span></pre><p id="ddfa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们将加载数据集并检查数据集的形状，以获得实例(观察)的数量和特征的数量</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="f283" class="nd mh it oh b gy ol om l on oo">dataset = load_dataset('durudataset.txt')<br/>num_instances, num_features = dataset.shape</span></pre><p id="daf5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将使用 Numpy.random 中的随机数来定义 k 原型:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="d91b" class="nd mh it oh b gy ol om l on oo">prototypes = dataset[np.random.randint(0, num_instances - 1, size=k)]</span></pre><p id="dbf7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">之后，我们会将它们添加到质心历史列表中:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="c8e4" class="nd mh it oh b gy ol om l on oo">history_centroids.append(prototypes)</span></pre><p id="73c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我创建了这些列表来存储我们的质心聚类，并在每次迭代中跟踪它们:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="7f7c" class="nd mh it oh b gy ol om l on oo">prototypes_old = np.zeros(prototypes.shape)<br/>belongs_to = np.zeros((num_instances, 1))<br/>norm = dist_method(prototypes, prototypes_old)<br/>iteration = 0</span></pre><p id="86e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们将按照我之前陈述的公式执行 for 循环:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="ceb9" class="nd mh it oh b gy ol om l on oo">while norm &gt; epsilon:<br/>        iteration += 1<br/>        norm = dist_method(prototypes, prototypes_old)<br/>        for index_instance, instance in enumerate(dataset):<br/>            dist_vec = np.zeros((k,1))<br/>            for index_prototype, prototype in enumerate(prototypes):<br/>                #compute the distance between x and centroid<br/>                dist_vec[index_prototype] = dist_method(prototype, instance)<br/>            belongs_to[index_instance, 0] = np.argmin(dist_vec)<br/>            <br/>        tmp_prototypes = np.zeros((k, num_features))</span></pre><p id="ea69" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当范数大于ε时，对于数据集中的每个实例，我们将定义一个 k 大小的距离向量。然后对于每个质心，我们将计算 x 和质心之间的差。接下来，我们将遍历我们的原型列表，并获取分配给该原型的所有点。然后，我们将找到这些点的平均值，这将为我们提供新的质心。最后但同样重要的是，我们将把相应的值添加到列表的索引中。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="8c37" class="nd mh it oh b gy ol om l on oo">for index in range(len(prototypes)):<br/>            instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]<br/>            prototype = np.mean(dataset[instances_close], axis=0)<br/>            #add our new centroid to our new temporary list<br/>            tmp_prototypes[index, :] = prototype</span></pre><p id="83d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们可以将我们的临时原型设置为等于我们的最终质心列表。为了创建一个 Matplotlib 动画，我还会在这里添加质心列表的历史。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="5875" class="nd mh it oh b gy ol om l on oo">prototypes = tmp_prototypes<br/>history_centroids.append(tmp_prototypes)<br/>return prototypes, history_centroids, belongs_to</span></pre><p id="02e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于最终的函数，如下所示:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="bd2a" class="nd mh it oh b gy ol om l on oo">def kmeans(k, epsilon=0, distance='euclidian'):<br/>    history_centroids = []<br/>    if distance == 'euclidian':<br/>        dist_method = euclidian<br/>    #set the dataset<br/>    dataset = load_dataset('durudataset.txt')<br/>    num_instances, num_features = dataset.shape <br/>    prototypes = dataset[np.random.randint(0, num_instances - 1, size=k)]<br/>    history_centroids.append(prototypes)<br/>    prototypes_old = np.zeros(prototypes.shape)<br/>    belongs_to = np.zeros((num_instances, 1))<br/>    norm = dist_method(prototypes, prototypes_old)<br/>    iteration = 0<br/>    while norm &gt; epsilon:<br/>        iteration += 1<br/>        norm = dist_method(prototypes, prototypes_old)<br/>        for index_instance, instance in enumerate(dataset):<br/>            dist_vec = np.zeros((k,1))<br/>            for index_prototype, prototype in enumerate(prototypes):<br/>                dist_vec[index_prototype] = dist_method(prototype, instance)<br/>            belongs_to[index_instance, 0] = np.argmin(dist_vec)<br/>            <br/>        tmp_prototypes = np.zeros((k, num_features))<br/>        for index in range(len(prototypes)):<br/>            instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]<br/>            prototype = np.mean(dataset[instances_close], axis=0)<br/>            #add our new centroid to our new temporary list<br/>            tmp_prototypes[index, :] = prototype<br/>        prototypes = tmp_prototypes<br/>        history_centroids.append(tmp_prototypes)<br/>    return prototypes, history_centroids, belongs_to</span></pre><p id="89bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们决定绘制它，我的结果看起来有点像这样:</p><div class="kj kk kl km gt ab cb"><figure class="op kn oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5381738d4fefb4b9f8b2c974d3dcaf6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*cqmSJkIIEYe50eWP-OfaWw.png"/></div></figure><figure class="op kn ov or os ot ou paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/735f083b168904e4d692141c9150dfb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*iOXiRKI_7cKvsbipFOECuA.png"/></div></figure><figure class="op kn ow or os ot ou paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/1e9ea6b5f8b0c652d463e5f17358b983.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*6ocl-ieELVe5pZZ-ZCyHcQ.png"/></div></figure></div><blockquote class="ox"><p id="f7ee" class="oy oz it bd pa pb pc pd pe pf pg lp dk translated">很酷，对吧？</p></blockquote><h1 id="5cde" class="mg mh it bd mi mj ph ml mm mn pi mp mq jz pj ka ms kc pk kd mu kf pl kg mw mx bi translated">结论</h1><p id="bd5b" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">虽然非监督学习可能不会像大多数监督学习模型那样受到喜爱或使用，但仅仅因为结果没有被标记并不意味着不能从数据中学习到很多信息。无监督学习是探索和真正理解数据如何分组以及不同特征如何相互作用的一个很好的工具。虽然在很多情况下，数据科学家可能会偏离使用无监督学习有点远，但很容易明白为什么它有时会非常有益！</p></div></div>    
</body>
</html>