<html>
<head>
<title>Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索SimCLR:视觉表征对比学习的简单框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e?source=collection_archive---------10-----------------------#2020-02-23">https://towardsdatascience.com/exploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e?source=collection_archive---------10-----------------------#2020-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="528b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从头开始用一个<a class="ae ki" href="https://github.com/sthalles/SimCLR" rel="noopener ugc nofollow" target="_blank"> PyTorch实现SimCLR </a></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/e66cca0f76d32ea2277b3d53b88c6067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1uaA1tE5PDnVpSljxSTEoQ.png"/></div></div></figure><h1 id="5bfe" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="2138" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">很长一段时间以来，我们知道迁移学习在计算机视觉(CV)应用中的好处。如今，预训练的深度卷积神经网络(DCNNs)是学习新任务的第一个首选预解决方案。这些大型模型是在巨大的监督语料库上训练的，比如ImageNet。最重要的是，它们的特点是能很好地适应新问题。</p><p id="1ffb" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">当缺少带注释的训练数据时，这一点尤其有趣。在这种情况下，我们采用模型的预训练权重，在其上添加一个新的分类器层，并重新训练网络。这被称为<strong class="lp iu">迁移学习</strong>，是CV中使用最多的技术之一。除了在执行<strong class="lp iu">微调</strong>时的一些技巧(如果是这样的话)，已经(多次)表明:</p><blockquote class="mo"><p id="ccc7" class="mp mq it bd mr ms mt mu mv mw mx mi dk translated">如果为一项新任务进行训练，用预训练权重初始化的模型往往比使用随机初始化从零开始训练的模型学习得更快、更准确。</p></blockquote><p id="49ed" class="pw-post-body-paragraph ln lo it lp b lq my ju ls lt mz jx lv lw na ly lz ma nb mc md me nc mg mh mi im bi translated">然而，正如人们可能猜测的那样，在这个过程中存在一个瓶颈。目前大多数迁移学习方法依赖于在监督语料库上训练的模型。但是问题是注释数据并不便宜。</p><p id="1f30" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果我们环顾四周，数据，以一种无人监管的方式，是丰富的。因此，使用未标记的数据来学习表示是有意义的，这些表示可以用作训练更好的监督模型的代理。<strong class="lp iu">事实上，这是一个长期存在的问题，正如我们将看到的，当前对无监督表示学习的研究终于赶上了有监督的方法。</strong></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="3abd" class="kv kw it bd kx ky nk la lb lc nl le lf jz nm ka lh kc nn kd lj kf no kg ll lm bi translated">无监督表示学习</h1><p id="09d9" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">无监督表示学习关注于解决以下问题:</p><blockquote class="mo"><p id="ab41" class="mp mq it bd mr ms mt mu mv mw mx mi dk translated">我们如何从未标记的数据中学习好的表示？</p></blockquote><p id="ea83" class="pw-post-body-paragraph ln lo it lp b lq my ju ls lt mz jx lv lw na ly lz ma nb mc md me nc mg mh mi im bi translated">除了什么是好的表示的问题之外，从未标记的数据中学习也有很大的潜力。它可以开启许多当前迁移学习无法解决的应用。然而，从历史上看，无监督的表示学习比有监督的表示学习要困难得多。</p><p id="c5e4" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">作为一个简单的例子，让我们考虑乳腺癌检测的任务。目前，所有最佳解决方案都使用ImageNet预训练模型作为优化流程的起点。有趣的是，尽管乳腺癌幻灯片图像和常规ImageNet样本之间存在显著差异，但迁移学习假设在某种程度上仍然成立。</p><p id="db3d" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">大致来说，大多数用于乳腺癌检测的监督数据集，如<a class="ae ki" href="https://camelyon17.grand-challenge.org/" rel="noopener ugc nofollow" target="_blank">camelion数据集</a>，在大小和可变性方面都无法与常见的计算机视觉监督数据集相比。另一方面，我们有大量未加注释的乳腺癌幻灯片图像。因此，如果我们可以从无监督的(大得多的语料库)中学习良好的表示，这肯定会有助于学习更多特定的下游任务，这些任务具有有限的注释数据。</p><p id="683b" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">幸运的是，视觉无监督表示学习已经显示出巨大的前景。更具体地说，使用基于对比的技术学习的视觉表现现在达到了通过监督方法学习的水平——在一些自我监督的基准测试中<em class="np"/>。</p><p id="afb5" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">让我们探索一下无监督的对比学习是如何工作的，并仔细看看该领域的一项主要工作。</p><h1 id="fd79" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">对比学习</h1><blockquote class="mo"><p id="a0e2" class="mp mq it bd mr ms mt mu mv mw mx mi dk translated">对比法的目的是通过强制相似元素相等而不相似元素不同来学习表征。</p></blockquote><p id="3c5d" class="pw-post-body-paragraph ln lo it lp b lq my ju ls lt mz jx lv lw na ly lz ma nb mc md me nc mg mh mi im bi translated">最近几个月，我们看到了基于这些原则的无监督深度学习方法的爆炸式增长。事实上，在线性分类基准中，一些自我监督的基于对比的表示已经匹配基于监督的特征。</p><p id="5b1c" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对比学习的核心是噪声对比估计(NCE)损失。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nq"><img src="../Images/60101b51c246c864ebab4503f45e7257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hkhFuHjzdAjyAVY5.png"/></div></div></figure><p id="2236" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">上式中，可以把<strong class="lp iu"> <em class="np"> x+ </em> </strong>看作类似于输入<strong class="lp iu"> <em class="np"> x </em> </strong>的数据点。换句话说，观察值<strong class="lp iu"> <em class="np"> x </em> </strong>和<strong class="lp iu"> <em class="np"> x+ </em> </strong>是相关的，并且一对<strong class="lp iu"> <em class="np"> (x，x+) </em> </strong>代表一个<strong class="lp iu">正例</strong>。通常情况下，<strong class="lp iu"> <em class="np"> x+ </em> </strong>是对<strong class="lp iu"> <em class="np"> x </em> </strong>进行某种变换的结果。这可以是旨在改变<strong class="lp iu"> <em class="np"> x </em> </strong>的大小、形状或方向的几何变换，或者任何类型的数据增强技术。一些例子包括<em class="np">旋转、透明、调整大小、剪切等等</em>。</p><p id="1266" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">另一方面，<strong class="lp iu"> <em class="np"> x- </em> </strong>是与<strong class="lp iu"> <em class="np"> x </em> </strong>不同的例子。对<strong class="lp iu"> <em class="np"> (x，x-) </em> </strong>形成了一个<strong class="lp iu">反例</strong>，它们应该是不相关的。在这里，NCE损失将迫使他们不同于积极的一对。<strong class="lp iu">注意，对于每个正对<em class="np"> (x，x+) </em>我们有一组K个负对</strong>。事实上，经验结果表明，需要大量的否定才能获得良好的表象。</p><p id="f611" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="lp iu"> <em class="np"> sim(。</em>)</strong>函数是一个相似性(距离)度量。它负责最小化阳性之间的差异，同时最大化阳性和阴性之间的差异。<strong class="lp iu"> <em class="np">经常，sim(。)</em> </strong>用<strong class="lp iu">点积</strong>或<strong class="lp iu">余弦相似度</strong>来定义。</p><p id="0f84" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">最后，<strong class="lp iu"> <em class="np"> g(。</em>)</strong>是一种卷积神经网络编码器。具体来说，最近的对比学习架构使用暹罗网络来学习正面和负面示例的嵌入。这些嵌入然后作为输入传递给对比损失。</p><p id="45d6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">简单地说，我们可以把对比任务看作是试图在一堆否定中找出肯定的例子。</p><h1 id="3c01" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">视觉表征对比学习的简单框架——sim clr</h1><p id="12a9" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">SimCLR 使用上述对比学习的相同原则。在本文中，该方法在自监督和半监督学习基准中实现了SOTA。它引入了一个简单的框架来学习基于大量数据扩充的无标签图像的表示。<strong class="lp iu">简单来说，SimCLR使用对比学习来最大化同一图像的两个增强版本之间的一致性</strong>。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nr"><img src="../Images/59ea0e6b755e439a7a6c18c4a4b689e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9_wrCWuIQCx2BXz7.png"/></div></div></figure><p id="2383" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="np">学分:</em> <a class="ae ki" href="https://sthalles.github.io/simple-self-supervised-learning/#1" rel="noopener ugc nofollow" target="_blank"> <em class="np">视觉表征对比学习的简单框架</em> </a></p><p id="1b57" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了理解SimCLR，让我们探索一下它是如何建立在对比学习框架的核心组件之上的。</p><p id="4b3c" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">给定一个输入图像，我们通过应用两个独立的数据扩充操作符来创建它的两个相关副本。这些变换包括(1) <em class="np">随机裁剪和调整大小</em> , (2) <em class="np">随机颜色扭曲</em>,( 3)<em class="np">随机高斯模糊</em>。</p><p id="e22a" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">操作的顺序是固定的，但是由于每个操作都有自己的不确定性，这使得结果视图在视觉上有所不同。<strong class="lp iu">请注意，由于我们在同一幅图像上应用了2个不同的增强函数，如果我们对5幅图像进行采样，我们最终会得到批次</strong>中的<em class="np">2</em><em class="np">×</em><strong class="lp iu"><em class="np">5 = 10</em>个增强观察值。参见下面的视觉概念。</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ns"><img src="../Images/078213bea2f6f7469d75bd244c78374e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pt2ljMoVO5E1UgFz.png"/></div></div></figure><p id="8de1" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了最大化底片的数量，想法是将批中的每个图像(索引为<strong class="lp iu"> <em class="np"> i </em> </strong>)与所有其他图像(索引为<strong class="lp iu"> <em class="np"> j </em> </strong>)配对。<strong class="lp iu">注意，我们避免将观察<em class="np"> i </em>与其自身以及其扩充版本</strong>配对。结果，对于批中的每个图像，我们得到<strong class="lp iu"> <em class="np"> 2 ×(N-1) </em> </strong>负对— <em class="np">，其中N是批大小</em>。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nt"><img src="../Images/a9344c7f89234fe1d7fa1321b16c22f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3BhXuoV0IPVJC-vLaUBIQ.png"/></div></div></figure><p id="d501" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">注意，同样的方法适用于给定观测的两个扩充版本。这样，负对的数量增加得更多。</p><p id="63e8" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">而且通过这种方式排列否定样本，<strong class="lp iu"> SimCLR的优点是不需要额外的逻辑来挖掘否定。为了有一个概念，最近的实现像PIRL T21和MOCO分别使用一个存储体和一个队列来存储和采样大批量的底片。</strong></p><p id="f154" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">事实上，在最初的实现中，SimCLR的批处理大小为8192。按照这些想法，这个批量产生每个正样本对16382个负样本。此外，作者还表明，更大的批次(因此更多的阴性)往往会产生更好的结果。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nu"><img src="../Images/cf6532a7c8c80334c3cea095530c7b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HvU2ziU3UceCDuMo.png"/></div></div></figure><p id="c2d4" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">SimCLR使用<a class="ae ki" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>作为主要的ConvNet主干。ResNet接收形状为<strong class="lp iu"> (224，224，3) </strong>的增强图像，并输出2048维嵌入向量<strong class="lp iu"> <em class="np"> h </em> </strong>。然后，一个投影头<strong class="lp iu">g<em class="np">(。)</em> </strong>应用于嵌入向量<strong class="lp iu"><em class="np"/></strong>产生最终表示<strong class="lp iu"> <em class="np"> z = g(h) </em> </strong>。<strong class="lp iu">投影头<em class="np"> g(。)</em> </strong>是一个有2个密集层的多层感知器(MLP)。两层都有2048个单位，隐藏层具有非线性(ReLU)激活功能。</p><p id="9ebf" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于相似度函数，作者使用余弦相似度。它测量d维空间中两个非零向量之间角度的余弦值。如果两个向量之间的角度为0度，则余弦相似度为1。否则，它输出一个小于1一直到-1的数。注意，对比学习损失作用于投影头<strong class="lp iu"> <em class="np"> g(.)</em></strong>—<em class="np"/><strong class="lp iu"><em class="np">z</em></strong><em class="np">嵌入向量</em>。</p><p id="d183" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">一旦系统被训练好，我们就可以通过把投影头<strong class="lp iu">g<em class="np">(。)</em> </strong>并使用表象<strong class="lp iu"> <em class="np"> h </em> </strong>(直接来自ResNet)来学习新的下游任务。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nv"><img src="../Images/5fcf2f5cf8925da225b907e5dae2f857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Un6PLALSJn84Byil.png"/></div></div></figure><h1 id="a396" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">培训和评估</h1><p id="16ed" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">一旦对比学习目标的组成部分就位，系统的训练就简单了。你可以在这里看一下我的<a class="ae ki" href="https://github.com/sthalles/SimCLR" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><p id="9b5d" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了训练模型，我使用了STL-10数据集。它包含10个不同的类，每个类有合理的少量观察值。最重要的是，它包含一个更大的无监督集，其中有<em class="np"> 100000 </em>个未标记的图像——这是用于训练的大部分图像。</p><p id="ae05" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于这个实现，我使用ResNet-18作为ConvNet主干。它接收形状为<strong class="lp iu"> (96，96，3) </strong>、常规STL-10尺寸的图像，并输出尺寸为<strong class="lp iu"> 512 </strong>的矢量表示。<strong class="lp iu">投影头<em class="np"> g(。)</em> </strong>有2个全连通层。每层有512个单元，产生最终的64维特征表示<strong class="lp iu"> <em class="np"> z </em> </strong>。</p><p id="dafe" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了训练SimCLR，我采用了数据集的<strong class="lp iu"> <em class="np">训练+未标记的</em> </strong>部分— <em class="np">，总共给出了105000个图像</em>。</p><p id="6081" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在训练之后，我们需要一种方法来评估SimCLR所学习的表示的质量。一种标准方法是使用<strong class="lp iu">线性评估协议<em class="np">。</em> </strong></p><p id="a3a5" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">想法是在来自SimCLR编码器的固定表示上训练线性分类器。为此，我们获取训练数据，将其传递给预训练的SimCLR模型，并存储输出表示。注意，此时，我们不需要投影头<strong class="lp iu"> <em class="np"> g(。)</em>再也没有</strong>了。</p><p id="b8e5" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这些固定的表示然后被用于使用训练标签作为目标来训练逻辑回归模型。然后，我们可以测量测试的准确性，并使用它作为特性质量的度量。</p><p id="226b" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这个<a class="ae ki" href="https://github.com/sthalles/SimCLR" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>显示了评估协议。使用SimCLR固定表示作为训练信号，我们达到了64%的测试精度。有一个想法，对训练数据执行PCA并保留最重要的主成分，我们得到的测试精度只有36%。这强调了SimCLR所学习的特征的质量。</p><h1 id="483d" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">一些最后的评论</h1><p id="50b7" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">最初的SimCLR论文还提供了其他有趣的结果。其中包括:</p><ul class=""><li id="694b" class="nw nx it lp b lq mj lt mk lw ny ma nz me oa mi ob oc od oe bi translated"><em class="np">非监督对比特征学习在半监督基准上的结果；</em></li><li id="4c52" class="nw nx it lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><em class="np">向投影头添加非线性层的实验和益处；</em></li><li id="3bcc" class="nw nx it lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><em class="np">使用大批量的实验和好处；</em></li><li id="6619" class="nw nx it lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><em class="np">用对比目标训练大型模型的结果；</em></li><li id="be44" class="nw nx it lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><em class="np">对比学习中使用多种强数据扩充方法的消融研究；</em></li><li id="ef52" class="nw nx it lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><em class="np">标准化嵌入对训练基于对比学习的模型的好处；</em></li></ul><p id="fb66" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我鼓励你看看这篇文章，了解更多的细节。</p><p id="5e45" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="lp iu">感谢阅读！</strong></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="6f17" class="kv kw it bd kx ky nk la lb lc nl le lf jz nm ka lh kc nn kd lj kf no kg ll lm bi translated"><strong class="ak">参考文献</strong></h1><p id="f139" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">陈，丁等，“视觉表征对比学习的一个简单框架”arXiv预印本arXiv:2002.05709 (2020)。</p><p id="0b33" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">米斯拉、伊山和劳伦斯·范德马腾。"借口不变表征的自我监督学习."arXiv预印本arXiv:1912.01991 (2019)。</p><p id="6126" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">何，，等，“无监督视觉表征学习中的动量对比”arXiv预印本arXiv:1911.05722 (2019)。</p><p id="4c88" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="np">-何，，等，“深度残差学习在图像识别中的应用”IEEE计算机视觉和模式识别会议录。2016.</em></p><p id="6c01" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="np">原载于</em><a class="ae ki" href="https://sthalles.github.io/simple-self-supervised-learning/" rel="noopener ugc nofollow" target="_blank"><em class="np">https://sthalles . github . io</em></a><em class="np">。</em></p></div></div>    
</body>
</html>