<html>
<head>
<title>Is Upside-Down Reinforcement Learning = Imitation Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">颠倒强化学习=模仿学习吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98?source=collection_archive---------53-----------------------#2020-04-26">https://towardsdatascience.com/is-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98?source=collection_archive---------53-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4356" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解和实现 UDRL 算法</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="4d27" class="kp kq it bd kr ks kt ku kv kw kx ky kz jz la ka lb kc lc kd ld kf le kg lf lg bi translated">什么是颠倒强化学习？</h1><p id="a894" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我是在参加 NeurIPS 2019 的 RL 研讨会时发现这项工作的。老实说，这是我在会议上偶然发现的最酷的想法之一。你可以在这里看看我在会议上最喜欢的其他想法。无论如何，这个帖子是关于检查倒挂强化学习更多。你可以在这里阅读完整的论文<a class="ae md" href="https://arxiv.org/abs/1912.02877" rel="noopener ugc nofollow" target="_blank"/>，但是这里是摘要所说的:</p><blockquote class="me mf mg"><p id="112f" class="lh li mh lj b lk mi ju lm ln mj jx lp mk ml ls lt mm mn lw lx mo mp ma mb mc im bi translated">传统的强化学习(RL)算法要么用价值函数来预测回报，要么使用策略搜索来最大化回报。我们研究了一种替代方法:倒置强化学习(倒置 RL 或 UDRL)，它主要使用监督学习技术来解决 RL 问题。在这里，我们提出了 UDRL 的第一个具体实现，并证明了它在某些情景学习问题上的可行性。实验结果表明，其性能可以与经过几十年研究开发的传统基线算法相媲美，甚至超过后者。</p></blockquote><p id="5cdb" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">如果你想更深入地了解这篇论文，你可以观看这个优秀的<a class="ae md" href="https://www.youtube.com/watch?v=RrvC8YW0pT0" rel="noopener ugc nofollow" target="_blank">视频</a>。但是 TL；dr——他们设计了一种新的监督学习算法来解决强化学习任务。没有政策梯度，没有价值函数估计，只是简单的旧的监督学习。下面是论文中的一个图表，可以更好地说明这一点:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/97e47198fcad1429fa0f4607c37cf324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RY-aU8a8h_l_Y8gy.png"/></div></div></figure><p id="80e5" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">行为功能试图预测的是— <em class="mh">根据这一观察，在这一期望的时间范围(总时间步长)</em>内，采取什么样的最佳行动来实现这一期望的回报(总回报)。它们不是学习/模拟奖励，而是被用作直接预测行动的输入(命令)。</p><p id="c842" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">为了更好地理解整个算法，我进行了一些快速实验。这篇文章记录了我的发现。</p><p id="3256" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">我特别想回答以下两个问题:</p><ol class=""><li id="ff8d" class="nc nd it lj b lk mi ln mj lq ne lu nf ly ng mc nh ni nj nk bi translated">既然实现大多数 RL 算法是极其重要的，那么让这个算法运行起来有多容易呢？</li><li id="28dd" class="nc nd it lj b lk nl ln nm lq nn lu no ly np mc nh ni nj nk bi translated">只是巧妙伪装的模仿学习算法吗？</li></ol><h1 id="29d5" class="kp kq it bd kr ks nq ku kv kw nr ky kz jz ns ka lb kc nt kd ld kf nu kg lf lg bi translated">要解决的任务—稀疏月球着陆器</h1><p id="ba52" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了回答上述问题，我实现了算法来解决稀疏月球着陆器任务(论文中提到的任务之一)。任务是学习一个代理人能够成功地登陆月球情人，如下所示:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/594f40a848a49d4e23d2d0db5884c531.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*o-dCTcbv5COvlpFK.gif"/></div></figure><p id="a7bb" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">我通过修改奖励函数将 OpenAI Gym 的“LunarLander-v2”环境手动转换为稀疏环境，如下所示:</p><ul class=""><li id="12f3" class="nc nd it lj b lk mi ln mj lq ne lu nf ly ng mc nw ni nj nk bi translated">0 奖励所有非终端步骤</li><li id="6f8a" class="nc nd it lj b lk nl ln nm lq nn lu no ly np mc nw ni nj nk bi translated">最后一步的每集总奖励</li></ul><h1 id="dde8" class="kp kq it bd kr ks nq ku kv kw nr ky kz jz ns ka lb kc nt kd ld kf nu kg lf lg bi translated">实施说明</h1><ul class=""><li id="6268" class="nc nd it lj b lk ll ln lo lq nx lu ny ly nz mc nw ni nj nk bi translated">所有的学习组件都是使用 Pytorch 实现的</li><li id="717e" class="nc nd it lj b lk nl ln nm lq nn lu no ly np mc nw ni nj nk bi translated">论文列出了他们在实验中扫描的所有超参数值。我根本没有调过音。只是选择了每一个的中间值。</li><li id="5264" class="nc nd it lj b lk nl ln nm lq nn lu no ly np mc nw ni nj nk bi translated">模型架构:</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oa"><img src="../Images/d07354689c3356f25f1439ab6f9a47c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uer75WSqjV_QIzW7.png"/></div></div></figure><ul class=""><li id="3c48" class="nc nd it lj b lk mi ln mj lq ne lu nf ly ng mc nw ni nj nk bi translated">使用<a class="ae md" href="http://comet.ml/" rel="noopener ugc nofollow" target="_blank"> comet.ml </a>进行度量跟踪</li></ul><p id="04d9" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">这个实验的代码可以在<a class="ae md" href="https://github.com/bprabhakar/upside-down-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">这里</a>获得。现在，让我们来看看实验结果。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="b7ae" class="kp kq it bd kr ks kt ku kv kw kx ky kz jz la ka lb kc lc kd ld kf le kg lf lg bi translated">让算法发挥作用有多容易？</h1><p id="a455" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回答——出奇的快。我能够在我的第三个(！)运行尝试；而不需要对超参数进行太多的修改。我不记得任何基于策略梯度的算法上一次出现这种情况是什么时候了。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ob"><img src="../Images/5c682bf5df5a9fc47c2852743e73cbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OiiuLTp1mZozOPaC.png"/></div></div></figure><p id="2751" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">当然，这需要半信半疑，因为“月球着陆器”是一项相对简单的任务。但是如果你看看代码中实现的算法，你会同意这个算法简单得可笑！</p><h1 id="81b4" class="kp kq it bd kr ks nq ku kv kw nr ky kz jz ns ka lb kc nt kd ld kf nu kg lf lg bi translated">只是变相的模仿学习吗？</h1><p id="2803" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我读这篇论文的第一印象是，这个想法听起来非常类似于模仿学习。考虑描述重放缓冲策略的论文中的以下摘录:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oc"><img src="../Images/fa44913d56504eb69fe90c0eb1fa2035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-GFLNRxUqTo8lpKx.png"/></div></div></figure><p id="c30c" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">因此，随着训练的进行，通过设计，存储在缓冲器中的轨迹开始越来越像<em class="mh">专家轨迹</em>(高回报的剧集)。仅仅学习从状态到这些专家轨迹上的行为的映射(模仿学习)就足够了。这个想法实际上在之前的工作中得到更好的表达，叫做<a class="ae md" href="https://arxiv.org/abs/1806.05635" rel="noopener ugc nofollow" target="_blank">自我模仿学习</a>。</p><p id="0fe7" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">为了测试这一假设，我进行了以下实验——试图通过屏蔽行为功能的命令输入(保持其他一切不变)来了解要采取的最佳行动。所以有效地，仅仅学习仅仅从观察中预测行动。事情是这样的:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi od"><img src="../Images/703a7d821bd664318aab374c32c85dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nmmV7Lh_u_dYJZFc.jpeg"/></div></div></figure><p id="0d3a" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">显然，将<em class="mh">命令</em>作为行为函数的输入会产生不同。我的猜测是，拥有这些命令有助于代理进一步区分不同类型的高回报轨迹，从而帮助它学习得更快。这显然需要在更复杂的环境中进一步测试，但我的微观实验结果绝对令人鼓舞。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="296d" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">实验的所有代码都可以在这个<a class="ae md" href="https://github.com/bprabhakar/upside-down-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>上获得</p><p id="035c" class="pw-post-body-paragraph lh li it lj b lk mi ju lm ln mj jx lp lq ml ls lt lu mn lw lx ly mp ma mb mc im bi translated">这个帖子最初发布在<a class="ae md" href="https://bprabhakar.github.io/2020/04/18/udrl.html" rel="noopener ugc nofollow" target="_blank"> bprabhakar.github.io </a></p></div></div>    
</body>
</html>