<html>
<head>
<title>Decision Trees Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6?source=collection_archive---------2-----------------------#2020-03-08">https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6?source=collection_archive---------2-----------------------#2020-03-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="50e2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习机器学习决策树的所有知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f9da4aba7422e118f4274743e913f3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0pQGRhMVYfU5ZYXg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://unsplash.com/photos/tGTVxeOr_Rs" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="c62a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在这篇文章中，我将简单地解释决策树。它可以被认为是一个傻瓜帖子的决策树，然而，我从来没有真正喜欢过这个表达。</em></p><p id="dc12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在我们开始之前，这里有一些额外的资源，可以让你的机器学习生涯一飞冲天</em></p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a899" class="mb mc it lx b gy md me l mf mg"><em class="lv">Awesome Machine Learning Resources:</em></span><span id="5cc1" class="mb mc it lx b gy mh me l mf mg"><em class="lv">- For </em><strong class="lx iu"><em class="lv">learning resources</em></strong><em class="lv"> go to </em><a class="ae ky" href="https://howtolearnmachinelearning.com/books/machine-learning-books/" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu"><em class="lv">How to Learn Machine Learning</em></strong></a><em class="lv">! <br/>- For more awesome Data Science news, tools, educative articles and platforms </em><a class="ae ky" href="https://z-ai.medium.com/subscribe" rel="noopener"><strong class="lx iu"><em class="lv">subscribe to my newsletter</em></strong></a>!</span></pre><div class="mi mj gp gr mk ml"><a href="https://z-ai.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd iu gy z fp mq fr fs mr fu fw is bi translated">订阅我的专属列表！</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅我的专属列表！获取你喜欢的所有新鲜文章&lt;3! By signing up, you will create a Medium…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">z-ai.medium.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz ks ml"/></div></div></a></div><h1 id="d1e4" class="na mc it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">Introduction and Intuition</h1><p id="6198" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">In the Machine Learning world, <strong class="lb iu">决策树</strong>是一种<strong class="lb iu">非参数模型，</strong>可用于分类和回归。</p><p id="a711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着决策树是灵活的模型，不会随着我们添加更多的特征而增加它们的参数数量(如果我们正确地构建了它们)，并且它们可以输出<strong class="lb iu">分类</strong>预测(<em class="lv">像植物是否属于某种类型</em>)或者<strong class="lb iu">数字</strong>预测(<em class="lv">像房子的价格</em>)。</p><p id="d6cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们由两种元素构成:<strong class="lb iu">节点和</strong>分支。在每个节点上，我们会评估数据的一个特征，以便在训练过程中拆分观察值，或者在进行预测时使特定的数据点遵循特定的路径。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e228c273514b6dc079768c4c4dcf5f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*YeBfiTiX072I8t2HD-z8aQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在每个节点，对一个变量进行求值，以决定走哪条路径。</p></figure><p id="33b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">构建决策树时，通过<strong class="lb iu"> <em class="lv">递归</em> </strong>评估不同的特征，并在每个节点使用最能拆分数据的特征来构建决策树。这个后面会详细解释。</p><p id="09b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也许开始解释的最好方法是看看决策树是什么样子的，以建立如何使用它们的快速直觉。下图显示了这些树的一般结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/33f0e64de18fbead355a6913899829fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3P1333UmqEww6YMpjisj4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树的图形。<a class="ae ky" href="https://www.diagrams.net/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="98ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在该图中，我们可以观察到三种节点:</p><ul class=""><li id="f7d0" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated"><strong class="lb iu">根节点:</strong>是开始图的节点。在普通的决策树中，它评估最能分割数据的变量。</li><li id="fa51" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">中间节点:</strong>这些是评估变量的节点，但不是进行预测的最终节点。</li><li id="fae0" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">叶节点:</strong>这些是树的最终节点，在这里进行类别或数值的预测。</li></ul><p id="a441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在我们对什么是决策树有了一个大致的概念，让我们看看它们是如何构建的。</p><h1 id="2ff9" class="na mc it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">决策树的训练过程</h1><p id="1db4" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">正如我们之前提到的，决策树是通过递归分割训练样本来构建的，使用的是最适合特定任务的数据特征。这是通过评估某些指标来完成的，如分类决策树的<strong class="lb iu"> <em class="lv">基尼指数</em> </strong>或<strong class="lb iu"> <em class="lv">熵</em> </strong>，或回归树的<strong class="lb iu"> <em class="lv">残差或均方差</em> </strong>。</p><p id="37a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们在节点处评估的特征是离散的或连续的，则过程也是不同的。<strong class="lb iu">对于</strong> <strong class="lb iu">离散特征</strong>对其所有可能值进行评估，从而为每个变量计算出 N 个度量，N 为每个分类值的可能值的数量。对于连续特征，训练数据的每两个连续值(从最低到最高排序)的平均值被用作可能的阈值。</p><p id="0cb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于某个节点，该过程的结果是变量列表，每个变量具有不同的阈值，以及每个变量/阈值串联的计算度量(Gini 或 MSE)。然后，我们选择变量/阈值组合，该组合为我们提供用于结果子节点的特定度量的<strong class="lb iu">最高/最低值</strong>(度量中的<strong class="lb iu">最高减少或增加)。</strong></p><p id="7a1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不会深入研究这些指标是如何计算的，因为这与这篇介绍性文章的主题无关，但是如果你感兴趣，我会在最后留下一些资源供你深入研究。目前，只要把这些度量(分类树的基尼系数和回归树的均方误差)想象成某种我们想要减少的误差。</p><p id="175b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个两个决策树的例子，一个分类决策树和一个回归决策树，以更清楚地了解这个过程。下图显示了为著名的鸢尾数据集构建的<strong class="lb iu">分类树，其中我们试图使用花瓣宽度、长度、萼片长度等特征来预测三种不同花朵的类别</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/dda24690757e0bae8e2b98db876cdaa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*4ztnUngyDvkAd4dcR8H6dA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为 Iris 数据集构建决策树</p></figure><p id="040a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，根节点从三个类别中的每一个的 50 个样本开始，并且基尼指数(因为它是分类树，所以基尼指数越低越好)为 0.667。</p><p id="7896" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在该节点中，最好地分割不同类别的数据的特征是以 cm 为单位的<strong class="lb iu"> <em class="lv">花瓣宽度</em> </strong>，使用值 0，8 作为阈值。这产生了两个节点，一个具有 Gini 0(只有一种花的完美纯节点),另一个具有 Gini 0.5，在那里两种其他种类的花被分组。</p><p id="a8be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个中间节点(来自根节点的错误路径)中，使用阈值 1，75 来评估相同的特征(是的，这可能发生，并且如果该特征很重要，它实际上经常发生)。现在这导致了另外两个子节点，它们不是纯粹的，但是基尼系数很低。</p><p id="68f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有这些节点中，对数据的所有其他特征(<em class="lv">萼片长度、萼片宽度和花瓣长度</em>)进行评估，并计算其结果基尼指数，然而，给我们最好结果(最低基尼指数)的特征是花瓣宽度。</p><p id="3842" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">树没有继续增长的原因是因为决策树总是配置一个停止增长的条件，否则它们会一直增长，直到每个训练样本都被分成它自己的叶节点。这些停止条件是树的最大深度、叶节点中的最小样本或误差度量的最小缩减。</p><p id="72a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们检查一下<strong class="lb iu">回归树</strong>，为此，我们将使用<strong class="lb iu"> <em class="lv">波士顿房价</em> </strong>数据集，结果如下图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e4797c27643b02a67c91dc195cef0755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*J4-rXeUjeSvyK5GM84BGEA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为波士顿住房数据集构建的决策树</p></figure><p id="10be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，现在我们没有基尼指数，而是有了<strong class="lb iu"> MSE </strong>(均方差)。与前面的基尼系数示例一样，我们的树是使用最大程度地减少这种误差的特征/阈值组合来构建的。</p><p id="03b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根节点使用阈值为 9.725 的变量<strong class="lb iu"><em class="lv">LSTAT</em></strong>(<em class="lv">%区域</em>中人口的下层状态)对样本进行初始划分。我们可以看到，在根节点我们有 506，我们分为 212(左子节点)和 294(右子节点)。</p><p id="bf7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左边的子节点使用阈值为 6.631 的变量<strong class="lb iu"> <em class="lv"> RM </em> </strong> ( <em class="lv">每个住所的房间数</em>)，右边的节点使用阈值为 16.085 的相同的<em class="lv"> LSTAT </em>变量，产生了四个漂亮的叶节点。像以前一样，在每个节点上评估所有其他变量，但这两个变量是最好的数据分割变量。</p><p id="82d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">厉害！现在我们知道决策树是如何构建的。让我们来学习它们是如何被用来做预测的。</p><h1 id="9b58" class="na mc it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">用决策树做预测</h1><p id="27f6" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">使用决策树预测新样本的类别或数值目标值非常容易。这是这类算法的主要优点之一。我们所要做的就是从根节点开始，查看它所评估的特性的值，并根据该值转到左边或右边的子节点。</p><p id="886b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复这个过程，直到我们到达一个叶节点。当这种情况发生时，根据我们面对的是分类问题还是回归问题，可能会发生两种情况:</p><p id="d570" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a) <strong class="lb iu">如果我们面临一个分类问题</strong>，预测的类别将是该叶节点上的类别的模式。还记得在分类树中，我们在中间的叶节点上有 value = [0，49，5]吗？这意味着到达该节点的测试样本最有可能属于在该节点上具有 49 个训练样本的类，因此我们将其分类。</p><p id="c8da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b) <strong class="lb iu">对于一棵回归树</strong>，我们在最后做出的预测是目标变量在这个叶节点上的值的平均值。在我们的住房示例中，如果叶节点有 4 个价格为 20、18、22 和 24 的样本，则该节点的预测值将是 21，这是在此结束的 4 个训练示例的平均值。</p><p id="81f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，我们可以看到如何为先前的回归树预测一个新的测试样本(一栋房子)。</p><p id="8e13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">注:</em> </strong> <em class="lv">仅显示树中使用的房屋特征。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/a456fc4ede7195c4d75790c8a7e60040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9YLCJdDVvm4UU8NN4KkJXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特定样本遵循的路径和给定预测的值。来自<a class="ae ky" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">平面图标</a>的图标。</p></figure><p id="8763" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧！现在我们知道如何使用决策树进行预测。让我们通过学习他们的优点和缺点来结束。</p><h1 id="6268" class="na mc it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">决策树的利与弊</h1><h2 id="5a8f" class="mb mc it bd nb op oq dn nf or os dp nj li ot ou nl lm ov ow nn lq ox oy np oz bi translated">优势:</h2><ul class=""><li id="6be2" class="ny nz it lb b lc nr lf ns li pa lm pb lq pc lu od oe of og bi translated">决策树的主要优势是<strong class="lb iu">如何容易</strong>解释<strong class="lb iu">。当其他机器学习模型接近黑盒时，决策树提供了一种图形和直观的方式来理解我们的算法做什么。</strong></li><li id="4a37" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">与其他机器学习算法相比，决策树需要<strong class="lb iu">更少的数据</strong>来训练。</li><li id="d78d" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">它们可用于<strong class="lb iu">分类</strong>和<strong class="lb iu">回归</strong>。</li><li id="7da6" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">他们是<strong class="lb iu">简单的</strong>。</li><li id="6294" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">他们容忍<strong class="lb iu">缺失值</strong> s。</li></ul><h2 id="ce10" class="mb mc it bd nb op oq dn nf or os dp nj li ot ou nl lm ov ow nn lq ox oy np oz bi translated">不足之处</h2><ul class=""><li id="1a87" class="ny nz it lb b lc nr lf ns li pa lm pb lq pc lu od oe of og bi translated">他们很容易使<strong class="lb iu">过度拟合</strong>训练数据，并且对异常值敏感。</li><li id="d9fa" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">他们是弱学习者</strong>:单个决策树通常不会做出很好的预测，因此多棵树经常被组合起来形成“<em class="lv">森林</em>”以产生更强的集成模型。这将在以后的文章中讨论。</li></ul><div class="mi mj gp gr mk ml"><a href="https://z-ai.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd iu gy z fp mq fr fs mr fu fw is bi translated">订阅我的专属列表！</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅我的专属列表！获取您喜欢的所有新鲜文章&lt;3! By signing up, you will create a Medium…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">z-ai.medium.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz ks ml"/></div></div></a></div><h1 id="1bdd" class="na mc it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">Conclusion and additional resources</h1><p id="91ed" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated"><em class="lv">决策树是简单而直观的算法，正因为如此，在试图解释机器学习模型的结果时，它们被大量使用。尽管很弱，但它们可以结合起来产生非常强大的 bagging 或 boosting 模型。在接下来的文章中，我们将探索其中的一些模型。</em></p><p id="3890" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">如果你想知道构建一棵树的完整过程，可以看看下面的视频:</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="7fc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">就这些，我希望你喜欢这个帖子。随时</em> <a class="ae ky" href="https://twitter.com/Jaimezorno" rel="noopener ugc nofollow" target="_blank"> <em class="lv">在 Twitter 上关注我</em> </a> <em class="lv">在</em><strong class="lb iu"><em class="lv">@ jaimezorno</em></strong><em class="lv">。还有，你可以看看我在数据科学和机器学习上的帖子</em><strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://medium.com/@jaimezornoza" rel="noopener"><strong class="lb iu"><em class="lv">这里</em> </strong> </a> <em class="lv">。好好读！</em></p><p id="f352" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">关于机器学习和数据科学的更多资源，请查看以下资源库:</em> <a class="ae ky" href="https://howtolearnmachinelearning.com/books/machine-learning-books/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">如何学习机器学习</em> </a> <em class="lv">！有关职业资源(工作、事件、技能测试)，请访问</em><a class="ae ky" href="https://aigents.co/" rel="noopener ugc nofollow" target="_blank"><em class="lv">AIgents.co——数据科学家的职业社区&amp;机器学习工程师</em> </a> <em class="lv">。</em></p><p id="2fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">还有，更多类似这样的帖子</em><strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://medium.com/@jaimezornoza" rel="noopener"><strong class="lb iu"><em class="lv">关注我上媒</em> </strong> </a> <em class="lv">，敬请关注！</em></p><p id="4f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">还有，你可以订阅我的邮箱列表在这里获取最新更新和独家内容:</em> <a class="ae ky" href="https://z-ai.medium.com/subscribe" rel="noopener"> <em class="lv">订阅邮箱列表</em> </a> <em class="lv">。</em></p><p id="69d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">最后，为了更深入地了解决策树和机器学习，看看下面文章中描述的书:</em></p><div class="mi mj gp gr mk ml"><a rel="noopener follow" target="_blank" href="/the-book-to-really-start-you-on-machine-learning-47632059fd0e"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd iu gy z fp mq fr fs mr fu fw is bi translated">让你真正开始机器学习的书</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">让你的机器学习知识更上一层楼</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">towardsdatascience.com</p></div></div><div class="mu l"><div class="pf l mw mx my mu mz ks ml"/></div></div></a></div><p id="1921" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢您的阅读，祝您有美好的一天！</p></div></div>    
</body>
</html>