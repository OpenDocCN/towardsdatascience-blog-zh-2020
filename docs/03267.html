<html>
<head>
<title>How to Train Regression and not to Lose a Sense of Reality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练回归而不失去真实感</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-your-model-and-not-to-lose-a-sense-of-reality-252351b89824?source=collection_archive---------31-----------------------#2020-03-28">https://towardsdatascience.com/how-to-train-your-model-and-not-to-lose-a-sense-of-reality-252351b89824?source=collection_archive---------31-----------------------#2020-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9b54" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">培训和分析中的常见陷阱</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9040d21b20be4e2f4d9e8173ca3e02a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZJkzUKbpcVgzNB_T1oSLA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">詹姆斯·霍姆通过<a class="ae kv" href="https://pixabay.com/photos/windmills-don-quixote-windmill-hill-1924129/" rel="noopener ugc nofollow" target="_blank">皮克斯贝</a>拍摄的照片(CC0)</p></figure><p id="1806" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">在无所不包的人工智能时代，如果我们有足够的数据，很容易陷入机器学习(ML)可以最终解决任何问题的想法。在这里，以一个大的带标签的数据集为例，我们发现当大量的数据不够时，没有领域专业知识的 ML 的直接应用是如何误导的，以及我们有什么工具来避免错觉。</em></p><p id="01ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">大纲</strong> <br/> -材料科学中的数据科学<br/> -特征化<br/> -用神经网络回归<br/> -范畴分类<br/> -随机森林能表现得更好吗？<br/> -混淆矩阵<br/> -数据扩充&amp;分析</p><p id="a735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi lt translated">我们想要解决的一个问题存在于组合学、概率和材料科学中。为了在技术上取得进步，我们需要发现新的更高效的材料:磁铁、压电材料、你刚刚在屏幕上触摸向下滚动的透明导电氧化物，或者其他许多材料。大多数现代材料是多组分的，由三种或三种以上的原子元素组成。随着集合中元素数量的增加，组合搜索很快变得难以处理。为了进一步测试研究人员的士气，不是所有的原子组合都能产生稳定的成分，更不用说功能性的了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/045b0f10f27b0bbf3c6662feb8c6e127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hElfqAnO5e1lBwxMfUUbSQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。原子元素周期表。一组随机的原子会形成一种新材料吗？</p></figure><p id="e152" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过几十年的研究和实验，我们已经收集了超过 200，000 种材料的数据库[1]，其中几千种材料由不同的原子组成，这将构成我们的训练数据集。我们的目标是预测一组新的原子是否可能富含新的稳定物质。</p><h2 id="fc20" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">特色化</h2><p id="6bda" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">为了尽可能多地编码关于一组元素的信息，可以使用元素描述符:原子序数、质量、价电子数等来产生候选集合的多维向量。我们可以从参考文献中选择 40 种不同的原子描述符。2，构成 4 个元素的候选集合的 160 个特征。</p><blockquote class="nb nc nd"><p id="09ed" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">高度相关的特征增加了复杂性，同时没有提供额外的描述。一个好的做法是确保描述符是正交的和可缩放的。</p></blockquote><p id="2dd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">来自实验数据库的目标是由一组特定元素组成的大量实验验证的稳定材料——对于一些组，它小到 1，对于另一些组，大到 162。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="6ce5" class="md me iq ni b gy nm nn l no np"># An example of featurisation<br/>import numpy as np<br/>def symbols2numbers(data, symbols, tables):<br/>    ''' Featurisation of atomic sets into a n-dim vectors <br/>        data are atomic sets of interests<br/>        symbols are atomic symbols<br/>        tables are tabulated elemental descriptors, i.e.<br/>        atomic numbers, masses, etc.'''</span><span id="2e37" class="md me iq ni b gy nq nn l no np">    transit = [ {} for i in range(len(tables))]<br/>    for i, table in enumerate(tables): <br/>       transit[i]  = {sym: num for sym, num in zip(symbols, table)}<br/>    <br/>    vectors = []<br/>    for set in data:<br/>        for atom in set: <br/>            numbers = [t[atom] for t in transit]<br/>        vectors.append(numbers)<br/>    return np.asarray(vectors)</span></pre><h2 id="e0dd" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">回归</h2><p id="97bd" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">我们将尝试训练一个深度神经网络(NN)来识别一组元素的 160 个特征的函数形式，从而产生许多[一组的稳定成分]。我们从实验神经网络架构开始——向层中添加节点并将层堆叠起来。我们的度量是回归损失函数。我们也可以试验它的形式，但是具有均方误差(mse)的稳定解决方案应该足以看到损失很大，更糟糕的是，它不会随着训练的进行而减少。</p><blockquote class="nb nc nd"><p id="66ec" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">在选择验证集时，必须小心谨慎。具有多个时期和自适应学习率的大型 NN 可以减少训练期间的损失——事实上，如果验证损失仍然很大，这可能只是过度拟合的迹象。</p></blockquote><p id="591a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将自适应 momenta optimizer (Adam)更改为随机梯度下降，并将内斯特罗夫 momenta 更新作为最后手段，说服我们回到我们应该开始的地方——分析用于训练的可用数据。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="1826" class="md me iq ni b gy nm nn l no np"># Implementation of a NN with one hidden layer</span><span id="5ef7" class="md me iq ni b gy nq nn l no np">from keras.model import Sequential<br/>from keras.layers import Dense, Dropout<br/>from keras.callbacks import ReduceLROnPlateau<br/>from sklearn.preprocessing import StandardScaler</span><span id="94b8" class="md me iq ni b gy nq nn l no np">x_train = symbols2numbers(data, symbols, tables)<br/>x_train = StandardScaler().fit_transform(x_train)<br/>original_dim = x_train.shape[1]</span><span id="d2eb" class="md me iq ni b gy nq nn l no np">model = Sequential()<br/>model.add(Dense(<br/>    original_dim,<br/>    activation='relu',<br/>    input_shape=(original_dim,),<br/>    activity_regularizer=l2(0.1)))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(<br/>    original_dim,<br/>    activation='relu',<br/>    input_shape=(original_dim,),<br/>    activity_regularizer=l2(0.1)))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(1))<br/>model.compile(loss='mse',optimizer='adam')</span><span id="da3b" class="md me iq ni b gy nq nn l no np">rlr = ReduceLROnPlateau(<br/>         monitor='val_loss',<br/>         factor=0.2, patience=10, min_lr=1e-8)<br/>model.fit(x_train, y_train,<br/>        batch_size=30,<br/>        shuffle=True,<br/>        validation_split=0.1,<br/>        epochs=100,<br/>        callbacks=[rlr])<br/>checkback = model.predict(x_train)</span></pre><h2 id="b0ed" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">分类</h2><p id="ca77" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">我们的数据严重不平衡:超过 50%的 4 原子组只有 1 个稳定的组成。为了改善平衡，我们可以通过增加稳定成分的数量来将数据分成标记组。</p><blockquote class="nb nc nd"><p id="5913" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">回归依赖于数据平衡。如果数据删减或增加是不可能的，将数据分成不同的类别可能会实现分类。</p></blockquote><p id="a3ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们将损失函数改为分类交叉熵，将“softmax”激活添加到输出层，并重新训练 NN 进行分类。这减少了几次交互后的验证损失函数，但是模型仍然不足:它将所有原子集作为最有可能的集合归入第一组。</p><p id="0ded" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以进一步简化任务，将数据分成两半:一个和多个稳定成分——现在两组平衡，第一组(一个稳定成分)的目标为 0，第二组(多个稳定成分)的目标为 1。损失函数:二元交叉熵。再培训。不合身。所有人的概率相等:55%对 45%。</p><h2 id="f475" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">随机森林能表现更好吗？</h2><p id="dde9" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">这是一个流行的问题。诚实的回答是，视情况而定。众所周知，具有大型决策树集合的随机森林(RF)分类器可以防止过拟合，并适于特征工程，例如处理缺失值。此外，RF 还可以用于多项选择分类和回归，这使得该模型在各种应用中具有通用性[3]。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="2bc2" class="md me iq ni b gy nm nn l no np">from sklearn.model_selection import cross_val_score as cvs<br/>from sklearn.model_selection import RepeatedStratifiedKFold as RSKF<br/>from sklearn.ensemble import RandomForestClassifier as RF<br/>from sklearn.preprocessing import StandardScaler, LabelEncoder</span><span id="ab18" class="md me iq ni b gy nq nn l no np">def evaluate(model, x, y)<br/>''' Model evaluation in cross-validation '''<br/>    cv = RSKF(n_splits=5, n_repeats=3, random_state=1)<br/>    scores = cvs(model, x, y, scoring='accuracy', cv=cv, n_jobs=4)<br/>    return scores</span><span id="c535" class="md me iq ni b gy nq nn l no np">x_train = symbols2numbers(data, symbols, tables)<br/>x_train = StandardScaler().fit_transform(x_train)</span><span id="9969" class="md me iq ni b gy nq nn l no np">model = RF(n_estimators=1000)<br/>scores = evaluate(x_train, y_train, RF(n_estimators=1000))<br/>model.fit(x_train,y_train)<br/>checkback = model.predict(x_train)</span></pre><p id="d5ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用默认的超参数，在交叉验证中对模型的评估证明了 70%的准确性。对保留的 1000 个标记数据条目(在训练期间没有暴露给模型)的预测产生以下混淆矩阵，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/977868b6cc2e9262ac13e48e3e218045.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*HVNXexisLFmwh37gN9atTg.png"/></div></figure><p id="c7e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从中我们计算出相关的度量:<br/>精度= TP/(TP+FP)= 100%<br/>NPV = TN/(TN+FN)= 35%<br/>召回= TP/(TP+FN)= 4%<br/>TNR = TN/(TN+FP)= 100%<br/>准确率= (TP + TN) /总计= 37%</p><blockquote class="nb nc nd"><p id="af5b" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">当与其他指标分开考虑时，定量指标(如准确度或精确度)可能会产生误导，因为它们并不总是您模型的最佳指标。</p></blockquote><p id="a378" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">F1 =精度*召回率/ 2(精度+召回率)= 22% <br/>平衡精度= (TPR + TNR) /2 = 52 %</p><p id="44ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的模型中，精度很高，因为零假阴性。然而，我们对预测尽可能多的真阳性感兴趣，因为这些是产生多种稳定成分的原子集合。但是召回的车型太少了。<br/>所以一节问题的答案是</p><blockquote class="nb nc nd"><p id="9cba" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">在特定的问题设置中，RF 可以胜过 NN。通过在 scikit-learn 中作为黑盒模型的简单实现，RF 可以是建立基本线的分类器的首选。</p></blockquote><h2 id="74ec" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">数据扩充、分析和结论</h2><p id="d767" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">有许多数据扩充技术可以通过增强机器学习和促进优化来击败欠拟合。一些技术，例如图像反射，也有助于评估模型，因为您可能会对反射图像有相同的预测。</p><blockquote class="nb nc nd"><p id="7dac" class="kw kx ls ky b kz la jr lb lc ld ju le ne lg lh li nf lk ll lm ng lo lp lq lr ij bi translated">数据扩充可以通过改进优化来帮助克服欠拟合。一些增强方法，例如图像反射，也有助于评估。</p></blockquote><p id="f13a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的问题中，我们可以通过原子集的排列来增加数据流形，因为你选择原子形成新材料的顺序不应该影响结果。观察具有排列不变性的真阳性，我们发现它们的数量甚至更少，最终丢弃该模型。</p><p id="46ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，在上述假设的基础上，仅仅通过观察组成原子集合来预测丰富物质相的 ML 的雄心勃勃的目标是无法实现的。这里的一个薄弱环节不是我们拥有的数据量，而是数据不完整的事实:我们的标签(报告的情况)很可能是错误的，因为我们并不真正知道自然实际上允许给定原子集有多少稳定的组成。这并不奇怪，毕竟，大多数原子集合只被报道一次，因为实验者倾向于竞相发现新的相位场。</p><p id="f010" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在科学传统中，有一个确实是最积极的，那就是不报告失败。这对于数据有偏差效应(没有负面影响),并且不利于进步的速度，因为我们不可避免地会沿着错误的路线前进，这些错误的路线可能已经被发现了多次，但是被保密了。我希望，上面描述的常见陷阱和问题设置列表将有助于在一个面向数据的时代减轻积极传统的影响。</p><h2 id="4dd3" class="md me iq bd mf mg mh dn mi mj mk dp ml lf mm mn mo lj mp mq mr ln ms mt mu mv bi translated">参考</h2><p id="e852" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">[1] <a class="ae kv" href="https://icsd.products.fiz-karlsruhe.de/" rel="noopener ugc nofollow" target="_blank">无机晶体结构数据库</a><br/>【2】A Seko，A Togo，I Tanaka，<a class="ae kv" href="https://link.springer.com/chapter/10.1007/978-981-10-7617-6_1" rel="noopener ugc nofollow" target="_blank">材料数据机器学习的描述符</a>，纳米信息学。新加坡斯普林格。<br/>【3】Saimadhu pola muri，<a class="ae kv" href="https://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/" rel="noopener ugc nofollow" target="_blank">随机森林在机器学习中如何工作</a>。</p></div></div>    
</body>
</html>