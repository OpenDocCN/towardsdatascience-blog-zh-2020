# Python ä¸­çš„æƒ…æ„Ÿåˆ†ç±»

> åŸæ–‡ï¼š<https://towardsdatascience.com/sentiment-classification-in-python-da31833da01b?source=collection_archive---------10----------------------->

## ä½¿ç”¨ VADER å’Œ TextBlob è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œä½¿ç”¨ scikit-learn è¿›è¡Œç›‘ç£æ–‡æœ¬åˆ†ç±»

è¿™ç¯‡æ–‡ç« æ˜¯æ„å»ºæƒ…æ„Ÿåˆ†ç±»å™¨çš„ä¸‰ç¯‡è¿ç»­æ–‡ç« ä¸­çš„æœ€åä¸€ç¯‡ã€‚åšäº†ä¸€äº›[æ¢ç´¢æ€§æ–‡æœ¬åˆ†æ](/exploratory-text-analysis-in-python-8cf42b758d9e)å’Œ[é¢„å¤„ç†æ–‡æœ¬](/preprocessing-text-in-python-923828c4114f)ä¹‹åï¼Œæ˜¯æ—¶å€™å°†è¯„è®ºåˆ†ç±»ä¸ºæƒ…ç»ªäº†ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é¦–å…ˆçœ‹çœ‹ä¸¤ç§ä¸ç”¨å»ºç«‹æ¨¡å‹å°±èƒ½è·å¾—æƒ…æ„Ÿçš„æ–¹æ³•ï¼Œç„¶åå»ºç«‹ä¸€ä¸ªå®šåˆ¶æ¨¡å‹ã€‚

![](img/ac22805965e89381a9356617f4d95a9f.png)

ç…§ç‰‡ç”±[æ··åˆ](https://unsplash.com/@artbyhybrid?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åé€€ä¸€æ­¥ï¼Œå¿«é€Ÿåœ°çœ‹ä¸€ä¸‹æ›´å¤§çš„ç”»é¢ã€‚ [CRISP-DM](https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome) æ–¹æ³•æ¦‚è¿°äº†æˆåŠŸçš„æ•°æ®ç§‘å­¦é¡¹ç›®çš„æµç¨‹ã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†åšä¸€äº›æ•°æ®ç§‘å­¦å®¶åœ¨**å»ºæ¨¡**é˜¶æ®µä¼šç»å†çš„ä»»åŠ¡ã€‚

![](img/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png)

CRISP-DM å·¥è‰ºæµç¨‹æ‘˜å½•

# 0.Python è®¾ç½®

è¿™ç¯‡æ–‡ç« å‡è®¾è¯»è€…(ğŸ‘€æ˜¯çš„ï¼Œä½ ï¼)å¯ä»¥è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœæ‚¨æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨åœ°æ–¹ã€‚

æˆ‘åœ¨ Jupyter ç¬”è®°æœ¬é‡Œæµ‹è¯•è¿‡ Python 3.7.1 çš„è„šæœ¬ã€‚

è®©æˆ‘ä»¬åœ¨å¼€å§‹ä¹‹å‰ç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“:
â—¼ï¸ **æ•°æ®æ“ä½œ/åˆ†æ:** *numpyï¼Œpandas* â—¼ï¸ **æ•°æ®åˆ†åŒº:***sk learn* â—¼ï¸**æ–‡æœ¬é¢„å¤„ç†/åˆ†æ:** *nltkï¼Œtextblob*
â—¼ï¸ **å¯è§†åŒ–:** *matplotlibï¼Œseaborn*

ä¸€æ—¦ä½ å®‰è£…äº† *nltk* ï¼Œè¯·ç¡®ä¿ä½ å·²ç»ä» *nltk* ä¸‹è½½äº†*ã€åœç”¨è¯ã€‘**ã€wordnetã€‘*å’Œ*ã€Vader _ lexiconã€‘*ï¼Œè„šæœ¬å¦‚ä¸‹:

```
import nltk
nltk.download('stopwords') 
nltk.download('wordnet')
nltk.download('vader_lexicon')
```

å¦‚æœä½ å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œè¿™ä¸ªä¼šé€šçŸ¥ä½ ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¯¼å…¥åŒ…äº†:

```
# Set random seed
seed = 123# Data manipulation/analysis
import numpy as np
import pandas as pd# Text preprocessing/analysis
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
from scipy.sparse import hstack, csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler# Modelling
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
sns.set(style="whitegrid", context='talk')
```

# 1.æ•°æ®ğŸ“¦

æˆ‘ä»¬å°†ä½¿ç”¨ IMDB ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æ•°æ®é›†[ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨æ‚¨çš„å·¥ä½œç›®å½•ä¸­ã€‚ä¿å­˜åï¼Œè®©æˆ‘ä»¬å°†å…¶å¯¼å…¥ Python:](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

```
sample = pd.read_csv('IMDB Dataset.csv')
print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")
sample.head()
```

![](img/6b23a671ee90c83029239e074f5dfca9.png)

è®©æˆ‘ä»¬æ¥çœ‹çœ‹æƒ…ç»ªä¹‹é—´çš„åˆ†æ­§:

```
sample['sentiment'].value_counts()
```

![](img/5e5c06fc380f7ac8acfe9701d65dedac.png)

åœ¨æ ·æœ¬æ•°æ®ä¸­ï¼Œæƒ…æ„Ÿæ˜¯å¹³å‡åˆ†é…çš„ã€‚è®©æˆ‘ä»¬å°†ç›®æ ‡ç¼–ç æˆæ•°å­—å€¼ï¼Œå…¶ä¸­æ­£æ•°ä¸º 1ï¼Œè´Ÿæ•°ä¸º 0:

```
# Encode to numeric
sample['target'] = np.where(sample['sentiment']=='positive', 1, 0)# Check values
sample.groupby(['sentiment', 'target']).count().unstack()
```

![](img/986b5b6a0665e784ca4af419c1882fe7.png)

è®©æˆ‘ä»¬ç•™å‡º 5000 ä¸ªæ¡ˆä¾‹è¿›è¡Œæµ‹è¯•:

```
# Split data into train & test
X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, 
                                                    stratify=sample['sentiment'])# Append sentiment back using indices
train = pd.concat([X_train, y_train], axis=1)
test = pd.concat([X_test, y_test], axis=1)# Check dimensions
print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")
print(f"{train['sentiment'].value_counts()}\n")print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")
print(test['sentiment'].value_counts())
```

![](img/bed36a246927452e55efe8cab219564f.png)

æˆ‘ä»¬å°†å¿«é€Ÿæ£€æŸ¥è®­ç»ƒæ•°æ®é›†çš„å¤´éƒ¨:

```
train.head()
```

![](img/82cbcc4362d2f190d2edb80a4266416c.png)

å¥½å§ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼ğŸ³

# 1.æƒ…æ„Ÿåˆ†æğŸ’›

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘æƒ³å‘æ‚¨å±•ç¤ºä¸¤ç§éå¸¸ç®€å•çš„æ–¹æ³•æ¥è·å¾—æƒ…æ„Ÿï¼Œè€Œæ— éœ€æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚æˆ‘ä»¬å°†ç”¨ *VADER* å’Œ*æ–‡æœ¬å—*æå–ææ€§å¼ºåº¦åˆ†æ•°ã€‚

## 1.1.VADER çš„æƒ…æ„Ÿåˆ†æ

> â€œVADER (Valence Aware å­—å…¸å’Œæƒ…æ„Ÿæ¨ç†å™¨)æ˜¯ä¸€ä¸ªåŸºäºè¯å…¸å’Œè§„åˆ™çš„æƒ…æ„Ÿåˆ†æå·¥å…·ï¼Œä¸“é—¨é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­è¡¨è¾¾çš„æƒ…æ„Ÿã€‚â€

è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹ï¼Œçœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ *VADER* æƒ…æ„Ÿåˆ†æå™¨æå–æƒ…æ„Ÿå¼ºåº¦åˆ†æ•°:

```
example = 'The movie was awesome.'
sid = SentimentIntensityAnalyzer()
sid.polarity_scores(example)
```

![](img/639049cd63ae4b021f48f8b4eb2ff2e2.png)

> [***negï¼Œneuï¼Œpos:*** *è¿™ä¸‰ä¸ªåˆ†æ•°ç›¸åŠ ä¸º 1ã€‚è¿™äº›åˆ†æ•°æ˜¾ç¤ºäº†å±äºè¯¥ç±»åˆ«çš„æ–‡æœ¬çš„æ¯”ä¾‹ã€‚* ***å¤åˆ:*** *è¿™ä¸ªåˆ†æ•°èŒƒå›´ä»-1(æœ€è´Ÿ)åˆ° 1(æœ€æ­£ã€‚*](https://github.com/cjhutto/vaderSentiment)

è™½ç„¶ä¸æ˜¯æ‰€æœ‰çš„è¯„è®ºéƒ½åƒæˆ‘ä»¬æ‰‹å¤´çš„ä¾‹å­ä¸€æ ·ç®€å•ï¼Œä½†å¾ˆé«˜å…´çœ‹åˆ°ä¾‹å­è¯„è®ºçš„åˆ†æ•°çœ‹èµ·æ¥å¤§å¤šæ˜¯æ­£é¢çš„ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†å¼ºåº¦åˆ†æ•°æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­:

```
train[['neg', 'neu', 'pos', 'compound']] = train['review'].apply(sid.polarity_scores).apply(pd.Series)
train.head()
```

![](img/fc86ac61eff74116216f9fd52dcbaf25.png)

ä¸€æ—¦æˆ‘ä»¬åˆå§‹åŒ–äº†åˆ†æå™¨å¯¹è±¡ï¼Œè·å¾—æƒ…æ„Ÿåˆ†æ•°åªéœ€è¦ä¸€è¡Œä»£ç ã€‚æˆ‘ä»¬è¦è¿›ä¸€æ­¥æ£€æŸ¥åˆ†æ•°å—ï¼Ÿè®©æˆ‘ä»¬ä»å¾—åˆ†æœ€é«˜çš„ 5 æ¡è®°å½•å¼€å§‹:

```
train.nlargest(5, ['pos'])
```

![](img/3bede19fd7b9b34af4869f795b49e1ad.png)

å¾ˆé«˜å…´çœ‹åˆ°æ‰€æœ‰çš„è¯„è®ºéƒ½æ˜¯æ­£é¢çš„ã€‚è®©æˆ‘ä»¬ä¸º*é˜´æ€§*åšåŒæ ·çš„äº‹æƒ…:

```
train.nlargest(5, ['neg'])
```

![](img/c19ec41e170de76a57dd4603a0cfa1e2.png)

è¿™ä¸ªçœ‹èµ·æ¥ä¹Ÿä¸é”™ã€‚ä½†æˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°æ•°æ®çš„æç«¯ï¼Œé‚£é‡Œçš„æƒ…ç»ªæ›´åŠ æ˜æ˜¾ã€‚è®©æˆ‘ä»¬ç”¨ç›´æ–¹å›¾ç›´è§‚æ˜¾ç¤ºåˆ†æ•°ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£:

```
for var in ['pos', 'neg', 'neu', 'compound']:
    plt.figure(figsize=(12,4))
    sns.distplot(train.query("target==1")[var], bins=30, kde=False, 
                 color='green', label='Positive')
    sns.distplot(train.query("target==0")[var], bins=30, kde=False, 
                 color='red', label='Negative')
    plt.legend()
    plt.title(f'Histogram of {var} by true sentiment');
```

![](img/c8dba99f7cea373ef92d12e2679dd288.png)![](img/ad256d5f1ce725fd3a27d06b610932a8.png)![](img/06fa365b74cba695b41e833d66e626df.png)![](img/765183de3297673e920d423422a3a54a.png)

ä»ç›´æ–¹å›¾æ¥çœ‹ï¼Œä¼¼ä¹ *posã€neg* å’Œæ½œåœ¨çš„ *compound* åˆ—åœ¨å¯¹ç§¯æå’Œæ¶ˆææƒ…ç»ªè¿›è¡Œåˆ†ç±»æ—¶æ˜¯æœ‰ç”¨çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›åˆ†æ•°å¿«é€Ÿåœ°å°†æ¯ä¸ªè¯„è®ºåˆ†ä¸ºæ­£é¢æˆ–è´Ÿé¢ç±»åˆ«ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä¼šåšå¾—å¤šå¥½:

```
train['vader_polarity'] = np.where(train['pos']>train['neg'], 1, 0)
target_names=['negative', 'positive']
print(classification_report(train['target'], 
                            train['vader_polarity'], 
                            target_names=target_names))
```

![](img/c6436164a5dd9042d12020b32d9be151.png)

ä½¿ç”¨ *VADER* ï¼Œæˆ‘ä»¬å¯ä»¥ä¸è´¹å¹ç°ä¹‹åŠ›è·å¾—å¤§çº¦ 69%çš„å‡†ç¡®ç‡ã€‚ä¸è¿‡ï¼Œæ­£é¢å’Œè´Ÿé¢è¯„è®ºçš„è¡¨ç°çœ‹èµ·æ¥æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬å¯¹æ­£é¢è¯„è®ºçš„å¬å›ç‡æ›´é«˜ï¼Œå‡†ç¡®ç‡æ›´ä½â€”â€”è¿™æ„å‘³ç€æˆ‘ä»¬æœ‰æ›´å¤šçš„è¯¯æŠ¥(çœ‹åˆ°æˆ‘åœ¨é‚£é‡Œåšäº†ä»€ä¹ˆå—ï¼Ÿä½ æ˜ç™½æˆ‘ä¸ºä»€ä¹ˆæŠŠæ­£é¢è¯„ä»·ç¼–ç ä¸º 1 äº†å—ï¼ŸğŸ™Š).è®©æˆ‘ä»¬çœ‹çœ‹æ··æ·†çŸ©é˜µ:

```
# Create function so that we could reuse later
def plot_cm(y_test, y_pred, target_names=['negative', 'positive'], 
            figsize=(5,3)):
    """Create a labelled confusion matrix plot."""
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=figsize)
    sns.heatmap(cm, annot=True, fmt='g', cmap='BuGn', cbar=False, 
                ax=ax)
    ax.set_title('Confusion matrix')
    ax.set_xlabel('Predicted')
    ax.set_xticklabels(target_names)
    ax.set_ylabel('Actual')
    ax.set_yticklabels(target_names, 
                       fontdict={'verticalalignment': 'center'});# Plot confusion matrix
plot_cm(train['target'], train['vader_polarity'])
```

![](img/e4ed892a54bca13264cc6905b7da869d.png)

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šçœŸé˜³æ€§å’Œå‡é˜³æ€§ã€‚äº‹å®ä¸Šï¼Œå¤§çº¦ 67%çš„é¢„æµ‹æ˜¯ç§¯æçš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœä½¿ç”¨*å¤åˆ*åˆ†æ•°ï¼Œæ€§èƒ½æ˜¯å¦ä¼šæé«˜ã€‚

```
train['vader_compound'] = np.where(train['compound']>0, 1, 0)
print(classification_report(train['target'], 
                            train['vader_compound'], 
                            target_names=target_names))
```

![](img/30da55ff4c8c4f8e4e87bbb9f113f5e0.png)

```
plot_cm(train['target'], train['vader_compound'])
```

![](img/1c9f128b42011bc1d60a16116ca4f31c.png)

æ€§èƒ½çœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä½¿ç”¨è®­ç»ƒæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œä¸æ˜¯è®­ç»ƒæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå¦‚æœå¯¹æµ‹è¯•æ•°æ®è¿›è¡ŒåŒæ ·çš„æ“ä½œï¼Œç»“æœåº”è¯¥éå¸¸ç›¸ä¼¼ã€‚

ğŸ”—nltk ä¸­å…³äº[*ã€VADERã€‘*](https://github.com/cjhutto/vaderSentiment)*[*VADER çš„æ›´å¤šä¿¡æ¯ã€‚*](https://www.nltk.org/howto/sentiment.html)*

## *1.2.ä½¿ç”¨ TextBlob è¿›è¡Œæƒ…æ„Ÿåˆ†æ*

*å¦ä¸€ç§è·å¾—æƒ…æ„Ÿåˆ†æ•°çš„æ–¹æ³•æ˜¯åˆ©ç”¨ *TextBlob* åº“ã€‚ä½¿ç”¨æ¥è‡ª *TextBlob* å¯¹è±¡çš„æƒ…æ„Ÿå±æ€§ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æå–ç›¸ä¼¼çš„åˆ†æ•°ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¹‹å‰çš„ç¤ºä¾‹è¿›è¡Œæå–:*

```
*TextBlob(example).sentiment*
```

*![](img/f2647eef1449ffc98c15661891a8e11f.png)*

> ****ææ€§:*** *èŒƒå›´ä»-1(æœ€è´Ÿ)åˆ° 1(æœ€æ­£)* ***ä¸»è§‚æ€§:*** *èŒƒå›´ä» 0(éå¸¸å®¢è§‚)åˆ° 1(éå¸¸ä¸»è§‚)**

*æˆ‘ä»¬çš„ä¾‹å­è¢«åˆ†æä¸ºéå¸¸ä¸»è§‚çš„è‚¯å®šé™ˆè¿°ã€‚æ˜¯çœŸçš„ï¼Œä¸æ˜¯å—ï¼Ÿåœ¨è¿™ä¸¤ä¸ªåˆ†æ•°ä¸­ï¼Œ*ææ€§*ä¸æˆ‘ä»¬æ›´ç›¸å…³ã€‚è®©æˆ‘ä»¬å°†å¼ºåº¦åˆ†æ•°æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œå¹¶æ£€æŸ¥å…·æœ‰æœ€é«˜*ææ€§*åˆ†æ•°çš„ 5 ä¸ªè®°å½•:*

```
*train[['polarity', 'subjectivity']] = train['review'].apply(lambda x:TextBlob(x).sentiment).to_list()columns = ['review', 'target', 'polarity', 'subjectivity']
train[columns].nlargest(5, ['polarity'])*
```

*![](img/daeaf74850f11f4e8bb4ac6d604101da.png)*

*å¦‚ä½ æ‰€è§ï¼Œç”¨ *TextBlob* æ·»åŠ æƒ…æ„Ÿå¼ºåº¦åˆ†æ•°ä¹Ÿå¾ˆç®€å•ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ææ€§*åˆ†æ•°æœ€ä½çš„ 5 æ¡è®°å½•:**

```
*train[columns].nsmallest(5, ['polarity'])*
```

*![](img/c483f87a7f591be357643395ef7a72a0.png)*

*æ˜¯æ—¶å€™ç»˜åˆ¶ä¸€äº›ç›´æ–¹å›¾æ¥æ›´å¥½åœ°ç†è§£åˆ†æ•°äº†:*

```
*for var in ['polarity', 'subjectivity']:
    plt.figure(figsize=(12,4))
    sns.distplot(train.query("target==1")[var], bins=30, kde=False, 
                 color='green', label='Positive')
    sns.distplot(train.query("target==0")[var], bins=30, kde=False, 
                 color='red', label='Negative')
    plt.legend()
    plt.title(f'Histogram of {var} by true sentiment');*
```

*![](img/3cdf061d1a6cb0203edd7fe6d1e0907a.png)**![](img/1d48d1bdcbc3be13f39c0c7ef2778243.png)*

*æ­£å¦‚æ‰€æ–™ï¼Œ*ææ€§*åˆ†æ•°çœ‹èµ·æ¥å¯èƒ½æœ‰åŠ©äºå¯¹ç§¯ææƒ…ç»ªå’Œæ¶ˆææƒ…ç»ªè¿›è¡Œåˆ†ç±»ã€‚è®©æˆ‘ä»¬ä½¿ç”¨*ææ€§*åˆ†æ•°è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æŸ¥çœ‹æ€§èƒ½:*

```
*train['blob_polarity'] = np.where(train['polarity']>0, 1, 0)
print(classification_report(train['target'], 
                            train['blob_polarity'], 
                            target_names=target_names))*
```

*![](img/41710a95225dd457dfccaef9a6cd1685.png)*

*ä½¿ç”¨ *TextBlob* ï¼Œæˆ‘ä»¬å¯ä»¥ä¸è´¹å¹ç°ä¹‹åŠ›è·å¾—å¤§çº¦ 69%çš„å‡†ç¡®ç‡ã€‚åŒæ ·ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šå‡é˜³æ€§ï¼Œäº‹å®ä¸Šï¼Œç”šè‡³æ¯”ä»¥å‰æ›´å¤šã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ··æ·†çŸ©é˜µ:*

```
*plot_cm(train['target'], train['blob_polarity'])*
```

*![](img/3be0a2d32ac061dd2c97f8bd207e6132.png)*

*è¿™ä¸€æ¬¡ï¼Œå‡é˜³æ€§çš„æ•°é‡é«˜äºçœŸé˜´æ€§çš„æ•°é‡ã€‚é¢„æµ‹åå‘ç§¯ææƒ…ç»ªï¼Œå› ä¸º 76%çš„é¢„æµ‹æ˜¯ç§¯æçš„ã€‚*

*ğŸ”—å…³äº [*TextBlob* çš„æ›´å¤šä¿¡æ¯ã€‚](https://textblob.readthedocs.io/en/dev/)*

## *1.3.ä¸¤è€…ä¹‹é—´çš„å…³ç³»*

*è®©æˆ‘ä»¬æ¥æ¯”è¾ƒä¸€ä¸‹ *VADER* å’Œ *TextBlob* çš„åˆ†æ•°æœ‰å¤šç›¸ä¼¼:*

```
*pd.crosstab(train['vader_polarity'], train['blob_polarity'])*
```

*![](img/1b2682057bd48af7d1f65f73283e69af.png)*

*ä»–ä»¬çš„åˆ†ç±»ä¸­æœ‰å¤§çº¦ 79%çš„é‡å ï¼Œå¤§å¤šæ•°æ˜¯æ­£é¢æƒ…ç»ªã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ææ€§å¾—åˆ†:*

```
*plt.figure(figsize=(12,12))
sns.scatterplot(data=train, x='polarity', y='compound',
                hue='target', palette=['red', 'green'], 
                alpha=.3)
plt.axhline(0, linestyle='--', color='k')
plt.axvline(0, linestyle='--', color='k')
plt.title('Scatterplot between polarity intensity scores');*
```

*![](img/0aa19bf51624efda89c23521a1f9836a.png)*

*è¿™å¼ å›¾æ˜¾ç¤ºçš„ä¿¡æ¯æ¯”å‰ä¸€å¼ è¡¨å¤šä¸€ç‚¹ã€‚åœ¨å·¦ä¸‹è±¡é™ï¼Œæˆ‘ä»¬ä¸»è¦çœ‹åˆ°çº¢è‰²åœ†åœˆï¼Œå› ä¸ºä¸¤ç§æ–¹æ³•ä¸­çš„è´Ÿé¢åˆ†ç±»éƒ½æ›´ç²¾ç¡®ã€‚åœ¨å³ä¸Šè±¡é™ï¼Œæœ‰å¤§é‡çš„åœ†åœˆï¼Œå¤§éƒ¨åˆ†æ˜¯ç»¿è‰²çš„ï¼Œä½†é¢œè‰²æ··åˆä¸åƒä»¥å‰é‚£ä¹ˆçº¯ã€‚å‰©ä¸‹çš„ä¸¤ä¸ªè±¡é™æ˜¾ç¤ºäº†ä¸¤ä¸ªåˆ†æ•°ä¸ä¸€è‡´çš„åœ°æ–¹ã€‚æ€»çš„æ¥è¯´ï¼Œåœ¨å›¾çš„å³åŠéƒ¨åˆ†ï¼Œé¢œè‰²æ¯”å·¦åŠéƒ¨åˆ†æ›´æ··åˆã€‚*

*æˆ‘ä»¬ä»ä¸¤è€…ä¸­å¾—åˆ°éå¸¸ç›¸ä¼¼çš„ 69%çš„æ€»ä½“å‡†ç¡®åº¦ï¼›ç„¶è€Œï¼Œå½“æˆ‘ä»¬ä»”ç»†è§‚å¯Ÿé¢„æµ‹æ—¶ï¼Œè¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½æ˜¯ä¸åŒçš„ã€‚*

*ç°åœ¨ä½ çŸ¥é“å¦‚ä½•ç”¨ *VADER* æˆ–*æ–‡æœ¬å—*è·å¾—æƒ…æ„Ÿææ€§åˆ†æ•°ã€‚å¦‚æœæ‚¨æœ‰æœªæ ‡è®°çš„æ•°æ®ï¼Œè¿™ä¸¤ä¸ªå·¥å…·ä¸ºè‡ªåŠ¨æ ‡è®°æ‚¨çš„æ•°æ®æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚æ˜¯æ—¶å€™å»ºç«‹æ¨¡å‹äº†ï¼âœ¨*

# *2.æ¨¡æ‹Ÿâ“œï¸*

*åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†:*

1.  *é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ–¹æ³•å’Œç®—æ³•*
2.  *æ¢ç´¢æ·»åŠ  *VADER* å’Œ*æ–‡æœ¬æ–‘ç‚¹*æƒ…æ„Ÿåˆ†æ•°ä½œä¸ºç‰¹å¾æ˜¯å¦ä¼šæé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›*
3.  *æ„å»ºç®¡é“å¹¶è°ƒæ•´å…¶è¶…å‚æ•°*
4.  *åœ¨çœ‹ä¸è§çš„æ•°æ®ä¸Šæµ‹è¯•æœ€ç»ˆç®¡é“*

*æƒ…æ„Ÿåˆ†ç±»æ˜¯ç›‘ç£åˆ†ç±»æ¨¡å‹çš„ä¸€ä¸ªåº”ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé‡‡ç”¨çš„æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°ä»»ä½•ç›‘ç£åˆ†ç±»ä»»åŠ¡ã€‚*

## *2.1.é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ–¹æ³•å’Œç®—æ³•*

*åœ¨[æˆ‘ä¹‹å‰çš„å¸–å­](/preprocessing-text-in-python-923828c4114f)ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§ä¸åŒçš„æ–‡æœ¬é¢„å¤„ç†æ–¹æ³•ï¼Œå¹¶åˆ—å‡ºå…¶ä¸­ä¸¤ç§:*ç®€å•æ–¹æ³•*å’Œ*ç®€å•æ–¹æ³•*ã€‚åœ¨è¿™ä¸¤ä¸ªé€‰é¡¹ä¸­ï¼Œæˆ‘ä»¬ç°åœ¨å°†æµ‹è¯•è¿™ä¸¤ä¸ªé€‰é¡¹ä¹‹é—´çš„æ¨¡å‹æ€§èƒ½æ˜¯å¦æœ‰ä»»ä½•å·®å¼‚ï¼Œå¹¶é€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ¥ä½¿ç”¨å‘å‰ç§»åŠ¨ã€‚ä¸ºäº†ä½¿äº‹æƒ…å˜å¾—ç®€å•ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸¤ä¸ªå‡½æ•°(è¿™äº›å‡½æ•°çš„æƒ³æ³•æ˜¯ä»[è¿™é‡Œ](https://www.kaggle.com/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86/comments)å¾—åˆ°çš„å¯å‘):*

```
*# Define functions
def create_baseline_models():
    """Create list of baseline models."""
    models = []
    models.append(('log', LogisticRegression(random_state=seed, 
                                             max_iter=1000)))
    models.append(('sgd', SGDClassifier(random_state=seed)))
    models.append(('mnb', MultinomialNB()))
    return modelsdef assess(X, y, models, cv=5, scoring=['roc_auc', 
                                        'accuracy', 
                                        'f1']):
    """Provide summary of cross validation results for models."""
    results = pd.DataFrame()
    for name, model in models:
        result = pd.DataFrame(cross_validate(model, X, y, cv=cv, 
                                             scoring=scoring))
        mean = result.mean().rename('{}_mean'.format)
        std = result.std().rename('{}_std'.format)
        results[name] = pd.concat([mean, std], axis=0)
    return results.sort_index()*
```

*æˆ‘æŒ‘äº†ä¸‰ä¸ªç®—æ³•æ¥è¯•: [*é€»è¾‘å›å½’åˆ†ç±»å™¨*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) ã€ [*éšæœºæ¢¯åº¦ä¸‹é™åˆ†ç±»å™¨*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd#sklearn.linear_model.SGDClassifier) å’Œ [*å¤šé¡¹æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨*](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB) ã€‚è®©æˆ‘ä»¬å¯åŠ¨æ¨¡å‹:*

```
*models = create_baseline_models()
models*
```

*![](img/c7c50e0630f51a84019c1d0a7b51c0b9.png)*

*ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä½¿ç”¨*æ›´ç®€å•æ–¹æ³•*æ—¶çš„æ¨¡å‹æ€§èƒ½:*

```
*# Preprocess the data
vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', 
                             stop_words='english', 
                             min_df=30, 
                             max_df=.7)
X_train_simpler = vectoriser.fit_transform(X_train)# Assess the model
assess(X_train_simpler, y_train, models)*
```

*![](img/16ffbde7506d1a91ffd3d9aac9324c61.png)*

*å¾ˆé«˜å…´çœ‹åˆ°æˆ‘ä»¬è·å¾—äº†æ›´å¥½çš„æ€§èƒ½:ä¸ä»…ä½¿ç”¨æƒ…æ„Ÿåˆ†æ•°ç›¸æ¯”ï¼ŒåŸºçº¿æ¨¡å‹çš„å‡†ç¡®ç‡ä¸º 86â€“89%ã€‚ç”±äºè¿™äº›èŒä¸šç›¸å½“å¹³è¡¡ï¼Œæˆ‘ä»¬å°†ä¸»è¦å…³æ³¨å‡†ç¡®æ€§ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å°†ç¡®ä¿ç¨åæ›´ä»”ç»†åœ°æ£€æŸ¥é¢„æµ‹ï¼Œä»¥è¯„ä¼°æ¨¡å‹ã€‚æ€§èƒ½æŒ‡æ ‡åœ¨*é€»è¾‘å›å½’*å’Œ*éšæœºæ¢¯åº¦ä¸‹é™*ä¹‹é—´çœ‹èµ·æ¥éå¸¸æ¥è¿‘ï¼Œåè€…åœ¨è®­ç»ƒä¸­æ›´å¿«(å‚è§ *fit_time* )ã€‚æœ´ç´ è´å¶æ–¯æ˜¯ä¸‰ä¸ªäººä¸­è®­ç»ƒé€Ÿåº¦æœ€å¿«çš„ï¼Œä½†æ˜¯è¡¨ç°æ¯”å…¶ä»–ä¸¤ä¸ªäººç¨å·®ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥è¯„ä¼°ä¸€ä¸‹*ç®€å•æ–¹æ³•*:*

```
*# Define function
def preprocess_text(text):
    # 1\. Tokenise to alphabetic tokens
    tokeniser = RegexpTokenizer(r'[A-Za-z]+')
    tokens = tokeniser.tokenize(text)

    # 2\. Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') 
              for t in tokens]
    return tokens# Preprocess the data
vectoriser = TfidfVectorizer(analyzer=preprocess_text, 
                             min_df=30, 
                             max_df=.7)
X_train_simple = vectoriser.fit_transform(X_train)# Assess models
assess(X_train_simple, y_train, models)*
```

*![](img/2c5b287fef37fe8a72b13de655718986.png)*

*æ€§èƒ½çœ‹èµ·æ¥å’Œä»¥å‰å·®ä¸å¤šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å€¾å‘äºä½¿ç”¨*æ›´ç®€å•çš„æ–¹æ³•*,å¹¶ç»§ç»­ä½¿ç”¨å®ƒã€‚åœ¨è¿™ä¸‰ç§ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å°†é€‰æ‹©*éšæœºæ¢¯åº¦ä¸‹é™*ï¼Œå› ä¸ºå®ƒæœ€èƒ½å¹³è¡¡é€Ÿåº¦å’Œé¢„æµ‹èƒ½åŠ›ã€‚*

## *2.2.è¯„ä¼°é™„åŠ åŠŸèƒ½*

*åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨æ·»åŠ  *VADER* å’Œ*æ–‡æœ¬æ–‘ç‚¹*æƒ…æ„Ÿåˆ†æ•°ä½œä¸ºç‰¹å¾æ˜¯å¦ä¼šæé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚è®©æˆ‘ä»¬å¿«é€Ÿæ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰ä»»ä½•é«˜åº¦ç›¸å…³çš„ç‰¹æ€§:*

```
*plt.figure(figsize = (14,5))
columns = ['target', 'neg', 'neu', 'pos', 'compound', 'polarity', 
           'subjectivity']
sns.heatmap(train[columns].corr(), annot=True, cmap='seismic_r');*
```

*![](img/fbd4350aeedae0035ac78e299b1390fb.png)*

*æœ€ç›¸å…³çš„ç‰¹å¾æ˜¯*å¤åˆ*å’Œ*è´Ÿ*ã€‚è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ªå¿«é€Ÿæ¨¡å‹ï¼Œçœ‹çœ‹å“ªäº›åˆ†æ•°æ›´æœ‰ç”¨:*

```
*# Initialise a model
sgd = SGDClassifier(random_state=seed)# Initialise a scaler
scaler = MinMaxScaler()# Assess the model using scores
scores = train[['neg', 'neu', 'pos', 'compound', 'polarity', 
                'subjectivity']]
assess(scaler.fit_transform(scores), y_train, [('sgd', sgd)])*
```

*![](img/876f0027833428e71bc12a099d2403b3.png)*

*æˆ‘ä»¬ä½¿ç”¨åˆ†æ•°å¾—åˆ°å¤§çº¦ 77%çš„å‡†ç¡®ç‡ã€‚ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥ç³»æ•°:*

```
*# Fit to training data
sgd.fit(scores, y_train)# Get coefficients
coefs = pd.DataFrame(data=sgd.coef_, columns=scores.columns).T
coefs.rename(columns={0: 'coef'}, inplace=True)# Plot
plt.figure(figsize=(10,5))
sns.barplot(x=coefs.index, y='coef', data=coefs)
plt.title('Coefficients');*
```

*![](img/a9ad1551203a63acf1f66b43898fe0e1.png)*

*ä¼¼ä¹æˆ‘ä»¬åªèƒ½ä½¿ç”¨*é˜´æ€§ã€é˜³æ€§*å’Œ*ææ€§*ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å¾—åˆ†*ä¸­æœ€ä¸»è¦çš„ç‰¹å¾ã€‚*è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡å°†è¿™äº›é€‰æ‹©çš„åˆ†æ•°æ·»åŠ åˆ°å…ˆå‰é¢„å¤„ç†çš„æ•°æ®æ¥æ”¹è¿›æ¨¡å‹ç»“æœã€‚*

```
*# Add features to sparse matrix
selected_scores = train[['neg', 'pos', 'polarity']]
X_train_extended = hstack([X_train_simpler, csr_matrix(scaler.fit_transform(selected_scores))])# Assess
assess(X_train_extended, y_train, [('sgd', sgd)])*
```

*![](img/d03ad39844fbbb002f2b3b9d0b0e7374.png)*

*ç”±äºæ·»åŠ è¿™äº›åˆ†æ•°å¹¶æ²¡æœ‰æ”¹è¿›æ¨¡å‹ï¼Œå› æ­¤æ²¡æœ‰å¿…è¦å°†å®ƒä»¬ä½œä¸ºç‰¹å¾æ·»åŠ ã€‚è¿™ä¹Ÿå°†ä½¿æˆ‘ä»¬çš„æ¸ é“ä¿æŒç®€å•ï¼*

## *2.3.æ„å»ºç®¡é“å¹¶è°ƒæ•´å…¶è¶…å‚æ•°*

*æ˜¯æ—¶å€™æ„å»ºä¸€ä¸ªå°ç®¡é“ï¼Œå°†é¢„å¤„ç†å™¨å’Œæ¨¡å‹æ”¾åœ¨ä¸€èµ·äº†ã€‚æˆ‘ä»¬å°†å¾®è°ƒå®ƒçš„è¶…å‚æ•°ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½æ”¹è¿›è¿™ä¸ªæ¨¡å‹ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°è¯•ç†è§£ä¸‰ä¸ªè¶…å‚æ•°çš„å½±å“:å¯¹äºå‘é‡æœºçš„`min_df`ã€`max_df`å’Œå¯¹äºéšæœºæœç´¢æ¨¡å‹çš„`loss`:*

```
*# Create a pipeline
pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+')),
                 ('model', SGDClassifier(random_state=seed))])# Prepare a random search
param_distributions = {'vectoriser__min_df': np.arange(10, 1000, 10),
                       'vectoriser__max_df': np.linspace(.2, 1, 40),
                       'model__loss': ['log', 'hinge']}
r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, 
                              n_iter=30, cv=5, n_jobs=-1, random_state=seed)
r_search.fit(X_train, y_train)# Save results to a dataframe
r_search_results = pd.DataFrame(r_search.cv_results_).sort_values(by='rank_test_score')*
```

*è¿™é‡Œï¼Œæˆ‘ä»¬æ­£åœ¨å°è¯• 30 ç§ä¸åŒçš„è¶…å‚æ•°ç©ºé—´æŒ‡å®šçš„éšæœºç»„åˆã€‚è¿™éœ€è¦ä¸€æ®µæ—¶é—´æ¥è¿è¡Œã€‚éšæœºæœç´¢çš„è¾“å‡ºå°†ä¿å­˜åœ¨åä¸º`r_search_results`çš„æ•°æ®å¸§ä¸­ã€‚è®©æˆ‘ä»¬åˆ›å»ºå¦ä¸€ä¸ªæ•°æ®æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›æˆ‘ä»¬æ›´æ„Ÿå…´è¶£çš„åˆ—:*

```
*columns = [col for col in r_search_results.columns 
           if re.search(r"split|param_", col)]
r_summary = r_search_results[columns].copy()
r_summary.columns = [re.sub(r'_test_score|param_', '', col) 
                     for col in r_summary.columns]
columns = [col.split('__')[1] if '__' in col else col 
           for col in r_summary.columns ]
r_summary.columns = columns
r_summary.head()*
```

*![](img/8acd6efc3bbd8f64736433483f85e398.png)*

*è®©æˆ‘ä»¬å°†è¾“å‡ºå¯è§†åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£è¶…å‚æ•°çš„å½±å“:*

```
*# Create a long dataframe
r_summary_long = pd.melt(r_summary, 
                         id_vars=['min_df', 
                                  'max_df', 
                                  'loss'], 
                         value_vars=['split0', 
                                     'split1', 
                                     'split2', 
                                     'split3', 
                                     'split4'])# Plot hyperparameter 'loss'
plt.figure(figsize=(8,4))
plt.title('Performance by loss')
sns.boxplot(x='value', y='loss', data=r_summary_long, 
            orient='h')
plt.xlim(.8, .9);*
```

*![](img/83ae826acc46dc66465c1b4f8226b6f8.png)*

*çœ‹èµ·æ¥`loss='hinge'`ä¼šå¸¦æ¥ç¨å¾®å¥½ä¸€ç‚¹çš„æ€§èƒ½ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ•°å­—è¶…å‚æ•°:*

```
*for param in ['min_df', 'max_df']:
    plt.figure(figsize=(8,4))
    sns.scatterplot(x=param, y="value", data=r_summary_long, 
                    x_jitter=True, alpha=0.5)
    plt.ylim(.8, .9);*
```

*![](img/151a67356e7dda36782f71291e607603.png)*

*ç”±äº`min_df`å’Œå‡†ç¡®åº¦ä¹‹é—´ä¼¼ä¹å­˜åœ¨è´Ÿç›¸å…³å…³ç³»ï¼Œæˆ‘ä»¬å°†æŠŠ`min_df`ä¿æŒåœ¨ 200 ä»¥ä¸‹ã€‚`max_df`æ²¡æœ‰æ˜æ˜¾çš„è¶‹åŠ¿ï¼Œå¯èƒ½æ˜¯å› ä¸ºä¸šç»©å—`min_df`å’Œ`loss`çš„å½±å“æ›´å¤§ã€‚è™½ç„¶ä»–ä»¬ä¸‰ä¸ªéƒ½æ˜¯è¿™æ ·ï¼Œä½†æ˜¯å¯¹äº`max_df`æ¥è¯´æ›´æ˜æ˜¾ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹è¿™äº›è¶…å‚æ•°å¦‚ä½•å½±å“æ¨¡å‹æœ‰äº†ä¸€äº›äº†è§£ï¼Œè®©æˆ‘ä»¬æ›´ç²¾ç¡®åœ°å®šä¹‰ç®¡é“(`max_df=.6`å’Œ`loss=â€™hinge'`)å¹¶å°è¯•ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›ä¸€æ­¥è°ƒæ•´å®ƒ:*

```
*# Create a pipeline
pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.6)),
                 ('model', SGDClassifier(random_state=seed, loss='hinge'))])# Prepare a grid search
param_grid = {'vectoriser__min_df': [30, 90, 150],
              'vectoriser__ngram_range': [(1,1), (1,2)],
              'vectoriser__stop_words': [None, 'english'],
              'model__fit_intercept': [True, False]}
g_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5, n_jobs=-1)
g_search.fit(X_train, y_train)# Save results to a dataframe
g_search_results = pd.DataFrame(g_search.cv_results_).sort_values(by='rank_test_score')*
```

*ç½‘æ ¼æœç´¢ä¹Ÿéœ€è¦ä¸€äº›æ—¶é—´ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ 24 ç§ä¸åŒçš„è¶…å‚æ•°ç»„åˆè¦å°è¯•ã€‚åƒä»¥å‰ä¸€æ ·ï¼Œè¾“å‡ºå°†ä¿å­˜åˆ°åä¸º`g_search_results`çš„æ•°æ®å¸§ä¸­ã€‚è®©æˆ‘ä»¬å°†æ›´å¤šç›¸å…³åˆ—æå–åˆ°å¦ä¸€ä¸ªæ•°æ®æ¡†æ¶ä¸­:*

```
*columns = [col for col in g_search_results.columns 
           if re.search(r"split|param_", col)]
g_summary = g_search_results[columns+['mean_test_score']].copy()
g_summary.columns = [re.sub(r'_test_score|param_', '', col) 
                     for col in g_summary.columns]
columns = [col.split('__')[1] if '__' in col else col 
           for col in g_summary.columns ]
g_summary.columns = columns
g_summary.head()*
```

*![](img/149961dc1f3c89edb3027ca7755e9f37.png)*

*ä½¿ç”¨è¿™äº›ç»„åˆä¸­çš„ä»»ä½•ä¸€ç§ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è¾¾åˆ°çº¦ 0.9 çš„äº¤å‰éªŒè¯ç²¾åº¦ã€‚å¾ˆé«˜å…´çœ‹åˆ°è¾¹é™…å¢é•¿ã€‚*

```
*# Create a long dataframe
g_summary_long = pd.melt(g_summary, 
                         id_vars=['min_df', 
                                  'ngram_range', 
                                  'stop_words', 
                                  'fit_intercept'], 
                         value_vars=['split0', 
                                     'split1', 
                                     'split2', 
                                     'split3', 
                                     'split4'])
g_summary_long.replace({None: 'None'}, inplace=True)# Plot performance
for param in ['ngram_range', 'stop_words', 'fit_intercept']:
    plt.figure(figsize=(8,4))
    plt.title(f'Performance by {param}')
    sns.boxplot(x='value', y=param, data=g_summary_long, orient='h')
    plt.xlim(.85, .95);*
```

*![](img/e796b678dfbea21adf4bd9575e19dbbe.png)*

*æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¢æˆ`ngram_range=(1,2)`ï¼Œmodel è¡¨ç°æ›´å¥½ã€‚`stop_words=None`ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æ˜¯å¦æ‹Ÿåˆæˆªè·å¹¶æ²¡æœ‰å¤ªå¤§çš„å½±å“ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªè¶…å‚æ•°ä¿ç•™ä¸ºé»˜è®¤å€¼ã€‚æˆ‘è®¤ä¸ºè¿™å·²ç»è¶³å¤Ÿå¥½äº†ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å®šä¹‰æœ€ç»ˆçš„ç®¡é“äº†ã€‚*

## *2.4.åœ¨çœ‹ä¸è§çš„æ•°æ®ä¸Šæµ‹è¯•æœ€ç»ˆç®¡é“*

*ä½¿ç”¨ç½‘æ ¼æœç´¢ä¸­çš„é¡¶éƒ¨ç»„åˆï¼Œè¿™æ˜¯æˆ‘ä»¬æœ€ç»ˆç®¡é“çš„æ ·å­:*

```
*pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', min_df=30, max_df=.6, ngram_range=(1,2))),
                 ('model', SGDClassifier(random_state=seed, loss='hinge'))])pipe.fit(X_train, y_train)*
```

*![](img/3e382ebce812a60d3de7f7ab3602d275.png)*

*æˆ‘ä»¬çš„ç®¡é“å¾ˆå°å¾ˆç®€å•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„ç³»æ•°:*

```
*coefs = pd.DataFrame(pipe['model'].coef_, 
                     columns=pipe['vectoriser'].get_feature_names())
coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')
coefs*
```

*![](img/2434aae0d16dfe2732867e3bcbf40583.png)*

*å…·æœ‰æœ€é«˜æˆ–æœ€ä½ç³»æ•°çš„ç‰¹å¾çœ‹èµ·æ¥å¾ˆç›´è§‚ã€‚ä½†æ˜¯çœ‹çœ‹æˆ‘ä»¬æ‹¥æœ‰çš„ç‰¹æ€§æ•°é‡:49ï¼Œ577ï¼è¿™ä¸»è¦æ˜¯å› ä¸ºæ”¾æ¾äº†`min_df`ï¼Œå¢åŠ äº†äºŒå…ƒæ¨¡å‹ï¼Œæ²¡æœ‰åˆ é™¤åœç”¨è¯ã€‚å¦‚æœæˆ‘ä»¬çƒ­è¡·äºå‡å°‘ç‰¹å¾çš„æ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜ç®¡é“ä¸­çš„è¿™äº›è¶…å‚æ•°ã€‚å¦‚æœæˆ‘ä»¬å¼€å§‹å‡å°‘ç‰¹å¾ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°ç‰¹å¾æ•°é‡å’Œæ¨¡å‹ç²¾åº¦ä¹‹é—´çš„æƒè¡¡ã€‚æœ€ä½³å¹³è¡¡æ˜¯ä»€ä¹ˆæ ·çš„å–å†³äºå…·ä½“æƒ…å†µã€‚è®©æˆ‘ä»¬æ¥è¯„ä¼°ç®¡é“:*

```
*train_pred = pipe.predict(X_train)
print(classification_report(train_pred, 
                            y_train, 
                            target_names=target_names))*
```

*![](img/99fe7fd7a1bf19a45c2ffbbbd1ba4518.png)*

```
*test_pred = pipe.predict(X_test)
print(classification_report(test_pred, 
                            y_test, 
                            target_names=target_names))*
```

*![](img/58f324f37df2290d1c30f81f3033cb94.png)*

*åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«çº¦ä¸º 0.94 å’Œ 0.92ã€‚è¿™ä¸¤ç§è§‚ç‚¹çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡çœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬æœ‰ç¨å¾®å¤šä¸€ç‚¹çš„å‡é˜´æ€§ã€‚è®©æˆ‘ä»¬ç»˜åˆ¶æ··æ·†çŸ©é˜µ:*

```
*plot_cm(test_pred, y_test, target_names=target_names)*
```

*![](img/8ff0c63a8faa0796066ea8781de9ab35.png)*

*çœ‹èµ·æ¥ä¸é”™ã€‚è¿™ä¹ˆğŸŠç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå°†å¤§çº¦ 90%çš„è¯„è®ºå½’ç±»ä¸ºæ­£ç¡®è§‚ç‚¹çš„ç®¡é“ã€‚è®©æˆ‘ä»¬çœ‹çœ‹åšä¸€æ¬¡é¢„æµ‹éœ€è¦å¤šé•¿æ—¶é—´ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Jupyter ç¬”è®°æœ¬çš„é­”æ³•å‘½ä»¤`%timeit`:*

```
*for i in range(10):
    lead = X_test.sample(1)
    %timeit pipe.predict(lead)*
```

*![](img/9941c7acdf6954a9c868040226d8c62f.png)*

*å°½ç®¡`%timeit`è¿è¡Œäº†å¤šä¸ªå¾ªç¯ï¼Œå¹¶ç»™å‡ºäº†è¿è¡Œæ—¶é—´çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œä½†æˆ‘æ³¨æ„åˆ°æˆ‘æ¯æ¬¡å¾—åˆ°çš„è¾“å‡ºéƒ½ç•¥æœ‰ä¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹`%timeit`çš„ 10 ä¸ªå¾ªç¯ä»¥è§‚å¯ŸèŒƒå›´ã€‚*

*å•æ¬¡é¢„æµ‹å¤§çº¦éœ€è¦ 1.5 åˆ° 4 æ¯«ç§’ã€‚è¿™éœ€è¦åœ¨ç”¨ä¾‹çš„ç”Ÿäº§ç¯å¢ƒçš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œè¯„ä¼°ã€‚*

*å¥½äº†ï¼Œè¿™å°±æ˜¯è¿™ç¯‡æ–‡ç« çš„å†…å®¹ã€‚ğŸ’«*

*![](img/c71d93a750d94bf7ee21bbafa3142f9c.png)*

*ç…§ç‰‡ç”±[ä¼¯çˆµå…‹é‡Œæ–¯](https://unsplash.com/@countchris?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„*

**æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership) ï¼Œ*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚**

*è°¢è°¢ä½ çœ‹æˆ‘çš„å¸–å­ã€‚å¸Œæœ›æ‚¨å·²ç»å­¦ä¼šäº†ä¸€äº›ä¸åŒçš„å®ç”¨æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ„å»ºæˆ–ä¸æ„å»ºå®šåˆ¶æ¨¡å‹çš„æƒ…å†µä¸‹å°†æ–‡æœ¬åˆ†ç±»ä¸ºæƒ…æ„Ÿã€‚ä»¥ä¸‹æ˜¯æœ¬ç³»åˆ—å…¶ä»–ä¸¤ç¯‡æ–‡ç« çš„é“¾æ¥:â—¼ï¸[python ä¸­çš„æ¢ç´¢æ€§æ–‡æœ¬åˆ†æ](/exploratory-text-analysis-in-python-8cf42b758d9e)
â—¼ï¸[python ä¸­çš„æ–‡æœ¬é¢„å¤„ç†](/preprocessing-text-in-python-923828c4114f)*

*ä»¥ä¸‹æ˜¯æˆ‘çš„å…¶ä»– NLP ç›¸å…³å¸–å­çš„é“¾æ¥:
â—¼ï¸[Python ä¸­çš„ç®€å• word cloud](/simple-wordcloud-in-python-2ae54a9f58e5)
*(ä¸‹é¢åˆ—å‡ºäº†ä¸€ç³»åˆ—å…³äº NLP ä»‹ç»çš„å¸–å­)*
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„é¢„å¤„ç†æ–‡æœ¬](/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:è¯æ¡æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)*

*å†è§ğŸƒğŸ’¨*