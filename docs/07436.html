<html>
<head>
<title>Dismantling Neural Networks to Understand the Inner Workings with Math and Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Math和Pytorch分解神经网络以理解其内部运作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595?source=collection_archive---------21-----------------------#2020-06-05">https://towardsdatascience.com/dismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595?source=collection_archive---------21-----------------------#2020-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="94f8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">简化的数学与例子和代码，以揭示黑盒内的光</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7635e14e8d83640ff7c86fc0f096e711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2T-7IqAQ9XDVRFt0ViFCA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗洛里安·克劳尔在<a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="03db" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">动机</h1><p id="000e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">作为一个孩子，你可能会在狂热的好奇心驱使下拆开一个玩具。你可能被它发出的声音的来源吸引了。或者可能是二极管发出的诱人的彩色光召唤你，让你的手去打开塑料。</p><p id="28b0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有时候，你可能会觉得被欺骗了，因为它的内部与外表的光鲜亮丽让你想象的相去甚远。我希望你足够幸运，打开了正确的玩具。那些充满了足够的错综复杂，值得打开它们。也许你发现了一辆未来外观的DC汽车。或者可能是一个奇怪的扬声器，背面有一个强磁铁，你在冰箱上试过。我敢肯定，当你发现是什么让你的控制器振动的时候，感觉就很好。</p><p id="c1db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们也要做同样的事情。我们正在用数学和Pytorch拆除一个神经网络。这将是值得的，我们的玩具甚至不会打破。也许你会感到气馁。这可以理解。在神经网络中有如此多不同而复杂的部分。这是压倒性的。这是通往更明智状态的仪式。</p><p id="4ac9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以为了帮助我们自己，我们需要一个参照物，某种北极星来确保我们在正确的方向上。Pytorch的预建功能将是我们的北极星。他们会告诉我们必须得到的产量。我们有责任找到将我们引向正确输出的逻辑。如果差异听起来像你曾经熟悉的被遗忘的陌生人，不要烦恼！我们会再做一次介绍，这将是非常愉快的。<br/>我希望你会喜欢。</p><h1 id="8983" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">线性</h1><p id="7cb3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">神经元的价值取决于其输入、权重和偏差。为了计算一层中所有神经元的这个值，我们计算输入矩阵与权重矩阵的点积，并添加偏置向量。当我们写道:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/322284249d7dd8d9f3d8173d13d0580f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*ursVMYF9wj84_xZhGcCEyA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一层中所有神经元的值。</p></figure><p id="557d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，数学方程的简洁是通过对内部工作的抽象来实现的。我们为简洁所付出的代价是让理解和在头脑中想象所涉及的步骤变得更加困难。为了能够编码和调试像神经网络这样复杂的结构，我们既需要深刻的理解，也需要清晰的思维可视化。为此，我们倾向于冗长:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/d99190424997d118d1120cf876d7ed25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82UPcO44NSA8s1MzFXLmAg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有三个输入、三个权重和一个偏差的一个神经元的值。</p></figure><p id="ea97" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，这个等式的基础是一个特定案例所施加的约束:一个神经元、三个输入、三个权重和一个偏差。我们已经从抽象转向了更具体、更容易实现的东西:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="79d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了计算<strong class="lq ir"> <em class="na"> z </em> </strong>，我们已经从一层输入前进到下一层神经元。当神经网络一路向前通过其层并获得知识时，它需要知道如何向后调整其先前的层。我们可以通过衍生品实现这种知识的反向传播。简单地说，如果我们对<strong class="lq ir"> <em class="na"> z </em> </strong> <em class="na"> </em>的每个参数(权重和偏差)进行微分，我们可以得到输入层<strong class="lq ir"> <em class="na"> x </em> </strong>的值。</p><p id="eab8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你已经忘记了如何求导，请放心:你不会被告知去温习微积分的整个分支。我们将在需要时回忆差异化规则。<strong class="lq ir"> <em class="na"> z </em> </strong>对一个参数的偏导数告诉你把那个参数看成一个变量，其他所有参数都看成常数。变量的导数等于它的系数。常数的导数等于零:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/eaae1a37235fed57084000726f264bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rf9pn-_3XZK1Gy0Go4vD-w@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">w0是变量，其他都是常数。w0的系数是x0。</p></figure><p id="3305" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">同样，你可以相对于<strong class="lq ir"><em class="na"/></strong><strong class="lq ir"><em class="na">w2</em></strong><strong class="lq ir"><em class="na"/></strong><strong class="lq ir"><em class="na">b</em></strong>来区分<strong class="lq ir"> <em class="na"> z </em> </strong>(其中<strong class="lq ir"> <em class="na"> b </em> </strong>的不可见系数为1)。你会发现<strong class="lq ir"> <em class="na"> z </em> </strong>的每一个偏导数都等于它所微分的参数的系数。考虑到这一点，我们可以用<strong class="lq ir"> Pytorch亲笔签名</strong>来评估我们数学的正确性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h1 id="82bd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">非线性</h1><p id="8ef8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们引入激活函数的非线性。这使得神经网络成为<em class="na">通用函数逼近器</em>。有各种类型的激活，每一种实现不同的目的，产生不同的效果。我们将讨论ReLU、Sigmoid和Softmax的公式和区别。</p><h2 id="96d5" class="nc kx iq bd ky nd ne dn lc nf ng dp lg lx nh ni li mb nj nk lk mf nl nm lm nn bi translated">热卢</h2><p id="5cb2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">整流线性单位函数将神经元的值与零进行比较，并输出最大值。我们可以认为ReLU将所有非阳性神经元标记为同样不活跃。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5da4ff65410c2290c01bfdf7671a1c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*TctGQHmDuQVIi4q2-D7eIA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">所有非负值保持不变，而负值被零取代。</p></figure><p id="08b5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了实现我们自己的ReLU，我们可以将z与0进行比较，并输出较大的值。但是Torch包中提供的<a class="ae kv" href="https://pytorch.org/docs/stable/torch.html#torch.clamp" rel="noopener ugc nofollow" target="_blank"> <em class="na">夹钳</em> </a>方法已经可以为我们做到这一点了。在Numpy中，等效的函数叫做<a class="ae kv" href="https://numpy.org/doc/stable/reference/generated/numpy.clip.html" rel="noopener ugc nofollow" target="_blank"> <em class="na"> clip </em> </a>。在使用Pytorch的<a class="ae kv" href="https://pytorch.org/docs/stable/nn.functional.html#relu" rel="noopener ugc nofollow" target="_blank"> <em class="na"> relu </em> </a>评估其输出之前，以下代码实现了一个基于<em class="na"> clamp </em>的ReLU。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="424a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ReLU的区别很简单:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/c273c2983c8203207040c4ff37c54f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9DnC-VVZT7XNAbR2CfGaDQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“ReLU”为1或0，取决于<strong class="bd nq"> z </strong>。</p></figure><ul class=""><li id="6e69" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj nw nx ny nz bi translated">对于所有正的<strong class="lq ir"> <em class="na"> z </em> </strong>，ReLU的输出为<strong class="lq ir"> <em class="na"> z </em> </strong>。因此微分就是<strong class="lq ir"> <em class="na"> z </em> </strong>的系数，等于1。</li><li id="1827" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">对于所有非正的<strong class="lq ir"> <em class="na"> z </em> </strong>，ReLU的输出等于零。因此，微分也等于零。</li></ul><p id="0731" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们把我们的理解翻译成Python代码。我们先实现自己的ReLU'( <strong class="lq ir"> <em class="na"> z </em> </strong>)再和Pytorch的ReLU自动微分进行对比。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2b58" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为什么我们要给<em class="na"> backward() </em>一个1的张量？ <br/> <em class="na"> backward() </em>默认在单个标量上被调用的情况，并使用默认参数<em class="na"> torch.tensor(1。)</em>这是以前我们调用<em class="na"> z.backward()时的情况。</em>由于<em class="na"> torch_relu </em>不是单个标量，我们需要明确提供一个与<em class="na"> torch_relu </em>形状相同的张量。</p><h2 id="7ab2" class="nc kx iq bd ky nd ne dn lc nf ng dp lg lx nh ni li mb nj nk lk mf nl nm lm nn bi translated">乙状结肠的</h2><p id="649b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">sigmoid激活函数产生从ℝ到范围[0，1]的映射<strong class="lq ir"> <em class="na"> z </em> </strong>的效果。当执行二进制分类时，我们通常用值1标记属于目标类的实例，用值0标记所有其他实例。我们将sigmoid的输出解释为一个实例属于目标类的概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b67d923134d05f4ad320de1e22591bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*g1p8IEciR1rykMKrJ6kJqg@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">sigmoid激活函数产生从ℝ到范围[0，1]的映射<strong class="bd nq"> <em class="og"> z </em> </strong>的效果。</p></figure><p id="c661" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">小测验:</strong>神经网络的任务是进行二元分类。该网络的输出层由sigmoid激活等于0.1的单个神经元组成。在下列解释中，哪一个是正确的？</p><ol class=""><li id="bd4e" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj oh nx ny nz bi translated">该实例属于类1(目标类)的概率为0.1。</li><li id="40c2" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">该实例属于类0的概率为0.1。</li><li id="7ecc" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">该实例属于类0的概率为0.9。</li></ol><p id="38af" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">解法:</strong>只有1和3是正确的。重要的是要理解，具有一些输出<strong class="lq ir"> <em class="na"> p </em> </strong>的乙状结肠激活的神经元，对于非目标类隐含地给出了输出<strong class="lq ir"> <em class="na"> 1-p </em> </strong>。同样需要记住的是<strong class="lq ir"> <em class="na"> p </em> </strong>是与目标类关联的概率(通常标记为1)，而<strong class="lq ir"> <em class="na"> 1-p </em> </strong>是与非目标类关联的概率(通常标记为0)。</p><p id="e98d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">观察:</strong>认为<strong class="lq ir"> <em class="na"> p </em> </strong>和<strong class="lq ir"> <em class="na"> (1-p) </em> </strong>之和等于1。这在现阶段似乎太明显而无法指出，但当我们讨论Softmax时，记住这一点将会很有用。</p><p id="b4ca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们再次用Python翻译数学，然后用sigmoid的Pytorch实现检查我们的结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ff3c3d32180f89174ff48bd0fb52f721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*jiIXsyYhc-xVDY2M-_qejQ@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乙状结肠分化。</p></figure><p id="86ee" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">乙状结肠的分化是优雅的。然而，它确实需要一条曲折的道路来达到它的优雅。一旦我们回忆起一些微分规则，我们就有了在蜿蜒的道路上漫步所需的一切。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/30c69ba292d0fba4ef3c50e610d517d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqAB5TezQxNPc50jEZIXmw@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乙状结肠的详细分化。</p></figure><p id="878e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">了解了如何区分sigmoid，我们现在可以实现数学，并用Pytorch的亲笔签名对其进行评估。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">注:</strong> <em class="og"> sigmoid_prime() </em>依赖于前面实现的同一个<em class="og"> sigmoid() </em>。</p></figure><p id="9a87" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如今，ReLU已被广泛用作乙状结肠的替代物。但是sigmoid仍然在附近徘徊，隐藏在它的更一般化的形式的名字之下:Softmax。</p><h2 id="df1b" class="nc kx iq bd ky nd ne dn lc nf ng dp lg lx nh ni li mb nj nk lk mf nl nm lm nn bi translated">Softmax</h2><p id="6e70" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们想到sigmoid用于二分类，softmax用于多类分类。这种联系虽然是正确的，但却误导我们许多人认为sigmoid和softmax是两种不同的函数。当我们查看sigmoid和softmax的方程时，这一点得到了强调，它们之间似乎没有明显的联系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/4f8a68d73f85043b5d7179a9fff01114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5cO_-H1p06CKaNGpWyAvPw@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">softmax激活的神经元是其值的指数除以共享同一层的所有其他神经元的指数之和。</p></figure><p id="ad4e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">再一次，公式的抽象使它乍一看一点也不直观。举个例子会更具体。我们以两个输出神经元为例，第一个<strong class="lq ir"> <em class="na"> (z0) </em> </strong>输出实例属于标记为0的类别的概率，第二个<strong class="lq ir"> <em class="na"> (z1) </em> </strong>输出实例属于标记为1的类别的概率。换句话说，对于<strong class="lq ir"> <em class="na"> z0 </em> </strong>目标类标记为0，对于<strong class="lq ir"> <em class="na"> z1 </em> </strong>目标类标记为1。为了用softmax激活<strong class="lq ir"> <em class="na"> z0 </em> </strong>和<strong class="lq ir"> <em class="na"> z1 </em> </strong>，我们计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c51e80d0adda0ab7801e53cc6c778961.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*iSikWq04IfHtZp2RAdb1ng@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Softmax应用于输出层中的每个神经元。除了将来自ℝ的所有神经元映射到范围[0，1]之外，它还使它们的值相加为1。</p></figure><p id="b5cd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我们可以纠正乙状结肠和softmax之间似乎缺乏明显的联系。我们将通过简单地重写sigmoid来做到这一点:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/df3c57506b7fe41140b6810b74eafa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZR-zBxr5gROAkCvcE8SASQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">sigmoid的另一种写法，显示它实际上是softmax，有两个类。</p></figure><p id="3038" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第一种形式的乙状结肠比第二种形式的更常见。这是因为后一个版本在计算上更昂贵。然而，它的优势仍然在于帮助我们理解softmax。</p><p id="02cf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在输出层只有两个神经元，并且给定softmax使所有输出神经元总和为1的事实:我们总是知道<strong class="lq ir"><em class="na"/></strong>soft max(z0)<strong class="lq ir"><em class="na">将等于【1-Softmax(z1) </em> </strong>。因此，对于二进制分类，将<strong class="lq ir"><em class="na"/></strong>视为等于0是有意义的，并且使用sigmoid仅计算<strong class="lq ir"> <em class="na"> z1 </em> </strong>的激活。</p><p id="a3db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面的代码实现了softmax，并以三个输出神经元为例进行了测试。然后将我们的结果与Pytorch的<a class="ae kv" href="https://pytorch.org/docs/stable/nn.functional.html#softmax" rel="noopener ugc nofollow" target="_blank"> <em class="na"> softmax </em> </a>的结果进行比较。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="8059" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们根据每个神经元区分softmax激活。保持具有两个神经元的输出层的相同示例，我们得到四个softmax微分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/0643608438e4e11fba305c01d3fcebed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bvom8GBEydafg6E06uJx_w@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">softmax的雅可比矩阵。</p></figure><p id="0e25" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">不考虑输出神经元的数量，softmax微分只有两个公式。当我们对神经元相对于其自身的softmax进行微分(雅可比矩阵中左上和右下的微分)时，应用第一个公式。当我们将一个神经元的softmax相对于某个其他神经元进行微分(雅可比矩阵中右上和左下的微分)时，应用第二个公式。</p><p id="e5b6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了理解softmax的差异化所涉及的步骤，我们需要回忆另一条差异化规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3ef8245127533ecb02d8b167cf4f3ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*9bZUC5em03RqPONaYMtm8g@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">除法法则。</p></figure><p id="5658" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">以下差异包含详细的步骤。虽然它们看起来很密集，看起来令人生畏，但我向你保证，它们比看起来容易得多，我鼓励你在纸上重做它们。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/4f52b740a2b6396df17cf97309b0af5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKogo5viGGNmgcqsM8NBYg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">softmax的详细偏微分。</p></figure><p id="cf7f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">softmax微分的实现要求我们遍历神经元列表，并对每个神经元进行微分。因此涉及两个循环。请记住，这些实现的目的不是为了提高性能，而是为了显式地翻译数学，并达到Pytorch的内置方法所达到的相同结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h1 id="c05d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">交叉熵损失</h1><p id="d9db" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在神经网络所涉及的操作序列中，softmax之后通常是交叉熵损失。事实上，这两个函数联系如此紧密，以至于Pytorch中的方法<a class="ae kv" href="https://pytorch.org/docs/stable/nn.functional.html#cross-entropy" rel="noopener ugc nofollow" target="_blank"> cross_entropy </a>将两个函数合二为一。</p><p id="047d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我记得当我看到交叉熵损失的公式时的第一印象。这是接近欣赏象形文字。解读之后，我希望你能和我一样，对简单的想法有时会有最复杂的表现感到敬畏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/0f307ff8001863d79804b45d014d1604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRS7or39tLQv3dxGa7BzjQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">交叉熵损失函数。</p></figure><p id="59eb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">参与计算交叉熵损失的变量有<strong class="lq ir"> <em class="na"> p </em> </strong>，<strong class="lq ir"> <em class="na"> y </em> </strong>，<strong class="lq ir"> <em class="na"> m </em> </strong>，<strong class="lq ir"> <em class="na"> K </em> </strong>。都以<strong class="lq ir"> <em class="na"> i </em> </strong>和<strong class="lq ir"> <em class="na"> k </em> </strong>为计数器，分别从1迭代到<strong class="lq ir"> <em class="na"> m </em> </strong>和<strong class="lq ir"> <em class="na"> K </em> </strong>。</p><ul class=""><li id="4ee0" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj nw nx ny nz bi translated"><strong class="lq ir"> <em class="na"> Z </em> : </strong>是一个数组，其中每一行代表一个实例的输出神经元。<strong class="lq ir"> <em class="na"> m </em> : </strong>是实例数。</li><li id="26c5" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated"><strong class="lq ir"> <em class="na"> K </em> : </strong>是班级人数。</li><li id="c516" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated"><strong class="lq ir"> <em class="na"> p </em> : </strong>是实例<strong class="lq ir"> <em class="na"> i </em> </strong>属于类<strong class="lq ir"> <em class="na"> k. </em> </strong>的神经网络的概率，这与从softmax计算的概率相同。</li><li id="f05d" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated"><strong class="lq ir"> <em class="na"> y </em> : </strong>是实例<strong class="lq ir"> <em class="na"> i </em> </strong>的标签。根据<strong class="lq ir"> <em class="na"> y </em> </strong>是否属于类<strong class="lq ir"> <em class="na"> k </em> </strong>为1或0。</li><li id="a07a" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated"><strong class="lq ir"> <em class="na"> log </em> : </strong>为自然对数。</li></ul><p id="0890" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">假设我们正在执行一个多类分类任务，其中可能的类的数量是三个(<strong class="lq ir"> <em class="na"> K </em> </strong> =3)。每个实例只能属于一个类。因此，每个实例被分配给一个带有两个0和一个1的标签向量。例如<strong class="lq ir"><em class="na">y</em></strong>=【0，0，1】<strong class="lq ir"><em class="na"/></strong>表示<strong class="lq ir"> <em class="na"> y </em> </strong>的实例属于类2。同样，<strong class="lq ir"><em class="na">y</em></strong>=【1，0，0】<strong class="lq ir"/>表示<strong class="lq ir"> <em class="na"> y </em> </strong>的实例属于0类。1的索引是指实例所属的类。我们说标签是<em class="na">一键编码</em>。</p><p id="8fd4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们来举两个实例(<strong class="lq ir"> <em class="na"> m </em> </strong> =2)。我们计算它们的<strong class="lq ir"> <em class="na"> z </em> </strong>值，我们发现:<strong class="lq ir"> <em class="na"> Z </em> </strong> = [[0.1，0.4，0.2]，[0.3，0.9，0.6]]。然后我们计算他们的softmax概率，发现:<strong class="lq ir"> <em class="na">激活</em> </strong> = [[0.29，0.39，0.32]，[0.24，0.44，0.32]]。我们知道第一个实例属于类2，第二个实例属于类0，因为:<strong class="lq ir"> <em class="na"> y </em> </strong> = [[0，0，1]，[1，0，0]]。</p><p id="91b5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要计算交叉熵:</p><ol class=""><li id="4953" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj oh nx ny nz bi translated">我们取softmax激活的日志:<strong class="lq ir"> <em class="na"> log(激活</em> </strong> ) = [[-1.24，-0.94，-1.14]，[-1.43，-0.83，-1.13]]。</li><li id="72e2" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">我们乘以-1得到负对数:-log(激活)= [[1.24，0.94，1.14]，[1.43，0.83，1.13]]。</li><li id="a89b" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">将log(activations)乘以<strong class="lq ir"> <em class="na"> y </em> </strong>得出:[[0。, 0., 1.14], [1.43, 0., 0.]].</li><li id="d6b8" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">所有类别的总和给出:[[0。+0.+1.14], [1.43+0.+0.]] = [[1.14], [1.43]]</li><li id="bebe" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">所有实例的总和为:[1.14+1.43] = [2.57]</li><li id="2e3c" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj oh nx ny nz bi translated">除以实例数得到:[2.57 / 2] = [1.285]</li></ol><p id="f292" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">观察:</strong></p><ul class=""><li id="9c63" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj nw nx ny nz bi translated">步骤3和4相当于简单地检索目标类的负日志。</li><li id="3835" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">第5步和第6步相当于计算平均值。</li><li id="7e5c" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">当神经网络预测实例以0.32的概率属于目标类时，损失等于1.14。</li><li id="fbf4" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">当神经网络预测实例以0.24的概率属于目标类时，损失等于1.43。</li><li id="86b8" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">我们可以看到，在这两种情况下，网络未能给正确的类最高的概率。但是与第一个实例相比，网络更确信第二个实例不属于正确的类。因此，它被处以1.43英镑的更高损失。</li></ul><p id="46a9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在交叉熵的实现中，我们结合了上述步骤和观察结果。像往常一样，在比较两个输出之前，我们还将经历Pytorch等效方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="5c9a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>我们不是存储标签的独热编码，而是简单地存储1的索引。比如之前的<strong class="lq ir"> <em class="na"> y </em> </strong>变成了【2，0】。注意，在索引0处，<strong class="lq ir"> <em class="na"> y </em> </strong>的值是2，而在索引1处，<strong class="lq ir"> <em class="na"> y </em> </strong>的值是0。使用索引<strong class="lq ir"> <em class="na"> y </em> </strong>及其值，我们可以直接检索目标类的负日志。这是通过访问第0行第2列和第1行第0列的-log(activations)来完成的。这使得我们可以避免第3步和第4步中浪费的乘法和零加法。这个技巧叫做<em class="na">整数数组索引</em>，杰瑞米·霍华德在他的《基础深度学习》第9讲34:57中解释了这个技巧</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="0fa6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果向前穿过神经网络的层可以被看作是获取某种知识的旅程，那么这里就是可以找到知识的地方。使用损失函数的微分可以通知神经网络它在每个实例上有多少误差。把这个误差倒过来看，神经网络可以自我调整。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/1afacc5b91d80514e61216551e1dc775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTGVpspuC_D0xUSdNNQChg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">交叉熵微分。</p></figure><p id="c96b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在回忆了几个微分规则后，我们将经历交叉熵的微分步骤:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/84c478808bee6624faea07a26506a7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*IemBxGPCTdu8vjLSeCmAow@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">回想一下这两条微分法则。还记得<strong class="bd nq"> ln </strong>和<strong class="bd nq">log</strong>based<strong class="bd nq"><em class="og">e</em></strong><em class="og">是一样的。</em> <strong class="bd nq"> <em class="og"> </em> </strong> <em class="og">基</em> <strong class="bd nq"> <em class="og"> e </em> </strong> <em class="og">贯穿全文。</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/e3b4713c3a649994421234754e073fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMCAa0VBwR1QRZoFFCaPCA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">交叉熵微分步骤。</p></figure><p id="1b50" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还不能用Pytorch的输出来评估下面的实现。原因要追溯到Pytorch的<a class="ae kv" href="https://pytorch.org/docs/stable/nn.functional.html#cross-entropy" rel="noopener ugc nofollow" target="_blank"> cross_entropy </a>结合softmax和cross-entropy。因此，使用<a class="ae kv" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward" rel="noopener ugc nofollow" target="_blank">向后</a>也会涉及链规则中softmax的微分。我们将在下一节“反向传播”中讨论和实现这一点。现在，这是我们对交叉熵的实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h1 id="125b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">反向传播</h1><p id="2443" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对于我们讨论的每一个函数，我们都在神经网络的层中前进了一步，并且我们还利用函数的微分进行了等效的后退。由于神经网络在往回走之前一直向前移动，我们需要讨论如何连接我们的功能。</p><h2 id="3ad2" class="nc kx iq bd ky nd ne dn lc nf ng dp lg lx nh ni li mb nj nk lk mf nl nm lm nn bi translated">向前</h2><p id="7626" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一路向前，具有一个隐藏层的神经网络从将输入馈送到线性函数开始，然后将其输出馈送到非线性函数，然后将其输出馈送到损失函数。以下是具有实例<strong class="lq ir"> <em class="na"> x </em> </strong>的示例，其对应的标签<strong class="lq ir"> <em class="na"> y </em> </strong>，三个线性神经元<strong class="lq ir"> <em class="na"> z </em> </strong>，每个神经元使用其三个权重<strong class="lq ir"> <em class="na"> w </em> </strong>和一个偏置<strong class="lq ir"><em class="na">b</em></strong><strong class="lq ir"><em class="na"/></strong>计算，后跟一个softmax激活层和一个交叉熵损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="51a0" class="nc kx iq bd ky nd ne dn lc nf ng dp lg lx nh ni li mb nj nk lk mf nl nm lm nn bi translated">向后的</h2><p id="6562" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一路回溯，同一个神经网络从给予损失函数的相同输入开始，并将其馈送给该损失函数的导数。损失函数的导数的输出就是误差，我们称之为获得的知识。为了调整其参数，神经网络必须将该误差再向后一步带到非线性层，并从那里再向后一步到线性层。</p><p id="0d75" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下一步不是简单地将误差反馈给非线性函数的导数。我们需要使用链式法则(我们之前在sigmoid的微分中回忆过)，我们还需要注意我们应该给每个导数的输入。</p><p id="64bb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">前馈和反向传播的关键规则:</strong></p><ul class=""><li id="0ef2" class="nr ns iq lq b lr mk lu ml lx nt mb nu mf nv mj nw nx ny nz bi translated">函数及其导数接受相同的输入。</li><li id="0e03" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">函数向前发送它们的输出，作为下一个函数的输入。</li><li id="339f" class="nr ns iq lq b lr oa lu ob lx oc mb od mf oe mj nw nx ny nz bi translated">导数将其输出向后发送，以乘以前一个导数的输出。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">愿你的机器的输出总是与你的数学相一致。</p></figure><h1 id="f5f2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="cc26" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我的印象是，很多来自不同学科背景的人都对机器学习充满好奇和热情。不幸的是，有一种合理的趋势，即在试图远离令人生畏的数学的同时获取知识。我认为这很不幸，因为我相信许多人实际上渴望加深他们的理解；要是他们能找到更多的资源就好了，因为他们来自不同的背景，可能到处都需要一点提醒和一点鼓励。</p><p id="02c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇文章包含了我写读者友好的数学的尝试。我指的是提醒读者需要遵循的规则的数学。我的意思也是数学和方程，避免跳过这么多步骤，让我们思考一行和下一行之间发生了什么。因为有时候我们真的需要有人牵着我们的手，和我们一起走过陌生概念的领域。我真诚地希望我能牵到你的手。</p><h1 id="2447" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="30c0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">米（meter的缩写））胺，<a class="ae kv" href="https://drive.google.com/drive/folders/1EkhYkKE74Kz2rYCvGM1BMTJ5ih7Q-toA?usp=sharing" rel="noopener ugc nofollow" target="_blank">神经网络的内部运作，我的Colab笔记本</a> (2020)。<br/> M. Amine，<a class="ae kv" href="https://gist.github.com/Mehdi-Amine" rel="noopener ugc nofollow" target="_blank">神经网络的内部工作原理，我的Gists </a>，(2020)。<br/> A. Géron，<a class="ae kv" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">用Scikit-Learn、Keras和TensorFlow进行动手机器学习</a>，(2019)。<br/> S. Gugger，<a class="ae kv" href="https://sgugger.github.io/a-simple-neural-net-in-numpy.html" rel="noopener ugc nofollow" target="_blank">numpy</a>中的一个简单神经网络，(2018)。<br/> J. Howard，<a class="ae kv" href="https://course.fast.ai/videos/?lesson=9" rel="noopener ugc nofollow" target="_blank"> Fast.ai:从基础开始深度学习第9课</a>，(2019)。<br/><a class="ae kv" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank">py torch文档</a>。</p></div></div>    
</body>
</html>