<html>
<head>
<title>Creating An Interactive Data Visualisation for ‘The Office’, Using D3.js — Data Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 D3 . js——数据处理，为“办公室”创建交互式数据可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-an-interactive-data-visualisation-for-the-office-using-d3-js-part-1-894df7f01730?source=collection_archive---------27-----------------------#2020-02-20">https://towardsdatascience.com/creating-an-interactive-data-visualisation-for-the-office-using-d3-js-part-1-894df7f01730?source=collection_archive---------27-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2ead" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为创建交互式可视化准备数据</h2></div><div class="ki kj gp gr kk kl"><a href="https://cuthchow.github.io/the-office-visualisation/" rel="noopener  ugc nofollow" target="_blank"><div class="km ab fo"><div class="kn ab ko cl cj kp"><h2 class="bd iu gy z fp kq fr fs kr fu fw is bi translated">办公室:数据故事</h2><div class="ks l"><h3 class="bd b gy z fp kq fr fs kr fu fw dk translated">编辑描述</h3></div><div class="kt l"><p class="bd b dl z fp kq fr fs kr fu fw dk translated">cuthchow.github.io</p></div></div><div class="ku l"><div class="kv l kw kx ky ku kz la kl"/></div></div></a></div><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1c0f40359ce49f469d00aa2c561eb9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8j_7h3hnYAa1M0-SPU6Nw.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">我为这个项目创作的一个视觉效果是，一季一季地描绘剧中每个主要角色的情绪得分。</p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lq"><img src="../Images/ed5a0c3583dfd34d2665fe78ae065611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NeMHgol1Pn5PS-FENEYlbg.png"/></div></div></figure><p id="3b0b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我一直认为，最酷的数据可视化是那些你可以与之互动和玩耍的数据，因为它们给你自己探索数据的自由，并从你自己的角度理解它。我还发现，我在网上看到的许多符合这一描述的可视化是在 D3.js 的帮助下创建的，D3 . js 是由 Mike Bostock 创建的 Javascript 库，其特定意图是为 web 创建高度可定制和交互式的可视化。</p><p id="167c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因此，我决定学习 D3.js，以便创造我自己的观想。由于我非常支持基于项目的学习，以便更快更深刻地理解一个主题，我决定将电视节目《办公室》中的所有台词可视化，这实际上结合了我最大的两个兴趣。我在这里的目标是记录整个过程，并分享我在这个过程中学到的一些东西，希望也能帮助任何试图学习 D3.js 的人。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="5a5b" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">数据处理</h1><p id="987a" class="pw-post-body-paragraph lr ls it lt b lu nm ju lw lx nn jx lz ma no mc md me np mg mh mi nq mk ml mm im bi translated">这第一篇文章将关注我在创建实际可视化之前所做的数据预处理。我做的所有预处理都是在 Jupyter 笔记本中使用 Python 和 Pandas 完成的，还有一些其他用于特定任务(如情感分析)的库。</p><p id="2171" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我开始使用的数据集包含了《办公室》中每一句台词的信息，以及关于季节、剧集、场景、演讲者以及场景是否被删除的信息。</p><p id="59c4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我从一开始就知道我想在我的观想中包含以下信息:</p><ul class=""><li id="75eb" class="nr ns it lt b lu lv lx ly ma nt me nu mi nv mm nw nx ny nz bi translated">口语线长度</li><li id="b681" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">台词的感悟</li><li id="3a06" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">文本的词汇复杂性</li></ul><p id="4bde" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了得到每一行的长度，我简单地取了 line_text 列，在每个空格处将字符串分成一个列表，并计算结果数组的长度。</p><pre class="lc ld le lf gt of og oh oi aw oj bi"><span id="6604" class="ok mv it og b gy ol om l on oo">import pandas as pd</span><span id="40c2" class="ok mv it og b gy op om l on oo">df = pd.read_csv('the-office-lines.csv')<br/>df['word_count'] = df['line_text'].apply(lambda x: len(x.split(' ')))</span></pre><p id="1b9d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了获得每行的情感，我不得不利用自然处理库“NLTK”，它有几个预先训练好的模型，适合分析情感的任务。(对于未来的项目，我的目标是使用“The Office”语料库从头构建一个语言模型，以生成更准确的情感分数，但由于这个项目是关于数据可视化的，我满足于使用 NLTK 的内置模型。)NLTK 提供了<a class="ae oq" href="https://www.nltk.org/_modules/nltk/sentiment/vader.html" rel="noopener ugc nofollow" target="_blank"> VADER 情绪分析器</a>供我们使用，它提供了一个从-1 到 1 的复合情绪分数，其中-1 表示绝对负面，1 表示绝对正面。</p><pre class="lc ld le lf gt of og oh oi aw oj bi"><span id="8841" class="ok mv it og b gy ol om l on oo">from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>sid = SentimentIntensityAnalyzer()</span><span id="f775" class="ok mv it og b gy op om l on oo">df['sentiment'] = df['line_text'].apply(lambda x: sid.polarity_score(x)[0]</span><span id="8710" class="ok mv it og b gy op om l on oo">#The 0th element of the polarity scores gives us the compound score, whereas the 1st, 2nd and 3rd element return the negative, neutral and positive scores respectively </span></pre><p id="f2db" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最后，我想分析每一句口语的词汇复杂性。谢天谢地，这种测试已经存在了。我选定的一个是 Flesch-Kincaid 可读性测试，它基本上返回可读性或等级分数，对应于大致的学校等级难度。分数是基于每个单词的平均音节数和每个句子的平均单词数，所以这是一个相当初级的工具。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi or"><img src="../Images/5076a0ffce6aa85d67f44bdb8d5c499d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGEnY9rLxVs33VkYxV2G9w.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">弗莱施-金凯试验</p></figure><p id="6ada" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于它的简单性，它使得实现起来相当简单。然而，应该注意的是，这个测试是针对书面语言的，而电视节目的台词是针对口语的。</p><p id="3b04" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">此外，我意识到对每一行都进行测试并不是最好的方法，因为测试在更大的文本上效果更好。因此，我为每个关键人物创建了一个单独的语料库，然后对每个单独的语料库应用测试，以确保返回的分数尽可能准确。</p><p id="e7e2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">创建单独的搜索集:</p><pre class="lc ld le lf gt of og oh oi aw oj bi"><span id="ec1c" class="ok mv it og b gy ol om l on oo">df = df.groupby('speaker').count().reset_index()<br/>chars = list(df[df['scene'] &gt; 100].speaker)</span><span id="e493" class="ok mv it og b gy op om l on oo">#chars is the list of characters with over 100 scenes, as I wanted to remove the characters with only a handful of appearances from the dataset </span><span id="72b0" class="ok mv it og b gy op om l on oo">char_corpi = dict()</span><span id="598f" class="ok mv it og b gy op om l on oo">for char in chars: <br/>    corpus = []<br/>    for row in df[df.speaker == char]['line_text']:<br/>        corpus.append(row)<br/>    corpus = ' '.join(corpus)<br/>    char_corpi[char] = corpus</span><span id="b814" class="ok mv it og b gy op om l on oo">#char_corpi is a dictionary of corpuses, where the key is each character, and the value is a string of all their lines in the show.</span></pre><p id="d3fb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">创建和应用 Flesch-Kincaid 测试:</p><pre class="lc ld le lf gt of og oh oi aw oj bi"><span id="af10" class="ok mv it og b gy ol om l on oo">import re </span><span id="be2e" class="ok mv it og b gy op om l on oo">def flesch_kincaid_grade(corpus):<br/>    words = len(corpus.split(' '))<br/>    sents = len(re.split('! |? |. ', corpus))<br/>    check = lambda x: 1 if x in 'aeiouyAEIOUY' else 0<br/>    sylls = sum(list(map(check, corpus)))<br/>    score = 206.835 - 1.015 * (words/sents) - 84.6 * (sylls/words)<br/>    return score</span><span id="08a1" class="ok mv it og b gy op om l on oo"><br/>kincaid_scores = []</span><span id="f4cd" class="ok mv it og b gy op om l on oo">for char in char_corpi:<br/>    score = flesch_kincaid_grade(char_corpi[char])<br/>    kincaid_scores.append({'speaker': char, 'score': score})</span><span id="d87f" class="ok mv it og b gy op om l on oo">df = pd.toDataframe(kincaid_scores)<br/>df.to_csv('kincaid_score.csv', index = False)</span><span id="4c40" class="ok mv it og b gy op om l on oo">## This is the new set of data which I will use directly in the visualisation</span></pre><h1 id="f2e6" class="mu mv it bd mw mx os mz na nb ot nd ne jz ou ka ng kc ov kd ni kf ow kg nk nl bi translated">结论</h1><p id="6831" class="pw-post-body-paragraph lr ls it lt b lu nm ju lw lx nn jx lz ma no mc md me np mg mh mi nq mk ml mm im bi translated">这就是我为这个项目做的所有数据预处理。相当简单，对吗？</p></div></div>    
</body>
</html>