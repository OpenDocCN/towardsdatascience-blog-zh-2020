<html>
<head>
<title>Using YOLOv3 for real-time detection of PPE and Fire</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 YOLOv3 实时检测个人防护设备和火灾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-yolov3-for-real-time-detection-of-ppe-and-fire-1c671fcc0f0e?source=collection_archive---------14-----------------------#2020-05-12">https://towardsdatascience.com/using-yolov3-for-real-time-detection-of-ppe-and-fire-1c671fcc0f0e?source=collection_archive---------14-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0ab7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">工业安全中的人工智能:这篇文章解释了我们如何使用 YOLOv3:一种用于实时检测个人防护设备(PPE)和火灾的对象检测算法</h2></div><p id="8a5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“安全不是一个小玩意，而是一种心态！”</p><blockquote class="lf lg lh"><p id="2fde" class="ki kj le kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">世界摩天大楼的不断攀升是一个不断演变的挑战的故事。我们每天生活在摩天大楼、购物中心和仓库周围。大多数时候，我们对建造这些建筑需要什么一无所知。建筑业负责创造如此宏伟的杰作。截至 2020 年 3 月，建筑业拥有 1.3 万亿美元的年支出(约占 GDP 的 6.3%)[1]和 760 万名员工(约占总劳动力的 5%)[2]，是美国经济中最大的行业之一。然而，由于建筑工地的高死亡率，它也被认为是最危险的行业之一。</p></blockquote><p id="a77f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与建筑相关的伤亡的主要原因是跌倒、人员被设备卡住、触电和碰撞。如果工人穿戴适当的个人防护装备，如安全帽、安全背心、手套、护目镜和钢头靴，大多数伤害都是可以避免的。为了确保工作场所的安全，美国职业安全与健康管理局(OSHA)要求承包商强制执行并监控个人防护设备(PPE)的正确使用。</p><p id="dd7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，PPE 检测技术可以分为两种类型:基于传感器的和基于视觉的。基于传感器的方法使用 RFID(射频识别 RFID)标签，需要在购买、安装和维护复杂的传感器网络方面进行大量投资。相反，基于视觉的方法使用摄像机记录施工现场的图像或视频，然后对其进行分析以验证 PPE 合规性。这种方法提供了关于场景的更丰富的信息，可用于更迅速、精确和全面地理解复杂的建筑工地。</p><p id="e3a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文解释了如何使用基于 YOLOv3(你只看一次)架构的深度学习模型来满足各种安全要求，如建筑工地工人的 PPE 合规性和使用最少硬件(即监控摄像头)的火灾探测。深度学习算法通过单个卷积神经网络(CNN)框架同时检测单个工人并验证 PPE 合规性。在当今世界，卷积神经网络(CNN)最广泛地用于图像分类和对象检测，因为它能够从大量带注释的训练数据中进行自我学习。</p><h1 id="d5d6" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">各种实时物体检测技术</strong></h1><p id="df08" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">计算机视觉中的任何对象检测问题都可以定义为识别图像中的对象(也称为分类)，然后精确估计其在图像中的位置(也称为定位)。如今，物体检测几乎在任何地方都被使用。用例是无穷无尽的，无论是跟踪对象、视频监控、行人检测、异常检测、自动驾驶汽车还是人脸检测。</p><p id="7a7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前在工业中使用的主要对象检测算法有三种:</p><h2 id="ea19" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak"> R-CNN:基于区域的卷积网络</strong></h2><p id="bb45" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">R-CNN 首先识别几个感兴趣的区域，然后使用 CNN 对这些区域进行分类，以检测其中的对象。由于原始的 R-CNN 很慢，所以已经提出了它的更快的变体，例如快速 R-CNN、更快 R-CNN 和屏蔽 R-CNN。在 R-CNN 中，图像首先被分成大约 2000 个区域推荐(区域比例)，然后 CNN (ConvNet)被分别应用于每个区域。</p><p id="be97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确定区域的大小，并将正确的区域插入到人工神经网络中。这种方法最大的问题是时间。由于图中的每个区域分别应用 CNN，训练时间约为 84 小时，预测时间约为 47 秒。</p><h2 id="a94f" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak"> YOLO:你只看一次</strong></h2><p id="1c5b" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">大多数目标检测算法使用区域来定位图像中的目标。网络不会看到完整的图像。取而代之的是，他们观察图像中包含物体的概率较高的部分。YOLO 或你只看一次是一个对象检测算法，与上面看到的基于区域的算法有很大不同。在 YOLO，单个卷积网络预测边界框和这些框的类别概率。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/c8757de9eea9723dc493522412e72148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JClMisKksSSjzDpz"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">YOLO 天体探测器管道的简化图(<a class="ae nk" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nl">来源</strong> </a>)</p></figure><p id="d74d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLO 使用的策略与早期的探测系统完全不同。单个神经网络被应用于整个图像。该网络将图片分成多个区域，并计算每个区域的边界框和概率。这些边界框由预测概率加权。</p><h2 id="6732" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak"> SSD:单次多盒探测器</strong></h2><p id="98b7" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">SSD <a class="ae nk" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank"> </a>是物体检测中比较流行的算法。一般比更快的 RCNN 快。SSD 将边界框的输出空间离散化为一组默认框，每个要素地图位置具有不同的纵横比和比例。在预测时，网络为每个默认框中每个对象类别的存在生成分数，并对框进行调整以更好地匹配对象形状。此外，该网络结合了来自不同分辨率的多个特征地图的预测，以自然地处理各种尺寸的物体。[4]</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/272f86d7300fe1d385f40acaf107fcef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E_mxbrAw2SICDROq"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">截图来自解释固态硬盘架构的<a class="ae nk" href="https://medium.com/analytics-vidhya/beginners-guide-to-object-detection-algorithms-6620fb31c375" rel="noopener">文章</a></p></figure><p id="29f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将看到我们如何为这个项目和 YOLOv3 的实现收集数据。</p><h1 id="fd6f" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">数据集采集</strong></h1><p id="ad98" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在这个项目中，我们首先将正在探测的物体分为四类:头盔(戴头盔的人)、人(不戴头盔的人)、消防和安全背心。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/58761ba3de7224041d8d062f9540c266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/0*fDgTMyqQvL8GO4ea"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者照片</p></figure><h2 id="09fa" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak">YOLO 格式的图像标注</strong></h2><p id="6187" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">训练数据集是作为样本收集的图像，并被注释用于训练深度神经网络。我们从多个来源为我们的数据集收集图像，如可以在<a class="ae nk" href="https://github.com/njvisionpower/Safety-Helmet-Wearing-Dataset" rel="noopener ugc nofollow" target="_blank"> github </a>上找到的安全头盔数据集，来自 google 和 Flickr 的图像，并使用 labelIMg 对它们进行注释。此外，为了获得这些对象的定制数据，我们使用 Flickr API &amp;下载了使用<a class="ae nk" href="https://github.com/shwetashambhavi/object-classification-with-yolo/blob/master/flickrImageDownloader.ipynb" rel="noopener ugc nofollow" target="_blank"> python </a>代码的图像。<strong class="kk iu"> </strong>为了给图像加标签，我们使用了<a class="ae nk" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">标签工具</a>。</p><h2 id="27d5" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak">标签工具</strong></h2><p id="c74c" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">LabelImg 是一个图形化的图像注释工具。它是用 Python 编写的，图形界面使用 Qt。对于对象检测，有许多格式可用于准备和注释训练数据集。图像注释以 PASCAL VOC 格式保存为 XML 文件，这是<a class="ae nk" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>使用的格式。它也支持 YOLO 格式，我们使用了这种格式，注释保存在下面的文本文件中。</p><h2 id="af48" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated"><strong class="ak">文本文件中的格式</strong></h2><p id="fe90" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated"><object-class> <x> <y> <width> <height/></width></y></x></object-class></p><p id="18b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><x> / <y>对应于边界框对应轴的中心</y></x></p><p id="ff57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><width> / <height>对应边界框的宽度和高度</height></width></p><p id="f9a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><x><y><width>T3】；范围从[0；1]</width></y></x></p><p id="3c2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">举例:</strong></p><p id="ffbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">2 0.513125 0.209167 0.173750 0.358333</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/4b1598c299d77259803b01adeabd4419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zb_TmgOY0zhU-rQ_"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者照片</p></figure><p id="08f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以参考这篇文章<a class="ae nk" href="https://medium.com/deepquestai/object-detection-training-preparing-your-custom-dataset-6248679f0d1d" rel="noopener"><em class="le">https://medium . com/deepquestai/object-detection-training-preparing-your-custom-dataset-6248679 f0d 1d</em></a><em class="le"/>来全面了解如何使用这个工具来注释你的定制数据集。</p><h2 id="5021" class="mi lm it bd ln mj mk dn lr ml mm dp lv kr mn mo lx kv mp mq lz kz mr ms mb mt bi translated">贴标签时的重要注意事项</h2><p id="1da0" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">为了避免收集火灾图像数据时的错误预测，我们确保我们包括每个对象的各种方向、背景和角度，以更好地训练我们的模型。例如，我们已经包含了落日的图像，将注释置为空，以避免它被误认为是火。</p><p id="a5ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个<a class="ae nk" href="https://github.com/rroy1212/PPE_Detection_YOLOv3/blob/master/dataset" rel="noopener ugc nofollow" target="_blank">链接</a>用于我们的数据集，其中包含一个戴头盔的人、一个不戴头盔的人、消防和安全背心的图像。</p><h1 id="cb32" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">为什么使用 YOLO？</strong></h1><p id="10b7" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">与其他检测系统相比，使用 YOLO 的最大优势之一是，它的预测是由测试时整个图像的全球背景提供的。与 R-CNN 等基于分类器的系统不同，它还通过单一网络评估进行预测。YOLO 比 R-CNN 快 1000 多倍，比快速 R-CNN 快 100 多倍。[10]</p><ul class=""><li id="a9a8" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">速度(每秒 45 帧，比实时速度快)</li><li id="0d04" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">网络理解一般化的对象表示(这允许他们在真实世界的图像上训练网络，并且对艺术品的预测仍然相当准确)</li><li id="7b9a" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">更快的版本(架构更小)—每秒 155 帧，但精度较低</li><li id="cb44" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">开源:<a class="ae nk" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/darknet/yolo/</a></li></ul><h1 id="29c3" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">不同版本的 YOLO </strong></h1><p id="0edd" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">我们在项目中使用的版本是 YOLOv3。YOLO 目前有 4 个版本:v1、v2、v3 和 v4。每个版本都有不断的改进。我们使用的 v3 中最显著的改进是在类别预测和特征提取方面。对于类预测，YOLOv3 不使用 softmax，因为它假设每个“盒子”只能有一个类，这通常是不正确的。相反，研究人员使用独立的逻辑分类器。对于特征提取，他们使用了一个有 53 个卷积层的网络——称为 Darknet-53。[11]</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi od"><img src="../Images/9cf7bfb4c2091e95d35e5fe84cc96b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aQ8dMs4IFr0Ayegs"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">地图上 5 IOU 度量的速度/精度权衡参考<a class="ae nk" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure><h1 id="2a35" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">YOLO 的不同实现</h1><ol class=""><li id="8f75" class="np nq it kk b kl md ko me kr oe kv of kz og ld oh nv nw nx bi translated">暗网(<a class="ae nk" href="https://pjreddie.com/darknet/" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/darknet/</a>)</li><li id="ccda" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">AlexeyAB/darknet(<a class="ae nk" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">https://github.com/AlexeyAB/darknet</a>)</li><li id="1cb7" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">暗流(【https://github.com/thtrieu/darkflow/】T2</li></ol><p id="af2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用 YOLO 的暗网实现</p><h1 id="02a2" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">yolov 3 是如何工作的？</strong></h1><p id="c23f" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">请参考关于“<a class="ae nk" href="https://www.sciencedirect.com/science/article/abs/pii/S0926580519308325#!" rel="noopener ugc nofollow" target="_blank">现场安全深度学习:个人防护装备实时检测</a>的论文，以更好地了解 YOLOv3 的工作情况。</p><ol class=""><li id="1063" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld oh nv nw nx bi translated">在 YOLO-v3 中，我们将 416 × 416 RGB 图像作为输入，它包含三个输出层，每个层将输入图像分别划分为 13 × 13 网格(输出-1)、26 × 26 网格(输出-2)和 52 × 52 网格(输出-3)，如下图所示。[12]</li></ol><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oi"><img src="../Images/af4257b38dada7b9bb4d0294f2becb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kBGCs7aNibSd4_KXc_Ykg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">YOLOv3 算法，如“<a class="ae nk" href="https://www.sciencedirect.com/science/article/abs/pii/S0926580519308325#!" rel="noopener ugc nofollow" target="_blank">现场安全深度学习:个人防护设备的实时检测</a>”中所述</p></figure><p id="6eef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.三个锚盒连接到三个输出层中的每一层，总共有九个锚盒。[12]</p><p id="2222" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.输出-1 层指的是三个最大的锚定框(用于检测大的对象)，输出-2 层连接到下三个最大的锚定框(用于检测中等大小的对象)，输出-3 层与三个最小的锚定框相关联(用于检测小的对象)。[12]</p><p id="2eff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.在训练过程中，输出层中的每个网格单元采用适当的锚定框，并学习如何移动和/或缩放这些锚定框，以使改变后的框(也称为边界框)完美地适合感兴趣的对象，如图所示..[12]</p><p id="be71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.每个预测的边界框与一个(N +5)维向量相关联。[12]</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/41f69445428391908e67b3f1a75ce4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*Lp6qO58EmACHY8Do"/></div></figure><p id="8fef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的向量 tx 中，ty、tw 和 th 分别表示中心的 x 和 y 坐标，以及盒子的宽度和高度。</p><p id="bc5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.值 p0(也称为客体分数)是主体/客体在边界框内的概率。其余的值是 N 个条件概率，P(Ci|object)，每个表示给定一个存在于盒子内的对象，它属于 Ci 类的概率是多少，其中 i = 1，…，N[12]</p><p id="c6b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.总的来说，YOLO-v3 对于单个图像产生总共 10，647 个盒子，因为(13×13×3)+(26×26×3)+(52×52×3)= 10，647。然而，直觉上，输入图片中的大多数输出框要么是假阳性，要么代表相同的东西。[12]</p><p id="2c94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8.为了避免推理部分的不一致和重复，YOLO 使用非最大抑制(NMS)技术来消除重叠百分比较高但置信度较低的框，从而确保给定项目的单个边界框的保留。[12]</p><h1 id="e703" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">结果的定量分析</h1><p id="4ede" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">衡量对象检测算法性能的一个常见度量是交集/并集(又称 IoU)。如下图所示，IoU 表示两个框(如地面实况框(G)和预测框(P ))之间的重叠百分比，并使用下面给出的公式进行计算:[12]</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/1dd2df55642839f53dfb17cb5da5486a.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*ILArQ3pT0og8RqZd4-PWCQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">计算欠条</p></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/63a2a255a13d94f7c1a92757290d3165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*06tuYUzTp15oe-p705Atrw.png"/></div></figure><p id="0833" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下图中，我们可以看到白色的边界框代表真实情况，红色的边界框代表预测。地面真实值和预测值之间的差异用于获得我们模型的平均精度。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi om"><img src="../Images/bdb6dc29d69199159f65cb8ea61ae610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhaSI8lddetn0UZWrp_X2Q.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者图片</p></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/177182e073e57107b1ddbdfcfedb6105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*rt9cg70eCCSHL9XqEQv_rQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者图片</p></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oo"><img src="../Images/24e12a8e00adc031a294f3533bbcd523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0zL4124jG9E0afVCFMG8A.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者图片</p></figure><p id="58b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AP(平均精度)是测量物体检测器(如更快的 R-CNN、SSD、YOLOv3 等)精度时使用最广泛的指标。平均精度计算 0 到 1 之间的召回值的平均精度值。</p><p id="eee5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了我们对整个数据集的预测结果。我们获得了 77.58%的平均精度</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi op"><img src="../Images/47cbee3163576979e26268b501043f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9ZarvAFP1fyMcoy7ZKFBA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者图片</p></figure><p id="ade8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在一个视频上测试了我们训练过的模型，视频中有人戴着头盔，穿着安全背心，还有一个有火的场景。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="oq or l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">我们训练好的模型对视频输入的结果:由我的队友上传到他的 YouTube 频道</p></figure><p id="e7e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对在我们的自定义数据集上实现 YOLOv3 的代码感兴趣，请使用下面的链接访问它。</p><div class="os ot gp gr ou ov"><a href="https://github.com/rroy1212/PPE_Detection_YOLOv3/blob/master/PPE_and_Fire_Detection.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">rroy1212/PPE_Detection_YOLOv3</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">github.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ne ov"/></div></div></a></div><h1 id="3266" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated"><strong class="ak">我们项目的视频讲解</strong></h1><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="oq or l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">视频讲解个人防护设备和使用 Yolov3 进行火灾探测</p></figure><h1 id="c9a4" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">贡献者</h1><blockquote class="lf lg lh"><p id="0bb7" class="ki kj le kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">由 Aastha Goyal、Harshit Nainwani、Rishi Dhaka、Rupali Roy 和 Shweta Shambhavi 为 UT Austin 的数据科学实验室课程 EE 460J 设计的项目。特别感谢 Alex Dimakis 教授！</p></blockquote><h1 id="2dd0" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">参考</h1><ol class=""><li id="65da" class="np nq it kk b kl md ko me kr oe kv of kz og ld oh nv nw nx bi translated">美国人口普查局，建筑支出，<a class="ae nk" href="https://www.census.gov/" rel="noopener ugc nofollow" target="_blank">https://www.census.gov/建筑/c30/prpdf.html </a>，访问日期:2020 年 5 月 11 日。</li><li id="9a8d" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">劳工统计局，《行业一览:建筑业》，<a class="ae nk" href="https://www.bls." rel="noopener ugc nofollow" target="_blank">https://www . bls . gov/iag/TGS/iag 23 . htm</a>，访问日期:2020 年 5 月 11 日。</li><li id="ee0a" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">https://towardsdatascience . com/r-CNN-fast-r-CNN-faster-r-CNN-yolo-object-detection-algorithms-36d 53571365 e</li><li id="d89d" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.02325</a></li><li id="8a93" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" href="https://www.sciencedirect.com/science/article/abs/pii/S0926580519308325?via%3Dihub" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/ABS/pii/s 0926580519308325？通过%3Dihub </a></li><li id="42b6" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">https://medium . com/@ Jonathan _ hui/map-mean-average-precision-for-object-detection-45c 121 a 31173</a></li><li id="a9d8" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">T.-林毅夫、戈亚尔、吉希克、贺铿和杜大伟。密集物体探测的聚焦损失。arXiv 预印本:1708.02002，2017</li><li id="420e" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02640</a></li><li id="1d5b" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" href="https://medium.com/analytics-vidhya/beginners-guide-to-object-detection-algorithms-6620fb31c375" rel="noopener">https://medium . com/analytics-vid hya/初学者-目标探测指南-算法-6620fb31c375 </a></li><li id="1d5a" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated"><a class="ae nk" rel="noopener" target="_blank" href="/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006">https://towards data science . com/yolo-you-only-look-once-real-time-object-detection-explained-492 DC 9230006</a></li><li id="4cf7" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">j .雷德蒙，a .法尔哈迪。YOLOv3:增量改进，2018。arXiv </li><li id="eeb1" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld oh nv nw nx bi translated">Nath、Nipun D .、Amir H. Behzadan 和 Stephanie G. Paal 关于“<a class="ae nk" href="https://www.sciencedirect.com/science/article/abs/pii/S0926580519308325#!" rel="noopener ugc nofollow" target="_blank">现场安全的深度学习:个人防护设备的实时检测</a>”<em class="le">建筑自动化</em> 112 (2020): 103085</li></ol></div></div>    
</body>
</html>