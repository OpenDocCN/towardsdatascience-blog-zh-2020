<html>
<head>
<title>How to implement Kohonen’s Self Organizing Maps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何实现 Kohonen 的自组织地图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-kohonens-self-organizing-maps-989c4da05f19?source=collection_archive---------14-----------------------#2020-09-04">https://towardsdatascience.com/how-to-implement-kohonens-self-organizing-maps-989c4da05f19?source=collection_archive---------14-----------------------#2020-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7a87" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">安|索姆| <strong class="ak"> SOFM | </strong> MATLAB</h2><div class=""/><div class=""><h2 id="fa53" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用 MATLAB 玩和学 SOMs</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e446158379f83919574a6a67654a06e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IA693b_4Q7bDKBQfEty95w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">巴黎艾菲尔铁塔——照片由<a class="ae lh" href="https://unsplash.com/@dnevozhai?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">丹尼斯·内沃扎伊</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="li lj lk"><p id="22ba" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">人工神经网络(ANN)的灵感来自大脑早期的感觉处理模型。可以通过在计算机中模拟模型神经元网络来创建人工神经网络。通过应用模拟真实神经元过程的算法，我们可以让网络“学习”解决许多类型的问题。—安德斯·克拉夫(<a class="ae lh" href="https://www.nature.com/articles/nbt1386" rel="noopener ugc nofollow" target="_blank">自然生物技术</a>)</p></blockquote><p id="0787" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在后现代生活中，我们在每一项活动中都参与了大量惊人的人工神经网络应用，但我们对它们的能力和复杂性一无所知。人工神经网络已被用于解决从语音识别到蛋白质二级结构预测、癌症分类和基因预测等难题。由于对这些高级性能的认识在不久的将来将是必要的，我们应该对这些有更好的了解，并且我们可以从简单的水平开始我们理解 ann 的旅程。</p><p id="838e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">作为一种基本类型的人工神经网络，让我们考虑一种<strong class="lo jd">自组织映射(SOM) </strong>或<strong class="lo jd">自组织特征映射(SOFM) </strong>，使用<em class="ln">无监督学习</em>对其进行训练，以产生训练样本输入空间的低维离散化表示，称为映射。</p><blockquote class="ml"><p id="5bea" class="mm mn it bd mo mp mq mr ms mt mu mh dk translated">自组织地图？</p><p id="feaf" class="mm mn it bd mo mp mq mr ms mt mu mh dk translated">它将高维数据之间的非线性统计<em class="mv">关系</em>转化为它们在低维显示器上的图像点的简单几何关系，通常是规则的二维节点网格。因为 SOM 由此压缩信息，同时在显示器上保留主要数据元素的最重要的拓扑和/或度量关系— <a class="ae lh" href="https://www.springer.com/gp/book/9783540679219" rel="noopener ugc nofollow" target="_blank"> Teuvo Kohonen </a></p></blockquote><h1 id="e74f" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">🔎为什么是 SOM？</h1><p id="eac2" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">基本上，SOMs 被表征为高维输入数据流形到常规低维数组元素的非线性、有序、平滑映射。在训练 SOM 的神经元之后，我们得到高维输入数据的低维表示，而不会扰乱数据分布的形状和每个输入数据元素之间的关系。自组织映射不同于其他人工神经网络，因为与误差校正学习(梯度下降反向传播等)相比，自组织映射应用无监督学习，并且自组织映射使用邻域函数来保持输入空间的拓扑属性。由于它的简单性，我们可以很容易地解释和演示它的功能。详细的解释请参考 Teuvo Kohonen 的自组织地图。</p><h1 id="4012" class="mw mx it bd my mz na nb nc nd ne nf ng ki nt kj ni kl nu km nk ko nv kp nm nn bi translated">💡SOM 是如何工作的？</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/24493b96f747d2c19e071195f00bc189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYjG1GwdP-ShiS2va1dSFA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">SOM 学习数据表示——在第 1、10、50 和 100 时段，输入用蓝点表示，模型的神经元值用红点表示(图片由作者提供)</p></figure><p id="52fe" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">上图给出了学习过程的简单说明，我们可以很容易地从这种表示中理解 SOMs 的特性。最初，输入数据(<em class="ln">蓝点</em>)在 2D 空间占据特殊分布，未学习的神经元(权值)(<em class="ln">红点</em>)随机分布在一个小区域内，神经元经过输入的修改和学习后，在学习过程中逐步得到输入数据分布的形状。此外，每个神经元代表一个小的输入数据空间集群。因此，在这个演示中，我们能够用 100 个神经元表示 1000 个数据点，保留输入数据的拓扑结构。这意味着我们已经在高维数据和低维表示(map)之间建立了联系。对于进一步的计算和预测，我们可以利用这几个神经元值来表示巨大的输入数据空间，这使得处理速度更快。</p><h1 id="ce0d" class="mw mx it bd my mz na nb nc nd ne nf ng ki nt kj ni kl nu km nk ko nv kp nm nn bi translated">📄学习算法</h1><p id="52cc" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">作为 SOM 的基本模型，我们将从'<strong class="lo jd">N '-维</strong>输入数据空间映射到一个<strong class="lo jd">二维神经元阵列(' N '个神经元)</strong>。该 SOM 可以使用以下过程来实现:</p><h2 id="4d42" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">🛠设置:</h2><ul class=""><li id="e499" class="oi oj it lo b lp no ls np mi ok mj ol mk om mh on oo op oq bi translated"><strong class="lo jd"> "P" </strong>输入矢量数量可用。(i= 1，2，…，P)</li><li id="3c68" class="oi oj it lo b lp or ls os mi ot mj ou mk ov mh on oo op oq bi translated"><strong class="lo jd">第 I 个</strong>输入向量有<strong class="lo jd"> n 个</strong>元素:<strong class="lo jd"> Xᵢ = (xᵢ1，xᵢ2，…，xᵢn) </strong></li><li id="e475" class="oi oj it lo b lp or ls os mi ot mj ou mk ov mh on oo op oq bi translated"><strong class="lo jd">“N”</strong>神经元数量(节点或权重)可用。(i= 1，2，…，N)</li><li id="72da" class="oi oj it lo b lp or ls os mi ot mj ou mk ov mh on oo op oq bi translated"><strong class="lo jd">第 I 个</strong>神经元向量有<strong class="lo jd"> n 个</strong>元素:<strong class="lo jd"> mᵢ = (mᵢ1，mᵢ2，…，mᵢN) </strong></li><li id="3792" class="oi oj it lo b lp or ls os mi ot mj ou mk ov mh on oo op oq bi translated">这些神经元向量排列成 2D 矩阵来表示。</li><li id="abb3" class="oi oj it lo b lp or ls os mi ot mj ou mk ov mh on oo op oq bi translated">假设所有向量元素都是实数。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5f04bac918a58d91866adc0844a47ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*lNJo7peVV-Pz0D5fnKvhsw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">SOM 基本模型的建立—输入向量和神经元矩阵(图片由作者提供)</p></figure><h2 id="6354" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">🔖活动动态:</h2><p id="a1b8" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">对于给定的输入<strong class="lo jd"> Xᵢ </strong>，找到离给定输入最近(最小欧氏距离)的神经元，并用<strong class="lo jd"> c </strong>表示该神经元。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/27ef43edea232da7164650c7b86d4f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k84NFtuQr0DngELz_3ya6g.png"/></div></div></figure><h2 id="d95a" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">✏️学习动力公司:</h2><p id="8431" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">对于给定的输入<strong class="lo jd"> Xᵢ </strong>，在找到<strong class="lo jd"> mc </strong>神经元后，<strong class="lo jd">仅</strong>更新<strong class="lo jd"> mc </strong>的<strong class="lo jd">邻域</strong>神经元集合:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/ec89e5db4f3ed57e55589ff08fe6152a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFGY3CbGGNxuuppE3CPezw.png"/></div></div></figure><p id="8fb6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">对于<strong class="lo jd"> t = 0，1，2，… T </strong>。(t 是模型将被更新的迭代次数，并且<strong class="lo jd"> mᵢ (0) </strong>可以是初始的任意向量)。函数<strong class="lo jd"> hci(t) </strong>是所谓的<strong class="lo jd">邻域函数</strong>，在格点(矩阵元素)上定义的平滑核。</p><p id="4060" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">由于我们<strong class="lo jd">只有</strong>需要更新<strong class="lo jd"> mc </strong>神经元周围的邻近神经元，首先我们需要找到神经元<strong class="lo jd"> mc </strong>周围矩阵点的<strong class="lo jd">邻域集合。下面给出了一个简单的拓扑邻域查找方法，更高级的平滑邻域查找方法可以在文献中找到。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/a8f09916509394498943383a6ccb2b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*AFEFwA_uqUVVZBQ6nHjhzg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<strong class="bd pa"> Nc(t) </strong>内寻找数组点的邻域集——绿色神经元是<strong class="bd pa"> mc </strong>神经元，蓝色神经元是神经元的邻域集，只会更新(图片由作者提供)</p></figure><p id="b935" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">标记为<strong class="lo jd"> Nc(t) </strong>的圆内的所有神经元，我们认为它们是‘MC’神经元的<strong class="lo jd">邻域集合。神经元的<strong class="lo jd">邻域集合</strong>半径<strong class="lo jd"> Nc(t) </strong>通常在迭代(t)中单调递减。我们通常从<strong class="lo jd"> Nc(0) = √N/2 </strong>开始，我们需要在每次迭代中减小半径。</strong></p><p id="eca8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在找到需要更新的邻域神经元集后，我们可以使用下面的<strong class="lo jd"> hci(t) </strong>函数，更新<strong class="lo jd"> mc </strong>周围的神经元。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/8171698b1567bdd90ae233766ce511ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*joJiDSMlTf-fIf4pwE7_Tg.png"/></div></div></figure><p id="73f0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在这个方程中，<strong class="lo jd"> || rc — ri|| </strong>定义了神经元的 2D 矩阵位置之间的距离(<strong class="lo jd"> √N x √N </strong>矩阵)。<strong class="lo jd"> α(t) </strong>的值被识别为学习率因子(0 &lt; α(t) &lt; 1)。<strong class="lo jd"> α(t) </strong> <em class="ln"> </em>和<strong class="lo jd"> σ(t) </strong>都是时变的单调递减函数，如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/df358b07ac934cdcb22e37864b078371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFuyzbg-tohw5rcPwbNflA.png"/></div></div></figure><p id="74f0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">对每个输入数据向量(<strong class="lo jd"> P </strong>个输入数据向量)进行学习。然后在相同的输入数据向量上重复进行相同的过程<strong class="lo jd"> T </strong>次迭代。经过<strong class="lo jd"> T </strong>次迭代后，你将得到一个完全学习过的神经元矩阵，它映射了我们的输入数据值。</p><h1 id="88de" class="mw mx it bd my mz na nb nc nd ne nf ng ki nt kj ni kl nu km nk ko nv kp nm nn bi translated">📦MATLAB 实现</h1><p id="1f17" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">使用上述算法，Teuvo Kohonen 的《自组织地图》一书中提到的几个有趣的例子已经使用 MATLAB 实现，您可以将其复制到您的本地计算机，如下所示:</p><pre class="ks kt ku kv gt pd pe pf pg aw ph bi"><span id="553d" class="nx mx it pe b gy pi pj l pk pl">git clone <a class="ae lh" href="https://github.com/KosalaHerath/kohonen-som.git" rel="noopener ugc nofollow" target="_blank">https://github.com/KosalaHerath/kohonen-som.git</a></span></pre><p id="bcc2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">让我们将存储库的 home 定义为<em class="ln"> &lt; REPO_HOME &gt; </em>。然后，转到以下位置，您可以找到三个 MATLAB 实现示例，并且可以在您的计算机上使用任何 MATLAB 版本运行它们:</p><pre class="ks kt ku kv gt pd pe pf pg aw ph bi"><span id="c435" class="nx mx it pe b gy pi pj l pk pl">&lt;REPO_HOME&gt;/source/kohonen_examples</span></pre><p id="780b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">否则，您只需单击以下链接并转到实施库:</p><div class="pm pn gp gr po pp"><a href="https://github.com/KosalaHerath/kohonen-som.git" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd jd gy z fp pu fr fs pv fu fw jc bi translated">科萨拉赫拉特/科霍宁-索姆</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">github.com</p></div></div><div class="py l"><div class="pz l qa qb qc py qd lb pp"/></div></div></a></div><p id="20c9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这些例子从均匀随机分布的二维(<strong class="lo jd"> n=2 </strong>)输入数据向量开始。共有<strong class="lo jd"> 1000 个</strong>输入数据值(<strong class="lo jd"> P=1000 </strong>)。此外，我们将神经元的数量定义为<strong class="lo jd"> N = 10 x 10 = 100 </strong>，迭代次数定义为<strong class="lo jd"> T = 300 </strong>。您可以更改这些参数，并使用上面的实现来处理模型。</p><h2 id="b9d4" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">📍示例 1:平方输入分布</h2><p id="6c8f" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">这个例子的输入数据值是在 2 维空间上随机分布的正方形。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qe"><img src="../Images/36fbcda9cbc733356a33fa677509fd65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYNQnSrB4GEieE3hhDf-bg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">方形输入分布的 SOM 学习数据表示-输入以蓝点表示，模型的神经元值以红点表示，时间为 1、50、250 和 300(图片由作者提供)</p></figure><h2 id="53d9" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">📍示例 2:三角形输入分布</h2><p id="1487" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">该示例的输入数据值是在二维空间上随机分布的三角形。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/55389fcd64b3ea1a524a7c8ff5c745c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKWfzWCCtTpoyDloBiw9kw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">三角形输入分布的 SOM 学习数据表示——在第 1、50、250 和 300 时段，输入用蓝点表示，模型的神经元值用红点表示(图片由作者提供)</p></figure><h2 id="6266" class="nx mx it bd my ny nz dn nc oa ob dp ng mi oc od ni mj oe of nk mk og oh nm iz bi translated">📍示例 3:具有 1D 神经元阵列的三角形输入分布</h2><p id="6c14" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">这个例子的输入数据值是在 2 维空间上的正方形形状的随机分布，并且特别地，我们考虑 1D 神经元阵列而不是 2D 矩阵。因此，由神经元阵列所作的线将试图覆盖如下的所有输入数据分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/e04d327e11bb63a7c077738f4cad87b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0IC8E4wcAnq2cLiVcAbIMA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">1D 神经元阵列的 SOM 学习数据表示——在第 1、50、250 和 300 时段，输入用蓝点表示，模型的神经元值用红点表示(图片由作者提供)</p></figure></div><div class="ab cl qg qh hx qi" role="separator"><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql"/></div><div class="im in io ip iq"><p id="5c21" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">因此，现在您可以通过不同的输入和修改来学习和使用这些实现，并且您尝试的越多，您会理解得越好。请在此 处提出任何<strong class="lo jd">修改</strong>来改进<a class="ae lh" href="https://github.com/KosalaHerath/kohonen-som/issues" rel="noopener ugc nofollow" target="_blank"> <strong class="lo jd">的这些实现。</strong></a></p><p id="2c39" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">干杯！🍺</p><h1 id="bc93" class="mw mx it bd my mz na nb nc nd ne nf ng ki nt kj ni kl nu km nk ko nv kp nm nn bi translated">🗞参考</h1><p id="86c6" class="pw-post-body-paragraph ll lm it lo b lp no kd lr ls np kg lu mi nq lx ly mj nr mb mc mk ns mf mg mh im bi translated">[1] Krogh，A. (2008 年)。什么是人工神经网络？<em class="ln">自然生物技术</em>，26(2)，第 195–197 页。</p><div class="pm pn gp gr po pp"><a href="https://www.nature.com/articles/nbt1386" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd jd gy z fp pu fr fs pv fu fw jc bi translated">什么是人工神经网络？</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">人工神经网络已被应用于从语音识别到蛋白质预测等问题</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">www.nature.com</p></div></div><div class="py l"><div class="qn l qa qb qc py qd lb pp"/></div></div></a></div><p id="2c5c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">[2] Teuvo Kohonen (2001 年)。<em class="ln">自组织地图</em>。纽约斯普林格。</p><div class="pm pn gp gr po pp"><a href="https://www.springer.com/gp/book/9783540679219" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd jd gy z fp pu fr fs pv fu fw jc bi translated">自组织地图| Teuvo Kohonen | Springer</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">自从这本书的第二版在 1997 年初出版以来，在……</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">www.springer.com</p></div></div><div class="py l"><div class="qo l qa qb qc py qd lb pp"/></div></div></a></div></div></div>    
</body>
</html>