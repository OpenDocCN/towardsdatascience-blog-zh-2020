<html>
<head>
<title>Log Book — XGBoost, the math behind the algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">日志— XGBoost，算法背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/log-book-xgboost-the-math-behind-the-algorithm-54ddc5008850?source=collection_archive---------30-----------------------#2020-04-21">https://towardsdatascience.com/log-book-xgboost-the-math-behind-the-algorithm-54ddc5008850?source=collection_archive---------30-----------------------#2020-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f7d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这篇文章以一种简单的方式讲述了XGBoost算法背后的数学原理</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e19cd79663f7ba75eb6090ef915b3ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z9JQkTeoV6UZAp7y.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><blockquote class="kz"><p id="6e7b" class="la lb it bd lc ld le lf lg lh li lj dk translated"><strong class="ak">如果你想<em class="lk">很好地理解</em>某事，试着简单地解释它。— <em class="lk">费曼</em> </strong></p></blockquote><p id="fd56" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf lj im bi translated">XGBoost是一个很好的算法，它的经历非常有启发性。这些概念往往简单而美丽，却迷失在数学术语中。我在理解数学的过程中也面临着同样的挑战，这是一次巩固我的理解的尝试，同时也帮助其他人走上类似的旅程。</p><p id="4017" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">为了理解XGBoost做什么，我们需要理解什么是梯度增强，所以让我们首先理解它背后的概念。请注意，这篇文章假设读者对升压过程非常熟悉，并且将试图触及梯度升压和XGBoost背后的直觉和数学。让我们直接开始吧。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h2 id="b32a" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">了解梯度增强</h2><h2 id="6e7d" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">步骤1 —初始功能</h2><p id="ca64" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">像往常一样，让我们从一个原始的初始函数F₀开始，类似于回归情况下所有值的平均值。它会给我们一些输出，不管多糟糕。</p><h2 id="554c" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">步骤2——损失函数</h2><p id="bd13" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">接下来我们将计算由<strong class="ln iu"><em class="nq">【l(yᵢ,fₜ(xᵢ】)</em></strong>给出的损失函数。</p><p id="83e7" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">什么是损失函数？这只是一种测量实际值和预测值之间差异的方法。以下是几个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/57e60c25f865bbb952abcf2812f58045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*YB_-1ur8kbsCeC2uCc4f0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="3cdf" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">从下表中可以理解为什么这种对异常值的鲁棒性很重要:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3b5fc45e390265045293d878d50ab49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*R9HrSkyyQts18XnrNaoGuA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" rel="noopener ugc nofollow" target="_blank">异常值及其对损失函数的影响，这里5是异常值。检查不同损失函数的值</a></p></figure><p id="8d8c" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">这个想法是，损失函数的值越低，我们的预测就越准确，所以现在获得更好的预测已经成为损失函数的最小化问题。</p><h2 id="0c65" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">步骤2 —新目标</h2><p id="0a76" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">到目前为止，我们已经建立了我们的初始模型，进行了预测。<strong class="ln iu">接下来，我们应该对损失函数给出的残差拟合一个新的模型，但是有一个微妙的变化:我们将改为拟合损失函数的负梯度</strong>，我们为什么这样做以及它们为什么相似的直觉如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/3b155d74d9cf5a552583791b3dc09063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h_nBYl6uCNAEpTnyBkX8jA.png"/></div></div></figure><p id="e596" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated"><strong class="ln iu">梯度可以解释为函数的“最快增长的方向和速率”，所以负梯度告诉我们函数最小值的方向，在这种情况下就是损失函数的最小值。</strong></p><p id="93d1" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">我们将遵循同样的梯度下降法。向损失函数的最小值前进，算法的学习速率将给出步长的大小。并且在损失函数的最小值处，我们将具有最低的错误率。</p><p id="e9a3" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">所以我们将在损失函数的-ve梯度上建立一个新的hₜ₊₁模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/7276d270b6eeecdc50f17feca819a887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsj4_pmqB9o1yDC8Cy2F5w.png"/></div></div></figure><h2 id="137c" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">步骤3——加法方法</h2><p id="d0ee" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">这个在-ve梯度上迭代拟合模型的过程将继续，直到我们<strong class="ln iu">达到由T </strong>给出的弱学习者数量的最小值或极限，这被称为加法方法</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/5f89b989f4609370019a67416cf96a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1Apsv8jcfrZop7u5qBfwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加法方法</p></figure><p id="a50d" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/log-book-adaboost-the-math-behind-the-algorithm-a014c8afbbcc"> <strong class="ln iu">回想一下，在Adaboost中，“缺点”是通过高权重数据点来识别的。</strong> </a> <strong class="ln iu">在梯度提升中，通过梯度来识别“缺点”。</strong></p><p id="074e" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">这缺乏关于梯度增强如何工作的直觉。<strong class="ln iu">在回归和分类的情况下，唯一不同的是使用的损失函数。</strong></p><p id="61b9" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">接下来，我们将了解XGBoost与梯度增强有何不同。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h2 id="4947" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">XGBoost</h2><p id="1ed9" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">XGBoost和GBM都遵循梯度增强树的原理，但<strong class="ln iu"> XGBoost使用更正则化的模型公式来控制过拟合，这使其具有更好的性能</strong>，这也是它也被称为“正则化增强”技术的原因。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/356592a026a0c96e03627026cd0c6677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q22pitRR0PXNLnmJlwXVdw.png"/></div></div></figure><h2 id="2d66" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">牛顿方法</h2><p id="7696" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">那么这个牛顿法是什么呢？<strong class="ln iu">在随机梯度下降中，我们用更少的点，用更少的时间来计算我们应该去的方向，以便使它们更多，希望我们更快地到达那里。在牛顿的方法中，我们花更多的时间来计算我们想要进入的方向，希望我们可以走更少的步来到达那里。</strong></p><blockquote class="kz"><p id="5b66" class="la lb it bd lc ld nx ny nz oa ob lj dk translated">需要注意的重要一点是，即使在梯度提升的情况下，当使用梯度下降来解决回归问题时，分类问题仍然使用牛顿方法来解决最小化问题。XGBoost在分类和回归两种情况下都使用这种方法。</p></blockquote><figure class="od oe of og oh kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/aa6d0d939a1588eced3b43b0a67cb333.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*oD0l4n-jTkC6eU6F.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降法(绿色)和牛顿法(红色)在最小化函数(小步长)方面的比较。牛顿的方法使用曲率信息(即二阶导数)来采取更直接的路线。(<a class="ae ky" href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="19fd" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">牛顿的方法试图通过从初始猜测(起点)<em class="nq"> x₀∈ R </em>构造序列<em class="nq"> {xₖ} </em>来解决最小化问题，该序列通过使用迭代周围的<em class="nq"> f </em>的二阶泰勒近似序列收敛到<em class="nq"> f </em>的极小值x*。围绕<em class="nq"> {xₖ} </em>的<em class="nq"> f </em>的二阶<a class="ae ky" href="https://en.wikipedia.org/wiki/Taylor_expansion" rel="noopener ugc nofollow" target="_blank">泰勒展开</a>为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/4fb108996f75022962e589a33d0d4cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*mk9FmmttfGiz9GU6-O_84w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="6230" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">二阶导数在加速梯度下降中很重要，因为如果你的算法引导你之字形穿过一个山谷，那么你在山谷的实际坡度上取得的进展很小，相反，只是逐步重复穿过这个山谷，通过二阶导数调整方向将使你的下降方向朝这个山谷的方向倾斜，从而将缓慢下降转换为更快的下降。</p><h2 id="0c55" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">损失函数</h2><p id="a48f" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">我们已经了解了平方损失在梯度增强框架中的表现，让我们快速了解一下XGBoost方法中平方损失函数的变化情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/f9f4a9776d7cbabd89a20fdd53c4c709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nS7Ttypb8voe3E6w8tk5wg.png"/></div></div></figure><blockquote class="ok ol om"><p id="5c2a" class="ll lm nq ln b lo mg ju lq lr mh jx lt on mi lw lx oo mj ma mb op mk me mf lj im bi translated">MSE的形式很友好，有一阶项(通常称为残差)和二次项。对于其他的利益损失(比如物流损失)，就不那么容易拿到这么好看的表格了。因此，在一般情况下，我们将损失函数的泰勒展开式提升至二阶— <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost Docs </a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/17f3285ff76565160fd6f240718673bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1SI7x5jf-yhTqLfMhtKGA.png"/></div></div></figure><blockquote class="ok ol om"><p id="cd93" class="ll lm nq ln b lo mg ju lq lr mh jx lt on mi lw lx oo mj ma mb op mk me mf lj im bi translated">这成为我们对新树的优化目标。这个定义的一个重要优点是目标函数的值只取决于pᵢ和qᵢ.这就是XGBoost支持定制损失函数的方式。我们可以优化每个损失函数，包括逻辑回归和成对排名，使用完全相同的求解器，将pᵢ和qᵢ作为输入！— <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a></p></blockquote><h2 id="388a" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">正规化</h2><p id="9ef9" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">接下来我们将处理正则化项，但在此之前，我们需要理解决策树是如何被数学定义的。直觉上，如果你认为一个决策树是或者主要是它的输出是树叶和一个将数据点分配给那些树叶的函数的组合。数学上它被写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/826d16eaf0a33f359e4678de8c9b282a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5P2E5TWpusVSbN4OOxDzg.png"/></div></div></figure><p id="1881" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">其中JT是叶子的数量。该定义将树上的预测过程描述为:</p><ol class=""><li id="1284" class="os ot it ln b lo mg lr mh lu ou ly ov mc ow lj ox oy oz pa bi translated"><em class="nq">将x的数据点分配给m的一片叶子</em></li><li id="097f" class="os ot it ln b lo pb lr pc lu pd ly pe mc pf lj ox oy oz pa bi translated"><em class="nq">将第m(x)叶上相应的分数wₘ₍ₓ₎赋给数据点。</em></li></ol><p id="36e3" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">在XGBoost中，复杂性被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3c8616dcc57fe93e8941770fed09ff0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*0o6b_mp8vqk_Yqdp-QghCw.png"/></div></figure><p id="cccd" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">XGBoost中的这些超参数描述如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/772b308ded38270c56f6dd88db201199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TnGjVIgDy0nyEJy-jiuJ1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost正则化参数</a></p></figure><blockquote class="ok ol om"><p id="f22c" class="ll lm nq ln b lo mg ju lq lr mh jx lt on mi lw lx oo mj ma mb op mk me mf lj im bi translated">当然，定义复杂性的方法不止一种，但是这种方法在实践中效果很好。正则化是大多数树包处理得不够仔细，或者干脆忽略的一部分。这是因为传统的树学习方法只强调改进杂质，而复杂性控制则留给了启发式算法。通过正式定义它，我们可以更好地了解我们正在学习什么，并获得在野外表现良好的模型。— <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/addb3829998007e7446f2b85d95f10d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZ1ClJHmY-doj4XEvinUQQ.png"/></div></div></figure><p id="b8a7" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">最后一个等式衡量<em class="nq">一个树形结构有多好。</em></p><p id="76d5" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">如果这一切听起来有点复杂，我们来看看图片，看看分数是怎么算出来的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/759075a906cb01b22bfba280895e0844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9bMbRRhLk_nqyETI-15xQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><blockquote class="ok ol om"><p id="24b4" class="ll lm nq ln b lo mg ju lq lr mh jx lt on mi lw lx oo mj ma mb op mk me mf lj im bi translated">基本上，对于一个给定的树结构，我们把统计量<em class="it"> gi </em>和<em class="it"> hi </em>推到它们所属的叶子上，把统计量加在一起，用公式计算出树有多好。<strong class="ln iu">该分数类似于决策树中的杂质度量，只是它还考虑了模型的复杂性。</strong> — <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a></p></blockquote><h2 id="d5a5" class="ms mt it bd mu mv mw dn mx my mz dp na lu nb nc nd ly ne nf ng mc nh ni nj nk bi translated">学习树形结构</h2><p id="e6eb" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated">现在我们有了一种方法来衡量一棵树有多好，理想情况下，我们应该列举所有可能的树，然后选出最好的一棵。实际上这是很难做到的，所以我们将尝试一次优化一级树。具体来说，我们尝试将一片叶子分成两片，得到的分数是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/91f3b501b82984363ec5798b997c8037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*BJMP1__VGLRPs5V-eRLaCQ.png"/></div></figure><blockquote class="ok ol om"><p id="ecf1" class="ll lm nq ln b lo mg ju lq lr mh jx lt on mi lw lx oo mj ma mb op mk me mf lj im bi translated">这个公式可以分解为1)新左叶上的分数2)新右叶上的分数3)原始叶上的分数4)附加叶上的正则化。<strong class="ln iu">我们在这里可以看到一个重要的事实:如果增益小于<em class="it"> γ </em>，我们最好不要增加那个分支。</strong>这正是基于树的模型中的<strong class="ln iu">修剪</strong>技术！通过使用监督学习的原则，我们可以自然地得出这些技术工作的原因。— <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a></p></blockquote></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="7282" class="pw-post-body-paragraph ll lm it ln b lo mg ju lq lr mh jx lt lu mi lw lx ly mj ma mb mc mk me mf lj im bi translated">很好，现在希望我们对XGBoost有一个初步的了解，以及为什么它会这样工作。下期帖子再见！</p><h1 id="a03b" class="pl mt it bd mu pm pn po mx pp pq pr na jz ps ka nd kc pt kd ng kf pu kg nj pv bi translated">参考</h1><p id="751c" class="pw-post-body-paragraph ll lm it ln b lo nl ju lq lr nm jx lt lu nn lw lx ly no ma mb mc np me mf lj im bi translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/tutorials/model . html</a><br/><a class="ae ky" href="https://drive.google.com/file/d/1CmNhi-7pZFnCEOJ9g7LQXwuIwom0ZN_D/view" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/D/1 cmnhi-7 pzfnceoj 9g 7 lqxwuiwomo 0 Zn _ D/view</a><br/><a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.02754.pdf</a><br/><a class="ae ky" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" rel="noopener ugc nofollow" target="_blank">http://www . CCS . neu . edu/home/VIP/teach/ml course/4 _ boosting/slides/gradient _ boosting . PDT</a></p></div></div>    
</body>
</html>