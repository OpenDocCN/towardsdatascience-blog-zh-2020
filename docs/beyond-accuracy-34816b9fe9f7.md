# 超出准确度

> 原文：<https://towardsdatascience.com/beyond-accuracy-34816b9fe9f7?source=collection_archive---------55----------------------->

![](img/ed25d5a1e27aa496edb253f209f9d437.png)

在许多行业中，准确性是必不可少的(图片:作者)

## 现在，2010 年只是一个怀旧的记忆，我们在建立机器学习模型时，需要考虑的不仅仅是一个准确的结果。

准确性长期以来一直是机器学习模型和训练它们的人的核心追求——教科书中充满了仅从特定训练集的准确性方面进行比较的算法，而几乎没有关于如何衡量或实现其他理想品质的建议。然而，最近几年，人们意识到仅有准确性是不够的。偏见会扼杀模型的有用性，不透明性和复杂性也是如此。

最受关注的偏见形式可能是模型无意中对人的歧视，例如预测犯罪的模型强化了警察的偏见。尽管这类有偏差的模型受到关注并需要改进是正确的，但还有许多其他领域存在偏差问题。另一个例子是商业模型通过偏见限制了它们自己的有用性。

一家试图在某一人群中扩大市场份额的信用卡公司并不适合一个默认模型，该模型认为该人群是最大的风险因素。一家商店的经理在一个特定的位置试图增加销售额，这种模式可能不太适合位置是销售额最重要的预测因素的商店，因为她不容易搬迁商店。

公平性(或缺乏偏见)和可解释性都作为理想的模型特征受到更多的关注，但是仍然很少有标准的方法来度量它们。虽然它们是不同的特征，但它们在某种意义上是有联系的，即要理解一个模型是公平的还是无偏见的，就需要一定程度的透明度，而更好的可解释性就能更好地理解公平。

一些建模者可能会觉得，通过说我们需要透明度来实现公平，我们正在与准确性进行权衡，因为我们正在使使用黑盒方法(如 GBM 或神经网络)变得更加困难。事实并非如此。正如 Carlson 和 Rudin 的论文[“机器学习的秘密”](https://arxiv.org/abs/1906.01998)中所讨论的，模型在保持透明和可解释的同时表现出最高水平的准确性，这一点经常被忽视——只要这是从一开始的意图，并且足够谨慎。

同样的想法也适用于公平。如果你需要一个公平公正的模型，你需要从一开始就选择这条路。同一篇文章指出，如果你需要确保你的模型是无偏的和可解释的，你需要从一开始就有这个目标。确保公平的一个建议方法是定义公平对于您的模型的上下文意味着什么，并且从过程的开始就将它作为模型训练的一个目标。

类似的原则也适用于可解释性，它严重依赖于算法的选择。某些算法本质上意味着结果模型几乎保证永远不可解释。其他算法为构建高度可解释的模型提供了一个巨大的开端。

然而，这并不能保证一个可解释的模型。还需要考虑其他因素，例如模型看到的特征或输入变量本身是否可解释或有意义，以及模型中允许的关系类型。例如，所有的关系都应该指向同一个方向——单调的吗？或者相反方向的关系在你的环境中是允许的吗？

连接这些想法的主题是，无论何时你想超越准确性，都需要仔细规划，包括仔细选择目标。为了实现公平，从一开始就决定你所指的公平是什么，并确保你在整个开发周期中衡量朝着这个目标的进展。这同样适用于透明性和可解释性——你需要在你的上下文中决定可解释性意味着什么。

推动更细致入微的建模方法需要比以往更深入地理解手头的问题。在训练单个模型之前，需要做出更多的决定。

罗伯特·德格拉夫的书《管理你的数据科学项目》[](https://www.amazon.com/Managing-Your-Data-Science-Projects/dp/1484249062/ref=pd_rhf_ee_p_img_1?_encoding=UTF8&psc=1&refRID=4X4S14FQEBKHZSDYYMZY)**》已经通过出版社出版。**