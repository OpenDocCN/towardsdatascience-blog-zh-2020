<html>
<head>
<title>The Demand Sales Forecast Technique Every Data Scientist Should be Using to Reduce Error</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都应该使用需求销售预测技术来减少错误</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-demand-sales-forecast-technique-every-data-scientist-should-be-using-to-reduce-error-1c6f25add9cb?source=collection_archive---------24-----------------------#2020-07-08">https://towardsdatascience.com/the-demand-sales-forecast-technique-every-data-scientist-should-be-using-to-reduce-error-1c6f25add9cb?source=collection_archive---------24-----------------------#2020-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f3ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 在分步教程中创建破纪录的需求销售预测</h2></div><blockquote class="ki kj kk"><p id="2de1" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">为了更好地了解市场，组织有责任超越他们自己的四面墙去寻找数据源</p></blockquote><p id="cd8e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">Douglas Laney(Gartner Research 副总裁)</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/5436291a9e65c7348a818ca0c6ecbdd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILheVGi9_Ipeq3yYqr5xBQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图片来自 Shutterstock</p></figure><h1 id="6b3c" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">介绍</h1><p id="2ebf" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">世界上成千上万的公司，从小创业公司到全球公司，都发现<strong class="ko iu">能够<strong class="ko iu">准确预测销售</strong>的巨大价值</strong>，这几乎总是他们的数据科学/分析团队的优先事项之一。</p><p id="ac1a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">然而，他们似乎都<strong class="ko iu">试图通过主要关注<strong class="ko iu">两件事</strong>来提高精度</strong>(减少误差)<strong class="ko iu"> </strong>:</p><p id="6892" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">1) <strong class="ko iu">功能工程</strong>(充分利用您的功能)</p><p id="3af6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">2) <strong class="ko iu">模型/参数优化</strong>(选择最佳模型&amp;最佳参数)</p><p id="805f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">以上两者确实非常必要，但是还有第三件事，即<strong class="ko iu">以一种互补的<strong class="ko iu"> </strong>方式增加价值</strong>，并且它不仅在这个用例中，而且在大多数数据科学项目中都被广泛地低估了:</p><ul class=""><li id="ff6e" class="my mz it ko b kp kq ks kt li na lj nb lk nc lh nd ne nf ng bi translated"><strong class="ko iu">结合外部信息</strong>。</li></ul><p id="02fe" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">在本文中，我们将做一个简单的销售<strong class="ko iu">预测模型</strong>，然后<strong class="ko iu">混合外部变量</strong>(正确完成)。</p><h1 id="0891" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">我们要做什么</h1><ul class=""><li id="2ce7" class="my mz it ko b kp mt ks mu li nh lj ni lk nj lh nd ne nf ng bi translated">第一步:定义并理解<strong class="ko iu">目标</strong></li><li id="2b04" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated">第二步:制作一个<strong class="ko iu">简单预测模型</strong></li><li id="1677" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated">第三步:添加<strong class="ko iu">财务指标和新闻</strong></li><li id="0e6a" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated">第四步:测试<strong class="ko iu">型号</strong></li><li id="d1be" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated">步骤 5:测量<strong class="ko iu">结果</strong></li></ul><h1 id="440b" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">第一步。定义和理解目标</h1><p id="60d8" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">流行的<a class="ae np" href="https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">沃尔玛销售预测</strong> </a>竞赛已经有好几个实施来预测他们的销售额。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nq"><img src="../Images/150b6b5db0a9a3c667831bf76c5494db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6AeZXQ-bLr32gL4F.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来自 Kaggle 比赛的截图</p></figure><p id="eb51" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">在本文中，我们将使用数据，并通过结合外部信息来改进模型。</p><p id="1141" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">沃尔玛发布了包含每个实体店 99 个部门(服装、电子产品、食品……)的<strong class="ko iu">周销售额</strong>以及一些其他附加功能的数据。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nr"><img src="../Images/984c0441075d862ed7f599aab970a0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OkXbOmsA9p3OCoeB.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">沃尔玛数据集截图</p></figure><p id="a65b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">为此，我们将<strong class="ko iu">创建一个 ML 模型</strong>，以“<em class="kn"> Weekly_Sales”为目标，</em>使用前 70%的观察值进行训练，并在后 30%进行测试。</p><p id="f872" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">目标是<strong class="ko iu">最小化未来周销售额的预测误差</strong>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ns"><img src="../Images/c962c38f7315ff0309c25be9cfd9d167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IlgHr82Ugo0cngj5.png"/></div></div></figure><p id="be7a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们将<strong class="ko iu">添加影响销售或与销售有关系的外部变量</strong>，如<strong class="ko iu">美元</strong>指数、<strong class="ko iu">石油</strong>价格和关于沃尔玛的<strong class="ko iu">新闻</strong>。</p><p id="3a8d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们<strong class="ko iu">不会</strong>使用模型/参数优化<strong class="ko iu">或</strong>特征工程，因此我们可以区分<strong class="ko iu">收益</strong> <strong class="ko iu">和添加</strong>外部特征。</p><h1 id="967b" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">第二步。制作一个简单的预测模型</h1><p id="9e61" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">首先，您需要安装 Python 3 和以下库:</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="a119" class="ny mc it nu b gy nz oa l ob oc">$ pip install pandas OpenBlender scikit-learn</span></pre><p id="8be4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">然后，打开一个 Python 脚本(最好是 Jupyter notebook)，让我们导入所需的库。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="d8d3" class="ny mc it nu b gy nz oa l ob oc">from sklearn.ensemble import RandomForestRegressor<br/>import pandas as pd<br/>import OpenBlender<br/>import json</span></pre><p id="92f0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们定义用于所有实验的方法和模型。</p><p id="bba9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">首先是</strong>，数据的<em class="kn">日期范围</em>是从 2010 年 1 月到 2012 年 12 月<strong class="ko iu">。</strong>让我们定义用于<strong class="ko iu">训练</strong>的数据的第一个<strong class="ko iu"> 70% </strong>和用于<strong class="ko iu">测试</strong>的后一个<strong class="ko iu"> 30% </strong>(因为我们不希望我们的预测出现数据泄漏)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi od"><img src="../Images/4e5209520cdff09498aab3543b9a1ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CIsq73R7lxXmSB13.png"/></div></div></figure><p id="93b2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">接下来</strong>，让我们定义一个<strong class="ko iu">标准模型</strong>和一个<em class="kn">随机森林回归量</em>，有 50 个估计量，这是一个相当好的选择。</p><p id="0bbf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">最后，为了使事情尽可能简单，让我们将误差定义为误差的绝对和。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dceef16d4b0360c8d97a8871baa4d472.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/0*JJYFNYiOz5GomYpT.png"/></div></figure><p id="e340" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们把它放到一个 Python 类中。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="cecd" class="ny mc it nu b gy nz oa l ob oc">class <strong class="nu iu">StandardModel</strong>:<br/>    <br/>    model = RandomForestRegressor(n_estimators=50, criterion='mse')<br/>    <br/>    def <strong class="nu iu">train</strong>(self, df, target):        <br/># Drop non numerics<br/>        df = df.dropna(axis=1).select_dtypes(['number'])        <br/># Create train/test sets<br/>        X = df.loc[:, df.columns != target].values<br/>        y = df.loc[:,[target]].values        <br/># We take the first bit of the data as test and the <br/>        # last as train because the data is ordered desc.<br/>        div = int(round(len(X) * 0.29))        <br/>        X_train = X[div:]<br/>        y_train = y[div:]        <br/>        print('Train Shape:')<br/>        print(X_train.shape)<br/>        print(y_train.shape)        <br/>#Random forest model specification<br/>        self.model = RandomForestRegressor(n_estimators=50)        <br/># Train on data<br/>        self.model.fit(X_train, y_train.ravel())</span><span id="6959" class="ny mc it nu b gy of oa l ob oc">    def <strong class="nu iu">getMetrics</strong>(self, df, target):<br/>        # Function to get the error sum from the trained model        # Drop non numerics<br/>        df = df.dropna(axis=1).select_dtypes(['number'])        <br/># Create train/test sets<br/>        X = df.loc[:, df.columns != target].values<br/>        y = df.loc[:,[target]].values        <br/>        div = int(round(len(X) * 0.29))        <br/>        X_test = X[:div]<br/>        y_test = y[:div]        <br/>        print('Test Shape:')<br/>        print(X_test.shape)<br/>        print(y_test.shape)</span><span id="f436" class="ny mc it nu b gy of oa l ob oc"># Predict on test<br/>        y_pred_random = self.model.predict(X_test)        <br/># Gather absolute error<br/>        error_sum = sum(abs(y_test.ravel() - y_pred_random))        <br/>        return error_sum</span></pre><p id="6da4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">上面我们有一个包含 3 个元素的对象:</p><ul class=""><li id="da91" class="my mz it ko b kp kq ks kt li na lj nb lk nc lh nd ne nf ng bi translated"><strong class="ko iu">模型</strong> (RandomForestRegressor)</li><li id="9a09" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu">训练:</strong>用数据帧和目标训练模型功能</li><li id="592a" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu"> getMetrics: </strong>用测试数据对训练好的模型进行测试并检索错误的函数</li></ul><p id="ec57" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们将在所有实验中使用这种配置，尽管您可以根据需要修改它来测试不同的模型、参数、配置或其他任何东西。附加值将保持不变，并有可能提高。</p><p id="2c7b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们得到沃尔玛的数据。你可以在这里得到那个 CSV<a class="ae np" href="https://github.com/federico2001/walmart_data" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="8644" class="ny mc it nu b gy nz oa l ob oc">df_walmart = pd.read_csv('walmartData.csv')<br/>print(df_walmart.shape)<br/>df_walmart.head()</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi og"><img src="../Images/4977f3459bce2768489d64b5813d5d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ECARcsaTRLbhFBr6.png"/></div></div></figure><p id="3d1f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">有 421，570 个观察值。如前所述，观察值是每个部门每个商店的每周销售额的记录。</p><p id="ff8e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们将数据插入到模型中，而不要篡改它。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="d206" class="ny mc it nu b gy nz oa l ob oc">our_model = StandardModel()<br/>our_model.train(df_walmart, 'Weekly_Sales')<br/>total_error_sum = our_model.getMetrics(df_walmart, 'Weekly_Sales')<br/>print("Error sum: " + str(total_error_sum))</span><span id="aa58" class="ny mc it nu b gy of oa l ob oc"><em class="kn">&gt; Error sum: 967705992.5034052</em></span></pre><p id="e34a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">整个模型的所有误差总和为<strong class="ko iu"> $ 967，705，992.5 美元</strong>。</p><p id="7fdc" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这个本身没有太大的意义，唯一的参考就是那个时期所有销售额的总和<strong class="ko iu"> $ 6，737，218，987.11 美元</strong>。</p><p id="c2f2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">由于有大量的数据，在本教程中，我们将<strong class="ko iu">仅关注商店#1 </strong>，但该方法绝对可用于所有商店。</p><p id="3d63" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">再来看看<strong class="ko iu">商店 1 </strong>单独产生的<strong class="ko iu">错误</strong>。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="a6d8" class="ny mc it nu b gy nz oa l ob oc"># Select store 1<br/>df_walmart_st1 = df_walmart[df_walmart['Store'] == 1]</span><span id="d057" class="ny mc it nu b gy of oa l ob oc"># Error of store 1<br/>error_sum_st1 = our_model.getMetrics(df_walmart_st1, 'Weekly_Sales')<br/>print("Error sum: " + str(error_sum_st1))</span><span id="be62" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># &gt; Error sum: 24009404.060399983</strong></span></pre><p id="0698" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">因此，商店 1 应对<strong class="ko iu">24，009，404.06 美元</strong>误差负责，而<strong class="ko iu">这个</strong> <strong class="ko iu">将是我们进行比较的阈值。</strong></p><p id="2051" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们按部门分解错误，以便稍后有更多的可见性。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="4c50" class="ny mc it nu b gy nz oa l ob oc">error_summary = []<br/>for i in range(1,100):<br/>    try:<br/>        df_dept = df_walmart_st1[df_walmart_st1['Dept'] == i]<br/>        error_sum = our_model.getMetrics(df_dept, 'Weekly_Sales')<br/>        print("Error dept : " + str(i) + ' is: ' + str(error_sum))<br/>        error_summary.append({'dept' : i, 'error_sum_normal_model' : error_sum})<br/>    except: <br/>        error_sum = 0<br/>        print('No obs for Dept: ' + str(i))<br/>error_summary_df = pd.DataFrame(error_summary)<br/>error_summary_df.head()</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1787e75c976a4d247350acac492cb6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/0*1UizcQ5RgM3CfibJ.png"/></div></figure><p id="db0e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，我们有了一个数据框架，其中包含了与阈值模型中商店 1 的每个部门相对应的错误。</p><p id="dd1d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们<strong class="ko iu">提高</strong>这些数字。</p><h1 id="e505" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">第三步。添加财务指标和新闻</h1><p id="5ec9" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">让我们选择部门 1 作为一个简单的例子。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="2c10" class="ny mc it nu b gy nz oa l ob oc">df_st1dept1 = df_walmart_st1[df_walmart_st1['Dept'] == 1]</span></pre><p id="864e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们搜索相交的数据集。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="be3c" class="ny mc it nu b gy nz oa l ob oc"># First we need to add the <strong class="nu iu">UNIX timestamp</strong> which is the number <br/># of seconds since 1970 on UTC, it is a very convenient <br/># format because it is the same in every time zone in the world!</span><span id="f802" class="ny mc it nu b gy of oa l ob oc">df_st1dept1['timestamp'] = OpenBlender.dateToUnix(df_st1dept1['Date'], <br/>                       date_format = '%Y-%m-%d', <br/>                       timezone = 'GMT')</span><span id="49f6" class="ny mc it nu b gy of oa l ob oc">df_st1dept1 = df_st1dept1.sort_values('timestamp').reset_index(drop = True)</span></pre><p id="d7bd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们在 OpenBlender 中搜索关于“商业”或“沃尔玛”的时间交叉(重叠)数据集。</p><p id="23ec" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">注意:</strong>要获得您<em class="kn">需要的令牌</em>必须在<a class="ae np" href="https://www.openblender.io/#/welcome/or/39" rel="noopener ugc nofollow" target="_blank"> openblender.io </a>(免费)上创建一个帐户，您可以在个人资料图标的“帐户”选项卡中找到它。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5ae067841f53ddbba55e20393640c115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*wJ0LJWBuGUAAYHiq.png"/></div></figure><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="4199" class="ny mc it nu b gy nz oa l ob oc">token = '<strong class="nu iu">YOUR_TOKEN_HERE</strong>'</span><span id="4f18" class="ny mc it nu b gy of oa l ob oc">print('From : ' + OpenBlender.unixToDate(min(df_st1dept1.timestamp)))<br/>print('Until: ' + OpenBlender.unixToDate(max(df_st1dept1.timestamp)))</span><span id="81df" class="ny mc it nu b gy of oa l ob oc"># Now, let's search on OpenBlender<br/>search_keyword = 'business walmart'</span><span id="99c2" class="ny mc it nu b gy of oa l ob oc"># We need to pass our timestamp column and <br/># search keywords as parameters.<br/>OpenBlender.searchTimeBlends(token,<br/>                             df_st1dept1.timestamp,<br/>                             search_keyword)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oj"><img src="../Images/a089ae8d48c0f0e1636fbf7c09c9b38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7VGI9eprNtS8ZM7Ax5S2Ag.png"/></div></div></figure><p id="49fd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">搜索找到了几个数据集。我们可以看到名称、描述、url、特征，最重要的是，我们的时间交互，因此我们可以将它们混合到我们的数据集。</p><p id="4719" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们从混合这个<a class="ae np" href="https://www.openblender.io/#/dataset/explore/5e1deeda9516290a00c5f8f6/or/39" rel="noopener ugc nofollow" target="_blank">沃尔玛推文</a>数据集开始，寻找宣传片。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ok"><img src="../Images/3918f090e09dfdedb91b040f7f0d260c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDu1EejMpYKV91QOEKCbBw.png"/></div></div></figure><ul class=""><li id="cd96" class="my mz it ko b kp kq ks kt li na lj nb lk nc lh nd ne nf ng bi translated">注意:我选择这个是因为它有意义，但是你可以搜索成百上千个其他的。</li></ul><p id="e263" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们可以<strong class="ko iu">通过搜索按时间聚合的文本或新闻，将新的列混合到我们的数据集</strong>中。例如，我们可以创建一个“<strong class="ko iu"> promo </strong>”功能，其提及次数将与我们自制的 ngrams 相匹配:</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="a410" class="ny mc it nu b gy nz oa l ob oc"><strong class="nu iu">text_filter</strong> = {'name' : 'promo', <br/>               'match_ngrams': ['promo', 'dicount', 'cut', 'markdown','deduction']}</span><span id="893d" class="ny mc it nu b gy of oa l ob oc"># <strong class="nu iu">blend_source</strong> needs the id_dataset and the name of the feature.</span><span id="93dd" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                'id_dataset':'<strong class="nu iu">5e1deeda9516290a00c5f8f6</strong>',<br/>                'feature' : '<strong class="nu iu">text</strong>',<br/>                'filter_text' : <strong class="nu iu">text_filter</strong><br/>            }</span><span id="5dca" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                  anchor_ts = df_st1dept1.timestamp,<br/>                                  blend_source = blend_source,<br/>                                  blend_type = 'agg_in_intervals',<br/>                                  interval_size = 60 * 60 * 24 * 7,<br/>                                  direction = 'time_prior',<br/>                                  interval_output = 'list')</span><span id="5af3" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_st1dept1, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span></pre><p id="beec" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">timeBlend 功能的参数(您可以在此处找到文档<a class="ae np" href="https://www.openblender.io/#/api_documentation" rel="noopener ugc nofollow" target="_blank">):</a></p><ul class=""><li id="311b" class="my mz it ko b kp kq ks kt li na lj nb lk nc lh nd ne nf ng bi translated"><strong class="ko iu"> anchor_ts </strong>:我们只需要发送我们的时间戳列，这样它就可以作为一个锚来混合外部数据。</li><li id="380c" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu"> blend_source </strong>:关于我们想要的特性的信息。</li><li id="2c4d" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu">blend _ type</strong>:‘agg _ in _ intervals’因为我们希望对我们的每个观察进行 1 周时间间隔的聚合。</li><li id="bc98" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu"> inverval_size </strong>:间隔的大小，以秒为单位(本例中为 24 * 7 小时)。</li><li id="b6c1" class="my mz it ko b kp nk ks nl li nm lj nn lk no lh nd ne nf ng bi translated"><strong class="ko iu">方向</strong>:‘time _ prior’因为我们希望间隔收集前 7 天的观察值，而不是转发以避免数据泄漏。</li></ul><p id="0ace" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们现在有了我们的原始数据集，但是增加了两个新列，我们的“推广”功能的“计数”和一个实际文本的列表，以防有人想要遍历每个文本。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="55f7" class="ny mc it nu b gy nz oa l ob oc">df_anchor.tail()</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ol"><img src="../Images/3a6c8911256497d006de5e6c55b791c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRY1280qnYzu24ei_j5t9Q.png"/></div></div></figure><p id="ab8f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在我们有了一个数字特征，关于我们的 ngrams 被提及的次数。如果我们知道哪个商店或部门对应于“1”，我们可能会做得更好。</p><p id="53f2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们应用标准模型，并将误差与原始值进行比较。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="7bfe" class="ny mc it nu b gy nz oa l ob oc">our_model.train(df_anchor, 'Weekly_Sales')<br/>error_sum = our_model.getMetrics(df_anchor, 'Weekly_Sales')<br/>error_sum</span><span id="eaa0" class="ny mc it nu b gy of oa l ob oc">#&gt; 253875.30</span></pre><p id="cc58" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">目前的模型有 253，975 美元的误差，而以前的模型有 290，037 美元的误差。这是一个<strong class="ko iu"> 12% </strong>的改进。</p><p id="3a5d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">但是这个<strong class="ko iu">证明不了什么</strong>，可能是随机森林运气好。毕竟，原始模型是用超过 299K 的观测值训练的。目前的一架<strong class="ko iu">只训练了 102 架！！</strong></p><p id="4ee3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们也可以混合数字特征。让我们尝试混合<a class="ae np" href="https://www.openblender.io/#/dataset/explore/5e91029d9516297827b8f08c" rel="noopener ugc nofollow" target="_blank">美元指数</a>、<a class="ae np" href="http://5e91045a9516297827b8f5b1" rel="noopener ugc nofollow" target="_blank">油价</a>和<a class="ae np" href="https://www.openblender.io/#/dataset/explore/5e979cf195162963e9c9853f" rel="noopener ugc nofollow" target="_blank">月度消费者情绪</a></p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="254d" class="ny mc it nu b gy nz oa l ob oc"><strong class="nu iu"># OIL PRICE</strong></span><span id="0bef" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                'id_dataset':'5e91045a9516297827b8f5b1',<br/>                'feature' : 'price'<br/>            }</span><span id="f50a" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                  anchor_ts = df_anchor.timestamp,<br/>                                  blend_source = blend_source,<br/>                                  blend_type = 'agg_in_intervals',<br/>                                  interval_size = 60 * 60 * 24 * 7,<br/>                                  direction = 'time_prior',<br/>                                  interval_output = 'avg')</span><span id="723d" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span><span id="78ba" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># DOLLAR INDEX</strong></span><span id="3167" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                'id_dataset':'5e91029d9516297827b8f08c',<br/>                'feature' : 'price'<br/>            }</span><span id="8915" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                  anchor_ts = df_anchor.timestamp,<br/>                                  blend_source = blend_source,<br/>                                  blend_type = 'agg_in_intervals',<br/>                                  interval_size = 60 * 60 * 24 * 7,<br/>                                  direction = 'time_prior',<br/>                                  interval_output = 'avg')</span><span id="f4b4" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span><span id="b4e7" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># CONSUMER SENTIMENT</strong></span><span id="c372" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                'id_dataset':'5e979cf195162963e9c9853f',<br/>                'feature' : 'umcsent'<br/>            }</span><span id="24be" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                  anchor_ts = df_anchor.timestamp,<br/>                                  blend_source = blend_source,<br/>                                  blend_type = 'agg_in_intervals',<br/>                                  interval_size = 60 * 60 * 24 * 7,<br/>                                  direction = 'time_prior',<br/>                                  interval_output = 'avg')</span><span id="e693" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span><span id="06c0" class="ny mc it nu b gy of oa l ob oc">df_anchor</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi om"><img src="../Images/cd9066837fc949fa487309ac6c97c8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipoAwsczX22agPTnp68pbg.png"/></div></div></figure><p id="f9a4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在我们又有了<strong class="ko iu"> 6 个特征</strong>，石油指数、美元指数和消费者情绪在 7 天时间间隔内的平均值，以及每个特征的计数(这在本例中是不相关的)。</p><p id="5389" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们再运行一次那个模型。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="d1f6" class="ny mc it nu b gy nz oa l ob oc">our_model.train(df_anchor, 'Weekly_Sales')<br/>error_sum = our_model.getMetrics(df_anchor, 'Weekly_Sales')<br/>error_sum</span><span id="e865" class="ny mc it nu b gy of oa l ob oc">&gt;223831.9414</span></pre><p id="ed51" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，我们减少到 223，831 美元的错误。相对于最初的 290，037 美元，提高了<strong class="ko iu">24.1%</strong>！！</p><p id="1885" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在，让我们分别对每个部门进行尝试，以衡量收益的一致性。</p><h1 id="315b" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">第四步。在所有部门进行测试</h1><p id="f6e2" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">为了一目了然，我们将首先对<strong class="ko iu">前 10 个</strong>部门进行实验，并比较添加每个额外资源的<strong class="ko iu">优势。</strong></p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="1855" class="ny mc it nu b gy nz oa l ob oc"><strong class="nu iu"># Function to filter features from other sources</strong></span><span id="4297" class="ny mc it nu b gy of oa l ob oc">def excludeColsWithPrefix(df, prefixes):<br/>    cols = df.columns<br/>    for prefix in prefixes:<br/>        cols = [col for col in cols if prefix not in col]<br/>    return df[cols]</span><span id="7f86" class="ny mc it nu b gy of oa l ob oc">error_sum_enhanced = []</span><span id="4021" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># Loop through the first 10 Departments and test them.</strong></span><span id="7f78" class="ny mc it nu b gy of oa l ob oc">for dept in range(1, 10):<br/>    print('---')<br/>    print('Starting department ' + str(dept))<br/>    <br/>    # Get it into a dataframe<br/>    df_dept = df_walmart_st1[df_walmart_st1['Dept'] == dept]<br/>    <br/>    <br/>    # Unix Timestamp<br/>    df_dept['timestamp'] = OpenBlender.dateToUnix(df_dept['Date'], <br/>                                           date_format = '%Y-%m-%d', <br/>                                           timezone = 'GMT')</span><span id="9b41" class="ny mc it nu b gy of oa l ob oc">df_dept = df_dept.sort_values('timestamp').reset_index(drop = True)<br/>    <br/>    <br/>    <strong class="nu iu"># "PROMO" FEATURE OF MENTIONS ON WALMART</strong><br/>    <br/>    text_filter = {'name' : 'promo', <br/>               'match_ngrams': ['promo', 'dicount', 'cut', 'markdown','deduction']}<br/>    <br/>    blend_source = {<br/>                    'id_dataset':'5e1deeda9516290a00c5f8f6',<br/>                    'feature' : 'text',<br/>                    'filter_text' : text_filter<br/>                }</span><span id="6757" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                      anchor_ts = df_st1dept1.timestamp,<br/>                                      blend_source = blend_source,<br/>                                      blend_type = 'agg_in_intervals',<br/>                                      interval_size = 60 * 60 * 24 * 7,<br/>                                      direction = 'time_prior',<br/>                                      interval_output = 'list')</span><span id="6ad6" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_st1dept1, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)<br/>    <br/>    <strong class="nu iu"># OIL PRICE</strong></span><span id="27fd" class="ny mc it nu b gy of oa l ob oc">    blend_source = {<br/>                    'id_dataset':'5e91045a9516297827b8f5b1',<br/>                    'feature' : 'price'<br/>                }</span><span id="0472" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                      anchor_ts = df_anchor.timestamp,<br/>                                      blend_source = blend_source,<br/>                                      blend_type = 'agg_in_intervals',<br/>                                      interval_size = 60 * 60 * 24 * 7,<br/>                                      direction = 'time_prior',<br/>                                      interval_output = 'avg',<br/>                                      missing_values = 'impute')</span><span id="a4d7" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span><span id="b27c" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># DOLLAR INDEX</strong></span><span id="b752" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                    'id_dataset':'5e91029d9516297827b8f08c',<br/>                    'feature' : 'price'<br/>                }</span><span id="8847" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                      anchor_ts = df_anchor.timestamp,<br/>                                      blend_source = blend_source,<br/>                                      blend_type = 'agg_in_intervals',<br/>                                      interval_size = 60 * 60 * 24 * 7,<br/>                                      direction = 'time_prior',<br/>                                      interval_output = 'avg',<br/>                                      missing_values = 'impute')</span><span id="439f" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)</span><span id="6f67" class="ny mc it nu b gy of oa l ob oc"><strong class="nu iu"># CONSUMER SENTIMENT</strong></span><span id="0c65" class="ny mc it nu b gy of oa l ob oc">blend_source = {<br/>                    'id_dataset':'5e979cf195162963e9c9853f',<br/>                    'feature' : 'umcsent'<br/>                }</span><span id="d536" class="ny mc it nu b gy of oa l ob oc">df_blend = OpenBlender.timeBlend( token = token,<br/>                                      anchor_ts = df_anchor.timestamp,<br/>                                      blend_source = blend_source,<br/>                                      blend_type = 'agg_in_intervals',<br/>                                      interval_size = 60 * 60 * 24 * 7,<br/>                                      direction = 'time_prior',<br/>                                      interval_output = 'avg',<br/>                                      missing_values = 'impute')</span><span id="e77a" class="ny mc it nu b gy of oa l ob oc">df_anchor = pd.concat([df_anchor, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)<br/>    <br/>    <br/>    try:<br/>        <br/>        error_sum = {}<br/>        <br/>        # Gather errors from every source by itself</span><span id="fa03" class="ny mc it nu b gy of oa l ob oc"># Dollar Index<br/>        df_selection = excludeColsWithPrefix(df_anchor, ['WALMART_TW', 'US_MONTHLY_CONSUMER', 'OIL_INDEX'])<br/>        our_model.train(df_selection, 'weekly_sales')</span><span id="ff7e" class="ny mc it nu b gy of oa l ob oc">        error_sum['1_features'] = our_model.getMetrics(df_selection, 'weekly_sales')<br/>        <br/>        # Walmart News<br/>        df_selection = excludeColsWithPrefix(df_anchor, [ 'US_MONTHLY_CONSUMER', 'OIL_INDEX'])<br/>        our_model.train(df_selection, 'weekly_sales')</span><span id="b651" class="ny mc it nu b gy of oa l ob oc">        error_sum['2_feature'] = our_model.getMetrics(df_selection, 'weekly_sales')<br/>        <br/>        # Oil Index<br/>        df_selection = excludeColsWithPrefix(df_anchor,['US_MONTHLY_CONSUMER'])<br/>        our_model.train(df_selection, 'weekly_sales')</span><span id="5044" class="ny mc it nu b gy of oa l ob oc">        error_sum['3_features'] = our_model.getMetrics(df_selection, 'weekly_sales')<br/>        <br/>        # Consumer Sentiment (All features)<br/>        df_selection = df<br/>        our_model.train(df_selection, 'weekly_sales')</span><span id="7cc8" class="ny mc it nu b gy of oa l ob oc">        error_sum['4_features'] = our_model.getMetrics(df_selection, 'weekly_sales')<br/>        <br/>    except:<br/>        import traceback<br/>        print(traceback.format_exc())<br/>        print("No observations found for department: " + str(dept))<br/>        error_sum = 0<br/>        <br/>    error_sum_enhanced.append(error_sum)</span></pre><p id="cb1f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们把结果放到一个数据框架中，然后可视化。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="99c1" class="ny mc it nu b gy nz oa l ob oc">separated_results = pd.DataFrame(error_sum_enhanced)<br/>separated_results['original_error'] = error_summary_df[0:10]['error_sum_normal_model']</span><span id="4e5d" class="ny mc it nu b gy of oa l ob oc">separated_results = separated_results[['original_error', '1_feature', '2_features', '3_features', '4_features']]<br/></span><span id="40c2" class="ny mc it nu b gy of oa l ob oc">separated_results.transpose().plot(kind='line')</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/35d1e5f7c33eea683e74d6fd3be5b287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qLh3ciH5BbjqHyjy.png"/></div></div></figure><p id="5e96" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">部门 4 和 6 的级别比其他部门高，让我们将它们移除，以便更仔细地了解其他部门。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="9cf9" class="ny mc it nu b gy nz oa l ob oc">separated_results.drop([6, 4]).transpose().plot(kind=’line’)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oo"><img src="../Images/3caebcaa5dda0410ce840edef90d6bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4QvIROeWqf2mC2bb.png"/></div></div></figure><p id="4a5e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们可以看到，随着我们<strong class="ko iu">增加新功能</strong>，几乎所有部门的错误都降低了。我们还可以看到，石油指数(第三个特征)对某些部门不仅没有帮助，甚至是有害的。</p><p id="a86b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我排除了石油指数，并在所有部门上运行具有 3 个特征<strong class="ko iu">的算法(这可以通过迭代所有<em class="kn"> error_summary_df </em>而不仅仅是前 10 个来完成)。</strong></p><p id="41bd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们看看结果。</p><h1 id="41ce" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">第五步。测量结果</h1><p id="f76b" class="pw-post-body-paragraph kl km it ko b kp mt ju kr ks mu jx ku li mv kx ky lj mw lb lc lk mx lf lg lh im bi translated">这些是所有部门的“3 个特征”混合和改进百分比<strong class="ko iu">的结果:</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi op"><img src="../Images/961a924a5f977cf844e32b11caec9736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nOYBwFFwBOexcI79.png"/></div></div></figure><p id="57d0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">添加的功能<strong class="ko iu">不仅改善了超过 88%的部门的错误</strong>，而且一些改进是显著的。</p><p id="bf37" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这是<strong class="ko iu">改善百分比的直方图。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/185d6f5c1da6501785e3275bfe9afca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jVY9FadIDQVmQQ17.png"/></div></div></figure><p id="3363" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">原始误差(在文章开头计算)为<strong class="ko iu">24，009，404.06 美元</strong>，最终误差为<strong class="ko iu">9，596，215.21 美元</strong>，表示<strong class="ko iu">减少了 60%以上</strong></p><p id="22d4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">而这只是<strong class="ko iu">一家店</strong>。</p><p id="cab2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>