<html>
<head>
<title>Intuitive Explanation of BERT- Bidirectional Transformers for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中伯特双向变压器的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e?source=collection_archive---------9-----------------------#2020-04-16">https://towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e?source=collection_archive---------9-----------------------#2020-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="93a7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种理解BERT的直观方法——来自用于理解语言的转换器的双向编码器表示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/ffc0d84f221ab5aa35a241fcc25e5ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*bRJ1q8wpSJzGQMmXUAvhLg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由<a class="ae ku" href="https://unsplash.com/@laviperchik?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">狮式战斗机·珀奇克</a>在<a class="ae ku" href="https://unsplash.com/s/photos/child-reading-book?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3335" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="lr">在本帖中，我们将用直观的方式来了解NLP中的进步，包括BERT。使BERT如此强大和受欢迎的预训练策略，以及BERT对大多数NLP任务的微调。</em> </strong></p><h2 id="6492" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">自然语言处理算法的发展</h2><p id="ac2c" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">考虑一下，如果你想学一门新的语言，说印地语，你就很懂英语了。</p><p id="96e4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第一件事是在已知语言的背景下理解新语言的每个单词的意思。你还将了解该语言的同义词和反义词，以获得更好的词汇。这将帮助你理解语义关系。这是<a class="ae ku" rel="noopener" target="_blank" href="/word-embeddings-for-nlp-5b72991e01d4"> <strong class="kx iu"> Word2Vec和</strong> </a>中使用的基本概念</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/51ef14a05375aae111c1100c026af0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3XxbFeOOrNv-AAo_j6cjQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">word2vec和GloVe word嵌入。来源:<a class="ae ku" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>和<a class="ae ku" href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/embeddings/translating-to-a-low-dimension-space</a></p></figure><p id="2202" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下一步是将简单短句从英语翻译成印地语。你会听英语句子中的每个单词，然后根据训练，你会把每个单词从英语逐字翻译成印地语。这与<a class="ae ku" rel="noopener" target="_blank" href="/intuitive-explanation-of-neural-machine-translation-129789e3c59f"> <strong class="kx iu">编码器和</strong> </a> <strong class="kx iu">中使用的概念相同。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mv"><img src="../Images/de8e7641ac507c5a29a28c19eb4fe1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wp3nDMfPEzyt8V0fGYUhRg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">编码器-解码器</p></figure><p id="b7af" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你现在可以翻译短句，但要翻译较长的句子，你需要注意句子中的某些单词，以便更好地理解上下文。这是通过向编码器-解码器模型添加注意机制来实现的。<a class="ae ku" rel="noopener" target="_blank" href="/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a"> <strong class="kx iu">注意机制</strong> </a>让你注意到句子中特定的输入单词，以便更好地翻译，但仍然逐字阅读句子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a4836fdac21b30b8395d72ba3bf6b70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*AL77Ifnlmg3neOXFsXBU8A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">注意机制</p></figure><p id="f693" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">您现在擅长翻译，并且希望提高翻译的速度和准确性。您需要某种并行处理，并且了解上下文以理解长期的依赖性。<a class="ae ku" rel="noopener" target="_blank" href="/simple-explanation-of-transformers-in-nlp-da1adfc5d64f"> <strong class="kx iu">变形金刚</strong> </a> <strong class="kx iu"> </strong>解决了这一要求。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/495e4fcc937eb7606c7e276b1432b2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*9eY7j4VTBftqpf_tO9a6jg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">变形金刚(电影名)</p></figure><p id="f8f4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们来看下面的两句话</p><p id="bbdd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">推荐信被寄到你的</em> <strong class="kx iu"> <em class="lr">地址</em> </strong></p><p id="09c9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">全球各地的领导人需要</em> <strong class="kx iu"> <em class="lr">称呼</em> </strong> <em class="lr">他们在新冠肺炎的人民。</em></p><p id="8220" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">同一个词“<strong class="kx iu"> <em class="lr">地址</em> </strong>”在不同的语境中有不同的含义。你需要把句子作为一个整体来看，以理解句法和语义。<strong class="kx iu">来自语言模型的ELMo嵌入</strong>查看整个句子以理解语法、语义和上下文，从而提高NLP任务的准确性。</p><p id="9abc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你现在开始通过阅读大量文本来学习语言(<strong class="kx iu">迁移学习</strong>)。获得的知识现在被转移和微调，以应用于各种语言任务，如文本分类，翻译文本等。，这个模型被<strong class="kx iu">通用语言模型微调(ULM-Fit) </strong></p><p id="e211" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你使用变形金刚是为了速度、准确性和理解长期依赖性，也是为了从大量的词汇中学习，以便对语言有很强的理解，这种模式就是<strong class="kx iu">生成式预训练变形金刚(GPT) </strong>。它只使用变压器的解码器部分。你也可以应用所学的知识(<strong class="kx iu">迁移学习</strong>)，开始从左向右解释单词(<strong class="kx iu">单向</strong>)。</p><p id="5cc2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">随着你学习语言的不同方面，你意识到接触各种文本对应用迁移学习非常有帮助。你开始阅读书籍，以建立强大的词汇和对语言的理解。当一个句子中的某些单词被屏蔽或隐藏时，那么根据你的语言知识，从左到右和从右到左阅读整个句子(<strong class="kx iu">双向</strong>)。您现在可以更准确地预测屏蔽词(<strong class="kx iu">屏蔽语言建模</strong>)。就像填空一样。也可以预测两句话什么时候有关联或者没有关联(<strong class="kx iu">下一句预测</strong>)。这是一个简单的工作<strong class="kx iu"> BERT:来自变压器的双向编码器表示。</strong></p><p id="fe34" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是对NLP中不同进步的直观解释。</p><h2 id="a2b0" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">来自变压器的双向编码器表示:BERT</h2><blockquote class="my"><p id="66fb" class="mz na it bd nb nc nd ne nf ng nh lq dk translated">BERT旨在通过联合调节所有层中的左右上下文来预训练来自未标记文本的深度双向表示。</p></blockquote><ul class=""><li id="a279" class="ni nj it kx b ky nk lb nl le nm li nn lm no lq np nq nr ns bi translated"><strong class="kx iu"> BERT具有深度双向表示，意味着模型从左到右和从右到左学习信息</strong>。与从左到右模型或者从左到右和从右到左模型的浅层连接相比，双向模型非常强大。</li><li id="ddde" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu"> BERT框架分两步:预训练和微调</strong></li><li id="4575" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu">根据从图书语料库</strong> (800M单词)<strong class="kx iu">和英语维基百科</strong>(2500m单词)中提取的未标注数据进行预训练</li><li id="d771" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu"> BERT预训练模型可以通过一个额外的输出层进行微调，以解决多个NLP任务</strong>，如文本摘要、情感分析、问答聊天机器人、机器翻译等。</li><li id="db6b" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated">BERT的一个与众不同的特点是它的<strong class="kx iu">跨不同任务的统一架构</strong>。预训练的架构和用于各种下游任务的架构之间的差别很小。</li><li id="bf3f" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu"> BERT使用掩蔽语言模型(MLM)在预训练期间使用左右上下文来创建深度双向转换器</strong>。</li></ul><h1 id="3dd0" class="ny lt it bd lu nz oa ob lx oc od oe ma jz of ka md kc og kd mg kf oh kg mj oi bi translated">伯特建筑</h1><p id="4ebb" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">BERT架构是一种多层双向变压器编码器。我们有两个版本的BERT: <strong class="kx iu"> BERT base </strong>和<strong class="kx iu"> BERT large </strong>。</p><p id="f019" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> BERT base有12个编码器，12个双向自关注头，1.1亿个参数</strong></p><p id="f565" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> BERT large有24个编码器，带有24个双向自关注头和3.4亿个参数</strong></p><blockquote class="my"><p id="11d1" class="mz na it bd nb nc nd ne nf ng nh lq dk translated">BERT是一个两步框架:预训练和微调。</p></blockquote><p id="9589" class="pw-post-body-paragraph kv kw it kx b ky nk ju la lb nl jx ld le oj lg lh li ok lk ll lm ol lo lp lq im bi translated"><em class="lr">“序列”是指输入给BERT的令牌序列，可以是单句或两句打包在一起</em></p><h2 id="42ab" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">输入序列</h2><p id="cb5a" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">每个序列的第一个标记总是唯一的分类标记<strong class="kx iu">【CLS】</strong>。成对的句子被打包成一个序列，并用一个特殊的符号<strong class="kx iu">分开。对于给定的标记，其输入表示是通过对相应的标记、段和位置嵌入求和来构建的。</strong></p><h2 id="73e0" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">输出层</h2><p id="136e" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated"><strong class="kx iu">除了输出层，预训练和微调都使用相同的架构。相同的预训练模型参数用于为不同的下游任务初始化模型。</strong></p><h1 id="f100" class="ny lt it bd lu nz oa ob lx oc od oe ma jz of ka md kc og kd mg kf oh kg mj oi bi translated">预训练伯特</h1><blockquote class="my"><p id="3770" class="mz na it bd nb nc nd ne nf ng nh lq dk translated">BERT使用两种无监督策略:掩蔽语言模型(MLM)和下一句预测(NSP)作为预训练的一部分。</p></blockquote><p id="532b" class="pw-post-body-paragraph kv kw it kx b ky nk ju la lb nl jx ld le oj lg lh li ok lk ll lm ol lo lp lq im bi translated">在预训练期间，BERT模型在不同预训练任务的未标记数据上被训练。<strong class="kx iu">根据从图书语料库</strong> (8亿字)<strong class="kx iu">和英语维基百科</strong>(2500万字)中提取的未标记数据对BERT进行预训练</p><h2 id="e504" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">掩蔽语言模型(MLM)</h2><p id="34ae" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">BERT中的双向条件允许每个单词间接“看到自己”为了训练深度双向表示，我们使用MLM随机屏蔽15%的输入标记，然后预测这些屏蔽的标记。</p><p id="5946" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">MLM就像填空一样，我们<strong class="kx iu">随机屏蔽15%的输入标记来预测原始词汇id </strong>。<strong class="kx iu">在BERT中，我们预测屏蔽的令牌，而不是重建整个输入。我们仅在预训练中使用[MASK]标记，它们不用于微调，因为它们会造成不匹配</strong>。为了缓解这一问题，我们并不总是用实际的[MASK]标记替换被屏蔽的单词。</p><p id="eda5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">在随机选择的15%屏蔽令牌中，</strong></p><ul class=""><li id="6f68" class="ni nj it kx b ky kz lb lc le om li on lm oo lq np nq nr ns bi translated"><strong class="kx iu"> 80%的时候，我们用[MASK]标记替换屏蔽词</strong></li><li id="45d8" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu"> 10%的时候，用随机令牌替换</strong></li><li id="2ce1" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu">剩余10%的时间，是不变的。</strong></li></ul><p id="ea54" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> MLM也被称为完形填空任务</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0298625b0bb94f62e5923bdf75611f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*i8zICfESnaGt4EVRcWBLKw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">伯特与MLM和NSP一起进行赛前训练</p></figure><h2 id="5fda" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">下一句预测(NSP)</h2><p id="b79d" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">NSP用于在预训练期间理解句子之间的关系。当我们有两个句子A和B时，B有50%的时间是跟在A后面的实际下一个句子，标记为<strong class="kx iu"> <em class="lr"> IsNext </em> </strong>，有50%的时间是从标记为<strong class="kx iu"> <em class="lr"> NotNext </em> </strong>的语料库中随机抽取的句子。</p><p id="f81a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">NSP有助于自然语言处理任务，如问答(QA)和自然语言推理(NLI)。</p><h1 id="edb9" class="ny lt it bd lu nz oa ob lx oc od oe ma jz of ka md kc og kd mg kf oh kg mj oi bi translated">微调伯特</h1><p id="3e98" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">有两种策略可以应用于下游任务的预训练语言表示:<strong class="kx iu">基于特征和微调。</strong></p><p id="fab4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">伯特使用微调方法。微调方法效果更好，因为它允许通过反向传播来调整语言模型。</p><blockquote class="my"><p id="b115" class="mz na it bd nb nc nd ne nf ng nh lq dk translated">为了微调BERT模型，我们首先使用预训练的参数进行初始化，然后使用来自下游任务的标记数据对所有参数进行微调。</p></blockquote><figure class="or os ot ou ov kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f5690b3d6041e88663999b9dbdf45f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*4TtRj44DLeuTowoq3YzsNQ.png"/></div></figure><p id="815b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">每个下游任务都有单独的微调模型，但用相同的预训练参数进行初始化。对于每个任务，我们只需将特定于任务的输入和输出插入到BERT中，并端到端地微调所有参数。</p><p id="de0e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">微调是在预先训练好的BERT的基础上增加一层未经训练的神经元作为前馈层。</strong></p><p id="593a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">预培训很昂贵，而且是一次性的，但是微调却不贵。</strong></p><h2 id="4e05" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">应用微调的优势</h2><ul class=""><li id="8f1a" class="ni nj it kx b ky ml lb mm le ow li ox lm oy lq np nq nr ns bi translated"><strong class="kx iu">利用迁移学习:</strong>预先训练的BERT已经编码了大量关于语言的语义和句法信息。因此，训练微调模型所需的时间较少。</li><li id="1876" class="ni nj it kx b ky nt lb nu le nv li nw lm nx lq np nq nr ns bi translated"><strong class="kx iu">需要更少的数据:</strong>使用预训练的BERT，我们需要非常少的特定于任务的微调，因此需要更少的数据来获得任何NLP任务的更好性能。</li></ul><h2 id="4840" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">结论:</h2><p id="4bc3" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">BERT设计用于使用来自变压器的编码器预先训练深度双向表示。BERT预训练通过联合调节所有层中的左右上下文来使用未标记的文本。预训练的BERT模型可以通过额外的输出层进行微调，以便为各种NLP任务创建最先进的模型。</p><h2 id="0496" class="ls lt it bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">参考资料:</h2><p id="9f3c" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><div class="oz pa gp gr pb pc"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">谷歌研究/bert</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">这是一个24个较小的BERT模型的版本(只有英语，未封装，用单词屏蔽训练),参考…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">github.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq ko pc"/></div></div></a></div><p id="ec9b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1905.05583.pdf" rel="noopener ugc nofollow" target="_blank">如何微调BERT进行文本分类？</a></p><p id="1051" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/" rel="noopener ugc nofollow" target="_blank">https://mccormickml.com/2019/07/22/BERT-fine-tuning/</a></p></div></div>    
</body>
</html>