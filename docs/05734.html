<html>
<head>
<title>A Beginner’s Guide to Supervised Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督机器学习算法初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-supervised-machine-learning-algorithms-6e7cd9f177d5?source=collection_archive---------52-----------------------#2020-05-12">https://towardsdatascience.com/a-beginners-guide-to-supervised-machine-learning-algorithms-6e7cd9f177d5?source=collection_archive---------52-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="40fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测分析路线图</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dbf2e8ea6763484c9e471f801501d5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfKUMyyss2BQ5v39ine4rw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@williambout?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">威廉·布特</a>在<a class="ae ky" href="https://unsplash.com/s/photos/guide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="7b31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你在这里阅读这篇文章，你一定很清楚机器学习的重要性。近年来，由于许多商业领域的高需求和技术的进步，机器学习的流行程度已经大大增加。有各种各样的机器学习算法，可以分为三个主要类别:</p><ul class=""><li id="414d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">监督学习</strong>算法在给定一组观察值的情况下，对特征(独立变量)和标签(目标)之间的关系进行建模。然后，该模型被用于使用这些特征来预测新观察的标签。</li><li id="30ad" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">无监督学习</strong>算法试图在未标记的数据中找到结构。</li><li id="8788" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">强化学习</strong>基于行动奖励原则工作。一个<strong class="lb iu">代理</strong>通过迭代计算其行动的<strong class="lb iu">回报</strong>来学习达到一个目标。</li></ul><p id="2389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将向您概述常用的监督机器学习算法。监督学习算法试图使用特征(自变量)来预测目标(因变量)。根据目标变量的特性，可以是<strong class="lb iu">分类</strong>(离散目标变量)或<strong class="lb iu">回归</strong>(连续目标变量)任务。我们将讨论的算法包括:</p><ul class=""><li id="918d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">线性回归</li><li id="984a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">支持向量机</li><li id="1fbc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">朴素贝叶斯</li><li id="5ca0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">物流回归</li><li id="7b9a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">k-最近邻</li><li id="b655" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">决策树</li><li id="c322" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随机森林</li><li id="9c50" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">梯度推进决策树</li></ul><p id="9a63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧。</p><h1 id="a338" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">线性回归</strong></h1><p id="71ac" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">线性回归试图通过对数据拟合线性方程来模拟目标变量和一个或多个自变量之间的关系。顾名思义，目标变量是连续的，如价格、对象的面积或长度。</p><p id="150d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是一个好的选择，自变量和目标变量之间需要有线性关系。有许多工具可以探索变量之间的关系，如散点图和相关矩阵。例如，下面的散点图显示了两个变量之间的正相关关系。一个增加，另一个也增加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/df005fe6e20f9a590ecc0a5f7ee04054.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*44xojz4DbMqwLkeFEQ0-8A.png"/></div></figure><p id="e6dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归模型试图将回归线拟合到最能代表关系或相关性的数据点。最常用的技术是<strong class="lb iu">普通最小二乘法</strong> (OLE)。使用这种方法，通过最小化数据点和回归线之间距离的平方和来找到最佳回归线。对于上面的数据点，使用OLE获得的回归线看起来像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a0e937f72eb78b8e71f8ee1ea3c7a3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*FQNvWCILZk4fcB0XSyxZnA.jpeg"/></div></figure><p id="d8e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个自变量和一个目标变量的情况。对于包含多个独立变量的问题，我们也可以使用线性回归。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="0d5b" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">支持向量机</strong></h1><p id="725b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">支持向量机(SVM)主要用于分类任务，但也适用于回归任务。</p><p id="0cad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM通过画一个<strong class="lb iu">决定边界来区分阶级。</strong>如何绘制或确定决策边界是SVM算法中最关键的部分。在创建决策边界之前，在n维空间中绘制每个观察值(或数据点)。“n”是使用的特征数量。例如，如果我们使用“长度”和“宽度”来对不同的“细胞”进行分类，则观察值被绘制在二维空间中，而决策边界是一条线。如果我们使用3个特征，则判定边界是三维空间中的平面。如果我们使用3个以上的特征，决策边界就变成了一个超平面，很难可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d548449694b12de29ae349a1a31d419b.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/0*OfUF3hQKk7vdHUHI.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二维空间中的决策边界</p></figure><p id="3f62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以到支持向量的距离最大化的方式绘制决策边界。<strong class="lb iu"> </strong>如果决策边界离支持向量太近，会对噪声高度敏感，不能很好地泛化。即使独立变量非常小的变化也可能导致错误分类。</p><p id="6247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想阅读一个关于SVM实现的更详细的帖子，你可以访问下面的帖子:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">支持向量机—解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">详细的理论解释和代码示例</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="ed67" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">朴素贝叶斯</strong></h1><p id="d0cd" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">朴素贝叶斯是一种用于分类任务的监督学习算法。因此，它也被称为朴素贝叶斯分类器。</p><p id="3aab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朴素贝叶斯假设<strong class="lb iu">特征相互独立</strong>和<strong class="lb iu">特征</strong>之间没有相关性。然而，现实生活中并非如此。这种特征不相关的天真假设是这种算法被称为“天真”的原因。</p><p id="c3ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朴素贝叶斯算法背后的直觉是贝叶斯定理:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bedc6a4491f3ffa2f698a5d3afd4d426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*21qPq0fhS5TZ5nch.png"/></div></figure><p id="8541" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">p(A|B):给定事件B已经发生的概率</p><p id="9792" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">p(B|A):事件B的可能性给定事件A已经被占用</p><p id="a217" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">p(A):事件A的概率</p><p id="a63e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">p(B):事件B的概率</p><p id="fdfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朴素贝叶斯分类器计算给定一组特征值(即p(yi | x1，x2，…，xn))的类的概率。<strong class="lb iu"> </strong>将此输入贝叶斯定理:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7de7d45bf87a67923b991d366a071d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*NSBIFKj0nKgP9jhe.png"/></div></figure><p id="6f83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> p(x1，x2，…，xn | yi) </strong>表示给定类别标签的特征的特定组合的概率。为了能够计算这一点，我们需要非常大的数据集来估计所有不同特征值组合的概率分布。为了克服这个问题，<strong class="lb iu">朴素贝叶斯算法假设所有特征都是相互独立的。</strong>此外，分母(p(x1，x2，…，xn))可以被移除以简化方程，因为它仅归一化给定观察值的类的条件概率的值(p(yi | x1，x2，…，xn))。</p><p id="2e21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个类的概率(p(yi))很容易计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/be8cd909f0b101221609df42b9262740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/0*qrAvmHJX8ioEcJmw.png"/></div></figure><p id="a532" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在特征相互独立的假设下，<strong class="lb iu"> p(x1，x2，…，xn | yi) </strong>可以写成<strong class="lb iu"> : </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/99284175ae9588f1d4a5f0d323a04954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/0*9eiJ0mN7LGD31WBE.png"/></div></div></figure><p id="463c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定类标签(即p(x1 | yi))的单个特征的条件概率可以更容易地从数据中估计出来。该算法需要独立地存储每个类别的特征的概率分布。例如，如果有5个类别和10个特征，则需要存储50个不同的概率分布。</p><p id="3568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，对于朴素贝叶斯算法来说，计算观察一类给定值的特征的概率(<strong class="lb iu"> p(yi | x1，x2，…，xn) ) </strong>变得很容易</p><p id="12dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一篇关于朴素贝叶斯的更详细的文章，并附有一个例子:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/naive-bayes-classifier-explained-50f9723571ed"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">朴素贝叶斯分类器-解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">scikit-learn的理论和实现</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oq l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="5923" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">逻辑回归</strong></h1><p id="c8bb" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">逻辑回归是一种监督学习算法，主要用于<strong class="lb iu">二元</strong>分类问题。虽然“回归”与“分类”相矛盾，但这里的重点是“逻辑”一词，指的是在该算法中执行分类任务的逻辑函数<strong class="lb iu">。逻辑回归是一种简单但非常有效的分类算法，因此它通常用于许多二元分类任务。客户流失、垃圾邮件、网站或广告点击预测是逻辑回归提供强大解决方案的一些领域的例子。</strong></p><p id="4751" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归的基础是逻辑函数，也称为sigmoid函数，它接受任何实数值并将其映射到0到1之间的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/125e66b5a7d5a217d1a7f7a36d68078d.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*7JrgJLpDxzK5TEi6.png"/></div></figure><p id="ba91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们要求解以下线性方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/df0af6cd5c9bc7044993a23f5545abad.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*_8Q8qO6YDP1FY1NM.png"/></div></figure><p id="689a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归模型将线性方程作为输入，并使用逻辑函数和对数比值来执行二元分类任务。然后，我们会得到著名的逻辑回归s形图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b3498cb7c9651d31562ebb79b6c90bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/0*8Jys3l2FW-DObBYD.png"/></div></figure><p id="8643" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以“原样”使用计算出的概率。例如，输出可以是电子邮件是垃圾邮件的概率是95%,或者客户将点击该广告的概率是70%。然而，在大多数情况下，概率被用来分类数据点。如果概率大于50%，则预测为正类(1)。否则，预测为负类(0)。</p><p id="1b2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想深入了解，这里有一个更详细的帖子:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/logistic-regression-explained-593e9ddb7c6c"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">逻辑回归—已解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">详细的理论解释和scikit-learn示例</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="ou l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="5b72" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">K-最近邻(kNN) </strong></h1><p id="78b6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">k-最近邻(kNN)是一种受监督的机器学习算法，可用于解决分类和回归任务。kNN背后的主要思想是，数据点的值或类由其周围的数据点决定。</p><p id="c61c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">kNN分类器通过多数表决原则确定数据点的类别。例如，如果k设置为5，则检查5个最近点的类。根据多数类进行预测。类似地，kNN回归取5个最近点的平均值。让我们看一个例子。考虑以下属于4个不同类别的数据点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/cb603815ca8ad44e9514e3b416c6c36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*4M94_XNFFtVkH2Iq.png"/></div></figure><p id="08cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看预测类如何根据k值变化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/edbb5ac52d06e8d18b3b4010a6519ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*PfXzoSFN_yIksRtl.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/b4a51fad229b7234e9a842d5ecab4a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*kXFV4eXgsX9A7Zlz.png"/></div></figure><p id="3466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确定最佳k值是非常重要的。如果k太低，模型太具体，不能很好地概括。它对噪音也很敏感。该模型在训练集上实现了高精度，但是在新的、以前看不到的数据点上将是差的预测器。因此，我们很可能以一个过度拟合的模型而告终。另一方面，如果k太大，则该模型太一般化，并且在训练集和测试集上都不是好的预测器。这种情况被称为欠适配。</p><p id="9565" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是kNN上更详细的帖子:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbors-knn-explained-cbc31849a7e3"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">k-最近邻(kNN)-解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">详细的理论解释和scikit-learn实现</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="ow l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="e5ae" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">决策树</strong></h1><p id="f346" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">决策树建立在反复询问问题以划分数据的基础上。用决策树的可视化表示来概念化分区数据更容易:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/d984316ac9275944606236c28be42215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRk57fgsYTqvpnPTf69FRQ.png"/></div></div></figure><p id="87b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这代表了预测客户流失的决策树。第一次拆分基于每月费用金额。然后算法不断提问分离类标签。随着树越来越深，问题也越来越具体。</p><p id="46a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你在每一步问什么是最关键的部分，并极大地影响决策树的性能。例如，假设数据集的“要素A”范围为0到100，但大多数值都在90以上。在这种情况下，首先要问的问题是“特征A大于90吗？”。问“特性A大于50吗？”是没有意义的因为它不会给我们太多关于数据集的信息。</p><p id="0eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是在每次分区时尽可能增加模型的预测性，以便模型不断获得关于数据集的信息。随机分割要素通常不会给我们提供关于数据集的有价值的信息。增加节点纯度的分割更能提供信息。节点的纯度与该节点中不同类的分布成反比。要问的问题是以增加纯度或减少杂质的方式选择的。</p><p id="b14a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们要问多少问题？我们什么时候停止？什么时候我们的树足以解决我们的分类问题？所有这些问题的答案将我们引向机器学习最重要的概念之一:<strong class="lb iu">过拟合</strong>。模型可以一直提问，直到所有节点都是纯的。纯节点只包括来自一个类的数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f9452ac2ba0e633da718734be2b45a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*e7NN1uYvttBZroat.png"/></div></figure><p id="34d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型可以一直提问(或者拆分数据)，直到所有的叶子节点都是纯的。然而，这将是一个过于具体的模型，不能很好地概括。它在训练集上实现了高精度，但在新的、以前看不到的数据点上表现不佳。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="ed09" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">随机森林</strong></h1><p id="4234" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">随机森林是许多决策树的集合。随机森林是用一种叫做<strong class="lb iu">装袋</strong>的方法构建的，其中每个决策树都被用作并行估计器。如果用于分类问题，结果基于从每个决策树接收的结果的多数投票。对于回归，叶节点的预测是该叶中目标值的平均值。随机森林回归取决策树结果的平均值。</p><p id="1806" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林降低了过度拟合的风险，并且准确性比单个决策树高得多。此外，随机森林中的决策树并行运行，因此时间不会成为瓶颈。</p><p id="f367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林的成功高度依赖于使用不相关的决策树。如果我们使用相同或非常相似的树，总体结果将不会比单个决策树的结果有太大的不同。随机森林通过<strong class="lb iu">自举</strong>和<strong class="lb iu">特征随机性</strong>实现不相关的决策树。</p><ul class=""><li id="e54e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">Bootsrapping是从带有替换的训练数据中随机选择样本。它们被称为bootstrap样本。下图清楚地解释了这一过程:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/0979c805185ed90982db1e4993722538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/0*zowIgw0LS7eCURB4.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.researchgate.net/figure/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some_fig2_322179244" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><ul class=""><li id="e907" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">通过为随机森林中的每个决策树随机选择特征来实现特征随机性。随机森林中每棵树使用的特征数量可通过<strong class="lb iu"> max_features </strong>参数控制。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/23dc9265cec42d4a2607dacbbbb6358d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/0*x4HoilQH3Dav4Yva.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征随机性</p></figure><p id="5b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自助样本和特征随机性为随机森林模型提供了不相关的树。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/9eb9837dc10b4e60833a684fab1050ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0UetJMAaBKff1ifv.png"/></div></div></figure><p id="8e3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一篇关于决策树和随机森林的详细文章，并附有一个实现示例:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/decision-tree-and-random-forest-explained-8d20ddabc9dd"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">决策树和随机森林—解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">详细的理论解释和代码示例</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="pc l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="6743" class="mj mk it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">梯度推进决策树(GBDT) </strong></h1><p id="7c4d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu"> GBDT </strong>算法，该算法使用<strong class="lb iu">助推</strong>方法来组合个体决策树。</p><p id="d435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boosting是指将一个学习算法串联起来，从许多顺序连接的弱学习器中实现一个强学习器。在梯度提升决策树算法的情况下，弱学习器是决策树。</p><p id="2809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每棵树都试图最小化前一棵树的错误。boosting中的树是弱学习器，但是连续添加许多树，并且每个树集中于前一个树的错误，使得boosting成为高效且准确的模型。与装袋不同，增压不涉及自举取样。每次添加新树时，它都适合初始数据集的修改版本。</p><p id="ee0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于树是按顺序添加的，boosting算法学习起来很慢。在统计学习中，学习速度慢的模型表现更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/1bae18b794bf8f329eafe1512024adb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5LbQY_URIDJEj8XB.png"/></div></div></figure><p id="7c74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升算法以这样的方式顺序地组合弱学习器，即每个新的学习器适合于来自前一步骤的残差，从而改进模型。最终的模型汇总了每一步的结果，从而形成了一个强学习者。<strong class="lb iu">梯度提升决策树</strong>算法使用决策树作为弱学习器。损失函数用于检测残差。例如，均方误差(MSE)可用于回归任务，对数损失(log loss)可用于分类任务。值得注意的是，当添加新的树时，模型中现有的树不会改变。添加的决策树符合当前模型的残差。</p><p id="840c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GBDT算法非常强大，已经实现了许多升级版本，如XGBOOST、LightGBM、CatBoost。</p><p id="6a66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是关于GBDT的详细帖子:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">梯度推进决策树-解释</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">详细解释了boosting和scikit-learn实现</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="pe l oj ok ol oh om ks ny"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="ab1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="0e08" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="a523" class="lv lw it lb b lc nb lf nc li pf lm pg lq ph lu ma mb mc md bi translated"><a class="ae ky" href="https://www.researchgate.net/figure/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some_fig2_322179244" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/An-example-of-bootstrap-sampling-Since-objects-is-sub-sampled-with-replacement-some _ fig 2 _ 322179244</a></li></ul></div></div>    
</body>
</html>