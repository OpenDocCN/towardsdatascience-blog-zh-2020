<html>
<head>
<title>Beating the GAN Game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打甘游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beating-the-gan-game-afbcce0a20be?source=collection_archive---------39-----------------------#2020-05-15">https://towardsdatascience.com/beating-the-gan-game-afbcce0a20be?source=collection_archive---------39-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="78af" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">开发“自切片面包以来最酷的东西”的技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fc820646bec8562ddfdc6cd54194cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-SGVHATHqS1R0Fg1rnCiA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/File:Platon_Cave_Sanraedam_1604.jpg" rel="noopener ugc nofollow" target="_blank">“柏拉图的洞穴</a>”作者Jan Saenredam，之后是Cornelis Cornelisz。范哈勒姆，1604年。在公共领域下许可。</p></figure><p id="5945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成对抗网络(GANs)是深度学习中最令人着迷的发展之一。Yann LeCun自己将GANs命名为“自切片面包以来最酷的东西”[1]。</p><p id="2dbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些架构正被应用于大量的问题。在从暗物质研究[2]到室内设计[3]的应用中获得成功。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c05e01baebd5d0c88ec53b97ddab7a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*uPuBdkN-hdr8kwRtlhyP4g.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GAN-build Mona Lisa，来自<a class="ae ky" href="https://arxiv.org/pdf/1905.08233.pdf" rel="noopener ugc nofollow" target="_blank">“现实神经说话头部模型的少数镜头对抗学习”</a>，2019年论文</p></figure><p id="d6e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到如此广泛的令人着迷的应用，研究和开发GANs的愿望已经在我的脑海里发痒了很长时间。最近，我屈服了，决定是时候投入进去了。</p><p id="d566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我很快了解到甘人不容易驯服。正因为如此，我对第一次进入GANs的世界时常见的许多误解和错误有了新的体验。</p><p id="3889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在本文中总结了我从中学到的错误，这些错误可能是GAN实施和融合中成功与失败的区别。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="30bf" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">坦，不是乙状结肠</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/7bbc3f76463e4ff5131f676af5a6622c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwgUJuaVsRDiYlTYuj-8Iw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tanh v形(逻辑)激活</p></figure><p id="819a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在预处理图像数据时，我们应该将其归一化到-1和1之间，而不是0和1之间[4]。</p><p id="2d45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也意味着我们在G中的最终层激活应该是<code class="fe mw mx my mz b">tanh</code>而不是<code class="fe mw mx my mz b">sigmoid</code>。</p><h2 id="d212" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi">255</h2><p id="8bda" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">这里需要注意的另一个重要的事情是后生成，不要忘记将值乘以127.5，再加上127.5，以返回到0–255的原始范围。</p><pre class="kj kk kl km gt nr mz ns nt aw nu bi"><span id="c7f3" class="na me it mz b gy nv nw l nx ny">image_array = generated_tensor.numpy() * 127.5 + 127.5</span></pre><h1 id="ef24" class="md me it bd mf mg nz mi mj mk oa mm mn jz ob ka mp kc oc kd mr kf od kg mt mu bi translated">不要过度使用过滤器</h1><p id="eb91" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">最初，当我的DCGAN(深度卷积GAN)在亮度变化中挣扎时，我认为生成器的转置卷积层只是缺乏复杂性。</p><p id="6f9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我加了更多的滤镜。事实证明，这与我本该做的事情完全相反。</p><p id="bd9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">减少过滤器的数量可以让生成器更好地表示值的范围。</p><p id="5102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是一些过滤大量生成的MNIST数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/e4b1ae93c96a8c58b5f58ef59f7ad7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbE3zPstAVumHQfxcJempg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">发电机过滤器尺寸为:256 &gt; 128 &gt; 64 &gt; 1</p></figure><p id="fb65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在使用最少的<strong class="lb iu">过滤器:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/8f6184983c6a66d6b4bbdfa7c0065abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2JZSJo2oSYiy-2abd2YAA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">发电机过滤器尺寸为:32 &gt; 16 &gt; 8 &gt; 1(您也可以降低尺寸)</p></figure><p id="b1ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过多的过滤器会将发生器值推到极限。对于<code class="fe mw mx my mz b">tanh</code>来说，这是-1或+1。导致生成的图像缺乏说服力。</p><h1 id="6afa" class="md me it bd mf mg nz mi mj mk oa mm mn jz ob ka mp kc oc kd mr kf od kg mt mu bi translated">稀疏渐变不好</h1><p id="ad09" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">稀疏梯度本质上是弱信号。或者具有非常低的值的信号。</p><p id="059c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些低值信号的问题是，随着对它们进行许多数值运算，它们会变得越来越小。</p><p id="37c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熟悉RNNs的人无疑会遇到梯度消失的问题。这是完全一样的。</p><p id="613b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，我们可以将<strong class="lb iu">批量标准化</strong>添加到我们的网络中[5]。</p><p id="dcc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">需要注意的是</strong>，批量标准化应该发生在 s形激活(tanh，sigmoid/logistic)之后<strong class="lb iu">，非高斯激活(ReLU，LeakyReLU)之前【5】。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/f73d8b46463481e4d69ecc1957e9d422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*btNvLTk6N8N5DTiFZiDrCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">泄漏激活</p></figure><p id="ebca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，对发生器和鉴别器*[4]始终使用<strong class="lb iu">leaky relu</strong>——除了最后一层，我们使用s形激活函数。</p><p id="b671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oh">*最初的DCGAN在发生器中使用ReLU激活，在鉴别器中使用leaky ReLU[3]</em></p><h1 id="21bc" class="md me it bd mf mg nz mi mj mk oa mm mn jz ob ka mp kc oc kd mr kf od kg mt mu bi translated">学习率</h1><p id="07bb" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">有时候我们需要在鉴别器和生成器之间找到一个更好的平衡。</p><p id="6dbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初，我的鉴别器学得太快了。实质上冻结了发电机，使其无法取得任何进展。</p><p id="f163" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过降低或提高双方的学习率来平衡这一点，让我们较弱的网络有更多的喘息空间。</p><h1 id="3c09" class="md me it bd mf mg nz mi mj mk oa mm mn jz ob ka mp kc oc kd mr kf od kg mt mu bi translated">从MNIST开始</h1><p id="874d" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">我尝试做的第一件事就是创造这些复杂的艺术风格。当它不起作用时，我不知道为什么。图像太复杂了吗？我对它们的预处理正确吗？也许问题出在网络上，网络可能在任何地方。</p><p id="53f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我是从零开始构建的，所以没有其他实现可以比较和基准测试。最后，代码中出现了几个问题，但我只是通过为MNIST数据集重新构建代码来识别这些问题。</p><p id="1033" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这允许您看到您的结果和代码与其他结果和代码的不同之处，因此问题更容易诊断。一旦您的网络产生了合理的输出，您也可以将质量与其他实现进行比较。</p><p id="b7a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在开始某个很酷但很复杂的项目之前，先尝试一下MNIST(或另一个已建立的数据集)。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="7beb" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">最后一个音符</h1><p id="8b70" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">最后，甘人并不像他们第一次看到的那样狂野。有了合适的资源，构建我们的第一个GAN是一个相当简单的过程。</p><p id="aa98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是每一个错误都会提高我们的知识，并使我们更接近驯服这些众所周知的困难的架构。</p><p id="8ac7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在下面列出了我在研究和实现GANs时非常依赖的资源列表(用<strong class="lb iu">粗体</strong>)。我怎么推荐这些都不为过。</p><p id="95d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="a7e9" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">参考</h1><p id="125f" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">[1] <a class="ae ky" href="https://www.quora.com/session/Yann-LeCun/1" rel="noopener ugc nofollow" target="_blank"> Quora与Yann LeCun的会议</a> (2016)，Quora</p><p id="e673" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] M. Mustafa，D. Bard，W. Bhimji，Z. Luki，R. Al-Rfou，J. Kratochvil，<a class="ae ky" href="https://arxiv.org/pdf/1706.02390.pdf" rel="noopener ugc nofollow" target="_blank"> CosmoGAN:使用生成对抗网络创建高保真弱透镜会聚图</a> (2019)，计算天体物理学和宇宙学6:1</p><p id="b4d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【3】a .拉德福德，l .梅斯，s .钦塔拉，</strong> <a class="ae ky" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">深度卷积生成对抗网络的无监督表示学习</strong> </a> <strong class="lb iu"> (2016)，ICLR 2016 </strong></p><p id="3041" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【4】s .钦塔拉，</strong> <a class="ae ky" href="https://github.com/soumith/ganhacks" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">如何训练一个甘？</strong> </a> <strong class="lb iu">、GitHub </strong></p><p id="6841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【5】j . Brownlee，</strong> <a class="ae ky" href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">深度神经网络批量归一化的温和介绍</strong> </a> <strong class="lb iu"> (2019)，machining Learning Mastery</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="6d5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对其他机器学习项目感兴趣，你可能会喜欢我以前的项目，在那里我在复制斯多葛派哲学中使用递归神经网络进行文本生成:</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/stoic-philosophy-built-by-algorithms-9cff7b91dcbd"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">斯多葛派哲学——由算法构建</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">再现历史上最有权势的人之一所写的斯多葛派哲学</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div></div></div>    
</body>
</html>