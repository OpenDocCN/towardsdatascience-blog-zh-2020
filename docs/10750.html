<html>
<head>
<title>Convert to New Spark 3.0 UDF Style</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">转换到新的火花 3.0 UDF 风格</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convert-to-new-spark-3-0-udf-style-8d2e4cee3c0f?source=collection_archive---------40-----------------------#2020-07-27">https://towardsdatascience.com/convert-to-new-spark-3-0-udf-style-8d2e4cee3c0f?source=collection_archive---------40-----------------------#2020-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ccab" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">提示和技巧</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2236a2f6aafb5ed9a0f3fce3db749247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gJk6u9pPZmb73AIP"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伊恩·巴塔格利亚在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="dd7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Apache Spark 2.3 版本在 2017 年引入了熊猫 UDF。自从 Apache Arrow 取代 Py4J 来加速 JVM 和 Python 之间的数据传输以来，这个新特性显著提高了 Python UDFs 的性能。通过 Spark 与 Pandas 的接口，用户可以通过将 Python 库(例如<code class="fe ls lt lu lv b">sklearn</code>、<code class="fe ls lt lu lv b">scipy</code>)中编写的方法封装在 Pandas UDFs 中来轻松扩展它们。</p><p id="4460" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了原有的 Python UDF ( <code class="fe ls lt lu lv b">pyspark.sql.functions.udf</code>在 1.3 版本引入)，Spark 2.3+还有 3 种熊猫 UDF，包括<code class="fe ls lt lu lv b">PandasUDFType.SCALAR</code>、<code class="fe ls lt lu lv b">PandasUDFType.GROUPED_MAP</code>(均在 2.3.0 版本引入)，以及<code class="fe ls lt lu lv b">PandasUDFType.GROUPED_AGG</code>(2.4 版本引入，也可作为窗口函数)。2020 年 6 月，Spark 3.0 的发布为熊猫 UDF 引入了一套新的接口。在本文中，我将简要地探讨两个如何将旧样式(Pandas)UDF 转换成新样式的例子。</p><h1 id="411d" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">数据准备</h1><p id="371d" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">我模拟了一个包含以下 4 列的数据框架</p><ul class=""><li id="85fa" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><code class="fe ls lt lu lv b">name</code>:5 到 10 个字符之间的随机字符串名称</li><li id="a2aa" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe ls lt lu lv b">email</code>:随机假冒邮箱地址</li><li id="a320" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe ls lt lu lv b">secret</code>:长度为 4096 的十六进制字符串</li><li id="68fe" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe ls lt lu lv b">n</code>:重复次数。我将重复一个更小的数据帧 1000 次</li></ul><p id="e7cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，为模拟定义一些函数</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d850" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在熊猫身上模拟 10000 个样本</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="c4a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将得到如下所示的数据帧</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/dfa9b8f5109dbd73f7e1802aa1d2f761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*emvQF83l4p0c_GmoxlvEgA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">示例模拟数据</p></figure><p id="89cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，重复这个数据帧 1000 次</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="b11a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于 Spark 来说，这是一个很好的数据量</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="e7ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和最终模式</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="4236" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">从 UDF 到熊猫 UDF 迭代器</h1><p id="3849" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">最初的<code class="fe ls lt lu lv b">spark.sql.functions.udf</code>将一个定制函数应用于列中的每一项，将另一列的值作为输入。在这个例子中，我们使用<code class="fe ls lt lu lv b">withColumn</code>来存储<code class="fe ls lt lu lv b">udf</code>应用程序的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d9e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9917560b86963c7667f4099a1f09b5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*AADn3Kgnku1EpXAopC4Ijw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">应用 udf 的输出示例</p></figure><p id="4f8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 Spark 3.0 中，我们可以使用迭代器熊猫 UDF 来代替</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="b1e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迭代器使用 Python 类型作为提示，让函数知道它正在迭代一对作为输入的<code class="fe ls lt lu lv b">pandas.series</code>，并返回另一个<code class="fe ls lt lu lv b">pandas.series</code>。上面的<code class="fe ls lt lu lv b">pandas_udf</code>装饰器指定了系列元素将被返回的数据类型。请注意，输入序列的长度需要与输出序列的长度相同。嵌套函数<code class="fe ls lt lu lv b">apply_custom_mapper</code>将两列作为熊猫系列，用户需要迭代该系列的每个元素来应用函数(例如<code class="fe ls lt lu lv b">custom_mapper</code>)。然后接下来，用户需要构造迭代器来<code class="fe ls lt lu lv b">yield</code>函数应用的结果。这看起来像一个双 for 循环，但它实际上是分离输入和输出的迭代，即首先迭代输入以应用初等函数<code class="fe ls lt lu lv b">custom_mapper</code>来获得结果的迭代器，然后将结果逐步映射回输出序列。</p><h1 id="0cae" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">具有分组和应用功能的熊猫 UDF</h1><p id="ac08" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated"><code class="fe ls lt lu lv b">groupby.apply(pandas_udf)</code>模式通常用于通过迭代数据帧的每个子组来应用自定义函数。自定义<code class="fe ls lt lu lv b">pandas_udf</code>将每个组的数据作为 Pandas Dataframe(我们称之为<code class="fe ls lt lu lv b">pdf_in</code>)作为输入，并返回另一个 Pandas Dataframe 作为输出(我们称之为<code class="fe ls lt lu lv b">pdf_out</code>)。<code class="fe ls lt lu lv b">pdf_out</code>的模式不需要与<code class="fe ls lt lu lv b">pdf_in</code>的模式相同。在下面的例子中，我们将用组中列<code class="fe ls lt lu lv b">n</code>的累积和(转换为字符串)替换<code class="fe ls lt lu lv b">secret</code>列。然后我们将删除列<code class="fe ls lt lu lv b">n</code>。我们需要在<code class="fe ls lt lu lv b">panads_udf</code>装饰器中指定从<code class="fe ls lt lu lv b">df.drop("n").schema</code>获得的输出模式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="6651" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于在模式中添加新列，我们可以这样做</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d878" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述代码的输出如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d906c3fc410ca0fd9a184bb791a16442.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*4zOu1i1rAoWnuMzwVmG8Bg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">应用熊猫 UDF 的输出，后降一列</p></figure><p id="9279" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">UDF 返回的行数不必与输入的行数相同。在下面的例子中，我们修改了上面的脚本，这样我们只返回每隔一行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="9c81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新界面改用<code class="fe ls lt lu lv b">groupby.applyInPandas(python_func, schema)</code>。</p><p id="71ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个示例随后更改为</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="424c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个示例更改为</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="4ffa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，<code class="fe ls lt lu lv b">apply</code>采用了一个<code class="fe ls lt lu lv b">pandas_udf</code>函数，其中模式是在装饰器中指定的。另一方面，<code class="fe ls lt lu lv b">applyInPandas</code>采用 Python 函数，其中模式在<code class="fe ls lt lu lv b">applyInPandas</code>参数中指定。</p><h1 id="3b8a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">熊猫 UDF 的记忆使用</h1><p id="fb0c" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">最后要注意的是这些函数的内存使用情况。Spark 的熊猫 UDF 文档表明整组数据将被加载到内存中。在我们的<code class="fe ls lt lu lv b">groupby</code>示例中，我们将<code class="fe ls lt lu lv b">pdf</code>作为一个 10000 行的数据帧，因此我们期望每个执行器内核有大约 43 MB 的数据。如果每个执行器有 5 个内核。那么在内存中就变成了 215 MB 的数据。此外，我们的 UDF 的内存堆栈也增加了内存使用。假设我们将长度为 4096 的十六进制字符串转换为长度为 16384 的二进制<code class="fe ls lt lu lv b">numpy </code>布尔数组，每个数组将消耗大约 16kB 的内存，其中 10000 个数组将额外消耗 165 MB。5 个内核总共会额外增加 825 MB，也就是总共超过 1GB 的内存。因此，在涉及熊猫 UDF 的情况下调优 Spark 时，除了作为熊猫数据帧的公开数据组的大小之外，还需要注意 UDF 使用的内存量。</p></div></div>    
</body>
</html>