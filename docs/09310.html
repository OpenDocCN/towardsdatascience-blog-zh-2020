<html>
<head>
<title>Comparison and Optimization on ML Models for Loan Application Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贷款申请预测的 ML 模型比较与优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparison-and-optimization-on-ml-models-for-loan-application-prediction-675923d83d04?source=collection_archive---------30-----------------------#2020-07-03">https://towardsdatascience.com/comparison-and-optimization-on-ml-models-for-loan-application-prediction-675923d83d04?source=collection_archive---------30-----------------------#2020-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d05" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入研究常见的机器学习模型，以进行模型选择、验证和优化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0ec1e18e84111ead2ecfb4292c8362cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*6ynAuROetU8-a4hF8i_kFw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自 Pixabay 的 Img 通过<a class="ae ku" href="https://pixabay.com/photos/housebuilding-house-money-expensive-1005491/" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="1376" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在本文中，我们将使用在线贷款申请数据详细阐述 ML 模型的选择、验证和优化。<strong class="kx iu">你将学习如何创建、评估和优化 ML 模型。</strong>具体来说，我们将重点介绍<strong class="kx iu">逻辑回归、支持向量机和随机森林</strong>。它分为 7 个部分。</p><ol class=""><li id="82e7" class="lr ls it kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">商业挑战</li><li id="619f" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">数据审查</li><li id="1de4" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">电子设计自动化(Electronic Design Automation)</li><li id="d854" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">数据处理</li><li id="e9d5" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">模型结构</li><li id="f5cf" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">模型验证</li><li id="32e5" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">参数调谐</li><li id="cddb" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">外卖食品</li></ol><p id="ac27" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，让我们开始旅程🏃‍♀️🏃‍♂️.</p><p id="8d8c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 1。商业挑战</strong></p><p id="8079" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们被一家贷款公司委派预测优质申请人的任务。这项工作是开发一个模型，通过分析申请过程中输入的申请人数据来预测申请人的兴趣。如果申请人感兴趣，他或她将对产品进行电子签名，否则不会。</p><p id="1b78" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 2。数据回顾</strong></p><p id="342d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">快速查看视频中显示的数据，您会发现有 21 列 17，908 行。有了这么多特性，让我们为每个变量的解释创建一个摘要，如图 1 所示，以增强我们的理解。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf mg l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">视频原始数据的简要视图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/a4354ca4599e970726ffea8667b1db41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMHXDYlfvOrjcUUQ8pT4hQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 1 变量定义汇总</p></figure><p id="03c3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 3。EDA </strong></p><p id="ec5e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">EDA 是任何数据处理之前必须要做的一步。通常，它包括数据清理、数据可视化和相关性分析。这里我就不细说 EDA 了。如果你想熟悉 EDA 的步骤，请随意阅读这篇<a class="ae ku" rel="noopener" target="_blank" href="/exploratory-data-analysis-on-mobile-app-behavior-data-2777fc937973">文章</a>。</p><p id="b438" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 4。数据处理</strong></p><p id="c087" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">1)数据工程</p><p id="53c5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在 EDA 数据可视化过程中，您可能会发现，对于变量'<em class="mm"> months_employed </em>'，大多数申请人的就业时间为 0 个月。这似乎不正确，所以我们将删除这个不可靠的列。</p><p id="189c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第二，可以将两列'<em class="mm">个人账户 m' </em>和'<em class="mm">个人账户 y '，</em>组合成一个新的变量来表示申请人账户的总月数。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="f6b6" class="ms mt it mo b gy mu mv l mw mx">dataset = dataset.drop(columns = [‘months_employed’])</span><span id="285f" class="ms mt it mo b gy my mv l mw mx">dataset[‘personal_account_months’] = (dataset.personal_account_m + (dataset.personal_account_y * 12))</span><span id="f83a" class="ms mt it mo b gy my mv l mw mx">dataset = dataset.drop(columns = [‘personal_account_m’, ‘personal_account_y’])</span></pre><p id="7db0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2)一键编码</p><p id="438e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们需要将分类变量转换成数字变量。只有一个分类变量'<em class="mm"> pay_schedule' </em>。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="e8e3" class="ms mt it mo b gy mu mv l mw mx">dataset = pd.get_dummies(dataset)</span></pre><p id="2e0e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">以上将<em class="mm">'付薪时间表'</em>转换为 4 列，<em class="mm">付薪时间表 _ 双周'</em>，<em class="mm">'付薪时间表 _ 月'，</em>付薪时间表 _ 半月'，<em class="mm">付薪时间表 _ 周'</em>。因为每一列都是互斥的，所以我们必须删除一列，使它们线性独立。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="b720" class="ms mt it mo b gy mu mv l mw mx">dataset = dataset.drop(columns = [‘pay_schedule_semi-monthly’])</span></pre><p id="8189" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3)数据分割</p><p id="4ceb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了给模型准备好数据，我们将自变量和响应变量分成测试集和训练集。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="2e8a" class="ms mt it mo b gy mu mv l mw mx">response = dataset[“e_signed”]<br/>users = dataset[‘entry_id’]<br/>dataset = dataset.drop(columns = [“e_signed”, “entry_id”])</span><span id="abe8" class="ms mt it mo b gy my mv l mw mx">from sklearn.model_selection import train_test_split</span><span id="3c76" class="ms mt it mo b gy my mv l mw mx">X_train, X_test, y_train, y_test = train_test_split(dataset, response,test_size = 0.2, random_state = 0)</span></pre><p id="64bc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4)特征缩放</p><p id="0125" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们需要缩放自变量。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="8676" class="ms mt it mo b gy mu mv l mw mx">from sklearn.preprocessing import StandardScaler</span><span id="b05e" class="ms mt it mo b gy my mv l mw mx">sc_X = StandardScaler()<br/>X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))<br/>X_test2 = pd.DataFrame(sc_X.transform(X_test))<br/>X_train2.columns = X_train.columns.values<br/>X_test2.columns = X_test.columns.values<br/>X_train2.index = X_train.index.values<br/>X_test2.index = X_test.index.values<br/>X_train = X_train2<br/>X_test = X_test2</span></pre><p id="b620" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">注意，从 scaler 得到的数据是一个 Numpy 数组，我们将它转换为 Dataframe 以保留索引和列名。</strong>同样，我们用<em class="mm"> X_train </em>训练定标器，用训练好的定标器直接转换测试数据。</p><p id="1638" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 5。模型构建</strong></p><p id="7da3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，让我们试验不同的模型，找出最好的一个。</p><h2 id="fafb" class="ms mt it bd mz na nb dn nc nd ne dp nf le ng nh ni li nj nk nl lm nm nn no np bi translated">1)逻辑回归</h2><p id="5cf5" class="pw-post-body-paragraph kv kw it kx b ky nq ju la lb nr jx ld le ns lg lh li nt lk ll lm nu lo lp lq im bi translated">让我们先创建一个线性模型。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ec07" class="ms mt it mo b gy mu mv l mw mx">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0, penalty = ‘l1’)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span></pre><p id="964d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上面的代码块将对逻辑回归模型进行定型和测试。注意我们使用了<strong class="kx iu"> L1 套索正则化</strong>来惩罚模型的系数。如果你想了解更多的细节，请查看这篇关于正规化的<a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-how-to-on-app-behavior-data-8a95802a988f">文章</a>。</p><p id="058d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终我们得到了 56%的准确率，57%的准确率，70%的召回率。<strong class="kx iu">这意味着在所有积极的预测中，只有 57%是正确的，而这些正确的预测占实际预测的 70%😆。</strong></p><h2 id="ff5f" class="ms mt it bd mz na nb dn nc nd ne dp nf le ng nh ni li nj nk nl lm nm nn no np bi translated">2) SVM(线性核)</h2><p id="4124" class="pw-post-body-paragraph kv kw it kx b ky nq ju la lb nr jx ld le ns lg lh li nt lk ll lm nu lo lp lq im bi translated">让我们建立一个基于支持向量机的分类器。</p><blockquote class="nv nw nx"><p id="6ea8" class="kv kw mm kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">SVM 的一个关键概念是内核。<strong class="kx iu">使用核进行数据转换，使得当前数据空间中不可分的数据可以在更高维空间中分离。</strong>它接收数据作为输入，并根据选择的内核将其转换成所需的形式。显然，选择正确的内核至关重要，因为错误的转换会使模型表现不佳。如果你想深入了解 SVM 的细节，请随意阅读这篇<a class="ae ku" rel="noopener" target="_blank" href="/svm-and-kernel-svm-fed02bef1200">文章</a>和这篇<a class="ae ku" rel="noopener" target="_blank" href="/support-vector-machines-svm-c9ef22815589">文章</a>。</p></blockquote><p id="c8e8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，让我们训练一个线性核 SVM。通常，当数据集是线性可分的，即使用单线可分时，使用线性核。它比其他任何内核都快。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ba26" class="ms mt it mo b gy mu mv l mw mx">from sklearn.svm import SVC<br/>classifier = SVC(random_state = 0, kernel = ‘linear’)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span></pre><p id="a392" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终我们得到了 57%的准确率，58%的准确率，73%的召回率。与逻辑回归模型相比，这是一个稍好的性能🤪。</p><h2 id="f308" class="ms mt it bd mz na nb dn nc nd ne dp nf le ng nh ni li nj nk nl lm nm nn no np bi translated">3) SVM 银行</h2><blockquote class="nv nw nx"><p id="01aa" class="kv kw mm kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">径向基函数是 SVM 中使用的非线性核。与线性核相比，RBF 不是一个参数模型，其复杂性随着数据的大小而增加。因此，训练模型以及进行预测的成本更高。此外，超参数越多，复杂模型越容易过度拟合。</p></blockquote><p id="24b5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">具体来说，要实现 RBF，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="812c" class="ms mt it mo b gy mu mv l mw mx">classifier = SVC(random_state = 0, kernel = ‘rbf’)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span></pre><p id="0c77" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">同样，经过训练和测试，我们获得了 59%准确率、60%的精确度和 69%的召回率。与线性核相比没有太大的改进😒。</p><p id="adda" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4)随机森林</p><blockquote class="nv nw nx"><p id="77f3" class="kv kw mm kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated"><strong class="kx iu">随机森林通过构建大量个体决策树并输出分类问题的模式类或回归问题的均值预测来运行。</strong>运行许多决策树的潜在逻辑是，<em class="it">最优决策来自群体的智慧</em>，简单而强大。</p></blockquote><p id="5966" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了实现 RF，具体地说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="e43d" class="ms mt it mo b gy mu mv l mw mx">from sklearn.ensemble import RandomForestClassifier<br/>classifier = RandomForestClassifier(random_state = 0, n_estimators = 100, criterion = ‘entropy’)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span></pre><p id="ccd3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如上所述，我们创建了 100 棵决策树。最终我们得到了 62%的准确率，64%的准确率，67%的召回率。总结图 2 中的所有模型性能，我们发现 RF 的性能最佳😁。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c57977d0687888ac7e166df4e606a144.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*HOyy1rLm7U-pHa5Ir7gVpQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 2 模型性能总结</p></figure><p id="e548" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 6。模型验证</strong></p><p id="831f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在优化随机森林模型之前，我们先验证一下它的性能。这里，我们使用训练数据应用 K 重交叉验证。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="b267" class="ms mt it mo b gy mu mv l mw mx">from sklearn.model_selection import cross_val_score<br/>accuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train, cv = 10)</span></pre><p id="31bb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们得到了 63%的准确率和 3%的标准偏差。这表明随机森林模型对于训练集一直表现良好😎。</p><p id="508f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 7。参数调谐</strong></p><p id="e317" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因为我们知道随机森林与其他模型相比性能最佳，所以让我们通过使用参数调整来优化随机森林模型。</p><p id="5bae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">随机森林分类器有几个参数，包括:</p><ul class=""><li id="1120" class="lr ls it kx b ky kz lb lc le lt li lu lm lv lq oc lx ly lz bi translated"><em class="mm">e _ estimators】:</em>树的数量</li><li id="a17e" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated"><strong class="kx iu"> <em class="mm">判据</em> </strong>”:衡量分割质量的判据</li><li id="13c5" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated">"<strong class="kx iu"> <em class="mm"> max_depth </em> </strong>":每棵树的最大层数</li><li id="c559" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated">"<strong class="kx iu"><em class="mm">min _ samples _ split</em></strong>":拆分一个节点所需的最小样本数</li><li id="e372" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated">"<strong class="kx iu"> <em class="mm"> max_features </em> </strong>":寻找最佳分割时要考虑的最大特征数</li><li id="a229" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated">"<strong class="kx iu"> min_samples_leaf </strong>":一个叶节点需要的最小样本数</li><li id="0015" class="lr ls it kx b ky ma lb mb le mc li md lm me lq oc lx ly lz bi translated">"<strong class="kx iu"> <em class="mm"> bootstrap </em> </strong>":数据点采样的一种方法(有替换或无替换)</li></ul><p id="fbda" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这里，我们选择下面的参数，并为每个参数设置搜索范围。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ff6c" class="ms mt it mo b gy mu mv l mw mx">parameters = {“<strong class="mo iu">max_depth</strong>”: [3, None],“<strong class="mo iu">max_features</strong>”: [1, 5, 10],‘<strong class="mo iu">min_samples_split</strong>’: [2, 5, 10],‘<strong class="mo iu">min_samples_leaf</strong>’: [1, 5, 10],“<strong class="mo iu">bootstrap</strong>”: [True, False],“<strong class="mo iu">criterion</strong>”: [“entropy”]}</span></pre><p id="541b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们使用<strong class="kx iu">网格搜索</strong>来遍历所有不同的参数组合以及每个组合的交叉验证。我们也可以使用<strong class="kx iu">随机搜索</strong>。具体来说，</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="c9ad" class="ms mt it mo b gy mu mv l mw mx">from sklearn.model_selection import GridSearchCV</span><span id="c581" class="ms mt it mo b gy my mv l mw mx">grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = “accuracy”, cv = 10, n_jobs = -1)</span><span id="2ece" class="ms mt it mo b gy my mv l mw mx">grid_search = grid_search.fit(X_train, y_train)</span></pre><p id="c0af" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意，我们使用 10 重交叉验证。最后，网格搜索发现最佳参数如下图 3 所示，准确率为 63%😃。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c163ea2093e8e4acc59d67d478ef06b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*Gd0YUucF4HQVZnw_JRaIhg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 3 使用“熵”的网格搜索第一轮结果</p></figure><p id="a5d4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这样，我们可以使用这些参数在测试数据集上测试模型，或者执行第二轮网格搜索来微调参数。如果你想要第二轮，试试下面的参数。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="3097" class="ms mt it mo b gy mu mv l mw mx">parameters = {“max_depth”: [None], “max_features”: [3, 5, 7], ‘min_samples_split’: [8, 10, 12],‘min_samples_leaf’: [1, 2, 3], “bootstrap”: [True], “criterion”: [“entropy”]}</span></pre><p id="83c0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终，在相同的参数下，我们得到了同样的 63%的准确率。这意味着我们从第一轮得到的结果已经是最佳的了👌。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oe"><img src="../Images/d28d8078e4e2a796a56fcbf1dce90670.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*cOuX4xP1FhtdzS4P1FoIYw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 4 使用“熵”的网格搜索第二轮结果</p></figure><p id="618f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，让我们在测试数据上使用优化的模型。最后，我们得到了图 5 中的结果。巨大的成果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c3071ab21240cd830d8c6b2a74d3121e.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*2KtkhWkj9jED6DgtEH8-jw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 5 基于熵优化射频的模型性能总结</p></figure><p id="7ae6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意上面我们使用<em class="mm">熵</em>作为分裂标准。你也可以用'<em class="mm">基尼'</em>。<strong class="kx iu">熵和基尼系数是衡量一个节点不纯程度的指标。有多个类的节点是不纯的，而只有一个类的节点是纯的。一般来说，决策树是通过递归分割变量或响应变量的特征来工作的。它旨在通过最大化每个节点的纯度来优化每个拆分。<strong class="kx iu">熵旨在最大化每次分裂所获得的信息内容，并且期望从分裂节点获得的信息越多，分裂就越好。另一方面，基尼系数衡量的是贴错标签的可能性。</strong>根据使用的标准，决策树结果可能会有所不同。如果你需要更多关于这个话题的细节，请随意阅读这篇文章。</strong></p><p id="e164" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在让我们使用“G <em class="mm"> ini </em>作为标准执行相同的网格搜索，保持其余参数不变。第一轮之后，我们获得了 64%的准确率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8bfe57f6f7f05c7417bc0649e05015ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*yb6kSWNMN54iFRK2KQO0rA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 6 使用“基尼”的网格搜索第一轮结果</p></figure><p id="c560" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果我们执行第二轮来微调上述参数，我们使用下面的最佳参数再次获得了 64%的准确度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/494226db02f03c1cc3114d6af2d1de03.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*aPTf8ShXv9MSF_VjIM0uRQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 7 使用“基尼”的网格搜索第二轮结果</p></figure><p id="1390" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有了微调的参数，我们来测试模型。如图 8 所示，基于熵和基尼的模型之间存在微小的性能差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3854cadacfab39cb003c360af946603e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*mMXVT1hqOkhxe8vP5IaX6g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 8 基于基尼系数优化射频的模型性能总结</p></figure><p id="b5fa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> 8。外卖</strong></p><blockquote class="nv nw nx"><p id="79b9" class="kv kw mm kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">我们使用相同的数据测试了 4 个不同的模型，逻辑回归、SVM(线性)、SVM (RBF)和随机森林。随机森林被证明具有 63%的准确率。使用网格搜索，我们将模型性能提高了 1–3%。</p></blockquote><p id="eb40" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从商业角度来说，利用该模型的一种方法是针对那些预测不会签署定制入职流程贷款的人。这将有助于公司赢得更多客户，改善业务。</p><p id="82ac" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">太好了！如果你觉得这篇文章有帮助，请随意点击👏s！如果您需要源代码，请随时访问我的</strong><a class="ae ku" href="https://github.com/luke4u/Customer_Behaviour_Prediction/tree/main/Loan-prediction" rel="noopener ugc nofollow" target="_blank"><strong class="kx iu">Github</strong></a><strong class="kx iu">页面🤞🤞🤞。</strong></p></div></div>    
</body>
</html>