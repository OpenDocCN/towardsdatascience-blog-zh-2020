<html>
<head>
<title>Multi Class Text Classification With Deep Learning Using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 BERT 深度学习的多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613?source=collection_archive---------0-----------------------#2020-08-02">https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613?source=collection_archive---------0-----------------------#2020-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/cae587e112414e50da2829104ff6b2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vO75HqnqS4bz_hJqBmLEtA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来源:Pexels</p></figure><div class=""/><div class=""><h2 id="d34d" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">自然语言处理，自然语言处理，拥抱脸</h2></div><p id="b5d4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">大多数研究人员将他们的研究论文提交给学术会议，因为这是一种更快捷的公布结果的方式。寻找和选择一个合适的会议一直具有挑战性，尤其是对年轻的研究人员来说。</p><p id="c1b2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然而，根据以前的会议进展数据，研究人员可以增加论文被接受和发表的机会。我们将尝试使用<a class="ae lq" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank"> BERT </a>用深度学习解决这个文本分类问题。</p><p id="9b85" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">几乎所有的代码都取自这个<a class="ae lq" href="https://www.coursera.org/projects/sentiment-analysis-bert" rel="noopener ugc nofollow" target="_blank">教程</a>，唯一的区别就是数据。</p><h1 id="3819" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">数据</h1><p id="2cff" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv" rel="noopener ugc nofollow" target="_blank">数据集</a>包含 2507 篇研究论文标题，已被人工分类为 5 类(即会议)，可从<a class="ae lq" href="https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv" rel="noopener ugc nofollow" target="_blank">此处</a>下载。</p><h2 id="1dea" class="mo ls jf bd lt mp mq dn lx mr ms dp mb ld mt mu md lh mv mw mf ll mx my mh mz bi translated">探索和预处理</h2><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">conf _ explorer . py</p></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5576e42a48a22562b06819321f2959be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*hTqhwvVGJtl-z8FZ7PFCPA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表 1</p></figure><pre class="na nb nc nd gt nh ni nj nk aw nl bi"><span id="1468" class="mo ls jf ni b gy nm nn l no np">df['Conference'].value_counts()</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/87e9410512882fcbbafdad8cb4c8eb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*BKjZItVJH0E3KZgS-ZQqRg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图 1</p></figure><p id="1ee8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">你可能已经注意到了我们的班级不平衡，我们将在稍后解决这个问题。</p><h2 id="448d" class="mo ls jf bd lt mp mq dn lx mr ms dp mb ld mt mu md lh mv mw mf ll mx my mh mz bi translated">给标签编码</h2><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标签 _ 编码. py</p></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/c4b016c312e16d54c46154362f28e046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*focrY1e8cFIvGCh_w9kqGw.png"/></div></div></figure><pre class="na nb nc nd gt nh ni nj nk aw nl bi"><span id="15e9" class="mo ls jf ni b gy nm nn l no np">df['label'] = df.Conference.replace(label_dict)</span></pre><h1 id="5a37" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">培训和验证分割</h1><p id="39f7" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">因为标签是不平衡的，我们以分层的方式分割数据集，用它作为类别标签。</p><p id="8de7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">拆分后，我们的标签分布将如下所示。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练 _ 测试 _ 拆分. py</p></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/37cc1521153d950b87fa4ae9c3e3268c.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*G9GE1COQ4aKTlTB2zera6A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图 2</p></figure><h1 id="2809" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">BertTokenizer 和编码数据</h1><p id="84d8" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="https://huggingface.co/transformers/main_classes/tokenizer.html" rel="noopener ugc nofollow" target="_blank">记号化</a>是将原始文本分割成记号的过程，记号是表示单词的数字数据。</p><ul class=""><li id="805a" class="nt nu jf kw b kx ky la lb ld nv lh nw ll nx lp ny nz oa ob bi translated">构造一个<a class="ae lq" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank"> BERT 记号化器</a>。基于文字部分。</li><li id="2986" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">实例化一个预训练的 BERT 模型配置来编码我们的数据。</li><li id="503c" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">为了将所有的标题从文本转换成编码形式，我们使用了一个名为<code class="fe oh oi oj ni b">batch_encode_plus</code>的函数，我们将分别处理训练和验证数据。</li><li id="7d6e" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">上述函数中的第一个参数是标题文本。</li><li id="fac4" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated"><code class="fe oh oi oj ni b">add_special_tokens=True</code>表示序列将使用与其模型相关的特殊标记进行编码。</li><li id="4f3a" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">当将序列批处理在一起时，我们设置了<code class="fe oh oi oj ni b">return_attention_mask=True</code>，因此它将根据由<code class="fe oh oi oj ni b">max_length</code>属性定义的特定标记器返回注意掩码。</li><li id="057d" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我们还想填充所有的标题到一定的最大长度。</li><li id="ee6e" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我们实际上不需要设置<code class="fe oh oi oj ni b">max_length=256</code>，只是为了安全起见。</li><li id="2af4" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated"><code class="fe oh oi oj ni b">return_tensors='pt'</code>返回 PyTorch。</li><li id="deaf" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">然后我们需要将数据拆分成<code class="fe oh oi oj ni b">input_ids</code>、<code class="fe oh oi oj ni b">attention_masks</code>和<code class="fe oh oi oj ni b">labels</code>。</li><li id="9264" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">最后，在我们得到编码数据集之后，我们可以创建训练数据和验证数据。</li></ul><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">tokenizer_encoding.py</p></figure><h1 id="5a67" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">BERT 预训练模型</h1><p id="2143" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">我们将每个标题视为其唯一的序列，因此一个序列将被归类到五个标签之一(即会议)。</p><ul class=""><li id="7835" class="nt nu jf kw b kx ky la lb ld nv lh nw ll nx lp ny nz oa ob bi translated"><code class="fe oh oi oj ni b">bert-base-uncased</code>是一个较小的预训练模型。</li><li id="3b85" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">用<code class="fe oh oi oj ni b">num_labels</code>表示输出标签的数量。</li><li id="3903" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我们其实并不关心<code class="fe oh oi oj ni b">output_attentions</code>。</li><li id="b362" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我们也不需要<code class="fe oh oi oj ni b">output_hidden_states</code>。</li></ul><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">BERT_pretrained_model.py</p></figure><h1 id="c691" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">数据加载器</h1><ul class=""><li id="6c61" class="nt nu jf kw b kx mj la mk ld ok lh ol ll om lp ny nz oa ob bi translated"><code class="fe oh oi oj ni b">DataLoader</code>组合数据集和采样器，并在给定的数据集上提供可迭代的。</li><li id="5b99" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我们使用<code class="fe oh oi oj ni b">RandomSampler</code>进行训练，使用<code class="fe oh oi oj ni b">SequentialSampler</code>进行验证。</li><li id="a817" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">鉴于我的环境内存有限，我设置了<code class="fe oh oi oj ni b">batch_size=3</code>。</li></ul><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">data_loaders.py</p></figure><h1 id="bdfe" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">优化器和调度器</h1><ul class=""><li id="f9f7" class="nt nu jf kw b kx mj la mk ld ok lh ol ll om lp ny nz oa ob bi translated">要构建一个优化器，我们必须给它一个包含要优化的参数的 iterable。然后，我们可以指定特定于优化器的选项，如学习率、ε等。</li><li id="836a" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">我发现<code class="fe oh oi oj ni b">epochs=5</code>很适合这个数据集。</li><li id="105e" class="nt nu jf kw b kx oc la od ld oe lh of ll og lp ny nz oa ob bi translated">创建一个学习率从优化程序中设置的初始学习率线性降低到 0 的调度，在此期间，学习率从 0 线性增加到优化程序中设置的初始学习率。</li></ul><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">优化程序 _ 调度程序. py</p></figure><h1 id="5cfe" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">性能指标</h1><p id="1d27" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">我们将使用 f1 分数和每节课的准确度作为性能指标。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">performance_metrics.py</p></figure><h1 id="48ea" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">训练循环</h1><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">培训 _loop.py</p></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/69fa45d8f105e1f557250794f29bd943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*8unFwxDZTMgpPUUDH4XbPA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图 3</p></figure><h1 id="1375" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">加载和评估模型</h1><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">loading_evaluating.py</p></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/433e75ece8a3d42b302b179cbc1d8427.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*r_aRhmjPGW_cz_yiPdE-dw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图 4</p></figure><p id="63e3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Text_Classification_With_BERT.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Text_Classification_With_BERT.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受余下的周末吧！</p></div></div>    
</body>
</html>