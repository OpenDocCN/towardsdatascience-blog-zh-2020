<html>
<head>
<title>Sentiment classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的情感分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-classification-in-python-da31833da01b?source=collection_archive---------10-----------------------#2020-09-12">https://towardsdatascience.com/sentiment-classification-in-python-da31833da01b?source=collection_archive---------10-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="31fe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 VADER 和 TextBlob 进行情感分析，使用 scikit-learn 进行监督文本分类</h2></div><p id="8890" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章是构建情感分类器的三篇连续文章中的最后一篇。做了一些<a class="ae le" rel="noopener" target="_blank" href="/exploratory-text-analysis-in-python-8cf42b758d9e">探索性文本分析</a>和<a class="ae le" rel="noopener" target="_blank" href="/preprocessing-text-in-python-923828c4114f">预处理文本</a>之后，是时候将评论分类为情绪了。在这篇文章中，我们将首先看看两种不用建立模型就能获得情感的方法，然后建立一个定制模型。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/ac22805965e89381a9356617f4d95a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*29rMZ77wiiW861Ui"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@artbyhybrid?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">混合</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d294" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始之前，让我们后退一步，快速地看一下更大的画面。<a class="ae le" href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome" rel="noopener ugc nofollow" target="_blank"> CRISP-DM </a>方法概述了成功的数据科学项目的流程。在本帖中，我们将做一些数据科学家在<strong class="kk iu">建模</strong>阶段会经历的任务。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*qvgzhs_qixhx0SRCDN5-jg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">CRISP-DM 工艺流程摘录</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="6d7c" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">0.Python 设置</h1><p id="a298" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">这篇文章假设读者(👀是的，你！)可以访问并熟悉 Python，包括安装包、定义函数和其他基本任务。如果您是 Python 的新手，<a class="ae le" href="https://www.python.org/about/gettingstarted/" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的入门地方。</p><p id="fa10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在 Jupyter 笔记本里测试过 Python 3.7.1 的脚本。</p><p id="a567" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在开始之前确保您已经安装了以下库:<br/> ◼️ <strong class="kk iu">数据操作/分析:</strong> <em class="na"> numpy，pandas <br/> </em> ◼️ <strong class="kk iu">数据分区:</strong><em class="na">sk learn<br/></em>◼️<strong class="kk iu">文本预处理/分析:</strong> <em class="na"> nltk，textblob </em> <br/> ◼️ <strong class="kk iu">可视化:</strong> <em class="na"> matplotlib，seaborn </em></p><p id="657e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你安装了<em class="na"> nltk </em>，请确保你已经从<em class="na"> nltk </em>下载了<em class="na">【停用词】</em><em class="na">【wordnet】</em>和<em class="na">【Vader _ lexicon】</em>，脚本如下:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="926f" class="ng me it nc b gy nh ni l nj nk">import nltk<br/>nltk.download('stopwords') <br/>nltk.download('wordnet')<br/>nltk.download('vader_lexicon')</span></pre><p id="fe3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你已经下载了，运行这个会通知你。</p><p id="ece1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们准备好导入包了:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="ee2a" class="ng me it nc b gy nh ni l nj nk"># Set random seed<br/>seed = 123</span><span id="c1ba" class="ng me it nc b gy nl ni l nj nk"># Data manipulation/analysis<br/>import numpy as np<br/>import pandas as pd</span><span id="c298" class="ng me it nc b gy nl ni l nj nk"># Text preprocessing/analysis<br/>import re<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer<br/>from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>from textblob import TextBlob<br/>from scipy.sparse import hstack, csr_matrix<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="ac0c" class="ng me it nc b gy nl ni l nj nk"># Modelling<br/>from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV<br/>from sklearn.linear_model import LogisticRegression, SGDClassifier<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.metrics import classification_report, confusion_matrix<br/>from sklearn.pipeline import Pipeline</span><span id="520c" class="ng me it nc b gy nl ni l nj nk"># Visualisation<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline<br/>sns.set(style="whitegrid", context='talk')</span></pre><h1 id="b31f" class="md me it bd mf mg nm mi mj mk nn mm mn jz no ka mp kc np kd mr kf nq kg mt mu bi translated">1.数据📦</h1><p id="ea4f" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我们将使用 IMDB 电影评论数据集。您可以在这里下载数据集<a class="ae le" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">，并将其保存在您的工作目录中。保存后，让我们将其导入 Python:</a></p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="9c2e" class="ng me it nc b gy nh ni l nj nk">sample = pd.read_csv('IMDB Dataset.csv')<br/>print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")<br/>sample.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6b23a671ee90c83029239e074f5dfca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*PHe-BswQXmJWcDyd9CHtLA.png"/></div></figure><p id="bd69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看情绪之间的分歧:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="8a02" class="ng me it nc b gy nh ni l nj nk">sample['sentiment'].value_counts()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5e5c06fc380f7ac8acfe9701d65dedac.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*e3f7OlQu1JOKN3FCw-HBMA.png"/></div></figure><p id="87c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在样本数据中，情感是平均分配的。让我们将目标编码成数字值，其中正数为 1，负数为 0:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="30cc" class="ng me it nc b gy nh ni l nj nk"># Encode to numeric<br/>sample['target'] = np.where(sample['sentiment']=='positive', 1, 0)</span><span id="14d4" class="ng me it nc b gy nl ni l nj nk"># Check values<br/>sample.groupby(['sentiment', 'target']).count().unstack()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/986b5b6a0665e784ca4af419c1882fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*_o8-6fAXAEddsI5jDCz3wQ.png"/></div></figure><p id="d7fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们留出 5000 个案例进行测试:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="1dc4" class="ng me it nc b gy nh ni l nj nk"># Split data into train &amp; test<br/>X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, <br/>                                                    stratify=sample['sentiment'])</span><span id="21aa" class="ng me it nc b gy nl ni l nj nk"># Append sentiment back using indices<br/>train = pd.concat([X_train, y_train], axis=1)<br/>test = pd.concat([X_test, y_test], axis=1)</span><span id="62d8" class="ng me it nc b gy nl ni l nj nk"># Check dimensions<br/>print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")<br/>print(f"{train['sentiment'].value_counts()}\n")</span><span id="2e91" class="ng me it nc b gy nl ni l nj nk">print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")<br/>print(test['sentiment'].value_counts())</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bed36a246927452e55efe8cab219564f.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*0vDtVCpnqFtdFs7lvTIr5A.png"/></div></figure><p id="2544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将快速检查训练数据集的头部:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="318d" class="ng me it nc b gy nh ni l nj nk">train.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/82cbcc4362d2f190d2edb80a4266416c.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*fWrQ9vPZgj7uTTKS2Fl8sQ.png"/></div></figure><p id="ccba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧，我们开始吧！🐳</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="2266" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">1.情感分析💛</h1><p id="168f" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在这一节中，我想向您展示两种非常简单的方法来获得情感，而无需构建自定义模型。我们将用<em class="na"> VADER </em>和<em class="na">文本块</em>提取极性强度分数。</p><h2 id="07d0" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">1.1.VADER 的情感分析</h2><blockquote class="oh oi oj"><p id="47f3" class="ki kj na kk b kl km ju kn ko kp jx kq ok ks kt ku ol kw kx ky om la lb lc ld im bi translated">“VADER (Valence Aware 字典和情感推理器)是一个基于词典和规则的情感分析工具，专门针对社交媒体中表达的情感。”</p></blockquote><p id="c484" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从一个简单的例子开始，看看我们如何使用<em class="na"> VADER </em>情感分析器提取情感强度分数:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="76e3" class="ng me it nc b gy nh ni l nj nk">example = 'The movie was awesome.'<br/>sid = SentimentIntensityAnalyzer()<br/>sid.polarity_scores(example)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/639049cd63ae4b021f48f8b4eb2ff2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*9yjnJzdvgeI6z8jpEWASzA.png"/></div></figure><blockquote class="oh oi oj"><p id="08c4" class="ki kj na kk b kl km ju kn ko kp jx kq ok ks kt ku ol kw kx ky om la lb lc ld im bi translated"><a class="ae le" href="https://github.com/cjhutto/vaderSentiment" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="it"> neg，neu，pos: </em> </strong> <em class="it">这三个分数相加为 1。这些分数显示了属于该类别的文本的比例。<br/> </em> <strong class="kk iu"> <em class="it">复合:</em> </strong> <em class="it">这个分数范围从-1(最负)到 1(最正。</em>T24】</a></p></blockquote><p id="8e65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然不是所有的评论都像我们手头的例子一样简单，但很高兴看到例子评论的分数看起来大多是正面的。现在，让我们将强度分数添加到训练数据中:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="72be" class="ng me it nc b gy nh ni l nj nk">train[['neg', 'neu', 'pos', 'compound']] = train['review'].apply(sid.polarity_scores).apply(pd.Series)<br/>train.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oo"><img src="../Images/fc86ac61eff74116216f9fd52dcbaf25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFUNsTNBemTHPq6UQPtC4A.png"/></div></div></figure><p id="8a3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们初始化了分析器对象，获得情感分数只需要一行代码。我们要进一步检查分数吗？让我们从得分最高的 5 条记录开始:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="722c" class="ng me it nc b gy nh ni l nj nk">train.nlargest(5, ['pos'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi op"><img src="../Images/3bede19fd7b9b34af4869f795b49e1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nJsK_rGH1H5Y-dVwi3a_Q.png"/></div></div></figure><p id="fcb4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很高兴看到所有的评论都是正面的。让我们为<em class="na">阴性</em>做同样的事情:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="3e8e" class="ng me it nc b gy nh ni l nj nk">train.nlargest(5, ['neg'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi op"><img src="../Images/c19ec41e170de76a57dd4603a0cfa1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9insjQnIZadpys-iRDoLA.png"/></div></div></figure><p id="5280" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个看起来也不错。但我们可能会看到数据的极端，那里的情绪更加明显。让我们用直方图直观显示分数，以便更好地理解:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="cf2a" class="ng me it nc b gy nh ni l nj nk">for var in ['pos', 'neg', 'neu', 'compound']:<br/>    plt.figure(figsize=(12,4))<br/>    sns.distplot(train.query("target==1")[var], bins=30, kde=False, <br/>                 color='green', label='Positive')<br/>    sns.distplot(train.query("target==0")[var], bins=30, kde=False, <br/>                 color='red', label='Negative')<br/>    plt.legend()<br/>    plt.title(f'Histogram of {var} by true sentiment');</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oq"><img src="../Images/c8dba99f7cea373ef92d12e2679dd288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAMG_t6QHvN5-N6DM6u4SQ.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi or"><img src="../Images/ad256d5f1ce725fd3a27d06b610932a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0Bq548egMlNd21TdlUrqA.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi os"><img src="../Images/06fa365b74cba695b41e833d66e626df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqOM1oxKjwb5wIdtj_D2Ag.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ot"><img src="../Images/765183de3297673e920d423422a3a54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmv-G_DKykyleh9i_-RJ2A.png"/></div></div></figure><p id="9c52" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从直方图来看，似乎<em class="na"> pos、neg </em>和潜在的<em class="na"> compound </em>列在对积极和消极情绪进行分类时是有用的。我们可以使用这些分数快速地将每个评论分为正面或负面类别。让我们看看它会做得多好:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="aaa7" class="ng me it nc b gy nh ni l nj nk">train['vader_polarity'] = np.where(train['pos']&gt;train['neg'], 1, 0)<br/>target_names=['negative', 'positive']<br/>print(classification_report(train['target'], <br/>                            train['vader_polarity'], <br/>                            target_names=target_names))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c6436164a5dd9042d12020b32d9be151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*jmtgwaADH1yvX967toh62g.png"/></div></figure><p id="bc85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<em class="na"> VADER </em>，我们可以不费吹灰之力获得大约 69%的准确率。不过，正面和负面评论的表现看起来有所不同。我们对正面评论的召回率更高，准确率更低——这意味着我们有更多的误报(看到我在那里做了什么吗？你明白我为什么把正面评价编码为 1 了吗？🙊).让我们看看混淆矩阵:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="b965" class="ng me it nc b gy nh ni l nj nk"># Create function so that we could reuse later<br/>def plot_cm(y_test, y_pred, target_names=['negative', 'positive'], <br/>            figsize=(5,3)):<br/>    """Create a labelled confusion matrix plot."""<br/>    cm = confusion_matrix(y_test, y_pred)<br/>    fig, ax = plt.subplots(figsize=figsize)<br/>    sns.heatmap(cm, annot=True, fmt='g', cmap='BuGn', cbar=False, <br/>                ax=ax)<br/>    ax.set_title('Confusion matrix')<br/>    ax.set_xlabel('Predicted')<br/>    ax.set_xticklabels(target_names)<br/>    ax.set_ylabel('Actual')<br/>    ax.set_yticklabels(target_names, <br/>                       fontdict={'verticalalignment': 'center'});</span><span id="17be" class="ng me it nc b gy nl ni l nj nk"># Plot confusion matrix<br/>plot_cm(train['target'], train['vader_polarity'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e4ed892a54bca13264cc6905b7da869d.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*zQ-BNaGmy9i2w3KpzjSGbw.png"/></div></figure><p id="09ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所见，我们有许多真阳性和假阳性。事实上，大约 67%的预测是积极的。让我们看看如果使用<em class="na">复合</em>分数，性能是否会提高。</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="1c7e" class="ng me it nc b gy nh ni l nj nk">train['vader_compound'] = np.where(train['compound']&gt;0, 1, 0)<br/>print(classification_report(train['target'], <br/>                            train['vader_compound'], <br/>                            target_names=target_names))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/30da55ff4c8c4f8e4e87bbb9f113f5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*hKc5mr0gw1sByg7I89BXig.png"/></div></figure><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="a1bd" class="ng me it nc b gy nh ni l nj nk">plot_cm(train['target'], train['vader_compound'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ox"><img src="../Images/1c9f128b42011bc1d60a16116ca4f31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*ukwsadCsVAqQXFLlp6Wxfw.png"/></div></div></figure><p id="40ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">性能看起来非常相似。我使用训练数据集进行评估，因为我们在这里不是训练模型。但是，如果对测试数据进行同样的操作，结果应该非常相似。</p><p id="2beb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🔗nltk  中关于<a class="ae le" href="https://github.com/cjhutto/vaderSentiment" rel="noopener ugc nofollow" target="_blank"><em class="na">【VADER】</em></a><em class="na"/><a class="ae le" href="https://www.nltk.org/howto/sentiment.html" rel="noopener ugc nofollow" target="_blank"><em class="na">VADER 的更多信息。</em></a></p><h2 id="2762" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">1.2.使用 TextBlob 进行情感分析</h2><p id="1c30" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">另一种获得情感分数的方法是利用<em class="na"> TextBlob </em>库。使用来自<em class="na"> TextBlob </em>对象的情感属性，我们也可以提取相似的分数。下面是我们如何使用之前的示例进行提取:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="fa25" class="ng me it nc b gy nh ni l nj nk">TextBlob(example).sentiment</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f2647eef1449ffc98c15661891a8e11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*pkNPreTSFYqnxVxhKM2imA.png"/></div></figure><blockquote class="oh oi oj"><p id="7b7e" class="ki kj na kk b kl km ju kn ko kp jx kq ok ks kt ku ol kw kx ky om la lb lc ld im bi translated"><strong class="kk iu"> <em class="it">极性:</em> </strong> <em class="it">范围从-1(最负)到 1(最正)<br/> </em> <strong class="kk iu"> <em class="it">主观性:</em> </strong> <em class="it">范围从 0(非常客观)到 1(非常主观)</em></p></blockquote><p id="5829" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的例子被分析为非常主观的肯定陈述。是真的，不是吗？在这两个分数中，<em class="na">极性</em>与我们更相关。让我们将强度分数添加到训练数据中，并检查具有最高<em class="na">极性</em>分数的 5 个记录:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="098a" class="ng me it nc b gy nh ni l nj nk">train[['polarity', 'subjectivity']] = train['review'].apply(lambda x:TextBlob(x).sentiment).to_list()</span><span id="15b3" class="ng me it nc b gy nl ni l nj nk">columns = ['review', 'target', 'polarity', 'subjectivity']<br/>train[columns].nlargest(5, ['polarity'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/daeaf74850f11f4e8bb4ac6d604101da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*-ultlUoZ3pufwWZhSIp6Zg.png"/></div></figure><p id="8835" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，用<em class="na"> TextBlob </em>添加情感强度分数也很简单。让我们来看看极性<em class="na">分数最低的 5 条记录:</em></p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="3634" class="ng me it nc b gy nh ni l nj nk">train[columns].nsmallest(5, ['polarity'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c483f87a7f591be357643395ef7a72a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*gT93FYT3jxBfEgKl65yc8g.png"/></div></figure><p id="9450" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是时候绘制一些直方图来更好地理解分数了:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="fd1b" class="ng me it nc b gy nh ni l nj nk">for var in ['polarity', 'subjectivity']:<br/>    plt.figure(figsize=(12,4))<br/>    sns.distplot(train.query("target==1")[var], bins=30, kde=False, <br/>                 color='green', label='Positive')<br/>    sns.distplot(train.query("target==0")[var], bins=30, kde=False, <br/>                 color='red', label='Negative')<br/>    plt.legend()<br/>    plt.title(f'Histogram of {var} by true sentiment');</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi or"><img src="../Images/3cdf061d1a6cb0203edd7fe6d1e0907a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7SLSmVsZjnqsJnlO6EERA.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pb"><img src="../Images/1d48d1bdcbc3be13f39c0c7ef2778243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRQ9zhbsn_AyoxIMqVqPFw.png"/></div></div></figure><p id="e6db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所料，<em class="na">极性</em>分数看起来可能有助于对积极情绪和消极情绪进行分类。让我们使用<em class="na">极性</em>分数进行分类，并查看性能:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="bba9" class="ng me it nc b gy nh ni l nj nk">train['blob_polarity'] = np.where(train['polarity']&gt;0, 1, 0)<br/>print(classification_report(train['target'], <br/>                            train['blob_polarity'], <br/>                            target_names=target_names))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/41710a95225dd457dfccaef9a6cd1685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*BAGdbyCN-1Vg2xDd1JLFmw.png"/></div></figure><p id="cc55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<em class="na"> TextBlob </em>，我们可以不费吹灰之力获得大约 69%的准确率。同样，我们有许多假阳性，事实上，甚至比以前更多。让我们看看混淆矩阵:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="3d6d" class="ng me it nc b gy nh ni l nj nk">plot_cm(train['target'], train['blob_polarity'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3be0a2d32ac061dd2c97f8bd207e6132.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*alKDx1ZNC9x8xRm1vCB1sg.png"/></div></figure><p id="3e8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次，假阳性的数量高于真阴性的数量。预测偏向积极情绪，因为 76%的预测是积极的。</p><p id="8375" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🔗关于<a class="ae le" href="https://textblob.readthedocs.io/en/dev/" rel="noopener ugc nofollow" target="_blank"> <em class="na"> TextBlob </em>的更多信息。</a></p><h2 id="5ed2" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">1.3.两者之间的关系</h2><p id="f133" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">让我们来比较一下<em class="na"> VADER </em>和<em class="na"> TextBlob </em>的分数有多相似:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="f101" class="ng me it nc b gy nh ni l nj nk">pd.crosstab(train['vader_polarity'], train['blob_polarity'])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pd"><img src="../Images/1b2682057bd48af7d1f65f73283e69af.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*T0OAiYigTgiNl_0cxPbVoA.png"/></div></div></figure><p id="ebf2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们的分类中有大约 79%的重叠，大多数是正面情绪。让我们来看看极性得分:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="baf3" class="ng me it nc b gy nh ni l nj nk">plt.figure(figsize=(12,12))<br/>sns.scatterplot(data=train, x='polarity', y='compound',<br/>                hue='target', palette=['red', 'green'], <br/>                alpha=.3)<br/>plt.axhline(0, linestyle='--', color='k')<br/>plt.axvline(0, linestyle='--', color='k')<br/>plt.title('Scatterplot between polarity intensity scores');</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pe"><img src="../Images/0aa19bf51624efda89c23521a1f9836a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-Pl7Hob_8dLaIIthFvOLg.png"/></div></div></figure><p id="2ea8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这张图显示的信息比前一张表多一点。在左下象限，我们主要看到红色圆圈，因为两种方法中的负面分类都更精确。在右上象限，有大量的圆圈，大部分是绿色的，但颜色混合不像以前那么纯。剩下的两个象限显示了两个分数不一致的地方。总的来说，在图的右半部分，颜色比左半部分更混合。</p><p id="7d1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从两者中得到非常相似的 69%的总体准确度；然而，当我们仔细观察预测时，这两种方法之间的性能是不同的。</p><p id="f6c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在你知道如何用<em class="na"> VADER </em>或<em class="na">文本块</em>获得情感极性分数。如果您有未标记的数据，这两个工具为自动标记您的数据提供了一个很好的起点。是时候建立模型了！✨</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="ea0b" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">2.模拟ⓜ️</h1><p id="4df3" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在本节中，我们将:</p><ol class=""><li id="d275" class="pf pg it kk b kl km ko kp kr ph kv pi kz pj ld pk pl pm pn bi translated">选择合适的预处理方法和算法</li><li id="7698" class="pf pg it kk b kl po ko pp kr pq kv pr kz ps ld pk pl pm pn bi translated">探索添加<em class="na"> VADER </em>和<em class="na">文本斑点</em>情感分数作为特征是否会提高模型的预测能力</li><li id="509e" class="pf pg it kk b kl po ko pp kr pq kv pr kz ps ld pk pl pm pn bi translated">构建管道并调整其超参数</li><li id="2558" class="pf pg it kk b kl po ko pp kr pq kv pr kz ps ld pk pl pm pn bi translated">在看不见的数据上测试最终管道</li></ol><p id="b4a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">情感分类是监督分类模型的一个应用。因此，我们在这里采用的方法可以推广到任何监督分类任务。</p><h2 id="626c" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">2.1.选择合适的预处理方法和算法</h2><p id="b7b8" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/preprocessing-text-in-python-923828c4114f">我之前的帖子</a>中，我们探索了三种不同的文本预处理方法，并列出其中两种:<em class="na">简单方法</em>和<em class="na">简单方法</em>。在这两个选项中，我们现在将测试这两个选项之间的模型性能是否有任何差异，并选择其中一个来使用向前移动。为了使事情变得简单，我们将创建两个函数(这些函数的想法是从<a class="ae le" href="https://www.kaggle.com/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86/comments" rel="noopener ugc nofollow" target="_blank">这里</a>得到的启发):</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="5a54" class="ng me it nc b gy nh ni l nj nk"># Define functions<br/>def create_baseline_models():<br/>    """Create list of baseline models."""<br/>    models = []<br/>    models.append(('log', LogisticRegression(random_state=seed, <br/>                                             max_iter=1000)))<br/>    models.append(('sgd', SGDClassifier(random_state=seed)))<br/>    models.append(('mnb', MultinomialNB()))<br/>    return models</span><span id="da20" class="ng me it nc b gy nl ni l nj nk">def assess(X, y, models, cv=5, scoring=['roc_auc', <br/>                                        'accuracy', <br/>                                        'f1']):<br/>    """Provide summary of cross validation results for models."""<br/>    results = pd.DataFrame()<br/>    for name, model in models:<br/>        result = pd.DataFrame(cross_validate(model, X, y, cv=cv, <br/>                                             scoring=scoring))<br/>        mean = result.mean().rename('{}_mean'.format)<br/>        std = result.std().rename('{}_std'.format)<br/>        results[name] = pd.concat([mean, std], axis=0)<br/>    return results.sort_index()</span></pre><p id="5b3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我挑了三个算法来试:<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> <em class="na">逻辑回归分类器</em> </a>、<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd#sklearn.linear_model.SGDClassifier" rel="noopener ugc nofollow" target="_blank"> <em class="na">随机梯度下降分类器</em> </a>和<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB" rel="noopener ugc nofollow" target="_blank"> <em class="na">多项朴素贝叶斯分类器</em> </a>。让我们启动模型:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="f7bc" class="ng me it nc b gy nh ni l nj nk">models = create_baseline_models()<br/>models</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/c7c50e0630f51a84019c1d0a7b51c0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*2GSfUv9BLoLO9MtGaNFG9A.png"/></div></figure><p id="812c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们检查使用<em class="na">更简单方法</em>时的模型性能:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="8ccf" class="ng me it nc b gy nh ni l nj nk"># Preprocess the data<br/>vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', <br/>                             stop_words='english', <br/>                             min_df=30, <br/>                             max_df=.7)<br/>X_train_simpler = vectoriser.fit_transform(X_train)</span><span id="618e" class="ng me it nc b gy nl ni l nj nk"># Assess the model<br/>assess(X_train_simpler, y_train, models)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/16ffbde7506d1a91ffd3d9aac9324c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*Bwvb1tc0ZXCr8n7pMvrnAA.png"/></div></figure><p id="6db3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很高兴看到我们获得了更好的性能:与仅使用情感分数相比，基线模型的准确率为 86–89%。由于这些职业相当平衡，我们将主要关注准确性。但是，我们将确保稍后更仔细地检查预测，以评估模型。性能指标在<em class="na">逻辑回归</em>和<em class="na">随机梯度下降</em>之间看起来非常接近，后者在训练中更快(参见<em class="na"> fit_time </em>)。朴素贝叶斯是三个人中训练速度最快的，但是表现比其他两个人稍差。现在让我们来评估一下<em class="na">简单方法</em>:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="b1b0" class="ng me it nc b gy nh ni l nj nk"># Define function<br/>def preprocess_text(text):<br/>    # 1. Tokenise to alphabetic tokens<br/>    tokeniser = RegexpTokenizer(r'[A-Za-z]+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # 2. Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') <br/>              for t in tokens]<br/>    return tokens</span><span id="439d" class="ng me it nc b gy nl ni l nj nk"># Preprocess the data<br/>vectoriser = TfidfVectorizer(analyzer=preprocess_text, <br/>                             min_df=30, <br/>                             max_df=.7)<br/>X_train_simple = vectoriser.fit_transform(X_train)</span><span id="0a7e" class="ng me it nc b gy nl ni l nj nk"># Assess models<br/>assess(X_train_simple, y_train, models)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/2c5b287fef37fe8a72b13de655718986.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*8pwnnDLD5rY3sNkPS6GKdg.png"/></div></figure><p id="0144" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">性能看起来和以前差不多。因此，我们将倾向于使用<em class="na">更简单的方法</em>,并继续使用它。在这三种算法中，我们将选择<em class="na">随机梯度下降</em>，因为它最能平衡速度和预测能力。</p><h2 id="a50f" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">2.2.评估附加功能</h2><p id="0a28" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在本节中，我们将探讨添加<em class="na"> VADER </em>和<em class="na">文本斑点</em>情感分数作为特征是否会提高模型的预测能力。让我们快速检查一下是否有任何高度相关的特性:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="ce1e" class="ng me it nc b gy nh ni l nj nk">plt.figure(figsize = (14,5))<br/>columns = ['target', 'neg', 'neu', 'pos', 'compound', 'polarity', <br/>           'subjectivity']<br/>sns.heatmap(train[columns].corr(), annot=True, cmap='seismic_r');</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pw"><img src="../Images/fbd4350aeedae0035ac78e299b1390fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBTXK4CxD9X5QRik-N7qeA.png"/></div></div></figure><p id="b800" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最相关的特征是<em class="na">复合</em>和<em class="na">负</em>。让我们运行一个快速模型，看看哪些分数更有用:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="6b64" class="ng me it nc b gy nh ni l nj nk"># Initialise a model<br/>sgd = SGDClassifier(random_state=seed)</span><span id="5b63" class="ng me it nc b gy nl ni l nj nk"># Initialise a scaler<br/>scaler = MinMaxScaler()</span><span id="562b" class="ng me it nc b gy nl ni l nj nk"># Assess the model using scores<br/>scores = train[['neg', 'neu', 'pos', 'compound', 'polarity', <br/>                'subjectivity']]<br/>assess(scaler.fit_transform(scores), y_train, [('sgd', sgd)])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/876f0027833428e71bc12a099d2403b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*341aIzrIS9UsU26rXe-JzQ.png"/></div></figure><p id="0c96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用分数得到大约 77%的准确率。现在让我们检查系数:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="aa9a" class="ng me it nc b gy nh ni l nj nk"># Fit to training data<br/>sgd.fit(scores, y_train)</span><span id="04b7" class="ng me it nc b gy nl ni l nj nk"># Get coefficients<br/>coefs = pd.DataFrame(data=sgd.coef_, columns=scores.columns).T<br/>coefs.rename(columns={0: 'coef'}, inplace=True)</span><span id="7fcd" class="ng me it nc b gy nl ni l nj nk"># Plot<br/>plt.figure(figsize=(10,5))<br/>sns.barplot(x=coefs.index, y='coef', data=coefs)<br/>plt.title('Coefficients');</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi py"><img src="../Images/a9ad1551203a63acf1f66b43898fe0e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*lykkZKbjFS_oKr3yC3UOLA.png"/></div></figure><p id="04bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似乎我们只能使用<em class="na">阴性、阳性</em>和<em class="na">极性</em>，因为它们是得分<em class="na">中最主要的特征。</em>让我们看看是否可以通过将这些选择的分数添加到先前预处理的数据来改进模型结果。</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="77e2" class="ng me it nc b gy nh ni l nj nk"># Add features to sparse matrix<br/>selected_scores = train[['neg', 'pos', 'polarity']]<br/>X_train_extended = hstack([X_train_simpler, csr_matrix(scaler.fit_transform(selected_scores))])</span><span id="0ec7" class="ng me it nc b gy nl ni l nj nk"># Assess<br/>assess(X_train_extended, y_train, [('sgd', sgd)])</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/d03ad39844fbbb002f2b3b9d0b0e7374.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*1kKaP9hJE7OQtgYp1xv79Q.png"/></div></figure><p id="a972" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于添加这些分数并没有改进模型，因此没有必要将它们作为特征添加。这也将使我们的渠道保持简单！</p><h2 id="5de7" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">2.3.构建管道并调整其超参数</h2><p id="2e4b" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">是时候构建一个小管道，将预处理器和模型放在一起了。我们将微调它的超参数，看看我们是否能改进这个模型。首先，让我们尝试理解三个超参数的影响:对于向量机的<code class="fe pz qa qb nc b">min_df</code>、<code class="fe pz qa qb nc b">max_df</code>和对于随机搜索模型的<code class="fe pz qa qb nc b">loss</code>:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="a2c9" class="ng me it nc b gy nh ni l nj nk"># Create a pipeline<br/>pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+')),<br/>                 ('model', SGDClassifier(random_state=seed))])</span><span id="d135" class="ng me it nc b gy nl ni l nj nk"># Prepare a random search<br/>param_distributions = {'vectoriser__min_df': np.arange(10, 1000, 10),<br/>                       'vectoriser__max_df': np.linspace(.2, 1, 40),<br/>                       'model__loss': ['log', 'hinge']}<br/>r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, <br/>                              n_iter=30, cv=5, n_jobs=-1, random_state=seed)<br/>r_search.fit(X_train, y_train)</span><span id="391a" class="ng me it nc b gy nl ni l nj nk"># Save results to a dataframe<br/>r_search_results = pd.DataFrame(r_search.cv_results_).sort_values(by='rank_test_score')</span></pre><p id="680b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们正在尝试 30 种不同的超参数空间指定的随机组合。这需要一段时间来运行。随机搜索的输出将保存在名为<code class="fe pz qa qb nc b">r_search_results</code>的数据帧中。让我们创建另一个数据框架，其中包含一些我们更感兴趣的列:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="1c91" class="ng me it nc b gy nh ni l nj nk">columns = [col for col in r_search_results.columns <br/>           if re.search(r"split|param_", col)]<br/>r_summary = r_search_results[columns].copy()<br/>r_summary.columns = [re.sub(r'_test_score|param_', '', col) <br/>                     for col in r_summary.columns]<br/>columns = [col.split('__')[1] if '__' in col else col <br/>           for col in r_summary.columns ]<br/>r_summary.columns = columns<br/>r_summary.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/8acd6efc3bbd8f64736433483f85e398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*e0ty_8YaVEsE6MVlhP1snw.png"/></div></figure><p id="848b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将输出可视化，以便更好地理解超参数的影响:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="f425" class="ng me it nc b gy nh ni l nj nk"># Create a long dataframe<br/>r_summary_long = pd.melt(r_summary, <br/>                         id_vars=['min_df', <br/>                                  'max_df', <br/>                                  'loss'], <br/>                         value_vars=['split0', <br/>                                     'split1', <br/>                                     'split2', <br/>                                     'split3', <br/>                                     'split4'])</span><span id="7c3f" class="ng me it nc b gy nl ni l nj nk"># Plot hyperparameter 'loss'<br/>plt.figure(figsize=(8,4))<br/>plt.title('Performance by loss')<br/>sns.boxplot(x='value', y='loss', data=r_summary_long, <br/>            orient='h')<br/>plt.xlim(.8, .9);</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/83ae826acc46dc66465c1b4f8226b6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*jZwA50wrMYhbXOVl8xsOeA.png"/></div></figure><p id="35f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来<code class="fe pz qa qb nc b">loss='hinge'</code>会带来稍微好一点的性能。让我们看看数字超参数:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="50df" class="ng me it nc b gy nh ni l nj nk">for param in ['min_df', 'max_df']:<br/>    plt.figure(figsize=(8,4))<br/>    sns.scatterplot(x=param, y="value", data=r_summary_long, <br/>                    x_jitter=True, alpha=0.5)<br/>    plt.ylim(.8, .9);</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/151a67356e7dda36782f71291e607603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*b0r-a8udnfyisq8WL4JjgA.png"/></div></figure><p id="eb82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<code class="fe pz qa qb nc b">min_df</code>和准确度之间似乎存在负相关关系，我们将把<code class="fe pz qa qb nc b">min_df</code>保持在 200 以下。<code class="fe pz qa qb nc b">max_df</code>没有明显的趋势，可能是因为业绩受<code class="fe pz qa qb nc b">min_df</code>和<code class="fe pz qa qb nc b">loss</code>的影响更大。虽然他们三个都是这样，但是对于<code class="fe pz qa qb nc b">max_df</code>来说更明显。现在，我们对这些超参数如何影响模型有了一些了解，让我们更精确地定义管道(<code class="fe pz qa qb nc b">max_df=.6</code>和<code class="fe pz qa qb nc b">loss=’hinge'</code>)并尝试使用网格搜索进一步调整它:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="92d1" class="ng me it nc b gy nh ni l nj nk"># Create a pipeline<br/>pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.6)),<br/>                 ('model', SGDClassifier(random_state=seed, loss='hinge'))])</span><span id="6f26" class="ng me it nc b gy nl ni l nj nk"># Prepare a grid search<br/>param_grid = {'vectoriser__min_df': [30, 90, 150],<br/>              'vectoriser__ngram_range': [(1,1), (1,2)],<br/>              'vectoriser__stop_words': [None, 'english'],<br/>              'model__fit_intercept': [True, False]}<br/>g_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5, n_jobs=-1)<br/>g_search.fit(X_train, y_train)</span><span id="b043" class="ng me it nc b gy nl ni l nj nk"># Save results to a dataframe<br/>g_search_results = pd.DataFrame(g_search.cv_results_).sort_values(by='rank_test_score')</span></pre><p id="b92b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网格搜索也需要一些时间，因为我们有 24 种不同的超参数组合要尝试。像以前一样，输出将保存到名为<code class="fe pz qa qb nc b">g_search_results</code>的数据帧中。让我们将更多相关列提取到另一个数据框架中:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="7bc5" class="ng me it nc b gy nh ni l nj nk">columns = [col for col in g_search_results.columns <br/>           if re.search(r"split|param_", col)]<br/>g_summary = g_search_results[columns+['mean_test_score']].copy()<br/>g_summary.columns = [re.sub(r'_test_score|param_', '', col) <br/>                     for col in g_summary.columns]<br/>columns = [col.split('__')[1] if '__' in col else col <br/>           for col in g_summary.columns ]<br/>g_summary.columns = columns<br/>g_summary.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi qf"><img src="../Images/149961dc1f3c89edb3027ca7755e9f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvhEC60GwYGZsV1mcRkYHA.png"/></div></div></figure><p id="967a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用这些组合中的任何一种，我们都可以达到约 0.9 的交叉验证精度。很高兴看到边际增长。</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="af1a" class="ng me it nc b gy nh ni l nj nk"># Create a long dataframe<br/>g_summary_long = pd.melt(g_summary, <br/>                         id_vars=['min_df', <br/>                                  'ngram_range', <br/>                                  'stop_words', <br/>                                  'fit_intercept'], <br/>                         value_vars=['split0', <br/>                                     'split1', <br/>                                     'split2', <br/>                                     'split3', <br/>                                     'split4'])<br/>g_summary_long.replace({None: 'None'}, inplace=True)</span><span id="9704" class="ng me it nc b gy nl ni l nj nk"># Plot performance<br/>for param in ['ngram_range', 'stop_words', 'fit_intercept']:<br/>    plt.figure(figsize=(8,4))<br/>    plt.title(f'Performance by {param}')<br/>    sns.boxplot(x='value', y=param, data=g_summary_long, orient='h')<br/>    plt.xlim(.85, .95);</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/e796b678dfbea21adf4bd9575e19dbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*iduUq6zLkCb4Ud2JgsDHMQ.png"/></div></figure><p id="7d63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，换成<code class="fe pz qa qb nc b">ngram_range=(1,2)</code>，model 表现更好。<code class="fe pz qa qb nc b">stop_words=None</code>也是如此。另一方面，我们是否拟合截距并没有太大的影响，这意味着我们可以将这个超参数保留为默认值。我认为这已经足够好了，我们现在可以定义最终的管道了。</p><h2 id="6440" class="ng me it bd mf nw nx dn mj ny nz dp mn kr oa ob mp kv oc od mr kz oe of mt og bi translated">2.4.在看不见的数据上测试最终管道</h2><p id="f840" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">使用网格搜索中的顶部组合，这是我们最终管道的样子:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="cbb1" class="ng me it nc b gy nh ni l nj nk">pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', min_df=30, max_df=.6, ngram_range=(1,2))),<br/>                 ('model', SGDClassifier(random_state=seed, loss='hinge'))])</span><span id="acab" class="ng me it nc b gy nl ni l nj nk">pipe.fit(X_train, y_train)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi qh"><img src="../Images/3e382ebce812a60d3de7f7ab3602d275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bN3hSFWlBmIgqe2mWNHiQ.png"/></div></div></figure><p id="b355" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的管道很小很简单。让我们看看它的系数:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="1d9f" class="ng me it nc b gy nh ni l nj nk">coefs = pd.DataFrame(pipe['model'].coef_, <br/>                     columns=pipe['vectoriser'].get_feature_names())<br/>coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')<br/>coefs</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/2434aae0d16dfe2732867e3bcbf40583.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*q5Tc6cA47-bIjMGXdEyMVg.png"/></div></figure><p id="fc0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有最高或最低系数的特征看起来很直观。但是看看我们拥有的特性数量:49，577！这主要是因为放松了<code class="fe pz qa qb nc b">min_df</code>，增加了二元模型，没有删除停用词。如果我们热衷于减少特征的数量，我们可以改变管道中的这些超参数。如果我们开始减少特征，我们会注意到特征数量和模型精度之间的权衡。最佳平衡是什么样的取决于具体情况。让我们来评估管道:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="32c3" class="ng me it nc b gy nh ni l nj nk">train_pred = pipe.predict(X_train)<br/>print(classification_report(train_pred, <br/>                            y_train, <br/>                            target_names=target_names))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/99fe7fd7a1bf19a45c2ffbbbd1ba4518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*28_I5B0S8xn44tppJpTgiA.png"/></div></figure><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="ca8d" class="ng me it nc b gy nh ni l nj nk">test_pred = pipe.predict(X_test)<br/>print(classification_report(test_pred, <br/>                            y_test, <br/>                            target_names=target_names))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/58f324f37df2290d1c30f81f3033cb94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*5V6FegW-sQWlnfQahVWCmQ.png"/></div></figure><p id="c814" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练集和测试集上的准确率分别约为 0.94 和 0.92。这两种观点的精确度和召回率看起来非常相似。我们有稍微多一点的假阴性。让我们绘制混淆矩阵:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="9f21" class="ng me it nc b gy nh ni l nj nk">plot_cm(test_pred, y_test, target_names=target_names)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/8ff0c63a8faa0796066ea8781de9ab35.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*tfaQt2CpcSXv8TuDLVw7Bg.png"/></div></figure><p id="e713" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来不错。这么🎊现在，我们有了一个将大约 90%的评论归类为正确观点的管道。让我们看看做一次预测需要多长时间。我们将使用 Jupyter 笔记本的魔法命令<code class="fe pz qa qb nc b">%timeit</code>:</p><pre class="lg lh li lj gt nb nc nd ne aw nf bi"><span id="f2e8" class="ng me it nc b gy nh ni l nj nk">for i in range(10):<br/>    lead = X_test.sample(1)<br/>    %timeit pipe.predict(lead)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi qk"><img src="../Images/9941c7acdf6954a9c868040226d8c62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bbdOeHuBnRGZ5rPXIeZ6AQ.png"/></div></div></figure><p id="8fc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管<code class="fe pz qa qb nc b">%timeit</code>运行了多个循环，并给出了运行时间的均值和标准差，但我注意到我每次得到的输出都略有不同。因此，我们正在查看<code class="fe pz qa qb nc b">%timeit</code>的 10 个循环以观察范围。</p><p id="2a1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单次预测大约需要 1.5 到 4 毫秒。这需要在用例的生产环境的上下文中进行评估。</p><p id="24a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，这就是这篇文章的内容。💫</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ql"><img src="../Images/c71d93a750d94bf7ee21bbafa3142f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cjnWuJ-_PYrTZ6HX"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@countchris?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">伯爵克里斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e618" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="na">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae le" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="na">我的推荐链接</em> </a>，<em class="na">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="25cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你看我的帖子。希望您已经学会了一些不同的实用方法，可以在构建或不构建定制模型的情况下将文本分类为情感。以下是本系列其他两篇文章的链接:<em class="na"><br/></em>◼️<a class="ae le" rel="noopener" target="_blank" href="/exploratory-text-analysis-in-python-8cf42b758d9e">python 中的探索性文本分析</a><br/>◼️<a class="ae le" rel="noopener" target="_blank" href="/preprocessing-text-in-python-923828c4114f">python 中的文本预处理</a></p><p id="6643" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是我的其他 NLP 相关帖子的链接:<br/>◼️<a class="ae le" rel="noopener" target="_blank" href="/simple-wordcloud-in-python-2ae54a9f58e5">Python 中的简单 word cloud</a><br/><em class="na">(下面列出了一系列关于 NLP 介绍的帖子)</em> <br/> ◼️ <a class="ae le" rel="noopener" target="_blank" href="/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96">第一部分:Python 中的预处理文本</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">第二部分:词条满足和词干的区别</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">第三部分:TF-IDF 解释</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener">第四部分:python 中的监督文本分类模型</a><a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener"/></p><p id="7874" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再见🏃💨</p></div></div>    
</body>
</html>