<html>
<head>
<title>Gaussian Mixture Models(GMM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型(GMM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-gmm-6e95cbc38e6e?source=collection_archive---------11-----------------------#2020-04-23">https://towardsdatascience.com/gaussian-mixture-models-gmm-6e95cbc38e6e?source=collection_archive---------11-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db5f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解GMM:思想、数学、EM算法和python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b75c50ac2c0389af49b8de05658e15eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tFTLI-Yc9SEf9Q2FAGrGg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://unsplash.com/@franckinjapan" rel="noopener ugc nofollow" target="_blank">弗兰克诉</a>通过<a class="ae ky" href="https://unsplash.com/photos/JjGXjESMxOY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="6b9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> B </span> <strong class="lb iu"> <em class="me"> rief </em>:高斯混合模型是一种流行的无监督学习算法。GMM方法类似于K-Means聚类算法，但是更健壮，因此由于其复杂性而更有用。在这篇文章中，我将给出一个鸟瞰图，数学(<em class="me"> ba </em> ye <em class="me"> s </em> ic maths，nothing ab <em class="me"> normal </em>)，python从头实现以及使用sklearn库。</strong></p><h1 id="544f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">介绍</h1><p id="3ec6" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">查看我关于K-means聚类的博客是一个好主意(3分钟阅读)，以获得聚类、无监督学习和K-Means技术的基本概念。在聚类中，给定一个未标记的数据集<strong class="lb iu"> <em class="me"> X </em> </strong>，我们希望将样本分组到<strong class="lb iu"> <em class="me"> K </em> </strong>个聚类中。在GMMs中，假设<strong class="lb iu"/>的不同子群体(<strong class="lb iu"> <em class="me"> K </em> </strong>共)遵循一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>，虽然我们只有总体<strong class="lb iu"> <em class="me"> X( </em> </strong>因此得名高斯混合模型)的概率分布信息。我们的任务是能够找到<strong class="lb iu"> <em class="me"> K </em> </strong> <em class="me">高斯的</em>的参数以便将数据<strong class="lb iu"><em class="me"/></strong>X<a class="ae ky" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank"><em class="me">进行探索性的数据分析</em></a><strong class="lb iu"/>或者对新的数据做出预测。</p><h1 id="4170" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">对K-均值聚类的改进</h1><p id="bbc6" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">K-means使用欧几里德距离函数来发现数据中的聚类。只要数据相对于质心遵循圆形分布，这种方法就能很好地工作。但是如果数据是非线性的，椭圆形的呢？还是数据有非零协方差？如果聚类有不同的均值和协方差呢？</p><p id="6941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是高斯混合模型拯救世界的地方！</p><blockquote class="nc"><p id="43e8" class="nd ne it bd nf ng nh ni nj nk nl lu dk translated">GMM假设产生数据的是高斯分布的混合物。它使用数据点到聚类的软分配(即，概率性的，因此更好),与数据点到聚类的硬分配的K-means方法形成对比，假设数据围绕质心呈圆形分布。</p></blockquote><p id="a1d7" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">简而言之，GMM捕获工作得更好，因为<strong class="lb iu"> (A) </strong>它通过使用软分配来捕获属于不同聚类的数据点的不确定性，并且<strong class="lb iu"> (B) </strong>它对圆形聚类没有偏见。因此，即使对于非线性数据分布，它也能很好地工作。</p><h1 id="7edf" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">戈-梅-莫三氏:男性假两性畸形综合征</h1><p id="f2d4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><em class="me"> GMM </em>的目标函数是最大化数据X、<strong class="lb iu"> <em class="me"> p(X) </em> </strong>的<em class="me">似然值或对数似然值<strong class="lb iu"> <em class="me"> L </em> </strong>(因为log是单调递增函数)。通过假设混合了<strong class="lb iu"> <em class="me"> K </em> </strong>高斯分布来生成数据，我们可以将<strong class="lb iu"><em class="me">【p(X)</em></strong>写成边缘化概率，对所有数据点的所有<strong class="lb iu"> <em class="me"> K </em> </strong>聚类求和。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e793cfd77b73bded1061721b5f8be753.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*iEmFaU2kGTkx0D_wNTNUgw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/889f0f8eb7e4fdbdce4467fb5f510601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Udq9JkCv3z9s_WXYtAT2iw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">似然值</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/0937274604f8d56835227e41f72deb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*sZlcLTgOugx_iX-ndqwtPg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对数似然值</p></figure><p id="798f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用上面对数函数内的求和，我们无法获得解析解。虽然看起来很龌龊，但这个问题有一个优雅的解决方案:<a class="ae ky" href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="me">期望最大化(em)算法</em> </strong> </a>。</p><h1 id="d6ab" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">数学</h1><p id="5c26" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu"> <em class="me"> EM算法</em> </strong>是一种<em class="me">迭代</em>算法，用于寻找<em class="me">模型的最大似然估计(MLE) </em>，其中参数无法直接找到，就像我们这里的情况。它包括两个步骤:e <em class="me">预期</em>步骤和<em class="me">最大化</em>步骤。</p><ol class=""><li id="faa0" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated"><strong class="lb iu"> <em class="me">期望步骤</em> </strong>:计算隶属值<em class="me"> r </em> _ <em class="me"> ic。</em>这是数据点<em class="me"> x_i </em>属于聚类<em class="me"> c </em>的概率。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/591b6f5e22a859240577a970c6a05d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qwgw-6MIfjaeqczqg7HuQw.png"/></div></div></figure><p id="10da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu"> <em class="me">最大化步骤</em> </strong>:计算一个新的参数<em class="me"> mc </em>，该参数决定了属于不同聚类的点的分数。通过计算每个聚类c的<a class="ae ky" href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian" rel="noopener ugc nofollow" target="_blank"> MLE的</a>来更新参数μ、π、σ</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6931c47e9c0362f4e2a1775a33e95393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*0L_HhYtzU1PmntPrOpG9gQ.png"/></div></figure><p id="a7d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复E-M步骤，直到对数似然值<em class="me"> L </em>收敛。</p><h1 id="d998" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">密码</h1><p id="b3cc" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">让我们从头开始用python写一个GMM的基本实现。</p><p id="82d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成一维数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="2765" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始化GMM的参数:μ，π，σ。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="69c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行EM算法的第一次迭代。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">EM算法的单次迭代</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/3f7ea392138b10a570fdb5bb002af923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLqrqBWlCPwdE3eERvpxtA.png"/></div></div></figure><p id="d674" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这段代码放在for循环中，并将其放入一个class对象中。现在我们正在谈话！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM-1D级</p></figure><div class="kj kk kl km gt ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9dc514400cdb0ba77b581aefff5c131a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dOIr3nPuZGWv9hiawlDGoA.png"/></div></figure><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8785578293bbd36be11257202843e09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XSy3sZKTBGVYWOhFXXvs-Q.png"/></div></figure></div><p id="6538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经有了一个模型，可以运行一维数据。同样的原理也适用于更高维度(≥ 2D)。唯一不同的是，我们将在这种情况下使用多元高斯分布。让我们为2D模型编写代码。</p><p id="0e8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们生成一些数据并编写我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/db7e80c9b312abf86e9a6896e9eb91c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*nu0tAapGQsl4nWhaQjFm7g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">2D斑点</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="fb8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对这个模型做一些预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><div class="kj kk kl km gt ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d881e589668e7c96ee62ea9c9488f1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZSpDCoip56QPLt2MDuxZ6g.png"/></div></figure><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/6ca5e6ddeaaaa9ea01cf5872168763ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7jJIlDnyPTvpzxtMqqXmpg.png"/></div></figure></div><div class="ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/021c3da9059a6901ec3562d9be8d33b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rnumu6d8_8kFFdCiZ_s8uw.png"/></div></figure><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d99baf14d33321d320e839186e7f46bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NYxM_vuTSt-fUf1fZJjRhg.png"/></div></figure></div><div class="ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/07405385995ea302d065c384a25650ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tbo99j-fL1hzzZpG8BKPHg.png"/></div></figure><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9d62570036f4e575559abcfee4eb6504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Wnvlfy1BuauVSak7UaFcJQ.png"/></div></figure></div><p id="4a3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://scikit-learn.org/stable/tutorial/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="me"> sklearn </em> </strong> </a>，同样的任务可以在几行代码内完成。很圆滑，是吧？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/90ac5d54c1fe742165ae553a0fcbc52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKs7niOt2YAZrwPpOH5uvw.png"/></div></div></figure><p id="9c8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，GMM将样本分类为属于第二类。有用！</p><h1 id="5839" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="e110" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">实现高斯混合模型并不困难。一旦你对数学有了清晰的认识，就可以找到模型的最大似然估计，无论是1D还是更高维的数据。这种方法在执行聚类任务时是健壮且有用的。既然您已经熟悉了GMMs的python实现，那么您就可以使用数据集执行一些很酷的事情了。假设给你一个病人的数据集，包含两个参数:红细胞体积和红细胞血红蛋白浓度，没有病人和健康病人的标签。插入上面的模型来聚集数据将会给你两个不同的(<em class="me">几乎是</em>)质量，你可以使用它们来进行进一步的分析和预测。</p><h1 id="0211" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">来源</h1><ol class=""><li id="60fc" class="nu nv it lb b lc mx lf my li op lm oq lq or lu nz oa ob oc bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=qMTuMa86NzU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=qMTuMa86NzU</a></li><li id="77de" class="nu nv it lb b lc os lf ot li ou lm ov lq ow lu nz oa ob oc bi translated"><a class="ae ky" href="https://www.python-course.eu/expectation_maximization_and_gaussian_mixture_models.php" rel="noopener ugc nofollow" target="_blank">https://www . python-course . eu/expectation _ maximization _ and _ Gaussian _ mixture _ models . PHP</a></li></ol></div></div>    
</body>
</html>