<html>
<head>
<title>Designing a Feature Selection Pipeline in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python设计要素选择管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/designing-a-feature-selection-pipeline-in-python-859fec4d1b12?source=collection_archive---------9-----------------------#2020-02-09">https://towardsdatascience.com/designing-a-feature-selection-pipeline-in-python-859fec4d1b12?source=collection_archive---------9-----------------------#2020-02-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bf35" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">用PYTHON进行机器学习</h2><div class=""/><div class=""><h2 id="3b9d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如此强大，以至于你的奶奶会试着用它来代替她那经验丰富的铸铁锅</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/791b62581275a7d71dfce4a5c8fd4c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kMMK-BW7oCGC8dMq"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">汉斯·维维克在Unsplash<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上的照片</a></p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/c4ab7475b5a82e56780e7ae4c08cc4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QRxFZsoMEpO7J_N52On2g.png"/></div></div></figure><p id="da56" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">目的:</strong>用Python设计开发特征选择流水线。</p><p id="fc07" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">材料和方法:</strong>使用Scikit-learn，我们为一个分类任务生成一个<a class="ae lh" href="http://archive.ics.uci.edu/ml/datasets/madelon" rel="noopener ugc nofollow" target="_blank"> Madelon </a>样的数据集。我们的工作流程的主要组成部分可以总结如下:(1)生成数据集(2)创建训练集和测试集。(3)应用特征选择算法来减少特征的数量。</p><p id="ab93" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">硬件</strong>:我们在配备英特尔酷睿i7–8700处理器(12个CPU，3.70 Ghz)和英伟达GeForce RTX 2080的工作站上训练和评估我们的模型。</p><p id="edec" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">注意:</strong>如果你是从零开始，我会建议你按照这篇<a class="ae lh" href="https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb?source=post_page---------------------------" rel="noopener">文章</a>安装所有必要的库。最后，假设读者熟悉Python、<a class="ae lh" href="https://pandas.pydata.org/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Pandas </a>和<a class="ae lh" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>。这篇文章的全部内容可以在<a class="ae lh" href="https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-" rel="noopener ugc nofollow" target="_blank">我的GitHub </a>上找到。欢迎你来叉它。我不得不提到一些特征选择方法是从Will Koehrsen的文章中得到启发的。</p><p id="f84b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">符号</strong>:粗体文本将用于表示Python对象，例如列表、字典、元组、数据帧，或者将引用一个图形或脚本。<code class="fe mf mg mh mi b">This notation will be used to represent classes and function parameters as well as Python packages.</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/4327fb00de01b77ad359652ffdef76fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3-KkZ0hlRZxjMn7Z6uXDGg.png"/></div></div></figure><h1 id="1bfa" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">特征选择</h1><p id="a521" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated"><em class="ng">特征选择</em>是从更大的群体中识别特征的代表性子集的过程。用户可以选择手动选择功能，也可以应用多种自动方法中的一种。手动选择特征的困难在于它需要关于手边数据的专业知识。例如，在放射肿瘤学中，放射治疗计划(其中每个体素代表一个特征的3D图像)被简化为一系列手工制作的特征，并且被几十年的放射治疗研究和合作临床调查所支持。这些特征中的每一个都量化了放射治疗计划的具体特征，这些特征已经被证明与患者的临床结果相关。令人欣慰的是，用于特征选择的自动化方法已经被开发出来，并且可以很容易地用于减轻这项任务；然而，我们必须有保留地应用这些自动化方法，因为它们可能会导致错误的结论。根据我的经验，我认为应用特性选择方法的最佳方式是使用领域知识和自动化方法来确定一个有代表性的特性子集。</p><p id="a5d8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在机器学习的背景下，人们从特征选择中获得的优势是很多的。例如，找到最具描述性的特征可以降低模型的复杂性，使找到最佳解决方案变得更容易，最重要的是，它减少了训练模型所需的时间。此外，如果你理解了典型特征的含义，你将对你所面临的问题有更深的理解。在某些情况下，可以获得轻微的性能提升。必须注意的是，通过从系统中删除特性，您的模型的性能可能会稍微差一些(因为您试图用更少的信息进行预测)。</p><p id="b279" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">通常情况下，我们没有多余的时间来对我们的数据进行彻底的研究，以确定要制作什么功能以及如何使用它们。面对超过1000个特征的数据集，确定特征的代表性子集的任务似乎令人生畏，但幸运的是，冗余是我们的数据可以成为我们的朋友。通常，当面对具有+1000个特征的数据集时，您会注意到很大一部分特征高度相关(冗余)，与结果无关，或者只是噪声。从开始的+1000个特征中，您可能最终会有10个特征“充分地”代表了整个数据集。</p><p id="57c9" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在本文中，我们将讨论以下特征选择算法及其局限性:</p><ul class=""><li id="c4bc" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">一种过滤方法，可移除可变性很小的要素</li><li id="9431" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">移除高度相关要素的过滤方法</li><li id="de16" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">递归特征消除(RFE ),用于确定最大化模型性能所需的特征子集</li><li id="35ea" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">确定与结果相关的特征子集的Boruta方法</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/a6e9b957cd9ee1fe158a8b97ab4be233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kR2_xa84KANSBrH6MWtmg.png"/></div></div></figure><h1 id="d1fa" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">安装必要的软件包</h1><p id="8dd7" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">在深入本文之前，让我们安装必要的包。我们将假设读者已经安装了以下包的Anaconda:<code class="fe mf mg mh mi b">Numpy</code>、<code class="fe mf mg mh mi b">Seaborn</code>、<code class="fe mf mg mh mi b">Matplotlib</code>、<code class="fe mf mg mh mi b">Pandas</code>和<code class="fe mf mg mh mi b">Sklearn</code>。</p><p id="2a9b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">你可能遗漏的唯一一个包是<code class="fe mf mg mh mi b">BorutaPy</code>，所以让我们来处理它。打开您的终端(如果您在Mac上)或Anaconda提示符(如果您在Windows机器上)。激活您的环境并运行以下命令:</p><p id="9dc2" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><code class="fe mf mg mh mi b">conda install -c saravji boruta</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/c09541ece3aad1a11800cb99c564b0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSA_Chg7FCh5QyMcygxrqA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用Mac终端在名为PythonFinance的Anaconda环境中安装BorutaPy。</p></figure><p id="ea5f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">你需要访问我的GitHub <a class="ae lh" href="https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-" rel="noopener ugc nofollow" target="_blank">这里</a>并下载工具文件夹。本文的全部内容都在名为FeatureSelectionPipe.py的文件中</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/06a1ae4a6f2cb44449c6562923004119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qnQjicEOBQ1oLwpfKVr2YQ.png"/></div></div></figure><p id="94d2" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">把tools文件夹放在你的工作目录下，你就设置好了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1ad9340f8a7c775f721db26334619c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*8IxIPKaBWtYxrSArwc6hWQ.png"/></div></figure><p id="9537" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">所以在我们开始之前，让我们导入所有的包。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本1 </strong> —导入所有必需的包。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/72d5e05be8e82313447e629f7f369a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlrvJph46QKz1C_riN4DmA.png"/></div></div></figure><h1 id="d05e" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">数据</h1><p id="e8d4" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">我们将创建多组合成特征，以探索不同特征选择算法的性能和有效性。首先，我们将生成一个类似Madelon的合成数据集。Madelon数据集(我们不会使用)是一个人工数据集，它包含放置在边长为1的五维超立方体顶点上的32个簇。聚类被随机标记为0或1 (2类)。我们将生成的类似马德隆的数据集将包含100个特征，其中10个特征将提供信息，50个将是冗余的(但提供信息)，25个将重复，15个将是无用的，因为它们将充满随机噪声。总共，我们将有1000个样品。为了给数据增加一点噪声，我们将随机翻转5%的标签。第二组功能将包含3个关键的重要功能。通过自动特征选择算法来选择设计的关键重要特征应该是具有挑战性的。最后，最后一组特性只是将类似Madelon的特性和关键特性添加到一个更大的数据集中。</p><p id="c101" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们现在准备创建类似Madelon的数据。在<strong class="ll jd">脚本2a </strong>的第4–9行，我们定义了我们希望从每种类型中获得的特征数量(信息性的、冗余的、重复的、无用的)。之后，在第12–16行，我们创建列标签。从第19–23行开始，我们创建数据和标签。在第27行，我们将numpy数组转换成pandas数据帧。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2a </strong> —创建类似Madelon的数据集。</p></figure><p id="f510" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">接下来，我们将创建我们至关重要的特性。这里，我们将假设经过几十年的理论、数值和经验研究，这三个特征已经被确定为对分类过程特别重要。为了更清楚地说明这一点，让我给你举个例子。在放射肿瘤学中，传递到器官的最大辐射可以与接受放射疗法治疗的患者所经历的毒性相关联。因此，为了确保对患者的安全治疗，考虑传送到每个器官的最大辐射量是至关重要的。为了便于说明，让我们假设在患者开始出现中毒症状之前，可以传递到膀胱的最大辐射可以是100。所以我们的数据可能看起来像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ce17158a87437271b87ff2488e93a6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*EEtQB2wnDa6AEkRNTa9wTg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">表1 </strong> —输送到膀胱的最大剂量。</p></figure><p id="ac17" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">因为医生知道对膀胱的大量辐射会导致并发症，所以产生的大多数辐射治疗计划将满足建立的最大辐射标准。然而，在<strong class="ll jd">表1 </strong>中，6号患者未能满足膀胱接受少于100次辐射的标准。换句话说，95%的数据符合标准，只有5%不符合。因此，如果您要实现一种自动选择功能的方法，如果您不采取适当的预防措施，这种方法很可能无法识别膀胱的最大剂量至关重要。尽管如此，我们知道这样的特征是非常重要的，为了病人的安全，应该总是考虑到。因此，将极其重要的特征引入我们的数据集中以测试自动特征选择方法的效率的动机。</p><p id="d15e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了创建3个关键的重要特征，将从三个不同的高斯分布中提取，每个分布以不同的值为中心。这些特征将由我在<a class="ae lh" href="https://stackoverflow.com/questions/36200913/generate-n-random-numbers-from-a-skew-normal-distribution-using-numpy" rel="noopener ugc nofollow" target="_blank"> StackOverFlow </a>中找到的以下辅助函数生成:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2b </strong>:生成关键重要特征的辅助函数。</p></figure><p id="c34f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">脚本2c </strong>的第5–10行，我们设置了关键特性分布的参数。然后在第13–25行，我们运行一个for循环，创建三个关键特性，然后存储在一个列表中(第25行)。我想指出的是，在第15行，我们修复了种子，这样我们就可以使用相同的发行版。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2c </strong>:创建三个关键特征。</p></figure><p id="6d4c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在我们将这些关键特性添加到数据集<strong class="ll jd">、</strong>之前，我们需要定义<strong class="ll jd"> y_critical </strong>目标。在<strong class="ll jd">脚本2d </strong>的第5–7行中，我们通过设置阈值来确定真假情况，从而为每个关键特性定义一个目标。选择这些阈值，使得每个关键特征包含95%的阳性病例和5%的阴性病例。最后，在第10行，我们通过乘以关键特性的目标来定义<strong class="ll jd"> y_critical </strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2d </strong>:创建每个关键特征的标签和<strong class="ak"> y_critical </strong>标签。<strong class="ak"> y_critical </strong>应该有74个阳性病例。</p></figure><p id="f4ba" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">作为健全性检查，让我们可视化关键特性的分布。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2e </strong>:可视化关键特征。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/68537c2e4364df57ea8c5831482e0756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGHBgcBZI9RnJmyrJEcJHw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图1 </strong> —关键特征分布。绿色区域代表阳性类别，红色区域代表阴性类别。</p></figure><p id="2649" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">从<strong class="ll jd">图1 </strong>中，您会注意到有三种不同的高斯分布。最左边的分布代表第一关键特征，最中间的分布代表第二关键特征，最右边的分布代表第三关键特征。标记为绿色的区域表示每个要素中的阳性类别，标记为红色的区域表示阴性类别。</p><p id="d18e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们现在准备把所有东西放在一起。在<strong class="ll jd">脚本2f </strong>的第12行，我们连接了<strong class="ll jd"> X_madelon </strong>和<strong class="ll jd"> X_critical </strong>。然后在第15–16行，目标<strong class="ll jd"> y_all </strong>被定义为<strong class="ll jd"> y_madelon </strong>和<strong class="ll jd"> y_critical </strong>的逐元素乘法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2f </strong>:加入<strong class="ak"> X_madelon </strong>和<strong class="ak"> X_critica </strong> l创建<strong class="ak"> X_all </strong>和创建<strong class="ak"> y_all </strong>。</p></figure><p id="c677" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在我们继续之前，让我们回顾一下我们已经创建的一组功能:</p><p id="1bef" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd"> X_madelon: </strong>这是一组使用<code class="fe mf mg mh mi b">make_classification()</code>类创建的合成特征。原则上，从该组中选择的最佳特征数量应该是信息特征(总共10个)。</p><p id="fd67" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd"> X_critical: </strong>关键特性一共3个。每个都具有不同平均值的高斯分布。每个关键特征包含90 %的肯定案例和10 %的错误案例。通过构造，这些特征可能很难通过自动化方法来选择；然而，对于一个对手头数据有领域知识的人来说，这些数据很容易被确定为对结果很重要。</p><p id="712c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd"> X_all: </strong>这是一组包含了<strong class="ll jd"> X_madelon </strong>和<strong class="ll jd"> X_critical </strong>特性的特性。该数据集的列将按如下顺序排列:</p><ol class=""><li id="19c4" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me od nn no np bi translated"><strong class="ll jd">信息特征—第1–10列</strong>:这些特征与结果高度相关，理想情况下应该是您选择的特征。</li><li id="2463" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated"><strong class="ll jd">冗余特征—第11–60列:</strong>这些特征通过线性组合具有不同随机权重的信息特征而形成。您可以将这些视为工程特性。</li><li id="e9ba" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated"><strong class="ll jd">重复特征—第61–85列</strong>:这些特征是从信息特征或冗余特征中随机抽取的。</li><li id="12cb" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated"><strong class="ll jd">无用特征—列86–100:</strong>这些特征充满了随机噪声。</li><li id="5b5f" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated"><strong class="ll jd">关键重要特征—101–103列</strong>:这些特征具有高斯分布，我们将要求它们绝对需要包含在所选特征中。</li></ol><p id="640e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在我们做任何其他事情之前，让我们分割我们的数据。对于每组功能，我们将使用70/30分割创建一个训练和测试集。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本2g </strong> —将数据分割成70个训练/ 30个测试分割。随机状态/种子被设置为42。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/24e9586a7e6e7701037c3ee65aafc1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24K69ngAoz30ZdmYCuQocw.png"/></div></div></figure><h1 id="0f5a" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">探索性数据分析</h1><p id="9310" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">在本节中，我们将可视化数据，以确认我们已经知道的内容。我们将从探索我们特征之间的相关性开始。在<strong class="ll jd">脚本3a </strong>中，我们首先计算<a class="ae lh" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">斯皮尔曼相关矩阵</a>，其值在-1到1的范围内(第5行)。值1表示强正相关，值-1表示强负相关。当相关值接近零时，这意味着特征之间不存在相关性。由于负相关和正相关只是相关，我们然后取相关矩阵的绝对值(第5行)并使用<code class="fe mf mg mh mi b">seaborn</code>包创建热图(第14行)，参见<strong class="ll jd">图2 </strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本3a</strong>—可视化Spearman相关矩阵。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/02da1fe523d79155891ddbfb57bef786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vuAO1Gc7WNZvvIXETaPghg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图2 </strong> —斯皮尔曼相关矩阵可视化为热图。注释是用PowerPoint添加的。</p></figure><p id="f791" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">图2 </strong>中，浅色表示低相关性，深蓝色表示高相关性。请注意，信息特征之间的相关性很低(左上角)。冗余特征开始显示更高的相关性(热图在该区域变得更蓝)，就像我们预期的那样。此外，有些重复与我们预期的其他特征高度相关。最后，无用特征和关键特征与其他特征不相关。我们知道无用的特征充满了随机噪声，不应该与结果相关联。另一方面，关键特征与结果相关，尽管它们与其他特征没有相关性。</p><p id="3a3f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">接下来，让我们使用箱线图来可视化每个特征的分布。箱线图显示了数据集的四分位数，并允许我们确定数据中的任何异常和异常值。在<strong class="ll jd">脚本3b </strong>中，我们首先设置图形样式，这样我们的图形(第5–8行)就不会有难看的灰色默认<code class="fe mf mg mh mi b">seaborn</code>背景。然后，我们为特性<strong class="ll jd"> X_all_ </strong>系列创建一个方框图(第14行)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本3b </strong> —用箱线图可视化特征分布。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/1953d1e9af6659ea796192a34ac88331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0J67Zmy8TK_z4u_nW92_aw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图3 </strong> —特征矩阵方框图。</p></figure><p id="4116" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">图3 </strong>中，我们可以看到大多数特征都包含离群值。此外，所有类似马德隆的特征都以零为中心，并且大多数似乎具有高斯分布。关键特征以不同的平均值为中心，就像我们之前在<strong class="ll jd">图1 </strong>中展示的那样。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/617b4ead4fbe8186391f9fcdb2f6bd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVkt64IP4IBHE1ZiLt52lA.png"/></div></div></figure><h1 id="ef6b" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">特征选择</h1><p id="127e" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">在本节中，我们将使用<code class="fe mf mg mh mi b">FeatureSelector</code>，这是一个应用以下四种特征选择方法的工具:</p><ol class=""><li id="205f" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me od nn no np bi translated">一种过滤方法，可移除具有给定比例重复值的要素。</li><li id="18d1" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated">一种基于皮尔逊或斯皮尔曼系数移除相关要素的过滤方法。</li><li id="5325" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated">一种带有交叉验证的递归要素消除算法，用于对要素进行排序并确定最大化模型性能的要素。</li><li id="5110" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me od nn no np bi translated">选择与结果相关的特征的Boruta方法。</li></ol><p id="1ae7" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">该工具设计用于在给定的序列中应用不同的特征选择方法。例如，您可以首先移除相关要素，然后使用递归要素消除来进一步减少所选要素的数量。</p><p id="e758" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">移除大部分常数值的特征</strong></p><p id="408e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">有人可能会说，当你改变样本时，一个特性表现出的变化很小或者没有变化，对于一个模型来说是不太有用的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi gj"><img src="../Images/51a34e615626d328334c3e890170f5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQQk7KdUcPHO-WfGK0Liug.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">表2 </strong> —特征3显示可变性很小的样本数据集。</p></figure><p id="1c33" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">例如，如果您的数据看起来像右侧的表，则选择要素1、要素2、要素4并删除要素3是合理的。这是因为特征3的值在不同的样本之间变化不大。保留特性3会增加模型的复杂性，并且保留它很可能不会观察到任何预测性能。然而，最后一种说法需要经验验证。要建立阈值来标记包含大量常数值的要素，以便移除它们，您需要进行实验。仅仅因为一个特性在90%的时间里包含相同的值，并不意味着它对一个模型没有用。</p><p id="f6b8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">脚本4a中，</strong>我们从导入特征选择工具开始。在第7行，我们定义了一个名为<strong class="ll jd"> step1 </strong>的字典，其中我们指定了要应用的特性选择方法及其参数。例如，要删除具有大部分常量值(95 %或更多)的特性，我们将键设置为<code class="fe mf mg mh mi b">'Constant Features'</code>，将值设置为<code class="fe mf mg mh mi b">{'frac_constant_values': 0.95}</code>。在第10行，我们将<strong class="ll jd">步骤1 </strong>保存在一个名为<strong class="ll jd">步骤</strong>的列表中，然后我们启动一个<code class="fe mf mg mh mi b">FeatureSelector</code>的实例(第13行)。<code class="fe mf mg mh mi b">FeatureSelector</code>有一个<code class="fe mf mg mh mi b">fit()</code>和<code class="fe mf mg mh mi b">transform()</code>方法，很像Sklearn transformer。<code class="fe mf mg mh mi b">fit()</code>方法学习从训练集中选择哪些特征(第16行),而<code class="fe mf mg mh mi b">transform()</code>方法将数据集减少到仅选择的特征(第19行)。<code class="fe mf mg mh mi b">fit()</code>方法将以下内容作为输入:</p><ul class=""><li id="7943" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">X_all_train :熊猫数据帧</li><li id="9851" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated"><strong class="ll jd"> y_all_train </strong>:一个numpy数组</li><li id="473e" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated"><strong class="ll jd">步骤:</strong>字典列表</li></ul><p id="63d4" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><code class="fe mf mg mh mi b">transform()</code>方法将以下内容作为输入:</p><ul class=""><li id="e5ab" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">一个熊猫的数据帧</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4a </strong> —移除大部分常数值的特征。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/af7e285dac9f6d0d670370afd1c6e3a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*6A0lO20xCI0B7U892t5Oew.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图4 </strong> —在IPython控制台中执行的脚本4a。执行时间不到一秒钟。</p></figure><p id="e4db" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><code class="fe mf mg mh mi b">fit</code>的输出将被打印到被移除特征的控制台上。在这种情况下，您可以看到显示一个空列表，这表示在此步骤中没有删除任何功能。</p><p id="8cc8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">移除相关特征</strong></p><p id="e5a3" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">让我们假设，如果一组特征高度相关，我们可以随机选择其中一个，丢弃其余的，而不会丢失太多信息。为了衡量特征之间的相关性，我们将使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Spearman的相关系数</a>。为了移除相关特征，我们将再次使用<code class="fe mf mg mh mi b">FeatureSelector</code>。在<strong class="ll jd">脚本4b </strong>中，我们首先定义一个描述特征选择方法及其参数的字典。例如，在这种情况下，我们将键设置为<code class="fe mf mg mh mi b">'Correlated Features'</code>，将值设置为<code class="fe mf mg mh mi b">{'correlation_threshold': 0.95}</code>。该算法将对相关值大于或等于0.95的要素进行分组。然后，对于每组相关的特征，选择一个特征，其余的被丢弃。在第8行，我们将<strong class="ll jd">步骤1 </strong>保存在一个列表中，然后初始化<code class="fe mf mg mh mi b">FeatureSelector</code>的一个实例(第11行)。最后，我们拟合<code class="fe mf mg mh mi b">FeatureSelector</code>，并转换我们的数据。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4b </strong> —移除高度相关的特征。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/9d06ea4de275c0fa590932d4994b59ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-sIzzd5I1v3bydcacpAww.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图5 </strong> —在IPython控制台中执行的脚本4b。执行时间不到一秒钟。</p></figure><p id="47eb" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">注意<code class="fe mf mg mh mi b">fit()</code>的输出显示所有的<strong class="ll jd"> </strong>重复特征被移除。在我们继续之前，让我们通过检查<code class="fe mf mg mh mi b">X_selected</code>中的特征类型来结束这一部分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/546bb5917dd5d29730a0baef30920f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFwgDMjCbA3_K_n-4XA_1w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图6</strong>——从<strong class="bd ob"> X_all_train </strong>中去除高度相关特征后所选特征的条形图。</p></figure><p id="934f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在图6 的<strong class="ll jd">中，请注意没有删除任何多余或无用的功能。换句话说，如果您仅从群组中移除相关特征，预计仍会有冗余和无用的特征。从积极的方面来看，我们看到10个信息性特征和3个关键性特征是所选特征的一部分。</strong></p><p id="6e20" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">确定相关特征</strong></p><p id="187c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们已经表明，简单的特征选择方法可以快速执行，并可以从我们的队列中删除大部分特征；然而，其余的特征不一定与结果相关或者可能是多余的(见<strong class="ll jd">图6 </strong>)。为了克服这个限制，开发了<a class="ae lh" href="http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/" rel="noopener ugc nofollow" target="_blank"> BorutaPy </a>算法。简而言之，它使用基于树的模型的要素重要性属性来确定携带可用于预测的信息的要素。然而，<a class="ae lh" href="https://explained.ai/rf-importance/" rel="noopener ugc nofollow" target="_blank"> Terrence Parr </a>和其他人对基于树的模型的特征重要性属性的有效性提出了关注。</p><p id="6015" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了选择相关特征，我们首先定义一个基于树的Sklearn估计器(一个具有<code class="fe mf mg mh mi b">feature_importances_</code>属性的<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>、<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener ugc nofollow" target="_blank">额外树分类器</a>、<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">梯度提升分类器</a>)。在我们的例子中，我们将使用随机森林分类器，参见<strong class="ll jd">脚本4c </strong>中的第5–7行。接下来，在第10–15行，我们定义了一个名为<strong class="ll jd"> step1 </strong>的字典，带有一个键<code class="fe mf mg mh mi b">'Relevant Features'</code>。<code class="fe mf mg mh mi b">'Relevant Features'</code>的键值是一个字典，指定交叉验证结果的次数(第10行)和BorutaPy参数。对于每个训练文件夹，建立相关的特征。然后，在每个训练文件夹中被认为相关的特征被选择作为训练集中的相关特征。在第15行，我们修复了<code class="fe mf mg mh mi b">random_state</code>，以便获得确定性的结果。至此，脚本的其余部分应该不言自明了。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4c </strong> —确定与结果相关的特征。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/a38d93a86162723d101f0a93b0f3a3e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1T7OCpooccIzA26NP_Rag.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图7 </strong> —在IPython控制台中执行的脚本4c。执行大约需要5分钟。</p></figure><p id="a290" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">脚本4c </strong>，大约花了5分钟执行。如果你要增加<code class="fe mf mg mh mi b">cv</code>、<code class="fe mf mg mh mi b">n_estimators</code>或<code class="fe mf mg mh mi b">max_iter</code>，期待时间的增加。此外，包含更多样本或特征的大型数据集将增加找出相关特征所需的时间。您在图7 的<strong class="ll jd">中看到的进度条将跟踪每次在训练文件夹中确定相关特征时的进度。所以，如果你一开始没有看到它填满，请耐心等待。</strong></p><p id="e510" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们准备检查所选特性的内容。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/9ba185a57c0f68edf6316609bf25cd3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CPUAYQ_dSDZktkFggvVcQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图8</strong>——在<strong class="bd ob"> X_all_train </strong>中确定相关特征后所选特征的条形图。在这种情况下，估计量是随机森林分类器。</p></figure><p id="3b55" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在图8 的<strong class="ll jd">所示的条形图中，注意所有无用的功能都被删除了。这就是我们对Boruta算法的期望。尽管如此，我们仍然有20个冗余、9个重复、5个信息性和2个关键特征。这意味着我们的Boruta算法的实现不能去除所有冗余的特征，保留一些重复的特征，去除一半的信息特征，并丢弃一个关键特征。<strong class="ll jd">图8 </strong>表明我们不能盲目相信自动化特征选择方法的结果。记住，我们要求所有三个关键特性都必须包含在我们选择的特性中。失去其中一个会导致致命的预言！此外，如果我们使用不同的估计器，例如额外的树分类器，所选择的特征将会不同(见<strong class="ll jd">图9 </strong>)。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/d4ec56b50494bdec979de4268f66a4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Du7AEMR0dcvaNMagB3tpw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图9 </strong> —使用额外树分类器和Boruta算法确定相关特征后，所选特征的条形图。</p></figure><p id="6674" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">当使用Boruta算法选择特征时，重要的是要记住被选择为相关的特征只对被检查的特定模型有意义。此外，当用于对特征进行排序的模型具有过度拟合的能力时，很可能特征的排序是误导的。为了演示过拟合随机森林<strong class="ll jd"> </strong>的缺陷，我们训练了一个模型，并评估了它在训练集和测试集中的性能，参见<strong class="ll jd">脚本4d </strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4d </strong> —训练一个过度适应的随机森林。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ca72f42b683b2ca1d7ec1a5e68dfaea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*NyzbhgbN66dQFeJZoVGcmQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">脚本4d </strong>的输出</p></figure><p id="c65a" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">这里我们可以观察到，训练集中随机森林的训练准确率为100%。当评估测试集中的性能时，我们看到准确率下降到80 %(显然我们过度拟合)。检查过拟合随机森林的特征重要性，发现许多无用特征(随机噪声)的等级高于有用和冗余特征，见<strong class="ll jd">图10 </strong>。理想情况下，无用的功能应该排在最低的位置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/70118db16830727b5dcebf033b2e7363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aaetUlavAZB7l6N2ZSPzbA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图10 </strong> —一个过度生长的随机森林的特征重要性。</p></figure><p id="681f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">鉴于过度拟合模型的排名特征不可靠，我建议首先调整您的模型，交叉验证结果，并评估其在训练和测试集中的性能，参见<strong class="ll jd">脚本4e </strong>。最小化过度拟合后，使用模型选择特征。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4e </strong> —调整分类器。要将GridSearchCV使用的折叠数更改为10，请在第19行设置cv = 10。同样，你也可以改变得分。执行大约需要6分钟。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/40e32045c43e7baa5e953b0e56045dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*Hmo4rJqBq0Qz1-SKq7liRg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">脚本4e的输出</strong></p></figure><p id="68c3" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">结果表明，训练集的准确率为96%，测试集的准确率为81%。他妈的，我们还是太合适了！让我们检查一下调整后的随机森林是如何排列特性的，参见<strong class="ll jd">图11 </strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/35e41f53711d2a9fcb161c5e1a4d982c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOLjW242RFF-7haWRIw8UQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图11 </strong> —显示了调整后的随机森林的重要性。</p></figure><p id="cbb7" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">有意思！我们得到了比预期好得多的结果。尽管调整后的随机森林过度拟合，但它在排列特性方面做得更好。例如，我们可以看到，关键的功能排名很高，而大多数无用的功能排名垫底。</p><p id="62d1" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">要确定已调优随机森林的相关特性，请运行<strong class="ll jd">脚本4f </strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4f </strong> —确定与优化随机森林的结果相关的功能。执行大约需要5分钟。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/d9a38d5b25b2d1c38721a2731117e1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKm2Bj1ToX2qrBesKDIQbA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图12 </strong> —用调整随机森林的Boruta算法确定的相关特征的条形图。</p></figure><p id="ac20" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">图12 </strong>中，我们可以看到所选特征包含了对我们非常重要的三个关键特征。此外，请注意，与使用失调随机森林获得的结果相比，您可以更信任这些结果。</p><p id="917a" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">现在，我们将只使用选定的功能来训练一个随机森林，参见<strong class="ll jd">脚本4g </strong>。我们将随机森林的参数设置为之前在<strong class="ll jd">脚本4e </strong>中确定的参数。让我提一下，使用选择的特性来调整随机森林是一个好主意。记得当我们在<strong class="ll jd">脚本4e </strong>中调整随机森林时，我们使用了所有103个特性。现在选的特征大概有35个。然而，我们将跳过这一步，但你应该这样做。最后，我们将评估它在训练集和测试集中的性能。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4g </strong> —使用所选功能评估模型的性能。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/81e3ee94def78add29cc4eb266911b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*GyU3WDecVeft5pAb_txIEA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">脚本4g </strong>的输出</p></figure><p id="057c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在我们讨论结果之前，让我提醒你我们开始的内容。当训练具有所有特征的失调随机森林时，我们在训练集中获得了100 %的准确度，在测试集中获得了80 %的准确度。让我们称之为我们的基线。在“调优”随机森林(使特征排序更加可靠)，用Boruta算法选择特征，然后用选择的特征训练随机森林之后，我们在训练集中获得了94%的准确率，在测试集中获得了83%的准确率(见上面的<strong class="ll jd">脚本4g </strong>的输出)。我们可以从这些结果中得出几个结论:</p><ul class=""><li id="1f09" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">调整你的模型以减少过度拟合。</li><li id="4d80" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">经过优化的模型可以更准确地对功能进行排序。</li><li id="64ea" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">应用特征选择方法有助于减少过度拟合并提高模型的性能。</li></ul><p id="c51e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">尽管我们仍然有相当多的过度拟合，但我们已经在性能上取得了一些进展，我们减少了过度拟合，并且降低了模型的复杂性。我会说这是一场胜利，但还不够好。</p><p id="4f6c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">确定最大化模型性能的特征</strong></p><p id="a386" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们还可以通过应用递归特征消除(RFE)算法来选择特征。RFE方法确定最大化模型性能所需的最小要素子集。然而，你冒着丢掉有意义的特性的风险——所以记住这一点。因此，如果您的任务是确定对结果重要的相关特征，那么使用RFE可能不合适。算法基本上是这样工作的。首先，训练一个可以使用数据集中所有可用要素对要素进行分级的模型。其次，衡量模型的性能。第三，对特征进行排序，并移除排序最低的特征。重复步骤1至步骤3，直到所有功能都用尽。通过在跟踪模型性能的同时迭代移除特征，您可以确定实现最高性能所需的特征数量。</p><p id="5fa7" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了确定将最大化模型性能的特征，我们首先定义一个基于树的Sklearn估计器(一个<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>、<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener ugc nofollow" target="_blank">额外树分类器</a>、<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">梯度提升分类器</a>)，具有<code class="fe mf mg mh mi b">feature_importances_</code>或<code class="fe mf mg mh mi b">coef_</code>属性。确保您的基本估计值不会过度拟合您的数据。在我们的例子中，我们将使用一个随机森林分类器，带有在<strong class="ll jd">脚本4e </strong>中找到的调整参数，参见<strong class="ll jd">脚本4h </strong>中的第5–8行。接下来，在第11–15行，我们定义了一个名为<strong class="ll jd"> step1 </strong>的字典，其中包含一个关键字<code class="fe mf mg mh mi b">'RFECV Features'</code>。<code class="fe mf mg mh mi b">'RFECV Features'</code>的键值是一个字典，指定交叉验证结果的次数(第11行)和其他<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV </a>参数。RFECV对象是RFE方法的Sklearn实现，其中结果被交叉验证。至此，脚本的其余部分应该不言自明了。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4h </strong> —确定最大化分类器性能的特征。这将需要大约5分钟来运行。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/87aa7634d9332c316634a7cefd82badd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGseVBy9kYCulifvGN9KHQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图13 </strong> —使用递归消除算法确定的最大化随机森林分类器性能的选定特征的条形图。结果是交叉验证的。</p></figure><p id="cc82" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">图13 </strong>显示了由Sklean RFECV方法确定的选定特征。不幸的是，我们仍然有很多冗余和重复的功能。<code class="fe mf mg mh mi b">FeatureSelector</code>类有一个<code class="fe mf mg mh mi b">rfecv</code>属性，保存适合的<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV Sklearn </a>对象。通过检查拟合的RFECV对象的内容，可以更深入地了解RFE特征的结果。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本4f </strong> —可视化RFECV的结果。所有这些代码都是为了做一个图形。呸！如果更改数据集或基本估计量，可能需要调整xlim()、ylim()和ax.axhline()。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/b71bdb6eb124184597e770b27f7ecb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ys5Ky_vIsFj2W7iqWtzUQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图14 </strong> —作为特征数量函数的精度。该模型的性能峰值约为40个特征。</p></figure><p id="aa96" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<strong class="ll jd">图14 </strong>中，显示了模型的性能与多个特征的函数关系。如您所见，性能峰值约为40个特征，精度约为0.80。由于所选特征的数量约为50(见<strong class="ll jd">图13 </strong>)，我们可以得出结论，RFECV Sklearn对象高估了我们最大化模型性能所需的最小特征数量。在我看来，如果你简单地选择排名前13位的特征，模型的准确率大约为79%，你会更好。然而，RFECV Skelarn对象确实为您提供了这些信息。如果你急于知道排名前13位的特性是什么，你将需要编写你自己版本的RFE算法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/a6e9b957cd9ee1fe158a8b97ab4be233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kR2_xa84KANSBrH6MWtmg.png"/></div></div></figure><h1 id="d2d4" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">构建特征选择管道</h1><p id="967f" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">在上一节中，我们了解到我们需要谨慎和警惕自动特征选择方法的危险。通过应用它们，然后盲目地相信结果，你可能最终会犯下可怕的错误。此外，没有一个单一的特征选择算法产生了我们想要的结果——只选择了信息和关键的特征。</p><p id="8c4b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了改进我们的特征选择，我们可以将特征选择算法作为一系列步骤来应用。例如，我们可以首先移除中度到高度相关的特征(第11行)，然后应用RFE方法，如<strong class="ll jd">脚本5a </strong>所示。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本5a </strong> —特征选择管道。首先，移除中度到高度相关的特征。然后，确定最大化模型性能的特征。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/121c6de9193283a30e70a87b47409ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSmqHM0vFYwVPmJCyu70JA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图15 </strong> —使用递归消除算法确定的最大化随机森林分类器性能的选定特征的条形图。这一次，我们首先移除中度到高度相关的特征，然后应用RFE方法。</p></figure><p id="d020" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">这一次，在所选功能中，我们有8个信息性功能、所有3个关键功能和20个冗余功能。这些是迄今为止我们得到的最好的结果，但是让我们再深入研究一下。请记住，<code class="fe mf mg mh mi b">FeatureSelector</code>对象将适配的RFECV对象保存在其<code class="fe mf mg mh mi b">rfecv</code>属性中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/ce987382893f5bb4ce49736b1a6297cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-b4LOfVmX1BCAgPzQCZWg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">图16 — </strong>作为特征数量函数的精度。该模型的性能峰值约为20个特征。</p></figure><p id="bb0c" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">因此，我们已经可以从性能曲线中看到，模型的性能在20个特性附近达到峰值。此外，我们可以得出结论，15个特性将足以获得高性能。现在，我们使用<strong class="ll jd">脚本5b </strong>中的选定功能来评估模型的性能。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">脚本5b —具有选定特征的模型评估。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/63053e907ee608796349207f81cabf32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*30uV4AONokHTRNBSmojSVw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ob">脚本5b </strong>的输出</p></figure><p id="b03a" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">从<strong class="ll jd">脚本5b </strong>的输出中，我们可以看到我们将训练集中的准确率提高到了84%。我们缓慢但确实取得了进展。您可以使用<strong class="ll jd">脚本4e </strong>再次调优该模型，看看您是否能获得更多的性能增益。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi li"><img src="../Images/a6e9b957cd9ee1fe158a8b97ab4be233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kR2_xa84KANSBrH6MWtmg.png"/></div></div></figure><h1 id="f388" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">结束语</h1><p id="d58d" class="pw-post-body-paragraph lj lk it ll b lm nb kd lo lp nc kg lr ls nd lu lv lw ne ly lz ma nf mc md me im bi translated">那么，我们究竟如何设计一个特征选择管道，以产生所有的信息和关键特征，而没有多余或无用的特征呢？嗯，简单的回答是我不知道。如果你想解决这个问题，你可以把你的算法带到银行去。你应该问的问题是，“我真的需要确定最有用的特征，还是我想用最小的特征子集来训练模型？”如果你的答案是“确定最有用的特性”，那么这里介绍的方法将有助于你寻找最佳特性。然而，如果你只关心用较小的特征子集训练最好的可能模型，这里给出的特征选择算法将完成这项工作。</p><p id="cde8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">如果您想使用特征选择工具，请确保您的工作目录中有该工具，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1ad9340f8a7c775f721db26334619c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*8IxIPKaBWtYxrSArwc6hWQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从<a class="ae lh" href="https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-" rel="noopener ugc nofollow" target="_blank"> my Github </a>下载功能选择工具。</p></figure><p id="8546" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在使用它选择特征之前，请确保满足以下条件:</p><ul class=""><li id="fe4f" class="nh ni it ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">加载数据并对所有分类变量进行编码。</li><li id="8151" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">注意任何缺失值或异常值。如果不这样做，您可能会得到一个严重的错误。</li><li id="742c" class="nh ni it ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated">将特征矩阵<strong class="ll jd"> X </strong>存储到pandas DataFrame对象中。目标变量<strong class="ll jd"> y </strong>应该是一个numpy数组。</li></ul><p id="9883" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">准备好数据后，您可以使用<strong class="ll jd">脚本6a </strong>来指导您的特性选择过程。您将需要试验来确定应用什么特征选择算法。在<strong class="ll jd">脚本6a </strong>中，我们首先调整我们的模型(第5–29行)，然后我们定义四种特征选择方法并应用它们(第32–56行)。然后，我们使用选定的特征来训练模型并评估其性能(第59–75行)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="ak">脚本6a </strong> —模型调整、特征选择、模型训练和评估。</p></figure><p id="aab3" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在<a class="ae lh" href="https://www.linkedin.com/in/frank-ceballos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>找到我。下次见！小心每天编码！</p><div class="op oq gp gr or os"><a href="https://www.frank-ceballos.com/" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">弗兰克·塞瓦洛斯</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">图表</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.frank-ceballos.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://www.linkedin.com/in/frank-ceballos" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">Frank Ceballos -威斯康星医学院博士后| LinkedIn</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">我是威斯康星医学院的博士后研究员，在那里我分析高维复杂的临床数据…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.linkedin.com</p></div></div></div></a></div><div class="op oq gp gr or os"><a href="https://github.com/frank-ceballos" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">弗兰克-塞瓦洛斯-概述</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">在GitHub上注册你自己的个人资料，这是托管代码、管理项目和构建软件的最佳地方…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">github.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg lb os"/></div></div></a></div></div></div>    
</body>
</html>