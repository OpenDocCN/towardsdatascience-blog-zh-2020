<html>
<head>
<title>Gradient Boosting Classification explained through Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过 Python 解释梯度增强分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d?source=collection_archive---------4-----------------------#2020-09-05">https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d?source=collection_archive---------4-----------------------#2020-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/0b1bb7e97c7463ce7d62c4cef2fb3311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AaVX5TGtR5xwUEIw3DU7mw.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">马切伊·鲁明凯维奇在<a class="ae kc" href="https://unsplash.com/s/photos/rocket-boost?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e1c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的<a class="ae kc" href="https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e" rel="noopener">上一篇文章</a>中，我讨论并经历了一个用于回归的梯度推进的 python 实例。在这篇文章中，我想讨论梯度推进是如何用于分类的。如果你没有读过那篇文章，没关系，因为我会重申我在上一篇文章中讨论的内容。所以，让我们开始吧！</p><h1 id="4734" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">集成方法</h1><p id="613c" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">通常，你可能有几个好的预测器，你想把它们都用上，而不是痛苦地选择一个，因为它有 0.0001 的准确性增加。来了<em class="me">合奏学习。</em>在集成学习中，不是使用单个预测器，而是聚合数据中的多个预测器和训练及其结果，通常比使用单个模型给出更好的分数。例如，一个随机森林就是一组打包(或粘贴)的决策树。</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/d372964cd39a9f56bb10cf77655c992b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZC6cupczI3rwBLQJ0D6fg.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Samuel Sianipar 在<a class="ae kc" href="https://unsplash.com/s/photos/orchestra?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f3ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以把合奏方法想象成一个管弦乐队；不是只有一个人演奏一种乐器，而是多人演奏不同的乐器，通过组合所有的音乐组合，音乐听起来通常比单人演奏要好。</p><p id="814d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然梯度增强是一种集成学习方法，但它更具体地说是一种<em class="me">增强</em>技术。那么，是什么在推动？</p><h1 id="0017" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">助推</h1><p id="3b21" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><em class="me"> Boosting </em>是一种特殊类型的集成学习技术，它通过将几个<em class="me">弱学习器(</em>准确性差的预测器<em class="me"> ) </em>组合成一个强学习器(准确性强的模型)来工作。这是通过每个模型关注其前任的错误来实现的。</p><p id="b8e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两种最流行的升压方法是:</p><ul class=""><li id="a431" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">适应性增强(你可以在这里阅读我的文章<a class="ae kc" href="https://divingdeep.hashnode.dev/adaptive-boosting-simply-explained-through-python" rel="noopener ugc nofollow" target="_blank"/>)</li><li id="0204" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">梯度推进</li></ul><p id="3ab0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将讨论梯度推进。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="35b9" class="lb lc iq bd ld le nf lg lh li ng lk ll lm nh lo lp lq ni ls lt lu nj lw lx ly bi translated">梯度推进</h1><p id="ebfb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在梯度推进中，每个预测器都试图通过减少误差来改进其前任。但是梯度推进背后的有趣想法是，它不是在每次迭代中对数据拟合预测器，而是实际上对前一个预测器产生的残差拟合一个新的预测器。我们来看一下<em class="me"> </em>梯度推进分类工作原理的逐步示例:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/f0e72bdef0106eb02e33b29eeb16b4c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65jJVTsS3TAtX8I5kR8dlg.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@lindsayhenwood?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">林赛·亨伍德</a>在<a class="ae kc" href="https://unsplash.com/s/photos/steps?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><ol class=""><li id="16cb" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la nl mq mr ms bi translated">为了对数据进行初始预测，算法将获得目标特征的几率的<em class="me">对数。这通常是真值(值等于 1)的数量除以假值(值等于 0)的数量。</em></li></ol><p id="268a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，如果我们有一个包含 6 个实例的乳腺癌数据集，其中 4 个实例是患有乳腺癌的人(4 个目标值= 1)，2 个实例是没有患乳腺癌的人(2 个目标值= 0)，那么 log(odds) = log(4/2) ~ 0.7。这是我们的基本估计。</p><ol class=""><li id="ec6a" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la nl mq mr ms bi translated">一旦有了对数(赔率)，我们通过使用逻辑函数将该值转换成概率，以便进行预测。如果我们继续我们之前的 0.7 的对数(比值)值的例子，那么逻辑函数也将等于 0.7 左右。</li></ol><p id="7091" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于该值<em class="me">大于 0.5，</em>该算法将为每个实例预测 0.7 作为其基本估计值。将对数(赔率)转换成概率的公式如下:</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="acc2" class="nr lc iq nn b gy ns nt l nu nv">e * log(odds) / (1 + e * log(odds))</span></pre><ol class=""><li id="4811" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la nl mq mr ms bi translated">对于训练集中的每个实例，它计算该实例的<em class="me">残差</em>，或者换句话说，观察值减去预测值。</li><li id="59fe" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la nl mq mr ms bi translated">一旦它完成了这些，它就建立一个新的决策树，实际上试图预测先前计算的残差。然而，与梯度推进回归相比，这是它变得稍微棘手的地方。</li></ol><p id="1039" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建决策树时，允许有固定数量的叶子。这可以由用户设置为一个参数，通常在 8 到 32 之间。这导致了两种可能的结果:</p><ul class=""><li id="b9ba" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">多个实例落在同一片叶子中</li><li id="6774" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">单个实例有自己的叶</li></ul><p id="df60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与回归的梯度推进不同，我们可以简单地平均实例值以获得输出值，并将单个实例作为其自己的一片叶子，我们必须使用一个公式来转换这些值:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/1e0ff39aae352588253e5c6a6ee55261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EdvDbO8ttkIdwkBG0Gq7Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">鸣谢:博客空间</p></figure><p id="027c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">σ符号表示“的和”，而<em class="me"> PreviousProb </em>指的是我们之前计算的概率(在我们的例子中，为 0.7)。我们对树中的每一片叶子都应用这种变换。我们为什么要这样做？因为请记住，我们的基本估计量是一个对数(赔率)，而我们的树实际上是建立在概率上的，所以我们不能简单地将它们相加，因为它们来自两个不同的来源。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h2 id="f171" class="nr lc iq bd ld nx ny dn lh nz oa dp ll ko ob oc lp ks od oe lt kw of og lx oh bi translated">做预测</h2><p id="6ee8" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">现在，为了进行新的预测，我们做两件事:</p><ol class=""><li id="5ac8" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la nl mq mr ms bi translated">获取训练集中每个实例的对数(赔率)预测</li><li id="4db3" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la nl mq mr ms bi translated">将预测转换成概率</li></ol><p id="604b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于训练集中的每个实例，进行预测的公式如下:</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="94ff" class="nr lc iq nn b gy ns nt l nu nv">base_log_odds + (<strong class="nn ir">learning_rate * </strong>predicted residual value)</span></pre><p id="dc77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me"> learning_rate </em>是一个超参数，用于缩放每棵树的贡献，牺牲偏差以获得更好的方差。换句话说，我们将这个数字乘以预测值，这样我们就不会过度拟合数据。</p><p id="e869" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们计算出对数(赔率)预测，我们现在必须使用前面将对数(赔率)值转换为概率的公式将其转换为概率。</p><h2 id="4706" class="nr lc iq bd ld nx ny dn lh nz oa dp ll ko ob oc lp ks od oe lt kw of og lx oh bi translated">重复&amp;对看不见的数据进行预测</h2><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/fdeccb4cdbfd484fc88369fc9f77eeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXa8_WMhfNtMH37iWPnP5Q.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@brett_jordan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae kc" href="https://unsplash.com/s/photos/repeat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="5847" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完成这个过程后，我们计算树的新残差，并创建一个新的树来拟合新的残差。再次重复该过程，直到达到某个预定义的阈值，或者残差可以忽略。</p><p id="34e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们训练了 6 棵树，并且我们想要对一个看不见的实例进行新的预测，那么它的伪代码将是:</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="f765" class="nr lc iq nn b gy ns nt l nu nv">X_test_prediction = base_log_odds + </span><span id="1482" class="nr lc iq nn b gy oj nt l nu nv">(<strong class="nn ir">learning_rate * </strong>tree1_scaled_output_value) + <br/>(<strong class="nn ir">learning_rate * </strong>tree2_scaled_output_value) +<br/>(<strong class="nn ir">learning_rate * </strong>tree3_scaled_output_value) +<br/>(<strong class="nn ir">learning_rate * </strong>tree4_scaled_output_value) +<br/>(<strong class="nn ir">learning_rate * </strong>tree5_scaled_output_value) +<br/>(<strong class="nn ir">learning_rate * </strong>tree6_scaled_output_value) +</span><span id="d8fc" class="nr lc iq nn b gy oj nt l nu nv">prediction_probability = <br/>e*X_test_prediction / (1 + e*X_test_prediction)</span></pre><p id="3b94" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，现在你应该对用于分类的梯度推进的底层机制有所了解了，让我们开始编码来巩固这些知识吧！</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ok"><img src="../Images/843ea9feaab2c5f156ca199e97a54b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45J9_prmiMi6bgyXCjrP1A.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">罗布·萨米恩托在<a class="ae kc" href="https://unsplash.com/s/photos/brick-and-cement?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="021d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用 Scikit-Learn 进行梯度增强分类</h1><p id="260c" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们将使用预先构建到 scikit-learn 中的乳腺癌数据集作为示例数据。首先，让我们弄清楚一些重要的东西:</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="9e91" class="nr lc iq nn b gy ns nt l nu nv">import pandas as pd<br/>import numpy as np</span><span id="053c" class="nr lc iq nn b gy oj nt l nu nv">from sklearn.metrics import classification_report<br/>from sklearn.model_selection import KFold<br/>from sklearn.datasets import load_breast_cancer</span><span id="64ff" class="nr lc iq nn b gy oj nt l nu nv">from sklearn.ensemble import GradientBoostingClassifier</span></pre><p id="ddf2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们只是导入 pandas、numpy、我们的模型和一个评估我们模型性能的指标。</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="b2c5" class="nr lc iq nn b gy ns nt l nu nv">df = pd.DataFrame(load_breast_cancer()['data'],<br/>columns=load_breast_cancer()['feature_names'])</span><span id="66c8" class="nr lc iq nn b gy oj nt l nu nv">df['y'] = load_breast_cancer()['target']<br/>df.head(5)</span></pre><p id="1dac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了方便起见，我们将数据转换成 DataFrame，因为这样更容易操作。请随意跳过这一步。</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="a9a8" class="nr lc iq nn b gy ns nt l nu nv">X,y = df.drop('y',axis=1),df.y</span><span id="8c18" class="nr lc iq nn b gy oj nt l nu nv">kf = KFold(n_splits=5,random_state=42,shuffle=True)</span><span id="5e21" class="nr lc iq nn b gy oj nt l nu nv">for train_index,val_index in kf.split(X):<br/>    X_train,X_val = X.iloc[train_index],X.iloc[val_index],<br/>    y_train,y_val = y.iloc[train_index],y.iloc[val_index],</span></pre><p id="2ed5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们定义了我们的特征和标签，并使用 5 折交叉验证将 ur 数据分成一个训练和验证。</p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="d1ef" class="nr lc iq nn b gy ns nt l nu nv">gradient_booster = GradientBoostingClassifier(learning_rate=0.1)<br/>gradient_booster.get_params()</span><span id="8a1f" class="nr lc iq nn b gy oj nt l nu nv">OUT:</span><span id="201c" class="nr lc iq nn b gy oj nt l nu nv">{'ccp_alpha': 0.0,<br/> 'criterion': 'friedman_mse',<br/> 'init': None,<br/> 'learning_rate': 0.1,<br/> 'loss': 'deviance',<br/> 'max_depth': 3,<br/> 'max_features': None,<br/> 'max_leaf_nodes': None,<br/> 'min_impurity_decrease': 0.0,<br/> 'min_impurity_split': None,<br/> 'min_samples_leaf': 1,<br/> 'min_samples_split': 2,<br/> 'min_weight_fraction_leaf': 0.0,<br/> 'n_estimators': 100,<br/> 'n_iter_no_change': None,<br/> 'presort': 'deprecated',<br/> 'random_state': None,<br/> 'subsample': 1.0,<br/> 'tol': 0.0001,<br/> 'validation_fraction': 0.1,<br/> 'verbose': 0,<br/> 'warm_start': False}</span></pre><p id="2b17" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有很多参数，所以我只讨论最重要的:</p><ul class=""><li id="a44d" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated"><strong class="kf ir">标准</strong>:用于寻找分割数据的最佳特征和阈值的损失函数</li><li id="3976" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><strong class="kf ir"> learning_rate </strong>:该参数衡量每棵树的贡献</li><li id="081d" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><strong class="kf ir"> max_depth </strong>:每棵树的最大深度</li><li id="87ed" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><strong class="kf ir"> n_estimators </strong>:要构建的树的数量</li><li id="63cd" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><strong class="kf ir"> init:初始估计量。</strong>默认情况下，它是将对数(赔率)转换成概率(就像我们之前讨论的那样)</li></ul></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="4ac9" class="nr lc iq nn b gy ns nt l nu nv">gradient_booster.fit(X_train,y_train)</span><span id="09c0" class="nr lc iq nn b gy oj nt l nu nv">print(classification_report(y_val,gradient_booster.predict(X_val)))</span><span id="5893" class="nr lc iq nn b gy oj nt l nu nv">OUT:</span><span id="76a9" class="nr lc iq nn b gy oj nt l nu nv">precision    recall  f1-score   support</span><span id="98a0" class="nr lc iq nn b gy oj nt l nu nv">0       0.98      0.93      0.96        46<br/>1       0.96      0.99      0.97        67</span><span id="d66f" class="nr lc iq nn b gy oj nt l nu nv">    accuracy                           0.96       113<br/>   macro avg       0.97      0.96      0.96       113<br/>weighted avg       0.96      0.96      0.96       113</span></pre><p id="f16b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，96%的准确率！</p><p id="d554" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这篇文章已经帮助你理解了梯度推进分类(以某种形式)。我祝你在 ML 的努力中一切顺利，并记住；知之甚少却知之甚详的人，胜过知之甚多却知之甚少的人！</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/79f050733d12bcc83ce13aaceff626cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqPmYQLNFGwldjEokOd_HA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com/@kellysikkema?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Kelly Sikkema </a>在<a class="ae kc" href="https://unsplash.com/s/photos/thank-you?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>