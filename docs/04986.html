<html>
<head>
<title>A Complete Beginners Guide to Document Similarity Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文档相似性算法初学者完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-beginners-guide-to-document-similarity-algorithms-75c44035df90?source=collection_archive---------18-----------------------#2020-04-30">https://towardsdatascience.com/a-complete-beginners-guide-to-document-similarity-algorithms-75c44035df90?source=collection_archive---------18-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b3ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解欧几里得距离、余弦相似度和皮尔逊相关度背后的代码和数学，为推荐引擎提供动力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/358c9e0d8da37e8306613c27dea5a3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yB-r2k7m9eW7Z5BzVCzlAw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jtylernix?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">泰勒·尼克斯</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="6f16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经在产品中部署了一些推荐系统。毫不奇怪，最简单的被证明是最有效的。</p><p id="b642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数推荐系统的核心是协同过滤。协同过滤的核心是文档相似性。</p><p id="0828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将介绍计算文档相似性的 3 种算法。</p><p id="02fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1)欧几里德距离<br/> 2)余弦相似度<br/> 3)皮尔逊相关系数</p><p id="bb03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使是对它们如何工作的一般直觉也会帮助你选择合适的工具来工作，并建立一个更智能的引擎。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5aec" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">这些算法的共同点</strong></h1><p id="14ce" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在我们讨论算法时，请记住这些警告。</p><ol class=""><li id="a4e3" class="mz na it lb b lc ld lf lg li nb lm nc lq nd lu ne nf ng nh bi translated">计算在对象的<a class="ae ky" rel="noopener" target="_blank" href="/lets-understand-the-vector-space-model-in-machine-learning-by-modelling-cars-b60a8df6684f">矢量表示上进行。每个对象必须首先转换成一个数字向量。</a></li><li id="1ae0" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">一次计算一对向量之间的相似性/距离。</li><li id="08bc" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">无论算法如何，特征选择都会对结果产生巨大影响。</li></ol></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a1ae" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">欧几里得距离</h1><h2 id="0561" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">简单地</h2><p id="8872" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">欧几里德距离是多维空间中两点之间的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/642e0fe8f3c3e67280a47fb20f63ce8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSHZPW5REDYgzTBO4Ukd7w.png"/></div></div></figure><p id="0fd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更近的点彼此更相似。进一步的点彼此更不同。所以以上，马里奥和卡洛斯比卡洛斯和珍妮更相似。</p><p id="8dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我有意选择了二维(又名。特点:【财富，朋友】)因为容易剧情。我们仍然可以计算超过 2 维的距离，但是需要一个公式。</p><p id="0f13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，这种方法作为距离测量是有意义的。您将文档绘制成点，并且可以用尺子测量它们之间的距离。</p><h2 id="926a" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated"><strong class="ak">用欧几里德距离比较城市</strong></h2><p id="4b38" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们比较三个城市:纽约、多伦多和巴黎。</p><p id="9f72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多伦多= <code class="fe oa ob oc od b">[3,7]</code> <br/>纽约= <code class="fe oa ob oc od b">[7,8]</code> <br/>巴黎= <code class="fe oa ob oc od b">[2,10]</code></p><p id="4ad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征向量包含两个特征:<code class="fe oa ob oc od b">[population, temperature]</code>。人口以百万计。温度单位是摄氏度。</p><p id="ae07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，因为我们再次将问题框定为二维，我们可以用尺子测量点之间的距离，但我们将使用这里的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/c8a11b3a6ed0fffec3f9499daf966884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yp9lLpqi7Dj4X8edIO2x2A.png"/></div></div></figure><p id="b14e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论是 2 维还是 1000 维，该公式都适用。</p><h2 id="2e33" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">在 Python 中实现欧几里德距离</h2><p id="d91c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">没有人比我更讨厌数学符号，但下面是欧几里得距离的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/092383be46b47b495990c114db252ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gufZbPFOABt-xVfZ1t8u9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原谅我的徒手画</p></figure><p id="fede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们写一个函数来实现它并计算两点之间的距离。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="aa4d" class="nn md it od b gy ok ol l om on">from math import sqrt</span><span id="eb0f" class="nn md it od b gy oo ol l om on">def euclidean_dist(doc1, doc2):<br/>    '''<br/>    For every (x,y) pair, square the difference<br/>    Then take the square root of the sum<br/>    '''<br/>    pre_square_sum = 0<br/>    for idx,_ in enumerate(doc1):<br/>        pre_square_sum += (doc1[idx] - doc2[idx]) ** 2<br/>    <br/>    return sqrt(pre_square_sum)</span><span id="0788" class="nn md it od b gy oo ol l om on"><br/>toronto = [3,7]<br/>new_york = [7,8]</span><span id="1d0c" class="nn md it od b gy oo ol l om on"><br/>euclidean_dist(toronto, new_york)<br/>#=&gt; 4.123105625617661</span></pre><p id="bbe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧。多伦多和纽约之间的距离是<code class="fe oa ob oc od b">4.12</code>。</p><h2 id="9500" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">使用 Sklearn 的欧几里德距离</h2><p id="5dcf" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们上面写的函数效率有点低。Sklearn 使用 Numpy 实现了一个更快的版本。在生产中，我们只用这个。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="b099" class="nn md it od b gy ok ol l om on">toronto = [3,7]<br/>new_york = [7,8]</span><span id="a59f" class="nn md it od b gy oo ol l om on">import numpy as np<br/>from sklearn.metrics.pairwise import euclidean_distances</span><span id="1daa" class="nn md it od b gy oo ol l om on">t = np.array(toronto).reshape(1,-1)<br/>n = np.array(new_york).reshape(1,-1)</span><span id="b92e" class="nn md it od b gy oo ol l om on">euclidean_distances(t, n)[0][0]<br/>#=&gt; 4.123105625617661</span></pre><p id="c41c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，它需要数组而不是列表作为输入，但是我们得到了相同的结果。布雅！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="41c6" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">余弦相似性</h1><h2 id="82c2" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">简单地</h2><p id="b032" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">余弦相似度是多维空间中两点之间夹角的余弦。角度越小的点越相似。角度越大的点差别越大。</p><p id="16cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然难以理解，余弦相似性解决了欧几里德距离的一些问题。即大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/81f6fcf96a32955423160e2e3094fcc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D93WU8HLfHGaZg4-0Ij-Wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一篇文章中提到“烹饪”和“餐馆”的次数</p></figure><p id="3e43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，我们根据三个文档包含单词“cooking”和“restaurant”的次数来比较它们。</p><p id="f967" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欧几里德距离告诉我们博客和杂志比博客和报纸更相似。但我认为这是误导。</p><p id="c8cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">博客和报纸可以有相似的内容，但在欧几里得意义上是遥远的，因为报纸更长，包含更多的单词。</p><p id="a687" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实中，他们都更多地提到“餐馆”而不是“烹饪”,可能彼此更相似。余弦相似度不属于这个陷阱。</p><h2 id="d4d8" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated"><strong class="ak">用余弦相似度比较书籍和文章</strong></h2><p id="9459" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们完成上面的例子。我们将基于特定单词的计数来比较文档</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="f299" class="nn md it od b gy ok ol l om on">magazine_article = [7,1]<br/>blog_post = [2,10]<br/>newspaper_article = [2,20]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/43c4d97ddbc224a1c28927de2f02924e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hcVwixCWDjTP44TImHegoA.png"/></div></div></figure><p id="9921" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在从原点开始，取它们之间角度的余弦，而不是取它们之间的距离。现在，即使只是目测，博客和报纸看起来更相似。</p><h2 id="2563" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">在 Python 中实现余弦相似性</h2><p id="98d3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">注意余弦相似度不是角度本身，而是角度的余弦。因此，较小的角度(小于 90 度)返回较大的相似性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/805f889edc84494a47bb3efafc633cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YObjpgE_1qOTavYe9as-sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cosine_similarity</a></p></figure><p id="455e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们自己实现一个函数来计算这个。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="32b5" class="nn md it od b gy ok ol l om on">import numpy as np<br/>from math import sqrt</span><span id="2097" class="nn md it od b gy oo ol l om on">def my_cosine_similarity(A, B):<br/>    numerator = np.dot(A,B)<br/>    denominator = sqrt(A.dot(A)) * sqrt(B.dot(B))<br/>    return numerator / denominator<br/>    <br/>    <br/>magazine_article = [7,1]<br/>blog_post = [2,10]<br/>newspaper_article = [2,20]</span><span id="a3b8" class="nn md it od b gy oo ol l om on">m = np.array(magazine_article)<br/>b = np.array(blog_post)<br/>n = np.array(newspaper_article)<br/>    <br/>print( my_cosine_similarity(m,b) ) #=&gt; 0.3328201177351375<br/>print( my_cosine_similarity(b,n) ) #=&gt; 0.9952285251199801<br/>print( my_cosine_similarity(n,m) ) #=&gt; 0.2392231652082992</span></pre><p id="3840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们看到，博客和报纸确实更加相似。</p><h2 id="a44e" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">与 Sklearn 的余弦相似性</h2><p id="9e14" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在生产中，我们最好导入 Sklearn 更高效的实现。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="864b" class="nn md it od b gy ok ol l om on">from sklearn.metrics.pairwise import cosine_similarity</span><span id="7285" class="nn md it od b gy oo ol l om on">m = np.array(magazine_article).reshape(1,-1)<br/>b = np.array(blog_post).reshape(1,-1)<br/>n = np.array(newspaper_article).reshape(1,-1)</span><span id="2110" class="nn md it od b gy oo ol l om on">print( cosine_similarity(m,b)[0,0] ) #=&gt; 0.3328201177351375<br/>print( cosine_similarity(b,n)[0,0] ) #=&gt; 0.9952285251199801<br/>print( cosine_similarity(n,m)[0,0] ) #=&gt; 0.2392231652082992</span></pre><p id="965e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相同的价值观。太好了！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b4bc" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">皮尔逊相关</h1><p id="0739" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这通常量化了两个变量之间的关系。比如教育和收入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/bd682cc2c9cd5e324ca1912772510334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z4xgxtKMhifLly527yYVkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">完全虚构的数据</p></figure><p id="1eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们也可以用它来度量两个文档之间的相似性，我们将第一个文档的向量视为<code class="fe oa ob oc od b">x</code>，将第二个文档的向量视为<code class="fe oa ob oc od b">y</code>。</p><p id="c111" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为皮尔逊相关系数<code class="fe oa ob oc od b">r</code>返回 1 和-1 之间的值，所以皮尔逊距离可以计算为<code class="fe oa ob oc od b">1 — r</code>以返回 0 和 2 之间的值。</p><h2 id="9217" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">用 Python 实现皮尔逊相关系数</h2><p id="073a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们自己实现这个公式来理解它是如何工作的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/192c8f9b17805058b12dd79c223c4407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zz5y6OT1fmOTEJOx0XjEqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/皮尔逊相关系数</a></p></figure><p id="01ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将生成一些代表少数人的假数据。我们将基于三特征向量来比较它们有多相似。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="3648" class="nn md it od b gy ok ol l om on">emily = [1,2,5]<br/>kartik = [1,3,5]<br/>todd = [5,3,5]</span></pre><p id="f2d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的实现。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="5842" class="nn md it od b gy ok ol l om on">import numpy as np</span><span id="df58" class="nn md it od b gy oo ol l om on">def pearsons_correlation_coef(x, y):<br/>    x = np.array(x)<br/>    y = np.array(y)<br/>    <br/>    x_mean = x.mean()<br/>    y_mean = y.mean()</span><span id="e056" class="nn md it od b gy oo ol l om on">    x_less_mean = x - x_mean<br/>    y_less_mean = y - y_mean<br/>    <br/>    numerator = np.sum(xm * ym)<br/>    denominator = np.sqrt(<br/>        np.sum(xm**2) * np.sum(ym**2)<br/>    )<br/>    <br/>    return r_num / r_den</span><span id="5fe4" class="nn md it od b gy oo ol l om on">pearsons_correlation_coef(emily,kartik)<br/>#=&gt; 0.9607689228305226</span></pre><p id="1eaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">酷毙了。艾米莉和卡蒂克看起来很相似。我们一会儿将把这三个和 Scipy 进行比较。</p><h2 id="4cc9" class="nn md it bd me no np dn mi nq nr dp mm li ns nt mo lm nu nv mq lq nw nx ms ny bi translated">皮尔逊相关系数与 Scipy</h2><p id="48a8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Scipy 实现了一种更高效、更健壮的计算。</p><pre class="kj kk kl km gt og od oh oi aw oj bi"><span id="0c3e" class="nn md it od b gy ok ol l om on">emily = [1,2,5]<br/>kartik = [1,3,5]<br/>todd = [5,3,5]</span><span id="9748" class="nn md it od b gy oo ol l om on">pearsonr2(emily,kartik)</span><span id="9082" class="nn md it od b gy oo ol l om on">print( pearsonr2(emily, kartik) ) <br/>#=&gt; (0.9607689228305226, 0.1789123750220673)<br/>print( pearsonr2(kartik, todd) ) <br/>#=&gt; (0.0, 1.0)<br/>print( pearsonr2(todd, emily) ) <br/>#=&gt; (0.27735009811261446, 0.8210876249779328)</span></pre><p id="5bd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我选择了随机数作为数据点，但我们可以看到艾米丽和卡蒂克比艾米丽和托德更相似。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1ca6" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="592a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">虽然我们已经解决了这个难题的一部分，但是通往完全成熟的推荐器的道路还没有完成。</p><p id="a6d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在电子商务引擎的上下文中，我们接下来将构建每对用户之间的相似性得分矩阵。然后我们可以用它来推荐相似用户购买的产品。</p><p id="312a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，一个好的推荐系统可能还包含基于领域的规则和用户偏好。</p></div></div>    
</body>
</html>