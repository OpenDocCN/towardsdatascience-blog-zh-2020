<html>
<head>
<title>The Value Iteration Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">值迭代算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5?source=collection_archive---------7-----------------------#2020-06-13">https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5?source=collection_archive---------7-----------------------#2020-06-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2575" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 09 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从代理人的经历中评估过渡和奖励</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="b53f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<a class="ae lz" rel="noopener" target="_blank" href="/the-bellman-equation-59258a0d3fa7">的上一篇文章</a>中，我们介绍了基于价值的代理，并回顾了贝尔曼方程，这是许多强化学习算法的核心要素之一。在本帖中，我们将介绍<strong class="lf jd">价值迭代</strong>方法来计算基于价值的代理所需的V值和Q值。</p><blockquote class="ma mb mc"><p id="39d0" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae lz" href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">4.数字电视节目</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第4页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="5497" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">使用循环计算V值</h1><p id="95b8" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在前一篇文章中给出的简单例子中，我们在转换中没有循环，并且清楚如何计算状态的值:我们可以从终端状态开始，计算它们的值，然后继续到中心状态。然而，只有环境中存在循环才阻止了这种建议的方法。</p><p id="2379" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们看看如何用一个具有两种状态(状态1和状态2)的简单环境来解决这些情况，该环境呈现了下面的状态转换图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/d190f9ca4ab1388a23a882a513a9e1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwvl3-JSK3ZNhB70MrzGJA.png"/></div></div></figure><p id="da68" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们只有两种可能的转换:从状态1，我们只能采取一个带着+1的奖励进入状态2的动作，从状态2，我们只能采取一个带着+2的奖励回到状态1的动作。因此，由于两种状态之间的无限循环，我们的代理人的生命在无限的状态序列中移动。两种状态的价值是什么？</p><p id="fe22" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">假设我们有一个贴现因子<em class="md"> γ </em> &lt; 1，假设是0，9，从上一篇文章中记住，状态的最优值等于给我们最大可能预期即时回报的动作的值，加上下一个状态的贴现长期回报:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/18876020e629e308306fc6c77ed5ff40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fN-jJwItxrGyo2062dVnuQ.png"/></div></div></figure><p id="2c37" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们的示例中，由于每个状态中只有一个可用的操作，我们的代理没有其他选择，因此我们可以将前面的公式简化为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/a37a44dbe8a0f500a46c43946aced202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R587kAqsqPvFjA41ovs72A.png"/></div></div></figure><p id="d97c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，如果我们从状态1开始，状态的顺序将是[1，2，1，2，1，2，…]，由于从状态1到状态2的每次转换都给我们+1的奖励，并且每次返回转换都给我们+2的奖励，奖励的顺序将是[+1，+2，+1，+2，+1，+2，…]。因此，状态1的先前公式变为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/ad2ed22c17080b1eddb1c15bb70333e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meG1Sd0QQJQCQ-gU6OEFBg.png"/></div></div></figure><p id="35c7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">严格来说，不可能计算出我们状态的精确值，但是用一个贴现因子<em class="md"> γ </em> = 0，9，一个新动作的贡献随着时间的推移而减少。例如，对于扫描<em class="md"> i </em> =37，公式的结果是14.7307838，对于扫描<em class="md"> i </em> =50，结果是14.7365250，对于扫描<em class="md"> i </em> =100，结果是14.7368420。这意味着我们可以在某个点停止计算(例如在<em class="md"> i </em> =50)，但仍然可以获得V值的良好估计，在这种情况下<em class="md"> V </em> (1) = 14.736。</p><h1 id="ad73" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">值迭代算法</h1><p id="1aba" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">前面的例子可以用来获得一个更一般的过程的要点，这个过程叫做<strong class="lf jd">值迭代算法(VI) </strong>。这允许我们用已知的转移概率和回报来数值计算马尔可夫决策过程<strong class="lf jd">的状态值。</strong></p><p id="58d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">值迭代算法背后的思想是将截断的策略评估步骤(如前面的示例所示)和策略改进合并到同一个算法中。</p><p id="78d5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">基本上，值迭代算法通过迭代改进<em class="md"> V </em> (s)的估计来计算最佳状态值函数。该算法将<em class="md"> V </em> (s)初始化为任意随机值。它重复更新<em class="md"> Q(s，a) </em>和<em class="md"> V </em> (s)值，直到它们收敛。保证数值迭代<a class="ae lz" href="http://www.incompleteideas.net/book/first/ebook/node44.html" rel="noopener ugc nofollow" target="_blank">收敛到最优值</a>。下面的伪代码表达了这个建议的算法:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/7a73474eb82bdde1c88720b4f24f64d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3P9iSSjnY5DJ8bloaB8Uw.png"/></div></div></figure><h1 id="5d35" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">过渡和奖励的评估</h1><p id="4da3" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在实践中，这种值迭代方法有几个局限性。首先，状态空间应该是离散的，足够小，以便对所有状态进行多次迭代。对于我们的冰湖环境来说，这不是一个问题，但在一般的强化学习问题中，情况并非如此。我们将在本系列的后续文章中解决这个问题。</p><p id="e5d4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">另一个重要的实际问题来自于这样一个事实，即为了更新贝尔曼方程，算法需要知道环境的转移概率和每次转移的回报。</p><p id="2c97" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请记住，在我们的冰湖例子中，我们观察状态，决定行动，只有这样我们才能获得下一次观察和转换的奖励，但我们事先不知道这些信息。我们能做些什么来得到它们？</p><p id="f335" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">幸运的是，我们能得到的是代理与环境相互作用的历史。所以，前一个问题的答案是用我们代理人的经验作为对两个未知数的估计。下面我们来看看如何实现。</p><h2 id="4fe2" class="oa na it bd nb ob oc dn nf od oe dp nj lm of og nl lq oh oi nn lu oj ok np iz bi translated">报酬估算</h2><p id="0b9e" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">评估奖励是最简单的部分，因为奖励可以直接使用。我们只需要记住在从<strong class="lf jd"> <em class="md"> s </em> </strong>到<strong class="lf jd"><em class="md">s’</em></strong>使用动作<strong class="lf jd"> <em class="md"> a. </em> </strong>的过渡中我们得到了什么奖励</p><h2 id="6306" class="oa na it bd nb ob oc dn nf od oe dp nj lm of og nl lq oh oi nn lu oj ok np iz bi translated">转移估计</h2><p id="efa5" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">估计转换也很容易，例如通过维护代理的经验中每个元组的计数器(<strong class="lf jd"> <em class="md"> s </em> </strong>，<strong class="lf jd"> <em class="md"> a，</em></strong><strong class="lf jd"><em class="md">s’</em></strong>)并使它们正常化。</p><p id="63ad" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，我们可以创建一个简单的表来保存所经历的转换的计数器。表的关键字可以是复合的“状态”+“动作”，(<strong class="lf jd"> <em class="md"> s </em> </strong>，<strong class="lf jd"> <em class="md"> a </em> </strong>)，每个条目的值都有关于目标状态、<strong class="lf jd"> <em class="md"> s、</em> </strong>的信息，以及我们已经看到的每个目标状态、<strong class="lf jd"><em class="md">【c</em></strong>的次数。</p><p id="ff13" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们看一个例子。想象一下，在代理的体验过程中，在给定的状态<strong class="lf jd"> <em class="md"> s0 </em> </strong>中，它已经执行了一个动作<strong class="lf jd"><em class="md"/></strong>若干次，并最终在状态<strong class="lf jd"> s1 </strong>中执行了<strong class="lf jd"> <em class="md"> c1 </em> </strong>次，在状态s <strong class="lf jd"> <em class="md"> 2 </em> </strong>中执行了<strong class="lf jd"> <em class="md"> c2 </em> </strong>次。我们转换到这些状态的次数存储在我们的转换表中。即，表内容{s1: c1，s2: c2}中的条目(s，a)。也许在视觉上，您可以更容易地看到本例表格中包含的信息:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/3117cc1977aba7245e0118968e3dccc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpusPnc1nG9jKc4Khgi4Qg.png"/></div></div></figure><p id="1a76" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，很容易用这个表来估计我们转换的概率。动作将我们从状态0带到状态1的概率是c1 / (c1 + c2)，并且动作将我们从状态0带到状态2的概率是c2 / (c1 + c2)。</p><p id="3508" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，想象一下，从状态0开始，我们执行动作1十次，4次之后，它将把我们带到状态1，6次之后，它将把我们带到状态2。对于这个特定的例子，这个表中带有关键字(0，1)的条目内容为{1: 4，2: 6}。这表示从状态0转换到状态1的概率是4/10，即0.4，从状态0转换到状态2的概率是6/10，即0.6。</p><p id="50e7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">利用从代理的经验中估计的信息，我们已经拥有了能够应用价值迭代算法的所有必要信息。</p><h1 id="499e" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">下一步是什么？</h1><p id="1cd5" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">按照本系列的实践方法，在接下来的两篇文章中，您将通过解决冰湖环境看到价值迭代方法的实践。</p><p id="70b8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下一篇见<a class="ae lz" rel="noopener" target="_blank" href="/value-iteration-for-v-function-d7bcccc1ec24">！。</a></p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="b656" class="mz na it bd nb nc ot ne nf ng ou ni nj ki ov kj nl kl ow km nn ko ox kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae lz" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC巴塞罗那理工</strong> </a> <strong class="lf jd">和</strong> <a class="ae lz" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae lz" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="oy l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我在五月份开始写这个系列，那是在巴塞罗那的封锁期。老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae lz" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="4874" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳被封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为DRL地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>