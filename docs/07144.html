<html>
<head>
<title>Principal Component Analysis with NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于NumPy的主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-with-numpy-58917c1d0391?source=collection_archive---------7-----------------------#2020-06-01">https://towardsdatascience.com/pca-with-numpy-58917c1d0391?source=collection_archive---------7-----------------------#2020-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="60b4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">只使用NumPy库中的函数实现PCA的关键步骤</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5742e2162d42485ce78c895daf09c286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6_1mI8fSpAiCEJ69fz_iQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://pixabay.com/service/terms/#license" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>授权</p></figure><h1 id="464c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">主成分分析</h1><p id="b4d8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是一种常用于线性降维的技术。PCA背后的思想是找到尽可能保留更多信息的数据的低维表示。</p><p id="28c7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们开始遵循接下来的步骤。</p><h1 id="ac4f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">加载库</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5284" class="mu kx iq mq b gy mv mw l mx my">%matplotlib inline<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import seaborn as sns</span></pre><h1 id="bd86" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">资料组</h1><p id="8cf5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是模式识别文献中的经典数据库。数据集包含3类，每类50个实例，其中每类涉及一种鸢尾植物。从<a class="ae kv" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>中检索。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="cc66" class="mu kx iq mq b gy mv mw l mx my">iris = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",<br/>                  header=None)<br/>iris.columns = ["sepal_length","sepal_width",<br/>                'petal_length','petal_width','species']<br/>iris.dropna(how='all', inplace=True)<br/>iris.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/b4a55c64e6c01e393552ef5a734a06dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/0*HyVPJwwM-fg8aDAe"/></div></figure><h1 id="d338" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">可视化数据</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5db6" class="mu kx iq mq b gy mv mw l mx my"># Plotting data using seaborn</span><span id="d038" class="mu kx iq mq b gy na mw l mx my">plt.style.use("ggplot")<br/>plt.rcParams["figure.figsize"] = (12,8)<br/>sns.scatterplot(x = iris.sepal_length, y=iris.sepal_width,<br/>               hue = iris.species, style=iris.species)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b22329f25ff0195341c79d8514d41d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*JMuObX71JfH-mwvb"/></div></figure><h1 id="0852" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">标准化数据</h1><p id="d806" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在应用PCA之前，变量将被标准化为平均值为0，标准差为1。这一点很重要，因为所有变量都经过原点(所有轴的值都是0)并共享相同的方差。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="59f5" class="mu kx iq mq b gy mv mw l mx my">def standardize_data(arr):<br/>         <br/>    '''<br/>    This function standardize an array, its substracts mean value, <br/>    and then divide the standard deviation.<br/>    <br/>    param 1: array <br/>    return: standardized array<br/>    '''    <br/>    rows, columns = arr.shape<br/>    <br/>    standardizedArray = np.zeros(shape=(rows, columns))<br/>    tempArray = np.zeros(rows)<br/>    <br/>    for column in range(columns):<br/>        <br/>        mean = np.mean(X[:,column])<br/>        std = np.std(X[:,column])<br/>        tempArray = np.empty(0)<br/>        <br/>        for element in X[:,column]:<br/>            <br/>            tempArray = np.append(tempArray, ((element - mean) / std))<br/> <br/>        standardizedArray[:,column] = tempArray<br/>    <br/>    return standardizedArray</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/bcf418571550585fe84102eabeee4a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*EXhXUzd9Cg9eAjzw50tjhQ.png"/></div></figure><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="db9a" class="mu kx iq mq b gy mv mw l mx my"># Standardizing data</span><span id="faad" class="mu kx iq mq b gy na mw l mx my">X = iris.iloc[:, 0:4].values<br/>y = iris.species.values</span><span id="9d7f" class="mu kx iq mq b gy na mw l mx my">X = standardize_data(X)</span></pre><h1 id="692e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">计算特征向量和特征值</h1><ol class=""><li id="1625" class="nd ne iq lq b lr ls lu lv lx nf mb ng mf nh mj ni nj nk nl bi translated">计算协方差矩阵</li></ol><p id="69ee" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我将通过将特征矩阵乘以其<em class="nm">转置</em>来找到数据集的协方差矩阵。它是衡量每个维度相对于彼此偏离平均值的程度。像相关矩阵一样，协方差矩阵包含关于变量对之间共享的方差的信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/04188f334257a9e79bbdc6f054fd8695.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*Nw5vyMDOnUm4JJCj"/></div></figure><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="234e" class="mu kx iq mq b gy mv mw l mx my"># Calculating the covariance matrix</span><span id="1250" class="mu kx iq mq b gy na mw l mx my">covariance_matrix = np.cov(X.T)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b4fbf295e5e4903b17caa8cf9cfac342.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/0*viExWKs__bSp8ZlC"/></div></figure><p id="aa3c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">2.协方差矩阵的特征分解</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="fd61" class="mu kx iq mq b gy mv mw l mx my"># Using np.linalg.eig function</span><span id="b702" class="mu kx iq mq b gy na mw l mx my">eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)<br/>print("Eigenvector: \n",eigen_vectors,"\n")<br/>print("Eigenvalues: \n", eigen_values, "\n")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b0946afffb6aca6229b0569d37428686.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/0*-WAGLDw6VLYn3ZWm"/></div></figure><p id="1c25" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">特征向量是主成分。第一个主成分是值为0.52、-0.26、0.58和0.56的第一列。第二个主成分是第二列，依此类推。每个特征向量将对应于一个特征值，每个特征向量可以根据其特征值进行缩放，特征值的大小表示数据的可变性有多少是由其特征向量解释的。</p><h1 id="04bd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用解释的方差选择主成分</h1><p id="c4f0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我想看看这些成分中的每一个能解释多少数据差异。习惯上使用95%的解释方差</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="7205" class="mu kx iq mq b gy mv mw l mx my"># Calculating the explained variance on each of components</span><span id="2656" class="mu kx iq mq b gy na mw l mx my">variance_explained = []<br/>for i in eigen_values:<br/>     variance_explained.append((i/<em class="nm">sum</em>(eigen_values))*100)<br/>        <br/>print(variance_explained)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8c2845f7220f39f66b93868412b9362a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/0*uTULhJcvboH-YmKT"/></div></figure><p id="55d6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第一主成分解释了我们数据中72.77%的方差，第二主成分解释了23.03%的数据。</p><h1 id="cd41" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">确定有多少组件</h1><p id="edcb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">选择要保留的组件数量的一些指导原则:</p><ul class=""><li id="a6c8" class="nd ne iq lq b lr mk lu ml lx nr mb ns mf nt mj nu nj nk nl bi translated">保留特征值大于1的分量，因为它们会增值(因为它们包含的信息比单个变量更多)。这个规则倾向于保留比理想情况更多的组件</li><li id="4502" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">按照从高到低的顺序将特征值可视化，用一条线将它们连接起来。目视检查时，保持所有特征值落在直线斜率变化最剧烈的点(也称为“弯头”)上方的部件</li><li id="5727" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">包括方差截止值，我们只保留解释数据中至少95%方差的成分</li><li id="63c9" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">Keep归结于做PCA的原因。</li></ul><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c555" class="mu kx iq mq b gy mv mw l mx my"># Identifying components that explain at least 95%</span><span id="fb3b" class="mu kx iq mq b gy na mw l mx my">cumulative_variance_explained = np.cumsum(variance_explained)<br/>print(cumulative_variance_explained)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/2855cd1305820bdb94e1ab279f66d914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Hp1QZWaXLrm0TuN4"/></div></div></figure><p id="39bd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我用第一个特征，它会解释72.77%的数据；如果我使用两个特征，我能够捕获95.8%的数据。如果我使用所有特征，我将描述整个数据集。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="d8fc" class="mu kx iq mq b gy mv mw l mx my"># Visualizing the eigenvalues and finding the "elbow" in the graphic</span><span id="3c26" class="mu kx iq mq b gy na mw l mx my">sns.lineplot(x = [1,2,3,4], y=cumulative_variance_explained)<br/>plt.xlabel("Number of components")<br/>plt.ylabel("Cumulative explained variance")<br/>plt.title("Explained variance vs Number of components")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/02261d5ec873c75d21092957fadca8cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aKqeqYrfgeIfJWwE"/></div></div></figure><h1 id="be9c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">将数据投影到低维线性子空间</h1><p id="e55f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在最后一步中，我将计算原始数据集的PCA变换，得到原始标准化X和从特征分解中得到的特征向量的点积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/87c73e2f38f0dd06a4a598a82d070486.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*zCPRG5NXwocEF1vlCrJMHA.png"/></div></figure><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5b29" class="mu kx iq mq b gy mv mw l mx my"># Using two first components (because those explain more than 95%)</span><span id="9919" class="mu kx iq mq b gy na mw l mx my">projection_matrix = (eigen_vectors.T[:][:2]).T<br/>print(projection_matrix)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c0b2cd124c46ae1fef80ec3b89e0e60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/0*XPUPFUwYt4AU-deX"/></div></figure><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="6679" class="mu kx iq mq b gy mv mw l mx my"># Getting the product of original standardized X and the eigenvectors </span><span id="4e94" class="mu kx iq mq b gy na mw l mx my">X_pca = X.dot(projection_matrix)<br/>print(X_pca)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/4d45862832cd68e2a11cec1e7ef3aec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fY9ZnKM4B2kKAp5s"/></div></div></figure><p id="d5f7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我可以像使用变量一样，在任何分析中使用组件。</p><h1 id="5112" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><ul class=""><li id="00cb" class="nd ne iq lq b lr ls lu lv lx nf mb ng mf nh mj nu nj nk nl bi translated">PCA变换是使用这些NumPy函数实现的:np.cov、np.linalg.eig、np.linalg.svd(它是获得特征值和特征向量的替代方案)、np.cumsum、np.mean、np.std、np.zeros、np.empty和np.dot</li><li id="a8aa" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">主成分分析的好处是分量比变量少，从而简化了数据空间，减轻了维数灾难</li><li id="c649" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">当数据是线性的时，PCA也是最好的使用方法，因为它将数据投影到由特征向量构成的线性子空间上</li><li id="6930" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">使用主成分分析，它将把我们的数据投射到使沿轴的方差最大化的方向</li><li id="cce6" class="nd ne iq lq b lr nv lu nw lx nx mb ny mf nz mj nu nj nk nl bi translated">当然，Scikit-learn也有应用PCA的库</li></ul></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="6d8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">完整代码在我的<a class="ae kv" href="https://github.com/mwpnava/Data-Science-Projects/tree/master/PCA_numpy" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> GitHub资源库</strong> </a> <strong class="lq ir">。</strong></p><p id="eb8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="nm">原载于2020年5月24日https://wendynavarrete.com</em><em class="nm"/><a class="ae kv" href="https://wendynavarrete.com/principal-component-analysis-with-numpy/" rel="noopener ugc nofollow" target="_blank"><em class="nm">。</em></a></p></div></div>    
</body>
</html>