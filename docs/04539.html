<html>
<head>
<title>Predictive Early Stopping — A Meta Learning Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测性早期停止——一种元学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predictive-early-stopping-a-meta-learning-approach-90561f0e9454?source=collection_archive---------44-----------------------#2020-04-22">https://towardsdatascience.com/predictive-early-stopping-a-meta-learning-approach-90561f0e9454?source=collection_archive---------44-----------------------#2020-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/956a05f1b56558b58f0f21b1d7369f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQFvzNy0v7QFe7SuYAIh2g.png"/></div></div></figure><p id="ef7b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">作者:Dhruv Nair，数据科学家，Comet.ml </em></p><h2 id="4a66" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">介绍</h2><p id="be4d" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">预测性提前停止是一种用于加速模型训练和超参数优化的最新方法。我们的基准研究表明，预测性早期停止可以加快模型训练<em class="kz">多达30%的底层基础设施独立</em>。</p><p id="ea52" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们基于从学习曲线外推、超带和中位数停止等项目中收集的见解，以创建一个预测模型，可以估计亏损曲线的收敛值。</p><p id="0ac9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Comet能够利用其平台的<em class="kz">公共</em>部分的<em class="kz">超过</em>200万个模型的模型数据，如超参数和损耗曲线，来创建一个模型，该模型的预测可以跨超参数和模型架构进行推广。</p><p id="1a16" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在某些情况下，我们能够在收敛实际发生之前的数百个时期提供收敛的估计。除了预测收敛值之外，我们的预测提前停止产品还提供了当前模型优于当前训练扫描中看到的最佳模型结果的概率估计。</p><p id="fe70" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">在某些情况下，我们能够在收敛实际发生之前的数百个时期提供收敛的估计。</em></p><p id="ac77" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些预测允许我们终止表现不佳的模型的训练，以便搜索过程只评估最有希望的候选人。</p><h2 id="7888" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">基准测试:</h2><p id="1c54" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们在三种不同的设置中测试了我们的预测性早期停止方法:</p><ol class=""><li id="69c9" class="ly lz it kd b ke kf ki kj km ma kq mb ku mc ky md me mf mg bi translated">一种超参数搜索，用于优化充当神经网络代理的函数的参数</li><li id="6b37" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated">使用SMAC优化器在CIFAR10上优化6层CNN的超参数搜索，具有和不具有预测性早期停止。</li><li id="7b28" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated">使用具有超波段的随机搜索与具有预测性提前停止的随机搜索来优化相同的6层CNN的超参数搜索。</li></ol><h2 id="0ad3" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">代理函数的结果:</h2><p id="200f" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">在我们的第一个测试中，我们设置了一个指数衰减函数作为神经网络的代理。我们将这个代理模型运行了20步，并使用Comet的贝叶斯优化器和预测性提前停止来确定这个函数的参数的最佳值。在超参数搜索过程中，我们观察到不允许次优模型训练完整的20个步骤。<strong class="kd iu">图1 </strong>，说明了我们的停止机制。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/ad4fdbf3b489e32c317ed9f24799f447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p8ipe27ujbKoVwMWJ0EGrQ.jpeg"/></div></div></figure><h2 id="7438" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">SMAC的CNN模型的结果:</h2><p id="d0a4" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们对CNN模型的基准测试是以如下方式建立的:</p><p id="65e2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们使用SMAC优化器来估计6层CNN模型中的以下超参数。模型超参数和结构基于AlexNet。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c839" class="la lb it ms b gy mw mx l my mz">{<br/>    "learning_rate":{<br/>        "type":"loguniform",<br/>        "value":[0.0000001, 0.01]<br/>    },<br/>    "learning_rate_decay":{<br/>        "type": "uniform",<br/>        "value":[0.000001, 0.001]<br/>    },<br/>    "weight_decay": {<br/>        "type": "loguniform",<br/>        "value": [0.0000005, 0.005]<br/>    }<br/>}</span></pre><p id="a0b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们对优化器进行了8次测试，包括有无预测性提前停止。每个优化器试验都有6个小时的预算来评估尽可能多的配置。允许每个超参数配置最多训练100个时期，并且在每个时期结束时评估验证集。我们的预测性提前停止模型使用验证损失来确定是否终止超参数配置。</p><p id="408a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在所有试验结束时，我们确定了所有试验中试验损失的平均值，作为达到该损失值所需的超参数扫描中总次数的函数。</p><p id="a5a7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以在<strong class="kd iu">图2 </strong>中看到，使用预测性早期停止允许SMAC获得可比的损失值，几乎快了300个时代。这减少了25%的超参数优化时间。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/d202ccc8648763972a9cbedc29adedb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByDaVQ4lCZXIoFF8oMYFyA.png"/></div></div></figure><p id="9efb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们还根据最终验证损失，将超参数配置划分为分位数。然后，我们计算了优化器在所有试验的每个分位数上花费的平均时期数。</p><p id="23a1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<strong class="kd iu">图3 </strong>中，我们看到SMAC和预测性提前停止SMAC花费大致相同的时间来评估前25%和前50%的配置。然而，预测性早期停止在结果的后50%中花费少30个时期训练模型，在后25%中花费少20个时期。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/4abd9875982cf102fe9817927335d01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjGNOcpGgGwp6jutjS3N_A.png"/></div></div></figure><p id="9f82" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<strong class="kd iu">图4 </strong>中，我们看到了预测性提前停止的超参数扫描的样本损失曲线。次优配置会在允许的训练步骤总数之前停止。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/73d481bcaa5aff3bba5b4ed180e0a5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEo1RtcwZ9yrFmJ5-jZ-Lg.jpeg"/></div></div></figure><h1 id="e61a" class="nc lb it bd lc nd ne nf lf ng nh ni li nj nk nl ll nm nn no lo np nq nr lr ns bi translated">具有超波段的CNN模型的结果:</h1><p id="6c3e" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们以类似于SMAC的方式为Hyperband设置了测试。我们特别使用了在<a class="ae nt" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a>中实现的<a class="ae nt" href="https://optuna.readthedocs.io/en/latest/_modules/optuna/pruners/successive_halving.html#SuccessiveHalvingPruner" rel="noopener ugc nofollow" target="_blank">异步连续二等分普鲁纳</a>。我们可以认为这是一个带有单个括号的超带。我们随机选择了120个超参数配置。每个配置被分配了最少10个时期的资源，并且被允许训练最多100个时期。这导致最大培训预算为12000个纪元。</p><p id="c7f9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在每个评估点，基于最差的验证损失，配置的数量减少N倍。在我们的实验中，我们评估了N值为2、4和8的超带。</p><p id="1b8b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于预测性提前停止，我们使用不同的间隔参数值测试了每个配置。每10、15和20个时期评估一次配置。在每一次评估中，我们都会估计当前配置优于目前最佳配置的可能性。如果这个概率小于一个阈值，在我们的例子中是90%，我们终止当前的配置。阈值和间隔都是用于预测性早期停止的可配置超参数。</p><p id="bbc7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们确定超参数扫描中实现的最佳验证损失，以及扫描后预算中剩余的历元数。</p><p id="8aba" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<strong class="kd iu">图5a </strong>中，我们看到所有方法都找到了验证损失的最佳值，然而，预测性提前停止仅使用总预算的15%就能评估配置，相比之下，Hyperband使用了25%。这在速度上提高了10%。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/b3490b1ca6f38a52da6162d59fb45c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tIseFmsu7aH49C2M_yyG4A.jpeg"/></div></div></figure><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/734567e477df291523800e03daca5ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_zHrp06YmcnpS0IeKZR9uA.jpeg"/></div></div></figure><h2 id="762d" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">结论</h2><p id="30ab" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">预测性提前停止具有非常明显的时间、能量和成本节约优势。浪费计算周期对环境或研究人员的预算都没有好处。</p><p id="4b87" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae nt" href="https://allenai.org/" rel="noopener ugc nofollow" target="_blank">艾伦人工智能研究所</a>最近发表了一份<a class="ae nt" href="https://arxiv.org/pdf/1907.10597.pdf" rel="noopener ugc nofollow" target="_blank">报告</a>，报告内容是关于训练机器学习模型的计算成本不断上升，以及这些日益增长的能源需求如何对环境产生不利影响。该论文指出，当前最先进的人工智能研究进展主要集中在准确性或误差等指标上，代价是对环境不友好。他们称这种模式为红色人工智能。为了对抗红色人工智能研究的盛行，他们提出了向强调计算效率的人工智能研究的转变:绿色人工智能。</p><p id="cb2f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该论文建议根据生成结果所需的浮点运算(FPO)总数来跟踪人工智能算法的效率。FPO的总数与调整期间评估的超参数配置的数量以及每个配置上花费的训练迭代次数直接相关。</p><p id="81ea" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们希望我们在预测性早期停止方面的努力有助于提高超参数搜索过程的计算效率。我们认为这个工具是降低人工智能研究相关货币壁垒的一种方式，也是朝着采用绿色人工智能实践迈出的一步。</p><p id="71ad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们的下一篇文章中，我们将描述在不属于较大参数搜索的单独运行中应用预测性早期停止。</p><p id="2bb4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">*购买Comet Teams或Comet Enterprise后，预测性提前停止功能可作为附加功能提供。正在通过一项或多项未决专利申请寻求对预测性提前停车的专利保护。</em></p><p id="f9d4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">了解更多信息并在此注册<a class="ae nt" href="https://www.comet.ml/site/what-is-meta-ml/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>