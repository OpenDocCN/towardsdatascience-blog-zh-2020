<html>
<head>
<title>Understanding AdaBoost for Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解用于决策树的 AdaBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-adaboost-for-decision-tree-ff8f07d2851?source=collection_archive---------12-----------------------#2020-01-11">https://towardsdatascience.com/understanding-adaboost-for-decision-tree-ff8f07d2851?source=collection_archive---------12-----------------------#2020-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5e0fb36b1712cd66a1f7124a218ca765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KYszvMnr3nCtjaGy.png"/></div></div></figure><div class=""/><div class=""><h2 id="6487" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">用 R 实现</h2></div><p id="5450" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><a class="ae lp" href="https://medium.com/analytics-vidhya/visualizing-decision-tree-with-r-774f58ac23c" rel="noopener">决策树</a>是流行的机器学习算法，用于回归和分类任务。它们之所以受欢迎，主要是因为它们的可解释性和可表达性，因为它们模仿了人脑做出决策的方式。</p><p id="2a3a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我以前的<a class="ae lp" href="https://medium.com/analytics-vidhya/ensemble-methods-for-decision-trees-f4a658af754d" rel="noopener">文章</a>中，我已经介绍了一些决策树的集成方法，其目的是将一堆弱分类器转换成一个更强的分类器。在这里，我将详细阐述 Boosting 方法，这是算法在每次迭代中从错误中“学习”的一种方法。更具体地说，我将一步一步地解释 Adaboost 背后的思想以及如何用 r 实现它。</p><h2 id="af54" class="lq lr je bd ls lt lu dn lv lw lx dp ly lc lz ma mb lg mc md me lk mf mg mh mi bi translated">Adaboost 背后的理念</h2><p id="9948" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc ml le lf lg mm li lj lk mn lm ln lo im bi translated">Adaboost 和 bagging 方法(包括随机森林)之间的主要区别在于，在过程结束时，当迭代期间构建的所有分类器都被要求为新观察的目标投票时，将会有比其他树投票更重的树。这些是在所有迭代中表现最好的树(因此，它们表现出很少的错误分类)。比方说，在一天结束的时候，有些树会比其他树说得更多。这种“发言权的重要性”在整个迭代过程中被测量(和更新)。</p><p id="26df" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">除此之外，Adaboost 的其他重要特性，如预期的那样，是它从过去的错误中学习的能力，因为在每次迭代中，下一个分类器是基于过去的错误分类错误建立的。</p><p id="0f3c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们看看算法是如何具体工作的。</p><h2 id="d204" class="lq lr je bd ls lt lu dn lv lw lx dp ly lc lz ma mb lg mc md me lk mf mg mh mi bi translated">该算法</h2><p id="3876" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc ml le lf lg mm li lj lk mn lm ln lo im bi translated">假设我们有一个数据集，有 N 个观察值，P 个协变量和一个二元目标变量 y=1，-1。这将是我们的火车布景。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mo"><img src="../Images/477e7bd84cd87b4059e56ad3927874cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwgJr9hH4li5NhETPhYTcQ.png"/></div></div></figure><p id="644d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以用这个框架设计任何你感兴趣的任务(即，一封电子邮件是否是垃圾邮件，明天的天气是否晴朗……)。</p><ul class=""><li id="44cb" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated">我们要做的第一件事是给每一行分配一个权重，这表明该观察值被很好地分类有多重要。我们将这些权重初始化为 1/N，这样在第一次迭代时，当训练第一个分类器时，它们不会产生差异。</li></ul><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/f9668bf61061a34452a82fa45be8c2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28ZpYFsPxbMt4vWio4rYqw.png"/></div></div></figure><ul class=""><li id="a17e" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated">现在，我们在数据集上训练我们的第一个分类器 G(x ),然后检查哪些是错误分类的观察值，因为我们想要计算总误差，它不同于标准的错误分类误差，因为它是加权求和:</li></ul><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1a9fe55bdbae95e1fa7b06be5f440d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*ZBVTt8ai0xWas_eCy5LC9Q.png"/></div></figure><p id="1c79" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如您所见，我们有一个自标准化的加权误差总和(当目标与拟合值不同时)。请注意，由于其构建方式，总误差将始终介于 0 和 1 之间。有了这个量，我们就可以计算出我们所说的‘话语权的重要性’:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/88435e776d3b430760d9ba74315fba23.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*ldNcSJssIVBc9ZYoh7E6OA.png"/></div></figure><p id="60a5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如你所见，误差越大，最后那棵树的投票就越不重要。我们也可以形象化地描述它:</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="dd0e" class="lq lr je ng b gy nk nl l nm nn">error = seq(0,1,0.001)<br/>alpha = log((1-error)/error)<br/>plot(error,alpha, type='line')</span></pre><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/877ce9bda49f6ab0b89cc768eacf104f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYgIgZ1VE2dR99vBzTg27g.png"/></div></div></figure><ul class=""><li id="3cca" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated">有了这个度量α，我们现在可以更新权重，使得对应于错误分类的观察值的权重在下一次迭代中将更高。通过这样做，当我们将在新的加权数据集上拟合下一棵树时，错误分类这些观察值的成本将会更高，并且树将会非常小心地不再错误分类它们(因为这将意味着更高的成本)。因此，对于所有的观察值，i=1，2，..，N，我们更新第 I 个权重如下。</li></ul><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/96356ccdd17b1e7348f8eb3c88d4986d.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*p6shb2lMAy9S6NCHZaWFgw.png"/></div></figure><ul class=""><li id="8154" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated">如果我们重复这个过程 M 次迭代，在一天结束时，我们将有 M 个分类器，每个分类器有一个加权投票，并且，如果我们想要预测新观察 x*的目标，公式是:</li></ul><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d960719b206143cf97e0702582f614e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*f8WCzQyZDDQ8Wi0eq6rEXA.png"/></div></figure><p id="4be8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，将会有一些树的输出在最终决策中具有很大的决定性(-1 或 1)，而其他的树则可以忽略不计。</p><p id="1b01" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们来看看这个过程其余部分的一个非常简短的实现:</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="10a6" class="lq lr je ng b gy nk nl l nm nn">data = data.frame( X=c(rnorm(100,0,1),rnorm(100,1,1)), Y=c(rep(0,100),rep(1,100) ) )</span><span id="676e" class="lq lr je ng b gy nq nl l nm nn">data$Y =factor(data$Y)<br/>model =adaboost(Y~X, data, 10)<br/>pred =predict( model ,newdata=data)</span></pre><p id="7626" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们来看看模型的总结和它的预测误差(等于训练集中的误分类误差):</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="ecfb" class="lq lr je ng b gy nk nl l nm nn">print(pred$error)<br/>print(model)</span></pre><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/42a377e5dda9845a559d3e51682dcbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rw1awnBLdeaBNJEmgloIjQ.png"/></div></div></figure><p id="3048" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们还可以检索最终的“重要性”权重:</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="aa9c" class="lq lr je ng b gy nk nl l nm nn">model$weights</span></pre><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/592df23f27157270cabbaa2eaf88912f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RA7a_UQZq0SXHbm2ZDu8bA.png"/></div></div></figure><p id="12f2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如你所见，有一些树(像第一棵树)的最终投票非常重要，而其他树(像最后一棵树)则不那么重要。</p><p id="2e0d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，我们可以将其误差与单棵树的误差进行比较:</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="4648" class="lq lr je ng b gy nk nl l nm nn">single_model=tree(Y∼X, data)<br/>summary(single_model)</span></pre><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/e025d1b2db183aed433341762f5bd64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*47AqI6-jhriLR3e0IeY0tA.png"/></div></figure><p id="795f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如你所见，我们的提升分类器比单一的树分类器更强。</p><h2 id="42ca" class="lq lr je bd ls lt lu dn lv lw lx dp ly lc lz ma mb lg mc md me lk mf mg mh mi bi translated">结论</h2><p id="f874" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc ml le lf lg mm li lj lk mn lm ln lo im bi translated">集成方法是强大的技术，可以大大提高决策树的预测准确性。然而，这些方法的警告是，它们使得呈现和解释最终结果变得不太容易。事实上，正如我们在开始时所说的，决策树最显著的特征是它们的可解释性和易于理解性，在一个看起来像“黑盒”的算法世界中，这是一个重要的价值。尽管如此，还是有一些视觉上的选择，如果集合多个树能提高那么多的准确性，那肯定是值得的。</p></div></div>    
</body>
</html>