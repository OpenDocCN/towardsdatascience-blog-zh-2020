<html>
<head>
<title>When your Docker Meets Pyspark to Do Sentiment Analysis of 10+ GB Customer Review Data-PART 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当您的 Docker 与 Pyspark 会面，对 10gb 以上的客户评论数据进行情感分析时——第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-your-docker-meets-pyspark-to-do-sentiment-analysis-of-10-gb-customer-review-data-part-1-277633d39bba?source=collection_archive---------55-----------------------#2020-05-11">https://towardsdatascience.com/when-your-docker-meets-pyspark-to-do-sentiment-analysis-of-10-gb-customer-review-data-part-1-277633d39bba?source=collection_archive---------55-----------------------#2020-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/82f8807fa8aafc1ee262b556ec74f844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GibbH2GpheToXi5x5BeLA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:<a class="kc kd ep" href="https://medium.com/u/6c997f2b4cff?source=post_page-----277633d39bba--------------------------------" rel="noopener" target="_blank">蒂亚戈梅洛</a>，途经:<a class="kc kd ep" href="https://medium.com/u/504c7870fdb6?source=post_page-----277633d39bba--------------------------------" rel="noopener" target="_blank">中</a></p></figure><p id="be72" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这篇博客中，我将向您展示如何轻松使用 Pyspark 来处理千兆字节规模的数据集。好了，让我们开始吧:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/54c5e0c31eaee65abefdc966e5127773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*8xHK79BmxxbYpjCR20Vtbg.jpeg"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">开始！(图片来自@Pixabay 的@Pexels)</p></figure><h2 id="a6f4" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">设置 Docker 引擎</h2><p id="22b9" class="pw-post-body-paragraph ke kf iq kg b kh ma kj kk kl mb kn ko kp mc kr ks kt md kv kw kx me kz la lb ij bi translated">现在，docker 将用于轻松下载 jupyter/pyspark docker 映像，然后使用它进行分布式处理。所以，你必须知道的第一件事就是你的 OS 是否有 Docker 引擎。</p><p id="8e09" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">Linux 用户在这方面不会有问题，他们只需按照说明在他们的操作系统中设置 docker，链接如下:</p><p id="6373" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">【Linux 用户 Docker 手册:</p><p id="9fb6" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于 Windows 和 Mac 用户，您可以通过官方链接来设置 docker 引擎:</p><p id="2925" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae mf" href="https://docs.docker.com/docker-for-windows/install/" rel="noopener ugc nofollow" target="_blank"> Docker 手册 Windows 用户:</a></p><p id="87eb" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae mf" href="https://docs.docker.com/docker-for-mac/install/" rel="noopener ugc nofollow" target="_blank"> Docker 手册 Mac 用户:</a></p><blockquote class="mg"><p id="178b" class="mh mi iq bd mj mk ml mm mn mo mp lb dk translated">注:如果你是一名数据科学家/分析师，在阅读这篇文章时，我强烈推荐你使用 Linux 操作系统发行版，因为它真的会对你有所帮助，尤其是在将数据科学成果投入生产的时候。</p></blockquote><p id="cecc" class="pw-post-body-paragraph ke kf iq kg b kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx mv kz la lb ij bi translated">现在，有了 Docker 引擎，我们必须做的下一件事是获得 pyspark 图像(如果您没有的话)。这可以通过 bash 中的以下命令轻松完成:</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="cc19" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是一个有点大的文件(大约 4.5GB)，在提取后，我们需要使用命令行仔细检查我们的图像:</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="1d82" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是我们本地机器中所有图像的列表，我们可以看到<em class="my">jupyter/pyspark-notebook</em>是我们将使用的图像之一:</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="345d" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，如果你的清单上有 jupyter/pyspark 笔记本，那太好了！</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/71f9b5c36375d68675e56197817ccf8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhgV-VezF8sWuNyR2mKWMQ.jpeg"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:疯狂宝贝 via: @imgflip</p></figure><h2 id="2cd6" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">将本地目录连接到 Docker 容器</h2><p id="9e13" class="pw-post-body-paragraph ke kf iq kg b kh ma kj kk kl mb kn ko kp mc kr ks kt md kv kw kx me kz la lb ij bi translated">现在，您有了一个讨论大数据的 spark 映像。现在，由于大多数情况下，我们的大数据不在 docker 所在的目录中，我们需要将大数据集移植到容器，这样容器就可以直接访问数据，在我的例子中，以下代码进行了这种装载(我将在下面分解):</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="69f0" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">好了，我们来破解上面的代码，全面了解一下:</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="3309" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，它将流量从我们机器上的端口 8888 传递到 Docker 映像上的端口 8888，在本例中为(jupyter/pyspark-notebook)</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="e19d" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，用本地工作目录替换“~/Documents/pyspark_docker”。这个目录将被容器访问，这就是选项“-v”在代码中所做的。该目录可能是空的，您稍后将需要放一些文件。所以，如果你完成了上述步骤，现在 Jupyter 笔记本应该会出现在你的浏览器中，你将会有你的数据的确切路径。现在，如果您已经达到了这个阶段，那么恭喜您，现在您已经准备好使用大数据了:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/02ea15054764889f08c6b20bf4584cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwrTiY-mUNq6v4VA7PaWhQ.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:<a class="kc kd ep" href="https://medium.com/u/5c7773c5a002?source=post_page-----277633d39bba--------------------------------" rel="noopener" target="_blank">像素</a></p></figure><p id="b70e" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这项工作中，yelp 数据集将用于 spark 的分布式计算。此链接提供的 Yelp 数据集将被用作典型的商业大数据:</p><p id="7bb5" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">【Yelp 数据集的开源链接</p><p id="40aa" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于这个特定的数据，我发现这个博客对数据的数据建模很有帮助，如下所示:</p><p id="9e40" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae mf" href="https://paulx-cn.github.io/blog/6th_Blog/" rel="noopener ugc nofollow" target="_blank">数据建模</a></p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/6eba74dd4600fb29cfecc831266feb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ihoPpVsD9tna4WVIZ8D1g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:保罗·谢，via:<a class="ae mf" href="https://paulx-cn.github.io/" rel="noopener ugc nofollow" target="_blank"/></p></figure><h2 id="af6b" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">开始与 Spark 会话的数据争论</h2><h2 id="4523" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">设置 Pyspark</h2><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="2674" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">通过 Spark 读取审查数据</h2><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="41e6" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">仅从数据的规模来看，我们有大约 8000 万条评论，确实是一个大数据！：</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/f873a34a8f6980e58fa77968bb5b339a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*jLprs8zZdSZKCF-Rf_INwA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:<a class="ae mf" href="https://changhsinlee.com/pyspark-udf/" rel="noopener ugc nofollow" target="_blank">https://changhsinlee.com/pyspark-udf/</a>，转自:<a class="ae mf" href="https://changhsinlee.com/" rel="noopener ugc nofollow" target="_blank">张信利</a></p></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="460f" class="lh li iq ne b gy ni nj l nk nl">number of rows:8021122<br/>number of columns:9</span></pre><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="3bf4" class="lh li iq ne b gy ni nj l nk nl">root<br/> |-- business_id: string (nullable = true)<br/> |-- cool: long (nullable = true)<br/> |-- date: string (nullable = true)<br/> |-- funny: long (nullable = true)<br/> |-- review_id: string (nullable = true)<br/> |-- stars: double (nullable = true)<br/> |-- text: string (nullable = true)<br/> |-- useful: long (nullable = true)<br/> |-- user_id: string (nullable = true)</span></pre><h2 id="1249" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">情感分析</h2><p id="ea85" class="pw-post-body-paragraph ke kf iq kg b kh ma kj kk kl mb kn ko kp mc kr ks kt md kv kw kx me kz la lb ij bi translated">在下面的代码中，我定义了<em class="my"> text_processing </em>函数，该函数将删除标点符号，使所有评论变成小写，并删除英文停用词:</p><p id="6529" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这一步中，您可能需要运行以下代码来安装 NLTK 包。</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="4d4e" class="lh li iq ne b gy ni nj l nk nl">/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.<br/>  warnings.warn(message, FutureWarning)</span></pre><h2 id="7b40" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">添加 clean_text 列</h2><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="8caa" class="lh li iq ne b gy ni nj l nk nl">+--------------------+--------------------+<br/>|          clean_text|                text|<br/>+--------------------+--------------------+<br/>|worked museum eag...|As someone who ha...|<br/>|actually horrifie...|I am actually hor...|<br/>|love deagans real...|I love Deagan's. ...|<br/>|dismal lukewarm d...|Dismal, lukewarm,...|<br/>|oh happy day fina...|Oh happy day, fin...|<br/>+--------------------+--------------------+<br/>only showing top 5 rows</span></pre><p id="6a62" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">好了，现在我们有了干净的文本，是时候做情感分析来看看每个评论的分数了</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="780e" class="lh li iq ne b gy ni nj l nk nl">+-----+--------------------+<br/>|score|          clean_text|<br/>+-----+--------------------+<br/>| 21.0|worked museum eag...|<br/>|-11.0|actually horrifie...|<br/>| 13.0|love deagans real...|<br/>| -7.0|dismal lukewarm d...|<br/>| 26.0|oh happy day fina...|<br/>+-----+--------------------+<br/>only showing top 5 rows</span></pre><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="0f87" class="lh li iq ne b gy ni nj l nk nl">root<br/> |-- business_id: string (nullable = true)<br/> |-- cool: long (nullable = true)<br/> |-- date: string (nullable = true)<br/> |-- funny: long (nullable = true)<br/> |-- review_id: string (nullable = true)<br/> |-- stars: double (nullable = true)<br/> |-- text: string (nullable = true)<br/> |-- useful: long (nullable = true)<br/> |-- user_id: string (nullable = true)<br/> |-- clean_text: string (nullable = true)<br/> |-- score: float (nullable = true)</span></pre><h2 id="6b8b" class="lh li iq bd lj lk ll dn lm ln lo dp lp kp lq lr ls kt lt lu lv kx lw lx ly lz bi translated">根据审查分数对业务进行分组</h2><p id="bcd4" class="pw-post-body-paragraph ke kf iq kg b kh ma kj kk kl mb kn ko kp mc kr ks kt md kv kw kx me kz la lb ij bi translated">在这里，我对所有业务的平均分进行分组:</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="052b" class="lh li iq ne b gy ni nj l nk nl">root<br/> |-- business_id: string (nullable = true)<br/> |-- avg(score): double (nullable = true)</span></pre><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><pre class="ld le lf lg gt nd ne nf ng aw nh bi"><span id="4bad" class="lh li iq ne b gy ni nj l nk nl">209393</span></pre></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="457d" class="pw-post-body-paragraph ke kf iq kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在 Markdown 中写入介质？试试<a class="ae mf" href="https://markdium.dev/" rel="noopener ugc nofollow" target="_blank">马克迪姆</a>！</p></div></div>    
</body>
</html>