<html>
<head>
<title>Machine Learning Basics: Support Vector Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础:支持向量回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226?source=collection_archive---------6-----------------------#2020-07-11">https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226?source=collection_archive---------6-----------------------#2020-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e73d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学会在机器学习中建立支持向量回归(SVR)模型，并分析结果。</h2></div><p id="3b12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的故事中，我解释了用 Python 构建线性和多项式回归模型的机器学习程序。在本文中，我们将介绍基于非线性数据构建支持向量回归模型的程序。</p><h2 id="758f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">SVR 概述</h2><p id="2bf3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">支持向量机(SVM)是一种非常流行的机器学习算法，用于回归和分类。支持向量回归类似于线性回归，在 SVR 中直线的方程是<code class="fe mc md me mf b">y= wx+b</code>，这条直线被称为<strong class="kk iu"> <em class="mg">超平面</em> </strong>。超平面任意一侧最接近超平面的数据点称为<strong class="kk iu"> <em class="mg">支持向量</em> </strong>，用于绘制边界线。</p><p id="cf39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与试图最小化实际值和预测值之间的误差的其他回归模型不同，SVR 试图在阈值(超平面和边界线之间的距离)内拟合最佳直线。因此，我们可以说 SVR 模型满足条件<code class="fe mc md me mf b">-a &lt; y-wx+b &lt; a</code>。它使用具有此边界的点来预测值。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/020b2ce24fa3013416f48e21d693e572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*u8cw2ZZW7PJ4Lvp0-IAP7A.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated"><a class="ae mt" href="https://gdcoder.com/support-vector-machine-vs-logistic-regression/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="9619" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于非线性回归，核函数将数据转换到更高维度，并执行线性分离。这里我们将使用<strong class="kk iu"> <em class="mg"> rbf </em> </strong>内核。</p><p id="83be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，我们将完成<strong class="kk iu"> <em class="mg">支持向量回归(SVM) </em> </strong>的实现，其中我们将根据学生投入学习的小时数来预测他或她的分数。</p><h2 id="892c" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">问题分析</h2><p id="7e97" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在这个数据中，我们有一个自变量<em class="mg">学习时间</em>和一个因变量<em class="mg">分数</em>。在这个问题中，我们必须使用这些数据训练一个 SVR 模型，以了解学习时间和学生分数之间的相关性，并能够根据学生投入学习的时间预测他们的分数。</p><h2 id="4b33" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 1:导入库</h2><p id="9993" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在第一步中，我们将导入构建 ML 模型所需的库。导入<strong class="kk iu"> <em class="mg"> NumPy </em> </strong>库和<strong class="kk iu"> <em class="mg"> matplotlib </em> </strong>。另外，我们导入了<strong class="kk iu"> <em class="mg">熊猫</em> </strong>库用于数据分析。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="da2f" class="le lf it mf b gy my mz l na nb">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><h2 id="03e7" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 2:导入数据集</h2><p id="ed5a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在这一步，我们将使用 pandas 来存储从我的 github 存储库中获得的数据，并使用函数“<strong class="kk iu"> pd.read_csv </strong>”将其存储为 Pandas DataFrame。</p><p id="a933" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们遍历我们的数据集，将自变量(x)分配给列“<strong class="kk iu"><em class="mg"/></strong>”的学习时间，将因变量(y)分配给最后一列，即要预测的“<strong class="kk iu"><em class="mg"/></strong>”标记。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="b22c" class="le lf it mf b gy my mz l na nb">dataset = pd.read_csv('<a class="ae mt" href="https://raw.githubusercontent.com/mk-gurucharan/Regression/master/SampleData.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/mk-gurucharan/Regression/master/SampleData.csv'</a>)</span><span id="8512" class="le lf it mf b gy nc mz l na nb">X = dataset.iloc[:, 0].values<br/>y = dataset.iloc[:, 1].values<br/>y = np.array(y).reshape(-1,1)</span><span id="8359" class="le lf it mf b gy nc mz l na nb">dataset.head(5)</span><span id="8801" class="le lf it mf b gy nc mz l na nb">&gt;&gt;</span><span id="8e0c" class="le lf it mf b gy nc mz l na nb">Hours of Study   Marks<br/>32.502345        31.707006<br/>53.426804        68.777596<br/>61.530358        62.562382<br/>47.475640        71.546632<br/>59.813208        87.230925</span></pre><p id="883f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用相应的。iloc 函数对数据帧进行切片，以将这些指标分配给 X 和 y。在这种情况下，<em class="mg">小时的学习时间</em>被视为独立变量，并被分配给 X。要预测的因变量是最后一列，即<em class="mg">标记的</em>，它被分配给 y。我们将使用<code class="fe mc md me mf b">reshape(-1,1)</code>将变量<strong class="kk iu"> <em class="mg"> y </em> </strong>整形为列向量。</p><h2 id="3821" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 3:特征缩放</h2><p id="5875" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">大多数可用的数据通常具有不同的范围和大小，这使得建立模型很困难。因此，数据的范围需要被标准化为更小的范围，这使得模型在训练时更加准确。在该数据集中，数据被归一化到接近零的小值之间。例如，<em class="mg"> 87.23092513 </em>的分数归一化为<em class="mg"> 1.00475931 </em>，而<em class="mg"> 53.45439421 </em>的分数归一化为<em class="mg"> -1.22856288 </em>。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="a299" class="le lf it mf b gy my mz l na nb">from sklearn.preprocessing import StandardScaler<br/>sc_X = StandardScaler()<br/>sc_y = StandardScaler()<br/>X = sc_X.fit_transform(X.reshape(-1,1))<br/>y = sc_y.fit_transform(y.reshape(-1,1))</span></pre><p id="8088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在大多数常见的回归和分类模型中，特征缩放主要是在内部执行的。支持向量机不是一个常用的类，因此数据被标准化到一个有限的范围。</p><h2 id="e71e" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 4:在训练集上训练支持向量回归模型</h2><p id="0109" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在构建任何 ML 模型时，我们总是需要将数据分成训练集和测试集。将使用<em class="mg">训练集</em>的值训练 SVR 模型，并在<em class="mg">测试集</em>上测试预测。在 100 行中，80 行用于训练，并且在由条件<code class="fe mc md me mf b">test_size=0.2</code>给出的剩余 20 行上测试模型</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="c081" class="le lf it mf b gy my mz l na nb">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)</span></pre><h2 id="c549" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 5:在训练集上训练支持向量回归模型</h2><p id="c128" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在此，函数<strong class="kk iu"> <em class="mg"> SVM </em> </strong>被导入并被赋给变量<code class="fe mc md me mf b">regressor</code>。使用内核<strong class="kk iu"><em class="mg">【RBF】</em></strong>(径向基函数)。径向基函数核用于将非线性引入到支持向量回归模型中。这样做是因为我们的数据是非线性的。<code class="fe mc md me mf b">regressor.fit</code>用于通过相应地调整数据来拟合变量<em class="mg"> X_train </em>和<em class="mg"> y_train </em>。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="4746" class="le lf it mf b gy my mz l na nb">from sklearn.svm import SVR<br/>regressor = SVR(kernel = 'rbf')<br/>regressor.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))</span></pre><h2 id="1c62" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 6:预测测试集结果</h2><p id="569f" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在这一步中，我们将使用构建的 SVR 模型来预测测试集的分数。<code class="fe mc md me mf b">regressor.predict</code>函数用于预测 X_test 的值。我们将预测值赋给 y_pred。我们现在有两个数据，y_test(真实值)和 y_pred(预测值)。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="86f5" class="le lf it mf b gy my mz l na nb">y_pred = regressor.predict(X_test)<br/>y_pred = sc_y.inverse_transform(y_pred)</span></pre><h2 id="9e26" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 7:将测试集与预测值进行比较</h2><p id="df4d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在这一步中，我们将在 Pandas DataFrame 中将每个 X_test 的<em class="mg"> y_test </em>的值显示为<strong class="kk iu"> <em class="mg">真实值</em> </strong>，将<em class="mg"> y_pred </em>值显示为<strong class="kk iu"> <em class="mg">预测值</em> </strong>。</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="cc54" class="le lf it mf b gy my mz l na nb">df = pd.DataFrame({'Real Values':sc_y.inverse_transform(y_test.reshape(-1)), 'Predicted Values':y_pred})<br/>df</span><span id="12be" class="le lf it mf b gy nc mz l na nb">&gt;&gt;<br/>Real Values   Predicted Values<br/>31.707006     53.824386<br/>76.617341     61.430210<br/>65.101712     63.921849<br/>85.498068     80.773056<br/>81.536991     72.686906<br/>79.102830     60.357810<br/>95.244153     89.523157<br/>52.725494     54.616087<br/>95.455053     82.003370<br/>80.207523     81.575287<br/>79.052406     67.225121<br/>83.432071     73.541885<br/>85.668203     78.033983<br/>71.300880     76.536061<br/>52.682983     63.993284<br/>45.570589     53.912184<br/>63.358790     76.077840<br/>57.812513     62.178748<br/>82.892504     64.172003<br/>83.878565     93.823265</span></pre><p id="d156" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，预测值与测试集的真实值之间存在显著偏差，因此我们可以得出结论，该模型并不完全适合以下数据。</p><h2 id="defd" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">步骤 8:可视化 SVR 结果</h2><p id="95b3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在最后一步中，我们将可视化使用给定数据构建的 SVR 模型，并在图上绘制“<strong class="kk iu"> <em class="mg"> y </em> </strong>”和“<strong class="kk iu"> <em class="mg"> y_pred </em> </strong>”的值，以可视化结果</p><pre class="mi mj mk ml gt mu mf mv mw aw mx bi"><span id="726a" class="le lf it mf b gy my mz l na nb">X_grid = np.arange(min(X), max(X), 0.1)<br/>X_grid = X_grid.reshape((len(X_grid), 1))<br/>plt.scatter(sc_X.inverse_transform(X_test), sc_y.inverse_transform(y_test.reshape(-1)), color = 'red')<br/>plt.scatter(sc_X.inverse_transform(X_test), y_pred, color = 'green')</span><span id="3658" class="le lf it mf b gy nc mz l na nb">plt.title('SVR Regression')<br/>plt.xlabel('Position level')<br/>plt.ylabel('Salary')<br/>plt.show()</span></pre><div class="mi mj mk ml gt ab cb"><figure class="nd mm ne nf ng nh ni paragraph-image"><img src="../Images/73bfea86cac1a0f67f4a27ba9f1e85ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Va6PRGxuihQlIWKIg2FC3A.png"/></figure><figure class="nd mm nj nf ng nh ni paragraph-image"><img src="../Images/448596bef431c0e000b6eeb5fe4599cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*Akxebt4QAibW38rCotVkUA.png"/><p class="mp mq gj gh gi mr ms bd b be z dk nk di nl nm translated">学习时间与分数(SVR)</p></figure></div><p id="a9d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在该图中，实际值用“<strong class="kk iu"> <em class="mg">红色</em> </strong>绘制，预测值用“<strong class="kk iu"> <em class="mg">绿色</em> </strong>绘制。SVR 模型的绘图也以<strong class="kk iu"> <em class="mg">黑色</em> </strong>颜色显示。</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="a30a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我附上了我的 github 资源库的链接，你可以在那里找到 Google Colab 笔记本和数据文件供你参考。</p><div class="np nq gp gr nr ns"><a href="https://github.com/mk-gurucharan/Regression" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">MK-guru charan/回归</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码、管理项目和构建…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">github.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og mn ns"/></div></div></a></div><p id="6952" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望我已经清楚地解释了用非线性数据集构建支持向量回归模型的程序。</p><p id="ef90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以在下面找到该程序对其他回归模型的解释:</p><ul class=""><li id="dbe8" class="oh oi it kk b kl km ko kp kr oj kv ok kz ol ld om on oo op bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/machine-learning-basics-simple-linear-regression-bc83c01baa07">简单线性回归</a></li><li id="464d" class="oh oi it kk b kl oq ko or kr os kv ot kz ou ld om on oo op bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/machine-learning-basics-multiple-linear-regression-9c70f796e5e3">多元线性回归</a></li><li id="6b70" class="oh oi it kk b kl oq ko or kr os kv ot kz ou ld om on oo op bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/machine-learning-basics-polynomial-regression-3f9dd30223d1">多项式回归</a></li><li id="c4f7" class="oh oi it kk b kl oq ko or kr os kv ot kz ou ld om on oo op bi translated">支持向量回归</li><li id="fd92" class="oh oi it kk b kl oq ko or kr os kv ot kz ou ld om on oo op bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/machine-learning-basics-decision-tree-regression-1d73ea003fda">决策树回归</a></li><li id="72c9" class="oh oi it kk b kl oq ko or kr os kv ot kz ou ld om on oo op bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/machine-learning-basics-random-forest-regression-be3e1e3bb91a">随机森林回归</a></li></ul><p id="ddfc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的文章中，我们将会遇到更复杂的回归、分类和聚类模型。到那时，快乐的机器学习！</p></div></div>    
</body>
</html>