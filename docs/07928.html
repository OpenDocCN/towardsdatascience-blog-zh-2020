<html>
<head>
<title>Different Types of Normalization in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流中不同类型的归一化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0?source=collection_archive---------22-----------------------#2020-06-12">https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0?source=collection_archive---------22-----------------------#2020-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="436e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解Tensorflow中的批次、组、实例、层和权重标准化，以及解释和实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/effe66bf79ad9973ee9ccfbea9901abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dNUulKJ9BbruWt6O.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当规范化被引入深度学习时，它是下一件大事，并大大提高了性能。对你的模型来说，做得正确是一个至关重要的因素。曾经有过这样的疑问，不管批处理规范化是什么，它是如何提高性能的。还有什么替代品吗？如果你像我一样有这些问题，但从来没有烦恼，只是为了在你的模型中使用它，这篇文章将澄清它。</p><h1 id="4861" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目录</h1><ul class=""><li id="2c20" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated">批量标准化</li><li id="1a50" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">群体规范化</li><li id="7419" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">实例规范化</li><li id="07ac" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">图层规范化</li><li id="49d3" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">权重标准化</li><li id="ba82" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">Tensorflow中的实现</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="44cb" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">批量标准化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/55b77c3eb27d650ba443d366442abda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KpmqqeilfbmYLO-D"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@upmanis?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kaspars Upmanis </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最广泛使用的技术为表演带来奇迹。它是做什么的？嗯，批量标准化是一种标准化方法，通过小批量标准化网络中的激活。它计算小批量中每个特征的平均值和方差。然后减去平均值，并将该特征除以其最小批量标准偏差。它还有两个额外的可学习参数，激活的平均值和幅度。这些用于避免与零均值和单位标准偏差相关的问题。</p><p id="3c9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些看起来很简单，但是为什么会对社区产生如此大的影响？它是如何做到的？答案还没有完全想出来。有些人说它改善了内部协变量的转移，而有些人不同意。但我们确实知道，它使损失表面更平滑，一层的激活可以独立于其他层进行控制，并防止重量到处乱飞。</p><p id="a68a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然如此，为什么我们还需要别人呢？当批量较小时，小批量的均值/方差可能远离全局均值/方差。这引入了大量噪声。如果批量大小为1，则不能应用批量标准化，并且它在RNNs中不起作用。</p><h1 id="a98a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">群体规范化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/53300adfb5b2cef60feabd872a135966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t79Kc7QxE4cpGxDd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@hudsonhintze?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">哈德逊·辛慈</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ef82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它计算每个训练示例的通道组的平均值和标准偏差。所以它基本上与批量大小无关。在ImageNet数据集上，组规范化与批处理大小为32的批处理规范化的性能相当，并且在较小的批处理大小上优于它。当图像分辨率很高并且由于内存限制而不能使用大批量时，组归一化是一种非常有效的技术。</p><p id="32b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于图像识别任务，实例规范化和层规范化(我们将在后面讨论)都不如批量规范化，但不是组规范化。层规范化考虑所有通道，而实例规范化只考虑导致其崩溃的单个通道。所有的通道并不同等重要，就像图像的中心到它的边缘，同时也不是完全相互独立的。因此，从技术上来说，组规范化结合了两者的优点，并消除了它们的缺点。</p><h1 id="bc34" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实例规范化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/9578bf08ece8902db7bc85e61cf756cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IejDZzA70VBjyrJC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@ericjamesward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃里克·沃德</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="0f9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，它计算每个训练图像的每个通道的平均值/方差。它用于风格转换应用，也建议作为GANs批量标准化的替代。</p><h1 id="7f70" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">图层规范化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/acb2d340e71dd32937017168b4fc3dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AP0bLLbFHYW5W0Ep"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@freetousesoundscom?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">免费使用声音</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上</p></figure><p id="b1de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">批次归一化是对各批次维度的输入进行归一化，而图层归一化是对各要素地图的输入进行归一化。再次像组和实例标准化一样，它一次对单个图像起作用，即它的平均值/方差是独立于其他示例计算的。实验结果表明，该算法在RNNs上表现良好。</p><h1 id="10b3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">权重标准化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/e1d6f8573cbf677426420e998fe9051e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P_EzOUYsncEpQQtm"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kelly Sikkema </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e0e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为描述它的最好方式是引用它的论文摘要。</p><blockquote class="nu nv nw"><p id="ff1d" class="kz la nx lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">通过以这种方式重新参数化权重，我们改进了优化问题的条件，并且我们加速了随机梯度下降的收敛。我们的重新参数化受批处理规范化的启发，但不会在小批处理中的示例之间引入任何依赖关系。这意味着我们的方法也可以成功地应用于循环模型，如LSTMs，以及噪声敏感的应用，如深度强化学习或生成模型，批量归一化不太适合这些应用。尽管我们的方法简单得多，但它仍然大大提高了整批规范化的速度。此外，我们的方法的计算开销更低，允许在相同的时间内采取更多的优化步骤。</p></blockquote><h1 id="6833" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Tensorflow中的实现</h1><p id="74e5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">如果我们不能实现它，理解理论又有什么用呢？所以我们来看看如何在Tensorflow中实现它们。使用稳定Tensorflow只能实现批量规范化。对于其他人，我们需要安装Tensorflow附加组件。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="fef4" class="oj lw it of b gy ok ol l om on">pip install -q  --no-deps tensorflow-addons~=0.7</span></pre><p id="b355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们创建一个模型，并添加这些不同的规范化层。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="840e" class="oj lw it of b gy ok ol l om on">import tensorflow as tf<br/>import tensorflow_addons as tfa</span><span id="232a" class="oj lw it of b gy oo ol l om on"><strong class="of iu">#Batch Normalization<br/></strong>model.add(tf.keras.layers.BatchNormalization())</span><span id="2114" class="oj lw it of b gy oo ol l om on"><strong class="of iu">#Group Normalization<br/></strong>model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))<br/>model.add(tfa.layers.GroupNormalization(groups=8, axis=3))</span><span id="4b65" class="oj lw it of b gy oo ol l om on"><strong class="of iu">#Instance Normalization<br/></strong>model.add(tfa.layers.InstanceNormalization(axis=3, center=True, scale=True, beta_initializer="random_uniform", gamma_initializer="random_uniform"))</span><span id="4f21" class="oj lw it of b gy oo ol l om on"><strong class="of iu">#Layer Normalization<br/></strong>model.add(tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True))</span><span id="253e" class="oj lw it of b gy oo ol l om on"><strong class="of iu">#Weight Normalization<br/></strong>model.add(tfa.layers.WeightNormalization(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')))</span></pre><p id="93e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当在组规范化中指定组的数量时，确保其值是当时存在的特征映射数量的完美除数。在上面的代码中是32，所以它的约数可以用来表示要分成的组的数量。</p><p id="1941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们知道如何使用它们，为什么不试一试呢？我们将使用具有简单网络架构的MNIST数据集。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="9c5c" class="oj lw it of b gy ok ol l om on">model = tf.keras.models.Sequential()<br/>model.add(tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))<br/>model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))<br/>#ADD a normalization layer here<br/>model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))<br/>#ADD a normalization layer here<br/>model.add(tf.keras.layers.Flatten())<br/>model.add(tf.keras.layers.Dense(128, activation='relu'))<br/>model.add(tf.keras.layers.Dropout(0.2))<br/>model.add(tf.keras.layers.Dense(10, activation='softmax'))<br/>model.compile(loss=tf.keras.losses.categorical_crossentropy,<br/>optimizer='adam', metrics=['accuracy'])</span></pre><p id="1a4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我用5种不同的批量大小，即128、64、32、16和8，尝试了所有的标准化。结果如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/4c9d16cde4dc61fb340c0d0b12ead3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3CDVkNsg3h7oKIA8ZrYEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训结果</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6bcaebe071c16621997823073867bc26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*I8HzsfKBiA4g8qBDapS2Kw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测试精度</p></figure><p id="1eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于数据集偏差和运气等差异，我不会深入研究这些结果！再训练一次，我们会看到不同的结果。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="9ae9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想更详细地了解这些或者发现更多的标准化技术，你可以参考这篇<a class="ae ky" href="http://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/" rel="noopener ugc nofollow" target="_blank">文章</a>，它对我写这篇文章有很大的帮助。如果您想进一步改善您的网络，您可以阅读以下内容:</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/beyond-the-standard-cnn-in-tensorflow-2-a7562d25ca2d"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">超越Tensorflow 2中的标准CNN</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">使用复杂的架构生成更深层次的模型，并了解不同的层，从而使模型更好。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div></div></div>    
</body>
</html>