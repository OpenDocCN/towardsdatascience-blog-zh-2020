<html>
<head>
<title>SVM Kernels: What Do They Actually Do?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SVM 内核:他们实际上做什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-kernels-what-do-they-actually-do-56ce36f4f7b8?source=collection_archive---------5-----------------------#2020-08-29">https://towardsdatascience.com/svm-kernels-what-do-they-actually-do-56ce36f4f7b8?source=collection_archive---------5-----------------------#2020-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fdca" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直观的视觉解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d7e57fec3863b3b4461e9995073a73b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yfSXkwbjGYvWodz2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·西克玛</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="23ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能听说过所谓的核技巧，这是一种允许支持向量机(SVM)很好地处理非线性数据的策略。这个想法是将数据映射到一个高维空间，在这个空间中数据变成线性的，然后应用一个简单的线性 SVM。听起来很复杂，在某种程度上确实如此。然而，尽管可能很难理解内核是如何工作的，但理解它们试图实现的目标是相当容易的。请继续阅读，自己去看看吧！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="0f7a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">当数据是线性可分的:线性 SVM</h2><p id="c8b6" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">首先:香草 SVM 是如何工作的？我们可以将支持向量机用于分类和回归任务，但是在本文中，我们将关注前者。让我们首先考虑具有线性可分的两个类的数据。我们将创建两个独立的点，并使用 scikit-learn 为它们拟合一个线性 SVM。请注意，我们在拟合模型之前对数据进行了归一化，因为支持向量机对特征的尺度很敏感。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="ef7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自定义的<code class="fe mw mx my mz b">plot_svm()</code>函数，你可以在下面看到它的输出，由相当多的 matplotlib 代码组成——如果你好奇，可以在<a class="ae ky" href="https://github.com/MichalOleszak/KnowledgeBank/blob/master/blog_posts/svm_kernels/svm_kernels.ipynb" rel="noopener ugc nofollow" target="_blank">this<strong class="lb iu"/>jupyter notebook</a>中查看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/256900bd05496a1c56fd606b303a8f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*pk6LtfZ7GuMIXdBF2Mr1pg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有线性可分数据的线性 SVM 效果很好。</p></figure><p id="c84e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多线条完美地分隔了这两个阶级，确切地说是无限多。由 SVM 拟合的线是特殊的，因为它是用虚线标记的波段的中线，并且该波段是可以在两个类别之间挤压的最宽的波段。这样，SVM 决策线(标记为黑色实线)尽可能远离这两个类，确保模型能够很好地推广到新的示例。</p><p id="00b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用红色圈出的波段边界上的观察值称为支持向量，因为它们支持或确定波段的位置。如果我们增加一些波段外的观察，它不会改变位置。</p><p id="536e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个旁注:这是一个硬边界分类的例子，这意味着不允许任何观察值进入波段。或者，我们可以做一个软边界分类:允许对波段进行一些观察，但不要太多，同时使波段更宽。这将对异常值更加鲁棒，并且可以通过<code class="fe mw mx my mz b">LinearSVC()</code>中的参数 C 来控制:我们已经将其设置为 1，但是例如将其降低到 0.1，将会产生更宽的波段，但是其中会有一些观察值。不管怎样，这和我们的故事没多大关系。</p><p id="c64b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数现实生活中的数据集不是漂亮的、线性可分的斑点。让我们看看线性 SVM 如何处理月亮形状的数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/04f6c125de7f5b28bbdc3511f63017a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*p27HkPJGZXbzev36ZRupww.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有线性不可分数据的线性 SVM 根本不起作用。</p></figure><p id="50a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这看起来不太好，是吗？让我们看看我们能做些什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="47f6" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">映射到更高维度</h2><p id="0a40" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在我们讨论内核和它们做什么之前，让我们先来看看它们利用的一个强大的思想:在高维空间中，数据有更大的机会变成线性可分的。</p><p id="dc52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的一对图清楚地说明了这一点。当我们只有一个特征时，<em class="nc"> x1 </em>，我们不能用一行来分隔数据。添加另一个特征，<em class="nc"> x2，</em>等于<em class="nc"> x1 </em>的平方，使得区分这两个类变得容易。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/96414061777afcdb3c23c45910453263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*npKj1DbEJNW5vtqQ6_pbTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">添加另一个特征使得数据可以线性分离。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="6f2e" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">内核到底是什么？</h2><p id="9a48" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">那么，内核的诀窍是什么呢？内核只是向数据添加更多特征的一种聪明方式，希望使数据线性分离。聪明，因为它们不是实际添加它们，这会使模型变慢，而是利用一些神奇的数学属性(这超出了本文的范围)，使我们能够获得与我们实际添加这些特性完全相同的结果<strong class="lb iu">，而不会使模型变慢。</strong></p><p id="ce6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个流行的内核是<strong class="lb iu">多项式内核</strong>和<strong class="lb iu">高斯径向基函数，或 RBF，内核</strong>。它们的不同之处在于它们(假装)添加的功能类型。让我们看看他们是怎么做的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="1c93" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">具有多项式核的多项式特征</h2><p id="6096" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">创建更多特征的一种方法是在一定程度上使用它们的多项式组合。例如，对于两个特征 A 和 B，2 次多项式将产生 6 个特征:1(任何 0 次幂的特征)、A、B、A、B 和 AB。我们可以使用 scikit-learn 的<code class="fe mw mx my mz b">PolynomialFeatures()</code>轻松地手动添加这些功能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="d5b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者我们可以简单地使用多项式核:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="daaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以自己验证两种实现产生大致相同的图，有点像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ae67c61e260333c155780913192c842e.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*7YkQXyCFqtBSaQ6OaEOJtg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有多项式核的决策边界。</p></figure><p id="0f9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用内核化版本的优点是，您可以指定较大的程度，从而增加数据在这个高维空间中线性分离的机会，而不会降低模型的速度。</p><p id="ef0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的月球数据示例，从散点图可以清楚地看出，3 次多项式就足够了。然而，对于更复杂的数据集，您可能需要使用更高的度数。这就是内核技巧的威力被释放出来的时候。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="fac4" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">高斯 RBF 核的相似性特征</h2><p id="f077" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">向数据添加更多特征的另一种方法是使用所谓的相似性特征。相似性特征测量现有特征的值与地标的距离。让我们实际一点:我们有一个具有单一特征的数据集，<em class="nc"> x1 </em>。我们想要创建两个相似性特征，因此我们选择两个界标，即来自我们的单个特征的参考值。让我们以-1 和 1 为例。然后，对于<em class="nc"> x1，</em>的每个值，我们计算它离第一个地标有多远(继续阅读，看看如何定义‘远’)。这是我们新的相似性特征，<em class="nc"> x2 </em>。然后我们做同样的事情，将<em class="nc"> x1 </em>的值与第二个界标进行比较，以获得<em class="nc"> x3 </em>。现在我们甚至不需要原来的功能<em class="nc"> x1 </em>！这两个新的相似性特征使我们的数据很容易分离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/7259c734154d6628d2ed29aa70475a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X244NSJOIAY0pGR3LSq1Fg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相似性特征使得数据可以线性分离。</p></figure><p id="65b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们如何计算每次观测到地标的距离呢？一个流行的选择是使用高斯径向基函数，或 RBF。它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/690e4b86e75610f7534c6b2b14576095.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*Ev94O5wZCBhtCq9DGgsZ4w.png"/></div></figure><p id="a01b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nc"> x </em>是我们的原始特征，γ是一个参数，我将其设置为 0.3。</p><p id="6ff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在我们唯一的原创特性上，第一个观察得分为-3。我们计算出<em class="nc"> x2 </em>为 exp(-0.3 * (-3-(-1)) )≈0.30，而<em class="nc"> x3 </em>为 exp(-0.3 * (-3-(1)) )≈0.01。这是右边图上最底部的点。</p><p id="8f94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我们幸运地选择了两个工作正常的地标。在实践中，可能需要大量的界标，这转化为许多新的相似性特征。这将大大降低 SVM 的速度——除非我们使用内核技巧！与多项式核类似，RBF 核允许我们获得完全相同的结果，就好像我们在原始要素的每个值处添加了一个界标，而不需要实际这样做。让我们用我们的月亮形状的数据来试试吧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8278711e09dd9fd16c1d89745050cb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*_YfFsAr_ZV-U8t9HDlmuKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有 RBF 核的决策边界，γ=0.3。</p></figure><p id="407d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策边界看起来相当不错，但是您可能已经注意到一些错误分类的例子。我们可以通过调整γ参数来修复它。它作为一个正则化因子，它越小，决策边界越平滑，从而防止过度拟合。然而，在这种情况下，我们似乎实际上欠拟合，所以让我们将γ增加到 0.5。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/860e4bf25764bb0a9db50ff817024151.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*oQpNTNrVYBNFSNK-clnU8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有 RBF 核的决策边界，γ=0.5。</p></figure><p id="f749" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在所有的例子都被正确分类了！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="cfc6" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">摘要</h2><ul class=""><li id="a7b5" class="nf ng it lb b lc mp lf mq li nh lm ni lq nj lu nk nl nm nn bi translated">支持向量机通过寻找尽可能远离数据的线性决策边界来执行分类。对于线性可分的数据，它们工作得很好，但在其他情况下却很糟糕。</li><li id="f953" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">为了使非线性数据线性可分(从而便于支持向量机)，我们可以向数据添加更多特征，因为在更高维度的空间中，数据线性可分的概率增加。</li><li id="73fd" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">要添加的两种常见类型的新特征是现有特征的多项式组合(多项式特征)和与地标的观察距离，即一些参考值(相似性特征)。</li><li id="62b1" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">实际上，添加它们可能会使模型变慢到无用的程度。</li><li id="6164" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">内核技巧是一个聪明的策略，它利用了一些数学属性，以提供相同的结果，就好像我们添加了额外的功能，但实际上并没有添加。</li><li id="a11d" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">多项式和 RBF 核(假装)分别增加了多项式和相似性特征。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><h2 id="7854" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">来源</h2><ul class=""><li id="3127" class="nf ng it lb b lc mp lf mq li nh lm ni lq nj lu nk nl nm nn bi translated">Geron A .，2019，第二版，使用 Scikit-Learn 和 TensorFlow 进行机器学习:构建智能系统的概念、工具和技术</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/eb6a925c36b8d64d7ebd9a45d3870b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vFWPhYxRKlf2PDC.png"/></div></div></figure><p id="6949" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="4d86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，为什么不在我的新文章上<a class="ae ky" href="https://michaloleszak.medium.com/subscribe" rel="noopener"> <strong class="lb iu">订阅电子邮件更新</strong> </a>？通过<a class="ae ky" href="https://michaloleszak.medium.com/membership" rel="noopener"> <strong class="lb iu">成为媒介会员</strong> </a>，你可以支持我的写作，并无限制地访问其他作者和我自己的所有故事。</p><p id="5b7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要咨询？你可以问我任何事情，也可以在这里 预定我 1:1 <a class="ae ky" href="http://hiretheauthor.com/michal" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">。</strong></a></p><p id="f1f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也可以试试<a class="ae ky" href="https://michaloleszak.github.io/blog/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">我的其他文章</strong> </a>中的一篇。不能选择？从这些中选择一个:</p><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/calibrating-classifiers-559abc30711a"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">校准分类器</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">你确定你的模型返回概率吗？🎲</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">线性回归中收缩法和选择法的比较</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">详细介绍 7 种流行的收缩和选择方法。</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok ks nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/non-linear-regression-basis-expansion-polynomials-splines-2d7adb2cc226"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">非线性回归:基础扩展、多项式和样条</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">如何用多项式和样条捕捉非线性关系？</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="om l oh oi oj of ok ks nw"/></div></div></a></div></div></div>    
</body>
</html>