<html>
<head>
<title>How to build a Neural Network for Voice Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何建立用于语音分类的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-neural-network-for-voice-classification-5e2810fe1efa?source=collection_archive---------8-----------------------#2020-05-25">https://towardsdatascience.com/how-to-build-a-neural-network-for-voice-classification-5e2810fe1efa?source=collection_archive---------8-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="28a8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python的完整代码演练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f95aa8e8de7e1d62e35d9c40417dde91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZjK3Nh7JBsLtXPR3jXe9g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://stock.adobe.com/search?k=neural&amp;search_type=default-asset-click&amp;asset_id=281653938" rel="noopener ugc nofollow" target="_blank">图片来自Adobe Stock281653938作者korkeng </a></p></figure><p id="8a4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果能将我们的Zoom会议记录下来，用纯文本的形式告诉我们谁在会议中说了什么，这不是很好吗？这个项目诞生于记录会议告诉你每个人说了什么的想法。我们有许多将语音转换为文本的转录工具，但没有多少工具可以识别每个说话者。为此，我们将使用这里包含的所有代码从头开始构建一个python语音分类器。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="0d33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，一如既往，我们需要数据。这个数据来自<a class="ae kv" href="http://www.openslr.org/12/" rel="noopener ugc nofollow" target="_blank"> OpenSLR </a>。对于我们的训练数据，我们将使用30个不同的说话者和每个说话者4个样本，给我们120个样本。每个样本平均15秒，因此每个说话者大约一分钟用于训练。对于我们的问题陈述来说，30个演讲者是一个很好的数量；因为我已经进行了这个项目，我可以断言一分钟足够给我们带来好的结果。</p><p id="27e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了用于训练的120个样本，我们还需要验证和测试数据。我们将对3个样品进行验证，并对3个样品进行测试。由于我们有30个扬声器，我们需要90个样品进行验证，90个样品进行测试。因此，我们将需要每个发言者的10个语音剪辑。</p><p id="dade" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了总结样本，我们将使用120个语音剪辑作为训练数据，90个语音剪辑作为验证数据，90个语音剪辑作为测试数据，总共300个语音剪辑。</p><p id="de6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们在三个不同的文件夹中有了用于训练、验证和测试的语音剪辑，我们就可以创建我们的熊猫数据框架了。我们需要导入<em class="lz">操作系统</em>，这样我们就可以从文件夹中的文件创建一个数据框架(我在mac上工作，对于其他操作系统，这个过程可能会有点不同)。我训练的文件夹叫<em class="lz"> 30_speakers_train </em>。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="73f2" class="mf mg iq mb b gy mh mi l mj mk">import os</span><span id="1a0c" class="mf mg iq mb b gy ml mi l mj mk">#list the files<br/>filelist = os.listdir('30_speakers_train') </span><span id="6a0e" class="mf mg iq mb b gy ml mi l mj mk">#read them into pandas<br/>train_df = pd.DataFrame(filelist)</span></pre><p id="420f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将为我的训练数据创建一个数据框架。检查数据帧的大小非常重要，这样可以确保它与文件夹中的文件数量相匹配。</p><p id="5bf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们将文件列命名为“文件”。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="684b" class="mf mg iq mb b gy mh mi l mj mk"># Renaming the column name to file<br/>train_df = train_df.rename(columns={0:'file'})</span></pre><p id="4aae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有时一个<a class="ae kv" href="https://en.wikipedia.org/wiki/.DS_Store" rel="noopener ugc nofollow" target="_blank"><em class="lz">’。DS_Store' </em> </a>文件已创建，我们必须找到并删除该特定文件才能继续。之后，我们需要重置数据帧的索引。这里有一个例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/4f0f2b0ae2555dd80a49ae088cc44bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bt6FcYvosozQ0q_du9W76w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">如何删除'的示例。“DS_Store”文件</p></figure><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="9b7a" class="mf mg iq mb b gy mh mi l mj mk"># Code in case we have to drop the '.DS_Store' and reset the index<br/>train_df[train_df['file']=='.DS_Store']<br/>train_df.drop(16, inplace=True)<br/>train_df = train_df.sample(frac=1).reset_index(drop=True)</span></pre><p id="4784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们需要确定每个发言者。在我的文件名中，第一组数字是发言者ID，因此我写了一点代码来创建一个具有该ID的列。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="00d1" class="mf mg iq mb b gy mh mi l mj mk"># We create an empty list where we will append all the speakers ids for each row of our dataframe by slicing the file name since we know the id is the first number before the hash</span><span id="60be" class="mf mg iq mb b gy ml mi l mj mk">speaker = []<br/>for i in range(0, len(df)):<br/>    speaker.append(df['file'][i].split('-')[0])</span><span id="0cda" class="mf mg iq mb b gy ml mi l mj mk"># We now assign the speaker to a new column <br/>train_df['speaker'] = speaker</span></pre><p id="8860" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的数据帧应该是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/7ee763b923ba111d630c9b3fdc513431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_pwNBMDBPg8t5Dmly8giw.png"/></div></div></figure><p id="c6cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们对验证和测试文件做同样的工作，得到三个不同的数据帧。</p><p id="bb10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了数据帧，我们需要写一个函数来提取每个音频文件的音频属性。为此，我们使用librosa，这是一个很好的用于音频操作的python库。我们需要pip安装librosa并导入librosa。下面是解析我们文件夹中的每个文件并从每个文件中提取5个数字特征的函数，即mfccs、chroma、mel、contrast和tonnetz。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="b0f1" class="mf mg iq mb b gy mh mi l mj mk">def extract_features(files):<br/>    <br/>    # Sets the name to be the path to where the file is in my computer<br/>    file_name = os.path.join(os.path.abspath('30_speakers_train')+'/'+str(files.file))</span><span id="290e" class="mf mg iq mb b gy ml mi l mj mk"># Loads the audio file as a floating point time series and assigns the default sample rate<br/>    # Sample rate is set to 22050 by default<br/>    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')</span><span id="3ca8" class="mf mg iq mb b gy ml mi l mj mk"># Generate Mel-frequency cepstral coefficients (MFCCs) from a time series <br/>    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)</span><span id="0489" class="mf mg iq mb b gy ml mi l mj mk"># Generates a Short-time Fourier transform (STFT) to use in the chroma_stft<br/>    stft = np.abs(librosa.stft(X))</span><span id="cd05" class="mf mg iq mb b gy ml mi l mj mk"># Computes a chromagram from a waveform or power spectrogram.<br/>    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)</span><span id="c671" class="mf mg iq mb b gy ml mi l mj mk"># Computes a mel-scaled spectrogram.<br/>    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)</span><span id="6bac" class="mf mg iq mb b gy ml mi l mj mk"># Computes spectral contrast<br/>    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)</span><span id="573a" class="mf mg iq mb b gy ml mi l mj mk"># Computes the tonal centroid features (tonnetz)<br/>    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),<br/>    sr=sample_rate).T,axis=0)</span><span id="feac" class="mf mg iq mb b gy ml mi l mj mk">return mfccs, chroma, mel, contrast, tonnetz</span></pre><p id="aec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个mfcc是长度为40的数组，色度为12，mel为128，对比度为7，tonnetz为6。总的来说，从我们选择的音频特征中，每个语音片段有193个数字特征。如果你愿意，请随意尝试不同的方法:l <a class="ae kv" href="https://librosa.github.io/librosa/0.6.0/feature.html" rel="noopener ugc nofollow" target="_blank"> ibrosa特征提取</a>。我们将该函数应用于每个单独的音频文件，并存储该信息。请注意，这确实需要几分钟，甚至可能需要几个小时，这取决于您使用的数据量和您的计算能力。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="60c5" class="mf mg iq mb b gy mh mi l mj mk">train_features = train_df.apply(extract_features, axis=1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/44d1e63e93d55b7b0f8076044d22f625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gs2kNwLxIe69hlzidxWhhw.png"/></div></div></figure><p id="c288" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将每个文件的所有这些数字特征连接起来，这样我们就有一个包含193个数字的单个数组来输入我们的神经网络。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="056d" class="mf mg iq mb b gy mh mi l mj mk">features_train = []<br/>for i in range(0, len(train_features)):<br/>    features_train.append(np.concatenate((<br/>        train_features[i][0],<br/>        train_features[i][1], <br/>        train_features[i][2], <br/>        train_features[i][3],<br/>        train_features[i][4]), axis=0))</span></pre><p id="99b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以将X_train设置为我们的特性的numpy数组:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="e27a" class="mf mg iq mb b gy mh mi l mj mk">X_train = np.array(features_train)</span></pre><p id="68ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，我们从一开始就为我们的验证和测试数据做同样的步骤。我们应该得到一个<em class="lz"> X_val </em>和<em class="lz"> X_test </em>。</p><p id="b98b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们完成了我们的<em class="lz"> X </em>数据。对于<em class="lz"> Y </em>，我们需要演讲者的id。请记住，我们正在进行监督学习，因此我们需要目标数据。我们从原始数据帧中得到这些，因此我们只得到这些值。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="75f7" class="mf mg iq mb b gy mh mi l mj mk">y_train = np.array(train_df['speaker'])<br/>y_val = np.array(val_df['speaker'])</span></pre><p id="7bc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要对y进行热编码，以便能够将其用于我们的神经网络。您需要从<em class="lz"> sklearn </em>中导入<em class="lz"> LabelEncoder </em>，从使用<em class="lz"> Tensorflow </em>的<em class="lz"> keras </em>中导入<em class="lz">to _ categorial</em>。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="a2a7" class="mf mg iq mb b gy mh mi l mj mk">from sklearn.preprocessing import LabelEncoder<br/>from keras.utils.np_utils import to_categorical</span><span id="f781" class="mf mg iq mb b gy ml mi l mj mk"># Hot encoding y<br/>lb = LabelEncoder()<br/>y_train = to_categorical(lb.fit_transform(y_train))<br/>y_val = to_categorical(lb.fit_transform(y_val))</span></pre><p id="6bab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们需要扩展我们的<em class="lz"> X </em>:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="ee2a" class="mf mg iq mb b gy mh mi l mj mk">from sklearn.preprocessing import StandardScaler</span><span id="a8f4" class="mf mg iq mb b gy ml mi l mj mk">ss = StandardScaler()<br/>X_train = ss.fit_transform(X_train)<br/>X_val = ss.transform(X_val)<br/>X_test = ss.transform(X_test)</span></pre><p id="724e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们准备好了有趣的部分:构建神经网络！我们将使用一个简单的前馈神经网络。输入将是我们的193个特征。我摆弄了一下辍学率，但是如果你愿意，你可以改变这些数值。使用relu是相当标准的。输出是softmax，因为我们有30个不同的类，并且因为不同的类，我们使用分类交叉熵。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="4cb6" class="mf mg iq mb b gy mh mi l mj mk">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation, Flatten<br/>from keras.callbacks import EarlyStopping</span><span id="980c" class="mf mg iq mb b gy ml mi l mj mk"># Build a simple dense model with early stopping and softmax for categorical classification, remember we have 30 classes</span><span id="4a81" class="mf mg iq mb b gy ml mi l mj mk">model = Sequential()</span><span id="21f2" class="mf mg iq mb b gy ml mi l mj mk">model.add(Dense(193, input_shape=(193,), activation = 'relu'))<br/>model.add(Dropout(0.1))</span><span id="2a88" class="mf mg iq mb b gy ml mi l mj mk">model.add(Dense(128, activation = 'relu'))<br/>model.add(Dropout(0.25))</span><span id="2459" class="mf mg iq mb b gy ml mi l mj mk">model.add(Dense(128, activation = 'relu'))<br/>model.add(Dropout(0.5))</span><span id="439a" class="mf mg iq mb b gy ml mi l mj mk">model.add(Dense(30, activation = 'softmax'))</span><span id="79ea" class="mf mg iq mb b gy ml mi l mj mk">model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')</span><span id="b069" class="mf mg iq mb b gy ml mi l mj mk">early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')</span></pre><p id="95d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们用训练和验证数据拟合模型:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="0e3f" class="mf mg iq mb b gy mh mi l mj mk">history = model.fit(X_train, y_train, batch_size=256, epochs=100, <br/>                    validation_data=(X_val, y_val),<br/>                    callbacks=[early_stop])</span></pre><p id="0ff2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一些代码，用于查看训练和验证准确性的图表:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="0696" class="mf mg iq mb b gy mh mi l mj mk"># Check out our train accuracy and validation accuracy over epochs.<br/>train_accuracy = history.history['accuracy']<br/>val_accuracy = history.history['val_accuracy']</span><span id="01c4" class="mf mg iq mb b gy ml mi l mj mk"># Set figure size.<br/>plt.figure(figsize=(12, 8))</span><span id="f22c" class="mf mg iq mb b gy ml mi l mj mk"># Generate line plot of training, testing loss over epochs.<br/>plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')<br/>plt.plot(val_accuracy, label='Validation Accuracy', color='orange')</span><span id="94af" class="mf mg iq mb b gy ml mi l mj mk"># Set title<br/>plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)<br/>plt.xlabel('Epoch', fontsize = 18)<br/>plt.ylabel('Categorical Crossentropy', fontsize = 18)<br/>plt.xticks(range(0,100,5), range(0,100,5))</span><span id="3211" class="mf mg iq mb b gy ml mi l mj mk">plt.legend(fontsize = 18);</span></pre><p id="7a58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它应该是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/dabf6808b303fe77eaf53b5902a8efdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B14xkszkgUBU2h0MZtLuIA.png"/></div></div></figure><p id="d50b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来我们得到了一些很好的准确性！让我们用测试数据来检查这些值。我们可以用神经网络的结果生成预测，并将它们与实际值进行比较。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="4b01" class="mf mg iq mb b gy mh mi l mj mk"># We get our predictions from the test data<br/>predictions = model.predict_classes(X_test)</span><span id="986c" class="mf mg iq mb b gy ml mi l mj mk"># We transform back our predictions to the speakers ids<br/>predictions = lb.inverse_transform(predictions)</span><span id="7e33" class="mf mg iq mb b gy ml mi l mj mk"># Finally, we can add those predictions to our original dataframe<br/>test_df['predictions'] = predictions</span></pre><p id="5ed0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们的测试数据dataframe应该是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/a6bebd5a808081390b940b0bd9df932b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dv2plry_kkjCY_DWNJ9g0Q.png"/></div></div></figure><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="6211" class="mf mg iq mb b gy mh mi l mj mk"># Code to see which values we got wrong<br/>test_df[test_df['speaker'] != test_df['predictions']]</span><span id="85dd" class="mf mg iq mb b gy ml mi l mj mk"># Code to see the numerical accuracy<br/>(1-round(len(test_df[test_df['speaker'] != test_df['predictions']])/len(test_df),3))*100</span></pre><p id="c682" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您应该得到这样的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/c86aa67cd890475a0b03582ed48b6b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dcjeJTjR1Qg5wweDzYPuyA.png"/></div></div></figure><p id="8377" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">98.9%的正确预测！我不得不运行神经网络四次，最终得到一个错误的预测。在前三次运行中，神经网络得到了所有正确的预测，但我想展示如何找到一个错误的预测。因此，通过一分钟的训练音频，神经网络对于30个扬声器来说近乎完美！</p><p id="c618" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这证明了一个简单的神经网络有多么强大！希望对你有帮助！</p><p id="a9f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是我的<a class="ae kv" href="https://github.com/jurgenarias/Portfolio/blob/master/Blogs/Code/Voice_Classification_full_code_for_blogpost.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter笔记本</a>的链接，里面有所有的代码。我也使用卷积神经网络(CNN)做了这个和类似的项目，如果你感兴趣，你可以在这里  <em class="lz"> </em>看到我的另一篇解释那个过程的帖子<a class="ae kv" rel="noopener" target="_blank" href="/voice-classification-with-neural-networks-ff90f94358ec"> <em class="lz">。</em></a></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="db71" class="mf mg iq bd ms mt mu dn mv mw mx dp my lf mz na nb lj nc nd ne ln nf ng nh ni bi translated"><strong class="ak">来源:</strong></h2><p id="b3a0" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">数据集:<a class="ae kv" href="http://www.openslr.org/12/" rel="noopener ugc nofollow" target="_blank">http://www.openslr.org/12/</a></p><p id="78f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]大卫·卡斯帕，亚历山大·贝利，帕特里克·富勒，<a class="ae kv" href="https://medium.com/@patrickbfuller/librosa-a-python-audio-libary-60014eeaccfb" rel="noopener"> Librosa:一个Python音频库(2019) </a></p><p id="bad8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Rami S. Alkhawaldeh，<strong class="ky ir"> </strong> <a class="ae kv" href="https://www.hindawi.com/journals/sp/2019/7213717/" rel="noopener ugc nofollow" target="_blank"> DGR:使用一维常规神经网络的人类语音性别识别</a> (2019)</p><p id="ef79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]科里·贝克尔，<a class="ae kv" href="http://www.primaryobjects.com/2016/06/22/identifying-the-gender-of-a-voice-using-machine-learning/" rel="noopener ugc nofollow" target="_blank">使用机器学习识别声音的性别</a> (2016)</p><p id="7be9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" href="https://towardsdatascience.com/@ultimatist?source=post_page-----1ef708ec5f53----------------------" rel="noopener" target="_blank">乔纳森·巴拉班</a>，<a class="ae kv" rel="noopener" target="_blank" href="/deep-learning-tips-and-tricks-1ef708ec5f53">深度学习技巧和窍门</a> (2018)</p><p id="8f2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] <a class="ae kv" href="https://medium.com/@CVxTz?source=post_page-----b0a4fce8f6c----------------------" rel="noopener"> Youness Mansar </a>，<a class="ae kv" href="https://medium.com/@CVxTz/audio-classification-a-convolutional-neural-network-approach-b0a4fce8f6c" rel="noopener">音频分类:一种卷积神经网络方法</a> (2018)</p><p id="52a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6] <a class="ae kv" href="https://www.analyticsvidhya.com/blog/author/jalfaizy/" rel="noopener ugc nofollow" target="_blank"> Faizan Shaik </a> h，<a class="ae kv" href="https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/" rel="noopener ugc nofollow" target="_blank">使用深度学习开始音频数据分析(附案例研究)</a> (2017)</p><p id="7bd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7] <a class="ae kv" href="https://medium.com/@mikesmales?source=post_page-----8bc2aa1990b7----------------------" rel="noopener">麦克·斯梅尔斯</a>，<a class="ae kv" href="https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7" rel="noopener">利用深度学习的声音分类</a>，(2019)</p><p id="4ac1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8] <a class="ae kv" href="http://aqibsaeed.github.io/" rel="noopener ugc nofollow" target="_blank"> Aaqib Saeed </a>，<a class="ae kv" href="http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/" rel="noopener ugc nofollow" target="_blank">城市声音分类，第1部分</a>，(2016)</p><p id="09a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9] Marc Palet Gual，<a class="ae kv" href="https://upcommons.upc.edu/bitstream/handle/2117/86673/113166.pdf" rel="noopener ugc nofollow" target="_blank">使用运行在FPGA上的深度神经网络进行声音性别识别</a>，(2016)</p><p id="a7a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[10] <a class="ae kv" href="https://www.endpoint.com/team/kamil_ciemniewski" rel="noopener ugc nofollow" target="_blank"> Kamil Ciemniewski </a>，<a class="ae kv" href="https://www.endpoint.com/blog/2019/01/08/speech-recognition-with-tensorflow" rel="noopener ugc nofollow" target="_blank">在TensorFlow </a>，【2019】中使用扩展卷积和CTC从零开始进行语音识别</p><p id="ac92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[11] <a class="ae kv" href="https://towardsdatascience.com/@admond1994?source=post_page-----81d0fe3cea9a----------------------" rel="noopener" target="_blank"> Admond Lee </a>，<a class="ae kv" rel="noopener" target="_blank" href="/how-to-build-a-speech-recognition-bot-with-python-81d0fe3cea9a">如何用Python构建语音识别机器人</a> (2019)</p><p id="3c5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[12] <a class="ae kv" href="https://medium.com/@adrianitsaxu?source=post_page-----486e92785df4----------------------" rel="noopener">阿德里安·易捷·许</a>，<a class="ae kv" href="https://medium.com/gradientcrescent/urban-sound-classification-using-convolutional-neural-networks-with-keras-theory-and-486e92785df4" rel="noopener">利用卷积神经网络结合Keras进行城市声音分类:理论与实现</a>，【2019】</p><p id="d2cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[13] Sainath Adapa，K <a class="ae kv" href="https://github.com/sainathadapa/kaggle-freesound-audio-tagging" rel="noopener ugc nofollow" target="_blank"> aggle freesound音频标记</a> (2018)</p><p id="8e28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[14] <a class="ae kv" href="https://librosa.github.io/librosa/0.6.0/feature.html" rel="noopener ugc nofollow" target="_blank">歌词特征提取</a></p></div></div>    
</body>
</html>