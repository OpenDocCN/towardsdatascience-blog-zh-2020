# GPT 3 号是未来。但是 NLP 在当下能做什么呢？

> 原文：<https://towardsdatascience.com/gpt-3-is-the-future-but-what-can-nlp-do-in-the-present-7aae3f21e8ed?source=collection_archive---------43----------------------->

## GPT 3 号闪闪发光，但仍有一段距离。让我们使用当今最先进的语言模型。再加点《星际迷航》来找乐子

![](img/cf7393281e9065059584a8c6d3875440.png)

Jelleke Vanooteghem 在 [Unsplash](https://unsplash.com/s/photos/words?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

关于 OpenAI 最新和最伟大的语言模型 GPT 3 的奇迹，已经有很多笔墨(或像素照明)了。用[风投](https://venturebeat.com/2020/05/29/openai-debuts-gigantic-gpt-3-language-model-with-175-billion-parameters/)的话说:

> 由 30 多名 OpenAI 研究人员组成的团队发布了一篇关于 **GPT** - **3** 的论文，这是一种语言模型，能够在一系列基准和独特的自然语言处理任务上实现最先进的结果，这些任务从语言翻译到生成新闻文章到回答 SAT 问题。

麻省理工学院在推特上写道:

公平地说，有些例子很惊人:

但是像大多数由语言模型产生的例子一样，几乎所有的例子都是经过多次运行后由人类手工选择的。因为不太好的结果不会成为新闻。

![](img/6a5cc64d4e5a92525c6a7ceb2a0fab07.png)

即使考虑到这一点，我仍然被我所看到的 GPT 3 号震撼了。但是现在我没有办法得到它的任何实际用途——目前它只对少数研究人员开放。我们这些凡人呢？当然除了玩 [AI 地牢](https://play.aidungeon.io/)之外…

# 玩 GPT 2！

![](img/eb5fe0d7de64d100b77dcaf459ed73e4.png)

现在，我们可以和 GPT 3 号的哥哥 GPT 2 号一起玩。几周前我写了 [easy_text_generator](https://github.com/alexcg1/easy_text_generator) ，这是一个在浏览器中从语言模型生成任意长度文本的工具。你可以使用 GPT-2 或几个预先训练或提炼的版本(从 [huggingface](https://huggingface.co/) 下载)来生成不同类型的文本，如[电影脚本](https://huggingface.co/cpierse/gpt2_film_scripts)、[星际旅行脚本](https://huggingface.co/alexcg1/trekbot)、 [IMDB 电影评论](https://huggingface.co/lvwerra/gpt2-imdb-pos)，或者只是一般的文本。

或者如果你想零安装，你可以直接从 huggingface 的网站生成文本，只是长度有限。例如，在这里试试 [GPT-2](https://huggingface.co/gpt2) 。

# 让我们生成一些文本！

正如我所说，你可能见过的大多数 GPT-3 的例子都是精心挑选的。为了进行合理的比较，我将每个语言模型运行 5 次，并为每个模型挑选一个输出。其他的我会放在另一篇博文里，否则，这篇博文会太长。

我们先来看看通用文本。由于我刚开始在一家公司从事神经搜索，我们将使用一些适当的种子文本:

> `In short, neural search is a new approach to retrieving information.`

## GPT-2

GPT-2 可以创造惊人的结果(查看[说英语的独角兽的例子](https://openai.com/blog/better-language-models/))，但它需要它的甜蜜时间，吞噬 CPU 和内存。大约 10 分钟后，它吐出了以下内容:

![](img/7f7cdd32a09587017f9aa9a22fdd7651.png)

> 简而言之，神经搜索是一种检索信息的新方法。在理想的情况下，应该有很多新的数据需要搜索，但是大部分数据已经存储在关系数据库中。这就是为什么大多数时候最好不要在数据库中存储任何信息。在未来，我们想要的是信息可以被简单地搜索到。但是，即使信息很容易被搜索到，仍然存在如何在数据库中组合多个搜索词的问题。组合搜索词的一种方法是使用搜索引擎，但当我们在谷歌或任何其他搜索引擎上搜索某个东西时，它们通常会很好，但有时它们会出错，它们只会显示部分结果，有时它们不会显示所有结果，或者它们会说一些非常具体的东西，但你无法在特定的搜索中找到它。神经搜索最重要的目标之一是让他们更容易地搜索，以便他们能够做出更好的决定。但仍然有一个问题:一些类型的问题并不适合它，如果搜索引擎没有在正确的地方使用正确的工具，这可能会导致大量信息丢失。现在，我认为我们可以从神经搜索中吸取一些教训来解决这个问题...

对我来说看起来有点连贯，虽然它没有保持一个清晰的叙述，亲爱的上帝，其中一些连续的句子！

## 蒸馏 2

distilGPT2 是 GPT-2 的一个更有效的版本，删除了一些多余的神经元。从它的输出来看，也许神经元太多了！生成每个结果大约需要一分钟:

> 简而言之，神经搜索是一种检索信息的新方法。但近年来，该领域鲜有进展，现在有证据表明，它在检测和分析人类皮肤上的神经网络方面更加准确。这是这个故事的要点。中国国家科学研究基金会(CSRF)的研究人员与加州大学圣地亚哥分校的研究人员合作，他们正在研究一种神经成像和学习网络，该网络将绘制和重建人类皮肤上的神经网络。这个大脑项目旨在研究大脑的视觉皮层，以便它可以在不丢失信息的情况下向个人学习。CSRF 还将使用神经网络绘制大脑的感觉皮层和大脑皮层，以及前部和尾部皮层，这是大脑中央皮层的一部分。但这可能不是探索破译神经网络的第一步，神经网络是一种用于搜索各种人的信息模式的方法，包括那些不在药物接受端的人。加州大学圣地亚哥分校(USDA)的研究人员与中国国家科学研究基金会(CSRF)的科学家合作，与 CSRF 合作，开发了一种允许科学家搜索皮肤信息模式的技术。研究人员分析了大脑的感觉...

如果我们把种子文本扔向一些旨在产生不同东西的东西，比如电影剧本，会怎么样？

## gpt2 _ 电影 _ 剧本

gpt2_film_scripts 是通过[在 1，300 个剧本上微调 GPT-2，](/film-script-generation-with-gpt-2-58601b00d371)完成的，与常规文本相比，它们有着怪异的间距和边距。当你生成例子时，这真的显示出来了。像 GPT-2(这个模型就是基于它)一样，每个例子需要 10 分钟:

> 简而言之，神经搜索是一种检索信息的新方法。已经有很多很多的研究表明，我们找到的，我们寻找的。这是一种计算机辅助搜索。
> 
> 艾丽斯
> 我不喜欢那样。我觉得自己像个傻瓜。
> 
> 她还在起床，已经上了一上午的课。她坐在书桌前，研究着电脑屏幕。这是一大堆文件，她很快浏览了一遍，很快注意到一只猫的照片。她盯着照片，拿出一个小文件。

![](img/dabcc658102b673a8e975ae093dbf895.png)

Via [gfycat](https://gfycat.com/thoseevenkinkajou)

正如我们所看到的，语言模型已经学会了如何骂人，而且不怕这么做！(我不得不手动审查它的便壶嘴)。我们还可以看到它理解角色台词和舞台方向。就像每个网民一样，它对猫情有独钟。(如果与其他样品相比，它看起来有点短，那是因为它增加了许多间隔(对于 magins ),所以 Medium 剥离了。

## trekbot

我的一个梦想是创作完全由人工智能生成的《星际迷航》剧集。少校数据科学，如果你愿意的话。所以我在《t2》上训练了 gpt2_film_scripts，这是我能找到的每一个《星际迷航》剧本。

**然后……在说了那句悲惨的《星际迷航》双关语后，我的 easy_text_generator 崩溃了。我有一种感觉，这些人工智能可能比我们想象的更聪明(也更情绪化)。**

![](img/9069b5018197bab7c5c5908aed9697dd.png)

通过 [gfycat](https://gfycat.com/scratchycalculatinggerbil-reactions-star-trek-the-next-generation)

重新启动后:

> 简而言之，神经搜索是一种检索信息的新方法。新的前进方向。
> 
> 法里斯:关键是神经模式本身，而不是计算机。
> 
> 西斯科:但是电脑可以做到这一点。
> 
> 法里斯:这是计算神经网络理论的突破。很明显，它们需要被设计成能够学习、复制，然后分享它们的知识以用于实际应用。
> 
> 西斯科:那你就有问题了。我们怎么知道会有用？
> 
> 法里斯:我们可能永远不会在纸上看到它，但从各方面考虑，神经模式发生器是我见过的最有前途的硬件。
> 
> 西斯科:我们最终会到达那里的。当我们这么做的时候，我们能相信这个吗？
> 
> [法里斯的办公室]
> 
> 法里斯:神经模式发生器将是我见过的最有前途的硬件。
> 
> [船长办公室]
> 
> (莫恩正在星际舰队总部发表演讲，雷丁。)
> 
> 法里斯:我和我的同事花了一年时间开发原型，将于今年年底推出。
> 
> 西斯科:那会如何影响星际舰队的运作？

一点也不差。对于任何了解《星际迷航》的人来说，这听起来很有说服力(尽管 Morn 发表演讲显然很搞笑。)

![](img/1d49a60a9542b1209893beb3a5ef96c3.png)

不太爱说话的早晨。Via [gfycat](https://gfycat.com/smartuntimelyangora-star-trek-deep-space-nine-who-mourns-for-morn)

# 这是一个总结(目前)

正如我们在上面看到的，GPT 2 号相当强大。既用于生成通用文本，也用于更具体的用例(如 Star Trek 脚本)。

但是对于这些特定的用例，GPT-2 需要大量的数据，以及大量的历元来检查所有的数据。GPT-3 承诺“少量学习”,所以我们只需向它扔几个例子，它就会很快学会这种风格。因此，如果你喜欢《指环王》系列的新书，或者关于<insert product="" here="">的新闻稿，GPT-3 可能是你的语言模型！</insert>

如果你想要一个简单的方法来开始生成你自己的语言，看看[easy _ text _ generator repo](https://github.com/alexcg1/easy_text_generator)。我很想看看会有什么结果！

Alex 是纪娜人工智能公司的开源倡导者，电子蝴蝶的建造者，编写糟糕的星际旅行脚本的 janky 语言模型的训练者。