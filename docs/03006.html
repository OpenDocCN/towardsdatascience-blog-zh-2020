<html>
<head>
<title>Introduction to Natural Language Processing with the Beatles and Taylor Swift</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">甲壳虫乐队和泰勒·斯威夫特的自然语言处理介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-natural-language-processing-with-the-beatles-and-taylor-swift-2a06055cbc14?source=collection_archive---------35-----------------------#2020-03-22">https://towardsdatascience.com/introduction-to-natural-language-processing-with-the-beatles-and-taylor-swift-2a06055cbc14?source=collection_archive---------35-----------------------#2020-03-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6caa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用不同的技术操作非结构化数据，如标记化、词条化、停用词、TF-IDF。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d509642f2a18ab879890577a69d9eef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsjpbFyItXA_xWPIIB9OBg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">利物浦的披头士雕像。<a class="ae ky" href="https://pixabay.com/photos/beatles-liverpool-statue-2395311/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="f708" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">自然语言处理是一个有趣的领域，因为消除输入句子的歧义以产生机器表示语言是发人深省的。看看著名的格劳乔·马克斯的笑话:</p><blockquote class="me mf mg"><p id="4e58" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">一天早上，我射杀了一头穿着睡衣的大象。我永远不知道他是怎么穿上我的睡衣的。</p></blockquote><p id="dc83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在人的层面上，这句话有几种解读。但是计算机几乎不可能理解。</p><p id="dc8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管如此，NLP的学习曲线并不陡峭，而且可能很吸引人。在这个项目中，我将用披头士和泰勒斯威夫特的歌词来解释自然语言处理的初级水平。最终目标是<strong class="lb iu">比较披头士和泰勒·斯威夫特的歌词的用词</strong>，看看我们能否将它们聚类。要实现这一目标，有6个步骤:</p><ul class=""><li id="744e" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated"><strong class="lb iu">过滤和清洗</strong></li><li id="5668" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">标记化</strong></li><li id="4e30" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">词干化、词汇化、</strong>和<strong class="lb iu">停用词</strong></li><li id="0bac" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">矢量化</strong></li><li id="1c73" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">词频—逆文档频率(TF-IDF) </strong></li><li id="5d0a" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">可视化</strong></li></ul><h2 id="02ff" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">0)导入歌曲:</h2><p id="a79d" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">你不需要运行代码来理解这个博客。但是如果你对它感兴趣，你可以在这里找到我的Github库<a class="ae ky" href="https://github.com/williamhuybui/Blog-NLP-with-the-Beatles-and-Taylor-Swift" rel="noopener ugc nofollow" target="_blank"/>。该文件夹的树形结构为:</p><ul class=""><li id="8f68" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">main.ipynb</li><li id="7658" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">数据-&gt;(披头士-&gt; 10首歌)，(泰勒斯威夫特-&gt; 10首歌)</li></ul><p id="a96e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">甲壳虫乐队:爱我吧，我想牵着你的手，在我的吉他轻轻哭泣的时候，一起来吧，一些事情，埃莉诺·里格比，生活中的一天，不要让我失望，救命，嘿，裘德</p><p id="2ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">泰勒斯威夫特(Taylor Swift):嫌隙，你需要冷静下来，秀气，男人，你属于我，很快你就会好起来，看你让我做什么，我！，爱情故事，22。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="a9dc" class="mz na it ny b gy oc od l oe of">import glob<br/>TheBeatle=glob.glob("Data/TheBeatle/*.txt")<br/>TaylorSwift=glob.glob("Data/TaylorSwift/*.txt")<br/>filenames=TheBeatle+TaylorSwift</span></pre><p id="587d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要一首歌来做测试，让我们看看“嘿，裘德”的前4行</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="dcba" class="mz na it ny b gy oc od l oe of">with open('Data/TheBeatle/HeyJude.txt') as f:<br/>    test_song = f.readlines()<br/>    test_song = test_song[:4]<br/>    print(test_song)</span><span id="768d" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; ["Hey Jude, don't make it bad\n", 'Take a sad song and make it better\n', 'Remember to let her into your heart\n', 'Then you can start to make it better\n']</span></pre><p id="7676" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了，我们刚刚完成了项目的第一步。</p><h2 id="9c64" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">1) <strong class="ak">过滤和清洗</strong></h2><p id="f516" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">我们需要问自己的第一个问题是我们要清洁什么。我们的目标是降低句子的复杂性，删除无意义的字符。要更改为不同的歌曲，请查看<code class="fe oh oi oj ny b">filenames</code>列表并选择歌词对应的目录。从这一点来说，我把链接改为<code class="fe oh oi oj ny b">‘Data/TaylorSwift/YouNeedToCalmDown.txt'</code>作为例子，因为它的内容有更多有趣的字符需要清理。让我们来看看泰勒·斯威夫特的《你需要冷静》的前几行。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="786a" class="mz na it ny b gy oc od l oe of">["You are somebody that I don't know\n", "But you're takin' shots at me like it's Patrón\n", "And I'm just like, damn, it's 7 AM\n", "Say it in the street, that's a knock-out\n"]</span></pre><p id="9696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要删除<code class="fe oh oi oj ny b">\n</code>和一些标点符号，如果他们存在。我们想把号码留在那里，它可能表明歌手的风格。小写所有的单词使其易于处理，但可能会使专有名词变成常规名词。我们还需要把你是这个词转换成你是，我是我是，那是那是，采取，但这一步将在后面的步骤中实现。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="113c" class="mz na it ny b gy oc od l oe of">import string</span><span id="bb42" class="mz na it ny b gy og od l oe of">punctuations = list(string.punctuation)<br/>punctuations.remove("'") #Not include the apostrophe<br/>punctuations+="\n"</span><span id="40d4" class="mz na it ny b gy og od l oe of">def clean_song(song):<br/>    cleaned = []<br/>    for line in song:<br/>        for symbol in punctuations:<br/>            line = line.replace(symbol, '').lower()<br/>        cleaned.append(line)<br/>    return cleaned</span><span id="fde8" class="mz na it ny b gy og od l oe of">clean_song(test_song)[:4]</span><span id="6137" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; ["you are somebody that i don't know",<br/> "but you're takin' shots at me like it's patrón",<br/> "and i'm just like, damn, it's 7 am",<br/> "say it in the street, that's a knock-out"]</span></pre><p id="02ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，我想把撇号<code class="fe oh oi oj ny b">'</code>留在那里，因为去掉它可能会使一个单词变得毫无意义</p><p id="dbab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2) </strong> <strong class="lb iu">标记化</strong></p><p id="6b0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">标记化</strong>是将字符串、文本标记化或拆分成一系列标记的过程。我们可以把记号看作是部分，就像单词是句子中的记号，句子是段落中的记号一样。</p><p id="b18e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标记一个单词最简单的方法是<code class="fe oh oi oj ny b">split()</code>方法，它将一个字符串转换成一个字符串数组。然而，这种方法有一个问题。像<code class="fe oh oi oj ny b">don’t</code>或<code class="fe oh oi oj ny b">Taylor’s</code>这样的单词是两个不同单词的组合。很难列出适用于所有情况的所有拆分规则。因此，我们需要NLTK标记器的帮助。NLTK代表自然语言处理工具包。它是文本处理的一站式商店，每个人都可以使用。让我们看看NLTK函数<code class="fe oh oi oj ny b">word_tokenize</code>，看看它对一个随机的句子做了什么。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="fa34" class="mz na it ny b gy oc od l oe of">from nltk import word_tokenize</span><span id="5495" class="mz na it ny b gy og od l oe of">sentence= "I'm dangerous. Don't underestimate!!!"<br/>print(word_tokenize(sentence))<br/>print(sentence.split(" "))</span><span id="f4d2" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt;['I', "'m", 'dangerous', '.', 'Do', "n't", 'underestimate', '!', '!', '!']<br/>&gt;&gt;&gt;["I'm", 'dangerous.', "Don't", 'underestimate!!!']</span></pre><p id="419b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意<code class="fe oh oi oj ny b">word_tokenize</code>将否定形式的<code class="fe oh oi oj ny b"><em class="mh">verb</em></code> <em class="mh">拆分为<code class="fe oh oi oj ny b">verb</code> + <code class="fe oh oi oj ny b">n’t</code>。它还将感叹号和点号从单词中分离出来。在这种情况下，我们已经在前面的部分照顾到了标点符号，但重复是可以的。下面是接收歌曲的干净版本并对其进行标记的函数:</em></p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="9edb" class="mz na it ny b gy oc od l oe of">def tokenize(song):<br/>    joined_song = ' '.join(song) #Join sentences together<br/>    tokenized_song = word_tokenize(joined_song)<br/>    return tokenized_song</span></pre><h2 id="bff4" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated"><strong class="ak"> 3)词干化、词汇化、</strong>和<strong class="ak">停用词</strong></h2><p id="e0b5" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">根据Geek to Geek的说法，<strong class="lb iu">词干化</strong>是产生词根/基本词的形态变体的过程。<strong class="lb iu">词尾变化</strong>是<strong class="lb iu"> </strong>将一个词的不同屈折形式组合在一起的过程，因此它们可以作为一个单项进行分析。</p><p id="e40e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几个例子展示了每种方法对我们工作的不同影响，并决定了我们想要包含哪种方法</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="c297" class="mz na it ny b gy oc od l oe of">from nltk.stem import PorterStemmer  <br/>from nltk.stem import WordNetLemmatizer <br/>   <br/>ps = PorterStemmer() <br/>lemmatizer = WordNetLemmatizer()</span><span id="3de2" class="mz na it ny b gy og od l oe of"># choose some words to be stemmed <br/>words = ["takin'", "knock-out", "don't", "dangerous","working", "apples", 'better', 'died' ] <br/>  <br/>for w in words: <br/>    print(w,":", ps.stem(w),lemmatizer.lemmatize(w,pos ="v"))</span><span id="7ed1" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; <br/>takin' : takin' takin'<br/>knock-out : knock-out knock-out<br/>don't : don't don't<br/>dangerous : danger dangerous<br/>working : work work<br/>apples : appl apples<br/>better : better better<br/>died : die die</span></pre><p id="527a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一列是我们文本中的单词，后面是<strong class="lb iu">词干</strong>和<strong class="lb iu">词条列</strong>。词干是处理单词的一种简单方法。它只是根据某些特定的规则截断单词，将其转换为词根。有时候会把单词搞得很乱。我们不希望<code class="fe oh oi oj ny b">apples</code>变成<code class="fe oh oi oj ny b">appl</code>。这就是为什么我做<strong class="lb iu">不选</strong>炮泥的原因。</p><p id="5772" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，词汇化之后是更复杂的规则。注意在方法中<code class="fe oh oi oj ny b">lemmatize</code>把<code class="fe oh oi oj ny b">pos</code> <em class="mh"> </em> ( <strong class="lb iu">词性</strong>)改成了<code class="fe oh oi oj ny b">“v”</code>(动词)。所以<code class="fe oh oi oj ny b">died</code>变成了<code class="fe oh oi oj ny b">die</code>。如果我们把<code class="fe oh oi oj ny b">pos</code>改成<code class="fe oh oi oj ny b">"n"</code>，<code class="fe oh oi oj ny b">apples</code>就会变成<code class="fe oh oi oj ny b">apple</code>。如果我们把<code class="fe oh oi oj ny b">pos</code>改成<code class="fe oh oi oj ny b">"a"</code>(形容词)，<code class="fe oh oi oj ny b">better</code>就会变成<code class="fe oh oi oj ny b">good</code>。我们可以过滤许多其他层，但是为了更简单，我将依次应用<code class="fe oh oi oj ny b">“a”</code>、<code class="fe oh oi oj ny b">”v”</code>和<code class="fe oh oi oj ny b">“n”</code>层。</p><p id="8f44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">停用词:</strong>停用词是我们可以简单忽略的常用词(如“the”、“A”、“an”、“in”)，因为它们不会给我们的分析带来任何价值。下面的代码显示了我们如何从<code class="fe oh oi oj ny b">nltk</code>导入停用词列表</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="50e3" class="mz na it ny b gy oc od l oe of">from nltk.corpus import stopwords</span><span id="e1e1" class="mz na it ny b gy og od l oe of">stopwords_list = stopwords.words('english')<br/>print(len(stopwords_list))<br/>print(stopwords_list[:10])<br/>print(stopwords_list[35:45])</span><span id="3f83" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt;179<br/>&gt;&gt;&gt;['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]<br/>&gt;&gt;&gt;['what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am']</span></pre><p id="8538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有179个停用词，但我不想全部使用。对于这个项目，我认为<strong class="lb iu">代词</strong>可能表明作者对这首歌的观点。例如，我在本文中使用“我们”,因为我希望读者和我在同一页上。你和我在歌曲中重复的词可能表明这首歌实际上是作者的故事。泰勒·斯威夫特因写她过去恋情的爱情故事而闻名，这可能是她与披头士的区别。然而，第35个停用词可能就没那么有趣了</p><p id="67ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还创建了几个不在列表中的停用词。我知道该补充些什么，因为我尝试过不同的歌曲来理解缺失了什么。让我们为我们已经讨论过的内容创建一个函数:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7837" class="mz na it ny b gy oc od l oe of">from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer</span><span id="b7da" class="mz na it ny b gy og od l oe of">stopwords_list = stopwords.words('english')<br/>stopwords_list=stopwords_list[35:]<br/>stopwords_list+=["n't", "it", "'s","'m", "'re","'"]<br/>lemmatizer = WordNetLemmatizer()</span><span id="76f5" class="mz na it ny b gy og od l oe of">def lemmatize_remove_stopwords(tokenize_list):<br/>    lemmatized_words=[]<br/>    for w in tokenize_list:<br/>        w=lemmatizer.lemmatize(w,pos ="a")<br/>        w=lemmatizer.lemmatize(w,pos ="n")<br/>        w=lemmatizer.lemmatize(w,pos ="v")<br/>        lemmatized_words.append(w)<br/>    song_noStopword = [i for i in lemmatized_words if i not in stopwords_list]<br/>    return song_noStopword</span><span id="0031" class="mz na it ny b gy og od l oe of">print(lemmatize_remove_stopwords(tokenize(test_song))[:23])</span><span id="ba9c" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; ['you', 'somebody', 'i', 'know', 'you', 'takin', 'shoot', 'me', 'like', 'patrón', 'i', 'like', 'damn', '7', 'say', 'street', 'knockout']</span></pre><h2 id="b12d" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">4) <strong class="ak">矢量化和计数频率</strong></h2><p id="1dbd" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">有了一个非常好的可以清理和标记歌曲的函数后，我们的下一步是计算每个单词出现的次数。该函数将接收一个令牌列表，并返回唯一的令牌及其计数。注意，在后面一步，我们要比较<em class="mh">所有歌曲的相似度。</em>因此<strong class="lb iu">，t </strong>第二个输入(可选)应该是所有歌曲的<strong class="lb iu">唯一令牌。</strong>我们来看看下面的功能:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="713f" class="mz na it ny b gy oc od l oe of">def vectorize_and_freq(song, vocab=None):<br/>    <br/>    if vocab:<br/>        unique_words = vocab<br/>    else:<br/>        unique_words = list(set(song))</span><span id="37e8" class="mz na it ny b gy og od l oe of">    #initial bag of word dictionary<br/>    song_dict = {i:0 for i in unique_words} <br/>    <br/>    for word in song:<br/>        song_dict[word] += 1<br/>    <br/>    return song_dict</span><span id="ab64" class="mz na it ny b gy og od l oe of">output_=lemmatize_remove_stopwords(tokenize(test_song))<br/>output_=vectorize_and_freq(output_)<br/>list(output_.items())[:10]</span><span id="17a6" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; [('le', 1), ('But', 6), ('okay', 1), ('bone', 1), ('calm', 8), ('When', 1), ('must', 1), ('your', 2), ('see', 1), ('like', 7)]</span></pre><p id="c83c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太棒了。你差不多完成了。现在到了项目中最重要的一步。</p><h2 id="059d" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated"><strong class="ak"> 4)词频—逆文档频率(TF-IDF) </strong></h2><p id="6ca3" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">TF-IDF是两个单独指标的组合，即TF和IDF。当我们有多个文档时，使用TF-IDF。这个想法是一个单词在一种类型的文档中出现的次数比其他的多。这一步的最终目标是有一个包含20首歌曲的表格，其中的列是所有文档中唯一的vocabs。每列中的值是由TF-IDF公式分配的每个单词的相应权重</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/801009e8de9e1fec2071c2503b55ca4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*s6h_ebvP0_SnjMQytfan5g.png"/></div></figure><p id="f18d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> a)术语频率</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/87e39572533ef9d92cb81d85ab88a1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFXKneYTar9F_RKlUempPg.png"/></div></div></figure><p id="8db0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，我们希望找到单词<code class="fe oh oi oj ny b">t</code>在单个文档中的出现率。让我们创建词频函数，并显示在<strong class="lb iu">整首歌</strong>中最常见的前10个词。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6bca" class="mz na it ny b gy oc od l oe of">def term_frequency(BoW_dict):<br/>    total_word_count = sum(BoW_dict.values()) <br/>    <br/>    for ind, val in BoW_dict.items():<br/>        BoW_dict[ind] = val/ total_word_count<br/>    <br/>    return BoW_dict</span><span id="a63a" class="mz na it ny b gy og od l oe of">#Apply different function <br/>output_=clean_song(test_song)<br/>output_=tokenize(output_)<br/>output_=lemmatize_remove_stopwords(output_)<br/>output_=vectorize_and_freq(output_)<br/>ouput_=term_frequency(output_)</span><span id="91b3" class="mz na it ny b gy og od l oe of">#Sort the output_ dictionary by values and then keys<br/>ouput_=sorted(ouput_.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)</span><span id="c430" class="mz na it ny b gy og od l oe of">for i in ouput_[:5]:<br/>    print(i)</span><span id="a25d" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; ('you', 0.1542056074766355)<br/>&gt;&gt;&gt; ('ohoh', 0.14018691588785046)<br/>&gt;&gt;&gt; ('need', 0.056074766355140186)<br/>&gt;&gt;&gt; ('like', 0.04672897196261682)<br/>&gt;&gt;&gt; ('i', 0.037383177570093455)</span></pre><p id="efa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oh oi oj ny b">term_frequency's</code>函数只是一个标准化的<code class="fe oh oi oj ny b">vectorize_and_freq</code>函数，在一首歌中使用vocabs。</p><p id="192c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> b)逆文档频率</strong></p><p id="a20d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逆文档频率取文档总数与包含单词<code class="fe oh oi oj ny b">t</code>的文档数之比的自然对数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8d543e45a80112bf15e5f5d2ae11f5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*C3fUuyibt6g7WN44L8ZfoA.png"/></div></figure><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="962a" class="mz na it ny b gy oc od l oe of">def inverse_document_frequency(dicts_list):<br/>    #0 : Total Number of Document<br/>    num_doc=len(dicts_list)<br/>    <br/>    #1: Find all the unique words for all the document<br/>    vocab_set=[]<br/>    for d in dicts_list:<br/>        for word in d.keys():<br/>            vocab_set.append(word)<br/>    vocab_set=set(vocab_set)<br/>    <br/>    #2: Number of document with t in it<br/>    idf_t = {i:0 for i in vocab_set} #initial lize the list<br/>    <br/>    for word in idf_t.keys():<br/>        docs = 0<br/>        # Find number of doc for each word<br/>        for d in dicts_list:<br/>            if word in d:<br/>                docs += 1<br/>        <br/>        # Compute idf for each t<br/>        idf_t[word] = np.log((num_doc/ float(docs)))<br/>    <br/>    return idf_t</span></pre><p id="c67e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在分别计算了每个TF和IDF之后，让我们将它们组合在一起</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="1d01" class="mz na it ny b gy oc od l oe of">def tf_idf(dicts_list):<br/>    # Vocab for corpus<br/>    doc_tf_idf = {}<br/>    idf = inverse_document_frequency(dicts_list)<br/>    full_vocab_list = {i:0 for i in list(idf.keys())}<br/>    <br/>    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document<br/>    tf_idf_list_of_dicts = []<br/>    <br/>    # Now, compute tf and then use this to compute and set tf-idf values for each document<br/>    for doc in dicts_list:<br/>        doc_tf = term_frequency(doc)<br/>        for word in doc_tf:<br/>            doc_tf_idf[word] = doc_tf[word] * idf[word]<br/>        tf_idf_list_of_dicts.append(doc_tf_idf)<br/>    return tf_idf_list_of_dicts</span></pre><p id="8788" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数获取每首歌曲的词汇字典列表，并为每首歌曲中的每个词汇创建相应的“tf-idf”权重。注意，这些权重不能为0，因为TF和IDF项总是大于0。</p><p id="8847" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们希望将我们创建的所有函数组合成一个元函数:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="c27f" class="mz na it ny b gy oc od l oe of">def main(filenames):<br/>    # Iterate through list of filenames and read each in<br/>    cvad = [] #count vectorized all documents<br/>    for file in filenames:<br/>        with open(file) as f:<br/>            raw_data = f.readlines()<br/>        # Clean and tokenize raw text<br/>        ouput_ = clean_song(raw_data)<br/>        ouput_ = tokenize(ouput_)<br/>        ouput_=lemmatize_remove_stopwords(ouput_)  <br/>        ouput_ = vectorize_and_freq(ouput_)<br/>        cvad.append(ouput_)<br/>    # tf-idf representation of everything<br/>    tf_idf_all_docs = tf_idf(cvad)<br/>    <br/>    return tf_idf_all_docs</span><span id="c17f" class="mz na it ny b gy og od l oe of">tf_idf_all_docs = main(filenames)</span><span id="7847" class="mz na it ny b gy og od l oe of">print("Number of Dimensions: {}".format(len(tf_idf_all_docs[12])))<br/>for key, value in list(tf_idf_all_docs[15].items())[20:25]:<br/>    print(key,value)</span><span id="727b" class="mz na it ny b gy og od l oe of">&gt;&gt;&gt; Number of Dimensions: 712<br/>&gt;&gt;&gt; let 0.003921735518976991<br/>&gt;&gt;&gt; happy 0.016807190459810554<br/>&gt;&gt;&gt; get 0.006288801694780355<br/>&gt;&gt;&gt; your 0.01002310915754525<br/>&gt;&gt;&gt; think 0.0033410363858484165</span></pre><p id="6f85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个主函数的输出是20首歌曲的列表，每个单词对应一个tf-idf权重。20首歌里有712个vocabs(词根)。上面的输出显示了泰勒·斯威夫特的《微妙》中一些词汇的tf-idf权重。</p><p id="5605" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可视化</p><p id="bc45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是把这20首歌画在一个可视化的空间里。然而，有712个维度。谈到降维，我想到的有两种方法:<strong class="lb iu">主成分分析(PCA) </strong>和<strong class="lb iu">T-分布式随机邻居嵌入(t-SNE)。</strong>PCA侧重于保持数据点从高维到低维的方差，而t-SNE侧重于相似点的概率分布。因此t-SNE更适合这个项目。</p><p id="0e97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义将数据转换为2D的t-SNE实例:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="c900" class="mz na it ny b gy oc od l oe of">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.manifold import TSNE</span><span id="5ffa" class="mz na it ny b gy og od l oe of">np.random.seed(4)<br/>tf_idf_all_docs = main(filenames)<br/>tf_idf_vals_list= [list(i.values()) for i in tf_idf_all_docs] <br/>TSNE_2d = TSNE(n_components=2)<br/>data_2d = TSNE_2d.fit_transform(tf_idf_vals_list)</span></pre><p id="4673" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我将种子更改为4，因为在不同的运行中可能会有不同的结果。原因是不像PCA，t-SNE有一个非凸的目标函数。使用随机启动的梯度下降优化来最小化目标函数。让我们来看看我们歌曲中的2D情节:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="86cb" class="mz na it ny b gy oc od l oe of">TheBeatle = data_2d[:10]<br/>beatle_x = [i[0] for i in TheBeatle]<br/>beatle_y = [i[1] for i in TheBeatle]</span><span id="e9c0" class="mz na it ny b gy og od l oe of">TaylorSwift = data_2d[10:]<br/>ts_x = [i[0] for i in TaylorSwift]<br/>ts_y = [i[1] for i in TaylorSwift]</span><span id="5878" class="mz na it ny b gy og od l oe of">fig = plt.figure(figsize=(20,10))<br/>ax = fig.add_subplot(222)<br/>ax.scatter(beatle_x, beatle_y, color='g', label='The Beatles')<br/>ax.scatter(ts_x, ts_y, color='r', label='Taylor Swift')<br/>ax.legend()<br/>plt.show()</span></pre><p id="2cc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是4号种子的结果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/39fc8a2925caefa15d3b87dd551cc0ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1z75Cre0qwGDrYkWVS_LA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">种子=4的t-SNE 2D地图。资料来源:Huy Bui</p></figure><p id="62a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图没有真正显示清晰的集群。让我们试试10号种子</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/f79c2fdd797c7431504890eb5c8f0386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MTfWOxwfAUXUfAfDQ9VMLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">种子=10的t-SNE 2D地图。资料来源:Huy Bui</p></figure><p id="4590" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些点更加分散。关于这个结果，我们可以总结出几点:</p><ul class=""><li id="610f" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">也许我们没有足够的歌曲</li><li id="12ee" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">也许预处理步骤没有真正概括每个歌曲作者的特征</li><li id="f07f" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">首先，也许没有太大的区别。</li><li id="796b" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">也许我们可以做更多的特色工程。但是这超出了本文的范围。</li></ul><p id="8ee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多不确定性，但我们肯定知道的一件事是，NLP需要很多思想和技术来产生一些有意义的东西。</p><h1 id="5f19" class="op na it bd nb oq or os ne ot ou ov nh jz ow ka nk kc ox kd nn kf oy kg nq oz bi translated">摘要</h1><p id="d1d6" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">在这个项目中，我们学习了自然语言处理中的一些基本概念，如标记化、词干化、词汇化、TF-IDF，以及各种删除停用词和可视化结果的策略。</p><p id="ee9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div></div>    
</body>
</html>