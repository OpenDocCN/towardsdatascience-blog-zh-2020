<html>
<head>
<title>Language Modeling II: ULMFiT and ELMo</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言建模II: ULMFiT和ELMo</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f?source=collection_archive---------27-----------------------#2020-04-29">https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f?source=collection_archive---------27-----------------------#2020-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="3982" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">这是语言建模5部分系列的第2部分。</p></blockquote><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/32627b3cc7536d3b125a51e2473dad77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCJtttoDKHtc-r46Ov034w.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">搜索引擎中常用的语言模型</p></figure><h1 id="89c6" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">介绍</h1><p id="c16d" class="pw-post-body-paragraph jt ju it jw b jx mg jz ka kb mh kd ke mi mj kh ki mk ml kl km mm mn kp kq kr im bi translated">在之前的<a class="ae mo" rel="noopener" target="_blank" href="/language-modeling-c1cf7b983685">帖子</a>中，我们了解了语言建模的概念，以及它与word2vec和GloVe等常规预训练嵌入的不同之处。</p><p id="a504" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">在我们迈向REALM(检索增强语言模型预训练)的旅程中，我们将简要浏览这些关于语言模型的开创性著作:</p><ol class=""><li id="ed50" class="mp mq it jw b jx jy kb kc mi mr mk ms mm mt kr mu mv mw mx bi translated"><a class="ae mo" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iu"> ELMo </strong> </a> <strong class="jw iu"> : </strong>来自语言模型的嵌入</li><li id="ee8c" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr mu mv mw mx bi translated"><a class="ae mo" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"><strong class="jw iu">ul mfit</strong></a><strong class="jw iu">:</strong>通用语言模型微调方法</li></ol><h1 id="8a01" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">ELMo:来自语言模型的嵌入(2018)</h1><p id="8ae3" class="pw-post-body-paragraph jt ju it jw b jx mg jz ka kb mh kd ke mi mj kh ki mk ml kl km mm mn kp kq kr im bi translated">像word2vec和GloVe这样的预训练单词嵌入是许多神经语言理解模型中的关键元素。如果我们坚持在语言建模任务中使用手套嵌入，那么无论单词“major”是否出现在任何上下文中，它都将具有相同的表示。<strong class="jw iu">语境对人类理解一个单词的意思起着重要作用。</strong></p><p id="3682" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">例如,“少校:高级军官”和“少校:重要的、严肃的或重要的”,根据GloVe vectors，对于单词“少校”,具有相同的嵌入。</p><p id="0eca" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">创造如此高质量的作品是一项艰巨的任务。具体来说，任何单词表示都应该模拟:</p><ol class=""><li id="bdf2" class="mp mq it jw b jx jy kb kc mi mr mk ms mm mt kr mu mv mw mx bi translated"><strong class="jw iu">句法和语义:</strong>用词的复杂特征</li><li id="d8e2" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr mu mv mw mx bi translated">一词多义:一个词或短语在不同的语言环境中有多种可能的含义</li></ol><p id="2b30" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">ELMo引入了一个<strong class="jw iu">深度上下文化的单词表示</strong>，它解决了我们上面定义的任务，同时仍然很容易集成到现有的模型中。这在一系列高要求的语言理解问题上取得了最先进的成果，如问答、NER、Coref和SNLI。</p><p id="440f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated"><strong class="jw iu">语境化的单词嵌入<br/> </strong>既捕捉单词含义又捕捉语境中可用信息的表征被称为语境嵌入。与使用静态单词表示的word2vec或GloVe不同，ELMo使用双向LSTM来完成特定任务，在对单词进行编码之前查看整个句子。</p><p id="17f8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">很像我们在[上一篇文章](插入链接)中观察到的，ELMo的LSTM是在一个巨大的文本数据集上训练的(与我们的下游任务使用相同的语言)。一旦这个<a class="ae mo" rel="noopener" target="_blank" href="/language-modeling-c1cf7b983685">预训练</a>完成，我们就可以重用这些提取的单词嵌入作为另一种语言或NLP任务的构建块。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nd"><img src="../Images/8fa85b2de8b6919b31d694f2de650278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDcfDSx8YdkngET38sSvXA.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">ELMo <a class="ae mo" href="https://www.linkedin.com/in/mandardeshpande1995/" rel="noopener ugc nofollow" target="_blank"> (Mandar Deshpande) </a>中使用的展开的前向语言模型</p></figure><p id="38ac" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated"><strong class="jw iu">我们如何在这个庞大的数据集上训练模型？<br/> </strong>我们简单地训练我们的模型来预测给定单词序列的下一个单词，即语言建模本身。此外，我们可以很容易地做到这一点，因为我们已经有了这个数据集，而不需要像其他监督学习任务中需要的显式标签。</p><p id="5cf5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated"><strong class="jw iu"> ELMo架构<br/> </strong>由一个正向和一个反向语言模型组成，ELMo的隐藏状态可以访问下一个单词和上一个世界。每个隐藏层都是一个双向LSTM，因此它的语言模型可以从任一方向查看隐藏状态。您可以查看上图，了解这个LSTM如何访问其他隐藏状态。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ne"><img src="../Images/6e23c41ac81d68ba9e729901a578045b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MrVUve0VaUGgOFWQBuSEvw.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">ELMo中第k个令牌特定嵌入的隐藏层连接和求和(<a class="ae mo" href="https://www.linkedin.com/in/mandardeshpande1995/" rel="noopener ugc nofollow" target="_blank"> Mandar Deshpande) </a></p></figure><p id="14ae" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">一旦前向和后向语言模型被训练，ELMo将隐藏层权重连接在一起成为单个嵌入。此外，每个这样的权重串联乘以基于被解决的任务的权重。</p><p id="b9e9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">正如您在上面看到的，ELMo将这些串联的嵌入相加，并将其分配给正在从输入文本中处理的特定标记。ELMo将令牌t_k表示为相应隐藏层的线性组合(包括其嵌入)。这意味着输入文本中的每个标记都有ELMo分配的个性化嵌入。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nf"><img src="../Images/f0cdbd3e6e7c6daec9db20f3f48964c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvkmjcoZQ8qILtVELfuRfA.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">通过串联嵌入将EMLo集成到其他NLP任务中<a class="ae mo" href="https://www.linkedin.com/in/mandardeshpande1995/" rel="noopener ugc nofollow" target="_blank"> (Mandar Deshpande) </a></p></figure><p id="df65" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">一旦ELMo的biLMs(双向语言模型)在一个巨大的文本语料库上被训练，它可以通过简单地连接到嵌入层而被集成到几乎所有的神经NLP任务中。</p><p id="dc5a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">较高层似乎学习语义，而较低层可能捕捉句法特征。此外，ELMo增强模型可以更有效地利用小数据集。</p><p id="367c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">你可以在这里阅读更多关于ELMo的信息。</p><h1 id="b95d" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">乌尔姆菲特(2018年)</h1><p id="fbdc" class="pw-post-body-paragraph jt ju it jw b jx mg jz ka kb mh kd ke mi mj kh ki mk ml kl km mm mn kp kq kr im bi translated">在ULMFiT之前，归纳迁移学习广泛用于计算机视觉，但NLP中的现有方法仍然需要针对特定任务的修改和从头开始的训练。ULMFiT提出了一种有效的迁移学习方法，可以应用于任何NLP任务，并进一步展示了微调语言模型的关键技术。</p><p id="0472" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">代替模型参数的随机初始化，我们可以获得预训练的好处并加速学习过程。</p><p id="0045" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">常规的LSTM单元被用于ULMFiT的3层架构，从<a class="ae mo" href="https://arxiv.org/pdf/1708.02182.pdf" rel="noopener ugc nofollow" target="_blank"> AWD-LSTM </a>得到启示。</p><p id="d5e0" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">ULMFiT的<strong class="jw iu">三个阶段</strong>包括:</p><ol class=""><li id="225e" class="mp mq it jw b jx jy kb kc mi mr mk ms mm mt kr mu mv mw mx bi translated"><strong class="jw iu">通用领域LM预训练:</strong>在通用领域语料库上训练语言模型，以捕获不同层面的语言的通用特征</li><li id="4d92" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr mu mv mw mx bi translated"><strong class="jw iu">目标任务区别微调:</strong>使用<em class="jv">区别微调</em>和学习速率表(倾斜三角形学习速率)在目标任务数据集上对训练的语言模型进行微调，以学习特定于任务的特征</li><li id="efe4" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr mu mv mw mx bi translated"><strong class="jw iu">目标任务分类器微调:</strong>使用<em class="jv">逐步解冻</em>并重复阶段2，对目标任务上的分类器进行微调。这有助于网络保留低级表示并适应高级表示。</li></ol><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ng"><img src="../Images/6eeb26bd637cc85db7014ee96af9d560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FsUjFMzusadyNmCfr3AS1w.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">乌尔姆菲特的三个阶段<a class="ae mo" href="https://www.linkedin.com/in/mandardeshpande1995/" rel="noopener ugc nofollow" target="_blank">(曼达尔·德什潘德)</a></p></figure><p id="3990" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">正如我们在上面看到的，阶段1在所有层上使用相同的学习速率，而阶段2和3具有逐层的三角形学习速率表。此外，请注意层权重如何在三阶段过程中逐渐达到最佳值。(较深的颜色最适合于表示目的)</p><p id="fe80" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated"><strong class="jw iu">区别性微调</strong>(具有倾斜三角形学习率的阶段2/3的学习时间表)<strong class="jw iu"> </strong>是本文的主要启示，因为它来自于模型中的不同层捕获不同类型的特征的直觉。因此，对他们中的每一个有不同的学习速度是有意义的。像计算机视觉一样，即使在语言建模任务中，初始层也捕捉关于语言的最一般的信息，因此一旦预先训练，就需要最低量的微调。</p><p id="1131" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">在该过程的阶段2之后，该模型已经非常接近指定任务所需的最佳权重，因此目标任务分类器微调据说非常敏感。如果微调过程在此阶段显著改变了权重，那么模型预训练的所有好处都将丢失。为了解决这个问题，本文提出了<strong class="jw iu">逐步解冻</strong>:</p><ul class=""><li id="675d" class="mp mq it jw b jx jy kb kc mi mr mk ms mm mt kr nh mv mw mx bi translated">首先，解冻最后一个LSTM图层，并对模型进行一个时期的微调</li><li id="51f5" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr nh mv mw mx bi translated">接下来，上一层之前的层被解冻并微调</li><li id="5690" class="mp mq it jw b jx my kb mz mi na mk nb mm nc kr nh mv mw mx bi translated">对每一层重复类似的过程，直到收敛</li></ul><p id="0415" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">你可以在这里  <strong class="jw iu">看论文<a class="ae mo" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">。</a></strong></p><p id="a505" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">希望这篇博客有助于你对这个令人兴奋的预训练语言模型领域有一个基本的了解！</p><p id="7d8f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">在下一篇博客中，我们将讨论用于学习可微调预训练模型的Transformers和BERT。</p><p id="5edd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated"><a class="ae mo" rel="noopener" target="_blank" href="/language-modeling-c1cf7b983685">链接到第一部分</a>:语言建模I <a class="ae mo" rel="noopener" target="_blank" href="/language-modelingii-ulmfit-and-elmo-d66e96ed754f"> <br/>链接到第二部分</a>:语言建模II: ELMo和ULMFiT <br/> <a class="ae mo" rel="noopener" target="_blank" href="/the-transformer-a-quick-run-through-ce9b21b4f3ed">链接到第三部分</a>:变压器:快速浏览</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="d4cc" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">继续学习和成长，直到那时！</p><p id="c05a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke mi kg kh ki mk kk kl km mm ko kp kq kr im bi translated">在推特上和我联系:<a class="ae mo" href="https://twitter.com/mandroid_6" rel="noopener ugc nofollow" target="_blank">https://twitter.com/mandroid_6</a></p><blockquote class="jq jr js"><p id="1e3c" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated"><em class="it">以上所有图片均由我创作，我有权使用它们</em></p></blockquote></div></div>    
</body>
</html>