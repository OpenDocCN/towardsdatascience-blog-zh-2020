<html>
<head>
<title>To Distil or Not To Distil: BERT, RoBERTa, and XLNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒸馏还是不蒸馏:伯特、罗伯塔和XLNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8?source=collection_archive---------11-----------------------#2020-02-07">https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8?source=collection_archive---------11-----------------------#2020-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9b7f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">变形金刚是自然语言处理中无可争议的王者。但是周围有这么多不同的模型，很难只选择一个。希望这能有所帮助！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c8e8801d0002c9e6a14c9d43f9b2117d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3tBR3x4P8CGpeBwx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@victoriano?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">维多利亚诺·伊斯基耶多</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="4449" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这已经有点老生常谈了，但是Transformer模型已经改变了自然语言处理。BERT(和他的朋友们)在几乎所有常见的NLP任务中都扫除了之前设定的基准。这种成功，以及随之而来的流行，已经见证了基于BERT架构和训练技术的一系列新模型(前面提到的朋友)的开发。事实上，我们有幸拥有如此多的新模型，以至于对于哪个模型应该用于哪个任务有些困惑。这个问题我已经被问过很多次了，但在大多数情况下，答案是明确的“视情况而定”。</p><p id="f6c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一点，我不会尝试选择最佳模型，而是将几种最常见的模型进行比较，特别是在最终精度和训练时间方面。因此，我将使用相同的超参数来训练所有模型，这意味着这不一定是每个(或任何)模型的最佳情况。然而，我希望这个比较能让你对每个型号在相同情况下的表现有一个<em class="lv">的感受</em>。</p><h1 id="f064" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">设置</h1><h2 id="4394" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">环境</h2><p id="355e" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">这个实验是使用简单的变形金刚库进行的，这个库的目的是使变形金刚模型简单易用。这个库建立在流行的<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚</a>库之上。</p><p id="b149" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你希望跟随实验，你可以在几个简单的步骤中准备好环境；</p><ol class=""><li id="bf34" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated">从<a class="ae ky" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank">这里</a>安装Anaconda或Miniconda包管理器</li><li id="be2d" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">创建新的虚拟环境并安装软件包。<br/> <code class="fe nt nu nv nw b">conda create -n transformers python</code> <br/> <code class="fe nt nu nv nw b">conda activate transformers</code> <br/>如果使用Cuda: <br/> <code class="fe nt nu nv nw b">conda install pytorch cudatoolkit=10.1 -c pytorch</code> <br/>其他:<br/> <code class="fe nt nu nv nw b">conda install pytorch cpuonly -c pytorch</code></li><li id="1f01" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">如果您使用fp16培训，请安装Apex。请按照这里<a class="ae ky" href="https://github.com/NVIDIA/apex" rel="noopener ugc nofollow" target="_blank">的指示</a>。(从pip安装Apex给一些人带来了问题。)</li><li id="7aee" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">安装简单变压器。<br/> <code class="fe nt nu nv nw b">pip install simpletransformers</code></li></ol><p id="b075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请注意，我将同时使用CUDA和FP16培训(使用Apex)。</em></p><h2 id="d190" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">数据</h2><p id="bbe5" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">AG News数据集用于训练和评估模型。准备好数据集；</p><ol class=""><li id="ae77" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated"><a class="ae ky" href="https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz" rel="noopener ugc nofollow" target="_blank">从Fast.ai下载</a>数据集。</li><li id="456a" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">提取<code class="fe nt nu nv nw b">train.csv</code>和<code class="fe nt nu nv nw b">test.csv</code>并将它们放在目录<code class="fe nt nu nv nw b">data/</code>中。</li></ol><h2 id="fbea" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">使用的硬件</h2><p id="2676" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">任何机器学习模型的训练时间将在很大程度上取决于所使用的硬件，因此您的里程数可能会有所不同！我附上了我用来对比的硬件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/16da1dbf149748776e25a1741692e37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*t-Loc2y741AXs6xOkZZYVA.png"/></div></figure><p id="f14d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面列出的两个GPU中，所有实验都只在NVIDIA Titan RTX GPU上运行。此外，这个GPU在实验期间执行的唯一任务是实验本身(包括操作系统GUI在内的一切都由RTX 2080处理)，以确保所有模型在任何时候都可以访问相同的硬件资源。</p><h1 id="1c46" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">模特们</h1><p id="66e7" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我们将试用下面给出的模型。</p><ul class=""><li id="0f0a" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu ny nl nm nn bi translated"><strong class="lb iu">Bert-base-cased</strong><br/><em class="lv">12层，768隐，12头，110M参数。<br/>接受过小写英文文本的训练。</em></li><li id="742a" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">RoBERTa-base</strong><br/><em class="lv">12层，768-隐藏，12头，125M参数<br/> RoBERTa使用BERT-base架构</em></li><li id="f52f" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">distil bert-base-uncased</strong><br/><em class="lv">6层，768-隐藏，12头，66M参数<br/>从BERT模型中提取的distilbert模型bert-base-uncased检查点</em></li><li id="b38b" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">xlnet-base-cased<br/></strong><em class="lv">12层，768隐，12头，110M参数。<br/> XLNet英文型号</em></li><li id="1610" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">distilloberta-base<br/></strong><em class="lv">6层，768隐藏，12头，82M参数<br/>distilloberta模型是从roberta模型中提炼出来的roberta-base检查点。</em></li><li id="eb3d" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">Bert-base-multilingual-cased<br/></strong><em class="lv">12层，768-hidden，12头，110M参数。<br/>使用最大的维基百科对前104种语言的大小写文本进行培训</em></li><li id="62d9" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated"><strong class="lb iu">distilbert-base-Multilingual-cased<br/></strong><em class="lv">6层，768隐藏，12头，134M参数<br/>多语言distil bert模型是从多语言BERT模型BERT-base-Multilingual-cased check point中提炼出来的。</em></li></ul><h1 id="7f4e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">超参数</h1><p id="2b9c" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">成功训练一个好模型的一个重要部分是获得正确的超参数。根据我的经验，只要超参数值合理，大多数预训练的变压器模型都会收敛并给出良好的结果。因此，以下给出的值可被视为所有型号的合理默认值，而非针对任何特定型号的优化值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="1874" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对所选值的快速解释；</p><h2 id="5850" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">最大序列长度</h2><p id="1744" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">变压器模型被限制为每个输入示例最多有<em class="lv">个标记</em>。令牌是模型词汇表中的一个“单词”(不一定是正确的英语单词)。任何输入文本都会被拆分，直到整个输入都由模型词汇表中的标记表示。如果结果表示包含的令牌数超过了允许的最大数量，它将被截断到最大长度。如果更短，它将被填充到最大长度。</p><p id="746d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于AG News数据集中除了少数例子之外，其他例子的长度都小于256个标记，所以我决定使用这个值。请注意，增加最大长度会增加资源消耗。</p><h2 id="5f39" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">训练时期的最大数量</h2><p id="83c5" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">很明显，每个模型将被训练最多5个时期，其中一个时期是所有训练数据的一次通过。</p><h2 id="8b59" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">训练批量</h2><p id="ca31" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">培训期间将并行处理的示例数量。较大的批处理大小往往会减少训练时间(直到某个特定点，直到GPU的计算核心被完全利用)，但会消耗更多的GPU内存。一般来说，使用GPU能够处理的最大批量是安全的。</p><h2 id="78de" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">学习率</h2><p id="dce2" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">控制在训练期间更新模型权重时的步长。</p><h2 id="4cd8" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">早期停止耐心</h2><p id="b57f" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">早期停止是一种用于防止机器学习模型过度适应训练数据的技术。一般的想法是，一旦模型停止提高其在验证/测试数据上的性能，就终止训练。<em class="lv">耐心</em>是在终止之前等待多少步。耐心为3，如果连续3次评估损失没有改善，我们将终止培训。</p><h2 id="ead6" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">人工种子</h2><p id="d1e7" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我们正在设置手动种子值<code class="fe nt nu nv nw b">4</code>，以确保结果可以重现。</p><h1 id="907a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">代码</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="3912" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该脚本可用于在农业新闻数据集上训练每个模型。这还会将训练进度发送到Weights &amp; Biases以便于可视化。您可以通过移除<code class="fe nt nu nv nw b">train_args</code>中的线路<code class="fe nt nu nv nw b">wandb_project</code>和<code class="fe nt nu nv nw b">wandb_kwargs</code>来禁用此功能。然后可以从文件<code class="fe nt nu nv nw b">x-y-training_progress_scores.csv</code>中获得训练进度，其中x和y分别是<code class="fe nt nu nv nw b">model_type</code>和<code class="fe nt nu nv nw b">model_name</code>。Tensorboard也支持可视化(运行保存到<code class="fe nt nu nv nw b">runs/</code>)。</p><p id="5508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该脚本需要两个参数，<code class="fe nt nu nv nw b">model_type</code>和<code class="fe nt nu nv nw b">model_name</code>。这里，<code class="fe nt nu nv nw b">model_type</code>是Transformer架构，<code class="fe nt nu nv nw b">model_name</code>是该架构的特定预训练模型。下面的bash脚本将为这个实验中使用的每个模型运行训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="bda7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">方法</h1><p id="026a" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我们将在AG News数据集上训练每个模型，并比较它们的性能。</p><p id="5c81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之前指定的超参数将用于每个模型。重要的是，我们将使用早期停止来确保模型不会过度适应训练数据，这使我们能够进一步了解每个模型收敛的速度。</p><p id="d26e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于评估的指标是<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html" rel="noopener ugc nofollow" target="_blank">马修斯相关系数</a> (MCC)。将每隔1000个训练步骤并在每个训练时期结束时计算评估数据集上的模型的MCC分数。</p><h1 id="7c89" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="98b2" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">给聪明人一个忠告:结果可能因所用的数据集和所选的超参数而异。这不是也不打算作为模型的决定性基准。</p><p id="c3da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我已经(希望)给出了足够的警告来保护自己免受热心学者的亵渎指控，让我们开始吧！</p><h2 id="07c9" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">完成时间</h2><p id="3e37" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">首先，看一下每个模型的运行时间。请注意，这同时考虑了培训时间和培训期间执行评估所花费的时间。</p><p id="d54d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，由于提前停止，更快达到峰值性能的模型将具有更短的运行时间。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="9a01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制上述值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/ff283fe0b08e87f55fa363242c482873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_qRd1c5CrnI0jKZjh5vlw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">运行时与模型</p></figure><p id="2199" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了看到整个画面，重要的是要考虑到提前停止的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/ef1933e361e64150c2ba6a6b6c18a593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2a58omQMipggp_gZTCMwPw.png"/></div></div></figure><p id="f0e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键要点:</p><ul class=""><li id="64d6" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu ny nl nm nn bi translated">对于给定的模型类型(提取模型、基本BERT/RoBERTa模型和XLNet ),执行给定数量的训练步骤所花费的时间是相同的。这一点从每条线的梯度就可以看出来。</li><li id="348a" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">推理时间(在这种情况下为评估)遵循类似的模式，对于给定的模型类型，时间是相等的。从线的平坦(水平)部分可以观察到单次通过评估数据集所花费的时间。平坦部分越长，评估时间越长。</li></ul><p id="5323" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">收敛步骤的数量也呈现出同样的趋势。</p><ul class=""><li id="8e76" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu ny nl nm nn bi translated">基本模型(bert-base-cased、bert-base-multilingual-cased、roberta-base)收敛最快(平均8 500步)。</li><li id="56ac" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">接下来是经过提炼的模型，平均10 333步。</li><li id="4344" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">XLNet收敛于11 000步，与蒸馏模型相当。</li></ul><p id="84ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制总步骤数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/3794d0e329ca85cf63963a300d707f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W62CHm9li8bVQsdXw07jOA.png"/></div></div></figure><h2 id="873a" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">最终性能</h2><p id="ab33" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">所有这些模型在AG News数据集上都表现得非常好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/e773743aea228ab058675eb3867fb6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxinR8cGjnjSPWC4IJ226A.png"/></div></div></figure><p id="f3c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">差一点就打电话了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/98dc5f627c48b87b336c3c517c06b969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZpOcI-JdFYaGVsM5C9fTA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="51b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，所有模型在测试数据上都表现出良好的性能，这证明了Transformer模型在NLP任务中的能力。</p><p id="3e66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些可视化(以及更多)可以在<a class="ae ky" href="https://app.wandb.ai/thilina/ag-news-transformers-comparison?workspace=user-thilina" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="521a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="0dc5" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">虽然不同的Transformer模型在性能和训练时间方面存在明显的差异，但是最佳的选择通常取决于特定的数据集、任务和需求。</p><p id="b364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，我们可以得出一些通用的经验法则(拇指？).</p><ul class=""><li id="f0b8" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu ny nl nm nn bi translated">与BERT和其他类似BERT的体系结构相比，XLNet通常需要更多的资源，并且需要更长的时间来正确训练。</li><li id="f2a9" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">XLNet在推理方面也比较慢。</li><li id="d06a" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">然而，当数据与预训练数据明显不同时，XLNet表现得更好。</li><li id="c399" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">经过预先训练的变形金刚的精华版本通常非常接近原始模型的性能。在这种情况下，它们超过了原始模型，但这很可能是因为所选择的超参数值。</li><li id="401e" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">经过提炼的模型训练速度更快，推理速度也更快。</li><li id="30f7" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu ny nl nm nn bi translated">然而，提取的模型可能难以收敛于某些数据集，尤其是在任务复杂的情况下。</li></ul><p id="9625" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这些观察(以及我的个人经验)，我通常会建议从精选模型开始，如果性能不令人满意，就转到基本模型和/或XLNet。</p><p id="8520" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些模型的“大”版本是另一种选择，但我发现它们通常更难正确训练，而且由于它们的尺寸，肯定需要更长的时间来训练。</p><p id="2cc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">思想？经历？见解？如果你有，请分享！</em></p></div></div>    
</body>
</html>