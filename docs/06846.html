<html>
<head>
<title>Day 148: NLP Papers Summary — A Transformer-based Approach for Source Code Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第 148 天:NLP 论文摘要—一种基于转换器的源代码摘要方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization-f07ecdeacf40?source=collection_archive---------76-----------------------#2020-05-27">https://towardsdatascience.com/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization-f07ecdeacf40?source=collection_archive---------76-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。<a class="ae jc" href="https://unsplash.com/@sloppyperfectionist?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">汉斯-彼得·高斯特</a>在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">号航天飞机</a>上拍摄的照片。</p></figure><h2 id="e53b" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线 AI </a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="e80c" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">自然语言处理论文摘要是一个系列，在这里我总结了自然语言处理研究论文的要点</h2></div><p id="fc03" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我记录 2020 年每一天 NLP 学习历程的地方。请随意在此查看我在过去 300 天<a class="ae jc" href="https://ryanong.co.uk/natural-language-processing-365/" rel="noopener ugc nofollow" target="_blank">所学的内容。在本文的最后，您可以找到以前的论文摘要(按 NLP 领域分组，您可以订阅# NLP 365 @</a><a class="ae jc" href="http://eepurl.com/gW7bBP" rel="noopener ugc nofollow" target="_blank">http://eepurl.com/gW7bBP</a>:)</p><p id="ec8e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的 NLP 论文是<strong class="lf jp"> <em class="lz">一种基于转换器的源代码摘要方法</em> </strong>。以下是该研究论文的主要收获。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="59ce" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="5351" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">利用一个简单的基于变压器的模型与相对位置表示和复制注意机制，以生成源代码总结的 SOTA 结果。我们发现源代码标记位置的绝对编码阻碍了总结的性能，而相对编码显著提高了性能。</p><h2 id="366c" class="ne mi jf bd mj nf ng dn mn nh ni dp mr lm nj nk mt lq nl nm mv lu nn no mx jl bi translated">什么是源代码摘要？</h2><p id="320c" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">目标是对源代码进行编码，并生成描述程序功能的可读摘要。</p><h1 id="6c56" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">数据集</h1><p id="4e99" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们有两个评估数据集:来自 GitHub 的 Java 和 Python 数据集，如下所示。我们的评估指标为 BLEU、METEOR 和 ROUGE-L。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e46b39bdf96d517185efddf1b5e3b8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/0*eGctjrX0VsxF17BD.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">描述性数据集统计[1]</p></figure><h1 id="f56a" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">方法学</h1><p id="ff11" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们提出的模型是普通的变压器。我们将代码和概要编码为嵌入序列。普通的 Transformer 在编码器和解码器中堆叠了多头注意力和线性变换层。我们还在 Transformer 中包含了复制注意事项，以使模型能够从源代码中复制稀有令牌。</p><h2 id="0899" class="ne mi jf bd mj nf ng dn mn nh ni dp mr lm nj nk mt lq nl nm mv lu nn no mx jl bi translated">位置表示</h2><p id="4cd9" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">在这里，我们探索了源代码标记顺序上的绝对位置编码和 Transformer 中的成对关系编码。绝对位置编码旨在捕获源标记的顺序信息，然而，我们发现顺序信息实际上对学习源代码表示没有帮助，并且会导致错误的总结。我们发现，正是令牌之间的相互作用影响了源代码的含义，这也是我们探索成对关系编码的原因。为了捕获输入令牌之间的这种成对关系，我们为每个令牌捕获两个位置 I 和 j 的相对位置表示。</p><h1 id="8ef0" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">结果</h1><p id="58e3" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">如下所示，我们的完整模型优于所有基线模型。事实上，在没有 CamelCase 和 snake_case 代码标记处理的数据集上训练的基本模型，在 ROUGE-L 度量上优于所有基线模型。我们的基线模型没有包含复制注意机制，我们表明复制注意机制确实提高了我们完整模型的性能。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oa ob di oc bf od"><div class="gh gi nz"><img src="../Images/b615ce0e5c0e25c4730866c3d66e5e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UF5jA69DM_dn-ywk.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">BLEU、METEOR 和 ROUGE-L 的总体结果-我们的方法与基线方法的比较[1]</p></figure><h1 id="46fe" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">消融研究</h1><h2 id="5e75" class="ne mi jf bd mj nf ng dn mn nh ni dp mr lm nj nk mt lq nl nm mv lu nn no mx jl bi translated">位置表示的影响</h2><p id="71ee" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">下面的表 3 展示了对源和目标执行绝对位置编码的性能。它展示了当包含绝对位置编码时性能的下降。表 4 展示了学习源代码标记之间成对关系的好处。我们尝试了不同的裁剪距离，以及是否应该包含双向信息。不同裁剪距离的性能与我们完整模型的性能非常相似，包含方向信息的模型优于不包含方向信息的模型。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2e6db0d0c3b77986e65f886d7d29974c.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*TnkIc3sp99ZarLpl.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">消融研究-变压器的相对位置表示[1]</p></figure><h2 id="daa5" class="ne mi jf bd mj nf ng dn mn nh ni dp mr lm nj nk mt lq nl nm mv lu nn no mx jl bi translated">不同的模型大小和层数</h2><p id="531b" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们下面的结果表明，更深的模型(更多层)比更宽的模型(每层更多神经元)表现更好。我们怀疑更深的模型在源代码总结中更有益，因为它更多地依赖语义信息而不是句法信息。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/990afb98158ce3ea655a305029c53dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*t6wIu4txylU8BIc6.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">消融研究 Java 数据集基础模型的隐藏大小和层数[1]</p></figure><h2 id="2186" class="ne mi jf bd mj nf ng dn mn nh ni dp mr lm nj nk mt lq nl nm mv lu nn no mx jl bi translated">定性分析</h2><p id="9465" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们下面的定性例子展示了复制注意力机制使模型能够用更合适的关键词生成更短的摘要。我们观察到，当我们使用相对位置表示时，源代码中的频繁标记具有较高的复制概率。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oa ob di oc bf od"><div class="gh gi og"><img src="../Images/7ea5a993641d64c959f16f595b41f7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g57PfUFw_Pt01RR7.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">Java 与 Python 数据集的定性示例[1]</p></figure><h1 id="22e6" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">结论和今后的工作</h1><p id="c4b5" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">未来的一项潜在工作是将代码结构合并到 Transformer 中，并将其应用到其他代码序列生成任务中，例如为源代码更改生成提交消息。</p><p id="29af" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">来源:</p><p id="f08c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[1] Ahmad，W.U .，Chakraborty，s .，Ray，b .和 Chang，K.W .，2020 年。一种基于转换器的源代码摘要方法。arXiv 预印本 arXiv:2005.00653 。</p><p id="883a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于 2020 年 5 月 27 日</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization/" rel="noopener ugc nofollow" target="_blank"><em class="lz"/></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="b025" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">最新论文</h1><h1 id="4e0b" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="bad1" class="oh oi jf lf b lg mz lj na lm oj lq ok lu ol ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="6baa" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="a852" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towards data science . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="4b2f" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">总结</h1><ul class=""><li id="3405" class="oh oi jf lf b lg mz lj na lm oj lq ok lu ol ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-144-of-nlp365-nlp-papers-summary-attend-to-medical-ontologies-content-selection-for-ff7cded5d95b">https://towards data science . com/day-144-of-NLP 365-NLP-papers-summary-attend-to-medical-ontology-content-selection-for-ff 7 cded 5d 95 b</a></li><li id="47b9" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-145-of-nlp365-nlp-papers-summary-supert-towards-new-frontiers-in-unsupervised-evaluation-188295f82ce5">https://towards data science . com/day-145-of-NLP 365-NLP-papers-summary-supert-forward-new-frontiers-in-unsupervised-evaluation-188295 f82ce 5</a></li><li id="65a9" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-146-of-nlp365-nlp-papers-summary-exploring-content-selection-in-summarization-of-novel-a13fa1f6111b">https://towards data science . com/day-146-of-NLP 365-NLP-papers-summary-exploring-content-selection-in-summary-of-novel-a 13 fa 1 f 6111 b</a></li></ul><h1 id="6e87" class="mh mi jf bd mj mk np mm mn mo nq mq mr ku nr kv mt kx ns ky mv la nt lb mx my bi translated">其他人</h1><ul class=""><li id="0ba9" class="oh oi jf lf b lg mz lj na lm oj lq ok lu ol ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-141-of-nlp365-nlp-papers-summary-textattack-a-framework-for-adversarial-attacks-in-aac2a282d72c">https://towards data science . com/day-141-of-NLP 365-NLP-papers-summary-text attack-a-framework-for-adversarial-attack-in-aac2a 282d 72 c</a></li><li id="2af3" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-142-of-nlp365-nlp-papers-summary-measuring-emotions-in-the-covid-19-real-world-worry-d565098a0937">https://towards data science . com/day-142-of-NLP 365-NLP-papers-summary-measuring-emotions-in-the-the-新冠肺炎-现实世界-忧虑-d565098a0937 </a></li><li id="76b1" class="oh oi jf lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-147-of-nlp365-nlp-papers-summary-two-birds-one-stone-a-simple-unified-model-for-text-35253aa8289e">https://towards data science . com/day-147-of-NLP 365-NLP-papers-summary-two-birds-one-stone-a-simple-unified-model-for-text-35253 aa 8289 e</a></li></ul></div></div>    
</body>
</html>