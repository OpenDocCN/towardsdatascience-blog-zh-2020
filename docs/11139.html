<html>
<head>
<title>Realtime Multiple Person 2D Pose Estimation using TensorFlow2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow2.x 的实时多人 2D 姿态估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/realtime-multiple-person-2d-pose-estimation-using-tensorflow2-x-93e4c156d45f?source=collection_archive---------4-----------------------#2020-08-03">https://towardsdatascience.com/realtime-multiple-person-2d-pose-estimation-using-tensorflow2-x-93e4c156d45f?source=collection_archive---------4-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a7d2cd1990fd498ea69c8036baedc34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJ2g_u6GJwUSaUeMYqB9kg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:左:Bailarine<a class="ae kf" href="https://www.instagram.com/eugenia_delgrossi/" rel="noopener ugc nofollow" target="_blank">Eugenia Delgrossi</a>—右:<a class="ae kf" href="https://arxiv.org/pdf/1812.08008.pdf" rel="noopener ugc nofollow" target="_blank"> OpenPose — IEEE-2019 </a></p></figure><h1 id="7fcc" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="e641" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">正如 Zhe Cao 在他的<a class="ae kf" href="https://arxiv.org/pdf/1611.08050.pdf" rel="noopener ugc nofollow" target="_blank"> 2017 论文</a>中所描述的，实时多人 2D 姿势估计对于让机器理解图像和视频中的人至关重要。</p><h2 id="4233" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">然而，什么是姿态估计呢？</h2><p id="1d62" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">顾名思义，它是一种用于估计一个人身体位置的技术，例如站、坐或躺。获得这种估计的一种方法是找到 18 个“身体的关节”，或者如人工智能领域中所命名的:“关键点”下图显示了我们的目标，即在图像中找到这些点:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/009f1b744cd894bace7ec515f2856533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URFitbiuBPBDTOYj8HPopg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://physicsworld.com/a/einstein-in-oxford/" rel="noopener ugc nofollow" target="_blank">物理世界——爱因斯坦在牛津(1933) </a></p></figure><p id="856b" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">关键点从<em class="my">点#0 </em>(顶颈)向下到身体关节，回到头部，以<em class="my">点#17 </em>(右耳)结束。</p><p id="8dfe" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">使用基于人工智能的方法出现的第一项重要工作是谷歌的 Toshev 和 Zegedy 在 2014 年发表的论文<a class="ae kf" href="https://arxiv.org/pdf/1312.4659.pdf" rel="noopener ugc nofollow" target="_blank"> DeepPose </a>。提出了一种基于深度神经网络(DNNs)的人体姿态估计方法，其中姿态估计被表述为一个基于 DNN 的针对身体关节的回归问题。</p><p id="2e89" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">该模型由一个 AlexNet 后端(7 层)和一个输出 2k 关节坐标的额外最终层组成。这种方法的显著问题是，首先，必须检测单个人(经典的对象检测),随后是模型应用。因此，在图像上发现的每个人体必须分别处理，这大大增加了处理图像的时间。这种方法被称为“自上而下”,因为首先要找到身体，然后找到与之相关的关节。</p><h2 id="a109" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">姿态估计的挑战</h2><p id="b86f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有几个与姿态估计相关的问题，如:</p><ol class=""><li id="ae18" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb ne nf ng nh bi translated">每个图像可能包含未知数量的人，这些人可能出现在任何位置或比例。</li><li id="8e16" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb ne nf ng nh bi translated">由于接触、闭塞或肢体关节，人与人之间的交互会导致复杂的空间干扰，使各部分的关联变得困难。</li><li id="f874" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb ne nf ng nh bi translated">运行时的复杂性往往会随着图像中人数的增加而增加，这使得实时性能成为一个挑战。</li></ol><p id="3448" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">为了解决这些问题，一种更令人兴奋的方法(这就是这个项目使用的方法)是<em class="my"> OpenPose </em>，这是 ZheCao 和他来自卡内基梅隆大学机器人研究所的同事在 2016 年推出的。</p><h2 id="75f4" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">开放姿势</h2><p id="7768" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">提出的 OpenPose 方法使用非参数表示，称为部分亲和场(PAF ),以“连接”每个在图像上找到的身体关节，将它们与个人相关联。换句话说，OpenPose 与 DeepPose 相反，首先找到图像上的所有关节，然后“向上”寻找最有可能包含该关节的身体，而不使用任何人探测器(“自下而上”的方法)。OpenPose 查找图像上的关键点，而不管图像上有多少人。下面的图片是从 ILSVRC 和 COCO workshop 2016 上的<a class="ae kf" href="http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf" rel="noopener ugc nofollow" target="_blank"> OpenPose 演示中获取的，让我们了解一下这个过程。</a></p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/4ba7de534faa77385ffa4fa7337408c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kunSQSSLXv8dvgdjBnNKg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf" rel="noopener ugc nofollow" target="_blank">ILSVRC 和 COCO workshop 2016 </a>上的 OpenPose 演示</p></figure><p id="a292" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">下图显示了用于训练的双分支多级 CNN 模型的架构。首先，前馈网络同时预测身体部位位置的一组 2D 置信图(S )(来自(dataset/COCO/annotations/)的关键点注释)和一组部位亲和力的 2D 矢量场(L ),其对部位之间的关联程度进行编码。在每个阶段之后，两个分支的预测以及图像特征被连接起来用于下一阶段。最后，通过贪婪推理来解析置信图和亲和域，以输出图像中所有人的 2D 关键点。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/8fd1b19dd99a0ebc9983d0c0bacd38d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNy6d8ZXBtsU7IpJrdg1iw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://arxiv.org/pdf/1611.08050.pdf" rel="noopener ugc nofollow" target="_blank"> 2017 OpenPose 论文</a></p></figure><p id="2e72" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">在项目的执行过程中，我们将回到这些概念中的一些来进行澄清。然而，强烈建议关注 OpenPose <a class="ae kf" href="http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf" rel="noopener ugc nofollow" target="_blank"> ILSVRC 和 COCO workshop 2016 </a>演示和 CVPR 2017 的<a class="ae kf" href="https://www.youtube.com/watch?v=OgQLDEAjAZ8&amp;list=PLvsYSxrlO0Cl4J_fgMhj2ElVmGR5UWKpB" rel="noopener ugc nofollow" target="_blank">视频录制</a>，以便更好地理解。</p><h2 id="786e" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">TensorFlow 2 开放姿态安装(tf 姿态估计)</h2><p id="3743" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最初的 OpenPose 是使用基于模型的 VGG 预训练网络和使用<a class="ae kf" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> Caffe 框架</a>开发的。然而，对于这次安装，我们将遵循<a class="ae kf" href="https://github.com/ildoonet" rel="noopener ugc nofollow" target="_blank">伊尔杜·金</a>张量流方法，详见他的<a class="ae kf" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank"> tf 姿态估计 GitHub </a>。</p><h2 id="f43e" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">什么是 tf 姿态估计？</h2><p id="effa" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">tf-pose-estimation 是“Openpose”，一种使用 Tensorflow 实现的人体姿态估计算法。它还提供了几个变体，对网络结构进行了一些更改，以便在 CPU 或低功耗嵌入式设备上进行实时处理。</p><p id="94e0" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">tf-pose-estimation GitHub 显示了使用不同模型的几个实验，如下所示:</p><ul class=""><li id="2d3e" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated"><strong class="lg iu"> cmu: </strong>原论文中描述的基于模型的 VGG 预训练网络，将 Caffe 格式的权重转换为 TensorFlow 中使用。</li><li id="fe2d" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb np nf ng nh bi translated"><strong class="lg iu"> dsconv </strong>:与 cmu 版本的架构相同，除了 mobilenet 的深度方向可分离卷积。</li><li id="76d6" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb np nf ng nh bi translated"><strong class="lg iu"> mobilenet </strong>:基于 mobilenet V1 论文，使用 12 个卷积层作为特征提取层。</li><li id="22a7" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb np nf ng nh bi translated"><strong class="lg iu"> mobilenet v2 </strong>:类似于 mobilenet，但是使用了它的改进版本。</li></ul><p id="dd79" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">本文中的研究是使用 mobilenet V1(“mobilenet _ thin”)完成的，它在计算预算和延迟方面的性能居中:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/27c33de1bb74ff8f552f4a3de8d46763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jnmuj59KHRLn6D7vFJwu8g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://github.com/ildoonet/tf-pose-estimation/blob/master/etcs/experiments.md" rel="noopener ugc nofollow" target="_blank">https://github . com/ildoonet/TF-pose-estimation/blob/master/ETCS/experiments . MD</a></p></figure><h1 id="044a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">第 1 部分—安装 tf 姿态估计</h1><p id="172d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们在这里跟随，优秀的<a class="ae kf" href="https://medium.com/@gsethi2409" rel="noopener"> Gunjan Seth </a>文章<a class="ae kf" href="https://medium.com/@gsethi2409/pose-estimation-with-tensorflow-2-0-a51162c095ba" rel="noopener">用 TensorFlow 2.0 进行姿态估计</a>。</p><ul class=""><li id="9485" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">转到终端并创建一个工作目录(例如，“Pose_Estimation”)，移动到该目录:</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="97c4" class="mc kh it ns b gy nw nx l ny nz">mkdir Pose_Estimation<br/>cd Pose_Estimation</span></pre><ul class=""><li id="76c2" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">创建一个虚拟环境(例如 Tf2_Py37)</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="52a9" class="mc kh it ns b gy nw nx l ny nz">conda create --name Tf2_Py37 python=3.7.6 -y <br/>conda activate Tf2_Py37</span></pre><ul class=""><li id="f01a" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">安装 TF2</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="9fa4" class="mc kh it ns b gy nw nx l ny nz">pip install --upgrade pip<br/>pip install tensorflow</span></pre><ul class=""><li id="40af" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">安装将在开发期间使用的基本包:</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="df21" class="mc kh it ns b gy nw nx l ny nz">conda install -c anaconda numpy<br/>conda install -c conda-forge matplotlib<br/>conda install -c conda-forge opencv</span></pre><ul class=""><li id="ddaf" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">克隆 tf 姿态估计库；</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="471a" class="mc kh it ns b gy nw nx l ny nz">git clone https://github.com/gsethi2409/tf-pose-estimation.git</span></pre><ul class=""><li id="5b10" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">转到 tf-pose-estimation 文件夹并安装需求</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="b0c1" class="mc kh it ns b gy nw nx l ny nz">cd tf-pose-estimation/<br/>pip install -r requirements.txt</span></pre><p id="3e1e" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">下一步，安装<a class="ae kf" href="http://www.swig.org/exec.html" rel="noopener ugc nofollow" target="_blank"> SWIG </a>，这是一个接口编译器，将 C 和 C++编写的程序与 Python 等脚本语言连接起来。它的工作原理是获取 C/C++头文件中的声明，并使用它们来生成脚本语言访问底层 C/C++代码所需的包装器代码。</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="73f6" class="mc kh it ns b gy nw nx l ny nz">conda install swig</span></pre><ul class=""><li id="2945" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">使用 Swig，构建 C++库进行后处理。</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="db32" class="mc kh it ns b gy nw nx l ny nz">cd tf_pose/pafprocess<br/>swig -python -c++ pafprocess.i &amp;&amp; python3 setup.py build_ext --inplace</span></pre><p id="4da4" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">现在，安装<a class="ae kf" href="https://github.com/adrianc-a/tf-slim" rel="noopener ugc nofollow" target="_blank"> <em class="my"> tf-slim </em> </a> <em class="my"> </em>库，这是一个轻量级的库，用于定义、训练和评估 TensorFlow 中的复杂模型。</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="fe4c" class="mc kh it ns b gy nw nx l ny nz">pip install git+https://github.com/adrianc-a/tf-slim.git@remove_contrib</span></pre><p id="8683" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">就是这样！现在，有必要进行一个快速测试。以便返回到主 tf 姿态估计目录。</p><blockquote class="oa ob oc"><p id="bb27" class="le lf my lg b lh mt lj lk ll mu ln lo od mv lr ls oe mw lv lw of mx lz ma mb im bi translated">如果你按照顺序，你必须在 tf_pose/pafprocess 里面。否则，使用相应的命令来更改目录。</p></blockquote><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="dc21" class="mc kh it ns b gy nw nx l ny nz">cd ../..</span></pre><p id="94fa" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">在 tf-pose-estimation 目录中有一个 python 脚本<em class="my"> run.py </em>，让我们运行它，参数为:</p><ul class=""><li id="c1af" class="mz na it lg b lh mt ll mu lp nb lt nc lx nd mb np nf ng nh bi translated">model=mobilenet_thin</li><li id="7591" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb np nf ng nh bi translated">resize=432x368(预处理时图像的大小)</li><li id="e179" class="mz na it lg b lh ni ll nj lp nk lt nl lx nm mb np nf ng nh bi translated">图像=。/images/ski.jpg(图像目录中的示例图像)</li></ul><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="8b4b" class="mc kh it ns b gy nw nx l ny nz">python run.py --model=mobilenet_thin --resize=432x368 --image=./images/ski.jpg</span></pre><p id="f331" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">请注意，在几秒钟内，什么也不会发生，但大约一分钟后，终端应该会显示类似下图的内容:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/5cac5e32aca6d09bd8ed561557096d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2sYmLdMbXps574heYdNWEg.png"/></div></div></figure><p id="c128" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">然而，更重要的是，图像将出现在一个独立的 OpenCV 窗口中:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/aec98b83d6bfd865a3feb6aad9adcbc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*utGHpAdjhExfODYcYC2saA.png"/></div></figure><p id="9c2d" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">太好了！这些图像证明了一切都已正确安装并运行良好！我们将在下一节详细介绍。然而，为了快速解释这四个图像的含义，左上角(“结果”)是以原始图像(在这种情况下，是 ski.jpg)作为背景绘制的姿态检测骨架。右上角的图像是“热图”，其中显示了“检测到的零件”(Ss)，底部的两个图像显示了零件关联(Ls)。“结果”是将个人的 S 和 L 联系起来。</p><p id="67c6" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">接下来的测试是现场视频:</p><blockquote class="oa ob oc"><p id="40c1" class="le lf my lg b lh mt lj lk ll mu ln lo od mv lr ls oe mw lv lw of mx lz ma mb im bi translated">如果计算机只安装了一个摄像头，请使用:camera=0</p></blockquote><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="c807" class="mc kh it ns b gy nw nx l ny nz">python run_webcam.py --model=mobilenet_thin --resize=432x368 --camera=1</span></pre><p id="2a2f" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">如果一切顺利，将会出现一个窗口，显示真实的实时视频，如下图所示:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/4ee9108c1497bb1427bfa95a80c6e6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JlF3bKo-NvWw_ZzQfvITXw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:PrintScreen 作者的网络摄像头</p></figure><h1 id="70e8" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">第 2 部分——深入研究图像中的姿态估计</h1><p id="ccd9" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在本节中，我们将更深入地研究 TensorFlow 姿态估计的实现。建议跟随文章，尝试重现 Jupyter 笔记本:<a class="ae kf" href="https://github.com/Mjrovai/TF2_Pose_Estimation/blob/master/10_Pose_Estimation_Images.ipynb" rel="noopener ugc nofollow" target="_blank">10 _ Pose _ Estimation _ Images</a>，可从 Project GitHub 下载。</p><blockquote class="oa ob oc"><p id="4881" class="le lf my lg b lh mt lj lk ll mu ln lo od mv lr ls oe mw lv lw of mx lz ma mb im bi translated">作为参考，这个项目是 100%在 MacPro (2.9Hhz 四核 i7 16GB 2133Mhz RAM)上开发的。</p></blockquote><h2 id="968a" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">导入库</h2><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="cfa9" class="mc kh it ns b gy nw nx l ny nz">import sys<br/>import time<br/>import logging<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import cv2</span><span id="33c1" class="mc kh it ns b gy oi nx l ny nz">from tf_pose import common<br/>from tf_pose.estimator import TfPoseEstimator<br/>from tf_pose.networks import get_graph_path, model_wh</span></pre><h2 id="bb9e" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">模型定义和目标估计器创建</h2><p id="9d2a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">可以使用位于模型/图形子目录中的模型，如 mobilenet_v2_large 或 cmu (VGG 预训练模型)。</p><blockquote class="oa ob oc"><p id="647c" class="le lf my lg b lh mt lj lk ll mu ln lo od mv lr ls oe mw lv lw of mx lz ma mb im bi translated">对于 cmu，*。pb 文件在安装过程中没有下载，因为它们非常大。要使用它，运行位于/cmu 子目录中的 bash 脚本<em class="it"> download.sh </em>。</p></blockquote><p id="8692" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">这个项目使用 mobilenet_thin (MobilenetV1)，考虑到使用的所有图像都要整形为 432x368。</p><p id="86da" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">参数:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="501e" class="mc kh it ns b gy nw nx l ny nz">model='mobilenet_thin'<br/>resize='432x368'<br/>w, h = model_wh(resize)</span></pre><p id="0a90" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">创建评估者:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="fa28" class="mc kh it ns b gy nw nx l ny nz">e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))</span></pre><p id="a987" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">为了便于分析，让我们加载一个简单的人类图像。OpenCV 用于读取图像。图像以 RGB 格式存储，但在内部，OpenCV 支持 BGR。使用 OpenCV 显示图像没有任何问题，因为在图像显示在特定的窗口之前，图像将从 BGR 转换为 RGB(如前一节中的 ski.jpg 所示)。</p><p id="3e0b" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">一旦在 Jupyter 单元格上绘制图像，将使用 Matplotlib 代替 OpenCV。因此，图像应该在显示前进行转换，如下所示:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="89c7" class="mc kh it ns b gy nw nx l ny nz">image_path = ‘./images/human.png’<br/>image = cv2.imread(image_path)<br/>image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>plt.imshow(image)<br/>plt.grid();</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1d8a6760c968807de1747a766a4e5da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*B-8V7-21DiOn_4khAvO8iw.png"/></div></figure><p id="5a03" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">请注意，此图像的形状为 567x567。OpenCV 在读取图像时，自动将其转换为数组，其中每个值从 0 到 255，其中 0=白色，255=黑色。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f0f53cb86b388d31f94091f8ca17ca7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*ImIIZDoPLCDTS9UWmddjMg.png"/></div></figure><p id="533f" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">一旦图像是一个数组，就很容易使用 shape 来验证它的大小:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="f361" class="mc kh it ns b gy nw nx l ny nz">image.shape</span></pre><p id="f119" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">结果将是(567，567，3)，其中形状是(宽度，高度，颜色通道)。</p><p id="588f" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">尽管可以使用 OpenCV 读取图像；我们将使用 tf_pose.common 库中的函数 read_imgfile(image_path)来防止颜色通道出现任何问题。</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="2272" class="mc kh it ns b gy nw nx l ny nz">image = common.read_imgfile(image_path, None, None)</span></pre><p id="9187" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">一旦我们将图像作为一个数组，我们就可以将方法推理应用于估计器(e)，将图像数组作为输入(图像将使用原则上定义的参数 w 和 h 来调整大小)。</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="cceb" class="mc kh it ns b gy nw nx l ny nz">humans = e.inference(image, resize_to_default=(w &gt; 0 and h &gt; 0), upsample_size=4.0)</span></pre><p id="22fc" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">运行上述命令后，让我们检查数组 e.heatmap。该数组的形状为(184，216，19)，其中 184 是 h/2，216 是 w/2，19 与特定像素属于 18 个关节之一的概率相关(0 到 17)+1(18:无)。例如，检查左上角的像素，应该会得到“无”的结果:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/f8366538e9ffe8960c964a62e01a70ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jnB8b1VZXfC2k96wgZnBw.png"/></div></div></figure><p id="1997" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">可以验证该数组的最后一个值</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/11b56fa71e388da4b4f854a6cb8a6f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*2YS0V9VkiILl-Ue16TeqOg.png"/></div></figure><p id="86a9" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">这是所有价值中最高的；可以理解的是，有 99.6%的几率，这个像素不属于 18 个关节中的任何一个。</p><p id="45e1" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">让我们试着找到颈部的底部(两肩之间的中点)。它位于原始图片上大约半宽(0.5 *宽= 108)和大约 20%高的位置，从上/下开始(0.2 *高= 37)。因此，让我们检查这个特定的像素:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/31ce60d9187c76d84c4a82ae45db523e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4F-OUs4fpLpnE9tKqUx7vQ.png"/></div></div></figure><p id="99d8" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">很容易实现位置<strong class="lg iu"> 1 </strong>的最大值为 0.7059……(或者通过计算<em class="my"> e.heatMat[37][108】)。max() </em>)，这意味着该特定像素有 70%的概率是“基颈”。下图显示了所有 18 个 COCO 关键点(或“身体关节”)，显示“1”对应于“基础颈部”。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d6338b28da55de88793f82e2e0d79cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*E0JhHO5sqAG8ZvwRAN-Axg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">人体姿态骨骼的 COCO 关键点格式。</p></figure><p id="b451" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">可以为每个像素绘制代表其最大值的颜色。这样，显示关键点的热图就会神奇地出现:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="ebd8" class="mc kh it ns b gy nw nx l ny nz">max_prob = np.amax(e.heatMat[:, :, :-1], axis=2)<br/>plt.imshow(max_prob)<br/>plt.grid();</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ad3919b6bcdd2589e159094d769ead8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*VwbDr2E8Bd55s_zxYEtpvg.png"/></div></figure><p id="27fe" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">现在，让我们在重塑的原始图像上绘制关键点:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="4381" class="mc kh it ns b gy nw nx l ny nz">plt.figure(figsize=(15,8))<br/>bgimg = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2RGB)<br/>bgimg = cv2.resize(bgimg, (e.heatMat.shape[1], e.heatMat.shape[0]), interpolation=cv2.INTER_AREA)<br/>plt.imshow(bgimg, alpha=0.5)<br/>plt.imshow(max_prob, alpha=0.5)<br/>plt.colorbar()<br/>plt.grid();</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/f57e042b1130e9f4a7d8afc559dcd120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KHXbHGPTGKr4_HXevIkRJg.png"/></div></div></figure><p id="2873" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">因此，可以在图像上看到关键点，颜色条上显示的值意味着黄色越多，可能性越大。</p><p id="9a9e" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">要获得关键点(或“关节”)之间最可能的连接(或“骨骼”)，我们可以使用 e.pafMat 的结果数组。该数组的形状为(184，216，38)，其中 38 (2 x 19)与该像素成为 18 个特定关节+ nones 之一的水平(x)或垂直(y)连接的一部分的概率相关。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/316f3b23eb0c0dc03f38e5361bf72ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WZ1suJGFZ3V3gNvHE25G1A.png"/></div></div></figure><p id="317f" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">绘制上述图形的函数在笔记本中。</p><h2 id="fb15" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">使用 draw_human 方法绘制骨骼</h2><p id="4b25" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">使用列表<em class="my"> human </em>，e.inference()方法的结果，可以使用 draw_human 方法绘制骨架:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="0a80" class="mc kh it ns b gy nw nx l ny nz">image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)</span></pre><p id="e43f" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">结果如下图所示:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi os"><img src="../Images/24ac27a976601e136d2d4c6a885b8a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*tjhYZuodMnJxXX8AXl3RaA.png"/></div></figure><p id="e0cf" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">如果需要的话，可以只绘制骨架，如下所示(让我们重新运行所有代码来回顾一下):</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="3c6f" class="mc kh it ns b gy nw nx l ny nz">image = common.read_imgfile(image_path, None, None)<br/>humans = e.inference(image, resize_to_default=(w &gt; 0 and h &gt; 0), upsample_size=4.0)<br/>black_background = np.zeros(image.shape)<br/>skeleton = TfPoseEstimator.draw_humans(black_background, humans, imgcopy=False)<br/>plt.figure(figsize=(15,8))<br/>plt.imshow(skeleton);<br/>plt.grid(); <br/>plt.axis(‘off’);</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ebdb101307e015e0a2dad4ac9ffd67e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*9ZHgyZZFpYSvk4DcmjEtxQ.png"/></div></figure><h2 id="a902" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">获取关键点(关节)坐标</h2><p id="b485" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">姿态估计可用于机器人、游戏或医学等一系列应用。为此，从图像中获取物理关键点坐标以供其他应用程序使用可能会很有趣。</p><p id="02d0" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">查看 e.inference()产生的<em class="my">人类</em>列表，可以验证这是一个只有一个元素的列表，一个字符串。在这个字符串中，每个关键点都以其相对坐标和关联概率出现。例如，对于目前使用的人类图像，我们有:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/30dbf90ec307fdb4a801e848c3962d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALIVpj9xnxXJClPMR-YISQ.png"/></div></div></figure><p id="f900" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">例如:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="f85d" class="mc kh it ns b gy nw nx l ny nz">BodyPart:0-(0.49, 0.09) score=0.79<br/>BodyPart:1-(0.49, 0.20) score=0.75<br/>...<br/>BodyPart:17-(0.53, 0.09) score=0.73</span></pre><p id="70f8" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">我们可以从这个列表中提取一个数组(大小为 18 ),其中包含与原始图像形状相关的实际坐标:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="8d3d" class="mc kh it ns b gy nw nx l ny nz">keypoints = str(str(str(humans[0]).split('BodyPart:')[1:]).split('-')).split(' score=')</span><span id="18c7" class="mc kh it ns b gy oi nx l ny nz">keypts_array = np.array(keypoints_list)<br/>keypts_array = keypts_array*(image.shape[1],image.shape[0])<br/>keypts_array = keypts_array.astype(int)</span></pre><p id="3d5a" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">让我们在原始图像上绘制这个数组(因为数组的索引是关键点)。结果如下:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="1563" class="mc kh it ns b gy nw nx l ny nz">plt.figure(figsize=(10,10))<br/>plt.axis([0, image.shape[1], 0, image.shape[0]])  <br/>plt.scatter(*zip(*keypts_array), s=200, color='orange', alpha=0.6)<br/>img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>plt.imshow(img)<br/>ax=plt.gca() <br/>ax.set_ylim(ax.get_ylim()[::-1]) <br/>ax.xaxis.tick_top() <br/>plt.grid();</span><span id="a66f" class="mc kh it ns b gy oi nx l ny nz">for i, txt in enumerate(keypts_array):<br/>    ax.annotate(i, (keypts_array[i][0]-5, keypts_array[i][1]+5)</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/007abbd486762417b46bb84844dfa767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*WDm3qab8j4Rw_lw9meUXXA.png"/></div></figure><h2 id="7a4a" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">创建功能以快速再现对普通图像的研究:</h2><p id="9b2d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">笔记本显示了迄今为止开发的所有代码，“封装”成函数。例如，让我们看另一个图像:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="68fd" class="mc kh it ns b gy nw nx l ny nz">image_path = '../images/einstein_oxford.jpg'<br/>img, hum = get_human_pose(image_path)<br/>keypoints = show_keypoints(img, hum, color='orange')</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d24de45fcbaf15264bc2104ae4a9dff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*E-CGeuVgpjhY1ATv5NCmNQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://physicsworld.com/a/einstein-in-oxford/" rel="noopener ugc nofollow" target="_blank">物理世界——爱因斯坦在牛津(1933) </a></p></figure><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="977e" class="mc kh it ns b gy nw nx l ny nz">img, hum = get_human_pose(image_path, showBG=False)<br/>keypoints = show_keypoints(img, hum, color='white', showBG=False)</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5136905c62e57f46d6b33741ea9eeadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*SZnCTgDrAFSOQnAqxUlkkQ.png"/></div></figure><h2 id="f5f2" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">与多人一起研究图像</h2><p id="77c2" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">到目前为止，只探索了包含一个人的图像。一旦算法被开发来同时从图像中捕获所有关节(S)和 PAFs ),寻找最可能的连接只是为了简单。所以，得到结果的代码是一样的；例如，只有当我们得到结果(“人类”)时，列表才会具有与图像中的人数相兼容的大小。</p><p id="ceda" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">例如，让我们使用一个上面有五个人的“忙碌图像”:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="506d" class="mc kh it ns b gy nw nx l ny nz">image_path = './images/ski.jpg'<br/>img, hum = get_human_pose(image_path)<br/>plot_img(img, axis=False)</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/0d6db6690e03ffa5e3f82d8b5d3afb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*9w1hXbzS7iLWhBKwgJv6fA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://arxiv.org/pdf/1812.08008.pdf" rel="noopener ugc nofollow" target="_blank"> OpenPose — IEEE-2019 </a></p></figure><p id="1310" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">该算法找到了将他们与这五个人相关联的所有 Ss 和 Ls。成绩优秀！</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/a252eb987f4d83aac85b6ef90a977e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KtA7NktEmt2VGkV_AmXkUw.png"/></div></div></figure><blockquote class="oa ob oc"><p id="b4e0" class="le lf my lg b lh mt lj lk ll mu ln lo od mv lr ls oe mw lv lw of mx lz ma mb im bi translated">从读取图像路径到绘制结果，整个过程不到 0.5s，与图像中找到的人数无关。</p></blockquote><p id="bdb4" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">让我们把它复杂化，看一个图像，当一对夫妇跳舞时，人们更加“混杂”:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="3522" class="mc kh it ns b gy nw nx l ny nz">image_path = '../images/figure-836178_1920.jpg<br/>img, hum = get_human_pose(image_path)<br/>plot_img(img, axis=False)</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/bca4a4fc2e4fda8f5565751da9818caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*3OFUsN5OpfqKcvSISLSlAg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://pixabay.com/es/photos/figura-patinaje-campeonatos-836178/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="407d" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">结果似乎也很好。让我们只画关键点，每个人用不同的颜色:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="10ec" class="mc kh it ns b gy nw nx l ny nz">plt.figure(figsize=(10,10))<br/>plt.axis([0, img.shape[1], 0, img.shape[0]])  <br/>plt.scatter(*zip(*keypoints_1), s=200, color='green', alpha=0.6)<br/>plt.scatter(*zip(*keypoints_2), s=200, color='yellow', alpha=0.6)<br/>ax=plt.gca() <br/>ax.set_ylim(ax.get_ylim()[::-1]) <br/>ax.xaxis.tick_top() <br/>plt.title('Keypoints of all humans detected\n')<br/>plt.grid();</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/e3f14b1f59019fb7cc52866356bd5c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gvin5ZeKqAJmetCywagzrA.png"/></div></div></figure><h1 id="eb44" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">第 3 部分:视频和现场摄像机中的姿态估计</h1><p id="510b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在视频中获取姿态估计的过程与我们处理图像的过程相同，因为视频可以被视为一系列图像(帧)。建议跟着小节走，尽量重现 Jupyter 笔记本:<a class="ae kf" href="https://github.com/Mjrovai/TF2_Pose_Estimation/blob/master/20_Pose_Estimation_Video.ipynb" rel="noopener ugc nofollow" target="_blank"> 20_Pose_Estimation_Video </a>可从 Project GitHub 下载。</p><p id="3e23" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">OpenCV 在处理视频方面做得非常出色。</p><p id="b23d" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">因此，让我们获取一个. mp4 视频，并通知 OpenCV 我们将捕获它的帧:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="5b01" class="mc kh it ns b gy nw nx l ny nz">video_path = '../videos/dance.mp4<br/>cap = cv2.VideoCapture(video_path)</span></pre><p id="cf73" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">现在让我们创建一个捕捉每一帧的循环。有了框架，我们将应用 e.inference()，从结果中，我们将绘制骨架，就像我们对图像所做的一样。结尾包含了一段代码，用于在按下某个键(例如“q”)时停止播放视频。</p><p id="fbad" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">下面是必要的代码:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="c24d" class="mc kh it ns b gy nw nx l ny nz">fps_time = 0</span><span id="ba94" class="mc kh it ns b gy oi nx l ny nz">while True:<br/>    ret_val, image = cap.read()</span><span id="4a9f" class="mc kh it ns b gy oi nx l ny nz">    humans = e.inference(image,<br/>                         resize_to_default=(w &gt; 0 and h &gt; 0),<br/>                         upsample_size=4.0)<br/>    if not showBG:<br/>        image = np.zeros(image.shape)<br/>        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)</span><span id="3b98" class="mc kh it ns b gy oi nx l ny nz">    cv2.putText(image, "FPS: %f" % (1.0 / (time.time() - fps_time)), (10, 10),<br/>                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)<br/>    cv2.imshow('tf-pose-estimation result', image)<br/>    fps_time = time.time()<br/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>        break</span><span id="ceff" class="mc kh it ns b gy oi nx l ny nz">cap.release()<br/>cv2.destroyAllWindows()</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d3e2856a2bad90deffd2c2fd8fbb63c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*2917bsGnIc3tS_DeeUdhSA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://github.com/ildoonet/tf-pose-estimation/tree/master/etcs" rel="noopener ugc nofollow" target="_blank"> tf-pose-estimation GitHub </a>上的视频截图</p></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d6329867240198ac6a17f9d23228ba7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*lKNjXchPLnqn4u82VP8ilQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:截图自<a class="ae kf" href="https://github.com/ildoonet/tf-pose-estimation/tree/master/etcs" rel="noopener ugc nofollow" target="_blank"> tf-pose-estimation GitHub </a>上的视频样本</p></figure><p id="0044" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">结果很棒，但是有点慢。最初大约 30 FPS(每秒帧数)的电影将在“慢速相机”中运行，大约 3 FPS。</p><p id="aa58" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">这是另一个经历，电影放了两次，记录了有背景视频和没有背景视频的姿态估计骨架。视频是手动同步的，但如果结果不完美，那就太棒了。我剪掉了 1928 年卓别林电影《马戏团》的最后一个场景，流浪汉走路的方式很经典。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h2 id="b0f4" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">用现场摄像机测试</h2><p id="aa89" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">建议按照小节，尝试重现 Jupyter 笔记本:<a class="ae kf" href="https://github.com/Mjrovai/TF2_Pose_Estimation/blob/master/30_Pose_Estimation_Camera.ipynb" rel="noopener ugc nofollow" target="_blank">30 _ Pose _ Estimation _ Camera</a>可从 Project GitHub 下载。</p><p id="d836" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">运行现场摄像机所需的代码与视频使用的代码几乎相同，只是 OpenCV videoCapture()方法将接收一个整数作为输入参数，该整数表示使用的是什么真实摄像机。例如，内置摄像头使用“0”，外置摄像头使用“1”。此外，相机应设置为捕捉模型使用的“432x368”帧。</p><p id="b774" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">参数初始化:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="c594" class="mc kh it ns b gy nw nx l ny nz">camera = 1<br/>resize = '432x368'     # resize images before they are processed<br/>resize_out_ratio = 4.0 # resize heatmaps before they are post-processed<br/>model = 'mobilenet_thin'<br/>show_process = False<br/>tensorrt = False       # for tensorrt process</span><span id="424a" class="mc kh it ns b gy oi nx l ny nz">cam = cv2.VideoCapture(camera)<br/>cam.set(3, w)<br/>cam.set(4, h)</span></pre><p id="e1c6" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">代码的循环部分应该与视频中使用的非常相似:</p><pre class="mp mq mr ms gt nr ns nt nu aw nv bi"><span id="2f87" class="mc kh it ns b gy nw nx l ny nz">while True:<br/>    ret_val, image = cam.read()</span><span id="f6bc" class="mc kh it ns b gy oi nx l ny nz">    humans = e.inference(image,<br/>                         resize_to_default=(w &gt; 0 and h &gt; 0),<br/>                         upsample_size=resize_out_ratio)<br/>    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)<br/>    cv2.putText(image, "FPS: %f" % (1.0 / (time.time() - fps_time)), (10, 10),<br/>                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)<br/>    cv2.imshow('tf-pose-estimation result', image)<br/>    fps_time = time.time()<br/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>        break</span><span id="3de1" class="mc kh it ns b gy oi nx l ny nz">cam.release()<br/>cv2.destroyAllWindows()</span></pre><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9a56e233b3da37e77c1ecf60d361c94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Z3V6ZwAPSD9IH9KVf_sNWQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:PrintScreen 作者的网络摄像头</p></figure><p id="197c" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">同样，当使用该算法时，30 FPS 的标准视频捕获减少到大约 10%。这里有一个完整的视频，可以更好地观察到延迟。然而结果却是极好的！</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="pf pe l"/></div></figure><h1 id="96f7" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="14d5" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一如既往，我希望这篇文章可以激励其他人在 AI 的奇幻世界中找到自己的路！</p><p id="95c6" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">本文中使用的所有代码都可以从 GitHub 项目下载:<a class="ae kf" href="https://github.com/Mjrovai/TF2_Pose_Estimation" rel="noopener ugc nofollow" target="_blank">TF2 _ 姿势 _ 估计</a></p><p id="f582" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">来自世界南方的问候！</p><p id="ded1" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">我的下一篇文章再见！</p><p id="f350" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">谢谢你</p><p id="52a5" class="pw-post-body-paragraph le lf it lg b lh mt lj lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb im bi translated">马塞洛</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/782fc8f95a50033b04127fe237327a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Qv4I6FamrEJeldAvMpEMw.png"/></div></div></figure></div></div>    
</body>
</html>