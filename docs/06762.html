<html>
<head>
<title>Detecting Constructiveness in Online Article Comments</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检测在线文章评论中的建设性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-constructiveness-in-online-article-comments-477bf4b4bd8?source=collection_archive---------79-----------------------#2020-05-26">https://towardsdatascience.com/detecting-constructiveness-in-online-article-comments-477bf4b4bd8?source=collection_archive---------79-----------------------#2020-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7f05" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用带ktrain的Distilbert的端到端分类教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b1d8c538e78f78e8fb689a5c948dcab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DUgT7xYleAPBokXm"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马修·施瓦茨在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2794" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated">促进在线评论区的建设性是让互联网成为一个更有生产力的地方的重要一步。除了通过简单地指出错误或试图伤害来给出反馈，建设性可以通过论证和尊重的话语技巧来使用，以便利用这些过去的错误来进行未来的改进。</p><p id="5306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似于情感分析或毒性检测，可以使用深度学习技术来建模建设性和分类文章评论。当前最先进的模型使用transformer架构，在并行处理输入序列时，它比通常的递归单元(LSTM、GRU……)更有效。在这篇文章中，我们将使用由<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>创建的<strong class="ky ir"> Distilbert </strong> (Sanh等人，2019)。该模型是BERT (Devlin等人，2018年)的精华版本，本质上意味着一个轻得多的模型，几乎达到类似的性能。我们将使用<strong class="ky ir"> ktrain </strong> Python库(Maiya，2020)，它允许在Tensorflow Keras中非常容易地实现最先进的模型。关于ktrain实现的更多信息，<a class="ae kv" rel="noopener" target="_blank" href="/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c">本教程</a>和<a class="ae kv" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank">官方文档</a>可以帮到你！</p><p id="eac6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于建设性来说，标记数据的数量相当有限，所以我们将使用最大和最新的一个，建设性评论语料库(C3)，可在<a class="ae kv" href="https://www.kaggle.com/mtaboada/c3-constructive-comments-corpus/kernels?sortBy=hotness&amp;group=everyone&amp;pageSize=20&amp;datasetId=581971" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上获得，并在Kolhatkar等人(2020)中详细描述。该数据集由12000条新闻评论组成，包含几个建设性和毒性标签，但我们将使用的唯一标签是<em class="mb"> constructive_binary </em>。</p><h1 id="4ac4" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">数据处理</h1><p id="b313" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">像往常一样，第一步是导入项目所需的所有库。您将需要安装<em class="mb"> ktrain </em>，根据您的设置，这可能会很棘手，然后导入它。我们还从<em class="mb"> scikit-learn </em>导入了基本的三重奏<em class="mb">熊猫</em>、<em class="mb"> numpy </em>和<em class="mb"> matplotlib </em>，以及一些度量和拆分工具。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e50d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从读取熊猫数据帧中的数据集文件开始。我们还应该看看我们感兴趣的列，<em class="mb"> comment_text </em>和<em class="mb"> constructive_binary </em>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">C3数据帧头</p></figure><p id="d721" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们很快就需要知道我们想要允许的最大输入长度，所以了解一下注释长度是很有用的。运行以下代码以显示DataFrame comment列的描述。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="d98b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果显示长度平均值为71个标记，第90个和第99个百分点分别对应于157和362个标记，因此将MAXLEN设置在150到200个标记之间似乎是个好主意。为了节省一些内存空间，就说150吧。</p><p id="8ef9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们应该定义几个全局变量，包括<strong class="ky ir">最大输入长度</strong>、数据集的<strong class="ky ir">路径和保存训练模型</strong>的<strong class="ky ir">路径、<strong class="ky ir">目标标签名称</strong>和我们想要使用的<strong class="ky ir"> HuggingFace模型</strong>，在我们的例子中是<em class="mb"> bert-base-uncased </em>，它只使用小写输入。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="bf46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看一下输出类分布，以检查数据集是否平衡:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5b4a7fb607970bf32b96af9f245b8f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*MQ7QijdOMGKRhk8NHx9pog.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标类别分布</p></figure><p id="e70e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建设性的评论比非建设性的评论多，所以数据集略有不平衡。在验证和测试步骤中，我们希望保持输出类的良好代表性，这意味着我们希望目标类分布在所有train/val/test集中是相同的。为此，我们将使用scikit-learn实现的分层分裂。让我们从划分训练集(我们称之为中间集)和测试集开始，将完整集的20%放在一边。为了更加方便，数据帧的索引可以在以后重新设置。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="10ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将另外10%到20%的训练数据设置为验证目的是一种好的做法，此外，在ktrain中使用验证集是可能的，因此我们再次使用分层分裂，并将10%设置为备用。再次重置每个数据帧的索引。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="f574" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完美！我们现在已经准备好使用三个数据集，所以让我们将输入和输出分开，以馈入我们的机器学习模型。您可以将输入列(X)读入一个numpy数组，并将输出(y)转换为一个小int。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="eedb" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">分类</h1><p id="f971" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">现在是时候通过加载一个Transformer对象来初始化ktrain模块了，该对象采用前面定义的<strong class="ky ir"> MODEL_NAME </strong>、<strong class="ky ir"> MAXLEN </strong>和<strong class="ky ir">标签</strong>。最先进的体系结构不一定需要特殊的预处理，如停用词删除、标点符号删除等。相反，他们使用一种特殊的无监督标记化方法，优化了称为WordPiece的词汇外(OOV)的数量。由于这一点，并且因为我们的数据集不是很嘈杂，我们简单地使用ktrain预处理器。最后，您可以获得一个分类器对象，然后获得一个采用<strong class="ky ir"> batch_size </strong>超参数的学习器对象。这个超参数在逻辑上可以根据您的需要进行调整(就像MAXLEN一样)，但是在增加它时要注意内存问题。</p><p id="8d35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下代码为您完成了所有这些工作:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="5678" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就在训练模型之前，您可以选择使用学习率查找器(Smith，2018)轻松优化学习率调整过程，它基本上在短期内训练模型，同时以指数方式提高学习率:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/be9668b622eef42b39ad72e31a92b96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Rj9C5qZl5r-hCLQ0TORyw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">学习率查找图表</p></figure><p id="47f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个可供选择的学习率值位于第一个显著下降的斜坡上，比第一个平坦区域稍靠前一点，红色箭头指向的位置。在这种情况下，合适的值是0.00001。让我们现在<em class="mb">最后</em>训练模型。我们使用循环训练政策(Smith，2017)来连续增加和减少学习率，但如果您想改变，也可以使用其他政策！<strong class="ky ir"> 4个时期</strong>对于模型收敛来说肯定是足够的，但是这也可以根据您的方便进行调整(训练这样大的模型需要大量的时间和资源！).</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/86a710b05d6481eb9ab4cc7c1fede15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbOnMFXd5jnykMaaWFTcrw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">详细培训</p></figure><p id="27f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦训练结束，您可以使用学习者对象的<em class="mb"> validate </em>方法进行验证，但是为了简短起见，我们将跳过这一部分。</p><p id="3a35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ktrain的<em class="mb">预测器</em>对象允许对新数据进行预测。运行下面的代码对整个测试集进行预处理和分类。它还打印了几个常用的指标来帮助您解释结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/2976c3d86d1c4b074035eed48d65d461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULtwxA7oA4yYcJRXeWPiDg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试集上的构造性分类结果</p></figure><p id="578b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们达到了<strong class="ky ir"> 0.94 </strong>加权F1和<strong class="ky ir"> 0.94 </strong>的精度，非常好，干得好！您的Distilbert模型现在适合检测建设性的新闻文章评论。您可以用下面的代码行保存模型，但是要注意，它相当重(~300MB)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3977" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在由您来调整超参数，使用更高级的机器学习方法或更重的模型(BERT、XLNet等)。)去尝试，去取得更好的成绩！</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="37cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">感谢阅读！</em> <em class="mb">我希望这对你来说是有益的，对我来说也是有趣的</em></p><h1 id="f732" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">参考</h1><p id="59b3" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">德夫林、雅各布、张明蔚、肯顿·李和克里斯蒂娜·图塔诺瓦(2018)。“BERT:用于语言理解的深度双向转换器的预训练”。<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> arXiv预印本arXiv:1810.04805 </em> </a>。</p><p id="d015" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kolhatkar、Varada、Nithum Thain、Jeffrey Sorensen、Lucas Dixon和Maite Taboada(2020)。《C3:建设性评论文集》。竖锯和西蒙·弗雷泽大学”。d O I:<a class="ae kv" href="https://doi.org/10.25314/ea49062a-5cf6-4403-9918-539e15fd7b52" rel="noopener ugc nofollow" target="_blank"><em class="mb">10.25314/ea 49062 a-5c F6–4403–9918–539 e 15 FD 7b 52</em></a>。</p><p id="4695" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">马亚，阿伦S (2020)。“ktrain:用于增强机器学习的低代码库”。<a class="ae kv" href="https://arxiv.org/abs/2004.10703" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> arXiv预印本arXiv:2004.10703 </em> </a>。</p><p id="a5f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">莱斯利·史密斯(2017)。“训练神经网络的循环学习率”。2017年IEEE计算机视觉应用冬季会议。<a class="ae kv" href="https://ieeexplore.ieee.org/abstract/document/7926641/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> IEEE，第464–472页</em> </a>。</p><p id="389a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">莱斯利·N·史密斯(2018)。“神经网络超参数的训练方法:第1部分-学习速率、批量大小、动量和权重衰减”。<a class="ae kv" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> arXiv预印本arXiv:1803.09820 </em> </a>。</p></div></div>    
</body>
</html>