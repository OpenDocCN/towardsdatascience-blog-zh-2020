<html>
<head>
<title>Multilingual Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多语言变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multilingual-transformers-ae917b36034d?source=collection_archive---------11-----------------------#2020-01-17">https://towardsdatascience.com/multilingual-transformers-ae917b36034d?source=collection_archive---------11-----------------------#2020-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7b7e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么BERT不是多语言任务的最佳选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/374bd69af9480855abed295b93b02e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*2wwI2-PoSHMjvsnUwSY7nA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用<a class="ae ku" href="https://translatr.varunmalhotra.xyz/" rel="noopener ugc nofollow" target="_blank">https://translatr.varunmalhotra.xyz/</a>和<a class="ae ku" href="https://www.wordclouds.com/" rel="noopener ugc nofollow" target="_blank">https://www.wordclouds.com/</a>翻译“多语言变形金刚”获得的图像</p></figure><p id="fefa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">去年，我们看到了<strong class="kx iu">变压器架构</strong>的快速改进。作为最先进的语言理解任务的主要参考点，大多数研究工作都集中在英语数据上。<a class="ae ku" rel="noopener" target="_blank" href="/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8"> BERT、RoBERTa、DistilBERT、XLNet——使用哪一个？</a>概述了最近的变压器架构及其优缺点。</p><p id="bc18" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">跟踪GLUE leader板是一项挑战，因为语言理解任务的进度非常快。每个月都有不同的团队占据榜首。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/dc18fd6c9ddc1946ee0c52320dd1eaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F3e-RGZTJ-5H8m2LIxyDIw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">胶水导板快照(2020年1月初)<a class="ae ku" href="https://gluebenchmark.com/leaderboard/" rel="noopener ugc nofollow" target="_blank">https://gluebenchmark.com/leaderboard/</a></p></figure><p id="e680" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">与此同时，transformer架构已经应用于多语言任务。为了评估这些任务，这里讨论的方法使用由15种语言的带标签句子组成的<a class="ae ku" href="https://www.nyu.edu/projects/bowman/xnli/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">跨语言自然语言推理(XNLI)语料库</strong> </a>。每个数据点由一个前提和一个假设组成。前提和假设已经被标记为文本蕴涵:即假设如何与前提相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/0731f7df9f8925501241f617a348616f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2pLmmwa4O97B5dU9ku5sw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">XNLI <a class="ae ku" href="https://arxiv.org/pdf/1809.05053.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的例子</p></figure><p id="b612" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其他一些标签的例子是“矛盾的，中性的”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/e3f63a1c5d3d743eaac05ea413e0e301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-82p0pE-Z2qGnH2vov5m7A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自<a class="ae ku" href="https://www.nyu.edu/projects/bowman/xnli/" rel="noopener ugc nofollow" target="_blank">跨语言NLI系词(XNLI) </a></p></figure><p id="5089" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">目前，没有关于多语言理解任务的商定基准。XNLI数据集似乎是跟踪多语言模型发展的主要参考。在本笔记中，简要概述了用于多语言理解的<strong class="kx iu">多语言转换器</strong>的发展。</p><h1 id="bb80" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">多语种伯特语</h1><p id="c271" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">在提出BERT之后不久，Google research就推出了一个多语言版本的BERT，能够支持100多种语言。</p><h2 id="78dc" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">参考资料:</h2><ul class=""><li id="a7ae" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">来自谷歌的多语言BERT，<a class="ae ku" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">链接</a>。</li><li id="ec68" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><a class="ae ku" href="https://arxiv.org/abs/1906.01502" rel="noopener ugc nofollow" target="_blank">多语伯特的多语水平如何？</a></li><li id="3fba" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><a class="ae ku" href="https://openreview.net/forum?id=HJeT3yrtDr" rel="noopener ugc nofollow" target="_blank">多语BERT跨语言能力实证研究</a>(ICLR 2020接受)。</li></ul><h2 id="bb67" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">亮点:</h2><ul class=""><li id="9146" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">110k跨所有104种语言共享单词表。低资源语言被上采样。</li><li id="e9c6" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">它提供了跨不同语言的某种共享表示。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nv"><img src="../Images/bb5b479f2e2ea3e8c62921a6200e7b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ew4LkreuVjwrFYmAeoZ5Ng.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">此图是一个代理，用于估计两种不同语言的表示之间的相似程度。例如，EN-DE上的100%将意味着英语和德语被映射到相同的表示。更多详情在<a class="ae ku" href="https://arxiv.org/abs/1906.01502" rel="noopener ugc nofollow" target="_blank">多语BERT到底有多多语言？</a></p></figure><ul class=""><li id="6721" class="nh ni it kx b ky kz lb lc le nw li nx lm ny lq nm nn no np bi translated">然而，该模型没有被明确地训练成具有跨语言的共享表示。因此，上面的结果有点令人惊讶。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nz"><img src="../Images/81885dd462b1b7bc24a152b415f93014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AINZinaTwH_t1Y0VI2ZCsw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自<a class="ae ku" href="https://arxiv.org/abs/1906.01502" rel="noopener ugc nofollow" target="_blank">多语伯特到底有多多语？</a></p></figure><ul class=""><li id="27c0" class="nh ni it kx b ky kz lb lc le nw li nx lm ny lq nm nn no np bi translated">最新的结果表明，语言之间的词汇重叠在跨语言表现中几乎不起作用。</li><li id="83a9" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">相反，更深的网络提供了更好的跨语言性能。</li></ul><h2 id="c726" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">所需资源:</h2><p id="8e5d" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">预先接受了4到16个云TPU的培训。</p><h2 id="bccf" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">许可证:</h2><p id="63ed" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated"><a class="ae ku" href="https://github.com/google-research/bert/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">阿帕奇许可证2.0 </a></p><h1 id="77ce" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">XLM (croX语言模型)</h1><p id="23fd" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">这个模型是由脸书的研究人员在2019年初提出的。</p><h2 id="8243" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">参考资料:</h2><ul class=""><li id="21c9" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated"><a class="ae ku" href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a>(neur IPS 2019受理)。</li><li id="ee19" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">脸书代码库，<a class="ae ku" href="https://github.com/facebookresearch/XLM" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul><h2 id="06b7" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">亮点:</h2><ul class=""><li id="f29d" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">使用定义翻译语言建模方法(TLM)进行平行语料库培训</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oa"><img src="../Images/63ac1c644ccb0b41c86c1dfdeecc5896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1x_sxVcm16h8tmbSu6xYuw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">英语和法语中的并列句举例。来自<a class="ae ku" href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a></p></figure><ul class=""><li id="d050" class="nh ni it kx b ky kz lb lc le nw li nx lm ny lq nm nn no np bi translated">在95k词汇表上训练的80k <strong class="kx iu"> BPE </strong>(字节对编码)令牌。BPE非常类似于词块标记化的方法，<a class="ae ku" href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener">链接</a>。BPE的工作方式是以一种层次化的方式对人物进行聚类，似乎更普遍地应用于跨语言模型。</li><li id="de48" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">在XNLI基准测试上，它在零拍上取得了非常好的性能。如果在训练期间使用翻译的数据，性能会更好。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ob"><img src="../Images/26da04ef1548646e1ece9b159c51d021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPniF9BbpiuKhlmwIdb5Zg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">从<a class="ae ku" href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a></p></figure><h2 id="ffc3" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">所需资源:</h2><ul class=""><li id="e484" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">在64个GPU上进行了预培训</li></ul><h2 id="ecec" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">许可证:</h2><p id="6cf1" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated"><a class="ae ku" href="https://github.com/facebookresearch/XLM/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">归属-非商业性4.0国际版</a></p><h1 id="bd44" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">XLM-R(XLM-罗伯塔)</h1><p id="5322" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">脸书的研究人员在2019年底跟随<a class="ae ku" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>的脚步提出了这个模型。至于罗伯塔，主要的贡献是关于选择一个更好的训练设置。(作为RoBERTa的补充说明，即使这推动了性能的提高，也被认为没有提供足够的技术贡献，无法在2020年ICLR奥运会上被接受。</p><h2 id="28c0" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">参考资料:</h2><ul class=""><li id="6da7" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated"><a class="ae ku" href="https://arxiv.org/abs/1911.02116" rel="noopener ugc nofollow" target="_blank">大规模无监督跨语言表征学习</a></li></ul><h2 id="f914" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">亮点:</h2><ul class=""><li id="c381" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">更多的数据和更强的计算能力！</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/2ab6aa02f6ff3b18f83d760af9c13277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BnSK5zKnN0_0PGNdDvYn6A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">XLM只在维基百科数据上接受训练，而XLM-R只在普通抓取数据上接受训练。来自<a class="ae ku" href="https://arxiv.org/abs/1911.02116" rel="noopener ugc nofollow" target="_blank">无监督的跨语言表征大规模学习</a></p></figure><ul class=""><li id="6d95" class="nh ni it kx b ky kz lb lc le nw li nx lm ny lq nm nn no np bi translated"><strong class="kx iu">句子片段</strong>250k词汇上的标记化。他们也使用单语法语言模型而不是BPE。</li><li id="f8f6" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">没有语言嵌入来更好地处理代码转换(即不同语言交替的文本)</li><li id="c46d" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">实现最先进的结果(2019年底)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi od"><img src="../Images/a05da5036336a34406dfe5839ee0aac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SALW26V6Fqxd3at_uwdA2w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自<a class="ae ku" href="https://arxiv.org/abs/1911.02116" rel="noopener ugc nofollow" target="_blank">无监督的跨语言表征大规模学习</a></p></figure><h2 id="b372" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">所需资源:</h2><ul class=""><li id="827d" class="nh ni it kx b ky mq lb mr le nj li nk lm nl lq nm nn no np bi translated">在500个GPU上进行了预培训</li></ul><h2 id="a2c7" class="mv lz it bd ma mw mx dn me my mz dp mi le na nb mk li nc nd mm lm ne nf mo ng bi translated">许可证:</h2><p id="2f74" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated"><a class="ae ku" href="https://github.com/facebookresearch/XLM/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">归属-非商业性4.0国际版</a></p><h1 id="3981" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">总结与展望</h1><p id="9d19" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">XLM-R似乎是迄今为止最好的解决方案。训练多语种变形金刚的<strong class="kx iu"> TLM </strong>(翻译语言模型)方法很有可能会与其他技术相结合。特别是，很容易预见的技术组合在顶部胶水董事会和TLM。在机器学习社区中，对变形金刚还是很感兴趣的。比如<a class="ae ku" href="https://openreview.net/forum?id=H1eA7AEtvS" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">阿尔伯特</strong> </a> <strong class="kx iu"> </strong>和<a class="ae ku" href="https://openreview.net/forum?id=BJgQ4lSFPH" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">爱丽丝</strong> </a> <strong class="kx iu"> </strong>最近已经在ICLR 2020接受了。</p><p id="67fe" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里讨论的多语言转换器可以分别在谷歌和脸书的库中找到<strong class="kx iu">预先训练好的</strong>:</p><ul class=""><li id="69a1" class="nh ni it kx b ky kz lb lc le nw li nx lm ny lq nm nn no np bi translated">谷歌的M-BERT ，<a class="ae ku" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">链接</a>。</li><li id="577a" class="nh ni it kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><strong class="kx iu"> XLM </strong>，以及<strong class="kx iu">脸书的XLM-R </strong>，<a class="ae ku" href="https://github.com/facebookresearch/XLM" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul><p id="a672" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">使用<a class="ae ku" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">hugging face Transformers code</a>可以非常容易地测试出所有的模型。用PyTorch写的。许可证:<a class="ae ku" href="https://github.com/huggingface/transformers/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank"> Apache许可证2.0 </a></p></div></div>    
</body>
</html>