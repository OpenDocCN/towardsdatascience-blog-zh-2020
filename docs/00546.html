<html>
<head>
<title>A Layman’s Guide to Fuzzy Document Deduplication</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模糊文档重复数据删除的外行指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7?source=collection_archive---------6-----------------------#2020-01-16">https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7?source=collection_archive---------6-----------------------#2020-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="08b6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">检测近似重复文档的实用概念，后面是一个 Python 代码示例。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/913910992f1493b23245d20cf709a724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJAqpNXfAN_aW0BbVt6lYw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:aitoff via <a class="ae ky" href="https://pixabay.com/photos/stormtrooper-star-wars-lego-storm-1343772/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="3a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们这个数据呈指数级增长、项目复杂、团队庞大、渴望进入下一个截止日期的时代，小事情经常被忽略。</p><p id="4536" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据分析师团队在一家大型律师事务所看到了这种情况，在那里，重要的文档经常被共享、编辑、再次共享，并存储在专用的文档服务器上。二十年和一百万份文档之后，服务器变成了一个由嵌套文件夹、难以理解的文件名和损坏的文件组成的迷宫。</p><p id="c2df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是在海量存储中筛选重要文件，但我们首先要解决一个问题。</p><p id="7150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们为非常重要的文件建议了一个新的干净的地方。然而，这需要文档是可搜索的，相关的，特别是<strong class="lb iu">独特的</strong>。快速浏览一下文件，我们可以发现数据中有许多重复的地方。</p><p id="c1b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不能依赖文件名、日期或任何其他唯一的标识符。我们所拥有的只是文本文档本身的内容。</p><p id="6dae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么这个问题比编写一个简单的“完全相等”的脚本来查找文本完全相同的文档更复杂呢？问题在于，在实践中，搜索<em class="lv"> 100% </em> <em class="lv">相似度</em>会漏掉许多重复的文档。原因多种多样，但包括:</p><ul class=""><li id="1a1e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">OCR(光学字符识别—将 PDFs 图像转换为文本时使用)期间的差异</li><li id="ec8e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">小的修改、草稿</li><li id="f435" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">用于保存或加载文档的软件版本/方法的可变性</li></ul><p id="20c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过允许“模糊”的重复数据删除，其中脚本识别出<em class="lv">大部分</em>相似的文档，我们可以避开任何可能导致两个重复文档彼此不一致的问题，例如标点符号、大写字母、空格等。如您所见，我们将有一个杠杆来调整<em class="lv">如何将</em>相似的文档归类为副本。</p><p id="230d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然此使用情形非常具体，但在许多情况下可能需要大规模重复数据消除，包括:</p><ul class=""><li id="aa68" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">节省存储空间</li><li id="6f9d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">检查抄袭/版权</li><li id="b41a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">确保问答网站上只有新问题</li><li id="c8c0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">检查数据库中的新条目以确保唯一性</li><li id="83f3" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">消除膨胀数据集中的噪音</li></ul><p id="4c2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们从理论的角度来看解决方案。之后，我们可以向那些对理论背后的代码感兴趣的人展示一个简短的 Python 实现。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="44d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于说明，我们将考虑一个现实生活中的用例。我们将使用来自<a class="ae ky" href="https://www.kaggle.com/c/avito-duplicate-ads-detection/data" rel="noopener ugc nofollow" target="_blank"> Kaggle 竞赛</a>的数据集，其目标是检测在线市场中的重复广告。在本指南中，我们将重点关注广告文本。</p><p id="bbd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提交广告的人通常会对他们现有的广告进行调整，然后重新提交，以提高人们找到他们产品的几率。</p><p id="e26f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 Craigslist 上的一次快速旅行马上向我们展示了这样一个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/901275aa3b76ef4f2b5b97978fbc6924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*lRdyXZ2yilWlJF5HTJUsKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Craigslist 上发布的多余广告</p></figure><p id="afc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到其中一些措辞略有不同了吗？精确匹配系统永远不会发现这些。</p><p id="3717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望通过标记和删除重复广告(即使它们不是逐字相同的)来减少用户的混乱。</p><p id="5901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于数据集的一些基本信息:</p><ul class=""><li id="8675" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">我们使用了 100，000 个广告样本。这个解决方案可以扩展到数百万个文档，这取决于您有多少时间和计算能力/内存可用。</li><li id="4892" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">执行一个简单的“完全相同”的脚本，就像我们上面描述的那样，我们看到大约 96，000 个广告是原创的(或者至少，不是任何其他广告的精确复制)。</li><li id="121b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">大约有 1000 个广告是完全相同的。有些是复制了几十次的<em class="lv"> </em>，有些只是一次。</li><li id="0152" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">虽然重复的广告应该只占数据集的 1%，但大量的副本使它们膨胀到数据的 3.7%。</li><li id="f6f9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">文本本身是俄语的——幸运的是这并不重要，因为解决方案对语言来说是矛盾的。必要的地方我会用谷歌翻译。</li></ul><p id="debb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用“完全相等”脚本来看看数据集中重复次数最多的广告:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/1b5a9f01a7d7d2bd153bcf25dbe18ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-23EFcKmm81owRmy4G9dw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网上销售跨越语言障碍！</p></figure><p id="627c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以马上看到严格重复检测的一些缺点。请注意“状况良好”是如何因为大写或标点符号而被分成三个独立的类别的。我们简单的脚本错误地将这些归类为完全独立的原始文本。</p><p id="3c4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据中的另一个常见模式更加可疑——更长、措辞略有不同的特定广告:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/db3712a53b0a5ee4883a1d95872a1417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15R_yjRQWwZj3Iyh4cMfKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">太巧了！</p></figure><p id="5508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们将开发一个更复杂的解决方案来检测附近的<em class="lv">副本以及精确副本。</em></p><p id="3c37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们深入研究这个案例之前，让我们先来谈谈支持我们努力的概念。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="10f0" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak">字符串相似度与余弦距离</strong></h2><p id="5442" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">为了以一种“模糊”的方式删除这些冗余广告，我们需要找到一种方法，将文本转化为数字，以定量分析相似性。</p><p id="50ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几种不同的方法，但最流行和有效的测量“字符串相似性”或文本相同性的方法之一被称为<strong class="lb iu">余弦相似性</strong>。</p><p id="6f26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">背后的逻辑非常简单。如果我们可以将文本转换成一个<strong class="lb iu">向量</strong>(图上的一条线)，我们就可以在数字上将其与其他向量(同一张图上更多的线)进行比较。线条的位置取决于文档的内容。</p><p id="988a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，考虑以下三个短语:</p><p id="5d23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">"黄油爆米花"</p><p id="bfa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“咸爆米花”</p><p id="9671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">"黄油羊角面包"</p><p id="305f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，我们可以看到这些短语或<strong class="lb iu">文档</strong>之间的一些相似之处。其中两个是黄油，两个是爆米花。然而，为了扩展到比较数百万个文档，我们需要使问题能够被机器理解。</p><p id="a9b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们简单统计一下一个单词在每个短语中出现的时间。结果将如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/ea31433a99c45d14f7fa5d4fc2bc8039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QQlkUpnH_RJCgwKzSDxMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将文档转换成文档向量</p></figure><p id="32f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一行都是我们称之为<strong class="lb iu">的文档向量</strong>，定量地表示每个文档。</p><p id="435b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里只包括了两个特性(列)，尽管您可以拥有与您的<strong class="lb iu">语料库</strong>(整个文档集合)中的独特单词一样多的特性。例如，我们可以为<em class="lv">羊角面包</em>添加第三维度。</p><p id="33f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们只有<em class="lv"> buttery </em>和<em class="lv"> popcorn </em>，我们可以在 2D 绘制这个数据集来说明余弦相似性是如何工作的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/afcecfba3b8de6d995e81ed4e8439e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcTTlmWQW0751NkdH-ra_Q.png"/></div></div></figure><p id="6b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">y 轴测量我们看到“黄油”的频率，而 x 轴测量我们看到“爆米花”的频率。我们可以看到我们的三个文档标有蓝点。</p><ul class=""><li id="3b4d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv">黄油羊角面包</em>富含黄油</li><li id="e417" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">咸爆米花</em>在爆米花维度上很高</li><li id="7d02" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">黄油爆米花</em>两样都有</li></ul><p id="5b28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绿色文本表示我们的余弦相似度计算。我们可以看到<em class="lv">黄油羊角面包</em>和<em class="lv">咸爆米花</em>之间有一个非常大的 90 度角。<em class="lv">黄油爆米花</em>和<em class="lv">咸爆米花</em>的角度更小，只有 45 度。</p><p id="9641" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以停在这里，但是取这些角度的余弦值将会使它们归一化，分别为 0.0 和 0.7。很方便，余弦计算的最小值总是-1(180 度)，最大值总是 1(0 度)。</p><p id="7ae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经成功地用两个特征量化了这些文本的不同之处。考虑到我们的文档包含成百上千个独特的单词，我们将拥有同样多的特性。不可能在上面这样的图上可视化这么多的文档向量，但是请放心，逻辑在数千个维度上是完全相同的。电脑会帮我们处理的。</p><p id="1b63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，还有一个问题。如果我们只是计算文档中的字数，我们的方法不会考虑词序。例如，我们上面的余弦相似性方法如何解释以下两者之间的差异:</p><ul class=""><li id="f20b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv">“条件好，没得商量”</em></li><li id="a33f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">“条件不好，可以商量”</em></li></ul><p id="09eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不是很清楚，因为它们包含完全相同的单词，只是顺序不同。我们可以看到这些短语具有非常不同的含义，但是我们的余弦相似性将简单地看到它们包含所有相同的单词，对于 100%相似的分数！</p><h2 id="0149" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak"> N-Grams 保持词序</strong></h2><p id="321e" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">一个<strong class="lb iu"> n-gram </strong>简单地描述了来自给定样本的一系列<em class="lv"> n </em>项。在文本中，这可能是给定句子中的一系列单词。因此，假设您想要文档“条件良好，不可转让”中的所有 2 个字母(或<strong class="lb iu">字母组合</strong>),以下是您将收到的三个字母组合:</p><p id="a2ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“状态良好”</p><p id="8aef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">"条件 _ 非"</p><p id="d22a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“不可协商”</p><p id="2b9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，如果您想要 3-grams(或<strong class="lb iu">三元组</strong>，您将收到:</p><p id="8cee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“良好 _ 条件 _ 不”</p><p id="c01f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“条件不可协商”</p><p id="ef7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你能明白为什么拥有这些额外的特性会有用吗？</p><p id="22bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不仅在单个单词上创建文档向量，还在所有二元模型和三元模型上创建它！现在“条件好，不可协商”和“条件不好，可协商”将理所当然地获得较低的相似性分数，因为它们的二元模型和三元模型不一致。</p><h2 id="6509" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak">为什么不是欧几里德距离？</strong></h2><p id="8f47" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">你可能会问的一个问题是——为什么要费力测量文档向量之间的角度？从上面的图表中我们可以看到，您可以为每个文档绘制一个点—为什么不只是查看点之间的距离(<strong class="lb iu">欧几里德距离</strong>)？</p><p id="aa3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个很好的问题。要回答这个问题，让我们看看下图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ca85b28cf8c1fc56b4ab9105c1ef52a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgBONEtOu2Ycdm5ltbF0CA.png"/></div></div></figure><p id="e80f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经为文档增加了数量，其中一些单词出现了多次。我们有一个非常黄油的文档(x4)，一个是黄油加爆米花的文档(x2，x1)，一个只是爆米花的文档(x1)。</p><p id="0cdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然直觉告诉我们第二个文档更接近第一个文档，因为它们都在黄油上超载，但是欧几里得距离 A (2.2)实际上大于欧几里得距离 B (2.0)。使用这些距离，我们可以说共享一个“爆米花”术语的文档比共享两个“黄油”术语的文档更相似！</p><p id="70ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，角度 A 和 B(用余弦相似性度量)支持我们的直觉，即 buttery 文档比 popcorn 文档更相似。</p><p id="20ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，当量值被归一化时(所有的点从图形的原点开始都是等长的)，欧几里德距离和余弦相似度将返回相同的结果。</p><p id="ff9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">余弦相似性的另一个好处是计算机计算起来更便宜，当我们谈论数百万或数十亿次比较时，这可以节省大量时间。</p><h2 id="e378" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak">其他标准化</strong></h2><p id="4d01" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">自然语言处理领域既深且广，根据您的用例，还有许多其他方法可以通过预处理来提高数据质量。</p><p id="1000" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如，<strong class="lb iu">词干化</strong>会将所有单词规格化为它们最基本的形式(<em class="lv">运行</em>变成<em class="lv">运行</em>，<em class="lv">交付</em>变成<em class="lv">交付</em>)。</p><p id="a792" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个转换采用我们创建的文档向量，称为<strong class="lb iu">单词袋</strong>向量(因为我们只是简单地计算每个文档中有多少单词)，并将其转换为<strong class="lb iu">术语频率—逆文档频率</strong>向量(<strong class="lb iu"> TF-IDF) </strong>。这是一个非常强大的转换，可以帮助您完成像文档分类这样的任务。你忽略了在你的整个语料库中频繁出现的单词(<em class="lv"> the，he，her，it，</em>等)。<em class="lv"> ) </em>，<em class="lv">，</em>关注在特定文档中出现的较罕见的单词，但<em class="lv">而不是</em>在整个语料库中出现，因为后面这些单词更有可能是有意义的。</p><p id="1d03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些转换值得了解，但我们不会在这里使用它们，因为副本通常会使用相同的单词形式(不需要词干)和术语频率(不需要特别加权不同的术语/文档)。</p><p id="4452" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，如果你处于这个领域的前沿，你会考虑使用<strong class="lb iu">文档嵌入</strong>，这是将文本意义映射到数字向量的另一种方法。这超出了本文的范围(对于我们的重复数据删除应用程序来说可能有些过头了)。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="5b9b" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak">用余弦相似度去重广告</strong></h2><p id="188f" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">做得好！您现在已经掌握了一个非常有用的工具。让我们看看如何将它应用到前面的重复广告检测示例中。</p><p id="0dfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步是创建一个<strong class="lb iu">文档术语矩阵</strong>。您已经在前面的 popcorn 示例中看到了其中的一个——它只是一个在行上跟踪文档的结构，在列上跟踪特定的<strong class="lb iu">术语</strong>(注意:不再是“单词”,因为我们也可以放入 n-grams)。文档中每出现一个术语，矩阵中相应的单元格就加 1。</p><p id="b1a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看数据中的几个例子的矩阵是什么样的。为了更好的说明，我把俄语的例子翻译成了英语，尽管这种方法适用于任何语言。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/bb9ba651e71e8bf14cb50c7bc5b21fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMJNJaMUXvMLNn6JTh6hOQ.png"/></div></div></figure><p id="849b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">橙色单元格代表我们的文档和术语之间的交叉点。我们可以看到多个文档包含相同的术语——就像<em class="lv">条件</em>属于四个文档。这将有助于提高这些文档之间的余弦相似性得分。</p><p id="795b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经将文本数据转换为文档向量，我们可以应用<strong class="lb iu">成对</strong>余弦计算来评估所有文档之间的相似性。成对只是意味着我们将从数据集中的每两个例子中得到一个数字——每个文档都需要与其他每个文档进行比较，以查看是否有任何相似之处。</p><p id="8200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这可能会变得计算量很大——在技术部分，我们将讨论一个在执行该计算时可以节省资源的伟大包。</p><p id="ffcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看上面例子的余弦相似性计算为我们提供了什么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/3b30cdf0430847d4983f956e1258b293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CW9XPtHCAHWdi_aEzGO7zQ.png"/></div></div></figure><p id="2ff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个文档沿着行和列放置，成对余弦相似性填充在单元格中。</p><p id="8af7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所料，我们在对角线上看到了 100%的相似性——所有文档都是完全相同的。例如，<em class="lv">“状态极佳的 14.5cm”</em>在第一行第一列，导致第一个单元格为橙色 100%。当然，当我们进行分析时，我们不会将这些自相似的细胞算作数据集中的“重复细胞”。</p><p id="5cec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">马上，我们看到一些有趣的结果:</p><ul class=""><li id="ee5b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">“14.5 厘米处于极好状态”和“处于极好状态”共享足够多的术语，以至于它们具有 71%的余弦相似性。</li><li id="81a8" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">“状态良好的衬衫”和“状态良好的衬衫”获得 100%的相似性，因为我们的过程不区分大小写。</li></ul><p id="e35e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可能会认为第一场比赛是<em class="lv">而不是</em>重复，而第二场<em class="lv">是</em>重复。我们可以开始看到在 71%和 100%之间形成复制截止的直觉，但更多的是在以后选择这个阈值。</p><p id="457b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我还在图形中包含了原始的俄语文本。这有助于解释为什么“<em class="lv"> 14.5 厘米状况极佳”</em>和<em class="lv">“状况良好”</em>没有收到任何相似性。在俄语中，两个 c<em class="lv">condition</em>术语的拼写略有不同(一个表示“条件”，另一个在技术上表示“状态”)，导致该术语背后的含义完全不同/独立。</p><p id="5a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些结果来自 6 个文档的小样本，为我们提供一些直觉。让我们看看在 100，000 个文档的数据集上的结果是什么样的。</p><p id="263b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了进行比较，让我们将调查结果与本文前面采用精确/严格的重复数据消除标准得出的结果进行对比:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/f19ffbfdb869e8389c125a4f06cf3e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2WYBFNaWKpxNH4C_2oqLjA.png"/></div></div></figure><p id="e1bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模糊重复数据删除发现了 2，244 个重复文档，约占总数据集的 2%。当考虑到这些重复广告的多个副本的膨胀效应时，这些重复广告占我们数据的 7.5%！</p><p id="b1df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过允许模糊重复数据删除，我们发现的<strong class="lb iu">重复文档是以前的两倍。</strong></p><p id="71d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将看看三个例子，这些模糊的发现是我们现在发现的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c6012a350b3186d14d3e1492d14c5f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*uCZa8-2a12DGJSVT7Q4llQ.png"/></div></figure><p id="4a4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一个例子中，我们选取了“良好状态”的变体。虽然很有趣，但这并不是欺骗性广告的例子——这只是销售商品的人们的常用语！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/b82c005b82da93a9c07389f0f4ec749d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOa6zNIcNt5Ua1Xp38YeXA.png"/></div></div></figure><p id="14b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个例子代表了在整个数据集中看到的许多广告的模式。虽然语序不同，但内容是一样的！我们可以确信这是一个销售同一双运动鞋的欺骗性广告。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/dad836812e9f0d9f133f9c74d2fed4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z9GbwDrWQ4dKW1YNJgF96Q.png"/></div></div></figure><p id="4956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为我们的最后一个例子，我们看到几十个蹦床广告，其中唯一的变化是蹦床本身的尺寸。一个对蹦床不感兴趣的本网站用户当然不需要看到这样的品种。作为网站所有者，你可以选择对这种冗余采取行动。</p><p id="2ad9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他适合这种模式的例子是重复广告，唯一的区别是地址、电话号码或产品颜色。</p><p id="0972" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经完全开发了余弦相似性重复数据删除方法，并看到了一些结果，我们几乎完成了本文的理论部分。有一个非常重要的悬而未决的问题需要讨论。</p><h2 id="d5a9" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><strong class="ak">设置带有精度/召回的截止值</strong></h2><p id="833a" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">需要考虑的一个重要方面是我们选择的余弦相似性阈值。记住，当我们比较两个文档向量时，我们会得到一个介于 0%和 100%之间的余弦相似度。由我们来决定结果是通过还是未通过预定义的设定点。</p><p id="cd61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0%的余弦相似性截止值会将所有内容标记为与其他内容重复，而 100%的截止值需要完全重复(非模糊)才能抛出重复标记。</p><p id="740d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有普遍“正确”的答案，因为我们设置的截止值将取决于我们特定的用例。</p><p id="b85a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种极端的情况是，假设您正在删除稀有且非常有价值的旧手稿的重复数据。你会因为删除哪怕一份原稿而陷入麻烦，因为你认为这是一份非常相似但不同的原稿的副本。如果一些重复的数据没有出现在数据集中，也没什么大不了的。在这种情况下，您可能会错误地选择一个更高的重复数据删除截止值，该值对重复数据的检测更加严格。</p><p id="998f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在另一个极端，您正在删除数百万条推文的重复数据，作为大型项目的预处理步骤。每条推文本身没多大关系，你的主要目标是缩小数据。例如，你想通过垃圾邮件删除大量冗余的推文。如果你不小心删除了原创推文，那也没什么——你只是想通过删除唾手可得的果实来缩小你的语料库。</p><p id="bd9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是<strong class="lb iu">精度</strong>和<strong class="lb iu">召回</strong>的概念变得有用的地方——这些相反的力量将帮助您设置一个良好的重复数据删除截止值。</p><p id="d691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Precision </strong>询问—在我们标记的“肯定”文档中(在我们的例子中，肯定与重复同义)，有多少是真正的肯定/重复文档？</p><p id="eda2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，如果我们将文档标记为“正”/副本，但它不是真正的“正”/副本，我们只是错误地将原始文档标记为副本！根据我们的用例，这可能意味着我们已经提示删除一个原始文档。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/8de9ee915701e23d12b22ed0212552fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iV1vjoDAB4ySJEFY3sl7_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="c197" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在前一个罕见的手稿例子中，我们优化了精确度<strong class="lb iu">。</strong>我们希望确保我们标记为重复的所有文档都是真正的重复文档。没有人会责怪我们为了安全而设置 99%的重复数据消除截止值。</p><p id="9d46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，<strong class="lb iu">回忆</strong>询问——在所有真实的肯定/重复文档中，我们正确识别出多少是重复的？</p><p id="e500" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这和精确有什么不同？在精度方面，我们关心的是<strong class="lb iu">我们得到了多少真阳性与假阳性</strong>。在回忆中，我们关心的是<strong class="lb iu">我们得到了多少真阳性与假阴性</strong>。</p><p id="9183" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在推文重复数据删除的例子中，我们更关心整个推文的重复数据删除，而不是确保我们标记的每条推文都是重复的。我们愿意为了召回而牺牲精确性。由于这种权衡，我们可以接受较低的余弦相似性截止值，比如说 70%。</p><p id="5473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一个包含 100 个文档的数据集。我们已经手动查看了数据，并且<em class="lv">知道</em>有 20 个重复的文档应该被删除。</p><p id="c7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在余弦相似性截止值为 99%时，我们删除了 15 个重复的文档。然而，我们 100%确定我们删除的 15 个都是真正的重复。我们非常精确(15/15–100%)，但是召回率很低(15/20–75%)。</p><p id="e009" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在余弦相似性截止值为 70%时，我们删除了 30 个文档。我们当然删除了 20 个真正的重复项，以获得非常好的回忆分数(20/20–100%)。然而，我们也删除了 10 份并非真正重复的文件。这导致精确度分数很低(20/30–66%)。</p><p id="37ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到精度和召回是怎么对立的了吧？在实践中，有必要提取数据集的样本，标注原件和副本，并尝试针对您的具体情况优化截止阈值。</p><p id="bff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，余弦相似性可能只是更大的统计模型的一个特征。在广告重复数据删除示例中，包含关于产品图像、用户名、发布时间戳等信息可能是有意义的，最终将这些信息输入到二进制分类模型中以标记重复广告。</p><p id="8699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于说明，我们使用了 95%的相似性截止值。相对严格，但正如我们在上面看到的，我们仍然在模糊重复数据删除方面获得了显著的性能。</p><p id="f269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论到此为止！如果您已经坚持了这么长时间，谢谢您——您现在已经实际熟悉了一种有价值的方法来帮助您驯服重复数据。</p><p id="53d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有技术爱好，并且想知道如何编写类似上述示例的代码，我们将继续提供一些 Python 代码片段，它们可以执行我们刚刚讨论的方法。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="3289" class="oc mv it bd mw od oe of mz og oh oi nc jz oj ka nf kc ok kd ni kf ol kg nl om bi translated"><strong class="ak">使用 Gensim 在 Python 中实现</strong></h1><h2 id="92d8" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">为什么是 Gensim？</em></h2><p id="b567" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">Gensim 是一个 Python 库，广泛用于主题建模。但是，它对于重复数据消除也有非常有价值的实用程序。</p><p id="1d5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然在 Python 中有几种有效的方法来计算余弦相似性，包括使用流行的<em class="lv"> SKLearn </em>库，但当数据集变得非常大时，Gensim 的主要优势就来了。</p><p id="5ca9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你的语料库增长超过你的计算机在内存中存储数据的能力时，Gensim 会自动将你的语料库分割成计算机可读的数据块，然后<strong class="lb iu">将其保存到磁盘</strong>。通过将数据放在磁盘上，您可以在遇到内存问题之前处理更大的数据集。您还可以加载并重用存储在磁盘上的碎片。</p><p id="876f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与此相关的是，一旦你生成了一个相似性索引(我们将在下面看到)，你可以把它保存到磁盘上，并逐步增加更多的文档，而不是从头开始重新计算。这允许一次性的时间密集型提升来加载您现有的语料库，每当您向您的语料库添加新数据并想要重新计算相似性时，更新都非常快。</p><h2 id="b442" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">导入包</em></h2><p id="2812" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">首先，我们从 Gensim 导入我们需要的模块。注意，要自己运行代码，您需要下载带有<em class="lv"> pip </em>或<em class="lv"> conda </em>的 Gensim。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="2c5e" class="mu mv it op b gy ot ou l ov ow">#Import necessary gensim packages</span><span id="3806" class="mu mv it op b gy ox ou l ov ow">from gensim.utils import simple_preprocess<br/>from gensim.models.phrases import Phrases, Phraser<br/>from gensim import corpora<br/>from gensim.similarities import Similarity</span></pre><p id="93ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一些测试数据。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="7905" class="mu mv it op b gy ot ou l ov ow">#Test dataset</span><span id="8c5b" class="mu mv it op b gy ox ou l ov ow">documents = [<br/>    “Used SpaceX rocket as-is, buyer must transport.”,<br/>    “Used SpaceX rocket as-is, buyer must transport.”,</span><span id="a14d" class="mu mv it op b gy ox ou l ov ow">    “For sale: bulk 100lbs pack of spaghetti noodles”,<br/>    “Spaghetti noodles for sale — 100lbs bulk pack”,</span><span id="ba64" class="mu mv it op b gy ox ou l ov ow">    “Pale blue tuxedo, used, good condition. Call 555–555–5555”,<br/>    “Brand new yellow tuxedo in great condition!”<br/>]</span></pre><h2 id="317f" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">数据预处理</em></h2><p id="362d" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">一般来说，我们在单词级别处理文档。这里有一个简单的方法将文档分解成正确的格式:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="935b" class="mu mv it op b gy ot ou l ov ow"><em class="lv">#Convert documents to collection of words</em></span><span id="8a59" class="mu mv it op b gy ox ou l ov ow"><em class="lv">texts = [[text for text in simple_preprocess(doc, deacc=True)] for doc in documents]</em></span></pre><p id="b66d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> simple_preprocess </strong>是一个有用的 Gensim 函数，用于将文档解析成单个单词。您可以根据需要向它传递额外的参数。在这里，我们通过“deacc=True”来去除标点符号、重音符号和数字。</p><p id="d8f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是<em class="lv">“二手 SpaceX 火箭原样，买家必须运输。”</em>简单预处理后:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="822d" class="mu mv it op b gy ot ou l ov ow">[‘used’, ‘spacex’, ‘rocket’, ‘as’, ‘is’, ‘buyer’, ‘must’, ‘transport’]</span></pre><h2 id="1205" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">构建 N-Grams </em></h2><p id="7411" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们将使用单词级结构来训练一个 Gensim n-gram 短语器。</p><p id="8136" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">普通的二元模型生成器会为每个词对(used_spacex，spacex_rocket，rocket_as)生成一个二元模型，而 Gensim 短语生成器经过预处理，可以选择搭配，或者通常会在一起出现的词。例如，只有 spacex_rocket 将被保留，因为短语者认为它是单词的可能搭配。</p><p id="23b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种选择性是节省程序运行时间的一个很好的方法，因为它不需要为每个可能的单词排列存储一个特征。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="4f42" class="mu mv it op b gy ot ou l ov ow">#Build a bigram model to capture every pair of words in the texts</span><span id="9d30" class="mu mv it op b gy ox ou l ov ow">bigram = Phrases(texts, min_count=1)<br/>bigram_phraser = Phraser(bigram)</span></pre><p id="8b85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> min_count </strong>参数指定短语必须在语料库中找到多少次才能被保留。因为我们的数据集很小，所以我们把它保持在 1。</p><p id="d024" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了一个训练有素的 Gensim Phraser，我们将在句子转换成术语的过程中使用它。这看起来和以前一样，除了我们包含了<strong class="lb iu"> bigram_phraser </strong>变量。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="5eed" class="mu mv it op b gy ot ou l ov ow">#Reconvert documents to collection of words/bigrams</span><span id="a3bc" class="mu mv it op b gy ox ou l ov ow">texts_bigrams = [[text for text in bigram_phraser[ simple_preprocess(doc, deacc=True)]] for doc in documents]</span></pre><p id="81e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<em class="lv">“出售意大利面条——100 磅散装”</em>现在变成了:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="e09b" class="mu mv it op b gy ot ou l ov ow">['spaghetti_noodles', 'for_sale', 'lbs', 'bulk', 'pack']</span></pre><h2 id="457e" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">创建 Gensim 词典和语料库</em></h2><p id="7b00" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">Gensim 使用特定于包的结构来提高运行效率。具体来说，我们需要创建一个 Gensim 字典和 Gensim 语料库。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="fd4c" class="mu mv it op b gy ot ou l ov ow">#Create dictionary</span><span id="a73b" class="mu mv it op b gy ox ou l ov ow">dictionary = corpora.Dictionary(texts_bigrams)</span></pre><p id="1e97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Gensim  <strong class="lb iu">字典</strong>简单地将每个特定的术语(键)映射到一个唯一的 ID(值)。一个普通的 Python 字典表示如下:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="0f48" class="mu mv it op b gy ot ou l ov ow">In: {k: v for k, v in dictionary.items()}</span><span id="d671" class="mu mv it op b gy ox ou l ov ow">Out: <br/>    {0: 'as_is',<br/>    1: 'buyer_must',<br/>    2: 'spacex_rocket',<br/>    3: 'transport',<br/>    4: 'used',<br/>    ...<br/>    18: 'great',<br/>    19: 'in',<br/>    20: 'new',<br/>    21: 'yellow'}</span></pre><p id="7895" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来是语料库:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="85c3" class="mu mv it op b gy ot ou l ov ow">#Create corpus</span><span id="2437" class="mu mv it op b gy ox ou l ov ow">corpus = [dictionary.doc2bow(docString) for docString in texts_bigrams]</span></pre><p id="aa95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Gensim </strong> <strong class="lb iu">语料库</strong>使用我们刚刚创建的字典将我们的原始文本文档转换成数字表示。通过用数字代替文本，计算变得更快。</p><p id="0a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们数据中的第一个文档——“二手 SpaceX 火箭，买方必须运输。”—现在看起来像:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="bc63" class="mu mv it op b gy ot ou l ov ow">[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]</span></pre><p id="eecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每组括号(元组)代表一个单词。第一个数字是单词在字典中的位置，第二个数字是它在文档中出现的次数。</p><p id="a228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于(0，1)，0 表示我们的二元模型“as_is”，它在文档中出现一次。</p><p id="69c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些数字实际上只是代表一个单词包——我们之前使用的术语，意思是从文档中解构出来的、独立的术语集合。</p><p id="c5b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些元组中的每一个都代表我们的文档术语向量中的一个维度。</p><h2 id="2e27" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">创建相似性指数</em></h2><p id="5cbd" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们已经准备好为我们的文档术语向量集合计算余弦相似度了！</p><p id="c884" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Gensim 为我们做了所有繁重的工作，我们只需使用正确的语法:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="d57f" class="mu mv it op b gy ot ou l ov ow">#Build similarity index<br/>index = Similarity(corpus=corpus,<br/>                   num_features=len(dictionary),<br/>                   output_prefix='on_disk_output')</span></pre><p id="2031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这实例化了<strong class="lb iu">相似性索引</strong>，它计算并跟踪每个文档相对于所有其他文档的余弦相似性——成对计算。Gensim 经过优化，可以相对快速地执行这一计算。<strong class="lb iu"> output_prefix </strong>参数就是将要保存到硬盘上的文件的名称——如果需要的话，可以在以后重新加载。</p><p id="db69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们打开索引，看看里面有什么:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="47ff" class="mu mv it op b gy ot ou l ov ow">#Parse similarities from index</span><span id="7c87" class="mu mv it op b gy ox ou l ov ow">doc_id = 0<br/>similar_docs = {}<br/>for similarities in index:<br/>    similar_docs[doc_id] = list(enumerate(similarities))<br/>    doc_id += 1</span></pre><p id="84fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> similar_docs </strong>现在是一个 Python 字典，它跟踪每个文档，以及它和语料库中所有其他文档之间的相似性得分。</p><p id="34bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，第一个条目看起来像:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="36b9" class="mu mv it op b gy ot ou l ov ow">{0: [(0, 0.9999), #“Used SpaceX rocket as-is, buyer must transport.”<br/>    (1, 0.9999),  #“Used SpaceX rocket as-is, buyer must transport.”<br/>    (2, 0.0),<br/>    (3, 0.0),<br/>    (4, 0.1690), #“Pale blue tuxedo, used, good condition...<br/>    (5, 0.0)],<br/> ...<br/>}</span></pre><p id="7261" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们集合中的第一个文档(“0”——Python 是零索引的)(“二手 SpaceX 火箭，买家必须运输。”)和自己一模一样，和第二个文档一模一样。</p><p id="cec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它与第五份文件有 17%的相似性(“淡蓝色燕尾服，用过，状况良好。致电 555–555–5555”)。我们可以看到“used”是一个共享令牌。</p><h2 id="a14e" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">设置截止阈值</em></h2><p id="2d88" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们将使用 90%相似的阈值。根据用例，我们可以将它上移或下移，如上所述。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="2088" class="mu mv it op b gy ot ou l ov ow">sim_threshold = 0.9</span></pre><h2 id="e14d" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated"><em class="on">查找相似文档</em></h2><p id="484d" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">下面的帮助器代码将向我们返回超出阈值的文档对:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="e6a6" class="mu mv it op b gy ot ou l ov ow">for doc_id, sim_doc_tuples in similar_docs.items():<br/>    for sim_doc_tuple in sim_doc_tuples:<br/>        sim_doc_id = sim_doc_tuple[0]<br/>        sim_score = sim_doc_tuple[1]</span><span id="ffe1" class="mu mv it op b gy ox ou l ov ow">if sim_score &gt;= sim_threshold and doc_id != sim_doc_id:<br/>            print(f"Found similar documents, score of {sim_score:.2f}:")<br/>            print('\t', documents[doc_id])<br/>            print('\t', documents[sim_doc_id], '\n')</span></pre><p id="a4b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印的结果包括:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/9d26879fed422d673c515a8d8acc4c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGTPWieBQUNesabU0IWJOQ.png"/></div></div></figure><p id="2f0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们找到了。我们能够验证第一对文档是精确的副本，第二对是模糊的副本。请随意使用这个测试代码来处理您自己的一些数据。</p><p id="75d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本指南到此为止！现在，您已经熟悉了有价值的文本分析技术，并准备在您的下一个项目中实施一种复杂的模糊重复数据删除方法。</p><h2 id="a1e9" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">感谢阅读！</h2></div></div>    
</body>
</html>