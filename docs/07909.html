<html>
<head>
<title>Understand Local Receptive Fields In Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解卷积神经网络中的局部感受野</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-local-receptive-fields-in-convolutional-neural-networks-f26d700be16c?source=collection_archive---------3-----------------------#2020-06-12">https://towardsdatascience.com/understand-local-receptive-fields-in-convolutional-neural-networks-f26d700be16c?source=collection_archive---------3-----------------------#2020-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5608" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">技术和解释</h2><div class=""/><div class=""><h2 id="eeb3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">想过为什么卷积神经网络中的所有神经元都是相连的吗？</h2></div><blockquote class="kr ks kt"><p id="b125" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae lr" href="https://www.oreilly.com/live-events/practical-introduction-to-the-world-of-computer-vision-and-deep-learning-with-tensorflow-keras/0636920060577/0636920061406/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">在我主持的这个现场培训环节，用TensorFlow和Keras学习AI和深度学习的基础知识。</strong>T3】</a></p></blockquote><blockquote class="ls"><p id="aae4" class="lt lu it bd lv lw lx ly lz ma mb lq dk translated">这篇文章面向所有水平的练习机器学习或更具体地说深度学习的个人。</p></blockquote><p id="25a0" class="pw-post-body-paragraph ku kv it kx b ky mc kd la lb md kg ld me mf lg lh mg mh lk ll mi mj lo lp lq im bi mk translated"><span class="l ml mm mn bm mo mp mq mr ms di"> C </span>旋转神经网络(CNN)具有对通过网络输入的图像的仿射变换保持不变的特性。这提供了识别图像中偏移、倾斜或轻微扭曲的图案的能力。</p><p id="a5c6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">由于CNN架构的三个主要属性，引入了仿射不变性的这些特征。</p><ol class=""><li id="bc5d" class="mt mu it kx b ky kz lb lc me mv mg mw mi mx lq my mz na nb bi translated"><strong class="kx jd">局部感受野</strong></li><li id="376e" class="mt mu it kx b ky nc lb nd me ne mg nf mi ng lq my mz na nb bi translated">共享权重</li><li id="1ccd" class="mt mu it kx b ky nc lb nd me ne mg nf mi ng lq my mz na nb bi translated">空间子采样</li></ol><p id="e8fa" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">在这篇文章中，我们将探索局部感受野，理解它们的目的和它们在CNN架构中的优势。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="d34b" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">介绍</h1><p id="e1ca" class="pw-post-body-paragraph ku kv it kx b ky og kd la lb oh kg ld me oi lg lh mg oj lk ll mi ok lo lp lq im bi translated">在CNN架构中，有几个层的组合，其中有一组单元或神经元。</p><p id="5721" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">这些单元接收来自前一层中类似子部分的相应单元的输入。在传统的全连接前馈神经网络中，层内的单元/神经元接收来自前一层的所有单元的输入。</p><p id="d36d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">曾经想知道为什么卷积神经网络中的所有神经元没有连接起来吗？</p><p id="36d6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">将前一层的所有单元连接到当前层的单元是不切实际的。由于连接的增加，训练这种网络的计算资源将是巨大的。此外，这样的网络将需要一组更广泛的训练数据来利用网络的全部容量。</p><p id="cc1b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">但更重要的是，CNN中的每个神经元负责输入数据的一个定义区域，这使神经元能够学习构成图像的线条、边缘和小细节等模式。</p><p id="e4ef" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">神经元或单元在输入数据中暴露的这一限定的空间区域被称为<strong class="kx jd">局部感受野</strong>。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="3a92" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">感受野</h1><p id="41d9" class="pw-post-body-paragraph ku kv it kx b ky og kd la lb oh kg ld me oi lg lh mg oj lk ll mi ok lo lp lq im bi translated">感受野是空间或空间结构的定义部分，包含向相应层内的一组单元提供输入的单元。</p><p id="7f3a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">感受野由卷积神经网络中一层的滤波器大小来定义。感受野也是一个层内的神经元或单元可以接触到的输入数据范围的指示<em class="kw">(见下图)。</em></p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="d54c" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">例子</h1><p id="d6f1" class="pw-post-body-paragraph ku kv it kx b ky og kd la lb oh kg ld me oi lg lh mg oj lk ll mi ok lo lp lq im bi translated">下图显示了输入体积为32x32x3的输入数据(红色)。</p><p id="6e9e" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">输入体积基本上告诉我们，输入数据中的图像具有32×32(高/宽)的尺寸，沿着三个颜色通道:红色、绿色和蓝色。</p><p id="0226" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">图像中的第二个对象(蓝色)代表一个卷积层。conv层的滤波器大小为5×5，对应于该层中每个神经元对输入数据的局部感受野面积。</p><p id="59fb" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">感受野不仅作用于输入体积的面积，而且也作用于深度，在本例中是3。</p><p id="4223" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">对于下图中的示例，我们可以根据输入量得出每个神经元具有的可训练参数的数量。这是感受野乘以输入体积的深度(5x5x3 = 75个可训练参数)。</p><p id="c8e5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">假设我们有(32，32，3)的输入量，卷积层的感受野是5×5，那么卷积层中的每个神经元将具有5×5×3区域的权重，这是神经元内的75个权重。</p><figure class="om on oo op gt oq gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/305ee6a118f34b45c2548d36e5336e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*cVkrqxqhtLOY2az8F_Bj7A.png"/></div><p class="ot ou gj gh gi ov ow bd b be z dk translated"><a class="ae lr" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">局部感受野示意图</a></p></figure><p id="7dcc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">卷积图层的输出是特征地图，图层中特征地图的数量是一个定义的超参数，通过将特征地图的维度乘以可训练参数的数量，可以推导出特征地图中的连接数。</p><blockquote class="ls"><p id="0f32" class="lt lu it bd lv lw ox oy oz pa pb lq dk translated">局部感受野是由卷积层中的神经元在卷积过程中暴露的输入数据内容所占据的定义的分段区域。</p></blockquote><p id="3e03" class="pw-post-body-paragraph ku kv it kx b ky mc kd la lb md kg ld me mf lg lh mg mh lk ll mi mj lo lp lq im bi translated"><a class="ae lr" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener ugc nofollow" target="_blank"> LeNet论文</a>介绍了利用卷积神经网络进行字符识别的第一个用例。它还介绍了CNN中局部感受野的概念和实现。</p><figure class="om on oo op gt oq gh gi paragraph-image"><div role="button" tabindex="0" class="pd pe di pf bf pg"><div class="gh gi pc"><img src="../Images/23bacec548f097794e77ec7ad928024c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mlzp5il4kQMQ4BHd"/></div></div><p class="ot ou gj gh gi ov ow bd b be z dk translated">照片由Cole Wyland在Unsplash上拍摄</p></figure><p id="3aae" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">但是，局部感受域或者更确切地说是仅暴露于一段输入数据的后续单位——局部连接——的概念早在20世纪60年代就在一项研究中引入了，该研究由<a class="ae lr" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf" rel="noopener ugc nofollow" target="_blank">探索猫的视觉皮层</a>。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="06ab" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">优势</h1><p id="c7e3" class="pw-post-body-paragraph ku kv it kx b ky og kd la lb oh kg ld me oi lg lh mg oj lk ll mi ok lo lp lq im bi translated">局部感受野在识别视觉模式方面的优势在于，层内的单元或神经元直接负责从一小块输入数据区域中学习视觉特征——这不是全连接神经网络的情况，在全连接神经网络中，单元接收来自前一层内单元的输入。</p><p id="bbbf" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">在CNN内的较低层中，单元/神经元学习图像内的低级特征，例如线条、边缘、轮廓等。较高层学习图像的更多抽象特征，例如形状，因为较高层内的单元暴露的图像区域较大，这是先前较低层的感受野累积的结果。</p><figure class="om on oo op gt oq gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/496c2c2e26478c8043351ce4ef5df24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*cXWK5GcoYOghqsjc6LLl-Q.gif"/></div><p class="ot ou gj gh gi ov ow bd b be z dk translated"><a class="ae lr" href="https://www.cybercontrols.org/" rel="noopener ugc nofollow" target="_blank">神经网络模拟</a>功劳归于<a class="ae lr" href="https://www.youtube.com/channel/UC8m-a4A0jk2bkesfPdz1z_A" rel="noopener ugc nofollow" target="_blank">丹尼斯·德米特列夫</a></p></figure><p id="54fe" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated">下面的代码片段展示了如何使用TensorFlow深度学习python库定义卷积层。</p><p id="d88c" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mg lj lk ll mi ln lo lp lq im bi translated"><a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> Conv2D </em> </a>类构造函数接受参数“过滤器”,该参数对应于过滤器产生的输出数量，也是特征映射的数量。参数“kernel_size”采用一个表示内核/筛选器的高度和宽度的整数；在这种情况下，整数5对应于尺寸5×5。</p><pre class="om on oo op gt pi pj pk pl aw pm bi"><span id="d91b" class="pn np it pj b gy po pp l pq pr">simple_conv_layer = tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', input_shape=(28,28,1))</span></pre></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="e1b2" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">我希望这篇文章对你有用。</h1><p id="81e6" class="pw-post-body-paragraph ku kv it kx b ky og kd la lb oh kg ld me oi lg lh mg oj lk ll mi ok lo lp lq im bi translated">要联系我或找到更多类似本文的内容，请执行以下操作:</p><ol class=""><li id="665c" class="mt mu it kx b ky kz lb lc me mv mg mw mi mx lq my mz na nb bi translated">订阅我的<a class="ae lr" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> YouTube频道</strong> </a>即将发布的视频内容<a class="ae lr" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">这里</strong> </a></li><li id="358f" class="mt mu it kx b ky nc lb nd me ne mg nf mi ng lq my mz na nb bi translated">跟着我上<a class="ae lr" href="https://medium.com/@richmond.alake" rel="noopener"> <strong class="kx jd">中</strong> </a></li><li id="d6cf" class="mt mu it kx b ky nc lb nd me ne mg nf mi ng lq my mz na nb bi translated">通过<a class="ae lr" href="https://www.linkedin.com/in/richmondalake/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> LinkedIn </strong> </a>联系我</li></ol></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><div class="om on oo op gt ps"><a rel="noopener follow" target="_blank" href="/batch-normalization-explained-algorithm-breakdown-23d2794511c"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd jd gy z fp px fr fs py fu fw jc bi translated">解释了神经网络中的批量标准化(算法分解)</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">理解深度神经网络中使用的一种常见转换技术</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">towardsdatascience.com</p></div></div><div class="qb l"><div class="qc l qd qe qf qb qg or ps"/></div></div></a></div><div class="qh qi gp gr qj ps"><a rel="noopener follow" target="_blank" href="/5-soft-skills-you-need-as-a-machine-learning-engineer-and-why-41ef6854cef6"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd jd gy z fp px fr fs py fu fw jc bi translated">作为机器学习工程师你需要的5个软技能(以及为什么)</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">包括成为任何劳动力的有用组成部分的提示</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">towardsdatascience.com</p></div></div><div class="qb l"><div class="qk l qd qe qf qb qg or ps"/></div></div></a></div></div></div>    
</body>
</html>