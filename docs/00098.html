<html>
<head>
<title>Understanding Regularization in Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解逻辑回归中的正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-regularization-in-machine-learning-5a0369ac73b9?source=collection_archive---------17-----------------------#2020-01-03">https://towardsdatascience.com/understanding-regularization-in-machine-learning-5a0369ac73b9?source=collection_archive---------17-----------------------#2020-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9ece" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用正则化处理过拟合</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f37db44dca27dd56f03c651806a4d9e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7S6lwTgY129EFhBIKQVYUg.png"/></div></div></figure><h1 id="6af7" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">介绍</h1><p id="3150" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在我上一篇<a class="ae mf" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-r-b5b122fd8e83">帖子</a>中，我只使用了两个特征(<strong class="ll ir"> <em class="mg"> x </em> 1，<em class="mg"> x </em> 2 </strong>)，判定边界是2D坐标上的一条直线。在大多数真实世界的情况下，数据集将具有更多的特征，并且决策边界更加复杂。有了这么多的特性，我们经常会过度拟合数据。过度拟合是与数据集紧密拟合的函数中的建模错误。它捕获数据集中的噪声，可能不适合新的输入数据。</p><p id="ffeb" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">为了克服这个问题，我们主要有两个选择:1)删除不太有用的特征，2)使用正则化。这里我们将重点讨论正规化。</p><p id="0153" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我们将分类的数据创建如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="b537" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">生成的数据无法通过线性方程进行分类，因此我们必须添加高阶项作为特征。然而，更多的特征将允许模型拾取数据中的噪声。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/dc8ee44ea140a1c057d84ce78415a785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3lyMCmTzlorO6G_h4VP3Q.png"/></div></div></figure><h1 id="b674" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">逻辑回归的正则化</h1><p id="a68f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">以前，为了预测logit(概率对数),我们使用以下关系:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mn l"/></div></figure><p id="aa15" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">随着我们添加更多功能，等式的<strong class="ll ir"> <em class="mg"> RHS </em> </strong>变得更加复杂。正则化用于通过施加惩罚来降低预测函数的复杂性。在线性关系的情况下，正则化将以下项添加到成本函数中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mn l"/></div></figure><p id="61ca" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">其中<strong class="ll ir"> <em class="mg"> D </em> </strong>为特征尺寸。它惩罚特征的系数(不包括偏差项)。现在成本函数变成了:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mn l"/></div></figure><p id="437b" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">利用我们对<a class="ae mf" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-r-b5b122fd8e83">逻辑回归</a>的先验知识，我们现在可以开始构建带有正则化的模型。</p><h1 id="ef73" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">R中的正则化</h1><p id="0138" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">同样，我们需要首先创建一些助手函数。注意<code class="fe mq mr ms mt b">%*%</code>是<strong class="ll ir"> R </strong>中的点积。如果你想了解更多的模型细节，你可以在这里阅读我以前的文章<a class="ae mf" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-r-b5b122fd8e83"/>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="72af" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">逻辑回归函数，原本以训练数据<strong class="ll ir"> <em class="mg"> X </em> </strong>，标签<strong class="ll ir"> <em class="mg"> y </em> </strong>作为输入，现在需要多加一个输入:正则化强度<strong class="ll ir"> <em class="mg"> λ </em> </strong> <em class="mg">。</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="9778" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">由于我们希望使用一个包含许多特征的示例来演示过拟合和正则化的概念，因此我们需要通过包含多项式项来扩展特征矩阵。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="5ebb" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在，我们可以像以前一样定义预测函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="48db" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">分类和决策边界</h1><p id="2588" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">接下来，让我们在上面的数据集上训练模型。我使用了6次方的多项式特征矩阵。降低功率也有助于过拟合。实际上，我们正在删除不必要的功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="9cf5" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">首先，不使用正则化(<strong class="ll ir"> <em class="mg"> λ </em> </strong> =0)，结果如下。该模型明显过度拟合了数据，并错误地将该区域分类为11点钟。另一种模型用正则化(<strong class="ll ir"> <em class="mg"> λ </em> </strong> =5)训练，更能代表总趋势。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f37db44dca27dd56f03c651806a4d9e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7S6lwTgY129EFhBIKQVYUg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">左:无正规化；右图:正则化模型。</p></figure><h1 id="5cba" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结论</h1><p id="45e5" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">正则化对于克服过度拟合非常有用。它允许我们保留甚至稍微有用的特征，并自动降低这些特征的系数。</p><p id="9a40" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">你可以访问我的github获取R 中的<a class="ae mf" href="https://github.com/JunWorks/Understanding-Regularization-in-Machine-Learning" rel="noopener ugc nofollow" target="_blank">源代码。同样的主题还有一个</a><a class="ae mf" href="https://github.com/JunWorks/ML-Algorithm-with-Python/blob/master/logistic_regression/logistic_regression.ipynb" rel="noopener ugc nofollow" target="_blank"> Python版本</a>。</p></div></div>    
</body>
</html>