<html>
<head>
<title>Topic Modeling Quora Questions with LDA &amp; NMF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主题建模与LDA和Quora问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd?source=collection_archive---------3-----------------------#2020-02-06">https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd?source=collection_archive---------3-----------------------#2020-02-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b436dfe7f3b292d2d479e55d97e2e26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcb3u1DBYZHcWiHCgawyKg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来源:Unsplash</p></figure><div class=""/><div class=""><h2 id="10c9" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">潜在狄利克雷分配，非负矩阵分解</h2></div><p id="da26" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Quora 用来给问题分配主题的算法是专有的，所以我们不知道他们是如何做到的。然而，这并不妨碍我们用自己的方式去尝试。</p><ul class=""><li id="e76a" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw jg">问题描述</strong></li></ul><p id="bd9f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Quora有所有这些未标记的现有问题，他们需要对它们进行分类，以适应机器学习管道的下一步。</p><ul class=""><li id="c7fb" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw jg">识别问题</strong></li></ul><p id="f6dd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是一个无监督的学习问题，我们在Quora问题中发现了各种各样的主题。</p><ul class=""><li id="c997" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw jg">目标</strong></li></ul><p id="1370" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们的目标是确定主题的数量，并确定每个主题的主题。此外，我测试了我的模型是否可以预测任何新问题的主题。</p><ul class=""><li id="9f23" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw jg">方法</strong></li></ul><p id="0006" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">将一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank"> LDA </a>和一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" rel="noopener ugc nofollow" target="_blank"> NMF </a>模型应用到Quora问题数据集上，确定主题数量和每个主题的主题。</p><p id="cd60" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我已经在Github上加载了一个<a class="ae lq" href="https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/quora_sample.csv" rel="noopener ugc nofollow" target="_blank">更小的样本数据集</a>，可以随意使用。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/4ee0379f7a1a27da35bc1397d7d6cdbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*bXWfW1nrI3XTerIg7vGFtA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:Quora</p></figure><p id="3dcd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是这个话题的一个例子，以及它在Quora网站上的样子。</p><h1 id="834b" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated"><strong class="ak">数据预处理</strong></h1><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">启动. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/9793da801b29c8504ff1121da688c63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*UqLLTmU5WSz3qnODxc316g.png"/></div></figure><p id="fc52" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在文本预处理之前，问题看起来是这样的:</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">prior_preprocessing.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/e339757ea289e26e19521ad3ea32df16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ae6V81TPFfmy2ykC0fc6g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1</p></figure><p id="e405" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"><em class="nb"/></strong>文本预处理期间我们要做什么:</p><ul class=""><li id="9428" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">小写字母</li><li id="0b69" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">删除停用字词、括号、标点和数字</li><li id="2670" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">使用空间然后移除“-PRON-”的词条解释。</li></ul><p id="ca45" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">在文本预处理过程中我们不打算做什么:</em> </strong></p><ul class=""><li id="f839" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">堵塞物</li></ul><p id="05a3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">词干删除一个单词中的最后几个字符，往往会导致不正确的意思和拼写错误，或者使单词无法识别，例如:business → busi，busy → busi，situation → situ。</p><p id="b149" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们应该尽量不要做太多的文本预处理，因为大多数问题都很短，删除更多的单词有失去意义的风险。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">text _预处理_lda.py</p></figure><h1 id="ccc4" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">电子设计自动化(Electronic Design Automation)</h1><p id="0978" class="pw-post-body-paragraph ku kv jf kw b kx nh kg kz la ni kj lc ld nj lf lg lh nk lj lk ll nl ln lo lp ij bi translated">探究问题一般有多长。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">问题_长度. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/184b8f59fff1121ccfef7c80efcec867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WZNYQzplN1ibMsyFZLIiw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2</p></figure><p id="2608" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于什么问题。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">word_cloud.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/762e53c46a53e20f8d0b472763eb70a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3K0nRqPUneehuvRomirdQA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3</p></figure><p id="1084" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb"> Unigrams </em> </strong></p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">unigram.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/9a426f386084c6ad87a27edea6d7c7dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrOLu5ZYLw7IaLSahRjbzg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4</p></figure><p id="6204" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">二元组</em> </strong></p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">bigram.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/a71f13e87d01bd9711ef7727cc5ca867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJnzw8mEV8b35CbCoZxJcQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5</p></figure><p id="f483" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">三元组</em> </strong></p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">三元模型. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/f1a700a47ae80ca02bb7fb3bb069e014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvoJGnVihB6cgEDWR7YSeQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6</p></figure><p id="e4db" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">观察结果</em> </strong>:</p><ul class=""><li id="0e42" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">很多问题都在问“开始的有用技巧……”(我想是开始一份新的职业或新的工作？)</li><li id="8fbc" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">很多问题都是关于学习的好方法。</li><li id="8165" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">很多问题都是询问短期商务旅客的酒店推荐。</li><li id="0559" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">许多人在询问好邻居或坏邻居。假设他们想搬到一个新的地方？</li><li id="53db" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">有些人在考虑写传记。</li><li id="1341" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">似乎有一些重复的问题，例如:短期商务旅客的酒店。检测重复的问题会很有趣，这是另一个时间的主题。</li></ul><h1 id="bf33" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">主题建模</h1><ul class=""><li id="aa15" class="lr ls jf kw b kx nh la ni ld nr lh ns ll nt lp lw lx ly lz bi translated">做主题建模，我们需要的输入是:文档-术语矩阵。单词的顺序不重要。所以，我们称之为“词汇袋”。</li><li id="58e8" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">我们既可以使用<a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>也可以使用<a class="ae lq" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>库，该技术被称为“<a class="ae lq" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配</a>”，简称“LDA”。</li><li id="9f63" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">输出是数据中所有问题的主题数量以及每个主题的主题。</li></ul><p id="23df" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们更深入地研究文档术语矩阵到底是什么。假设我们总共有三个问题:</p><blockquote class="nu nv nw"><p id="d795" class="ku kv nb kw b kx ky kg kz la lb kj lc nx le lf lg ny li lj lk nz lm ln lo lp ij bi translated">1).足球运动员如何保持安全？</p><p id="100d" class="ku kv nb kw b kx ky kg kz la lb kj lc nx le lf lg ny li lj lk nz lm ln lo lp ij bi translated">2).最讨厌的NFL足球队是哪支？</p><p id="fdc3" class="ku kv nb kw b kx ky kg kz la lb kj lc nx le lf lg ny li lj lk nz lm ln lo lp ij bi translated">3).谁是最伟大的政治领袖？</p></blockquote><p id="5ce1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这三个问题的文档术语矩阵是:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/5db699fd8e4b347121091eebadb9cdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqEq6Ur2R95KqAvoYTVkWQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表1</p></figure><p id="bc2b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">每行是一个问题(或文档)，每列是一个术语(或单词)，如果该文档不包含该术语，我们标记“0”，如果该文档包含该术语一次，我们标记“1”，如果该文档包含该术语两次，我们标记“2”，依此类推。</p><h1 id="d546" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">潜在狄利克雷分配</h1><p id="0a8c" class="pw-post-body-paragraph ku kv jf kw b kx nh kg kz la ni kj lc ld nj lf lg lh nk lj lk ll nl ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">潜伏</em> </strong>意为隐藏，<strong class="kw jg"> <em class="nb">狄利克雷</em> </strong>是一种概率分布类型。<strong class="kw jg"> <em class="nb">潜在的狄利克雷分配</em> </strong>意味着我们试图找到所有的概率分布，它们是隐藏的。</p><p id="454c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们继续我们的例子。我们有四个问题。作为人类，我们确切地知道每个问题是关于什么的。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/29c3c4a6fe30f560e845daf93be6f440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swsMXXB4uIJdqLDFswU2ZQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图7</p></figure><p id="9f87" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第一个问题是关于足球，第三个问题是关于政治，第四个问题是关于政治和足球。当我们将这4个问题与LDA相匹配时，它会给我们这样的反馈:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/e6a6e3c192647f26d9e66a87048a00e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wCpLXtvtZVYgrR5OzNOpQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图8</p></figure><p id="48a3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第一个问题是话题A的100%，第三个问题是话题B的100%，最后一个问题是话题A和话题B的拆分，解读它们是我们人类的工作。</p><p id="d5a9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">话题A中“足球”一词权重最高，其次是“NFL”，再次是“球员”。所以我们可以推断这个话题是关于体育的。</p><p id="8c9a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">“政治”一词在B题中权重最高，其次是“领袖”，再次是“世界”。所以我们可以推断这个话题是关于政治的。如下图所示:</p><ul class=""><li id="3c89" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw jg">话题A </strong> : 40%足球，30% NFL，10%球员……<strong class="kw jg"><em class="nb">体育</em> </strong></li><li id="3f6b" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated"><strong class="kw jg">题目B </strong> : 30%政治，20%领袖，10%世界… <strong class="kw jg"> <em class="nb">政治</em> </strong></li></ul><p id="71cf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然后我们回到原来的问题，下面是题目！</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/f7233564777c8295f546feb722340efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdzkEJmzfXnp5QkLD_L1yg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图9</p></figure><p id="89ce" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">再深入一点，每个问题都是话题的混合，每个话题都是文字的混合。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/1da9da84f43ea9065b24d279df095049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPbOsDbNCbBibFv65Bq_ww.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图10</p></figure><p id="1326" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">严格来说</em> </strong>:</p><ul class=""><li id="908f" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">一个问题就是一个话题的概率分布，每个话题就是一个词的概率分布。</li><li id="37e7" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">LDA所做的是，当你用所有这些问题来匹配它时，它会尽力找到最佳的主题组合和最佳的单词组合。</li></ul><p id="2b5c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"><em class="nb">LDA工作步骤</em> </strong>:</p><ol class=""><li id="4fbf" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp of lx ly lz bi translated">我们希望LDA学习每个问题中的主题组合以及每个主题中的单词组合。</li><li id="6b04" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp of lx ly lz bi translated">选择我们认为在整个问题数据集中有多少个主题(例如:num_topics = 2)。</li><li id="24a1" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp of lx ly lz bi translated">将每个问题中的每个单词随机分配给两个主题中的一个(例如:上述问题中的单词“足球”被随机分配给类似政治的主题B)</li><li id="b8e3" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp of lx ly lz bi translated">仔细阅读每个问题中的每个单词及其主题。看看1)该主题在问题中出现的频率，以及2)该单词在整个主题中出现的频率。根据这些信息，给单词分配一个新的主题(例如:看起来“足球”在主题B中不常出现，所以单词“足球”可能应该分配给主题A)。</li><li id="e5a2" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp of lx ly lz bi translated">经历多次这样的迭代。最终，这些主题将开始变得有意义，我们可以解释它们并赋予它们主题。</li></ol><p id="6154" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">幸运的是，我们只需要给LDA输入，LDA会为我们做所有这些脏活。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/5292f1f6aa8d2b6e60568c630ffe213d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ysVBg9xqJ_rgWYyzOrkzVQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图11</p></figure><h1 id="24e4" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">LDA关于983，801个Quora问题</h1><ul class=""><li id="4c9c" class="lr ls jf kw b kx nh la ni ld nr lh ns ll nt lp lw lx ly lz bi translated">我们收到了983，801个Quora问题。</li><li id="fc0b" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">我们希望在整个问题集合中发现隐藏或潜在的主题(例如，技术、政治、科学、体育)，并尝试按主题对它们进行分类。</li><li id="0d2f" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">选择主题数量(更多主题—更精细)</li></ul><blockquote class="nu nv nw"><p id="46cc" class="ku kv nb kw b kx ky kg kz la lb kj lc nx le lf lg ny li lj lk nz lm ln lo lp ij bi translated">比如体育，当我们有很多话题的时候，可能会有足球(topic1)、网球(topic2)、曲棍球(topic3)等等。如果我们减少话题的数量，在某个时候，这些与体育相关的话题会自行消失，变成一个话题。像任何无监督学习一样，没有衡量标准，这个比那个好。我们基本上必须为我们自己的用例进行选择。</p></blockquote><ul class=""><li id="b269" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">使用各种近似方案运行LDA，使用不同的方案运行多次，以查看哪种方案最适合特定的用例。</li></ul><h1 id="6834" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">带有Gensim的LDA</h1><p id="ba31" class="pw-post-body-paragraph ku kv jf kw b kx nh kg kz la ni kj lc ld nj lf lg lh nk lj lk ll nl ln lo lp ij bi translated">我先试了一下Gensim，有20个话题，还有…</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/5f10f99f3002b2d8a81d9315cb1f53f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bfjmXSuxN0-fDj8ePXXXMg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图12</p></figure><p id="e752" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一个好的主题模型很少或者没有重叠，这里绝对不是这样。让我们试试Scikit-Learn。</p><h1 id="14ba" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">带Scikit的LDA学习</h1><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">vec_lda.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/0534b7645437ec3e99f5af588857ecc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPhO3J2ei83-HiWafIB1kQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图13</p></figure><p id="7856" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我保留了LDA找到的每个主题中出现频率最高的前20个单词。这里我们展示的是部分表格:</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">显示_主题. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/07d1ad4310c28184d85ccc954db4c288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrckEqTtzXX_uXyymS_yYg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表2</p></figure><p id="3e7f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然后查看每个主题中的热门词汇，手动为每个主题分配主题。这需要一些领域的专业知识和创造力，主要是，我只是用了前3或4个词作为主题。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">topic_theme.py</p></figure><p id="a6d0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">以下是主题-关键字分配的部分表格。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/fb92ffe8c7e42568406a5aece5304977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQ5gmzziyzk07MtcLwBTCQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表3</p></figure><p id="42f4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">要将某个问题归类到某个特定的主题，一个合乎逻辑的方法就是看哪个主题对那个问题的贡献最大，然后分配给它。在下面的过程中，我们将在一系列的数据操作后，为每个问题分配最具主导性的主题。下面的一些代码是从这个<a class="ae lq" href="https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/" rel="noopener ugc nofollow" target="_blank">教程</a>中借来的。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">分配_主题. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/9377f92f3bef1d9d62f25c491aa8892f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bscrfn0p68SD4kyz8NcFWw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表4</p></figure><p id="ed06" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">从上表中可以看出，问题2、3和8都被分配给了主题18。让我们目测一下它们是否有意义。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/7a21dd8c8a08f38ee8d7cfb0bce3381a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pszSouj3WsjPDaHW-1drWg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图14</p></figure><p id="342b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们在左边有这3个问题，它们被分配给右边的主题，以及该主题中的每个关键词。</p><p id="268d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我完全同意第一个和第二个问题的作业，不确定第三个问题，可能是因为“穿着谦虚”和“性格”这两个词？</p><h1 id="4334" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">做预测</h1><p id="b340" class="pw-post-body-paragraph ku kv jf kw b kx nh kg kz la ni kj lc ld nj lf lg lh nk lj lk ll nl ln lo lp ij bi translated">更准确地说，也许我们应该说一个新问题将如何分配给这20个主题中的一个。</p><p id="425f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我去了Quora.com<a class="ae lq" href="https://www.quora.com/" rel="noopener ugc nofollow" target="_blank">的</a>，取了几个流行的问题来测试我们的模型。同样，下面的代码是从这个<a class="ae lq" href="https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/" rel="noopener ugc nofollow" target="_blank">教程</a>中借来的。</p><p id="3339" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第一个测试问题是“<strong class="kw jg"> <em class="nb">你在生活中学到的最重要的经验是什么，你是什么时候学到的</em> </strong>？”</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">预测_话题. py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/08c12bc2e62e23daaaa53b8c937fdec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1K866ohud3U10O0trzs6g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图15</p></figure><p id="dcfb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们的LDA模型能够找到这个新问题中的主题组合，以及每个主题中的单词组合。这个新问题已经被分配到题目中了，题目中最重要的关键词有“<strong class="kw jg"> <em class="nb">工作、学习、研究</em> </strong>”等等，而这个题目的主题是“<strong class="kw jg"> <em class="nb">工作/学习/技能提升</em> </strong>”。我同意。</p><p id="6d50" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第二个测试问题是“<strong class="kw jg"> <em class="nb">就像拉里·佩奇和谢尔盖·布林用一个更好的搜索引擎取代了他们的现任一样，两个计算机科学博士生创造一个取代谷歌的搜索引擎的可能性有多大？谷歌对这种可能性</em> </strong>有多脆弱？”</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/cf6d229b142666502f89051866ac481a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LhZRr27y7QRH9f2ccfM9g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图16</p></figure><p id="d684" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">用同样的方法，这个新问题被分配到以“<strong class="kw jg"> <em class="nb">国家、学生、计算机、科学</em> </strong>”等为顶级关键词的题目中。对于这个新问题，“计算机”和“项目”这两个词的权重最高。而题目的主题是“<strong class="kw jg"> <em class="nb">学生/互联网/计算机/科学/科研</em> </strong>”。我也同意。</p><h1 id="e000" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated"><strong class="ak">非负矩阵分解(NMF) </strong></h1><ul class=""><li id="56f0" class="lr ls jf kw b kx nh la ni ld nr lh ns ll nt lp lw lx ly lz bi translated">一系列线性代数算法，用于识别以非负矩阵表示的数据中的潜在结构。</li><li id="7df1" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">NMF可以应用于主题建模，其中输入是术语-文档矩阵，通常是TF-IDF标准化的。</li><li id="4cac" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">输入:术语-文档矩阵，主题数量。</li><li id="88fd" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">输出:原始的n个单词乘以k个主题以及m个原始文档乘以相同的k个主题的两个非负矩阵。</li><li id="a305" class="lr ls jf kw b kx nc la nd ld ne lh nf ll ng lp lw lx ly lz bi translated">基本上我们准备用<a class="ae lq" href="https://en.wikipedia.org/wiki/Linear_algebra" rel="noopener ugc nofollow" target="_blank">线性代数</a>进行话题建模。</li></ul><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/f1fae27acda4d3ce1a8d841211e56233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-Gx8v5otnhBiCIr1f1qvw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:<a class="ae lq" href="https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF分解-of-a_fig1_312157184 </a></p></figure><p id="d1b6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如上所示，我们将术语-文档矩阵分解为两个矩阵，第一个矩阵包含每个主题和其中的术语，第二个矩阵包含每个文档和其中的主题。</p><p id="7840" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="nb">再用我们的足球和政治举例:</em> </strong></p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/93c63f5e0d6fa63f67a454949830e8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5kdwUvX4PYQVDU5uGwlS1g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图17</p></figure><p id="a7bc" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">左边是3个问题，右边是这3个问题的术语文档矩阵。我们选择k=2个话题。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/866b4d309fa26e90dffddd73242bf899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_-WJxsAw2GIF6h_dEXI4g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图18</p></figure><p id="e59f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">经过分解，我们得到了两个非负矩阵，一个由k个主题组成，另一个由m个原始文档组成。</p><p id="b8d3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">文本预处理部分与LDA模型相似，我只为NMF模型保留了名词，只是为了看看是否有什么不同。</p><p id="7f40" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可以看到，与LDA模型相比，每个关键词在每个主题中是如何分配的，主题是如何组织的。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mx my l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">nmf_topic.py</p></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/2f805636f2ed48a937f9f900325bbd41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YS9qg5aOrrTY0UwpQrKwQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">表5</p></figure><p id="32e0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">比如一个题目，通过查看每个题目中的所有关键词，除了都以“ph”开头之外，我似乎找不到彼此之间有什么关系。所以，我把“单词从ph开始”作为这个题目的主题。</p><h1 id="c0c9" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated"><strong class="ak">进行预测</strong></h1><p id="1e6d" class="pw-post-body-paragraph ku kv jf kw b kx nh kg kz la ni kj lc ld nj lf lg lh nk lj lk ll nl ln lo lp ij bi translated">使用同样的问题，我们将会看到NMF模型如何将它分配到这20个主题中的一个。</p><p id="7d0f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">问题1:“<strong class="kw jg"><em class="nb">你人生中学到的最重要的一课是什么，是什么时候学到的？”</em> </strong></p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/b504fc615747662d346ac81575ebcb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a1oFKdLP0oKkKcKTeoqLqg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图19</p></figure><p id="f5bd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">问题1已分配到以“<strong class="kw jg"> <em class="nb">人生、变化、瞬间、意义、经历</em> </strong>”等为顶级关键词的题目，该题目主题为“<strong class="kw jg"> <em class="nb">人生/经历/爱情/目的</em> </strong>”。我同意！</p><p id="97e2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">问题2:“<strong class="kw jg"><em class="nb">就像拉里·佩奇和谢尔盖·布林用一个更好的搜索引擎推翻了他们的现任一样，两个计算机科学博士生创造一个推翻谷歌的搜索引擎的可能性有多大？谷歌在这种可能性面前有多脆弱？”</em>T11】</strong></p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/021feaa23aa2a99f45c01814144b8ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcXEj-Snxz6YySupAq-eSw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图20</p></figure><p id="63f7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">问题2已经被分配到具有类似于“<strong class="kw jg"> <em class="nb">学校、学生、学院、大学、工程</em> </strong>”等顶部关键词的主题，并且该主题的主题是“<strong class="kw jg"> <em class="nb">学校/学生/学院/大学</em> </strong>”。我找不到更好的方法。</p><p id="023a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这个帖子的Jupyter笔记本可以在这里找到<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Quora%20Topic%20Modeling_scikit%20learn_LDA.ipynb" rel="noopener ugc nofollow" target="_blank">，在这里</a>找到<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Quora%20Topic%20Modeling_scikit%20learn_NMF.ipynb" rel="noopener ugc nofollow" target="_blank">。享受这周剩下的时光。</a></p><p id="c0db" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">参考资料:</p><div class="ip iq gp gr ir ou"><a href="https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jg gy z fp oz fr fs pa fu fw je bi translated">LDA -如何网格搜索最佳主题模型？(带有python中的示例)</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">Python的Scikit Learn提供了一个方便的接口，用于使用潜在的Dirichlet等算法进行主题建模。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.machinelearningplus.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ix ou"/></div></div></a></div><p id="3ab6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://learning.oreilly.com/videos/oreilly-strata-data/9781492050681/9781492050681-video328137" rel="noopener ugc nofollow" target="_blank">https://learning . oreilly . com/videos/oreilly-strata-data/9781492050681/9781492050681-video 328137</a></p><p id="6b3a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/applications/plot _ topics _ extraction _ with _ NMF _ LDA . html</a></p><p id="fba3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a></p></div></div>    
</body>
</html>