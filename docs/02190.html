<html>
<head>
<title>Swish: Booting ReLU from the Activation Function Throne</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嗖嗖:从激活功能王座引导 ReLU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/swish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb?source=collection_archive---------10-----------------------#2020-03-02">https://towardsdatascience.com/swish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb?source=collection_archive---------10-----------------------#2020-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6a77ec2108455788f86935b53cb26320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qUgy0IxSKaAA0CB-5vh_dQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">未修改的<a class="ae jg" href="https://www.buzzle.com/images/drawings/ninja-costume/martial-arts-kick.jpg" rel="noopener ugc nofollow" target="_blank">来源</a>，图片免费分享。</p></figure><div class=""/><div class=""><h2 id="5c48" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">Swish 如何在深度学习激活函数竞赛中击败 ReLU</h2></div><p id="2763" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">答</span>激活函数长期以来一直是神经网络的兴趣焦点——它们反复归纳输入，是神经网络成功的关键。长期以来，ReLU 一直被默认为深度学习社区中最好的激活函数，但现在有一个新的激活函数——Swish——来夺取这一宝座。</p><p id="0183" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">激活函数有着悠久的历史。首先，选择 sigmoid 函数是因为它容易求导，范围在 0 到 1 之间，并且具有平滑的概率形状。tanh 函数也被认为是 sigmoid 函数的替代函数，适合于-1 和 1 之间的范围，但是这些经典的激活函数已经被 ReLU 代替。整流线性单元(ReLU)是目前最流行的激活函数，因为当 ReLU 函数的输入为正时，梯度可以流动。它的简单性和有效性推动了 ReLU 和分支方法，如漏 ReLU 和参数化 ReLU，超越了 sigmoid 和 tanh 单元。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/9a9d4c2a171bbfdfc2730e709a45adac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*9X_cq19Vp1brXdhjtFCoyQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">各种激活函数图示。来源:Swish Paper。</p></figure><p id="62b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Prajit Ramachandran、Barret Zoph 和 Quoc V. Le 在他们的论文中提出了一个新的激活函数(底部的链接)，他们称之为 Swish。Swish 很简单——它是 sigmoid 函数的<em class="mi"> x </em>倍。</p><p id="a387" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">论文作者的研究表明，简单地用 Swish 单元替换 ReLU 单元可以将 ImageNet 上的最佳分类精度提高 0.9%，对于 Mobile NASNet-A 和 Inception-ResNet-v2 分别提高 0.6%。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5ceb" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated"><strong class="ak">什么是 Swish 激活功能？</strong></h1><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e254eb5c4b67cd2185a06885b18ab188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*rBfSD7_bNCmmhGnyksNEPg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Swish 激活功能。上面画着德斯莫斯。</p></figure><p id="4e8f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正式来说，Swish 激活功能是…</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/07b302e3686cd5fb4bee711c71cfbc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*XXX8PfRvndmBqCsbTUl8hw.png"/></div></figure><p id="5898" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像 ReLU 一样，Swish 在下面是有界的(意味着当<em class="mi"> x </em>接近负无穷大，<em class="mi"> y </em>接近某个常数值)，但是在上面是无界的(意味着当<em class="mi"> x </em>接近正无穷大，<em class="mi"> y </em>接近无穷大)。然而，与 ReLU 不同，Swish 是<em class="mi">平滑的</em>(它没有运动或顶点的突然变化):</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fec40b96c61e46d6adffaf39a6d73a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*TtE5dj2zjn86_WfvXbAjIg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="http://csci431.artifice.cc/images/relu.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="790c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，Swish 是<em class="mi">非单调的</em>，这意味着在整个函数中并不总是有一个奇异且连续的正(或负)导数。(重申一下，Swish 函数在某些点上具有负导数，在其他点上具有正导数，而不是像 Softplus 或 Sigmoid 那样在所有点上都只有正导数。</p><p id="dde6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Swish 函数的导数是…</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d838c04e34afdfd4c4f48c93fde6e2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*tBh2zoZNBjAD1pIlCqqudQ.png"/></div></figure><p id="fc7b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Swish 的一阶和二阶导数，标绘:</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e4995eb0b0d1c9bef132e70268d3f83f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*vocz-lSR5jgRlxfWxgONpw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:Swish paper</p></figure><p id="c014" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于小于约 1.25 的输入，导数的大小小于 1。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="819b" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated"><strong class="ak">Swish 的属性</strong></h1><p id="0a32" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">对于激活函数来说，无界是可取的，因为它避免了在近零梯度期间的缓慢训练时间 sigmoid 或 tanh 等函数上下有界，因此网络需要仔细初始化，以保持在这些函数的限制范围内。</p><p id="ad3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ReLU 函数是对 tanh 的改进，因为它是上界的——这个属性非常重要，以至于 ReLU 之后的每个成功的激活函数都是上界的。</p><p id="d821" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于强正则化，在下面有界可能是有利的-在负无穷大的极限中接近零的函数在正则化方面很好，因为大的负输入被丢弃。这在训练开始时很重要，因为大量的负面激活输入是常见的。</p><p id="5021" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些界限由 Softplus、ReLU 和 Swish 满足，但 Swish 的非单调性增加了输入的“表现力”并改善了梯度流。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/98e392c157110ed4353ce31666ab4d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*PDh1TxATv49f2naGSknq3g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">输入，在通过 Swish 之前和之后。来源:Swish paper</p></figure><p id="46a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，平滑度有助于优化和推广神经网络。在下面的输出景观中，显而易见的是，ReLU 的输出景观由于其不平滑的性质而显得尖锐和不和谐，而 Swish 网络景观则平滑得多。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f917695637a22ee1af9f0907cfcae9e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*w2iJqeyJ1Rs-Uc8MwjVmnw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:Swish paper</p></figure><p id="a1b9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出景观平滑度与误差景观直接相关。更平滑的误差空间更容易遍历和找到最小值，就像走在喜马拉雅山脉不和谐的高度范围内，而不是走在英国乡村平滑起伏的山丘上。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/68eacf25cf909d9a7a8126217ee62560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*Ckbv1ocd6gK68ZhsBH8koQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">你更愿意走在哪个上面？<a class="ae jg" href="https://www.awesomestories.com/images/user/4ae0e8b631.jpg" rel="noopener ugc nofollow" target="_blank">左图像源</a>、<a class="ae jg" href="https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/2015_Swaledale_from_Kisdon_Hill.jpg/1200px-2015_Swaledale_from_Kisdon_Hill.jpg" rel="noopener ugc nofollow" target="_blank">右图像源</a>。</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="8e55" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated"><strong class="ak">嗖嗖性能</strong></h1><p id="6647" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">Swish 论文的作者将 Swish 与以下其他激活功能进行了比较:</p><ul class=""><li id="b32d" class="nu nv jj la b lb lc le lf lh nw ll nx lp ny lt nz oa ob oc bi translated">漏 ReLU，其中<em class="mi">f</em>(<em class="mi">x</em>)=<em class="mi">x</em>如果<em class="mi"> x </em> ≥ 0，以及<em class="mi"> ax </em>如果<em class="mi"> x </em> &lt; 0，其中<em class="mi"> a </em> = 0.01。当<em class="mi">x</em>T33】为 0 时，这允许少量的信息流动，并且被认为是对 ReLU 的改进。</li><li id="3cb6" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">参数 Relu 与泄漏 ReLU 相同，但<em class="mi"> a </em>是一个可学习的参数，初始化为 0.25。</li><li id="3038" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">Softplus 由<em class="mi">f</em>(<em class="mi">x</em>)= log(1+exp(<em class="mi">x</em>))定义，是一个平滑函数，具有类似 Swish 的性质，但严格为正且单调。</li><li id="6ea9" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">指数线性单位(ELU)，定义为<em class="mi">f</em>(<em class="mi">x</em>)=<em class="mi">x</em>if<em class="mi">x</em>≥0 和<em class="mi">a</em>(exp(<em class="mi">x</em>)—1)if<em class="mi">x</em>&lt;0 其中<em class="mi"> a </em> = 1。</li><li id="fc75" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">比例指数线性单位(SELU)，与 ELU 相同，但输出乘以一个值<em class="mi"> s </em>。</li></ul><p id="81a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下表显示了在 9 次实验中，Swish 比概述的基线激活功能执行得更好、相等或更差的次数。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/af1167c009e27d0b9b5bfccc62e5dd00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Wxp7nmlCGgphBlG85oQvqA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">表格数据:Swish 纸</p></figure><h2 id="d31c" class="oi mr jj bd ms oj ok dn mw ol om dp na lh on oo nc ll op oq ne lp or os ng ot bi translated">Swish vs. ReLU</h2><p id="5a09" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">作者发现，通过用 ReLU 单元代替 Swish 单元，当层数从 42 层增加时(当优化变得更加困难时)，与 ReLU 相比有显著的改进。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/756242e5d877436161efb43bbedede5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*CLept0q38ahdaCmrPNkg-w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:Swish paper</p></figure><p id="9dc0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者还发现，对于不同大小的批处理，Swish 的性能优于 ReLU。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/231f1345f58c2f276ae1fd508dac31c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*nTTE6XYkk7OqHmjk6nE6FA.png"/></div></figure><h2 id="09c1" class="oi mr jj bd ms oj ok dn mw ol om dp na lh on oo nc ll op oq ne lp or os ng ot bi translated">在各种数据集上测试</h2><p id="6472" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">作者针对以下基线激活功能测试了 Swish，结果如下:</p><ul class=""><li id="4df4" class="nu nv jj la b lb lc le lf lh nw ll nx lp ny lt nz oa ob oc bi translated">CIFAR-10 和 CIFAR-100 数据集——Swish 在 CIFAR-10 和 CIFAR-100 的每个型号上都始终与 ReLU 相当或超过 ReLU。</li><li id="93c9" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">ImageNet——Swish 比 ReLU 高出 0.6%，在 Mobile NASNet-A 上高出 0.9%，在 MobileNet 上高出 2.2%。</li><li id="4315" class="nu nv jj la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">WMT 2014 英语到德语— Swish 在所有四个测试数据集上都优于 ReLU。</li></ul><h2 id="c6f2" class="oi mr jj bd ms oj ok dn mw ol om dp na lh on oo nc ll op oq ne lp or os ng ot bi translated"><strong class="ak">实施</strong></h2><p id="8728" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">实现 Swish 非常简单——大多数深度学习库应该支持 Swish…</p><pre class="me mf mg mh gt ov ow ox oy aw oz bi"><span id="72ae" class="oi mr jj ow b gy pa pb l pc pd">tn.nn.swish(x)</span></pre><p id="11e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">…或者可以表示为<em class="mi"> x </em>乘以 sigmoid 函数</p><pre class="me mf mg mh gt ov ow ox oy aw oz bi"><span id="fd9c" class="oi mr jj ow b gy pa pb l pc pd"><em class="mi">x *</em> tf.sigmoid(x)</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="89bf" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">相信了吗？</h1><p id="9a57" class="pw-post-body-paragraph ky kz jj la b lb nn kk ld le no kn lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">下次你在训练深度神经网络的时候，试试 Swish 吧！</p><p id="9b98" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！如果你喜欢，也可以看看我的其他作品。</p><p id="0556" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此处可找到 Swish paper<a class="ae jg" href="https://arxiv.org/pdf/1710.05941v1.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>