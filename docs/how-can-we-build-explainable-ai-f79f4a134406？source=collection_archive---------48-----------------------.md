# 如何在你的机器学习模型中考虑可解释性？

> 原文：<https://towardsdatascience.com/how-can-we-build-explainable-ai-f79f4a134406?source=collection_archive---------48----------------------->

![](img/bdc978b47ba04bfc107b8ac92befcb5f.png)

[来源](https://www.freepik.com/free-photo/orange-floor-from-white-jigsaw-puzzle_5507779.htm#page=1&query=complicated&position=4)

## 理解模型行为、解释预测和构建可信模型的分步指南

# 为什么还要谈赛？

随着 AI 模型越来越复杂，以及这些模型在现实应用中的使用，这些 AI 模型对我们生活的影响是不可估量的。可以毫不夸张地说，这些人工智能模型比我们自己更了解我们！虽然这很惊人，但同时也很可怕。随着算法复杂性的增加，我们正在付出降低可解释性和信任度的代价。这意味着一个模型越复杂，我们越不可能理解它是如何工作的。这是这些人工智能模型被称为黑盒的主要原因:我们不知道这些复杂算法背后的原因。

![](img/6f46213f70106a6f93080903d03117eb.png)

准确性与可解释性。来源:ExplainX.ai 内部

现实是，这些算法正在成为我们日常生活的一部分:从脸书新闻供稿到抖音，再到我们最近的在线信用卡应用，这些交互都是由强大的人工智能算法驱动的——其中大多数是黑盒。当我们在关键任务用例中积极使用这些算法时，如预测癌症或事故，或根据代表它们的文本评估候选人，我们需要框架和技术来帮助我们导航和打开这些黑盒。

简而言之，模型开发人员需要回答这五个主要问题:

1.  ***为什么我的模型会给出这个预测？***
2.  我的模型预测与业务/领域逻辑一致吗？如果答案是否定的，那么识别并处理那些更有分量的不相关的特征。
3.  ***我的模型行为在不同的数据子集之间是一致的吗？如果答案是肯定的，那么比较和对比，以确定行为在哪里以及如何变化。***
4.  ***我的模型是否偏向数据集内的某个特定特征？如果答案是肯定的，那么找到偏见在哪里，并消除它？***
5.  ***我能做些什么来影响预测，或者我能采取什么行动来实现理想的结果？***
6.  ***可选:我的模型是否支持审计以符合法规要求？*** (相关主要针对金融机构)

数据科学家或模型开发人员的角色是自信地回答所有这些问题。负责任地构建这些算法、消除偏见、理解模型行为并确保信任是数据科学家需要完成的核心任务。

# 我们开始行动吧！

鉴于 xAI 日益增长的重要性，我提供了一个模型可解释性框架来确保 ML 生命周期中的可解释性和公平性。

请注意，我不会在这篇文章中深入探究各种可解释性技术，但是我会发表一系列文章和视频，以一种彻底而有趣的方式介绍可解释性概念。敬请关注。

为了避开框架的范围，我将只关注机器学习生命周期的**模型构建&验证阶段**。我还假设您已经定义了用例，准备了数据，现在准备好构建模型。

在模型构建和验证阶段，您必须实现两个个人目标:

***信心:确保你的模型健壮、可靠、公正。
沟通&信任:与利益相关者分享模型见解***

这是五步框架:

1.  ***从可解释的模型开始，逐步发展到黑盒模型***
2.  ***定义测试用例以及边缘用例***
3.  ***使用可解释性技术来理解模型行为，消除偏见并解释预测——利用*** [***解释***](https://www.explainx.ai)
4.  ***通过交互式仪表盘建立叙述并与利益相关方交流结果***
5.  ***通过反馈改进你的模型***

首先，在高风险的环境中，即使是一个小小的错误也会产生巨大的影响，或者会花费大量的金钱，你应该总是从可解释的模型开始。从黑箱模型开始通常是一场艰苦的战斗。

不管您决定使用哪种类型的模型，从定义测试用例开始——模型应该按照预期行为的用例集。做这个练习来验证你的模型。这一步还需要与其他拥有领域和业务知识的利益相关者合作。

在定义了测试用例之后，更进一步，构建边缘用例——模型行为特别不确定或出乎意料的用例集。对你的机器学习模型进行压力测试是一个很好的策略。

接下来，你应该采用模型不可知的可解释性技术，如 SHAP、反事实、代理决策树、原型或 IG。这些方法提供了全局级别(解释整个模型)和局部级别(模型做出的单个预测)的解释。

不幸的是，有这么多的技术，选择一个正确的是压倒性的。一旦确定了可解释性方法，您就必须对其进行编码、定制以适应您的模型和数据、修复过程中的任何错误、优化速度并使用其他工具集来可视化结果。

每种不同的技术都有自己的挑战。幸运的是，你可以使用[explainex](https://www.explainx.ai)，这是一个**开源的可解释人工智能库**，它在一个屋檐下提供了最先进的模型可解释性技术，并且只需一行代码就可以访问它们——节省了你大量的编码、调试和工作时间。

通过使用这些可解释性技术，您应该从四个方面来描述您的模型:

1.  ***关注整体模型行为***
2.  ***关注实例***
3.  ***关注特性***
4.  ***关注车型对比***

这些镜头中的大部分都非常简单，但是你也应该把重点放在特征上，以便更好地理解你的模型。通过分析特征排名，您可以快速测试模型的合理性和适用性。此外，解释一个模型是有帮助的，但是比较和对比两个或更多的平行模型更有效。它帮助您逐步构建、比较和完善您的模型。

在你从多个角度理解了你的模型之后，是时候与利益相关者交流你的叙述了。这里的目标是通过反馈改进你的模型，并获得他们的信任。请记住，交流将是反复的，反馈对于帮助您进一步优化模型性能至关重要。

理想情况下，使用利益相关者可以理解的语言和表示来共享具有洞察力的交互式仪表板。利益相关者应该能够无缝地添加他们的评论或要求澄清。

# 让那里有光

坚持到最后的荣誉。解释和调试黑盒机器学习模型是一项艰巨的任务。但是有了正确的框架和解释技巧，你很快就能做到。对你来说好消息是:[explax](https://www.explainx.ai)是开源的。

在下一篇文章中，我将分享一个部署、维护和使用阶段的可解释性框架。

在那之前，让那里有光。