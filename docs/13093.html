<html>
<head>
<title>From von Neumann to Memory-Augmented Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从冯·诺依曼到记忆增强神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4?source=collection_archive---------47-----------------------#2020-09-08">https://towardsdatascience.com/from-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4?source=collection_archive---------47-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9c7d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直观的高层次概述</h2></div><p id="6a31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">2014 年，两项并行的研究工作推出了记忆增强神经网络(MANNs):<a class="ae ln" href="https://arxiv.org/pdf/1410.5401.pdf" rel="noopener ugc nofollow" target="_blank">神经图灵机</a>和<a class="ae ln" href="https://arxiv.org/abs/1410.3916" rel="noopener ugc nofollow" target="_blank">记忆网络</a>从那时起，他们扩展到了一个更广泛的主题，跨越了这些最初的实现。然而，我将坚持高层次的直观概述。这篇文章旨在将过去 7 年的研究浓缩成一篇 7 分钟的文章，去掉那些没有通过时间考验的特定于论文的术语和实现细节。</p><blockquote class="lo"><p id="255e" class="lp lq it bd lr ls lt lu lv lw lx ld dk translated"><strong class="ak">记忆增强神经网络(MANNs) </strong>是冯诺依曼架构的可微分版本。n <em class="ly">通用存储器</em>与模型参数的其余部分分开，并且与 RAM 类似，存储长期信息。</p></blockquote><figure class="ma mb mc md me mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi lz"><img src="../Images/49cb272b3f631e0dc3ddfacf28ae25de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2srjbcAH-HU3Y2YC"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">大象有惊人的记忆力。在一个例子中，23 年后，两只在一起表演的马戏团大象相遇时欢欣鼓舞。大象也认出了曾与它们相处数十年的人类。Tobias Adam 在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="96fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统的<a class="ae ln" href="https://en.wikipedia.org/wiki/Von_Neumann_architecture" rel="noopener ugc nofollow" target="_blank">冯诺依曼架构</a>区分了 CPU(中央处理器)和三级内存:<em class="mq">寄存器</em> —非常快，但存储能力仅限于几个值；主存储器<em class="mq"/>(例如 RAM)——速度更快，有足够的存储空间来容纳运行程序的指令和数据，以及<em class="mq">外部存储器</em>(例如硬盘)——速度较慢，但有空间存储计算机使用的几乎所有数据。</p><p id="acce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">记忆增强神经网络(MANNs) </strong>是冯诺依曼架构的<em class="mq">可微分</em>版本(下一节将详细介绍)。神经网络的主体可以被认为是 CPU。某些架构，如 RNNs(递归神经网络),具有类似于寄存器的内置存储器，存储短期信息。<em class="mq">神经存储器</em>与模型参数的其余部分分离，并且与 RAM 类似，存储长期信息。它由内存插槽阵列(即矩阵)组成，最常见的是存储信息的连续表示(文本、图像等)。)</p><figure class="ms mt mu mv gt mf gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f63767eea705324733b8d20b81c3d286.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ERikO9Hvz-g8T_yzRrx02g.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">神经记忆与外部世界的相互作用由控制器调节。图 1 中的<a class="ae ln" href="https://arxiv.org/pdf/1410.5401.pdf" rel="noopener ugc nofollow" target="_blank"> Graves 等人</a>【1】</p></figure><p id="ba59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过读写操作直接与神经存储器交互的组件称为<strong class="kk iu">控制器</strong>。在早期的工作中，控制器与模型的其余部分(即记忆之外的所有参数)相吻合，因此它充当了记忆与“外部世界”之间的接口。它通常被实现为一个递归神经网络。最近，随着基于变压器的大规模架构的出现，控制器只是该模型的一个小子集，并协调存储器与网络其余部分之间的通信。</p><h2 id="1c09" class="mw mx it bd my mz na dn nb nc nd dp ne kr nf ng nh kv ni nj nk kz nl nm nn no bi translated">什么是可区分的架构？</h2><p id="794b" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">曼恩是可微分的，冯诺依曼架构不是——但这到底意味着什么？您可能还记得以下定义:</p><blockquote class="nu nv nw"><p id="7bd9" class="ki kj mq kk b kl km ju kn ko kp jx kq nx ks kt ku ny kw kx ky nz la lb lc ld im bi translated">实变量的<strong class="kk iu">可微函数</strong>是其导数存在于其定义域内每一点的函数——<a class="ae ln" href="https://en.wikipedia.org/wiki/Differentiable_function" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p><p id="59e0" class="ki kj mq kk b kl km ju kn ko kp jx kq nx ks kt ku ny kw kx ky nz la lb lc ld im bi translated">实变量的函数的<strong class="kk iu">导数衡量函数值(输出值)对其自变量(输入值)变化的敏感度— <a class="ae ln" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</strong></p></blockquote><figure class="ms mt mu mv gt mf gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ba2e608f5af426bebb9f2bfbd942a853.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*dJgQrU1O7EnWQgAjk1XKbw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">函数在特定点的导数。图片来自<a class="ae ln" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></figure><p id="c2f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">冯·诺依曼体系结构执行不可微运算。例如，考虑一个<em class="mq">读</em>操作:当 CPU 从 RAM 中取出下一条指令时，它指定一个地址(输入)并接收回一条指令(输出)。因此，输入域是无符号整数，所以运算不是在<em class="mq">实数</em>变量上定义的。根据上面的定义，微分是不可能的。</p><h2 id="b877" class="mw mx it bd my mz na dn nb nc nd dp ne kr nf ng nh kv ni nj nk kz nl nm nn no bi translated">使操作可区分:<em class="ly">软</em>读取和写入</h2><p id="9c4b" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">RAM 读取不可区分的核心原因是它们在离散的地址空间上操作。神经记忆提出了一种调整:</p><blockquote class="nu nv nw"><p id="1d7d" class="ki kj mq kk b kl km ju kn ko kp jx kq nx ks kt ku ny kw kx ky nz la lb lc ld im bi translated">不是从单个条目中读取，而是从所有条目中执行加权读取。</p></blockquote><p id="23a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个存储槽<em class="mq"> i </em>，控制器指定一个实值权重<em class="mq"> wᵢ </em>，使得所有权重总和为 1。这将读取操作的输入从单个<em class="mq">整数</em>值(地址)改变为一个<em class="mq">实数</em>值的向量(每个插槽的权重)，这是区分性的第一个要求。请注意，这个修改后的操作更加通用:当一个权重设置为 1.0，所有其他权重设置为 0.0 时，我们实际上是从一个条目中读取。同样的推理也适用于写操作:我们用一个加权值<em class="mq"> wᵢ * x. </em>来更新每个条目<em class="mq"> i </em>，而不是将值<em class="mq"> x </em>写入单个内存槽</p><p id="53ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于权重<em class="mq"> wᵢ.的连续性质，这些操作被称为<strong class="kk iu"> <em class="mq">软</em>读和写</strong></em>这与<em class="mq">硬</em>读写 RAM 形成对比，在硬读写 RAM 时，控制器会对要操作的内存插槽做出硬决定。</p><h2 id="141c" class="mw mx it bd my mz na dn nb nc nd dp ne kr nf ng nh kv ni nj nk kz nl nm nn no bi translated">计算软权重:基于内容与基于位置的寻址</h2><blockquote class="lo"><p id="128f" class="lp lq it bd lr ls lt lu lv lw lx ld dk translated">RAM 的访问基于位置—读操作指定要读取的确切地址。神经存储器通常是基于内容来访问的——查询指定要读取什么，而不是从哪里读取。</p></blockquote><p id="d543" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">控制器如何计算每个插槽的重量<em class="mq"> wᵢ </em>？</p><p id="7da7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，关于术语的说明:计算权重的机制<em class="mq"> wᵢ </em>通常被称为<em class="mq">内存寻址，</em>因为它决定了哪些内存插槽被<em class="mq">寻址</em>，以及对每个插槽的关注程度。可以基于内容或位置对神经存储器进行寻址。</p><figure class="ms mt mu mv gt mf gh gi paragraph-image"><div class="gh gi og"><img src="../Images/05ef7bd800b0c129385793c4382959ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*q1_EYq4jRqRmIxYnM9R6jA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者插图。</p></figure><p id="8f8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用<strong class="kk iu">基于内容的寻址</strong>，权重<em class="mq"> wᵢ </em>反映了槽<em class="mq"> i </em>的内容在解析传入查询时的相关程度。例如，对于问答任务，内存查询可以是实际问题的嵌入。然后，控制器必须向上移动作为候选答案的内存插槽。最常见的是，<em class="mq"> wᵢ </em>是槽<em class="mq"> i </em>中的内容嵌入和查询之间的点积或余弦相似性。最后，通过 softmax 对所有权重进行归一化，使其总和为 1。</p><p id="9d29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用<strong class="kk iu">基于位置的寻址<em class="mq">、</em>、</strong>，权重<em class="mq">、</em>反映了应该对位置<em class="mq"> i </em>给予多少关注，而不管其内容如何。这种技术不太常见；它是由谷歌 DeepMind 的神经图灵机[1]在 2014 年推出的，然后在他们的 2016 年迭代 MANNs [3]中放弃了。最初的理由是，对于某些需要算术运算的任务，如添加两个变量<em class="mq"> x </em>和<em class="mq"> y </em>，重要的是控制器能够从内存中检索操作数<em class="mq"> x </em>和<em class="mq"> y </em>而不管它们的确切值。</p><h2 id="3609" class="mw mx it bd my mz na dn nb nc nd dp ne kr nf ng nh kv ni nj nk kz nl nm nn no bi translated">降低计算成本:稀疏读写</h2><p id="a76a" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">使读取和写入可区分会带来计算成本。现在，每个查询都在线性时间<em class="mq"> O(N) </em>内解决，其中<em class="mq"> N </em>是内存槽的数量(相比之下，硬读取和写入需要常数<em class="mq"> O(1) </em>时间)。当网络的输入是长度为<em class="mq"> L </em>的序列(例如文本文档)时，通常对序列中的每个元素进行一次查询——这使得成本高达<em class="mq"> O(N*L)。</em>训练时，软读写也是内存低效的；计算整个内存的梯度需要复制一份。</p><p id="45a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后续研究集中在将<em class="mq"> O(N) </em>成本降低到<em class="mq"> O(log N) </em> (Rae 等人【2】)或<em class="mq"> O(sqrt N) </em> (Lample 等人【4】)。虽然这两种方法非常不同，但它们的共同点是对内存的子集进行操作，而不是对所有条目进行操作。换句话说，他们将非零权重<em class="mq"> wᵢ </em>的数量限制为一个小常数 k(介于 2 和 8 之间)，并且仅在非零权重的槽上应用梯度下降。</p><h1 id="aa50" class="oh mx it bd my oi oj ok nb ol om on ne jz oo ka nh kc op kd nk kf oq kg nn or bi translated">当前事态</h1><p id="9658" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">记忆增强的神经网络已经在人工任务(例如，它们学习将序列复制给定次数)、一些自然语言任务(问答、机器翻译)和一些计算机视觉任务(字符识别)中显示出有希望的结果。然而，它们尚未成为主流。在多个方向都有有趣的研究机会:降低它们的计算成本，加快训练速度，了解它们在什么情况下最有用，以及将它们与最先进的变压器集成。</p><h1 id="c86c" class="oh mx it bd my oi oj ok nb ol om on ne jz oo ka nh kc op kd nk kf oq kg nn or bi translated">参考</h1><ol class=""><li id="0532" class="os ot it kk b kl np ko nq kr ou kv ov kz ow ld ox oy oz pa bi translated">格雷夫斯等人，<a class="ae ln" href="https://arxiv.org/pdf/1410.5401.pdf" rel="noopener ugc nofollow" target="_blank">神经图灵机</a> (2014)</li><li id="5883" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">Rae 等人，<a class="ae ln" href="https://arxiv.org/pdf/1610.09027.pdf" rel="noopener ugc nofollow" target="_blank">利用稀疏读写扩展记忆增强神经网络</a> (2016)</li><li id="1d6f" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">桑托罗等人，<a class="ae ln" href="https://arxiv.org/pdf/1605.06065.pdf" rel="noopener ugc nofollow" target="_blank">使用记忆增强神经网络的一次性学习</a> (2016)</li><li id="855b" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">Lample 等人，<a class="ae ln" href="https://arxiv.org/pdf/1907.05242.pdf" rel="noopener ugc nofollow" target="_blank">带有产品密钥的大型存储层</a> (2019)</li></ol></div></div>    
</body>
</html>