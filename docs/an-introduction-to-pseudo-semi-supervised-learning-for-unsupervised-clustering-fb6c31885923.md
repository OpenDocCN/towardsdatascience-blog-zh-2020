# 用于无监督聚类的伪半监督学习介绍

> 原文：<https://towardsdatascience.com/an-introduction-to-pseudo-semi-supervised-learning-for-unsupervised-clustering-fb6c31885923?source=collection_archive---------29----------------------->

这篇文章概述了我们基于深度学习的技术，通过利用半监督模型来执行无监督聚类。获取未标记的数据集，并且使用以完全无监督的方式生成的伪标签来标记该数据集的子集。伪标记数据集结合完整的未标记数据用于训练半监督模型。

这是原帖的转载:[https://divamgupta . com/unsupervised-learning/2020/10/31/pseudo-semi-supervised-learning-for-unsupervised-clustering . html](https://divamgupta.com/unsupervised-learning/2020/10/31/pseudo-semi-supervised-learning-for-unsupervised-clustering.html)

这项工作于 2020 年在 ICLR 发表，论文可以在[这里](https://openreview.net/pdf?id=rJlnxkSYPS)找到，源代码可以在[这里](https://github.com/divamgupta/deep_clustering_kingdra)找到。

# 介绍

在过去的 5 年中，一些方法在半监督分类中取得了巨大的成功。当给定大量未标记数据和少量标记数据时，这些模型工作得非常好。

未标记的数据有助于模型发现数据集中的新模式，并学习高级信息。标记的数据有助于模型使用学习到的信息对数据点进行分类。例如，梯形网络可以产生 98%的测试准确度，只需标记 100 个数据点，其余的不标记。

为了使用半监督分类模型进行完全无监督的聚类，我们需要以某种方式以完全无监督的方式生成少量的标记样本。这些自动生成的标签称为伪标签。

具有用于训练半监督模型的高质量伪标签是非常重要的。如果标签中有大量噪声，分类性能会下降。因此，假设伪标签中的噪声更少，我们可以接受更少数量的伪标签数据点。

下面是一种简单的方法:

1.  从一个未标记的数据集开始。
2.  获取数据集的子集并为其生成伪标签，同时确保伪标签的质量良好。
3.  通过将完整的未标记数据集与小的伪标记数据集相结合来训练半监督模型。

![](img/73d7c2b7dc7422c6b54eff400db7e22a.png)

作者图片

> *这种方法使用了半监督学习的一些元素，但没有使用实际的标记数据点。因此，我们称这种方法为伪半监督学习。*

# 生成伪标签

生成高质量的伪标签是获得良好的总体聚类性能的最棘手也是最重要的一步。

生成伪标签数据集的简单方法是

1.  对整个数据集运行标准聚类模型，并使伪标签等于模型中的聚类 id。
2.  运行一个标准的集群模型，其中的集群数量远远超过所需数量。然后只保留几个聚类来标记相应的数据点，而丢弃其余的。
3.  运行一个标准的聚类模型，只保留模型的置信度大于某个阈值的数据点。

实际上，上述方法都行不通。

第一种方法是没有用的，因为伪标签只是由标准聚类模型返回的聚类，因此我们不能期望半监督模型比这表现得更好。

第二种方法不起作用，因为没有好的方法来选择不同的集群。

第三种方法行不通，因为在实践中，单个模型的置信度并不是质量的指标。

在试验了几种生成伪标记数据集的方法后，我们观察到多个无监督聚类模型的一致性通常是质量的良好指标。单个模型的聚类并不完美。但是如果大量的聚类模型将数据集的子集分配到同一个聚类中，那么它们实际上很有可能属于同一个类。

在下图中，两个模型的聚类分配的交叉点上的数据点可以被分配以高置信度的相同伪标签。伪标签子集中的 Rest 可以忽略。

![](img/4ac9059d815b4c7f16de51b654c2af94.png)

作者图片

# 使用图来生成伪标签

有一种更正式的方法来生成伪标签数据集。我们首先构建所有数据点的图，对模型的成对一致性进行建模。

该图包含两种类型的边。

1.  强正边缘—当大部分模型认为两个数据点应该在同一个聚类中时
2.  强负边缘—当大部分模型认为两个数据点应该在不同的聚类中时。

可能在两个数据点之间既没有强的正边缘也没有强的负边缘。这意味着这些数据点的聚类分配的置信度很低。

在构建该图之后，我们需要挑选 K 个小聚类，使得一个聚类内的数据点与强正边相连，而不同聚类的数据点与强负边相连。

图表示例如下:

![](img/3ead9ce9adcb7b986729d4ad0ba757fb.png)

*构造图的例子。强正边缘—绿色，强负边缘—红色。图片作者*

我们首先选择具有最大数量的强正边的节点。在示例中，该节点被圈起来:

![](img/7eb5fc1e985799048afb24348ff1c57f.png)

*选中的节点被圈起来。*作者图片

然后，我们将伪标签分配给连接到具有强正边的所选节点的邻居:

![](img/aa8a7f7da58e235af872474e03f8b333.png)

作者图片

既没有与强正边也没有与强负边连接的节点被移除，因为我们不能以高置信度分配任何标签:

![](img/3fdfd10e22ab41669e300ca68e6b7af9.png)

作者图片

然后，我们重复步骤 K 次以上，以获得 K 个小型集群。一个小型集群中的所有数据点被分配相同的伪标签:

![](img/1f581071370aff4a6c1907cbfe7ef124.png)

*最终伪标签子集。*作者图片

我们可以看到，在这一步中，许多数据点将被丢弃，因此，将这些伪标记的数据点发送到半监督学习模型进行下一步是理想的。

# 使用伪标签训练半监督模型

现在，我们有了一个经过修剪的伪标记数据集以及完整的未标记数据集，用于训练半监督分类网络。网络的输出是一个软最大化的向量，它可以被看作是聚类分配。

如果伪标签具有良好的质量，那么与单独的聚类模型相比，这种多阶段训练产生更好的聚类性能。

我们可以有一个能够执行无监督聚类和半监督分类的单一模型，而不是有单独的聚类和半监督模型。实现这一点的一种简单方法是使用通用的神经网络架构，并应用聚类损失和半监督分类损失。

我们决定使用结合信息最大化损失的半监督梯形网络进行聚类。你可以在这里阅读更多关于不同深度学习聚类方法[。](https://divamgupta.com/unsupervised-learning/2019/03/08/an-overview-of-deep-learning-based-clustering-techniques.html)

# 把所有东西放在一起

在第一阶段，仅应用聚类损失。在获得伪标签之后，聚类和分类损失都被应用于模型。

在半监督训练之后，我们可以使用更新的模型提取更多的伪标记数据点。生成伪标签和半监督训练的过程可以重复多次。

整体算法如下:

1.  使用聚类损失训练多个独立模型
2.  构建模型成对一致的图形模型
3.  使用该图生成伪标签数据
4.  通过应用聚类和分类损失，使用未标记+伪标记数据训练每个模型
5.  从步骤 2 开始重复

![](img/1ae8ae2cb995f1fe24df1736b911d384.png)

*最终系统概述。*作者图片

# 估价

我们希望我们的聚类接近真实标签。但是因为该模型是以完全无监督的方式训练的，所以没有基础事实类和聚类的固定映射。因此，我们首先找到地面真实与具有最大重叠的模型集群的一对一映射。然后，我们可以应用准确性等标准指标来评估集群。这是一个非常标准的集群定量评估指标。

我们可以通过从最终聚类中随机采样图像来可视化聚类。

![](img/a9f10a0ba2af448e3a810041566d905b.png)

*可视化 MNIST 数据集的聚类。来源:原始论文。*

![](img/81f37c60fadaa9172fd8d95f0619bb96.png)

*可视化 CIFAR10 数据集的聚类。来源:原始论文。*

在这篇文章中，我们讨论了一种基于深度学习的技术，通过利用伪半监督模型来执行无监督聚类。这种技术优于其他几种基于深度学习的聚类技术。如果你有任何问题或想建议任何改变，请随时联系我或在下面写评论。

**从** [**这里**](https://github.com/divamgupta/deep-clustering-kingdra) 获取完整源代码

*原载于 2020 年 10 月 31 日 https://divamgupta.com*[](https://divamgupta.com/unsupervised-learning/2020/10/31/pseudo-semi-supervised-learning-for-unsupervised-clustering.html)**。**