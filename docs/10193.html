<html>
<head>
<title>Polynomial Regression — which python package to use?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多项式回归-使用哪个 python 包？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-which-python-package-to-use-78a09b0ac87b?source=collection_archive---------21-----------------------#2020-07-18">https://towardsdatascience.com/polynomial-regression-which-python-package-to-use-78a09b0ac87b?source=collection_archive---------21-----------------------#2020-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ce1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 numpy、scipy、sklearn、tensorflow 表达的“Hello world”。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/250b3e4ea46569be95e67cf199467a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_eBaDIyhVMt1Li8Af8OfQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“协同捕鱼”。葡萄牙<a class="ae ky" href="https://private.zerowithdot.com/travelling/" rel="noopener ugc nofollow" target="_blank"> 2019 </a>。(作者私人收藏)</p></figure><h1 id="abfb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="f09a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">多项式回归是数据分析和预测中最基本的概念之一。不仅任何(无限可微的)函数至少在一定区间内可以通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Taylor_series" rel="noopener ugc nofollow" target="_blank">泰勒级数</a>表示为多项式，这也是机器学习初学者首先面临的问题之一。它被用于各种学科，如金融分析、信号处理、医学统计等。</p><p id="3b1b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然多项式回归是一个相对简单的概念，但它成为了机器学习中的一种“hello world”问题，因为它涉及到许多核心概念。尽管如此，有些部分可能听起来令人困惑，尤其是因为:</p><ul class=""><li id="efc1" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">它一次引入了几个东西，尽管只有一些是多项式回归特有的。</li><li id="9878" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">它通常以多种不同的方式实现。</li></ul><p id="e909" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇博文中，我们将使用这两个方程从头构建它，并通过代码来表达它们。然后，我们将我们的实现与数据科学中最广泛使用的四个 python 库进行比较:</p><ul class=""><li id="eb10" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">numpy，</li><li id="91e5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">暴躁，</li><li id="0524" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">scikit-learn，</li><li id="951e" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">张量流，</li></ul><p id="bf06" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并讨论它们之间的差异，同时指出基本层面上的相似之处。</p><h1 id="97cc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">问题是</h1><p id="8bcb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">先讨论数学基础。</p><p id="d241" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从数学上讲，<em class="ng">回归</em>的问题是试图对自变量<em class="ng"> x </em>和因变量<em class="ng"> y </em>之间的关系进行建模。假设<em class="ng"> y' </em> s 依赖于<em class="ng"> x </em>用以下形式表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b59ccef32bc1aca0fb8ef48598dbffdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*A2cXdk9lY3iVDNgaCu8PUg.png"/></div></figure><p id="b8e7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们称之为<em class="ng">多项式回归</em>(ε表示噪声项)。自然，如果最大值<em class="ng"> n = 1 </em>，问题就变成了<em class="ng">线性回归</em>。</p><p id="3115" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">解决问题意味着确定所有<em class="ng">和</em> ₙ's 的值来很好地表示数据。它可以归结为以下步骤的要素:</p><ul class=""><li id="88db" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">关于<em class="ng"> y </em>形式的一个“猜想”。它叫做一个<em class="ng">假设</em>函数<em class="ng"> h(x) </em>。</li><li id="8215" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">在给定真实数据值的情况下，衡量假设有多“差”的一种方法。使用所谓的<em class="ng">损失</em>函数<em class="ng"> J </em>来量化这种“不良”。</li><li id="63cb" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">关于我们如何潜在地“调整”假设，使我们更好地逼近数据点的想法。换句话说，我们需要有某种<em class="ng">最小化</em> <em class="ng"> J </em>的方法。</li></ul><h1 id="b0a5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">选择假设</h1><p id="a564" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">说到多项式回归，我们首先需要假设的是我们将用作假设函数的多项式的<em class="ng">次。如果我们选择成为度，假设将采取以下形式:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2eb6da65122b3ba8ee5ba47c4b5ebb60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*SkMUmTG22pYEP82IYQdoAA.png"/></div></figure><h1 id="7c27" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">成本函数和均方误差</h1><p id="97f0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">接下来，我们选择用来表示成本函数的度量。对于回归问题，选择所谓的<em class="ng">均方误差(MSE) </em>是有意义的，它衡量单个点与我们的预测之间的平均距离，达到二次幂。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f568eaff9c2c0e7d41ab1c76631b022d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*zmc5XYsneIXxsRzdFyA2zg.png"/></div></figure><p id="1f79" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里<em class="ng"> m </em>与数据点个数相关，为了方便引入<em class="ng"> 1 </em> /2 因子(见后)。我们还将所有参数表示为参数向量<em class="ng"> θ = (θ₁、θ₂、……、θₙ) </em>。</p><p id="fd71" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">MSE 指标只是一种选择，但由于以下原因，它是合理的:</p><ul class=""><li id="522d" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">MSE 在误差方向上是不可知的(例如不像<em class="ng">平均误差</em>在一次幂中只与<em class="ng"> h-y </em>成比例)。</li><li id="38da" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">MSE 会因二次幂而严重惩罚较大差异(异常值)(例如，与 abs 成比例的<em class="ng">平均绝对误差</em>(<em class="ng">h-</em>y))不同。</li><li id="7197" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">对于均方误差，更容易获得梯度(例如不像<em class="ng">均方根误差</em>那样为ìMSE)。</li></ul><h1 id="43a3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降</h1><p id="c5d3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦我们建立了成本函数并设置了一些随机的初始值<em class="ng"> θ </em>，我们就希望最小化<em class="ng"> J </em>。其中一个可能的选择是<em class="ng">梯度下降</em>优化算法。顾名思义，它要求我们计算<em class="ng"> ∇J </em>并随后相应地更新所有<em class="ng"> θ </em>。</p><p id="de40" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">换句话说，我们需要计算每一个<em class="ng"> θ </em> ₖ的<em class="ng"> J </em>的导数，并用它们来改变<em class="ng"> θ </em>的值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/314abf0b025d5cb411a6319e7b869850.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*2dhsuyInjs7Lf9PErZjGJg.png"/></div></figure><p id="a6cb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从而使得假设越来越好地表示数据。</p><p id="b58f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">幸好<em class="ng"> d </em> J/d <em class="ng"> θ </em>很容易计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/48bea814008af7eeaa863ff9c7e81934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3IdmcaV3_nxvmcWbvY8jA.png"/></div></div></figure><p id="8848" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后一项是假设函数相对于<em class="ng"> θ </em>的导数。因为我们假设它是一个多项式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1b131e26eedfbfd8392df5bee6addc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*bigw94oeenYXNYJlulXCuw.png"/></div></figure><p id="39f9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">整个梯度变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3c1e1dd0f54975a9e2457f0d3af6afbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*hkDH2-c1SS3Aj8DUiW88TQ.png"/></div></figure><p id="de96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们把所有的<em class="ng"> x </em>组织成一个例子向量，我们可以用向量矩阵乘法来表示上面的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b55dd5b0a8bedae739ed6382aa321335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*4RV9lHiWN5Xk22LVbVT7iQ.png"/></div></figure><p id="7e02" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">或者以更紧凑的形式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2c0a73b3105eb989f995e7564f3536ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*sAw28_qVoZ9Vl8rEjHqVSQ.png"/></div></figure><p id="453e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里<strong class="lt iu">、T45】 X 、T47】表示由上升到连续幂的示例向量组成的矩阵<em class="ng"> j = 0、1、…、n </em>。</strong></p><h1 id="779b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">基本实现(幼稚)</h1><p id="422b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们称这种实现方式<em class="ng">为幼稚的</em>，因为它并没有针对性能进行优化。相反，重点是强调代码和基本方程之间的对应关系。</p><p id="8dd8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，我们将使用符号多项式的自定义实现(参见<a class="ae ky" href="https://gist.github.com/OlegZero13/8295d18d5339a96a8dee596d1189d0d6" rel="noopener ugc nofollow" target="_blank">要点</a>)。<code class="fe nq nr ns nt b">Polynomial</code>类基于多项式表达式(第一个等式)定义了一个可调用对象。它实现了许多类似于我们的<a class="ae ky" href="https://levelup.gitconnected.com/using-dunder-methods-to-refine-data-model-c58ee41102e9" rel="noopener ugc nofollow" target="_blank">四元数例子</a>的代数方法，但是现在最重要的部分是初始化。</p><p id="2c76" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们从创建一个具有随机系数<em class="ng"> θ </em>的<code class="fe nq nr ns nt b">Polynomial</code>对象开始。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="35a6" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>from nppoly import Polynomial<br/><br/>np.random.seed(42)<br/><br/>def hypothesis(degree):<br/>    return Polynomial(*np.random.rand(degree + 1))</span></pre><p id="8ee3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此方法接受并返回一个具有随机系数的多项式。接下来，我们定义我们的 MAE 成本函数:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="4411" class="ny la it nt b gy nz oa l ob oc">def cost(h, X, y):<br/>    return 0.5 / len(y) * ((h(X) - y) ** 2).sum()</span></pre><p id="c943" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的<code class="fe nq nr ns nt b">len(y)</code>相当于和<code class="fe nq nr ns nt b">h</code>、<code class="fe nq nr ns nt b">X</code>和<code class="fe nq nr ns nt b">y</code>是假设，那么<strong class="lt iu"> x </strong>和<strong class="lt iu">y</strong>——自变量和值的向量。请注意，大写<code class="fe nq nr ns nt b">X</code>更多的是一种传统，以强调其类似矩阵的特征。然而，在我们的例子中，<code class="fe nq nr ns nt b">X</code>只与一维数据相关联。</p><p id="f741" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们需要表达<em class="ng"> J </em>的梯度。多亏了这个推导，我们已经知道了这个表达式:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="4788" class="ny la it nt b gy nz oa l ob oc">def grad_theta(h, X, y):<br/>    diff = (h(X) - y).reshape(-1, 1)<br/><br/>    X = X.reashape(1, -1)<br/>    X = list(map(lambda i: x ** i, reversed(range(h.shape))))<br/>    X = np.concatenate(X)<br/><br/>    return 1 / len(y) * (X @ (diff)).reshape(1, -1)</span></pre><p id="7e7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个函数中，我们需要执行几次整形，以确保我们可以将矩阵乘以一个向量。第 4-6 行负责构造<strong class="lt iu"> X </strong>，<code class="fe nq nr ns nt b">reversed</code>函数用于遵守<em class="ng"> θ </em> ₙ站在<em class="ng"> θ </em> ₙ₋₁.之前的约定</p><p id="4e76" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，使用以下函数完成优化例程:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="9a4b" class="ny la it nt b gy nz oa l ob oc">def optimize(h, X, y, epochs=5000, lr=0.01):<br/>    theta = h._coeffs.reshape(1, -1)<br/>    for epoch in range(epochs):<br/>        theta -= lr * grad_theta(h, X, y)<br/>        h = Polynomial(*theta.flatten())<br/>    return h</span></pre><p id="2f7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里，第 2 行只是假设的初始化。然后，在 for 循环内部，我们执行<em class="ng"> θ ←θ - α∇J </em>，其中<em class="ng"> α </em>是所谓的学习率<code class="fe nq nr ns nt b">lr</code>，重复周期传统上称为“epochs”。</p><p id="ef71" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以通过执行一个小脚本来验证优化:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="56e3" class="ny la it nt b gy nz oa l ob oc"># fake data<br/>X = np.linspace(0, 10, num=5)<br/>y = 4 * X - 2 + np.random.randn(len(X))<br/><br/>h = hypothesis(1)<br/>h = optimize(h, X, y)<br/><br/># prediction<br/>X_test = np.linspace(-5, 25, num=31)<br/>y_pred = h(X_test)</span></pre><h1 id="6712" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python 库</h1><h2 id="eea1" class="ny la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">Numpy</h2><p id="456e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一个实现多项式回归的库是<a class="ae ky" href="https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html" rel="noopener ugc nofollow" target="_blank"> numpy </a>。它使用<code class="fe nq nr ns nt b">numpy.polyfit</code>函数来执行此操作，该函数给出数据(<code class="fe nq nr ns nt b">X</code>和<code class="fe nq nr ns nt b">y</code>)以及度数，执行该过程并返回系数数组<em class="ng"> θ </em>。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="e1a1" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>from numpy import polyfit<br/><br/><br/># fake data<br/>X = np.linspace(0, 10, num=5)<br/>y = 4 * X - 2 + np.random.randn(X)<br/><br/>u = polyfit(X, y, deg=1)<br/><br/># prediction<br/>X_test = np.linspace(-5, 25, num=31)<br/>y_pred = u[0] * X_test + u[1]</span></pre><p id="fa4b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果<code class="fe nq nr ns nt b">full</code>设置为<code class="fe nq nr ns nt b">True</code>，该功能将提供额外的诊断，为我们提供与不确定性相关的信息。</p><h2 id="edfd" class="ny la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">Scipy</h2><p id="e1fc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">解决多项式回归问题的另一个“配方”是<a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html" rel="noopener ugc nofollow" target="_blank"> scipy </a>中的<code class="fe nq nr ns nt b">curve_fit</code>。与<code class="fe nq nr ns nt b">polyfit</code>相比，该函数更加通用，因为它不需要我们的“模型”采用多项式形式。它的界面也不一样。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="1817" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>from scipy.optimize import curve_fit<br/><br/><br/>def linear_function(x, a, b):<br/>    return a * x + b<br/><br/>def quadratic_function(x, a, b, c):<br/>    return a * x**2 + b * x + c<br/><br/><br/># fake data<br/>X = np.linspace(0, 10, num=5)<br/>y1 = 4 * X - 2 + np.random.randn(len(X))<br/>y2 = 5 * X ** 2 - 3 * X + 10 + np.random.randn(len(X))<br/><br/>u = curve_fit(linear_function, X, y1)[0]<br/>v = curve_fit(quadratic_function, X, y2)[0]<br/><br/># prediction<br/>X_test = np.linspace(-5, 25, num=31)<br/>y_pred_1 = linear_function(X_test, u[0], u[1])<br/>y_pred_2 = quadratic_function(X_test, v[0], v[1], v[2])</span></pre><p id="ac93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与<code class="fe nq nr ns nt b">polyfit</code>相反，这个函数首先需要一个模型函数作为参数传入。它可以是任何参数化的数学公式，然而，<code class="fe nq nr ns nt b">curve_fit</code>强加了一个条件:模型函数本身接受数据<code class="fe nq nr ns nt b">x</code>作为它的第一个参数。最后，它返回优化的系数，类似于<code class="fe nq nr ns nt b">polyfit</code>，尽管它也返回诊断信息(因此在最后用<code class="fe nq nr ns nt b">[0]</code>来抑制它)。</p><h2 id="01ed" class="ny la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">sci kit-学习</h2><p id="7775" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就机器学习而言，Scikit-learn (或 sklearn)是一个“首选”库。它非常注重接口的一致性，这意味着它试图使用相同的方法(如<code class="fe nq nr ns nt b">.fit</code>、<code class="fe nq nr ns nt b">.transform</code>、<code class="fe nq nr ns nt b">.fit_transform</code>和<code class="fe nq nr ns nt b">.predict</code>)来统一对不同特性和算法的访问。求解线性回归解是相当容易的，然而，多项式回归的情况需要一点思考。</p><p id="5bd8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们从线性回归开始。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="1c29" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>from sklearn.linear_model import LinearRegression<br/><br/><br/># fake data<br/>X = np.linspace(0, 10, num=5).reshape(-1, 1)<br/>y = 4 * X - 2 + np.random.randn(len(X))<br/><br/>linreg = LinearRegression()<br/>linreg.fit(X, y)<br/><br/># prediction<br/>X_test = np.linspace(-5, 25, num=31).reshape(-1, 1)<br/>y_pred = linreg.predict(X_test)</span></pre><p id="708e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里，解决方案是通过<code class="fe nq nr ns nt b">LinearRegression</code>对象实现的。按照 scikit-learn 的逻辑，我们首先使用<code class="fe nq nr ns nt b">.fit</code>方法将对象调整到我们的数据，然后使用<code class="fe nq nr ns nt b">.predict</code>呈现结果。注意<code class="fe nq nr ns nt b">X</code>需要被重新整形为一个<code class="fe nq nr ns nt b">(m, 1)</code>向量列。</p><p id="988c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了解决更一般的多项式回归情况，我们需要将<code class="fe nq nr ns nt b">LinearRegression</code>与<code class="fe nq nr ns nt b">PolynomialFeatures</code>对象结合起来，因为没有直接的解决方案。事实上，sklearn 的构建意图是处理与数据相关的问题，而不是成为一个方程求解器。更多的差异可以在这个<a class="ae ky" rel="noopener" target="_blank" href="/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2">帖子</a>中找到。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="7e60" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>from sklearn.preprocessing import PolynomialFeatures<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.pipeline import make_pipeline<br/><br/># fake data<br/>X = np.linspace(0, 10, num=5).reshape(-1, 1)<br/>y = 5 * X ** 2 - 3 * X + 10 + np.random.randn(len(X))<br/><br/>polyreg = make_pipeline(<br/>        PolynomialFeatures(degree=2),<br/>        LinearRegression()<br/>        )<br/>polyreg.fit(X, y)<br/><br/># prediction<br/>X_test = np.linspace(-5, 25, num=31).reshape(-1, 1)<br/>y_pred = polyreg.predict(X_test)</span></pre><p id="3b40" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">起点是<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener ugc nofollow" target="_blank"> PolynomialFeatures </a>类，它基于通用变量和创建“混合”术语。这里，我们把这个变换应用到，这个获得一个特征向量(矩阵)(1，<strong class="lt iu"> x </strong>，<strong class="lt iu"> x </strong>)，因为<em class="ng"> n = 2 </em>，并且<strong class="lt iu"> x </strong>是一维的。然后，使用 sklearn 的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html" rel="noopener ugc nofollow" target="_blank">管道</a>，我们将<strong class="lt iu"> x </strong>与线性系数<em class="ng"> θ </em>结合起来，基本上将每个<strong class="lt iu"> x </strong>视为一个单独的变量。最后，我们解决它，就像我们面对标准的线性回归问题，获得<em class="ng"> θ </em>。</p><p id="0dcc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到这里采用的方法与 numpy 和 scipy 都有很大的不同。因为 sklearn 更多地从连续调整(<code class="fe nq nr ns nt b">fit</code>)、数据转换(这里不需要)和预测(<code class="fe nq nr ns nt b">predict</code>)的角度“看待”问题。无论哪种方式，结果都是一样的。</p><h1 id="ee8e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">张量流</h1><p id="e57c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们在这里介绍的最后一个软件包是<a class="ae ky" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow 2.0 </a>。由于在 2.0 版本中引入的 API 已经有了实质性的变化，我们不打算呈现 1.x 版本中提供的解决方案。但是，如果你感兴趣，请参考 trần ngọc·明的这篇精彩的<a class="ae ky" href="https://www.codeproject.com/Tips/5263107/Updating-Linear-Regression-Model-To-TensorFlow-2-0" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p><p id="1c1a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">流程的外观与我们最初的实现非常相似。事实上，在引擎盖下，它非常不同。我们先来看看下面的片段:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="282e" class="ny la it nt b gy nz oa l ob oc">import numpy as np<br/>import tensorflow as tf<br/><br/><br/>LEARNING_RATE = 0.0001<br/>N_EPOCHS = 100000<br/>DEGREE = 2<br/><br/><br/>X = np.linspace(0, 10, num=5).reshape(-1, 1)<br/>y = 5*X**2 - 3*X + 10 + np.random.randn(len(X))<br/><br/>theta = tf.Variable([0.0] * (DEGREE + 1), name='parameters')<br/><br/><br/>def hypothesis(x):<br/>    return sum([theta[i] * x**i for i in range(DEGREE + 1)])<br/><br/><br/>def cost(y_pred, y):<br/>    return tf.reduce_mean(tf.square(y_pred - y))<br/><br/><br/>for epoch in range(N_EPOCHS):<br/>    with tf.GradientTape() as tape:<br/>        y_predicted = h(X)<br/>        cost_value = cost(y_predicted, y2)<br/>    gradients = tape.gradient(cost_value, theta)<br/>    theta.assign_sub(gradients * LEARNING_RATE)<br/><br/><br/>X_test = np.linspace(-5, 25, num=31)<br/>y_test = hypothesis(X_test).numpy()</span></pre><p id="ffa8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与假设和成本函数相关的逻辑是相同的。此外，使用相同的方法执行训练，只有函数名称不同。然而，事实是 tensorflow 试图通过构建所谓的图来象征性地解决问题。该图是所有数学依赖关系的表示，以便导数可以很容易地计算出来。例如，<code class="fe nq nr ns nt b">theta.assign_sub(...)</code>转化为用梯度更新<em class="ng"> θ </em>，而<code class="fe nq nr ns nt b">tf.</code>前缀函数是 numpy 已知函数的张量流符号对应物。</p><h1 id="667c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="80bd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们从数学和编程的角度重新审视了多项式回归问题。此外，我们使用数据项目中常用的四个 python 库比较了它的实现。我们还将它们与一个定制的、面向演示的实现进行了比较，讨论了它们的异同。</p><p id="7202" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你仍然不知道该选择哪一个，这里是我们的建议:</p><ul class=""><li id="68e9" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">除非你想拟合一个非多项式函数，否则就用 numpy。您的项目可能无论如何都需要它。</li><li id="ca18" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">如果您确实有一个更奇特的函数或不容易转换成多项式的函数，请使用 scipy。它的界面非常清晰，安装速度也非常快。</li><li id="a4f0" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">如果你用 sklearn 做一个更大的机器学习项目，并且你的一个步骤需要某种多项式回归，这里也有一个解决方案。尽管如此，仅仅为了进行回归而安装这个库是没有意义的。</li><li id="139d" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">这同样适用于 tensorflow。除非这不是一个需求，否则不要把事情复杂化。</li><li id="89a3" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">最后，说到复杂性，不要在你的实现上浪费时间——当然，除非你正在学习。</li></ul><p id="c732" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果这篇文章已经帮助你抓住了一个更大的画面，加深了你的理解，那是很好的。这意味着它的主要目标已经实现。无论哪种方式，欢迎评论或分享！</p><h1 id="48e0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">还会有更多…</h1><p id="f1b4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我计划把文章带到下一个层次，并提供简短的视频教程。</p><p id="b613" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您想了解关于视频和未来文章的更新，<strong class="lt iu">订阅我的</strong> <a class="ae ky" href="https://landing.mailerlite.com/webforms/landing/j5y2q1" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">简讯</strong> </a> <strong class="lt iu">。</strong>您也可以通过填写<a class="ae ky" href="https://forms.gle/bNpf9aqZJGLgaU589" rel="noopener ugc nofollow" target="_blank">表格</a>让我知道您的期望。回头见！</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="5e6e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ng">原载于</em><a class="ae ky" href="https://zerowithdot.com/polynomial-regression-in-python/" rel="noopener ugc nofollow" target="_blank"><em class="ng">https://zerowithdot.com</em></a><em class="ng">。</em></p></div></div>    
</body>
</html>