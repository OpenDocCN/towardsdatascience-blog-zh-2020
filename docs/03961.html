<html>
<head>
<title>Day 103 of #NLP365: NLP Papers Summary— Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 103 天:NLP 论文摘要——通过构造辅助句利用 BERT 进行基于方面的情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3?source=collection_archive---------22-----------------------#2020-04-12">https://towardsdatascience.com/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3?source=collection_archive---------22-----------------------#2020-04-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/61a393f717c1685f581a48076c22cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDhaS8ed285Bb9L80C5U0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jg" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="a460" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="fbca" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="3a46" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 100 天里学到了什么。</p><p id="8a83" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天的 NLP 论文是<strong class="lj jt"> <em class="md">通过构造辅助句</em> </strong>利用 BERT 进行基于方面的情感分析。以下是研究论文的要点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="c6c6" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">目标和贡献</h1><p id="d23d" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">针对基于方面的目标情感分析(TABSA)，微调预训练的 BERT。TABSA 是一项任务，通过这项任务，您可以针对与给定目标相关的特定方面识别出细粒度的意见极性。在基于方面的情感分析(ABSA)中，你没有目标-方面对，只有方面。本文的贡献在于:</p><ol class=""><li id="7fae" class="ni nj jj lj b lk ll ln lo lq nk lu nl ly nm mc nn no np nq bi translated">一种新的处理 TABSA 的方法，通过创建辅助句把它当作一个句子对分类任务</li><li id="dcad" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc nn no np nq bi translated">通过使用微调的 BERT 在 SentiHood 和 SemEval-2014 任务 4 数据集上实现 SOTA 结果</li></ol><h1 id="f4ef" class="ml mm jj bd mn mo nw mq mr ms nx mu mv ky ny kz mx lb nz lc mz le oa lf nb nc bi translated">数据集</h1><h2 id="3062" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">感知</h2><ul class=""><li id="b8d5" class="ni nj jj lj b lk nd ln ne lq om lu on ly oo mc op no np nq bi translated">5215 个句子，其中 3862 个包含单个目标，其余包含多个目标</li><li id="430f" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">每个句子包含一个带有情感极性的目标体对列表</li><li id="67bd" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">给定一个句子和目标，我们需要 a)检测对目标体的提及，b)确定检测到的目标体对的极性</li></ul><figure class="or os ot ou gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/81ef0aec20e7bc9330e0daf1b57aae9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*v9vZ7-WLd5ep9MXC.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">感知标签示例[1]</p></figure><h2 id="e3fe" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">塞姆瓦尔-2014 年任务 4</h2><p id="11e8" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这是 ABSA 而不是塔布萨，所以他们没有目标-方面对，只有方面。允许模型同时处理子任务 3(方面检测)和子任务 4(方面极性)。</p><h1 id="ea80" class="ml mm jj bd mn mo nw mq mr ms nx mu mv ky ny kz mx lb nz lc mz le oa lf nb nc bi translated">方法学</h1><figure class="or os ot ou gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f93f85983ecf48e71c1ef01acb6cc010.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/0*7W0h2zY_lcLoSJUH.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">所有助句方法概述</p></figure><p id="29a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本文采用四种方法构造辅助句，将 TABSA 转换为句子对分类任务。使用(位置 1，安全)作为目标-特征对示例:</p><h2 id="89d2" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">问答的句子</h2><ul class=""><li id="0c94" class="ni nj jj lj b lk nd ln ne lq om lu on ly oo mc op no np nq bi translated">使用目标-体对生成一个问题。格式必须保持不变</li><li id="7541" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">例如，“你认为 location-1 的安全性如何？”</li></ul><h2 id="ed30" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">对 NLI 的判决</h2><ul class=""><li id="5cdb" class="ni nj jj lj b lk nd ln ne lq om lu on ly oo mc op no np nq bi translated">这里创造的句子不是一个标准句子，而是一个含有目标体对的假句子</li><li id="7ce0" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">例如，“位置-1-安全”</li></ul><h2 id="9422" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">问答部分的句子</h2><ul class=""><li id="63c6" class="ni nj jj lj b lk nd ln ne lq om lu on ly oo mc op no np nq bi translated">为句子添加情感标签，并将其视为二元分类问题，将每个句子分类为是或否</li><li id="9378" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">每个目标-特征对将有三个新的序列。下面是一个例子:</li></ul><blockquote class="ow"><p id="3199" class="ox oy jj bd oz pa pb pc pd pe pf mc dk translated">位置-1 的安全方面的极性为正</p><p id="b3d9" class="ox oy jj bd oz pa pg ph pi pj pk mc dk translated">位置-1 的安全方面的极性为负</p><p id="4923" class="ox oy jj bd oz pa pg ph pi pj pk mc dk translated">位置-1 的特征安全性的极性为“无”</p></blockquote><ul class=""><li id="c22f" class="ni nj jj lj b lk pl ln pm lq pn lu po ly pp mc op no np nq bi translated">我们将具有最高匹配分数概率的类别作为预测类别</li></ul><h2 id="54ab" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">对 NLI 的判决-B</h2><ul class=""><li id="3f1f" class="ni nj jj lj b lk nd ln ne lq om lu on ly oo mc op no np nq bi translated">同样，对于 NLI-B，助动词句从真题变成伪句如下:</li></ul><blockquote class="ow"><p id="7f06" class="ox oy jj bd oz pa pb pc pd pe pf mc dk translated">位置— 1 —安全—积极</p><p id="2048" class="ox oy jj bd oz pa pg ph pi pj pk mc dk translated">位置-1-安全-负</p><p id="38b5" class="ox oy jj bd oz pa pg ph pi pj pk mc dk translated">位置-1-安全-无</p></blockquote><h1 id="dd23" class="ml mm jj bd mn mo nw mq mr ms nx mu mv ky pq kz mx lb pr lc mz le ps lf nb nc bi translated">实验和结果</h1><p id="5e21" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这里有两个实验设置，因为我们有两个不同的数据集。</p><h2 id="6f24" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">感知实验设置</h2><p id="b423" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">感知度的评估只考虑数据集中最常见的 4 个方面(一般、价格、交通位置、安全)。以下是选择用于比较的模型:</p><ul class=""><li id="02a6" class="ni nj jj lj b lk ll ln lo lq nk lu nl ly nm mc op no np nq bi translated"><strong class="lj jt"><em class="md"/></strong>。具有 n 元语法和位置标签功能的 LR</li><li id="e055" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated"><strong class="lj jt"><em class="md">LSTM——决赛</em> </strong>。以最终状态表示的 biLSTM</li><li id="94de" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated"><strong class="lj jt"><em class="md">LSTM-洛克</em> </strong>。以与目标位置相关联的状态作为表示</li><li id="8541" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated"><strong class="lj jt"> <em class="md"> LSTM+TA+SA </em> </strong>。具有复杂目标级和句子级注意机制的 biLSTM</li><li id="6c30" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated"><strong class="lj jt"> <em class="md"> SenticLSTM </em> </strong>。LSTM+TA+SA 升级版，引入 Sentic Net 外部信息</li><li id="a9d1" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated"><strong class="lj jt"> <em class="md"> Dmu-Entnet </em> </strong>。具有跟踪实体的延迟存储器更新机制的外部存储器链的双向 EntNet</li></ul><h2 id="9f56" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">感知结果</h2><figure class="or os ot ou gt iv gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/fbc45a3cd388110b111343dbafba9a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*me7xBfb6M8nkq8M6.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">感知结果——所选模型、BERT-single 和 BERT-pair 之间的比较[1]</p></figure><ul class=""><li id="f362" class="ni nj jj lj b lk ll ln lo lq nk lu nl ly nm mc op no np nq bi translated">BERT-single 在特征检测方面优于 Dmu-Entnet，但在情感分类准确性方面得分较低</li><li id="6b1b" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">BERT-pair 在特征检测和情感分类方面远远超过其他模型。伯特对问答模型往往在情感分析上表现更好，而伯特对 NLI 模型往往在方面检测上表现更好</li></ul><h2 id="e3b8" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">SemEval-2014 任务 4 实验设置</h2><p id="d780" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">BERT-pair 模型与性能最好的系统进行了比较，即 XRCE、NRC-Canada 和 ATAE-LSTM。</p><h2 id="8ff2" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">SemEval-2014 年任务 4 成果</h2><figure class="or os ot ou gt iv gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/544f243bb33d8afe7c5803c6eb06ab17.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/0*rRUXO2l6dNWu8WcR.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">SemEval-2014 任务 4 子任务 3(表 4)和子任务 4(表 5)之间的结果细分。子任务 3 是方面检测，子任务 4 是方面极性[1]</p></figure><ul class=""><li id="d109" class="ni nj jj lj b lk ll ln lo lq nk lu nl ly nm mc op no np nq bi translated">BERT-single 模型足以在两个子任务上取得更好的结果</li><li id="6efb" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">伯特对模型实现了对伯特单模型的进一步改进</li></ul><h1 id="357e" class="ml mm jj bd mn mo nw mq mr ms nx mu mv ky ny kz mx lb nz lc mz le oa lf nb nc bi translated">结论和未来工作</h1><p id="8e8b" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">为什么 BERT-pair 的实验结果好那么多？</p><ul class=""><li id="d041" class="ni nj jj lj b lk ll ln lo lq nk lu nl ly nm mc op no np nq bi translated">把目标体对转换成辅助句相当于扩大了语料库，因此，我们的模型有了更多的数据</li><li id="b07b" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">伯特似乎在问答和 NLI 任务中表现良好，表明在处理句子对分类方面的优势。这可能归因于无监督的掩蔽语言模型和下一句预测任务</li><li id="f350" class="ni nj jj lj b lk nr ln ns lq nt lu nu ly nv mc op no np nq bi translated">在 TABSA 上直接微调预训练的 BERT 不会产生好的结果。通过将 TABSA 转换为句子对分类任务，上下文现在类似于问答和 NLI，因此可以利用预训练 BERT 的优势</li></ul><p id="4736" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从单句分类到句子对分类任务的转换已经产生了强有力的结果。未来的工作可以将这种转换方法应用于其他类似的任务。</p><h2 id="7f45" class="ob mm jj bd mn oc od dn mr oe of dp mv lq og oh mx lu oi oj mz ly ok ol nb jp bi translated">来源:</h2><p id="4124" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">[1]孙，迟，，黄，邱希鹏."通过构造辅助句，利用 bert 进行基于方面的情感分析."<em class="md"> arXiv 预印本 arXiv:1903.09588 </em> (2019)。网址:【https://arxiv.org/pdf/1903.09588.pdf T2】</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="34a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【https://ryanong.co.uk】原载于 2020 年 4 月 12 日<a class="ae jg" href="https://ryanong.co.uk/2020/04/12/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-auxiliary-sentence/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div></div>    
</body>
</html>