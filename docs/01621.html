<html>
<head>
<title>Naive Bayes Classifier — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器-解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-classifier-explained-50f9723571ed?source=collection_archive---------8-----------------------#2020-02-14">https://towardsdatascience.com/naive-bayes-classifier-explained-50f9723571ed?source=collection_archive---------8-----------------------#2020-02-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="69e9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">scikit-learn 的理论和实现</h2></div><p id="5df6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">朴素贝叶斯是一种用于分类任务的监督学习算法。因此，它也被称为朴素贝叶斯分类器。</p><p id="f3d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与其他监督学习算法一样，朴素贝叶斯使用特征对目标变量进行预测。关键区别在于，朴素贝叶斯假设特征是相互独立的，特征之间没有相关性。然而，现实生活中并非如此。这种特征不相关的天真假设是这种算法被称为“天真”的原因。</p><p id="0bb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将首先介绍一些关于概率的基本概念，并展示朴素贝叶斯分类器的核心——贝叶斯定理是如何推导出来的。然后，我将展示朴素贝叶斯分类器如何建立在贝叶斯定理之上，以及朴素贝叶斯的优点/缺点及其在 scikit-learn 上的实现。</p><p id="1617" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">概要:</strong></p><ol class=""><li id="e9d9" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">概率和条件概率</li><li id="d20f" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">贝叶斯定理</li><li id="9cff" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">朴素贝叶斯分类器</li><li id="b432" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">朴素贝叶斯分类器的利与弊</li><li id="089c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Scikit-learn 实现</li></ol><h1 id="0000" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">概率和条件概率</h1><p id="1632" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">我们已经知道了“天真”的来源。“贝叶斯”怎么样？贝叶斯来自托马斯·贝叶斯著名的<a class="ae mp" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>。要全面理解贝叶斯定理，先说概率和条件概率。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/c7edc85f4ddc5b611e2ff0bc0e16088d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*sjp5R5AMsjVf4K_RtNGP1w.png"/></div></figure><p id="ed31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">概率只是指事件发生的可能性，通常取 0 到 1 之间的值(0 和 1 包括在内)。事件 A 的概率被表示为<strong class="kk iu"> p(A) </strong>，并且被计算为期望结果的数量除以所有结果的数量。例如，当你掷骰子时，得到小于 3 的数字的概率是 2 / 6。期望结果的数量是 2 (1 和 2)；总结果数为 6。</p><p id="afb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">条件概率是在与事件 A 相关的另一事件已经发生的情况下，事件 A 发生的可能性。假设我们有 6 个蓝色球和 4 个黄色球放在两个盒子里，如下图所示。我让你随机选一个球。得到蓝球的概率是 6 / 10 = 0，6。如果我让你从盒子 A 中选一个球呢？选到蓝球的概率明显降低。这里的条件是从明显改变事件概率的盒子 A 中选择(选择一个蓝球)。假设事件 B 已经发生，事件 A 的概率表示为<strong class="kk iu"> p(A|B) </strong>。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a7cf73be9ca6e538f78c1b321362f379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*vJG7O2ua2Eba61EXKGymkg.png"/></div></figure><p id="0a0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在介绍贝叶斯定理之前，还有一个概念需要学习。联合<strong class="kk iu">概率</strong>是两个事件一起发生的概率，表示为<strong class="kk iu"> p(A 和 B) </strong>。对于<strong class="kk iu">独立的</strong>事件，联合概率可以写成:</p><p id="9d89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(A 和 B) = p(A)。p(B)…………(1)</strong></p><p id="f10c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我掷骰子，掷硬币。得到 1 和正面的概率是:</p><p id="ce64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">(1 / 6).(1/2) = 1/12 = 0.08</p><p id="2c93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使计算正确，事件必须独立于。抛硬币的结果对掷骰子的结果没有任何影响，所以这些事件是独立的。让我们也举一个<strong class="kk iu">依赖</strong>事件的例子。我从一副牌中选了一张，又从同一副牌中选了第二张。在第二次选择中某一特定观察值肯定受第一次选择影响的概率。在非独立事件的情况下，等式 1 无效。应该稍微修改一下，使其适用于任何两个事件:</p><p id="0b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(A 和 B) = p(A)。p(B | A)…………(2)</strong></p><p id="5fd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等式(1)是等式(2)对于独立事件的特例，因为如果事件 B 和事件 A 独立，<strong class="kk iu"> p(B|A) = p(B)。</strong></p><h1 id="0bf8" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">贝叶斯定理</strong></h1><p id="aa67" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">我们将从任意两个事件的联合概率是可交换的这一事实开始。那就是:</p><p id="1e80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(A 和 B) = p(B 和 A)…………(3)</strong></p><p id="4eed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从等式 2 中，我们知道:</p><p id="f234" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(A 和 B) = p(A)。p(B|A) </strong></p><p id="d92f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(B 和 A) = p(B)。p(A|B) </strong></p><p id="46a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将等式 3 改写为:</p><p id="94fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(一)。p(B|A) = p(B)。p(A|B) </strong></p><p id="5bec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用 p(B)除两边给出了贝叶斯定理:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/6b52e1ff726d0b5fd5e8d634f7c147d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*TyxReRHaU0ONvRong0NUxQ.png"/></div></figure><p id="7d18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们对贝叶斯定理有了理解。是时候看看朴素贝叶斯分类器是如何使用这个定理的了。</p><h1 id="8ec2" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">朴素贝叶斯分类器</strong></h1><p id="88c4" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">朴素贝叶斯是一种用于分类的监督学习算法，因此任务是在给定特征值的情况下找到观察值(数据点)的类别。朴素贝叶斯分类器计算给定一组特征值(即 p(yi | x1，x2，…，xn))的类的概率。<strong class="kk iu"> </strong>把这个输入到贝叶斯定理中:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/50196ad4845da957cda8f2e99909e56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4WaYSK-CXy6FYpPiQ87zRA.png"/></div></figure><p id="1587" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p(x1，x2，…，xn | yi) </strong>表示给定类别标签的特征的特定组合的概率。为了能够计算这一点，我们需要非常大的数据集来估计所有不同特征值组合的概率分布。为了克服这个问题，<strong class="kk iu">朴素贝叶斯算法假设所有特征都是相互独立的。</strong>此外，分母(p(x1，x2，…，xn))可以被移除以简化方程，因为它仅归一化给定观察值的类的条件概率的值(p(yi | x1，x2，…，xn))。</p><p id="5a08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个类的概率(p(yi))很容易计算:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f65270eb407b2feadb6411ef098c9f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*jQIOi2ywgWEAO_Ove-u_iw.png"/></div></figure><p id="0be8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在特征相互独立的假设下，<strong class="kk iu"> p(x1，x2，…，xn | yi) </strong>可以写成<strong class="kk iu"> : </strong></p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/2919519d0169695a4a0542a16e33f2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*qIqZ-6m7GCts4XLesxwRLA.png"/></div></div></figure><p id="d53e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定类标签(即 p(x1 | yi))的单个特征的条件概率可以更容易地从数据中估计出来。该算法需要独立地存储每个类别的特征的概率分布。例如，如果有 5 个类别和 10 个特征，则需要存储 50 个不同的概率分布。分布的类型取决于特征的特性:</p><ul class=""><li id="7381" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nh lk ll lm bi translated">对于二元特征(Y/N，真/假，0/1):伯努利分布</li><li id="923e" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated">对于离散特征(即字数):多项式分布</li><li id="9d5d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated">对于连续特征:高斯(正态)分布</li></ul><p id="f82c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常用特征分布来命名朴素贝叶斯(即高斯朴素贝叶斯分类器)。对于混合型数据集，不同的要素可能需要不同类型的分布。</p><p id="f872" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将所有这些加起来，对于朴素贝叶斯算法来说，计算观察一类给定值的特征的概率(<strong class="kk iu"> p(yi | x1，x2，…，xn) ) </strong>变得很容易</p><h1 id="732b" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">朴素贝叶斯算法的利弊</strong></h1><p id="2f87" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">优点:</p><ul class=""><li id="cfd5" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nh lk ll lm bi translated">所有特征都是独立的假设使得朴素贝叶斯算法<strong class="kk iu">比复杂算法</strong>更快。<strong class="kk iu"> </strong>在某些情况下，速度优先于更高的精度。</li><li id="1b9d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated">它可以很好地处理文本分类、垃圾邮件检测等高维数据。</li></ul><p id="a7c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">缺点:</p><ul class=""><li id="16c1" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nh lk ll lm bi translated">所有特征都是独立的假设在现实生活中并不常见，因此这使得朴素贝叶斯算法不如复杂算法准确。速度是有代价的！</li></ul><h1 id="dd5b" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak"> Scikit-learn 实现</strong></h1><p id="c111" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">首先导入依赖项:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1488a998fa0f81480e413b1665a1454e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*LtqaHHpjV_OXCcMb8vBMSg.png"/></div></figure><p id="16e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将使用 scikit-learn 上的一个可用数据集(乳腺癌)。scikit-learn 的 train_test_split 模块用于将数据集分为训练集和测试集，以便我们可以首先训练模型，然后在新的、以前未见过的数据上测量其性能。</p><p id="2461" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们加载数据集，并将其分为训练集和测试集:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0df3d0b2e2e5e95c5f041b832c416571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*ImLYtX0xcXa4mEiOw5mBRw.png"/></div></figure><p id="a8cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们创建一个高斯朴素贝叶斯分类器对象，并使训练数据适合分类器:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d5ce2e67bd2da71fc8163dec55401d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*kctzAYjM0BwUOiF9Y_Sv2g.png"/></div></figure><p id="4f9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们在训练集和测试集上测试了分类器的性能。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/06fefeb517fa09f67cb76d68baec1224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*oRrzdI7dLbC7Uj-u3l325w.png"/></div></figure><p id="3568" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">准确度相当高，但这是一个简单的任务，用于展示概念并完成实施步骤。需要记住的一件重要事情是，训练集的精度不应该比测试集的精度高很多，这表明我们的模型太具体，没有很好地概括。这导致过拟合，这对于任何机器学习算法都是一个严重的问题。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="a575" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="1ff2" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">我的其他帖子</h1><ul class=""><li id="b46a" class="le lf it kk b kl mk ko ml kr ns kv nt kz nu ld nh lk ll lm bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd">支持向量机—解释</a></li><li id="6bbc" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/decision-tree-and-random-forest-explained-8d20ddabc9dd">决策树和随机森林—解释</a></li><li id="99b1" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/handling-missing-values-with-pandas-b876bf6f008f">用熊猫处理缺失值</a></li><li id="4ce0" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/predicting-used-car-prices-with-machine-learning-fea53811b1ab">用机器学习预测二手车价格</a></li><li id="7d2d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nh lk ll lm bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/data-cleaning-and-analysis-with-a-bonus-story-36b3ae39564c">数据清理和分析，附带奖励故事</a></li></ul></div></div>    
</body>
</html>