<html>
<head>
<title>Dealing with class imbalanced image datasets using the Focal Tversky Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用聚焦Tversky损失处理类别不平衡图像数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dealing-with-class-imbalanced-image-datasets-1cbd17de76b5?source=collection_archive---------4-----------------------#2020-05-07">https://towardsdatascience.com/dealing-with-class-imbalanced-image-datasets-1cbd17de76b5?source=collection_archive---------4-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e9f2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">局灶性特沃斯基损失</h2><div class=""/><div class=""><h2 id="a3de" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">课堂不平衡问题的损失比较以及为什么聚焦特沃斯基损失可能是你的最佳选择</h2></div><p id="60d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当试图训练分割网络时，类不平衡数据集是经常遇到的问题。我第一次训练图像分割模型时，马上就获得了超过96%的准确率。要么是我在不知不觉中取得了自第一个CNN成立以来最大的突破，要么是我的方法有问题。</p><p id="17c8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我很快意识到，要分割的像素只占图像总像素的很小一部分。该模型所要做的就是预测一个全黑的图像(也就是没有分割)，并且获得了超过90%的准确率。这是在大多数图像分割任务中遇到的常见问题，其中背景类比其他类大得多。</p><p id="e183" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个故事中，我讲述了用来处理班级失衡问题的技巧，以及为什么集中精力学习可能是你最好的选择。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="ec2a" class="lu lv it bd lw lx ly lz ma mb mc md me ki mf kj mg kl mh km mi ko mj kp mk ml bi translated">概述</h1><ol class=""><li id="e642" class="mm mn it kt b ku mo kx mp la mq le mr li ms lm mt mu mv mw bi translated">处理阶级不平衡的损失</li><li id="e832" class="mm mn it kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">什么是特沃斯基指数？</li><li id="bca5" class="mm mn it kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">局灶性特沃斯基损失</li><li id="bce0" class="mm mn it kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">结论</li></ol></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="2d3a" class="lu lv it bd lw lx ly lz ma mb mc md me ki mf kj mg kl mh km mi ko mj kp mk ml bi translated">1.处理阶级不平衡的损失</h1><p id="5e79" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated"><strong class="kt jd"> a .加权二元交叉熵</strong></p><p id="ac4b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通常用于处理类别不平衡的损失之一是加权二元交叉熵。正常的二值交叉熵的关键在于，它在计算损失时平等地考虑所有像素。在90%的像素为0而只有10%的像素为1的掩码中，即使网络错过了所有的1，它也会收到较低的损失，这意味着网络没有学到任何东西。</p><p id="e976" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">加权二进制交叉熵(WBCE)试图通过<strong class="kt jd">加权正类</strong>来解决这个问题。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0c6851e5ea7708f40264c38d75808c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*JP_UvwQFzIrrOpoTHVLeOw.png"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">p(0类)= p̂，p(1类)= p</p></figure><p id="1594" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于网络预测的每个1级，BCE将<strong class="kt jd"> <em class="nr"> log(p) </em> </strong>加到损失上，而WBCE将<em class="nr">𝜷</em><strong class="kt jd"><em class="nr">log(p)</em></strong>加到损失上。</p><p id="ed4a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，如果β &gt; 1，类别1的权重较高，这意味着网络不太可能忽略它(较少的假阴性)。反之，如果β &lt; 1, class 0 is weighted higher, meaning there will be lesser false positives.</p><p id="63d3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">By controlling, the value of β, you can reduce the problem of class imbalance by weighting the smaller class higher. However, the optimal value of β is hard to ascertain and requires many rounds of trial and error.</p><p id="daf9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Pros: Simple smooth loss surface that is fast in training</p><p id="1752" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Cons: Difficult to optimise and find the sweet spot</p><p id="4d78" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> b .骰子系数</strong></p><p id="27a6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">众所周知，Dice系数是图像分割的首选评估指标，但它也可以用作损失函数。虽然没有像二元交叉熵等其他损失函数那样广泛使用，但dice系数在处理类别不平衡时确实很神奇。</p><p id="7e1d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与BCE不同，dice系数只考虑分割类，不考虑背景类。像素被分类为真阳性(TP)、假阴性(FN)和假阳性(FP)。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a9868fcdec40abaa3cf5e634a80bd831.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*mUCRE8D1b2qLbg-3cciXIQ.png"/></div></figure><p id="eb03" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">dice系数是预测遮罩和地面实况的重叠的<strong class="kt jd">度量。因为它不考虑背景类，所以它不能支配较小的分割类。dice系数输出[0，1]范围内的分数，其中1表示完全重叠。因此，(1-DSC)可以用作损失函数。</strong></p><p id="a4d4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">考虑到dice系数的最大化是网络的目标，直接使用它作为损失函数可以产生良好的结果，因为它在设计上对类不平衡数据工作良好。</p><p id="6f89" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，dice系数的一个缺陷是存在爆炸梯度的可能性。在训练的早期，dice系数接近于0，这可能导致训练中的不稳定性，因为网络对权重进行大的改变会导致梯度爆炸。然而，使用批量规格化和ReLUs，这很容易管理。</p><p id="9c89" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">优点:无需手动优化任何参数，轻松解决等级不平衡问题</p><p id="0279" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">缺点:潜在的梯度爆炸，通常比BCE训练慢</p><h1 id="cd6f" class="lu lv it bd lw lx nt lz ma mb nu md me ki nv kj mg kl nw km mi ko nx kp mk ml bi translated">2.什么是特沃斯基指数？</h1><p id="b2cf" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated">Tversky指数(TI)是一种不对称的相似性度量，是dice系数和Jaccard指数的推广。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/17f55a5c2cf6294786629b668bce3b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*Xl2ApQjxasd5C8xq987JhA.png"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">(1 — T1)可以用作损失函数</p></figure><p id="6f28" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">特沃斯基指数增加了两个参数，α和𝜷，其中α + 𝜷 = 1。在α = 𝜷 = 0.5的情况下，它简化为dice系数。如果α = 𝜷 = 1，则简化为雅克卡指数。</p><p id="a102" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过设置α &gt; 𝜷的值，可以更多地惩罚假阴性。这在高度不平衡的数据集中变得有用，在这种情况下，对损失函数的额外控制级别比正常的dice系数产生更好的小规模分割。</p><p id="99f6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然tversky指数只是对dice系数的一个简单改进，但它可以证明在需要更精细控制的边缘情况下是有用的。</p><p id="f6f5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在Salehi等人的论文<a class="ae nz" href="https://arxiv.org/abs/1706.05721" rel="noopener ugc nofollow" target="_blank">使用3D完全卷积深度网络进行图像分割的Tversky损失函数</a>中，tversky损失用于获得多发性硬化病变分割的最理想性能，多发性硬化病变分割是典型的类别不平衡问题。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/a4cd204aeee97834e91a8aa521346236.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*NCVn5ztG-vka6j8UiV9OQQ.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">不同损失版本的绩效比较。注:α和𝜷分别控制FPs和FNs本文与上图相反。</p></figure><h1 id="bc61" class="lu lv it bd lw lx nt lz ma mb nu md me ki nv kj mg kl nw km mi ko nx kp mk ml bi translated">3.局灶性特沃斯基损失</h1><p id="fbd2" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated">聚焦特沃斯基损失(FTL)是特沃斯基损失的概括。损耗的非线性特性使您可以控制在不同的tversky指数值下损耗的表现。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0e4677e20d79d7cb28dd0c35968d60f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*JYmatznuuyTDenw8NVoxXg.png"/></div></figure><p id="dede" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">γ是控制损耗非线性的参数。当γ趋于正<strong class="kt jd"> ∞ </strong>时，随着特沃斯基指数(TI)趋于1，损耗的梯度趋于<strong class="kt jd"> ∞ </strong>。当γ趋向于0时，损耗梯度趋向于0，当TI趋向于1时。</p><p id="d4d3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本质上，γ &lt; 1, the gradient of the loss is higher for examples where TI &gt;的值为0.5，迫使模型关注这样的例子。这种行为在训练接近尾声时可能是有用的，因为即使TI接近收敛，模型仍然有学习的动机。然而，与此同时，在训练的早期阶段，它会将较容易的例子置于较高的权重，这可能导致较差的学习。</p><p id="5c37" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在类不平衡的情况下，当γ &gt; 1时，FTL变得有用。对于TI &lt; 0.5. This forces the model to focus on harder examples, especially small scale segmentations which usually receive low TI scores.</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bd75e2503bc23d26d18ecafe9fdf7691.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*mMpOoMfSjiNrckpu1BohjQ.png"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">A plot of the FTL with varying values of γ. In the case where γ = 1, it simplifies into a stanard tversky loss</p></figure><p id="7429" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">In the image above, the blue line is the standard tversky loss. The purple line shows the higher gradient and higher loss when TI &gt;为0.5的示例，这导致了较高的损耗梯度，而绿线显示了当TI &lt; 0.5.</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oh"><img src="../Images/b79ed959a67c0b5f3612aac0aed6fc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w20q6hJ5O6YwQv5JDv9mOg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">(<a class="ae nz" href="https://arxiv.org/abs/1810.07842" rel="noopener ugc nofollow" target="_blank">为0.5时较高的损耗。al，2018 </a>)表格稍作修改，仅显示相关部分。γ的值在现实中是3/4。本文用1/γ代替γ。</p></figure><p id="6f79" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上表摘自论文<a class="ae nz" href="https://arxiv.org/abs/1810.07842" rel="noopener ugc nofollow" target="_blank">，一种新的病灶Tversky损失函数，采用改进的注意力U-Net进行病灶分割</a>。</p><p id="ef78" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在公共汽车数据集中，FTL在所有类别中都有显著改善。在国际标准行业分类数据集中，尽管总体dice系数最高，但该模型没有最高的精确度或召回率。</p><p id="193f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然用FTL训练的模型可能不具有最高的精确度或召回率，但重要的是要注意，当用FTL训练时，精确度和召回率之间的平衡是最好的，这表明解决类别不平衡的目标得到了满足。最终结果是最佳的整体dice系数。</p><h1 id="9fa5" class="lu lv it bd lw lx nt lz ma mb nu md me ki nv kj mg kl nw km mi ko nx kp mk ml bi translated">4.结论</h1><p id="032d" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated">聚焦特沃斯基损失是解决阶级不平衡的一个简单方法。尽管最佳参数值需要一些反复试验才能确定，但您应该会看到以下结果:</p><pre class="ng nh ni nj gt oi oj ok ol aw om bi"><span id="f4ec" class="on lv it oj b gy oo op l oq or">α = 0.7, 𝜷 = 0.3, γ = 3/4</span></pre><p id="fe1a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> Keras实施</strong></p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="a7f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">要获得预先实施了注意力、递归和初始层的U-Net的完整实施，请检查<a class="ae nz" href="https://github.com/robinvvinod/unet/" rel="noopener ugc nofollow" target="_blank">https://github.com/robinvvinod/unet/</a>。</p></div></div>    
</body>
</html>