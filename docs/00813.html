<html>
<head>
<title>Group similar Image by using the Gaussian mixture model (EM algorithm)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用高斯混合模型(EM算法)对相似图像进行分组</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/group-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c?source=collection_archive---------7-----------------------#2020-01-23">https://towardsdatascience.com/group-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c?source=collection_archive---------7-----------------------#2020-01-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ad98" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为图像聚类问题从头实现GMM</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1b30bbe5f455a60b5626733556c52351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qHRFyCJ21ccWvECMuMMFRg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/photos/L68SOwRShLU" rel="noopener ugc nofollow" target="_blank">图片由什洛莫·沙莱夫拍摄</a></p></figure><p id="6244" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">聚类</strong>最流行的无监督机器学习问题之一。我们已经熟悉了k-means聚类算法，但是这里有一个问题:</p><ol class=""><li id="51fa" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">高度依赖于质心的初始值</strong>:如果我们改变它的初始化，然后改变集群的位置，那么最终的集群很可能会改变它的位置。</li><li id="6139" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">聚类具有不同的大小和密度</strong>:具有不同形状(分布的数据点)和大小(数据点的数量)的聚类不能通过这种简单的技术来处理。</li><li id="dafa" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">离群点</strong>:可能出现一个数据点离任何一个聚类都很远，它可以作为一个单独的聚类。为了避免这样的问题，我们通常会删除这些点。</li></ol><p id="def7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不想理解这个令人讨厌的数学，那么请向下滚动到GMM的<strong class="lb iu">实现</strong>部分，在那里我将讨论python中的实现。</p><h1 id="ad5a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">为了理解高斯混合模型，你需要理解一些术语</h1><ol class=""><li id="70e6" class="lv lw it lb b lc nb lf nc li nd lm ne lq nf lu ma mb mc md bi translated">密度估计:概率密度估计是基于不可观察的潜在概率密度函数的观察数据的近似构造。这一步包括选择概率分布函数和描述观测数据的最佳联合概率分布的函数参数。</li><li id="6e68" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最大似然估计(MLE):是一种通过最大化对数似然函数来估计概率分布参数的方法。</li><li id="585d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">潜在变量:与观察变量相反，即不是直接观察到的，而是从数学模型中推断出来的。我们以后再谈。</li></ol><p id="716c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在最大似然估计在潜在变量存在的情况下不太适用。期望最大化算法是一种在潜在变量存在的情况下可以找到合适的模型参数的方法。</p><p id="2bc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EM是一种寻找参数的最大似然或最大后验估计的迭代方法，其中模型依赖于未观察到的潜在变量。</p><p id="b193" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们稍微简化一下。</p><p id="cd98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定N个观察值</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/cd69b095f6442a22c441a3e63f3d8536.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*g6Zv18Xln1PH0yGnq4oQZA.png"/></div></figure><p id="bee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和概率模型p(X |θ)</p><p id="8e62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是找到使p(X |θ)最大化的θ值，这被称为最大似然估计。</p><p id="2628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果模型是简单的高斯分布，那么参数就像</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9248b8f9d2daf2baf4d9dd99fb018647.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*dGjD2cxzmTtC_QjH0WCf5w.png"/></div></figure><p id="1e1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/35be9f7adc089ba756f3181b1fb504c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*yAvWbwaoMHpPLGBFLYD5bg.png"/></div></div></figure><p id="164f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果是这种情况，在ML中解决这个问题的最常见方法是梯度下降，通过最小化作为损失函数的对数似然函数-log(p(X |θ))(记住我们实际上最大化参数的概率函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ee0f5a906d340bab95f7c4f3080b3031.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*WymntuQeQqyKG6vEOawXJA.png"/></div></figure><p id="397a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是数值稳定性的一种简单方法，因为它将乘积变为和，并且该函数的负值的最小化与该函数的最大化是相同的)。</p><p id="532e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是梯度下降是一种解决非线性问题的方法，典型的是针对一些多维分布的最小值。计算高斯混合的均值(或其他矩)不是非线性问题，因此不需要为非线性问题设计的方法。那么有没有更好的方法来解决这个问题。</p><p id="bc71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EM算法来了。让我们看看EM算法是如何在高斯混合模型中使用的。</p><p id="b4e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过引入潜在变量，极大似然估计可以得到简化。潜在变量模型假设观测xi是由一些潜在的潜在变量引起的。嗯……还是不清楚，好吧，考虑一下这张图片</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/fd40de7d41f929269f8be186050fd6d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MRy9CfzsEeUDJznw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://fallfordata.com/soft-clustering-with-gaussian-mixture-models-gmm/" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="4129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种类型的概率模型被称为高斯混合模型(GMM)，在本例中C个高斯分量的加权和为C = 3</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a581ae2d9300fe5f3fc9dffff6323714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ltIYsRI1qFCk7cjLw8AXOg.png"/></div></figure><p id="b723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">πc和σc分别是混合分量Cc的权重、均值向量和协方差矩阵。权重是非负的，总和为1，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cdc3e60d3e8591e92428f56f2884ec6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*SqFtwWSDOcmfcmlfP170Pg.png"/></div></figure><p id="8579" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参数向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/19b95f9d21aaa4a3416c368b87f29ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*Md-KsBKj8gKjgfIoy58Wtg.png"/></div></figure><p id="da8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">表示所有模型参数的集合。如果我们引入一个离散的潜在变量‘t ’,它决定了对混合物成分的观察值的分配，我们可以用条件分布p(x| t，θ)和先验分布p( t | θ)来定义观察变量和潜在变量p(x，t| θ)的联合分布</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6b4a3b9b325f630f64bb6ea74c030d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*U-zMo64P0CMkwx_a4K0z3w.png"/></div></figure><p id="5d16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/165e660966cd57fc324494633d426556.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*88yk-88eJFmZuA3-0uuk0Q.png"/></div></figure><p id="0ff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t的值是一位热编码的。例如，t2=1指的是第二个混合成分，这意味着如果总共有C=3个成分，则t = (0，1，0)。边际分布p (x | θ)是通过对t的所有可能状态求和得到的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/ed129ca8e2b807e0c4ea891a76fc20fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WScmxTbglq8eiaeLlELQqg.png"/></div></div></figure><p id="97d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个观察xi，我们有一个潜在变量ti，也称为责任。</p><p id="35e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到X是所有观测值的集合，所有潜在变量T的集合，那么我们可以很容易地最大化完全数据的对数似然p(X，T | θ)。因为找到对数似然后，我们就知道点的聚类分配，这就是为什么我们会找到边际对数似然或不完全数据对数似然p (X | θ)。所以从数学上来说，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9296d1adefd8421f8f98da9952b71e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qCLoQQfhtOPIhIh2Q9mPw.png"/></div></div></figure><h1 id="5571" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">GMM的实施</h1><p id="7474" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">让我们一步一步地看看如何使用高斯混合模型来聚类我们的图像。我在这里使用python来实现GMM模型:</p><p id="8668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要外部Python库:</p><ol class=""><li id="dfe9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">imageio:用于从图像中获取RGB特征</li><li id="e0ee" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">熊猫:用于处理数据集</li><li id="5122" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">numpy:用于数学运算</li></ol><p id="069f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步:</p><p id="e1a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从我们的数据集开始，我们已经给出了所有图像驻留的文件夹路径。我们将创建一个方法，从中我们可以提取每个图像的R，G，B向量的平均值。对于单个图像，函数应该是:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="e370" class="nz mk it nv b gy oa ob l oc od">######## loading external package dependency ####################</span><span id="46f1" class="nz mk it nv b gy oe ob l oc od">Import pandas as pd</span><span id="bfdc" class="nz mk it nv b gy oe ob l oc od">Import numpy as np</span><span id="f71b" class="nz mk it nv b gy oe ob l oc od">from scipy.stats import multivariate_normal</span><span id="c95a" class="nz mk it nv b gy oe ob l oc od">Import imageio</span><span id="02d9" class="nz mk it nv b gy oe ob l oc od">From functools import reduce</span><span id="f665" class="nz mk it nv b gy oe ob l oc od">def get_image_feature(path):</span><span id="3457" class="nz mk it nv b gy oe ob l oc od">    Im = imageio.imread(os.path.join(path), pilmode=’RGB’)</span><span id="4960" class="nz mk it nv b gy oe ob l oc od">    temp = Im/255. # divide by 255 to get in fraction</span><span id="d168" class="nz mk it nv b gy oe ob l oc od">    mn = temp.sum(axis=0).sum(axis=0)/(temp.shape[0]*temp.shape[1])</span><span id="5a1c" class="nz mk it nv b gy oe ob l oc od">    return mn/np.linalg.norm(mn, ord=None) # taking 2nd norm to scale vector</span></pre><p id="9ee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设提供的图像路径是'(some_path)/Image_SunSet.jpg '这个函数返回[红、绿、蓝]图像像素的强度像[0.9144867979，0.3184891297，0.2495567485]我们可以清楚地看到红色具有更大的强度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/468ee3ee29c5e9e1230c590d1e7dbe7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bflwKaCTGrkL8I-yW_ixXA.png"/></div></div></figure><p id="df21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我们可以对所有图像执行此操作，并创建一个包含图像细节的熊猫数据帧:</p><ul class=""><li id="35df" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu og mb mc md bi translated">文件名:文件的名称</li><li id="183c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">路径:文件的路径</li><li id="1430" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">扩展:图像扩展。jpg，。巴布亚新几内亚经济贸易委员会</li><li id="25d2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">ImageBase64:图像的Base64编码</li><li id="4cfd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">红色:红色平均强度</li><li id="02e8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">绿色:绿色平均强度</li><li id="33c5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">蓝色:蓝色平均强度</li></ul><p id="0ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们的数据集包含特征(即数据集的[红、绿、蓝]列，而不是仅用于查看的所有其余数据)。如果你想得到更高k值(聚类数)，你可以从图像中提取自己的特征，但要代表更广泛的颜色代码，不仅仅是r、g和b。</p><p id="2cfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二步:</p><p id="8b67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型生成(实现高斯混合模型的EM算法):</p><p id="19da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为在GMM，我们假设所有的数据点都来自k高斯分布，其中k只不过是一些集群。嗯……有点棘手，让我们简化一下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/56cee14ea280c8bd344ee252c2b2922c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fq5Lxj8ARsqWz-H7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="6a4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这个1D概率分布。如果我们假设所有数据点都来自2高斯分布，这意味着我们假设k=2(聚类数)，那么如果我们想知道哪些数据来自哪个高斯分布，我们需要做的就是计算两个高斯分布的均值和方差。但是等等，我们还不知道。但有一秒钟，如果我们假设我们知道这些参数(即k高斯分布的均值和方差)，那么我们就可以知道每个数据点来自哪个高斯的可能性(这个可能性指的是责任)。哇，但是我们也不知道这个。所以我们的结论是</p><p id="0b7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们知道参数(即高斯的均值和方差)，我们可以计算责任，如果我们知道责任，那么我们可以计算参数。</p><p id="8d0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是EM算法的由来，它首先将“k”个高斯随机放入图像中(为高斯生成随机平均值和方差),然后在E和M步之间迭代，直到收敛，其中:</p><p id="c64c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">紧急步骤:根据当前参数分配集群责任</strong></p><p id="f5bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> M步:更新参数，给定当前集群职责</strong></p><p id="7f70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面更详细的说一下它的实现。</p><p id="d5f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们记住为什么GMM被用来。与k- mean不同的是，它不会对聚类进行硬分配，例如图像</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e43f79fae3f7873aab6cac2d3a30804e.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/0*T4LqnS7Nqc3N0mWn"/></div></figure><p id="9a2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们使用k=4，即4个集群模型，即日落、森林、云和天空，那么在k均值算法中，这张图片只属于云，这称为硬分配，但在GMM，它属于97%的云和3%的森林，这称为软分配。</p><p id="8ef9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> E步骤:</strong></p><p id="3ba7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一步中，我们计算集群责任。让rik代表聚类k对于数据点I的责任。您可以将责任假定为数据点‘I’属于‘k’聚类的概率。因为这种可能性意味着</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4c48c0594a9875db7905fdf23d93cb2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*CuFnZ0hn4dP17KSxldNNkQ.png"/></div></figure><p id="8978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了知道一个聚类对一个给定的数据点有多大影响，我们计算该数据点在特定的聚类分配下的可能性，乘以该聚类的权重。对于数据点I和聚类k，数学上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/26d0e8eeeb299c5efa3a9803c1da6078.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*sW1sYlLS2Qr0qYkcI0EJPA.png"/></div></figure><p id="ef07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中N (xi | μk，σk)是聚类k的高斯分布(具有均值μk和协方差σk)。</p><p id="68e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用∝，是因为量N (xi | μk，σk)还不是我们想要的责任。为了确保每个数据点上的所有责任加起来等于1，我们在分母中添加了归一化常数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/482133fe88aa9caa51e1a1bf5c096d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*GcvZwSwREZ86WfXOV8M03Q.png"/></div></figure><p id="d042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们的数据是三维向量，即[红、绿、蓝]。Scipy提供了计算多元正态分布的便捷函数。<a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html?highlight=multivariate_normal#scipy.stats.multivariate_normal" rel="noopener ugc nofollow" target="_blank">检查这个</a></p><p id="4b0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是multivariate _ normal . pdf([数据点]，mean =[均值向量]，cov =[协方差矩阵])</p><p id="c245" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们后面会讲到如何计算M步的均值向量和协方差矩阵。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="242e" class="nz mk it nv b gy oa ob l oc od"># data (numpy array) : array of observations<br/># weights (numpy array) : numpy array of weight of each clusters of size (1, n_clusters)<br/>#means (numpy array) : numpy array of means of each clusters of size (n_cluster, dimension)<br/>#covariances(numpy array) : numpy array of covariance metrix of size (n_clusters, dimension, dimension)<br/>def get_responsibilities( data, weights, means, covariances):<br/>    n_data = len(data)<br/>    n_clusters = len(means)<br/>    resp = np.zeros((n_data, n_clusters))<br/>    for i in range(n_data):<br/>       for k in range(n_clusters):<br/>          resp[i, k] = weights[k]*   multivariate_normal.pdf(data[i],means[k],covariances[k],allow_singular=True)<br/>        # Add up responsibilities over each data point and normalize<br/>    row_sums = resp.sum(axis=1)[:, np.newaxis]<br/>    resp = resp / row_sums<br/>    return resp</span></pre><p id="fdd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住这个责任是我们潜在的变量。</p><p id="f6ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> M步:</strong></p><p id="bb19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在计算了聚类责任，我们必须更新与每个聚类相关的聚类参数，即(权重(πk)、均值(μk)和协方差(σk))。</p><p id="5c70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新权重:</strong></p><p id="1daa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类权重给出了每个聚类代表所有数据点的多少。数学上它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ba7111d4ba5cabc5351ce5420310f3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBNsKJLGIUkfhUzO-7ussw.png"/></div></div></figure><p id="ca78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c6a4" class="nz mk it nv b gy oa ob l oc od"># resp(numpy array) : responsibility numpy array size (n_sample, n_clusters)<br/>def get_soft_counts(resp):<br/>    return np.sum(resp, axis=0)</span><span id="0ff1" class="nz mk it nv b gy oe ob l oc od"># counts (numpy array) : count list of sum of soft counts for all clusters of size (n_cluster)<br/>def get_weights(counts):<br/>    n_clusters = len(counts)<br/>    sum_count = np.sum(counts)<br/>    weights = np.array(list(map(lambda k : counts[k]/sum_count, range(n_clusters))))<br/>    return weights</span></pre><p id="b8fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新的意思是:</strong></p><p id="7e11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个分类的平均值等于所有数据点的加权平均值，由分类责任进行加权。数学上，对于每个数据点，第k个集群的xi和责任rik是第k个集群的平均值，可以定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/af66815f0de21140e209422e3df683ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*fQc9xeN_8wHUbqphzlybSw.png"/></div></figure><p id="a7fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="20a5" class="nz mk it nv b gy oa ob l oc od"># data (numpy array): array of observation points<br/># resp (numpy array) : responsibility numpy array size (n_sample, n_clusters)<br/># counts (numpy array) : count list of sum of soft counts for all clusters of size (n_cluster) <br/>def _get_means( data, resp, counts):<br/>    n_clusters = len(counts)<br/>    n_data = len(data)<br/>    means = np.zeros((n_clusters, len(data[0])))<br/>    <br/>    for k in range(n_clusters):<br/>        weighted_sum = reduce(lambda x,i : x + resp[i,k]*data[i],  range(n_data), 0.0)<br/>        means[k] = weighted_sum/counts[k]<br/>    return means</span></pre><p id="dcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新协方差:</strong></p><p id="0f51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个集群的协方差等于由集群责任加权的所有<strong class="lb iu">外部产品</strong>的加权平均值。数学上它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bdbeaf8eef1b991ba867ccb01adfdb76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/0*BOsHyV9BFx1tWdyu"/></div></figure><p id="6b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/36bdd484cbc1b25462a0e000e4f387c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/0*ipweo35GxlVHg7no"/></div></figure><p id="0d8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是外积。让我们举一个简单的例子，看看外部产品是什么样子的。</p><p id="0e6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e28daa58448c8d6746ce936d94da677f.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*dpf35JZ0xxQqqHy-E8U-wg.png"/></div></figure><p id="37bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是两个向量，那么a和b的外积定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/74eba35e8ba204250e911a8054f4ec9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9wEQVAWC6poL9VSRMyX8Q.png"/></div></div></figure><p id="fa48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转置只是为了把列向量变成行向量。因为按照惯例，向量在机器学习中表示为列向量。</p><p id="8e24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="7aa3" class="nz mk it nv b gy oa ob l oc od"># data (numpy array) : array of observation points<br/># resp(numpy array) : responsibility numpy array size (n_sample, n_clusters)<br/># counts (numpy array) : count list of sum of soft counts for all clusters of size (n_cluster)<br/># means (numpy array) : numpy array of means of each clusters of size (n_cluster, dimension)<br/>def _get_covariances( data, resp, counts, means):<br/>    n_clusters = len(counts)<br/>    dimension = len(data[0]) # to get dimention of data<br/>    n_data = len(data)<br/>    covariances = np.zeros((n_clusters, dimension, dimension))<br/>    <br/>    for k in range(n_clusters):<br/>       weighted_sum = reduce (lambda x, i :x + resp[i,k] *  np.outer((data[i]-means[k]), (data[i]- means[k]).T), range(n_data), np.zeros((dimension, dimension)))<br/>       <br/>       covariances[k] = weighted_sum /counts[k] # normalize by total sum of counts<br/>    return covariances</span></pre><p id="40e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而是我们将如何度量我们的混合高斯模型。为此，我们将计算高斯混合的对数似然。它量化了在特定参数(即均值、协方差和权重)下观察到一组给定数据的概率。我们将继续使用不同的参数集迭代EM算法，并检查其收敛性，即对数似然不会改变太多，或者直到固定的迭代次数。</p><p id="3f09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0174" class="nz mk it nv b gy oa ob l oc od"># Z (numpy array) : numpy array of size (1, n_clusters)<br/>def _ln_sum_exp( Z):<br/>    return np.log(np.sum(np.exp(Z)))</span><span id="ca94" class="nz mk it nv b gy oe ob l oc od"># data (numpy array) : array of observation points<br/># weights (numpy array) : numpy array of weight of each clusters ofsize (1, n_clusters)<br/># means (numpy array) : numpy array of means of each clusters of size (n_cluster, dimension)<br/># covs (numpy array) : numpy array of covariance metrix of size (n_clusters, dimension, dimension)<br/>def get_log_likelihood( data, weights, means, covs):<br/>    n_clusters = len(means)<br/>    dimension = len(data[0])<br/>    sum_ln_exp = 0<br/>    <br/>    for d in data:<br/>      <br/>      Z = np.zeros(n_clusters)<br/>      for k in range(n_clusters):<br/>        # compute exponential term in multivariate_normal<br/>        delta = np.array(d) — means[k]<br/>        <br/>        inv = np.linalg.inv(covs[k])<br/>        exponent_term = np.dot (delta.T, np.dot (inv, delta))<br/>        # Compute loglikelihood contribution for this data point and this cluster<br/>        Z[k] += np.log (weights[k])<br/>        det = np.linalg.det(covs[k])<br/>        <br/>        Z[k] -= 1/2. * (dimension * np.log (2*np.pi) + np.log (det) + exponent_term)<br/>        # Increment loglikelihood contribution of this data point across all clusters<br/>      <br/>      sum_ln_exp += _ln_sum_exp(Z)<br/>    return sum_ln_exp</span></pre><p id="90e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是EM算法的实际主要实现方法的时候了</p><p id="ac09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9cc3" class="nz mk it nv b gy oa ob l oc od"># data (numpy array) : array of observation points<br/># init_weights (numpy array) : numpy array of initial weight of each clusters ofsize (1, n_clusters)<br/># init_means (numpy array) : numpy array of initial means of each clusters of size (n_cluster, dimension)<br/># init_covariances (numpy array) : numpy array of initial covariance metrix of size (n_clusters, dimension, dimension)<br/># maxiter (int) : maximum iteration to rum EM (optional)<br/># threshold (float) : maximum threshold to stop EM (optional)<br/>def em_from_parameter(data, init_means, init_covariances, init_weights, maxiter=1000, thresh=1e-4):<br/>    # Make copies of initial parameters<br/>    means = init_means[:]<br/>    covariances = init_covariances[:]<br/>    weights = init_weights[:]<br/>    <br/>    # Infer length of dataset<br/>    n_data = len(data)<br/>    <br/>    # Infer number of cluster<br/>    n_clusters = len(means)<br/>    <br/>    # Initialize some useful variables<br/>    resp = np.zeros((n_data, n_clusters), dtype=np.float64)<br/>    <br/>    l1 = get_log_likelihood(data, weights, means, covariances)<br/>    l1_list = [l1]<br/>    <br/>    for it in range(maxiter):<br/>      <br/>      # E-step: calculate responsibilities<br/>      <br/>      resp = get_responsibilities(data, weights, means, covariances)<br/>      <br/>      # M-step calculate cluster parameter<br/>      counts = get_soft_counts(resp)<br/>      weights = _get_weights(counts)<br/>      means = _get_means(data, resp, counts)<br/>      covariances = _get_covariances(data, resp, counts, means)<br/>      <br/>      l1_new = get_log_likelihood(data, weights, means, covariances)<br/>      <br/>      l1_list.append(l1_new)<br/>      <br/>      if abs(l1_new – l1) &lt; thresh :<br/>        break<br/>      <br/>      l1 = l1_new<br/>    <br/>    param = {'weights': weights, 'means': means, 'covariances': covariances, 'loglikelihood': l1_list, 'responsibility': resp, 'Iterations': it}<br/>    return param</span></pre><p id="0137" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在都准备好了，但是我们的初始参数是什么。我们有许多方法可以采用。</p><p id="cd79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是相等地初始化所有聚类的权重，即每个聚类的(1/k ),其中k是聚类的总数。</p><p id="9da1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1个聚类的协方差矩阵的初始化是维数等于数据维数*数据维数的对角方阵(在这种情况下，数据维数是3，即[r，g，b])</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8b6daec7b4a3e0626ccf1ffdc6ef6398.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/0*JR8T1SzRn33VRYkY"/></div></figure><p id="87bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，σxy =是x和y之间的协方差</p><p id="7ea5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于均值初始化来说，有点棘手。还记得k均值算法吗？我们使用这些算法的输出来初始化我们的EM均值参数。因为通过使用k-均值算法，我们推动均值坐标到达最近的收敛点。因此，如果我们从这一点出发，EM算法将花费更少的时间来收敛。但是有一个问题，我们将如何初始化k均值的初始质心。</p><p id="4723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯。对于这个问题，我们将采用k mean++初始化策略，它选择k个彼此相距最远的质心。现在我们已经有了所有的三参数。</p><p id="b90c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:完整的解决方案请看我的git-hub链接<a class="ae ky" href="https://github.com/Aksh-kumar/PYTHONMLAPI" rel="noopener ugc nofollow" target="_blank"> PYTHONAPI </a>你会发现这个集群问题端到端的完整Flask代码，可以被任何客户端技术使用。我选择Angular 8作为客户端，其代码可在<a class="ae ky" href="https://github.com/Aksh-kumar/ML_AlgorithmUI" rel="noopener ugc nofollow" target="_blank"> ML_algorithmUI </a>上获得。</p><p id="8851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于那些只想检查EM相关代码的人来说，链接是<a class="ae ky" href="https://github.com/Aksh-kumar/PYTHONMLAPI/tree/master/ML_algorithms/cluster" rel="noopener ugc nofollow" target="_blank">这个</a>，它包含两个文件夹Expectation _ Maximization和K_Mean，这两个文件夹分别包含相关代码。请浏览代码，如果您面临任何问题，请随时问我。</p><p id="67e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在回到代码，这个程序的驱动代码</p><p id="a0dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="3575" class="nz mk it nv b gy oa ob l oc od"># k: number of clusters<br/># data: observations coming from distribution<br/># return final weights, means, covariances, list of loglikelihood till convergence and final responsibility of each observation and number of Iterations it takes till convergence</span><span id="5a0f" class="nz mk it nv b gy oe ob l oc od">def em(k, data) :<br/>    dimension = len(data[0])<br/>    # mean co-ordinate of all clusters not recommended at all since it is a very bad method to initialized the mean points alternatively use the k-mean initializing method as discussed early<br/>    means = np.random.rand(k, dimension)<br/>    #### alternate better method to initialize mean points by kmean ####<br/>    ## from KM import KM ## self created KM class available in my github repos<br/>    ## check K_Mean folder in listed github repos path<br/>    km = KM()<br/>    means, dont_require = km.kmeans(data, k) # return centroids and cluster assignments<br/>    <br/>    cov = np.diag(np.var(data, axis=0)) # get covariance of data initialize by diagonal metrix of covariance of data<br/>    <br/>    covariances = np.array([cov] * k) # initiate covariance metrix with diagonal element is covariance of data<br/>    weights = np.array([1/ k] * k) # initialize equal weight to all clusters<br/>    return em_from_parameter(data, means, covariances, weights)</span></pre><p id="a66b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">images_path = ['../image2.jpg '，'../image2.jpg '，… ] #图像路径列表……..…………………………………………………………………………..(1)</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="3f37" class="nz mk it nv b gy oa ob l oc od">########### Data generation ################<br/>data = []<br/>For path in images_path:</span><span id="c0f2" class="nz mk it nv b gy oe ob l oc od">data.append(get_image_feature(path))</span><span id="b4fc" class="nz mk it nv b gy oe ob l oc od">data = np.array(data)</span></pre><p id="3c2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们的数据已经准备好了，k的值，即集群的数量，都是个人的选择。为简单起见，我取k = 4意味着我们假设数据的分布来自4高斯分布。既然我们的数据和k值已经准备好了，那么我们就准备创建一个模型。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="39ef" class="nz mk it nv b gy oa ob l oc od">####### create model #######</span><span id="9b90" class="nz mk it nv b gy oe ob l oc od">parameters = em(k, data) # store returned parameters of model along with responsibilities</span></pre><p id="ce21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">. . . . . . ………………………………………………………………………….(2)</p><p id="5dd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这需要时间，别担心，但最终，你会得到你想要的。现在，如果您有新的观察数据，您可以找出软分配，因为我们已经获得了正确的集群参数。</p><p id="a14b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python代码</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="3b5a" class="nz mk it nv b gy oa ob l oc od"># data (numpy array) : array of observation points<br/># weights (numpy array) : numpy array of weight of each clusters of size (1, n_clusters)<br/># means (numpy array) : numpy array of means of each clusters of size (n_cluster, dimension)<br/># covs (numpy array) : numpy array of covariance metrix of size (n_clusters, dimension, dimension)<br/>def get_responsibilities(self, data, weights, means, covariances):<br/>    n_data = len(data)<br/>    n_clusters = len(means)<br/>    <br/>    resp = np.zeros((n_data, n_clusters))</span><span id="cc82" class="nz mk it nv b gy oe ob l oc od">for i in range(n_data):</span><span id="8495" class="nz mk it nv b gy oe ob l oc od">for k in range(n_clusters):</span><span id="2c7a" class="nz mk it nv b gy oe ob l oc od">resp[i, k] = weights[k]* multivariate_normal.pdf(data[i],means[k],covariances[k])</span><span id="367a" class="nz mk it nv b gy oe ob l oc od"># Add up responsibilities over each data point and normalize</span><span id="1761" class="nz mk it nv b gy oe ob l oc od">row_sums = resp.sum(axis=1)[:, np.newaxis]</span><span id="e116" class="nz mk it nv b gy oe ob l oc od">resp = resp / row_sums</span><span id="ef17" class="nz mk it nv b gy oe ob l oc od">return resp</span></pre><p id="c4ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建观察值的numpy数组，并使用em返回的参数调用该方法，如(2)所示，并将其传递给该函数，您将获得每个集群的软分配。例如，让你有一个带有路径的图像”../../test_example.jpg "</p><p id="5cf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先获取图像的[r，g，b ]向量，如(1)所示，您将得到类似于[0.9144867979，0.3184891297，0.2495567485 ]的内容，将这些数据以及返回的集群参数传递给get _ responsibilities函数。您将在参数变量中获得该值，即</p><p id="5f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>记住这个函数采用一个观察值数组，在这种情况下，我们只有一个图像，即一个观察值，所以首先将这个观察值转换成一个数组，然后调用这个函数。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f629" class="nz mk it nv b gy oa ob l oc od">data = np.array([ np.array([0.9144867979, 0.3184891297, 0.2495567485 ])])</span><span id="9f0a" class="nz mk it nv b gy oe ob l oc od">mean = parameters[‘means’]</span><span id="4b73" class="nz mk it nv b gy oe ob l oc od">cov = parameters[‘covariances’]</span><span id="6f1a" class="nz mk it nv b gy oe ob l oc od">weights = parameters[‘weights’]</span><span id="d6e4" class="nz mk it nv b gy oe ob l oc od">resp = get_responsibilities(data, weights, mean, cov)</span></pre><p id="0b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">万岁，你得到了你的集群任务。我对一些截图做了一个基于网络的可视化结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/8967660c31c98ba8cce48af35b04810a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SdLp17GtMv1omT2m"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/77884ddee92ef06458141d795fa34e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gaDjwaeS9jIh7dGG"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/652e3f668328f55c81d0940a61927e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sSDuhdH_NI7-Ug8o"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/d6f22b9fdfe8f0910cbc3532a8195fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rSJx4xPXk1IekvtN"/></div></div></figure><p id="d4b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来谈谈它是如何消除k均值的所有缺点的:</p><ol class=""><li id="199c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">更好的初始化策略</strong>:因为我们使用K mean++初始化策略，所以我们不太可能得到错误的集群结构。</li><li id="bb36" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">变化的聚类密度</strong>:与k均值不同，它具有变化的聚类密度，这为我们提供了对具有各自聚类分配的观测值的概率估计。</li><li id="2244" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">离群点</strong>:离群点被考虑，因为它包含非常少的对其最近的集群的责任。</li></ol><p id="1e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论:</strong>最后我们可以说，任何具有原色集合的图像都可以用这种技术进行聚类。我们可以使用这种方法对任意数量的图像进行聚类。在这里，我只是举了一个简单的例子，主要颜色，但根据需要，特征生成可能会改变多种颜色的阴影。GMM是最适合的情况下，你想要的概率图像分配到一个特定的集群。</p><p id="4522" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候结束我的博客了，如果你真的对这段代码感兴趣，请访问我的git-hub链接，并随时贡献和提出改进建议。快乐学习…</p><p id="b969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">链接和参考:</strong></p><p id="1d60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">马丁·克拉瑟潜变量模型Part-1高斯混合模型与EM算法(2019年11月21日):<a class="ae ky" href="https://krasserm.github.io/2019/11/21/latent-variable-models-part-1/" rel="noopener ugc nofollow" target="_blank">https://克拉瑟姆. github . io/2019/11/21/Latent-variable-models-Part-1/</a></p><p id="31d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ethen 8181:<a class="ae ky" href="http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html" rel="noopener ugc nofollow" target="_blank">http://ethen 8181 . github . io/machine-learning/clustering/GMM/GMM . html</a></p><p id="29ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维基参考:<a class="ae ky" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Expectation–maximization _ algorithm</a></p><p id="8144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谷歌开发者资源:<a class="ae ky" href="https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages" rel="noopener ugc nofollow" target="_blank">https://Developers . Google . com/machine-learning/clustering/algorithm/advantage-visits</a></p></div></div>    
</body>
</html>