<html>
<head>
<title>Finding Natural Groups through Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过聚类发现自然群体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-the-clustering-algorithms-through-hands-on-da4b462503ff?source=collection_archive---------56-----------------------#2020-07-21">https://towardsdatascience.com/learning-the-clustering-algorithms-through-hands-on-da4b462503ff?source=collection_archive---------56-----------------------#2020-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="62cd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">通过简单的实验学习流行的聚类算法</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6df3989274842a6ae56a3b8f17920baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3aR-U6w4I0W8giZw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然群体。<a class="ae ky" href="https://unsplash.com/photos/aCDHRbxRvAM" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><p id="d087" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集群具有跨应用领域的重要用例，以了解数据的整体可变性结构。无论是客户、学生、基因、心电图信号、图像、股票价格的聚类。在我们的一些研究文章中，我们已经在特性选择和投资组合的优化中成功地使用了这一点，但在后面的故事中会有更多。聚类属于无监督类别，根据定义，它试图在数据中找到自然的组。</p><p id="76c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使在深度学习时代，聚类也可以为您提供关于数据分布的宝贵见解，从而影响您的设计决策。</p><p id="d493" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们给你一些关于五种聚类算法和 Kaggle 笔记本链接的概述，我们在那里做了一些实验。目的是给你一个快速总结，让你开始申请。</p><h1 id="6e1e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">K-表示:</h1><p id="1876" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是所有基于距离的算法中最简单的一种。这是一种分割算法。从 k 个随机点作为聚类中心开始，然后将其余的点分配到最近的聚类中心。完成此分配后，重新计算聚类中心。这个过程一直持续到集群分配没有太大变化。</p><p id="9d4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中一些问题是:</p><p id="e90e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a)我们需要知道“k”的值</p><p id="e428" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b)受离群值影响。</p><p id="1644" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">c)取决于初始化</p><p id="3027" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有些实验是在下面的 Kaggle 笔记本上做的:【https://www.kaggle.com/saptarsi/kmeans-dbs-sg T4】</p><p id="c81d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先解释 k-means 是如何工作的，然后举例说明为什么需要缩放。我们还讨论了如何使用肘图绘制误差平方和，以找到 k 的最佳值。一个简单的例子是“iris”</p><p id="b9eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应视频:</p><p id="afba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论:<a class="ae ky" href="https://www.youtube.com/watch?v=FFhmNy0W4tE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=FFhmNy0W4tE</a></p><p id="1fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动手:<a class="ae ky" href="https://www.youtube.com/watch?v=w0CTqS_KFjY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=w0CTqS_KFjY</a></p><h1 id="a856" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">K-Medoid:</h1><p id="d64d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">K-Means 对异常值并不稳健。当我们有一个以上的属性时，我们要找到一个整体的中值，这就是所谓的中值。有些实验是用下面的笔记本做的。为了演示异常观测值的问题，我们做了一个添加三个异常观测值的小实验。如你所知，iris 有三个类(Setosa、Verginca 和 Versicolor ),它们被绘制成两个特性。这些由蓝色圆圈标记的特征的添加完全扭曲了聚类。原来的三个类合并成两个聚类，离群点在一个聚类中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/76eb6e60dbf90400277d23d70495c206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*vuTAeOk1VlNfH57R1Ap_BA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:受极值影响的 K 均值。图片来源:作者笔记本<a class="ae ky" href="https://www.kaggle.com/saptarsi/kmedoid-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/kmedoid-sg</a></p></figure><p id="15be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Kaggle 笔记本链接:</p><p id="cd1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.kaggle.com/saptarsi/kmedoid-sg<a class="ae ky" href="https://www.kaggle.com/saptarsi/kmedoid-sg" rel="noopener ugc nofollow" target="_blank"/></p><p id="2683" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们讨论的主要问题是，当标签不可用(轮廓宽度)和可用(纯度)时，如何测量聚类的质量。下面是一个比较 K 均值和 K 中值的极端观测值(异常值)的简短示例。</p><p id="9c96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应视频:</p><p id="56c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论:【https://www.youtube.com/watch?v=q3plVFIgjGQ】T4</p><p id="f822" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动手:<a class="ae ky" href="https://www.youtube.com/watch?v=L1ykPtlonAU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=L1ykPtlonAU</a></p><h1 id="c552" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据库扫描:</h1><p id="5334" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">DBSCAN 最有前途的一点是，它将一些点识别为<strong class="lb iu">噪声</strong>点，这些点如果包含在聚类练习中，将会扭曲整个聚类。该算法可以找到基于密度的聚类，而不是距离。</p><p id="a0e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在卫星和同心圆上比较了 DBSCAN、K-Medoid、K-Means。下面的笔记本包含简单的实验。</p><p id="006f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/saptarsi/dbscan-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/dbscan-sg</a></p><p id="58db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个这样的实验中，我们创造了一个噪音点。</p><p id="82f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">X = np.array([[1，2]，[4，2]，[3，3]，[8，7]，[9，9]，[10，10]，[25，80]])</p><p id="fe57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">散点图如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/4fe4f1e28f420fe6005d27724bf09f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*6LthvPcnoLIPWTjoRlTZAQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:具有一个噪声点的 2D 数据的散点图</p></figure><p id="dcc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图片来源:作者笔记<a class="ae ky" href="https://www.kaggle.com/saptarsi/dbscan-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/dbscan-sg</a></p><p id="ab3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，我们理解可能有三个集群和一个外围点。</p><p id="c33e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图比较了聚类结果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/9375f6332c62c4a287e633766e1fa0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*_zExHpvz5vtIkMLAk6d3Qw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3: DBSCAN、K-Means 和 PAM 处理噪声点的能力</p></figure><p id="99a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图片来源:作者笔记<a class="ae ky" href="https://www.kaggle.com/saptarsi/dbscan-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/dbscan-sg</a></p><p id="dce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到 DBSCAN 清楚地将点置于另一种颜色中，因为 k-意味着原始的两个聚类丢失，对于 k-medoid 更好，但是它没有单独识别外围点。</p><p id="eef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的 make circle 提供了算法的另一个比较，它清楚地显示了 DBSCAN 如何正确地识别属于两个集群的两个圆。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6dbbb74c6061f51bbffd44a542c65778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*by4Y33MeuSnJvwuIF3CVlg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:同心圆上的 DBSCAN、K-Means 和 K-Medoid</p></figure><p id="1109" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图片来源:作者笔记 https://www.kaggle.com/saptarsi/dbscan-sg<a class="ae ky" href="https://www.kaggle.com/saptarsi/dbscan-sg" rel="noopener ugc nofollow" target="_blank"/></p><p id="c193" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应视频:</p><p id="b2db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论:【https://www.youtube.com/watch?v=RZg51WH7caQ】T4</p><p id="b318" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动手:<a class="ae ky" href="https://www.youtube.com/watch?v=A8OnRH42hWE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=A8OnRH42hWE</a></p><h1 id="2abe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">凝聚聚类:</h1><p id="2946" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以上三种是基于分区的聚类，而这是一种<strong class="lb iu">分层</strong>聚类，通过树状图揭示了关于整体结构的更多语义。层次聚类中的一个重要考虑是当合并到聚类中时要考虑哪个距离，是最小距离、最大距离还是平均距离。在同心圆、半月形、高斯数据和种子数据集上进行了各种实验。包含一些实验的笔记本如下:</p><div class="mv mw gp gr mx my"><a href="https://www.kaggle.com/saptarsi/agglomarative-clustering-sg" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">聚集聚类 SG</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用 UCISeeds 的数据</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">www.kaggle.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ks my"/></div></div></a></div><p id="7525" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">笔记本上的符号图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6b8c9a50ff018d6045c9926edaaad56e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*FcencwPD_0HECHR8oeovJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:简单的 dendorgam</p></figure><p id="9593" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图片来源:作者笔记<a class="ae ky" href="https://www.kaggle.com/saptarsi/agglomarative-clustering-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/agglomarative-clustering-sg</a></p><p id="ea45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应视频:</p><p id="abcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论:<a class="ae ky" href="https://www.youtube.com/watch?v=RZg51WH7caQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=RZg51WH7caQ</a></p><p id="2e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动手:<a class="ae ky" href="https://www.youtube.com/watch?v=z7jXh_RzL_k" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=z7jXh_RzL_k</a></p><h1 id="14c6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预期最大化:</h1><p id="20ef" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">其实现方式是通过高斯混合建模。基本上，每个聚类可以被认为来自不同的多元正态分布。与所有 4 种方法相比，这是一种<strong class="lb iu">软聚类</strong>方法。下面是一个笔记本，里面有一些简单的实验。这是一种更灵活的方法，允许圆具有非球形形状。</p><p id="d0b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/saptarsi/gmm-sg" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/gmm-sg</a></p><p id="ba65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应视频:</p><p id="f02f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论:【https://www.youtube.com/watch?v=LaL0BfvUurs】T2</p><p id="70ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动手:【https://www.youtube.com/watch?v=A8OnRH42hWE T4】</p><h1 id="d38d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">总结:</h1><p id="5cc0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在下表中，我们总结了算法的优缺点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/13deb03df4f14116eef050e3ff60a648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*JawYMtcXRlcNwuOloOnGEQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">聚类算法的比较。作者创建的图像</p></figure><h1 id="c3a3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">何时使用哪种算法？</h1><p id="7838" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我总是从 K-Means 开始，观察聚类质量，只有当数据非常随机，密度变化时，我才会选择 DBS can。如果我想应用聚类进行总结<strong class="lb iu"> k-medoid </strong>是一个不错的选择。<strong class="lb iu">凝聚聚类</strong>因其呈现的可视化而具有吸引力。EM 是一个理论上非常健壮的算法，它有许多其他的应用，在有重叠簇的地方 EM 可能是一个很好的选择。</p><p id="bcef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考资料:</p><p id="ba77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]<a class="ae ky" rel="noopener" target="_blank" href="/clustering-clearly-explained-5561642ec20c">https://towards data science . com/clustering-clearly-explained-5561642 ec20c</a></p><p id="3678" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" rel="noopener" target="_blank" href="/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e">https://towards data science . com/unsupervised-machine-learning-clustering-analysis-d 40 F2 b 34 AE 7e</a></p><p id="a5fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Tan PN，Steinbach M，Kumar V .数据挖掘导论。培生教育印度公司；2016.</p></div></div>    
</body>
</html>