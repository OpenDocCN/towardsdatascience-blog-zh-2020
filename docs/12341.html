<html>
<head>
<title>Quickprop, an Alternative to Back-Propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速推进，反向传播的替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quickprop-an-alternative-to-back-propagation-d9a78069e2a7?source=collection_archive---------29-----------------------#2020-08-25">https://towardsdatascience.com/quickprop-an-alternative-to-back-propagation-d9a78069e2a7?source=collection_archive---------29-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="519f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">斯科特·法尔曼加速梯度下降的想法</h2></div><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="ab gu cl kn"><img src="../Images/ca2aa612e976d780f2c7d1d39e4a4f2f.png" data-original-src="https://miro.medium.com/v2/1*54TQ7Uxo96nDu48d5d_O4w.gif"/></div></figure><p id="9c5b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">由于 20 世纪 80/90 年代的普通反向传播算法收敛缓慢，Scott Fahlman 发明了一种学习算法，称为 Quickprop [1]，它大致基于<a class="ae lm" href="https://en.wikipedia.org/wiki/Newton's_method" rel="noopener ugc nofollow" target="_blank">牛顿法</a>。在“N-M-N 编码器”任务等问题领域，他的简单想法优于反向传播(经过各种调整)，即训练一个具有 N 个输入、M 个隐藏单元和 N 个输出的解码器/编码器网络。【Quickprop 专门解决的问题之一是找到特定领域的最佳学习速率，或者说:一种动态适当调整它的算法。</p><p id="25c0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在本文中，我们将看看 Quickprop 背后的简单数学思想。我们将用 Python 和 PyTorch 实现基本算法和法尔曼建议的一些改进。</p><p id="800f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Giuseppe Bonaccorso 在发表的这篇有用的博客文章<a class="ae lm" href="https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/" rel="noopener ugc nofollow" target="_blank">中已经介绍了算法的粗略实现和一些背景知识。我们将在理论和代码方面对此进行扩展，但如果有疑问，可以看看 Giuseppe 是如何解释的。</a></p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="e546" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lu">研究 Quickprop 的动机来自于撰写我上一篇关于“级联相关学习架构”的文章</em><a class="ae lm" rel="noopener" target="_blank" href="/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92"><em class="lu"/></a><em class="lu">[2]。在那里，我用它来训练神经网络的输出</em>和<em class="lu">隐藏神经元，这是一个我后来才意识到的错误，我们也将在这里进行研究。</em></p><p id="df68" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lu">要继续阅读本文，您应该熟悉如何使用损耗梯度的反向传播来训练神经网络(截至 2020 年，这是一种广泛使用的方法)。也就是说，您应该了解梯度通常是如何计算的，并应用于网络的参数，以尝试迭代实现损耗收敛到全局最小值。</em></p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="8ee2" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">概观</h2><p id="3488" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">我们将从 Quickprop 背后的数学开始，然后看看如何一步一步地实现和改进它。<br/>为了使后续工作更容易，所有使用的方程和完成的推理步骤都比原始论文中解释得更详细。</p><h2 id="20f4" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">快速推进背后的数学</h2><p id="0420" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">通常用于神经网络的反向传播学习方法是基于这样的思想，即通过在其梯度的反方向上采取短的步骤，迭代地“向下”函数的斜率。</p><p id="ca15" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这些“短步骤”是这里的症结所在。它们的长度通常取决于一个学习率因子，并且故意保持很小以不超过潜在的最小值。</p><p id="3793" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">回到法尔曼开发 Quickprop 的时候，选择一个好的学习速率是一个大问题。正如他在他的论文中提到的，在性能最好的算法中，科学家“通过眼睛”(即手动和基于经验)选择学习速率。[1]</p><p id="7b12" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">面对这种情况，法尔曼提出了一个不同的想法:解决一个更简单的问题。</p><p id="9374" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最小化损失函数<strong class="ks iu"><em class="lu">【L】</em></strong>，特别是对于深度神经网络，在分析上(即，在整个域上以一般方式)会变得极其困难。<br/>例如，在反向传播中，我们只是逐点计算，然后在正确的方向上迈出小步。如果我们知道函数的“地形”通常是什么样子，我们就可以直接“跳到”最小值。</p><p id="4e02" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是，如果我们可以用一个更简单的，我们知道它的地形的形式来代替损失函数，会怎么样呢？这正是 Fahlmans 在 Quickprop 中采用的假设:他假定<strong class="ks iu"> <em class="lu"> L </em> </strong>可以用一个简单的正方向开口的抛物线来近似。这样，计算(抛物线的)最小值就像找到一条线与 x 轴的交点一样简单。</p><p id="4068" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果该点还不是损失函数的最小值，那么下一条抛物线可以从该点开始近似计算，如下图所示。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="ab gu cl kn"><img src="../Images/ca2aa612e976d780f2c7d1d39e4a4f2f.png" data-original-src="https://miro.medium.com/v2/1*54TQ7Uxo96nDu48d5d_O4w.gif"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">抛物线拟合到原始函数，并向其最小值前进一步。从那里开始，拟合下一条抛物线，然后进行下一步。两条虚线是抛物线的当前点和前一个驻点。(图片由作者提供)</p></figure><p id="b252" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">那么…我们究竟怎样才能近似<strong class="ks iu"> <em class="lu"> L </em> </strong>？简单——使用泰勒级数和一个小技巧。</p><p id="90ea" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lu">注意，对于下面的等式，我们认为权重向量</em> <strong class="ks iu"> <em class="lu"> w </em> </strong> <em class="lu">的分量是要独立训练的，所以</em> <strong class="ks iu"> <em class="lu"> w </em> </strong> <em class="lu">的意思是看做标量。但是我们仍然可以利用 GPU 的 SIMD 架构，使用组件式计算。</em></p><p id="3179" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们从<strong class="ks iu"> <em class="lu"> L </em> </strong>的二阶泰勒展开开始，给出一条抛物线(没有误差项):</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/e38a52adf605b2ecdc56f71133474835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDOkgxY85FC-r_Ywa0oAzw.png"/></div></div></figure><p id="c53d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">(要了解这是如何创建的，请查看上面链接的关于泰勒级数的维基百科文章——这很简单，只需将<strong class="ks iu"> <em class="lu"> L </em> </strong>输入到一般泰勒公式中，直到第二项，然后放弃其余部分。)</p><p id="bde3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们现在可以根据重量差异定义重量的更新规则，并将其输入到<strong class="ks iu"><em class="lu"/></strong>:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nc"><img src="../Images/661af6bf31e8d17c04d794b772319eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dnnh2tFA16C3lQZf6ttt3w.png"/></div></div></figure><p id="a2cf" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Quickprop 现在使用差商进一步线性逼近<strong class="ks iu"><em class="lu"/></strong>(这就是上面提到的小技巧):</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nd"><img src="../Images/3b7b6ec97c9d402c7614af88b1d4ff8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5U7bkEj45OF0ePMyyIYag.png"/></div></div></figure><p id="5d56" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">利用这一点，我们可以将泰勒多项式改写为“快速推进”调整版本，并构建其梯度:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/a6d6cd625636b25ac72368a00c80bf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-N0ccaNoJNr1_dKZ_UZGoQ.png"/></div></div></figure><p id="5756" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后一个方程可以用来计算抛物线的驻点:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nf"><img src="../Images/ec7c6049aba0d782a3d28de2eabce37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_3BeU12yUBrQFg6yKpYzA.png"/></div></div></figure><p id="0975" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">就是这样！</strong>现在，为了综合考虑，给定以前的重量、以前的重量差以及以前和当前重量下的损耗斜率，Quickprop 只需通过以下方式计算新的重量:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ng"><img src="../Images/5faeecba1cc456ae9440c36345fbc6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zWbpky9WAyjg9Sp53LVjLg.png"/></div></div></figure><h2 id="b5e2" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">把它编成代码</h2><p id="205d" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">在开始实际的 Quickprop 实现之前，让我们导入一些基础库:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="7fa8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有了前面数学等式的最后两行，我们可以从 Quickprop 开始！如果您阅读了关于级联相关的第一篇文章，您可能已经熟悉了这一点——在这里，我们将首先关注算法的基本部分，并在最后将它们放在一起。</p><p id="59aa" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lu">注意，我们使用 PyTorch 为我们进行自动梯度计算。我们还假设已经预先定义了激活和损失函数。</em></p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="94b3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是一个学习时期最简单的快速推进版本。要真正利用它，我们必须运行几次，看看损失是否收敛(我们稍后会谈到这一点)。</p><p id="0c62" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是，这种实现在几个方面存在缺陷，我们将在下面几节中研究并解决这些问题:</p><ul class=""><li id="b2bb" class="nj nk it ks b kt ku kw kx kz nl ld nm lh nn ll no np nq nr bi translated">我们实际上没有初始化任何<code class="fe ns nt nu nv b">..._prev</code>变量——在上一篇文章中，我用 1 静态地初始化了它们，但这也不是一个好主意(见下一点)</li><li id="f076" class="nj nk it ks b kt nw kw nx kz ny ld nz lh oa ll no np nq nr bi translated">权重增量变量可能会停留在零值上，因为它在自己的更新步骤中被用作一个因子</li><li id="3d32" class="nj nk it ks b kt nw kw nx kz ny ld nz lh oa ll no np nq nr bi translated">如果梯度“爆炸”,实现可能会过冲或通常无法收敛</li><li id="ed82" class="nj nk it ks b kt nw kw nx kz ny ld nz lh oa ll no np nq nr bi translated">如果梯度在一次迭代中不变，它将导致除以零</li></ul><h2 id="c444" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">改进:通过梯度下降初始化</h2><p id="fbf7" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">我们可以应用的第一个简单的修正是使用梯度下降(具有非常小的学习率)来准备<code class="fe ns nt nu nv b">dw_prev</code>和<code class="fe ns nt nu nv b">dL_prev</code>变量。这将使我们对损失函数地形有一个很好的初步了解，并在正确的方向上启动 Quickprop。</p><p id="5b52" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">再次使用 pytorch 很容易实现梯度下降——我们还将利用这个机会对上面的代码进行一点重构:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="a3f0" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">改进:有条件的渐变添加</h2><p id="04eb" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">有时，当使用快速推进抛物线法时，重量增量变得非常小。为了防止梯度不为零时发生这种情况，法尔曼建议有条件地将斜率添加到权重增量中。<br/>这个想法可以这样描述:无论如何如果你一直朝着那个方向前进，那就走得更远，但是如果你之前的更新把你送到了相反的方向，就不要再往前推了(防止振荡)。</p><p id="9230" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通过一小段 decider 代码，这可以很容易地实现:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="ef33" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有了这个，我们就可以把它全部放进一个小函数里:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="17d1" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">改进:最大生长因子</h2><p id="f9a2" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">第二步，我们将修复一些函数特征附近的爆炸权重增量问题(例如，奇点附近)。<br/>为此，Fahlman 建议，如果权重更新大于上次权重更新乘以最大增长因子，则截取权重更新:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="d36f" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">改进:防止被零除</h2><p id="b5a2" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">在某些情况下，先前和当前计算的斜率可以相同。结果是，我们将尝试在权重更新规则中被零除，并且之后将不得不继续使用<code class="fe ns nt nu nv b">NaN</code>的，这显然破坏了训练。<br/>这里简单的解决方法是做一个梯度下降步骤。</p><p id="6cb8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">遵守两个更新规则:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="ec34" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">除了最后一个因素，他们看起来很相似，不是吗？<br/>这意味着我们可以再次无分支(即，为我们节省一些 if 子句)，保持元素方式，并将所有内容打包在一个公式中:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="c970" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">细心的读者可能注意到了我们上面使用的“学习率”因素——一个我们认为可以去掉的参数… <br/>嗯，实际上我们多少去掉了，或者至少我们确实去掉了在培训过程中必须调整学习率的问题。快速推进学习率可以在整个过程中保持固定。开始时，每个域只需调整一次。实际的动态步长通过抛物线跳跃来选择，而抛物线跳跃又严重依赖于当前和最后计算的斜率。</p><p id="ee0d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果你认为这听起来非常熟悉反向传播学习率优化器的工作方式(想想:动量)，那你就对了。本质上，Quickprop 实现了与它们非常相似的东西——只是它的核心没有使用反向传播。</p><p id="b8a7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">回到代码上来:由于我们之前已经实现了梯度下降，我们可以在此基础上构建并尽可能多地重用:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="fb81" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">把所有的放在一起</h2><p id="5c6c" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">有了所有这些功能，我们可以把它们放在一起。仍然需要的样板代码只是进行初始化和检查每个时期平均损失的收敛。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="9e55" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">警告</h2><p id="a62a" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">Quickprop 有一个大大降低其实用性的主要警告:我们使用的数学“技巧”,即损失函数的二阶导数与简单差商的近似依赖于该二阶导数是连续函数的假设。<br/>这不适用于激活功能，例如整流线性单元，简称 ReLU。二阶导数是不连续的，并且算法的行为可能变得不可靠(例如，它可能发散)。</p><p id="67de" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">回顾一下<a class="ae lm" rel="noopener" target="_blank" href="/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92">我之前的文章</a>关于级联相关的实现，我们使用 Quickprop 训练网络的隐藏单元，并使用协方差函数作为一种方法来估计该过程中的损失。然而，协方差(如那里实现的)被包装在绝对值函数中。即它的二阶导数是不连续的，因此不应使用 Quickprop。仔细阅读 Fahlman 等人的级联相关论文[2]的读者可能已经注意到，他们实际上是使用梯度上升来计算这个最大协方差。</p><p id="c46d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">除此之外，Quickprop 似乎在某些领域比其他领域提供了更好的结果。Brust 等人的有趣总结表明，与基于反向传播的技术相比，它在一些简单的图像分类任务(对基本形状进行分类)上取得了更好的训练结果，而同时在更现实的图像分类任务上表现较差[3]。<br/>我没有在这方面做过任何研究，但我想知道这是否意味着 Quickprop 可能在更少模糊和更结构化的数据上工作得更好(想想商业环境中使用的数据框架/表格)。这肯定是一个有趣的研究。</p><h2 id="c78a" class="lv lw it bd lx ly lz dn ma mb mc dp md kz me mf mg ld mh mi mj lh mk ml mm mn bi translated">摘要</h2><p id="b9b4" class="pw-post-body-paragraph kq kr it ks b kt mo ju kv kw mp jx ky kz mq lb lc ld mr lf lg lh ms lj lk ll im bi translated">本文介绍了 Scott Fahlman 改进反向传播的想法。我们看了一下数学基础和可能的实现。</p><p id="0f46" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在开始在你自己的项目中尝试吧——我很想看看 Quickprop 能有什么用途！</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="bb32" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果你想看看 Quickprop 的变体，请查看我的关于级联相关学习架构的系列文章。</p><p id="fd3c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该系列的所有成品笔记本和代码也在 Github 上<a class="ae lm" href="https://github.com/ephe-meral/cascor" rel="noopener ugc nofollow" target="_blank">可用。请留下反馈并提出改进建议。</a></p><p id="c64f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后，如果你想支持这篇和类似精彩文章的创作，你可以<a class="ae lm" href="https://medium.com/@johannaappel/membership" rel="noopener">注册一个中级会员</a>和/或<a class="ae lm" href="https://medium.com/subscribe/@johannaappel" rel="noopener">关注我的账户</a>。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="324b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[1] S. E. Fahlman，<a class="ae lm" href="http://www.it.uu.se/edu/course/homepage/mil/vt11/handouts/fahlman.quickprop-tr.pdf" rel="noopener ugc nofollow" target="_blank">反向传播网络学习速度的实证研究</a> (1988)，卡内基梅隆大学计算机科学系</p><p id="fe59" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[2] S. E. Fahlman 和 C. Lebiere，<a class="ae lm" href="http://web.cs.iastate.edu/~honavar/fahlman.pdf" rel="noopener ugc nofollow" target="_blank">级联相关学习架构</a> (1990)，神经信息处理系统的进展(第 524-532 页)</p><p id="b534" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[3] C. A. Brust，S. Sickert，M. Simon，E. Rodner 和 J. Denzler，<a class="ae lm" href="https://arxiv.org/pdf/1606.04333.pdf" rel="noopener ugc nofollow" target="_blank">《既不快速也不恰当——学习深度神经网络的 QuickProp 评估》</a> (2016)，arXiv 预印本 arXiv:1606.04333</p></div></div>    
</body>
</html>