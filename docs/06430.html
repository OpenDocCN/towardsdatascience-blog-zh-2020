<html>
<head>
<title>Support Vector Machines explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-explained-25a685e4d228?source=collection_archive---------39-----------------------#2020-05-22">https://towardsdatascience.com/support-vector-machines-explained-25a685e4d228?source=collection_archive---------39-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ebee68b2291d096098221a509083d5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50fOoj4H4ASpMCEQLW7HvQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://www.pexels.com/es-es/foto/al-aire-libre-amanecer-arboles-asfalto-531321/" rel="noopener ugc nofollow" target="_blank">像素</a>的图像。</p></figure><div class=""/><div class=""><h2 id="4b73" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">了解关于支持向量机的一切</h2></div><p id="3ec6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">在本帖中，我们将</em> <strong class="la jk"> <em class="lu">揭开发生在SVM</em></strong><em class="lu"/><strong class="la jk"><em class="lu">身上的所有魔法，并讲述它们的一点历史</em> </strong> <em class="lu">，并阐明它们何时该用，何时不该用。</em></p><p id="f7cf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">我们将浏览支持向量机的理论和直觉</em> </strong> <em class="lu">，看到理解万物如何工作所必需的最少的数学量，而不深入细节。</em></p><p id="d944" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">言归正传！</em>T25】</strong></p><h1 id="49ad" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated"><strong class="ak"> 1。简介</strong></h1><p id="b796" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated"><strong class="la jk">支持向量机</strong>或SVMs是一个广泛使用的机器学习模型家族，它<strong class="la jk">可以解决许多ML问题</strong>，如线性或非线性分类、回归，甚至异常值检测。</p><p id="b29a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">话虽如此，<strong class="la jk">最好的应用</strong>是在<strong class="la jk">应用于小型</strong>或中型<strong class="la jk">复杂数据集</strong>的分类时。贯穿本文，将会清楚为什么。</p><p id="b8ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解支持向量机是如何工作的，我们最好先研究一下线性支持向量机、硬边界和软边界分类。</p><h1 id="a7bb" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">2.线性SVM分类</h1><p id="fec1" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">假设我们有以下一组数据，只有两个特征(<em class="lu">特征1和特征2 </em>，代表两个不同的类别(<em class="lu">A类</em>和<em class="lu">B类)</em>)。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/fe34c83f6290535a850505f66481a436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*WLMSoj1ItEDzvzrdTFZTzw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们数据的散点图</p></figure><p id="c62b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个正常的线性分类器会试图画一条线，完美地分开我们的两类数据。但是，从下图可以看出，有很多行可以做到这一点。<strong class="la jk">我们该选哪个？</strong></p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e8c137ab14cba857c826106d452a2e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*bN4zevx7QsnpZnEUQyX1rA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">使用线性分类器分离两类</p></figure><p id="02f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">之前的</strong>决策边界<strong class="la jk">将训练数据</strong>完全分离，<strong class="la jk">然而</strong>它们如此接近训练实例(红色和紫色的点，周围有黑色圆圈)，以至于<strong class="la jk">它们可能会对新数据</strong>(图中的<em class="lu">新样本</em>)进行非常糟糕的概括。</p><h2 id="1fbc" class="mx lw jj bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">硬边界和软边界分类器</h2><p id="cf1f" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">支持向量机分类器试图通过对模型拟合一条线来解决这个问题，该线试图<strong class="la jk">最大化到最近训练实例</strong>(称为<em class="lu">支持向量</em>)的距离，以便平行于决策边界线的余量尽可能宽。</p><p id="bc05" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将决策边界想象成乡村道路的中心，将数据想象成树，道路的每一侧都有不同类型的树。支持向量机试图做的是找到尽可能宽的道路来分隔我们的两种树，这样我们就可以安全地开车穿过它，同时感到安全。它通过试图最大化利润来做到这一点。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/e06fdfbc34c927e9eb297dd46781eb22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqGQ36tPCXGnhEyTewQ2dA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://www.flaticon.com/free-icon/tree_489969" rel="noopener ugc nofollow" target="_blank"> Flaticon的支持向量机图标背后的直觉。</a></p></figure><p id="d502" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们现在考虑之前的数据集，对其拟合线性支持向量分类器(线性SVC ),并绘制决策边界及其余量，我们会得到下图:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/84d9e97a155420c772f7655eca6e85cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*N_SVyMgKcC1iW_TF26OstQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">线性SVC的判定边界</p></figure><p id="dec2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种类型的SVC中，<strong class="la jk">没有点被允许越过边界</strong>:我们正在谈论一个<strong class="la jk"> <em class="lu">硬边界分类器</em> </strong>。当一些点被允许穿过边缘线，允许我们适应更宽的街道时，我们谈论的是一个<em class="lu">软边缘分类器</em>。</p><p id="4f3d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，对于这个<em class="lu">硬边界分类器</em>，<strong class="la jk">来说，拟合只取决于<em class="lu">支持向量</em> </strong>的位置:向我们的训练数据添加不接触街道(但在正确的一侧)的点，将使决策边界和边界保持不变。</p><p id="8511" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">边界由这些关键<em class="lu">支持向量</em>点完全定义或<em class="lu">支持</em>，给它们和算法命名。<strong class="la jk"> <em class="lu">硬边界分类器</em> </strong>然而，只有当数据以线性方式完全可分时才起作用，并且对异常值也非常敏感。</p><p id="425d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了解决这个问题，我们进入了<strong class="la jk"> <em class="lu">软边界分类器</em> </strong>，这是一种更加灵活的模型，它<strong class="la jk">允许一些点穿过边界</strong>，这使我们在街道有多宽以及有多少<em class="lu">边界违规</em>(位于街道内部或决策边界错误一侧的点)之间达成妥协。下图显示了一个线性软边距分类器。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/27dc20cc67c85ce0948fe0f7aabf7cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*tDE_VSgmoTQJH0xREcdG5w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">软边界分类器</p></figure><p id="3a23" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在大多数SVC实现中，<strong class="la jk">我们用模型</strong>(Scikit-Learn中的C)的超参数来控制它。此参数的值越低，意味着我们的利润越大，违反利润的情况也越多，因此模型更加灵活，可以更好地概括。通过减少C，我们可以调整我们的模型，如果我们认为它可能是过度拟合。</p><p id="fd3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">增加C的值将我们引向硬边界分类器。下图显示了这种行为:随着C值的减小，街道变宽了，但是我们有更多的点穿过它。</strong></p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/5293d2c8d6a6d0f426fbbcb0b0214502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0e5CYZRO9qcJLRi4djmeg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">当我们减小c的值时，SVC分类器的裕量发生变化。</p></figure><h1 id="22b8" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">非线性SVM分类器</h1><p id="097f" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">然而，大多数时候，数据集并不是线性可分的，软化我们的利润并不完全奏效。</p><p id="a935" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使非线性可分离数据集成为可分离数据集的一种方法是<strong class="la jk">包括从原始数据集</strong>中得到的附加特征，例如使用<strong class="la jk">多项式特征</strong>或相似性特征技术，如<strong class="la jk"> <em class="lu">径向基函数</em> </strong> (RBF)。</p><p id="08d1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了如何使用<strong class="la jk">二次多项式</strong>将只有1个要素的数据集转换为2个要素的数据集，从而线性分离两个现有的类。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/af4fa1cb3de382fbe87c6bb6c41f4920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*354ccCkXAVMDj1REBBpEzg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">将一个非线性可分离的数据集转换成一个可以用二次多项式线性分离的数据集</p></figure><p id="2629" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的问题是，随着要素数量的增加，计算高次多项式要素的<strong class="la jk">计算成本</strong>急剧增加，使得模型运行缓慢。</p><p id="a758" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">另一种使数据集可线性分离的方式</strong><strong class="la jk">是前面提到的</strong><strong class="la jk"><em class="lu">径向基函数</em> </strong> <em class="lu"> </em>。下面的例子显示了我们如何将一个非线性可分的数据集转换成一个可以使用RBF轻松划分的数据集:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/78bdfc5119e7a2e7fd61e7af65418cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*swDrKuc9SR_xYOFiHgtc4A.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">具有两个特征的两个不可分离类的散点图。</p></figure><p id="5af6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果在前面图像所示的数据集上，<strong class="la jk">我们使用以原点</strong>为中心的径向基函数计算第三个特征、<strong class="la jk">，并且我们将这个新特征与前两个特征一起绘制，我们的数据集被转换成下面的数据集，它可以被新特征<em class="lu">r</em>T31】上高度为0.7的水平面线性分隔。</strong></p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bafc6b25e6443970a6c09aeecc562cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/1*p-rc6-N4Bma6gATeKqR_yg.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">先前数据的表示，带有使用RBF计算的附加特征</p></figure><p id="7b54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在前面的例子中，我们<strong class="la jk">选择了一个点</strong>来集中我们的RBF，这给了我们很好的结果，在大多数情况下<strong class="la jk">这不是一件小事</strong>。大多数情况下<strong class="la jk">所做的是<strong class="la jk">使用原始特征计算从数据集中的每个点</strong>到所有其他数据点的径向基函数，并使用这些径向基函数计算的距离作为新特征。</strong></p><p id="a753" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的问题是，<strong class="la jk">如果我们有一个大的训练集</strong>，计算所有这些特征是非常<strong class="la jk">计算昂贵的</strong>。当试图使用线性SVC来分离非线性数据时，我们再次遇到问题。</p><p id="0b1b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这些问题的解决方案中，支持向量机的真正力量在于:<strong class="la jk">内核。</strong></p><h1 id="c973" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">内核技巧</h1><p id="2288" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在前面的两个例子中，为了能够分离我们的数据，我们需要计算一个变换(一个多项式变换，和一个RBF相似性函数)，正如我们所看到的，这是一个计算量非常大的过程。</p><p id="707b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">机器学习中的内核是一个数学函数</strong>，它允许我们计算两个向量的变换的点积，而实际上不必计算变换本身。</p><p id="ecde" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的公式说明了内核背后的思想。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/e75a9f82b7abb0c5d8b382c7d2759f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHAK0yFj-oJw-foVUJX9xQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公式1:向量a和b的核</p></figure><p id="6813" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">前面公式中的<em class="lu"> K(a，b) </em>代表向量的<strong class="la jk">核<em class="lu"> a </em>和<em class="lu"> b </em> </strong>(一个随机核，后面我们会看到有很多不同的)。我们可以看到，这个内核等于<em class="lu"> ϕ(a) </em>和<em class="lu"> ϕ(b) </em>  <em class="lu">，</em>的<strong class="la jk">点积，其中<strong class="la jk"> <em class="lu"> ϕ </em>表示作为参数提供给它的向量的特定变换</strong>(可以是多项式，也可以是RBF变换)。</strong></p><p id="da99" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解这如何极大地简化了我们的问题，并且通过使用核，我们可以使用支持向量机来对非线性可分数据集进行分类，我们最好先看看<strong class="la jk">支持向量机是如何被训练的</strong>。</p><h1 id="18cf" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">训练SVM:另一个优化问题</h1><p id="faca" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">最后，<strong class="la jk">训练一个SVM分类器，归结为解决一个优化问题</strong>，称为<em class="lu">对偶问题</em>。SVM像任何其他分类器一样进行预测:它获取输入向量<em class="lu"> x </em>，将其乘以某个权重向量<em class="lu"> w </em>，并添加一个偏差项<em class="lu"> b </em>，如以下公式所示。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1e51ea58de27d4da4ab68d6297facf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*bYfqcaT6QDdMn2Qcn_G1sg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公式2:预测方程。</p></figure><p id="5c9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果这个操作给出的结果大于某个阈值(在我们的例子中是0)，那么这个样本被分类为阳性实例(1)。如果它产生的结果低于阈值，那么它被归类为负实例(0)。</p><p id="3959" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">为了获得权重和偏差向量，我们必须解决一个优化问题</strong>，该问题试图最大化我们谈到的街道的利润，同时限制利润违规。从数学上来说，这被转化为找到满足某个模糊条件或<em class="lu">松弛</em>(多少次以及有多严重的余量可以被越过)的权重向量的最小值。</p><p id="768e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个优化问题可以使用普通的QP求解器来解决，因为它是一个凸二次问题。这个问题解决的最终方程如下</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/7f9e5bc2acb5b415c250fa18fdb4e21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RpwSMeeItozC-Jd6ZOxrkw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公式3:优化。</p></figure><p id="cec3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，公式3并不完全正确。<strong class="la jk"> <em class="lu"> α </em> </strong>应该是<strong class="la jk">大于或等于0 </strong>，对于每个不是支持向量的数据点取这个最后的值。</p><p id="2e68" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要在意<em class="lu"> t(i) </em>或<em class="lu"> t(j) </em>，关于这个方程有两件重要的事情:一旦我们求解它并得到<strong class="la jk"> <em class="lu"> α </em> </strong>的值，我们就可以计算权重和偏置向量。另一件要注意的事情是橙色虚线内的术语:<strong class="la jk">两个训练实例的点积</strong>，我们必须对所有训练集重复这个术语。</p><p id="f469" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">这就是我们之前谈到的内核技巧派上用场的地方:</strong>如果我们使用任何类型的转换来转换我们的训练实例，使它们可以分离，我们将不得不计算每个训练实例上的转换，做点积……从计算的角度来看，这将是非常昂贵的。这显示在下面的公式中。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/fcf00c102f183ce08cddadbf4e04562f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKE8iDak9iFhQlS4jFSNHA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公式4:对我们的数据计算变换，并用它们代替内核。</p></figure><p id="f0e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">如果</strong>像上面一样，<strong class="la jk">我们使用一个内核</strong>，那么<strong class="la jk">我们不需要实际计算变换</strong>。酷吧？让我们看看一些最流行的内核:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/221a889e859fc7a147adde5a5ef9d076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*p92WCWk8A7AaOoyy10qO6g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一些最流行的内核。</p></figure><p id="6a81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，对于我们应用了d次多项式变换的两个向量的点积，理论上我们会增加数据的n个特征，从而增加复杂性，我们可以使用内核来避免计算该变换，只需计算<strong class="la jk"> <em class="lu"> (γaTb+r)d. </em> </strong></p><p id="33ab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是支持向量机的魅力所在。我们可以训练它们并做出预测，这有利于对我们的数据进行运算，而不需要实际计算这些运算。</p><p id="5e00" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们结束吧，看看这些预测是如何做出的。</p><h2 id="33cd" class="mx lw jj bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">用SVM进行预测:支持向量</h2><p id="e54f" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">正如我们之前看到的，为了进行预测，我们必须计算权重向量和新数据点<em class="lu"> x(n) </em>的点积，并添加一个偏差项。为此，如果我们将通过求解优化问题获得的<em class="lu"> w </em>的值代入公式2的方程，我们得到:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a908e2b6c3c26ef70747a7d4a1a95089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Bi8brk4ypmhovxeXIg8Eiw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公式5:用SVM预测</p></figure><p id="fe34" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，还记得我们怎么说<strong class="la jk"> <em class="lu"> α </em> </strong> <em class="lu">对于所有不是</em> <strong class="la jk"> <em class="lu">支持向量的数据点都是0吗？</em> </strong>这意味着，为了进行预测，我们只需计算支持向量的核和我们的新数据点，并添加偏差项。<strong class="la jk">牛逼吧？</strong></p><p id="dab0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">内核和支持向量机再次展示了它们的魔力。最后，这里有一些使用支持向量机的实用技巧和诀窍。</p><h1 id="ba91" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">其他提示和技巧:</h1><ul class=""><li id="7d35" class="nw nx jj la b lb mn le mo lh ny ll nz lp oa lt ob oc od oe bi translated"><strong class="la jk">在拟合SVM之前标准化或规范化数据。</strong>在训练SVM之前，应用某种缩放技术是很重要的。如果我们的特征具有非常不同的比例，我们的‘街道’将看起来非常怪异，并且我们的算法将不会很好地工作。</li><li id="d179" class="nw nx jj la b lb of le og lh oh ll oi lp oj lt ob oc od oe bi translated">根据经验，<strong class="la jk">在拟合SVM时，首先使用线性核</strong>，尤其是当训练集非常大或者它具有大量特征时。如果你对结果不满意，<strong class="la jk">试试RBF核</strong>；它往往工作得很好。然后，如果你有稀疏的时间和计算能力，尝试另一种交叉验证的内核。</li><li id="4098" class="nw nx jj la b lb of le og lh oh ll oi lp oj lt ob oc od oe bi translated">支持向量机也可用于<strong class="la jk">回归和异常值检测。</strong></li><li id="e419" class="nw nx jj la b lb of le og lh oh ll oi lp oj lt ob oc od oe bi translated">然而，它们的<strong class="la jk">最佳结果</strong>出现在用于复杂但较小数据集的<strong class="la jk">分类任务时。</strong></li></ul><h1 id="264f" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">结论</h1><p id="e44e" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated"><em class="lu">仅此而已！我们已经了解了支持向量机背后的直觉，以及它们如何工作的一些数学知识。</em></p><p id="fdc0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">关于机器学习和数据科学的更多资源，请查看以下资源库:</em> <a class="ae jg" href="https://howtolearnmachinelearning.com/books/machine-learning-books/" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="lu">如何学习机器学习</em> </strong> </a> <em class="lu">！有关职业资源(工作、事件、技能测试)，请访问</em><a class="ae jg" href="https://aigents.co/" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"><em class="lu">AIgents.co——一个面向数据科学家的职业社区&amp;机器学习工程师</em></strong> </a> <em class="lu">。</em></p><p id="1174" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">保重，享受人工智能！</p><p id="bc5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">感谢阅读！</em></p></div></div>    
</body>
</html>