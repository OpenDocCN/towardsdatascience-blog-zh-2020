<html>
<head>
<title>Value Function Approximation — Prediction Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">价值函数逼近—预测算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=collection_archive---------22-----------------------#2020-03-22">https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=collection_archive---------22-----------------------#2020-03-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3441" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">强化学习之旅</a></h2><div class=""/><div class=""><h2 id="dbf9" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">大型未知环境的学习方法</h2></div><p id="b45f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">欢迎再次深入强化学习！这一次，我们将回顾价值函数近似，更具体地说，它背后的预测算法，理解它的用途，并围绕实现进行思考。在我的下一篇文章中，我将像往常一样，通过将<em class="ln">预测</em>和<em class="ln">控制</em>与一个挑战结合起来，将价值函数近似法绑在一起。然而，这篇文章将主要包括价值函数逼近和我们目前所学的一切之间的联系。</p><p id="ea81" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们逐渐意识到的，有可能遇到非常大的问题或状态空间；甚至是无限/连续的状态空间。有时，环境就像gridworld一样简单，就像国际象棋等游戏的状态空间一样定义明确，但情况并非总是如此。例如，在机器人领域，遥控直升机可能会遇到无限多的情况/状态，无法对每一种情况/状态的实际值进行分类(以前从未见过类似的情况)。</p><p id="8e2d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我在<a class="ae lo" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">专栏</a>的前几篇文章中提到了这篇文章中使用的很多符号。我会在文章底部贴上非常有用的资源，帮助我理解这些概念。我们来看看什么是价值函数逼近。</p></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="b984" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">回顾过去，我们会注意到，我们一直在通过非常具体地表示价值函数，利用<em class="ln">查找表</em>来解决马尔可夫决策过程(MDP)。每个状态<em class="ln"> s </em>都有一个条目<em class="ln"> V(s) </em>，每个状态-动作对<em class="ln"> s，a </em>都有一个条目<em class="ln"> Q(s，a) </em>。实际上，我们能够查看我们的表，并通过最大化所有可能的行动来决定下一步做什么。</p><p id="946c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在大型MDP事件中使用这种策略会暴露出两个问题，第一个问题是最终我们会耗尽内存。在某些时候，将会有太多的状态和/或动作需要存储。第二个问题很简单，即使我们有足够的内存，单独估计每个值的过程也太慢了。价值函数逼近是解决这一问题的方法。</p><p id="874e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">价值函数逼近试图通过创建使用较少参数的价值函数的紧凑表示来构建一些函数以估计真实的价值函数:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/b823bf5ee83a97f99c22ce6eec604c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*JKPPRHyI-kvX9j_5QjTPHg.png"/></div></figure><p id="94d0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一种常见的做法是使用深度学习——在这种情况下，神经网络的权重是权重向量<strong class="kt jd"><em class="ln"/></strong>，它将用于估计整个状态/状态-动作空间的价值函数。这个权重向量将使用我们之前见过的方法更新，<em class="ln">蒙特卡罗</em>或<em class="ln">时间差</em>学习。</p><p id="8249" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将从查看如何利用价值函数近似<em class="ln"> </em>中的<em class="ln">随机梯度下降</em>来调整每个示例后的权重向量开始。目标是找到使近似值函数和真实值函数之间的均方误差最小化的参数向量<strong class="kt jd"> w </strong>。梯度下降通过找到局部最小值来做到这一点:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/1c6186e933247ab306db03363dc2d160.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*4tvsoWn1A7JbVw7bJtRoSQ.png"/></div></figure><p id="6d77" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了更好地理解这个概念，让我们来理解什么是<em class="ln">特征向量</em>。每个特征都告诉我们关于状态空间的任何事情。它允许我们严格定义我们用什么来代表环境/我们与环境的互动。通过将特征的线性组合编辑在一起，我们用加权和表示价值函数。从数学上讲，我们的特征向量和我们的权重向量的点积将是我们对价值函数的估计。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/741e38299d22409c525a7149fc757631.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*OQmXw3fVXiFpoG9d9Agapw.png"/></div></figure><p id="1363" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的目标函数，我们希望优化的均方误差函数，在参数<strong class="kt jd"> w </strong>中变成二次函数。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/7d763d2305404e65299c5e37862640f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*YJL2P9W1J0ufx96J_ShesQ.png"/></div></figure><p id="6254" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用以下更新规则，随机梯度下降收敛于<em class="ln">全局</em>最优:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/b09d82303ef295ed48556412e12d6e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*vmxOmOrUlICBF5PsorKFnw.png"/></div></figure><p id="ad93" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们权重的变化来自于一小步，取决于步长参数，根据特定情况调整<em class="ln">相关的</em>功能。</p><p id="c2fa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，为了使这与强化学习相关，我们必须利用我们的学习方法。显然，最初并不知道真正的价值函数。因此，我们在一个<em class="ln">目标中代入</em>。在蒙特卡罗学习中，目标是收益<em class="ln"> Gₙ </em></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/45fd96892ac75b723d2efe4c08df2a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*dTRQ2rMVxfb5XcZdCLZm8g.png"/></div></figure><p id="8b62" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在TD(0)中，目标是TD目标:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8151ab161f7b9a95999dd2aaf8d4937d.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*yMFjF6dp6-Pv9u56L6kLSg.png"/></div></figure><p id="2707" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">更新看起来像这样:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/9fb6b08dbd46c90d1e374e70d47c3e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*Md9Rt-PKv6kVWMvBCj1bHA.png"/></div></div></figure></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="c895" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作为即将到来的高峰，下图应该看起来很熟悉！我们将基于<em class="ln">广义策略迭代</em> (GPI)的思想，利用价值函数逼近进行控制。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/219e536e8179e60d05e673da5acdffef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*u-InqznB84CECrZ6NzQp_A.png"/></div></figure><p id="bc2b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">类似于上面的状态值函数近似，我们可以用随机梯度下降来近似动作值函数。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/ba9ba9a2acdb65c61380520cadaef849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*OVNovFkeKQaO1JTwark4pA.png"/></div></figure></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="77f9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一篇文章中，我们将能够将这些预测信息与必要的控制方法结合起来，以便能够使用价值函数近似法实际解决问题。感谢阅读！</p></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><h1 id="86f5" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">资源</h1><p id="e28e" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated"><a class="ae lo" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">强化学习:萨顿<em class="ln">和巴尔托</em> </strong></a>介绍</p><p id="5e8a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">YouTube上大卫·西尔弗的RL课程</p><p id="6263" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae lo" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">强化学习Github </a> by <em class="ln"> dennybritz </em></p></div></div>    
</body>
</html>