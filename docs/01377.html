<html>
<head>
<title>Ignite the Spark!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点燃火花！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ignite-the-spark-68f3f988f642?source=collection_archive---------7-----------------------#2020-02-07">https://towardsdatascience.com/ignite-the-spark-68f3f988f642?source=collection_archive---------7-----------------------#2020-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d47" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用PySpark在Kubernetes上运行Apache Spark</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7c08466d571ed02634f52c28f3b2abac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gj_mxMOlCQ3vgIjnDKeY-Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:免费图片来自Pixabay</p></figure><h1 id="a703" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">简介</strong></h1><p id="1f6b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在这本初级读本中，您将首先了解一些Apache Spark的集群管理器是如何工作的，然后了解如何在现有的Kubernetes (k8s)集群上以交互方式在Jupyter笔记本中运行PySpark。</p><p id="580f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">完成本文后，您应该能够自信地在任何Kubernetes集群上开发Spark应用程序，并对两者之间的联系有更深的理解。</p><h1 id="0fb7" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">这都是关于环境的</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/afde587281ce3d826aa7b549a74dc53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*EzZs4uEuO30lV51KV07_RA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:Apache文档</p></figure><p id="021f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Spark是一个快速通用的<strong class="lp ir">集群计算系统</strong> <em class="mp"> </em>，这意味着根据定义，计算是以分布式方式在许多互连的节点上共享的。</p><p id="8408" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是Spark实际上是如何在集群中分配给定的工作负载的呢？</p><p id="efe8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Spark采用主/从方式，其中一个<strong class="lp ir">驱动程序</strong>(“主”)创建一个<strong class="lp ir"> SparkContext </strong>对象，该对象连接到<strong class="lp ir"> </strong>一个<strong class="lp ir">集群管理器</strong>。</p><p id="96b8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">创建的SparkContext(由您喜欢的语言绑定以编程方式创建，或者在您提交作业时以您的名义创建)将集群抽象为一个大型计算节点，您的应用程序使用它来执行工作。</p><p id="99c2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">另一方面，<strong class="lp ir">集群管理器</strong>负责生成和管理大量<strong class="lp ir">工作节点</strong>(“从节点”)，每个节点代表SparkContext运行一个<strong class="lp ir">执行器进程</strong>来完成实际工作。</p><p id="c1f5" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">当Spark应用程序操作<strong class="lp ir">弹性分布式数据帧(RDD) </strong>形式的数据时，RDD被分割成多个分区，分布在这些工作节点/执行器组合上进行处理。最终结果在节点间汇总并发送回驱动程序。</p><h1 id="2aa8" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">选择一个管理器，任何集群管理器</h1><p id="69ab" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这种设计的一个主要优点是集群管理器与应用程序是分离的，因此可以互换。</p><p id="db4e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">传统上，Spark支持三种类型的集群管理器:</p><ul class=""><li id="58b8" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated"><strong class="lp ir">独立</strong></li><li id="a19e" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated"><strong class="lp ir">阿帕奇Mesos </strong></li><li id="b395" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated"><strong class="lp ir"> Hadoop YARN </strong></li></ul><p id="73d0" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae ne" href="https://spark.apache.org/docs/latest/spark-standalone.html" rel="noopener ugc nofollow" target="_blank">独立</a>集群管理器是默认的集群管理器，随Spark的每个版本一起提供。这是一个没有虚饰，有能力的经理，这意味着让你尽快启动和运行。</p><p id="b81b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Apache Mesos 本身就是一种集群技术，旨在将集群的所有资源抽象化，就像一台大型计算机一样。Mesos附带了一个集群管理器，您可以在Spark中利用它。</p><p id="07e9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae ne" href="https://spark.apache.org/docs/latest/running-on-yarn.html" rel="noopener ugc nofollow" target="_blank"> Hadoop YARN </a>(“又一个资源谈判者”)是作为Apache Hadoop项目的副产品开发的，主要专注于分发MapReduce工作负载。因此，它也是Spark可以与之进行本地对话的集群管理器。</p><h1 id="e1f2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">进入Kubernetes</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/3aef08845c50891b3a469fa52d14ee6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkWi16Clt_6RjLJNmyLVkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:Apache文档</p></figure><p id="8982" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">从2.3.0开始，Spark现在支持直接使用Kubernetes作为集群管理器。</p><p id="2078" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是那个<em class="mp">到底是什么意思呢？</em></p><p id="be73" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这取决于你想如何在Kubernetes上运行Spark。</p><p id="4dca" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在<strong class="lp ir">集群模式</strong>中，在您使用<strong class="lp ir"> spark-submit </strong>提交应用程序之后，代表您的应用程序创建的SparkContext将要求<strong class="lp ir"> kube-apiserver </strong>设置一个驱动程序节点和一些相应的工作节点(Pods ),并在它们之上运行您的工作负载。</p><p id="d7f6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">一旦所有数据处理完成，在拆除时，临时工作节点窗格将自动终止，但驱动程序节点窗格将保留，以便您可以在手动删除它之前检查任何日志。</p><p id="1a5e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在<strong class="lp ir">客户端模式</strong>中，您创建Spark driver节点作为Pod，然后在最终提交工作之前，使用您最喜欢的语言的API绑定创建SparkContext。</p><p id="70b0" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">出于几个原因，本初级读本将着重于配置客户端模式:</p><ul class=""><li id="3a58" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">web上的大多数文档都是以集群模式提供的</li><li id="3768" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">使用spark-submit提交应用程序的步骤与使用现有的集群管理器没有什么不同</li><li id="c361" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">大多数Spark开发人员都希望使用客户机模式，这样他们就可以开发新的应用程序并交互式地探索数据</li></ul><h1 id="05ec" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">先决条件</h1><p id="5ad6" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">以下说明假设如下:</p><ul class=""><li id="61a2" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">您对Kubernetes集群或该集群中的专用名称空间拥有管理权限</li></ul><p id="f596" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我正在运行一个小型的1.17.2八节点集群，它是使用<a class="ae ne" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" rel="noopener ugc nofollow" target="_blank"> kubeadm </a>工具安装的。我相信任何集群1.15+都应该工作得很好。</p><p id="925b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果你没有集群，我强烈建议在你的桌面上安装<a class="ae ne" href="https://kubernetes.io/docs/tasks/tools/install-minikube/" rel="noopener ugc nofollow" target="_blank"> MiniKube </a>作为一种跟进方式。</p><ul class=""><li id="05dc" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">您对Kubernetes的核心概念有些熟悉，比如pod、部署和服务</li><li id="b484" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">您已经安装了Docker，理解了容器背后的概念，并且能够轻松地使用Docker文件构建自定义映像</li><li id="5506" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">您可以访问内部、云中或外部的Docker注册中心，如<a class="ae ne" href="https://hub.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker Hub </a>。</li><li id="34f7" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">在撰写本文时，您使用的是Spark的最新稳定版本v2.4.4</li></ul><h1 id="1695" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">集群设置</h1><p id="8101" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在我们开始构建映像和部署Pods之前，让我们设置一个专用的名称空间，我们的combo Jupyter notebook/Spark driver节点以及所有对应的worker节点都将部署在这个名称空间中。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="fc78" class="nl kw iq nh b gy nm nn l no np">$ kubectl create namespace spark<br/>namespace/spark created</span></pre><p id="e9b9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">为了让你的驱动程序Pod从集群中请求资源，它必须使用某种类型的<strong class="lp ir">服务帐户</strong>，该帐户拥有正确的<strong class="lp ir">角色基础访问控制(RBAC) </strong>。</p><p id="08ba" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Spark文档建议创建一个<strong class="lp ir">角色绑定</strong>或<strong class="lp ir">集群角色绑定</strong>来完成这个任务。选择实现哪种方法完全取决于集群的安全策略。</p><p id="fca7" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">对于本例，我们将在<strong class="lp ir"> spark名称空间</strong> <em class="mp"> </em>中创建一个名为<strong class="lp ir"> spark </strong>的ServiceAccount，并使用ClusterRoleBinding赋予该帐户适当的权限:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="caca" class="nl kw iq nh b gy nm nn l no np">$ kubectl create serviceaccount spark -n spark<br/>serviceaccount/spark created</span><span id="93c8" class="nl kw iq nh b gy nq nn l no np">$ kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=spark:spark --namespace=spark<br/>clusterrolebinding.rbac.authorization.k8s.io/spark-role created</span></pre><h1 id="37ee" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">Docker设置</h1><p id="b364" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如上所述，由于每个worker节点实际上只是我们集群中的一个Pod，所以我们需要一个docker映像和正确的Spark运行时。</p><p id="e27a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">幸运的是，Spark团队提供了一组<strong class="lp ir"> Dockerfiles </strong>和一个工具(<strong class="lp ir"> docker-image-tool.sh </strong>)来创建映像，这些映像可以部署在任何Kubernetes集群上。</p><p id="3911" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">每个docker文件都要与Spark支持的三种主要语言绑定一起使用——Scala、Python和r。</p><p id="8895" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们构建默认图像，这样我们至少有一些基础图像可以使用:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="5b7b" class="nl kw iq nh b gy nm nn l no np">$ wget -qO- <a class="ae ne" href="http://mirrors.gigenet.com/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">http://mirrors.gigenet.com/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</a> | tar -xzf -</span><span id="daa5" class="nl kw iq nh b gy nq nn l no np">$ cd spark-2.4.4-bin-hadoop2.7 &amp;&amp; bin/docker-image-tool.sh build<br/>Sending build context to Docker daemon  262.2MB<br/>...<br/>Successfully built 02fb36ac3ee0<br/>Successfully tagged spark:latest<br/>Sending build context to Docker daemon  262.2MB<br/>...<br/>Successfully built 0d33be47d094<br/>Successfully tagged spark-py:latest<br/>Sending build context to Docker daemon  262.2MB<br/>...<br/>Successfully built dc911ac3678e<br/>Successfully tagged spark-r:latest</span></pre><p id="055f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">docker-image-tool.sh脚本完成后，您应该有三个新的映像准备好部署在Kubernetes集群上:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="bc77" class="nl kw iq nh b gy nm nn l no np">$ docker images spark:latest<br/>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br/>spark               latest              02fb36ac3ee0        7 days ago          344MB</span><span id="c234" class="nl kw iq nh b gy nq nn l no np">$ docker images spark-py:latest<br/>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br/>spark-py            latest              0d33be47d094        7 days ago          432MB</span><span id="1392" class="nl kw iq nh b gy nq nn l no np">$ $ docker images spark-r:latest<br/>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br/>spark-r             latest              dc911ac3678e        7 days ago          539MB</span></pre><p id="2e99" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">由于本教程将集中使用PySpark，我们将为我们的worker Pod使用<strong class="lp ir"> spark-py </strong>图像。通常，您只需将这些映像推送到集群使用的任何docker注册表中。但是，我们将创建它们的自定义版本，以便解决一个bug。</p><h1 id="a2bd" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">通信故障</h1><p id="80fc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在撰写本文时，由于最近修补了一个安全CVE，在较新的集群上使用现有的Spark v2.4.4 Kubernetes客户端jar文件存在问题。</p><p id="d24d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">你可以在这里和<a class="ae ne" href="https://stackoverflow.com/questions/57643079/kubernetes-watchconnectionmanager-exec-failure-http-403" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多关于这个问题的信息。</p><p id="9903" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">解决方法是构建一个启用Spark的docker映像，但是其中包含较新的客户机jar文件。</p><p id="3b91" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们使用我们刚刚构建的基础映像来实现这一点。因为我们关注的是使用PySpark，所以我们将只重建spark-py映像，但是相同的步骤适用于所有映像。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="8e28" class="nl kw iq nh b gy nm nn l no np">$ mkdir my-spark-py &amp;&amp; cd my-spark-py</span><span id="87f6" class="nl kw iq nh b gy nq nn l no np">$ wget -qO- <a class="ae ne" href="http://apache.mirrors.hoobly.com/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">http://apache.mirrors.hoobly.com/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz</a> | tar -xzf -</span><span id="ff73" class="nl kw iq nh b gy nq nn l no np">$ cp spark-3.0.0-preview2-bin-hadoop2.7/jars/kubernetes*jar . &amp;&amp; rm -f <a class="ae ne" href="http://apache.mirrors.hoobly.com/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz</a></span><span id="4fa9" class="nl kw iq nh b gy nq nn l no np">$ cat &lt;&lt; EOF &gt; Dockerfile<br/>FROM spark-py:v2.4.4</span><span id="e0a9" class="nl kw iq nh b gy nq nn l no np">COPY *.jar /opt/spark/jars/<br/>RUN rm /opt/spark/jars/kubernetes-*-4.1.2.jar<br/>EOF</span><span id="5ed3" class="nl kw iq nh b gy nq nn l no np">$ docker build --rm -t my-spark-py:v2.4.4 .<br/>...<br/>Successfully built 29abceec5cb2<br/>Successfully tagged my-spark-py:v2.4.4</span></pre><p id="5ae9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">所以现在<strong class="lp ir"> my-spark-py:v2.4.4 </strong>包含了相同的官方spark运行时，但是更新了来自v3.0.0-preview2版本的Kubernetes客户端jar文件。</p><p id="611b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">每当我们生成SparkContext时，Kubernetes会根据需要创建一个worker Pod，并提取该图像。</p><h1 id="fa90" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">创建“驱动程序”窗格图像</h1><p id="d93e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">因为我们的最终目标是在Jupyter笔记本中的Kubernetes上交互开发PySpark应用程序，所以这个Pod也将充当我们的“驱动程序”节点。</p><p id="64a0" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">因此，您有两种选择:</p><ul class=""><li id="473c" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">使用<a class="ae ne" href="https://hub.docker.com/r/jupyter/pyspark-notebook/" rel="noopener ugc nofollow" target="_blank">官方Jupyter PySpark图片</a></li><li id="930b" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated"><a class="ae ne" href="https://github.com/pisymbol/docker/blob/master/spark/Dockerfile" rel="noopener ugc nofollow" target="_blank">打造自己的定制形象</a></li></ul><p id="8995" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">PySpark docker官方图片是一个很好的起点，我强烈推荐它。</p><p id="c261" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我创建了一个自定义的docker图片，你可以点击上面的链接找到它。我把这张图片叫做<strong class="lp ir">我的笔记本:最新的</strong>。</p><p id="c96c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">您应该构建、标记和推送您最终选择的任何映像到您的集群使用的docker注册表。</p><h1 id="aeb0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">设置您的部署</h1><p id="e79c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">正如Kubernetes的所有事情一样，我们需要编写一个YAML文件来部署我们的驱动程序Pod，它将用于登录Jupyter并创建一个SparkContext。</p><p id="79eb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">以下是我的部署情况:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="b9df" class="nl kw iq nh b gy nm nn l no np">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/><strong class="nh ir">  namespace: spark</strong><br/>  name: my-notebook-deployment<br/>  labels:<br/>    app: my-notebook<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: my-notebook<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: my-notebook<br/>    spec:<br/>      <strong class="nh ir">serviceAccountName: spark</strong><br/>      containers:<br/>      - name: my-notebook<br/>        image: pidocker-docker-registry.default.svc.cluster.local:5000/my-notebook:latest<br/>        ports:<br/>          - containerPort: 8888<br/>        volumeMounts:<br/>          - mountPath: /root/data<br/>            name: my-notebook-pv<br/>        workingDir: /root<br/>        resources:<br/>          limits:<br/>            memory: 2Gi<br/>      volumes:<br/>        - name: my-notebook-pv<br/>          persistentVolumeClaim:<br/>            claimName: my-notebook-pvc<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  namespace: spark<br/>  name: my-notebook-deployment<br/>spec:<br/>  selector:<br/>    app: my-notebook<br/>  <strong class="nh ir">ports:<br/>    - protocol: TCP<br/>      port: 29413<br/>  clusterIP: None</strong></span></pre><p id="bc7a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">突出显示的文本是您应该注意的指令:</p><ul class=""><li id="7083" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">我们希望我们的部署存在于<strong class="lp ir"> spark </strong>名称空间中</li><li id="2f8a" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我们希望将这个部署的<strong class="lp ir"> ServiceAccount </strong>设置为<strong class="lp ir"> spark </strong>，它具有我们在上面创建的ClusterRoleBinding<strong class="lp ir">spark-role</strong></li><li id="fcee" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">根据<a class="ae ne" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#client-mode" rel="noopener ugc nofollow" target="_blank"> Spark客户端模式文档</a>，我们希望通过将服务的ClusterIP设置为None来将我们的部署公开为一个无头服务。这允许工人舱通过端口29413与司机舱通信。您选择的端口号可以是任何有效的未使用的端口，但是我在这个例子中选择29413作为对<a class="ae ne" href="http://blog.brainlounge.de/memoryleaks/getting-started-with-spark-on-kubernetes/" rel="noopener ugc nofollow" target="_blank">这个有用帖子</a>的敬意。</li></ul><h1 id="1dcf" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">创建部署</h1><p id="19f9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">当您对部署感到满意时，通过以下方式创建它:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="2242" class="nl kw iq nh b gy nm nn l no np">$ kubectl create -f k8s/deploy.yaml<br/>deployment.apps/my-notebook-deployment created<br/>service/my-notebook-deployment created</span></pre><p id="8c5b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">检查您的部署是否正在运行:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="9f6a" class="nl kw iq nh b gy nm nn l no np">$ kubectl get all -n spark<br/>NAME                                          READY   STATUS    RESTARTS   AGE<br/>pod/my-notebook-deployment-7bf574447c-pdn2q   1/1     Running   0          19s</span><span id="9188" class="nl kw iq nh b gy nq nn l no np">NAME                             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE<br/>service/my-notebook-deployment   ClusterIP   None         &lt;none&gt;        29413/TCP   19s</span><span id="ca3e" class="nl kw iq nh b gy nq nn l no np">NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE<br/>deployment.apps/my-notebook-deployment   1/1     1            1           19s</span><span id="fb48" class="nl kw iq nh b gy nq nn l no np">NAME                                                DESIRED   CURRENT   READY   AGE<br/>replicaset.apps/my-notebook-deployment-7bf574447c   1         1         1       19s</span></pre><p id="545c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">现在将端口转发到8888，这样您就可以登录Jupyter:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="cc0a" class="nl kw iq nh b gy nm nn l no np">$ kubectl port-forward -n spark deployment.apps/my-notebook-deployment 8888:8888<br/>Forwarding from 127.0.0.1:8888 -&gt; 8888<br/>Forwarding from [::1]:8888 -&gt; 8888</span></pre><p id="4e55" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">现在，您应该能够将您最喜欢的浏览器指向<a class="ae ne" href="http://localhost:8888" rel="noopener ugc nofollow" target="_blank"> http://localhost:8888 </a>并登录Jupyter(如果您使用我的图像作为参考，密码是<strong class="lp ir">‘Jupyter’</strong>，否则您将需要复制/粘贴在容器的日志文件中显示的生成的auth token)。</p><h1 id="e410" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">点燃它！</h1><p id="1f93" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">启动一个新的Python3笔记本，并在单元格中键入以下内容:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="6590" class="nl kw iq nh b gy nm nn l no np">import os</span><span id="6fd1" class="nl kw iq nh b gy nq nn l no np">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SparkSession</span><span id="39e3" class="nl kw iq nh b gy nq nn l no np"># Create Spark config for our Kubernetes based cluster manager<br/>sparkConf = SparkConf()<br/>sparkConf.setMaster("<strong class="nh ir">k8s://</strong><a class="ae ne" href="https://kubernetes.default.svc.cluster.local:443" rel="noopener ugc nofollow" target="_blank"><strong class="nh ir">https://kubernetes.default.svc.cluster.local:443</strong></a><strong class="nh ir">"</strong>)<br/>sparkConf.setAppName(<strong class="nh ir">"spark"</strong>)<br/>sparkConf.set(<strong class="nh ir">"spark.kubernetes.container.image", "pidocker-docker-registry.default.svc.cluster.local:5000/my-spark-py:v2.4.4"</strong>)<br/>sparkConf.set(<strong class="nh ir">"spark.kubernetes.namespace", "spark"</strong>)<br/>sparkConf.set(<strong class="nh ir">"spark.executor.instances", "7"</strong>)<br/>sparkConf.set("spark.executor.cores", "2")<br/>sparkConf.set("spark.driver.memory", "512m")<br/>sparkConf.set("spark.executor.memory", "512m")<br/>sparkConf.set("spark.kubernetes.pyspark.pythonVersion", "3")<br/>sparkConf.set(<strong class="nh ir">"spark.kubernetes.authenticate.driver.serviceAccountName", "spark"</strong>)<br/>sparkConf.set(<strong class="nh ir">"spark.kubernetes.authenticate.serviceAccountName", "spark"</strong>)<br/>sparkConf.set(<strong class="nh ir">"spark.driver.port", "29413</strong>")<br/>sparkConf.set(<strong class="nh ir">"spark.driver.host", "my-notebook-deployment.spark.svc.cluster.local</strong>")</span><span id="e4df" class="nl kw iq nh b gy nq nn l no np"># Initialize our Spark cluster, this will actually<br/># generate the worker nodes.<br/>spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()<br/>sc = spark.sparkContext</span></pre><p id="997f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们来分解一下上面的Spark配置:</p><ul class=""><li id="fa29" class="mq mr iq lp b lq mj lt mk lw ms ma mt me mu mi mv mw mx my bi translated">我们将主URL设置为"<strong class="lp ir">k8s://</strong><a class="ae ne" href="https://kubernetes.default.svc.cluster.local:443" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir">https://Kubernetes . default . SVC . cluster . local:443</strong></a><strong class="lp ir">"</strong>，这告诉Spark我们的集群管理器是Kubernetes (k8s)并且在哪里可以找到请求资源的kube-apiserver</li><li id="0b7d" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我们将应用程序名称设置为<strong class="lp ir">“spark”</strong>，它将被添加到worker Pod名称的前面。你可以随意使用任何你认为合适的名字</li><li id="9567" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我们将worker Pods使用的容器映像设置为我们在上面构建的自定义映像，其中包含更新的Kubernetes客户端jar文件</li><li id="a2dc" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我们指定希望所有工作节点都创建在<strong class="lp ir"> spark </strong>名称空间中</li><li id="6055" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我将工作节点的数量设置为<strong class="lp ir">“7”</strong>，因为我在八个节点上(一个节点是主节点，七个可用于工作)。您需要选择一个对您的集群有意义的数字。</li><li id="d9d1" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">我们将驱动程序和工人的服务帐户名称设置为我们在上面创建的服务帐户(<strong class="lp ir">“spark”</strong>)</li><li id="192c" class="mq mr iq lp b lq mz lt na lw nb ma nc me nd mi mv mw mx my bi translated">最后，我们将端口号设置为<strong class="lp ir"> 29413 </strong>，并将集群内服务的完全限定域名指定给工作节点，以便联系驱动节点。</li></ul><p id="e6cd" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在执行这个单元之后，您应该会看到spark名称空间中产生了许多worker Pods。</p><p id="3f65" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们检查一下:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="8d2e" class="nl kw iq nh b gy nm nn l no np">$ kubectl get pods -n spark<br/>NAME                                      READY   STATUS    RESTARTS   AGE<br/>my-notebook-deployment-7bf574447c-pdn2q   1/1     Running   0          5m6s<br/>spark-1581043649282-exec-1                1/1     Running   0          76s<br/>spark-1581043651262-exec-2                1/1     Running   0          75s<br/>spark-1581043651392-exec-3                1/1     Running   0          75s<br/>spark-1581043651510-exec-4                1/1     Running   0          75s<br/>spark-1581043651623-exec-5                1/1     Running   0          75s<br/>spark-1581043656753-exec-6                1/1     Running   0          70s<br/>spark-1581043656875-exec-7                1/1     Running   0          70s</span></pre><p id="1784" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">您现在有一个运行在Kubernetes上的Spark集群等待工作！</p><p id="9d00" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">按照传统，让我们在新的单元格中使用我们的集群来计算数字Pi:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="d530" class="nl kw iq nh b gy nm nn l no np">from random import random<br/>from operator import add</span><span id="9116" class="nl kw iq nh b gy nq nn l no np">partitions = 7<br/>n = 10000000 * partitions</span><span id="b2c9" class="nl kw iq nh b gy nq nn l no np">def f(_):<br/>    x = random() * 2 - 1<br/>    y = random() * 2 - 1<br/>    <br/>    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0</span><span id="3661" class="nl kw iq nh b gy nq nn l no np">count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)<br/>print("Pi is roughly %f" % (4.0 * count / n))</span></pre><p id="67cf" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">您的输出应该是:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="5351" class="nl kw iq nh b gy nm nn l no np">Pi is roughly 3.141397</span></pre><h1 id="4193" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">熄灭火花</h1><p id="1f97" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">完成开发后，只需执行以下命令就可以关闭Spark集群:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="b5b4" class="nl kw iq nh b gy nm nn l no np">sc.stop()</span></pre><p id="77e9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这将指示您的SparkContext通过kube-apiserver拆除上面创建的worker Pods。</p><p id="1368" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">要验证:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="35d2" class="nl kw iq nh b gy nm nn l no np">$ kubectl get pods -n spark<br/>NAME                                      READY   STATUS        RESTARTS   AGE<br/>my-notebook-deployment-7bf574447c-pdn2q   1/1     Running       0          20m<br/>spark-1581043649282-exec-1                0/1     Terminating   0          16m<br/>spark-1581043651262-exec-2                1/1     Terminating   0          16m<br/>spark-1581043651392-exec-3                1/1     Terminating   0          16m<br/>spark-1581043651510-exec-4                0/1     Terminating   0          16m<br/>spark-1581043651623-exec-5                0/1     Terminating   0          16m<br/>spark-1581043656753-exec-6                0/1     Terminating   0          16m<br/>spark-1581043656875-exec-7                0/1     Terminating   0          16m</span></pre><p id="0ddd" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">现在您只剩下spark名称空间中的部署/驱动程序Pod。</p><h1 id="81e3" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="cbf7" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我希望您现在更有信心使用PySpark在Kubernetes上创建Spark集群，并欣赏其背后的架构。</p><p id="dff2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我知道在我早期的Spark开发工作中，大部分时间我都是在独立模式下运行的，但是现在有了Kubernetes的支持，它要么集群，要么破产！</p><p id="add6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">欢迎在下面发表任何问题、更正或担忧，祝您愉快！</p></div></div>    
</body>
</html>