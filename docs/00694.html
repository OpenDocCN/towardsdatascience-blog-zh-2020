<html>
<head>
<title>Mostly Painless Introduction to Applied Bayesian Inference using (Py)Stan</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用(Py)Stan 的应用贝叶斯推理的简单介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/painless-introduction-to-applied-bayesian-inference-using-py-stan-36b503a4cd80?source=collection_archive---------14-----------------------#2020-01-20">https://towardsdatascience.com/painless-introduction-to-applied-bayesian-inference-using-py-stan-36b503a4cd80?source=collection_archive---------14-----------------------#2020-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c3f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">贝叶斯回归在 PyStan 中的应用</h2></div><blockquote class="ki kj kk"><p id="ec08" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们鼓励你在阅读这篇文章之前先看看这个<a class="ae li" rel="noopener" target="_blank" href="/conceptual-background-for-painless-introduction-to-applied-bayesian-regression-using-pystan-c8f744e3823f">概念背景</a>。</p></blockquote><h1 id="9dff" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">设置</h1><p id="e705" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated"><strong class="ko iu">Stan</strong>【1】是一个用于贝叶斯推理和模型拟合的计算引擎。它依赖于哈密尔顿蒙特卡罗(HMC) [2]的变体，从大量分布和模型的后验分布中进行采样。</p><p id="b8a6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">下面是设置 Stan 的详细安装步骤:<a class="ae li" href="https://pystan.readthedocs.io/en/latest/installation_beginner.html" rel="noopener ugc nofollow" target="_blank">https://pystan . readthedocs . io/en/latest/installation _ beginner . html</a></p><p id="76c8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">对于 MacOS:</p><ul class=""><li id="1c49" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">安装<code class="fe ms mt mu mv b">miniconda</code> / <code class="fe ms mt mu mv b">anaconda</code></li><li id="8afb" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">安装<code class="fe ms mt mu mv b">xcode</code></li><li id="a284" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">更新你的 C 编译器:<code class="fe ms mt mu mv b">conda install clang_osx-64 clangxx_osx-64 -c anaconda</code></li><li id="8cc9" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">创造环境<code class="fe ms mt mu mv b">stan</code>或<code class="fe ms mt mu mv b">pystan</code></li><li id="5926" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">键入<code class="fe ms mt mu mv b">conda install numpy</code>安装 numpy 或用您需要安装的包替换 numpy</li><li id="a0aa" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">安装 pystan: <code class="fe ms mt mu mv b">conda install -c conda-forge pystan</code></li><li id="9f73" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">或者:<code class="fe ms mt mu mv b">pip install pystan</code></li><li id="da19" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">还要安装:<code class="fe ms mt mu mv b">arviz</code>、<code class="fe ms mt mu mv b">matplotlib</code>、<code class="fe ms mt mu mv b">pandas</code>、<code class="fe ms mt mu mv b">scipy</code>、<code class="fe ms mt mu mv b">seaborn</code>、<code class="fe ms mt mu mv b">statsmodels</code>、<code class="fe ms mt mu mv b">pickle</code>、<code class="fe ms mt mu mv b">scikit-learn</code>、<code class="fe ms mt mu mv b">nb_conda</code>和<code class="fe ms mt mu mv b">nb_conda_kernels</code></li></ul><p id="81f3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">设置完成后，我们可以打开(Jupyter)笔记本，开始工作。首先，让我们用以下代码导入我们的库:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="e1ad" class="nj lk it mv b gy nk nl l nm nn">import pystan<br/>import pickle<br/>import numpy as np<br/>import arviz as az<br/>import pandas as pd<br/>import seaborn as sns<br/>import statsmodels.api as statmod<br/>import matplotlib.pyplot as plt<br/>from IPython.display import Image<br/>from IPython.core.display import HTML</span></pre><h1 id="ea79" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">掷硬币推理</h1><p id="4c96" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">回想一下在概念背景中，我是如何谈到在地上找到一枚硬币，并把它扔 K 次以获得公平感的。</p><p id="a65e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">我们满足于以下模型:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f7360d610790741df9bd1fa08202d381.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*_Oj9khuFHdOgzXjxGpdv9w.png"/></div></figure><p id="a282" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">Y 的概率质量函数(PMF)如下:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a6e006a6cb879f03dd47ae2f776dc69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*gwTSrA6WuRo8R1B6iWwtJQ.png"/></div></figure><p id="cca4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">首先，通过使用 NumPy 的<code class="fe ms mt mu mv b">random</code>功能进行仿真，我们可以了解参数 a=b=5 的先验的行为:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="5e14" class="nj lk it mv b gy nk nl l nm nn">sns.distplot(np.random.beta(5,5, size=10000),kde=False)</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/540f8a38ac976b1ae2d4bf8c16dbf58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*zGrmwVTRwxpAMzarwmZX_w.png"/></div></figure><p id="5c3c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">正如在概念背景中提到的，这个先验似乎是合理的:它在 0.5 左右是对称的，但在两个方向都有可能出现偏差。然后我们可以用下面的语法在<code class="fe ms mt mu mv b">pystan</code>中定义我们的模型:</p><ul class=""><li id="c5f6" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">data</code>对应我们模型的数据。在这种情况下，整数<code class="fe ms mt mu mv b">N </code>对应于投掷硬币的次数，而<code class="fe ms mt mu mv b">y</code>对应于长度为<code class="fe ms mt mu mv b">N</code>的整数向量，它将包含我们实验的观察结果。</li><li id="b2f0" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">parameters</code>对应于我们模型的参数，在本例中为<code class="fe ms mt mu mv b">theta</code>，或者获得“人头”的概率。</li><li id="68d4" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">model</code>对应于我们的先验(<code class="fe ms mt mu mv b">beta</code>)和似然(<code class="fe ms mt mu mv b">bernoulli</code>)的定义。</li></ul><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="f9a1" class="nj lk it mv b gy nk nl l nm nn"># bernoulli model<br/>model_code = """<br/>    data {<br/>      int&lt;lower=0&gt; N;<br/>      int&lt;lower=0,upper=1&gt; y[N];<br/>    }<br/>    parameters {<br/>      real&lt;lower=0,upper=1&gt; theta;<br/>    }<br/>    model {<br/>      theta ~ beta(5, 5);<br/>      for (n in 1:N)<br/>          y[n] ~ bernoulli(theta);<br/>    }<br/>    """</span><span id="1c31" class="nj lk it mv b gy nu nl l nm nn">data = dict(N=4, y=[0, 0, 0, 0])<br/>model = pystan.StanModel(model_code=model_code)<br/>fit = model.sampling(data=data,iter=4000, chains=4, warmup=1000)</span><span id="f5d4" class="nj lk it mv b gy nu nl l nm nn">la = fit.extract(permuted=True)  # return a dictionary of arrays</span><span id="f3fc" class="nj lk it mv b gy nu nl l nm nn">print(fit.stansummary())</span></pre><p id="e43b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">注意<code class="fe ms mt mu mv b">model.sampling</code>的默认参数是<code class="fe ms mt mu mv b">iter=1000</code>、<code class="fe ms mt mu mv b">chains=4</code>和<code class="fe ms mt mu mv b">warmup=500</code>。我们根据我们的时间和可用的计算资源来调整这些。</p><ul class=""><li id="3e11" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">iter</code> ≥ 1 对应于我们每个 MCMC 链的运行次数(对于大多数应用来说，不应少于 1000 次)</li><li id="5085" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">warmup</code>或“老化”≥ 0 对应于我们采样开始时的初始运行次数。考虑到这些链在开始运行时非常不稳定和幼稚，实际上我们通常定义这个量来丢弃第一个 B= <code class="fe ms mt mu mv b">warmup</code>数量的样本。如果我们不丢弃 B，那么我们就在估计中引入了不必要的噪声。</li><li id="4bfe" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">chains</code> ≥ 1 对应于我们采样中的 MCMC 链数。</li></ul><p id="9d27" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">下面是上述模型的输出:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="1aa7" class="nj lk it mv b gy nk nl l nm nn">Inference for Stan model: anon_model_d3835c4370ff5e66f1e88bd3eac647ff.<br/>4 chains, each with iter=4000; warmup=1000; thin=1; <br/>post-warmup draws per chain=3000, total post-warmup draws=12000.</span><span id="a48c" class="nj lk it mv b gy nu nl l nm nn">        mean se_mean     sd   2.5%    25%    50%    75%  97.5% <br/>theta   0.36  1.8e-3   0.12   0.14   0.27   0.35   0.44   0.61   <br/>lp__   -9.63  9.3e-3   0.71 -11.63  -9.81  -9.36  -9.18  -9.13   </span></pre><ul class=""><li id="7aa4" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">我们的<code class="fe ms mt mu mv b">theta</code>的后验均值大约是<code class="fe ms mt mu mv b">0.36</code> &lt; 0.5。尽管如此，95%的后验可信区间还是相当宽的:(0.14，0.61)。因此，我们可以说这个结果在统计学上不是结论性的，但是它指向偏见，而没有<em class="kn">偶然</em>跳到 0。</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="f2fa" class="lj lk it bd ll lm oc lo lp lq od ls lt jz oe ka lv kc of kd lx kf og kg lz ma bi translated">应用回归:汽车和每加仑英里数(MPG)</h1><figure class="nb nc nd ne gt np gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/f823997723d915d068696e6fe0f339fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XJhcz4FnTk6uFT9ZTFg1g.jpeg"/></div></div><p class="om on gj gh gi oo op bd b be z dk translated">图片来自<a class="ae li" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=980992" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae li" href="https://pixabay.com/users/Picsues-1416498/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=980992" rel="noopener ugc nofollow" target="_blank">苏珊·苏厄特</a></p></figure><p id="72fb" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">让我们构建一个贝叶斯线性回归模型来解释和预测不同规格和品牌的汽车数据集中的每加仑英里数(MPG)。尽管我的方法是一种“经典的”基于可能性的方法，或者更确切地说，是一种以分布为中心的方法，但是我们可以(并且应该！)使用原始的 ML 训练测试分割来评估我们预测的质量。</p><p id="0275" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">该数据集来自<a class="ae li" href="https://archive.ics.uci.edu/ml/datasets/auto+mpg" rel="noopener ugc nofollow" target="_blank"> UCI ML 知识库</a>，包含以下信息:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="7f2b" class="nj lk it mv b gy nk nl l nm nn"> 1. mpg: continuous <br/> 2. cylinders: integers<br/> 3. displacement: continuous <br/> 4. horsepower: continuous <br/> 5. weight: continuous <br/> 6. acceleration: continuous <br/> 7. model year: integers<br/> 8. origin: categorical<br/> 9. car name: string (index for each instance)</span></pre><p id="4547" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">为什么我们不坚持我们的基本原则，使用标准线性回归？[3]回忆其功能形式如下:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/71c96f042771f5a13123dcf67453c7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*v7vTBDSeYEQZUh-PJLEPJQ.png"/></div></figure><ul class=""><li id="e00c" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">y 对应于我们的因变量，或感兴趣的结果—这里是<code class="fe ms mt mu mv b">mpg</code>。</li><li id="696b" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">x 是具有 P 个特征或独立变量的 N 个样本的 N×P 矩阵。</li><li id="2d41" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">α是一个 N x 1 向量，表示模型截距，根据您的编码方案，可能有不同的解释。</li><li id="ad24" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">β是回归量/特征系数的 P×1 向量。</li><li id="5d85" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">ε是一个 N×1 随机向量，它遵循一个多变量正态分布，N×N<strong class="ko iu">协方差</strong>矩阵表示为σ。标准的线性回归情况要求该σ是沿对角线σ &gt;为 0 的对角矩阵，即观测值之间的<strong class="ko iu">独立性</strong>。</li></ul><p id="f7ff" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">现在，对于贝叶斯公式:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3f67170677809e10a8ae418f824e682b.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*4tmsPRYKB8NNEDLj_an5Rw.png"/></div></figure><p id="850b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">虽然前两条线看起来完全一样，但我们现在需要建立α、β和σ的先验分布。</p><p id="1ac2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">在执行 EDA 时，我们应该始终考虑以下几点:</p><ul class=""><li id="a05b" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">我的可能性有多大？</li><li id="6f4b" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">我的模型应该是什么？互动 vs 无互动？多项式？</li><li id="3576" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">我的参数(和超参数)是什么，我应该选择什么样的先验？</li><li id="220b" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">我是否应该考虑任何聚类、时间或空间相关性？</li></ul><h1 id="2ec4" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">电子设计自动化(Electronic Design Automation)</h1><p id="216c" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">与任何类型的数据分析一样，<strong class="ko iu">至关重要的是</strong>首先了解我们感兴趣的变量/特征，以及它们之间以及与我们的 y /结果之间的关系。</p><p id="91cf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">让我们从加载数据集开始:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="20ec" class="nj lk it mv b gy nk nl l nm nn">cars_data = pd.read_csv("<a class="ae li" href="https://raw.githubusercontent.com/sergiosonline/sergiosonline.github.io/master/files/cars.csv" rel="noopener ugc nofollow" target="_blank">~/cars.csv</a>").set_index("name")<br/>print(cars_data.shape)<br/>cars_data.head()</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi os"><img src="../Images/6aa6865c663e7775debe891d95eca72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*dSqXZJPkW6HrVAzl76gnYA.png"/></div></figure><p id="46e3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">让我们检查一下目标变量和预测变量之间的关系(如果有的话)。这里<code class="fe ms mt mu mv b">origin</code>表示汽车的产地——它有三个等级:“美国”、“日本”和“欧洲”</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="f6c4" class="nj lk it mv b gy nk nl l nm nn">sns.distplot(cars_data[cars_data['origin']=='American']['mpg'],color="skyblue", label="American",kde=False)<br/>sns.distplot(cars_data[cars_data['origin']=='Japanese']['mpg'],color="red", label="Japanese",kde=False)<br/>sns.distplot(cars_data[cars_data['origin']=='European']['mpg'],color="yellow", label="European",kde=False)<br/>plt.legend()</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/59365f1b9fb2c2d36df3bc7916376bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*mm1Tl7kYmAhTHoIqOGxW_g.png"/></div><p class="om on gj gh gi oo op bd b be z dk translated">美国汽车和日本汽车似乎有一些有趣的区别。</p></figure><p id="cd31" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">现在让我们检查一下数字变量和<code class="fe ms mt mu mv b">mpg</code>之间的关系:</p><ul class=""><li id="376b" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">我更喜欢将这些可视化，而不是简单地计算相关性，因为这给了我它们之间关系的视觉和数学感觉，超越了简单的标量。</li></ul><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="f25a" class="nj lk it mv b gy nk nl l nm nn">f, axes = plt.subplots(2, 3, figsize=(7, 7), sharex=False)</span><span id="38f2" class="nj lk it mv b gy nu nl l nm nn">sns.relplot(x="cylinders", y="mpg", data=cars_data, ax=axes[0, 0]);<br/>sns.relplot(x="displacement", y="mpg", data=cars_data, ax=axes[0, 1]);<br/>sns.relplot(x="horsepower", y="mpg", data=cars_data, ax=axes[0, 2]);<br/>sns.relplot(x="acceleration", y="mpg", data=cars_data, ax=axes[1, 0]);<br/>sns.relplot(x="year", y="mpg", data=cars_data, ax=axes[1, 1]);<br/>sns.relplot(x="weight", y="mpg", data=cars_data, ax=axes[1, 2]);</span><span id="f330" class="nj lk it mv b gy nu nl l nm nn"># close pesky empty plots<br/>for num in range(2,8):<br/>    plt.close(num)<br/>    <br/>plt.show()</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/7a140ab1f42b3c9e7cac14ed6914e8c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*_yzHJ-5xcCTcIWTGNrdgFw.png"/></div><p class="om on gj gh gi oo op bd b be z dk translated">除了<strong class="bd ou">年份</strong>和<strong class="bd ou">加速度</strong>之外，其他年份都与<strong class="bd ou"> mpg </strong>负相关，即排量每增加 1 个单位，mpg 就会减少。</p></figure><h1 id="10aa" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">培训/安装</h1><p id="68ab" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">让我们为拟合和测试准备数据集:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="8140" class="nj lk it mv b gy nk nl l nm nn">from numpy import random<br/>from sklearn import preprocessing, metrics, linear_model<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error</span><span id="caa9" class="nj lk it mv b gy nu nl l nm nn">random.seed(12345)</span><span id="a32c" class="nj lk it mv b gy nu nl l nm nn">cars_data = cars_data.set_index('name')<br/>y = cars_data['mpg']<br/>X = cars_data.loc[:, cars_data.columns != 'mpg']<br/>X = X.loc[:, X.columns != 'name']<br/>X = pd.get_dummies(X, prefix_sep='_', drop_first=False) <br/>X = X.drop(columns=["origin_European"]) # This is our reference category</span><span id="bb53" class="nj lk it mv b gy nu nl l nm nn">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)</span><span id="0891" class="nj lk it mv b gy nu nl l nm nn">X_train.head()</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi ow"><img src="../Images/7dd37713397f4a6649cb0f8801a52ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8S3fcinZn1PpQkyz_6KsQ.png"/></div></div></figure><p id="fd0f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">现在，转到我们的模型规范，它基本上没有偏离上面的抛硬币问题，除了我使用矩阵符号来尽可能简化模型表达式的事实:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="fb7e" class="nj lk it mv b gy nk nl l nm nn"># Succinct matrix notation</span><span id="95a8" class="nj lk it mv b gy nu nl l nm nn">cars_code = """<br/>data {<br/>    int&lt;lower=1&gt; N; // number of training samples<br/>    int&lt;lower=0&gt; K; // number of predictors - 1 (intercept)<br/>    matrix[N, K] x; // matrix of predictors<br/>    vector[N] y_obs; // observed/training mpg<br/>    <br/>    int&lt;lower=1&gt; N_new;<br/>    matrix[N_new, K] x_new;<br/>}<br/>parameters {<br/>    real alpha;<br/>    vector[K] beta;<br/>    real&lt;lower=0&gt; sigma;<br/>    <br/>    vector[N_new] y_new;<br/>}<br/>transformed parameters {<br/>    vector[N] theta;<br/>    theta = alpha + x * beta;<br/>}<br/>model {<br/>    sigma ~ exponential(1);<br/>    alpha ~ normal(0, 6);<br/>    beta ~ multi_normal(rep_vector(0, K), diag_matrix(rep_vector(1, K)));<br/>    y_obs ~ normal(theta, sigma);<br/>    <br/>    y_new ~ normal(alpha + x_new * beta, sigma); // prediction model<br/>}<br/>"""</span></pre><ul class=""><li id="4124" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">data</code>对应于我们模型的数据，如上面掷硬币的例子。</li><li id="599b" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">parameters</code>对应我们模型的参数。</li><li id="15c0" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">transformed parameters</code>这里允许我将<code class="fe ms mt mu mv b">theta</code>定义为我们的模型在训练集上的拟合值</li><li id="a681" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">model</code>对应于我们对<code class="fe ms mt mu mv b">sigma</code>、<code class="fe ms mt mu mv b">alpha</code>、<code class="fe ms mt mu mv b">beta</code>的先验的定义，以及我们对 P(Y|X，α，β，σ) ( <code class="fe ms mt mu mv b">normal</code>)的似然。</li></ul><p id="ce91" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">上述先验是在初始数据检查后选择的:</p><ul class=""><li id="cba3" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">为什么σ是指数先验？嗯，σ ≥ 0(根据定义)。为什么不是制服或者伽玛？<strong class="ko iu"> PC 框架！</strong>【4】——我的目标是最节俭的模式。</li><li id="0fdd" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">α <em class="kn"> </em>和β呢？这里最简单的事情是，给定一个正态概率，为这些参数选择正态先验是很方便的。您可能希望为这两个参数中的每一个选择不同的超参数。</li><li id="0d1d" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">为什么β是一个<code class="fe ms mt mu mv b">multi_normal()</code>？数理统计中有一个众所周知的结果，即长度为&lt; ∞且具有单位对角协方差矩阵和向量均值μ &lt; ∞的多正态随机变量(向量 W)等价于一个服从 N(μ，1)分布的<strong class="ko iu">独立</strong>正态随机变量 W 的向量。</li></ul><p id="ab47" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">关于 stan 有趣的是，我们可以请求在测试集上进行预测而无需重新拟合——更确切地说，它们可以在第一次调用中被请求。</p><ul class=""><li id="7f57" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated">我们通过在上面的调用中定义<code class="fe ms mt mu mv b">y_new</code>来实现这一点。</li><li id="31ba" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated"><code class="fe ms mt mu mv b">theta</code>是我们对训练集的拟合预测。</li></ul><p id="2c2a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">我们指定要摄取的数据集，并继续从模型中取样，同时保存它以供将来参考:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="06dc" class="nj lk it mv b gy nk nl l nm nn">cars_dat = {'N': X_train.shape[0],<br/>            'N_new': X_test.shape[0],<br/>            'K': X_train.shape[1],<br/>            'y_obs': y_train.values.tolist(),<br/>            'x': np.array(X_train),<br/>            'x_new': np.array(X_test)}</span><span id="2d5a" class="nj lk it mv b gy nu nl l nm nn">sm = pystan.StanModel(model_code=cars_code)<br/>fit = sm.sampling(data=cars_dat, iter=6000, chains=8)</span><span id="0ebd" class="nj lk it mv b gy nu nl l nm nn"># Save fitted model!<br/>with open('bayes-cars.pkl', 'wb') as f:<br/>    pickle.dump(sm, f, protocol=pickle.HIGHEST_PROTOCOL)</span><span id="fb1e" class="nj lk it mv b gy nu nl l nm nn"># Extract and print the output of our model<br/>la = fit.extract(permuted=True)<br/>print(fit.stansummary())</span></pre><p id="9985" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">以下是我打印的输出(出于本教程的目的，我去掉了<code class="fe ms mt mu mv b">Rhat</code>和<code class="fe ms mt mu mv b">n_eff</code>):</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="6233" class="nj lk it mv b gy nk nl l nm nn">Inference for Stan model: anon_model_3112a6cce1c41eead6e39aa4b53ccc8b.<br/>8 chains, each with iter=6000; warmup=3000; thin=1; <br/>post-warmup draws per chain=3000, total post-warmup draws=24000.</span><span id="976f" class="nj lk it mv b gy nu nl l nm nn">mean se_mean     sd   2.5%    25%    50%    75%  97.5%  <br/>alpha       -8.35    0.02   3.81 -15.79 -10.93  -8.37  -5.74  -0.97  <br/>beta[1]     -0.48  1.9e-3   0.33  -1.12   -0.7  -0.48  -0.26   0.17  <br/>beta[2]      0.02  4.6e-5 7.9e-3 6.1e-3   0.02   0.02   0.03   0.04  <br/>beta[3]     -0.02  8.7e-5   0.01  -0.04  -0.03  -0.02-6.9e-3   0.01  <br/>beta[4]   -7.0e-3  4.0e-6 7.0e-4-8.3e-3-7.4e-3-7.0e-3-6.5e-3-5.6e-3  <br/>beta[5]       0.1  5.9e-4    0.1   -0.1   0.03    0.1   0.17    0.3  <br/>beta[6]      0.69  2.7e-4   0.05    0.6   0.65   0.69   0.72   0.78  <br/>beta[7]     -1.75  2.9e-3   0.51  -2.73  -2.09  -1.75  -1.41  -0.73  <br/>beta[8]      0.36  2.7e-3   0.51  -0.64   0.02   0.36   0.71   1.38  <br/>sigma        3.33  7.6e-4   0.13   3.09   3.24   3.32   3.41   3.59  <br/>y_new[1]    26.13    0.02   3.37  19.59  23.85  26.11  28.39  32.75  <br/>y_new[2]    25.38    0.02   3.36  18.83  23.12  25.37  27.65  31.95<br/>...<br/>...<br/>theta[1]    24.75  2.7e-3   0.47  23.83  24.44  24.75  25.07  25.68  <br/>theta[2]    19.59  2.6e-3   0.43  18.76   19.3  19.59  19.88  20.43 <br/>...<br/>...  </span></pre><p id="edb1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated"><code class="fe ms mt mu mv b">fit.stansummary()</code>是一个像表格一样排列的字符串，它给出了拟合过程中估计的每个<strong class="ko iu">参数</strong>的<strong class="ko iu">后验</strong>均值、标准差和几个百分点。当<code class="fe ms mt mu mv b">alpha</code>对应于截距时，我们有 8 个<code class="fe ms mt mu mv b">beta</code>回归量，一个复杂性或观察误差<code class="fe ms mt mu mv b">sigma</code>，我们在训练集<code class="fe ms mt mu mv b">theta</code>上的拟合值，以及我们在测试集<code class="fe ms mt mu mv b">y_new</code>上的预测值。</p><h1 id="d867" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">诊断</h1><p id="39d2" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">在引擎盖下运行 MCMC，我们以图表的形式检查基本诊断是至关重要的。对于这些图，我依靠优秀的<code class="fe ms mt mu mv b">arviz</code>库进行贝叶斯可视化(和<code class="fe ms mt mu mv b">PyMC3</code>一起工作也一样)。</p><ul class=""><li id="45c4" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated"><strong class="ko iu">链式混合</strong> —轨迹图。这些系列图应显示出<strong class="ko iu">“厚毛</strong>”链在真实直线的“合理尺寸”区间内振荡，以指示良好的混合。“稀疏”链意味着 MCMC 没有有效地探索，并且可能在某些异常处停滞不前。</li></ul><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="906a" class="nj lk it mv b gy nk nl l nm nn">ax = az.plot_trace(fit, var_names=["alpha","beta","sigma"])</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi ox"><img src="../Images/70edd3a3b0bdfd35cc702c8e023fbd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKmu_23d4JLJvVxM4-tzrg.png"/></div></div><p class="om on gj gh gi oo op bd b be z dk translated">粗毛铁链！由于图像大小，我省略了 alpha 和 beta[1:2]。</p></figure><ul class=""><li id="817b" class="mj mk it ko b kp kq ks kt md ml mf mm mh mn lh mo mp mq mr bi translated"><strong class="ko iu">我们参数的后验可信区间</strong> —森林样地。这些应该作为我们的模型参数(和超参数)的比例和位置的可视化指南！).例如，σ(这里是 PC 框架[4]下的一个<strong class="ko iu">复杂度</strong>参数)不应该达到&lt; 0。与此同时，如果ϐ预测值的可信区间不包含 0，或者如果 0 几乎不在其中，我们就可以获得“统计显著性”的意义</li></ul><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="c75b" class="nj lk it mv b gy nk nl l nm nn">axes = az.plot_forest(<br/>    post_data,<br/>    kind="forestplot",<br/>    var_names= ["beta","sigma"],<br/>    combined=True,<br/>    ridgeplot_overlap=1.5,<br/>    colors="blue",<br/>    figsize=(9, 4),<br/>)</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/27f8c0185364dbee4cd2a6ca5afcd798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*tNmOJG2qeUnnEBlTiw8zJg.png"/></div><p class="om on gj gh gi oo op bd b be z dk translated">看起来不错。<code class="fe ms mt mu mv b"><strong class="bd ou">alpha</strong></code>这里可以看作是代表我们引用类别的“collector”(我用的是引用编码)。我们当然可以规范化我们的变量，以实现更友好的缩放——我鼓励你尝试一下。</p></figure><h1 id="4d86" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">预言；预测；预告</h1><p id="02fc" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">贝叶斯推理具有预测能力，我们可以通过从预测后验分布中取样来产生预测能力:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4b47cf3f896eba5e08f2a4b2d9d94ebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*2kywD3K8gB4OnXozm0WpGg.png"/></div></figure><p id="ed74" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">这些(以及整个贝叶斯框架)的伟大之处在于，我们可以使用<strong class="ko iu">(预测)可信区间</strong>来给我们估计的方差/波动性的感觉。毕竟，如果一个预测模型的预测 50 次中有 1 次“足够接近”目标，那么它有什么用呢？</p><p id="1441" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">让我们看看我们的预测值与测试集中的观察值相比如何:</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="0343" class="nj lk it mv b gy nk nl l nm nn">dff = pd.DataFrame({'y_pred':la['y_new'].mean(0), 'y_obs':y_test})<br/>grid = sns.JointGrid(dff.y_pred, dff.y_obs, space=0, height=6, ratio=50,<br/>                    xlim=(0,50), ylim=(0,50))<br/>grid.plot_joint(plt.scatter, color="b")<br/>x0, x1 = grid.ax_joint.get_xlim()<br/>y0, y1 = grid.ax_joint.get_ylim()<br/>lims = [max(x0, y0), min(x1, y1)]<br/>grid.ax_joint.plot(lims, lims, ':k')</span><span id="33de" class="nj lk it mv b gy nu nl l nm nn">plt.subplots_adjust(top=0.9)<br/>grid.fig.suptitle('Bayes Test Predicted vs Obs',fontsize=20)<br/>plt.show()</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ddc7ecea055a855f6278eb9b3f794802.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*vSCzcJwsP6-lGLWo21IHrA.png"/></div><p class="om on gj gh gi oo op bd b be z dk translated">直虚线表示完美的预测。我们离它不远了。</p></figure><p id="60cd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">然后，我们可以对这些预测进行 ML 标准度量(MSE ),以评估它们相对于保留测试集中的实际值的质量。</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="50d9" class="nj lk it mv b gy nk nl l nm nn">bay_test_mse = metrics.mean_squared_error(y_test, la['y_new'].mean(0))<br/>print('Bayes Test MSE:', bay_test_mse)</span><span id="e72f" class="nj lk it mv b gy nu nl l nm nn">##### Bayes Test MSE: 10.968931376358526</span></pre><p id="9e6d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">当我们改变一些模型输入时，我们还可以可视化我们预测的值(以及它们相应的 95%可信区间):</p><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="5030" class="nj lk it mv b gy nk nl l nm nn">az.style.use("arviz-darkgrid")</span><span id="443b" class="nj lk it mv b gy nu nl l nm nn">sns.relplot(x="weight", y="mpg")<br/>            data=pd.DataFrame({'weight':X_test['weight'],'mpg':y_test}))<br/>az.plot_hpd(X_test['weight'], la['y_new'], color="k", plot_kwargs={"ls": "--"})</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/e336312a539c0d542064cdcabcedf97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*DisfCHr37FGntyN-Qnrn9Q.png"/></div></figure><pre class="nb nc nd ne gt nf mv ng nh aw ni bi"><span id="a622" class="nj lk it mv b gy nk nl l nm nn">sns.relplot(x="acceleration", y="mpg",<br/>            data=pd.DataFrame({'acceleration':X_test['acceleration'],'mpg':y_test}))<br/>az.plot_hpd(X_test['acceleration'], la['y_new'], color="k", plot_kwargs={"ls": "--"})</span></pre><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/85e983d2ceae6044c3d66fd880e7c2b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*0Anandsod-ErCc_htUN4pQ.png"/></div></figure><p id="7d60" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">在下文中，我使用<code class="fe ms mt mu mv b">statsmodels</code>比较了我的贝叶斯模型和普通最大似然线性回归模型的性能:</p><figure class="nb nc nd ne gt np gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/c61d4bacd43cf937fbcc881e52481471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*z3_NcdzVntTGmWS3kFqUQw.png"/></div><p class="om on gj gh gi oo op bd b be z dk translated">他们表现得非常接近(为了这颗特殊的种子)。</p></figure><p id="7fd2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated"><strong class="ko iu">最佳实践:</strong>为了获得模型性能的“更好”想法，您应该通过运行 K ≥ 30 训练测试分割的实现，在测试 MSE 上引导 95% <strong class="ko iu">置信度</strong>区间(CLT)。</p><p id="dd4e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">这是整个笔记本。py 导出格式:</p><p id="0c39" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated"><a class="ae li" href="https://gist.github.com/sergiosonline/6a3e0b1345c8f002d0e7b11aaf252d44" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/sergiosonline/6 a3 E0 b 1345 c8 f 002d 0e 7 b 11 AAF 252d 44</a></p><h1 id="7b29" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">考虑</h1><ul class=""><li id="34a3" class="mj mk it ko b kp mb ks mc md pe mf pf mh pg lh mo mp mq mr bi translated">请注意，回归分析是一个现代数据科学问题大家族的一个非常容易的起点。在本教程之后，我希望你开始思考它的两种风格:ML(基于损失)和 classical(基于模型/可能性)。</li><li id="3a55" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">贝叶斯推理并不难结合到您的 DS 实践中。采用基于损失的方法作为贝叶斯方法是可能的，但这是我将在后面讨论的主题。</li><li id="d243" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">只要你知道自己在做什么就有用！事先说明<strong class="ko iu">难</strong>。<strong class="ko iu"> </strong>但是当你没有有限数量的数据(昂贵的数据收集/标记、罕见事件等)或者你确切知道你想要测试或避免什么时，它特别有用。</li><li id="fe5b" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">贝叶斯框架很少在 ML 任务(预测、分类等)中胜过它的(频繁主义者)ML 对应物，特别是当数据集的规模增长时。当处理大型数据集(大数据)时，您的先验知识很容易被数据淹没。</li><li id="f40f" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">正确指定的贝叶斯模型是一个生成模型，因此您应该能够轻松地生成数据，以检查您的模型与正在讨论的数据集/数据生成过程的一致性。</li><li id="b120" class="mj mk it ko b kp mw ks mx md my mf mz mh na lh mo mp mq mr bi translated">EDA 和 plots 在模型构建和检查过程之前、之中和之后都是至关重要的。[5]是一篇关于贝叶斯工作流中可视化重要性的精彩论文。</li></ul><h1 id="5db7" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated">进一步的方向</h1><p id="e32c" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">我鼓励你批判性地思考改进这种预测的方法，不仅从贝叶斯的角度，而且从 ML 的角度。</p><p id="c3d2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">数据集是否足够大或丰富，可以从严格的 ML 方法中受益？有哪些方法可以改进这个贝叶斯模型？在什么情况下，这个模型一定会优于 MLE 模型？如果观察值以某种方式相关(聚集的、自相关的、空间相关的等等)会怎样？当可解释性是建模优先考虑的问题时，该怎么做？</p><p id="06cf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">如果你想知道如何将贝叶斯方法扩展到深度学习，你也可以研究一下<strong class="ko iu">变分推理</strong> [6]方法，作为纯贝叶斯治疗的可扩展替代方案。<a class="ae li" href="https://www.cs.jhu.edu/~jason/tutorials/variational.html" rel="noopener ugc nofollow" target="_blank">这里的</a>是“简单”的概述，<a class="ae li" href="https://arxiv.org/abs/1601.00670" rel="noopener ugc nofollow" target="_blank">这里的</a>是技术回顾。</p><h1 id="4bc1" class="lj lk it bd ll lm ln lo lp lq lr ls lt jz lu ka lv kc lw kd lx kf ly kg lz ma bi translated"><strong class="ak">参考文献</strong></h1><p id="6bea" class="pw-post-body-paragraph kl km it ko b kp mb ju kr ks mc jx ku md me kx ky mf mg lb lc mh mi lf lg lh im bi translated">[1] B. Carpenter 等人<strong class="ko iu"> Stan:一种概率编程语言</strong> (2017)。统计软件杂志 76 卷 1 期。DOI 10.18637/jss.v076.i01。</p><p id="08c4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">[2]贝当古先生。<strong class="ko iu">哈密顿蒙特卡罗概念介绍</strong> (2017)。arXiv:1701.02434</p><p id="9253" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">[3] J .韦克菲尔德。<strong class="ko iu">贝叶斯和频率主义回归方法</strong> (2013)。统计学中的斯普林格级数。斯普林格纽约。doi:10.1007/978–1–4419–0925–1。</p><p id="d12c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">[4] D. Simpson 等人<strong class="ko iu">惩罚模型组件复杂性:构建先验的原则性实用方法</strong> (2017)。摘自:统计科学 32.1，第 1-28 页。doi: 10.1214/16-STS576。</p><p id="ee3f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">[5] J. Gabry 等<strong class="ko iu">贝叶斯工作流中的可视化</strong> (2019)。J. R. Stat。社会主义者答:182:389–402。doi:10.1111/rssa.12378</p><p id="b254" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku md kw kx ky mf la lb lc mh le lf lg lh im bi translated">[6] D.M. Blei et al. <strong class="ko iu">变分推断:统计学家综述</strong> (2016)。美国统计协会杂志，第 112 卷，Iss。518，2017 年 DOI:10.1080/01621459 . 5565656567</p></div></div>    
</body>
</html>