<html>
<head>
<title>How to export millions of records from Mysql to AWS S3?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将数百万条记录从 Mysql 导出到 AWS S3？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-export-millions-of-records-from-mysql-to-aws-s3-fe30e80832e4?source=collection_archive---------14-----------------------#2020-02-20">https://towardsdatascience.com/how-to-export-millions-of-records-from-mysql-to-aws-s3-fe30e80832e4?source=collection_archive---------14-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a76" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建自我恢复和可扩展的系统</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5b60be114da7a0978d6d9208c4ce125b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*TiPsFP0AbcP2G0P6h06p8A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Apache Spark 生态系统信用:<a class="ae ku" href="https://databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank">数据块</a></p></figure><p id="d2c8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<a class="ae ku" href="http://www.twilio.com" rel="noopener ugc nofollow" target="_blank"> Twilio </a>，我们每天处理世界各地发生的数百万个电话。一旦通话结束，就会被记录到 MySQL 数据库中。客户能够通过 API 查询<a class="ae ku" href="https://www.twilio.com/docs/voice/api/call-resource" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">调用</strong> </a> <strong class="kx iu"> </strong>的详细信息。(是的，Twilio 是 API 驱动的公司)</p><p id="786a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我最近做的一个任务是建立一个系统，允许客户导出他们的历史通话数据。这将允许他们导出直到最近的所有历史通话记录</p><p id="8d44" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">乍一看，这似乎是微不足道的，但是如果我们更深入地思考，一个系统将如何为从一开始就与我们合作的一些最大的客户进行扩展，这个问题可以归类为<em class="lr">构建一个可扩展的系统</em>。我们典型的客户规模从每天打几百个电话到几百万个不等。</p><p id="2ff0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">当一个每天打 100 万个电话的大客户请求过去 5 年的数据时，这个问题突然变成了一个大数据问题。呼叫总数可以在 1，000，000 * 5 * 365 的范围内。</p><p id="efc4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">我不得不考虑如何优化从 Mysql 读取数据，并高效地向 S3 发送数据，以便下载文件。</em></p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="f093" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">可能的解决方案</h1><p id="1657" class="pw-post-body-paragraph kv kw it kx b ky mr ju la lb ms jx ld le mt lg lh li mu lk ll lm mv lo lp lq im bi translated">1.<em class="lr">编写一个 cron 作业</em>，查询 Mysql 数据库中的特定帐户，然后将数据写入 S3。这对于获取较小的记录集可能很有效，但是为了使作业能够很好地存储大量记录，我需要构建一种机制来在失败时重试，并行读取和写入以实现高效下载，添加监控来衡量作业的成功。我将不得不编写连接器或使用库来连接 MySql 和 S3。</p><p id="a0a2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.<em class="lr">使用</em> <a class="ae ku" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">火花流</em> </a></p><p id="3ca4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我决定使用<strong class="kx iu"> Apache Spark </strong>来处理这个问题，因为在我的团队中(<a class="ae ku" href="http://www.twilio.com/voice/insights" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu"> Voice Insights </strong> </a>)我们已经大量使用它来进行实时数据处理和构建分析。<em class="lr"> Apache Spark </em>是一个用于构建可扩展实时处理应用的流行框架，在行业中广泛用于解决大数据和机器学习问题。Spark 的一个关键特性是它能够从 Kafka、Kinesis、S3、Mysql、文件等各种来源产生/消费数据。</p><p id="697e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr"> Apache Spark 也是容错的，并提供了一个处理失败和优雅地重试的框架</em>。它<em class="lr">使用检查点机制来存储所执行任务的中间偏移量，以便在任务失败的情况下，任务可以从最后保存的位置重新开始。为水平扩展配置作业效果很好。</em></p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><p id="542c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">假设读者对 Spark 有基本的了解(Spark 官方<a class="ae ku" href="https://spark.apache.org/docs/2.4.0/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">文档</strong> </a>是一个很好的起点)。我将深入研究代码。</p><p id="f9f0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们看看如何从 MySQL 数据库中读取数据。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="8fa6" class="nb ma it mx b gy nc nd l ne nf"><strong class="mx iu">val</strong> jdbcDF <strong class="mx iu">=</strong> spark.read<br/>  .format("jdbc")<br/>  .option("url", "jdbc:mysql://localhost:port/db")<br/>  .option("driver", "com.mysql.jdbc.Driver")<br/>  .option("dbtable", "schema.tablename")<br/>  .option("user", "username")<br/>  .option("password", "password")<br/>  .load()</span></pre><p id="97a5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">与 JDBC 连接的另一种方法是将配置作为地图。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="f15d" class="nb ma it mx b gy nc nd l ne nf"><strong class="mx iu">val </strong>dbConfig = Map("username" -&gt; "admin", <br/>                   "password" -&gt; "pwd", <br/>                   "url" -&gt; "http://localhost:3306")<br/><strong class="mx iu">val</strong> query = "select * FROM CallLog where CustomerId=1"</span><span id="bfa4" class="nb ma it mx b gy ng nd l ne nf"><strong class="mx iu">val</strong> jdbcDF <strong class="mx iu">=</strong> spark.read<br/>                  .format("jdbc")      <br/>                  .options(dbConfig)      <br/>                  .option("<strong class="mx iu">dbtable</strong>", s"${query} AS tmp")      <br/>                  .load()<br/><strong class="mx iu">OR</strong></span><span id="a751" class="nb ma it mx b gy ng nd l ne nf"><strong class="mx iu">val</strong> jdbcDF <strong class="mx iu">=</strong> spark.read<br/>                  .format("jdbc")      <br/>                  .options(dbConfig)      <br/>                  .option("<strong class="mx iu">query</strong>", s"${query} AS tmp")      <br/>                  .load()</span></pre><p id="156a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">使用<strong class="kx iu">查询</strong>和<strong class="kx iu"> dbtable 有细微的区别。</strong>这两个选项都会在 FROM 子句中创建一个子查询。上述查询将被转换为</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="fc02" class="nb ma it mx b gy nc nd l ne nf">SELECT * FROM (SELECT * FROM <!-- -->CallLog where CustomerId=1<!-- -->) tmp WHERE 1=0</span></pre><p id="9ba5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里最重要的一点是<code class="fe nh ni nj mx b">query</code>不支持<code class="fe nh ni nj mx b">partitionColmn</code>，而<code class="fe nh ni nj mx b">dbTable</code>支持分区，这通过并行实现了更好的吞吐量。</p><p id="4239" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">比方说，如果我们必须为我们的一个大客户导出<strong class="kx iu">通话记录</strong>，我们将需要利用分而治之的方法，并且需要一种更好的方法来并行化这项工作。</p><h1 id="3ac8" class="lz ma it bd mb mc nk me mf mg nl mi mj jz nm ka ml kc nn kd mn kf no kg mp mq bi translated">按列值对 SQL 查询进行分区</h1><p id="37af" class="pw-post-body-paragraph kv kw it kx b ky mr ju la lb ms jx ld le mt lg lh li mu lk ll lm mv lo lp lq im bi translated">这意味着 Spark 可以同时对同一个表执行多个查询，但是每个查询通过为一个列(分区列)设置不同的范围值来运行。这可以在 Spark 中通过多设置几个参数来实现。让我们再看几个参数:</p><p id="68dc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe nh ni nj mx b">numPartitions</code>选项定义了表中可用于并行读取的最大分区数。这也决定了并发 JDBC 连接的最大数量。</p><p id="50bf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe nh ni nj mx b">partitionColumn</code>必须是相关表中的数值、日期或时间戳列。这些参数描述了在多个工作线程并行读取时如何对表进行分区。</p><p id="11de" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe nh ni nj mx b">lowerBound</code>和<code class="fe nh ni nj mx b">upperBound</code>只是用来决定分区步距，而不是用来过滤表中的行。因此表中的所有行将被分区并返回。此选项仅适用于阅读。</p><p id="fbc2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe nh ni nj mx b">fetchSize</code>JDBC 提取大小，决定每次往返要提取多少行。这有助于提高默认为低读取大小的 JDBC 驱动程序的性能(例如，Oracle 有 10 行)。此选项仅适用于阅读。</p><p id="1ce0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上例中的<code class="fe nh ni nj mx b">partitionColumn</code>可以是<code class="fe nh ni nj mx b">CallId</code>。让我们试着把所有的参数联系起来。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="c31a" class="nb ma it mx b gy nc nd l ne nf"><strong class="mx iu">val </strong>dbConfig = Map("username" -&gt; "admin", <br/>                   "password" -&gt; "pwd", <br/>                   "url" -&gt; "http://localhost:3306",<br/>                   "numPartitions" -&gt; 10,<br/>                   "paritionColumn" -&gt; "CallId",<br/>                   "lowerBound" -&gt; 0, <br/>                   "upperBound" -&gt; 10,000,000)<br/><strong class="mx iu">val</strong> query = "select * from CallLog where CustomerId=1"</span><span id="4842" class="nb ma it mx b gy ng nd l ne nf"><strong class="mx iu">val</strong> jdbcDF <strong class="mx iu">=</strong> spark.read<br/>  .format("jdbc")      <br/>  .options(dbConfig)      <br/>  .option("dbtable", s"(${query}) AS tmp")      <br/>  .load()</span></pre><p id="fcd6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上述配置将导致运行以下并行查询:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="84ec" class="nb ma it mx b gy nc nd l ne nf">SELECT * from CallLog where CustomerId =1 AND CallId &gt;=0 AND CallId &lt;1,000,000<br/>SELECT * from CallLog where CustomerId =1 AND CallId &gt;= 1,000,000 AND CallId &lt;2,000,000<br/>SELECT * from CallLog where CustomerId =1 AND CallId&gt;= 2,000,000 AND CallId &lt;3,000,000<br/>....<br/>SELECT * from CallLog where CustomerId =1 AND CallId&gt;= 10,000,000</span></pre><p id="63b2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上述查询并行化将有助于更快地从表中读取结果。</p><p id="ee99" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一旦 Spark 能够从 Mysql 读取数据，将数据转储到 S3 就变得轻而易举了。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="ed8d" class="nb ma it mx b gy nc nd l ne nf">jdbcDF.write<br/>      .format("json")      <br/>      .mode("append")<br/>      .save("${s3path}")</span></pre><p id="7e83" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="lr">结论:</em> </strong></p><p id="5a41" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上面的方法让我们有机会使用<strong class="kx iu"> Spark </strong>来解决一个经典的批处理作业问题。我们正在用 Apache Spark 做更多的事情，这是众多用例中的一个。我很想听听你用火花在做什么。</p></div></div>    
</body>
</html>