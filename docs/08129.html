<html>
<head>
<title>Linear Regression: The (Actually) Complete Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:(实际上)完全介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-the-actually-complete-introduction-67152323fcf2?source=collection_archive---------38-----------------------#2020-06-15">https://towardsdatascience.com/linear-regression-the-actually-complete-introduction-67152323fcf2?source=collection_archive---------38-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b12e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一位同学用Python对这个简单的机器学习算法进行了全面、深入的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cf5bd82f2f93eba469169ebeb869d0db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0TY_Z105uYqwsEfbOIaZQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python线性回归代码(所有照片由作者提供)</p></figure><h1 id="2f63" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated"><span class="l lq lr ls bm lt lu lv lw lx di"> I </span>简介</h1><p id="6b0a" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi mu translated"><span class="l lq lr ls bm lt lu lv lw lx di">我</span>记得我第一次钻研机器学习领域时是多么兴奋。炒作是可以理解的，有哪个软件工程专业的学生不想投身到当今最激动人心的相关技术中呢？</p><p id="8047" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">但是随着我兴趣的增长，我开始在这个问题上自学，我经常被我读到的一些文章的技术性吓到。精彩的文章，毫无疑问，但写得太超前了，即使是那些针对初学者的文章。另一方面，许多人过于务实，忽略了理论解释，而倾向于让新的学习者尽快上手。虽然两者都很有价值，但我觉得，作为一名学生，我可以在这里填补一个空白。</p><p id="c0b8" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu">我的目标是整合我希望在开始时就能掌握的所有信息，概述这个简单的机器学习算法背后的理论，然后以一种可理解但全面的方式给出一个解释透彻的实际例子。</strong> <em class="na">一个学生对另一个学生。</em></p><blockquote class="nb nc nd"><p id="cda5" class="ly lz na ma b mb mv ju md me mw jx mg ne mx mj mk nf my mn mo ng mz mr ms mt im bi translated"><strong class="ma iu">因此，欢迎阅读我希望在构建第一个线性回归模型时能够读到的文章。</strong></p></blockquote><h1 id="bd45" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">一些理论</h1><p id="7b9e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi mu translated"><span class="l lq lr ls bm lt lu lv lw lx di"> R </span>回归分析是一套统计过程，我们通过它来估计一个或多个给定自变量<em class="na">【x】</em>的因变量<em class="na">【y】</em>之间的关系。在机器学习的背景下，它是监督学习的一个子领域。</p><p id="e159" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">回归有几种类型，每一种描述自变量和因变量之间不同的数学关系。一些常见的例子包括多项式，逻辑和，本文的主题，线性。</p><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/polynomial-regression-the-only-introduction-youll-need-49a6fb2b86de"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">多项式回归:你需要的唯一介绍</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">一名学生对Python中机器学习算法背后的理论和应用的深入探究</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny ks nk"/></div></div></a></div><p id="5bc1" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">但是你如何选择呢？有什么区别？嗯，就像我上面说的，要看数据。举个例子:比方说，我们希望预测一种疾病在人群中蔓延并逐渐消失的过程。自然地，随着天数的增加，病例数也会增加——直到它们开始下降，形成抛物线形状。如下图所示，最佳拟合直线无法准确预测第100天的病例数。但是多项式回归可以。但是我们将在下一篇文章中深入探讨这个问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/7f4e3ccd0a816fd996a03b3db8095a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2s3cezqBBGZkeftpcw3S6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归不适用的例子</p></figure><p id="d199" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">相反，当我们有如下图所示的趋势变化的数据时，一条直线就相当准确。这是一个线性回归:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/db88804644e3af148e750b839176a16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvXat65u06w5sxL5oCiiTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归适用的例子</p></figure><blockquote class="nb nc nd"><p id="6aa8" class="ly lz na ma b mb mv ju md me mw jx mg ne mx mj mk nf my mn mo ng mz mr ms mt im bi translated">因此，当因变量和自变量之间的关系可以相当准确地建模为直线时，就使用线性回归。</p></blockquote><p id="601f" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">这将是我们的<em class="na">最佳拟合线，</em>你可能还记得高中时的等式:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="7a9b" class="og kz it oc b gy oh oi l oj ok">The way I learnt it in high school:   y = mx + c<br/>Machine Learning convention:          h(X) = W0 + W1.X</span></pre><p id="67b5" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">其中:</p><ol class=""><li id="53b8" class="ol om it ma b mb mv me mw mh on ml oo mp op mt oq or os ot bi translated"><em class="na"> y </em>或<em class="na"> h(x) </em> =因变量(也就是我们试图估计的)</li><li id="15c9" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated"><em class="na"> m </em>或<em class="na"> W1 </em> =坡度</li><li id="559e" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated"><em class="na"> x </em>或<em class="na"> X </em> =因变量(又名输入值)</li><li id="aae1" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated"><em class="na"> c </em>或<em class="na">W0</em>= y轴上的截距</li></ol><h1 id="a188" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">术语</h1><p id="c25c" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi mu translated">我们如何找到最佳拟合直线的方程？通过调整一组参数(W0和W1)直到我们找到它们各自的值，使得模型的残差平方和(实际值和预测值之间的差)<em class="na">尽可能小。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/a8e25e60f1c3cd9cfb6346132da209a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNF_9okjbo3P8yMyFlE7ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归的一些残差</p></figure><p id="b614" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">在继续之前，让我们复习一些重要的术语。很容易混淆这些术语，但是理解这些指标对于确定模型的可靠性至关重要。</p><h2 id="533b" class="og kz it bd la pa pb dn le pc pd dp li mh pe pf lk ml pg ph lm mp pi pj lo pk bi translated">差异</h2><p id="91f1" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">本质上，方差是对我们的最佳拟合线有多不准确的一种度量，并通过<em class="na"> R </em>分数来量化。我们的目标是使方差尽可能小，所以我们的<em class="na"> R </em>得分越高越好。</p><h2 id="21e7" class="og kz it bd la pa pb dn le pc pd dp li mh pe pf lk ml pg ph lm mp pi pj lo pk bi translated">稀有</h2><p id="4624" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">有时称为<em class="na">成本</em>函数，用于将方差表示为预测的<em class="na">决定系数R </em>，其范围从0到1，1为最佳拟合。</p><h2 id="0e53" class="og kz it bd la pa pb dn le pc pd dp li mh pe pf lk ml pg ph lm mp pi pj lo pk bi translated">均方误差</h2><p id="170d" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">误差平方的平均值(我们将它们平方，因此没有负值)。数字越大，误差越大。我们的目标是尽量减少这种情况。</p><h1 id="5ef7" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">该算法</h1><p id="366e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们将使用<strong class="ma iu"> <em class="na">普通最小二乘法</em> </strong> <em class="na"> </em>方法，这是一种简单的解析的非迭代解法。如果我们想要应用更复杂的机器学习算法，比如支持向量机，那么我们需要使用<strong class="ma iu"> <em class="na">梯度下降</em>，</strong>，这将给我们一个迭代完成的OLS解的近似值。但是这是另一篇文章的主题。</p><p id="fd66" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">因此，使用上述函数，我们训练我们的模型，直到它学习到最小化残差平方和的最佳系数。一旦我们在一些数据(比如说，数据集的前80%)上训练了我们的模型，我们将在其余的数据(另外的20%)上测试它。</p><h1 id="eecd" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">这个例子</h1><p id="f1e0" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">让我们从头开始，进口:</p><ol class=""><li id="bc11" class="ol om it ma b mb mv me mw mh on ml oo mp op mt oq or os ot bi translated">matplotlib(py plot &amp; RC params)——创建我们的数据可视化</li><li id="eb51" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">sci kit-Learn(load _ diabetes &amp; linear _ model)—执行机器学习</li><li id="1150" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">NumPy——做科学计算</li></ol><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="e036" class="og kz it oc b gy oh oi l oj ok">import matplotlib.pyplot as plt<br/>from matplotlib import rcParams<br/>from sklearn.datasets import load_diabetes<br/>from sklearn import linear_model<br/>import numpy as np</span></pre><p id="d9dd" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">接下来，我们加载数据集并创建一个对象<em class="na"> dx。</em>糖尿病数据集来自Scikit-Learn，由10个生理变量(年龄、性别、体重、血压等)组成。)和一年后疾病进展的指标。目标是从生理变量预测疾病进展。</p><p id="a614" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">现在，Scikit-Learn数据集返回一个叫做<em class="na"> Bunch </em>的东西，它类似于一个字典。这一堆有各种属性，其中之一是<em class="na">数据。</em>这是我们希望使用的数据矩阵。另一个是<em class="na">目标</em>，我们很快就会谈到。但是我们不需要所有的数据，所以我们选择我们想要的特性，并使用numpy.newaxis将数组维数从1增加到2。我们现在已经把数组变成了一个列向量。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c756" class="og kz it oc b gy oh oi l oj ok">d =​ ​load_diabetes​()<br/>dx = d.data[:, np.newaxis, 2]</span></pre><p id="f5f2" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">如果这一步有点混乱，没关系。重点是，我们现在有了一个包含数据的2D数组，这是必要的格式。您真的可以用任何数据集(自定义列表或. csv文件)来实现这一点，其中您有带有x和y值的数据点。所以现在我们的看起来像这样:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="cec0" class="og kz it oc b gy oh oi l oj ok">[[ 0.06169621]<br/> [-0.05147406]<br/> [ 0.04445121]<br/> [-0.01159501]<br/> [-0.03638469]<br/> [-0.04069594]<br/> [-0.04716281]<br/> [...        ]]</span></pre><p id="3b84" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">接下来，我们将数据集分成训练集和测试集——这是机器学习的基本部分。你会注意到。我前面提到的<em class="na">目标</em>属性。这些基本上是正确的值，或<em class="na">响应变量</em>。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="7b1a" class="og kz it oc b gy oh oi l oj ok">dx_train = dx[:-20]<br/>dy_train = d.target[:-20]<br/>dx_test = dx[-20:]<br/>dy_test = d.target[-20:]</span></pre><p id="41a9" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">此时，散点图会有所帮助。仅仅通过观察，我们就可以推断出线性回归是否会提供一个准确的模型。我将使用rcParams添加一些样式，使它看起来更有吸引力，但不要担心这一点。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="6b79" class="og kz it oc b gy oh oi l oj ok">rcParams['axes.spines.top'] = False<br/>rcParams['axes.spines.right'] = False<br/>rcParams['lines.linewidth'] = 2</span><span id="ffc3" class="og kz it oc b gy pl oi l oj ok">plt.scatter(dx_train, dy_train, c='#9dd4a7', label='Training data')<br/>plt.scatter(dx_test, dy_test, c='#d66565', label='Testing data')</span><span id="d572" class="og kz it oc b gy pl oi l oj ok">plt.legend(loc="upper left")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/c83ddec4f47e96ec4fedb95074285947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I28wJj_KGdYvXrMKW1t3nQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">散点图上我们的训练和测试数据</p></figure><p id="8129" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">你可能知道，看起来好像一条直线可以或多或少地预测这一趋势的走向。</p><p id="8418" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">现在有趣的部分来了。我们将为线性回归创建一个对象<em class="na"> lr </em>，并将数据拟合到其中。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="19ef" class="og kz it oc b gy oh oi l oj ok">lr = linear_model.LinearRegression()<br/>lr.fit(dx_train, dy_train)</span></pre><p id="dbae" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">我们剩下要做的就是在散点图上绘制最佳拟合线:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="22ff" class="og kz it oc b gy oh oi l oj ok">plt.plot(dx_test, lr.predict(dx_test), c='#404040', label='Line of best fit')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/4a6adefaa8c31ee501590aaeb6cb3795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmNbpOS5wOIaUujSbvq2SQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的最佳拟合和测试数据系列</p></figure><blockquote class="nb nc nd"><p id="ac10" class="ly lz na ma b mb mv ju md me mw jx mg ne mx mj mk nf my mn mo ng mz mr ms mt im bi translated"><strong class="ma iu">恭喜你！</strong>您已经成功训练并测试了一个线性回归模型。</p></blockquote><p id="fb2c" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">但是我们现在还不能沾沾自喜…</p><h1 id="755b" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">潜得更深</h1><p id="61f0" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在这个阶段，我觉得我们应该更深入。我们必须了解到底发生了什么。</p><p id="8112" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">LinearRegression()类是好事发生的地方。这就是线性模型<em class="na"> lr </em>适合最小化预测值和目标值之间的残差平方和的系数的地方，正如我前面提到的。</p><p id="83a1" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">这个类包含了<em class="na">。fit() </em>函数，我们可以看到它被应用于线性回归对象<em class="na"> lr </em>。我们将训练数据(x和y值)作为参数传入，函数返回对象的一个实例，现在该实例已与数据相匹配。</p><p id="51c3" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">最后，我们看到。<em class="na"> predict()，</em>linear regression()类的另一个函数。这是通过计算最佳拟合直线的方程返回预测值的函数。</p><p id="c81d" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu">理解这些函数的最好方法是<em class="na">重写没有它们的程序。</em> </strong></p><p id="c4d7" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">这是普通最小二乘算法的起点。我们需要做的第一件事是找到最佳拟合线<em class="na">的梯度<em class="na"> m </em>和y轴截距<em class="na"> c </em>。</em>以下是各自的公式:</p><ul class=""><li id="9482" class="ol om it ma b mb mv me mw mh on ml oo mp op mt po or os ot bi translated"><em class="na">m</em>=(μ(<em class="na">x</em>)*μ(<em class="na">y</em>)—μ(<em class="na">x</em>*<em class="na">y</em>)/((μ(<em class="na">x</em>))2μ(<em class="na">x</em>2))</li><li id="76d5" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt po or os ot bi translated"><em class="na">c</em>=μ(<em class="na">y</em>)—<em class="na">m</em>*μ(<em class="na">x</em>)</li></ul><p id="1ed6" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">我们用numpy.mean来求平均值<em class="na"> μ </em>。我将这两个公式实现为一个函数:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="7154" class="og kz it oc b gy oh oi l oj ok">def find_gradient_and_y_intercept():</span><span id="cad9" class="og kz it oc b gy pl oi l oj ok">   m = (np.mean(dx_train) * np.mean(dy_train)<br/>   - np.mean(dx_train *  dy_train)) / ((np.mean(dx_train)) **<br/>   2 -  np.mean(dx_train ** 2))</span><span id="be4d" class="og kz it oc b gy pl oi l oj ok">   c = np.mean(dy_train) - m * np.mean(dx_train)</span><span id="0eff" class="og kz it oc b gy pl oi l oj ok">   return m, c</span></pre><p id="fec8" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">请注意，现在我们不必像以前一样将数组更改为2D，因为我们没有使用。<em class="na"> fit() </em>功能了。因此，将我们之前使用<em class="na"> numpy.newaxis </em>的那一行修改成这样:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c091" class="og kz it oc b gy oh oi l oj ok">dx = d.data[:, 2]</span></pre><p id="3dc7" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">现在，当我们绘制最佳拟合线时，不使用。<em class="na"> predict() </em>函数，我们实际上输入了我们对于最佳拟合线的方程，<em class="na"> mx + c，</em>作为<em class="na"> y </em>值。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="960b" class="og kz it oc b gy oh oi l oj ok">plt.plot(dx_test, ((m * dx_test) + b), c='0.2', label="Line of Best Fit")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/83ee03e1f25f1160adea34cbf7f3b692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTM01ycqwaDpChkMVWWhCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一条与之前完全相同的最佳拟合线</p></figure><blockquote class="nb nc nd"><p id="d59e" class="ly lz na ma b mb mv ju md me mw jx mg ne mx mj mk nf my mn mo ng mz mr ms mt im bi translated"><strong class="ma iu">这次真的恭喜你了！</strong>你刚刚从零开始写了一个线性回归算法。希望您现在已经对算法及其相关功能有了透彻的理解。</p></blockquote><p id="5759" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">作为奖励，让我们计算我们模型的<em class="na">均方误差</em>和<em class="na">得分</em>(前面定义的预测的决定系数<em class="na"> R、</em>)。</p><p id="9942" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">使用LinearRegression()类:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="91b1" class="og kz it oc b gy oh oi l oj ok">mse = np.mean((lr.predict(dx_test)-dy_test)**2)<br/>score = lr.score(dx_test, dy_test)</span></pre><p id="8990" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">不使用类:系数<em class="na"> R </em>定义为<em class="na"> (1 — u/v) </em>，其中u为残差平方和<em class="na"> ((y_true — y_pred) ** 2)。sum() </em>和<em class="na"> v </em>是平方和的总和<em class="na"> ((y_true — y_true.mean()) ** 2)。sum(): </em></p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="b6e2" class="og kz it oc b gy oh oi l oj ok">mse = np.mean((((m * dx_test) + b) - dy_test) ** 2)<br/>score = (1 - ((dy_test - ((m * dx_test) + b)) ** 2).sum() / ((dy_test - dy_test.mean()) ** 2).sum())</span></pre><p id="0984" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">答案得出<em class="na"> mse </em> = 2548.07和<em class="na"> R </em> = 0.47。</p><h1 id="73a2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><p id="9318" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">这就是对机器学习最简单的算法——线性回归的全面介绍。我希望，作为一名学生，我能够以一种相关和全面的方式解释这些概念。</p><p id="3681" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu">简单回顾一下我们讲过的内容:</strong></p><ol class=""><li id="3157" class="ol om it ma b mb mv me mw mh on ml oo mp op mt oq or os ot bi translated">线性回归的定义</li><li id="22fd" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">一些重要术语</li><li id="d5b9" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">对算法的解释</li><li id="1e40" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">Python中的一个实际例子</li><li id="62fe" class="ol om it ma b mb ou me ov mh ow ml ox mp oy mt oq or os ot bi translated">对示例中函数的详细检查</li></ol><p id="be75" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">如果您觉得这篇文章有帮助，我很乐意与您合作！关注我<a class="ae pq" href="https://www.instagram.com/adenhaus/" rel="noopener ugc nofollow" target="_blank"> Instagram </a>了解更多机器学习、软件工程和创业内容。</p><p id="d29d" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated">编码快乐！</p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><p id="76f0" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><a class="ae pq" href="https://medium.com/subscribe/@adenhaus" rel="noopener"> <strong class="ma iu">订阅</strong> </a>📚为了不错过我的一篇新文章，如果你还不是中等会员，<a class="ae pq" href="https://medium.com/@adenhaus/membership" rel="noopener"> <strong class="ma iu">加入</strong> </a>🚀去读我所有的，还有成千上万的其他故事！</p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><h1 id="ac0e" class="ky kz it bd la lb py ld le lf pz lh li jz qa ka lk kc qb kd lm kf qc kg lo lp bi translated">资源</h1><p id="1325" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><strong class="ma iu"> Scikit学习</strong> <em class="na">线性_模型。LinearRegression()文档:</em><a class="ae pq" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。linear regression . html # sk learn . linear _ model。线性回归.预测</a></p><p id="43c5" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu"> Scikit Learn </strong> <em class="na">线性回归示例:</em><a class="ae pq" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html" rel="noopener ugc nofollow" target="_blank">https://Scikit-Learn . org/stable/auto _ examples/Linear _ model/plot _ ols . html</a></p><p id="a7a9" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu">sci kit Learn</strong><em class="na">load _ diabetes文档:</em><a class="ae pq" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . datasets . load _ diabetes . html # sk Learn . datasets . load _ diabetes</a></p><p id="9f2d" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu"> Scikit Learn </strong> <em class="na">机器学习简介:</em><a class="ae pq" href="https://scikit-learn.org/stable/tutorial/basic/tutorial.html" rel="noopener ugc nofollow" target="_blank">https://Scikit-Learn . org/stable/tutorial/basic/tutorial . html</a></p><p id="3998" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu">真实Python </strong> <em class="na">线性回归:</em><a class="ae pq" href="https://realpython.com/linear-regression-in-python/#simple-linear-regression" rel="noopener ugc nofollow" target="_blank">https://Real Python . com/Linear-Regression-in-Python/# simple-Linear-Regression</a></p><p id="1c18" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu"> Statisticsbyjim </strong> <em class="na">解读R:</em><a class="ae pq" href="https://statisticsbyjim.com/regression/interpret-r-squared-regression/" rel="noopener ugc nofollow" target="_blank">https://Statisticsbyjim . com/regression/interpret-R-squared-regression/</a></p><p id="366d" class="pw-post-body-paragraph ly lz it ma b mb mv ju md me mw jx mg mh mx mj mk ml my mn mo mp mz mr ms mt im bi translated"><strong class="ma iu"> BMC </strong> <em class="na">均方差&amp;R:</em><a class="ae pq" href="https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/" rel="noopener ugc nofollow" target="_blank">https://www . BMC . com/blogs/Mean-squared-error-R2-and-variance-in-regression-analysis/</a></p></div></div>    
</body>
</html>