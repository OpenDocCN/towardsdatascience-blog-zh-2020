<html>
<head>
<title>Algorithms From Scratch: Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的算法:支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-support-vector-machine-6f5eb72fce10?source=collection_archive---------15-----------------------#2020-08-04">https://towardsdatascience.com/algorithms-from-scratch-support-vector-machine-6f5eb72fce10?source=collection_archive---------15-----------------------#2020-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ea71" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="ccb7" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始详述和构建支持向量机</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a97a545ac7b672ee73e9a29a6d1b5e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iTlk1guwsKqTBh3z"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@willsudds?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄的照片将在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">的</a>上突然出现</a></p></figure><p id="48b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">支持向量机是一种流行的算法，能够执行线性或非线性分类和回归，在深度学习兴起之前，由于令人兴奋的内核技巧，支持向量机是一个热门话题——如果这个术语现在对你没有意义，不要担心。在这篇文章结束时，你会对支持向量机的直觉有很好的理解，在线性支持向量机下发生了什么，以及如何用 Python 实现一个支持向量机。</p><p id="c2c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要从头开始查看完整的<strong class="lk jd"> <em class="me">算法</em> </strong>系列，请点击下面的链接。</p><div class="mf mg gp gr mh mi"><a href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener follow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">从零开始的算法——走向数据科学</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">阅读《走向数据科学》中关于算法的文章。分享概念、想法和…</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">towardsdatascience.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw lb mi"/></div></div></a></div><h2 id="c6a3" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">直觉</h2><p id="8ed2" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">在分类问题中，SVM 的目标是拟合两个类别之间的最大可能差值。相反，回归任务改变了分类任务的目标，并试图在裕度内适应尽可能多的实例——我们将首先关注分类。</p><p id="0a76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们只关注数据的极端值(位于聚类边缘的观察值),并且我们将阈值定义为两个极端值之间的中点，那么我们就剩下了一个用于分隔两个类的裕度，这通常被称为超平面。当我们应用一个给我们最大余量的阈值(意味着我们严格确保没有实例落在余量内)来进行分类时，这被称为<strong class="lk jd">硬余量分类</strong>(一些文本称之为<em class="me">最大余量分类</em>)。</p><p id="b3bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当详细说明硬利润分类时，直观地看到发生了什么总是有帮助的，因此<em class="me">图 2 </em>是硬利润分类的一个例子。为此，我们将使用来自 scikit 的<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>-学习和实用函数<code class="fe nu nv nw nx b">plot_svm()</code>,您可以在下面的 github 链接上获得完整代码。</p><div class="mf mg gp gr mh mi"><a href="https://github.com/kurtispykes/ml-from-scratch/blob/master/support_vector_machine.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">kurtispykes/ml-从零开始</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">github.com</p></div></div><div class="mr l"><div class="ny l mt mu mv mr mw lb mi"/></div></div></a></div><blockquote class="nz oa ob"><p id="c582" class="li lj me lk b ll lm kd ln lo lp kg lq oc ls lt lu od lw lx ly oe ma mb mc md im bi translated"><strong class="lk jd">注意</strong>:这个故事是使用 python 包<code class="fe nu nv nw nx b"><em class="it">jupyter_to_medium</em></code>直接从 jupyter 笔记本上写的——要了解关于这个包<a class="ae lh" rel="noopener" target="_blank" href="/publishing-to-medium-from-jupyter-notebooks-53978dd21fac?source=activity---post_recommended"> <strong class="lk jd">的更多信息，请点击这里</strong></a>——github 上提交的版本是初稿，因此你可能会注意到这篇文章的一些改动。</p></blockquote><pre class="ks kt ku kv gt of nx og oh aw oi bi"><span id="5d3d" class="mx my it nx b gy oj ok l ol om">import pandas as pd <br/>import numpy as np <br/>from sklearn.svm import LinearSVC<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.datasets import load_iris<br/>import matplotlib.pyplot as plt <br/>%matplotlib inline</span><span id="84e6" class="mx my it nx b gy on ok l ol om"># store the data <br/>iris = load_iris()<br/><br/># convert to DataFrame<br/>df = pd.DataFrame(data=iris.data,<br/>                  columns= iris.feature_names)<br/><br/># store mapping of targets and target names<br/>target_dict = dict(zip(set(iris.target), iris.target_names))<br/><br/># add the target labels and the feature names<br/>df["target"] = iris.target<br/>df["target_names"] = df.target.map(target_dict)<br/><br/># view the data<br/>df.tail()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/d39d5c29d83eae43c0dde02282d38efb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*sVRcYsOpsl7yOJaeDVLBhA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 1:原始数据集</p></figure><pre class="ks kt ku kv gt of nx og oh aw oi bi"><span id="1168" class="mx my it nx b gy oj ok l ol om"># setting X and y  <br/>X = df.query("target_names == 'setosa' or target_names == 'versicolor'").loc[:, "petal length (cm)":"petal width (cm)"] <br/>y = df.query("target_names == 'setosa' or target_names == 'versicolor'").loc[:, "target"] <br/><br/># fit the model with hard margin (Large C parameter)<br/>svc = LinearSVC(loss="hinge", C=1000)<br/>svc.fit(X, y)<br/><br/>plot_svm()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/3d096ef14fef7fb513f3781c06e66a63.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ca-SYJlnnJh8DfLzRKnd6w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 2:SVM 的可视化决策边界</p></figure><p id="3e44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">图 2 </em>显示了线性 SVM 如何使用硬边界分类来确保没有实例落入边界内。尽管这对于我们当前的场景来说看起来不错，但我们必须小心考虑执行硬利润分类带来的陷阱:</p><ol class=""><li id="7635" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated">对异常值非常敏感</li><li id="002e" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated">只有当类是线性可分的时候它才起作用</li></ol></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="8f3b" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">处理异常值和非线性数据</h2><p id="2469" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">硬边界分类的一种更灵活的替代方法是软边界分类，这是一种很好的解决方案，可以克服上面列出的硬边界分类中的缺陷，主要是解决对异常值的敏感性问题。当我们允许存在一些错误分类时(意味着一些负面观察可能被分类为正面，反之亦然)，从阈值到观察的距离被称为<strong class="lk jd"> <em class="me">软余量</em> </strong>。在软裕度分类中，我们的目标是在最大化裕度大小和限制裕度中的违规数量(落在裕度中的观察数量)之间实现良好的平衡。</p><p id="4569" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是的，线性 SVM 分类器(硬边界和软边界)非常有效，并且在许多情况下工作得非常好，但是当数据集不是线性可分的时候，就像许多数据集经常出现的情况一样，更好的解决方案是利用支持向量机内核技巧(一旦你理解了内核技巧，你可能会注意到它不是支持向量机独有的)。内核技巧将非线性可分离数据映射到更高维度，然后使用超平面来分离类别。让这个技巧如此令人兴奋的是，将数据映射到更高的维度实际上并没有增加新的功能，但我们仍然得到了相同的结果，就好像我们做了一样。因为我们不需要在数据中加入新的特征，我们的模型计算效率更高，效果也一样好。</p><p id="54b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你会在下面看到这种现象的一个例子。</p><h2 id="3bfd" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">术语</h2><ul class=""><li id="1e37" class="op oq it lk b ll np lo nq lr pk lv pl lz pm md pn ov ow ox bi translated"><strong class="lk jd">决策边界</strong>:将数据集分成两类的超平面</li><li id="54ba" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pn ov ow ox bi translated"><strong class="lk jd">支持向量</strong>:观察值位于聚类的边缘(离分离超平面最近)。</li><li id="031c" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pn ov ow ox bi translated"><strong class="lk jd">硬边界</strong>:当我们严格规定所有观察值都不在边界内时</li><li id="4d0c" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pn ov ow ox bi translated"><strong class="lk jd">软边界</strong>:当我们允许一些错误分类时。我们试图找到一个平衡点，既保持尽可能大的差值，又限制违规次数(<a class="ae lh" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">偏差/方差权衡</a>)</li></ul><pre class="ks kt ku kv gt of nx og oh aw oi bi"><span id="3049" class="mx my it nx b gy oj ok l ol om">from sklearn.datasets import make_moons<br/>from mlxtend.plotting import plot_decision_regions<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.svm import SVC</span><span id="f230" class="mx my it nx b gy on ok l ol om"># loading the data<br/>X, y = make_moons(noise=0.3, random_state=0)<br/><br/># scale features<br/>scaler = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)<br/><br/># fit the model with polynomial kernel<br/>svc_clf = SVC(kernel="poly", degree=3, C=5, coef0=1)<br/>svc_clf.fit(X_scaled, y)<br/><br/># plotting the decision regions<br/>plt.figure(figsize=(10, 5))<br/>plot_decision_regions(X_scaled, y, clf=svc_clf)<br/><br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/fc80158f37dccb48ec47c4056686fa69.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3_HQ4tfoWmfkEXrQHaurpw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 3:应用于非线性数据的内核技巧</p></figure><blockquote class="nz oa ob"><p id="ee77" class="li lj me lk b ll lm kd ln lo lp kg lq oc ls lt lu od lw lx ly oe ma mb mc md im bi translated">注意:我们对这个数据集应用了一个多项式核，但是 RBF 也是一个非常受欢迎的核，应用于许多机器学习问题，并且经常在数据不是线性可分时用作默认核。</p></blockquote></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="4428" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">创建模型</h2><p id="81a1" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">既然我们已经对 SVM 正在做的事情建立了概念上的理解，让我们来理解在这个模型的引擎盖下正在发生什么。线性 SVM 分类器计算决策函数<code class="fe nu nv nw nx b">w.T * x + b</code>并预测结果为肯定的肯定类别，否则为否定类别。训练线性 SVM 分类器意味着找到使边缘尽可能宽的值<code class="fe nu nv nw nx b">w</code>和<code class="fe nu nv nw nx b">b</code>，同时避免边缘违规(硬边缘分类)或限制它们(软边缘分类)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/7f8b2bc89024ef1623aad56a4d3fffba.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*Os1ujB0TtBsfmZ8IGP7gVQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 4:线性 SVM 分类器预测</p></figure><p id="c323" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策函数的斜率等于权重向量的范数，因此为了实现最大可能的裕度，我们希望最小化权重向量的范数。但是，我们有办法实现硬利润分类和软利润分类。</p><p id="931d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">硬边界优化问题如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/a597f881a576816cfbc714d86eedcfae.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*XejjglR640Siy5b74VAwlw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 5:线性 SVM(硬边界分类器)目标</p></figure><p id="a063" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">和软利润:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/33a6bd5688116296328893f7952666db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*CAZU6-dUN3nChQrKyif5tw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 6:线性 SVM(软间隔分类器)目标；请注意，为了实现软余量，我们为每个实例添加了一个松弛变量(zeta ≥ 0)，该变量测量每个实例允许违反余量的程度。</p></figure></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="cbf6" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">履行</h2><blockquote class="nz oa ob"><p id="596b" class="li lj me lk b ll lm kd ln lo lp kg lq oc ls lt lu od lw lx ly oe ma mb mc md im bi translated"><strong class="lk jd">注意</strong>:对于这个实现，我将进行硬边界分类，但是进一步的工作将包括软边界的 Python 实现和对不同数据集执行的内核技巧，包括基于回归的任务——要获得这些帖子的通知，您可以在<a class="ae lh" href="https://github.com/kurtispykes" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p></blockquote><pre class="ks kt ku kv gt of nx og oh aw oi bi"><span id="8965" class="mx my it nx b gy oj ok l ol om">from sklearn.datasets.samples_generator import make_blobs <br/><br/># generating a dataset<br/>X, y = make_blobs(n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=23)</span><span id="9eb0" class="mx my it nx b gy on ok l ol om">def initialize_param(X): <br/>    """<br/>    Initializing the weight vector and bias<br/>    """<br/>    _, n_features = X.shape<br/>    w = np.zeros(n_features)<br/>    b = 0 <br/>    return w, b</span><span id="3b6f" class="mx my it nx b gy on ok l ol om">def optimization(X, y, learning_rate=0.001, lambd=0.01, n_iters=1000): <br/>    """<br/>    finding value of w and b that make the margin as large as possible while<br/>    avoiding violations (Hard margin classification)<br/>    """<br/>    t = np.where(y &lt;= 0, -1, 1)<br/>    w, b = initialize_param(X)<br/>    <br/>    for _ in range(n_iters): <br/>        for idx, x_i in enumerate(X): <br/>            condition = t[idx] * (np.dot(x_i, w) + b) &gt;= 1<br/>            if condition: <br/>                w -= learning_rate * (2 * lambd * w)<br/>            else: <br/>                w -= learning_rate * (2 * lambd *  w - np.dot(x_i, t[idx]))<br/>                b -= learning_rate * t[idx]<br/>    return w, b</span><span id="bb6c" class="mx my it nx b gy on ok l ol om">w, b = gradient_descent(X, y)</span><span id="621b" class="mx my it nx b gy on ok l ol om">def predict(X, w, b):<br/>    """<br/>    classify examples<br/>    """<br/>    decision = np.dot(X, w) + b<br/>    return np.sign(decision)</span><span id="a4df" class="mx my it nx b gy on ok l ol om"># my implementation visualization<br/>visualize_svm()<br/><br/># convert X to DataFrame to easily copy code<br/>X = pd.DataFrame(data=X,<br/>                 columns= ["x1", "x2"])<br/><br/># fit the model with hard margin (Large C parameter)<br/>svc = LinearSVC(loss="hinge", C=1000)<br/>svc.fit(X, y)<br/><br/># sklearn implementation visualization<br/>plot_svm()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/536c13153e019ce9079403372a7446a4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*25b2NPi8pVirJvOaSKfJ7A.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/0452d886393d22d0825fbe44372adcf9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-4UNnGGOwq6kZn6Ctr2VJw.png"/></div></figure></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="b530" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">赞成的意见</h2><ul class=""><li id="5834" class="op oq it lk b ll np lo nq lr pk lv pl lz pm md pn ov ow ox bi translated">非常好的线性分类器，因为它找到了最佳决策边界(在硬边界分类意义上)</li><li id="e741" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pn ov ow ox bi translated">易于转换成非线性模型</li></ul><h2 id="0cff" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">骗局</h2><ul class=""><li id="b717" class="op oq it lk b ll np lo nq lr pk lv pl lz pm md pn ov ow ox bi translated">不适合大型数据集</li></ul></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="3b7c" class="mx my it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">包裹</h2><p id="8e3b" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">SVM 是一种很难编码的算法，它很好地提醒了我们为什么应该感谢机器学习库，让我们可以用几行代码实现它们。在这篇文章中，我没有深入到支持向量机的全部细节，仍然有相当多的空白，你可能想要阅读，例如<a class="ae lh" href="https://en.wikipedia.org/wiki/Support_vector_machine#Computing_the_SVM_classifier" rel="noopener ugc nofollow" target="_blank">计算支持向量机</a>和<a class="ae lh" href="https://en.wikipedia.org/wiki/Support_vector_machine#Empirical_risk_minimization" rel="noopener ugc nofollow" target="_blank">经验风险最小化</a>。</p><blockquote class="nz oa ob"><p id="38c4" class="li lj me lk b ll lm kd ln lo lp kg lq oc ls lt lu od lw lx ly oe ma mb mc md im bi translated">此外，可能值得观看吴恩达关于支持向量机的讲座— <a class="ae lh" href="https://www.youtube.com/playlist?list=PLNeKWBMsAzboNdqcm4YY9x7Z2s9n9q_Tb" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">点击此处</strong> </a></p></blockquote><p id="2c83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢您花时间通读这个故事(因为它被称为媒体)。现在，您对支持向量机、SVM 下发生的事情以及如何用 Python 编写硬边界分类器有了很好的概念性理解。如果你想和我联系，我在 LinkedIn 上很容易找到。</p><div class="mf mg gp gr mh mi"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">Kurtis Pykes -人工智能作家-走向数据科学| LinkedIn</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">在世界上最大的职业社区 LinkedIn 上查看 Kurtis Pykes 的个人资料。Kurtis 有一个工作列在他们的…</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">www.linkedin.com</p></div></div><div class="mr l"><div class="pr l mt mu mv mr mw lb mi"/></div></div></a></div></div></div>    
</body>
</html>