# 当人工智能算法出轨时

> 原文：<https://towardsdatascience.com/when-ai-algorithms-go-off-the-rails-ace5cc8794f1?source=collection_archive---------73----------------------->

这里有一个非虚构的场景:一个运行了多年的算法警告信用卡使用中的异常行为，突然识别出不是异常值的人的行为异常(意味着假警报显著增加)。信用卡无缘无故被封；沮丧和紧张的客户打电话到客户服务中心投诉。

在客户服务中心经历了几个小时的异常压力后，服务团队意识到可能有不寻常的事件发生，并将问题传递给他们的开发团队。在一段额外的时间后，他们意识到算法出现了故障，并禁用它，直到它“回到正轨”。与此同时，其他客户也受到了信用卡异常行为警报缺失的影响，信用卡公司的财务和声誉也遭受了进一步的损害。

> 这是一个肯定会发生的场景。

随着人工智能越来越受欢迎，算法在我们日常生活和各种组织的关键流程中的使用越来越多，过去两年来，关于故障的报告越来越多，即人工智能算法犯了重大错误。

以[亚马逊的](https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28)人脸识别算法为例，该算法“漏掉”了几名著名的国会议员，并将他们识别为著名的罪犯。在另一个不幸的案例中，[谷歌的](https://www.google.com/amp/s/www.theverge.com/platform/amp/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)物体检测算法将一名女子标记为大猩猩。

这听起来可能很有趣，但图像识别中的错误可能会导致致命的结果，特别是在医疗领域——例如当另一种[谷歌](https://techcrunch.com/2020/04/27/google-medical-researchers-humbled-when-ai-screening-tool-falls-short-in-real-life-testing/amp/?guccounter=1)算法通过视网膜图像分析检测糖尿病时，未能在现实环境中准确检测。在另一起事件中，IBM 的沃森系统为癌症患者创造了错误的见解，迫使医生立即停止使用该机器。

> **“本品为一片 s——”**
> 
> **朱庇特医院的医生**

不正确的算法结果的不利和破坏性影响是显而易见的。虽然人工智能还不能取代人，但它无疑可以为我们做出糟糕的决定。

这种情况肯定会继续发生。数据治理团队(如首席数据官)负责创建和防止这种风险。

但是，组织对算法日益增长的依赖可能会像回旋镖一样回来吗？答案是，当然要看情况。这取决于它们允许持续维护过程的能力，以及理想情况下算法性能随时间的改进。此外，它还依赖于实时故障检测。当疾驰的列车脱离轨道时，他们必须有效地处理损害。

![](img/5feea5dc2d57fcb4e17c268d1dc82a5c.png)

来源:Johannes Plenio 在 Unsplash 上拍摄的照片

组织如何为这种情况做好准备，制定控制、预防和上报流程？在本文中，我将提出几种方法来处理这个问题。

但是在我们讨论解决方案之前，理解上述场景的起源是至关重要的。算法和人工智能的预测是基于反映特定时间点和描绘特定现实的数据训练过程。当这些算法遇到不断变化的现实并因此遇到不断变化的数据时，它们在学习过程中仍然保持静态，这正是这个问题出现的原因。

> 因此，首先需要的是机器学习算法运行时的**监控能力。**

算法的监控能力是正确和成功使用人工智能的必要条件。监控假设如果出了问题，它会随着时间的推移而发生。也就是说，如果将算法作为黑盒来监控，而输入和输出是已知的，则在时间线上，可以检测到实时异常和偏差(即，在训练阶段观察到的数据的预期分布的中断)。这种监控可以在伤害大量消费者之前被发现。

![](img/0182f15af5a8138b620c401894b04717.png)

来源:斯蒂芬·道森在 Unsplash 上拍摄的照片

但这还不够。理解到算法应该保持“活性”，我们需要产生一个解决最新数据的频繁版本升级机制。需要这样的工具来实现对算法的最终用户结果的反馈。这种反馈简化了实时学习过程。**在线学习使模型适应变化的现实**并减少损害。算法更新过程需要将其重新部署并集成到生产系统中。与敏捷时代的任何软件更新一样，需要进行健全性和逆向测试，以确保这种部署不会造成新的二次损害。

最后，向最终用户反映和解释算法的性能和结果是非常重要的。这种经历产生了必要的信任和解释，以防出现任何不相容的情况。仅仅为了澄清结果([解释权](https://en.m.wikipedia.org/wiki/Right_to_explanation))而将[解释](https://www.cio.com/article/3431658/enhancing-trust-in-artificial-intelligence-audits-and-explanations-can-help.html)的倾向是不充分的。**解释的额外价值在于能够监控不想要的现象并允许它们被纠正**，类似于质量保证软件过程。

解释用途是直接面向最终消费者，或面向客户服务团队。除了解释结果，它还允许即时反馈，从而可以实时改进算法。

所有这些都导致了一个结论，即人工智能算法不能再被视为企业应用程序的固有组件。之所以这样，是因为这种方法离不开监控和改进那些“黑箱”算法的过程。同时，我们希望他们的结果能够反映在组织应用程序的决策过程中。

> 因此，正确的技术解决方案是生产可以消费、监控、维护和改进的**人工智能微服务(SaaS)——不管它们与什么应用程序集成。**

![](img/2fc631dfb28b06183076eca2bf4b358e.png)

来源:Bill Oxford 在 Unsplash 上的照片；罗伊·梅赫雷兹编辑

让我们回到信用卡的场景。不满意的顾客打电话给服务中心。他们的问题会对照算法的解释结果进行检查，并立即进行纠正。结果被反馈到算法的实时学习过程中。与此同时，数据运营和治理团队的监控会警告正在出现的异常行为。在算法稳定之前，他们努力将对数以千计的客户的有害影响降至最低，甚至不惜以手动处理被检查为警告的查询为代价。

总之，从某个时间点的快照中学习的人工智能算法很可能随着时间的推移而不再使用。为了减轻静态使用人工智能算法的潜在损害，需要一个**持续维护流程**，监控异常实时事件，给出每个结果的解释，并确保算法的持续学习。我们还推荐将算法打包为服务来支持这种动态方法。

**努里特·科恩·英格**

**产品副总裁@ BeyondMinds**

BeyondMinds 是一家企业人工智能公司，它在学术研究和企业级人工智能应用的大规模采用之间架起了一座桥梁。BeyondMinds 解决方案套件使企业能够加速其人工智能转型，促进增长和降低成本。

*我们开发了一种高效的技术来构建、部署和维护最大价值的人工智能应用，将价值实现时间从数年缩短到数周。*