<html>
<head>
<title>Coursera DL Specialization Course in TF 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF 2.x中的Coursera DL专业化课程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coursera-dl-specialization-course-in-tf-2-x-18a1189e2a4?source=collection_archive---------24-----------------------#2020-03-13">https://towardsdatascience.com/coursera-dl-specialization-course-in-tf-2-x-18a1189e2a4?source=collection_archive---------24-----------------------#2020-03-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0ae9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Coursera课程转换为TF 2.x</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/500375a126acffe52352ce4c6c63e7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cozITwlOo6tzXSWrSxv_Cg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF 2.x是新的规范！</p></figure><p id="5f13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="http://deeplearning.ai" rel="noopener ugc nofollow" target="_blank"> deeplearning.ai </a>(可在Coursera上获得)的深度学习专业化非常出色，我会向任何对这一领域感兴趣的人强烈推荐它。在本课程中，首先学习如何在没有任何库的情况下从头开始编写神经网络代码，然后继续学习TensorFlow库的高级功能。不幸的是(在撰写本文时)所有的编码练习都是在TF &lt; 2.0中实现的。在不久的将来，TF 2+将成为标准，TF 2.x与TF1有很大的不同。大多数练习都需要完全重写代码。</p><p id="d403" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于本课程的编程练习非常出色，我决定将练习版本2.x兼容。我在下面(<a class="ae lu" href="https://drive.google.com/drive/folders/1a9A9O04ODfYJgslYbklFqR58GDuoOeYQ?usp=sharing" rel="noopener ugc nofollow" target="_blank">链接</a>)维护练习转换后的TF2.x代码。这篇文章假设读者对TF有些了解。</p><p id="dd44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博文中，我将回顾将TF1.x代码(尤其是本课程中的代码)转换成TF2+代码的基础知识。我已经在TF1=( <a class="ae lu" href="https://github.com/sol0invictus/Blog_stuff/blob/master/DL%20Coursera/TF%201.x/TensorFlow_Tutorial_v3b.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>)和TF2 =( <a class="ae lu" href="https://github.com/sol0invictus/Blog_stuff/blob/master/DL%20Coursera/TF%202.x/TensorFlow_20_version.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>)的第7周练习中实现了深度神经网络。下面我将解释两种代码的一些关键区别。其他练习的TF2+版本请访问我的Github repo : ( <a class="ae lu" href="https://drive.google.com/drive/folders/1a9A9O04ODfYJgslYbklFqR58GDuoOeYQ?usp=sharing" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><blockquote class="lv lw lx"><p id="6f15" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> 1) <em class="it"> Session()消失=进入急切执行</em> </strong></p></blockquote><p id="81bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先看看下面的简单代码片段，它计算TF 1.x中的平方误差损失，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div></figure><p id="d3b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TF2+摆脱了会话和图形构建。在TF 1.x中，你通常会定义变量之间的关系(创建一个图),然后一个会话执行路径。<strong class="la iu"><em class="ly">session . run()</em></strong>以输入为参数运行路径，并输出输出。在TF2+中，默认情况下启用快速执行。这意味着变量是自动和并发执行/计算的，而不需要会话/图形。</p><p id="c5ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">转换后的TF 2+代码为:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div></figure><p id="5a4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">区别:</strong>不需要创建会话对象或者<strong class="la iu"><em class="ly">TF . GLO abal _ variable _ initializer()</em></strong>。解释器一点击<strong class="la iu"> <em class="ly"> loss= </em> </strong>命令，就执行。</p><blockquote class="lv lw lx"><p id="fabb" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> 2)急切执行(另一个例子)</strong></p></blockquote><p id="8bdd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我将使用TF1和TF2+实现两个张量(标量)的乘法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF 1.x中的乘法</p></figure><p id="bc85" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">c在通过<strong class="la iu"> <em class="ly"> tf.session() </em> </strong>运行时，只会输出一个张量(20)。在TF2+中，缺省情况下启用急切执行，下面的代码将自动输出一个张量(20)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF 2.x中的乘法</p></figure><blockquote class="lv lw lx"><p id="ea21" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> 3)占位符不见了</strong></p></blockquote><p id="6ab7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">占位符是ehh..TF &lt;2. They are basically a container for input variables. These have been removed in TF2+. Let us first look at this example of a function that calculates the sigmoid of input variables/tensor. Here x is a placeholder variable that stores the argument of the function and then sigmoid is acted on it.</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Placeholder in TF 1.x</p></figure><p id="a271" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">In TF2+ placeholders are not required and sigmoid can be directly applied to the input variable.</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Working without placeholder in TF2.x</p></figure><blockquote class="lv lw lx"><p id="1f7b" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> 4)中的井占位变量执行梯度下降</strong></p></blockquote><p id="4834" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果使用<strong class="la iu"> <em class="ly"> tf.keras </em> </strong>建立模型，可以直接使用<strong class="la iu"> <em class="ly"> tf.train() </em> </strong>用预定义的损失函数训练你的数据集。然而，要定制任何东西，你必须从头开始实现梯度下降。让我们首先回顾TF &lt; 2中的梯度下降。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF1.x中的梯度下降</p></figure><p id="dc30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">撇开所有的<strong class="la iu"> <em class="ly">。session() </em> </strong>业务梯度下降通过<strong class="la iu"><em class="ly">TF . train . adamoptimizer(learning _ rate = learning _ rate)实现。</em> </strong>最小化(成本)功能。<strong class="la iu"> <em class="ly">最小化</em> </strong>功能尝试最小化自变量，并相应地调整参数。在TF2+最小化功能不存在，需要在低得多的水平上实现梯度下降。这给了整个过程更多的控制。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF2.x中的梯度下降</p></figure><p id="fc6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先定义一个函数(<strong class="la iu"><em class="ly">【get _ grad()</em></strong>)来计算参数的梯度。此功能使用<strong class="la iu">的<em class="ly"> tf。GradientTape() </em> </strong>用于梯度的自动微分和计算。关于它的更多细节可以在:<a class="ae lu" href="https://www.tensorflow.org/guide/eager#computing_gradients" rel="noopener ugc nofollow" target="_blank">链接找到。</a>我们使用函数应用渐变:<strong class="la iu"><em class="ly">optimizer . apply _ gradients(zip(grads，list(parameters . values())))</em></strong>。这基本上是对<strong class="la iu"> <em class="ly">的啰嗦替换。最小化</em>功能。</strong></p><blockquote class="lv lw lx"><p id="a1ab" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> 5)其他细微变化</strong></p></blockquote><p id="6bf3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了下面这些，还有很多小的改变，比如重新安排库。让我列举一些简单的例子</p><p id="1bd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">a)所有预定义的图层都被移动到:<br/><strong class="la iu"><em class="ly">TF . keras . layers . *</em></strong></p><p id="5d83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b)一些数学功能移入子类:<br/><strong class="la iu"><em class="ly">TF . math . *</em></strong></p><p id="1f58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">c)更多信息请访问:<a class="ae lu" href="https://www.tensorflow.org/guide/migrate#top_of_page" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/migrate#top_of_page</a></p><blockquote class="lv lw lx"><p id="3c10" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">底线</em> </strong></p></blockquote><p id="5bfb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文的目的是提供TF &lt;2 and TF2+ especially focusing on Coursera Deep Learning Specialization. I have converted all the exercises of the specialization into TF2+ code and have uploaded them on (<a class="ae lu" href="https://drive.google.com/drive/folders/1a9A9O04ODfYJgslYbklFqR58GDuoOeYQ?usp=sharing" rel="noopener ugc nofollow" target="_blank"> link </a>之间代码差异的快速总结。我希望这个博客是有用的</p><blockquote class="lv lw lx"><p id="45a9" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">参考文献</em> </strong></p></blockquote><ol class=""><li id="9597" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated">关于移民的TF文件:【https://www.tensorflow.org/guide/migrate#top_of_page】<a class="ae lu" href="https://www.tensorflow.org/guide/migrate#top_of_page" rel="noopener ugc nofollow" target="_blank"/></li><li id="8ddd" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">TF2 DL专精:<br/><a class="ae lu" href="https://drive.google.com/drive/folders/1a9A9O04ODfYJgslYbklFqR58GDuoOeYQ?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/drive/folders/1a 9 a9 o 04 odfyjgslybklfqr 58 gduooeyq？usp =共享</a></li></ol></div></div>    
</body>
</html>