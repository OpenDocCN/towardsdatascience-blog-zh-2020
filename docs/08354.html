<html>
<head>
<title>Softmax Activation Function Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了Softmax激活功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60?source=collection_archive---------21-----------------------#2020-06-18">https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60?source=collection_archive---------21-----------------------#2020-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d9a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并从零开始实现。</h2></div><p id="b87b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你做过深度学习，你可能会注意到两种不同类型的激活函数——一种用于隐藏层，一种用于输出层。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/d2b03117e01a755f3cbe1339143efb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9lqIc-wTlMmIG0No"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="b902" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于隐藏层的激活函数对于所有隐藏层来说几乎是相同的。不太可能看到ReLU用在第一个隐藏层上，后面是双曲正切函数——一般都是ReLU或者tanh。</p><p id="9941" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们在这里说的是输出层。这里我们需要一个函数，它接受任何值，并将它们转换成概率分布。</p><p id="f25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Softmax功能来拯救。</p><p id="adcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数对于<strong class="kk iu">分类</strong>问题非常有用，尤其是当你处理多类分类问题时，因为它会报告每个类的“置信度”。因为我们在这里处理的是概率，所以softmax函数返回的分数总和将为1。</p><p id="a636" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，预测类是列表中置信度得分最高的项目。</p><p id="7bac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将看到softmax函数是如何用数学方法表达的，然后将它翻译成Python代码是多么容易。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1843" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数学表示</h1><p id="4da9" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">根据官方<a class="ae lu" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank">维基百科页面</a>，下面是softmax函数的公式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/008fc3fa83be17a15e5479a727f128b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*D4p2CsxVe50MoJIjN0YFNw.png"/></div></figure><p id="f532" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">乍一看可能会令人望而生畏，但这是你在研究深度学习时会遇到的最简单的函数之一。</p><p id="8662" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它指出，我们需要对输出层的每个元素应用一个标准指数函数，然后通过除以所有指数的总和来归一化这些值。这样做可以确保所有取幂值的总和等于1。</p><p id="b7b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果需要，请花时间多次通读前一段，因为这对进一步理解至关重要。如果仍然有点模糊，我们准备了一个(希望)有用的图表:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi na"><img src="../Images/b3e7258c3136e5013a527f625c042e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg"/></div></div></figure><p id="c40c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是步骤:</p><ol class=""><li id="a776" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">对输出图层的每个元素取幂，并将结果相加(本例中约为181.73)</li><li id="7af8" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">取输出层的每个元素，对其取幂并除以步骤1中获得的和<em class="np">(exp(1.3)/181.37 = 3.67/181.37 = 0.02)</em></li></ol><p id="7ebb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我希望您已经了解了softmax激活函数在理论上是如何工作的，在下一节中，我们将在Numpy中从头开始实现它。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="dc7e" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">履行</h1><p id="3eba" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如果你已经理解了前一部分，这一部分将会很容易和直观。如果没有，简单的Python实现应该仍然有助于一般的理解。</p><p id="db15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们声明一个模拟神经网络输出层的数组:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="9334" class="nv md it nr b gy nw nx l ny nz">output_layer = np.array([1.3, 5.1, 2.2, 0.7, 1.1])<br/>output_layer</span><span id="38fd" class="nv md it nr b gy oa nx l ny nz"><strong class="nr iu">&gt;&gt;&gt; array([1.3, 5.1, 2.2, 0.7, 1.1])</strong></span></pre><p id="23ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">把这想成K类分类问题，其中K是5。接下来，我们需要对输出层的每个元素求幂:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="d453" class="nv md it nr b gy nw nx l ny nz">exponentiated = np.exp(output_layer)<br/>exponentiated</span><span id="6e3d" class="nv md it nr b gy oa nx l ny nz"><strong class="nr iu">&gt;&gt;&gt; array([ 3.66929667, 164.0219073 , 9.0250135 , 2.01375271,<br/> 3.00416602])</strong></span></pre><p id="d037" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们准备计算概率！我们可以使用Numpy将每个元素除以取幂和，并将结果存储在另一个数组中:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="b29c" class="nv md it nr b gy nw nx l ny nz">probabilities = exponentiated / np.sum(exponentiated)<br/>probabilities</span><span id="ce1d" class="nv md it nr b gy oa nx l ny nz"><strong class="nr iu">&gt;&gt;&gt; array([0.02019046, 0.90253769, 0.04966053, 0.01108076, 0.01653055])</strong></span></pre><p id="5a30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样-这些是从输出图层的原始值中获得的目标类概率。</p><p id="106d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之前我们提到过概率之和应该等于1，所以让我们快速验证一下这句话是否有效:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="5ad7" class="nv md it nr b gy nw nx l ny nz">probabilities.sum()</span><span id="1feb" class="nv md it nr b gy oa nx l ny nz"><strong class="nr iu">&gt;&gt;&gt; 1.0</strong></span></pre><p id="56e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们结束了。如果你明白了这一点，你就明白了softmax的激活功能。让我们在下一部分总结一下。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="576e" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">在你走之前</h1><p id="9386" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">softmax函数应该很容易理解。由于TensorFlow和PyTorch等复杂的库，我们不需要手动实现它。这并不意味着我们不应该知道他们是如何工作的。</p><p id="85a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇文章足够容易理解和理解。感谢阅读。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="26a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">喜欢这篇文章吗？成为 <a class="ae lu" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="np">中等会员</em> </a> <em class="np">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="ob oc gp gr od oe"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">medium.com</p></div></div><div class="on l"><div class="oo l op oq or on os lo oe"/></div></div></a></div></div></div>    
</body>
</html>