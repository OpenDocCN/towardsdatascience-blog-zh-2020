<html>
<head>
<title>Latent Semantic Analysis: intuition, math, implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在语义分析:直觉、数学、实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8?source=collection_archive---------4-----------------------#2020-05-10">https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8?source=collection_archive---------4-----------------------#2020-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9d84" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们如何使用无监督学习从文本中提取主题和话题</h2></div><p id="73f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TL；DR —文本数据深受高维度之苦。潜在语义分析(LSA)是一种流行的降维技术，遵循与奇异值分解相同的方法。LSA最终根据<em class="lb"> r </em> <strong class="kh ir">潜在</strong><em class="lb">T5(即<strong class="kh ir">隐藏</strong>)特征来重构文本数据，其中<em class="lb"> r </em>小于数据中的项数<em class="lb"> m </em>。我将解释<strong class="kh ir">概念上的</strong>和<strong class="kh ir">数学上的</strong>直觉上的<strong class="kh ir"> </strong>，并使用<a class="ae lc" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html" rel="noopener ugc nofollow" target="_blank"> 20个新闻组</a>数据集在<a class="ae lc" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>中运行一个基本的<strong class="kh ir">实现</strong>。</em></p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="0619" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">语言不仅仅是你面前单词的集合。当你阅读一篇文章时，你的脑海中会浮现出一些图像和概念。当你阅读许多文本时，主题开始浮现，即使它们从未被明确地表达出来。我们理解和处理语言的天生能力挑战了算法表达式(目前)。LSA是最流行的自然语言处理(NLP)技术之一，它试图用数学方法确定文本中的主题。LSA是一种无监督的学习技术，它基于两个支柱:</p><ul class=""><li id="531f" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">分布假说，即意思相近的词频繁出现在一起。JR Firth的名言“你应该从一个人交往的朋友那里知道一个词”很好地概括了这一点</li><li id="825c" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">奇异值分解(SVD——图1)是一种数学技术，我们将更深入地研究它。</li></ul><p id="6665" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，LSA是一种无人监督的学习技术——没有基础真理。潜在的概念可能存在，也可能不存在！在我们稍后将使用的数据集中，我们知道有20个新闻类别，我们可以对它们进行分类，但这只是出于说明的目的。通常情况下，我们会在非结构化、无标签的数据上使用LSA。</p><p id="5e56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像所有的机器学习概念一样，LSA可以分为3个部分:直觉、数学和代码。请随意使用目录中的链接，跳到与您最相关的部分。完整的代码可以在这个<a class="ae lc" href="https://github.com/Ioana-P/pca_and_clustering_for_edu_purposes/blob/master/newsgroups_LSA.ipynb" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到。</p><p id="9cb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">术语注释:通常，当对文本数据进行这种分解时，术语SVD和LSA(或LSI)可以互换使用。为了简单起见，从现在开始我将使用LSA。</p><p id="b5d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">本文假设对基本的自然语言处理预处理和单词矢量化有一些了解(特别是</em> <a class="ae lc" rel="noopener" target="_blank" href="/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558"> <em class="lb"> tf-idf矢量化</em> </a> <em class="lb">)。</em></p><h2 id="3ce3" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">内容:</h2><ol class=""><li id="a9da" class="lk ll iq kh b ki mr kl ms ko mt ks mu kw mv la mw lq lr ls bi translated"><a class="ae lc" href="#0c85" rel="noopener ugc nofollow">直觉</a>:用政治新闻话题解释</li><li id="4785" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la mw lq lr ls bi translated"><a class="ae lc" href="#d570" rel="noopener ugc nofollow">数学</a> : SVD作为矩阵的加权有序和<strong class="kh ir">或</strong>作为一组3个线性变换</li><li id="c8ee" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la mw lq lr ls bi translated"><a class="ae lc" href="#04db" rel="noopener ugc nofollow">代码实现</a>:python 3中的Scikit-Learn和20个新闻组数据</li><li id="55cc" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la mw lq lr ls bi translated"><a class="ae lc" href="#a131" rel="noopener ugc nofollow">参考文献</a></li></ol><h1 id="0c85" class="mx lz iq bd ma my mz na md nb nc nd mg jw ne jx mj jz nf ka mm kc ng kd mp nh bi translated">1.直觉</h1><p id="ed80" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">(<a class="ae lc" href="#3ce3" rel="noopener ugc nofollow">返回目录</a>)</p><p id="cdf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单来说:LSA把有意义的文本文件放在不同的地方重新创作，每一部分都表达了看待文本意义的不同方式。如果你把文本数据想象成一个想法，那么对于这个想法会有<em class="lb"> n </em>种不同的方式<em class="lb">看待，或者有<em class="lb"> n </em>种不同的方式<em class="lb">概念化</em>整个文本。LSA将我们的数据表简化为一个潜在的概念表。</em></p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nl"><img src="../Images/e643276b4d6d913f42f14ad59430a60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Sldip6QA_xwyyw7DI6SWw.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图1:奇异值分解的公式和矩阵维数</p></figure><p id="d2d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有一些数据表，在本例中是文本数据，其中每行是一个文档，每列代表一个术语(可以是一个单词或一组单词，如“baker's十二”或“Downing Street”)。这是表示文本数据的标准方式(在<em class="lb">文档术语矩阵</em>中，如图2所示)。表格中的数字反映了该单词在文档中的重要性。如果数字是零，那么这个单词就不会出现在文档中。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ob"><img src="../Images/7582a10fcc65a7b11a9739aa3a99c926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LuETpJGCTOaKAc8zyfjs4g.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图2:文档术语矩阵，在应用了某种矢量化之后，在我们的例子中是TF-IDF(但是单词包也可以)</p></figure><p id="8111" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同的文档将涉及不同的主题。假设所有文件都是<strong class="kh ir">政治</strong>文章，有3个主题:<strong class="kh ir">外交政策(F.P .)、选举和改革</strong>。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oc"><img src="../Images/55ac3e5a8e48fab65c4593025a417438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPHKa66FY0XBnpMM2Kx-1w.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图3:文档-主题矩阵(或者文档- <strong class="bd od">潜在的</strong>-概念，如果你喜欢的话)</p></figure><p id="b919" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设有些文章属于每一个类别，有些属于两个类别，有些属于所有三个类别。我们可以绘制一个表格，其中每行是一个不同的文档(一篇新闻文章)，每列是一个不同的主题。在单元格中，我们将使用不同的数字来表示该文档属于特定主题的程度(参见图3)。</p><p id="e0c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们从概念上将注意力转移到<strong class="kh ir">主题</strong>本身，我们应该问自己以下问题:<em class="lb">我们是否期望某些</em> <strong class="kh ir"> <em class="lb">单词</em> </strong> <em class="lb">在这些主题中更频繁地出现？</em></p><p id="5037" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们看外交政策，我们可能会看到像“中东”、“欧盟”、“大使馆”这样的术语。对于选举，可能是“选票”、“候选人”、“政党”；对于改革，我们可能会看到“法案”、“修正案”或“腐败”。因此，如果我们在不同的表中绘制这些主题和这些术语，其中的行是术语，我们会看到根据哪个主题最强烈地属于每个术语绘制的分数。自然会有在所有三个文档中出现的术语(“总理”、“议会”、“决定”)，这些术语会在所有三列中有分数，反映它们属于任一类别的程度-数字越高，其与该主题的关联越大。因此，我们的第二个表(图4)由术语和主题组成。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oe"><img src="../Images/e0228161d0636837b0dfc7f987c551c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2T1vn1LLGWbCjat4tolGg.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图4:术语-主题矩阵</p></figure><p id="1460" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，最后一个组件作为一个表来解释有点棘手。它实际上是一组数字，一个数字代表一个话题。这些数字代表什么？它们代表了每个主题<em class="lb">解释了</em>我们的数据的程度。</p><p id="7ed6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们如何“解释”这些数据？好吧，假设实际上，“改革”并不是贯穿我们文章的突出主题，大多数文章更适合“外交政策”和“选举”。因此,“改革”在这一组中会得到一个很低的数字，低于其他两个。另一种选择是，也许所有三个数字实际上都很低，我们实际上应该有四个或更多的主题——我们后来发现我们的许多文章实际上都与经济学有关！由于只坚持三个主题，我们已经剥夺了自己获得更详细和精确的数据的机会。这个数组的技术名称是“奇异值”。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi of"><img src="../Images/c853e7754d86ce3351b7c0f391becec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQY75Vct3QJJoPQlDLisKg.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图5:奇异值——我们的主题在文本中的相对重要性是什么？</p></figure><p id="fde7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是目前为止的直觉。您会注意到我们的两个表有一个共同点(文档/文章),这三个表都有一个共同点——主题，或者它们的某种表示。</p><p id="e9f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们解释一下这是一种怎样的降维技术。如果我们指定一些文档和主题，就更容易看到优点。假设我们有100篇文章和10，000个不同的术语(想想所有这些文章会有多少独特的词，从“修正”到“热心的”！).在我们最初的文档术语矩阵中，有100行和10，000列。当我们开始将数据分解为3个部分时，我们实际上可以选择主题的数量——我们可以选择10，000个不同的主题，如果我们真的认为这是合理的。然而，我们也许可以用更少的主题来表示数据，比如我们最初谈到的3个主题。这意味着在我们的文档主题表中，我们将削减大约<em class="lb"> 99，997列</em>，在我们的术语主题表中，我们将做同样的事情。我们从表中丢弃的列和行在图6中显示为散列矩形。M <em class="lb"> </em>是原始文档-术语表；<em class="lb"> U </em>是文档主题表，𝚺 (sigma)是奇异值的数组，而<em class="lb"> V-transpose </em>(上标t表示原始矩阵t已沿其对角线翻转)是文档主题表，但沿其对角线翻转(我将在数学部分解释原因)。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi og"><img src="../Images/590f774051053331f614b29d95205306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5najHCdleqnOpvZgJHB1kA.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图6 —我们丢弃了哪些散列</p></figure><p id="9b2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至于表示主题重要性的一组数字，从一组10，000个数字中，每个数字越来越小，因为它对应于一个不太重要的主题，我们削减到只有3个数字，用于我们剩余的3个主题。这就是为什么LSA的Python实现被称为<em class="lb">截断的</em> SVD:顺便说一下，我们截掉了表的一部分，但是我们稍后会看到代码。同样值得注意的是，我们事先不知道3个主题是什么，我们只是假设会有3个，一旦我们得到了组件，我们就可以探索它们，看看术语是什么。</p><p id="5e61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，我们不只是想回到原始数据集:我们现在有3个可以使用的低维组件。在代码和数学部分，我们将讨论我们实际上前进了哪一步。简而言之，一旦我们截断了表(矩阵)，我们将得到的产品是文档主题表(<em class="lb"> U </em> ) <em class="lb">乘以</em>奇异值(𝚺).这可以解释为文档(我们所有的新闻文章)以及它们属于每个主题的程度，然后根据每个主题的相对重要性进行<strong class="kh ir">加权</strong>。你会注意到，在这种情况下，这个最终表格中遗漏了一些东西，即<em class="lb">单词。</em>是的，我们已经超越了文字，我们丢弃了它们，但保留了<em class="lb">主题</em>，这是一种表达我们文本的更简洁的方式。</p><h1 id="7ed5" class="mx lz iq bd ma my mz na md nb nc nd mg jw ne jx mj jz nf ka mm kc ng kd mp nh bi translated">2.数学</h1><p id="5066" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">(<a class="ae lc" href="#3ce3" rel="noopener ugc nofollow">返回目录</a>)</p><p id="0793" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于数学，我将对SVD进行两种不同的解释:首先是可以用于实方阵M的一般几何分解，其次是与我们的例子更相关的可分离模型分解。SVD也用于基于模型的推荐系统。它非常类似于主成分分析(PCA)，但是它在稀疏数据上比PCA操作得更好(并且文本数据几乎总是稀疏的)。PCA对数据集的<em class="lb">相关性</em>矩阵进行分解，而SVD/LSA直接对数据集进行分解。</p><p id="4283" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将<strong class="kh ir">把这个矩阵分解</strong>成组成矩阵。我所说的因式分解本质上与我们用一个数来表示它的因子是一样的，当这些因子相乘时，我们就得到了原始数，例如A = B * C * D。</p><p id="1662" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也是为什么它被称为奇异值<strong class="kh ir">分解</strong>——我们将<em class="lb">分解</em>成它的组成部分。</p><h2 id="d724" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">一般几何分解</h2><p id="d2ff" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">我们在原始矩阵中无法获得的额外维度，即<em class="lb"> r </em>维度，是<em class="lb">潜在概念</em>的数量。一般来说，我们试图把我们的矩阵表示成其他矩阵，它们的一个轴是这组分量。您还会注意到，基于维度，3个矩阵的乘法(当V被转置时)将使我们回到原始矩阵的形状，维度<em class="lb"> r </em>实际上消失了。</p><p id="6f37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在理解数学的过程中，重要的不是确定u、v和𝚺中每个数字的代数算法，而是这些乘积的数学性质以及它们之间的关系。</p><p id="b1e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，重要的是首先考虑矩阵实际上是什么，它可以被认为是向量空间的变换。在图7的左上角，我们有两个垂直的向量。如果我们只有两个变量开始，那么特征空间(我们正在查看的数据)可以在这个空间的任何地方绘制，这个空间由这两个<strong class="kh ir">基</strong>向量描述。现在移到我们图的右边，矩阵M被应用到这个向量空间，这就把它转换成新的，在右上角的转换空间。在下图中，M的几何效应被称为“剪切”向量空间；两个向量<em class="lb"> 𝝈1 </em>和<em class="lb"> 𝝈2 </em>实际上是我们在这个空间中绘制的奇异值。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oh"><img src="../Images/a60869fe4c0f183e2c62a1ab6985a483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGflVq-hpWnmUto3thkR6g.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图7:来源:维基百科；<a class="ae lc" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a>；<a class="ae lc" href="https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg#filelinks;" rel="noopener ugc nofollow" target="_blank">链接</a>；作者:格奥尔格-约翰</p></figure><p id="f6f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，就像你们在学校可能记得的点的几何变换一样，我们可以把这个变换<em class="lb"> M </em>看作三个独立的变换:</p><ol class=""><li id="2638" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la mw lq lr ls bi translated"><em class="lb"> V*引起的旋转(或反射)。</em>注意<em class="lb">V * = V-转置</em>由于V是实酉矩阵，所以V的复共轭与其转置相同。在矢量项中，V或<em class="lb"> V* </em>的变换保持基矢量的长度不变；</li><li id="4814" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la mw lq lr ls bi translated">𝚺具有沿其奇异值拉伸或压缩所有坐标点的效果。想象一下我们在左下角的圆盘，我们朝着<em class="lb"> 𝝈2 </em>的方向垂直向下挤压它，然后沿着<em class="lb"> 𝝈1 </em>的方向水平拉伸它。这两个奇异值现在可以被描绘成椭圆的长半轴和短半轴。你当然可以将此推广到<em class="lb"> n </em>维度。</li><li id="20e2" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la mw lq lr ls bi translated">最后，应用<em class="lb"> U </em>旋转(或反射)我们的特征空间。我们已经得到了与直接从<em class="lb"> M </em>转换相同的输出。</li></ol><p id="c9ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还推荐关于SVD 的优秀的<a class="ae lc" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank"> Wikipedia条目，因为它对这个过程有特别好的解释和GIF。</a></p><p id="3200" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，换句话说，其中<em class="lb"> x </em>是任意列向量:</p><figure class="nm nn no np gt nq"><div class="bz fp l di"><div class="oi oj l"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">M对x的变换与右边矩阵对x的三次变换相同</p></figure><p id="0017" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵<em class="lb"> U </em>和<em class="lb"> V* </em>的性质之一是它们是酉矩阵，因此我们可以说这两个矩阵的列形成了两组正交基向量。换句话说，你可以从U得到的列向量会形成它们自己的坐标空间，这样如果有两列<em class="lb"> U1 </em>和<em class="lb"> U2，</em>你就可以写出空间的所有坐标，作为<em class="lb"> U1 </em>和<em class="lb"> U2 </em>的组合。这同样适用于<em class="lb"> V </em>、<em class="lb"> V1 </em>和<em class="lb">V2</em>的列，并且这将推广到<em class="lb">n</em>-尺寸(你将有<em class="lb">n</em>-列)。</p><h1 id="13f9" class="mx lz iq bd ma my mz na md nb nc nd mg jw ne jx mj jz nf ka mm kc ng kd mp nh bi translated">可分离模型分解</h1><p id="58a8" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">如果我们想象我们的矩阵M可以被分解成可分离矩阵的加权和，我们可以得到对PCA的相同理解，如下所示。</p><figure class="nm nn no np gt nq"><div class="bz fp l di"><div class="ok oj l"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">将我们的数据M分解成可分离矩阵的加权和，<em class="ol"> Ai </em></p></figure><p id="9d78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵𝐴𝑖据说是可分的，因为它们可以分解成两个向量的外积，用奇异值𝝈<em class="lb">I</em>加权。计算形状为(<em class="lb"> m，</em>)和(<em class="lb"> n，</em>)的两个向量的外积将给出形状为(m，n)的矩阵。换句话说，计算两个向量中任意两个数的每个可能乘积，并放入新矩阵中。奇异值不仅对和进行加权，而且对和进行排序，因为值是按降序排列的，所以第一个奇异值总是最高的一个。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi om"><img src="../Images/051137c7b5f61d7b12253624e7f5484d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8WG84Sg8zkOZl6olTK8ng.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图8:我们的可分离矩阵。请注意，≅符号代表这样一个事实，即只有3个乘积的分解集<strong class="bd od">近似于</strong>我们的原始矩阵，它并不完全等于它。</p></figure><p id="08c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图8中，你可以看到如何将它可视化。以前我们有高的<em class="lb"> U </em>，正方形<em class="lb">σ</em>和长的𝑉- <em class="lb">转置矩阵</em>。现在你可以想象从<em class="lb"> U </em>中取出第一个垂直切片，用第一个奇异值对其所有值进行加权(相乘)，然后通过与𝑉 <em class="lb">的第一个水平切片进行外积-转置</em>，用这些切片的维度创建一个新矩阵。然后我们把这些乘积加在一起，得到M。或者，如果我们不做完全求和，而只是部分完成，我们得到的是截断的版本。</p><p id="4886" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，对于我们的数据:</p><ul class=""><li id="4a68" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">其中<em class="lb"> M </em>是我们原来的(<em class="lb"> m，n </em>)数据矩阵——M行，n列；<em class="lb"> m个文档，n个术语</em></li><li id="ea5b" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">u是一个(<em class="lb"> m，r </em>)矩阵— <em class="lb"> m个文档和r个概念</em></li><li id="a2d4" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">σ是一个<em class="lb">对角线</em> ( <em class="lb"> r，r </em>)矩阵——除了对角线上的值以外，所有值都为零。(但是非零值代表什么呢？</li><li id="63f3" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">v是一个(<em class="lb"> n，r </em>)矩阵— <em class="lb"> n项，r个概念</em></li></ul><p id="2a46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">𝚺的值代表了每个潜在概念在多大程度上解释了我们数据中的差异。当这些乘以该潜在概念的<em class="lb"> u </em>列向量时，它将有效地加权该向量。</p><p id="1abc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们将它分解成5个组件，它看起来会像这样:</p><figure class="nm nn no np gt nq"><div class="bz fp l di"><div class="ok oj l"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">加权文档概念向量和术语概念向量的外积之和</p></figure><p id="2eae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里原本会有<em class="lb"> r </em>个<em class="lb"> u </em>个向量；5个奇异值和n个𝑣<em class="lb">-转置</em>向量。</p><h1 id="04db" class="mx lz iq bd ma my mz na md nb nc nd mg jw ne jx mj jz nf ka mm kc ng kd mp nh bi translated">3.代码实现</h1><p id="5f5d" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">(<a class="ae lc" href="#3ce3" rel="noopener ugc nofollow">返回内容</a>)</p><p id="92f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后一节中，我们将看到如何使用Scikit-Learn实现基本的LSA。</p><h2 id="c713" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">提取、转换和加载我们的文本数据</h2><pre class="nm nn no np gt on oo op oq aw or bi"><span id="ae6b" class="ly lz iq oo b gy os ot l ou ov">from sklearn.datasets import fetch_20newsgroups<br/>X_train, y_train = fetch_20newsgroups(subset='train', return_X_y=True)<br/>X_test, y_test = fetch_20newsgroups(subset='test', return_X_y=True)</span></pre><h2 id="671c" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">清洗和预处理</h2><p id="4487" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">文本数据的清理通常与数字数据的清理截然不同。您经常会发现自己已经准备好了矢量器、模型，并准备好进行网格搜索，然后提取特征，却发现cluster <em class="lb"> x </em>中最重要的特征是字符串“_ _”…因此您需要返回并做更多的清理工作。下面的代码块是我意识到需要从数据集中删除网站URL、号码和电子邮件的结果。</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="038b" class="ly lz iq oo b gy os ot l ou ov">from nltk.corpus import stopwords<br/>from nltk.tokenize import RegexpTokenizer<br/>import re<br/>tokenizer = RegexpTokenizer(r'\b\w{3,}\b')<br/>stop_words = list(set(stopwords.words("english")))<br/>stop_words += list(string.punctuation)<br/>stop_words += ['__', '___']</span><span id="5839" class="ly lz iq oo b gy ow ot l ou ov"># Uncomment and run the 3 lines below if you haven't got these packages already<br/># nltk.download('stopwords')<br/># nltk.download('punkt')<br/># nltk.download('wordnet')</span><span id="2c8b" class="ly lz iq oo b gy ow ot l ou ov">def rmv_emails_websites(string):<br/>    """Function removes emails, websites and numbers"""</span><span id="a95b" class="ly lz iq oo b gy ow ot l ou ov">    new_str = re.sub(r"\S+@\S+", '', string)<br/>    new_str = re.sub(r"\S+.co\S+", '', new_str)<br/>    new_str = re.sub(r"\S+.ed\S+", '', new_str)<br/>    new_str = re.sub(r"[0-9]+", '', new_str)<br/>    return new_str</span><span id="d977" class="ly lz iq oo b gy ow ot l ou ov">X_train = list(map(rmv_emails_websites, X_train))<br/>X_test  = list(map(rmv_emails_websites, X_test))</span></pre><h2 id="0d0c" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">文本数据的符号化和矢量化</h2><p id="5f22" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">我们的模型处理数字，而不是字符串！因此，我们将文本标记化(将所有文档转化为更小的观察实体——在本例中为单词),然后使用Sklearn的TF-IDF矢量器将它们转化为数字。我建议任何转换过程(尤其是那些需要时间运行的过程)都在数据的前10行进行，并检查结果:它们是您期望看到的吗？数据框的形状是你所希望的吗？一旦你对你的代码有信心了，就输入整个语料库。</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="1671" class="ly lz iq oo b gy os ot l ou ov">tfidf = TfidfVectorizer(lowercase=True, <br/>                        stop_words=stop_words, <br/>                        tokenizer=tokenizer.tokenize, <br/>                        max_df=0.2,<br/>                        min_df=0.02<br/>                       )<br/>tfidf_train_sparse = tfidf.fit_transform(X_train)<br/>tfidf_train_df = pd.DataFrame(tfidf_train_sparse.toarray(), <br/>                        columns=tfidf.get_feature_names())<br/>tfidf_train_df.head()</span></pre><p id="3e14" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将为您提供矢量化的文本数据——文档术语矩阵。对测试集也重复上面的步骤，但是<strong class="kh ir">只</strong>使用transform，<strong class="kh ir">不</strong> fit_transform。</p><h2 id="5aa9" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">探索性数据分析LSA</h2><p id="d83c" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">仅仅为了我们的分解数据的可视化和EDA的目的，让我们将我们的LSA对象(在Sklearn中是<a class="ae lc" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank"> TruncatedSVD类</a>)适合我们的训练数据，并且只指定20个组件。</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="9672" class="ly lz iq oo b gy os ot l ou ov">from sklearn.decomposition import TruncatedSVD</span><span id="cea5" class="ly lz iq oo b gy ow ot l ou ov">lsa_obj = TruncatedSVD(n_components=20, n_iter=100, random_state=42)</span><span id="52da" class="ly lz iq oo b gy ow ot l ou ov">tfidf_lsa_data = lsa_obj.fit_transform(tfidf_train_df)<br/>Sigma = lsa_obj.singular_values_<br/>V_T = lsa_obj.components_.T</span></pre><p id="6c02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们想象奇异值——下面的柱状图是否显示了我们对它们的预期？</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="969f" class="ly lz iq oo b gy os ot l ou ov">sns.barplot(x=list(range(len(Sigma))), y = Sigma)</span></pre><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/fe80888228fe211843f7eb65f501ce07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*U6GCUrfJ1hfOwI7fBkzwNw.jpeg"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图9 —我们的奇异值，代表每个潜在概念<em class="ol">在多大程度上解释了数据</em>中的差异</p></figure><p id="1742" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们通过术语-主题矩阵，<em class="lb">V-trans pose来探索我们减少的数据。</em> TruncatedSVD将把它作为形状的numpy数组(num_documents，num_components)返回，所以我们将把它变成一个Pandas dataframe以便于操作。</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="9090" class="ly lz iq oo b gy os ot l ou ov">term_topic_matrix = pd.DataFrame(data=lsa_term_topic, <br/>                                 index = eda_train.columns, <br/>                                 columns = [f'Latent_concept_{r}' for r in range(0,V_T.shape[1])])</span></pre><p id="c5a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们将术语-主题矩阵分割成Pandas系列(单列数据框)，按值排序并绘制它们。下面的代码为我们的第二个潜在组件绘制了这个图(回想一下，在python中我们从0开始计数),并返回图10中的图:</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="6727" class="ly lz iq oo b gy os ot l ou ov">data = term_topic_matrix[f'Latent_concept_1']<br/>data = data.sort_values(ascending=False)<br/>top_10 = data[:10]<br/>plt.title('Top terms along the axis of Latent concept 1')<br/>fig = sns.barplot(x= top_10.values, y=top_10.index)</span></pre><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c9757f67f125fb927768376ae8c72019.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*eBXrjYQF7Vao3pLSLGEHiA.jpeg"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图10:尽管看起来很嘈杂，但这里至少有三个术语有一个很强的主题</p></figure><p id="0e58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些词在我们的第二个潜在成分中排名很高。这个轴的另一端的单词呢(见图11)？</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/037701c2e7bad9faa3ccd6f45e61c93c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*2E2kZjCp4hUk-mVTrKj_-Q.jpeg"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图11:在这个时候，作者意识到了旅鼠/词干是多么有用</p></figure><p id="ccf8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以自己决定这种语义分歧意味着什么。添加更多的预处理步骤将有助于我们摆脱像“说”和“说”这样的词产生的噪音，但我们现在将继续努力。让我们为第六个潜在概念再做一对想象(图12和13)。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e8f56af14499217760508e973ee1ac5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*sH66WI7jpF5eTiaLhUylgQ.jpeg"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图12</p></figure><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/16873f7a2fbcbcc141ed8758fa1a22ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*J8wVB1n2hcrREesFjhxu_Q.jpeg"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图13:我们再一次看到技术术语在这些数据中非常突出</p></figure><p id="47e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一点上，由我们来从这些情节中推断出一些意义。概念5的轴的负端似乎与技术和科学主题有很强的关联(“空间”、“科学”、“计算机”)，但正端也是如此，尽管更侧重于计算机相关的术语(“硬盘”、“驱动器”、“系统”)。</p><p id="37f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在澄清一下，确定正确的组件数量需要调优，所以我没有将参数设置为20，而是将其改为100。您可能认为这仍然是一个很大的维数，但是我们最初的维数是220(这是对我们最小文档频率的限制！)，所以我们减少了相当大的一部分数据。我将在另一篇文章中探讨如何选择奇异值的最佳数量。现在，我们将继续我们所拥有的。</p><h2 id="af8d" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">在我们的建模任务中使用我们的潜在组件</h2><p id="9cec" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">虽然LSA是一种无监督的技术，通常用于在未标记的数据中发现模式，但我们在这里使用它来降低标记数据的维度，然后再将其输入模型。我们将比较LSA数据和标准TF-IDF数据的准确性，以衡量LSA从原始数据集中获取了多少有用信息。我们现在有一个形状的训练数据集(11314，100)。文档的数量保持不变，我们已经创建了100个潜在的概念。现在，让我们在这一点上和我们的标准TF-IDF数据上运行一个模型。下面的实现的目的不是得到一个好的模型，而是比较两个非常不同的数据集。我通过GridSearchCV加入了基本的交叉验证，并对tolerance超参数进行了少量的调整。如果你这样做是为了建立一个实际的模型，你会比下面写的走得更远。这只是为了帮助您进行基本的实现:</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="4074" class="ly lz iq oo b gy os ot l ou ov">logreg_lsa = LogisticRegression()<br/>logreg     = LogisticRegression()<br/>logreg_param_grid = [{'penalty':['l1', 'l2']},<br/>                 {'tol':[0.0001, 0.0005, 0.001]}]</span><span id="c15a" class="ly lz iq oo b gy ow ot l ou ov">grid_lsa_log = GridSearchCV(estimator=logreg_lsa,<br/>                        param_grid=logreg_param_grid, <br/>                        scoring='accuracy', cv=5,<br/>                        n_jobs=-1)</span><span id="9dc8" class="ly lz iq oo b gy ow ot l ou ov">grid_log = GridSearchCV(estimator=logreg,<br/>                        param_grid=logreg_param_grid, <br/>                        scoring='accuracy', cv=5,<br/>                        n_jobs=-1)</span><span id="c927" class="ly lz iq oo b gy ow ot l ou ov">best_lsa_logreg = grid_lsa_log.fit(tfidf_lsa_data, y_train).best_estimator_<br/>best_reg_logreg = grid_log.fit(tfidf_train_df, y_train).best_estimator_</span><span id="0e3f" class="ly lz iq oo b gy ow ot l ou ov">print("Accuracy of Logistic Regression on LSA train data is :", best_lsa_logreg.score(tfidf_lsa_data, y_train))<br/>print("Accuracy of Logistic Regression with standard train data is :", best_reg_logreg.score(tfidf_train_df, y_train))</span></pre><p id="8555" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它返回:</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="7d00" class="ly lz iq oo b gy os ot l ou ov">Accuracy of Logistic Regression on LSA train data is : 0.45<br/>Accuracy of Logistic Regression with standard train data is : 0.52</span></pre><p id="9c8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">性能的下降是显著的，但是您可以将这一点纳入优化流程，并调整潜在组件的数量。这在我们的测试数据(7532个文档)上表现如何呢？</p><pre class="nm nn no np gt on oo op oq aw or bi"><span id="17c6" class="ly lz iq oo b gy os ot l ou ov">Accuracy of Logistic Regression on LSA test data is : 0.35<br/>Accuracy of Logistic Regression on standard test data is : 0.37</span></pre><p id="bf86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两者的精确度都大大下降了，但是请注意模型之间的差距有多小！我们的LSA模型能够从我们的测试数据中获取与我们的标准模型一样多的信息，而尺寸却不到一半！由于这是一个多标签分类，最好用混淆矩阵来显示(图14)。当你考虑给定20个新闻类别的随机分类概率时，我们的结果看起来明显更好。如果你不熟悉混淆矩阵，作为一个经验法则，我们想最大化对角线上的数字，最小化其他地方的数字。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oz"><img src="../Images/6f650174c40feb327c5e0c52bfcca7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I24_6q3dvnKNuLSw2Sth0Q.jpeg"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图14— <a class="ae lc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">我们测试数据的混淆矩阵</a>(7532个文档)。y轴代表实际新闻类别，x轴代表预测新闻类别。对角线值是所有正确分类的文档。</p></figure><p id="5268" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就结束了我们在Scikit-Learn中对LSA的实现。我们已经讨论了这项技术的直觉、数学和编码。</p><p id="9ab1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢这篇文章，并会感谢任何数量的掌声。请在评论中留下任何反馈(积极的或建设性的)，尤其是关于数学部分，因为我发现这是最难表达的。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="a131" class="mx lz iq bd ma my pa na md nb pb nd mg jw pc jx mj jz pd ka mm kc pe kd mp nh bi translated">4.参考</h1><p id="a0be" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">(<a class="ae lc" href="http://3ce3" rel="noopener ugc nofollow" target="_blank">返回目录</a>)</p><h2 id="2ff9" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">参考资料:</h2><p id="1479" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">[1] L. Hobson，H. Cole，H. Hapke，<a class="ae lc" href="https://www.manning.com/books/natural-language-processing-in-action" rel="noopener ugc nofollow" target="_blank">自然语言处理在行动</a> (2019)，<a class="ae lc" href="https://www.manning.com/books/natural-language-processing-in-action" rel="noopener ugc nofollow" target="_blank">https://www . manning . com/books/Natural-Language-Processing-in-Action</a></p><p id="6d16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Pedregosa <em class="lb">等著，</em><a class="ae lc" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:Python中的机器学习</a> (2011)，JMLR 12，第2825–2830页。</p><p id="e55e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae lc" href="https://towardsdatascience.com/@yassine.hamdaoui?source=post_page-----6c2b61b78558----------------------" rel="noopener" target="_blank">哈姆达维</a> Y，<a class="ae lc" rel="noopener" target="_blank" href="/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558"> TF(词频)-IDF(逆文档频)从python中的无到有</a> (2019)，走向数据科学</p><p id="5a0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]维基贡献者，<a class="ae lc" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a>，<a class="ae lc" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Singular_value_decomposition</a></p></div></div>    
</body>
</html>