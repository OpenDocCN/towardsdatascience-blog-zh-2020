<html>
<head>
<title>Illustrated Guide to Siamese Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">暹罗网络图解指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrated-guide-to-siamese-network-3939da1b0c9d?source=collection_archive---------25-----------------------#2020-05-25">https://towardsdatascience.com/illustrated-guide-to-siamese-network-3939da1b0c9d?source=collection_archive---------25-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8aa8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用三重损失进行一次性学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e29e976d5f08abb8a4a4b5db3980be13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZjS1vAenL0FwWqRWCU85Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马尔科·祖彭在<a class="ae kv" href="https://unsplash.com/s/photos/signature?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c75a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建立一个准确的机器学习模型通常需要大量的训练数据，这仍然可能导致训练数据过度拟合，或者只适用于有限数量的训练数据类别。但是，如果一台机器能够“<strong class="ky ir">学会学习</strong>，例如，如果我们向我们的机器展示一只鹦鹉的单一图像，它可以通过理解它与作为参考展示的鹦鹉的图像如何相似来准确地识别另一只鹦鹉的图像。这个设计一个可以“学会学习”的机器的问题叫做<strong class="ky ir">元学习</strong>。</p><p id="abf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将关注一个著名的元学习模型——暹罗网络。如何运作，如何实施？</p><p id="73a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Siamese，顾名思义，来自“Siamese Twins”，其中我们使用两个或更多的网络(这里，在图像的情况下是CNN)，该网络使用共享的权重来学习图像之间的相似性和不相似性。网络输出n维嵌入，其中每个方向代表图像的一些视觉模式。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="d431" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">了解嵌入</strong></p><p id="7c8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设，我们使用n维空间来映射我们的图像，其中每个维度对应于特定特征/或模式的值。每个维度描述了输入图像的独特视觉特征。查看Jay alam mar<a class="ae kv" href="https://twitter.com/JayAlammar" rel="noopener ugc nofollow" target="_blank">的这篇令人惊叹的</a><a class="ae kv" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">文章</a>以获得更好的嵌入直觉<strong class="ky ir">。</strong>例如，在不同动物图像的情况下，输出嵌入可能如下所示(颜色强度表示其值在0到1之间)—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/b5fb87f2233e289f491adb5aaa88714a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHcP9zkUhB9se8hzRo6GAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="060d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">狗的前两个图像输出相似的嵌入，而猫和美洲驼的第三和第四输出嵌入与狗非常不同，因为它们具有非常不同的视觉特征包。</p><p id="637d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将使用<a class="ae kv" href="https://github.com/brendenlake/omniglot" rel="noopener ugc nofollow" target="_blank"> Omniglot数据集</a>。该数据集包含世界各地不同语言中使用的各种字母的字符图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/49f15c2e11dc730424dccbfc00fa35db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZT6J0i_Nt691-Zcrlm0IRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Ominglot数据集</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="868a" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">工作</h1><h2 id="f4ec" class="mt mc iq bd md mu mv dn mh mw mx dp ml lf my mz mn lj na nb mp ln nc nd mr ne bi translated">美国有线新闻网；卷积神经网络</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/8b33a291e918190723855296c248f37f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zSG0cSQDkpV5fUPH-4ddA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN架构的灵感来自<a class="ae kv" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a></p></figure><p id="503a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的CNN输出一个期望嵌入大小的一维数组。我们可以看到最后一层执行<a class="ae kv" href="https://stats.stackexchange.com/a/331967/283107" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> L2归一化</em> </a>，这将归一化输出向量并将其映射到半径为1的n维超球的<em class="ng">表面。这样做是为了确保可以通过计算两个嵌入之间的距离来比较图像之间的相似性值，因为所有的嵌入都将位于将给出更好结果的表面上。我们的模型有三个这样的CNN，所有的重量都一样。这将帮助我们的模型学习每个样本中的一个相似性和一个不相似性。每个样本将包含三个组成<em class="ng">锚</em>、<em class="ng">阳性</em>和<em class="ng">阴性</em>的样本图像。稍后会有更多的介绍。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/d53a74fa98b43a69172734c7221d33c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KZbSEPPcij2_XQWp9cr8w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终连体网络架构</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="b944" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">损失函数</h1><p id="54bc" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">现在是构建这个网络最重要的部分。在这里，我们将编写它如何比较输出嵌入的定义，并理解它们之间的相似性/不相似性，从而完成<em class="ng">元学习</em>的任务。</p><p id="d0a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们使用之前讨论的<em class="ng"> L2归一化</em>将输出嵌入映射到表面上，我们可以使用L2距离或余弦相似度。使用三重损失将允许我们的模型映射两个彼此接近的相似图像，并且远离不相似的样本图像。这种方法通过供给三个一组来实现，包括:</p><p id="79fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1。<em class="ng">锚点图像</em> </strong> —这是一个样本图像。</p><p id="14a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。<em class="ng">正面形象</em></strong>——这只是主播形象的另一种变体。这有助于模型学习两个图像之间的相似性。</p><p id="f68f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。<em class="ng">负像</em> </strong> —这是与上面两幅相似图像不同的图像。这有助于我们的模型学习锚图像的不同之处。</p><p id="8d75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了增加相似和不相似输出向量之间的距离，并进一步映射彼此接近的相似图像，我们引入了另一个术语，称为<em class="ng">边距</em>。这增加了相似和不相似向量之间的分离，也消除了任何<a class="ae kv" href="https://youtu.be/d2XB5-tuCWU?t=138" rel="noopener ugc nofollow" target="_blank">平凡解</a>的输出。由于人类很难想象一个嵌入映射到N维球体上，从而理解这种损失是如何工作的，我做了下面的渲染来建立对N=3(我们非常熟悉的3D)这种东西如何工作的直觉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/41d86a58144694dcf97ca98f031a0492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_-buv-mlXXke7UTLiqFyQ.png"/></div></div></figure><p id="70b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此图描述了我们的模型在训练后的输出。标记为<em class="ng">绿色</em>点的相似图像相互靠近映射，标记为<em class="ng">红色</em>映射的不相似图像相距较远，边距大小的最小距离显示为<em class="ng">黄色</em>。在训练后的理想情况下，不应在锚图像的黄色区域中映射任何点。然而，由于表面上的空间非常有限，点往往会有重叠区域。稍后将对此进行更多讨论。</p><p id="d739" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的模型使用了在<a class="ae kv" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中使用的0.2的边距大小。</p><p id="57d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种相似性/不相似性由使用L2距离和余弦距离的两个向量之间的距离来定义。</p><p id="d735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们的损失定义如下—</p><blockquote class="nn"><p id="d030" class="no np iq bd nq nr ns nt nu nv nw lr dk translated">loss(a，p，n) = max(0，d(a，p) -d(a，n)+余量)</p></blockquote><p id="6d61" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated"><strong class="ky ir"> <em class="ng">余弦三重损失</em> </strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="2f0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ng"> L2三重损失</em> </strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><h1 id="3a6d" class="mb mc iq bd md me oe mg mh mi of mk ml jw og jx mn jz oh ka mp kc oi kd mr ms bi translated">履行</h1><p id="d9ed" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">实现这个网络的棘手部分是定义损失函数。我们通过对所需批量的三元组进行采样来生成输入数据，并定义网络和我们的损失函数，然后进行训练！！！完整的实现可以在<a class="ae kv" href="https://github.com/pranjalg2308/siamese_triplet_loss" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ng">这里找到</em> </strong> </a>。</p><h1 id="a8d2" class="mb mc iq bd md me oe mg mh mi of mk ml jw og jx mn jz oh ka mp kc oi kd mr ms bi translated">结果和观察</h1><p id="14ea" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">在用不同的超参数和损失函数训练网络，并在n路一次分类(即网络从未见过的不同类别的“n”个单个图像)上测试网络准确度之后。可以观察到—</p><ol class=""><li id="115f" class="oj ok iq ky b kz la lc ld lf ol lj om ln on lr oo op oq or bi translated">对于较小的嵌入，余弦损失往往表现良好。</li><li id="af68" class="oj ok iq ky b kz os lc ot lf ou lj ov ln ow lr oo op oq or bi translated">增加嵌入的大小可以提高精确度，因为更多的维度现在可以表示图像的更多独特特征。</li><li id="8e71" class="oj ok iq ky b kz os lc ot lf ou lj ov ln ow lr oo op oq or bi translated">随着n向一次发射中的“n”增加，模型精度略微降低，因为网络难以在超球体上映射更多数量的相距较远的嵌入。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/2c616c669dd98f9c6009e67187e7e468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPo81HxVzH8s0AB1IKiopw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同大小的输出向量之间的精确度比较</p></figure><p id="cf68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">后续的<a class="ae kv" rel="noopener" target="_blank" href="/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d">文章</a>可以阅读，使用对比损失使暹罗网络仅学习相似性，使用一个角色的两个不同图像。毫无疑问，它确实表现不佳，往往在训练中过度适应，而三连音缺失根本不会过度适应。基于对比损失的连体网络的准确性随着nway一次性分类的增加而急剧下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/a5ba54ba1b17925fb75d7cd821b161ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSqZ9tQ6P3NZrdh2ASmSKA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同大小输出向量下对比损失和三重损失的准确性比较</p></figure><h1 id="f790" class="mb mc iq bd md me oe mg mh mi of mk ml jw og jx mn jz oh ka mp kc oi kd mr ms bi translated">应用</h1><p id="7709" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">这个网络用来比对指纹，<a class="ae kv" href="https://www.alliance.edu.in/aicaam-conference-proceedings/Papers/Siamese-Triple-Ranking-Convolution-Network.pdf" rel="noopener ugc nofollow" target="_blank">检测签名伪造</a>，<a class="ae kv" href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf" rel="noopener ugc nofollow" target="_blank">人脸识别</a>。这可以应用于任何相似和相异分数有用的地方。</p><h1 id="3dd9" class="mb mc iq bd md me oe mg mh mi of mk ml jw og jx mn jz oh ka mp kc oi kd mr ms bi translated">参考</h1><p id="e3bc" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">[1]https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf<a class="ae kv" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="a600" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://en.wikipedia.org/wiki/Siamese_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Siamese_neural_network</a></p><p id="16ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-word2vec/</a></p><p id="9640" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]<a class="ae kv" rel="noopener" target="_blank" href="/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d">https://towards data science . com/one-shot-learning-with-siamese-networks-using-keras-17 f 34 e 75 bb 3d</a></p><p id="690b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]<a class="ae kv" href="https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/content-based-image-retrieval-with-siamese-networks</a></p></div></div>    
</body>
</html>