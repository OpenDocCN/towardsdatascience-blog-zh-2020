<html>
<head>
<title>Bias-Variance tradeoff in Machine Learning models: A practical example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型中的偏差-方差权衡:一个实例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d?source=collection_archive---------20-----------------------#2020-06-17">https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d?source=collection_archive---------20-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="15c5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解模型误差以及如何改善它。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2b643751afc09aee2a061ae428577665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1mL0X5jv9JNPAXvHdYbfA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布雷特·乔丹在<a class="ae kv" href="/s/photos/scale?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6436" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在监督机器学习中，目标是建立一个高性能的模型，该模型善于预测手头问题的目标，并且以低偏差和低方差来进行预测。</p><p id="2324" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果你减少偏差，你最终会增加方差，反之亦然。这就是<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">偏差-方差权衡</a>发挥作用的地方。</p><p id="298f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将研究在机器学习模型的上下文中偏差和方差意味着什么，以及您可以做些什么来最小化它们。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="c9d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了建立一个有监督的机器学习模型，你需要一个看起来有点像这样的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/d3daa549ba5504e6bdfb6b9551718390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*vCEoN6G30Bsau43Sa9MEog.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">监督学习中使用的数据集结构。</p></figure><p id="715e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一系列的数据记录，每一个都有几个特征和一个目标，就是你要学会预测的东西。但是在开始构建模型之前，您需要将数据集分成两个不同的子集:</p><ul class=""><li id="6133" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">训练集</li><li id="eda1" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">测试设备</li></ul><p id="5cd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您通常会随机选择 20%的数据记录，并将其作为测试集，剩下 80%的数据集用于训练模型。这通常被称为 80/20 分割，但这只是一个经验法则。</p><h1 id="cfb2" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">对训练和测试集的需求</h1><p id="51d5" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">训练集和测试集有不同的目的。</p><p id="88ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练集教导模型如何预测目标值。至于测试集，顾名思义，它用于测试学习的质量，如果模型擅长预测学习过程中使用的数据之外的数据。</p><blockquote class="nl"><p id="e138" class="nm nn iq bd no np nq nr ns nt nu lr dk translated"><em class="nv">通过测试集，您将看到该模型是否能将其预测推广到训练数据之外。</em></p></blockquote><p id="8334" class="pw-post-body-paragraph kw kx iq ky b kz nw jr lb lc nx ju le lf ny lh li lj nz ll lm ln oa lp lq lr ij bi translated">我们可以用以下方法来衡量这个过程中两个阶段(学习和预测)的质量:</p><ul class=""><li id="1a53" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">训练错误，</li><li id="70d8" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">测试误差，也称为泛化误差。</li></ul><p id="7274" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好的模型具有低的训练误差。但是你必须小心，不要把训练误差压得太低，以至于模型过拟合训练数据。当模型过度拟合数据时，它会完美地捕获训练集的模式，从而成为仅预测训练集结果的专家。</p><p id="cb70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">乍一看，这听起来很棒，但它有一个缺点。如果模型擅长预测训练集中的目标，它就不太擅长预测其他数据。</p><h1 id="3bc9" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">偏差-方差权衡</h1><p id="657f" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">为了理解这种权衡，我们首先需要看看模型的误差。在实践中，我们可以将模型误差分成三个不同的部分。</p><blockquote class="nl"><p id="bd23" class="nm nn iq bd no np nq nr ns nt nu lr dk translated"><em class="nv">模型误差=不可约误差+偏差+方差</em></p></blockquote><p id="4a72" class="pw-post-body-paragraph kw kx iq ky b kz nw jr lb lc nx ju le lf ny lh li lj nz ll lm ln oa lp lq lr ij bi translated">不可约误差与偏差和方差无关。但是后两者是反向相关的，每当你降低偏差，方差就会增加。就像训练误差和测试误差一样。</p><h2 id="2b08" class="ob mp iq bd mq oc od dn mu oe of dp my lf og oh na lj oi oj nc ln ok ol ne om bi translated"><strong class="ak">不可约误差</strong></h2><p id="2a4f" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">这个误差是建立模型的机器学习工程师无法控制的。这是由数据中的噪声、不代表数据中真实模式的随机变化或尚未作为特征捕获的变量的影响所导致的错误。</p><p id="6794" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">减少这类错误的一种方法是识别对我们正在建模的问题有影响的变量，并将它们转化为特征。</p><h2 id="c039" class="ob mp iq bd mq oc od dn mu oe of dp my lf og oh na lj oi oj nc ln ok ol ne om bi translated"><strong class="ak">偏置</strong></h2><p id="91a7" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">偏差是指捕捉数据集中真实模式的能力。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/a04ca6246acbe646ef032897a9fe86f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9FmRnqAiORKgVU7Yc1FMg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单模型(左)与复杂模型(右)的偏差。</p></figure><p id="1b50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它在数学上表示为预期的预测目标值和真实目标值之间的平方差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/40fbd1d7e4537a6998e49276a4afae7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*F5nH7Yig9lejA5OMdVdyqA.jpeg"/></div></figure><p id="f6c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，当你有一个无偏的模型时，你知道平均预测值和真实值之间的差异是零。并且它被平方以更重地惩罚离目标的真实值更远的预测。</p><p id="ebc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有高偏差的模型会使数据欠拟合。换句话说，它将采用一种简单化的方法来模拟数据中的真实模式。</p><p id="ba45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是一个低偏差的模型比它应该的更复杂。它会过度适应它用来学习的数据，因为它会尽可能多地捕捉细节。因此，除了训练数据之外，它在概括方面做得很差。</p><p id="d1d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以通过观察训练误差来发现偏差。当模型具有高训练误差时，这是高偏差的迹象。</p><p id="df3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了控制偏差，您可以添加更多的特征并构建更复杂的模型，始终在数据的欠拟合和过拟合之间找到平衡。</p><h2 id="d563" class="ob mp iq bd mq oc od dn mu oe of dp my lf og oh na lj oi oj nc ln ok ol ne om bi translated"><strong class="ak">差异</strong></h2><p id="163f" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">方差捕获每个数据记录的预测范围。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/070d3a28e0005f30fcf040fa2c3cf55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I__XWjoKVD6TF76Un-_t0g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有高(左)和低(右)方差的模型中的预测范围。</p></figure><p id="c230" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个衡量每个预测与该测试集记录的所有预测的平均值相差多远的指标。并且它也被平方以惩罚离目标的平均预测更远的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/4dd08a71a4f07778ea9dc88967630240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5OJdMXWQeatvocEjjwyTBg.jpeg"/></div></div></figure><p id="a87c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">即使您构建的每个模型输出的预测值略有不同，您也不希望这些预测值的范围很大。</p><p id="3c3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您还可以通过查看测试误差来发现模型中的差异。当模型具有高测试误差时，这是高方差的标志。</p><p id="2ab4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">减少差异的一种方法是使用更多的训练数据来构建模型。该模型将有更多的实例可供学习，并提高其概括预测的能力。</p><p id="5f5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果无法用更多的训练数据构建模型，您可以构建一个包含<a class="ae kv" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">bootstrap aggregation</a>的模型，通常称为 bagging。</p><p id="6793" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">降低方差的其他方法包括减少特征的数量，使用特征选择技术，以及使用像<a class="ae kv" rel="noopener" target="_blank" href="/principal-component-analysis-algorithm-in-real-life-discovering-patterns-in-a-real-estate-dataset-18134c57ffe7">主成分分析</a>这样的技术来降低数据集的维度。</p><h1 id="b595" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak">现在让我们看看这是怎么回事</strong></h1><p id="09a8" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">我创建了一个随机数据集，它遵循系数为-5，-3，10 和 2.5 的四次多项式，从最高到最低。</p><p id="b815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们要用模型来拟合这些数据，所以我把它分成了训练集和测试集。训练数据是这样的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/f78a579ad6166a6a065ca6e1a791be5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdeGFElWAlK2iEX-eiT6NQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从随机数据生成的四次多项式的训练集。</p></figure><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="fdc5" class="ob mp iq os b gy ow ox l oy oz">dataset_size = 5000</span><span id="5525" class="ob mp iq os b gy pa ox l oy oz"># Generate a random dataset and that follows a quadratic distribution<br/>random_x = np.random.randn(dataset_size)<br/>random_y = ((-5 * random_x ** 4) + (-3 * random_x ** 3) + 10 * random_x ** 2 + 2.5 ** random_x + 0.5).reshape(dataset_size, 1)</span><span id="4257" class="ob mp iq os b gy pa ox l oy oz"># Hold out 20% of the dataset for training<br/>test_size = int(np.round(dataset_size * 0.2, 0))</span><span id="4a68" class="ob mp iq os b gy pa ox l oy oz"># Split dataset into training and testing sets<br/>x_train = random_x[:-test_size]<br/>y_train = random_y[:-test_size]</span><span id="d7b7" class="ob mp iq os b gy pa ox l oy oz">x_test = random_x[-test_size:]<br/>y_test = random_y[-test_size:]<br/></span><span id="5a96" class="ob mp iq os b gy pa ox l oy oz"># Plot the training set data<br/>fig, ax = plt.subplots(figsize=(12, 7))</span><span id="3263" class="ob mp iq os b gy pa ox l oy oz"># removing to and right border<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)</span><span id="4033" class="ob mp iq os b gy pa ox l oy oz"># adding major gridlines<br/>ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)<br/>ax.scatter(x_train, y_train, color='#021E73’)</span><span id="05b4" class="ob mp iq os b gy pa ox l oy oz">plt.show()</span></pre><p id="82f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以从检查模型的复杂性如何影响偏差开始。</p><p id="af9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从简单的线性回归开始，逐步用更复杂的模型来拟合这些数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/213c840c3f14b1fc4ff4afdb9640f4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqWv5_yvfhKFHN9f3zeALw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">适合训练数据的简单线性回归模型。</p></figure><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="41dd" class="ob mp iq os b gy ow ox l oy oz"># Fit model<br/># A first degree polynomial is the same as a simple regression line</span><span id="013b" class="ob mp iq os b gy pa ox l oy oz">linear_regression_model = np.polyfit(x_train, y_train, deg=1)</span><span id="d874" class="ob mp iq os b gy pa ox l oy oz"># Predicting values for the test set<br/>linear_model_predictions = np.polyval(linear_regression_model, x_test)<br/></span><span id="b8cb" class="ob mp iq os b gy pa ox l oy oz"># Plot linear regression line<br/>fig, ax = plt.subplots(figsize=(12, 7))</span><span id="4b0c" class="ob mp iq os b gy pa ox l oy oz"># removing to and right border<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)</span><span id="91f9" class="ob mp iq os b gy pa ox l oy oz"># adding major gridlines<br/>ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)<br/>ax.scatter(random_x, random_y, color='#021E73')</span><span id="4468" class="ob mp iq os b gy pa ox l oy oz">plt.plot(x_test, linear_model_predictions, color='#F2B950', linewidth=3)</span><span id="6a48" class="ob mp iq os b gy pa ox l oy oz">plt.show()</span></pre><p id="2b92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种类型的模型肯定太简单了，它根本不遵循数据的模式。</p><p id="24e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将量化该模型与训练和测试误差的拟合，使用均方误差计算，并查看偏差和方差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/f473f3fe8c3a3c21b147e022db5f56ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kwh-UVAfs2GLs51pWi_9Aw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单线性回归模型的度量。</p></figure><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="428c" class="ob mp iq os b gy ow ox l oy oz"># A few auxiliary methods<br/>def get_bias(predicted_values, true_values):<br/><em class="pc">""" Calculates model bias</em></span><span id="5b40" class="ob mp iq os b gy pa ox l oy oz"><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> predicted_values: values predicted by the model<br/></em><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> true_values: true target values for the data<br/></em><strong class="os ir"><em class="pc">:return</em></strong><em class="pc">: integer representing the bias of the model</em></span><span id="b8dd" class="ob mp iq os b gy pa ox l oy oz"><em class="pc">"""</em></span><span id="449a" class="ob mp iq os b gy pa ox l oy oz">return np.round(np.mean((predicted_values - true_values) ** 2), 0)</span><span id="fe58" class="ob mp iq os b gy pa ox l oy oz">def get_variance(values):<br/><em class="pc">""" Calculates variance of an array of values</em></span><span id="fc54" class="ob mp iq os b gy pa ox l oy oz"><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> values: array of values<br/></em><strong class="os ir"><em class="pc">:return</em></strong><em class="pc">: integer representing the variance of the values</em></span><span id="1684" class="ob mp iq os b gy pa ox l oy oz"><em class="pc">"""</em></span><span id="1830" class="ob mp iq os b gy pa ox l oy oz">return np.round(np.var(values), 0)</span><span id="8b17" class="ob mp iq os b gy pa ox l oy oz">def get_metrics(target_train, target_test, model_train_predictions, model_test_predictions):</span><span id="5e97" class="ob mp iq os b gy pa ox l oy oz"><em class="pc">"""<br/>Calculates<br/>1. Training set MSE<br/>2. Test set MSE<br/>3. Bias<br/>4. Variance<br/></em></span><span id="7a9e" class="ob mp iq os b gy pa ox l oy oz"><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> target_train: target values of the training set<br/></em><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> target_test: target values of the test set<br/></em><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> model_train_predictions: predictions from running training set through the model<br/></em><strong class="os ir"><em class="pc">:param</em></strong><em class="pc"> model_test_predictions: predictions from running test set through the model<br/></em><strong class="os ir"><em class="pc">:return</em></strong><em class="pc">: array with Training set MSE, Test set MSE, Bias and Variance</em></span><span id="229c" class="ob mp iq os b gy pa ox l oy oz"><em class="pc">"""</em></span><span id="7ea5" class="ob mp iq os b gy pa ox l oy oz">training_mse = mean_squared_error(target_train, model_train_predictions)</span><span id="7a54" class="ob mp iq os b gy pa ox l oy oz">test_mse = mean_squared_error(target_test, model_test_predictions)</span><span id="c61a" class="ob mp iq os b gy pa ox l oy oz">bias = get_bias(model_test_predictions, target_test)</span><span id="aa78" class="ob mp iq os b gy pa ox l oy oz">variance = get_variance(model_test_predictions)<br/></span><span id="e397" class="ob mp iq os b gy pa ox l oy oz">return [training_mse, test_mse, bias, variance]</span><span id="c709" class="ob mp iq os b gy pa ox l oy oz"># Fit simple linear regression model<br/># A first degree polynomial is the same as a simple regression line</span><span id="3e8b" class="ob mp iq os b gy pa ox l oy oz">linear_regression_model = np.polyfit(x_train, y_train, deg=1)</span><span id="3f3b" class="ob mp iq os b gy pa ox l oy oz"># Predicting values for the test set<br/>linear_model_predictions = np.polyval(linear_regression_model, x_test)</span><span id="91e4" class="ob mp iq os b gy pa ox l oy oz"># Predicting values for the training set<br/>training_linear_model_predictions = np.polyval(linear_regression_model, x_train)</span><span id="b5b7" class="ob mp iq os b gy pa ox l oy oz"># Calculate for simple linear model<br/># 1. Training set MSE<br/># 2. Test set MSE<br/># 3. Bias<br/># 4. Variance</span><span id="cd42" class="ob mp iq os b gy pa ox l oy oz">linear_training_mse, linear_test_mse, linear_bias, linear_variance = get_metrics(y_train, y_test, training_linear_model_predictions, linear_model_predictions)</span><span id="bf9f" class="ob mp iq os b gy pa ox l oy oz">print('Simple linear model')<br/>print('Training MSE %0.f' % linear_training_mse)<br/>print('Test MSE %0.f' % linear_test_mse)<br/>print('Bias %0.f' % linear_bias)<br/>print('Variance %0.f' % linear_variance)</span></pre><p id="85bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看使用复杂模型是否真的有助于降低偏差。我们将对该数据进行二阶多项式拟合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/62692d237f23619ba404550848f6f9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXmNj6OsxbVbKA54RYkw7g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拟合训练数据的二次多项式模型。</p></figure><p id="91c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二次多项式减少偏差是有意义的，因为它越来越接近数据的真实模式。</p><p id="0131" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也看到了偏差和方差之间的反比关系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/7f613abc5bf277946ae1ca97b42eda66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Mx3l_qQuoP4uaJux9IvJg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">二次多项式模型的度量。</p></figure><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="814c" class="ob mp iq os b gy ow ox l oy oz">#############################<br/># Fit 2nd degree polynomial #<br/>#############################</span><span id="2a88" class="ob mp iq os b gy pa ox l oy oz"># Fit model<br/>polynomial_2nd_model = np.polyfit(x_train, y_train, deg=2)<br/></span><span id="9e05" class="ob mp iq os b gy pa ox l oy oz"># Used to plot the predictions of the polynomial model and inspect coefficients<br/>p_2nd = np.poly1d(polynomial_2nd_model.reshape(1, 3)[0])<br/>print('Coefficients %s\n' % p_2nd)</span><span id="0d9e" class="ob mp iq os b gy pa ox l oy oz"># Predicting values for the test set<br/>polynomial_2nd_predictions = np.polyval(polynomial_2nd_model, x_test)</span><span id="5730" class="ob mp iq os b gy pa ox l oy oz"># Predicting values for the training set<br/>training_polynomial_2nd_predictions = np.polyval(polynomial_2nd_model, x_train)</span><span id="94e4" class="ob mp iq os b gy pa ox l oy oz"># Calculate for 2nd degree polynomial model<br/># 1. Training set MSE<br/># 2. Test set MSE<br/># 3. Bias<br/># 4. Variance</span><span id="12bc" class="ob mp iq os b gy pa ox l oy oz">polynomial_2nd_training_mse, polynomial_2nd_test_mse, polynomial_2nd_bias, polynomial_2nd_variance = get_metrics(y_train, y_test, training_polynomial_2nd_predictions, polynomial_2nd_predictions)</span><span id="b859" class="ob mp iq os b gy pa ox l oy oz">print('2nd degree polynomial')<br/>print('Training MSE %0.f' % polynomial_2nd_training_mse)<br/>print('Test MSE %0.f' % polynomial_2nd_test_mse)<br/>print('Bias %0.f' % polynomial_2nd_bias)<br/>print('Variance %0.f' % polynomial_2nd_variance)</span><span id="ce38" class="ob mp iq os b gy pa ox l oy oz"># Plot 2nd degree polynomial<br/>fig, ax = plt.subplots(figsize=(12, 7))</span><span id="3c6c" class="ob mp iq os b gy pa ox l oy oz"># removing to and right border<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)</span><span id="ade5" class="ob mp iq os b gy pa ox l oy oz"># Adding major gridlines<br/>ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)</span><span id="3cdf" class="ob mp iq os b gy pa ox l oy oz">x_linspace = np.linspace(min(random_x), max(random_x), num=len(polynomial_2nd_predictions))</span><span id="f0ab" class="ob mp iq os b gy pa ox l oy oz">plt.scatter(random_x, random_y, color='#021E73')<br/>plt.plot(x_linspace, p_2nd(x_linspace), '-', color='#F2B950', linewidth=3)</span><span id="3b99" class="ob mp iq os b gy pa ox l oy oz">plt.show()</span></pre><p id="aef4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们再次将模型的复杂度增加到三次多项式时，我们看到偏差略有改善。但是方差又增加了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/a1877f01e1720124be41261d4915f06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QfSDhhYfntX8baIk6g6aBw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拟合训练数据的三次多项式模型。</p></figure><p id="5eaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剧情变化不大，但是看指标就清楚了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/b3fddf973cef8e9a713ada9f730687a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgPSHyYm6wM3uXJZXOVz0A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三次多项式模型的度量。</p></figure><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="85d4" class="ob mp iq os b gy ow ox l oy oz">#############################<br/># Fit 3rd degree polynomial #<br/>#############################</span><span id="e26c" class="ob mp iq os b gy pa ox l oy oz">print('3rd degree polynomial')</span><span id="2cd1" class="ob mp iq os b gy pa ox l oy oz"># Fit model<br/>polynomial_3rd_model = np.polyfit(x_train, y_train, deg=3)</span><span id="056b" class="ob mp iq os b gy pa ox l oy oz"># Used to plot the predictions of the polynomial model and inspect coefficients</span><span id="4368" class="ob mp iq os b gy pa ox l oy oz">p_3rd = np.poly1d(polynomial_3rd_model.reshape(1, 4)[0])</span><span id="9e1a" class="ob mp iq os b gy pa ox l oy oz">print('Coefficients %s' % p_3rd)</span><span id="3817" class="ob mp iq os b gy pa ox l oy oz"># Predict values for the test set<br/>polynomial_3rd_predictions = np.polyval(polynomial_3rd_model, x_test)</span><span id="df2e" class="ob mp iq os b gy pa ox l oy oz"># Predict values for the training set<br/>training_polynomial_3rd_predictions = np.polyval(polynomial_3rd_model, x_train)</span><span id="c483" class="ob mp iq os b gy pa ox l oy oz"># Calculate for 3rd degree polynomial model<br/># 1. Training set MSE<br/># 2. Test set MSE<br/># 3. Bias<br/># 4. Variance</span><span id="8f83" class="ob mp iq os b gy pa ox l oy oz">polynomial_3rd_training_mse, polynomial_3rd_test_mse, polynomial_3rd_bias, polynomial_3rd_variance = get_metrics(y_train, y_test, training_polynomial_3rd_predictions, polynomial_3rd_predictions)<br/></span><span id="b23d" class="ob mp iq os b gy pa ox l oy oz">print('\nTraining MSE %0.f' % polynomial_3rd_training_mse)<br/>print('Test MSE %0.f' % polynomial_3rd_test_mse)<br/>print('Bias %0.f' % polynomial_3rd_bias)<br/>print('Variance %0.f' % polynomial_3rd_variance)</span><span id="b136" class="ob mp iq os b gy pa ox l oy oz"><br/># Plot 3rd degree polynomial<br/>fig, ax = plt.subplots(figsize=(12, 7))</span><span id="ab18" class="ob mp iq os b gy pa ox l oy oz"># removing to and right border<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)</span><span id="8f12" class="ob mp iq os b gy pa ox l oy oz"># Adding major gridlines<br/>ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)</span><span id="7c5a" class="ob mp iq os b gy pa ox l oy oz">x_linspace = np.linspace(min(random_x), max(random_x), num=len(polynomial_3rd_predictions))<br/>plt.scatter(random_x, random_y, color='#021E73')<br/>plt.plot(x_linspace, p_3rd(x_linspace), '-', color='#F2B950', linewidth=3)</span><span id="5976" class="ob mp iq os b gy pa ox l oy oz">plt.show()</span></pre><p id="4b12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结这个实验，我们可以真正看到偏差-方差权衡的作用。随着我们增加模型的复杂性，偏差不断减少，而方差增加。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/2511ae16d52695c4b5d8af8c5b912ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZX3dYv8sRPkyZqHiuhraaQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">实验结果总结。</p></figure><p id="00b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望你在建立机器学习模型时，对偏差和方差的作用有了更好的理解。</p><p id="3da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="pc">感谢阅读！</em></p></div></div>    
</body>
</html>