<html>
<head>
<title>Building an understanding of viruses by mining COVID-19 scientific corpus</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过挖掘新冠肺炎科学语料库建立对病毒的理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-understanding-of-viruses-by-mining-covid-19-scientific-corpus-f1d27c33b48?source=collection_archive---------42-----------------------#2020-09-17">https://towardsdatascience.com/building-an-understanding-of-viruses-by-mining-covid-19-scientific-corpus-f1d27c33b48?source=collection_archive---------42-----------------------#2020-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a7f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第 1 部分:熟悉数据</h2></div><p id="497f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我发表这篇文章的时候，我们正进入新冠肺炎冻结世界以来的第九个月。自 1 月初以来，我们都有不同的经历:我们中的一些人很幸运，被锁在自己的房子里，能够远程工作，过着相对不变的生活，我们中的一些人没有这样的奢侈，一些人不幸去世。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/6b37d4efda24f64d240432c386313416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBh5Ee9rX6IrcZCG2tRaiA.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源:https://unsplash.com/photos/EAgGqOiDDMg/download?force=true</p></figure><p id="d9bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">冠状病毒已经存在了几十年，但迄今为止很少有像新冠肺炎病毒这样致命和容易传播的。今年早些时候，AI2 艾伦人工智能研究所(Allen Institute for AI)和一个由研究机构组成的联盟与白宫一起策划了一个自 19 世纪以来发表的关于冠状病毒的科学论文语料库，并提供了一个<a class="ae lu" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank"> Kaggle 竞赛</a>来分析它并回答一些关于病毒不同方面的问题，如它如何传播，或它如何影响活的有机体。</p><p id="238e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我没有直接参加挑战本身，<a class="ae lu" rel="noopener" target="_blank" href="/building-an-understanding-of-viruses-by-mining-covid-19-scientific-corpus-bdd7427d6a8a">这个系列</a>试图通过建立深度学习 BERT-EM-SOM 模型(与自组织地图相关的 BERT 嵌入层)来更好地了解冠状病毒。正如在宣布这个系列的的<a class="ae lu" rel="noopener" target="_blank" href="/building-an-understanding-of-viruses-by-mining-covid-19-scientific-corpus-bdd7427d6a8a">帖子中提到的，在这一期中，我们将专注于理解论文的语料库，并将清理元数据，以便我们可以在一个干净的基础上工作。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6e51" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据</h1><p id="c1b0" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这些数据可以从几乎每天都在更新的 AI2 S3 数据库中下载。下面的代码片段下载数据并将其提取到数据目录中。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="00ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本例中，我们使用 2020 年 8 月 24 日发布的数据集。阅读这一天的元数据显示，应该有 233，539 篇论文讨论冠状病毒相关的主题。我们将测试数据集是干净的这一假设，即所有 233，539 个数据集都在作为归档一部分的 JSON 文件中找到，我们没有任何重复项，也没有丢失信息。</p><p id="7a5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但首先，让我们熟悉一下数据集的基础知识。</p><h1 id="91ee" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">基础数据探索</h1><p id="c28a" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在这个故事的剩余部分，我们将在使用 NVIDIA RAPIDS 的原生 cuDF 和使用 T2 BlazingSQL 的 SQL 之间进行常规切换。大部分时间我都在用<a class="ae lu" href="https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx/" rel="noopener ugc nofollow" target="_blank">泰坦 RTX </a>卡。然而，我很幸运仍然拥有租借的联想 P53 移动工作站和 NVIDIA Quadro RTX 5000 GPU ,所以当我最近和我的孩子露营时(适当地保持社交距离并戴上面具！)我仍然能够创作这个故事。这是另一款出色的硬件，我强烈推荐给所有想要在旅行或向客户或朋友展示工作时使用 RAPIDS、BlazingSQL 或 CUDA 的 GPU 爱好者。看看这个美女！</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/ade9808ff27bbaedbf573b6bc3c19125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owtt0LF4DWTi5Za4IltggA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源:联想，经许可使用</p></figure><blockquote class="nh ni nj"><p id="615b" class="ki kj nk kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">作为一个完整的披露——这篇文章不是由英伟达或联想赞助的，我也没有从他们那里得到报酬——我只是在测试他们借给我进行这项研究的设备。为此我非常感激！</p><p id="b0eb" class="ki kj nk kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">这个故事和我其他故事中的所有代码都可以在<a class="ae lu" href="https://github.com/drabastomek/rapids-notebooks" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。</p></blockquote><p id="2558" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">metadata.csv 文件中有 19 列；下面是一个例子。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="8219" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数列都是 id，比如<strong class="kk iu"> cord_uid </strong>或<strong class="kk iu"> doi </strong>等等。然而，对我们来说更有趣的是，我们将关注</p><ul class=""><li id="ed62" class="no np it kk b kl km ko kp kr nq kv nr kz ns ld nt nu nv nw bi translated"><strong class="kk iu">标题</strong>、<strong class="kk iu">摘要</strong>和<strong class="kk iu">作者</strong>我们将使用来查看是否有重复</li><li id="0323" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated">显示文章发表时间的</li><li id="b70c" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu"> source_x </strong>显示文章的来源</li><li id="862a" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu">期刊</strong>栏列出了发表文章的期刊</li><li id="452a" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu"> pdf_json_files </strong>显示带有论文正文的文件的位置；我们将使用它来查看我们是否有任何丢失的文件，以便我们可以排除它们。</li></ul><p id="c980" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们看看 AI2 研究人员收集这些文章的最常见来源是什么。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="5b77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码片段简单地获取元数据 cuDF 数据帧，按 source_x 列分组，并打印出一个表。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="367b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，最常见的来源是世卫组织，其次是 Medline 和 PMC。接下来，我们将看到通过 Elsevier、MedRxiv、ArXiv 和 BioRxiv 发布的文章。这些将占语料库中所有论文的大部分。</p><p id="5560" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便说一下，可以使用 BlazingSQL 用 SQL 创建相同的表。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="1c29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这显示了 RAPIDS 生态系统的美妙之处:如果你熟悉 pandas 或 SQL，你就拥有了在数据上释放 GPU 巨大能力所需的所有工具，甚至不用多想(因为你没有时间…看这个:<a class="ae lu" href="https://www.linkedin.com/posts/tomekd_blazingsql-notebooks-private-preview-activity-6710308422451126272-wXCb" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/posts/tome KD _ blazingsql-notebooks-private-preview-activity-6710308422451126272-wXCb</a>！).</p><p id="7b47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们来看看这些论文是在哪里发表的。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="2eae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，许多文章都是在 bioRxiv 上预先发表的，但我们也在《BMJ》、《公共科学图书馆综合版》、《病毒学杂志》、《柳叶刀》和《自然》上看到了很多。换句话说，不出所料。</p><p id="c52e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧，时间呢？让我们看看论文是什么时候发表的。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="0359" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码片段生成了下面的图表。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oc"><img src="../Images/3af41c4a74af904af7557e6ddefe103a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5L8M65wubgiD6WngfR2ug.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源:作者</p></figure><p id="9d28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣但并不令人意外的是，在 2002/2003 年左右发表的论文数量有所上升，因为这很可能与<a class="ae lu" href="https://www.cdc.gov/sars/about/fs-sars.html" rel="noopener ugc nofollow" target="_blank"> SARS 疫情</a>有关。</p><p id="e343" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能想知道为什么我把 2020 年排除在外……好吧，我最初把它放在图表上，但它实际上是<em class="nk">让其余时间序列的</em>相形见绌。我认为下面的图表应该解释为什么。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi od"><img src="../Images/b2bff02e23705a0b181a753754d996fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_ZrovzAX0aXFRytNWldvw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源:作者</p></figure><p id="c263" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自 1816 年至 2019 年(含)，作为该语料库一部分发表和报告的论文总数约为 9 万篇。在 2020 年的近 9 个月里，全球研究人员发表了超过 14 万篇关于冠状病毒的论文和其他信息。因此，我决定，排除 2020 年的报告论文数将使上面的时间序列图更清晰。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="215a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">清除</h1><p id="4b84" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">现在是我们查看数据本身的时候了。我喜欢对任何数据集首先做的是检查缺失的观察值。</p><h2 id="46dc" class="oe md it bd me of og dn mi oh oi dp mm kr oj ok mo kv ol om mq kz on oo ms op bi translated">遗漏的观察</h2><p id="8f16" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">每个数据集都有一些。这些可能是简单的遗漏或遥测问题(我们可以估算或删除它们)，也可能是我所说的有效缺失值，即根本不应该存在的值。有效缺失观察的一个例子是，在这个特定的数据集中，以及当论文没有在 arxiv 上发表时的<strong class="kk iu"> arxiv_id </strong>。</p><p id="8c48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 SQL 创建一个包含丢失值百分比的表非常容易。我是这样做的。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="40c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代替解释这里发生了什么:我使用了在上面要点的第 3–7 行中创建的两个 SQL 查询。如果您将这些查询打印出来，它们将如下所示。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="553d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最终的 SQL 查询只是返回列的列表及其相应的缺失值百分比。</p><p id="9d58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所料，许多 ID 列有许多遗漏的观察值，但是我们不会在后面的分析中用到这些，所以不要担心。然而，缺少<strong class="kk iu"> pdf_json_files </strong>列几乎有 60%的值丢失……这绝对是一个问题，因为我们需要能够找到论文的主体。因此，我们应该删除这些遗漏的观察。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="309c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上述操作之后，我们在语料库中得到大约 94，000 篇论文。重新运行前面的查询会产生以下每列缺失值百分比的细分。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="00f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们仍然错过了一些标题、作者或摘要，但这没关系——这不应该妨碍我们的方法。更重要的是，我们可以找到这个文件，然后用<strong class="kk iu"> cord_uid </strong>链接它，返回相关的文件。</p><h2 id="3927" class="oe md it bd me of og dn mi oh oi dp mm kr oj ok mo kv ol om mq kz on oo ms op bi translated">复制</h2><p id="ceae" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">接下来，我们将把目光转向寻找重复，这是任何数据集的另一个常见问题(我还没有找到一个没有重复的原始数据集)。查询元数据表，我在每一列中得到如下数量的重复记录。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="bb2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，在<strong class="kk iu"> pdf_json_files </strong>栏中，我们有超过 1200 个重复的标题、282 个重复的摘要和 6 个重复的链接。让我们从那些开始。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="fc91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看上表，我们可以清楚地看出前 3 篇论文不是重复的——它们只是错误地指向同一个 json 文件。然而，剩下的文件肯定是重复的。因为本专栏总共只有 6 个重复项，所以我们将使用。来自 cuDF 的 dropna()功能。</p><p id="d0c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们看看重复的标题。允许我们创建重复行的快速列表的查询如下所示，以及标题重复的前 10 条记录。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="7500" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以看到，这些是真正的重复记录:不知何故，它们与 doi 标识符不同，但标题、摘要和作者大多匹配。但是，由于有超过 1200 个重复的记录，我们不想删除所有这些记录，将使用。drop_duplicated()方法从每个副本中保留一条记录。</p><h2 id="2d19" class="oe md it bd me of og dn mi oh oi dp mm kr oj ok mo kv ol om mq kz on oo ms op bi translated">数据文件夹中缺少 JSON 文件</h2><p id="6adf" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">最终检查:让我们看看是否真的可以在 data 文件夹中找到 metadata.csv 文件<em class="nk">中列出的所有文件。为了尽可能提高效率，我们将采用的方法是列出<em class="nk">data/2020–08–24/document _ parses/pdf _ JSON</em>目录中的所有文件，从这些文件中创建一个 cuDF/BlazingSQL 表，然后将它们与我们已有的重复数据删除数据集连接起来。</em></p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="58df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，实际上非常令人惊讶的是，我们在 metadata.csv 文件中有链接到的 12.5k 文件，但在磁盘上找不到，还有存在于磁盘上但在 metadata.csv 文件中无法引用的 5k 文件。在这种情况下，我决定丢弃所有丢失的文件，从而只保留我在 metadata.csv 文件和磁盘上都能找到的 87，438 个文件。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ac98" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">有罪直到被证明无罪</h1><p id="c3fa" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这部分到此结束。正如我在本系列的介绍中提到的，我总是以怀疑的态度对待我得到的每个数据集，直到我向自己证明它足够干净，可以用于进行分析或构建 ML 或 DL 模型。</p><p id="7917" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下一期文章中，我们将着眼于从磁盘中读取文件，并使用来自 RAPIDS 的<a class="ae lu" href="https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.column.string.StringMethods.subword_tokenize" rel="noopener ugc nofollow" target="_blank"> subword_tokenizer </a>来标记文本，为第 3 部分中的 BERT 训练做准备。</p><p id="5dcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，如果您想尝试本系列中介绍的任何代码，请访问 https://app.blazingsql.com/的<a class="ae lu" href="https://app.blazingsql.com/" rel="noopener ugc nofollow" target="_blank">并发布一个免费集群！</a></p></div></div>    
</body>
</html>