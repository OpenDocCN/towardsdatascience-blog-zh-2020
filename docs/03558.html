<html>
<head>
<title>Film Script Generation With GPT-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用GPT-2生成电影剧本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/film-script-generation-with-gpt-2-58601b00d371?source=collection_archive---------15-----------------------#2020-04-04">https://towardsdatascience.com/film-script-generation-with-gpt-2-58601b00d371?source=collection_archive---------15-----------------------#2020-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/08f7ac83892f1fb23b6707b99a8a5a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kaCgw8yhzEclZTAt"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">卢卡·奥尼博尼在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="d930" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="44d1" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">微调GPT-2成为编剧使用拥抱脸的变形金刚包。</h2></div><h1 id="5f22" class="lh li jj bd lj lk ll lm ln lo lp lq lr ky ls kz lt lb lu lc lv le lw lf lx ly bi translated">介绍</h1><p id="7010" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在过去的两年中，NLP和语言一代经历了一个<em class="mv">复兴</em>的时刻。随着transformer神经网络架构的出现，围绕这些类型的模型的大量潜在用例的工作(和宣传)出现了爆炸式增长。</p><p id="27f7" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">语言生成是这些模型的一个特点，我觉得这个特点特别令人印象深刻，主要是因为它提供了一种非常人性化的方式来判断模型内部语言表达的质量。</p><p id="ffb8" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">Open AI的<a class="ae jg" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>作为一个语言模型已经获得了很多关注，特别是因为它的自动回归特性，这使它能够生成新的文本序列。</p><p id="f0cf" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">在这个项目中，我想在电影剧本数据集<em class="mv"> (~60mb) </em>上微调一个语言模型，这个数据集是我从IMSDB(互联网电影剧本数据库)中搜集的。</p><p id="3d0b" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我最初是在几年前为我本科时的另一个项目收集了这个数据集，该项目对电影剧本中的对话进行情感分析和聚类。当时我想尝试创作新的电影剧本片段，但研究还没有完成，拥抱脸的变形金刚包也没有完成。</p><p id="82b2" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">谢天谢地，它现在已经存在了，我可以用它来构建脚本伙伴了。如果您想自己快速启动并运行该模型，您可以:</p><ul class=""><li id="6b12" class="nb nc jj mb b mc mw mf mx mi nd mm ne mq nf mu ng nh ni nj bi translated">查看下面的python脚本片段，并亲自运行该模型。</li><li id="e2e3" class="nb nc jj mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">请访问Github上的项目，了解如何在您的本地机器上安装和运行Streamlit应用程序。</li><li id="bcea" class="nb nc jj mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">查看该机器人在twitter上的最新帖子。</li></ul><blockquote class="np nq nr"><p id="fcc3" class="lz ma mv mb b mc mw kt me mf mx kw mh ns my mk ml nt mz mo mp nu na ms mt mu im bi translated"><a class="ae jg" href="https://github.com/cdpierse/script_buddy_v2" rel="noopener ugc nofollow" target="_blank">https://github.com/cdpierse/script_buddy_v2 Github</a></p><p id="b2f2" class="lz ma mv mb b mc mw kt me mf mx kw mh ns my mk ml nt mz mo mp nu na ms mt mu im bi translated">推特:https://twitter.com/script_buddy</p></blockquote><p id="7932" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">如果你想更多地了解这个项目，或者想知道如何为你自己的项目微调GPT-2，请继续阅读。</p><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="nz oa l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">快速开始使用模型— <em class="ob">记得先安装变压器和py torch/tensor flow</em></p></figure></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="3367" class="lh li jj bd lj lk oj lm ln lo ok lq lr ky ol kz lt lb om lc lv le on lf lx ly bi translated">从IMSDB抓取剧本</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/c13ed14bcc5bffc84602ade18b9132d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBQSz2mh_RHWyVv6q8sVOw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">IMSDB主页</p></figure><p id="bb30" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">IMSDB是一个在线电影剧本库，包含大约1300个电影剧本。</p><p id="c351" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">虽然只有1300个剧本，但一个电影剧本平均包含约30，000个单词。所以在数据集中，我们有将近4000万个单词序列</p><p id="8794" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">由于剧本在网站上以纯文本的形式存储，设计一个scraper来用<a class="ae jg" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>遍历每个剧本url变得相对容易。</p><p id="8ab8" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">脚本以纯文本形式存储在IMSDB上的另一个巨大好处是，它允许我保持每个脚本的结构和特定的缩进不变。</p><p id="c9b9" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">电影剧本是高度结构化的文本，用视觉线索来表示场景动作、场景位置和人物对话。我希望模型能够生成完整的脚本序列，每个序列中包含混合的脚本元素，因此，在训练时输入模型的数据必须代表剧本的文本和结构布局，以使模型能够优化特定的结构，这一点很重要。</p><p id="a2d7" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">随着每一个剧本的搜集，我也能够收集类型元数据。虽然我没有使用它来训练模型，但过滤特定流派的训练脚本将是一种潜在的进一步微调模型的方法，以便在未来生成属于特定流派的片段。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="459c" class="lh li jj bd lj lk oj lm ln lo ok lq lr ky ol kz lt lb om lc lv le on lf lx ly bi translated">准备数据</h1><p id="fe08" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">要将脚本数据批量加载到模型中，它需要采用GPT新协议的正确标记化格式。谢天谢地，使用PyTorch的dataset类和transformers进行设置相对容易。</p><p id="9a2b" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我创建了一个ScriptData类<em class="mv">(见下文)</em>，它将整个脚本数据集分割成标记化的张量块。一旦运行，这些模块就可以在训练循环中批量加载到GPT-2中。如果您自己正在使用transformers包，那么根据您自己的项目需求修改下面的dataset类应该相对简单。</p><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="op oa l"/></div></figure></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="1ffc" class="lh li jj bd lj lk oj lm ln lo ok lq lr ky ol kz lt lb om lc lv le on lf lx ly bi translated">微调GPT-2</h1><p id="ff56" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在我动手之前，微调对我来说似乎非常令人生畏。然而，一旦我进入拥抱脸的文件，并找到一些资源，我开始拼凑如何微调和GPT-2的一些内部机制如何工作。感谢马丁·弗罗洛夫在他的<a class="ae jg" href="https://mf1024.github.io/2019/11/12/Fun-With-GPT-2_/" rel="noopener ugc nofollow" target="_blank">帖子</a>中详细介绍了他是如何在一组笑话上对GPT-2进行微调的，这是一个巨大的帮助。</p><p id="08cc" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">首先，你<strong class="mb jt">需要</strong>一个GPU来真实地训练一个像GPT-2或变形金刚模型一般的模型，我利用了<a class="ae jg" href="https://www.paperspace.com/" rel="noopener ugc nofollow" target="_blank"> Paperspace的</a>免费GPU支持的Jupyter笔记本。我选择这个而不是Google Colab的GPU笔记本，纯粹是因为我不喜欢Colab的按键绑定和界面。但是，如果Colab是您的笔记本电脑，它也将工作得很好。</p><p id="a02e" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我用的<code class="fe oq or os ot b">gpt2-medium</code>是变形金刚提供的GPT-2的中型版本。这个版本的模型有12层，大约3.45亿个参数。由于免费笔记本中GPU的内存限制，在这个<code class="fe oq or os ot b">gpt2-large</code>和<code class="fe oq or os ot b">gpt2-xl</code>之上还有两个版本的模型我在最初的培训中没有使用。希望将来我会使用更大的模型做一些进一步的微调。</p><p id="1449" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">用transformers包调用模型及其标记器非常简单，可以通过两行代码完成:</p><pre class="nv nw nx ny gt ou ot ov ow aw ox bi"><span id="e5b7" class="oy li jj ot b gy oz pa l pb pc">tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')<br/>model = GPT2LMHeadModel.from_pretrained('gpt2-medium')</span></pre><p id="e6b4" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">有了这两个对象，您可以按原样使用GPT-2，但是要在标记化文本的自定义数据集上对其进行微调或优化，您需要创建一个训练循环，从整个数据集中逐步加载一批脚本序列。</p><blockquote class="np nq nr"><p id="6a9d" class="lz ma mv mb b mc mw kt me mf mx kw mh ns my mk ml nt mz mo mp nu na ms mt mu im bi translated">C <!-- -->为数据加载器选择一个批处理大小可能很棘手，如果你选择了一个太大的大小，会导致你很快耗尽GPU内存。为了安全起见，我建议从1开始，测试你能推多远。最后，我成功地训练了批量为7的模型。</p></blockquote><p id="050c" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">对于每一批，标记化张量作为其输入和目标标签通过语言模型头运行。返回了许多结果，我们使用前两个结果:<code class="fe oq or os ot b">loss</code>和<code class="fe oq or os ot b">logits</code>对梯度进行反向传递，并报告一定步数的损失。</p><p id="95ee" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">每200个批次，我设置一个评估步骤，生成一批文本，这有助于在培训期间作为一个可视化的辅助工具来查看模型优化和捕获脚本结构的情况。从2.4版本开始，transformers中增加的<code class="fe oq or os ot b">generate</code>功能非常有用，它提供了许多不同的解码方法，您可以调整这些方法以获得最佳的生成结果。</p><blockquote class="np nq nr"><p id="42a2" class="lz ma mv mb b mc mw kt me mf mx kw mh ns my mk ml nt mz mo mp nu na ms mt mu im bi translated">查看他们的帖子<a class="ae jg" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/how-to-generate</a>以了解更多信息。</p></blockquote><p id="dc57" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我总共运行了3个完整时期的模型，每个时期花费了大约6个小时。</p><p id="eddf" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">要查看下面微调脚本的完整笔记本，请单击<a class="ae jg" href="https://github.com/cdpierse/script_buddy_v2/blob/master/script_buddy/script_generation.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="op oa l"/></div></figure></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="eae0" class="lh li jj bd lj lk oj lm ln lo ok lq lr ky ol kz lt lb om lc lv le on lf lx ly bi translated">简化应用程序</h1><p id="566a" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">模型本身被托管在huggingface的<a class="ae jg" href="https://huggingface.co/cpierse/gpt2_film_scripts" rel="noopener ugc nofollow" target="_blank">模型中心</a>中，可以使用这个包直接下载。</p><p id="7f2b" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我还利用这个模型作为最终使用<a class="ae jg" href="https://www.streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>的机会，这是一个新的python框架，用于快速构建ML和数据工具。</p><div class="is it gp gr iu pd"><a rel="noopener follow" target="_blank" href="/coding-ml-tools-like-you-code-ml-models-ddba3357eace"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd jt gy z fp pi fr fs pj fu fw js bi translated">将Python脚本变成漂亮的ML工具</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">介绍专为ML工程师打造的应用框架Streamlit</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">towardsdatascience.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr ja pd"/></div></div></a></div><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/368bed79f105aad002e1260d10af5d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E5gwo_kEPhdDnEEgwqNsbQ.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">编写好友Streamlit应用程序脚本</p></figure><p id="6bc9" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">这款应用的开发轻而易举，不到40行代码就完成了。一个特别棒的特性是内置的缓存装饰函数，它允许我只在应用启动时加载一次模型和标记器，而不是在侧边栏的元素改变时。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="bb8b" class="lh li jj bd lj lk oj lm ln lo ok lq lr ky ol kz lt lb om lc lv le on lf lx ly bi translated">推特机器人</h1><p id="0551" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">Script Buddy也作为tweepy开发的机器人在<a class="ae jg" href="https://twitter.com/script_buddy" rel="noopener ugc nofollow" target="_blank"> twitter </a>上运行。该机器人每天都会在推特上发布几次随机脚本样本。如果你想跟上机器人作为编剧的萌芽生涯，请随意关注。</p><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="pt oa l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">剧本伙伴的家庭情节剧</p></figure><h1 id="fff1" class="lh li jj bd lj lk ll lm ln lo lp lq lr ky ls kz lt lb lu lc lv le lw lf lx ly bi translated">包裹</h1><p id="4713" class="pw-post-body-paragraph lz ma jj mb b mc md kt me mf mg kw mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">希望这有助于你理解如何微调变形金刚包。</p><p id="3007" class="pw-post-body-paragraph lz ma jj mb b mc mw kt me mf mx kw mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我发现这个项目最有趣的地方是这个模型开始聚焦于剧本的一般结构的速度有多快。我想这很大程度上是因为电影剧本的格式在整个行业都是标准化的，因此为这个模型提供了大量可以优化的例子。展望未来，我希望在更大版本的GPT-2上训练模型，并将随着模型的改进更新机器人，希望脚本巴迪的巨著就在眼前。</p></div></div>    
</body>
</html>