<html>
<head>
<title>Understanding Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e?source=collection_archive---------13-----------------------#2020-08-31">https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e?source=collection_archive---------13-----------------------#2020-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a9f1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性回归背后的数学详细解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/81138d249163b51e15ccd933d05238c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TuxJQlb0P6v3kq8soxKysw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a62b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设你想从网上商店买一台新电脑(你最感兴趣的是它有多少内存)，你在首页上看到一些 4GB 的电脑售价 100 美元，还有一些 16 GB 的售价 1000 美元。你的预算是 500 美元。因此，你在脑子里估计，根据你目前看到的价格，一台 8 GB 内存的电脑应该在 400 美元左右。这将符合您的预算，并决定购买一台 8 GB 内存的电脑。</p><p id="e587" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种估计几乎可以在你的大脑中自动发生，而不知道它被称为线性回归，也不需要在你的大脑中明确计算回归方程(在我们的例子中:y = 75x - 200)。</p><p id="05b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，线性回归是什么<em class="lu">？</em></p><p id="c46f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将尝试简单地回答这个问题:</p><blockquote class="lv lw lx"><p id="9afd" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">线性回归就是根据一些已知的量来估计一个未知量的过程(这是回归部分)，条件是未知量可以通过只使用标量乘法和加法(这是线性部分)这两种运算从已知量中获得。我们将每个已知量乘以某个数，然后将所有这些项相加，得到未知量的估计值。</p></blockquote><p id="aacd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当它以正式的数学方式或代码描述时，它可能看起来有点复杂，但事实上，如上所述的简单估计过程，你可能在听到机器学习之前就已经知道了。只是你不知道这叫线性回归。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="330e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们深入线性回归背后的数学。</p><p id="d4d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在线性回归中，我们获得未知变量的估计值(用 y 表示；我们的模型的输出)通过计算我们的已知变量(用 xᵢ表示；输入)，我们向其添加了偏置项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/325e6eb6d8b4acb33b4c372e3a85e01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*hZF7OFTZkbyQv3hRfMJrSA.png"/></div></figure><p id="6918" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu"> n </strong>是我们拥有的数据点的数量。</p><p id="3f0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">添加偏差与想象我们有一个始终为 1 的额外输入变量并且只使用权重是一样的。我们将考虑这种情况，使数学符号变得简单一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/7fd0d056bf4ed3f26799e32fab62d2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*KtR-7yZUlgwl79lqL9aPRw.png"/></div></figure><p id="8537" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu"> x₀ </strong>永远是 1，<strong class="la iu"> w₀ </strong>是我们之前的 b</p><p id="e10a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使记法简单一点，我们将从上面的和记法过渡到矩阵记法。上述等式中的加权和相当于所有输入变量的行向量与所有权重的列向量的乘积。那就是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/85de3b179d55e1d08414e7284b84dd13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yTBp3Qj37E097lKWFAmIg.png"/></div></div></figure><p id="2aa6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的等式只针对一个数据点。如果我们想一次计算更多数据点的输出，我们可以将输入行连接成一个矩阵，用<strong class="la iu"> X </strong>表示。对于所有这些不同的输入行，权重向量将保持不变，我们将用<strong class="la iu"> w </strong>表示。现在<strong class="la iu"> y </strong>将用于表示具有所有输出的列向量，而不仅仅是单个值。这个新方程的矩阵形式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c02aabf43ab0bd410a76316aeaec75e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Lh_zWOlL33MiMUVliZtf9Q.png"/></div></figure><p id="47e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定一个输入矩阵<strong class="la iu"> X </strong>和一个权重向量<strong class="la iu"> w </strong>，我们可以使用上面的公式很容易地得到<strong class="la iu"> y </strong>的值。假设输入值是已知的，或者至少是容易获得的。</p><p id="82dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但问题是:我们如何获得权重向量？</p><p id="0b81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将从例子中学习它们。为了学习权重，我们需要一个数据集，其中我们知道 x 和 y 值，基于这些我们将找到权重向量。</p><p id="ef47" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们的数据点是定义我们的回归线所需的最小值(比输入数多 1)，那么我们可以简单地求解方程(1)得到<strong class="la iu"> w </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/85da2b5ec9b0bc9ad17f550c6e487ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*j-3Hv784ABNCwOmoJ2nr4g.png"/></div></figure><p id="46ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们称这个东西为回归线，但实际上，它是一条只针对 1 个输入的线。对于 2 个输入，它将是一个平面，对于 3 个输入，它将是某种“3D 平面”，等等。</p><p id="e20c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数情况下，上述解决方案的要求并不成立。大多数时候，我们的数据点不会完全符合一条直线。我们的回归线周围会有一些随机噪声，我们将无法获得<strong class="la iu"> w </strong>的精确解。然而，我们将尝试为<strong class="la iu"> w </strong>获得<em class="lu">的最佳可能解</em>，以使误差最小。</p><p id="bc8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果等式(1)无解，这意味着<strong class="la iu"> y </strong>不属于<strong class="la iu"> X </strong>的列空间。因此，我们将使用<strong class="la iu"> y </strong>在<strong class="la iu"> X </strong>的列空间上的投影，而不是<strong class="la iu"> y </strong>。这是离<strong class="la iu"> y </strong>最近的向量，也属于<strong class="la iu"> X </strong>的列空间。如果我们将等式两边相乘。(1)通过<strong class="la iu"> X </strong>的转置，我们将得到一个考虑了这个投影的方程。你可以在麻省理工学院的吉尔伯特·斯特朗的讲座中找到更多关于解决这个问题的线性代数方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/227cc4733c6083060f2416416e9b6fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRQLCuF37-VSi2OJs90erA.png"/></div></div></figure><p id="10fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这个解决方案对<strong class="la iu"> X </strong>的限制比我们之前的方案要少，但是在某些情况下还是不行；我们将在下面看到更多关于这个问题的内容。</p><p id="bfaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一种获得 w 的方法是使用微积分。主要思想是定义一个误差函数，然后用微积分找到最小化这个误差函数的权重。</p><p id="48af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将定义一个函数<strong class="la iu"> f </strong>,它将一个权重向量作为输入，并给出这些权重将在我们的线性回归问题上产生的平方误差。该函数只是查看数据集的每个真实 y 值与回归模型的估计 y 值之间的差异。然后将所有这些差值平方并相加。在矩阵符号中，这个函数可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/6b18cdd7a43f164c76ac41020df44a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RX9YbB93lqTXmiIfvbFSg.png"/></div></div></figure><p id="b6c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果这个函数有最小值，它应该在临界点之一(梯度<strong class="la iu"> ∇f </strong>为 0 的点)。所以，我们来找临界点。如果你不熟悉矩阵微分，可以看看<a class="ae mk" href="https://en.wikipedia.org/wiki/Matrix_calculus" rel="noopener ugc nofollow" target="_blank">这篇</a>维基百科的文章。</p><p id="9988" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从计算梯度开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/e5d67245d510a257b0fb1852720dcc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7sNq59gwU-ui1fSqqXGwQ.png"/></div></div></figure><p id="5947" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们将其设为 0，并求解<strong class="la iu"> w </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/afe776368c891799fffd4a67f725546a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZAI7t1XRikRDHIV8HUMHw.png"/></div></div></figure><p id="b2db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有一个关键点。现在我们应该弄清楚它是最小值还是最大值。为此，我们将计算<em class="lu"> Hessian 矩阵</em>并建立函数<strong class="la iu"> f </strong>的凸凹性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/fe9d109c3bcba7ec45d7b45f9a6516f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3_Tjk6I6YueTlETOSFGmA.png"/></div></div></figure><p id="9d3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们可以观察到关于<strong class="la iu"> H </strong>的什么？如果我们取任意实值向量<strong class="la iu"> z </strong>并将其乘以<strong class="la iu"> H </strong>的两边，我们将得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/44576e6ddcc38336f996d6b7e6a0c930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7V5DMl5_G3a4hSoQgg5_XA.png"/></div></div></figure><p id="3a2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为<strong class="la iu"> f </strong>是一个凸函数，这意味着我们上面找到的<strong class="la iu"> w </strong>的解是一个极小点，这正是我们要找的。</p><p id="3e11" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你可能注意到的，我们通过使用前面的线性代数方法和这种寻找权重的微积分方法得到了相同的解决方案。我们可以认为它要么是我们用<strong class="la iu"> y </strong>到<strong class="la iu"> X </strong>的列空间上的投影代替<strong class="la iu"> y </strong>时矩阵方程的解，要么是使误差平方和最小的点。</p><p id="fe63" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种解决方案总是有效吗？号码</p><p id="a7a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">比平凡解限制少:<strong class="la iu"> w = X </strong> ⁻ <strong class="la iu"> y </strong>其中我们需要<strong class="la iu"> X </strong>为方阵非奇异矩阵，但还是需要一些条件才能成立。我们需要<strong class="la iu"> Xᵀ X </strong>是可逆的，为此<strong class="la iu"> <em class="lu"> X </em> </strong> <em class="lu">需要有完整的列秩</em>；也就是说，它的所有列都是线性独立的。如果我们的行数比列数多，通常就满足了这个条件。但是如果我们的数据例子比输入变量少，这个条件就不可能成立。</p><p id="c763" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个<strong class="la iu"> X </strong>具有满列秩的要求与<strong class="la iu"> f </strong>的凸性密切相关。如果你看上面那个关于<strong class="la iu"> f </strong>是凸的小证明，你会注意到，如果<strong class="la iu"> X </strong>具有满列秩，那么<strong class="la iu"> X z </strong>不可能是零向量(假设<strong class="la iu"> z ≠ 0 </strong>)，这意味着<strong class="la iu"> H </strong>是正定的，因此<strong class="la iu"> f </strong>是严格<em class="lu">凸的。如果<strong class="la iu"> f </strong>是严格凸的，那么它只能有一个最小点，这就解释了为什么在这种情况下我们可以有一个封闭形式的解。</em></p><p id="9318" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，如果<strong class="la iu"> X </strong>没有满列秩，那么会有一些<strong class="la iu"> z ≠ 0 </strong>，对于这些<strong class="la iu"> X z = 0 </strong>，因此<strong class="la iu"> f </strong>是非严格凸的。这意味着<strong class="la iu"> f </strong>可能没有单一的最小点，但是有一个同样好的最小点的山谷，我们的封闭形式的解决方案不能捕获所有的最小点。视觉上，未满列秩的情况在 3D 中看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/f8863b749be305f9c9b2fabcb8e22429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*gjEd_JjpfKsRg5moH_Ii9g.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mk" href="https://www.geogebra.org/3d" rel="noopener ugc nofollow" target="_blank"> GeoGebra </a></p></figure><p id="4a63" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种即使在这种情况下也能给我们解决方案的方法是<em class="lu">随机梯度下降</em> (SGD)。这是一种迭代方法，从误差函数<strong class="la iu"> f </strong>表面上的随机点开始，然后在每次迭代中，沿着梯度<strong class="la iu"> ∇f </strong>的负方向向谷底前进。</p><p id="f7d7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个方法总会给我们一个结果(即使有时候需要大量迭代才能水落石出)；在<strong class="la iu"> X </strong>上不需要任何条件。</p><p id="a175" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，为了提高计算效率，它不会一次使用所有数据。我们的数据矩阵<strong class="la iu"> X </strong>被垂直分割成批。在每次迭代中，仅基于一个这样的批次进行更新。</p><p id="d4d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在列秩<strong class="la iu"> X </strong>不全的情况下，解不会唯一；在“最小谷”中的所有点中，SGD 将只给出一个依赖于随机初始化和批次随机化的点。</p><p id="5b9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SGD 是一种更通用的方法，它不仅仅局限于线性回归；它还用于更复杂的机器学习算法，如神经网络。但在最小二乘线性回归的情况下，我们的优势在于，由于误差函数的凸性，SGD 不会陷入局部最小值，而这在神经网络中是常见的情况。当这种方法将达到最小值时，它将是一种全球性的方法。下面是该算法的简要概述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/803006022f56db7fc7f8cddfcbc20828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXNS56MIal35_Bj9k61GXw.png"/></div></div></figure><p id="6604" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu"> α </strong>是一个常数，称为<em class="lu">学习率</em>。</p><p id="fdfb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果我们插入本文中上面计算的梯度，我们会得到下面专门针对最小二乘线性回归的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/10ce771fffe67668a5488a8a04325a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdsoOAIddbg2TEUdPNKuhw.png"/></div></div></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="ae8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">暂时就这样了。在接下来的两篇文章中，我还将展示如何使用一些数值库(如 NumPy、TensorFlow 和 PyTorch)实现线性回归。</p><div class="mm mn gp gr mo mp"><a rel="noopener follow" target="_blank" href="/how-to-implement-linear-regression-with-numpy-172790d2f1bc"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">如何用 NumPy 实现线性回归</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">更好地理解线性回归并提高您的数字技能</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">towardsdatascience.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd ks mp"/></div></div></a></div><div class="mm mn gp gr mo mp"><a href="https://medium.com/towards-artificial-intelligence/how-to-implement-linear-regression-with-tensorflow-406b2cff1ffa" rel="noopener follow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">如何用 TensorFlow 实现线性回归</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">通过实施线性回归学习张量流基础知识</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">medium.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd ks mp"/></div></div></a></div><div class="mm mn gp gr mo mp"><a rel="noopener follow" target="_blank" href="/how-to-implement-linear-regression-with-pytorch-5737339296a6"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">如何用 PyTorch 实现线性回归</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">通过实现线性回归学习 PyTorch 基础知识</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">towardsdatascience.com</p></div></div><div class="my l"><div class="nf l na nb nc my nd ks mp"/></div></div></a></div></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="1d83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">我希望这些信息对你有用，感谢你的阅读！</em></p><p id="fa57" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae mk" href="https://www.nablasquared.com/understanding-linear-regression/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>