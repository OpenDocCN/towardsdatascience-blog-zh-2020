<html>
<head>
<title>Training a GAN to Sample from the Normal Distribution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练GAN从正态分布中采样</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-gan-to-sample-from-the-normal-distribution-4095a11e78de?source=collection_archive---------9-----------------------#2020-01-12">https://towardsdatascience.com/training-a-gan-to-sample-from-the-normal-distribution-4095a11e78de?source=collection_archive---------9-----------------------#2020-01-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="70a7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可视化生成性对抗网络的基础</h2></div><p id="4959" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在最初的生成对抗网络<a class="ae le" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">论文</a>中，Ian Goodfellow描述了一个简单的GAN，当经过训练时，它能够生成与从正态分布中采样的样本不可区分的样本。此过程如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/de80444cefd63c302c81a92628b39c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WSabZDtvuL7chjAVEsG5ig.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图1:GAN论文中发表的第一张图，展示了GAN如何将均匀噪声映射到正态分布。黑点是真实的数据点，绿色曲线是GAN产生的分布，蓝色曲线是鉴别器对该区域中的样本是否真实的置信度。这里<strong class="bd lv"> x </strong>表示样本空间，<strong class="bd lv"> z </strong>表示潜在空间。(来源:<a class="ae le" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank"> Goodfellow 2014 </a>)</p></figure><p id="53d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此任务最简单的解决方案是让GAN逼近正态分布的<a class="ae le" href="https://en.wikipedia.org/wiki/Quantile_function" rel="noopener ugc nofollow" target="_blank">逆CDF </a>、<em class="lw">x=φ⁻(z)</em>。这是对GANs的直观而吸引人的看法:它们是在给定一组样本的情况下从未知分布中随机抽样的工具。在上面的例子中，这是正态分布。其他包括所有<a class="ae le" href="https://medium.com/intel-student-ambassadors/mnist-gan-detailed-step-by-step-explanation-implementation-in-code-ecc93b22dc60" rel="noopener">手写数字</a>的分布和所有<a class="ae le" href="https://medium.com/syncedreview/gan-2-0-nvidias-hyperrealistic-face-generator-e3439d33ebaf" rel="noopener">人脸</a>的分布。描述人脸的累积分布函数已被证明是一项艰巨的任务，这就是为什么GANs及其生成照片级人脸图像的能力(以及其他许多令人兴奋的结果)成为如此热门的话题。</p><p id="88a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，人们很容易陷入狂热，一头扎进复杂的GAN应用；大多数人从<a class="ae le" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>、<em class="lw">开始他们的GAN之旅，尽管其样本空间为784维</em>。我以前探索过将GANS应用于二维问题，但发现即使这样也太复杂了，无法恰当地形象化近似分布。因此，我决定用《甘正典》中发表的第一幅图来说明这个问题。</p><h1 id="2cb1" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">问题是</h1><p id="04ce" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">我们将训练GAN从标准正态分布N(0，1)中抽取样本。因此，样本<em class="lw"> x </em>位于从<em class="lw"> -∞ </em>到+∞的一维样本空间中。作为随机性的来源，GAN将被赋予从均匀分布U(-1，1)中提取的值。因此，值<em class="lw"> z </em>位于范围从-1到1的一维潜在空间中。因此，GAN应该接近<em class="lw">g(z)=φ⁻(f(z))</em>，使得<em class="lw"> f(z) </em>具有U(0，1)分布。<em class="lw"> f </em>的可能实现包括:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ee7c86ce303dbd1bcf657a35fcde2a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SUfMRs-kRrBfORzzfzDwBQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图2:保持一致性的[-1，1]到[0，1]的可能映射</p></figure><p id="4069" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些映射中的一个显然是“最佳”的，因为它简单且可解释。将逆CDF应用于这三个函数会产生以下结果:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ae146cb8fcc3134ffc6a8f5169e6ecef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*21I7ukzMXgwEOjEnR1S3KA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图3:图2 中描述的三个函数的x =<em class="mv">φ⁻(f(z))</em></p></figure><p id="9946" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这三种映射都将<em class="lw"> z~U(-1，1) </em>转换为<em class="lw"> x~N(0，1) </em>，但是蓝色曲线是迄今为止最简单和最容易解释的。不幸的是，众所周知，神经网络不关心这些有价值的特征。</p><h1 id="1174" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">解决方案</h1><p id="f10a" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">一个简单的、完全连接的GAN被训练600个训练步骤(代码如下)。在每一步之后，发生器和鉴别器的测量结果都是可视化的，如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f2b0a7740ecc045048d71d2527e3c7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*xCyToMbLKUn880RDBy5X7w.gif"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图4:GAN学习从600个时期的标准正态分布采样的动画。(左上)发生器和鉴别器的精度和损耗(右上)GAN的观测概率密度和真实N(0，1)密度(左下)潜在空间到样本空间的GAN映射和真实映射<em class="mv">φ⁻((z+1)/2)(右下)</em>鉴别器对样本空间的该区域中的值为真实值的置信度</p></figure><h2 id="8c09" class="mx ly it bd lz my mz dn md na nb dp mh kr nc nd mj kv ne nf ml kz ng nh mn ni bi translated">左上角:指标</h2><p id="d538" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">生成器和鉴别器都使用二进制交叉熵损失(或对数损失)，这在这里与精度一起显示。在步骤200之后，两个网络在剩余的训练中基本上保持同步。众所周知，发电机损耗与GAN输出质量无关，但它仍然是一个有用的跟踪指标；如果发生器或鉴别器明显优于另一个，那么GAN将可能无法收敛。</p><h2 id="057f" class="mx ly it bd lz my mz dn md na nb dp mh kr nc nd mj kv ne nf ml kz ng nh mn ni bi translated">右上角:概率密度</h2><p id="8591" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">右上面板显示GAN的观测概率密度与标准正态分布的真实概率密度。理想情况下，当训练结束时，这两个人会准确地排成一行。正如你所看到的，有一个强烈的波动行为，因为甘反复过校正。</p><h2 id="ab61" class="mx ly it bd lz my mz dn md na nb dp mh kr nc nd mj kv ne nf ml kz ng nh mn ni bi translated">左下方:已学习的映射</h2><p id="ea55" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">左下方的面板显示了本实验中使用的真实映射，以及GAN学习到的映射。正如你所看到的，GAN正在逼近非直观映射<em class="lw">φ⁻((1-z)/2)</em>，并取得了成功。</p><h2 id="8ec7" class="mx ly it bd lz my mz dn md na nb dp mh kr nc nd mj kv ne nf ml kz ng nh mn ni bi translated">右下角:鉴别器置信度</h2><p id="a0f8" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">此面板显示了鉴别器对样本空间中该点的值为实数的置信度(即，从实数分布中提取，而不是GAN)。从动画中可以立即看出两个观点。首先，鉴别器在识别整个样本空间中真实样本和虚假样本的相对频率方面相当有效；当真实样本多于虚假样本时，可信度增加，反之亦然。第二个观察是样本空间的极端，真实样本很少，显示出巨大的信心波动。这反映在右上图中，在该图中，极端区域中置信度的增加实际上促进了该区域中的样品生成(因此产生了波动行为)。</p><h1 id="4b54" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结果:</h1><p id="a59a" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">GAN总共被训练了30000步，并学会产生以下功能:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nj"><img src="../Images/49a5d4796124ec347a372eb129e8333a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRPEfOkTJ1NHmAi-hPooZg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图5:30000步训练后的GAN</p></figure><p id="50a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">粗略地看一下GAN的输出，它确实呈现正态分布。然而，通过应用核密度估计(图5，右上方)，我们看到该分布只是一个粗略的近似值。这个缺点在1维设置中是明显的，在1维设置中，分布可以容易地可视化，但是识别“真实的”面部生成函数和由GAN学习的面部生成函数之间的偏差是一个困难得多的问题；事实上，在评估生成图像的GANs时，我们经常看到它们生成各种各样的真实样本，并认为这就足够了。确保GAN输出和鉴频器的中间激活具有与真实样本相似的统计特性是一个活跃的研究领域。</p><p id="34df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">左下角的面板显示GAN已经学习了从潜在空间到样本空间的非直觉映射，这是在更高维度问题中呈指数增长的问题。这是不幸的，原因有二:首先，理想的GAN的输入应该与有用的、人类可理解的特征相关联。在这种情况下，这将是“增加潜在价值增加样本价值”，而在实践中，我们发现相反的情况发生。在生成人脸的情况下，可能是“增加这个潜在特征增加鼻子尺寸”或者“增加那个潜在特征增加头发亮度”。相反，典型地，潜在特征被映射到不止一个样本特征，例如鼻子尺寸<em class="lw">和</em>头发颜色，或者一个样本特征被映射到多个潜在特征。这使得将GANs应用于创造性任务变得困难。</p><p id="0c12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个问题是，GAN可以学习将潜在空间中的两个附近的点，比如说<em class="lw"> z=0.49 </em>和<em class="lw"> z=0.51 </em>，映射到样本空间中非常远的点(如图3中分段函数的情况)。这使得训练变得复杂，因为中间值<em class="lw"> z=0.50 </em>，将可能被映射到样本空间中的某个非常不现实的中间点。这在我以前关于同一主题的文章中已经观察到，并将在下一篇文章中进一步探讨。</p><p id="1150" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以你有它；甘的训练从来都不简单，即使是一维问题。作为结束语，我想重申我最喜欢的一条数据科学建议:<strong class="kk iu">将一切可视化，不管你认为自己了解多少。毕竟，科学上最激动人心的发现并不是来自“发现了！”而是“嗯，这太奇怪了…”</strong></p><p id="5b29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码可从以下github repo获得:</p><div class="nk nl gp gr nm nn"><a href="https://github.com/ConorLazarou/medium/blob/master/12020/visualizing_gan_uni2norm/1_uniform_to_normal_1D.py" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">ConorLazarou/培养基</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">可视化GAN训练，因为它学习将U(-1，1)映射到N(0，1)</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">github.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob lp nn"/></div></div></a></div></div></div>    
</body>
</html>