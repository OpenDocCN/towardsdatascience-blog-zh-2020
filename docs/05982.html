<html>
<head>
<title>Your Ultimate Data Mining &amp; Machine Learning Cheat Sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">您的终极数据挖掘和机器学习备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-ultimate-data-mining-machine-learning-cheat-sheet-9fce3fa16?source=collection_archive---------4-----------------------#2020-05-16">https://towardsdatascience.com/your-ultimate-data-mining-machine-learning-cheat-sheet-9fce3fa16?source=collection_archive---------4-----------------------#2020-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ce9f21095c2d03d148cb81fc80aaaf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kJhKgO1gGkKimte6.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/illustrations/neurons-brain-cells-brain-structure-1773922/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>。</p></figure><div class=""/><div class=""><h2 id="12f1" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">特性重要性、分解、转换等</h2></div><p id="e4f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本备忘单将涵盖数据挖掘和机器学习的几个领域:</p><ul class=""><li id="46bf" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">预测建模。</strong>用于监督学习(预测)的回归和分类算法，用于评估模型性能的度量。</li><li id="e90a" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">聚类。</strong>将无标签数据分组为聚类的方法:K-Means，选择基于客观度量的聚类数。</li><li id="a1d1" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">降维。</strong>降低数据和属性维数的方法:PCA和LDA。</li><li id="280d" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">特征重要性。</strong>寻找数据集中最重要特征的方法:排列重要性、SHAP值、部分相关图。</li><li id="9dd3" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">数据转换。</strong>转换数据以获得更大预测能力、更容易分析或揭示隐藏关系和模式的方法:标准化、规范化、box-cox转换。</li></ul></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="f1f3" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">预测建模</h1><p id="7bf6" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated"><strong class="la jk">训练-测试-分割</strong>是测试一个模型表现如何的重要部分，通过在指定的训练数据上训练它，并在指定的测试数据上测试它。通过这种方式，可以衡量模型归纳新数据的能力。在<code class="fe nm nn no np b">sklearn</code>中，两个列表、熊猫数据帧或NumPy数组都在<code class="fe nm nn no np b">X</code>和<code class="fe nm nn no np b">y</code>参数中被接受。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="291f" class="ny mq jj np b gy nz oa l ob oc">from sklearn.model_selection import train_test_split<br/>X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/3b6bf0f9d9bf2cd53780db4016302185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBx2D83rNUloJsq5gC_QIw.png"/></div></div></figure><p id="dfc2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">训练一个标准的监督学习模型</strong>采取导入、创建实例和拟合模型的形式。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/fc96edf1104672865aface484fb48ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nI6GdRcSol5js_IC5wTtbQ.png"/></div></div></figure><p id="7b85" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nm nn no np b"><strong class="la jk">sklearn</strong></code> <strong class="la jk">分类器型号</strong>如下所示，分支用蓝色突出显示，型号名称用橙色显示。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/07f14b88b870870729832f39c2b6f907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_waDH-LM5e-hl_2Qdrv6Q.png"/></div></div></figure><p id="6858" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nm nn no np b"><strong class="la jk">sklearn</strong></code> <strong class="la jk">回归器模型</strong>如下所示，分支用蓝色突出显示，模型名称用橙色显示。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/9a8477a4d7faa2c83e6285844baf092c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ue8DVc9CcHrIqn9dhko0Tg.png"/></div></div></figure><p id="9044" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">评估模型性能</strong>通过以下形式的列车测试数据完成:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/299c57f6def08f923a7c2aaae666eddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHoJvaA0SNzz_QM6Xi9ADQ.png"/></div></div></figure><p id="b321" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nm nn no np b"><strong class="la jk">sklearn</strong></code> <strong class="la jk">下面列出了分类和回归的指标</strong>，最常用的指标用绿色标出。在某些情况下，许多灰色指标比绿色指标更合适。每一种都有自己的优点和缺点，平衡优先级比较、可解释性和其他因素。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/c637ba8f187b2082d665d89df0d3138c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypc0PNKXsDlJR-1AaKnOeQ.png"/></div></div></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="8d6a" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">使聚集</h1><p id="691e" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">在聚类之前，需要对数据进行标准化(这方面的信息可以在数据转换一节中找到)。聚类是基于点距离创建聚类的过程。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/fdf3c3e0daa6aa7e591d6fb5755755ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ibd-a_gS3Mg_6-PT.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://i.imgur.com/S65Sk9c.jpg" rel="noopener ugc nofollow" target="_blank">来源</a>。图片免费分享。</p></figure><p id="4da3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">训练和创建K-Means聚类模型</strong>创建一个可以聚类和检索关于聚类数据的信息的模型。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="1a57" class="ny mq jj np b gy nz oa l ob oc">from sklearn.cluster import KMeans<br/>model = KMeans(n_clusters = number_of_clusters)<br/>model.fit(X)</span></pre><p id="4b25" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">访问数据中每个数据点的标签</strong>可以通过:</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="7432" class="ny mq jj np b gy nz oa l ob oc">model.labels_</span></pre><p id="e5d6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，每个数据点的标签可以存储在一列数据中，其中:</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="b8b3" class="ny mq jj np b gy nz oa l ob oc">data['Label'] = model.labels_</span></pre><p id="7307" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">访问新数据的簇标签</strong>可通过以下命令完成。<code class="fe nm nn no np b">new_data</code>可以是数组、列表或数据帧的形式。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="167f" class="ny mq jj np b gy nz oa l ob oc">data.predict(new_data)</span></pre><p id="1096" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">访问每个聚类的聚类中心</strong>以二维数组的形式返回:</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="c325" class="ny mq jj np b gy nz oa l ob oc">data.cluster_centers_</span></pre><p id="0984" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了<strong class="la jk">找到聚类的最佳数量</strong>，使用剪影得分，这是一个衡量特定数量的聚类与数据拟合程度的指标。对于预定义范围内的每个聚类数，训练K-Means聚类算法，并将其轮廓分数保存到列表中(<code class="fe nm nn no np b">scores</code>)。<code class="fe nm nn no np b">data</code>是模型被训练的<code class="fe nm nn no np b">x</code>。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="0652" class="ny mq jj np b gy nz oa l ob oc">from sklearn.metrics import silhouette_score</span><span id="671b" class="ny mq jj np b gy ok oa l ob oc">scores = []<br/>for cluster_num in range(lower_bound, upper_bound):<br/>     model = KMeans(n_clusters=cluster_num)<br/>     model.fit(data)<br/>     score = silhouette_score(data, model.predict(data))</span></pre><p id="bcd3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分数保存到列表<code class="fe nm nn no np b">scores</code>后，可以用图表显示或通过计算搜索找到最高的分数。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="7cf1" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">降维</h1><p id="0528" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">降维是用减少的维数表达高维数据的过程，使得每个维数包含最多的信息。降维可用于高维数据的可视化，或通过移除低信息或相关特征来加速机器学习模型。</p><p id="0ba0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">主成分分析</strong>或PCA是一种流行的方法，通过在特征空间中绘制几个正交(垂直)向量来表示减少的维数，从而减少数据的维数。变量<code class="fe nm nn no np b">number</code>表示缩减后的数据将具有的维数。例如，在可视化的情况下，它将是二维的。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/fb97872bc31718729b2453c5e530c455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2VjanmuMIrpFPUqd.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">PCA工作原理的可视化演示。<a class="ae jg" href="https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.09-PCA-rotation.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c29e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">拟合PCA模型</strong>:函数<code class="fe nm nn no np b">.fit_transform</code>自动将模型与数据拟合，并将其转换为减少的维数。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="a16a" class="ny mq jj np b gy nz oa l ob oc">from sklearn.decomposition import PCA</span><span id="d4ca" class="ny mq jj np b gy ok oa l ob oc">model = PCA(n_components=number)<br/>data = model.fit_transform(data)</span></pre><p id="56af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">解释差异率</strong>:调用<code class="fe nm nn no np b">model.explained_variance_ratio_</code>会产生一个列表，列表中的每一项都对应该维度的“解释差异率”，本质上就是该维度所代表的原始数据中的信息百分比。解释的方差比之和是保留在降维数据中的信息的总百分比。</p><p id="fa81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> PCA特征权重</strong>:在PCA中，每个新创建的特征都是以前数据特征的线性组合。这些<strong class="la jk"> </strong>线性权重可以通过<code class="fe nm nn no np b">model.components_</code>访问，并且是特征重要性的良好指示器(较高的线性权重表示该特征中表示的信息更多)。</p><p id="c74f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">线性判别分析</strong> (LDA，不要和潜在的狄利克雷分配混淆)是另一种降维方法。LDA和PCA的主要区别在于LDA是一种监督算法，这意味着它同时考虑了<em class="om"> x </em>和<em class="om"> y </em>。主成分分析只考虑<em class="om"> x </em>，因此是一种无监督算法。</p><p id="decd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PCA试图完全基于点与点之间的距离来保持数据的结构(方差),而LDA优先考虑类的清晰分离。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="ed85" class="ny mq jj np b gy nz oa l ob oc">from sklearn.decomposition import LatentDirichletAllocation<br/>lda = LatentDirichletAllocation(n_components = number)<br/>transformed = lda.fit_transform(X, y)</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="1af4" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">特征重要性</h1><p id="df5a" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">特征重要性是找到对目标最重要的特征的过程。通过主成分分析，可以找到包含最多信息的特征，但是特征重要性涉及特征对目标的影响。“重要”特征的变化会对<em class="om"> y </em>变量产生很大影响，而“不重要”特征的变化对<em class="om"> y </em>变量几乎没有影响。</p><p id="9205" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">排列重要性</strong>是评价一个特征有多重要的方法。训练了几个模型，每个模型都缺少一列。由于缺少数据而导致的模型准确性的相应降低表明了该列对模型预测能力的重要性。<code class="fe nm nn no np b">eli5</code>库用于排列重要性。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="22ef" class="ny mq jj np b gy nz oa l ob oc">import eli5<br/>from eli5.sklearn import PermutationImportance<br/>model = PermutationImportance(model)<br/>model.fit(X,y)<br/>eli5.show_weights(model, feature_names = X.columns.tolist())</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/0828afd8c1f42d4ccfe548c7f6911c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2SM0n2j60rqpry--B6bSw.png"/></div></div></figure><p id="f6f6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个排列重要性模型被训练的数据中，列<code class="fe nm nn no np b">lat</code>对目标变量(在这个例子中是房价)具有最大的影响。为了获得最佳预测性能，在决定移除模型中的哪些要素(实际上混淆模型的相关或冗余要素，由负的置换重要性值标记)时，置换重要性是最佳的使用要素。</p><p id="5bd4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> SHAP </strong>是另一种评估功能重要性的方法，借用21点中的博弈论原理来估算玩家能贡献多少价值。与排列重要性不同，<strong class="la jk">SH</strong>apley<strong class="la jk">A</strong>d active Ex<strong class="la jk">P</strong>lanation使用更公式化和基于计算的方法来评估特性重要性。SHAP需要一个基于树的模型(决策树，随机森林)，并适应回归和分类。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="0cd6" class="ny mq jj np b gy nz oa l ob oc">import shap<br/>explainer = shap.TreeExplainer(model)<br/>shap_values = explainer.shap_values(X)<br/>shap.summary_plot(shap_values, X, plot_type="bar")</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/edd2cdf054a1960e7ce4ebf0eb30c03d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dzP42g-lR4wEWKiLqRG1Lw.png"/></div></div></figure><p id="0fb3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> PD(P)图</strong>或部分相关图是数据挖掘和分析中的主要内容，显示一个特征的某些值如何影响目标变量的变化。所需的导入包括依赖图的<code class="fe nm nn no np b">pdpbox</code>和显示图的<code class="fe nm nn no np b">matplotlib</code>。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="2397" class="ny mq jj np b gy nz oa l ob oc">from pdpbox import pdp, info_plots<br/>import matplotlib.pyplot as plt</span></pre><p id="9ccb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">隔离的PDP</strong>:下面的代码显示了部分依赖图，其中<code class="fe nm nn no np b">feat_name</code>是<code class="fe nm nn no np b">X</code>内将被隔离并与目标变量进行比较的特征。第二行代码保存数据，而第三行构建画布来显示绘图。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="c4e0" class="ny mq jj np b gy nz oa l ob oc">feat_name = 'sqft_living'<br/>pdp_dist = pdp.pdp_isolate(model=model, <br/>                           dataset=X, <br/>                           model_features=X.columns,<br/>                           feature=feat_name)<br/>pdp.pdp_plot(pdp_dist, feat_name)<br/>plt.show()</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/469c7d568f206f153ca654f67238c6f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFUfOBywd3NYMgoQPNvhmA.png"/></div></div></figure><p id="aefb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">部分相关图显示了某些价值和居住空间平方英尺数的变化对房价的影响。阴影区域代表置信区间。</p><p id="513a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">等高线PDP</strong>:部分相关图也可以采用等高线图的形式，比较的不是一个孤立变量，而是两个孤立变量之间的关系。要比较的两个特征存储在变量<code class="fe nm nn no np b">compared_features</code>中。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="98c7" class="ny mq jj np b gy nz oa l ob oc">compared_features = ['sqft_living', 'grade']</span><span id="941e" class="ny mq jj np b gy ok oa l ob oc">inter = pdp.pdp_interact(model=model, <br/>                          dataset=X, <br/>                          model_features=X.columns, <br/>                          features=compared_features)<br/>pdp.pdp_interact_plot(pdp_interact_out=inter,<br/>                      feature_names=compared_features),<br/>                      plot_type='contour')<br/>plt.show()</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/3019d153cb10a144c3eadaf879ebec60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4nDkuE9_S1p7NmENmumGA.png"/></div></div></figure><p id="ad50" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两个特性之间的关系显示了只考虑这两个特性时对应的价格。部分相关图充满了数据分析和发现，但是要注意大的置信区间。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="71a6" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">数据转换</h1><p id="aeb9" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated"><strong class="la jk">标准化或缩放</strong>是“重塑”数据的过程，使其包含相同的信息，但平均值为0，方差为1。通过缩放数据，算法的数学本质通常可以更好地处理数据。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="142b" class="ny mq jj np b gy nz oa l ob oc">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scaler.fit(data)<br/>transformed_data = scaler.transform(data)</span></pre><p id="2234" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nm nn no np b">transformed_data</code>是标准化的，可用于许多基于距离的算法，如支持向量机和K-最近邻算法。使用标准化数据的算法的结果需要被“去标准化”,以便它们可以被正确地解释。<code class="fe nm nn no np b">.inverse_transform()</code>可用于执行与标准变换相反的操作。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="3e05" class="ny mq jj np b gy nz oa l ob oc">data = scaler.inverse_transform(output_data)</span></pre><p id="52c4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标准化数据将数据放在0到1的范围内，类似于标准化数据，使数据在数学上更容易用于模型。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="5656" class="ny mq jj np b gy nz oa l ob oc">from sklearn.preprocessing import Normalizer<br/>normalize = Normalizer()<br/>transformed_data = normalize.fit_transform(data)</span></pre><p id="2895" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然规范化不会像标准化那样改变数据的形状，但它限制了数据的边界。是否规范化或标准化数据取决于算法和上下文。</p><p id="9962" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> Box-cox变换</strong>涉及对数据进行各种幂次变换。Box-cox变换可以规范化数据，使其更加线性，或者降低复杂性。这些转换不仅涉及到数据的幂，还涉及到分数幂(平方根)和对数。</p><p id="bade" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，考虑沿着函数<em class="om"> g </em> ( <em class="om"> x </em>)的数据点。通过应用对数box-cox变换，可以容易地用线性回归对数据建模。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/d3a8674b1d2cdce704efa56399e991d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCdidQ4i1sprrV4gXEFJQw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">和德斯莫斯一起创造的。</p></figure><p id="51e8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nm nn no np b">sklearn</code>自动确定应用于数据的最佳box-cox变换系列，使其更接近正态分布。</p><pre class="nq nr ns nt gt nu np nv nw aw nx bi"><span id="342c" class="ny mq jj np b gy nz oa l ob oc">from sklearn.preprocessing import PowerTransformer<br/>transformer = PowerTransformer(method='box-cox')<br/>transformed_data = transformer.fit_transform(data)</span></pre><p id="da5a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于box-cox变换平方根的性质，box-cox变换的数据必须是严格正的(在hand可以处理这一点之前对数据进行归一化)。对于具有负数据点和正数据点的数据，通过类似的方法设置<code class="fe nm nn no np b">method = ‘yeo-johnson’</code>,使数据更接近钟形曲线。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="1839" class="mp mq jj bd mr ms mt mu mv mw mx my mz kp na kq nb ks nc kt nd kv ne kw nf ng bi translated">感谢阅读！</h1><p id="a9dd" class="pw-post-body-paragraph ky kz jj la b lb nh kk ld le ni kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">如果你觉得有帮助的话，一定要把这一页加入书签以便参考。通常，数据挖掘和分析需要可视化——请随意查看另一个可视化备忘单。当您创建可视化和执行机器学习操作时，您可能想看看数据操作和清理备忘单。</p><div class="is it gp gr iu os"><a href="https://medium.com/@andre_ye/your-ultimate-data-science-statistics-mathematics-cheat-sheet-d688a48ad3db" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">你的终极数据科学统计和数学小抄</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">机器学习指标、统计指标等</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">medium.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a href="https://medium.com/analytics-vidhya/your-ultimate-python-visualization-cheat-sheet-663318470db" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">您的最终Python可视化备忘单</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">这个备忘单包含了你最常需要的一个情节的要素，以一种清晰和有组织的方式，用…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">medium.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a href="https://medium.com/@andre_ye/your-ultimate-data-manipulation-cleaning-cheat-sheet-731f3b14a0be" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">您的终极数据操作和清理备忘单</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">解析日期、输入、异常检测等等</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">medium.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ja os"/></div></div></a></div></div></div>    
</body>
</html>