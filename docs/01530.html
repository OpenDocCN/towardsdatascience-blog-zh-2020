<html>
<head>
<title>How does the Bellman equation work in Deep RL?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝尔曼方程在深度RL中是如何工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a?source=collection_archive---------11-----------------------#2020-02-11">https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a?source=collection_archive---------11-----------------------#2020-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e5e5" class="pw-subtitle-paragraph jr is it bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">贝尔曼方程和神经网络之间的联系，有公式、例子和Python代码</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/6a24e495cbf9fa063aa9f9a19d159154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJcVGkCqMBUK4DBthyy6PA.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">资料来源:123rf.com</p></figure><p id="df0c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="lv">贝尔曼方程</em>中，价值函数<em class="lv">φ</em>(<em class="lv">t</em>)取决于价值函数<em class="lv">φ</em>(<em class="lv">t</em>+1)。<strong class="lb iu"> </strong>尽管如此，在状态达到时间<em class="lv"> t </em> +1之前，还是可以得到<em class="lv">φ</em>(<em class="lv">t</em>)的值。我们可以通过<em class="lv">神经网络</em>、<strong class="lb iu">T21、</strong>来实现，因为<strong class="lb iu">、<em class="lv">、</em>、</strong>它们可以在任何时间近似函数<em class="lv">φ</em>(<em class="lv">t</em>)<strong class="lb iu">T33<em class="lv">t</em>。我们将看到它在Python中的外观。在最后两节中，我们介绍了<em class="lv">深度Q学习</em>算法的实现以及使用<em class="lv"> PyTorch </em>软件包进行张量计算的一些细节。</strong></p><h1 id="9da2" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><em class="jq">状态、动作和政策图</em></h1><p id="982c" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">状态空间和行动空间</strong></p><p id="1763" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">马尔可夫决策过程</em> ( <em class="lv"> MDP </em>)为<em class="lv">深层强化学习</em> ( <em class="lv"> RL </em>或<em class="lv">深层RL </em>)提供了数学框架。对于现实问题，我们在<em class="lv"> MDP </em>中定义了以下参数:{ <strong class="lb iu"> <em class="lv"> S </em> </strong> <em class="lv">、</em><strong class="lb iu">T63【a64】、</strong>、<em class="lv">、</em>T68<em class="lv">R</em>、、、<strong class="lb iu">T75P</strong>、<em class="lv"> <strong class="lb iu">T93</strong></em><em class="lv">T97<em class="lv">是动作空间</em> <strong class="lb iu">、<em class="lv"> R </em> </strong>、<em class="lv">T105</em>是设定<em class="lv">奖励</em>、<em class="lv">P</em>T111是设定<em class="lv">概率</em>、<strong class="lb iu">T115</strong></em>γ</p><blockquote class="mt"><p id="4381" class="mu mv it bd mw mx my mz na nb nc lu dk translated">而在计算机视觉中，代理从<em class="jq">大量的图像</em>中学习，在<em class="jq">深层RL </em>中的代理从<em class="jq">大量的发作</em>中学习，其中对于任何状态<em class="jq">、</em>代理<strong class="ak">探究</strong>几个动作并从MDP环境接收不同的回复(奖励)。</p></blockquote><p id="88ac" class="pw-post-body-paragraph kz la it lb b lc nd jv le lf ne jy lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如果每个状态只有一个动作，并且所有奖励都相同，那么<em class="lv"> MDP </em>就是<em class="lv"> </em>简化为<em class="lv">马尔可夫链</em>。对于大量的MDP环境，参见OpenAI/gym的<a class="ae ni" href="https://github.com/openai/gym/wiki/Table-of-environments" rel="noopener ugc nofollow" target="_blank">环境表</a>。</p><p id="621f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ni" href="https://github.com/openai/gym/wiki/CartPole-v0" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">翻筋斗</strong> </a> <strong class="lb iu">或倒立摆</strong></p><p id="a743" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP环境的一个例子是<em class="lv">cart pole(</em>a . k . a .<a class="ae ni" href="https://en.wikipedia.org/wiki/Inverted_pendulum" rel="noopener ugc nofollow" target="_blank">an<em class="lv">倒立摆</em> </a> <em class="lv">)，</em>一个重心在支点上方的钟摆。不稳定，但是可以通过移动重心下的支点来控制<a class="ae ni" href="https://www.youtube.com/watch?time_continue=72&amp;v=XiigTGKZfks" rel="noopener ugc nofollow" target="_blank"/>。对于环境<a class="ae ni" href="https://github.com/openai/gym/wiki/CartPole-v0" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> CartPole-v0 </em> </a>，状态和动作如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f1dc6db8e5f4b9b19949f1148e9e1b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*XFgZfZg41RHAXHl8BlBLIw.jpeg"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">钢管混凝土柱的状态空间和动作空间</p></figure><p id="4e90" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这种环境，状态空间的维度=4，类型为Box(4)。动作空间的维度=2，类型为离散(2)。</p><p id="470a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">健身房的类型<a class="ae ni" href="https://gym.openai.com/docs/#spaces" rel="noopener ugc nofollow" target="_blank">空间</a>:</p><ul class=""><li id="e41b" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">gym.spaces.Discrete(n)</strong></code>:从0到n-1的离散值。</li><li id="0a30" class="nk nl it lb b lc nx lf ny li nz lm oa lq ob lu np nq nr ns bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">gym.spaces.Box</strong></code>:数值的多维向量，每一维的上下界由<code class="fe nt nu nv nw b"><strong class="lb iu">Box.low</strong></code>和<code class="fe nt nu nv nw b"><strong class="lb iu">Box.high</strong></code>定义。</li></ul><p id="01ef" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">政策地图<em class="lv"> </em> 𝜋，确定性</strong> <strong class="lb iu">和</strong> <strong class="lb iu">随机政策</strong></p><p id="d3f4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">策略图</em> <strong class="lb iu"> 𝜋 </strong>定义为<strong class="lb iu">𝜋</strong>(a | s)=<strong class="lb iu">pr</strong>{<em class="lv">at = a</em>|<em class="lv">ST = s</em>}意思是策略<strong class="lb iu"> 𝜋 </strong>是在状态<strong class="lb iu"><em class="lv"/></strong>下执行的动作<em class="lv"/>的概率</p><p id="69de" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">举例</strong>。考虑配备手臂来抓取罐子的回收机器人。状态空间<strong class="lb iu"> <em class="lv"> S </em> </strong> =[ <em class="lv">低</em>，<em class="lv">高</em> ]，其中“<em class="lv">低</em>”和“<em class="lv">高</em>”是机器人充电的状态，动作空间<strong class="lb iu"> <em class="lv"> A = </em> </strong> [ <em class="lv">搜索、充电、等待</em>。我们考虑两种政策类型:<em class="lv">确定性</em>和<em class="lv">随机性</em>。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e6b0fb1ff304150bda0876d07662de34.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*rM-R3fKjEeQMEAhJAYAKJw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">确定性和随机性策略的示例</p></figure><h1 id="a1a9" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><strong class="ak"> <em class="jq">状态值函数和贝尔曼方程</em> </strong></h1><p id="71a3" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">返回值</strong></p><p id="83de" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设状态空间是<em class="lv">离散的</em>，这意味着代理以离散的时间步长与其环境进行交互。在每次<strong class="lb iu"><em class="lv"/></strong>时，代理接收到一个<em class="lv">状态</em> <strong class="lb iu"> <em class="lv">状态</em> </strong> <em class="lv"> </em>包括<em class="lv">奖励</em><strong class="lb iu"><em class="lv">rt .</em></strong><em class="lv"/>累积奖励命名为<em class="lv">返回<strong class="lb iu"> <em class="lv">，</em> </strong>我们将其表示为<strong class="lb iu"> <em class="lv"> Gt 【T6</em>未来累计<em class="lv">折现</em>奖励计算如下:</strong></em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi od"><img src="../Images/30b72e1ac625cd466ccb863ddd9a0eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3e8YXJzWW2rfICtjj7DaCA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">未来累计<em class="jq">折扣</em>奖励</p></figure><p id="086e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu"> <em class="lv"> γ </em> </strong>是<em class="lv">贴现</em>因子，0&lt;<strong class="lb iu"><em class="lv">γ</em></strong><em class="lv">&lt;</em>1。这样，<em class="lv">return</em>at time<strong class="lb iu"><em class="lv"/></strong>就可以用<strong class="lb iu"><em class="lv"/></strong>return at time<strong class="lb iu"><em class="lv"/>+1，</strong>即</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oe"><img src="../Images/e7af9eaf3395d9690e63b6d6547b9067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_JU0cEN1aoq88-wO3W15g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">返回值的递归关系</p></figure><p id="485c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是返回值<strong class="lb iu"> <em class="lv"> Gt </em> </strong>的递归关系。</p><p id="9663" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">状态值函数和贝尔曼方程</strong></p><p id="0b45" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">策略<strong class="lb iu"> 𝜋 </strong>的状态值函数定义如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi of"><img src="../Images/b1da0ee0cf7af78e9d7a4dc05d674f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTUbl6NXs4OnGBYM5vdDvA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">状态值函数</p></figure><p id="e360" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，<strong class="lb iu"> 𝜋 </strong>是<em class="lv">的预期</em> <strong class="lb iu"> </strong>为<strong class="lb iu"> <em class="lv"> Gt </em> </strong>，<strong class="lb iu"> <em class="lv"> </em> </strong>和<strong class="lb iu"><em class="lv"/></strong>𝔼<strong class="lb iu">𝜋</strong>命名为<em class="lv">预期收益</em>。通过(1)和(2)我们推导出等式。(3).这就是<a class="ae ni" href="https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equ" rel="noopener ugc nofollow" target="_blank"> <em class="lv">贝尔曼方程</em> </a> <em class="lv">。</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi og"><img src="../Images/b4cd4e307c411cedd5a27a2d7c7b1c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ITrBf__kgyN4jPZHd7M0ZA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="jq">贝尔曼方程</em></p></figure><p id="97a0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，状态值<strong class="lb iu"><em class="lv">【v _</em><em class="lv">s</em>)</strong>为状态<strong class="lb iu"/><em class="lv"/>在时间<em class="lv"/><strong class="lb iu"><em class="lv">t</em></strong><em class="lv"/>可以使用当前奖励<strong class="lb iu"><em class="lv">r _ { t+</em>1<em class="lv">}</em></strong><strong class="lb iu"><em class="lv"/>找到</strong></p><h1 id="f4d3" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><strong class="ak">行动价值函数与最优策略</strong></h1><p id="0f8d" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">动作值功能</strong></p><p id="b4fb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将定义与策略<strong class="lb iu"> 𝜋 : </strong>相关联的<em class="lv">动作值函数</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oh"><img src="../Images/ede2bc82991ad83664bcdca9f4744ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmacjrR0MBESJQecy6N8Dw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="jq">动作值功能</em></p></figure><p id="9cf7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以介绍两种政策的比较如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oi"><img src="../Images/4ccd8d6e37819ad1afa12b81573f1505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmUtEOlDdrsCbCm3A-eOIg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两种政策的比较</p></figure><p id="9489" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们说政策<strong class="lb iu">𝜋<em class="lv">’</em></strong>比政策<strong class="lb iu">𝜋.<em class="lv">好</em></strong></p><p id="ffdb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最佳状态值函数</strong></p><p id="87aa" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有必要让任何两个政策具有可比性，但是，总有一个政策优于所有其他政策。这样的政策被说成是<em class="lv">最优政策</em>，它由<strong class="lb iu"> 𝜋*.表示一个最优策略肯定存在，但可能不是唯一的。</strong></p><blockquote class="mt"><p id="598a" class="mu mv it bd mw mx my mz na nb nc lu dk translated">代理的目标是找到最优策略。寻找最优策略是<em class="jq"> Deep RL </em>的主要目标。</p></blockquote><p id="c518" class="pw-post-body-paragraph kz la it lb b lc nd jv le lf ne jy lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">不同的最优策略具有相同的价值函数，我们用<strong class="lb iu"> <em class="lv"> v*表示。</em> </strong> <em class="lv"> </em>事实上，我们有<strong class="lb iu"> <em class="lv"> v* = v( </em> 𝜋*).</strong>函数<strong class="lb iu"><em class="lv">v *</em></strong>it<strong class="lb iu"><em class="lv"/></strong>据说是<em class="lv">最优状态值函数。</em>最佳状态值函数可定义如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oj"><img src="../Images/ad23db43966b11ac9d4c36795367c8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9OL7P5dM5M4MiOzxQ9buA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">最优状态值函数</p></figure><p id="11e1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最佳动作值函数</strong></p><p id="52fc" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于任何确定性策略<strong class="lb iu">【𝜋】</strong><strong class="lb iu"/><strong class="lb iu"><em class="lv">a</em></strong>都是由当前状态<strong class="lb iu"><em class="lv">s</em></strong><strong class="lb iu"><em class="lv"/></strong>唯一确定的，即<strong class="lb iu"><em class="lv">a</em>=</strong><strong class="lb iu">𝜋(<em class="lv">s</em></strong>。然后，对于(4)中的确定性策略<strong class="lb iu"> 𝜋 </strong>，动作可以被丢弃，即，我们得到等式。(2).换句话说，对于确定性策略<strong class="lb iu"> 𝜋，</strong>我们在<em class="lv">状态值</em>函数和<em class="lv">动作值</em>函数之间有如下关系:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi od"><img src="../Images/caab883b9ed058ac2a6a63ed40ff4987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIWbVys35z_XOnOoD_6Y2w.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">确定性策略的<em class="jq">状态值</em>函数和<em class="jq">动作值</em>函数之间的关系</p></figure><p id="8402" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对于随机政策来说，情况并非如此</strong></p><p id="294c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于最佳动作值函数<strong class="lb iu"><em class="lv">v *</em>(<em class="lv">s</em>)</strong>，参见(5)，我们将最佳动作值函数<strong class="lb iu"> <em class="lv"> q* </em> ( <em class="lv"> s，</em> ) </strong>定义如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ok"><img src="../Images/c9d2b853c147fbf5b943f31cbee3c2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIwBfEd6fpwFOIlKcUDAjg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">最佳行动价值函数</p></figure><p id="4f49" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设，我们有一个最优动作值函数<strong class="lb iu"> <em class="lv"> q* </em> ( <em class="lv"> s，a </em>)。<em class="lv"> </em> </strong>那么最优策略可以确定如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ol"><img src="../Images/52037a6ee72d9aad90579d17c037d729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8fGXq78O2QWlgaLyLnH0g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">最优策略</p></figure><p id="55c9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu"> <em class="lv"> A(s) </em> </strong>是状态<strong class="lb iu"> <em class="lv"> s </em> </strong>可能的动作集合。</p><blockquote class="mt"><p id="da07" class="mu mv it bd mw mx my mz na nb nc lu dk translated">对于确定性策略<strong class="ak"> 𝜋，</strong>我们通过关系<strong class="ak"><em class="jq">a</em>=</strong><strong class="ak">𝜋(<em class="jq">s</em>)</strong>找到当前状态的新动作。<strong class="ak"> </strong>对于随机策略<strong class="ak"> 𝜋，</strong>我们可以通过关系式a = 𝜋*(s找到新的行动)，其中𝜋*是最优策略，见(7)。</p></blockquote><p id="9d8d" class="pw-post-body-paragraph kz la it lb b lc nd jv le lf ne jy lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">下面是一个代理应该做的事情:首先找到最优行动值函数，然后使用公式(7)找到最优策略。最后一种说法是正确的，但有一些限制。例外是可能的，例如，由于<a class="ae ni" rel="noopener" target="_blank" href="/exploration-in-reinforcement-learning-e59ec7eeaa75">ε-贪婪机制</a>。</p><h1 id="6e45" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><strong class="ak"> <em class="jq"> Q-table和</em> </strong>时态差异学习</h1><p id="8f36" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"> Q-table </strong></p><p id="44b1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<em class="lv"> Q表</em>是形状【状态，动作】的矩阵。我们将这个矩阵的所有槽初始化为零。合适的Python代码如下:</p><pre class="kk kl km kn gt om nw on oo aw op bi"><span id="c0ab" class="oq lx it nw b gy or os l ot ou"><strong class="nw iu">import numpy as np<br/>Q = np.zeros((state_size, action_size))</strong></span></pre><p id="8c05" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们会在每一步或每一个动作之后，在<em class="lv"> Q </em> ( <em class="lv"> s </em>，<em class="lv"> a </em> ) <strong class="lb iu"> </strong>中对每一对(<em class="lv"> s </em>，<em class="lv"> a </em>)进行更新。</p><p id="4b7a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">时间差</strong></p><p id="13ec" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">Q-table</em><strong class="lb iu"><em class="lv"/>T5<strong class="lb iu"><em class="lv"/></strong>是如何单步更新的？更新<em class="lv"> Q-table </em>最常用的方法是<em class="lv">时间差学习</em>或<em class="lv">TD-学习</em>。我们在每集结束前都会添加更新。</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ov"><img src="../Images/9f6fc90332b8e453bb976ea5a8ec2cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kOpjsKejm_h0_tueM5rjw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="jq"> TD-learning </em></p></figure><p id="2d8f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归<strong class="lb iu"> <em class="lv"> Gt </em> </strong>情商。(8)一种叫做<em class="lv">的替代估计</em><strong class="lb iu"><em class="lv"/></strong>参见(1)。该值又名<em class="lv">TD-目标</em>。(8)中的值<em class="lv"> Q </em> (s_ <em class="lv"> t，a_t </em> ) <strong class="lb iu"> </strong>称为<em class="lv">电流估计值</em>。使用(1)，我们可以重写等式。(8)如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ow"><img src="../Images/e4e836f83917e1cd0e2c2cf1b77948e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yChC0DulFvcMj-HhrDyeJQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="jq"> TD-learning:备选方案和当前估计值</em></p></figure><p id="f0c8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">算法Sarsa </strong></p><p id="2253" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Sarsa是<em class="lv">的缩写</em>为<strong class="lb iu"> <em class="lv">状态-行动-奖励-状态-行动</em> </strong>的序列。该序列的这五个元素如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ox"><img src="../Images/09e1d932e5291d965a653c6de6eecd72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoSen4wQy9qgdTFTrD-zFw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Sarsa序列</p></figure><p id="a71e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代理处于当前状态<strong class="lb iu"> <em class="lv"> s_t </em> </strong>，<strong class="lb iu"> <em class="lv"> </em> </strong>然后代理<strong class="lb iu"> <em class="lv"> </em> </strong>选择动作<strong class="lb iu"> <em class="lv"> a_t </em> </strong>，<strong class="lb iu"> <em class="lv"> </em> </strong>获得奖励<strong class="lb iu"> <em class="lv"> r_t </em> </strong>，之后代理进入对所有剧集执行该循环，直到值<em class="lv"> num_episodes </em>，参见下面的算法<em class="lv"> Sarsa </em>的伪代码。在每一步中，<em class="lv"> Q- </em>值<strong class="lb iu"><em class="lv"/></strong><em class="lv">Q</em>(s<em class="lv">，a </em> ) <strong class="lb iu"> </strong>由(9)更新，参见Sarsa伪代码中的黄线。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oy"><img src="../Images/7b78ac82caaecd91b686a13582cb1388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAi3xpKl-4MiUzJXG4Rs4g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Sarsa算法</p></figure><p id="99b0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">学习率<em class="lv">α</em>T107】</strong></p><p id="c810" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习率<strong class="lb iu"> <em class="lv"> α </em> </strong> <em class="lv"> </em>决定了算法Sarsa的行为。太大的值<strong class="lb iu"> <em class="lv"> α </em> </strong>将使我们的算法远离收敛到最优策略。如果<strong class="lb iu"> <em class="lv"> α= </em> 1 </strong>那么<em class="lv"> Q(s_t，a_t) ← Gt，</em>即<em class="lv">Q</em>-值总是将是最近的返回，没有任何学习。值<strong class="lb iu"> <em class="lv"> α </em> </strong>太小导致学习太慢。如果<strong class="lb iu"> <em class="lv"> α= </em> 0 </strong>那么<em class="lv"> Q </em> (s_ <em class="lv"> t，a_t </em> ) ← <em class="lv"> Q </em> (s_ <em class="lv"> t，a_t </em>)，永不更新。</p><h1 id="0893" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><strong class="ak">算法</strong>Q-学习</h1><p id="54fd" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">Q-学习</strong></p><p id="7a8b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法<em class="lv"> Q-learning </em>(又名<em class="lv"> Sarsamax </em>)在eq上与<em class="lv"> Sarsa </em>不同。(9)如下:代替<em class="lv"> Q(s，a) </em>在<strong class="lb iu"> <em class="lv"> t </em> </strong>的值，我们使用<em class="lv"> Q(s，a) </em>的最大值，其中<strong class="lb iu"><em class="lv"/></strong>贯穿了此刻<strong class="lb iu"><em class="lv"/></strong><em class="lv"/><strong class="lb iu"><em class="lv"/>的所有可能动作T71】</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ow"><img src="../Images/8aca8e70f87a051c8b3e0ffd144cfe49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VbhvykNExWyg_t6jIgHQjQ.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oz"><img src="../Images/4fe81019910f56af26ee72ed530012ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vi7gjTl_GW9fSkQ0HN4inw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">算法<em class="jq">Q-学习</em></p></figure><p id="cbe2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">贪婪的行动</strong></p><p id="6473" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在任何时间步<strong class="lb iu"> <em class="lv"> t </em> </strong>，对于状态<strong class="lb iu"> <em class="lv"> s_t </em> </strong>，至少存在一个动作<strong class="lb iu"><em class="lv"/></strong>，其估计值<em class="lv"> Q </em> ( <em class="lv"> s_t，a </em>)为<em class="lv">最大值</em>。这个动作<strong class="lb iu"> <em class="lv">一个</em> </strong>叫做<em class="lv">贪婪动作</em>。相关联的政策<strong class="lb iu"> 𝜋*( <em class="lv"> s </em> ) </strong>叫做<strong class="lb iu"> <em class="lv"> </em> </strong> <em class="lv">贪政策</em>，参见eq。(7).当我们选择一个贪婪的行为时，我们正在<em class="lv">利用</em>我们当前对这些行为的知识。相反，如果我们选择了<em class="lv">非贪婪动作</em>中的一个，那么我们就是<strong class="lb iu"> </strong> <em class="lv">探索</em>，因为这能够提高我们对非贪婪动作的价值的估计。</p><p id="4a52" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Sarsa vs. Q-learning </strong></p><p id="a5f1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<em class="lv">偏离策略</em> <strong class="lb iu"> <em class="lv"> </em> </strong>智能体独立于智能体的动作学习<em class="lv">最优策略</em>。一个<em class="lv">策略上的</em>代理学习该代理正在执行的策略。<em class="lv"> Q-learning </em>是<em class="lv"/>an<em class="lv"/>off-policy<em class="lv"/>算法<em class="lv"> </em>因为<em class="lv">最优策略</em>是通过贪婪动作学习<strong class="lb iu"><em class="lv">a _ gr</em></strong><em class="lv"/><em class="lv"/>中的公式求最大值<strong class="lb iu">，</strong>见(10) <strong class="lb iu">，</strong>然而，下一个动作<strong class="lb iu"><em class="lv"/></strong><em class="lv">Sarsa</em>是一个<em class="lv"> on-policy </em>算法，因为在(9)中，代理学习最优策略并使用相同的策略表现<em class="lv"> Q(s_t，a_t)。</em></p><blockquote class="mt"><p id="6fb1" class="mu mv it bd mw mx my mz na nb nc lu dk translated"><em class="jq"> Q-learning </em>可能<em class="jq"> </em>每一集的表现都比<em class="jq"> Sarsa </em>差，然而<em class="jq"> Q-learning </em>学习<em class="jq"> </em>最优策略。</p></blockquote><h1 id="94b8" class="lw lx it bd ly lz ma mb mc md me mf mg ka pa kb mi kd pb ke mk kg pc kh mm mn bi translated">Python中的神经网络</h1><p id="e90b" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">问题</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pd"><img src="../Images/ff3032de215359b3431ba23713b8066f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afwMjB3xcySRL5fLkRh4Eg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">我怎么算？</p></figure><p id="b3d8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答案:通过一个<em class="lv">神经网络</em>。我们将看到它在Python中的样子。</p><p id="1630" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">函数逼近</strong></p><p id="6c92" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络受到如此多关注的原因是因为它们可以逼近任何连续数学函数的输出。这是可能的，因为<em class="lv"> Kolmogorov定理</em>指出多元函数可以通过一元函数的和与合成的组合来表达。</p><p id="3eaf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的<em class="lv">Q</em>-值函数，两个向量参数的函数<em class="lv"> Q(s，a) </em>可以表示成我们想要的某个人工神经网络(<em class="lv"> nn </em>)一样精确。</p><blockquote class="mt"><p id="220e" class="mu mv it bd mw mx my mz na nb nc lu dk translated">用<em class="jq">深度学习</em>技术，即用<em class="jq">神经网络</em>实现<em class="jq"> Q </em>学习算法，称为<em class="jq">深度Q-网络</em>或<em class="jq"> DQN </em> <strong class="ak">。</strong></p></blockquote><p id="889e" class="pw-post-body-paragraph kz la it lb b lc nd jv le lf ne jy lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu">项目<em class="lv">DQN</em>T89】</strong></p><p id="94e0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python包<em class="lv"> PyTorch </em>是由脸书人工智能研究实验室开发的开源深度学习库。我们提供了几个使用<em class="lv"> PyTorch </em>实现的<em class="lv"> DQN </em>片段。这段代码摘自我在'<em class="lv"> Banana </em>'环境中训练一个代理的实现。代理人被训练在某个方形世界中导航和收集香蕉。然而，这段代码相当通用，可以用于许多具有离散状态空间的环境。</p><p id="e797" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">片段</strong></p><p id="5c0d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们给出几个片段，帮助理解如何使用神经网络，我们可以优雅地实现<em class="lv"> DQN </em>算法。</p><p id="73e4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">Function Agent.__init__</strong></code>:两个<em class="lv">神经网络</em>(<code class="fe nt nu nv nw b"><strong class="lb iu">q_local</strong></code><em class="lv"/><em class="lv"/><code class="fe nt nu nv nw b"><strong class="lb iu">q_target</strong></code>)<em class="lv"/>由模型<code class="fe nt nu nv nw b"><strong class="lb iu">Qnetwork. </strong></code>构成，每个模型<code class="fe nt nu nv nw b"><strong class="lb iu">Qnetwork</strong></code>包含两个隐层。</p><p id="dc89" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">Method</strong></code> <code class="fe nt nu nv nw b"><strong class="lb iu">Agent.learn()</strong></code>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pe"><img src="../Images/ab14fd82b4006c700cec51cbf1aad9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9TdFLN2GCIt79WWkYGWqQ.png"/></div></div></figure><p id="17bb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">Q_expected</strong></code> <strong class="lb iu"> <em class="lv"> </em> </strong>和<code class="fe nt nu nv nw b"><strong class="lb iu">Q_targets</strong></code> <strong class="lb iu"> <em class="lv"> </em> </strong>之间的差异应使用<em class="lv"> PyTorch </em>方法最小化，参见方法<code class="fe nt nu nv nw b"><strong class="lb iu">learn()</strong></code>。</p><p id="ca9f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">In class ReplayBuffer: </strong></code><code class="fe nt nu nv nw b"><strong class="lb iu"><em class="lv">s_t</em></strong></code><em class="lv"/>(<code class="fe nt nu nv nw b"><strong class="lb iu">state</strong></code>)<em class="lv"/><code class="fe nt nu nv nw b"><strong class="lb iu"><em class="lv">s_</em>{<em class="lv">t+1</em>}</strong></code>(<code class="fe nt nu nv nw b"><strong class="lb iu">next_state</strong></code>)的值由函数<code class="fe nt nu nv nw b"><strong class="lb iu">sample()</strong></code> <strong class="lb iu">、</strong> <em class="lv"> </em>采样，数据由函数<code class="fe nt nu nv nw b"><strong class="lb iu">add()</strong></code> <strong class="lb iu">存储。</strong></p><p id="147a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nt nu nv nw b"><strong class="lb iu">In method dqn():</strong></code>按剧集和时间步长双循环；这里，生成值'<em class="lv">状态'</em>、'<em class="lv">下一个状态'</em>、'<em class="lv">动作'</em>、'<em class="lv">奖励'</em>和'<em class="lv">完成'</em>。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h1 id="9892" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated"><strong class="ak">py torch相关提示</strong></h1><p id="6850" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下面是一些与<em class="lv"> PyTorch </em>方法相关的小技巧，见下图“三张量”。</p><p id="b8f5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分离()</strong></p><p id="7b3f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方法<code class="fe nt nu nv nw b"><strong class="lb iu">detach()</strong></code>表示对于<code class="fe nt nu nv nw b"><strong class="lb iu">Q_targets_next.</strong></code> <strong class="lb iu"> </strong>不会执行张量<code class="fe nt nu nv nw b"><strong class="lb iu">loss</strong></code>梯度的反向传播，这是可能的，因为张量<code class="fe nt nu nv nw b"><strong class="lb iu">loss</strong></code> <strong class="lb iu"> </strong>仅依赖于<code class="fe nt nu nv nw b"><strong class="lb iu">Q_targets</strong></code> <strong class="lb iu"> </strong>和<strong class="lb iu"> </strong> <code class="fe nt nu nv nw b"><strong class="lb iu">Q_expected,</strong></code>见方法<code class="fe nt nu nv nw b"><strong class="lb iu">learn()</strong></code>。</p><p id="43a6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最大值(1) </strong></p><p id="913e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里每个网络的形状是[64，4]其中64是批中的状态数(<code class="fe nt nu nv nw b"><strong class="lb iu">BATCH_SIZE</strong></code> =64)，4是可能的动作数(<code class="fe nt nu nv nw b"><strong class="lb iu">move forward, move backward, turn left, turn right</strong></code>)。表达式<code class="fe nt nu nv nw b"><strong class="lb iu">max(1)</strong></code>表示获得64个状态中每个状态的最大值。最大值是通过运行所有4个动作获得的。回想一下<em class="lv"> Q-Learning </em>在所有动作中寻找最大值，见(10)。下图中，我们给出了一个64×4张量<code class="fe nt nu nv nw b"><strong class="lb iu">self.q_target(next_states).detach()</strong></code>的数值例子。</p><p id="a7f1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与神经网络相关的张量</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ph"><img src="../Images/47db858bae7620c399c591e5e94f5622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96gn_MBYaKl5g4ji9-PMQA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">三个张量</p></figure><p id="ea75" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> max (1)[0] </strong></p><p id="9973" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实是<code class="fe nt nu nv nw b"><strong class="lb iu">max(1)</strong></code>返回两个张量的列表:<code class="fe nt nu nv nw b"><strong class="lb iu">max(1)[0], </strong></code>包含最大值的张量；<code class="fe nt nu nv nw b"><strong class="lb iu">max(1)[1],</strong></code>包含找到最大值的值“列号”的张量。我们只需要<code class="fe nt nu nv nw b"><strong class="lb iu">max(1)[0]</strong></code>，见上图。</p><p id="cade" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">unsqueeze①</strong></p><p id="57b9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们希望将形状为[64]的行向量放在形式为[64，1]的列中。这是使用<code class="fe nt nu nv nw b"><strong class="lb iu">unsqueeze(1)</strong></code>方法完成的，见上图中的张量<code class="fe nt nu nv nw b"><strong class="lb iu">Q_targets_next</strong></code>。</p><p id="03d7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Q_targets </strong></p><p id="8e19" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何计算<code class="fe nt nu nv nw b"><strong class="lb iu"> Q_targets</strong></code> <strong class="lb iu">？</strong>对于批量中的任何一个'<em class="lv">状态</em> <strong class="lb iu"> ' </strong>，如果一集结束，则'<em class="lv">完成</em>为1，否则'<em class="lv">完成</em>为<strong class="lb iu"> 0。</strong>然后<code class="fe nt nu nv nw b"><strong class="lb iu">Q_targets</strong></code>的线由等式计算。(10)当且仅当相关联的情节没有结束时。</p><p id="7b4c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">聚(1，行动)</strong></p><p id="911d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法沿dim = 1指定的轴收集值。请注意，dim=0表示行，dim=1表示列。<code class="fe nt nu nv nw b"><strong class="lb iu">self.q_local(states)</strong></code>中的每一行都由与四个动作相关的四个<em class="lv"> Q </em>值组成。因此，对于每一行，沿着列，方法<code class="fe nt nu nv nw b"><strong class="lb iu">gather</strong></code> <strong class="lb iu"> </strong>取与张量<code class="fe nt nu nv nw b"><strong class="lb iu">actions</strong></code>中的动作号相关联的<em class="lv">Q</em>-值，见下图。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pi"><img src="../Images/0e912cc551c8dcad8b0340d99827b6ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nUXoudazcGspANPFcu2cLg.png"/></div></div></figure><h1 id="da9d" class="lw lx it bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">结论</h1><p id="b785" class="pw-post-body-paragraph kz la it lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">结合贝尔曼方程，神经网络，柯尔莫哥洛夫定理，我们得到了一个惊人的技术，深度RL。这项技术提供了新的方法和新的算法，可以解决以前无法解决的问题。</p><blockquote class="mt"><p id="3624" class="mu mv it bd mw mx my mz na nb nc lu dk translated">贝尔曼方程和Deep RL实际上允许哪些「不可解的问题」？让我们以AlphaZero项目为例，这是一个精通国际象棋、日本象棋和围棋的计算机程序。AlphaZero在24小时的训练中击败了世界冠军程序Stockfish，达到了超人的下棋水平。</p></blockquote><p id="cb12" class="pw-post-body-paragraph kz la it lb b lc nd jv le lf ne jy lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们研究了深度RL的一个特例，即深度Q学习算法。在最后两节中，我们介绍了这种算法的实现和使用PyTorch软件包进行张量计算的一些细节。</p><p id="b854" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="c6b5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] M. Tavora，<a class="ae ni" rel="noopener" target="_blank" href="/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58">神经网络的逼近能力(带Python代码)</a> (2019)，走向数据科学</p><p id="5f35" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] X .郑，<a class="ae ni" href="https://www.linkedin.com/pulse/brief-introduction-reinforcement-learning-xiaofei-zheng" rel="noopener ugc nofollow" target="_blank">强化学习简介</a> (2019)，领英</p><p id="c16b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] H. van Hasselt，A. Guez，D. Silver，<a class="ae ni" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank">双Q学习的深度强化学习</a> (2015)，arXiv:1509.06461</p><p id="38a2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] A .辛格，<a class="ae ni" rel="noopener" target="_blank" href="/reinforcement-learning-markov-decision-process-part-2-96837c936ec3">强化学习:贝尔曼方程与最优性(第二部分)</a> (2019)，走向数据科学</p><p id="2d9a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] J. Hui，<a class="ae ni" href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" rel="noopener"> RL — DQN深Q网</a> (2018)，中</p><p id="d15b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] D. Silver，T. Hubert，J. Schrittwieser，D. Hassabis，<a class="ae ni" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go" rel="noopener ugc nofollow" target="_blank"> AlphaZero:为国际象棋、松木棋和围棋带来新的启示</a> (2018)，DeepMind</p></div></div>    
</body>
</html>