<html>
<head>
<title>NLP in Python- Vectorizing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的 NLP 矢量化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-in-python-vectorizing-a2b4fc1a339e?source=collection_archive---------20-----------------------#2020-06-08">https://towardsdatascience.com/nlp-in-python-vectorizing-a2b4fc1a339e?source=collection_archive---------20-----------------------#2020-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="90a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在典型的 NLP 机器学习模型管道中使用的常见矢量化技术，使用来自 Kaggle 的真实假新闻数据集。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79b0348fda22bb4ecf6b676cc310f5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-oJXNjccMfo-o1_epkeFg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的<a class="ae kv" href="https://unsplash.com/@romankraft" rel="noopener ugc nofollow" target="_blank"> Roman Kraft </a>的照片</p></figure><p id="9dbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将学习矢量化以及 NLP 模型中采用的不同矢量化技术。然后，我们将把这些概念应用到一个问题的上下文中。</p><p id="04aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用一个将新闻分为真假的数据集。该数据集可在 Kaggle 上获得，数据集的链接如下:</p><blockquote class="ls lt lu"><p id="2699" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/clmentbisaillon/fake-and-real-news-dataset</a></p></blockquote><p id="8858" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">典型的机器学习文本管道的第一步是数据清理。这一步在以前的文章中有详细介绍，链接如下:</p><div class="lz ma gp gr mb mc"><a rel="noopener follow" target="_blank" href="/nlp-in-python-data-cleaning-6313a404a470"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">Python 中的 NLP 数据清理</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">使用真实或虚假新闻数据集的典型 NLP 机器学习模型管道中涉及的数据清洗步骤…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kp mc"/></div></div></a></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/1b749c7d9495bb91ff7e44da7bee2f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*qm6fTha5UlpAlF-LL-1-3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据清理后的数据集</p></figure><p id="beba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原始新闻标题被转换成一种干净的格式，只包含基本信息(上图的最后一栏)。下一步是将清洗后的文本进一步转化为机器学习模型可以理解的形式。这个过程被称为向量化。在我们的上下文中，每个新闻标题都被转换成一个代表该特定标题的数字向量。有许多矢量化技术，但在本文中，我们将重点介绍三种广泛使用的矢量化技术——计数矢量化、N-Grams、TF-IDF，以及它们在 Python 中的实现。</p><ol class=""><li id="5f0e" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir">计数矢量化</strong></li></ol><p id="3761" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，矢量化是将文本转换为矩阵形式的数字条目的过程。在计数矢量化技术中，生成文档术语矩阵，其中每个单元是对应于新闻标题的计数，指示单词在文档中出现的次数，也称为术语频率。<strong class="ky ir">文档术语矩阵</strong>是一组虚拟变量，指示特定单词是否出现在文档中。语料库中的每个单词都有一个专栏。该计数与新闻标题类别的相关性成正比。这意味着，如果一个特定的词在假新闻标题或真实新闻标题中出现多次，那么该特定的词具有确定该新闻标题是假还是真的高预测能力。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="be26" class="ng nh iq nc b gy ni nj l nk nl">def clean_title(text):<br/>   text = "".join([word.lower() for word in text if word not in            string.punctuation])<br/>   title = re.split('\W+', text)<br/>   text = [ps.stem(word) for word in title if word not in nltk.corpus.stopwords.words('english')]<br/>   return text<br/>count_vectorize = CountVectorizer(analyzer=clean_title) <br/>vectorized = count_vectorize.fit_transform(news['title'])</span></pre><p id="fca6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剖析上面的代码，函数“clean _ title”——连接小写的新闻标题，不加标点。然后，在任何非单词字符上拆分文本。最后，对不间断单词进行词干分析，并以列表的形式呈现出来。这篇<a class="ae kv" href="https://medium.com/@divyar2630" rel="noopener">文章</a>中给出了清洁过程的详细描述。</p><p id="f732" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们使用了 sklearn 库中 sk learn . feature _ extraction . text 下的“计数矢量器”包。默认值和定义可在 scikit-learn — <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>文档中找到。在上面的代码中，我们实例化了计数矢量器并定义了一个参数— <strong class="ky ir">分析器</strong>。其他参数是它的默认值。分析器参数调用一个字符串，我们传递了一个函数，它接收原始文本并返回一个干净的字符串。</p><p id="66a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文档术语矩阵的形状是 44898，15824。共有 44898 个新闻标题，所有标题中有 15824 个独特的词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/9514d053e3bdf842cd8196dc15ef55a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1L-4Cxmn8X4gwjCoFagatg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">新闻标题中 15824 个唯一单词的子集</p></figure><p id="b8f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矢量器产生稀疏矩阵输出，如图所示。为了节省空间，只存储非零值的位置。因此，矢量化的输出如下所示:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="dbd2" class="ng nh iq nc b gy ni nj l nk nl">&lt;20x158 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>	with 206 stored elements in Compressed Sparse Row format&gt;</span></pre><p id="cdcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，将上述内容转换为数组形式会产生以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/96068ed1c59f229721ef1ad1553f40e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*mvKtMXGxNhwMx3-g1Q025A.png"/></div></figure><p id="2c26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图所示，大多数单元格包含 0 值，这被称为<strong class="ky ir">稀疏矩阵</strong>。许多矢量化输出看起来与此类似，因为许多标题自然不会包含特定的单词。</p><p id="49fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ir"> N-Grams </strong></p><p id="db08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与计数向量化技术类似，在 N 元语法方法中，生成一个文档术语矩阵，每个单元代表一个计数。N-grams 方法的区别在于，计数表示标题中长度为 N 的相邻单词的组合。计数矢量化是 N 元语法，其中 n=1。比如“我爱这篇文章”有四个字，n=4。</p><p id="b114" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果 n=2，即 bigram，那么列将是— [“我爱”，“爱这个”，“这篇文章”]</p><p id="82f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果 n=3，即三元组，那么列将是— [“我喜欢这个”，“喜欢这篇文章”]</p><p id="fe92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果 n=4，即 4 克，那么列应该是-['“我喜欢这篇文章”]</p><p id="b024" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于性能选择 n 值。</p><p id="96f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于 python 代码，清理过程的执行类似于计数矢量化技术，但单词不是以标记化列表的形式。将标记化的单词连接起来形成一个字符串，因此可以将相邻的单词聚集在一起以有效地执行 N 元语法。</p><p id="c791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">清理后的标题文本如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3f0cc18015cfc121f443313815395d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*L3-btdS83EhH0xu1e37aRg.png"/></div></figure><p id="c86b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩下的矢量化技术与我们上面使用的计数矢量化方法相同。</p><p id="7908" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权衡是在 N 个值之间进行的。选择较小的 N 值可能不足以提供最有用的信息。而选择一个高的 N 值，将产生一个具有大量特征的巨大矩阵。N-gram 可能很强大，但是需要多一点小心。</p><p id="6c4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。术语频率-逆文档频率(TF-IDF) </strong></p><p id="30d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与计数矢量化方法类似，在 TF-IDF 方法中，生成一个文档术语矩阵，每一列代表一个唯一的单词。TF-IDF 方法的不同之处在于，每个单元格都不表示词频，但是单元格值表示一个权重，它突出了特定单词对文档的重要性。</p><p id="dbe5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF-IDF 公式:</p><div class="kg kh ki kj gt ab cb"><figure class="np kk nq nr ns nt nu paragraph-image"><img src="../Images/3c42c7ca333925e7eadf73ce2425ec22.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*09lKuWL7VnOU3xvzDZZ0SQ.png"/></figure><figure class="np kk nv nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b3fe992504ea27d9f745a08722441614.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*BqHPZDm_5wNUYZpfEP6ayw.png"/></div></figure></div><p id="053a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">等式的第二项有助于抽出生僻字。那是什么意思？如果一个单词在许多文档中多次出现，那么分母 df 将增加，从而减少第二个词的值。词频或 tf 是一个词(x)在文档(y)中出现的次数除以 y 中总字数的百分比。</p><p id="29dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于 python 代码，我们将使用与计数矢量器方法相同的清理过程。Sklearn 的 TfidfVectorizer 可以用于 Python 中的矢量化部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4992398e3f829b8d812e700a25635dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*xgVL4dslcoWuZPWgF_Wudw.png"/></div></figure><p id="798a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法的稀疏矩阵输出显示代表文档中单词权重的小数。高权重意味着该单词在几个文档中出现多次，低权重意味着该单词在许多文档中出现的次数较少或者在多个文档中重复出现。</p><p id="f49b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">总结思路</strong></p><p id="3d56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择矢量化方法没有经验法则。我经常根据手头的业务问题做出决定。如果没有限制，我经常从最简单的方法开始(通常是最快的)。</p><p id="4b74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">…</p><p id="cf00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我很想听听你对我的文章的想法和反馈。请在下面的评论区留下它们。</p></div></div>    
</body>
</html>