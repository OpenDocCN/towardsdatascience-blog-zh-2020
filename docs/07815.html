<html>
<head>
<title>Art With AI: Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能艺术:神经风格转移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/art-with-ai-neural-style-transfer-63d0ef76596a?source=collection_archive---------38-----------------------#2020-06-10">https://towardsdatascience.com/art-with-ai-neural-style-transfer-63d0ef76596a?source=collection_archive---------38-----------------------#2020-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0a82" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对卷积神经网络最有趣的应用之一的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cb67f137ec9f4dfbfe1e3e8c6ca1ca72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPfTQsDXbqQmuFpizA25HA.png"/></div></div></figure><h1 id="4b98" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="31a5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如果你能把你的图像转变成像文森特·梵高或莱昂纳多·达·芬奇这样的著名画家风格的艺术品，会怎么样？如果知道卷积神经网络不仅可以帮助你学习对图像进行分类或检测物体，还可以帮助你学习艺术家的绘画或绘画风格，以艺术家自己的风格重建图像，你不会感到惊讶吗？好吧，如果你想知道如何做到这一点，这篇文章就是为你准备的。</p><h1 id="4bd8" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">神经类型转移</h1><p id="1fae" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这种深度学习技术可以帮助你生成一个输出图像(<em class="mi">表示为 G </em>)基于内容图像(<em class="mi">表示为 C </em>)，但是以另一个图像的风格绘制或绘制(称为风格图像，<em class="mi">表示为 S </em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/ab82c975149dd48568716c4bbe9c6df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-EmpxfuBw-bNcNqY01fEA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">使用神经类型转移生成的图像示例</p></figure><p id="b8f8" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">这种技术主要依赖于 ConvNets 的初始层和更深层从给定的风格图像中提取特征并在内容图像上实现它的方式。</p><h1 id="c4ae" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">卷积神经网络及其学习</h1><p id="5863" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">让我们以一个 VGG 模型为例来说明学习过程</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b465925df2f939fce6fe6c09e90644a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*YJGVH-AB5eIvKJtd6qUROg.jpeg"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">VGG19 架构</p></figure><p id="7311" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">当图像被传送到网络上，并且训练开始时，每一层都有一些激活。现在，如果你试图想象每一层的激活，你会发现最初的层会试图识别图像的简单特征，如边缘或边界或特定颜色的阴影。而更深层的激活将倾向于学习图像的更复杂的特征，如形状、图案、物体、纹理等。接近尾声时，最终图层将能够识别前景或背景物体，如猫、狗、汽车等。</p><p id="1ce8" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">这是我们在神经风格转移中使用的基本直觉，即分离图像的风格和内容。我们从风格图像中提取出所有特征，将其分离，同样，我们从内容图像中分离出复杂的特征，这有助于我们生成输出图像。</p><h1 id="a3ec" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">构建神经风格转移系统</h1><p id="b5c3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在建立神经风格转换系统的过程中，第一步也是最重要的一步是定义损失函数:内容损失和风格损失。内容损失最小化确保内容图像和生成图像的更深层之间的差异<em class="mi">或误差</em>最小，风格损失最小化确保风格图像和生成图像的所有层之间的差异<em class="mi">或误差</em>。</p><p id="3a2b" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">最初，我们将生成的图像(<em class="mi"> g </em>)定义(<em class="mi">或初始化</em>)为可训练变量，事实上，它是唯一可训练的图像，因为内容(<em class="mi"> c </em>)和样式图像(<em class="mi"> s </em>)不会被训练。并且预训练模型的参数(权重和偏差)应该被冻结。</p><h1 id="0db7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">内容损失</h1><p id="28e3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">内容损失功能帮助生成的图像结合内容图像的内容。当生成的图像和内容图像被馈送到 CNN，并且计算不同层的激活时，选择更深的层(<em class="mi"> l </em>)的输出，其用于找出内容和生成的图像之间的误差。该损失函数基本上是被馈送了内容图像(<em class="mi"> C </em>)和生成图像(<em class="mi"> G </em>)的 CNN 的这两个中间层之间的欧几里德距离。用简单的话来解释，内容损失函数为我们提供了更深层次的内容和生成的图像之间有多大差异的答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/324c9400223e8d5c87ce48645d2710a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TGtrmFHo1nulyw3jKkullg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">内容损失函数</p></figure><p id="9fb8" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">这仅仅是 CNN 的层<em class="mi"> l </em>的激活与内容和生成的图像之间的均方误差。请注意，我们仅在一个且仅一个层(<em class="mi"> l </em>)应用内容损失，而不是多个层。</p><h2 id="5fdc" class="mv kv it bd kw mw mx dn la my mz dp le lv na nb lg lz nc nd li md ne nf lk ng bi translated">为什么我们要尽量减少内容损失</h2><p id="fe8c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如上所述，卷积神经网络的更深层捕获更复杂的特征(或者本质上是内容)，因此，如果我们需要生成的图像(<em class="mi"> g </em>)具有与内容图像(<em class="mi"> c </em>)相同的内容，那么我们将不得不最小化这两个图像的特征表示中的更深层之间的差异，或者换句话说，如果两个图像在更深层中具有非常相似的特征，这意味着两个图像中的内容是相同的。</p><h1 id="7cbb" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">风格丧失</h1><p id="93b8" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">获取样式损失的过程与我们在内容损失的计算中应用的策略类似，但是我们不使用中间层的原始输出，而是使用各个层的所生成的特征图的 gram 矩阵。然后，我们将损失应用于所有层，与内容损失相反，内容损失仅应用于单个层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/ed5387f49cbc580e6194dc75fed04f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DAaFA_L20wPmGlJf9nNzpA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">Gram 矩阵计算</p></figure><p id="4764" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">Gram 矩阵或 Style 矩阵是一个简单的矩阵，它的第(<em class="mi"> i，j </em>)个元素具有第<em class="mi"> i </em>个和第<em class="mi"> j </em>个特征图的逐元素乘法的输出，并在图像的宽度和高度上对其求和。</p><h2 id="ae6f" class="mv kv it bd kw mw mx dn la my mz dp le lv na nb lg lz nc nd li md ne nf lk ng bi translated">因此，总风格损失分两步计算:</h2><p id="6bd5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">首先，在给定层(<em class="mi"> l </em>)，计算风格(<em class="mi"> s </em>)的特征图表示的 gram 矩阵或风格矩阵与生成的图像(<em class="mi"> g </em>)之间的均方误差，其表示该层的风格损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/c9c57aae3e275623bcc6ad0550c308c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3X6PPRvCkXPSr7op93nZig.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">任何给定层的风格损失计算(l)</p></figure><p id="272d" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">其次，样式损失被应用于 CNN 中的每一层。它与附加权重(<em class="mi"> wl </em>)相乘，该权重是对总损失的计算有贡献的每一层的权重因子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f0916e18c8357fd05c334672a41aed62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q64JG25BzsgB2LwTpAqJLw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">总风格损失计算</p></figure><h2 id="949d" class="mv kv it bd kw mw mx dn la my mz dp le lv na nb lg lz nc nd li md ne nf lk ng bi translated">为什么我们要尽量减少风格损失</h2><p id="17ba" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如上所述，在任何给定层(<em class="mi"> l </em>)，风格损失通过从风格图像(<em class="mi"> s </em>)和生成图像(<em class="mi"> g </em>)的该层(<em class="mi"> l </em>)中的特征映射获得的 Gram 矩阵或风格矩阵的相关性之间的差来计算。并且 Gram 矩阵 G( <em class="mi"> l </em>)表示该层的特征图(<em class="mi"> l </em>)之间的特定类型的相关性。其中 G <em class="mi"> l </em> ( <em class="mi"> i，j </em>)表示特征图 I 和 j 之间的相关性。因此，非常直观的是，如果我们最小化风格的特征表示(<em class="mi"> s </em>)和生成的图像(<em class="mi"> g </em>)之间的距离，则生成的图像将具有与风格图像相似的特征，因为生成的图像是唯一的可训练图像而不是风格图像，因此生成的图像开始获得风格图像的特征。由于样式损失应用于所有图层的要素地图制图表达，因此它将为生成的影像提供在初始图层中识别的样式影像的最小要素。</p><h1 id="e900" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">全损</h1><p id="5a91" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">总损失或最终损失可定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/197756897c8c2ae47deefaad488eef53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6SN9MA4Crs_ggFJL-XHlg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">神经类型转移的最终损失</p></figure><p id="5413" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">术语<strong class="lo iu"><em class="mi">α</em></strong><em class="mi">【alpha】</em>和<strong class="lo iu"><em class="mi">β</em></strong><em class="mi">【beta】</em>是超参数，可以根据用户偏好进行调整。任何超参数的值越大，与其相关的损失在生成的图像中的贡献就越大。例如，如果用户想要较少的样式图像对生成的图像的影响和更多的内容图像，则用户可以减小超参数<strong class="lo iu"><em class="mi">【β】</em></strong><em class="mi">(β)</em>并增大超参数<strong class="lo iu"><em class="mi">α</em></strong><em class="mi">(α)。</em></p><h1 id="019c" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">优化损失</h1><p id="b795" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">现在，在我们定义了最终损失函数之后，该过程的最后一步仍然是使用优化算法来最小化该损失。</p><p id="1948" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">优化损失的推荐方法是使用 L-BFGS 优化器。其他方法包括使用随机梯度下降或使用 Adam 优化器，但不建议这样做，因为数据不是随机的，即将数据集分成小的小批量是行不通的，因为输入是单个静态图像。此外，对于神经类型转移的任务，L-BFGS 优化器在学习上比 Adam 优化器更快。</p><h1 id="15a7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/ab5c3fa8c82f9186396dca7b642d8591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThoWfl2TQVCEXm2l8PBaxw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">在历元的不同阶段输出图像</p></figure><p id="648d" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">因此，以类似的方式，你可以尝试不同的风格和内容图像的组合，以创建各种艺术作品，Python 笔记本可在我的<a class="ae nm" href="http://github.com/SivinX11" rel="noopener ugc nofollow" target="_blank"> GitHub 帐户</a>上<a class="ae nm" href="https://github.com/SivinX11/Neural_Style_Transfer" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="356c" class="pw-post-body-paragraph lm ln it lo b lp mo ju lr ls mp jx lu lv mq lx ly lz mr mb mc md ms mf mg mh im bi translated">现在你可以继续发挥你的创造力，创作出这样的作品:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/4e3f45154fac8d783b31a485ad9ed37f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02VdEgAUYH7t9V-jLXZbiw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">借助神经风格转移生成不同的艺术品。</p></figure><h1 id="40ab" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">参考:</h1><p id="3165" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">基于 Leon A. Gatys、Alexander S. Ecker 和 Matthias Bethge 的论文:<a class="ae nm" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mi">使用卷积神经网络的图像风格转移</em> </a> <em class="mi">。</em></p></div></div>    
</body>
</html>