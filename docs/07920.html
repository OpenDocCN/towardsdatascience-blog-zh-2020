<html>
<head>
<title>“Sklearn’s TF-IDF” vs “Standard TF-IDF”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“Sklearn 的 TF-IDF”与“标准 TF-IDF”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d?source=collection_archive---------14-----------------------#2020-06-12">https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d?source=collection_archive---------14-----------------------#2020-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="375c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们来看看与传统的 TF-IDF 相比，Sklearn 的 TF-IDF 的计算方法有何不同，并一步一步地进行分析。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a86c7c0e42ba14d4a82a2a75208f29a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rzFwV_2NnGWugFASWlQRWA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚历克斯·钱伯斯在 Unsplash 上的照片</p></figure><p id="0ea2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章是在我试图使用标准公式手工计算 TF-IDF 矩阵时受到启发的，在首先计算 TF 和 IDF，然后将两者相乘之后，我意识到，与我在我的样本语料库上用 Scikit-learn 获得的结果相比，有一些不同，在那里我意识到 Scikit-learn 版本与大多数标准和传统版本之间的差异。事不宜迟，我们来了解一下区别。</p><h1 id="cda9" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">TF-IDF 的标准定义是什么？</h1><p id="3bd3" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">TF-IDF 定义:<em class="mr">“词频-逆文档频率”，是一种数字统计，旨在反映一个词对集合或语料库中的文档有多重要。</em><a class="ae ms" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"><em class="mr">【2】</em>T5】</a></p><p id="9f21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">目的:</strong>使用 TF-IDF 而不是给定文档中记号出现的原始频率的目的是按比例缩小记号的影响，这些记号在给定语料库中非常频繁地出现，因此在经验上比在一小部分训练语料库中出现的特征信息少。<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank">【1】</a></p><p id="0171" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">如何计算到它？</strong></p><p id="2a42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它只是两个重量的乘积，TF 和 IDF 重量:</p><ul class=""><li id="0930" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu"> TF: Term Frequency </strong>衡量一个术语在一个文档中出现的频率，因为每个文档的长度不同，所以一个术语在长文档中出现的次数可能比短文档多得多。因此，TF 为:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/15bbef26eb0b13e1220d5dbc6638c341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*mXMyrHa5pembP977bW8LNg.png"/></div></figure><ul class=""><li id="606b" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu"> IDF:逆文档频率</strong>衡量一个术语的重要性。在 TF 中，所有术语都被认为是同等重要的。因此，我们需要通过计算以下各项来降低常用术语的权重，同时提高稀有术语的权重:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/de40f10b1f1ee07cd2d292ca8084391d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*a5CVURGPHOiSZtLw_ewoWw.png"/></div></figure><blockquote class="ne nf ng"><p id="c96a" class="ky kz mr la b lb lc ju ld le lf jx lg nh li lj lk ni lm ln lo nj lq lr ls lt im bi translated">注意:在 Scikit-learn 中，log 不是以 10 为底的，尽管它是<strong class="la iu">自然对数</strong>(它有一个底 e，e 是一个无理数和超越数，大约等于 2.718)，</p></blockquote><p id="7c5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，有一个 IDF 实际含义的小例子:这个例子有 4 个术语:(a，船，移动和 mobilegeddon)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3906e5df82a385780de52273160178e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/0*W6KHqq1Kx0re08ll.jpg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Moz 作者<a class="ae ms" href="https://moz.com/community/users/18040" rel="noopener ugc nofollow" target="_blank">埃里克·恩格</a></p></figure><p id="0ca8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以看到，IDF 最高的是最稀有的词(Mobilegeddon)，而词的频率越低，IDF 值就越小。</p><h1 id="3756" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">Scikit-learn TF-IDF</h1><p id="c08c" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated"><em class="mr">现在，Scikit-learn 的</em> <code class="fe nl nm nn no b"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="mr">TfidfTransformer</em></strong></a></code> <em class="mr">和</em> <code class="fe nl nm nn no b"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="mr">TfidfVectorizer</em></strong></a></code> <em class="mr">中计算出的 TF-IDF 与标准教科书符号有何细微差别？</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c5312bd16d482945906939b54389b947.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*xFbKEALUXjrWV_4nwSDBOg.png"/></div></figure><p id="cfa9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">不同之处</strong>如下:TF 保持不变，而 IDF 不同:IDF 的分子和分母中添加了“某个常数 1”，就好像看到一个额外的文档恰好包含集合中的每个术语一次，这防止了零除法“这是一种更具经验性的方法，而大多数教科书中的标准符号没有常数 1。</p><p id="bed1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们尝试将 Scikit learn TF-IDF 一步步应用到下面三个句子组成的语料库中，看看结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/efbf1f96512eade7f9e21d05d44b90f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*7B3UU_ZaZVaDwHYQc7WH9w.png"/></div></figure><p id="fddf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在文本语料库中，一些单词(例如，英语中的“some”、“a”、“h as”等)如果直接输入到分类器，这些非常频繁的术语将掩盖更罕见但更有趣的术语的频率。因此，使用停用字词删除来删除它们是最好且有效的方法，因为它们携带的关于文档实际内容的信息非常少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/91ea84dd9a84c082416c46e2101ab4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*4JMTezSFJrVuxrHwGfaNaA.png"/></div></figure><p id="924d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">形成来自语料库的所有独特单词的词汇将由这 5 个单词组成:“John”、“Cat”、“Eat”、“Fish”和“Big”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c48d1c1545cecb6d27e78b9d016f527b.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*NvpLfGGEAeYz-8-s2d5TZA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">词汇频率计数</p></figure><h1 id="3bad" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">术语频率(TF)</h1><p id="28f6" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">让我们从计算 Tf('John '，句子 1)开始</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/223d717b941fcff65e3ed30cf9238ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*gnyu9XCBl5lZ6GoYt8SigQ.png"/></div></div></figure><ul class=""><li id="2fd4" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">分子:</strong>“约翰”在句子 1 中只出现一次</li><li id="9036" class="mt mu it la b lb nu le nv lh nw ll nx lp ny lt my mz na nb bi translated"><strong class="la iu">分母:</strong>句子 1 中的总字数为:{John，cat}=2</li></ul><p id="5898" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TF('约翰'，句子 1) = 1/2</p><p id="21fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，整个词汇表的 TF 值如下表所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/712b287b7b3e5dca4b9edadf1fe8e3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*tU0ZUu28oq5u2ZsJQMHwdg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果 TF 表</p></figure><h1 id="aaae" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">反向文档频率(IDF)</h1><p id="d084" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">如上所述，IDF 是一个衡量术语重要性的指标。IDF 值是必不可少的，因为仅计算 TF 本身不足以理解单词的重要性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0f9262dbdb7c45ef2d581d7a632c1de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*2TDNyWI5qMERFG8tUfzSNg.png"/></div></figure><p id="aa84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来计算 IDF(“约翰”):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a895357d25fd02ee323e1bfcc3226f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*g3ViVtnnrSrogciX6nhsgw.png"/></div></figure><ul class=""><li id="a562" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">分子:</strong>句子总数=3</li><li id="e42d" class="mt mu it la b lb nu le nv lh nw ll nx lp ny lt my mz na nb bi translated"><strong class="la iu">分母:</strong>包含“John”的句子数=1</li></ul><p id="7ba5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">IDF(' John ')= log((3+1)/(1+1))+1 = 1.69</p><p id="5e59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，整个词汇表的 IDF 值如下表所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/5ad9e8ebcb834f7e5cbbda43cf3fa6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*HYykGWuET8GbThdXCxPzkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成的 IDF 表</p></figure><h1 id="90d5" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">应用(TF X IDF)产品</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/02eefeb29fdc6945f79b1a94aed0d131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvjC0_xxIITFtapEDrVMHw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/c57e133f3cb74c78890d38858fa1f429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W0mvTgPZ5QPnIq5HWF__NQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/6c9f4881467c1d41319045c9acc155bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoXZDdtzX2y2HF3Deny0yg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为第一个向量句子计算的标准化示例</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/4478577f3e4f19a47602776e1121f93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKL0gJsFSzw3jzU8cWlR_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终的 Tf-IDF 向量表</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a4a00f33ac051dad1066acde943d5b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*UPZX71OoU-CmTkcGu9CAgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相同语料库的 Sklearn_TF-IDF 输出</p></figure><h2 id="f4bd" class="oi lv it bd lw oj ok dn ma ol om dp me lh on oo mg ll op oq mi lp or os mk ot bi translated">标准化步骤</h2><p id="9412" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在 Scikit-Learn 中，产生的 TF-IDF 向量然后通过欧几里德范数归一化。这最初是为信息检索(作为搜索引擎结果的排序函数)开发的术语加权方案，在文档分类和聚类中也有很好的用途。[1]</p><h1 id="2ab0" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">摘要</h1><p id="ce6f" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">传统的 TF-IDF 和 Scikit-learn 的 TF-IDF 之间的主要区别在于分母和分子中存在酉常数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c5312bd16d482945906939b54389b947.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*xFbKEALUXjrWV_4nwSDBOg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/9fc103a2785b2faf5d399e91a69625c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rc5I3neBHH_3iqFW8vx86Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标准与 Sklearn TF-idf 结果矩阵</p></figure><p id="2cb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总的来说，这不会改变 TF-IDF 矩阵的向量表示，与标准的相比，词汇将由更高的权重来表示，而在标准的 TF-IDF 表中，相应的值反而被设置得更小。</p><p id="1e91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，TF-IDF 的核心原则将保持不变:<strong class="la iu"> <em class="mr"> TF-IDF 为不太频繁的词给出更大的值，并且当 IDF 和 TF 值都高时为高</em> </strong>，正如我们可以在 cor 中为我们的不太频繁的词确认的那样:{John 和 Big}:对于它们两者，TF-IDF 实现了比 TF-IDF 表中剩余的更高频率的词更大的值。</p><p id="e0b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总的来说，直到 2015 年，与同等重视每个单词的传统单词包方法相比，TF-IDF 一直是为表示文档相关性的单词分配权重的传统方法。t 仍然被积极用于有效地解决 NLP 问题。下面是 Sklearn 实现的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><h1 id="6118" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">参考</h1><p id="77ca" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">[<a class="ae ms" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank">1]https://sci kit-learn . org/stable/modules/feature _ extraction . html # text-feature-extraction</a></p><p id="a58f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]<a class="ae ms" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></p><p id="a32d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]墨子。2020.<em class="mr">逆文档频率和唯一性的重要性</em>。[在线]可从以下网址获得:https://moz . com/blog/inverse-document-frequency-and-the-importance-of-unique</p></div></div>    
</body>
</html>