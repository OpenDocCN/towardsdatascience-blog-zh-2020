<html>
<head>
<title>NLP: Classification &amp; Recommendation Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:分类和推荐项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-classification-recommendation-project-cae5623ccaae?source=collection_archive---------34-----------------------#2020-07-31">https://towardsdatascience.com/nlp-classification-recommendation-project-cae5623ccaae?source=collection_archive---------34-----------------------#2020-07-31</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="c589" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用 arxiv 数据的逐步 NLP 项目</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/a84412bdb7f9f52407aacb3a2999316c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNIQ-nr-F_GzNbRWZB7PdQ.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kz" href="https://unsplash.com/@iammrcup?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Cup 先生/杨奇煜·巴拉</a>拍摄的照片</p></figure><p id="8099" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">NLP 是从文本数据中提取意义和学习的科学，它是数据科学项目中使用最多的算法之一。文本数据无处不在，总之，NLP 有许多应用领域，如下图所示。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj lw"><img src="../Images/cf0623bd2239cf3e0438898d96fcf548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7LiAHEwObH6FBtjdPZQ0g.jpeg"/></div></div></figure><p id="89a2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在这种背景下，我决定做一个覆盖 arxiv 数据的 NLP 项目。通过这个项目，我的目标是对文章的标签进行分类，并通过文章的摘要、标题、作者和体裁特征建立一个推荐系统。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="ef7f" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">第一步:数据知识</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mw"><img src="../Images/e48df33918481d4632eb6a85393c0690.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*A8bCpIUSgqVqa1BVEcGxEg.png"/></div></figure><p id="e7cb" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了实现我的目标，我提取了一个包含 41000 行和 6 列的 arxiv 数据集:作者、链接、摘要、标签、标题和年份。我的数据集包含 1998 年至 2018 年之间的文章。如左图所示，它包含了大部分计算机科学领域的文章。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="c7f4" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">步骤 2:文本清理</h1><p id="1337" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">我的一些特性是字典类型的，所以我将这些列转换成列表类型，以便有一个可用的数据结构。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="3763" class="nh mf iu nd b gz ni nj l nk nl">from ast import literal_eval<br/># convert 'stringfield' lists to usable structure<br/>features = ['author', 'link', 'tag']<br/>for feature in features:<br/>    arxivData[feature] = arxivData[feature].apply(literal_eval)</span></pre><p id="a688" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在执行 literal_eval 之后，我为列表转换编写了三个函数:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="1b46" class="nh mf iu nd b gz ni nj l nk nl">def get_names(x):<br/>    if isinstance(x, list):<br/>        names = [i['name'] for i in x]<br/>        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.<br/>        if len(names) &gt; 3:<br/>            names = names[:3]<br/>        return names</span><span id="0266" class="nh mf iu nd b gz nm nj l nk nl">def get_link(x):<br/>    for i in x:<br/>        return i['href']<br/>    <br/>def get_tag(x):<br/>    if isinstance(x, list):<br/>        terms = [i['term'] for i in x]<br/>        #Check if more than 5 elements exist. If yes, return only first five. If no, return entire list.<br/>        if len(terms) &gt; 5:<br/>            terms = terms[:5]<br/>        return terms</span></pre><p id="5d70" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">好了，我现在准备写一个文本清理函数。这些是 NLP 项目中构建健壮模型结构的最重要的部分之一。为此，我们需要删除大写字母、标点和数字:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="32f4" class="nh mf iu nd b gz ni nj l nk nl"># Data Cleaning &amp; Preprocessing techniques<br/>def clean_text(text):<br/>    # remove everything except alphabets<br/>    text = re.sub("[^a-zA-Z]", " ", text)<br/>    # remove whitespaces<br/>    text = ' '.join(text.split())<br/>    text = text.lower()<br/>    return text</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nn"><img src="../Images/445691114a1837c7a3554bf5f508a5eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6nHIZSnldR4e881qTB7JKQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">没有停用词的 25 个最常用词</p></figure></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="3534" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">步骤 3:文本挖掘算法</h1><p id="7ce4" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">现在是时候从文本数据中提取特征了。文本文件最初只是一系列的单词。为了运行机器学习算法，我们必须将文本文件转换成数字特征向量。有两种常用的方法:CountVectorizer 和 Tfidf。</p><ul class=""><li id="418a" class="no np iu lc b ld le lg lh lj nq ln nr lr ns lv nt nu nv nw bi translated">count vectorizer:<a class="ae kz" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">count vectorizer</a>提供了一种简单的方法，既可以标记一组文本文档，构建已知单词的词汇表，还可以使用这个词汇表对新文档进行编码。</li><li id="76eb" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">Tfidf: TF-IDF 是一种统计度量，用于评估一个单词与文档集合中的一个文档的相关程度。这是通过将两个度量相乘来实现的:一个单词在一个文档中出现的次数，以及该单词在一组文档中的逆文档频率。</li></ul><p id="42e3" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">它们之间的区别基本上是，一个统计所有的单词来矢量化，另一个使用统计方法来代替。</p><p id="1222" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了在我的数据集上执行文本挖掘，我首先编写了一个 lemmatize 函数，然后执行矢量化。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="f903" class="nh mf iu nd b gz ni nj l nk nl"># Lemmatization process<br/>'''<br/>Words in the third person are changed to first person and verbs in past and future tenses are changed into the present by the <br/>lemmatization process. <br/>'''<br/>lemmatizer = WordNetLemmatizer()</span><span id="2323" class="nh mf iu nd b gz nm nj l nk nl">def tokenize_and_lemmatize(text):<br/>    # tokenization to ensure that punctuation is caught as its own token<br/>    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]<br/>    filtered_tokens = []<br/>    <br/>    for token in tokens:<br/>        if re.search('[a-zA-Z]', token):<br/>            filtered_tokens.append(token)<br/>    lem = [lemmatizer.lemmatize(t) for t in filtered_tokens]<br/>    return lem</span></pre><p id="8ffa" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">有了这个矩阵，我们现在可以建立一个机器学习模型。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="c666" class="nh mf iu nd b gz ni nj l nk nl"># Defining a Count Vectorizer object<br/>count_vec = CountVectorizer(stop_words='english', max_features=10000)</span><span id="75e4" class="nh mf iu nd b gz nm nj l nk nl"># Defining a TF-IDF Vectorizer<br/>tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), tokenizer=tokenize_and_lemmatize, max_features=10000, use_idf=True)</span></pre></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="1892" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">第四步:分类</h1><p id="5c20" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">有各种算法可用于文本分类。嗯，我从探索这些模型开始:逻辑回归、朴素贝叶斯、线性 SVC 和随机森林。我的方法是，在运行完本节中的所有模型后，选择最佳模型进行优化。因此，我用默认参数运行了所有的模型，以查看结果。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oc"><img src="../Images/11732f4272d18b84e2b0ef42506f8ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*sc7ZEJwvih80eer6ayfq4g.png"/></div></figure><p id="cd88" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这是我的分类模型的结果。准确性和 f1 分数之间冲突的原因是一篇文章有多个标签。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj od"><img src="../Images/655e61acff6990f4b686401ae692ffb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*mLFUwaIyu8Bgp83dhJtpZA.png"/></div></figure><p id="4da2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">机器学习模型未能预测一篇文章的所有标签。尽管如此，f1 分数还是高于准确度，因为我把 f1 的平均参数设置为‘微’。</p><p id="839d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我跳到了模型评估之后的优化部分。为此，我使用了 GridSearchCV:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="f641" class="nh mf iu nd b gz ni nj l nk nl">param = {'estimator__penalty':['l1', 'l2'], 'estimator__C':[0.001, 0.01, 1, 10]}</span><span id="507e" class="nh mf iu nd b gz nm nj l nk nl"># GridSearchCV<br/>kf=KFold(n_splits=10, shuffle=True, random_state=55)<br/>lr_grid = GridSearchCV(oneVsRest, param_grid = param, cv = kf, scoring='f1_micro', n_jobs=-1)<br/>lr_grid.fit(xtrain_tfidf, y_train)</span></pre><p id="abf8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">由于准确性得分，我选择了逻辑回归进行网格搜索，结果，我得到的 F1 得分增加到了%64，1。以下是优化回归模型的一些结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oe"><img src="../Images/0b211a3b9b093d4dff97931bd41c2ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjwD5b7reG6pdHl-_tcdwA.png"/></div></div></figure></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="8f78" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">第五步:推荐引擎</h1><p id="8711" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">我做了两个推荐系统，第一个基于摘要，另一个基于标题、标签和作者。为了建立一个推荐器，首先我计算了矢量化文本数据的余弦相似度。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="50f5" class="nh mf iu nd b gz ni nj l nk nl"># TfIdf matrix transformation on clean_summary column<br/>tfidf_matrix = tfidf_vec.fit_transform(arxivData['clean_summary'])<br/># Compute the cosine similarity<br/>cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)</span></pre><p id="5769" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">根据计算结果，我编写了推荐函数:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="e868" class="nh mf iu nd b gz ni nj l nk nl">def get_recommendations(title, similarity):<br/>    idx = indices[title]<br/>    # pairwsie similarity scores<br/>    sim_scores = list(enumerate(similarity[idx]))<br/>    # sorting<br/>    sim_scores = sorted(sim_scores, key=lambda x: x[1],  reverse=True)<br/>    sim_scores = sim_scores[1:11]<br/>    article_indices = [i[0] for i in sim_scores]<br/>    # Return the top 10 most related articles<br/>    return arxivData[['link', 'title']].iloc[article_indices]</span></pre><p id="c2d9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">它获得两个参数，然后通过余弦相似值对文章进行排序。最后，它返回 10 篇最相关的文章作为结果。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h1 id="145a" class="me mf iu bd mg mh mi mj mk ml mm mn mo ka mp kb mq kd mr ke ms kg mt kh mu mv bi translated">步骤 6:结论和进一步的工作</h1><ul class=""><li id="6922" class="no np iu lc b ld mx lg my lj of ln og lr oh lv nt nu nv nw bi translated">我们已经了解了 NLP 项目基本级别的一般方法。</li><li id="1ff0" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">我们已经知道有两种常见的方法来预处理我们的文本数据:计数和 Tfidf 矢量器。</li><li id="a311" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">我们看到，我们的分类模型在多类问题上失败了，但当我们将 f1 分数作为成功指标进行评估时，我们并没有那么糟糕。</li><li id="e07d" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">写得最多的文章来自计算机科学研究，最常用的词是“神经网络”。</li><li id="e54e" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">最后，我们学习了如何使用余弦相似度来构建推荐器。</li><li id="5148" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">作为进一步的工作，我计划做一个 Flask 应用程序，使我的推荐者在线。</li></ul><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oi"><img src="../Images/aa6c4ebfbb052fbae33fdcdf2ecb7922.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*0CCiA8qXu7vMVGCIdoxyhQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">最常用的词</p></figure></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><p id="1b19" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">好了，暂时就这些了。下一篇文章再见！</p><p id="af8b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另外，如果你想了解更多，也可以看看我的 Github 简介！</p></div></div>    
</body>
</html>