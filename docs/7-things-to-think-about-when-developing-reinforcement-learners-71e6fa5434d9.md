# 培养强化学习者时要考虑的 7 件事

> 原文：<https://towardsdatascience.com/7-things-to-think-about-when-developing-reinforcement-learners-71e6fa5434d9?source=collection_archive---------43----------------------->

![](img/cde2bf5e5cc7ebafa6e75eadbfdb9914.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Franck V.](https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral) 拍摄的照片

## 尽管我们在强化学习研究方面取得了很大的进展，但是仍然缺少一个统一的框架来比较这些算法。此外，研究论文中报告的指标没有提供足够的信息。在这里，我们讨论一些潜在的东西来使分析更加严谨。

我们都知道强化学习纸是如何工作的。研究人员 A 发布了一个算法 B，算法 B 在策略上选择的环境子集上优于其他“最先进”算法的子集，这些环境子集恰好对该算法工作良好。此外，作者可能会或可能不会优化基线的超参数，但反过来，报告算法 b 的最佳运行。

不去细说是什么导致了这种研究趋势，我们可以做一些事情来改善它。为了进行有效的比较，适当的算法评估指标(除了适当的基准之外)是必不可少的。大多数情况下，研究人员用来评估算法性能的是运行的平均性能，如果你幸运的话，他们甚至会报告中位数，这可能会提供更多的信息。虽然听起来有点愤世嫉俗，但我不得不说，与机器学习的其他子领域相比，RL 是面包和黄油，例如甚至没有多次运行算法(视觉人，我在跟你说话:)。

![](img/3d9013b392ef5a6b9679988ce67721be.png)

【https://uscresl.github.io/humanoid-gail/ 

因此，这篇文章是关于我们必须看什么来比较一种强化学习算法和另一种，一个很好的灵感来源是[1]，其中作者建议了一种计算 RL 算法的各种度量的具体方法，但我们将更多地从顶层角度来看它，因为错综复杂只是更大目标的技术细节。

最直观的是，当一个人开发一个算法时，你应该看看它对训练过程中的各种因素有多敏感，比如随机种子和超参数。较少的可变性意味着算法更加稳定、健壮、可靠等。除了一般可变性之外，我们想要查看不同事物的最坏情况，即当有度量时，它在分布的较低尾部是什么。难怪[1]的作者从金融中获得灵感来定义具体的指标，因为事实证明我们也关心 RL 中的风险和可变性。总而言之，不同的“可靠性”类别可以分为以下几类:

![](img/de3127a5f67c35ee6a72443f10b9deec.png)

[http://www . IRI . UPC . edu/files/scidoc/2168-Learning-cloth-manipulation-with-examples . pdf](http://www.iri.upc.edu/files/scidoc/2168-Learning-cloth-manipulation-with-demonstrations.pdf)

## 1.推广培训期间的可变性

理想情况下，我们希望有持续的、单调的改进。这意味着平均性能应该随着每次推出和推出内而增加，并且性能不应该随着推出而(显著地)变得更差。不幸的是，大多数情况下，RL 算法往往是不稳定的。可变性的来源可能是环境，所以您要做的是考虑环境中的随机性，并在指标中对其进行调整。理想情况下，你会
希望在训练结束时获得最佳表现，而不是在中途。

## 2.不同训练跑的可变性

训练的初始条件不应该显著影响算法的性能，这就是为什么在不同的训练运行中查看不同的随机种子是重要的(视觉人，我在看着你！).还应考虑对超参数的敏感性。

## 3.评估中各次展开的可变性

我们希望该算法在评估中产生类似的性能和行为。这显示了该算法如何处理环境的随机性和不同的初始化条件。还必须考虑到的是，在首次展示中可实现的最大性能可能取决于初始状态，也应该考虑到这一点。

## 4.培训推广中的短期风险

该算法应该在最坏情况下表现出一些保证。这在训练期间有安全考虑的情况下尤其重要。在短期情况下，我们希望算法的性能不会局部波动太大。有效地观察风险意味着观察(局部)分布最低尾部的期望值，低于某个百分点(假设为 5%)。

## 5.培训推广中的长期风险

纵观整个展示，我们希望缩小展示中最差和最佳性能之间的差距。与短期情况相比，这里我们将根据整个部署来拟合分布。我们很少在首次展示中获得最差性能的预期性能值，但这是可能的。显然，同样，这可能来自算法的不稳定性，也可能来自环境的特性。

## 6.训练中的风险

与 1 相反。我们在丢弃异常值后查看可变性的地方，这里我们想看看在低概率下会发生什么，我们得到一个非常差的种子或一组非常差的超参数。

## 7.评估时跨展开的风险

相比之下，3。点，我们在评估中查看许多部署的最差情况性能。同样，可变性的来源可能是算法，也可能是环境。

在开发强化学习算法时，这些都是需要考虑的好事情。获得这些指标后，可以使用标准的统计显著性测试来确定算法 A 是否优于算法 b。在研究论文中纳入这些指标可能会为科学家节省大量时间，并且还会引起现实的期望。不限于理科，如果你来自工业界，考虑这些方面可以帮助你决定针对你的具体问题选择哪种算法。

虽然我不完全同意[1]中所写的，但这是一本好书，作者提出了一些重要的观点。更重要的是，**代码是开源的**，因此您可以立即开始运行评估。

## 参考

[1]测量强化学习算法的可靠性，陈等，ICLR 2020