<html>
<head>
<title>Neural Networks Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-overview-c3e6ab3e366b?source=collection_archive---------31-----------------------#2020-04-28">https://towardsdatascience.com/neural-networks-overview-c3e6ab3e366b?source=collection_archive---------31-----------------------#2020-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="931c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">数学、代码、绘图、情节、类比和思维导图</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e5921b52f087d6bd096f235e5e3e13a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XOg4x-1m1q7BN5scP92pg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我桌子的照片。这项工作是由爱、核桃、葡萄干和茶推动的</p></figure><p id="ad4b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di">我的</span>意图是用类比、数学、代码、图表、绘图和思维导图与你一起走过神经网络的主要概念。我们关注神经网络的构建模块:感知器。</p><p id="5cfc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在整篇文章中，我们将直面令人生畏的数学，并使用 Python 代码和 Numpy 实现它。我们还将看看使用 Scikit-learn 的等效实现。我们的结果将使用 Matplotlib 和 Plotly 可视化。在每个概念的结尾，我们将使用思维导图来构建我们的理解。</p><h1 id="34cd" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">感知器的结构</h1><p id="0a82" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">感知器的输入层是一个占位符。它包含的结点数与训练数据集中的要素数一样多。这些节点中的每一个都通过一条边连接到输出节点。我们将权重分配给边，将偏差分配给输出节点。</p><p id="25da" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个很好的类比是把感知器想象成一只乌贼。它的输入层有许多分支。臂的数量等于它需要的输入的数量。在这个类比中，让我们想象我们的数据集包含三种类型的配料:咸的、酸的和辣的。我们的鱿鱼需要三只手臂从每一种类型中抓取一种成分。手臂连接到头部，头部是鱿鱼混合配料的输出节点，并为它们的味道打分。</p><p id="3d0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">鱿鱼一生都生活在海里，很难注意到咸的成分，所以它们不会影响整体味道。然而，对于酸味和辣味，它可能是一个真正的势利小人。感知器中的权重可以理解为代表我们的配料类型对最终味道的贡献。这种偏见可以理解为影响鱿鱼味觉的一个因素，就像它的情绪或食欲一样。</p><p id="e540" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入乘以相应的权重，然后与偏差相加。这种具有各自权重的成分的混合和具有偏差的相加是一个<strong class="kx ir">仿射函数:</strong> <em class="mx"> z=𝑤x+𝑏 </em></p><p id="23e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">混合后，鱿鱼输出其对味道的印象分数。该分数被称为激活，并使用<strong class="kx ir">激活函数</strong>进行计算。激活可能只是结果<em class="mx"> z </em>，因为它是<em class="mx">，</em>在这种情况下，我们可以使用<strong class="kx ir">身份</strong>函数。它可能是一个介于-1 和 1 之间的数字，在这种情况下，我们可以使用<strong class="kx ir">双曲正切</strong>函数。也可以是 0 到 1 之间的数字，在这种情况下，我们可以使用<strong class="kx ir"> sigmoid </strong>函数。或者 0 和∞之间的一个数，在这种情况下，我们可以使用<strong class="kx ir">整流器线性单元(ReLU) </strong>功能。最后，还可以要求 Squid 为相同的输入给出多个分数，每个分数基于不同的标准在 0 到 1 之间。在这最后一种情况下，我们可能会对将所有分数相加为 1 感兴趣，对于这个任务来说，<strong class="kx ir"> softmax </strong>函数将是理想的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/a107b5af4d14630662bc400bfd0e7e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzu8q3VBYhs4xbL623C_hA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">思维导图 1:激活功能</p></figure><p id="0c89" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">选择取决于任务和最适合您的输出间隔。利用权重<em class="mx"> 𝑤 </em>和偏差<em class="mx"> 𝑏 </em>从输入向量𝑎计算 sigmoid 激活𝑎′的示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0e8191b8c4a902228016aa950c494468.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*JI3XUBztM3rL72KkRTcVSg@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 1:乙状结肠活化</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4f1b9b68cae003d26ddb81f08a4f7a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*aAWlP9YJZr-OHoMSdNYJSw@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 2: sigmoid 函数</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="a943" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">术语:</strong></p><ul class=""><li id="a8d4" class="nd ne iq kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">看到输入向量在等式 1 中用<em class="mx"> 𝑎 </em>表示，而在<em class="mx"> z=𝑤x+𝑏.中用<em class="mx"> x </em>表示，可能会令人困惑</em>原因是输入层中的节点也被称为激活。在多层感知器的情况下，有两层以上，每一层都被认为是下一层的输入层。</li><li id="fde7" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">权重和偏差被认为是𝑎′.的参数可以将权重和偏差合并成一个参数向量。这可以通过在输入向量中预先加上 1，并在最初只包含权重的向量中预先加上偏差来实现。</li></ul><p id="84dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">关于激活函数的注释:</strong>因为仿射函数是线性的，所以它们不能表示非线性数据集。由于激活函数引入的非线性，神经网络被视为<em class="mx">通用函数逼近器</em>。</p><h1 id="12b9" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">训练感知器</h1><blockquote class="nr ns nt"><p id="7034" class="kv kw mx kx b ky kz jr la lb lc ju ld nu lf lg lh nv lj lk ll nw ln lo lp lq ij bi translated"><a class="ae nx" href="https://en.wikipedia.org/wiki/De_gustibus_non_est_disputandum" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"/></a></p></blockquote><p id="8cb1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们对朋友鱿鱼的产量不满意。它运行的参数似乎是随机的。果然，偏差和权重已经初始化为:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="edfa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们想训练鱿鱼获得更好的味道。我们对准确口味的标准是一个向量<em class="mx"> y </em>,包含我们的配料数据集中每一行的实际分数。将根据<em class="mx"> y. </em>中的分数对 Squid 的性能进行评估</p><p id="a9fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">评估 Squid 性能的目的是测量其相对于目标的误差<em class="mx"> y. </em>有不同的函数来计算该误差:</p><ul class=""><li id="9326" class="nd ne iq kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated"><strong class="kx ir">均方误差(MSE): </strong>如果任务是回归并且数据集不包含离群值，这是一个不错的选择。</li><li id="6879" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><strong class="kx ir">绝对平均误差(AME): </strong>如果任务是回归并且数据集包含异常值，这是一个不错的选择。</li><li id="675d" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><strong class="kx ir">胡伯损失:</strong>中小企业和 AME 的组合</li><li id="ddca" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><strong class="kx ir">交叉熵(对数损失):</strong>如果任务是分类，这是一个很好的选择:感知机的输出是一个概率分布。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/26c95e8c982b22a19f993a57183c4b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjxKu_DYKvHFc2naxWAiog@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">思维导图 2:成本函数</p></figure><p id="ed1a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了评估我们的感知器，我们将使用均方误差函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6dc660e3b4c2142896fc1ec571f5a12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*4Y757Jf3rUIdD84k0f94OA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 3:均方误差</p></figure><p id="84a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练是通过调整参数<em class="mx"> w </em>和<em class="mx"> b </em>来最小化误差𝐶。最形象化的比喻是，当天太黑看不见的时候，你在一座山上试图下山回家。下方的 Home down 代表最小的误差𝐶。计算 MSE 的平方根，就可以得到你和家之间的直线距离。然而，知道这个距离对你在黑暗中没有任何帮助。相反，你想知道的是你下一步的方向。</p><p id="1c78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3D 世界中的方向包括三个坐标 x、y 和 z。因此问题是“家在哪里？”必须相对于 x、y 和 z 来回答。同样，问题“最小误差𝐶在哪里？”必须根据参数<em class="mx"> w </em>和<em class="mx"> b </em>进行回答。这些方向的数学表示是𝐶.的<strong class="kx ir">梯度</strong>更具体地说，𝐶: -∇𝐶.的负梯度</p><p id="2e9c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">∇𝐶是一个向量，包含 c 对每个参数的所有偏导数。对于 MSE，我们从推导等式 3 开始:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e009f41cf8c1eb68dffbc4633a9efb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*6XszfDP2F_riszFTE8dtuw@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 MSE 的梯度等于层 L 的激活减去 y</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="264f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们仅仅推导了 MSE，在得到𝐶对参数的偏导数之前，我们还有一些工作要做。在等式 4 中，𝑎取决于激活函数的输出。在 sigmoid 激活的情况下，𝑎等价于等式 1 中的𝑎′。接下来是𝐶相对于<em class="mx"> z </em>的梯度(回想一下<em class="mx"> z=𝑤x+𝑏 </em>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b87a4ca0c3e4602838d1d1bcaeef41cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*1nSov5oFE1UcIrh5590vjA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 5:<strong class="bd oc">C</strong>相对于<strong class="bd oc"> z </strong>的梯度是<strong class="bd oc"> C </strong>相对于<strong class="bd oc"> a </strong>的梯度与<strong class="bd oc"> z </strong>的 s 形导数的<strong class="bd oc">的乘积</strong></p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 5 令人生畏，直到您在 Python 代码中看到它的对等物</p></figure><p id="6d64" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们有𝐶相对于 z 的梯度。由于<em class="mx"> z </em>中的偏差乘以 1，C 相对于偏差的偏导数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7cec872bcdbbacfeae257d185d4866e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*vA0NEuiI7wpFdGLO21JIxg@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 6:C<strong class="bd oc">相对于 b<strong class="bd oc">的偏导数</strong></strong></p></figure><p id="fe0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">并且由于权重乘以输入<em class="mx"> x，</em>C 相对于权重的偏导数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/77bf132c799fdcf681a12f8d3b8fef9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*6APXCOY4sdClsxK1u-gAkw@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 7:C<strong class="bd oc">相对于 w </strong>的偏导数</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="0ac3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有了成本𝐶对参数的偏导数，我们现在就可以知道下一步回家的方向了。现在我们需要知道我们应该走多远。选择一个好的步长很重要。如果你的步子太窄，你将无法跳过你前进道路上的障碍。如果你的脚步太宽，你可能会越过下面的整个城镇，最终到达另一座山。一个好的步长介于两者之间，可以通过将偏导数(等式 6 和 7)乘以一个选定值来计算，该选定值称为<strong class="kx ir">学习速率</strong>或<strong class="kx ir"> eta </strong> : <strong class="kx ir"> 𝜂 </strong>。</p><p id="419f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们可以走下山坡了。这相当于更新我们的坐标/参数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4eb3e6fb94be8999cf15814bb2fb5e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*Em1_sRQFIoDn1rl7v3tiTA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 8:更新偏差</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/66a46883c91290e2b84832ce4dd05a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*5ccDYqCrun4YOPYRncEIHA@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 9:更新权重</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="3817" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就结束了<strong class="kx ir">梯度下降:</strong>在更新参数之前计算下一步的方向和大小的过程。通过梯度下降，我们可以训练鱿鱼获得更好的味道。我们通过让 Squid 以一些输入为食并使用等式 1 输出一个分数来做到这一点:这被称为<strong class="kx ir">前馈</strong>。分数作为𝑎代入方程 4，其结果作为𝐶相对于𝑎的梯度代入方程 5。然后，我们在等式 6 中计算𝐶相对于 z 的梯度。最后，我们计算𝐶相对于参数的梯度，并更新 Squid 的初始随机参数。这个过程被称为<strong class="kx ir">反向传播</strong>，因为它将误差从输出层反向传播到输入层。</p><p id="7cff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降是迭代的。当满足以下条件之一时，它会停止:</p><ul class=""><li id="2f58" class="nd ne iq kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">已达到定义的最大迭代次数。</li><li id="a888" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">梯度达到 0 或接近 0 的某个定义值。</li><li id="e44b" class="nd ne iq kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">验证错误已达到最小值。这叫早停。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/84e08d6853235275e31f75dbb4e00408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_xSOCtmXSvp-dbz3_zTPsA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">思维导图 3:梯度下降</p></figure><h1 id="bd9d" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">将碎片拼在一起</h1><p id="9490" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">感知器的完整实现可以从我们看到的代码片段中构建。为了不让这篇文章被代码淹没，这里有一个链接指向一个感知器的完整实现。</p><p id="71a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了看到我们的感知机在工作，让我们做一个非常简单的数据集。我们将随机生成包含一百行整数的两列。然后，我们将创建第三列来存储我们的标签。标签将等于第一列加上第二列值的一半。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/41e04f8faa8808ac655640f7c3ea55e2" rel="noopener ugc nofollow" target="_blank">生成我们的数据集</a></p></figure><p id="2768" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们知道，为了达到目标，我们的感知器必须从随机参数开始，并优化它们，使偏差等于 0，第一个权重等于 1，第二个权重等于 0.5。让我们来测试一下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/7032ccfd615eca0e4752e572a4db852c" rel="noopener ugc nofollow" target="_blank">太好了！我们的感知机已经成功优化了所有参数。</a></p></figure><h2 id="be37" class="oi mb iq bd mc oj ok dn mg ol om dp mk le on oo mm li op oq mo lm or os mq ot bi translated">在 Scikit-learn 中</h2><p id="1f85" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">我们从最基本的感知机开始。因为它正在执行回归，所以不需要激活函数。到目前为止，它所做的只是随机梯度下降。在 Scikit-learn 中，这可以通过使用 SGDRegressor 类来实现。虽然 Scikit-learn 包含一个感知器类，但它并不服务于我们当前的目的，因为它是一个分类器，而不是回归器。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/4c74f9f7baa7b053dc9a97d498fd4088" rel="noopener ugc nofollow" target="_blank">检验 SGDRegressor 优化参数前的训练</a></p></figure><h2 id="33dc" class="oi mb iq bd mc oj ok dn mg ol om dp mk le on oo mm li op oq mo lm or os mq ot bi translated">可视化梯度下降</h2><p id="a35c" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">我们可以画出我们的感知器采取的步骤，看看它达到理想参数的路径。下面是使用 Matplotlib 绘制梯度下降的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/d0889b465d4a32b8e4f474fc74b10ad7" rel="noopener ugc nofollow" target="_blank">使用 Matplotlib 可视化梯度下降</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/d9e695b98089e960a4ea9f1236ee959a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYLIkoAALI4gJeR1VRGYRg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前面代码的输出:使用 Matplotlib 可视化梯度下降</p></figure><p id="0a78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是从不同的角度和使用 Plotly 的同一个情节:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/905c695c17af1c44752a305a99b5dfac" rel="noopener ugc nofollow" target="_blank">使用 Plotly 可视化梯度下降</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/05278dbf7e81999c872adb0e015b1c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJiGrHh6oUBr6J3XWXqPcg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前面代码的输出:使用 Plotly 可视化的梯度下降</p></figure><p id="d52e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我想让你看到我们的感知机的下降导致它回家。但这不是最直的路，远非如此。我们引入了对<strong class="kx ir">功能缩放</strong>的改进。</p><h1 id="6ce4" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">特征缩放</h1><p id="9989" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">通常情况下，机器学习算法在缩放数字输入的情况下表现更好。如果没有缩放，梯度下降需要更长时间才能收敛。在 2D 的世界里，你仍然试图在黑暗中下山回家，你需要减少你离家的垂直和水平距离。如果两个距离在不同的范围内，您将花费更多的时间来缩短范围较大的距离。</p><p id="97cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，如果你需要减少的垂直距离是以千为单位，而水平距离是以个为单位，那么你的下降主要是向下爬。当你接近最小水平距离时，你仍然需要减少垂直距离。</p><p id="6dfb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将这两个距离缩放到相等的范围会使您的脚步同时影响这两个距离，这使您能够沿着直线直接向家行进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/496368c9f4e60fe6a6f8b16ee27cfba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkZc5fuve65VXErnDpQOIA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">说明比例对梯度下降的影响的简单图形</p></figure><p id="48d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两种最常见的数据缩放方式是<strong class="kx ir">规范化</strong>和<strong class="kx ir">标准化</strong>。我们将实现这两个方法，并可视化它们对梯度下降的影响。</p><h2 id="5815" class="oi mb iq bd mc oj ok dn mg ol om dp mk le on oo mm li op oq mo lm or os mq ot bi translated">正常化</h2><p id="8d04" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">也被称为<strong class="kx ir">最小-最大缩放</strong>，是一种将数据压缩到 0 和 1 之间的方法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/15c8f7386c32511e258c66d0bbc1af00.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*H_oE5pB5kH-wXwGFvso27w@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 10:最小-最大缩放或归一化</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/dc78714366a3aed1d1f9204e0b08e80a" rel="noopener ugc nofollow" target="_blank">使用 Numpy </a>标准化我们的训练数据集</p></figure><p id="ec70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">sci kit-learn 中的标准化:</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/4cc3780057f37b63b4218863a514241c" rel="noopener ugc nofollow" target="_blank">使用 Scikit-learn 标准化我们的训练数据集</a></p></figure><h2 id="787d" class="oi mb iq bd mc oj ok dn mg ol om dp mk le on oo mm li op oq mo lm or os mq ot bi translated">标准化</h2><p id="0d66" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">也称为<strong class="kx ir"> z 分数归一化，</strong>是一种将数据集中在 0 附近，标准差等于 1 的方法。𝜇是平均值，𝜎是标准差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/8837c5afba37343b93f6fa73fa867e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*K26PRPgrqM2CbV5iZXCzyg@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 11: Z 分数标准化或规范化</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/dee046527cb8a9ae117b37e5cbdd5478" rel="noopener ugc nofollow" target="_blank">使用 Numpy 标准化我们的训练数据集</a></p></figure><p id="8502" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">sci kit-learn 中的标准化:</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/f19234372b93f1aa486bd79aa2f68eb0" rel="noopener ugc nofollow" target="_blank">使用 Scikit-learn 标准化我们的训练数据集</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/9befde216b9d8b0362ad3ed983bd7bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Il_bI-D54x0rw9et5cHCw@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">思维导图 4:功能扩展</p></figure><h2 id="0dbc" class="oi mb iq bd mc oj ok dn mg ol om dp mk le on oo mm li op oq mo lm or os mq ot bi translated">特征缩放对梯度下降的影响</h2><p id="86d7" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">为了研究特征缩放的效果，我们将再训练两个感知器。目的是比较缩放和不缩放时梯度下降中参数的收敛性。</p><p id="2683" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的第一个感知机是在未缩放的数据集上训练的。第二个将在标准化数据上训练。第三个将接受标准化数据的训练。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/f1577cfda558b38934738488c321f091" rel="noopener ugc nofollow" target="_blank">用标准化数据训练两个感知机</a></p></figure><p id="95f0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在可以想象我们的三个感知器所采取的路径。下面的代码使用 Plotly:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://gist.github.com/Mehdi-Amine/6ef489a0afda130c13cc51cff45aa185" rel="noopener ugc nofollow" target="_blank">可视化三个梯度下降</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/489c350bbb6294206471e4298748ea2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5k6_bAeX2AiACFWPKj_2w@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nx" href="https://mehdi-amine.github.io/sgd-plot/sgdplot.html" rel="noopener ugc nofollow" target="_blank">上一段代码的输出:点击此处与图形</a>交互</p></figure><p id="3e1d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在缩放数据上训练的感知机采取了更直接的路径来收敛。直接路径使它们的下降速度更快，步幅更宽(可能通过增加学习速率<code class="fe pb pc pd pe b">eta</code>实现)，步数更少(可能通过减少迭代次数<code class="fe pb pc pd pe b">epochs</code>)。</p><h1 id="3e95" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">概括起来</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/7233e9ec3e11abc774adaaaf01ce8b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5HrHOGtY-5Y1TzwZyT4_g@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">思维导图 5:概念总结</p></figure><h1 id="e524" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">参考</h1><p id="2dfc" class="pw-post-body-paragraph kv kw iq kx b ky ms jr la lb mt ju ld le mu lg lh li mv lk ll lm mw lo lp lq ij bi translated">A.Géron，<a class="ae nx" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用 Scikit-Learn、Keras 和 TensorFlow 进行动手机器学习</a> (2019)</p><p id="421e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">I. Goodfellow，Y. Bengio 和 a .库维尔，<a class="ae nx" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a> (2016)</p><p id="5eb5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">米（meter 的缩写））尼尔森，<a class="ae nx" href="http://neuralnetworksanddeeplearning.com/index.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a> (2019)</p><p id="b12c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">米（meter 的缩写））胺，<em class="mx">神经网络概述:</em> <a class="ae nx" href="https://github.com/Mehdi-Amine/neural-networks-overview-of-my-understanding/blob/gh-pages/perceptron-overview.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本和游乐场</a></p><p id="f56d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">米（meter 的缩写））胺，<em class="mx">神经网络概述:</em> <a class="ae nx" href="https://gist.github.com/Mehdi-Amine" rel="noopener ugc nofollow" target="_blank">代码片段</a></p><p id="68c6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">XMind，<a class="ae nx" href="https://www.xmind.net/" rel="noopener ugc nofollow" target="_blank">思维导图软件</a></p></div></div>    
</body>
</html>