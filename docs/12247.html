<html>
<head>
<title>Data Preprocessing and Network Building in CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN 的数据预处理和网络构建</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b?source=collection_archive---------2-----------------------#2020-08-24">https://towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b?source=collection_archive---------2-----------------------#2020-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3810" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习建立一个典型的端到端管道来训练 CNN</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6878504f0362eb6601ab8f475d3e1e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gFJcaJzdmXah1yZX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4e3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将经历训练卷积神经网络的端到端管道，即组织数据到目录、预处理、数据扩充、模型建立等。</p><p id="8ac6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在图像处理中常用的<strong class="lb iu">数据预处理技术</strong>上花费大量时间。这是因为在大多数深度学习项目中，预处理大约需要你 50–80%的时间，知道一些有用的技巧会对你的项目有很大帮助。我们将使用 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/alxmamaev/flowers-recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> flowers 数据集</strong> </a>来演示关键概念。为了直接进入代码，Kaggle 上发布了一个附带的<a class="ae ky" href="https://www.kaggle.com/tanyadayanand/end-to-end-pipeline-for-training-cnns-resnet" rel="noopener ugc nofollow" target="_blank">笔记本</a>(请<strong class="lb iu">使用 CPU </strong>运行代码的初始部分，使用<strong class="lb iu"> GPU </strong>进行模型训练)。</p><div class="lv lw gp gr lx ly"><a href="https://www.kaggle.com/tanyadayanand/end-to-end-pipeline-for-training-cnns-resnet" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">用于培训 CNN、ResNet 的端到端管道</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">www.kaggle.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><h1 id="cfbb" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">导入数据集</h1><p id="e1a0" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">让我们从导入必要的库和加载数据集开始。这是每个数据分析过程中必不可少的一步。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="82a0" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># Importing necessary libraries<br/></em></strong>import keras<br/>import tensorflow<br/>from skimage import io<br/>import os<br/>import glob<br/>import numpy as np<br/>import random<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="1025" class="np mo it nl b gy nv nr l ns nt"><strong class="nl iu"><em class="nu"># Importing and Loading the data into data frame<br/>#class 1 - Rose, class 0- Daisy<br/></em></strong>DATASET_PATH = '../input/flowers-recognition/flowers/'<br/>flowers_cls = ['daisy', 'rose']<strong class="nl iu"><em class="nu"><br/></em></strong><br/><strong class="nl iu"><em class="nu"># glob through the directory (returns a list of all file paths)<br/></em></strong>flower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')<br/>flower_path = glob.glob(flower_path)</span><span id="9af1" class="np mo it nl b gy nv nr l ns nt"><strong class="nl iu"><em class="nu"># access some element (a file) from the list</em></strong><br/>image = io.imread(flower_path[251])</span></pre><h1 id="b1d7" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据预处理</h1><h2 id="cd71" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated">图像—通道和尺寸</h2><p id="a1ac" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">图像有不同的形状和大小<strong class="lb iu">。</strong>他们也来自<strong class="lb iu">不同的来源</strong>。例如，一些图像是我们所说的“自然图像”，这意味着它们是在<strong class="lb iu">真实世界</strong>中以<strong class="lb iu">颜色</strong>拍摄的。例如:</p><ul class=""><li id="58d1" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated">花的图片是自然的图像。</li><li id="9d2c" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">x 光图像是<em class="nu">而不是</em>自然图像。</li></ul><p id="346f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到所有这些变化，我们需要对任何图像数据进行一些预处理。RGB 是最流行的编码格式，我们遇到的大多数“自然图像”都是 RGB 格式的。此外，数据预处理的第一步是<strong class="lb iu">使图像尺寸相同。让我们继续讨论如何改变图像的形状和形式。</strong></p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="02b2" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># plotting the original image and the RGB channels</em></strong><br/>f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True)<br/>f.set_figwidth(15)<br/>ax1.imshow(image)<br/><br/><strong class="nl iu"><em class="nu"># RGB channels</em><br/><em class="nu"># CHANNELID : 0 for Red, 1 for Green, 2 for Blue. </em></strong><br/>ax2.imshow(image[:, : , 0]) <em class="nu">#Red</em><br/>ax3.imshow(image[:, : , 1]) <em class="nu">#Green</em><br/>ax4.imshow(image[:, : , 2]) <em class="nu">#Blue</em><br/>f.suptitle('Different Channels of Image')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/175e01172c11c5db0cf5c3863231af9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iz4XYTJedKq53PiXafIIZw.png"/></div></div></figure><h2 id="e43c" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">形态变换</strong></h2><p id="e3bf" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">术语<em class="nu">形态变换</em>是指涉及图像的<strong class="lb iu">形状和形式</strong>的任何修改。这些非常常用于图像分析任务。虽然它们适用于所有类型的图像，但对于非自然的图像(来自真实世界的图片以外的来源)来说，它们尤其强大。典型的变换是侵蚀、扩张、张开和闭合。现在让我们看看实现这些形态变换的一些代码。</p><p id="f866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1。阈值处理</strong></p><p id="4785" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种更简单的操作，我们把亮度高于某个阈值的所有像素转换成 1；具有小于阈值的值的像素被转换为零。这产生了一个二进制图像<em class="nu">。</em></p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="81b7" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># bin_image will be a (240, 320) True/False array</em><br/><em class="nu">#The range of pixel varies between 0 to 255</em><br/><em class="nu">#The pixel having black is more close to 0 and pixel which is white is more close to 255</em><br/><em class="nu"># 125 is Arbitrary heuristic measure halfway between 1 and 255 (the range of image pixel) </em></strong><br/>bin_image = image[:, :, 0] &gt; 125<br/>plot_image([image, bin_image], cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/822aaa12e84b7cda0ae4a3bf65e31687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbYLw3x0tiNqxFvSWkVpXg.png"/></div></div></figure><p id="708e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。侵蚀、扩张、打开&amp;关闭</strong></p><p id="c1ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">侵蚀</strong>缩小亮区，放大暗区。<strong class="lb iu">另一方面，膨胀</strong>正好相反——它缩小黑暗区域，扩大明亮区域。</p><p id="5cc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">开口</strong>是侵蚀后的扩张。开口可以去除小亮点(即“盐”)，连接小暗裂纹。这往往会“打开”(亮)特征之间的(暗)间隙。</p><p id="346a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">关闭</strong>是扩张后侵蚀。闭合可以去除小暗斑(即“辣椒”)，连接小亮裂。这有助于“封闭”(亮)特征之间的(暗)间隙。</p><p id="2fbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都可以使用<code class="fe ox oy oz nl b">skimage.morphology</code>模块来完成。基本的想法是让一个特定大小的<strong class="lb iu">圆盘</strong>(下面的 3)在图像周围移动，并使用它应用这些变换。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="eea2" class="np mo it nl b gy nq nr l ns nt">from skimage.morphology import binary_closing, binary_dilation, binary_erosion, binary_opening<br/>from skimage.morphology import selem<br/><br/><strong class="nl iu"><em class="nu"># use a disk of radius 3</em></strong><br/>selem = selem.disk(3)<br/><br/><strong class="nl iu"><em class="nu"># oprning and closing</em></strong><br/>open_img = binary_opening(bin_image, selem)<br/>close_img = binary_closing(bin_image, selem)<br/><strong class="nl iu"><em class="nu"><br/># erosion and dilation</em></strong><br/>eroded_img = binary_erosion(bin_image, selem)<br/>dilated_img = binary_dilation(bin_image, selem)<br/><br/>plot_image([bin_image, open_img, close_img, eroded_img, dilated_img], cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/b99784cac665f1a019cc21da3a889e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bbyeA_tAcRdISH17L14wQ.png"/></div></div></figure><h2 id="56cf" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">正常化</strong></h2><p id="1881" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">归一化是预处理部分中最关键的步骤。这是指重新调整像素值，使它们位于一个限定的范围内。这样做的原因之一是有助于传播梯度的问题。我们将讨论多种归一化图像的方法。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0266" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu">#way1-this is common technique followed in case of RGB images </em></strong><br/>norm1_image = image/255<br/><strong class="nl iu"><em class="nu">#way2-in case of medical Images/non natural images</em></strong><em class="nu"> </em><br/>norm2_image = image - np.min(image)/np.max(image) - np.min(image)<br/><strong class="nl iu"><em class="nu">#way3-in case of medical Images/non natural images </em></strong><br/>norm3_image = image - np.percentile(image,5)/ np.percentile(image,95) - np.percentile(image,5)<br/><br/>plot_image([image, norm1_image, norm2_image, norm3_image], cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/89b6a17c4f91ffa3933caab4d06759f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNMPY6BOrjUzImP1RnLE-Q.png"/></div></div></figure><h2 id="8726" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">增强</strong></h2><p id="05cf" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">这就把我们带到了数据预处理的下一个方面——数据扩充。很多时候，我们拥有的数据量不足以很好地完成分类任务。在这种情况下，我们执行<strong class="lb iu">数据扩充</strong>。例如，如果我们正在处理将宝石分为不同类型的数据集，我们可能没有足够数量的图像(因为高质量的图像很难获得)。在这种情况下，我们可以执行扩充来增加数据集的大小。增强通常用于基于图像的深度学习任务，以增加训练数据的数量和方差。增强只能在训练集上进行，而不能在验证集上进行。</p><p id="8e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所知，<strong class="lb iu">汇集</strong>增加了<strong class="lb iu">不变性。</strong>如果一张狗的照片在图像的左上角，通过池化，您将能够识别出狗是否在左上角的左/右/上/下。但是训练数据由像<strong class="lb iu"> <em class="nu">翻转、旋转、裁剪、平移、照明、缩放、添加噪声、</em> </strong>等数据增强组成。模型学习所有这些变化。这大大提高了模型的准确性。因此，即使狗出现在图像的任何角落，模型也能够以很高的准确度识别它。</p><p id="7013" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能有多种类型的增强。基本类型使用以下变换类型之一来变换原始图像:</p><ol class=""><li id="800f" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu pc on oo op bi translated">线性变换</li><li id="674e" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu pc on oo op bi translated">仿射变换</li></ol><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="da6d" class="np mo it nl b gy nq nr l ns nt">from skimage import transform as tf<br/><strong class="nl iu"><br/><em class="nu"># flip left-right, up-down</em></strong><br/>image_flipr = np.fliplr(image)<br/>image_flipud = np.flipud(image)<br/><br/>plot_image([image, image_flipr, image_flipud])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/a45712b65b134cc5ab2e83080394dbf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxG0XFxrKT3V13nZHvn9XQ.png"/></div></div></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="aa2d" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># specify x and y coordinates to be used for shifting (mid points)</em></strong><br/>shift_x, shift_y = image.shape[0]/2, image.shape[1]/2<br/><br/><strong class="nl iu"><em class="nu"># translation by certain units</em></strong><br/>matrix_to_topleft = tf.SimilarityTransform(translation=[-shift_x, -shift_y])<br/>matrix_to_center = tf.SimilarityTransform(translation=[shift_x, shift_y])<br/><br/><strong class="nl iu"><em class="nu"># rotation</em></strong><br/>rot_transforms =  tf.AffineTransform(rotation=np.deg2rad(45))<br/>rot_matrix = matrix_to_topleft + rot_transforms + matrix_to_center<br/>rot_image = tf.warp(image, rot_matrix)<br/><br/><strong class="nl iu"><em class="nu"># scaling </em></strong><br/>scale_transforms = tf.AffineTransform(scale=(2, 2))<br/>scale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center<br/>scale_image_zoom_out = tf.warp(image, scale_matrix)<br/><br/>scale_transforms = tf.AffineTransform(scale=(0.5, 0.5))<br/>scale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center<br/>scale_image_zoom_in = tf.warp(image, scale_matrix)<br/><br/><strong class="nl iu"><em class="nu"># translation</em></strong><br/>transaltion_transforms = tf.AffineTransform(translation=(50, 50))<br/>translated_image = tf.warp(image, transaltion_transforms)<br/><br/><br/>plot_image([image, rot_image, scale_image_zoom_out, scale_image_zoom_in, translated_image])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/27869f07f408c066652b475c9cc417f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SI26fWVRnY-q6Mh90vfykA.png"/></div></div></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="1dd0" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># shear transforms</em></strong><br/>shear_transforms = tf.AffineTransform(shear=np.deg2rad(45))<br/>shear_matrix = matrix_to_topleft + shear_transforms + matrix_to_center<br/>shear_image = tf.warp(image, shear_matrix)<br/><br/>bright_jitter = image*0.999 + np.zeros_like(image)*0.001<br/><br/>plot_image([image, shear_image, bright_jitter])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/1915b899669e34bc631e29e741906dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4-M_x9YhMzsKozc5i9C8w.png"/></div></div></figure><h1 id="b474" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">网络建设</h1><p id="cf8b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">现在让我们构建和训练模型。</p><h2 id="a37e" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">选择架构</strong></h2><p id="db16" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在本节中，我们将使用'<strong class="lb iu"> ResNet </strong>'架构。由于 ResNets 在行业中已经变得相当普遍，所以有必要花一些时间来理解其架构的重要元素。先说这里提出的 原架构<a class="ae ky" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">。还有，在 2016 年，ResNet 团队已经在原架构<a class="ae ky" href="https://arxiv.org/pdf/1603.05027.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这里</strong> </a>提出了一些改进。利用这些修改，他们已经训练了超过 1000 层</strong>的<strong class="lb iu">网络(例如<a class="ae ky" href="https://github.com/KaimingHe/resnet-1k-layers" rel="noopener ugc nofollow" target="_blank"> ResNet-1001 </a>)。</strong></a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/7f09421fa2a01ba59da785323e16e878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*28in2B1mTlcCLeODc1rrTA.png"/></div></figure><p id="a402" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里使用的“ResNet builder”模块基本上是一个 Python 模块，包含了 ResNet 的所有构建模块。我们将使用该模块导入 ResNet 的变体(ResNet-18、ResNet-34 等。).resnet.py 模块取自<a class="ae ky" href="https://www.kaggle.com/tanyadayanand/resnet" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">此处</strong> </a>。它最大的好处是“跳过连接”机制允许非常深的网络。</p><h2 id="fa80" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated">运行数据生成器</h2><p id="6746" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">数据生成器支持预处理—它将图像归一化(除以 255)并裁剪图像的中心部分<strong class="lb iu"> (100 x 100) </strong>。</p><p id="9214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有特别的理由将<strong class="lb iu"> 100 </strong>作为尺寸，但是已经选择了它，以便我们可以处理所有大于 100*100 尺寸的图像。如果图像的任何尺寸(高度或宽度)小于 100 像素，则该图像将被自动删除。你可以根据需要把它改成 150 或者 200。</p><p id="d72d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们设置<strong class="lb iu">数据发生器</strong>。下面的代码设置了一个定制的数据生成器，与 keras API 自带的<a class="ae ky" href="https://keras.io/preprocessing/image/" rel="noopener ugc nofollow" target="_blank">略有不同。使用定制生成器的原因是为了能够根据手头的问题修改它(可定制性)。</a></p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="2bee" class="np mo it nl b gy nq nr l ns nt">import numpy as np<br/>import keras<br/><br/>class <strong class="nl iu">DataGenerator</strong>(keras.utils.Sequence):<br/>   <strong class="nl iu"> 'Generates data for Keras'</strong><br/>    <br/>    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], <br/>                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):<br/>       <strong class="nl iu"> <em class="nu">"""</em><br/><em class="nu">        Initialise the data generator</em><br/><em class="nu">        """</em></strong><br/>        self.dim = dim<br/>        self.batch_size = batch_size<br/>        self.labels = {}<br/>        self.list_IDs = []<br/>        <br/>        <strong class="nl iu"><em class="nu"># glob through directory of each class </em></strong><br/>        for i, cls <strong class="nl iu">in</strong> enumerate(flowers_cls):<br/>            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))<br/>            brk_point = int(len(paths)*0.8)<br/>            if mode == 'train':<br/>                paths = paths[:brk_point]<br/>            else:<br/>                paths = paths[brk_point:]<br/>            if ablation <strong class="nl iu">is</strong> <strong class="nl iu">not</strong> None:<br/>                paths = paths[:ablation]<br/>            self.list_IDs += paths<br/>            self.labels.update({p:i for p <strong class="nl iu">in</strong> paths})<br/>            <br/>        self.n_channels = n_channels<br/>        self.n_classes = len(flowers_cls)<br/>        self.shuffle = shuffle<br/>        self.on_epoch_end()<br/><br/>    def __len__(self):<br/>        <strong class="nl iu">'Denotes the number of batches per epoch'</strong><br/>        return int(np.floor(len(self.list_IDs) / self.batch_size))<br/><br/>    def __getitem__(self, index):<br/>        <strong class="nl iu">'Generate one batch of data'</strong><br/>        <strong class="nl iu"><em class="nu"># Generate indexes of the batch</em></strong><br/>        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]<br/><br/>        <strong class="nl iu"><em class="nu"># Find list of IDs</em></strong><br/>        list_IDs_temp = [self.list_IDs[k] for k <strong class="nl iu">in</strong> indexes]<br/><br/>        <strong class="nl iu"><em class="nu"># Generate data</em></strong><br/>        X, y = self.__data_generation(list_IDs_temp)<br/><br/>        return X, y<br/><br/>    def on_epoch_end(self):<br/>        <strong class="nl iu">'Updates indexes after each epoch'</strong><br/>        self.indexes = np.arange(len(self.list_IDs))<br/>        if self.shuffle == True:<br/>            np.random.shuffle(self.indexes)<br/><br/>    def __data_generation(self, list_IDs_temp):<br/>        <strong class="nl iu">'Generates data containing batch_size samples' <em class="nu"># X : (n_samples, *dim, n_channels)</em></strong><br/>       <strong class="nl iu"> <em class="nu"># Initialization</em></strong><br/>        X = np.empty((self.batch_size, *self.dim, self.n_channels))<br/>        y = np.empty((self.batch_size), dtype=int)<br/>        <br/>        delete_rows = []<br/><br/>       <strong class="nl iu"> <em class="nu"># Generate data</em></strong><br/>        for i, ID <strong class="nl iu">in</strong> enumerate(list_IDs_temp):<br/>            <strong class="nl iu"><em class="nu"># Store sample</em></strong><br/>            img = io.imread(ID)<br/>            img = img/255<br/>            if img.shape[0] &gt; 100 <strong class="nl iu">and</strong> img.shape[1] &gt; 100:<br/>                h, w, _ = img.shape<br/>                img = img[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]<br/>            else:<br/>                delete_rows.append(i)<br/>                continue<br/>            <br/>            X[i,] = img<br/>          <br/>            <strong class="nl iu"><em class="nu"># Store class</em></strong><br/>            y[i] = self.labels[ID]<br/>        <br/>        X = np.delete(X, delete_rows, axis=0)<br/>        y = np.delete(y, delete_rows, axis=0)<br/>        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)</span></pre><p id="d588" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将训练数据存储在 nn 目录中(如果有 nn 类的话)。对于给定的批量大小，我们希望生成批量数据点，并将它们提供给模型。</p><p id="4f5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个<code class="fe ox oy oz nl b">for</code>循环“遍历”每个类(目录)。对于每个类，它将每个图像的路径存储在列表<code class="fe ox oy oz nl b">paths</code>中。在训练模式中，它将<code class="fe ox oy oz nl b">paths</code>子集化，以包含前 80%的图像；在验证模式下，它对最后 20%进行子集划分。在消融实验的特殊情况下，它只是对每一类的第一个<code class="fe ox oy oz nl b">ablation</code>图像进行子集划分。</p><p id="e0df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将所有图像(所有类别)的路径存储在一个组合列表<code class="fe ox oy oz nl b">self.list_IDs</code>中。字典<code class="fe ox oy oz nl b">self.labels</code>包含标签(作为<code class="fe ox oy oz nl b">path: class_number (0/1)</code>的键:值对)。</p><p id="0cb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在循环之后，我们调用方法<code class="fe ox oy oz nl b">on_epoch_end()</code>，该方法创建一个长度为<code class="fe ox oy oz nl b">self.list_IDs</code>的数组<code class="fe ox oy oz nl b">self.indexes</code>，并对它们进行混洗(在每个时期的末尾混洗所有的数据点)。</p><p id="0bbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ox oy oz nl b">_getitem_</code>方法使用(混洗)数组<code class="fe ox oy oz nl b">self.indexes</code>从路径列表<code class="fe ox oy oz nl b">self.list_IDs</code>中选择<code class="fe ox oy oz nl b">batch_size</code>个条目(路径)。</p><p id="f06f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，方法<code class="fe ox oy oz nl b">__data_generation</code>返回一批图像作为对 X，y，其中 X 的形状为<code class="fe ox oy oz nl b">(batch_size, height, width, channels)</code>，y 的形状为<code class="fe ox oy oz nl b">(batch size, )</code>。注意<code class="fe ox oy oz nl b">__data_generation</code>也做一些预处理——它标准化图像(除以 255)并裁剪图像中心 100 x 100 的部分。因此，每个图像具有形状<code class="fe ox oy oz nl b">(100, 100, num_channels)</code>。如果图像的任何尺寸(高度或宽度)小于 100 像素，该图像将被删除。</p><h2 id="84f8" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">消融实验</strong></h2><p id="06a3" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">这些指的是获取一小块数据并在其上运行您的模型——这有助于判断模型是否正在运行。这被称为<strong class="lb iu">消融实验。</strong></p><p id="bd4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">构建网络的第一步是让网络在数据集上运行。让我们试着仅在几幅图像和一个时期上拟合网络。注意，因为指定了<code class="fe ox oy oz nl b">ablation=100</code>，所以使用每类 100 个图像，所以总批次数为<code class="fe ox oy oz nl b">np.floor(200/32)</code> = 6。</p><p id="b8a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，<code class="fe ox oy oz nl b">DataGenerator</code>类“继承”自<code class="fe ox oy oz nl b">keras.utils.Sequence</code>类，因此它拥有基础<code class="fe ox oy oz nl b">keras.utils.Sequence</code>类的所有功能(比如<code class="fe ox oy oz nl b">model.fit_generator</code>方法)。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="bb8f" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># using resnet 18</em></strong><br/>model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)<br/>model.compile(loss='categorical_crossentropy', optimizer='SGD',<br/>              metrics=['accuracy'])<br/><br/><strong class="nl iu"><em class="nu"># create data generator objects in train and val mode</em><br/><em class="nu"># specify ablation=number of data points to train on</em></strong><br/>training_generator = DataGenerator('train', ablation=100)<br/>validation_generator = DataGenerator('val', ablation=100)<br/><br/><strong class="nl iu"><em class="nu"># fit: this will fit the net on 'ablation' samples, only 1 epoch</em></strong><br/>model.fit_generator(generator=training_generator,<br/>                    validation_data=validation_generator,<br/>                    epochs=1,)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/9ee55ccb5d3c39b8002494482f4d24fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTJPFJoj_bwjY5YfAUsHeg.png"/></div></div></figure><h2 id="0e99" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">过拟合训练数据</strong></h2><p id="2e1f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">下一步是尝试在训练数据上过度拟合模型。为什么我们要故意夸大我们的数据呢？简单地说，这将告诉我们网络是否能够学习训练集中的模式。这将告诉您模型的行为是否符合预期。</p><p id="4260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用 ablation=100(即在每类的 100 个图像上训练)，因此它仍然是一个非常小的数据集，并且我们将使用 20 个时期。在每个时期，将使用 200/32=6 批。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="6211" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># resnet 18</em></strong><br/>model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)<br/>model.compile(loss='categorical_crossentropy',optimizer='SGD',<br/>              metrics=['accuracy'])<br/><br/><strong class="nl iu"><em class="nu"># generators</em></strong><br/>training_generator = DataGenerator('train', ablation=100)<br/>validation_generator = DataGenerator('val', ablation=100)<br/><br/><strong class="nl iu"><em class="nu"># fit</em></strong><br/>model.fit_generator(generator=training_generator,<br/>                    validation_data=validation_generator,<br/>                    epochs=20)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/3d2dcf47b76f3f15fff77246421461ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iFcME-rkVHZWPbCZblLc1Q.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练精度随着每个历元不断增加</p></figure><p id="e154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果表明，训练精度随着每个历元不断提高。验证准确性也会增加，然后趋于平稳，这是“良好拟合”的标志，即我们知道该模型至少能够从一个小数据集学习，因此我们可以希望它也能够从整个数据集学习。</p><p id="a4ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，对任何模型的一个好的测试是检查它是否能够<strong class="lb iu">过度适应训练数据</strong>(即训练损失随着时期持续减少)。这种技术在深度学习中特别有用，因为大多数深度学习模型都是在大型数据集上训练的，如果它们无法过度适应小版本，那么它们就不太可能从大版本中学习。</p><h2 id="66c5" class="np mo it bd mp nw nx dn mt ny nz dp mx li oa ob mz lm oc od nb lq oe of nd og bi translated"><strong class="ak">超参数调谐</strong></h2><p id="ccbe" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们在数据集的一小块上训练模型，并确认模型可以从数据集学习(通过过度拟合来指示)。在修复了模型和数据扩充之后，我们现在需要找到优化器的学习率(这里是 SGD)。首先，让我们列出想要优化的超参数:</p><ol class=""><li id="53be" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu pc on oo op bi translated">学习速度和变化+优化器</li><li id="0694" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu pc on oo op bi translated">增强技术</li></ol><p id="ba10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本思想是随着超参数的各种值的增加，跟踪验证损失。</p><p id="ab1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Keras 回调</strong></p><p id="a6ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在你继续之前，让我们讨论一下<strong class="lb iu">回调</strong>。回调基本上是您希望在培训的特定情况下执行的操作。例如，我们希望在每个时期结束时执行存储丢失的操作(这里的实例是一个时期的结束)。</p><p id="7f2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形式上，回调只是一个函数(如果您想要执行单个操作)，或者一个函数列表(如果您想要执行多个操作)，它们将在特定事件(一个时期的结束、每个批次的开始、准确性达到稳定状态时等)时执行。).Keras 通过类<code class="fe ox oy oz nl b">keras.callbacks.Callback</code>提供了一些非常有用的回调功能。</p><p id="09ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Keras 有很多内置的回调函数(这里列出的<a class="ae ky" href="https://keras.io/callbacks/" rel="noopener ugc nofollow" target="_blank"/>)。<strong class="lb iu">在 keras </strong>中创建自定义回调的一般方法是:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="4256" class="np mo it nl b gy nq nr l ns nt">from keras import optimizers<br/>from keras.callbacks import *<br/><br/><strong class="nl iu"><em class="nu"># range of learning rates to tune</em></strong><br/>hyper_parameters_for_lr = [0.1, 0.01, 0.001]<br/><br/><strong class="nl iu"><em class="nu"># callback to append loss</em></strong><br/>class <strong class="nl iu">LossHistory</strong>(keras.callbacks.Callback):<br/>    def on_train_begin(self, logs={}):<br/>        self.losses = []<br/><br/>    def on_epoch_end(self, epoch, logs={}):<br/>        self.losses.append(logs.get('loss'))<br/><br/><strong class="nl iu"><em class="nu"># instantiate a LossHistory() object to store histories</em></strong><br/>history = LossHistory()<br/>plot_data = {}<br/><br/><strong class="nl iu"><em class="nu"># for each hyperparam: train the model and plot loss history</em></strong><br/>for lr <strong class="nl iu">in</strong> hyper_parameters_for_lr:<br/>    print ('<strong class="nl iu">\n\n</strong>'+'=='*20 + '   Checking for LR=<strong class="nl iu">{}</strong>  '.format(lr) + '=='*20 )<br/>    sgd = optimizers.SGD(lr=lr, clipnorm=1.)<br/>    <br/><strong class="nl iu"><em class="nu">    # model and generators</em></strong><br/>    model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)<br/>    model.compile(loss='categorical_crossentropy',optimizer= sgd,<br/>                  metrics=['accuracy'])<br/>    training_generator = DataGenerator('train', ablation=100)<br/>    validation_generator = DataGenerator('val', ablation=100)<br/>    model.fit_generator(generator=training_generator,<br/>                        validation_data=validation_generator,<br/>                        epochs=3, callbacks=[history])<br/>    <br/><strong class="nl iu">    <em class="nu"># plot loss history</em></strong><br/>    plot_data[lr] = history.losses</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/7f988fb1d7388c432f1cd3bb12456675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HxZm-9i5qwSoTi_zkNRH2w.gif"/></div></div></figure><p id="ad85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们创建了一个自定义回调，在每个时期结束时将丢失的数据追加到一个列表中。注意，<code class="fe ox oy oz nl b">logs</code>是<code class="fe ox oy oz nl b">keras.callbacks.Callback</code>的一个属性(一个字典)，我们用它来获得键‘loss’的值。这个字典的其他一些关键字是<code class="fe ox oy oz nl b">acc</code>、<code class="fe ox oy oz nl b">val_loss</code>等。</p><p id="bf6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了告诉模型我们想要使用回调，我们创建了一个名为<code class="fe ox oy oz nl b">history</code>的<code class="fe ox oy oz nl b">LossHistory</code>对象，并使用<code class="fe ox oy oz nl b">callbacks=[history]</code>将其传递给<code class="fe ox oy oz nl b">model.fit_generator</code>。在这种情况下，我们只有一个回调<code class="fe ox oy oz nl b">history</code>，尽管您可以通过这个列表传递多个回调对象(多个回调的示例在下一节中——参见<code class="fe ox oy oz nl b">DecayLR()</code>的代码块)。</p><p id="a4df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu">我们调整了学习率超参数</strong>，并观察到与 0.01 和 0.001 相比，0.1 是最佳的学习率。然而，在整个训练过程中使用如此高的学习率并不是一个好主意，因为损失可能在稍后开始在最小值附近振荡。因此，在训练开始时，我们为模型使用高学习率来快速学习，但是随着我们进一步训练并向最小值前进，我们逐渐降低学习率。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="42d6" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># plot loss history for each value of hyperparameter</em></strong><br/>f, axes = plt.subplots(1, 3, sharey=True)<br/>f.set_figwidth(15)<br/><br/>plt.setp(axes, xticks=np.arange(0, len(plot_data[0.01]), 1)+1)<br/><br/>for i, lr <strong class="nl iu">in</strong> enumerate(plot_data.keys()):<br/>    axes[i].plot(np.arange(len(plot_data[lr]))+1, plot_data[lr])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/5f71e2c6d1c114deb966f170c8d47969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LhD2m4ruC9jEMejgX5RJw.png"/></div></div></figure><p id="115f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的结果表明 0.1 的学习率是最好的，尽管在整个训练中使用如此高的学习率通常不是一个好主意。因此，我们应该使用<strong class="lb iu">学习率衰减</strong>——从高学习率开始，并随着每个时期衰减。</p><p id="a60e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用另一个<strong class="lb iu">自定义回调</strong> ( <code class="fe ox oy oz nl b">DecayLR</code>)来衰减每个时期结束时的学习率。衰减率被指定为 0.5 ^历元。另外，注意这次我们告诉模型<strong class="lb iu">使用两个回调</strong>(作为列表<code class="fe ox oy oz nl b">callbacks=[history, decay]</code>传递给<code class="fe ox oy oz nl b">model.fit_generator</code>)。</p><p id="eb15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我们在这里使用了自己的自定义衰减实现，但是您也可以使用内置在<a class="ae ky" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank"> keras 优化器</a>中的实现(使用<code class="fe ox oy oz nl b">decay</code>参数)。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="f9a2" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># learning rate decay</em></strong><br/>class <strong class="nl iu">DecayLR</strong>(keras.callbacks.Callback):<br/>    def __init__(self, base_lr=0.001, decay_epoch=1):<br/>        super(DecayLR, self).__init__()<br/>        self.base_lr = base_lr<br/>        self.decay_epoch = decay_epoch <br/>        self.lr_history = []<br/>        <br/>    <strong class="nl iu"><em class="nu"># set lr on_train_begin</em></strong><br/>    def on_train_begin(self, logs={}):<br/>        K.set_value(self.model.optimizer.lr, self.base_lr)<br/><br/><strong class="nl iu">    <em class="nu"># change learning rate at the end of epoch</em></strong><br/>    def on_epoch_end(self, epoch, logs={}):<br/>        new_lr = self.base_lr * (0.5 ** (epoch // self.decay_epoch))<br/>        self.lr_history.append(K.get_value(self.model.optimizer.lr))<br/>        K.set_value(self.model.optimizer.lr, new_lr)<br/><br/><strong class="nl iu"><em class="nu"># to store loss history</em></strong><br/>history = LossHistory()<br/>plot_data = {}<br/><br/><strong class="nl iu"><em class="nu"># start with lr=0.1</em></strong><br/>decay = DecayLR(base_lr=0.1)<br/><br/><strong class="nl iu"><em class="nu"># model</em></strong><br/>sgd = optimizers.SGD()<br/>model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)<br/>model.compile(loss='categorical_crossentropy',optimizer= sgd,<br/>              metrics=['accuracy'])<br/>training_generator = DataGenerator('train', ablation=100)<br/>validation_generator = DataGenerator('val', ablation=100)<br/><br/>model.fit_generator(generator=training_generator,<br/>                    validation_data=validation_generator,<br/>                    epochs=3, callbacks=[history, decay])<br/><br/>plot_data[lr] = decay.lr_history</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/ee2e56f69e113b46def8fd214244ab71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21aNAzG023AddGqm9EVKBQ.png"/></div></div></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="5c25" class="np mo it nl b gy nq nr l ns nt">plt.plot(np.arange(len(decay.lr_history)), decay.lr_history)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/c66a7b9d3b6df34cbfccf44252de7f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*6XrnSEaF5-dHeSelgITysA.png"/></div></figure><p id="374b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">增强技术</strong></p><p id="2f84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们编写一些代码来实现数据扩充。扩充通常是通过数据生成器完成的，即扩充的数据是动态地批量生成的。你可以使用内置的 keras <code class="fe ox oy oz nl b">ImageDataGenerator</code>或者编写你自己的数据生成器(如果你想的话，可以定制一些特性等)。下面的代码显示了如何实现这些。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="aa94" class="np mo it nl b gy nq nr l ns nt">import numpy as np<br/>import keras<br/><br/><strong class="nl iu"><em class="nu"># data generator with augmentation</em></strong><br/>class <strong class="nl iu">AugmentedDataGenerator</strong>(keras.utils.Sequence):<br/>    <strong class="nl iu">'Generates data for Keras'</strong><br/>    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], <br/>                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):<br/>        <strong class="nl iu">'Initialization'</strong><br/>        self.dim = dim<br/>        self.batch_size = batch_size<br/>        self.labels = {}<br/>        self.list_IDs = []<br/>        self.mode = mode<br/>        <br/>        for i, cls <strong class="nl iu">in</strong> enumerate(flowers_cls):<br/>            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))<br/>            brk_point = int(len(paths)*0.8)<br/>            if self.mode == 'train':<br/>                paths = paths[:brk_point]<br/>            else:<br/>                paths = paths[brk_point:]<br/>            if ablation <strong class="nl iu">is</strong> <strong class="nl iu">not</strong> None:<br/>                paths = paths[:ablation]<br/>            self.list_IDs += paths<br/>            self.labels.update({p:i for p <strong class="nl iu">in</strong> paths})<br/>        <br/>            <br/>        self.n_channels = n_channels<br/>        self.n_classes = len(flowers_cls)<br/>        self.shuffle = shuffle<br/>        self.on_epoch_end()<br/><br/>    def __len__(self):<br/>        <strong class="nl iu">'Denotes the number of batches per epoch'</strong><br/>        return int(np.floor(len(self.list_IDs) / self.batch_size))<br/><br/>    def __getitem__(self, index):<br/>       <strong class="nl iu"> 'Generate one batch of data'</strong><br/>        <strong class="nl iu"><em class="nu"># Generate indexes of the batch</em></strong><br/>        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]<br/><br/>        <strong class="nl iu"><em class="nu"># Find list of IDs</em></strong><br/>        list_IDs_temp = [self.list_IDs[k] for k <strong class="nl iu">in</strong> indexes]<br/><br/>       <strong class="nl iu"> <em class="nu"># Generate data</em></strong><br/>        X, y = self.__data_generation(list_IDs_temp)<br/><br/>        return X, y<br/><br/>    def on_epoch_end(self):<br/>   <strong class="nl iu">     'Updates indexes after each epoch'</strong><br/>        self.indexes = np.arange(len(self.list_IDs))<br/>        if self.shuffle == True:<br/>            np.random.shuffle(self.indexes)<br/><br/>    def __data_generation(self, list_IDs_temp):<br/><strong class="nl iu">        'Generates data containing batch_size samples' <em class="nu"># X : (n_samples, *dim, n_channels)</em></strong><br/>        <strong class="nl iu"><em class="nu"># Initialization</em></strong><br/>        X = np.empty((self.batch_size, *self.dim, self.n_channels))<br/>        y = np.empty((self.batch_size), dtype=int)<br/>        <br/>        delete_rows = []<br/><br/>       <strong class="nl iu"> <em class="nu"># Generate data</em></strong><br/>        for i, ID <strong class="nl iu">in</strong> enumerate(list_IDs_temp):<br/>            <strong class="nl iu"><em class="nu"># Store sample</em></strong><br/>            img = io.imread(ID)<br/>            img = img/255<br/>            if img.shape[0] &gt; 100 <strong class="nl iu">and</strong> img.shape[1] &gt; 100:<br/>                h, w, _ = img.shape<br/>                img = img[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]<br/>            else:<br/>                delete_rows.append(i)<br/>                continue<br/>            <br/>            X[i,] = img<br/>          <br/>          <strong class="nl iu">  <em class="nu"># Store class</em></strong><br/>            y[i] = self.labels[ID]<br/>        <br/>        X = np.delete(X, delete_rows, axis=0)<br/>        y = np.delete(y, delete_rows, axis=0)<br/>        <br/><strong class="nl iu">        <em class="nu"># data augmentation</em></strong><br/>        if self.mode == 'train':<br/>            aug_x = np.stack([datagen.random_transform(img) for img <strong class="nl iu">in</strong> X])<br/>            X = np.concatenate([X, aug_x])<br/>            y = np.concatenate([y, y])<br/>        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)</span></pre><p id="f4c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">要优化的指标</strong></p><p id="e3c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据具体情况，我们选择合适的指标。对于二元分类问题，AUC 通常是最好的度量。</p><p id="2a99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AUC 通常是比准确性更好的指标。因此，让我们监控 AUC，并根据验证数据选择基于 AUC 的最佳模型，而不是针对准确性进行优化。我们将使用回调<code class="fe ox oy oz nl b">on_train_begin</code>和<code class="fe ox oy oz nl b">on_epoch_end</code>来初始化(在每个时期的开始)和存储 AUC(在时期的结束)。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="9f72" class="np mo it nl b gy nq nr l ns nt">from sklearn.metrics import roc_auc_score<br/><br/>class <strong class="nl iu">roc_callback</strong>(Callback):<br/>    <br/>    def on_train_begin(self, logs={}):<br/>        logs['val_auc'] = 0<br/><br/>    def on_epoch_end(self, epoch, logs={}):<br/>        y_p = []<br/>        y_v = []<br/>        for i <strong class="nl iu">in</strong> range(len(validation_generator)):<br/>            x_val, y_val = validation_generator[i]<br/>            y_pred = self.model.predict(x_val)<br/>            y_p.append(y_pred)<br/>            y_v.append(y_val)<br/>        y_p = np.concatenate(y_p)<br/>        y_v = np.concatenate(y_v)<br/>        roc_auc = roc_auc_score(y_v, y_p)<br/>        print ('<strong class="nl iu">\n</strong>Val AUC for epoch<strong class="nl iu">{}</strong>: <strong class="nl iu">{}</strong>'.format(epoch, roc_auc))<br/>        logs['val_auc'] = roc_auc</span></pre><h1 id="37d0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">最终运行</h1><p id="56a3" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">现在让我们来训练最终的模型。请注意，我们将继续在<code class="fe ox oy oz nl b">models/best_models.hdf5</code>保存最佳模型的权重，因此您需要创建一个目录<code class="fe ox oy oz nl b">models</code>。请注意，模型权重通常保存在 hdf5 文件中。</p><p id="5019" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">保存最佳模型</strong>是使用<code class="fe ox oy oz nl b">ModelCheckpoint</code>附带的回调功能完成的。我们基本上指定了保存模型权重的<code class="fe ox oy oz nl b">filepath</code>，<code class="fe ox oy oz nl b">monitor='val_auc'</code>指定了您正在基于验证准确性选择最佳模型，<code class="fe ox oy oz nl b">save_best_only=True</code>仅保存最佳权重，<code class="fe ox oy oz nl b">mode='max'</code>指定了验证准确性将被最大化。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0884" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu"># model</em></strong><br/>model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)<br/>model.compile(loss='categorical_crossentropy',optimizer= sgd,<br/>              metrics=['accuracy'])<br/>training_generator = AugmentedDataGenerator('train', ablation=32)<br/>validation_generator = AugmentedDataGenerator('val', ablation=32)<br/><br/><strong class="nl iu"><em class="nu"># checkpoint </em></strong><br/>filepath = 'models/best_model.hdf5'<br/>checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')<br/>auc_logger = roc_callback()<br/><br/><strong class="nl iu"><em class="nu"># fit </em></strong><br/>model.fit_generator(generator=training_generator,<br/>                    validation_data=validation_generator,<br/>                    epochs=3, callbacks=[auc_logger, history, decay, checkpoint])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/bc9dfedd21ece93c7c8b1dc95c867168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ok7yImevDzjbH1hnh2eqYQ.png"/></div></div></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="08d8" class="np mo it nl b gy nq nr l ns nt">plt.imshow(image)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/c0c1edf2bcc97a892fd6ea648614f154.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*3ucsTaXCD2_biTrdNATEDA.png"/></div></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="2389" class="np mo it nl b gy nq nr l ns nt"><strong class="nl iu"><em class="nu">#</em>standardizing <em class="nu">image<br/>#moved the origin to the centre of the image</em></strong><br/>h, w, _ = image.shape<br/>img = image[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]<br/><br/>model.predict(img[np.newaxis,: ])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/44a6b005a15293f07fd19a2bfcb36e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTtmfKIa1LDTazaBaQrkTw.png"/></div></div></figure><p id="db97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嘿，我们有一个非常高的概率为 1 类，即玫瑰。如果你还记得的话，0 班是黛西，1 班是罗斯(在博客上面)。因此，模型已经学习得很好了。我们已经建立了一个在 3 个时期结束时具有良好 AUC 的模型。如果您使用更多的纪元来训练，您应该能够达到更好的 AUC 值。</p><p id="03c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有任何问题、建议或批评，可以通过 LinkedIn 或评论区联系我。</p></div></div>    
</body>
</html>