<html>
<head>
<title>Roadmap to Natural Language Processing (NLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理(NLP)路线图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6?source=collection_archive---------27-----------------------#2020-06-24">https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6?source=collection_archive---------27-----------------------#2020-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1c38" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍自然语言处理(NLP)中最常用的一些技术和模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e58e26d5ab8c638dee5d9293c3dc4e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x2fhqjKco9CZsfeK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="kz la l"/></div></figure><h1 id="c4d9" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">介绍</h1><p id="c898" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">由于过去十年<a class="ae ky" rel="noopener" target="_blank" href="/big-data-analysis-spark-and-hadoop-a11ba591c057">大数据</a>的发展。组织现在每天都面临着分析来自各种来源的大量数据。</p><p id="ee02" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">自然语言处理(NLP)是人工智能的研究领域，专注于处理和使用文本和语音数据来创建智能机器和创造洞察力。</p><p id="890f" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">当今最有趣的自然语言处理应用之一是创造能够与人类讨论复杂话题的机器。迄今为止，IBM Project Debater代表了该领域最成功的方法之一。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu la l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">视频1: IBM项目辩手</p></figure><h1 id="86f5" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">预处理技术</h1><p id="93bf" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">为了准备用于推理的文本数据，一些最常用的技术是:</p><ul class=""><li id="e7c8" class="mv mw it lv b lw mp lz mq mc mx mg my mk mz mo na nb nc nd bi translated"><strong class="lv iu">标记化:</strong>用于将输入文本分割成其组成单词(标记)。这样，将我们的数据转换成数字格式就变得更容易了。</li><li id="7025" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><strong class="lv iu">停用词移除:</strong>用于从我们的文本中移除所有介词(如“an”、“the”等……)，这些介词仅被视为我们数据中的噪声源(因为它们在我们的数据中不携带额外的信息)。</li><li id="edfc" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><strong class="lv iu">词干化:</strong>最后使用是为了去掉我们数据中的所有词缀(如前缀或后缀)。通过这种方式，我们的算法实际上可以更容易地不将实际上具有相似含义的单词(例如，insight ~ insightful)视为区分单词。</li></ul><p id="d8f8" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">使用标准的Python NLP库，例如<a class="ae ky" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>和<a class="ae ky" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> Spacy </a>，所有这些预处理技术都可以很容易地应用于不同类型的文本。</p><p id="79a6" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">此外，为了推断我们文本的语言语法和结构，我们可以利用诸如词性(POS)标记和浅层解析等技术(图1)。事实上，使用这些技术，我们用词汇类别(基于短语句法上下文)显式地标记每个单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/76f89dc16e1941f531ea93a582beb5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*fRsFnY3Nk7y0hWcBbnzy6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:词性标注示例[1]。</p></figure><h1 id="82bd" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">建模技术</h1><h2 id="e076" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">一袋单词</h2><p id="7465" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">单词包是一种用于自然语言处理和<a class="ae ky" rel="noopener" target="_blank" href="/roadmap-to-computer-vision-79106beb8be4">计算机视觉</a>的技术，目的是为训练分类器创建新的特征(图2)。这种技术是通过构建一个统计文档中所有单词的直方图来实现的(不考虑单词顺序和语法规则)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/1ed14fe29aa56aad7d6869408ff8e2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBBlFr8_uj5owi4Z_YhzOw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:单词袋[2]</p></figure><p id="4e28" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">限制这种技术有效性的一个主要问题是在我们的文本中存在介词、代词、冠词等。事实上，这些都可以被认为是在我们的文本中频繁出现的词，即使在找出我们的文档中的主要特征和主题时不一定具有真正的信息。</p><p id="e310" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">为了解决这类问题，通常使用一种称为“术语频率-逆文档频率”(TFIDF)的技术。TFIDF旨在通过考虑我们文本中的每个单词在大样本文本中出现的频率来重新调整我们文本中的单词计数频率。使用这种技术，我们将奖励那些在我们的文本中经常出现但在其他文本中很少出现的单词(提高它们的频率值)，同时惩罚那些在我们的文本和其他文本中频繁出现的单词(降低它们的频率值)(如介词、代词等)。</p><h2 id="0cee" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">潜在狄利克雷分配</h2><p id="b506" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">潜在狄利克雷分配(LDA)是一种主题造型手法。主题建模是一个研究领域，其重点是找出聚类文档的方法，以便发现潜在的区分标记，这些标记可以根据它们的内容来表征它们(图3)。因此，主题建模在这个范围内也可以被认为是一种<a class="ae ky" rel="noopener" target="_blank" href="/feature-extraction-techniques-d619b56e31be">维度缩减技术</a>，因为它允许我们将初始数据缩减到一个有限的聚类集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c185d7eed37c8146acb37cb72b64d248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*Gct6iypi9qL8ZJ-kIWAswA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:主题建模[3]</p></figure><p id="d317" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">潜在狄利克雷分配(LDA)是一种无监督学习技术，用于发现能够表征不同文档的潜在主题，并将相似的文档聚集在一起。该算法将被认为存在的主题的数量<strong class="lv iu"> <em class="ny"> N </em> </strong>作为输入，然后将不同的文档分组为彼此密切相关的文档的<strong class="lv iu"> <em class="ny"> N </em> </strong>簇。</p><p id="bef3" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">LDA与其他聚类技术(如K-Means聚类)的区别在于，LDA是一种软聚类技术(基于概率分布将每个文档分配给一个聚类)。例如，可以将一个文档分配给聚类A，因为该算法确定该文档有80%的可能性属于该类，同时仍然考虑到嵌入到该文档中的一些特征(剩余的20%)更有可能属于第二个聚类b</p><h2 id="8adf" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">单词嵌入</h2><p id="be84" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">单词嵌入是将单词编码为数字向量的最常见方式之一，然后可以将其输入到我们的机器学习模型中进行推理。单词嵌入旨在可靠地将我们的单词转换到向量空间，以便相似的单词由相似的向量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/e4aab4a087c0586611900337d8ef1cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTia0WpgIGM8nAfJQndMhw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:单词嵌入[4]</p></figure><p id="f736" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">如今，有三种主要的技术用于创建单词嵌入:<a class="ae ky" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>、<a class="ae ky" href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> GloVe </a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/FastText" rel="noopener ugc nofollow" target="_blank"> fastText </a>。所有这三种技术都使用浅层神经网络，以便创建所需的单词嵌入。</p><p id="789f" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">如果您有兴趣了解更多关于单词嵌入是如何工作的，这篇文章是一个很好的起点。</p><h2 id="4ccd" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">情感分析</h2><p id="63e5" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">情感分析是一种NLP技术，通常用于理解某种形式的文本是否表达了对某个主题的积极、消极或中性的情感。例如，当试图了解公众对某个话题、产品或公司的普遍看法(通过在线评论、推文等)时，这可能特别有用。</p><p id="5ce1" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在情感分析中，文本中的情感通常表示为介于-1(消极情感)和1(积极情感)之间的值，称为极性。</p><p id="0a94" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">情感分析可以被认为是一种无监督的学习技术，因为我们通常不会为我们的数据提供手工制作的标签。为了克服这一障碍，我们使用预先标记的词典(一本单词书)，它被创建来量化不同上下文中大量单词的情感。情感分析中广泛使用的词汇的一些例子是<a class="ae ky" href="https://github.com/sloria/TextBlob/tree/eb08c120d364e908646731d60b4e4c6c1712ff63" rel="noopener ugc nofollow" target="_blank">文本块</a>和<a class="ae ky" href="https://github.com/cjhutto/vaderSentiment" rel="noopener ugc nofollow" target="_blank"> VADER </a>。</p><h1 id="6d63" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">变形金刚(电影名)</h1><p id="5aa3" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> Transformers </a>代表当前最先进的NLP模型，用于分析文本数据。一些广为人知的变形金刚模型的例子有<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GTP2 </a>。</p><p id="064d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在创建变压器之前，递归神经网络(RNNs)代表了顺序分析文本数据以进行预测的最有效的方法，但是这种方法发现很难可靠地利用长期依赖性(例如，我们的网络可能会发现很难理解在几次迭代之前输入的单词是否可能对当前迭代有用)。</p><p id="250d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">由于一种叫做<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力</a>的机制,《变形金刚》成功地克服了这种限制(这种机制用于确定文本的哪些部分需要关注并给予更多的权重)。此外，Transformers使得并行处理文本数据比顺序处理更容易(因此提高了执行速度)。</p><p id="c4d7" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">多亏了<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱人脸库</a>，变形金刚如今可以很容易地用Python实现。</p><h2 id="0544" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">文本预测演示</h2><p id="6454" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">文本预测是可以使用诸如GPT2之类的转换器轻松实现的任务之一。在这个例子中，我们将引用Carlos Ruiz Zafón的"<a class="ae ky" href="https://en.wikipedia.org/wiki/The_Shadow_of_the_Wind" rel="noopener ugc nofollow" target="_blank"> The Shadow of the Wind </a>"作为输入，然后我们的转换器将生成其他50个字符，这些字符在逻辑上应该遵循我们的输入数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa la l"/></div></figure><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="85c2" class="nk lc it oc b gy og oh l oi oj">A book is a mirror that offers us only what we already carry inside us. It is a way of knowing ourselves, and it takes a whole life of self awareness as we become aware of ourselves. This is a real lesson from the book My Life.</span></pre><p id="5e94" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">从上面显示的示例输出中可以看出，我们的GPT2模型在为输入字符串创建可重密封的延续方面表现得非常好。</p><p id="8074" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">点击<a class="ae ky" href="https://drive.google.com/open?id=1UVfieBsf4gb6J_s_FaRA93D0ueO4-7JE" rel="noopener ugc nofollow" target="_blank">此链接，您可以运行一个示例笔记本来生成自己的文本。</a></p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="5a0d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated"><em class="ny">希望您喜欢这篇文章，感谢您的阅读！</em></p><h1 id="939d" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">联系人</h1><p id="c843" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">如果你想了解我最新的文章和项目<a class="ae ky" href="https://medium.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener">，请通过媒体</a>关注我，并订阅我的<a class="ae ky" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="7254" class="mv mw it lv b lw mp lz mq mc mx mg my mk mz mo na nb nc nd bi translated"><a class="ae ky" href="https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></li><li id="5128" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="2d94" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="edb2" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><a class="ae ky" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中等轮廓</a></li><li id="9e6a" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><a class="ae ky" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="87ea" class="mv mw it lv b lw ne lz nf mc ng mg nh mk ni mo na nb nc nd bi translated"><a class="ae ky" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><h1 id="775a" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">文献学</h1><p id="1869" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">[1]使用python中的NLTK POS tagger提取自定义关键字，Thinkinfi，Anindya Naskar。访问位置:<a class="ae ky" href="https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html" rel="noopener ugc nofollow" target="_blank">https://www . thinkinfo . com/2018/10/extract-custom-entity-using-nltk-pos . html</a></p><p id="e922" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[2]字袋模型弓和字集模型播的比较，程序员求。访问地址:<a class="ae ky" href="http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012" rel="noopener ugc nofollow" target="_blank">http://www.programmersought.com/article/4304366575/;jsessionid = 0187 f8e 68 a 22612555 b 437068028 c 012</a></p><p id="4676" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[3]话题建模:NLP中讲故事的艺术，<br/> TechnovativeThinker。访问地址:<a class="ae ky" href="https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987" rel="noopener">https://medium . com/@ magesh dominator/topic-modeling-art-of-story-in-NLP-4d c83 e 96 a 987</a></p><p id="7a4d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[4] Word Mover的嵌入:来自Word2Vec的通用文本嵌入，IBM研究博客。访问网址:<a class="ae ky" href="https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/" rel="noopener ugc nofollow" target="_blank">https://www . IBM . com/blogs/research/2018/11/word-movers-embedding/</a></p></div></div>    
</body>
</html>