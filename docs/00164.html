<html>
<head>
<title>Log loss function math explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对数损失函数数学解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83?source=collection_archive---------5-----------------------#2020-01-06">https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83?source=collection_archive---------5-----------------------#2020-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c173" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">逻辑回归中对数损失函数的推导和数学</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="26fd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">你做过机器学习中的分类问题吗？如果是，那么你可能在<a class="ae ll" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>中遇到过<a class="ae ll" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" rel="noopener ugc nofollow" target="_blank">交叉熵</a>或对数损失函数。</p><p id="b701" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">那个功能是做什么用的？分类问题中函数的意义是什么？</p><p id="7a06" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们通过查看函数背后的数学来详细了解一下。</p><p id="571f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在我们开始钻研函数背后的数学知识并了解它是如何推导出来的之前，我们应该知道什么是损失函数。</p><blockquote class="lm ln lo"><p id="75e4" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">简单来说，损失函数(Loss function):用于评估用于解决任务的算法的性能的函数。<a class="ae ll" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">详细定义</a></p></blockquote><p id="6833" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在诸如逻辑回归的二元分类算法中，目标是最小化交叉熵函数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/9d73cca25ef53f30f6235c19af0e0ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*wilGXrItaMAJmZNl6RJq9Q.png"/></div></figure><blockquote class="lm ln lo"><p id="39f5" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">交叉熵是对给定随机变量或一组事件的两个概率分布之间的差异的度量</p></blockquote><p id="a1f2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">假设我们有病人的数据，任务是找出哪些病人患有癌症。在我们的例子中，由于我们没有整个人口的数据，我们试图从数据样本中预测一个人患癌症的可能性。我们只需要预测恶性类<em class="lp">即</em><strong class="kr iu"><em class="lp"/></strong><em class="lp">p(y = 1 | x)=p̂</em><strong class="kr iu"><em class="lp"/></strong>因为负类的概率可以从中导出<em class="lp">即</em><em class="lp">p(y = 0 | x)= 1-p(y = 1 | x)=1-p̂</em>。</p><p id="efaa" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">好的二进制分类算法应该产生<strong class="kr iu"><em class="lp"/></strong>的<strong class="kr iu"> <em class="lp">高</em> </strong>值(预测样本s的恶性类别的概率)，这是最接近于<em class="lp"> P </em>(预测总群体的恶性类别<em class="lp"> </em>的概率)。</p><blockquote class="lm ln lo"><p id="a368" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">在概率论中，<a class="ae ll" href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions" rel="noopener ugc nofollow" target="_blank">概率密度函数</a>或连续随机变量的密度是一个函数，其在样本空间中任何给定样本的值都可以被解释为提供随机变量的值等于该样本的相对可能性——维基百科</p></blockquote><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mb"><img src="../Images/25cc21dc972c3a10c68ec90144b82fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clgPiy5VaxBpq8sZaaMdOw.jpeg"/></div></div></figure><p id="7ab5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> <em class="lp">这个想法是为</em> θ </strong>的特定值找到似然函数的最大值</p><p id="ccc0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">求函数的最大值意味着对函数求导(dL/dθ= 0)</p><p id="9adb" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">由于似然函数L是每个Xi的概率分布函数的乘积，我们必须使用微分中的乘积法则来微分这样的函数，这将成为一项复杂的任务。</p><blockquote class="mg"><p id="2c64" class="mh mi it bd mj mk ml mm mn mo mp lk dk translated">这就是对数派上用场的地方。Log(xy) = Logx + Logy</p><p id="3394" class="mh mi it bd mj mk ml mm mn mo mp lk dk translated">微分:d(Logx)/dx = 1/x</p></blockquote><p id="0f63" class="pw-post-body-paragraph kp kq it kr b ks mq ju ku kv mr jx kx ky ms la lb lc mt le lf lg mu li lj lk im bi translated">对似然函数应用对数将表达式简化为概率对数之和，并且不会改变关于θ的图形。此外，对似然函数的对数求微分将给出相同的估计θ，因为对数函数的<a class="ae ll" href="http://mathworld.wolfram.com/MonotonicFunction.html" rel="noopener ugc nofollow" target="_blank">单调</a>属性。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mv"><img src="../Images/6f2109484a3bffa0ec4e9bae25ceba64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8-ziPD6ZYH6ivPDv9DqIMA.jpeg"/></div></div></figure><p id="ef96" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">似然函数的这种变换有助于找到θ的值，这使得似然函数最大化。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mw"><img src="../Images/164ffb677e513951cda301dbd523166b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVRujUR-CMkOMb9Q7xHEUA.jpeg"/></div></div></figure><h1 id="3f93" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">最大似然估计</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi np"><img src="../Images/416f26284b03ac16527cb6c141fa5f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpEtSI-EhZchk6y39Tm9Jw.jpeg"/></div></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nq"><img src="../Images/6ca920453f225b08d71333d9900b21b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MAF5T5pbcGzQvcU0ozosQ.jpeg"/></div></div></figure><p id="6418" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这个表达式也被称为伯努利分布。</p><p id="c666" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在我们的例子中，癌症是恶性的概率是p。癌症是良性的概率是1-P。</p><p id="43b0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在N个观测值的情况下，概率密度函数f作为各个概率密度函数的乘积给出。联合概率定义如下</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nr"><img src="../Images/358a081ef76a4a25a29a79269346fa48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtYP2_10jRRTOBD5q2qN1w.jpeg"/></div></div></figure><p id="b9a4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于最大似然估计，我们必须计算什么样的P值是dL/dP = 0，因此如前所述；似然函数被转换成对数似然函数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ns"><img src="../Images/bfc5959289313003314c03a582375959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrQqI25cQ91OmEfZhYiKYg.jpeg"/></div></div></figure><p id="d87b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如你所见，我们已经推导出一个几乎类似于对数损耗/交叉熵函数的方程，只是没有负号。在逻辑回归中，<strong class="kr iu"> <em class="lp">梯度下降</em> </strong>用于寻找最优值，而不是梯度上升，因为它被认为是损失最小化的<em class="lp">问题，所以这是我们将负号添加到等式的地方，这导致<em class="lp"/><strong class="kr iu"><em class="lp">二元交叉熵损失函数。</em> </strong></em></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nt"><img src="../Images/e5dd7a3a9e201f15e1cc02b704d1b2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5ev2dyQEWhjHPPP7Gj6Ng.jpeg"/></div></div></figure><p id="a66b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">另外，请注意，最大化对数似然函数与最小化负对数似然函数是相同的。</p><blockquote class="lm ln lo"><p id="ef45" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">损失函数计算单个训练示例的误差；成本函数是整个训练集的损失函数的平均值—吴恩达</p></blockquote><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nu"><img src="../Images/3697046f83ca873b9a92a14b50422b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0TALfsJiifyRCcLNbFdgA.jpeg"/></div></div></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="1e01" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如果你喜欢这篇文章，请支持我，我会感谢任何形式的反馈。此外，我希望与数据科学社区的人们建立联系。在<a class="ae ll" href="https://www.linkedin.com/in/saiharshithreddygaddam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系</p><p id="5a5e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">参考资料:</p><ol class=""><li id="8fc4" class="nv nw it kr b ks kt kv kw ky nx lc ny lg nz lk oa ob oc od bi translated"><a class="ae ll" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Loss_function</a></li><li id="4de3" class="nv nw it kr b ks oe kv of ky og lc oh lg oi lk oa ob oc od bi translated"><a class="ae ll" href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions" rel="noopener ugc nofollow" target="_blank">https://www . khanacademy . org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions</a></li><li id="f730" class="nv nw it kr b ks oe kv of ky og lc oh lg oi lk oa ob oc od bi translated">https://en.wikipedia.org/wiki/Maximum_likelihood_estimation<a class="ae ll" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"/></li><li id="d6c0" class="nv nw it kr b ks oe kv of ky og lc oh lg oi lk oa ob oc od bi translated">【http://mathworld.wolfram.com/MonotonicFunction.html T4】</li></ol></div></div>    
</body>
</html>