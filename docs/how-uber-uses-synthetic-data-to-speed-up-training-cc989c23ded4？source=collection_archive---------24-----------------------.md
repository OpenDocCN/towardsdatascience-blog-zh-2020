# 优步如何利用合成数据加快训练速度

> 原文：<https://towardsdatascience.com/how-uber-uses-synthetic-data-to-speed-up-training-cc989c23ded4?source=collection_archive---------24----------------------->

## 优步人工智能实验室最新成果综述

![](img/fde9827b8a3716eb9f2ba8f57c496e60.png)

照片由[freestocks.org](https://www.pexels.com/@freestocks)在[像素](https://www.pexels.com/photo/uber-smartphone-iphone-app-34239/)上拍摄

最近，优步人工智能实验室发表了一篇题为*生成式教学网络:通过学习生成合成训练数据来加速神经架构搜索*的论文。在这篇论文中，作者概述了一种新的学习算法，用于自动生成数据、学习环境和课程，以帮助 AI 智能体快速学习。论文可以在[这里](https://arxiv.org/pdf/1912.07768.pdf)找到。

**动机**

*什么时候使用合成训练数据比使用所有可用的真实数据更有帮助？*

人工智能研究的专家们正慢慢开始放弃数据越多越好的想法。通过对训练数据的子集进行智能采样而生成的替代数据集可以产生可比较的测试性能和较少的训练工作。在深度学习的背景下，这相当于有效地搜索最佳神经网络架构。智能子采样大大减少了训练工作量，因为在全数据集上训练成千上万的候选神经网络结构通常是昂贵的。

**相关工作**

> **可以通过对训练数据的子集进行智能采样来创建替代数据集。这种替代物能够以较少的训练努力实现有竞争力的测试性能。**

文献中有几个应用替代训练数据的例子，包括课程学习、主动学习和核心集选择:

**课程学习**

[Graves 等人介绍了一种自动选择路径或教学大纲的方法，神经网络通过课程遵循该方法，以最大化学习效率。](https://arxiv.org/pdf/1704.03003.pdf)

**主动学习**

[科纽什科娃等人。艾尔。，提出了一种新的数据驱动的主动学习(AL)方法，其中他们训练了一个回归器，该回归器预测特定学习状态下候选样本的预期误差减少。](https://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf)

**核心集选择**

[塞内尔等人。艾尔。，将主动学习定义为核心集选择。核心集选择通过选择一组点来工作，使得在所选子集上训练的模型对于剩余的数据点是有竞争力的。](https://arxiv.org/pdf/1708.00489.pdf)

优步论文的关键观点是，替代数据不需要来自原始数据分布。他们举的一个例子是，人们通过阅读一本书来学习一项新技能，或者通过练习传球或运球来准备像足球这样的团队比赛。

本文的目的是研究一个数据生成神经网络是否能够产生合成数据，从而有效地向学习者教授目标任务。

**生成式教学网络**

在他们的论文中，优步人工智能实验室提出了生成教学网络(GTNs):一种可扩展的，新颖的元学习方法，用于生成合成数据。

如前所述，GTNs 的任务是产生一个数据生成神经网络。在生成 GTN 时，神经网络学习器在由 GTN 生成的数据上被训练。GTN 和学习者网络合作(与竞争的 GANs 相反),以便在目标任务中快速产生高准确度。

**结果**

在这项研究中，作者证明了 GTNs 可以生成一个合成的训练集，与在完整数据集上的训练相比，它可以在两个监督学习域 MNIST 和 CIFAR10 中加速学习。

*MNIST*

MNIST 数据集包含手写数字(从 0 到 9)，通常用于训练图像处理系统。这种情况下的学习任务是手写数字的分类。

从 MNIST 结果中得到的关键信息是，GTN 方法比在真实数据和真实数据的子样本上训练收敛得更快，迭代次数更少。虽然收敛的误差在量级上比基于真实数据训练的模型大，但是它离真实数据的收敛值不远，即使它花费了较少的迭代。

*CIFAR10*

CIFAR10 数据集包含 10 类不同的图像，包括汽车、鸟类、猫、鹿、狗、青蛙、马、船只和卡车。它还通常用于训练图像处理系统，用于图像分类。

类似于 MNIST 的例子，GTN 差错率的平稳期比真实数据差错率的平稳期快得多，在真实数据的情况下，真实数据在 120 次内环迭代之后还没有达到收敛。

**结论**

在这篇文章中，我们回顾了优步人工智能实验室为加速学习生成合成数据的新方法。gtn 基本上与 GANs 相反，在 GANs 中，两个网络合作以最小化学习者的错误率。当在 MNIST 和 CIFAR10 数据上训练时，GTN 方法加速了图像分类任务中的误差收敛。可能的进一步应用包括使用强化学习来产生促进学习的新任务。另一个可能的应用是开发方法，可以用来哄 gtn 产生虚拟世界，供我们学习、玩耍或探索。

我希望你觉得这篇文章很有趣！