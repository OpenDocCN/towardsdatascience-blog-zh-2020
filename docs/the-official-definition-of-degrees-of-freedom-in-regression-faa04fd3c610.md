# 回归中自由度的官方定义

> 原文：<https://towardsdatascience.com/the-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610?source=collection_archive---------11----------------------->

![](img/0456c0d67a2faed1c948c69262edfda0.png)

照片由[罗恩·霍维尔](https://unsplash.com/@insolitus?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

## 计算岭回归中参数的有效数量等

早在初中和高中，你可能已经学会了计算数据集的平均值和标准差。而你的老师大概告诉过你，标准差有两种:总体和样本。两者的公式只是彼此之间的微小变化:

![](img/1dc388321c896d4164b215e42b1b8a13.png)

标准差的不同公式

其中μ是总体平均值，x-bar 是样本平均值。通常情况下，人们只是学习公式，并被告知何时使用它们。如果你问为什么，答案是含糊的，比如“在估计样本均值时，有一个自由度被用完了。”没有“自由度”的真正定义

自由度在统计学的其他几个地方也有表现，比如:做 t 检验，f 检验，χ检验，一般研究回归问题的时候。根据不同的情况，自由度可能意味着微妙不同的东西(据我统计，[维基百科](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))的文章列出了至少 9 个密切相关的定义)。

在本文中，我们将关注回归环境中自由度的含义。具体来说，我们将使用“自由度”是模型的“有效参数数”的含义。我们将了解如何计算上述标准偏差问题的自由度，以及线性回归、岭回归和 k-最近邻回归。我们还将简要讨论与统计推断(如 t 检验)和模型选择(如何使用有效自由度比较两个不同的模型)的关系。

# 自由度

在回归上下文中，我们有 N 个样本，每个样本都有一个实值结果值 *y* 。对于每个样本，我们都有一个协变量向量 *x* ，通常包含一个常数。换句话说，对于每个样本，*x*-向量的第一个条目是 1。我们有某种模型或程序(可以是参数化的或非参数化的)来拟合数据(或者使用数据)以产生关于我们认为 y 的值应该被给定为一个*x*-向量(可以是样本外的或者不是样本外的)的预测。

结果是 N 个样本中每个样本的预测值 y-hat。我们将定义自由度，用ν (nu)表示:

![](img/5fe7700bfc5c33b784c35fbf1242c13b.png)

自由度的定义

我们将自由度解释为模型的“有效参数数”。现在让我们看一些例子。

# 平均值和标准偏差

让我们回到我们开始的学龄问题。计算样本的平均值就是预测每个数据点的值都等于平均值(毕竟，这是在这种情况下你能做出的最佳猜测)。换句话说:

![](img/e5e4396f639dc8ee5b6121a699f4872f.png)

作为预测问题的均值估计

注意，估计平均值相当于运行一个只有一个协变量的线性回归，一个常数:x = [1]。希望这能让我们明白为什么我们可以把这个问题重铸为一个预测问题。

现在很容易计算自由度。不出所料，我们有 1 个自由度:

![](img/fb2191e1b316a25e2c8099205d43c890.png)

为了理解与标准差的关系，我们必须使用自由度的另一个密切相关的定义(我们不会深入讨论)。如果我们的样本是独立同分布的，那么我们可以非正式地说，我们从 N 个自由度开始。我们在估计平均值时损失了一个，剩下 N–1 个作为标准差。

# 标准线性回归

现在让我们把它扩展到常规的旧线性回归的环境中。在这种情况下，我们喜欢将样本数据收集到向量 Y 和矩阵 x 中。在本文中，我们将使用 *p* 来表示每个样本的协变量数量( *x* 向量的长度)。

自由度的数量最终会是 *p* ，这不应该成为剧透。但是，当我们转向岭回归时，用于计算这一点的方法将对我们有所帮助。

如果我们像往常一样在模型中包含一个常数，那么 p 协变量的计数就包含一个常数。X 矩阵中的每一行对应于我们样本中每个观察值的 *x-* 向量:

![](img/a862d7a48d7f387dbf3b2c2b13c383ce.png)

我们的响应矩阵 y(n⨉1 向量)和设计矩阵 x(n⨉p 形状)

模型是有 *p* 个参数集合成一个向量β。Y = Xβ加上误差项。我们将讨论推导过程，因为它对我们以后会有用。我们选择使误差平方和最小的β估计值。换句话说，我们的损失函数是

![](img/92534d6cd29dee151afb19bc7b55e95b.png)

线性回归的损失函数

第一次求和是根据具有由 *i* 标记的行向量 x 的每个样本。为了优化 l，我们对向量β进行微分，得到一个 *p⨉* 1 向量的导数。

![](img/5d3931feadfadc2b99bb7210be232006.png)

设其等于 0，求解β

![](img/d27f4a29c50665dcae2bd58a80e74ad8.png)

并最终形成我们的估计

![](img/cfc2e60180574901a369af6f307504a9.png)

我们对 Y 的估计，用一个帽子矩阵

我们称矩阵 H 为“帽子矩阵”，因为它在 Y 上“戴上了帽子”(产生我们的拟合/预测值)。帽子矩阵是 N⨉N 矩阵。我们假设 y 轴是独立的，因此我们可以计算有效自由度:

![](img/12b94b5e9e57d02366bcfe3a6d7c89dd.png)

标准线性回归的自由度

其中第二个和在矩阵的对角项上。如果你写出矩阵，写出样本 1 的预测值的公式，你会看到这些导数实际上只是帽子矩阵的对角线元素。一个矩阵的对角线之和称为矩阵的[迹](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) ，我们在第二行中表示了这一点。

## 计算轨迹

现在我们转向计算 *H* 的轨迹。*T7 我们最好希望是 *p* ！*

利用迹的循环性，有一个简单的方法来计算 H 的迹。但是我们将采用另一种方法，当我们讨论岭回归时，这种方法将被推广。

我们使用 x 的[奇异值分解](https://en.wikipedia.org/wiki/Singular_value_decomposition)(关于奇异值分解和我们将要做的线性代数的几何解释，见我的[早期文章](/the-singular-value-decomposition-without-algebra-ae10147aab4c))。矩阵的迹是一个与基无关的数，所以我们可以为包含 y 的向量空间选择我们想要的任何基，同样，我们也可以为包含参数β的向量空间选择我们想要的任何基。奇异值分解表示存在一个基使得矩阵 X 是对角的。第一个 *p* 行中对角线上的条目称为*奇异值*。线性回归的“无完美多重共线性”假设意味着没有奇异值为 0。矩阵的其余 N-p 行都是全 0。

![](img/77252d5b8006b741d7505b3a3ff94152.png)

x 使用奇异值分解给出的基

现在很容易计算 h。您只需手动乘以 X 的版本，并获得一个对角矩阵，其中第一个 *p* 对角元素全为 1，其余为 0。未显示的条目(非对角线的条目)也全部为 0。

![](img/c6a88e776b69776fe3dd4dafef0eb8aa.png)

帽子矩阵使用由奇异值分解给出的 Y 的基

所以我们得出 Tr(H) = *p* 。

## 标准差的模拟:标准误差

在我们之前的示例(平均值和标准差)中，我们在计算平均值后计算标准差，使用*n–1*作为分母，因为我们“失去了 1 个自由度”来估计平均值。

在这种情况下，标准偏差被重命名为“标准误差”,但公式看起来应该类似:

![](img/53b37944ae88f74e31e98eb82c38a720.png)

就像之前一样，我们比较每个测量值 *y* 和它的预测值之差的平方和。我们用完了 p 个自由度来计算估计值，所以只剩下 N-p 个。

# 里脊回归

在岭回归中，我们在损失函数中加入了一个正则项。如果处理得当，这将增加我们系数的偏差，但减少方差，从而降低我们预测的总体误差。

使用我们对自由度的定义，我们可以计算岭回归中参数的有效数量。我们期望正则化将这降低到参数的原始数量 *p* 以下(因为它们不再自由变化)。

我们通过与线性回归中相同的步骤来计算帽子矩阵。

1.  损失函数得到一个额外的项，它具有一个固定的、已知的超参数λ，用于设置正则化的量。

![](img/19d664ab19bca64e205c08ab64f10ae7.png)

2.我们求导，设为 0，求解β。I 是这里的单位矩阵

![](img/dd929b08dab33e64ae77c14e99bd5534.png)

3.我们计算拟合值并提取帽子矩阵 *H* 。公式和上次一样，除了我们在括号里给矩阵的每个对角元素加λ。

![](img/83609149956b6e3529327e49f376f7c9.png)

4.我们使用奇异值分解来为包含 Y 和β的向量空间选择基，这样我们可以将 X 写成对角矩阵，并计算 h。

![](img/592a6bd05b5765e578b2dc11a1a500f0.png)

这给我们留下了根据由 *i* 索引的奇异值 *d 【T5，的正则(λ = 0)回归和岭回归(λ>0)的自由度的以下公式。*

![](img/2d6804ebffda785149724e1226aa21d7.png)

## 讨论

使用奇异值分解的上述计算为我们提供了一个很好的岭回归的视角。

首先，如果设计矩阵是完全(多)共线的，则其奇异值之一将为 0。发生这种情况的常见情况是协变量多于样本。这是常规回归中的一个问题，因为这意味着 hat 矩阵中括号中的项是不可逆的(在上面的公式中分母是 0)。岭回归通过给每个平方奇异值添加一个正项来解决这个问题。

其次，我们可以看到，对于奇异值较小的项，系数收缩率较高。这些术语对应于在常规回归中具有高方差的β估计的成分(由于回归变量之间的高相关性)。另一方面，对于具有较高奇异值的项，收缩相对较小。

我们所做的自由度计算完美地概括了这种收缩，为我们实际使用的有效参数数量提供了一个估计值。另请注意，奇异值是设计矩阵 X 的函数，而不是 y 的函数。这意味着，理论上，您可以通过计算所需有效参数的数量来选择λ，并找到λ来实现这一点。

## 推理和模型选择

在我们的普通回归示例中，我们看到标准误差(或标准偏差)可以通过假设我们从 N 个自由度开始并减去我们使用的有效参数的数量来计算。这对于岭来说不一定有意义，它给出了系数的有偏估计量(尽管对于正确选择的λ具有较低的均方误差)。特别是，残差不再很好地分布。

相反，我们可以使用我们的有效参数数量插入到 [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) (Akaike 信息标准)，这是模型选择的交叉验证的替代方法。AIC 惩罚模型有更多的参数，并接近预期的测试误差，如果我们要使用一个坚持测试集。那么选择λ来优化它可以代替交叉验证，只要我们在 AIC 的公式中使用有效自由度。然而，请注意，如果我们在计算 AIC 之前自适应地选择λ，那么会增加额外的有效自由度。

# k 近邻回归

作为最后一个例子，考虑 k-最近邻回归。显然，每个数据点的拟合值是 k 个附近点的平均值，包括它本身。这意味着自由度是

![](img/c947c8f2870972bcc35cdd2acd1bc269.png)

k 近邻的自由度

这使我们能够在不同类型的模型之间进行模型比较(例如，使用上述 AIC 将 k-最近邻与岭回归进行比较)。

我希望您看到自由度是一个非常通用的度量，可以应用于所有种类的回归模型(核回归、样条等)。).

# 结论

希望这篇文章能让你对自由度有一个更坚实的理解，让整个概念不再是一个模糊的统计概念。我提到了自由度还有其他密切相关的定义。另一个主要版本是几何思想。如果你想了解更多，请阅读我的文章《T2 线性回归的几何方法》。如果你想了解更多我们用来计算自由度的代数，请阅读非代数方法的[奇异值分解](/the-singular-value-decomposition-without-algebra-ae10147aab4c)。

# 参考

你可以在统计学习的[元素中找到对这一材料的讨论(Hastie，Tibshirani 和 Friedman 2009)。第 7.4 节涵盖了乐观主义，这是我们所讨论的和模型选择之间的关系。第 3.4 节包括岭回归和我们在那里做的计算，以及讨论自由度。](https://web.stanford.edu/~hastie/ElemStatLearn/)

# 笔记

[1]具体来说，我正在计算(a)随机向量的几何定义；(b)概率分布的密切相关的自由度；(c) 4 个回归自由度公式；和(d)剩余自由度的 3 个公式。你可能会有不同的看法，但是希望你能明白我的观点，有几个密切相关的想法和公式有细微的区别。