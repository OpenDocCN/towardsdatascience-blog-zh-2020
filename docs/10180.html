<html>
<head>
<title>Understanding Detectron2 demo</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解检测器 2 演示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-detectron2-demo-bc648ea569e5?source=collection_archive---------8-----------------------#2020-07-18">https://towardsdatascience.com/understanding-detectron2-demo-bc648ea569e5?source=collection_archive---------8-----------------------#2020-07-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0ba7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解脸书人工智能研究图书馆，了解最先进的神经网络</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/3a696773d401b67580ca3ababeff65e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*3iLNCXCQ4j2CgJ0kI_lUBA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用检测器 2 ( <a class="ae kr" href="https://unsplash.com/photos/KTF-gr3uWvs" rel="noopener ugc nofollow" target="_blank">源</a>)进行实例分割</p></figure><h1 id="6732" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak">简介</strong></h1><p id="51e8" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">Detectron2 ( <a class="ae kr" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">官方库 Github </a>)是“FAIR 的下一代物体检测和分割平台”。FAIR(脸书人工智能研究所)创建了这个框架，以提供 CUDA 和 PyTorch 实现最先进的神经网络架构。它们还为对象检测、实例分割、人物关键点检测和其他用途提供预训练模型。</p><p id="09f2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">Detectron2 的重要但经常被忽略的特性是它的许可方案:库本身是在 Apache 2.0 许可下发布的，预训练模型是在 CC BY-SA 3.0 许可下发布的。这意味着你可以修改现有的代码，将其用于私人、科学甚至商业目的。你所需要做的就是给 FAIR 提供适当的信用。这在科学界很少见，科学界经常使用许可证来强制代码源发布和非商业使用。这非常有限，但幸运的是，对于 Detectron2 来说，情况并非如此。</p><p id="2bf7" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">然而，问题是，研究人员编写的代码通常不遵循干净的代码指南。对于 Detectron2 来说，与替代方案相比，它还不错，但是代码结构肯定很复杂，需要花很大力气才能理解。然而，要使用这个库的强大功能，确实需要了解它。在这篇文章中(希望还有后面的文章),我的目标是阐明 API、代码结构以及如何修改它并在你的项目中使用它。</p><p id="b2de" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">下面的代码假设你已经安装好了所有的东西，并且正在运行。特别是，你需要一个 Linux 系统(Windows 可能工作，但不被官方支持)，支持 CUDA 的 GPU(安装了 CUDA)，PyTorch &gt;= 1.4 和适当的 Detectron2 版本。鉴于运行这些神经网络所需的计算能力，该库目前不支持 CPU 计算，并且可能会继续支持。如果你们中的许多人在这方面遇到了问题，请在评论中告诉我，我会写另一篇关于所有适当工具的设置的文章。</p><h1 id="a846" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak">基本设置</strong></h1><p id="0639" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我们将通过修改的 Google Colab 演示使用 COCO 数据集类进行实例分割。目标是创建易于理解的代码框架，完美地用于未来基于 Detectron2 的项目。让我们从获取命令行参数开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="0674" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">作为技术细节，我们将编写 Python 3.5 中引入的 Python 类型注释。它们并不强制变量类型，而是旨在帮助程序员(以及 ide，它们提供了更好的注释帮助)更好地理解代码。这样也更容易知道在 Detectron2 文档和源代码中的何处寻找关于模型行为的线索。</p><p id="4f0d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">这里发生了一些重要的事情。首先，我们导入了<code class="fe mn mo mp mq b">argparse</code>模块，以便于参数解析。我们的演示可能需要 2 个参数:在 Detectron2 中使用的基本模型和一个或多个要处理的图像的列表。为解析和获取参数定义单独的函数是在<code class="fe mn mo mp mq b">demo.py</code>(官方 Detectron2 演示文件)和其他建立在它之上的项目(如 Centermask2)中是如何完成的。总的来说，这也是一个很好的代码实践。名称空间类型的行为类似于 Python 字典，但是对值的访问类似于对类中属性的访问，例如<code class="fe mn mo mp mq b">args.base_model</code>。如果没有提供参数，也没有设置 default，那么它将是适合该参数的空类型，通常是 None(或者是空列表，用于任意数量的选项，如上面的<code class="fe mn mo mp mq b">images</code>)。</p><p id="c1c8" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">重要的部分是基础模型的默认值。它将用于告诉检测器 2 应该使用模型动物园中的哪个预训练模型(基线)。模型动物园是一组由 FAIR 预先训练的模型，可以通过库 API 轻松下载。整个名单可以在<a class="ae kr" href="https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md" rel="noopener ugc nofollow" target="_blank">这里</a>找到。表格包含关于不同模型的各种统计数据，这些统计数据按照精度度量进行升序排序(比如用于实例分段的方框 AP)。根据你的需要，速度或准确性可能更重要；还要注意，这是平均 AP，在特殊情况下，平均分数较低的模型(最有可能是不太敏感的模型)可能工作得更好。我根据我的个人经验选择了默认值——X101-FPN 往往会给出太多的误报，而且在实践中它比 R101-FPN 慢 1.5-2 倍。为了获得标识您感兴趣的任何型号的字符串，请按照下面显示的步骤操作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/74159fc3981c9768d1cd2cc21ac73968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2bjAaGWIzWkf1pJlHgLhg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">选择您想要使用的模型</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/cec650aa3bb4c885d69df5822a539ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKc6pmZWgmuPILxbHEptAQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">突出显示的部分是模型路径—使用它来告诉 Detectron2 您想要使用动物园中的哪个模型</p></figure><p id="a2b9" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">第一次使用模型时，它是从模型动物园下载的，所以可能需要一点时间。</p><h1 id="3bdf" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak">车型配置</strong></h1><p id="a1a3" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">现在我们有了基本的设置，是时候配置我们的模型了。Detectron2 实际上需要被告知使用哪个模型，在哪里找到文件等等。幸运的是，使用来自模型动物园的预先训练好的模型非常简单:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="0ab2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">首先，我们添加了一些新的导入。它们应该被添加到文件的顶部，紧挨着前面的<code class="fe mn mo mp mq b">argparse</code>导入。<code class="fe mn mo mp mq b">get_cfg()</code>函数只初始化一个空的配置，稍后我们会用所需的设置填充它。类型<code class="fe mn mo mp mq b">CfgNode</code>很少出现在类似这里的几个配置行之外，它的行为类似于<code class="fe mn mo mp mq b">argparse.Namespace</code>，因为属性是通过点来访问的。</p><p id="47d2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><code class="fe mn mo mp mq b">merge_from_file()</code>方法需要一个字符串(本地文件的文件路径)或来自<code class="fe mn mo mp mq b">model_zoo.get_config_file()</code>的配置文件作为参数。它将另一个配置文件加载到我们在<code class="fe mn mo mp mq b">cfg</code>中的配置中。当模型在自定义数据集上接受训练并保存在磁盘上时，通常会使用本地文件。更多情况下，我们只想使用来自 Detectron2 model zoo 的预训练模型，并选择和加载此处由<code class="fe mn mo mp mq b">args.base_model</code>指定的配置文件。</p><p id="4bf9" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><code class="fe mn mo mp mq b">cfg.MODEL.WEIGHTS</code>是训练时学习的神经网络权重。我们也可以像加载配置文件一样加载它们，只需向模型动物园提供字符串。请注意，实际上我们正在加载一个检查点文件——实际上神经网络训练永远不会“完成”,模型可能会在以后被额外训练。较早停止训练的原因主要是避免过度拟合或缺乏计算能力/时间。当权重以这种方式加载时，使用权重文件中的最后一个检查点(训练最多的神经网络)。</p><p id="a540" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><code class="fe mn mo mp mq b">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST</code>是一个非常重要的可调超参数。它改变了我们的模型对于要被检测的对象必须具有的最小置信度。换句话说，当物体离得很远，不太明显时，等等。该阈值必须更低才能正确检测到它。然而，将其设置得太低可能会导致错误检测或多次检测到相同的对象(这可以通过非最大抑制(NMS)算法来减少，但不能完全消除)。因此，这个超参数需要根据您的具体情况进行调整。50% (0.5)是一个很好的默认值。</p><p id="98d7" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">现在我们终于准备好初始化将完成所有实际工作的类了— <code class="fe mn mo mp mq b">DefaultPredictor</code>。它只是使用配置中提供的神经网络来进行预测，一次在一台机器和一幅图像上进行预测。一个重要的警告是，它采用 BGR 格式的图像，而不是 RGB 格式——如果您使用 OpenCV ( <code class="fe mn mo mp mq b">cv2</code>)加载图像，这是默认的行为，但其他库如 Pillow 可能使用 RGB 并需要改变格式。</p><p id="d9e4" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们终于准备好使用我们的模型了！</p><h1 id="ea35" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak">执行实例分割</strong></h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="4fee" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">像以前一样，将导入放在文件的开头。新代码遍历提供的图片，并对每张图片进行预测，将它们可视化，并将结果保存到文件中。</p><p id="4b17" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">使用<code class="fe mn mo mp mq b">cv2.imread</code>自动使用 BGR 格式，所以在这里派上用场。结果图像是一个 3D Numpy 数组，其中维度为:</p><ul class=""><li id="2b35" class="mw mx iq lm b ln mg lq mh lt my lx mz mb na mf nb nc nd ne bi translated">行数(图像高度)</li><li id="1933" class="mw mx iq lm b ln nf lq ng lt nh lx ni mb nj mf nb nc nd ne bi translated">列数(图像宽度)</li><li id="7bcf" class="mw mx iq lm b ln nf lq ng lt nh lx ni mb nj mf nb nc nd ne bi translated">表示给定像素的 BGR 值的 3 元素数组(0-255 范围内的整数)</li></ul><p id="9eda" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">正如你所看到的，下面几行我们使用了<code class="fe mn mo mp mq b">[:, :, ::-1]</code>切片，这意味着“取所有行、所有列和所有第三维，但是颠倒最后一个的顺序”。这意味着为了可视化，我们将使用 RGB——没有它，颜色会很奇怪(但是你当然可以自己尝试！).</p><p id="a41d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">加载部分之后是真正的事情:使用<code class="fe mn mo mp mq b">predictor(img)["instances"]</code>来实际使用神经网络并进行实例分割。预测器返回一个只包含一个键-值对的字典，即映射到实例对象的“实例”键。这是一个专门为在 Detectron2 中返回结果而设计的类。它充当关于图像和实际结果的元数据的存储。实例对象包含相当多的信息，足够写一篇单独的文章，但是现在我们可以在不弄乱其内部结构的情况下使用它。现在重要的是要记住 predictor 在 GPU 上工作，并返回仍然在 GPU 上的数据，包括许多 PyTorch 张量。如果您想在 CPU 上处理这些数据(例如，大多数库只处理 PyTorch CPU tensors 或 Numpy 数组)，您必须用<code class="fe mn mo mp mq b">.to("cpu")</code>函数显式地转换它，我们在下面几行中做了这些。</p><p id="dcf4" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><code class="fe mn mo mp mq b">Visualizer</code>是一个用于在图像上绘制来自 Detectron2 神经网络(不仅仅是实例分割，还有其他类型)的结果的类(对于视频你应该使用<code class="fe mn mo mp mq b">VideoVisualizer</code>)。它的论据是:</p><ul class=""><li id="0323" class="mw mx iq lm b ln mg lq mh lt my lx mz mb na mf nb nc nd ne bi translated"><code class="fe mn mo mp mq b">img_rgb</code>:进行预测的基础图像</li><li id="b4f8" class="mw mx iq lm b ln nf lq ng lt nh lx ni mb nj mf nb nc nd ne bi translated"><code class="fe mn mo mp mq b">metadata</code>:提供来自数据集的附加数据，比如类(类别)名称映射。在 Detectron 内部，处理的是类编号，而不是名称——它们首先使用元数据从字符串转换成数字，在这里，我们给 Visualizer 这些信息以将数字映射回字符串。这样，我们将显示“汽车”而不是“2”。这里我们只传递训练期间使用的元数据(来自该模型的 COCO 数据集)</li><li id="2646" class="mw mx iq lm b ln nf lq ng lt nh lx ni mb nj mf nb nc nd ne bi translated"><code class="fe mn mo mp mq b">scale</code>:改变输出图像的尺寸，如果您的输入图像太小或太大，这很有用</li></ul><p id="1af0" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">调用<code class="fe mn mo mp mq b">draw_instance_predictions</code>告诉 Visualizer 我们已经完成了实例分割，并将结果作为参数传递。我们首先需要将它发送到 CPU，因为可视化代码不像神经网络那样在 GPU 上运行。它返回<code class="fe mn mo mp mq b">VisImage</code>对象，这是一个带有一些附加信息(比例、宽度和高度)的图像包装器。<code class="fe mn mo mp mq b">get_image()</code>方法从中提取图像矩阵。我们也应用与上面相同的技巧将其更改为 RGB。</p><p id="59a3" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">剩下的就是保存文件了。事实证明，在文件名后附加“_processed”将文件保存在原文件旁边并不容易，regex 是最简单的方法。<code class="fe mn mo mp mq b">"(.*)\."</code>提取完整的文件路径和文件名，直到扩展名之前的点，并用<code class="fe mn mo mp mq b">.group(0)</code>捕获。用<code class="fe mn mo mp mq b">[:-1]</code>去掉圆点，然后我们可以更改名称(和扩展名，因为 OpenCV 知道如何处理”。png”在目标文件路径中)。最后，保存处理后的图像。</p><h1 id="a8c9" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak">使用示例</strong></h1><p id="0fdd" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">下面是完整的代码(<a class="ae kr" href="https://gist.github.com/j-adamczyk/93f7b2f62c31c5e0b5e8ee32ba958588" rel="noopener ugc nofollow" target="_blank">直接链接</a>):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="13aa" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">让我们试试样本图像上的代码。如果图像与<code class="fe mn mo mp mq b">demo.py</code>文件在同一个目录中，您可以从那里用<code class="fe mn mo mp mq b">python --images image.png</code>运行它:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nk"><img src="../Images/578f4c8dffdcde6c71c43e42fad39b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wwp90zv1OAxWbdLk1P-blA.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">输入图像(<a class="ae kr" href="https://pixnio.com/media/crowd-town-architecture-street-building" rel="noopener ugc nofollow" target="_blank">源</a>)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nk"><img src="../Images/28a578424d5df01cad1f262e0de90e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6xqgI3wtjvLJ8FDGjfiMw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用 Detectron2 进行实例分割后的结果</p></figure><p id="121c" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">正如你所看到的，它工作得很好！可视化工具添加了边界框(检测到的对象周围的矩形)、类名(例如“盆栽”)和以%为单位的类的模型确定性度量。即使在具有挑战性的条件下(不同的比例、彼此非常接近的物体、部分障碍物),大多数物体也能被正确地检测到，并且分割掩模相当精确。这就是那些尖端神经网络的强大之处。</p><h1 id="8da2" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">摘要</h1><p id="bbcc" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在本文中，我们仅仅触及了探测器 2 的表面。这是一个庞大的库，有很多功能和技术细节，有非常复杂的类模型。我希望这能帮助你开始并更好地理解这个演示。如果你对我在这里写的东西还有疑问，或者想要关于 Detectron2 的其他文章(任何特定的主题)，请在评论中告诉我。</p></div></div>    
</body>
</html>