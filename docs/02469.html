<html>
<head>
<title>Generative Adversarial Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成对抗网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generative-adversarial-networks-gans-2231c5943b11?source=collection_archive---------16-----------------------#2020-03-09">https://towardsdatascience.com/generative-adversarial-networks-gans-2231c5943b11?source=collection_archive---------16-----------------------#2020-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8ea2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个温和的甘的介绍，为什么训练他们是有挑战性的，有什么主要的补救措施？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38563860020f788207c0de50d84d62e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CP3JlBkxe9U3KAKdLHuanQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这些超现实的人脸图像不是真实的；它们是由英伟达风格的 GAN 制作的，可以控制图像的不同方面 </p></figure><p id="2402" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">生成对抗网络(<em class="lw">又名</em> GANs)代表了深度学习领域最近最令人兴奋的创新之一。gan 最初是由蒙特利尔大学的 Ian Goodfellow 和 Yoshua Bengio 在 2014 年提出的，Yann LeCun 认为它们是<em class="lw">“过去 10 年中最有趣的想法”</em>。</p><p id="8504" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">GAN 是一个生成模型，其中两个神经网络在典型的博弈论场景中竞争。第一个神经网络是<strong class="lc iu">生成器</strong>，负责生成类似于你的训练数据的新合成数据实例，而它的对手<strong class="lc iu">鉴别器</strong>试图区分生成器生成的真实(训练)和虚假(人工生成)样本。生成器的任务是试图愚弄鉴别器，鉴别器试图抵制被愚弄。这就是为什么整个系统被描述为<strong class="lc iu">对抗</strong>的原因。</p><p id="8482" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如下图所示，生成器的输入只是一个随机噪声，而只有鉴别器可以访问用于分类目的的训练数据。该发生器仅基于鉴别器网络的反馈(在与训练数据匹配的情况下为正，在不匹配的情况下为负)来不断改进其输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/6c15873ffd174e136511db5ceceb1bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKanAdkjQbg1eDDMF2-4ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/" rel="noopener ugc nofollow" target="_blank">生成对抗性网络架构</a></p></figure><p id="dfe0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">形式上，GANs 是基于<strong class="lc iu">零和(极小最大)非合作博弈</strong>，一方赢对方输。当第一个玩家试图最大化其行动时，第二个玩家的行动旨在最小化它们。从博弈论的角度来看，当鉴别器和发生器达到众所周知的<strong class="lc iu">纳什均衡</strong>时，GAN 模型收敛，该均衡是上述极小极大博弈的最佳点。由于两个参与者试图误导对方，当其中一个参与者不管对手做什么都不改变其行动时，纳什均衡就发生了。</p><p id="5c48" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，众所周知，gan 在实践中很难训练，存在严重的不收敛问题，在生成器开始产生接近真实数据的假数据之前需要一段时间。</p><h1 id="83b8" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">共同面临的挑战</strong></h1><p id="2d35" class="pw-post-body-paragraph la lb it lc b ld mq ju lf lg mr jx li lj ms ll lm ln mt lp lq lr mu lt lu lv im bi translated">GANs 有许多常见的故障模式，在训练时可能会出现。在这些失败中，三大挑战问题是世界各地几个研究小组的工作重点。虽然这些问题都没有完全解决，但我们会提到一些人们已经尝试过的事情。</p><h2 id="94c1" class="mv lz it bd ma mw mx dn me my mz dp mi lj na nb mk ln nc nd mm lr ne nf mo ng bi translated"><strong class="ak"> 1。模式崩溃</strong></h2><p id="b93d" class="pw-post-body-paragraph la lb it lc b ld mq ju lf lg mr jx li lj ms ll lm ln mt lp lq lr mu lt lu lv im bi translated">在训练 GANs 时，目标是产生各种各样的模拟真实数据的假数据<em class="lw">(即符合相同的分布)</em>。从一个随机输入，我们想创造一个完全不同的新输出<em class="lw">(例如一个新的现实的人脸)</em>。然而，当发生器发现<strong class="lc iu">一个</strong>或一个<strong class="lc iu">有限多样性</strong>的样本而不考虑输入时，这对于鉴别器来说似乎是最合理的，发生器可以<strong class="lc iu"> <em class="lw">合法地</em> </strong>学习只产生那个输出。即使它起初看起来是训练进展的良好迹象，但当训练被称为<strong class="lc iu"> <em class="lw">模式崩溃</em> </strong>或<strong class="lc iu"> <em class="lw"> helvetica 场景</em> </strong>的 gan 时，这是最具挑战性的失败之一。一旦鉴别器陷入局部最小值并且不能区分真实输入和发电机的输出，这种情况就可能发生。此时，生成器将很容易注意到这个黑洞，并不断生成相同的输出，或者最多是略有不同的输出。</p><p id="ee99" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">尝试补救</em></p><ul class=""><li id="1986" class="nh ni it lc b ld le lg lh lj nj ln nk lr nl lv nm nn no np bi translated">使用不同的损失函数，例如<strong class="lc iu"> Wasserstein 损失</strong>，让你训练鉴别器达到最优，而不用担心消失梯度。如果鉴别器没有陷入局部最小值，它会学习拒绝发电机稳定的输出。所以发电机必须尝试新的东西。</li><li id="7d16" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated"><strong class="lc iu">展开的 GANs </strong>使用发电机损耗函数，该函数不仅包含当前鉴别器的分类，还包含未来鉴别器版本的输出。所以生成器不能对单个鉴别器进行过度优化。</li><li id="ae16" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">用<strong class="lc iu">不同的数据样本</strong>训练 GANs。</li></ul><h2 id="2bc8" class="mv lz it bd ma mw mx dn me my mz dp mi lj na nb mk ln nc nd mm lr ne nf mo ng bi translated">2.不收敛</h2><p id="1468" class="pw-post-body-paragraph la lb it lc b ld mq ju lf lg mr jx li lj ms ll lm ln mt lp lq lr mu lt lu lv im bi translated">GANs 经常无法收敛。通过假设两个神经网络相互竞争，目标是两个网络最终都将达到平衡，对抗性训练场景可能很容易看起来不稳定。如果不深入了解如何规避这种风险，这可能被认为是一个天真的假设，因为无法保证竞争梯度更新将导致收敛而不是随机振荡。</p><p id="109f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">尝试补救</em></p><ul class=""><li id="212e" class="nh ni it lc b ld le lg lh lj nj ln nk lr nl lv nm nn no np bi translated">一个简单的技巧是<strong class="lc iu">将噪声</strong>添加到鉴别器输入(真实和合成数据)中，以阻止它对其分类过于自信，或者依靠一组有限的特征来区分训练数据和生成器的输出。</li><li id="dd3e" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">在与上一个技巧相同的方向上，我们可以尝试使用 NIPS'2017 中提出的<a class="ae ky" href="https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">两个时标更新规则</strong> </a>，其中作者提供了收敛到纳什均衡的数学证明。为鉴别器选择比生成器更高的学习速率。因此，我们的生成器将比鉴别器有更多的训练迭代和更多的训练时间。很容易理解，训练一个分类器比训练一个生成模型要容易得多。</li><li id="b27e" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">惩罚歧视者权重:参见，例如，通过<strong class="lc iu">正则化</strong>稳定生成性对抗网络的训练。</li></ul><h2 id="86f8" class="mv lz it bd ma mw mx dn me my mz dp mi lj na nb mk ln nc nd mm lr ne nf mo ng bi translated">3.消失渐变</h2><p id="aa57" class="pw-post-body-paragraph la lb it lc b ld mq ju lf lg mr jx li lj ms ll lm ln mt lp lq lr mu lt lu lv im bi translated">研究表明，如果你的鉴别器太好，那么发电机训练可能会因梯度消失而失败。实际上，最佳鉴别器不能为生成器提供足够的信息来取得进展。当我们应用反向传播时，我们使用微分的链式法则，它具有乘法效应。因此，梯度反向流动，从最后一层到第一层。当它向后流动时，它变得越来越小。有时，梯度太小，以至于初始层学习非常慢或者完全停止学习。在这种情况下，梯度根本不改变初始层的权重值，因此网络中初始层的训练被有效地停止。这就是所谓的<strong class="lc iu">消失渐变</strong>问题。</p><p id="6fb4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">尝试补救</em></p><ul class=""><li id="329c" class="nh ni it lc b ld le lg lh lj nj ln nk lr nl lv nm nn no np bi translated">我们可以使用<strong class="lc iu">激活功能</strong>，例如<em class="lw"> ReLU </em>、<em class="lw"> LeakyReLU </em>，而不是<em class="lw"> sigmoid </em>或<em class="lw"> tanh </em>分别在 0 和 1 或-1 和 1 之间挤压输入值，导致梯度呈指数下降。</li><li id="66aa" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated"><strong class="lc iu"> Wasserstein 损失</strong>旨在防止梯度消失，即使您将鉴别器训练到最佳状态。</li><li id="f3ae" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">原<a class="ae ky" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">甘</a>论文提出了一种<strong class="lc iu">改进的极大极小损失</strong>来处理消失梯度。</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="5ded" class="mv lz it bd ma mw mx dn me my mz dp mi lj na nb mk ln nc nd mm lr ne nf mo ng bi translated">了解你的作者</h2><p id="16cd" class="pw-post-body-paragraph la lb it lc b ld mq ju lf lg mr jx li lj ms ll lm ln mt lp lq lr mu lt lu lv im bi translated">拉贝赫·阿亚里是一名高级数据科学家，致力于信用风险建模和欺诈分析的应用人工智能问题，并在机器学习方面进行原创研究。我的专业领域包括使用深度神经网络的数据分析、机器学习、数据可视化、特征工程、线性/非线性回归、分类、离散优化、运筹学、进化算法和线性编程。欢迎随时给我留言<a class="ae ky" href="http://rabeh.ayari@polymtl.ca" rel="noopener ugc nofollow" target="_blank">这里</a>！</p></div></div>    
</body>
</html>