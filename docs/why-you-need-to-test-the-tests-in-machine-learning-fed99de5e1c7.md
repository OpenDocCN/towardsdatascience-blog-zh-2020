# 为什么你需要测试机器学习中的测试

> 原文：<https://towardsdatascience.com/why-you-need-to-test-the-tests-in-machine-learning-fed99de5e1c7?source=collection_archive---------52----------------------->

## 即使您小心翼翼地分离训练和测试数据集，还有什么可能出错

![](img/b690f4b554a6b871b8fcc352d9e38fb8.png)

[国家癌症研究所](https://unsplash.com/@nci?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

如果你曾经与机器学习产品进行过交互，你很可能知道将模型的训练和测试分开的重要性，以避免[过度拟合](https://en.wikipedia.org/wiki/Overfitting)，并确保模型能够在看不见的数据上很好地推广。

# 培训与测试分离的背景

对于那些在这个领域的人来说，如今对培训和测试的关注似乎是显而易见的，但并不总是如此。在数据科学出现之前，它的祖先统计学还没有完全接受这个概念。

为什么？

**在机器学习和大数据出现之前，模型比较简单，**而现在更多的数据已经允许我们建立更复杂的模型。正如我们所知，当模型复杂性增加时，过度拟合会变得越来越成问题。如果你运行的是线性回归，你就不用担心过度拟合。此外，简单模型的模型可解释性通常更容易，一些过度拟合可以通过细致的手工工作来补偿。

**因此，分离训练和测试数据集更多地是一种需要，而不是最佳实践。**

# 培训 vs 测试分离就够了吗？

假设你是一个机器学习产品的商业消费者或产品经理。您的数据科学团队开发了一种新的 ML 产品，采用黄金标准的方法来分离训练集和测试集。

您需要决定是否将产品投入生产。但是你要对它的性能有 100%的把握，才能做最后的决定。

因此，您要求数据科学团队向您展示模型和数据。他们向您展示了完整的数据集，以及它是如何划分为 70%的训练和 30%的测试子集的，并发誓最终的模型纯粹是在训练数据集上训练的，其性能指标仅来自测试数据集。

你现在放心了。不会出错的。对吗？

嗯，**有三件常见的事情仍然可能出错。**

# 1.测试数据集以前已经使用过

理论上，这个规则看起来很简单:不到最后阶段，永远不要使用测试数据集。然而在实践中，100%地尊重这一点是非常罕见的，其影响可以从小的高估到非常严重的错误。

为了验证这一点，需要问两个问题。

**在哪个数据集上执行了初始数据探索？**

数据科学的第一部分(通常更耗时)是数据探索和数据争论。这是在划分训练和测试数据集中的数据之前还是之后执行的？

从纯理论的角度来看，在模型评估的最后一步之前，你不应该以任何方式看到测试数据。短于这个时间的任何东西都可能让你对自己的预测能力过于自信。

然而，这在实践中往往不那么容易，尤其是如果您的数据来自业务环境。如果作为数据探索的一部分，您需要评估来自手动或外部来源的缺失数据并决定如何处理这些数据，该怎么办？(例如，是填充还是删除缺失的行或列)您在查看完整数据集时做出这些决定的事实偏离了理想的场景，但在实践中这通常是无法避免的。

如果在这个阶段您已经执行了特征选择和工程，这一点会变得特别重要。如果您根据要素与其在整个数据集中的目标变量的相关性来选择要素，这肯定会影响概化误差。

**在测试数据集上评估了多少不同的模型？**

测试集失去其纯粹状态的最常见方式是当您在其上测试多个模型时。这有时可能是恶意的，但是在我的经验中，大多数时候是糟糕的计划迭代的结果。

典型案例:您的数据科学团队选择并构建了一个模型，在某种程度上，他们确信它是最终的，因此他们在测试集上测试它以评估它的性能。在此阶段，与业务部门讨论模型，给出反馈并建议尝试合并另一个功能，或者降低某些变量的重要性。这样的迭代做两到三次可能不会被注意到，但是做的越多，问题就越大。

**一个警告:如果你使用** [**交叉验证**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) **呢？**当您需要调整[超参数](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))时，通常最佳做法是进一步分割训练和验证集中的“训练”数据。如果您的数据有限，交叉验证可以帮助您将相同的子集用于训练和验证目的。然而，交叉验证不应该用来替代测试集，否则前面提到的问题仍然存在。

# 2.测试数据包含某种形式的数据泄漏

数据泄漏是指您的模型使用了在生产中不可用的数据。这可能会通过欺骗错误地增加模型性能指标。

发现数据泄露问题并不总是那么容易。实践中有两种常见的方式:

*   只有在必须做出预测的时刻之后，数据才是可用的
*   该数据在当时理论上是可用的，但在技术上对预测系统是不可用的

我在工作环境中遇到的一个场景是试图预测酒店预订的取消。预订酒店时，特定数据在当时可用(例如，关于客户、旅行、预订季节性)，但其他数据仅在预订和计划入住日期之间的时间跨度内收集。

问题是，有些数据可能在数据仓库的相同表中被跟踪，或者可能在不断更新的相同字段中被跟踪。如果没有这些字段的历史视图，这些字段将无法用于模型训练目的。

最极端的情况是，一些字段可能只有在取消被实际处理时才被填充。例如，我们可能只在发送取消请求时收集某些客户信息。如果我们尝试使用这些字段来训练我们的模型，我们将获得取消结果的完美预测！

# 3.你仍然不能测试未来的数据

现在，让我们假设您已经非常小心地使用了您的测试数据集，并且避免了任何数据泄漏。

您还能相信经过测试的车型性能会在生产中得到体现吗？

好吧，至少你的泛化性能仍然是参考过去的数据，最有可能的是，你将使用你的模型来预测未来的现象。

每个模型的基本潜在假设是，在一定限度内，你相信你试图预测的现象在未来不会变化太快，否则你无法从过去的数据中进行推断。

这个问题有多大很难讲，当然很大程度上取决于应用程序。试图对此建模也可能变得棘手，可能需要结合对未来预期的假设(如贝叶斯模型)，这通常不是微不足道的。

根据我在商业环境中的经验，这一点经常被忽视。然而，在后 COVID 时代，这种情况可能会改变。想想训练一个模型来预测酒店取消，就在 COVID 爆发之前(像我一样！).

当生活在 COVID 之后慢慢回归正常时，如果不进行某种调整，相信过去的数据来预测未来将变得越来越困难。但是这可能是另一篇文章的主题！