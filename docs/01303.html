<html>
<head>
<title>Facial Re-Identification Using Siamese Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用连体网的面部再识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/facial-re-identification-using-siamese-nets-d36df39da7c0?source=collection_archive---------20-----------------------#2020-02-05">https://towardsdatascience.com/facial-re-identification-using-siamese-nets-d36df39da7c0?source=collection_archive---------20-----------------------#2020-02-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bce7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">端到端教程</h2></div><p id="75fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">面部再识别(FRID)是根据一个人的面部来验证其身份的问题。例如，一个政府机构可以根除ID卡的使用，而简单地使用FRID系统来处理出席，而不用担心陌生人窃取某人的ID来进入系统。</p><p id="f78e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题的关键是一个分类问题，其中模型必须将人脸图像分类为数据库中的人。正因为如此，使用具有接近100%测试准确率的许多先进的图像分类器可能很有诱惑力。结案了对吗？只需微调ResNet50甚至ResNet101模型，即可从人脸图像中对人名进行分类。这样做时会出现一些基本问题:</p><ul class=""><li id="c766" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">我从哪里获得数据？对于您想要分类的每个人，最多会有一两张图像可用，并且该图像将是护照风格的照片，与正在分类的图像非常不同。图像分类器仅在至少有50幅图像时工作良好，以在测试图像上实现高置信度，这比可用图像多49幅图像。</li><li id="32e9" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kh ir">我如何确保对成千上万的课程有很高的信心？</strong>用于图像分类的最大数据集有300到1000个类别。然而，公司组织有100到50，000人，因此有100到50，000个班级。</li><li id="e0a1" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kh ir">当新员工被录用时，我该怎么做？</strong>深度学习网络的本质是，没有办法只是向模型中添加一个新类，然后让它做得很好。添加一个新的类意味着用添加的类重新训练整个网络。这既浪费了计算资源又浪费了时间，任何人都不会有多余的计算资源和时间，尤其是数据科学家。</li></ul><p id="1717" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些问题都可以用一种新的模型来回答:连体网络。</p><h1 id="1905" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">直觉</h1><p id="f300" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">让我们首先介绍暹罗网络。当人们将人类直觉应用到深度学习模型中时，大多数深度学习进步都发生了，例如当ResNet中首次使用残差块时，或者当人们更加努力地尝试将人类抽象融入计算机时，基于风格的损失变得更加流行时。暹罗网络是这些进步中的另一个，它严重依赖于人类对计算机应该如何解决问题的直觉。</p><p id="6fa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">暹罗网络背后的核心直觉是试图学习面部的表征。这种表征类似于人类储存在大脑中的关于面部特征的信息，如面部特征的大小、肤色、眼睛颜色等。人类可以理解，如果它看到另一张具有相似特征的脸，那么新的脸很有可能属于同一个人。另一方面，如果人类看到新的面部与其先前看到的面部不匹配，那么人类再次制作新面部的表示以存储在其存储器中。</p><p id="b4a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这正是暹罗网络的运作方式。一个函数将人脸的输入图像转换成包含人脸特征表示的向量。然后，我们希望这个向量与同一张脸的向量相似，而与另一张脸的向量非常不同。</p><p id="10d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，该模型学习如何提取人脸的重要特征，从而将人脸与其他人脸区分开来。一旦获得特征映射，就可以将其与数据库中要匹配的其他人脸的特征映射进行比较。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mm"><img src="../Images/61ef67fcb06430ed591c0b7d9307a82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VU22pw54nNdQQIwF.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">图片由(<a class="ae nc" href="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" rel="noopener ugc nofollow" target="_blank">签名验证使用“暹罗”时间延迟神经网络</a>)提供</p></figure><h1 id="ccef" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">三重态损失和其他细节</h1><p id="5579" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">任何神经网络最重要的部分是它是如何训练的。训练是一个庞大的、看似无用的功能被调整以完成其任务的方式。暹罗网络最初是成对训练的，类似于模型的用例。同一人脸的两个向量之间的距离最大化，不同人脸的两个向量之间的距离最小化。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/475d36f103cdae20ddf534dfe18c9974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*cLdLa8-3ix7WgHKX"/></div></figure><p id="dbb2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这种方法使训练过程变得复杂，并且在计算要求方面也是低效的。训练这种所谓的对比损失意味着，对于每个训练样本，模型必须计算四个向量，然后对每对图像执行两组模型优化。出现的另一个问题是，训练样本将是高度不平衡的，因为其中人脸相同的对的数量将会少得多，而其中人脸不同的对的数量将会更多。</p><p id="c43f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了降低对比损失的计算要求，也为了部分解决数据问题，创建了三元组损失。三重态损失分四步进行:</p><ol class=""><li id="35f3" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la ne lh li lj bi translated">从数据集中采样三个图像，锚、正面和反面。主播是任何一张脸，正面是和主播同一张脸的图像，反面是不同脸的图像。</li><li id="ed68" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la ne lh li lj bi translated">计算每个图像的向量</li><li id="3225" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la ne lh li lj bi translated">使用一个距离函数来找出锚点和正负图像之间的距离。</li><li id="a923" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la ne lh li lj bi translated">将损失值计算为锚点和负片图像之间的距离与锚点和正片图像之间的距离之差。</li></ol><p id="17c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下等式显示了三重态损耗的数学表示，以及一些重要的微妙之处。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ab4ca675c54e7eb74f3953693fd9584c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/0*zjb3qB8SMBH0G3SQ.png"/></div></figure><p id="c6a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述损失和等式之间的第一个差异是在差异和<code class="fe ng nh ni nj b">0.0</code>之间添加了<code class="fe ng nh ni nj b">Max()</code>函数。这是为了确保损失函数不会降到零以下。</p><p id="356f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二个更重要的区别是增加了一个边距，用<code class="fe ng nh ni nj b">m</code>表示。这个余量确保模型不会学习为所有的面输出相同的向量，不管这些面是否匹配。请注意，如果锚和正值之间的距离等于锚和负值之间的距离，如果没有保证金项，损失将最小化为零。</p><h1 id="f907" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">那么这怎么解决问题呢？</h1><ul class=""><li id="05bf" class="lb lc iq kh b ki mh kl mi ko nk ks nl kw nm la lg lh li lj bi translated">该模型只需要每个身份的最多两张脸来训练它，这与现实世界中可用的数据类型非常相似，因为一个人必须至少有一张被识别的人的图像和一张该人的实时图像。</li><li id="def2" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">由于该模型不依赖于从人脸中分类不同的身份，所以它不必处理可能的成千上万的人脸来分类，它只需要执行精确的比较。</li><li id="cc5e" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">出于同样的原因，新添加的雇员并不意味着网络的重新训练，该模型将简单地计算新雇员的面部嵌入，以添加到面部嵌入的数据库中。</li></ul><h1 id="34e7" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">履行</h1><p id="e434" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">对于这篇博文中的实现，我们将使用<code class="fe ng nh ni nj b">tensorflow==2.1</code>和gpu(如果有的话),因为这样一个网络的训练是一个繁重的计算。我将在英伟达GTX 1080 TI上训练我的模型，所以我将使用<code class="fe ng nh ni nj b">tensorflow-gpu==2.1</code>和<code class="fe ng nh ni nj b">CUDA==10.2</code>。</p><p id="4ef2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于强大的特征提取器网络的发展，对于几乎每种情况，通常很少需要从零开始设计定制的卷积神经网络。我们将使用ResNet50模型的行业标准来提取我们的特征。因此，唯一需要增加的是在tf中设计一个定制的损失函数。Keras API来训练我们的模型。</p><p id="1e0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种不同的方法将自定义损失函数合并到tf中。Keras:创建一个函数来计算传递给<code class="fe ng nh ni nj b">model.compile()</code>方法的损失，或者创建一个损失层并使用<code class="fe ng nh ni nj b">Layer.add_loss()</code>方法。</p><p id="c61f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建自定义损失函数的两种方法之间的唯一区别是，在第一种方法中，损失函数必须是<code class="fe ng nh ni nj b">loss_fn(y_true, y_pred)</code>的格式，而第二种方法可以是任何格式。</p><p id="b79b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于三重态损耗无法用<code class="fe ng nh ni nj b">y_true, y_pred</code>格式表示，因此将使用第二种方法。</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="33df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的代码中，<code class="fe ng nh ni nj b">triplet_loss</code>是损失函数，它接受锚、正面和负面图像。<code class="fe ng nh ni nj b">@tf.function</code>的注释简单地使用<code class="fe ng nh ni nj b">tf.Autograph</code>来绘制函数，并允许更快的计算。</p><p id="87e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ng nh ni nj b">TripletLoss</code>层继承自<code class="fe ng nh ni nj b">tf.keras.layers.Layer</code>，使用<code class="fe ng nh ni nj b">add_loss</code>方法将三重损失函数添加到该层的损失中。然后，图层返回输入而不更改它们。</p><p id="fdf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<code class="fe ng nh ni nj b">add_loss</code>方法不仅将损失张量添加到层的损失中，而且当使用<code class="fe ng nh ni nj b">model.fit()</code>训练模型时，所有的层损失也被自动优化，这允许我们在以下代码中编译没有任何整体损失函数的模型:</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="721b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的代码显示了模型的完整代码以及它是如何被训练的。</p><h1 id="d376" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结果</h1><p id="91ee" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在位于http://vis-www.cs.umass.edu/lfw/的<a class="ae nc" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank">LFW(野外标记的人脸)数据库上训练该模型，该模型能够以90%的准确率正确“预测”匹配的人脸，并能够以95%的准确率区分两张人脸是否不同。</a></p><p id="fa11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年里，已经发表了多篇论文，对普通的三重损失进行了重大改进，并在更困难的数据集上进行了改进，如https://megapixels.cc/duke_mtmc/的杜克MTMC。</p><p id="c8c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的几篇文章中，我将分析最近在重新识别方面的进展。</p></div></div>    
</body>
</html>