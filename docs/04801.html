<html>
<head>
<title>Checking grammar with BERT and ULMFiT.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT和ULMFiT检查语法。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=collection_archive---------18-----------------------#2020-04-27">https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=collection_archive---------18-----------------------#2020-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e980" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们来看看迁移学习的两个重量级人物在检查语法方面的表现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1cf7fcecda7eedd44c7148b301e48645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bioh69K7_vRZXp9LvDzi9Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@craftedbygc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">绿色变色龙</a>在<a class="ae ky" href="https://unsplash.com/s/photos/write-book?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="64ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迁移学习对于所有NLP研究人员来说是一个游戏改变者，所以在开始之前，让我们快速回顾一下什么是迁移学习。</p><blockquote class="lv"><p id="7319" class="lw lx it bd ly lz ma mb mc md me lu dk translated"><strong class="ak"> <em class="mf">迁移学习(Transfer learning)是一种机器学习方法，其中为一项任务开发的模型被重新用作第二项任务的模型的起点。</em> </strong></p></blockquote><p id="b05b" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">简单地说，它将来自一个领域和任务的先验知识运用到不同的领域和任务中。幸运的是，我们有各种这样的模型，它们具有关于语言及其语义的先验知识，所以我们将只使用那些(有知识的)模型，并看看它们如何执行我们手头的任务(这里检查语法)。</p><p id="b868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们许多人在各种NLP任务中面临的一个主要问题是缺乏良好的硬件(GPU和TPU)，这是NLP任务的一个必要条件，因为我们要处理高维密集向量。在本文中，我们将看到如何克服这一点，使用Google Colab来训练模型，并使用一台CPU机器来测试和使用我们的语法检查器。</p><h1 id="5c59" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">让我们见见竞争者</h1><p id="70c0" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">一方面，我们有fast.ai的杰瑞米·霍华德和NUI Galway Insight Center的Sebastian Ruder提出的ULMFiT。ULMFiT使用以下新技术实现了最先进的结果:</p><ul class=""><li id="bab7" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">区别微调</li><li id="3dbe" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">倾斜三角形学习率，以及</li><li id="f88b" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">逐步解冻</li></ul><p id="2330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法包括将在<a class="ae ky" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset" rel="noopener ugc nofollow" target="_blank"> Wikitext 103数据集</a>上训练的预训练语言模型(LM)微调到新的数据集，以使其不会忘记之前学习的内容。</p><p id="33fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了与Fast.ai的ULMFIT竞争，我们有谷歌的<strong class="lb iu"> BERT </strong>(来自变压器的双向编码器表示)，它为大多数NLP任务提供了最先进的结果。BERT的关键技术创新是将Transformer(一种流行的注意力模型)的双向训练应用于语言建模。这与以前从左到右或者结合从左到右和从右到左训练来查看文本序列的努力形成对比。</p><p id="8dff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们不会深入讨论每个模型的细节，而是看看它们在检查语法方面的表现。</p><h1 id="05fd" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">那么数据在哪里呢？</h1><p id="d942" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们将使用<a class="ae ky" href="https://nyu-mll.github.io/CoLA/" rel="noopener ugc nofollow" target="_blank">语言可接受性语料库(CoLA) </a>数据集进行单句分类。它是一组被标记为语法正确或不正确的句子。它于2018年5月首次发布，是“GLUE Benchmark”中包括的测试之一，BERT等模型正在进行竞争。</p><p id="75db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一下我们的数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/62e7f3d5af584f95ad23f6e70894e4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*3pOC2wzVygFhIR1LbjJrIQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的一小部分。(0 -&gt;错，1-&gt;对)。</p></figure><h1 id="c618" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">让我们开始战斗吧</h1><p id="597c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们将使用Google Colab来微调预训练的模型，因为我们可以轻松地使用Colab免费提供给我们的GPU，这将大大减少训练时间。</p><h2 id="828e" class="nx mm it bd mn ny nz dn mr oa ob dp mv li oc od mx lm oe of mz lq og oh nb oi bi translated">伯特</h2><p id="c24c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">对于伯特，我们将使用<a class="ae ky" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">拥抱脸的PyTorch </a>实现，因为它非常简单和直观。以下是我们大致遵循的步骤:</p><ol class=""><li id="152d" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu oj no np nq bi translated">加载<a class="ae ky" href="https://nyu-mll.github.io/CoLA/" rel="noopener ugc nofollow" target="_blank">数据集</a>并解析它。</li><li id="e83c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">将句子编码成伯特可以理解的格式。</li><li id="f42e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">训练(微调)，它涉及到这些步骤<strong class="lb iu"> : </strong></li></ol><ul class=""><li id="df18" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">打开我们的数据输入和标签</li><li id="a56f" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">将数据加载到GPU上进行加速</li><li id="30f1" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">清除上一步中计算的渐变。</li><li id="aba7" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">正向传递(通过网络输入数据)</li><li id="74f4" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">反向传递(反向传播)</li><li id="3b9f" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">告诉网络使用optimizer.step()更新参数</li><li id="8c67" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">跟踪用于监控进度的变量</li><li id="8b18" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">我们将指定<strong class="lb iu">BertForSequenceClassification</strong>作为最后一层，因为它是一个分类任务<strong class="lb iu">。</strong></li></ul><p id="982b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.将微调后的模型保存到我们的本地磁盘或驱动器。</p><p id="84b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.下载保存的模型，并在我们的本地机器上做一些语法检查。</p><p id="4f24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<a class="ae ky" href="https://gist.github.com/sayakmisra/dbb06efec99e760cf9e5d197175ad9c5#file-grammar-checker-bert-ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>包含了全部代码。</p><h2 id="9c69" class="nx mm it bd mn ny nz dn mr oa ob dp mv li oc od mx lm oe of mz lq og oh nb oi bi translated">乌尔菲特</h2><p id="d712" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><a class="ae ky" href="https://docs.fast.ai/" rel="noopener ugc nofollow" target="_blank"> Fast.ai </a>提供了一个简单得多的接口来实现ULMFIT。以下是涉及的步骤:</p><ol class=""><li id="844f" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu oj no np nq bi translated">加载<a class="ae ky" href="https://nyu-mll.github.io/CoLA/" rel="noopener ugc nofollow" target="_blank">数据集</a>并解析它。</li><li id="c8b6" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">使用我们的数据集建立一个语言模型。</li><li id="ab5c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">只使用语言模型的编码器来构建分类器。</li><li id="2e16" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">微调分类器(这里是语法检查器)。</li><li id="b076" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">保存微调后的模型。</li><li id="5192" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">下载模型在我们的本地设置中玩。</li></ol><p id="48e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是包含上述代码的<a class="ae ky" href="https://gist.github.com/sayakmisra/3f5a3fc7eb18e0a6f93dac4a08b08dd8#file-grammar-checker-ulmfit-ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="fc8c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">那么谁是赢家呢？</h1><p id="a549" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">为了可视化结果，让我们用HTML和CSS构建一个带有简单UI的小flask应用程序。这里是Flask应用程序的<a class="ae ky" href="https://github.com/sayakmisra/bert_ulmfit_grammar_checker" rel="noopener ugc nofollow" target="_blank"> Github repo </a>，在这里你可以很容易地下载你自己的模型并使用这个应用程序。</p><p id="720c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从一些句子开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8c29da8141f84a81506aec6c51393c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*0Q4bC7YHUkWI0DV5lTxbqQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下面是一个简单的陈述。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/dcacfc686cb29fa6033b569d3c3c2ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*vKmQUwssCME3TbTYfNy-3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伯特和乌尔菲特都预测正确。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4bc69680b56c920a926a5e815baed8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*VeK56H_ezyyXjJGKmlOINg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">干得好，伯特，你不适合，你需要更多的训练。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bdd382bfed8b8d4baec30cced8b1bef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*Tjw52x3PXTJy1gMEfkoZ8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">太好了，你们俩都在这里。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/93a35c3121c92d4feaa8dc26cb5ff266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*tEA0JqesS1Pmoc2qxLv63g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这是一个棘手的问题，但乌尔菲特可能有优势。</p></figure><p id="9fd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">玩了一天后，伯特似乎是这场战斗的明显赢家。尽管我们可以进一步微调超参数以改善结果。</p><p id="d258" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">评估模型的更好方法是通过使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html" rel="noopener ugc nofollow" target="_blank">马修相关系数</a>的预测，因为这是更广泛的NLP社区用来评估可乐性能的指标。按照这个标准，+1是最好的分数，而-1是最差的分数。通过这种方式，我们可以看到我们在这项特定任务中相对于最先进模型的表现有多好。</p><p id="3fa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的车型，BERT的MCC得分<strong class="lb iu">为0.55 </strong>优于ULMFiT的<strong class="lb iu"> 0.4042 </strong>。</p><h1 id="60e0" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">结论</h1><p id="f740" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在这场战斗中，BERT可能在理解语言语义方面比fast.ai的ULMFiT有优势，但我们可以进一步优化我们的模型，以获得更好的结果。我们也可以尝试其他迁移学习模式，如:脸书的RoBERTa，XLNET，ALBERT等。用于检查语法。</p><h1 id="4ad8" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">参考资料:</h1><ol class=""><li id="8426" class="ni nj it lb b lc nd lf ne li op lm oq lq or lu oj no np nq bi translated">Fast.ai文档<a class="ae ky" href="https://docs.fast.ai/" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/</a></li><li id="1511" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">拥抱脸变形金刚<a class="ae ky" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/index.html</a></li><li id="21e7" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oj no np nq bi translated">Chris mcCormick的博客文章<a class="ae ky" href="http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/" rel="noopener ugc nofollow" target="_blank">http://mccormickml . com/2019/11/11/Bert-research-EP-1-key-concepts-and-sources/</a></li></ol></div></div>    
</body>
</html>