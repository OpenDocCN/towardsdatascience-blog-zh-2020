<html>
<head>
<title>Reinforcement Learning in a few lines of code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几行代码中的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-in-a-few-lines-of-code-d6c8af1e0fd2?source=collection_archive---------13-----------------------#2020-01-07">https://towardsdatascience.com/reinforcement-learning-in-a-few-lines-of-code-d6c8af1e0fd2?source=collection_archive---------13-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2f34" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="d208" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用稳定基线和 Gym 训练 SOTA RL 算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/8c9c81ccad895569cf66d834713ff1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iFh4BZIV8Uvf9_TjjQOcGg.gif"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">从 Procgen 检索</p></figure><p id="af14" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">强化学习在过去的一年里有了很大的改进，最先进的方法每两个月出现一次。我们已经看到 AlphaGo 击败世界冠军围棋手<a class="ae lz" href="https://deepmind.com/alphago-china" rel="noopener ugc nofollow" target="_blank">柯洁</a>，多智能体玩<a class="ae lz" href="https://www.youtube.com/watch?v=kopoLzvh5jY&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">捉迷藏</a>，甚至 AlphaStar 在<a class="ae lz" href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" rel="noopener ugc nofollow" target="_blank">星际</a>中也竞技性地守住了自己。</p><p id="48cf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">实现这些算法可能非常具有挑战性，因为它需要对深度学习和强化学习有很好的理解。本文的目的是给你一个使用一些简洁的包的快速入门，这样你就可以很容易地开始强化学习。</p><p id="66eb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关于如何实现 SOTA 深度强化学习算法的深入教程，请参见<a class="ae lz" href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae lz" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">本</a>。强烈推荐他们！</p><h1 id="1194" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">环境</h1><p id="7ce5" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">在我们开始实现这些算法之前，我们首先需要创建一个工作环境，即游戏。对于算法来说，理解什么是动作和观察空间是很重要的。为此，我们将进入几个可用于选择有趣环境的包。</p><h2 id="4a3f" class="mx mb it bd mc my mz dn mg na nb dp mk lm nc nd mm lq ne nf mo lu ng nh mq iz bi translated">体育馆</h2><p id="74de" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">Gym 是一个开发和比较强化学习算法的工具包。它通常用于实验和研究目的，因为它提供了一个简单易用的环境工作界面。</p><p id="e604" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简单地用<code class="fe ni nj nk nl b">pip install gym</code>安装包。完成后，您可以使用以下代码创建一个环境:</p><pre class="ks kt ku kv gt nm nl nn no aw np bi"><span id="22e7" class="mx mb it nl b gy nq nr l ns nt">import gym<br/>env = gym.make(‘CartPole-v0’)</span></pre><p id="ccb4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<strong class="lf jd">推车</strong>环境中，你的任务是防止通过未启动的接头连接到推车上的杆子翻倒。</p><p id="d18a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><code class="fe ni nj nk nl b">env</code>变量包含关于环境(游戏)的信息。要了解侧手翻的<strong class="lf jd">动作空间</strong>是什么，只需运行<code class="fe ni nj nk nl b">env.action_space</code>就会产生<code class="fe ni nj nk nl b">Discrete(2)</code>。这意味着有两种可能的独立动作。要查看<strong class="lf jd">观察空间</strong>，您运行<code class="fe ni nj nk nl b">env.observation_space</code>，这将产生<code class="fe ni nj nk nl b">Box(4)</code>。这个方框表示 n (4)个闭区间的<br/>笛卡儿积。</p><p id="6726" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">要渲染游戏，请运行以下代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f06b829bc96961fcf14d8bbaafb2edb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*I-97TmXebhKGEj-nDw7mYA.gif"/></div></figure><p id="7e7e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，如果我们选择采取随机行动，购物车就会不断失灵。最终，目标将是运行一个强化学习算法，学习如何解决这个问题。</p><p id="5f22" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有关健身房环境的完整列表，请参见<a class="ae lz" href="http://gym.openai.com/envs/#classic_control" rel="noopener ugc nofollow" target="_blank">本</a>。</p><p id="6e6c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">注意</strong>:如果您在运行雅达利游戏时遇到问题，请参见<a class="ae lz" href="https://github.com/openai/gym/issues/1726" rel="noopener ugc nofollow" target="_blank">本</a>。</p><h2 id="3c43" class="mx mb it bd mc my mz dn mg na nb dp mk lm nc nd mm lq ne nf mo lu ng nh mq iz bi translated">重新流行</h2><p id="7243" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">创造有趣环境的另一个选择是使用<a class="ae lz" href="https://github.com/openai/retro" rel="noopener ugc nofollow" target="_blank">复古</a>。这个包是由 OpenAI 开发的，允许你使用 ROMS 来模拟游戏，如空袭者-创世纪。</p><p id="a9ad" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">只需用<code class="fe ni nj nk nl b">pip install gym-retro</code>安装软件包即可。然后，我们可以通过以下方式创建和查看环境:</p><pre class="ks kt ku kv gt nm nl nn no aw np bi"><span id="2612" class="mx mb it nl b gy nq nr l ns nt">import retro<br/>env = retro.make(game='Airstriker-Genesis')</span></pre><p id="642e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">再次，为了渲染游戏，运行下面这段代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9fc202f4ab57ee83956069856cd6bd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/1*6s1ZEmmBcHqzzKz_sSPRMg.gif"/></div></figure><p id="2038" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">要安装 ROMS，你需要找到相应的。sha 文件，然后运行:</p><pre class="ks kt ku kv gt nm nl nn no aw np bi"><span id="6113" class="mx mb it nl b gy nq nr l ns nt">python3 -m retro.import /path/to/your/ROMs/directory/</span></pre><p id="81a2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">注意</strong>:要获得可用环境的完整列表，运行<code class="fe ni nj nk nl b">retro.data.list_games()</code>。</p><h2 id="49db" class="mx mb it bd mc my mz dn mg na nb dp mk lm nc nd mm lq ne nf mo lu ng nh mq iz bi translated"><strong class="ak">宝洁公司</strong></h2><p id="332a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">强化学习的一个典型问题是，产生的算法通常在特定环境下工作得很好，但无法学习任何通用的技能。例如，如果我们要改变一个游戏的外观或者敌人的反应呢？</p><p id="9593" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了解决这个问题，OpenAI 开发了一个名为 Procgen 的包，它允许创建程序化生成的环境。我们可以用这个包来衡量强化学习代理学习一般化技能的速度。</p><p id="f744" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">渲染游戏很简单:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7d3380229fc6fe88f8648969bcba570e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*fpMnWkcjIY6obxwOIs3s2A.gif"/></div></figure><p id="f867" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这将生成一个可以训练算法的单一级别。有几个选项可用来有步骤地生成同一环境的许多不同版本:</p><ul class=""><li id="8c8c" class="nz oa it lf b lg lh lj lk lm ob lq oc lu od ly oe of og oh bi translated"><code class="fe ni nj nk nl b">num_levels</code> -可以生成的独特级别的数量</li><li id="e8f3" class="nz oa it lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><code class="fe ni nj nk nl b">distribution_mode</code> -使用哪种级别，选项为<code class="fe ni nj nk nl b">"easy", "hard", "extreme", "memory", "exploration"</code>。所有游戏都支持<code class="fe ni nj nk nl b">"easy"</code>和<code class="fe ni nj nk nl b">"hard"</code>，而其他选项则是游戏特有的。</li></ul><h1 id="052c" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">强化学习</h1><p id="bd95" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">现在，终于到了实际强化学习的时候了。虽然有许多可用的软件包可以用来训练算法，但由于它们的可靠实现，我将主要进入稳定的基线。</p><p id="b109" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意，我不会在这篇文章中解释 RL 算法实际上是如何工作的，因为这需要一篇全新的文章。有关 PPO、SAC 和 TD3 等最新算法的概述，请参见<a class="ae lz" href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="noopener ugc nofollow" target="_blank">本</a>或<a class="ae lz" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">本</a>。</p><h2 id="692a" class="mx mb it bd mc my mz dn mg na nb dp mk lm nc nd mm lq ne nf mo lu ng nh mq iz bi translated">稳定基线</h2><p id="c98a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><a class="ae lz" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">稳定基线</a> (SB)基于 OpenAI <a class="ae lz" href="https://github.com/openai/baselines/" rel="noopener ugc nofollow" target="_blank">基线</a>，旨在使研究社区和行业更容易复制、提炼和识别新想法。他们在基线的基础上进行改进，以制作一个更加稳定和简单的工具，允许初学者在不被实现细节所困扰的情况下尝试强化学习。</p><p id="17fd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于 SB 能够简单快速地应用最新的强化学习算法，因此经常被使用。此外，创建和训练 RL 模型只需要几行代码。</p><p id="1323" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">安装可以简单地通过<code class="fe ni nj nk nl b">pip install stable-baselines</code>完成。然后，为了创建和学习一个 RL 模型，例如，<a class="ae lz" href="https://openai.com/blog/openai-baselines-ppo/" rel="noopener ugc nofollow" target="_blank"> PPO2 </a>，我们运行下面几行代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="7aee" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有几件事可能需要一些解释:</p><ul class=""><li id="ffe7" class="nz oa it lf b lg lh lj lk lm ob lq oc lu od ly oe of og oh bi translated"><code class="fe ni nj nk nl b">total_timesteps</code> -用于训练的样本总数</li><li id="b81e" class="nz oa it lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><code class="fe ni nj nk nl b">MlpPolicy</code> -实现 actor-critical 的策略对象。在这种情况下，一个多层感知器有 2 层 64。还有针对视觉信息的策略，例如<code class="fe ni nj nk nl b">CnnPolicy</code>甚至<code class="fe ni nj nk nl b">CnnLstmPolicy</code></li></ul><p id="b30b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了将这个模型应用到 CartPole 示例中，我们需要将我们的环境包装在一个假人中，以使它对某人可用。在 CartPole 环境中训练 PPO2 的完整示例如下:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3ea8ea7db342949c310865d3ed2f210c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*d33vp4jIgeH3A0hbwo21PA.gif"/></div></figure><p id="7d07" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如我们在上面的图片中看到的，仅用了 50，000 步，PPO2 就设法找到了保持杆子稳定的方法。这只需要几行代码和几分钟的处理时间！</p><p id="cff2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果要将其应用于 Procgen 或 Retro，请确保选择允许基于卷积的网络的策略，因为观察空间可能是环境当前状态的图像。</p><p id="e673" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，横翻筋斗的例子非常简单，只需 50，000 步就可以完成。大多数其他环境通常需要数千万个步骤才能显示出显著的改进。</p><p id="c0ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">注意</strong>:稳定基线的作者警告初学者，在产品中使用这个包之前，要很好地理解强化学习。强化学习有许多至关重要的组成部分，如果其中任何一个出错，算法就会失败，并可能留下很少的解释。</p><h2 id="e6a8" class="mx mb it bd mc my mz dn mg na nb dp mk lm nc nd mm lq ne nf mo lu ng nh mq iz bi translated">其他包</h2><p id="c19f" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">还有其他几个常用于应用 RL 算法的软件包:</p><ul class=""><li id="80ff" class="nz oa it lf b lg lh lj lk lm ob lq oc lu od ly oe of og oh bi translated"><code class="fe ni nj nk nl b"><a class="ae lz" href="https://github.com/tensorflow/agents" rel="noopener ugc nofollow" target="_blank">TF-Agents</a></code> -比稳定基线需要更多的编码，但通常是强化学习研究的首选包。</li><li id="d876" class="nz oa it lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated">用 Pytorch 用最少的代码实现了最先进的 RL 算法。这无疑有助于理解算法。</li><li id="584a" class="nz oa it lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><code class="fe ni nj nk nl b"><a class="ae lz" href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" rel="noopener ugc nofollow" target="_blank">DeepRL</a></code> -另一个 Pytorch 实现，但是这个版本还实现了额外的环境来使用。</li><li id="45e3" class="nz oa it lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><code class="fe ni nj nk nl b"><a class="ae lz" href="https://github.com/Unity-Technologies/ml-agents" rel="noopener ugc nofollow" target="_blank">MlAgents</a></code> -一个开源的 Unity 插件，支持游戏和模拟作为训练代理的环境。</li></ul><h1 id="2b45" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">结论</h1><p id="f8b2" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">强化学习可能是一个棘手的问题，因为如果代码中出现问题，很难调试。希望这篇文章能帮助你开始强化学习。</p><p id="acf5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所有代码都可以在以下位置找到:</p><div class="on oo gp gr op oq"><a href="https://github.com/MaartenGr/ReinforcementLearning" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jd gy z fp ov fr fs ow fu fw jc bi translated">MaartenGr/强化学习</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">使用 Stable-baselines、Gym、Retro 和 Procgen 在动态环境中创建最先进的强化学习算法。</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe kx oq"/></div></div></a></div></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><p id="aaa3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果你和我一样，对人工智能、数据科学或心理学充满热情，请随时在<a class="ae lz" href="https://www.linkedin.com/in/mgrootendorst/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上添加我。</p></div></div>    
</body>
</html>