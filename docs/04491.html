<html>
<head>
<title>Day 112 of #NLP365: NLP Papers Summary — A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第112天:NLP论文摘要——基于方面的情感分析的挑战数据集和有效模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5?source=collection_archive---------60-----------------------#2020-04-21">https://towardsdatascience.com/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5?source=collection_archive---------60-----------------------#2020-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="34a0" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="c165" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="ce48" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的257天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="3246" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz">基于方面的情感分析的挑战数据集和有效模型</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="d013" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="9c12" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">引入了一种新的基于方面的情感分析(ABSA)数据集，称为多方面多情感(MAMS)，其中每个句子包含至少两个不同的方面和两个不同的情感。提出的MAMS数据集可以解决现有ABSA数据集的共同问题，其中大多数句子包含不同方面的相同情感，从而将ABSA退化为句子级情感分析。论文还为数据集提出了一个简单的基线模型CapsNet-BERT。</p><h1 id="c572" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">数据集构建</h1><p id="c5f7" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">MAMS的数据集构建分为三个步骤:</p><ol class=""><li id="d842" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated">数据收集</li><li id="0305" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">数据注释</li><li id="7135" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">数据集分析</li></ol><h2 id="0ec7" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">数据收集</h2><p id="4e08" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">对Citysearch New York数据集进行类似于SemEval-2014数据集的注记。删除任何超过70个单词的句子。</p><h2 id="3061" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">数据注释</h2><p id="22a7" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">创建了两个版本的MAMS数据集来处理基于方面的情感分析的两个领域:方面-术语情感分析(ATSA)和方面-类别情感分析(ACSA)。对于ATSA，我们提取句子中的方面术语，并将它们与适当的情感进行映射，并删除具有相同情感的一个方面或多个方面的任何句子。数据集还包括每个特征项的开始和结束位置。对于ACSA，我们预定义了八个方面类别:食物、服务、员工、价格、氛围、菜单、地点和其他。每个句子都被映射到一个方面类别，以及对该方面类别的适当情感。该数据集仅包括具有至少两个不同情感的独特体类别的句子。</p><h2 id="fb7d" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">数据集分析</h2><p id="cf72" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">ATSA包含13854个句子，平均2.62个体项。ACSA有8879个句子，平均2.25个体范畴。请注意，MAMS中的所有句子都包含不同情绪的多个方面。现有的ABSA数据集(SemEval-2014和Twitter)包含的多方面多情感句子不超过30%，有些甚至不到1%。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ab2194193e58db1233fd8e28590afde5.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*oH7cBg5vbMSMTbk3.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">MAMS数据集的描述性统计[1]</p></figure><h1 id="ad3b" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">CapsNet-BERT</h1><p id="c010" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">给定一个句子和一个方面术语或一个方面类别，我们希望该模型预测该句子关于方面的情感。提议的模型是CapsNet-BERT，它由4层组成:</p><ol class=""><li id="9213" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated">嵌入层</li><li id="45fd" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">编码层</li><li id="3e75" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">初级被膜层</li><li id="bc37" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">类别胶囊层</li></ol><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi on"><img src="../Images/81d8e5879681f9600d8959537759fa22.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/0*QTAzBkL3QrwOicoD.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">CapsNet-BERT [1]</p></figure><h2 id="ea72" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">嵌入层</h2><p id="6333" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">在这一层，我们将输入的句子和体转换成单词嵌入。对于方面项嵌入，我们将其计算为方面词嵌入的平均值。对于方面类别嵌入，我们随机初始化嵌入并在训练中学习。嵌入层的输出是方面感知的句子嵌入，其中我们将方面嵌入与句子中的每个单词嵌入连接起来。</p><h2 id="8085" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">编码层</h2><p id="3675" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们采用体貌感知句子嵌入，并通过剩余连接将其输入到双向GRU中，以获得上下文化的表示。</p><h2 id="1ed4" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">初级被膜层</h2><p id="b617" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">使用线性变换和挤压激活，我们使用上下文化表示得到主胶囊P，使用来自嵌入层的方面嵌入得到方面胶囊。这一层还有两种机制:</p><ol class=""><li id="4969" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz">方面感知标准化</em>。这是为了应对句子长度的变化导致训练不稳定的事实，因此我们使用方面胶囊来归一化主胶囊权重，以选择重要的主胶囊。</li><li id="e973" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">胶囊引导路由</em>。这利用了情感类别的先验知识来改进路由过程。在训练期间，情感矩阵被初始化，并被输入到squash激活中以获得情感胶囊。然后通过测量主胶囊和情感胶囊之间的相似性来计算路由权重。</li></ol><h2 id="5ea5" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">类别胶囊层</h2><p id="1d96" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">使用初级胶囊、方面感知的标准化权重和胶囊引导的路由权重，我们可以计算最终类别胶囊。注意，对于CapsNet-BERT，嵌入和编码层被替换为预训练的BERT。</p><h1 id="ab4d" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验和结果</h1><p id="c1f5" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有三个评估数据集:ATSA、ACSA和SemEval-2014餐厅评论。</p><h2 id="6f3d" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">模型比较</h2><p id="36ba" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">模型分为4类:</p><ol class=""><li id="a900" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated">总部设在LSTM</li><li id="8925" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">基于CNN的</li><li id="bbbd" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">基于注意力</li><li id="1a14" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">消融研究，以比较CapsNet和BERT组合的有效性以及所提出的机制的效果</li></ol><h2 id="180c" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">结果</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/760af653c93227ed80f3c85fcd54df15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*ij4jemo1__RYJiYT.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">ATSA和ACSA子任务的实验结果[1]</p></figure><ul class=""><li id="db62" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly ot np nq nr bi translated">如上所述，句子级情感分类器(TextCNN和LSTM)在SemEval-2014中表现有竞争力，但在MAMS数据集上表现不佳</li><li id="5fcf" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated">SemEval-2014上的SOTA ABSA方法在MAMS数据集上表现不佳或一般，表明MAMS数据集的高难度水平</li><li id="92e6" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated">没有正确模拟单词序列的基于注意力的模型在MAMS中表现很差，因为它们丢失了句子的序列信息，因此无法将上下文与方面联系起来</li><li id="076b" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated">CapsNet在6个数据集的4个数据集上表现优于BERT，显示了CapsNet的实力。CapsNet-BERT的组合在所有数据集上都优于所有模型</li><li id="1372" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated">CapsNet-DR和CapsNet-BERT-DR用于测量胶囊导向路径的有效性。我们使用标准化的动态路由(DR ),这降低了模型的性能，性能不如我们的CapsNet-BERT</li></ul><h2 id="9452" class="nx mi jf bd mj ny nz dn mn oa ob dp mr lm oc od mt lq oe of mv lu og oh mx jl bi translated">来源:</h2><p id="af00" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]蒋，q，陈，l，徐，r，敖，x，杨，m，2019，11月。基于方面的情感分析的挑战数据集和有效模型。在<em class="lz">2019自然语言处理经验方法会议和第九届自然语言处理国际联合会议(EMNLP-IJCNLP) </em>(第6281–6286页)的会议录中。</p><p id="bfd0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年4月21日</em><a class="ae jc" href="https://ryanong.co.uk/2020/04/21/day-112-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="7fed" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="26af" class="nj nk jf lf b lg mz lj na lm ou lq ov lu ow ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="f816" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="c563" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitivity-analysis-f 24 a2 EC 1 ca 32</a></li><li id="8a0f" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="f736" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="027c" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li></ul><h1 id="d586" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="e491" class="nj nk jf lf b lg mz lj na lm ou lq ov lu ow ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="47c3" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li></ul><h1 id="c2ec" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="bec6" class="nj nk jf lf b lg mz lj na lm ou lq ov lu ow ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="b13c" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly ot np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li></ul></div></div>    
</body>
</html>