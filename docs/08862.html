<html>
<head>
<title>Sparse and Variational Gaussian Process (SVGP) — What To Do When Data is Large</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稀疏变分高斯过程(SVGP) —当数据很大时该怎么办</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7?source=collection_archive---------3-----------------------#2020-06-26">https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7?source=collection_archive---------3-----------------------#2020-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a1cba2c65ee6891529abaccbcd6207fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeF2ZwGyJ1suNyvf-XSAXQ.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自<a class="ae kf" href="https://pixabay.com/photos/rabbit-cartoon-beach-sand-drawing-1664927/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="a16a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大数据是解决许多机器学习问题的良药。但是一个人的解药可能是另一个人的毒药。大数据导致许多贝叶斯方法不切实际地昂贵。我们需要做些什么，否则贝叶斯方法就会被大数据革命抛在后面。</p><p id="84f9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将解释为什么大数据使一种非常流行的贝叶斯机器学习方法-高斯过程-变得昂贵。然后我将介绍贝叶斯的解决方案——稀疏和变分高斯过程模型(SVGP 模型)，它将高斯过程带回了游戏中。</p><h1 id="c61d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">一些符号</h1><p id="43a3" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">中支持文本中的 Unicode。这允许我写很多数学下标符号，比如<em class="mh"> X₁ </em>和<em class="mh"> Xₙ.但是我写不出其他的下标。例如:</em></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/08799b6b0c7b165d5c935e31f9900443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*InBVSoSMP4cBU11V.png"/></div></div></figure><p id="711c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以在正文中，我会用一个下划线“_”来引出这样的下标，比如<em class="mh"> X_* </em>和<em class="mh"> X_*1 </em>。</p><p id="d73d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果一些数学符号在你的手机上显示为问号，请尝试从电脑上阅读这篇文章。这是某些 Unicode 呈现的已知问题。</p><h1 id="ef72" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">高斯过程回归模型</h1><p id="82bb" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">假设我们有一些训练数据<em class="mh"> (X，Y)。</em>X<em class="mh">和 Y<em class="mh">都是长度为<em class="mh"> n. </em>的浮点向量，所以<em class="mh"> n </em>是训练数据点的数量。我们想找到一个从<em class="mh"> X </em>到<em class="mh"> Y </em>的回归函数。这是一个典型的回归任务。我们可以使用高斯过程回归模型(GPR)来找到这样一个函数。</em></em></p><p id="487e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">高斯过程回归模型是最简单的高斯过程模型。如果你需要刷新你的记忆，文章<a class="ae kf" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">理解高斯过程，苏格拉底方式</a>是一个伟大的阅读。该模型有两部分——先验和似然:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/2df384dc5c4819c07373c4ab3eca709e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p2fkvij1RVKRrO8HNRWasA.png"/></div></div></figure><p id="05ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">GP 先验<br/> </strong>高斯过程先验(GP 先验)是随机变量向量<em class="mh"> f(X) </em>和<em class="mh"> f(X_*) </em>上的多元高斯分布:</p><ul class=""><li id="b909" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="mh"> f(X) </em>是长度为<em class="mh"> n 的随机变量向量；</em>它代表在训练位置<em class="mh"> X </em>的潜在回归函数的可能值。</li><li id="e182" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="mh"> f(X_*) </em>也是一个随机变量向量。它代表在测试位置<em class="mh"> X_* </em>的潜在回归函数的可能值。它的长度<em class="mh"> n_* </em>取决于您将要求模型对多少个测试位置进行预测。<em class="mh"> n_* </em>可以很大。</li></ul><p id="b1be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将把先验缩短为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/05f84d2726fea50b612a5b15e6c2fa0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzXNcxzGvBpWvlmBmf2wRQ.png"/></div></div></figure><p id="1419" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个多元高斯分布具有零均值，它使用一个核函数<em class="mh"> k </em>来定义它的协方差矩阵。协方差矩阵<em class="mh"> k(X，X) </em>是一个<em class="mh"> n×n </em>矩阵。对于函数<em class="mh"> k </em>，你有很多选择，在本文中，我们使用平方指数函数来表示<em class="mh"> k </em>:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/109ff84fe331aa38717cba7cec759c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXv8iJBjKRws9-Xg6F9j7A.png"/></div></div></figure><p id="a085" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中信号方差<em class="mh"> σ </em>和长度标度<em class="mh"> l </em>为模型参数<em class="mh">。</em></p><p id="f7ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">似然<br/> </strong>在似然中，<em class="mh"> y(X) </em>是一个长度为<em class="mh"> n </em>的随机变量向量。它来自多元高斯分布，具有均值<em class="mh"> f(X) </em>和协方差<em class="mh"> η Iₙ，</em>其中<em class="mh"> η </em>是称为噪声方差的标量模型参数，<em class="mh"> Iₙ </em>是一个<em class="mh"> n×n </em>单位矩阵，因为我们假设了独立的观测噪声。</p><p id="19ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练数据<em class="mh"> Y </em>被建模为随机变量向量<em class="mh"> y(X) </em>的样本。由于<em class="mh"> y(X) </em>依赖于<em class="mh"> f(X) </em>，我们也将可能性表示为<em class="mh"> p(y(X)|f(X)) </em>。我将用<em class="mh"> y </em>作为<em class="mh"> y(X) </em>的简写，用<em class="mh"> p(y|f) </em>作为可能性的简写。</p><p id="075f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<em class="mh"> f </em>和<em class="mh"> y </em>都是多元高斯随机变量，而<em class="mh"> f </em>是<em class="mh"> y </em>的均值，我们可以将<em class="mh"> y </em>改写为<em class="mh"> f: </em>的线性变换</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/f59b1c6bb1fbb5f771b753c13d5c2942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdNWyswWpcx6kRLaBORqIQ.png"/></div></div></figure><p id="b5ae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以应用多元高斯线性变换规则推导出<em class="mh"> y </em>的分布，而不用提及随机变量<em class="mh"> f </em>:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/b012502767e4d210325ddbd6d348b195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nv62UwYEbI3hNB7P16emg.png"/></div></div></figure><p id="6b35" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以写下<em class="mh"> y </em>的概率密度函数:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/2895b4e5b5030e1477377439d388163d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTge4bw7YkTb6LjgEZxPgQ.png"/></div></div></figure><p id="c709" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> p(y) </em>就是著名的边际似然。</p><p id="c3a0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">参数学习<br/> </strong>我们的高斯过程回归模型有三个模型参数，lengthscale <em class="mh"> l </em>，信号方差<em class="mh"> σ，</em>和观测方差<em class="mh"> η。我们在建模过程中引入了它们，但是我们不知道应该将这些模型参数设置为什么具体值。我们使用参数学习来找到它们的最佳具体值:</em></p><ul class=""><li id="2d98" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">“最优”是指用那些具体的值，我们的模型可以最好地解释训练数据<em class="mh"> (X，Y) </em>。而且，</li><li id="051d" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">“解释训练数据”，在概率建模(这是我们正在做的)的上下文中，意味着训练数据由我们的模型生成的可能性有多大，通过边际可能性<em class="mh"> p(y) </em>来衡量。</li></ul><p id="e212" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦训练数据<em class="mh"> X </em>和<em class="mh"> Y </em>被插入，边际可能性<em class="mh"> p(y) </em>是具有所有模型参数<em class="mh"> l、【σ】、</em>和<em class="mh"> η </em>的函数。为了找到它们的最佳值，参数学习使用梯度下降来最大化关于那些模型参数的目标函数<em class="mh"> log(p(y)) </em>。目标函数的公式如下:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/ab432e2ab0657fa8ddddd054426be603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6BJHxnRFyxnD_BcsFiOGA.png"/></div></div></figure><p id="a8e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">进行预测<br/> </strong>找到模型参数的最优值(也称为校准模型)后，我们就可以进行预测了。高斯过程模型使用后验分布进行预测。</p><p id="f9d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的具有高斯似然的高斯过程回归模型中，后验<em class="mh"> p(f_*，f|y) </em>具有封闭形式的解析表达式。为了在测试位置<em class="mh"> X_* </em>对<em class="mh"> f_* </em>进行预测，我们将<em class="mh"> f </em>从后面边缘化:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/11bbad8bda8bc2cc77e5b6ad64593931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ra-w0ZxHEm7D5rQ_kMrF8Q.png"/></div></div></figure><p id="40f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在行(3)处，被积函数包括两个概率密度— <em class="mh"> p(f_*|f) </em>和<em class="mh"> p(f|y) </em>。我们可以通过应用多元高斯条件规则从 GP 先验中导出<em class="mh"> p(f_*|f) </em>。贝叶斯法则告诉我们如何计算 p(f|y)。准备好它的成分后，我们可以推导出预测分布的解析表达式<em class="mh"> p(f_*|y) </em>:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/219befc14613e7f71d974586ef0842a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzDFBlNjQ_pCJBs6TwkkDw.png"/></div></div></figure><p id="c4e3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面表达式中矩阵的形状用红色小符号表示。</p><p id="d946" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请到<a class="ae kf" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">理解高斯过程，苏格拉底方式</a>(<em class="mh">计算后验</em>部分)获取<em class="mh"> p(f_*|y) </em>的完整推导。</p><p id="4a9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预测分布<em class="mh"> p(f_*|y) </em>是随机变量向量<em class="mh"> f_* </em>上的分布，其长度为<em class="mh"> n_* </em>。其结构揭示了在高斯过程回归模型中，<em class="mh"> f_* </em>是<em class="mh"> y </em>的线性变换。该模型试图使用来自<em class="mh"> y. </em>的信息来解释<em class="mh"> f_* </em>内核(函数<em class="mh"> k </em>，训练位置<em class="mh"> X，</em>和测试位置<em class="mh"> X_* </em>)主要决定从<em class="mh"> y </em>到<em class="mh"> f_* </em>的转换，当然还有来自观测噪声<em class="mh">ηiₙ</em>的帮助</p><p id="a853" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，测试位置<em class="mh"> n_* </em>的数量可以远大于训练点<em class="mh"> n </em>的数量。随着<em class="mh"> n_* </em>变大，矩阵<em class="mh"> K_** </em>，<em class="mh"> K_*X </em>变大，但是正在求逆的矩阵<em class="mh"> K+η Iₙ </em>，<em class="mh"> </em>保持不变，<em class="mh"> n×n </em>。</p><h1 id="8615" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">矩阵求逆的诅咒</h1><p id="3652" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在这个高斯过程回归模型中，参数学习的目标函数和预测分布都提到了矩阵求逆<em class="mh"> (K+η Iₙ)⁻ </em>)。矩阵求逆计算量很大。一个<em class="mh"> n×n </em>矩阵求逆尺度中运算(如两个数相加、相乘)的次数，其中<em class="mh"> n，</em>中<em class="mh"> n </em>为数据点数。</p><p id="0862" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有 10，000 个数据点的数据集的高斯过程回归模型需要 10 次运算来对其协方差矩阵求逆。作为比较，我们的宇宙包含 10⁷⁸到 10⁸原子。一个包含 10，000 个点的数据集在现代标准下是非常小的。</p><p id="8a21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> (K+σ Iₙ)⁻ </em>是将我们的高斯过程回归模型应用于即使是中等规模的数据集也过于昂贵的原因。</p><p id="340d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，下面是一个包含 10，000 个点的数据集:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/4c996ed9d0eb58e7c303e9083a05964d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOxsJ3dgCPItZFAOLm0-CQ.png"/></div></div></figure><p id="71c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那些小红叉是数据点。当我试图使用该数据集来执行高斯过程回归模型的参数学习时，我看到了以下警告消息，我等待了几个小时，然后我终止了该程序，因为我不想再等待它了——这是一个机器学习模型，不是我的孩子，我对模型没有无限的耐心:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nk"><img src="../Images/d72af5f112639cc807a0e19a6a42174c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMkKJ2O1FzrNIUyzf7Y6hg.png"/></div></div></figure><p id="3493" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此警告消息表明模型需要分配大量内存来执行矩阵运算。而漫长的等待让你对<em class="mh"> n </em>缩放<em class="mh">有了生动的印象。</em></p><h1 id="4f98" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">(K+ <em class="nl"> σ Iₙ </em> )⁻为什么会出现？</h1><p id="6e40" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">矩阵求逆<em class="mh"> (K+σ Iₙ)⁻ </em>出现在目标函数<em class="mh"> log p(y) </em>和预测分布中，因为我们使用了来自 g p 先验的以下因子分解:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/5b441c75e9b54a46bf1d6a50d7350446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VM-HYYW5hIWMhzKKWDZ2lQ.png"/></div></div></figure><ul class=""><li id="be4e" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">我们使用条件<em class="mh"> p(f_*|f) </em>部分进行预测，因为这部分将我们在训练地点知道的(即<em class="mh"> f </em>)和我们在测试地点不知道的(即<em class="mh"> f_* </em>)联系在一起。<em class="mh"> p(f_*|f) </em>见前面推导。</li><li id="523e" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">我们使用边际<em class="mh"> p(f) </em>部分，结合似然<em class="mh">p(y | f)</em>进行参数学习，因为该部分将观测值可用的随机变量(即<em class="mh"> y </em>)和观测值不可用的随机变量(即<em class="mh"> f </em>)联系在一起。<em class="mh"> p(y) </em>见前面的推导。</li></ul><p id="40b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">概率论允许我们进行上述因式分解，只要:</p><ol class=""><li id="30aa" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated"><em class="mh"> p(f_*|f) </em>是一个既提到<em class="mh"> f_* </em>又提到<em class="mh"> f </em>的函数；<em class="mh"> p(f) </em>是提到<em class="mh"> f </em>的函数。这确保了因式分解与关节<em class="mh"> p(f_*，f) </em>具有相同的 API。</li><li id="7c8f" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated"><em class="mh"> p(f_*|f) </em>和<em class="mh"> p(f) </em>都是合适的概率密度函数。这确保了因式分解仍然是有效的概率密度函数。</li></ol><p id="aadb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择多元高斯分布作为先验满足了这两个要求，并且使得<em class="mh"> p(f_*|f) </em>和<em class="mh"> p(f) </em>的推导变得容易:</p><ol class=""><li id="878e" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">通过对先验应用多元高斯条件规则来导出<em class="mh"> p(f_*|f) </em>。</li><li id="b4d4" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">通过对先验应用多元高斯边际化规则来导出<em class="mh"> p(f) </em>。</li></ol><p id="0366" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这种因式分解埋下了无法承受的矩阵求逆，因为边际<em class="mh"> p(f) </em>是多元高斯分布:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/436b83de27fb0329bad66d02a6f13ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*scMfhP9DRgs_jb7nAo5Pog.png"/></div></div></figure><p id="95bf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而<em class="mh"> K⁻ </em>就在这个概率密度函数里。边际似然<em class="mh"> p(y) </em>继承了这种反演，做出了自己的贡献<em class="mh"> σ Iₙ </em>，产生了那个负担不起的项<em class="mh"> (K+σ Iₙ)⁻。</em></p><h1 id="41ec" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">我们可以降采样吗？</h1><p id="f6ec" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">由于数据的大小<em class="mh"> n </em>控制着我们很难求逆的协变矩阵<em class="mh"> K </em>的形状，我们可以选择训练数据的子集吗？这称为下采样。我们可以，但是机器学习实践者不喜欢下采样，因为:</p><ul class=""><li id="38a6" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">它丢弃了部分训练数据。训练数据是机器学习任务中最有价值的东西。扔掉一部分是一种罪过。</li><li id="612d" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">很难决定丢弃训练数据的哪一部分。我们是等距降采样吗？</li></ul><h1 id="bd2d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">可以总结一下训练数据吗？</h1><p id="d85f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">既然我们不想对训练数据进行下采样，但是处理训练数据对我们来说代价太大，那么我们可以对训练数据进行汇总吗？</p><p id="d7fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们引入一组新的随机变量<em class="mh">f(xₛ</em>，在某些地方简称为<em class="mh">fₛ</em>xₛ.<em class="mh"/> <em class="mh"> Xₛ </em>是一个长度为<em class="mh"> nₛ.的标量向量</em>下标“<em class="mh">ₛ”</em>代表“稀疏”<em class="mh">。我们不知道那些标量的值，它们是新的模型参数。但是我们确实要求<em class="mh"> nₛ </em>比<em class="mh"> n </em>小得多——这就是总结的要点。并且我们需要为<em class="mh"> nₛ </em>选择数值，它不是模型参数<em class="mh">。</em></em></p><p id="026c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们称<em class="mh"> Xₛ诱导位置</em>，称<em class="mh"> fₛ诱导随机变量</em>。不要问为什么。众所周知，机器学习者作为一个群体并不擅长起名。接受这些名字有点意义。</p><p id="7e41" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，在一个已经很昂贵的模型中引入更多的随机变量似乎并不明智，但请继续阅读，因为这被证明是总结所有训练数据的关键。</p><h1 id="00fd" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">稀疏变分高斯过程模型</h1><p id="8882" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们用<em class="mh"> fₛ </em>来总结训练数据是什么意思？不出所料，我们的意思是我们的模型应该以很高的概率生成训练数据。为了实现这一目标:</p><ol class=""><li id="409b" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">位置<em class="mh"> Xₛ、</em>处的<em class="mh"> fₛ </em>和训练位置<em class="mh"> X、</em>处的随机变量<em class="mh"> f </em>之间必然存在某种关系，换句话说，<em class="mh"> fₛ </em>和<em class="mh"> f </em>不是独立的。这是因为我们要用<em class="mh"> fₛ </em>来概括训练数据<em class="mh"> Y. </em>和<em class="mh"> Y </em>是由观察随机变量<em class="mh"> y </em>来建模的，它取决于潜在随机变量<em class="mh"> f </em>。如果<em class="mh"> fₛ </em>和<em class="mh"> f </em>独立，则<em class="mh"> fₛ </em>和<em class="mh"> Y </em>之间没有联系；因此，<em class="mh"> fₛ </em>无法总结<em class="mh">y。</em>我们的 SVGP 模型使用模型的先验来建立这种关系。</li><li id="c522" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">在给定模型的情况下，需要对训练数据的概率进行度量。模型的可能性定义了这个度量。</li></ol><h2 id="16fb" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">稀疏先验</h2><p id="c1f2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们使用多元高斯分布来建立<em class="mh"> fₛ </em>和<em class="mh"> f </em>之间的关系作为我们的新先验，我们称之为稀疏先验，因为它包括了稀疏诱导随机变量<em class="mh"> fₛ </em>:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/86b8e80bbb8e5c465604c8519e9202b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVZcQMNJKHmV6cAGCd_pzQ.png"/></div></div></figure><p id="c6aa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将把它简化为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/2437c1c110358a2179a670847b6c6eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXPJ1bDuhFqYf6mzJHYYjg.png"/></div></div></figure><p id="99b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以在先前的密度函数定义中清楚地看到，随机变量<em class="mh"> fₛ </em>和 f 是相关的——因为协方差矩阵<em class="mh"> Kₓₛ </em>及其转置<em class="mh"> Kₓₛᵀ </em>中的非对角元素不为 0。</p><p id="bdd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请将上述公式与高斯过程回归模型中的旧先验公式进行比较，此处再次显示:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/fbe8b9a0c989401fef74abad50f2c18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lgXQ-MVcmTNcgAY1OBHQHQ.png"/></div></div></figure><p id="658d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过比较这两个分布，我们发现它们具有相同的结构。这给了我们一些关于诱导变量<em class="mh"> fₛ: </em>背后的直觉的提示</p><ul class=""><li id="ea98" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">在之前的 SVGP 中，我们用诱导位置<em class="mh">fₛ</em>xₛ来解释训练位置<em class="mh"> X </em>的<em class="mh"> f </em>。协方差矩阵<em class="mh"> Kₓₛ </em>定义了<em class="mh"> f </em>和<em class="mh"> fₛ.之间的相关性</em>和内核函数<em class="mh"> k </em>定义了<em class="mh"> Kₓₛ.中的每个条目</em></li><li id="ec4e" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">在 GPR 之前，我们使用训练位置<em class="mh"> X </em>处的<em class="mh"> f </em>来解释测试位置<em class="mh"> X_* </em>处的<em class="mh"> f_* </em>。协方差<em class="mh"> K_*X </em>定义了<em class="mh"> f_* </em>和<em class="mh"> f </em>之间的相关性。同样，同一个内核函数<em class="mh"> k </em>定义了<em class="mh"> K_*X. </em>的每个条目</li></ul><p id="6116" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SVGP 先验和 GPR 先验都使用多变量高斯条件规则作为从一个随机变量向量解释另一个随机变量向量的机制:</p><ul class=""><li id="845e" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">SVGP 用<em class="mh"> p(f|fₛ) </em>从<em class="mh"> fₛ </em>的信息中解释<em class="mh"> f </em>。</li><li id="ede1" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">GPR 用<em class="mh"> p(f_*|f) </em>从<em class="mh"> f </em>的信息中解释<em class="mh"> f_* </em>。</li></ul><h2 id="05fe" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">诱导变量背后的直觉</h2><p id="688b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">认识到 SVGP 和 GPR 先验之间的协同作用给了我们“使用诱导变量来总结训练数据”的定义。<em class="mh">总结</em>这个词的意思是“用简短的形式表达关于某事最重要的事实”。回到我们之前的 SVGP:</p><ul class=""><li id="75cd" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">我们用<em class="mh"> fₛ </em>通过条件概率密度函数<em class="mh">p(f|fₛ</em>来表示<em class="mh"> f </em>。</li><li id="104f" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">我们要求诱导变量<em class="mh"> nₛ </em>的数量小于(并且通常要小得多)训练数据点<em class="mh"> n </em>的数量。这就是诱导变量<em class="mh">汇总</em>训练数据的原因。这也是我们称 SVGP 模型<em class="mh">稀疏</em>的原因——我们想在关键诱导位置使用少量诱导变量来解释训练位置的大量随机变量。</li></ul><p id="fe3e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">诱导变量或位置数<em class="mh"> nₛ </em>不是模型参数<em class="mh">。</em>我们需要决定它的值<em class="mh">。</em>在我们决定了<em class="mh"> nₛ的值之后，</em>我们将有一个长度为<em class="mh"> nₛ </em>的矢量<em class="mh"> Xₛ </em>，代表那些诱导变量的位置。我们不知道那些位置在哪里，它们是模型参数，并且我们将使用参数学习来为那些诱导位置以及其他模型参数找到具体值。</p><p id="48a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能想知道，为什么我们假设我们可以为诱导位置<em class="mh"> nₛ </em>的数量提供一个好的值，但是把计算这些位置的工作留给优化器？因为对我们来说，为实际位置得出一个单一的整数比得出<em class="mh"> nₛ </em>浮点数(或浮点向量，如果<em class="mh"> X </em>是多维的)更容易。我们在使用聚类算法时做了同样的事情—我们假设我们很清楚数据中有多少个聚类，并让聚类算法计算出每个聚类的中心位置。</p><h2 id="8350" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">SVGP 先验应该超过 f，f <em class="nl"> ₛ和 f_*？</em></h2><p id="d7f2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在上面，我写的稀疏先验是在<em class="mh"> f </em>和<em class="mh"> fₛ.之间的联合分布</em>你可能会问，这个先验不也应该包括测试位置<em class="mh"> X_* </em>的随机变量向量<em class="mh"> f_* </em>吗？是的，你是对的，完全稀疏 SVGP 先验的确是:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/1da77ad6b83fe64dbbd44254f759e7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOAWN1t9ORBuS9rdnQerFw.png"/></div></div></figure><p id="dcfa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是为了参数学习的目的，我们可以将多元高斯边际化规则应用于这个完整的，然后积分出<em class="mh"> f_* </em>，就像我们一直做的那样。只有我们做预测的时候，才会通过多元高斯条件规则把<em class="mh"> f_* </em>带回来。</p><h2 id="e4fa" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">同样的可能性</h2><p id="bb23" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">和以前一样，我们使用可能性来建立先验数据和训练数据之间的联系。我们继续使用相同的高斯似然性:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/f7bf43c47345f7089d18c58a66cdb529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBopPTyKzMX5yfxNWD-lLg.png"/></div></div></figure><h2 id="62a3" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">衡量我们的模型解释训练数据的能力</h2><p id="464b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们需要一个量来衡量诱导变量对训练数据的总结程度。边际可能性<em class="mh"> p(y) </em>是我们的度量。它报告了一个介于 0 和 1 之间的浮点数——给定我们的模型的数据的概率。我们可以在我们的 SVGP 模型中推导出<em class="mh"> p(y) </em>的公式如下:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/6104fdffbc54100012daa201ca02cb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9A3XpldTspV1mXMwarFjw.png"/></div></div></figure><p id="b828" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行给出了我们为什么选择边际可能性<em class="mh"> p(y) </em>作为我们的度量的理由。第(2)行显示<em class="mh"> p(y) </em>被定义为关于 SVGP 先验中的随机变量<em class="mh"> f </em>和<em class="mh"> fₛ </em>的期望。因此<em class="mh"> p(y) </em>是数据<em class="mh"> y </em>的平均似然，通过权重<em class="mh"> p(f，fₛ).】考虑了<em class="mh"> f </em>和<em class="mh"> fₛ </em>的所有可能值</em>我们说:<em class="mh"> f </em>和<em class="mh"> fₛ </em>，作为随机变量，<em class="mh"> </em>可以以不同的概率取不同的值。<em class="mh"> f </em>和<em class="mh"> fₛ </em>的每一个不同的值通过可能性<em class="mh"> p(y|f，fₛ).)产生不同的数据概率</em>因此，为了衡量我们的模型平均生成训练数据的能力，我们使用这些可能性概率的加权和，即<em class="mh"> p(y) </em>。</p><p id="a6dc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便说一下，推导边际似然公式很重要，不仅因为它是我们的模型如何解释训练数据的度量，而且它是我们需要计算后验概率 p(f，fₛ|y)的一个量:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/80998fc66f9e49f3c870742cc63eb2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_csYR0eqmor4KbAmk2Nyg.png"/></div></div></figure><p id="34df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分母中，我写了两个整数符号，一个是给<em class="mh"> f </em>的，一个是给<em class="mh"> fₛ </em>的。你可以看到第(2)行分母上的边际可能性 p(y) 。</p><h1 id="b61e" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">回到起点？</h1><p id="9a6f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">从我们的 SVGP 模型来看边际可能性<em class="mh"> p(y) </em>的最终公式:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/1b3f286a48fc5e70553be4b214bf97b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mJokirV9Fi1O38YnP41rxA.png"/></div></div></figure><p id="1b21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们意识到这与我们在高斯过程回归模型中的公式是一样的。随之而来的有两个问题:</p><ol class=""><li id="7094" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">该公式提到了<em class="mh"> n×n </em>协方差矩阵<em class="mh"> K. </em>，因此我们仍然有昂贵的矩阵求逆。</li><li id="5870" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">这个公式只提到了长度尺度<em class="mh"> l </em>，信号方差<em class="mh"> σ，</em>和观测噪声方差<em class="mh"> η。</em>没有提到诱导位置<em class="mh"> Xₛ </em>，也是模型参数<em class="mh">。</em>参数学习的有效目标函数必须包括所有模型参数。所以这个公式不是一个有效的目标函数。</li></ol><p id="6400" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，即使在可能性是高斯的情况下，我们也不能使用贝叶斯规则来计算后验概率，并且我们不能使用<em class="mh"> log p(y) </em>作为参数学习的目标函数。</p><p id="71ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">推导哪里出错了？当我们从联合分布<em class="mh"> p(f，fₛ).】中整合出<em class="mh"> fₛ </em>时，就是上面推导的第(5)行</em>这一步不仅导致边际分布<em class="mh"> p(f) </em>，这是我们想要避免的；它还删除了所有提及<em class="mh"> Xₛ、</em>kₛₛ.的内容</p><p id="b94a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于应用贝叶斯规则计算后验概率需要计算边际似然<em class="mh"> p(y) </em>并且计算<em class="mh"> p(y) </em>最终会出现上述问题，所以我们需要考虑另一种计算后验概率的方法，不需要计算<em class="mh"> p(y) </em>。</p><p id="3a5d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变分推理技术直接逼近后验概率 p(f，fₛ|y)t29，而不是使用贝叶斯规则。因此，它不需要计算边际可能性<em class="mh"> p(y) </em>。为了刷新您对高斯过程模型的变分推断的记忆，请进入<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">变分高斯过程——当事情不是高斯的时候该怎么办</a>。</p><p id="b601" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便说一下，变分推断在高斯过程之外的贝叶斯模型中被广泛使用。<a class="ae kf" rel="noopener" target="_blank" href="/demystifying-tensorflow-time-series-local-linear-trend-9bec0802b24a">揭开 Tensorflow 时间序列的神秘面纱:局部线性趋势</a>展示了 Google 的 Tensorflow 时间序列库如何使用它来计算时间序列模型的后验概率。并且<a class="ae kf" href="https://medium.com/towards-data-science/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756" rel="noopener">去噪扩散模型</a>用它生成人脸。</p><p id="da92" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试试变分推理。</p><h1 id="dfca" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">变分推理</h1><p id="448f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">后验概率 p(f，fₛ|y) 是随机变量向量<em class="mh"> f </em>和<em class="mh"> fₛ.的联合分布</em>变分推理使用一种新的分布<em class="mh"> q(f，fₛ)</em>称为变分分布，来逼近真实的后验概率<em class="mh"> p(f，fₛ|y).</em>分布<em class="mh"> q(f，fₛ) </em>在同一组随机变量上，我们要求它的行为与真实后验相似。那就是<em class="mh"> q(f，fₛ) </em>逼近后<em class="mh"> p(f，fₛ|y) </em>的意思。</p><p id="a486" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变分推理技术给了我们<em class="mh"> ELBO </em>公式。通过相对于模型参数最大化<em class="mh"> ELBO </em>，我们最终得到一个<em class="mh"> q(f，fₛ) </em>，它近似于真实的后验概率<em class="mh"> p(f，fₛ|y) </em>。</p><p id="3578" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先决定变分分布<em class="mh"> q(f，fₛ) </em>的结构，然后推导出<em class="mh"> ELBO </em>的公式。</p><p id="38d5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便说一下，由于我们正在使用变分分布<em class="mh"> q(f，fₛ) </em>来逼近后验<em class="mh"> p(f，fₛ|y) </em>，我们的模型可以处理高斯和非高斯似然性——这是将变分推理引入高斯过程世界的最初动机，参见<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">此处</a>。</p><h2 id="7c91" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated"><em class="nl"> q(f，fₛ)应该是什么样子的？</em></h2><p id="f126" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们提出变分分布<em class="mh"> q(f，fₛ) </em>，它是一个联合分布<em class="mh">，</em>由下面的因式分解<em class="mh"> : </em>定义</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/f7e483ec2c73299a5f5d6526a847de09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KGxB-8SEUWxw_GEctvuJrw.png"/></div></div></figure><p id="d483" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="mh"> p(f|fₛ) </em>是通过将多元高斯条件规则应用于稀疏先验<em class="mh"> p(f，fₛ) </em>而得到的，并且<em class="mh"> q(fₛ) </em>是多元高斯分布。</p><p id="6877" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我说“提议”是因为我们是建模者，我们有充分的自由来决定<em class="mh"> q(f，fₛ) </em>的公式应该是什么，只要它是一个提到随机变量<em class="mh"> f </em>和<em class="mh"> fₛ </em>的表达式，并且它是一个在<em class="mh"> f </em>和<em class="mh"> fₛ </em>的所有可能值上积分为 1 的适当的概率密度。我们建议将联合<em class="mh"> q(f，fₛ) </em>分解成两个分量<em class="mh"> p(f|fₛ) </em>和<em class="mh"> p(fₛ) </em>，并为它们选择特定的形式，如下所述，因为它们给我们数学上的便利。</p><p id="73a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="mh">【p(f|fₛ】</em>分布<br/> </strong>稀疏 GP 先验再次显示如下，我用红色符号标注了它的不同组成部分，为多元高斯条件规则应用做准备。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/1400f0d490202c56a667e8c59ca05e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOGfIMF_lyL6ewaPCr8-vw.png"/></div></div></figure><p id="992e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多元高斯条件规则是:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/7d6dad37da0434e5d1fad3d08a591ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OCs34XvF5X0GEQb.png"/></div></div></figure><p id="9c8f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在推导之前将此规则应用于稀疏 GP:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/d18c9e40111fdc749e798a3a0f360c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tceYHFOC4TpWdNmYO_oJ3Q.png"/></div></div></figure><p id="cf74" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/4b3a7f11fa5ac5e944fc747e00f0883f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ptfllo_jDBTqZBTj-74VDQ.png"/></div></div></figure><p id="541b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要注意的是，<em class="mh"> A </em>和<em class="mh"> B </em>只提到了<em class="mh"> Kₛₛ </em>的逆，一个更小的<em class="mh"> nₛ×nₛ </em>矩阵。他们不提<em class="mh"> K </em>的逆，是一个更大的<em class="mh"> n×n </em>矩阵<em class="mh">。</em>在<em class="mh"> A </em>和<em class="mh"> B </em>中，矩阵<em class="mh"> Kₓₛ </em>的大小为<em class="mh"> n×nₛ </em>可以大，因为<em class="mh"> n </em>可以大。但是我们不需要逆<em class="mh"> Kₓₛ </em>，所以它的大小不成问题。</p><p id="5284" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样选择<em class="mh"> p(f|fs) </em>的好处是使<em class="mh"> ELBO </em>的推导更简单。我会在我们完成<em class="mh"> ELBO </em>的推导时指出这些优点。</p><p id="5546" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="mh">q(fₛ)</em>分布<br/> </strong> <em class="mh"> q(fₛ) </em>是变分分布<em class="mh"> q(f，fₛ) </em>因式分解<em class="mh">里面的一个分量。</em>我们称<em class="mh"> q(fₛ) </em>边际变分分布，因为它只提到联合中两个随机变量向量之一<em class="mh"> q(f，fₛ).</em></p><p id="5631" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要决定<em class="mh"> q(fₛ) </em>是什么样子，不出所料，我们将<em class="mh"> q(fₛ) </em>定义为多元高斯分布:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/46847b6640a6edd9c1f277532e00245f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__fCV9KcGUEd2LNiyGFsgw.png"/></div></div></figure><p id="2896" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> μ </em>是均值向量；它的长度是<em class="mh"> nₛ </em>，诱导变量的个数。<em class="mh">σ</em>是协方差矩阵；它的尺寸是<em class="mh"> nₛ×nₛ.</em></p><h2 id="820f" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">为什么我们选择把 q(f，f <em class="nl"> ₛ)因式分解成</em> p(f|fₛ)q(fₛ) <em class="nl">？</em></h2><p id="be59" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">尽管我们可以自由决定变分分布<em class="mh">q(f，fₛ)</em>应该是什么样子，但它值得我们解释一下为什么我们选择了这个特殊的因式分解<em class="mh">p(f|fₛ)q(fₛ).</em></p><p id="990f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，为什么我们决定进行因式分解，而不是直接处理联合 q(f，fₛ)？在接缝上工作意味着将变分分布定义为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/4658f2bca88de2b3868fcea033123e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZIehxRC9b_XQWl9Vm_ZhA.png"/></div></div></figure><p id="5ae1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以<em class="mh"> m₁ </em>和<em class="mh"> m₂ </em>、<em class="mh"> C₁ </em>、<em class="mh"> C₂、</em>和<em class="mh"> C₃ </em>为模型参数。</p><p id="0f2a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以这样做，但是<em class="mh"> ELBO </em>的推导就变得复杂了。更有问题的是，这个模型的参数会比数据点的数量更多。正如我们在<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">变分高斯过程中所解释的——当事情不是高斯的时候该怎么办</a>,<em class="mh">一个有太多参数的模型？</em>节，这样的模型容易过拟合。</p><p id="8923" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二，鉴于我们更喜欢做因式分解而不是联合，根据概率论，你要么做<em class="mh">【p(f|fₛ)q(fₛ】</em>要么做<em class="mh">【p(fₛ|f)p(f】</em>，没有其他办法<em class="mh">。</em>我们该选哪个？</p><p id="edcb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，我们已经吸取了教训，要远离边际<em class="mh"> p(f) </em>，这将导致<em class="mh"> n×n </em>矩阵求逆<em class="mh">。</em>取而代之，我们用 q( <em class="mh"> fₛ)，</em>其中只有<em class="mh">有</em>一个<em class="mh"> nₛ×nₛ </em>矩阵求逆<em class="mh">。</em>我们可以选择一个足够小的<em class="mh"> nₛ </em>这样计算成本是可以承受的，但是引入的变量数量太少就不能很好的概括训练数据。天下没有免费的午餐。所以我们最终选择了<em class="mh"> p(f|fₛ)q(fₛ) </em>因式分解。</p><h2 id="b102" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">厄尔布河</h2><p id="8f1b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">由于边际似然<em class="mh"> p(y) </em>是我们对 SVGP 模型解释训练数据的程度的度量，我们希望根据模型参数最大化它。为了计算方便，我们通常最大化<em class="mh"> log p(y) </em>。由于<em class="mh"> log p(y) </em>很难计算(此处<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">为</a>看原因)，变分推理技术使用<em class="mh"> ELBO </em>公式作为备选的最大化目标函数。<em class="mh"> ELBO </em>的推导如下:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/3b5ca47b5dc920ee80a017823a7077e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38bEwAfa2Y6apmJMf7E_1g.png"/></div></div></figure><p id="f826" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(7)行揭示出<em class="mh"> ELBO </em>由两项组成，第一项称为似然项，第二项称为 KL 项。我们需要推导这两项的解析表达式，这样梯度下降算法可以计算解析<em class="mh"> ELBO </em>相对于模型参数的梯度。</p><p id="147f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">似然项</strong></p><p id="4057" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们进一步操纵可能性项:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/156abd3488b9017c077687af3a50a6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9O-5ghhIuXdr62jpOiAKDg.png"/></div></div></figure><p id="c1b0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(1)行是在<em class="mh"> ELBO 中定义的似然项。</em></p><p id="13ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行将<em class="mh"> fₛ </em>上的积分归入括号中。</p><p id="88e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行计算括号中的积分到边缘<em class="mh"> q(f) </em>中。</p><p id="5fa6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> q(f) </em>是由联合变分分布<em class="mh"> q(f，fₛ)</em>通过积分<em class="mh"> fₛ </em>出<em class="mh">得到的随机变量向量<em class="mh"> f </em>的边际。</em>注意<em class="mh"> q(f) </em>:</p><ul class=""><li id="7cc1" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="mh"> q(f) </em>是随机变量向量的一个分布<em class="mh"> f </em>，它不是边际变分分布<em class="mh">q(fₛ)=q(fₛ；μ, Σ).</em>后者是一个分布在随机变量上的向量<em class="mh"> fₛ.</em></li><li id="f46c" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="mh"> q(f) </em>不是我们可以利用多元高斯边缘化规则直接从 SVGP 先验中读出的边际<em class="mh"> p(f) </em>。</li></ul><p id="fe4c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不能使用多元高斯边际化规则简单地从联合分布<em class="mh"> q(f，fₛ).】中读出边际分布<em class="mh"> q(f) </em></em>这是因为变分分布<em class="mh"> q(f，fₛ) </em>没有被定义为多元高斯分布；它被定义为因式分解<em class="mh"> p(f|fₛ) q(fₛ).</em></p><p id="5b93" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要推导出<em class="mh"> q(f)的解析表达式。</em>但是由于这种因式分解，推导边际<em class="mh"> q(f) </em>很简单:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/8791d24de5ca57661dd5f6c19f32f8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eV8uBJZrjOoKiSNcgH6leQ.png"/></div></div></figure><p id="3e4c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线(1)将联合变分分布<em class="mh"> q(f，fₛ).】中的随机变量<em class="mh"> fₛ </em>积分</em></p><p id="15a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行写出了我们之前定义的<em class="mh"> q(f，fₛ) </em>的因式分解公式。</p><p id="1a54" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行显示了<em class="mh"> p(f|fₛ)=𝒩(f 的概率密度函数；Afₛ，B) </em>即我们之前推导出的，<em class="mh">为 q(fₛ的概率密度函数；μ，σ)</em>，这是我们提出来的。</p><p id="11c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(4)行应用多元高斯线性变换规则将概率密度函数<em class="mh"> p(f|fₛ) </em>转换成不提及<em class="mh"> fₛ.的表达式</em>反而提到参数<em class="mh"> μ </em>是来自分布<em class="mh"> q(fₛ的<em class="mh">σ</em>；μ, Σ).</em>这一行显示了<strong class="ki iu">在变分分布<em class="mh"> q(f，fₛ)——</em>的定义中引入条件<em class="mh"> p(f|fₛ) </em>的一个好处</strong>我们可以记下<em class="mh">𝒩(f；Aμ，aσaᵀ+b)</em>没有提到随机变量<em class="mh"> fₛ.</em>这使得下一行成为可能。</p><p id="e45a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线(5)将随机变量<em class="mh"> f </em>的分布移出积分，因为它没有提到<em class="mh"> fₛ </em>，并且相对于<em class="mh"> fₛ.上的积分是常数</em></p><p id="4d13" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(6)行计算积分为 1，因为概率密度函数积分为 1。</p><p id="202b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着<em class="mh"> q(f) </em>的导出，<em class="mh"> ELBO </em>中的似然项变为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/141a42ff0816cf0bf8fa742f6c1f1dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGExbgt-vt5UfiiMuWrylA.png"/></div></div></figure><p id="ad79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">内部分布<em class="mh">𝒩(f；Aμ，aσaᵀ+b)，A </em>和<em class="mh"> B </em>只提到了一个单矩阵求逆<em class="mh"> — Kₛₛ⁻ </em>，其大小为<em class="mh"> nₛ×nₛ.公式中没有提到昂贵的 K⁻。</em></p><p id="b257" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似然项是一个 n 维积分。我们可以使用高斯求积(<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">这里</a>看如何)来导出近似这个积分结果的解析表达式。结果是具有以下模型参数的函数:</p><ul class=""><li id="557b" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">观测噪声方差<em class="mh"> η </em>，从似然<em class="mh"> p(y|f) </em>。</li><li id="371d" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">长度刻度<em class="mh"> l </em>，信号方差σ <em class="mh"> </em>和感应位置<em class="mh"> Xₛ </em>来自<em class="mh"> A </em>和<em class="mh">b</em></li><li id="28a3" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">参数<em class="mh"> μ，σ来自 q(fₛ).</em></li></ul><p id="0a4b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以似然项提到了所有的模型参数。</p><p id="29ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="mh">fₛkl(q(f,fₛ)||p(f)</em>任期</strong></p><p id="42b0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们推导出<em class="mh"> ELBO </em>中 KL 项的解析表达式。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/789dd59832402db2bf3aa7b09200ed6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_K6WGKoAoryVyWtVWBPkA.png"/></div></div></figure><p id="23fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行插入变分分布的定义<em class="mh"> q(f，fₛ).</em></p><p id="8dfa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行使用反向链规则在<em class="mh"> p(f，fₛ).之前分割 GP</em></p><p id="71a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(4)行从分数中取消了<em class="mh"> p(f|fₛ) </em>项。这是<strong class="ki iu">将变分分布<em class="mh"> q(f，fₛ) </em>定义为因式分解<em class="mh"> p(f|fₛ)q(fₛ的另一个优点</em></strong>——我们可以取消<em class="mh"> p(f|fₛ) </em>项来简化我们的计算。</p><p id="1558" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(5)行将提到随机变量向量的项重新组织成它自己的积分。</p><p id="9edf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(6)行计算这个内部积分，留下分布<em class="mh"> q(fₛ).</em>注意，之前我们将该分布定义为<em class="mh">q(fₛ)=𝒩(fₛ；μ, Σ).</em></p><p id="59d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(7)行认识到最终公式是边际变分分布<em class="mh">q(fₛ</em>和边际 GP 先验<em class="mh"> p(fₛ).之间的新 KL 散度</em></p><p id="ad8f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">双双<em class="mh">q(fₛ)=𝒩(fₛ；μ，σ)</em>和<em class="mh">p(fₛ)=𝒩(fₛ；0，Kₛₛ) </em>是多元高斯分布，所以它们的 KL 散度是解析的。这就是<strong class="ki iu"/><strong class="ki iu">选择<em class="mh">【q(fₛ】</em>作为多元高斯分布的优势</strong>。<em class="mh"> q(fₛ) </em>和<em class="mh"> p(fₛ) </em>之间的 KL 散度为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/9985f1b4c1b9330154c88c737de1665b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6kzClV189SQFTfUL8clB1A.png"/></div></div></figure><p id="164f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> det </em>是矩阵行列式运算符。<em class="mh"> tr </em>是追踪运算符。<em class="mh"> nₛ </em>是随机变量向量<em class="mh"> fₛ </em>的长度，或者等价地，诱导位置的个数<em class="mh">。</em>这个公式也只提到了单矩阵求逆<em class="mh"> Kₛₛ⁻。</em>这还不包括昂贵的<em class="mh"> K⁻ </em>，它的尺寸是<em class="mh"> n×n. </em></p><p id="4ae4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的表达式已经是解析的了。它提到了以下模型参数:</p><ul class=""><li id="12d6" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="mh"> μ </em>和<em class="mh">σ</em>来自边际变分分布<em class="mh">q(fₛ；μ, Σ).</em></li><li id="4b94" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">长度刻度<em class="mh"> l </em>，信号方差σ <em class="mh">和感应位置<em class="mh"> Xₛ </em>来自<em class="mh"> Kₛₛ.</em></em></li></ul><p id="b9da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意这个 KL 术语根本没有提到我们的训练数据<em class="mh"> (X，Y) </em>。</p><p id="3bed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经推导出了<em class="mh"> ELBO </em>的解析表达式，并准备将其用作参数学习的目标函数。</p><h1 id="ac5b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参数学习</h1><p id="6905" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><em class="mh"> ELBO </em>的解析表达式是一个以所有模型参数为自变量的函数。我们使用梯度下降来最大化<em class="mh"> ELBO </em>以找到那些模型参数的最优值。</p><h2 id="bffd" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">我们的模型中有多少参数？</h2><p id="c1c2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们首先计算一下模型参数中包含的标量的数量:</p><ul class=""><li id="1639" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">lengthscale <em class="mh"> l </em>，来自内核函数，是一个单标量<em class="mh">。</em></li><li id="9047" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">同样来自内核函数的信号方差σ <em class="mh"> </em>，是单标量<em class="mh">。</em></li><li id="52b2" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">来自高斯似然的观测噪声方差<em class="mh"> η </em>是单个标量。</li><li id="6988" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">边际变分分布<em class="mh"> q(fₛ) </em>的均值向量<em class="mh"> μ </em>有长度<em class="mh"> nₛ </em>，所以它包含<em class="mh"> nₛ </em>标量。</li><li id="4099" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">来自边际变分分布<em class="mh">q(fₛ</em>的协方差向量<em class="mh">σ</em>具有形状<em class="mh"> nₛ×nₛ </em>。它是一个对称矩阵，所以它包含<em class="mh"> nₛ(nₛ+1) </em>标量<em class="mh">。</em></li></ul><p id="b912" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的模型包含<em class="mh"> 3+ nₛ(nₛ+1) </em>可训练标量，我们需要通过梯度下降<em class="mh">找出它们的值。</em>而且我们可以选择诱导点数<em class="mh"> nₛ </em>来控制要学习的标量总数。我们可以选择一个小的<em class="mh"> nₛ </em>，这样我们模型中的标量总数就大大小于训练数据点<em class="mh"> n </em>的数量。</p><p id="151b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义<em class="mh">q(fₛ</em>以减少可训练标量数量的另一种流行方法是平均场参数化。关于平均场参数化的更多细节，参见<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">此处的</a>，q 部分的<em class="mh">平均场参数化。</em></p><h2 id="1e75" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">我们应该使用多少个诱导位置？</h2><p id="cf91" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们需要决定使用多少诱导位置<em class="mh"> nₛ </em>。我们使用的诱导位置越多，模型就能越好地用相应的诱导变量<em class="mh"> fₛ </em>总结你的训练数据，但是计算，即矩阵<em class="mh"> Kₛₛ </em>的逆矩阵，就越昂贵<em class="mh">。</em></p><p id="d23e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们希望我们的诱导变量能够很好地总结训练数据点，所以我们应该在遍历训练数据的基础函数中的每个波峰和波谷至少使用一个诱导点。</p><p id="6db5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，只有当你的<em class="mh"> X </em>是一个或者两个维度的时候，你才能够可视化你的训练数据来看到那些波峰和波谷。</p><p id="1a7e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，一个实用的经验法则是，在你的预算允许的情况下，使用尽可能多的诱导变量，比如时间、记忆。例如，你从较少的诱导位置开始，逐渐增加<em class="mh"> nₛ </em>，看看你是否能在预算内得到更大的<em class="mh"> ELBO </em>。</p><h2 id="e460" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">随机梯度下降</h2><p id="c4b5" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu">全梯度评估可能很昂贵<br/> </strong>在稀疏先验的情况下，即使我们不存在对大矩阵求逆的问题，但在使用梯度下降的参数学习期间，我们仍然存在潜在的内存和速度问题。这是因为梯度下降在每个优化步骤执行以下三个任务:</p><ol class=""><li id="0214" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">它首先从<em class="mh"> ELBO 的解析表达式中计算出每个可训练标量的部分梯度表达式。</em>这导致<em class="mh"> 3+ nₛ(nₛ+1) </em>解析偏梯度表达式<em class="mh">。</em>实际上，梯度下降只计算这些解析表达式一次。这里我们假设梯度表达式是在每一步计算的，以便于理解。</li><li id="2549" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">然后，它插入训练数据<em class="mh"> (X，Y) </em>，以及<em class="mh"> 3+ nₛ(nₛ+1) </em>标量的所有当前值，以将那些解析部分梯度表达式评估为具体梯度，具体梯度是长度为<em class="mh"> 3+ nₛ(nₛ+1).的浮点向量</em></li><li id="c07f" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">最后，它使用这个浮点梯度向量来更新那些可训练标量的值。</li></ol><p id="6dd1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当训练数据很大时，任务 2 的开销很大。让我们通过研究<em class="mh"> ELBO </em>公式来看看为什么:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ov"><img src="../Images/7417f845e18ac8c84be8ad605e2b9d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe88f1PRf2M4h98Ocko8pg.png"/></div></div></figure><p id="e3e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在右边，第一项是可能性项。第二项是 KL 项。</p><p id="dea6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">KL 术语没有提到训练数据(不是<em class="mh"> X </em>，也不是<em class="mh"> Y </em> ) <em class="mh"> : </em></p><ul class=""><li id="0e5f" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="mh">q(fₛ)=𝒩(fₛ；μ，σ)</em>，并未提及<em class="mh"> X 或</em>y</li><li id="57d5" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="mh">p</em>(<em class="mh">fₛ)=𝒩(fₛ；0，Kₛₛ)，</em>其中<em class="mh"> Kₛₛ </em>提到<em class="mh"> Xₛ </em>，而不是<em class="mh"> X. </em></li></ul><p id="0ef2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以计算 KL 项有多贵，取决于我们完全可以控制的诱导变量<em class="mh"> nₛ </em>的数量。换句话说，我们可以选择一个足够小的<em class="mh"> nₛ </em>，这样计算 KL 项的梯度就不会很昂贵。</p><p id="156c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，似然项提到了完整的训练数据<em class="mh"> (X，Y): </em></p><ul class=""><li id="5d96" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="mh"> log(p(f|y)) </em>通过观察随机变量<em class="mh"> y </em>提到<em class="mh"> Y </em>。</li><li id="f841" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="mh">q(f)=𝒩(f；Aμ，aσaᵀ+b)</em>通过矩阵<em class="mh"> A </em>和<em class="mh"> B </em>提到<em class="mh">x</em>是因为<em class="mh"> X </em>出现在<em class="mh"> K </em>和<em class="mh"> Kₓₛ </em>:</li></ul><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/4b3a7f11fa5ac5e944fc747e00f0883f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ptfllo_jDBTqZBTj-74VDQ.png"/></div></div></figure><p id="a9f9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当训练数据很大时，在每个优化步骤评估似然项的梯度是昂贵的。我们无法控制这个梯度计算有多贵——数据点的数量<em class="mh"> n </em>决定它有多贵，我们希望我们所有宝贵的数据点都参与参数学习。</p><p id="98be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">随机梯度评估救援<br/> </strong>随机梯度下降(SGD)是梯度下降算法的一种变体。它通过在每个优化步骤仅计算在训练数据的子集上评估的目标函数的梯度，解决了昂贵的梯度评估的问题。当训练数据的这个子集包含单个数据点时，使用术语随机梯度下降。当该子集包含多个训练数据点时(该子集被称为小批量)，使用术语小批量随机梯度下降。在下文中，对于较短的句子，我使用 batch 代替 mini-batch，并使用随机梯度下降来表示 mini-batch 随机梯度下降。</p><p id="4d38" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不确定“计算在一批数据上评估的目标函数的梯度”是什么意思，请参阅本文的附录。</p><p id="2ba6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在高层次上，随机梯度下降算法执行以下操作:</p><ol class=""><li id="f1a7" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">从训练数据中随机均匀抽样，替换一批<em class="mh"> m </em>个数据点，其中<em class="mh"> m </em>为该批的大小。</li><li id="1cde" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">然后按照原始梯度下降算法中的步骤，使用批处理，就好像它是完整的训练数据一样。</li></ol><p id="478a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过控制批量大小<em class="mh"> m </em>，我们可以控制每个优化步骤的梯度评估的成本。</p><p id="3d6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这具有巨大的实际影响。通过使用随机梯度下降，有时您可以将花费在参数学习上的时间从一天减少到一个小时以内。</p><p id="5d0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这种便利伴随着一个问题:由于对一个批次进行采样的随机性，对一个批次计算的梯度变得随机——不同的批次给你一个困难的梯度。我们称之为<em class="mh">随机梯度</em>。这种算法因此而得名。</p><p id="8b62" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将每个采样数据点视为一个随机变量。这个随机变量可以以<em class="mh"> 1/n </em>相等的概率取<em class="mh"> n </em>个不同的值，其中<em class="mh"> n </em>为训练数据点的数量。这是因为我们对一批数据点随机、均匀地进行采样，并进行替换(因此，当您绘制第二个数据点时，您仍然是从具有<em class="mh"> n </em>个点的同一数据集中进行绘制)，从训练数据中进行采样。对一批中的那些随机变量的梯度评估变成随机变量本身。更多详情见附录。</p><p id="214b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望确保随机梯度下降渐近收敛到梯度下降收敛到的相同模型参数值。这是因为来自梯度下降的结果是我们作为正确答案的参考，通过切换到快速随机梯度下降算法，我们仍然希望得到相同的结果，至少是渐近的。为了保证渐近地得到相同的结果，我们要求随机梯度是梯度下降算法计算的全梯度的无偏估计量。这意味着随机梯度的期望与完全梯度相同。</p><p id="a491" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这具有直观的意义:梯度下降使用在全部训练数据上计算的梯度作为方向来更新模型参数的值。另一方面，随机梯度下降使用在训练数据子集上计算的随机梯度。因此，单个随机梯度很可能不会像全梯度那样指向同一个方向。但是如果平均起来，随机梯度(许多批次的平均值)与完全梯度指向相同的方向，我们就可以了。</p><p id="3194" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文附录证明了随机梯度是全梯度的无偏估计量。因此，我们可以使用随机梯度下降来优化 SVGP 模型的<em class="mh"> ELBO </em>。</p><h1 id="73e3" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">优化难度</h1><p id="2571" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们使用梯度下降来最大化目标函数<em class="mh">艾尔博</em>。如果我们仔细想想，<em class="mh"> ELBO </em>在一个公式中结合了两种优化:</p><ol class=""><li id="1ccb" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">找到核参数<em class="mh"> l </em>和<em class="mh"> σ，</em>和噪声方差<em class="mh"> 𝜂，</em>和诱导位置<em class="mh"> Xₛ </em>的值，使得真实后验很好地解释训练数据。</li><li id="f1ab" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">找到变分参数<em class="mh"> μ，σ</em>的值，使得变分分布<em class="mh">q(f；μ，σ)</em>近似于真实的后井。</li></ol><p id="4a21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直到现在，我们假设梯度下降可以为我们做这些。但这对于梯度下降来说要求太高了。这在数学上意味着:ELBO 可以是高度非凹的，有很多局部极大值。梯度下降法作为一种局部优化方法，会卡在一个局部最大值上，不一定是一个好的局部最大值。因此，我们如何优化这些模型就成了它自己的艺术/科学:</p><ul class=""><li id="3379" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">使用哪个优化器，亚当还是自然渐变？</li><li id="6c77" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">我们如何衰减学习率，分段衰减，还是指数衰减？</li><li id="962d" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">我们如何在优化开始前初始化模型参数？</li><li id="1ecc" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">我们如何处理数值不稳定性？对于高斯过程模型，这意味着当我们对一个矩阵求逆时<a class="ae kf" href="https://en.wikipedia.org/wiki/Cholesky_decomposition" rel="noopener ugc nofollow" target="_blank">乔莱斯基分解</a>失败。</li></ul><p id="6303" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个很大的话题，我将在未来的几篇文章中解释它。</p><h1 id="8620" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">做预测</h1><p id="1960" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">贝叶斯模型使用后验分布进行预测。给定一些测试位置<em class="mh"> X_* </em>，我们可以导出预测分布<em class="mh"> p(f_*|y)。</em>我们用<em class="mh"> p(f_*|y) </em>来做预测。但在此之前，让我们想想我们的 SVGP 模型如何能够做出预测。</p><p id="bf3f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案归结为测试位置<em class="mh"> X_* </em>的随机变量<em class="mh"> f_* </em>与诱导位置<em class="mh"> Xₛ.的随机变量<em class="mh"> fₛ </em>如何相关</em>下面再次显示的完整 SVGP 先验定义了这种关系:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/1da77ad6b83fe64dbbd44254f759e7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOAWN1t9ORBuS9rdnQerFw.png"/></div></div></figure><p id="aac1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SVGP 先验使用相同的核函数<em class="mh"> k </em>将每对随机变量联系在一起，无论它们来自<em class="mh"> f_* </em>、<em class="mh"> f </em>还是<em class="mh"> fₛ.</em>核函数<em class="mh"> k </em>有两个参数，lengthscale <em class="mh"> l </em>和信号方差<em class="mh"> σ </em>。参数学习为<em class="mh"> l </em>和<em class="mh"> σ找到单个值。</em>这意味着模型将使用相同的相关结构(多变量高斯条件)来:</p><ul class=""><li id="c67c" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">从诱导变量<em class="mh">fₛ</em>xₛ.<em class="mh">解释或总结训练地点<em class="mh"> X </em>的训练数据<em class="mh"> f ( </em>通过可能性<em class="mh">p(y | f)】</em></em></li><li id="ee45" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">从诱发变量<em class="mh">fₛ</em>xₛ.<em class="mh">解释或预测测试点<em class="mh">f _ *</em>x _ *</em></li></ul><p id="d597" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们假设测试数据来自与训练数据相同的生成过程。如果这个假设不成立，就不会存在一个可以从过去学习并预测未来的模型。在这种假设下，如果具有通过梯度下降找到的参数值的核函数能够使用诱导变量(在高边际可能性<em class="mh"> p(y) </em>意义上，或者等价地，在高<em class="mh"> ELBO </em>意义上)来总结训练数据，则具有相同参数设置的相同核应该允许我们从相同的诱导变量集合对<em class="mh"> X_* </em>处的测试位置处的<em class="mh"> f_* </em>做出合理的预测。这就是 SVGP 模型能够做出预测的原因。</p><p id="b8ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们来推导实际的预测分布<em class="mh"> p(f_*|y) </em>。</p><h2 id="480f" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">导出预测分布 p(f_*|y)</h2><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/1bf74bb0309a67a9a90d0a4d37a6b42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sb_TWxghC0qVewaxnNA7Eg.png"/></div></div></figure><p id="91b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(1)行从全后验<em class="mh"> p(f_*，f，fₛ|y) </em>中边缘化<em class="mh"> f </em>和<em class="mh"> fₛ </em>给你预测分布<em class="mh"> p(f_*|y) </em>只针对<em class="mh"> f </em>。但是请注意，我们不知道<em class="mh"> p(f_*，f，fₛ|y) </em>的联合概率密度，我们必须将其分解成一些分量分布的乘积，这些分量分布的概率密度是我们已知的。</p><p id="0bdf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行对关节<em class="mh"> p(f_*，f，fₛ|y) </em>应用反向链规则，将其分解为两个分量<em class="mh"> p(f_*|f，f_ₛ，y) </em>和<em class="mh"> p(f，fₛ|y).</em></p><p id="ebc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行丢弃了不相关的<em class="mh"> y </em>，因为给定<em class="mh"> f </em>和<em class="mh"> fₛ </em>，随机变量<em class="mh"> f_* </em>独立于<em class="mh"> y. </em>我们可以做到这一步，因为我们将模型设计成具有这样的性质:f_* 独立于<em class="mh"> y，</em>给定<em class="mh"> f </em>和<em class="mh"> fₛ.</em></p><p id="af8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(4)行将变分分布<em class="mh"> q(f，fₛ) </em>插入后验分布<em class="mh"> p(f，fₛ|y) </em>的位置，因为<em class="mh"> q(f，fₛ) </em>近似于后验分布。</p><p id="229d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(5)行插入了 q(fₛ).的定义</p><p id="d97e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(6)行将提到<em class="mh"> f </em>的所有术语重新组织成自己的内部集成，覆盖<em class="mh"> f </em>。</p><p id="c69b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(7)行应用链式法则来简化内部积分。</p><p id="1c76" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(8)行通过将<em class="mh"> f </em>排斥在关节<em class="mh"> p(f_*，f|fₛ) </em>之外来计算内部积分，从而产生<em class="mh"> p(f_*|fₛ) </em>。</p><p id="d911" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">预测分布未提及训练数据<br/> </strong>第(8)行的公式揭示了预测分布仅取决于诱导变量<em class="mh"> fₛ </em>，而不取决于训练位置的随机变量<em class="mh"> f </em>。这意味着来自训练数据的所有信息都被吸收到分布 q(<em class="mh">fₛ；μ，σ)</em>和梯度下降为其他模型参数找到的值<em class="mh"> l </em>、<em class="mh"> σ、</em>和<em class="mh"> η。</em>参数学习后，模型不再需要训练数据。这说明诱导变量真实地概括了训练数据。这与高斯过程回归模型不同，高斯过程回归模型需要训练数据进行预测。</p><p id="855c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此<em class="mh"> f_* </em>的预测分布为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/04ad9736f9ba5fba3018f3c3279b651c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taAYH6gEZi1-ccRTsRZBhg.png"/></div></div></figure><p id="47f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们知道<em class="mh">q(fₛ)=𝒩(fₛ；μ，σ)</em>，我们可以通过对稀疏先验<em class="mh"> p(f_*，fₛ) </em>应用多元高斯条件规则来推导出<em class="mh"> p(f_*|fₛ) </em>的公式，它已经将<em class="mh"> f </em>边缘化了:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/42514f56665f72b2fad1be0a17abdae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVOPnSCPaMBU1NW_kWGzzw.png"/></div></div></figure><p id="9d78" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这导致了条件<em class="mh"> p(f_*|fₛ) </em>的公式:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/fb0bcca16ac6fb96bcc2aa0615985791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVf9yw_QEg4_19teuPJnjg.png"/></div></div></figure><p id="8fc7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(1)行是条件规则应用的结果。揭示了<em class="mh"> f_* </em>是随机变量<em class="mh"> fₛ.的线性变换</em></p><p id="2980" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行应用多元高斯线性变换规则推导出<em class="mh"> p(f_*|fₛ) </em>的公式，其中提到了来自<em class="mh"> q(fₛ的<em class="mh"> μ </em>和<em class="mh">σ</em>；μ，σ)，</em>但没有提到<em class="mh"> fₛ.</em></p><p id="087c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以写出预测分布的公式<em class="mh"> p(f_*|y) </em>:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/948bb0ba6cabbe978e53f4b1512a6fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_hj1rGscWxpGp39hkG47g.png"/></div></div></figure><p id="e54e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(1)行是预测分布的定义。</p><p id="ecc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行将<em class="mh"> p(f_*|fₛ) </em>移至积分之外，因为它是一个没有提到<em class="mh"> fₛ </em>(尽管有符号<em class="mh"> p(f_*|fₛ) </em>提到<em class="mh"> fₛ </em> ) <em class="mh">的公式。</em>所以它对于在<em class="mh"> fₛ.上的积分是常数</em></p><p id="4838" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行将积分计算为 1，因为概率密度函数积分为 1。</p><p id="b290" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(4)行显示了<em class="mh"> p(f_*|fₛ).的公式</em></p><p id="e7de" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预测分布的公式很长。不需要完全解析它们。这是机械地应用多变量高斯分布的一些规则的结果。需要注意的是，在这个公式中:</p><ol class=""><li id="0df4" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">我们只需要对矩阵<em class="mh"> Kₛₛ </em>求逆。我们可以通过决定诱导位置<em class="mh"> nₛ </em>的数量来完全控制这种转化的成本。</li><li id="d1f4" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">最终的公式证实，为了进行预测，SVGP 模型不再需要训练数据——该公式没有提到<em class="mh"> X </em>或<em class="mh"> Y </em>。</li></ol><h1 id="5af4" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结果，和更好的结果</h1><p id="26b4" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们将 SVGP 模型应用于具有 10，000 个点的示例数据集。我用 15 个等距诱导位置初始化模型，进行了 30000 步随机梯度下降进行参数学习，每批由 100 个数据点组成。这里的代码是<a class="ae kf" href="https://gist.github.com/jasonweiyi/7a79da8a4cf72c2c86a205027c5e2fb9" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><p id="ef14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不再有内存消耗警告，参数学习过程不到一分钟就完成了。</p><p id="ebbf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了参数学习前后我们的模型预测。顶部的图显示的是之前的情况，底部的图显示的是之后的情况。</p><p id="2712" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个图中，红叉是训练数据点。粗蓝色曲线是测试位置的平均模型预测(后验均值)，伴随着 95%的置信区间(根据后验方差计算)，显示为浅蓝色带。我选择测试位置为-1 和 1 之间的 100 个等距点。零 x 轴线上的小黑点是这 15 个诱导位置的位置。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pb"><img src="../Images/6e26fc4ccc044f077fe36792c6284208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gLT6LIr9i3cxRSTo05Qig.png"/></div></div></figure><p id="7bb8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看上面的部分，我们看到在参数学习之前，模型做出了非常糟糕的预测。预测的平均值全为零，置信区间很宽——这就是 GP prior 要做的事情。上半部分右边置信区间的大漏斗形状是因为我们最后的诱导位置(右边最后一个黑点)离训练数据很远。我们还没有使用参数学习来用训练数据拟合模型，难怪预测会很差。</p><p id="2b64" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看底部，我们可以看到优化器确实为模型参数找到了一些值，因此模型现在可以更好地解释训练数据。预测的平均值现在更接近数据点，也更接近置信区间。</p><p id="32dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但我不得不说，我对模型的表现感到失望——预测的平均值(蓝色曲线)并没有真正捕捉到训练数据；置信区间没有很好地限制训练数据。还有，看看优化后的诱导位置，有些甚至超出了训练数据<em class="mh"> X </em>的范围[-1，1]。对了，最大化的<em class="mh">爱尔博</em>是-5583。</p><p id="843b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的 SVGP 模型应该有能力做得更好。可能由于我们前面谈到的优化困难，优化器未能找到更好的模型参数值。我们能帮助优化程序吗？</p><h2 id="354f" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">更加优化友好的参数化</h2><p id="dc13" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们最初对随机变量向量<em class="mh"> fₛ </em>上的稀疏先验和变分分布的定义是<em class="mh"> : </em></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pc"><img src="../Images/7095a74084a0a35e4b5c6c5dc20a9733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDMsWOmm9B0ivuS9nuz3DQ.png"/></div></div></figure><p id="6a03" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(1)和(2)表明<em class="mh"> fₛ </em>被定义为来自稀疏先验和变分分布中的不同且不相关的分布。来自先验的<em class="mh"> Kₛₛ </em>中的模型参数 lengthscale <em class="mh"> l </em>和信号方差<em class="mh"> σ </em>与来自变分分布的参数<em class="mh"> μ </em>和<em class="mh">σ</em>无关。先验分布和变分分布与模型观点完全无关。这意味着优化器可以将先验移向后验(通过改变<em class="mh"> l </em>和<em class="mh"> σ </em>的值)，而完全独立于将变分分布移向真实后验(通过改变<em class="mh"> μ </em>和<em class="mh">σ</em>的值)。这种自由赋予了优化器不同的任务。</p><p id="95bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有办法降低自由程度吗？是的，有，这里有一个优雅的方式。</p><h2 id="1735" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated"><em class="nl"> fₛ </em>的重新参数化</h2><p id="b6b4" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们引入一个新的随机变量向量<em class="mh"> u </em>，它与<em class="mh"> fₛ.的长度相同</em>而我们将<em class="mh"/>重新定义为<em class="mh">u:fₛ=卢，</em>其中<em class="mh"> L </em>是从矩阵<em class="mh"> Kₛₛ </em>的乔莱斯基分解得到的下三角矩阵。换言之，<em class="mh">llᵀ=kₛₛ</em>；<em class="mh"> L </em>中的<em class="mh">和</em>条目是从<em class="mh"> Kₛₛ </em>中的条目构建的分析表达式。</p><p id="1ec4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在稀疏先验中，我们让<em class="mh"> u </em>来自一个标准的多元高斯分布，即<em class="mh"> u </em> ~ <em class="mh"> 𝒩(0，1) </em>。于是稀疏先验<em class="mh"> p(fₛ) </em>就变成了:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pd"><img src="../Images/db7329353cd3f3de2a0ec0d3c52085ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdGfqo3kayPhEz84hxPzCg.png"/></div></div></figure><p id="0f4f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和以前一样。</p><p id="66ab" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于变分分布，设<em class="mh"> u </em>来自均值为<em class="mh"> μᵤ </em>协方差矩阵为<em class="mh">σᵤ、</em>即<em class="mh">、u~𝒩(μᵤ、σᵤ).的多元高斯分布</em></p><p id="d457" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<em class="mh">= Lu，</em>我们可以使用多元高斯线性变换规则导出变分分布<em class="mh"/>的概率密度函数:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pe"><img src="../Images/9837d90145dc416984ced429572f7226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7_2tLycSoLl5FiKgA08QQ.png"/></div></div></figure><p id="9b80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，这个重新参数化的<em class="mh"> q </em> ( <em class="mh"> fₛ) </em>中的<em class="mh"> fₛ </em>仍然是一个多元高斯随机变量。这个新的变分分布和原来的变分分布之间的差别<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；μ，σ)</em>在于多元高斯分布的参数化不同:</p><ul class=""><li id="1f28" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">在老<em class="mh">问</em>(<em class="mh">fₛ)=𝒩(fₛ；μ，σ)</em>，高斯分布由<em class="mh"> μ </em>和<em class="mh">σ参数化。</em></li><li id="5f8a" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">在新的<em class="mh">问</em>(<em class="mh">fₛ)=𝒩(fₛ；Lμᵤ，lσᵤlᵀ)</em>，高斯分布由<em class="mh"> L ( </em>参数化，最终由长度标度<em class="mh"> l </em>和信号方差<em class="mh">σ</em>、<em class="mh"> μᵤ </em>和<em class="mh">σᵤ.参数化</em></li></ul><p id="857e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种不同的参数化没有什么奇特之处。不同地参数化一个量是我们在高中学过的概念— <a class="ae kf" href="https://en.wikipedia.org/wiki/Parametric_equation" rel="noopener ugc nofollow" target="_blank">参数方程</a>。</p><h2 id="5551" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">新参数化的动机</h2><p id="03f5" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">定义<em class="mh"> fₛ </em>为<em class="mh">卢</em>在新的变分分布<em class="mh"> q </em> ( <em class="mh"> fₛ)=𝒩(fₛ内将内核参数长度标度<em class="mh"> l、</em>信号方差<em class="mh"> σ </em>和变分参数<em class="mh"> μᵤ </em>和<em class="mh">σᵤ</em>联系在一起；lσᵤlᵀ).lμᵤ</em></p><p id="b15a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还记得在<em class="mh">优化难度</em>一节中，我们讨论过<em class="mh"> ELBO </em>公式在一个公式中实现了两个优化目标:</p><ol class=""><li id="76db" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">找出核参数<em class="mh"> l </em>和<em class="mh"> σ、</em>以及噪声方差<em class="mh"> 𝜂、</em>和诱导位置<em class="mh"> Xₛ </em>的值，使得真实后验很好地解释训练数据。</li><li id="92e5" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">找到变分参数<em class="mh"> μ，σ</em>的值，使得变分分布<em class="mh">q(f；μ，σ)</em>近似于真实的后井。</li></ol><p id="25ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">新的参数化<em class="mh">fₛ=卢</em>将这两个目标联系在一起——每次优化器改变内核参数<em class="mh"> l </em>和<em class="mh"> σ，</em>下面的三角形<em class="mh"> L </em>也会改变，因为<em class="mh"> L </em>中的条目是提到<em class="mh"> l </em>和<em class="mh"> σ的表达式。</em>当<em class="mh"> L </em>变化<em class="mh">时，</em>的变分分布<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；Lμᵤ，lσᵤlᵀ)</em>也发生变化，因为它被均值<em class="mh"> Lμᵤ </em>和协方差<em class="mh">lσᵤlᵀ.参数化</em></p><p id="c154" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你需要一个比喻，想想你的小指和无名指——它们的运动是相互依赖的。你移动你的小手指，你的无名指也会移动。</p><p id="8512" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过使用<em class="mh">fₛ=陆</em>参数化来达到同样的效果。当优化器通过改变内核参数<em class="mh"> l </em>和<em class="mh"> σ </em>的值将先验向后验移动时，变分分布<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；Lμᵤ，lσᵤlᵀ)</em>，<em class="mh">，</em>我们用来近似后验的，也移动(向最大化<em class="mh"> ELBO </em>的方向，我们的实验结果证明了这一点)。这种受限的自由度使得优化器的任务更加简单。</p><ul class=""><li id="8e67" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">当优化器通过改变内核参数<em class="mh"> l </em>和<em class="mh"> σ </em>的值将先验向后验移动时，变分分布<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；Lμᵤ，lσᵤlᵀ)</em>，<em class="mh"> </em>我们用来近似后验概率的，也移动了(向 ELBO 最大化的方向移动，正如我们的实验结果所证明的)因为它是由<em class="mh"> Lμᵤ </em>和<em class="mh">lσᵤlᵀ</em>和<em class="mh"> L </em>提到的内核参数来参数化的。这种受限的自由度使得优化器的任务更加简单。</li></ul><p id="d290" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这与原来的<em class="mh"> q </em> ( <em class="mh"> fₛ)=𝒩(fₛ不一样；μ, Σ).</em>在最初的情况下，优化器可以改变<em class="mh"> l </em>和<em class="mh"> σ </em>，但是如果优化器不改变<em class="mh"> μ或σ</em>，变分分布将保持不变。这种不相关的自由给优化器留下了更艰巨的任务。</p><p id="b9bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要注意一件微妙的事情:我们的小指和无名指确实是相互依赖的，你不能在移动另一个手指的时候保持一个手指不动，不管你先移动哪个手指。但是由<em class="mh">fₛ=卢</em>参数化引入的先验和变分分布的共同运动只发生在一个方向上。你移动先验(通过改变核参数<em class="mh"> l </em>和<em class="mh"> σ </em>的值)，然后变分分布移动。然而，如果你移动变分分布(通过改变<em class="mh"> μᵤ </em>和<em class="mh">σᵤ</em>的值)，先验不会移动。但是，正如我们在改进的实验结果中看到的，即使是这种单向的共同运动也可以大大帮助优化器。</p><h2 id="a3c3" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">更合适的模型</h2><p id="8dab" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">有了<em class="mh">=陆</em>参数化，经过参数学习，我们的模型在预测方面要好得多:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pf"><img src="../Images/0fac5fe0e23779535fb3f61852db8be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxMWjv58UUg2nml6li2SRg.png"/></div></div></figure><p id="6f92" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该图清楚地显示了正确参数学习的效果:</p><ol class=""><li id="fc6b" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated">在参数学习之前，诱导位置是等距离分离的。这是我们初始化它们的地方。参数学习过程在训练数据变化更快的区域放置更多的诱导位置，而在训练数据变化更平稳的区域放置更少的诱导位置。</li><li id="4b5b" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">在参数学习之前，模型对训练数据的解释很差，这反映在顶部图中的预测具有零均值和非常大的方差。经过参数学习后，模型能够很好地解释数据，体现为贯穿中间训练数据的后验均值，置信区间与训练数据紧密有界。此外，优化的诱导位置在数据<em class="mh"> X </em>的范围[-1，1]内。</li></ol><p id="a2cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对了，最大化的<em class="mh"> ELBO </em>值是-1665。因为我们最大化了 ELBO，所以从-5583 到-1665 是个好消息。</p><h2 id="ea0a" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">还是同一个型号？</h2><p id="7d85" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">你可能想知道，有了新的参数化，=陆，我们是否还像以前一样定义相同的 SVGP 模型？是的，我们是。这是因为无论有没有新的参数化，我们的模型:</p><ul class=""><li id="c77d" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">使用多元高斯分布作为先验</li><li id="b389" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">使用高斯似然</li><li id="cbd7" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">使用多元高斯分布作为变分分布来近似真实的后验分布。</li></ul><p id="5f70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，模型具有相同的结构，因此具有相同的表现力。的确，一些随机变量，即<em class="mh"> fₛ </em>，被不同地参数化。但是，您可以将这种参数化视为同一个多元高斯概率密度函数 API 的不同实现。如果你愿意，你可以用你自己的方式参数化<em class="mh"> fₛ </em>，也许这对优化器帮助更大。</p><p id="9993" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们说的是同一个模型。但是正如我们已经看到的，不同地参数化模型可以帮助参数学习找到更好的值。</p><h2 id="ae60" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">厄尔布看起来像什么？</h2><p id="7c36" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们看看<em class="mh"> ELBO </em>在<em class="mh"> fₛ=Lu </em>参数化后是什么样子。<em class="mh"> ELBO </em>由两项组成，似然项和 KL 项。</p><p id="3197" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">似然项<br/> </strong>没有<em class="mh"> fₛ=Lu </em>参数化，似然项为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/141a42ff0816cf0bf8fa742f6c1f1dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGExbgt-vt5UfiiMuWrylA.png"/></div></div></figure><p id="8b9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/4b3a7f11fa5ac5e944fc747e00f0883f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ptfllo_jDBTqZBTj-74VDQ.png"/></div></div></figure><p id="cc3f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> fₛ=Lu </em>参数化影响<em class="mh"> μ </em>和<em class="mh">σ</em>，它们来自原始变分分布<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；μ，σ)</em>以下述方式<em class="mh"> : </em>新的变分分布变成<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；lσᵤlᵀ).lμᵤ</em>这是因为:</p><ol class=""><li id="a6ed" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld nn mu mv mw bi translated"><em class="mh"> fₛ=Lu 和 u~𝒩(u；μᵤ，σᵤ)</em></li><li id="1ba4" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld nn mu mv mw bi translated">应用多元高斯线性变换规则后，你会得到<em class="mh">q</em>(<em class="mh">fₛ)=𝒩(fₛ；lσᵤlᵀ).lμᵤ</em>换句话说，<em class="mh"> μ=Lμᵤ，σ=lσᵤlᵀ.</em></li></ol><p id="41b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可能性术语中的其他术语不受影响。诚然，我们可以把<em class="mh"> Kₛₛ= LLᵀ </em>写成，但由于<em class="mh"> L </em>没有提到新引入的随机变量<em class="mh"> u </em>，<em class="mh"> Kₛₛ </em>不受参数化的影响，<em class="mh"> A </em>和<em class="mh"> B </em>也不受影响。</p><p id="d03c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们将受影响的项<em class="mh"> μ=Lμᵤ </em>和<em class="mh">σ=lσᵤlᵀ</em>代入原始可能性项，我们得到:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/513c0a3aa0f5940e1f7f69032e1ee691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqVPrSp0K2wMUlxYQ_41Cg.png"/></div></div></figure><p id="c887" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，我们仍然可以使用高斯积分来导出近似该积分结果的解析表达式。</p><p id="9dda" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">KL 术语</strong></p><p id="4ba1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh"> ELBO </em>中原来的 KL 术语是:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ph"><img src="../Images/f1c7fdebcee02aaadfe72bacadff22b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9D-9Tlt93uFICLom0fBTTA.png"/></div></div></figure><p id="9afe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，q( <em class="mh"> fₛ)、</em>、d <em class="mh"> fₛ </em>项都受到<em class="mh"> fₛ=Lu </em>参数化的影响。让我们一个一个地解决它们。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pi"><img src="../Images/72306290654c3039402d6ad88c2c4c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmPYo2n9h_Zhgc25fHLMZg.png"/></div></div></figure><p id="dbe5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的概率密度函数中，<em class="mh"> nₛ </em>是<em class="mh"> fₛ </em>随机变量向量的长度，也是诱导位置<em class="mh">的个数。</em></p><p id="6ae0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们简化上面公式中的不同部分。这些推导并不难，它们需要耐心。如果你和两个孩子被锁在家里三个月，你自然会意识到耐心有多重要。开始了。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pj"><img src="../Images/52e422640671b3422d0f217d79cf3928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeJ5peQTqeBKSx62fepF-w.png"/></div></div></figure><p id="530a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们可以在第(4)行进行重组，因为矩阵的行列式是数字，而不是矩阵。</p><p id="a0b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在指数运算符内部:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pk"><img src="../Images/cd681c9f1e004a960f5861691091b35d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPwJLZEloQ_cOB3Wje-mqQ.png"/></div></div></figure><p id="7ca8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">加上这些简化的量，<em class="mh">【q(fₛ】</em>就变成了:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pl"><img src="../Images/dac71f1ff3180cd4070682b99aade134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lCfqwh3zwVyb-18bijeWg.png"/></div></div></figure><p id="c8c2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到<em class="mh"> q(fₛ) </em>可以表示为概率密度<em class="mh">𝒩(u；μᵤ，σᵤ)</em>按系数<em class="mh"> 1/det(L) </em>缩放。</p><p id="e875" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">按照同样的程序，我们可以将<em class="mh"> p(fₛ) </em>表示为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pm"><img src="../Images/3e0f5be1570f388b169307f77e7a3e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ilgrsyUK5C4tgynlLB_yg.png"/></div></div></figure><p id="6578" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以 KL 项可以表示为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pn"><img src="../Images/a3ab03af87a021853475d1ba177986cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ofu6eh3SM9TwnruKLvgOg.png"/></div></div></figure><p id="dba3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过<em class="mh"> fₛ=Lu </em>参数化，<em class="mh"> ELBO </em>中的 KL 项可以表示为<em class="mh"> 𝒩(u 之间的 KL 散度；μᵤ、</em>和<em class="mh">𝒩(u；0，1) </em>，如第(8) <em class="mh">行。多优雅啊！</em></p><p id="ff33" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便说一下，我们所做的是推导这个新的 KL 项，应用<a class="ae kf" href="https://en.wikipedia.org/wiki/Integration_by_substitution" rel="noopener ugc nofollow" target="_blank">替代积分</a>过程，将原来对<em class="mh"> fₛ </em>的积分转化为对<em class="mh"> u. </em>的积分。在高斯过程中，我们已经多次使用这个过程。最后一次是在<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4"> <em class="mh">变分高斯过程——当事情不是高斯</em> </a>的时候我们推导出了<em class="mh">埃尔博</em>公式中似然项的解析表达式。你必须掌握代换积分法。</p><p id="8033" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和以前一样，由于这个新 KL 中的两个分布都是高斯分布，所以它的解析表达式是可用的。</p><p id="8c2e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<em class="mh"> fₛ=Lu </em>参数化的最终<em class="mh"> ELBO </em>公式为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi po"><img src="../Images/fb8ab188a50095199eddcc126c09b1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ad3iz2rFpjxyqe5quSBnpw.png"/></div></div></figure><h2 id="d5cb" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">理论上的模型等价与实践中的优化结果</h2><p id="5aa2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">通过<em class="mh"> fₛ=Lu </em>参数化，新导出的<em class="mh"> ELBO </em>表达式计算出与旧<em class="mh"> ELBO </em>相同的数量。这是因为我们所做的所有操控都是平等的操控。我们没有做任何引入不平等的事情。你可以理解，同一个<em class="mh"> ELBO </em>可以用不同的方式实现，但是它们有相同的语义。</p><p id="197a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，使用<em class="mh"> fₛ=Lu </em>参数化，我们仍然有相同的模型，并且我们在参数学习期间最大化相同的目标函数。从数学上讲，如果我们以相同的模型参数初始值开始优化，我们应该得到相同的优化值:</p><ul class=""><li id="e2c3" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">相同长度刻度<em class="mh"> l </em>，信号方差<em class="mh"> σ </em>和观测方差<em class="mh"> η </em>。</li><li id="9ba7" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">相同诱导位置<em class="mh"> Xₛ.</em></li><li id="f98c" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">相同的变分参数<em class="mh"> μ=Lμᵤ </em>和<em class="mh">σ=lσᵤ</em>用于<em class="mh">q(fₛ；μ, Σ).</em>注意这里我们应该得到相同的<em class="mh"> μ和σ，</em>但是有了<em class="mh"> fₛ=Lu </em>参数化，<em class="mh"> μ </em>和<em class="mh">σ</em>通过<em class="mh"> L、</em>和<em class="mh">σᵤ.表示</em></li></ul><p id="fa79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们已经看到了用<em class="mh"> fₛ=Lu </em>参数化改进的实验结果。由于模型结构没有改变，我们的模型能够预测更好结果的唯一方法是优化器为模型参数返回不同的最优值。</p><p id="fcaf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个更好的结果表明，即使使用<em class="mh"> fₛ=Lu </em>参数化，我们有相同的模型，我们用相同的目标函数优化，新的参数化有助于优化。这就是我们使用这个参数化的原因。</p><p id="4a14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在机器学习中，我们经常以不同的方式重新参数化模型，以获得数学上的便利和更好的优化结果。这是一项需要学习的重要技术。</p><p id="bc5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会想，我要怎么学呢？这些重新参数化似乎是随机出现的。这倒是真的。因此，学习它的一个实用方法是看它发生几次:</p><ul class=""><li id="8f17" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">在<a class="ae kf" rel="noopener" target="_blank" href="/demystifying-tensorflow-time-series-local-linear-trend-9bec0802b24a">揭开 Tensorflow 时间序列的神秘面纱:局部线性趋势</a>中，我们使用了重新参数化技巧(在<em class="mh">章节重新参数化技巧</em>中)来使样本平均在梯度下降中工作。</li><li id="bc07" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">在本文中，我们使用重新参数化来帮助梯度下降找到更好的模型参数值。</li><li id="4424" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">将来，当我们谈到自然梯度时，我们将再次讨论高斯分布的重新参数化。</li></ul><p id="7ff2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你看，我们逐渐建立起自己！</p><h2 id="170d" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">为什么优化器移动诱导位置？</h2><p id="8839" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我相信你已经熟悉了优化器为什么要改变内核参数，比如 lengthscale <em class="mh"> l </em>来让我们的模型很好地解释训练数据(如果没有，请看<a class="ae kf" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">这里的</a>，计算后验部分的<em class="mh">)。因此本节重点介绍新模型参数——诱导位置<em class="mh"> Xₛ.</em></em></p><p id="4ca7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑一下，为什么优化器将更多的诱导位置分配给训练数据变化更快的区域，而将较少的诱导位置分配给训练数据更平滑的区域。</p><p id="5fa4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优化器的工作是优化<em class="mh"> ELBO </em>，它是边际可能性<em class="mh"> p(y) </em>的代理，测量诱导变量<em class="mh"> fₛ </em>如何解释代表训练数据的随机变量<em class="mh"> f、</em>。正如我们之前提到的，该模型在导出条件<em class="mh"> p(f|fₛ) </em>之前将多变量高斯条件规则应用于 SVGP，以实现使用<em class="mh"> fₛ </em>解释<em class="mh"> f </em>的目标:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pp"><img src="../Images/d263ab725f4b45cabfdc7ee1f30122d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdjnU21vJCHvEVd_BdV-cQ.png"/></div></div></figure><p id="f9df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pq"><img src="../Images/ecd9d17e4eba83f85dd0378f397bf961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbsWyfGefzPxKLk_Y_AQzQ.png"/></div></div></figure><p id="abef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行显示<em class="mh"> f </em>是来自<em class="mh"> fₛ的线性变换— A </em>是变换矩阵，带有一些噪声矩阵<em class="mh"> B. </em>。第(3)行应用多元高斯线性变换规则来重写<em class="mh"> f </em>的概率密度函数，而没有提到<em class="mh"> fₛ — </em>它只提到来自<em class="mh"> fₛ~𝒩(μ，σ的分布的参数<em class="mh"> μ </em>和<em class="mh">σ</em>。</em>记住，如果我们使用<em class="mh"> fₛ=Lu </em>参数化<em class="mh">，μ=lμᵤ</em>和<em class="mh">σ=lσᵤlᵀ</em>。</p><p id="4959" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行显示<em class="mh"> f </em>的平均值，即<em class="mh"> Aμ </em>，是<em class="mh"> fₛ </em>的平均值的加权和，即<em class="mh"> μ </em>，其中<em class="mh"> A </em>给出权重。<em class="mh"> A </em>的公式包含矩阵<em class="mh"> Kₓₛ.Kₓₛ </em>使用核函数<em class="mh"> k </em>来定义任意两个潜在随机变量之间的相关性，无论它们来自<em class="mh"> f </em>还是<em class="mh"> fₛ.</em></p><p id="934a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优化器为内核的长度尺度<em class="mh"> l </em>选择一个单一的值，这个值是所有训练数据点之间的一个折衷——这个单一的长度尺度表示所有交易数据的平均值，即两个随机变量需要有多接近才能有足够大的相关性。</p><p id="277e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个单一值<em class="mh"> l </em>下，我们的模型能够解释训练数据变化更快的区域的唯一方法是在那些区域中有更多的诱导变量。这是因为只有附近的诱导变量才能更有效地参与加权求和公式，来解释那些快速变化的训练数据点。“有效地”，我的意思是只有足够接近训练数据点的诱导变量将具有足够大的权重(由<em class="mh"> A </em>中的<em class="mh"> Kₓₛ </em>组件定义)来解释该训练数据点。</p><p id="c6b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是优化器在训练数据变化更快的区域分配更多诱导位置的原因。</p><h1 id="5b3a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="dfd7" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">现在我们完成了稀疏和变分高斯过程模型。恭喜你！让我们回顾一下高斯过程的历程:</p><ul class=""><li id="5b20" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated">我们从<a class="ae kf" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">高斯过程回归(GPR)模型</a>开始。它只接受高斯似然，不能扩展到大型数据集。</li><li id="2b91" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">然后我们继续使用<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">变分高斯过程(VGP)模型</a>。它接受高斯和非高斯的可能性，如伯努利，但它仍然不能扩展到大型数据集。</li><li id="46d7" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated">然后，我们探讨了本文中的稀疏变分高斯过程(SVGP)模型。该模型接受高斯和非高斯可能性，并且可以扩展到大型数据集。</li></ul><p id="e5c9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当在具有大型数据集的实际设置中应用高斯过程时，SVGP 模型将是您的起点。</p><h1 id="155c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">支持我</h1><p id="2497" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">如果你喜欢我的故事，如果你考虑通过这个链接成为一名灵媒会员来支持我，我将不胜感激:<a class="ae kf" href="https://jasonweiyi.medium.com/membership" rel="noopener">https://jasonweiyi.medium.com/membership</a>。</p><p id="8f53" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我会继续写这些故事。</p><h1 id="a903" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><p id="2148" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1309.6835" rel="noopener ugc nofollow" target="_blank">大数据的高斯过程</a>(更多介绍)<br/> <a class="ae kf" href="http://proceedings.mlr.press/v38/hensman15.pdf" rel="noopener ugc nofollow" target="_blank">可扩展的变分高斯过程分类</a>(更多数学内容)</p><h1 id="9f06" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">附录:证明随机梯度下降对 SVGP 模型参数学习有效</h1><p id="016b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">由于随机梯度下降算法要求从一批训练数据计算的随机梯度是从全部训练数据计算的全部梯度的无偏估计量，让我们证明这是我们的 SVGP 模型的情况。不要被“证明”这个词吓呆了。证明非常简单，它展示了一种你在机器学习算法推理中常见的模式。</p><p id="73c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先看什么是梯度下降使用的所有训练数据计算的全梯度。相对于所有模型参数，在<em class="mh"> ELBO </em>上计算梯度，表示为<em class="mh"> θ={l，σ，η，μ，σ}。</em>ELBO<em class="mh">ELBO</em>是:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ov"><img src="../Images/7417f845e18ac8c84be8ad605e2b9d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe88f1PRf2M4h98Ocko8pg.png"/></div></div></figure><p id="4ccd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一项是似然项，第二项是 KL 项。KL 项没有提到训练数据<em class="mh"> (X，Y) </em>，它相对于<em class="mh"> θ </em>的梯度在全数据情况和分批情况下是相同的。所以我们只需要专注于用一批抽样数据证明似然项的梯度是全数据下似然项梯度的无偏估计量。</p><p id="821e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来操纵可能性项:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/eed897b3ebe559a7ce1b193c9fbaa3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MTn8R2z4WxX01NmavX8XbA.png"/></div></div></figure><p id="0931" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行使用<em class="mh"> log </em>的属性和多元高斯分布<em class="mh"> q(f) </em>的边缘化属性来导出作为一维积分之和的似然性。推导见<a class="ae kf" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">此处</a>。</p><p id="75aa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行引入了简写<em class="mh"> Lᵢ </em>来指代这些一维集成。<em class="mh"> Lᵢ </em>是模型参数的函数，为了节省篇幅我没有写出<em class="mh"> θ </em>。每个<em class="mh"> Lᵢ </em>都会提到来自我们训练数据集<em class="mh">的第<em class="mh">个</em>数据点<em class="mh"> (Xᵢ，Yᵢ) </em>。</em></p><h2 id="3c84" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">完全渐变</h2><p id="b45e" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">完整训练数据的似然项的梯度为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pr"><img src="../Images/07edd217fd6342f2cd37c9c1918abc17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ZgXwMQi_gulQPN-kV9Jmg.png"/></div></div></figure><p id="7df8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线(1)获取关于模型参数集<em class="mh"> θ的似然项的梯度。</em></p><p id="5aec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(2)行使用梯度的线性属性将梯度运算符推入求和中。</p><p id="9095" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行为全渐变引入了一个新名字<em class="mh"> G </em>。</p><p id="ec59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要证明一批评估的似然性的随机梯度的期望与<em class="mh"> G </em>相同。</p><h2 id="74b9" class="np lf it bd lg nq nr dn lk ns nt dp lo kr nu nv ls kv nw nx lw kz ny nz ma oa bi translated">随机梯度</h2><p id="1ac7" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">将一批定义为一个集合<em class="mh"> M={j₁、j₂、…、jₘ} </em>。每个元素<em class="mh"> jₖ </em> ( <em class="mh"> k </em> from 1 到<em class="mh"> m </em>)是一个随机变量，来自均匀分布<em class="mh"> jₖ~Uniform(1，n) </em>，其中<em class="mh"> n </em>为训练数据点的个数。这个随机变量表示批处理包含的数据点的索引。我们将<em class="mh"> jₖ </em>建模为均匀分布，以表示从我们的训练数据集中随机且均等地选择(替换)数据点，从而创建一个批次。</p><p id="091a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，<em class="mh"> M </em> ={ <em class="mh"> j₁= </em> 1，<em class="mh"> j₂= </em> 3，<em class="mh"> j </em> ₃=5}表示该批包含 3 个数据点。这些数据点是来自训练集<em class="mh"> (X，Y) </em>的第 1、第 2 和第 5 个数据点。</p><p id="59c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对批次<em class="mh"> M </em>评估的可能性表示为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ps"><img src="../Images/1e531613c6c67edb66a71d29e56117bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZbbAFk9ZFTeZjS4K2porg.png"/></div></div></figure><p id="3897" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相对于模型参数集<em class="mh"> θ的随机梯度为</em></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pt"><img src="../Images/d6e40b263b4a5f61f3f3562d9d2fc0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBtWB-0mbMxuNuMROzyCqA.png"/></div></div></figure><p id="b0a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每次你画一批，你有一个不同的集合 M，因此不同的梯度。换句话说，梯度变成了一个随机变量。所以我们称之为随机梯度。</p><p id="1e35" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述随机梯度相对于随机变量<em class="mh"> j </em>的期望值为:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pu"><img src="../Images/721180e57e82fe75adbbf1172562a8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kbklu1CaLvM8grS4KYsZQ.png"/></div></div></figure><p id="c427" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要证明这个期望等于全梯度<em class="mh"> G </em>。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pv"><img src="../Images/3398384309f1deba1987084a96ffd121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PuG69PuLzfWI9bd_Nb7sg.png"/></div></div></figure><p id="1660" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线(1)是对一批数据计算的随机梯度的期望值。</p><p id="2fc9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于每个随机变量<em class="mh"> j∈M </em>都来自同一个均匀分布<em class="mh"> Uniform(1，n) </em>，所以它们的期望应该是相同的。所以第(2)行使用单个元素<em class="mh"> j₁ </em>来计算这个期望值。最终结果将是<em class="mh"> m </em>，批量，乘以相对于<em class="mh"> j₁ </em>的预期结果。</p><p id="3a8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(3)行写出了计算相对于<em class="mh"> j₁ </em>的期望值的公式。并且线(4)插入均匀概率密度<em class="mh"> 1/n </em>。</p><p id="b3bc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第(5)行将通用表达式<em class="mh"> 1/n </em>移出求和。注意，剩余总和等于全梯度<em class="mh"> G </em>。因此线路(6)插入<em class="mh"> G </em>。</p><p id="d559" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们认识到随机梯度的期望是<em class="mh"> m/n </em>乘以全梯度，因此随机梯度不是全梯度的无偏估计量。但是我们可以通过将随机梯度乘以<em class="mh"> n/m </em>来构造一个无偏估计量。这就是随机梯度下降算法对 SVGP 模型所做的。</p><p id="faf0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述证明是一种流行的证明模式。你会在决定随机梯度下降是否适用于机器学习模型的过程中看到它。在文章“<a class="ae kf" rel="noopener" target="_blank" href="/can-we-use-stochastic-gradient-descent-sgd-on-a-linear-regression-model-e50327b07d33">中，我们可以在线性回归模型上使用随机梯度下降(SGD)吗？</a>”对于线性回归模型，我写了一个非常类似的证明。</p></div></div>    
</body>
</html>