<html>
<head>
<title>7 Key Points on 7 Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">7 个机器学习算法的 7 个要点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-key-points-on-7-machine-learning-algorithms-945ebda7a79?source=collection_archive---------43-----------------------#2020-09-09">https://towardsdatascience.com/7-key-points-on-7-machine-learning-algorithms-945ebda7a79?source=collection_archive---------43-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="29ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">引擎盖下到底发生了什么？</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/bdb4e98ddf9b584b7e9bb59d424b5a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNae5HXt7xFiCO1oZyJQXw.jpeg"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">在<a class="ae lf" href="https://unsplash.com/s/photos/seven?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lf" href="https://unsplash.com/@waldemarbrandt67w?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Waldemar Brandt </a>拍照</p></figure><p id="38ef" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">感谢各种库和框架，我们可以只用一行代码实现机器学习算法。有些则更进一步，让你立刻实现和比较多种算法。</p><p id="3572" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">易用性也有一些缺点。我们可能会忽略这些算法背后的关键概念或思想，而这些概念或思想对于全面理解它们是必不可少的。</p><p id="6379" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这篇文章中，我将提到 7 个机器学习算法的 7 个关键点。我想指出的是，这不会是对算法的完整解释，所以如果你对它们有一个基本的了解就更好了。</p><p id="7adb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们开始吧。</p><h1 id="50e1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak"> 1-支持向量机(SVM) </strong></h1><p id="4989" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">点:C 参数</p><p id="4707" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">SVM 创建了一个<strong class="li iu">决策边界</strong>来区分两个或更多的类。</p><p id="a174" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">软利润 SVM 试图通过以下目标来解决优化问题:</p><ul class=""><li id="42d9" class="mz na it li b lj lk lm ln lp nb lt nc lx nd mb ne nf ng nh bi translated">增加决策边界到类别(或支持向量)的距离</li><li id="6bc1" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">最大化训练集中正确分类的点数</li></ul><p id="8760" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这两个目标之间显然有所取舍。决策边界可能必须非常接近某个特定类，才能正确标注所有数据点。然而，在这种情况下，新观测值的准确性可能会较低，因为决策边界对噪声和独立变量的微小变化过于敏感。</p><p id="f564" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">另一方面，决策边界可能被放置在尽可能远的每个类，代价是一些错误分类的异常。这种权衡由<strong class="li iu"> c 参数控制。</strong></p><p id="808d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> C 参数</strong>为每个错误分类的数据点增加一个惩罚。如果 c 很小，则对误分类点的惩罚也很低，因此以更大数量的误分类为代价选择了具有大余量的决策边界。</p><p id="832d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果 c 很大，SVM 试图最小化由于高惩罚导致的错误分类的例子的数量，这导致了具有较小裕度的决策边界。对于所有错误分类的例子，惩罚是不一样的。它与到决策边界的距离成正比。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="2150" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak"> 2-决策树</strong></h1><p id="9977" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">要点:信息增益</p><p id="4f1c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当选择要分割的特征时，决策树算法试图实现</p><ul class=""><li id="244c" class="mz na it li b lj lk lm ln lp nb lt nc lx nd mb ne nf ng nh bi translated">更多的预测</li><li id="da8b" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">杂质少</li><li id="9e9a" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">低熵</li></ul><p id="a49e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">熵</strong>是不确定性或随机性的度量。一个变量的随机性越大，熵就越大。均匀分布的变量具有最高的熵。例如，掷一个公平的骰子有 6 个概率相等的可能结果，所以它有均匀的分布和高熵。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ns"><img src="../Images/c108a3ccdef6a6358fed02b902c1cc9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*4i4mBfEX6UYYxb43.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">熵 vs 随机性</p></figure><p id="f78a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">选择导致更纯节点的分裂。所有这些都表明“信息增益”，这基本上是分裂前后熵的差异。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/aadf02bd1c9f6ad378b23551353b7c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/0*xFiGUr4Pzv0xL47-.png"/></div></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="0654" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak"> 3-随机森林</strong></h1><p id="cf3d" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">要点:引导和特征随机性</p><p id="0ad3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随机森林是许多决策树的集合。随机森林的成功高度依赖于使用不相关的决策树。如果我们使用相同或非常相似的树，总体结果与单个决策树的结果不会有太大不同。随机森林通过<strong class="li iu">自举</strong>和<strong class="li iu">特征随机性</strong>实现不相关的决策树。</p><p id="e303" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Bootstrapping 是通过替换从训练数据中随机选择样本。它们被称为 bootstrap 样本。</p><p id="b311" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过为随机森林中的每个决策树随机选择特征来实现特征随机性。随机森林中每棵树使用的特征数量可通过<strong class="li iu"> max_features </strong>参数控制。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7f2277c1d968b128bf2befee9f991a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/0*yytkUaOI6oAfo8Sf.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">特征随机性</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="0424" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak"> 4-梯度提升决策树</strong></h1><p id="91b0" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">要点:学习率和 n _ 估计量</p><p id="176e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">GBDT 是决策树与 boosting 方法相结合的集合，这意味着决策树是顺序连接的。</p><p id="10aa" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">学习率</strong>和<strong class="li iu"> n_estimators </strong>是梯度推进决策树的两个关键超参数。</p><p id="6452" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">学习率就是模型学习的速度。较慢学习速率的优点是模型变得更加健壮和通用。然而，学习是要慢慢付出代价的。训练模型需要更多的时间，这就引出了另一个重要的超参数。</p><p id="2c2e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> n_estimator </strong>参数是模型中使用的树的数量。如果学习率低，我们需要更多的树来训练模型。然而，我们需要非常小心地选择树的数量。使用太多的树会产生过度适应的高风险。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="70cd" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak"> 5-朴素贝叶斯分类器</strong></h1><p id="63e1" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">点:天真有什么好处？</p><p id="5fa2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">朴素贝叶斯是一种用于分类的监督机器学习算法，因此任务是在给定特征值的情况下找到观察值的类别。朴素贝叶斯分类器计算给定一组特征值(即 p(yi | x1，x2，…，xn))的类的概率。</p><p id="9b93" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">朴素贝叶斯假设特征是相互独立的，特征之间没有相关性。然而，现实生活中并非如此。这种特征不相关的天真假设是这种算法被称为“天真”的原因。</p><p id="2454" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所有特征都是独立的假设使得<strong class="li iu">与复杂的算法相比</strong>非常快。<strong class="li iu"> </strong>在某些情况下，速度优先于更高的精度。</p><p id="6e1d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">它可以很好地处理文本分类、垃圾邮件检测等高维数据。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="b3ff" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak">6-K-最近邻</strong></h1><p id="4866" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">要点:何时使用和不使用</p><p id="1638" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">k-最近邻(kNN)是一种受监督的机器学习算法，可用于解决分类和回归任务。kNN 的主要原理是一个数据点的值是由其周围的数据点决定的。</p><p id="1bc3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随着数据点数量的增加，kNN 算法变得非常慢，因为模型需要存储所有数据点，以便计算它们之间的距离。正是这个原因也使得算法的内存效率不高。</p><p id="faf5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">另一个缺点是 kNN 对异常值很敏感，因为异常值对最近的点有影响(即使它太远)。</p><p id="e163" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">积极的一面是:</p><ul class=""><li id="3473" class="mz na it li b lj lk lm ln lp nb lt nc lx nd mb ne nf ng nh bi translated">简单易懂</li><li id="5aa8" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">不做任何假设，因此可以在非线性任务中实现。</li><li id="046f" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">适用于多个类别的分类</li><li id="8919" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">处理分类和回归任务</li></ul></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="1e84" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated"><strong class="ak">7-K-均值聚类</strong></h1><p id="257f" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">要点:何时使用和不使用</p><p id="42a6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">k-均值聚类旨在将数据划分为 k 个聚类，使得同一聚类中的数据点相似，而不同聚类中的数据点相距较远。</p><p id="c28f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">K-means 算法无法猜测数据中存在多少个聚类。集群的数量必须预先确定，这可能是一项具有挑战性的任务。</p><p id="a694" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随着样本数量的增加，算法会变慢，因为在每一步，它都要访问所有数据点并计算距离。</p><p id="3ae5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">K-means 只能画线性边界。如果数据中存在非线性结构来分隔组，k-means 将不是一个好的选择。</p><p id="200b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">积极的一面是:</p><ul class=""><li id="a9fb" class="mz na it li b lj lk lm ln lp nb lt nc lx nd mb ne nf ng nh bi translated">容易理解</li><li id="a1e0" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">相对较快</li><li id="85bb" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">可扩展用于大型数据集</li><li id="6211" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">能够以一种聪明的方式选择初始质心的位置，从而加速收敛</li><li id="f8e9" class="mz na it li b lj ni lm nj lp nk lt nl lx nm mb ne nf ng nh bi translated">保证收敛</li></ul></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="acb6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们已经介绍了每个算法的一些关键概念。给出的要点和注释绝对不是算法的全部解释。然而，在实现这些算法时，了解它们对于做出改变是非常重要的。</p><p id="11e0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>