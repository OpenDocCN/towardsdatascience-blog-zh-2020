<html>
<head>
<title>Autoencoding Generative Adversarial Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码生成对抗网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoding-generative-adversarial-networks-16082512b583?source=collection_archive---------12-----------------------#2020-04-18">https://towardsdatascience.com/autoencoding-generative-adversarial-networks-16082512b583?source=collection_archive---------12-----------------------#2020-04-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2677" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">AEGAN 架构如何稳定 GAN 训练并防止模式崩溃</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/37d6eae76f4007404d4fc4489b100d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8VdkglkY6YSdH7bSJcNJiw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双向街道 AEGAN(图片来源:<a class="ae ky" href="https://pixabay.com/photos/sign-one-way-two-way-direction-3219707/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="f435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">甘很难训练。当他们工作时，<a class="ae ky" href="https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/" rel="noopener ugc nofollow" target="_blank">他们创造奇迹</a>，但是任何试图训练他们自己的人都知道他们是该死的挑剔的混蛋。GAN 训练中最常见的两个问题是模式崩溃和缺乏收敛。在模式崩溃中，生成器学习只生成少量样本；在生成“手写”数字的过程中，经历模式崩溃的 GAN 可能只能学会画 7，尽管这是高度真实的 7。由于缺乏收敛性，发生器和鉴别器之间的良性竞争恶化，通常鉴别器变得比发生器好得多；当鉴别器能够容易且完全地区分真实样本和生成样本时，生成器不会得到有用的反馈，也无法改进。</p><p id="e355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://arxiv.org/abs/2004.05472" rel="noopener ugc nofollow" target="_blank">最近的一篇论文</a>中，我提出了一种技术，似乎可以稳定 GAN 的训练并解决上述两个问题。这种技术的副作用是它允许在真实样本之间进行有效和直接的插值。在这篇文章中，我的目标是逐步介绍本文的主要观点，并说明为什么我认为 AEGAN 技术有可能成为 GAN 培训师工具箱中非常有用的工具。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/6f26f56cbcf5a4c0909e47a5f2c17874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4MUKv3nI__sgmtl6BoZ08g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有人告诉我不要再埋 led 了，所以这是这篇论文最有趣的结果。这是怎么回事？请继续阅读，寻找答案。</p></figure><h1 id="5741" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">进入艾根</h1><h2 id="5c57" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">双射映射</h2><p id="0587" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">GANs 学习从某个潜在空间<em class="nf"> Z </em>(随机噪声)到某个样本空间<em class="nf"> X </em>(数据集，通常是图像)的映射。这些映射自然是<a class="ae ky" href="https://en.wikipedia.org/wiki/Injective_function" rel="noopener ugc nofollow" target="_blank">内射的</a>——Z<em class="nf">Z</em>中的每个点<em class="nf"> z </em>对应于<em class="nf"> X </em>中的某个样本<em class="nf"> x </em>。然而，它们很少<a class="ae ky" href="https://en.wikipedia.org/wiki/Surjective_function" rel="noopener ugc nofollow" target="_blank">满射</a>——X 中的许多样本在 z 中没有对应的点。实际上，当许多点<em class="nf"> zi </em>、<em class="nf"> zj </em>和<em class="nf"> zk </em>映射到单个样本<em class="nf"> xi </em>时，模式崩溃就发生了，GAN 无法生成点<em class="nf"> xj </em>或<em class="nf"> xk </em>。考虑到这一点，更理想的 GAN 应该具有以下品质:</p><ol class=""><li id="6f0a" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><em class="nf"> Z </em>中的每个潜在点<em class="nf"> z </em>应该对应于<em class="nf"> X. </em>中的一个唯一样本<em class="nf"> x </em></li><li id="0eae" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><em class="nf"> X </em>中的每个样本<em class="nf"> x </em>应该对应于<em class="nf"> Z </em>中的一个唯一潜在点<em class="nf"> z </em>。</li><li id="5ca2" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">从<em class="nf"> Z、</em> <em class="nf"> p(Z=z) </em>中抽取<em class="nf"> z </em>的概率，应该等于从<em class="nf"> X </em>、<em class="nf"> p(X=x) </em>中抽取<em class="nf"> x </em>的概率。</li></ol><p id="6cbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这三个性质表明，我们应该以潜在空间和样本空间之间的一对一关系(即双射映射)为目标。为此，我们训练一个函数<em class="nf"> G : Z ⟶ X </em>，这是我们的生成器，另一个函数<em class="nf"> E : X ⟶ Z </em>，我们称之为编码器。这些功能的目的是:</p><ul class=""><li id="50c5" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated"><em class="nf"> G(z) </em>应产生与分布在<em class="nf"> X </em>中的比例相同的真实样本。(这是普通 GANs 的目标)</li><li id="c15f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> E(x) </em>应该产生与它们在 z 上分布的比例相同的可能潜在点。</li><li id="662a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">构图<em class="nf"> E(G(z)) </em>应忠实再现原潜点<em class="nf"> z </em>。</li><li id="5dc8" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">构图<em class="nf"> G(E(x)) </em>要忠实再现原图像<em class="nf"> x </em>。</li></ul><h2 id="66f0" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">体系结构</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/76cc1fd2ceb45867a693b6ae89bf406f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F87VE1NwY2sNV_X_yLEPhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:高级 AEGAN 架构。网络用方框表示，价值用圆圈表示，损失用菱形表示。颜色代表组合网络，其中红色是常规图像生成 GAN，黄色是用于产生潜在向量的 GAN，蓝色是图像自动编码器，绿色是潜在向量自动编码器。</p></figure><p id="ce3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AEGAN 是一个四网络模型，由两个 GAN 和两个自动编码器组成，如图 1 所示，它是用于不成对图像到图像转换的<a class="ae ky" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank"> CycleGAN </a>技术的推广，其中一个图像域用随机噪声代替。简而言之，我们训练两个网络在样本空间<em class="nf"> X </em>和潜在空间<em class="nf"> Z </em>之间进行翻译，我们训练另外两个网络来区分真假样本和潜在向量。图 1 是一个复杂的图表，所以让我来分解它:</p><p id="02d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">网络(方块):</strong></p><ul class=""><li id="b5b9" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated"><em class="nf"> G </em>是发电机网络。它将一个潜在向量<em class="nf"> z </em>作为输入，并返回一个图像<em class="nf"> x </em>作为输出。</li><li id="6b3f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> E </em>是编码器网络。它将图像<em class="nf"> x </em>作为输入，并返回潜在向量<em class="nf"> z </em>作为输出。</li><li id="bb8a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> Dx </em>是图像鉴别器网络。它将图像<em class="nf"> x </em>作为输入，并将从原始数据集中提取的<em class="nf"> x </em>的概率作为输出返回。</li><li id="4c37" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> Dz </em>是潜在鉴别器网络。它取一个潜在向量<em class="nf"> z </em>作为输入，并返回从潜在分布中抽取<em class="nf"> z </em>的概率作为输出。</li></ul><p id="19a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数值(圆圈):</strong></p><ul class=""><li id="7f44" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated"><em class="nf"> x </em>:来自原始数据集的真实样本。这有点不明确，因为在某些地方我用<em class="nf"> x </em>来表示域 x 中的任何值。抱歉。</li><li id="72dd" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> z </em>:来自潜在生成分布(随机噪声)的真实样本。</li><li id="fe58" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf">x _ hat</em>:G 给定一个实随机向量产生的样本，即<em class="nf"> x_hat=G(z)。</em></li><li id="bd33" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> z_hat </em>:给定一个真实样本，E 产生的向量，即<em class="nf"> z_hat=E(x)。</em></li><li id="de7f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> x_tilde </em>:由<em class="nf"> G </em>从<em class="nf"> E </em>产生的编码中重现的样本，即<em class="nf"> x_tilde=G(z_hat)=G(E(x))。</em></li><li id="df21" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated"><em class="nf"> z_tilde </em>:由<em class="nf"> E </em>从<em class="nf"> G </em>生成的图像中重现的向量，即<em class="nf"> z_tilde=E(z_hat)=E(G(z))。</em></li></ul><p id="dd2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">损失(钻石):</strong></p><ul class=""><li id="bb57" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated">L1(蓝色):图像重建损失<em class="nf"> ||G(E(x))-x||_1 </em>，即原始图像像素与自动编码重建像素之间的曼哈顿距离。</li><li id="646d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">L2(绿色):潜在向量重建损失<em class="nf"> ||E(G(z))-z||_2 </em>，即原始潜在向量和自动编码重建之间的欧几里德距离。</li><li id="621d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">甘(红):图像的对抗性丧失。<em class="nf"> Dx </em>被训练来区分真实图像(<em class="nf"> x </em>)和虚假图像(<em class="nf"> x_hat </em>和<em class="nf"> x_tilde </em>(未显示))</li><li id="2f1d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">甘(黄色):潜在载体的对抗性损失。<em class="nf"> Dz </em>被训练来区分真实随机噪声(<em class="nf"> z </em>)和编码(<em class="nf"> z_hat </em>和<em class="nf"> z_tilde </em>(未显示))</li></ul><h2 id="50e2" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">培养</h2><p id="5551" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">AEGAN 的训练方式与 GAN 相同，交替更新发电机(<em class="nf"> G </em>和<em class="nf"> E </em>)和鉴别器(<em class="nf"> Dx </em>和<em class="nf"> Dz </em>)。然而，AEGAN 损耗函数比典型的 GAN 损耗稍微复杂一些。它由四个对立的部分组成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/2a74bff68434aba62a77d2c00cb3a882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZRQV1U2mBVWN5eMxdoMHyQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾根损失的对抗成分。</p></figure><p id="c782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和两个重建分量(这里显示为加在一起):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/0033b8782f7eff6d9e32f68a0b89db23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNDXCRPy1MCYlmtuTyMelg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾根损失的重建部分。λs 是控制重建分量的相对权重的超参数。</p></figure><p id="73aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些加在一起就形成了艾根损失。<em class="nf"> E </em>和<em class="nf"> G </em>尽量减少这种损失，而<em class="nf"> Dx </em>和<em class="nf"> Dz </em>尽量增加这种损失。如果你不关心数学，直觉很简单:</p><ol class=""><li id="bea7" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><em class="nf"> G </em>试图欺骗<em class="nf"> Dx </em>相信生成的样本<em class="nf"> x_hat </em>和自动编码的样本<em class="nf"> x_tilde </em>是真实的，而<em class="nf"> Dx </em>试图将它们与真实样本<em class="nf"> x </em>区分开来。</li><li id="fc9a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><em class="nf"> E </em>试图欺骗<em class="nf"> Dz </em>相信生成的样本<em class="nf"> z_hat </em>和自动编码的样本<em class="nf"> z_tilde </em>是真实的，而<em class="nf"> Dz </em>试图将它们与真实样本<em class="nf"> z </em>区分开来。</li><li id="a50f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><em class="nf"> G </em>和<em class="nf"> E </em>必须一起工作，以便自动编码的样本<em class="nf"> G(E(x))=x_tilde </em>与原始的<em class="nf"> x </em>相似，并且自动编码的样本<em class="nf"> E(G(z))=z_tilde </em>与原始的<em class="nf"> z </em>相似。</li></ol><h2 id="07dd" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">结果</h2><p id="cdac" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">首先，免责声明。由于个人原因，我只有时间和精力在单个数据集上进行测试。我按原样发布我的工作，以便其他人可以自己测试这项技术，验证我的结果，或者表明这是一个死胡同。也就是说，以下是 30 万步训练后的结果样本:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8b9f9e9762a443b88254a1b9ac492381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*0HtPHH9a4pEG7RyTn9hVwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 AEGAN 在 21552 张独特动漫人脸的数据集上经过 30 万步训练后生成的随机图像。</p></figure><p id="012d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图 2 本身并不那么令人兴奋。如果你正在读一篇关于 GANs 的中型文章，那么你可能已经看到了在动画人脸上训练的风格 GANs 产生了更好的结果。<em class="nf">令人兴奋的是</em>将上述结果与图 3 进行比较:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/cc5e3c4fa7dcd75ca25cd2665ec17f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zakR14_7yaltcYFf_YrnzQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:GAN 在 21552 张独特动漫人脸的数据集上经过 30 万步训练后生成的随机图像。</p></figure><p id="98fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于生成图 3 中图像的 GAN 和用于生成图 2 中图像的 AEGAN 对于<em class="nf"> G </em>和<em class="nf"> Dx </em>具有完全相同的架构；唯一的区别是，AEGAN 也学习相反的功能。这稳定了训练过程。在你问之前，不，这不是一次性的侥幸；我对 GAN 和 AEGAN 重复了五次训练，在每种情况下，AEGAN 产生了良好的结果，GAN 产生了垃圾。</p><p id="7159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AEGAN 技术的一个令人兴奋的副作用是它允许在真实样本之间进行直接插值。众所周知，GANs 能够在样本之间进行插值；画两个随机向量<em class="nf"> z1 </em>和<em class="nf"> z2 </em>，在向量之间进行插值，然后将插值结果反馈给发电机和<a class="ae ky" href="https://www.youtube.com/watch?v=djsEKYuiRFE" rel="noopener ugc nofollow" target="_blank">吊臂</a>！借助 AEGAN，我们可以在真实样本之间进行插值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/6f26f56cbcf5a4c0909e47a5f2c17874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4MUKv3nI__sgmtl6BoZ08g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:真实样本之间的插值。最左边和最右边的列是真实样本，而中间的图像是这些样本之间的插值。最下面一行显示了一个样本和水平镜像的同一样本之间的插值，给人一种角色正在转头的错觉。</p></figure><p id="ddb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为编码器<em class="nf"> E </em>能够将样本<em class="nf"> x </em>映射到其在潜在空间中的对应点<em class="nf"> z </em>，所以 AEGAN 允许我们为任何样本<em class="nf"> x1 </em>和<em class="nf"> x2 </em>找到点<em class="nf"> z1 </em>和<em class="nf"> z2 </em>并在它们之间进行插值，就像对典型 GAN 一样。图 5 显示了来自数据集的 50 个随机样本的重建:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/192d0ee7f48073e93e6af8f4178a0d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*iW_tVOazqlbB5yTjZZYFUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:排列在五列中的真实样本对(每对中的左边)及其重建(每对中的右边)。在这个意义上，AEGAN 的功能类似于自动编码器，但没有标志性的模糊。请注意，重建的眼睛颜色都是各种深浅不同的绿色，这是一种轻微的模式崩溃。</p></figure><h2 id="09c3" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">讨论</h2><p id="91be" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">首先，我想指出这个实验的缺点。正如我所说的，这只是在一个数据集上测试的。各个网络<em class="nf"> G </em>、<em class="nf"> E </em>、<em class="nf"> Dx </em>和<em class="nf"> Dz </em>的结构也没有被广泛探索，也没有进行有意义的超调(关于层数或形状、λs 等。).网络本身相当简单；一个更彻底和公平的实验是将 AEGAN 技术作为包装器应用于更复杂数据集上的更强大的 GAN，如<a class="ae ky" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank"> CelebA </a>。</p><p id="848f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，AEGAN 具有许多令人满意的理论特性，这使得它适合于进一步的探索。</p><ul class=""><li id="8b8d" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated">强制 AEGAN 保留生成图像中潜在向量的信息，可以防止模式崩溃。这也允许我们避免像批量标准化和批量区分这样的批量独立性破坏技术。顺便说一下，由于 TF.keras 2.0 中的实现问题，我不得不在这个实验中避免批处理规范化，但那是另外一个故事了…</li><li id="a075" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">学习双射函数允许在真实样本之间直接插值，而不依赖于辅助网络或可逆层。它还可以允许更好地探索和操纵潜在空间，可能通过试验不同的分布，就像在<a class="ae ky" href="https://arxiv.org/abs/1511.05644" rel="noopener ugc nofollow" target="_blank">敌对自动编码器</a>中所做的那样。</li><li id="4e3f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">将发生器直接暴露在真实样本下，可以减少它在像素空间的深渊中跋涉的时间。</li></ul><p id="7389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于最后一点，常规 GANs 的生成器从不直接接触训练数据，只通过鉴别器的反馈间接了解数据的样子(因此有了“<a class="ae ky" href="https://www.forbes.com/sites/bernardmarr/2019/06/12/artificial-intelligence-explained-what-are-generative-adversarial-networks-gans/#b915f137e007" rel="noopener ugc nofollow" target="_blank">blind forgers</a>”)的绰号。通过包括重建损失，生成器可以在高维像素空间中直线朝向低维流形。考虑图 6，它显示了 AEGAN 仅经过 200 步训练后的输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9c1ea1f31d23c9809c7c99bd205ebb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*4ccDM2eCje6rsCR3Mq6s_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:200 个训练步骤(不是 200 个时期)后的 AEGAN 输出。人脸清晰可见，头发和眼睛的颜色、表情和姿势也各不相同。</p></figure><p id="fdce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将其与图 7 进行比较，图 7 显示了一个与 AEGAN 具有相同架构的常规 GAN 在其训练中的相同点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/eba4ad6fb83a3059d3945388f40e7111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*dcxb5Bx9Co1ItOp-PJpyVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:200 个训练步骤(不是 200 个历元)后的 GAN 输出。因为你已经知道这些是脸，你可以看到头发，皮肤和眼睛，但有大量的模式崩溃和图像质量非常低。</p></figure><p id="3388" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，AEGAN 在寻找低维流形方面特别有效，尽管测量它适应该流形的能力需要进一步的实验。</p><h2 id="bcb7" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">进一步的工作</h2><ul class=""><li id="372e" class="ng nh it lb b lc na lf nb li nz lm oa lq ob lu nu nm nn no bi translated">将 AEGAN 应用于 StyleGAN 等最先进的技术，看看它是否能提高质量和/或收敛速度。</li><li id="165b" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">探索λ超参数以找到最佳值；探索课程方法，如随时间逐渐降低λs。</li><li id="0a94" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">将<a class="ae ky" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">条件</a>应用到培训中；探索伯努利和多伯努利潜在组件，就像在<a class="ae ky" href="https://arxiv.org/abs/1511.05644" rel="noopener ugc nofollow" target="_blank">敌对自动编码器</a>中所做的那样。</li><li id="cacf" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">将 AEGAN 应用于具有已知基础流形的设计图像数据集，以测量该技术能够多有效地再现它。</li><li id="4935" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nu nm nn no bi translated">找到一种方法来匹配潜在空间的维度和数据生成函数的流形的维度(说起来容易做起来难！)</li></ul><h2 id="bdbf" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">正误表</h2><p id="e80b" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">如果我没有在某个地方提到<a class="ae ky" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">变量自动编码器/ GANs </a>，这是一个有趣的相关技术，所以在这里。用于训练这些模型的数据可在<a class="ae ky" href="https://www.kaggle.com/soumikrakshit/anime-faces" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上获得。你可以在这里查看原论文<a class="ae ky" href="https://arxiv.org/abs/2004.05472" rel="noopener ugc nofollow" target="_blank">。我的这个网络的 tf.keras 实现可以从下面的 github repo 获得:</a></p><div class="oc od gp gr oe of"><a href="https://github.com/ConorLazarou/AEGAN-keras" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">ConorLazarou/AEGAN-keras</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">自动编码生成对抗网络(AEGAN)技术的 Keras 实现。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">github.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div></div></div>    
</body>
</html>