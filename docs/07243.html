<html>
<head>
<title>Visual Question Answering with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有深度学习的视觉问答</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visual-question-answering-with-deep-learning-2e5e7cbfdcd4?source=collection_archive---------18-----------------------#2020-06-02">https://towardsdatascience.com/visual-question-answering-with-deep-learning-2e5e7cbfdcd4?source=collection_archive---------18-----------------------#2020-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0292" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本博客包含在 Keras/Tensorflow 中实现“面向视觉问答的分层问题-图像共同关注”论文。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a294fffa5f4c2f181564453a35f05b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YvRS_XUNw5JJdE0o"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查尔斯·德鲁维奥在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="67a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目录:</p><ol class=""><li id="77de" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="#1719" rel="noopener ugc nofollow">简介</a></li><li id="a52d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#2c5f" rel="noopener ugc nofollow">商业问题</a></li><li id="32e2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#51eb" rel="noopener ugc nofollow">了解数据</a></li><li id="1af4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#cca2" rel="noopener ugc nofollow">将现实世界的问题映射到 ML/DL 问题</a></li><li id="4cc7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#140c" rel="noopener ugc nofollow">了解模型</a></li><li id="109d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#f035" rel="noopener ugc nofollow">实施细节</a></li><li id="083e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#e44d" rel="noopener ugc nofollow">代码</a></li><li id="3f0d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#5371" rel="noopener ugc nofollow">结果</a></li><li id="a350" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#8324" rel="noopener ugc nofollow">结论和未来工作</a></li><li id="8439" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#c49d" rel="noopener ugc nofollow">参考文献</a></li></ol></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1719" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">1.简介:</h1><p id="a19d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">视觉问答是一个关于建立一个人工智能系统来回答用自然语言提出的关于图像的问题的研究领域。</p><p id="843c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决这一任务的系统展示了对图像的更一般的理解:它必须能够回答关于图像的完全不同的问题，有时甚至可以处理图像的不同部分。</p><p id="6ad9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看几个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2985c79ed0227b5b4685f9392c9a7162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkKBxWdBcTuJZOaOuMM98Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:原文<a class="ae ky" href="http://arxiv.org/pdf/1512.02167.pdf" rel="noopener ugc nofollow" target="_blank"> VQA 论文</a></p></figure><p id="42f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有的图像，我们的人工智能系统应该能够定位问题中提到的主题并检测到它，并且应该有一些常识来回答它。</p><p id="1738" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在第一张图片中，对于问题“<em class="no">胡子是由什么组成的？“我们的人工智能系统应该能够判断出所指的对象是女性的脸，更具体地说是她嘴唇周围的区域，并且应该能够检测到香蕉。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="2c5f" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">2.业务问题:</h1><p id="a56a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">作为人类，我们很容易看到一幅图像，并利用我们的常识回答关于它的任何问题。然而，也有这样的情况，例如，视力受损的用户或智力分析师，他们想要主动地引出给定图像的视觉信息。</p><p id="ac04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们建立了一个人工智能系统，它将图像和关于图像的自由形式、开放式或自然语言问题作为输入，并产生自然语言答案作为输出。</p><p id="52cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">系统会在以下几个方面回答类似人类的问题:</p><ol class=""><li id="9cb0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">它将从输入(分别是图像和问题)中学习视觉和文本知识</li><li id="ddbc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">合并两个数据流</li><li id="e570" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用这些高级知识来得出答案</li></ol></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="51eb" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">3.理解数据:</h1><p id="f6a8" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">VQA 是一个包含关于图像的开放式问题的数据集。这些问题需要了解视觉、语言和常识知识才能回答。</p><ul class=""><li id="2b0e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu np mb mc md bi translated">82，783 张图像(可可列车图像)</li><li id="3965" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">每张图片至少 3 个问题(平均 5.4 个问题)(443，757 个问题)</li><li id="1b9a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">每个问题 10 个真实答案(443，7570 个答案)来自不同的员工</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="cca2" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">4.将现实世界的问题映射到 ML/DL 问题:</h1><ul class=""><li id="a588" class="lv lw it lb b lc ni lf nj li nq lm nr lq ns lu np mb mc md bi translated"><strong class="lb iu">机器学习问题的类型:</strong>我们将手头的问题提出为 K 类分类问题；其中 K 是数据集中一组固定答案类型的数量。</li><li id="9468" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated"><strong class="lb iu">性能指标:</strong>我们通过正确回答问题的数量来评估我们的人工智能系统。使用以下精度指标:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1facd1db97d9a2f18b30106e6c45c369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*kkcExx-vLW3IYkdJFlrSEg.jpeg"/></div></figure><ul class=""><li id="f8ae" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu np mb mc md bi translated"><strong class="lb iu">约束:</strong>没有严格的延迟要求。</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="140c" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">5.了解模型:</h1><p id="55b2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">让我们试着去理解<a class="ae ky" href="https://arxiv.org/pdf/1606.00061" rel="noopener ugc nofollow" target="_blank">《视觉问答的分层问题-图像共同关注》</a>一文中提出的方法。</p><h2 id="081a" class="nu mr it bd ms nv nw dn mw nx ny dp na li nz oa nc lm ob oc ne lq od oe ng of bi translated">论文概述:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/4cfcf12508cda3e99aecbe53e6f7eba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeWlYmV3qS8Dxs5vAijkKQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1606.00061" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.00061</a></p></figure><p id="6761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之前，所有关于 VQA 的论文主要集中在视觉注意力上。本文建议也关注问题注意。具体而言，本文提出了一种新颖的 VQA 多模态注意力模型，该模型具有以下两个独特的特征:</p><ol class=""><li id="053e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">共同注意:</strong>本文提出了一种新的机制，它联合推理视觉注意和问题注意，称为<strong class="lb iu">共同注意</strong>。更具体地，图像表示用于引导问题注意力，问题表示用于引导图像注意力。</li><li id="40db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">问题层次:</strong>构建一个层次结构，在三个层次上共同关注图像和问题:(a)单词层，(b)短语层，和(c)问题层。</li></ol><p id="a019" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a) <strong class="lb iu">在单词级</strong>，通过嵌入矩阵将单词嵌入到向量空间中。</p><p id="2862" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b) <strong class="lb iu">在短语级</strong>，一维卷积神经网络被用于具有不同支持的时间过滤器的单词表示，以捕获包含在单个词、双词和三词中的信息，然后通过将它们汇集成单个短语级表示来组合各种 n 元词响应。</p><p id="c27c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">c) <strong class="lb iu">在问题级</strong>，使用递归神经网络对整个问题进行编码。</p><p id="4421" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个层次结构中问题表示的每个级别，构建联合问题和图像共同关注图，然后递归地组合这些图以最终预测答案的分布。</p><h2 id="036c" class="nu mr it bd ms nv nw dn mw nx ny dp na li nz oa nc lm ob oc ne lq od oe ng of bi translated">方法:</h2><p id="91df" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">该论文提出了两种共同注意机制，这两种机制在图像和问题注意图的生成顺序上有所不同。第一种机制称为<strong class="lb iu">并行共同注意</strong>，它同时产生图像和问题注意。第二种机制叫做<strong class="lb iu">交替共同注意</strong>，它依次在生成图像和问题注意之间交替。这些共同关注机制在问题层级的所有三个级别上执行。</p><p id="b19b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">在这篇博客中，我们将讨论并行共同注意模型</em>的实现。</p><h2 id="4a43" class="nu mr it bd ms nv nw dn mw nx ny dp na li nz oa nc lm ob oc ne lq od oe ng of bi translated">平行共同关注:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6f56c27d3549c6ef34a7b8289ed33547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*9WbYfXBnSnDc0cpQ-YSUJA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1606.00061" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.00061</a></p></figure><p id="f297" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平行共同注意同时注意图像和问题。通过计算所有图像位置和问题位置对处的图像和问题特征之间的相似性来连接图像和问题。具体来说，给定一个图像特征图<em class="no"> V∈ R </em> ^(d×N)和问题表示<em class="no"> Q∈ R^( </em> d×T)，我们计算一个叫做亲和矩阵<em class="no"> C∈ R^( </em> T×N)的东西如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0c37fd026328b12942219e84b663f61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*pUHJSILzulNcgid7A_U4GQ.jpeg"/></div></figure><p id="432b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这个相似性矩阵 C 作为一个特征，图像和问题注意力图以下面的方式被预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/f5e6935ed86717a329a80108ad25c6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjShtJs5RDTU0xQJL6EAjw.jpeg"/></div></div></figure><p id="9524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于上述关注权重，图像和问题关注向量被计算为图像特征和问题特征的加权和，即，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f2a0d4ded365269cce5f426b29594dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*zG16uDpg-MAewyylTIh9sw.jpeg"/></div></figure><p id="e852" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并行的共同关注在层次结构的每一层完成，导致<strong class="lb iu"> vʳ </strong>和<strong class="lb iu"> qʳ </strong>其中<em class="no"> r ∈ {w，p，s} </em>。</p><p id="4754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，来自所有三个级别的共同参与的图像和问题特征被递归地组合，以最终预测答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/529611de6325c039b8ab394530dae388.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*beEjSF--zWjBvszIHOAkFw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1606.00061" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.00061</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cf4677b7983dc7f5c24d490a45b7ad89.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*CFsW32DvlONX5eKYA3xTuQ.jpeg"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="f035" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">6.实施细节:</h1><ul class=""><li id="0ad9" class="lv lw it lb b lc ni lf nj li nq lm nr lq ns lu np mb mc md bi translated">我们不直接使用图像作为模型的输入。将图像缩放到 224× 224，然后提取 VGGNet19 的最后一个 CONV 层的激活。形状[512 x 7 x 7]的这些激活被用作图像的输入特征。</li><li id="00dc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">我们使用 KERAS 标记器来提取问题特征。</li><li id="27ea" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">最终的输入特征是:a)形状[49，512]的图像特征和 b)形状[22，]的问题特征，其中 22 是预处理后问题的序列长度。</li><li id="56f9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">我们使用基本学习率为 1e-4 的 ADAM 优化器。</li><li id="b241" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">我们将批量大小设置为 300，训练 60 个时期。</li><li id="4e5a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">我们在每一层都使用正则化。</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e44d" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">7.代码:</h1><h2 id="ffce" class="nu mr it bd ms nv nw dn mw nx ny dp na li nz oa nc lm ob oc ne lq od oe ng of bi translated">建筑:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="c785" class="nu mr it bd ms nv nw dn mw nx ny dp na li nz oa nc lm ob oc ne lq od oe ng of bi translated">自定义图层:</h2><p id="7aba" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">一些自定义层用于定义上面的模型。下面是自定义层的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5371" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">8.结果:</h1><p id="2f18" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">该论文在 VQA 2.0 数据集上的准确率为 54%。我的实现准确率是 49.28%。</p><p id="3bb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是工作模型的演示视频:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oo l"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="8324" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">9.结论和未来工作:</h1><p id="879b" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们仅用了 60 个历元就达到了接近报道的论文精度的精度。为了获得与论文相同的准确性，我们可以尝试为更大的时期训练模型，并具有学习率衰减。</p><p id="dc69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将图像缩放到 448× 448，然后从 VGGNet19 的最后一个 CONV 层中提取[512 x 14 x14]激活作为图像特征，也可以提高性能。</p><p id="14be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。谢谢你阅读我的博客。如果你有任何想法，请留下评论、反馈和建议。</p><p id="3e62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的 GitHub repo 上的完整代码，<a class="ae ky" href="https://github.com/arya46/VQA_HieCoAtt" rel="noopener ugc nofollow" target="_blank"> <em class="no">这里</em> </a>。</p><p id="5b47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在领英上找到我，<a class="ae ky" href="http://www.linkedin.com/in/tulrose" rel="noopener ugc nofollow" target="_blank">T5 这里 T7】。</a></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c49d" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">10.参考资料:</h1><ul class=""><li id="412f" class="lv lw it lb b lc ni lf nj li nq lm nr lq ns lu np mb mc md bi translated">贾森·卢，，德鲁夫·巴特拉和德维·帕里克，<a class="ae ky" href="https://arxiv.org/pdf/1606.00061.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉问题回答的分层问题-图像共同注意</a> (2016)</li><li id="cf7c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated"><a class="ae ky" href="https://github.com/ritvikshrivastava/ADL_VQA_Tensorflow2" rel="noopener ugc nofollow" target="_blank">https://github.com/ritvikshrivastava/ADL_VQA_Tensorflow2</a></li><li id="98bd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated"><a class="ae ky" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ul><p id="daf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="#67a5" rel="noopener ugc nofollow">回到榜首^ </a></p></div></div>    
</body>
</html>