<html>
<head>
<title>Top Trends of Graph Machine Learning in 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年图形机器学习的主要趋势</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-trends-of-graph-machine-learning-in-2020-1194175351a3?source=collection_archive---------8-----------------------#2020-02-03">https://towardsdatascience.com/top-trends-of-graph-machine-learning-in-2020-1194175351a3?source=collection_archive---------8-----------------------#2020-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/32087c11d1134ad5912cb00bfd3650fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fj0k2m-9E04QtKe28IVg3g.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="http://shutterstock.com" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c1eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2020年刚刚开始，但我们已经可以在最新的研究论文中看到图机器学习(GML)的趋势。以下是我对2020年GML将面临的重要问题的看法，以及对这些论文的讨论。</p><h1 id="891a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">介绍</h1><p id="4fae" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">本文的目标不是介绍GML的基本概念，如图形神经网络(GNNs)，而是展示我们在顶级科学会议上可以看到的前沿研究。首先，我把作品提交给了ICLR 2020，这是GML最负盛名的提交作品的会议之一。在<a class="ae kf" href="https://medium.com/@sergei.ivanov_24894/iclr-2020-graph-papers-9bc2e90e56b0" rel="noopener">之前的帖子</a>中，我已经描述了这个领域的一些快速统计数据，但这里有一个简短的版本:</p><blockquote class="mh mi mj"><p id="bc56" class="kg kh mk ki b kj kk kl km kn ko kp kq ml ks kt ku mm kw kx ky mn la lb lc ld im bi translated">在GML有150篇投稿，三分之一的论文被接受。这相当于所有被接受论文的大约10%。</p></blockquote><p id="c65a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我阅读了大部分GML的论文，以下是我列出的2020年趋势:</p><ol class=""><li id="9d16" class="mo mp it ki b kj kk kn ko kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="ki iu">对GNN的理论理解更加扎实；</strong></li><li id="dc71" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="ki iu">GNN新酷应用；</strong></li><li id="818e" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="ki iu">知识图谱变得更受欢迎；</strong></li><li id="7915" class="mo mp it ki b kj mx kn my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="ki iu">图嵌入的新框架。</strong></li></ol><p id="af7d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看这些趋势。</p><h1 id="3982" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">1.对GNN的理论理解更加扎实</h1><p id="bb48" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我特别兴奋地看到这一趋势，因为它表明GML地区正在成熟，以前的启发式方法正在被新的理论解决方案所取代。关于图形神经网络还有很多需要理解的地方，但是关于GNNs如何工作已经有了相当多的重要结果。</p><p id="62ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我先说我最喜欢的:<a class="ae kf" href="https://openreview.net/forum?id=B1l2bp4YwS" rel="noopener ugc nofollow" target="_blank"> <em class="mk">神经网络学不到的图形:深度vs宽度</em> </a>作者Andreas Loukas。本文在技术简单性、高度的实际影响和深远的理论见解之间取得了惊人的平衡。</p><blockquote class="mh mi mj"><p id="7f58" class="kg kh mk ki b kj kk kl km kn ko kp kq ml ks kt ku mm kw kx ky mn la lb lc ld im bi translated">它表明，节点嵌入的维数(网络的宽度，<code class="fe nc nd ne nf b">w</code>)乘以层数(网络的深度，<code class="fe nc nd ne nf b">d</code>)应该与图的大小<code class="fe nc nd ne nf b">n</code>，即<code class="fe nc nd ne nf b">dw = O(n)</code>成比例，如果我们希望GNN能够计算出流行的图问题(如循环检测、直径估计、顶点覆盖等)的解决方案。).</p></blockquote><p id="f929" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，GNN的许多当前实现无法实现这一条件，因为层的数量(在许多实现中约为2–5)和嵌入的维度(约为100–1000)与图的大小相比不够大。另一方面，在当前的环境下，更大的网络太令人望而却步，这提出了一个问题，即我们应该如何设计高效的GNNs这是我们需要在未来解决的问题。很有说服力的是，这篇论文也从80年代的分布式计算模型中得到启发，表明gnn本质上做同样的事情。里面有更多的结果，所以我建议看一看。</p><p id="6290" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，另外两篇论文，乌诺&amp;铃木和巴塞洛等人，研究了GNNs的力量。第一个，<a class="ae kf" href="https://openreview.net/forum?id=S1ldO2EFPr" rel="noopener ugc nofollow" target="_blank"> <em class="mk">图神经网络指数级地失去对节点分类的表达能力</em> </a>，表明:</p><blockquote class="mh mi mj"><p id="fc52" class="kg kh mk ki b kj kk kl km kn ko kp kq ml ks kt ku mm kw kx ky mn la lb lc ld im bi translated">在某些权重条件下，当层数增加时，除了节点度数和连通分量(由拉普拉斯谱确定)之外，GCNs不能学习任何东西。</p></blockquote><p id="dc66" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个结果是众所周知的性质的推广，即Markov过程收敛到唯一的平衡点，其中收敛速度由转移矩阵的特征值决定。</p><p id="d704" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第二篇论文<a class="ae kf" href="https://openreview.net/forum?id=r1lZ7AEKvB" rel="noopener ugc nofollow" target="_blank"> <em class="mk">图神经网络的逻辑表达能力</em> </a>中，作者展示了GNNs和它们可以捕获的节点分类器类型之间的联系。我们已经知道，一些GNNs和同构的WL测试一样强大，即两个节点被WL着色为相同当且仅当它们被GNNs分类为相同。但是GNN能捕捉到其他分类功能吗？例如，想象一个布尔函数，当且仅当一个图有一个孤立的顶点时，它将true赋给所有的节点。GNNs能抓住这个逻辑吗？直觉上不会，因为GNN是一种消息传递机制，如果图的一部分和另一部分(两个相连的组件)之间没有链接，则两者之间不会传递消息。因此，一个简单的解决方案是在邻域聚合后添加一个读出操作，这样当每个节点更新所有要素时，它就拥有了图中所有其他节点的信息。</p><p id="b258" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其他理论方面的工作包括<a class="ae kf" href="https://openreview.net/forum?id=rkeIIkHKvS" rel="noopener ugc nofollow" target="_blank">侯等人</a>对的图信息使用的测量以及<a class="ae kf" href="https://openreview.net/forum?id=SJxzFySKwH" rel="noopener ugc nofollow" target="_blank"> Srinivasan &amp; Ribeiro </a>的基于角色和基于距离的节点嵌入的等价性。</p><h1 id="b2c5" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">2.GNN的新酷应用</h1><p id="9a35" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">看到GNNs如何应用于现实世界的任务也很棒。今年有一些应用程序可以修复Javascript中的错误、玩游戏、回答类似智商的测试、优化TensorFlow计算图、分子生成和对话系统中的问题生成。</p><p id="2ec6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae kf" href="https://openreview.net/forum?id=SJeqs6EFvB" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> HOPPITY:学习图变换来检测和修复程序中的bug</em></a><em class="mk"/>Dinella等人<strong class="ki iu">提出了</strong> <strong class="ki iu">一种在Javascript代码</strong>中同时检测和修复bug的方法。代码被转换成抽象语法树，然后通过GNN预处理以获得代码嵌入。其思想是给定一个处于初始状态的图，通过多轮图编辑操作符(添加或删除节点，替换节点值或类型)来修改它。为了理解图中的哪些节点应该被修改，他们采用了指针网络，该指针网络采用图嵌入和迄今为止的编辑历史，并选择节点。然后，使用LSTM网络执行修复，该网络也采用图形嵌入和编辑的上下文。作者在GitHub的提交上验证了这种方法，显示了对其他不太通用的基线的显著提升。精神上类似魏等人的工作<a class="ae kf" href="https://openreview.net/forum?id=Hkx6hANtwH" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> LambdaNet:使用图神经网络的概率类型推断</em> </a> <em class="mk"> </em>研究<strong class="ki iu">如何为Python或TypeScript等语言推断变量的类型。</strong>作者提出了一种类型依赖超图，它包含作为节点的程序变量以及它们之间的关系，例如逻辑(例如布尔类型)或上下文(例如相似的变量名)约束。然后，首先训练GNN模型，以产生图的变量和可能类型的嵌入，然后使用这些嵌入来预测具有最高可能性的类型。在实验中，LambdaNet在标准变量类型(例如boolean)和用户定义的类型上都有较高的性能。</p><p id="711e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">王等人<a class="ae kf" href="https://openreview.net/forum?id=ByxQB1BKwH" rel="noopener ugc nofollow" target="_blank"> <em class="mk">的一篇论文用多重图网络</em> </a>进行抽象图推理，展示了<strong class="ki iu">如何在类智商测验</strong> <strong class="ki iu">(瑞文递进矩阵(RPM)和图三段论(DS))中用GNNs进行推理。</strong>在RPM任务中，为矩阵的每一行组成一个图，对于该图，通过前馈模型获得边缘嵌入，随后是图摘要。由于最后一行有8个可能的答案，因此创建了8个不同的图表，每个图表都与前两行连接在一起，以获得ResNet模型对IQ分数的预测。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/ae1bda4bf74e7d524d0aec373e1aa11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X4v8Kra8sG6mzBPwxmpWGg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://openreview.net/forum?id=ByxQB1BKwH" rel="noopener ugc nofollow" target="_blank">王等</a></p></figure><p id="66b5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DeepMind的一篇论文<a class="ae kf" href="https://openreview.net/forum?id=rkxDoJBYPB" rel="noopener ugc nofollow" target="_blank"> <em class="mk">增强遗传算法学习优化计算图</em> </a> <em class="mk"> </em>提出<strong class="ki iu">一种RL算法优化张量流计算图的代价。</strong>图通过标准的消息传递GNN进行处理，产生与图中每个节点的调度优先级相对应的离散嵌入。这些嵌入被送入遗传算法BRKGA，该算法决定每个节点的设备放置和调度。该模型被训练以优化所获得的张量流图的实际计算成本。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/79f164fc5f95a2084132cfbed4216bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6EUvGW9sQlYfLms4SIZzQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><em class="nm">“用于优化计算图的增强型遗传算法学习”</em>作者<a class="ae kf" href="https://openreview.net/forum?id=rkxDoJBYPB" rel="noopener ugc nofollow" target="_blank"> Paliwal等人</a></p></figure><p id="ffe2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其他有趣的应用包括史等人的<a class="ae kf" href="https://openreview.net/forum?id=S1esMkHYPr" rel="noopener ugc nofollow" target="_blank">分子生成</a>、江等人的<a class="ae kf" href="https://openreview.net/forum?id=HkxdQkSYDB" rel="noopener ugc nofollow" target="_blank">游戏、陈等人的</a><a class="ae kf" href="https://openreview.net/forum?id=HygnDhEtvr" rel="noopener ugc nofollow" target="_blank">对话系统</a></p><h1 id="b360" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">3.知识图表变得更加流行</h1><p id="98ba" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">今年有不少关于知识图推理的论文。本质上，知识图是一种表示事实的结构化方式。与一般的图不同，在知识图中，节点和边实际上有一些含义，如演员的名字或电影中的表演(见下图)。知识图上的一个常见问题是回答复杂的查询，如“2000年前史蒂文·斯皮尔伯格的哪些电影获得了奥斯卡奖？”，可以翻译成一个逻辑查询∨ { <code class="fe nc nd ne nf b">Win(Oscar, V)</code> ∧ <code class="fe nc nd ne nf b">Directed(Spielberg, V)</code> ∧ <code class="fe nc nd ne nf b">ProducedBefore(2000, V)</code> }。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/175ad7db34189216b3e4c230a40a35ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sa0mx2u4EpOW1hgbPv9T7A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">知识图示例。(来源:<a class="ae kf" href="https://arxiv.org/abs/1503.00759" rel="noopener ugc nofollow" target="_blank">尼克尔等人</a>)</p></figure><p id="6a00" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在论文<a class="ae kf" href="https://openreview.net/forum?id=BJgr4kSFDS" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> Query2box:使用盒子嵌入对向量空间中的知识图进行推理</em> </a> <em class="mk"> </em>中，任等人<strong class="ki iu">提出将查询嵌入到潜在空间中，而不是作为单个点，而是作为矩形盒子。</strong>该方法允许执行自然交集操作，即合取∧，因为它产生新的矩形框。然而，对并集(即析取∨)建模并不那么简单，因为它可能会导致非重叠区域。此外，为了精确地对任何具有嵌入的查询建模，由VC维测量的嵌入之间的距离函数的复杂度应该与图中实体的数量成比例。相反，有一个很好的技巧可以将析取查询替换为DNF形式，其中联合只发生在计算图的末尾，这有效地简化了每个子查询的简单距离计算。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/11b2d34b474d3bca35408d0bd7e8a0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rZW3kW5VHN6lcaGypX2Zg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><em class="nm"/><a class="ae kf" href="https://openreview.net/forum?id=BJgr4kSFDS" rel="noopener ugc nofollow" target="_blank"><em class="nm">任等</em> </a>【查询2框:利用框嵌入对向量空间中的知识图进行推理】</p></figure><p id="6c9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在类似的主题中，Wang等人<strong class="ki iu">在标题为<a class="ae kf" href="https://openreview.net/forum?id=rJleKgrKwS" rel="noopener ugc nofollow" target="_blank"> <em class="mk">“知识图中数值规则的可微分学习”</em> </a> <em class="mk">的论文中提出了一种处理数值实体和规则</em></strong>的方法。例如，对于一个引用知识图，您可以有一个规则<code class="fe nc nd ne nf b">influences(Y,X)</code> ← <code class="fe nc nd ne nf b">colleagueOf(Z,Y)</code> ∧ <code class="fe nc nd ne nf b">supervisorOf(Z,X)</code> ∧ <code class="fe nc nd ne nf b">hasCitation&gt;(Y,Z)</code>，该规则指出，通常情况下，学生<code class="fe nc nd ne nf b">X</code>受到他们的主管<code class="fe nc nd ne nf b">Z</code>的同事<code class="fe nc nd ne nf b">Y</code>的影响，该同事引用的次数更多。该规则右侧的每个关系可以表示为矩阵，并且寻找缺失链接的过程可以表述为关系与实体向量的连续矩阵乘法，该过程称为规则学习。由于矩阵的构造方式，神经方法只能使用分类规则，如<code class="fe nc nd ne nf b">colleagueOf(Z,Y)</code>。作者的贡献是以一种新的方式有效地使用数字规则，如<code class="fe nc nd ne nf b">hasCitation&gt;(Y,Z)</code>和求反运算符，通过表明在现实中没有必要显式地物化这样的矩阵，这大大减少了运行时间。</p><p id="4399" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个在机器学习领域更频繁出现的主题是<strong class="ki iu">对现有模型的重新评估，以及它们在公平环境中的表现。像这样的论文<a class="ae kf" href="https://openreview.net/forum?id=BkxSmlBFvr" rel="noopener ugc nofollow" target="_blank"> <em class="mk">可以教老狗新把戏！Ruffinelli等人的关于训练知识图嵌入</em> </a>的研究表明，新模型的性能通常取决于实验训练的“次要”细节，例如损失函数、正则化子和采样方案的形式。在一项大型消融研究中，作者观察到，旧方法(如重新标度模型)仅通过适当调整超参数就可以实现SOTA性能。</strong></p><p id="33be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个领域还有许多其他有趣的作品。<a class="ae kf" href="https://openreview.net/forum?id=SygcSlHFvS" rel="noopener ugc nofollow" target="_blank"> Allen等人</a>基于单词嵌入的最新见解，了解更多关于关系和实体的学习表征的潜在空间。<a class="ae kf" href="https://openreview.net/forum?id=SJgVHkrYDH" rel="noopener ugc nofollow" target="_blank"> Asai et al. </a>展示了一个模型如何在回答给定查询的维基百科图上检索推理路径。<a class="ae kf" href="https://openreview.net/forum?id=S1g8K1BFwS" rel="noopener ugc nofollow" target="_blank">塔巴科夫&amp;科斯塔贝罗</a>触及了图嵌入模型的概率校准这个重要话题。他们表明，通过用sigmoid函数变换逻辑来获得概率的流行嵌入模型TransE和ComplEx校准得很差，即要么低估要么高估事实的存在。他们的方法依赖于生成被破坏的三元组作为负数，这些负数被已知的方法使用，例如Platt标度和保序回归来校准概率。</p><h1 id="f634" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">4.图嵌入的新框架</h1><p id="8a33" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">图形嵌入是图形机器学习的一个长期话题，今年有了关于我们应该如何学习图形表示的新观点。</p><p id="d67d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Deng等人<strong class="ki iu">在工作<a class="ae kf" href="https://openreview.net/forum?id=r1lGO0EKDH" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> GraphZoom:一种用于精确和可扩展图嵌入的多级谱方法</em> </a>中提出了一种在节点分类问题中改进任何无监督嵌入方法</strong>的运行时间和准确性的方法。总体思想是首先将原始图缩减为更小的图，这样可以快速计算节点嵌入，然后恢复原始图的嵌入。开始时，基于属性相似性，用对应于节点的k个最近邻居之间的链接的附加边来扩充原始图。然后，对图进行粗化:通过局部谱方法将每个节点投影到一个低维空间，并聚集成簇。任何无监督的图嵌入方法，如DeepWalk或Deep Graph Infomax，都可以用来获得小图上的节点嵌入。在最后一步中，所获得的节点嵌入(其本质上代表聚类的嵌入)用平滑算子迭代地广播回来，以防止不同的节点具有相同的嵌入。在实验中，GraphZoom框架比node2vec和DeepWalk方法实现了惊人的40倍加速，准确率提高了10%。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/13ad02b5a4be7731c6b99433a6d444ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKCJiCqGKIRR_gQocw-Lcw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">“GraphZoom:一种用于精确和可扩展图形嵌入的多级谱方法”，作者<a class="ae kf" href="https://openreview.net/forum?id=r1lGO0EKDH" rel="noopener ugc nofollow" target="_blank">邓等人</a></p></figure><p id="a189" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一些论文已经仔细研究了图分类问题的先前结果。<a class="ae kf" href="https://openreview.net/forum?id=HygDF6NFPB" rel="noopener ugc nofollow" target="_blank"><em class="mk">Errica等人对用于图形分类的图形神经网络</em> </a>的公平比较有助于对关于该问题的GNN模型的公平重新评估，<strong class="ki iu">表明，不利用图形拓扑的简单基线(即，它在聚集节点特征上工作)的性能与SOTA GNNs相当。</strong>这一令人惊讶的现象显然在2015年由<a class="ae kf" href="https://sites.google.com/site/feast2015/program/accepted-abstracts" rel="noopener ugc nofollow" target="_blank">奥尔洛娃等人</a>发表过，但并未获得大量读者。这项工作的一个很好的结果是在流行的数据集上的公平的SOTA结果和PyTorch-Geometric中的方便的代码库来运行未来论文的模型比较。在我们的工作<a class="ae kf" href="https://openreview.net/forum?id=rJlUhhVYvS" rel="noopener ugc nofollow" target="_blank"> <em class="mk">理解图数据集中的同构偏差</em> </a> <em class="mk">，</em>中，我们还发现，在常用的数据集如MUTAG或IMDB中，许多图具有同构副本，即使考虑节点属性。此外，在这些同构图中，<strong class="ki iu">许多具有不同的目标标签，这自然为分类器引入了标签噪声。</strong> <a class="ae kf" href="https://medium.com/@sergei.ivanov_24894/new-graph-classification-data-sets-43e134340d2d" rel="noopener">这表明</a>使用网络的所有可用元信息(例如节点或边属性)对于模型的更好性能的重要性。另一部作品<a class="ae kf" href="https://openreview.net/forum?id=BJxQxeBYwH" rel="noopener ugc nofollow" target="_blank"> <em class="mk">强大的图形神经网络有必要吗？陈等人对图分类</em> </a> <em class="mk"> </em>的剖析表明，如果用其线性对应物(包括邻居的度和传播的图属性)替换非线性邻域聚合函数，则模型的性能不会降低，这与先前的陈述一致，即许多图数据集对于分类来说是微不足道的，并且提出了对于该任务的适当验证框架的问题。</p><h1 id="55cf" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="6332" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">随着顶级会议的提交率不断增长，我们可以预计2020年GML领域将出现许多有趣的结果。我们已经可以看到这个领域的转变，从深度学习在图上的启发式应用到更合理的方法和关于图模型范围的基本问题。GNNs找到了自己的位置，成为许多可以用图来表达的实际问题的有效解决方案，但我希望总体而言，GML只是触及了我们在图论和机器学习的交叉点上所能实现的表面，我们应该继续关注即将到来的结果。</p><p id="8f25" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mk"> P.S .我会继续写关于图机器学习的内容，所以如果你有兴趣，可以在medium上关注我或者订阅我的</em> <a class="ae kf" href="https://t.me/graphML" rel="noopener ugc nofollow" target="_blank"> <em class="mk">电报频道</em> </a> <em class="mk">或者</em> <a class="ae kf" href="https://twitter.com/SergeyI49013776" rel="noopener ugc nofollow" target="_blank"> <em class="mk">我的twitter </em> </a> <em class="mk">。</em></p></div></div>    
</body>
</html>