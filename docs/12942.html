<html>
<head>
<title>Data Transformation in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 中的数据转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-transformation-in-pyspark-6a88a6193d92?source=collection_archive---------8-----------------------#2020-09-06">https://towardsdatascience.com/data-transformation-in-pyspark-6a88a6193d92?source=collection_archive---------8-----------------------#2020-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9da1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PySpark 中数据转换的演练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a648a155894a3f96c0e3f6f5f80004af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJ0hRRJMtUYV1h-FV3L2Dw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://www.pexels.com/@markusspiske?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a>Markus Spiske</p></figure><p id="a996" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，数据的增长速度超过了处理速度。这个问题的众多解决方案之一是在大型集群上并行计算。输入 PySpark。</p><p id="0ce0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，PySpark 要求您以不同的方式考虑数据。</p><p id="c432" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而不是逐行查看数据集。PySpark 鼓励您从专栏的角度来看待它。起初，这对我来说是一个艰难的转变。我会告诉你我学到的主要技巧，这样你就不用浪费时间去寻找答案了。</p><h1 id="1550" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">资料组</h1><p id="8dff" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我将使用来自 Kaggle 的<a class="ae kv" href="https://www.kaggle.com/epa/hazardous-air-pollutants" rel="noopener ugc nofollow" target="_blank">有害空气污染物</a>数据集。</p><p id="4d88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个数据集是<code class="fe mp mq mr ms b">8,097,069</code>行。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="7e89" class="mx lt iq ms b gy my mz l na nb">df = spark.read.csv(‘epa_hap_daily_summary.csv’,inferSchema=True, header =True)<br/>df.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/5e324035f0ee624648cbd0d9dce55342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N1H5dyVCVGmnDsCi.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="a243" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">条件 If 语句</h1><p id="0a47" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们要做的第一个转换是条件 if 语句转换。如下所示:如果数据集中的一个单元格包含一个特定的字符串，我们希望更改另一列中的单元格。</p><p id="bcfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上我们想从这里开始:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/8e3f12e5dc627d771bc65d9912b582a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*1Lk9C5VlONNqLBKQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6d35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/d91e335e15de17f8cf8b313aff1dd795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*iA8W-xgwlaWjIi_y.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dac7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<code class="fe mp mq mr ms b">local site name</code>包含单词<code class="fe mp mq mr ms b">police</code>，那么我们将<code class="fe mp mq mr ms b">is_police</code>列设置为<code class="fe mp mq mr ms b">1</code>。否则我们将其设置为<code class="fe mp mq mr ms b">0</code>。</p><p id="e534" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种条件如果在熊猫身上陈述是相当容易做到的。我们会使用<code class="fe mp mq mr ms b"><a class="ae kv" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html" rel="noopener ugc nofollow" target="_blank">pd.np.where</a></code>或<code class="fe mp mq mr ms b"><a class="ae kv" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html" rel="noopener ugc nofollow" target="_blank">df.appl</a>y</code>。在最坏的情况下，我们甚至可以遍历这些行。我们在 Pyspark 做不到这些。</p><p id="74ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 Pyspark 中，我们可以使用<code class="fe mp mq mr ms b"><a class="ae kv" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.when" rel="noopener ugc nofollow" target="_blank">F.when</a></code>语句或<code class="fe mp mq mr ms b"><a class="ae kv" href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.udf" rel="noopener ugc nofollow" target="_blank">UDF</a>.</code>语句，这允许我们获得与上面相同的结果。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="ca6c" class="mx lt iq ms b gy my mz l na nb">from pyspark.sql import functions as F</span><span id="0b41" class="mx lt iq ms b gy nf mz l na nb">df = df.withColumn('is_police',\<br/>     F.when(\<br/>     F.lower(\<br/>     F.col('local_site_name')).contains('police'),\<br/>     F.lit(1)).\<br/>                    otherwise(F.lit(0)))</span><span id="7f9c" class="mx lt iq ms b gy nf mz l na nb">df.select('is_police', 'local_site_name').show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/40b3756d2b25dd2c78e82aebe00ceff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*qjs7UJOPHH3HjuNk.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a1ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在假设我们想扩展我们上面所做的。这一次，如果一个单元格包含 3 个字符串中的任何一个，那么我们将在另一列中更改相应的单元格。</p><p id="7bda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果任何一个字符串:<code class="fe mp mq mr ms b">'Police', 'Fort' , 'Lab'</code>在<code class="fe mp mq mr ms b">local_site_name</code>列中，那么我们将相应的单元格标记为<code class="fe mp mq mr ms b">High Rating</code>。</p><p id="ad89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与我们之前看到的<code class="fe mp mq mr ms b">F.when</code>函数结合的<code class="fe mp mq mr ms b"><a class="ae kv" href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.Column.like" rel="noopener ugc nofollow" target="_blank">rlike</a></code>函数允许我们这样做。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="c071" class="mx lt iq ms b gy my mz l na nb">parameter_list = ['Police', 'Fort' , 'Lab']</span><span id="727e" class="mx lt iq ms b gy nf mz l na nb">df = df.withColumn('rating',\<br/>     F.when(\<br/>     F.col('local_site_name').rlike('|'.join(parameter_list)),\<br/>     F.lit('High Rating')).\<br/>     otherwise(F.lit('Low Rating')))</span><span id="ba9e" class="mx lt iq ms b gy nf mz l na nb">df.select('rating', 'local_site_name').show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/2c13f4731b3b333f3e6ff94b66aa9733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*7dOZco12eTm-vqWR.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dc7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mp mq mr ms b">F.when</code>实际上对很多不同的事情都有用。事实上你甚至可以做一个连锁<code class="fe mp mq mr ms b">F.when</code>:</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="5635" class="mx lt iq ms b gy my mz l na nb">df = df.withColumn('rating', F.when(F.lower(F.col('local_site_name')).contains('police'), F.lit('High Rating'))\<br/>                              .when(F.lower(F.col('local_site_name')).contains('fort'), F.lit('High Rating'))\<br/>                              .when(F.lower(F.col('local_site_name')).contains('lab'), F.lit('High Rating'))\<br/>                              .otherwise(F.lit('Low Rating')))</span><span id="3595" class="mx lt iq ms b gy nf mz l na nb">df.select('rating', 'local_site_name').show(</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/409c79022fffc92a0d9037b819e89958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*5Ho3BixJHbII5EEo.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f8e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这与我们在前面的例子中看到的完全一样。然而，要写的代码更多，要维护的代码也更多。</p><p id="f0ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我更喜欢上面讨论的<code class="fe mp mq mr ms b">rlike</code>方法。</p><h1 id="d707" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">删除空白</h1><p id="5829" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">空白真的很烦人。它确实会影响字符串匹配，并会导致查询中不必要的错误。</p><p id="c05b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，尽快删除空白是个好主意。</p><p id="ac2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mp mq mr ms b"><a class="ae kv" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.trim" rel="noopener ugc nofollow" target="_blank">F.trim</a></code>允许我们这样做。它将删除指定列中每一行的所有空白。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="9238" class="mx lt iq ms b gy my mz l na nb">df = df.withColumn('address', F.trim(F.col('address')))<br/>df.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/9502fb432bf38a65fb78fb558135fc8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qYMCI8PfqN-YwXTu.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="e736" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">删除特定列的空行</h1><p id="3dc8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">假设我们只想删除一列中的空行。如果我们在<code class="fe mp mq mr ms b">pollutant_standard</code>列中遇到<code class="fe mp mq mr ms b">NaN</code>值，则删除整行。</p><p id="7bd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以相当简单地完成。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="9c63" class="mx lt iq ms b gy my mz l na nb">filtered_data = df.filter((F.col('pollutant_standard').isNotNull())) # filter out nulls<br/>filtered_data.count()</span></pre><p id="bb1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">条件 OR 参数允许删除我们<code class="fe mp mq mr ms b">event_type</code>或<code class="fe mp mq mr ms b">site_num</code>所在的行<code class="fe mp mq mr ms b">NaN.</code></p><p id="bce6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是<a class="ae kv" href="https://stackoverflow.com/questions/3154132/what-is-the-difference-between-logical-and-conditional-and-or-in-c" rel="noopener ugc nofollow" target="_blank">所指的</a>的` |`。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="e60f" class="mx lt iq ms b gy my mz l na nb">filtered_data = df.filter((F.col('event_type').isNotNull()) | (F.col('site_num').isNotNull())) # filter out nulls<br/>filtered_data.count()</span></pre><p id="5578" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna" rel="noopener ugc nofollow" target="_blank"> df.na.drop </a>允许我们删除所有列都是<code class="fe mp mq mr ms b">NaN</code>的行。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="d0a5" class="mx lt iq ms b gy my mz l na nb">filtered_data = df.na.drop(how = 'all') # filter out nulls<br/>filtered_data.show()</span></pre><h1 id="7f31" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="cb26" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">PySpark 仍然是一门相当新的语言。可能正因为如此，互联网上没有太多的帮助。像熊猫或 R 这样的东西有丰富的信息。有了 Spark 就完全不是这么回事了。</p><p id="97b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我希望这些代码能帮助到一些人。他们肯定会帮助我，节省我很多时间。我经历的转变可能看起来很小或微不足道，但当它涉及到 Spark 时，没有多少人谈论这些东西。我希望这能在某种程度上帮助你。</p><p id="5e0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我犯了一个错误，或者你想联系我，请随时通过<a class="ae kv" href="https://twitter.com/neeliyer11" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="e469" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="np">原载于 2020 年 9 月 6 日</em><a class="ae kv" href="https://spiyer99.github.io/Pyspark-Hacks/" rel="noopener ugc nofollow" target="_blank"><em class="np">https://spiyer 99 . github . io</em></a><em class="np">。</em></p></div></div>    
</body>
</html>