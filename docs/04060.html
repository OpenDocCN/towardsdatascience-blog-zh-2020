<html>
<head>
<title>This is how to train better transformer models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这就是如何训练更好的变形金刚模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978?source=collection_archive---------11-----------------------#2020-04-14">https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978?source=collection_archive---------11-----------------------#2020-04-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c685" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">大规模训练，然后压缩</h2><div class=""/><div class=""><h2 id="e754" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何训练更快、更高性能的变压器</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/aac4aad116ee44fbeaba43c2daf0719c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PZSMd8skXCWIkKug"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1afb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> F </span>两年多以来，在大型文本语料库上训练的transformer模型是所有自然语言处理中最先进的。研究人员和从业者继续通过发明更好的架构或在更多数据上训练更大的模型来拓展边界。事实上，在其他条件相同的情况下，很少有人会不同意在更多数据上训练更大的模型可以提高性能。但是如果时间或资源有限呢？</p><p id="156c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">普遍的看法是接受精度的打击，训练更小的模型。更小的模型不仅训练和推理更快，而且更便宜，对吗？</p><p id="b809" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">伯克利人工智能研究所(BAIR)最近的研究表明情况并非如此。较大的模型训练速度更快，可以更有效地压缩，从而减少推理时间。因此，作者得出结论认为</p><blockquote class="mk ml mm"><p id="e2d1" class="lf lg mn lh b li lj ka lk ll lm kd ln mo lp lq lr mp lt lu lv mq lx ly lz ma ij bi translated">“资源受限训练的最佳策略是训练大型模型，然后大量压缩它们”</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mr"><img src="../Images/0a89a8e23756c298c3c450ab369edcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iwx3frVQ3JMczRrqDOgDWQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">与通常的实践相反，在资源约束设置中，训练大型模型是最佳的[ <a class="ae le" href="https://arxiv.org/pdf/2002.11794.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="4226" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">体型越大，训练速度越快</h1><p id="fbd2" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">作者用以下模型进行了实验:</p><ul class=""><li id="a93c" class="np nq iq lh b li lj ll lm lo nr ls ns lw nt ma nu nv nw nx bi translated">用于自我监督语言建模的RoBERTa模型的版本；和</li><li id="a67e" class="np nq iq lh b li ny ll nz lo oa ls ob lw oc ma nu nv nw nx bi translated">用于机器翻译的标准<a class="ae le" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变压器</a>型号。</li></ul><p id="29a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在每个实验中，作者根据模型的<strong class="lh ja">深度</strong>(2-24层)和<strong class="lh ja">宽度</strong>(隐藏尺寸128-2024)来改变模型的尺寸。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/ecf72b8b33385456b4cb0a3c62a5fa85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-oP42bDLY7EjlUmRR4RsPw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">更深的模型更快实现更好的性能[ <a class="ae le" href="https://arxiv.org/pdf/2002.11794.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="11f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">主要结果是较大的模型:</p><p id="24a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 1 </span>样本效率更高:在更少的梯度步骤后，它们获得了<strong class="lh ja">更好的结果</strong>(在语言建模任务上更低的复杂度，在翻译任务上更高的BLEU分数)<strong class="lh ja">；</strong>和</p><p id="89cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">即使调整挂钟时间后，较大型号的<strong class="lh ja">训练速度更快</strong>。也就是说，训练时间的减少远远抵消了模型规模增加带来的计算开销的增加。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/3f6d7ca3c0ab1d09dd2b0bd76396cb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAJae2EZ0V-DUpj1nfkiFg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">更宽的型号实现更好的性能更快[ <a class="ae le" href="https://arxiv.org/pdf/2002.11794.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="d9d9" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">较大的模型压缩得更好</h1><p id="611a" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi mb translated">更大的模型不仅训练更快，预测也更快。这是因为它们更具可压缩性，因此您可以将它们调整到与小模型相同的推理成本，同时实现更高的准确性。</p><p id="3950" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了得到这个结果，作者以两种方式压缩他们的模型:</p><ul class=""><li id="eabb" class="np nq iq lh b li lj ll lm lo nr ls ns lw nt ma nu nv nw nx bi translated"><strong class="lh ja">量化</strong>:它们以不同的精度格式(低至4位)存储参数，以节省计算时间和内存空间；和</li><li id="3a2c" class="np nq iq lh b li ny ll nz lo oa ls ob lw oc ma nu nv nw nx bi translated"><strong class="lh ja">修剪</strong>:它们迭代地将15%的最低幅度参数清零，然后再次微调，以减少操作次数和内存占用(因为权重矩阵现在很稀疏)。</li></ul><p id="4b5a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者发现:</p><p id="9509" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 1 </span>对于这两种压缩方法，较大的型号提供了<strong class="lh ja">更好的精度-效率权衡</strong>:压缩时精度下降低于较小的型号；和</p><p id="6ad8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 2 </span>两种压缩方法都可以在<strong class="lh ja">很少附加</strong> <strong class="lh ja">计算</strong> <strong class="lh ja">开销</strong>的情况下进行。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/a175bc22814c9f48baf0164d108d2863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dnsd-lBFXBRW9n3RrI9XrA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">较大的模型通常在任何压缩级别都能获得更好的精度[ <a class="ae le" href="https://arxiv.org/pdf/2002.11794.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="a208" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">我们何时以及为什么会取得这些成果？</h1><p id="6ab7" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">虽然有大量的文献说明为什么更大的模型获得更高的测试精度，但很少有人研究它们是否以及为什么收敛得更快。作者对为什么会出现这种情况给出了一些解释:</p><p id="a7fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 1 </span> <strong class="lh ja">这些结果适用于大型数据集</strong>(在这种情况下，过拟合问题不大)。根据经验，模型越大，减少训练误差的速度越快。由于对于大型数据集，泛化差距(训练和测试误差之间的差异)不是一个问题，因此较大的模型也可以更快地减少测试误差。作者指出</p><blockquote class="mk ml mm"><p id="f90e" class="lf lg mn lh b li lj ka lk ll lm kd ln mo lp lq lr mp lt lu lv mq lx ly lz ma ij bi translated">“MLM任务中的挑战不是<em class="iq">过度拟合</em>，而是<em class="iq">拟合</em>数据——即使80亿个参数模型也不会过度拟合大型预训练语料库”</p></blockquote><p id="1c76" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当将训练数据集减少到其原始大小的1%或5%时，作者发现训练大模型的好处消失了。</p><p id="1a97" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 2 </span> <strong class="lh ja">较大的型号使用计算的效率更高。</strong>在训练大型模型完成掩蔽语言建模等任务时，瓶颈不是计算，而是内存&amp;存储。因此，较大的模型更有效地利用可用的计算。</p><p id="2e4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> 3 </span> <strong class="lh ja">较大的型号获得较小的压缩误差</strong>。作者表明，对于大模型，量化&amp;修剪产生的参数比小模型更接近原始(未修剪)模型的参数。</p><h1 id="b953" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">这有什么关系？</h1><p id="6cf1" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">这篇论文构成了一个潜在的范式转变，在资源约束下如何训练模型。这具有明显的经济效益:花在训练模型上的时间和金钱更少，同时实现更高的性能。</p><p id="d628" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">应该注意的是，在上述所有实验中，作者除了选择模型深度和宽度之外，没有进行任何超参数优化。然而，在现实世界的设置中，好的超参数是未知的，因此花费了大量的时间和资源来寻找最佳设置。</p><p id="354d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇论文的发现之所以重要还有另一个原因。搜索好的超参数，或者只是简单地训练最终模型，可能需要许多GPU小时，因此可能会有大量的碳足迹。通过减少训练一定精度的模型所需的时间，减少排放是可能的。如果你有兴趣了解更多关于你的碳足迹的信息，请查看Mila实验室的<a class="ae le" href="https://mlco2.github.io/impact/#compute" rel="noopener ugc nofollow" target="_blank"> ML CO₂计算器</a>工具。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="73dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[1]:李，Eric Wallace，，，Kurt Keutzer，Dan Klein和Joseph E. Gonzalez，<a class="ae le" href="https://arxiv.org/abs/2002.11794" rel="noopener ugc nofollow" target="_blank">大规模训练，然后压缩:重新考虑模型大小以实现变形金刚的高效训练和推理</a> (2020)。</p><p id="7727" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]: Alexandre Lacoste，Alexandra Luccioni，Victor Schmidt和Thomas Dandres，<a class="ae le" href="https://arxiv.org/abs/1910.09700" rel="noopener ugc nofollow" target="_blank">量化机器学习的碳排放</a> (2019)，NeurIPS 2019的<a class="ae le" href="https://www.climatechange.ai/NeurIPS2019_workshop.html" rel="noopener ugc nofollow" target="_blank">气候变化AI工作坊</a>。</p></div></div>    
</body>
</html>