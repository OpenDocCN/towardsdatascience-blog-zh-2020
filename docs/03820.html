<html>
<head>
<title>Exploring MobileNets: From Paper To Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索移动互联网:从纸张到 Keras</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-mobilenets-from-paper-to-keras-f01308ada818?source=collection_archive---------41-----------------------#2020-04-09">https://towardsdatascience.com/exploring-mobilenets-from-paper-to-keras-f01308ada818?source=collection_archive---------41-----------------------#2020-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="073c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">拆卸 MobileNets，看看它们是如何轻便高效的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/30fa0b0ca7e549c271072c25d56b2857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3sNmTv3eHdkHEOku"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@angelacompagnone?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安吉拉·孔波妮</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cd43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNets </a>是流行的架构，用于图像分类、人脸检测、分割等等。它们在移动和嵌入式设备上的延迟是众所周知的。你可以从名字<em class="lv">“MobileNet”</em>中推断出这一点。由于使用了可分离卷积，它们的可训练参数数量少得多。如果你正在运行一个使用相机实时帧的图像分类模型，你可能需要一个快速、准确、占用移动设备更少内存的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/bad247c5eb2f2513f0555fbe2891cab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKPG5vn8RoOkfZRB06LPuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">移动互联网的使用。<a class="ae ky" href="https://www.arxiv-vanity.com/papers/1704.04861/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="c426" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，我们将从它的<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">研究论文</a>到 TensorFlow ( Keras！).在进一步阅读这个故事之前，你可以确保以下几点，</p><ol class=""><li id="d0f5" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated"><em class="lv">如果你对可分卷积的概念不感兴趣，请阅读“</em> <a class="ae ky" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728"> <em class="lv">可分卷积基础介绍</em> </a> <em class="lv">”。</em></li><li id="f8fd" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><em class="lv"/><a class="ae ky" href="https://colab.research.google.com/drive/1uUYdZk7EbOESRP7JFwHfsR7b9gUjxU75#scrollTo=K17Opyz2XLeN&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank"><em class="lv">Colab 笔记本</em> </a> <em class="lv">仅包含 MobileNet V1 的 TF 实现。</em></li><li id="7cc8" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><em class="lv">所有用粗体和斜体书写的术语，如</em> <strong class="lb iu"> <em class="lv">【示例】</em> </strong> <em class="lv">都可以直接在研究论文中找到。</em></li><li id="bc98" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><em class="lv">你可能会在 GitHub 的</em><a class="ae ky" href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py" rel="noopener ugc nofollow" target="_blank"><em class="lv">tensor flow/models</em></a><em class="lv">repo 上看到 MobileNet 的实现。</em></li></ol><p id="8eb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我建议您在另一个选项卡中打开 MobileNet 的 TensorFlow 实现，</p><div class="ml mm gp gr mn mo"><a href="https://colab.research.google.com/drive/1uUYdZk7EbOESRP7JFwHfsR7b9gUjxU75#scrollTo=K17Opyz2XLeN&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">移动网络 _With_TensorFlow</h2><div class="mv l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">colab.research.google.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ks mo"/></div></div></a></div><h1 id="6973" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">MobileNets 使用可分离的卷积。但是什么是可分卷积呢？它们有什么“可分”之处？</h1><p id="b92c" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">可分离盘旋由下面的两个(分离的)盘旋组成。它们是深度方向卷积和点方向卷积。深度方向卷积接受一个特征图，在每个输入核上运行一个核。逐点卷积增加了输出通道的数量。</p><p id="8b7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我强烈推荐阅读<a class="ae ky" href="https://towardsdatascience.com/@reina.wang" rel="noopener" target="_blank">齐-汪锋</a>的<a class="ae ky" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728">可分卷积基础介绍</a>。</p><div class="ml mm gp gr mn mo"><a rel="noopener follow" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">可分卷积的基本介绍</h2><div class="nz l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">解释空间可分离卷积，深度可分离卷积，以及在一个简单的。</h3></div><div class="mv l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">towardsdatascience.com</p></div></div><div class="mw l"><div class="oa l my mz na mw nb ks mo"/></div></div></a></div><blockquote class="ob"><p id="b762" class="oc od it bd oe of og oh oi oj ok lu dk translated">在本文中，所有的卷积都被认为是填充的。所以在卷积之后，输入和输出特征图的大小是相同的。所以在下面两张图中，<em class="ol"> Df — Dₖ + 1 </em>只等于<em class="ol"> Df </em>。</p></blockquote><h2 id="9805" class="om nd it bd ne on oo dn ni op oq dp nm li or os no lm ot ou nq lq ov ow ns ox bi translated">深度方向回旋</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/1d220824e0b124f99f22c3a43145f1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cg80_Xt3lTabHqURR-s1dQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度方向的回旋。</p></figure><p id="710d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设，我们有尺寸为<em class="lv"> Df </em>的<em class="lv"> M </em>正方形特征地图。使用大小为<em class="lv"> Dₖ </em>的内核，我们正在生成大小为<em class="lv"> Df — Dₖ + 1 </em>的输出特征图(假设没有填充，步长为 1)。我们对所有 m 个输入特征图重复这一过程，最后，我们剩下一个维度的特征图，<em class="lv">df—dₖ+1×df—dₖ+1×m。</em>注意，我们将对输入特征图的<em class="lv"> M </em>个通道使用<em class="lv"> M </em>个不同的核。这就是我们的<strong class="lb iu"> <em class="lv">【深度方向卷积】</em> </strong>。</p><p id="fcfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乘法次数(或文中提到的<strong class="lb iu"> <em class="lv">【计算成本】</em> </strong>)将为，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/55b7c38553b3ee174bb3a1f9ed2e1ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*c51j5JsA8lmQwS5Naihk3Q.png"/></div></figure></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当你只为一个时期训练了你的模型并且你用完了内存的时候！</p></figure></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><h2 id="652d" class="om nd it bd ne on pj dn ni op pk dp nm li pl os no lm pm ou nq lq pn ow ns ox bi translated">逐点卷积</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/6acd76883c0134fd59d4f3658c87ab8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ap6ylEhyt0l66_fW_cRE4Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逐点卷积。</p></figure><p id="d36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面生成的输出特征图有<em class="lv"> M </em>个通道，而我们需要<em class="lv"> N </em>个输出通道。因此，为了增加输出维度，我们使用了一个<em class="lv"> 1 × 1 </em>卷积。这些被称为<strong class="lb iu"> <em class="lv">【点态卷积】</em> </strong>。我们使用大小为<em class="lv"> 1 × 1 × M </em>的核，并产生大小为<em class="lv">df-dₖ+1×df-dₖ+1×1 的单一特征图。</em>我们将此重复<em class="lv"> N 次</em>并且我们留下大小为<em class="lv">df-dₖ+1×df-dₖ+1×n 的输出特征图。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/97b69ca5d84468cc7fd4eb8eb0b73ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*5-BUdsOgb7F6BGefagdzHg.png"/></div></figure></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><p id="09e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一个<strong class="lb iu">标准卷积</strong>，计算成本应该是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/8c4e2894ac1693d2d09ff228a14a7c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*xxfIQoExOFFONWIGnN7HNQ.png"/></div></figure><p id="4ad1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu">可分离卷积(深度方向+点方向)</strong>，计算成本将是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/a4118de223dfd0c00b5bf6cefa8f96f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0honJGQGU6PxosDPE9b8qw.png"/></div></div></figure><p id="a48e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文中还计算了参数的缩减量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/d51812074a2bc487cc665b2be41c96be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qi8kJwgkuW6AhQKHiQtb6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="b640" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">宽度和分辨率乘数</h1><p id="d9f8" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">尽管 MobileNet 的可训练参数有了相当大的减少，但你仍然希望它快，为此我们引入了一个名为<strong class="lb iu"> <em class="lv">【宽度乘数】</em> </strong>的参数。用<em class="lv"> α </em>表示。因此，模型中的任何层都将接收<em class="lv"> αM </em>特征图，并产生<em class="lv"> αN </em>特征图。它使 MobileNet 型号<em class="lv">更薄</em>并增加了延迟。为简单起见，我们将设置α = 1.0，其中α ∈ ( 0，1)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/8a40a10b7448de4b8d4c3b98d463d0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*SY6VW2XidRrcOT1-pudUOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如文中所述的宽度倍增器。<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="aaef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了进一步降低计算成本，他们还引入了一个<strong class="lb iu"> <em class="lv">【分辨率乘数】</em> </strong>记为ρ。它减少了输入图像以及每一层的内部表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/b8c7b9d237adeacf644d8de906e11178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*O5zIzwNeAzf7b36ZParY4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如论文中所述的分辨率倍增器。<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><h1 id="100d" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">TensorFlow 实现(带 Keras)</h1><p id="22c4" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">首先，我们将从论文本身来看一下架构。它看起来像这样，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/d37d76e8fb67935305a14c28dd574c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*7eWABWLOU-aIM8EH0Ke0ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MobileNet 架构。<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="fc3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu"> " <em class="lv"> Conv /s2" </em> </strong>表示步长为 2 的卷积层(非深度方向)。<strong class="lb iu"> " <em class="lv"> Conv dw /s1" </em> </strong>表示步长为 1 的可分卷积。所有层之后是批处理规范化和 LeakyReLU 层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/f93e8ff9fad1a27cd489389f0ae45910.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*ZKA3txiyCQXmUzdhHUZsGw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有 BatchNorm 和 LeakyReLU 的标准卷积层(右)。具有深度方向和 1 × 1 卷积(点方向)的可分离卷积(左)。<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="4888" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Keras 中的实现如下所示，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="px pi l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">可分卷积。</p></figure><p id="c48e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，模型由 29 层包装而成，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="px pi l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">组装模型。</p></figure><p id="43ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在<a class="ae ky" href="http://www.laurencemoroney.com/" rel="noopener ugc nofollow" target="_blank">劳伦斯·莫罗尼</a>的<a class="ae ky" href="http://www.laurencemoroney.com/rock-paper-scissors-dataset/" rel="noopener ugc nofollow" target="_blank">石头剪刀布</a>数据集上训练我们的模型。为了方便起见，它托管在<a class="ae ky" href="https://www.tensorflow.org/datasets" rel="noopener ugc nofollow" target="_blank"> TensorFlow 数据集</a>上。最初的 MobileNet 是在包括 ImageNet 在内的许多数据集上进行评估的。</p><p id="3b32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会在<a class="ae ky" href="https://colab.research.google.com/drive/1uUYdZk7EbOESRP7JFwHfsR7b9gUjxU75#scrollTo=K17Opyz2XLeN&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>中找到培训部分。恭喜你。您刚刚从头开始创建了一个 MobileNet。</p><h1 id="f400" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">进一步探索…</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="px pi l"/></div></figure><h1 id="ffcf" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">结束了</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="py pi l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://giphy.com/stickers/giphytext-text-thanks-wordart-fVDbDA873476u25ubP" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="76b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢 MobileNet 的概念。如果你想在大数据集上训练模型，你可以通读研究论文。他们还包括一些超参数，这将有助于你的训练。谢谢，再见。</p></div></div>    
</body>
</html>