<html>
<head>
<title>Optimising deep learning neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化深度学习神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-by-hazy-4c2f2352f3a0?source=collection_archive---------43-----------------------#2020-05-06">https://towardsdatascience.com/data-science-by-hazy-4c2f2352f3a0?source=collection_archive---------43-----------------------#2020-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2577" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">Hazy的数据科学</h2><div class=""/><div class=""><h2 id="6c5b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度学习神经网络有一系列令人眼花缭乱的元参数。了解如何将GANs应用于神经网络优化。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1cb4937b9bbdbb0c29921691bbbb4a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8OS_QHYY4N8IS8KPKtlXQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.flickr.com/photos/wocintechchat/" rel="noopener ugc nofollow" target="_blank"> WOCinTech </a>，知识共享</p></figure><p id="bdb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">道格拉斯·亚当斯在《银河系漫游指南》中曾断言:“空间很大。你不会相信它有多么巨大，令人难以置信的大。我的意思是，你可能认为去药店的路很长，但那对太空来说只是微不足道。”</p><p id="2e73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">神经网络也很大。T2微软图灵自然语言生成或T-NLG网络有大约190亿个参数。以大多数人的标准来看，这已经很大了。</p><p id="d7ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，我们关心的不仅仅是参数的数量，还有元参数。研究生成对抗网络(GAN)的数据科学家通常必须运行数百万次实验，以优化他们的神经网络。在这篇文章中，我们解释了在Hazy，我们如何将自动元参数优化注入到我们的GANs中，然后让您在较少挫折的情况下训练更好的模型。</p><h2 id="83b2" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">调整元参数:成为机器学习工程师的尝试</h2><p id="95c6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">深度学习神经网络有一系列令人眼花缭乱的元参数。</p><p id="107f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">元参数是提供给网络的参数，用于指导网络的训练过程，控制网络如何修改参数。</p><p id="a1ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">元参数包括学习速率、动量、隐藏层的数量、每层神经元的数量以及要使用的优化器的类型。</p><p id="746d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">机器学习工程师工作的一个标准部分是调整网络，也就是选择产生最佳性能的元参数。这可能是一个耗时的过程，因为网络越大，收敛的时间就越长，并且通常需要调整和调整的元参数就越多。</p><p id="0f1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">调整网络很像一个科学家，他有一台复杂的设备，有许多旋钮要旋转，要进行大量的实验，看哪一个能给你最好的结果。</p><p id="03ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">幸运的是，有一些软件包可以让这项任务变得更容易。</p><h2 id="93ad" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">应用于GANs的Optuna优化</h2><p id="279d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在合成数据生成公司<a class="ae lh" href="https://www.hazy.com" rel="noopener ugc nofollow" target="_blank"> Hazy </a>，我们是<a class="ae lh" href="https://optuna.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Optuna Python包</a>的忠实粉丝。</p><p id="dfa3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Optuna是一个自动元参数优化软件框架，专门为机器学习而设计。该代码是高度模块化和强制性的。它支持并行，分布式优化，动态修剪试验。它与机器学习框架无关，并且高度可定制。</p><p id="0571" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们举个例子。假设我们有一个GAN，我们希望优化它的生成器和鉴别器的学习速率。我们该怎么做呢？</p><h2 id="1762" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">成为神经网络的考验</h2><p id="8143" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Optuna有两个基本概念:研究和试验。</p><p id="a7ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该研究是优化的总体任务，基于返回优化结果的函数。这个函数通常被称为目标函数。试验是目标函数的一次执行。</p><p id="14bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来看一个例子，这个例子取自Optuna网站，并应用于Hazy的GAN模型，让您了解什么是可能的。</p><p id="eeeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们定义一个目标函数来研究。</p><p id="dc6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标函数封装了整个训练过程，并返回元参数的这个特定实例的值。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="1aef" class="me mf it nc b gy ng nh l ni nj">def objective(trial):<br/>    iris = sklearn.datasets.load_iris()<br/>    <br/>    n_estimators = trial.suggest_int('n_estimators', 2, 20)<br/>    max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))<br/>    <br/>    clf = sklearn.ensemble.RandomForestClassifier(<br/>        n_estimators=n_estimators, max_depth=max_depth)<br/>    <br/>    return sklearn.model_selection.cross_val_score(<br/>        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()</span></pre><p id="cca3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该函数加载了<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a> —一个众所周知的用于评估机器学习分类器的数据集。然后，它从试验对象获得建议的估计数和最大深度。然后它实例化一个<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>并返回分数。</p><p id="567a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们更详细地讨论一下。</p><p id="03b1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据集由三种不同种类的虹膜组成，机器学习<strong class="lk jd">任务</strong>是在给定四个测量值的情况下，将给定的数据点正确地分配给正确的虹膜种类:</p><ul class=""><li id="bc7b" class="nk nl it lk b ll lm lo lp lr nm lv nn lz no md np nq nr ns bi translated">萼片长度</li><li id="3532" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">萼片宽度</li><li id="80fc" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">花瓣长度</li><li id="4c4a" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">花瓣宽度</li></ul><p id="f367" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Iris数据集被认为是一个相当困难的分类问题，因为就这些测量而言，物种之间有相当多的重叠。很难在物种之间划出一个清晰的界限。</p><p id="5ab7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们希望优化的元参数是<strong class="lk jd">n _ estimates</strong>和<strong class="lk jd"> max_depth </strong>。</p><p id="2c73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://optuna.readthedocs.io/en/latest/reference/trial.html" rel="noopener ugc nofollow" target="_blank">试验对象</a>为<strong class="lk jd"> n_estimators </strong>建议一个2≤𝑛≤20范围内的整数:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="d907" class="me mf it nc b gy ng nh l ni nj">n_estimators = trial.suggest_int('n_estimators', 2, 20)</span></pre><p id="1bca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Optuna有许多不同的机制来提供元参数的值，以便在每次试验中进行测试；使用对数均匀分布分配<strong class="lk jd">最大深度</strong>的值:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="49c5" class="me mf it nc b gy ng nh l ni nj">max_depth = int(<strong class="nc jd">trial</strong>.suggest_loguniform('max_depth', 1, 32))</span></pre><p id="e1f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://optuna.readthedocs.io/en/latest/reference/trial.html#optuna.trial.Trial.suggest_loguniform" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">suggest _ log uniform</strong></a>函数接受一个范围，在本例中为1≤ 𝑥 ≤32，并返回该范围内的浮点值。这被转换为整数。</p><p id="73b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后创建随机森林分类器，具有建议的<strong class="lk jd">最大深度</strong>和<strong class="lk jd"> n估计器</strong>。对它进行评估，并返回一个分数:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="6829" class="me mf it nc b gy ng nh l ni nj">clf = sklearn.ensemble.RandomForestClassifier(<br/>        n_estimators=n_estimators, max_depth=max_depth)<br/>        <br/>return sklearn.model_selection.cross_val_score(<br/>        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()</span></pre><p id="478d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">创建了目标函数后，我们需要创建一个Optuna研究，并创建一些试验。然后，我们输出元参数的最佳值:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="a697" class="me mf it nc b gy ng nh l ni nj">import optuna<br/>import sklearn<br/>import sklearn.datasets<br/>import sklearn.ensemble<br/><br/>study = optuna.create_study(direction='maximize')<br/>study.optimize(objective, n_trials=100)<br/><br/>trial = study.best_trial<br/><br/>print('Accuracy: {}'.format(trial.value))<br/>print("Best hyperparameters: {}".format(trial.params))</span></pre><p id="9744" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将输出每次试验的分数，以及所使用的元参数。最终，在研究完成后，它输出最佳元参数:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="444e" class="me mf it nc b gy ng nh l ni nj">[I 2020-04-23 17:54:52,817] Finished trial#98 with value: 0.9738562091503268 with parameters: {'n_estimators': 17, 'max_depth': 3.303148836378194}. Best is trial#43 with value: 0.9738562091503268.<br/>[I 2020-04-23 17:54:52,899] Finished trial#99 with value: 0.960375816993464 with parameters: {'n_estimators': 17, 'max_depth': 3.136433926827928}. Best is trial#43 with value: 0.9738562091503268.<br/><br/>Accuracy: 0.9738562091503268<br/>Best hyperparameters: {'n_estimators': 12, 'max_depth': 4.419437654165229}</span></pre><h2 id="0ffb" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">优化GANs</h2><p id="8658" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们如何利用这一点来优化由生成性对抗网络所代表的高度复杂的系统呢？</p><p id="7fa8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们为那些想要优化的元参数定义几个函数。</p><p id="aadf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GAN由两个神经网络组成，即<em class="ny">发生器</em>和<em class="ny">鉴别器</em>。生成器的任务是试图通过创建假数据来欺骗鉴别器；给定真实和虚假数据的输入，鉴别器必须能够区分两者。</p><p id="ba11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设我们想要优化两个网络的学习速率:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="568f" class="me mf it nc b gy ng nh l ni nj">def opt_learning_rate(lr_key, trial):<br/>    """<br/>    lr_key: label to use for this learning rate<br/>    trial: optuna trial object<br/>    (one of those hyperparameters which may vary by orders of magnitude!)<br/>    Returns: a string containing which learning rate is being optimised, &amp; a suggested learning rate<br/>    """<br/>    return trial.suggest_loguniform(<br/>        lr_key,  1e-5, 1000<br/>    )</span></pre><p id="a906" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用了<strong class="lk jd">建议_日志统一</strong>功能，并赋予其广泛的范围。</p><p id="b407" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面我们使用<a class="ae lh" href="http://archive.ics.uci.edu/ml/datasets/Adult" rel="noopener ugc nofollow" target="_blank">成人数据集</a>来设置优化两种学习率的代码。</p><p id="913e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们从建立一个Python字典开始，该字典将包含用于构建网络的默认元参数。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="8d42" class="me mf it nc b gy ng nh l ni nj">default_network_dict = {<br/>    "epochs": 250,<br/>    "batch_size": 64,<br/>    "discriminator_learning_rate": 1e-5,<br/>    "generator_learning_rate": 5e-4,<br/>    "latent_dim": 100,<br/>    "input_output_dim": 20,<br/>    "num_bins": 100,<br/>    "layers": 3,<br/>    "hidden_dims": [64, 128, 256],<br/>    "num_critics": 4,<br/>    "dropout": 0.1,<br/>    "neuron_type": "LeakyReLU",<br/>    "optimiser": "RMSProp",<br/>    "output_folder": False,<br/>}</span></pre><p id="9af5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，我们已经为GAN的学习速率提供了默认值；这允许用户选择他们想要优化的元参数。所有这些都有可能得到优化；然而，字典也可以按原样使用，以创建神经网络。</p><p id="0524" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用户也可以从命令行指定元参数，所以让我们通过从<strong class="lk jd"> argparse </strong>创建一个命令行解析器名称空间来适应这种情况。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="0d53" class="me mf it nc b gy ng nh l ni nj">import argparse<br/><br/>params = argparse.Namespace(<br/>    experiment_name = 'my-experiment',<br/>    location = 'metaparameter-optimisation',<br/>    dataset_name = 'adult',<br/>    #output_folder = 'output',<br/>    num_bins = 100,<br/>    epochs = 500,<br/>    batch_size = 64,<br/>    discriminator_rate = 1e-05,<br/>    generator_rate = 0.0005,<br/>    sigma = 0.1,<br/>    latent_dim = 200,<br/>    num_critics = 4,<br/>    cuda = 0,<br/>    optimise = ['generator_learning_rate', 'discriminator_learning_rate'],<br/>    hidden_dims = [64, 128, 256],<br/>    structure = False,<br/>)</span></pre><p id="296e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们需要一种将来自名称空间的命令行元参数与默认网络字典相结合的方法:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="f98e" class="me mf it nc b gy ng nh l ni nj">def override_params(default_params, optimisable, structure=False):<br/>    if structure:<br/>        default_params['layers'] = 'METAPARAM'<br/>        default_params['hidden_dims'] = 'METAPARAM'<br/>    tmp = dict.fromkeys(optimisable, 'METAPARAM')<br/>    return { **default_params, **tmp }</span></pre><p id="f7b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们定义我们的目标函数。</p><p id="5b08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标函数必须覆盖许多基础，因为它必须包含创建、训练和测试GAN的所有代码。该函数创建一个GAN。然后它设置日志，这样我们就可以看到运行试验的结果。然后用给定的元参数调用一个函数来运行试验。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="ae00" class="me mf it nc b gy ng nh l ni nj">def objective(trial, params, network_dict):<br/>    # need to change all the requested metaparams<br/><br/>    logging.basicConfig(level=logging.INFO)<br/>    logger = logging.getLogger('__metaparams__')<br/><br/>    processor, df, processed_df, categories_n = initialise_processor(params['num_bins'],                                                                  params['dataset_name'],<br/>logger)<br/>   network_dict['input_output_dim'] = categories_n<br/>    fixed_params = fix_metaparams(trial, network_dict)<br/>    generator = build_generator_network(fixed_params)<br/>    discriminator = build_discriminator_network(fixed_params)<br/><br/>    # build the network's optimisers<br/>    gen_optimiser = build_network_optimiser(network_dict['optimiser'],<br/>                                            network_dict['generator_learning_rate'],<br/>                                            generator)<br/>    disc_optimiser = build_network_optimiser(network_dict['optimiser'],<br/>                                            network_dict['discriminator_learning_rate'],<br/>                                            discriminator)<br/><br/>    return run_experiment(<br/>        generator=generator,<br/>        discriminator=discriminator,<br/>        generator_solver=gen_optimiser,<br/>        discriminator_solver=disc_optimiser,<br/>        processor=processor,<br/>        df=df,<br/>        processed_df=processed_df,<br/>        latent_dim=fixed_params["latent_dim"],<br/>        output_folder=fixed_params["output_folder"],<br/>        num_bins=fixed_params["num_bins"],<br/>        epochs=fixed_params["epochs"],<br/>        batch_size=fixed_params["batch_size"],<br/>        sigma=params["sigma"],<br/>        num_critics=fixed_params["num_critics"],<br/>        cuda=params["cuda"],<br/>    )</span></pre><p id="ae7e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标函数和它调用的<strong class="lk jd"> run_experiment </strong>函数看起来都有点复杂，但本质上它们只是分配和解析参数。这两段代码中有相当多的内容。然而，他们实际上只是设置了生成器和鉴别器网络、数据集和赋值器。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="b21e" class="me mf it nc b gy ng nh l ni nj">import logging<br/>from hazy_auto_tuning import initialise_processor, run_experiment<br/><br/>from hazy_network_metaparameters import check_requested_metaparameters, optimisable, fix_metaparams<br/># from metaparameter_tuning import build_discriminator_network, build_generator_network, build_network_optimiser<br/><br/>from metaparameter_tuning import build_discriminator_network, build_generator_network, build_network_optimiser</span></pre><p id="f17b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在从<a class="ae lh" href="https://hazy.com/" rel="noopener ugc nofollow" target="_blank"> Hazy </a>代码库中再导入几次之后，你可以建立一个Optuna研究对象，并要求它为我们优化我们的元参数。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="cceb" class="me mf it nc b gy ng nh l ni nj">study = optuna.create_study(direction="maximize")<br/>study.optimize(lambda trial: objective(trial, params_dict,       network_dict), n_trials=20)</span></pre><p id="5245" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，对于一项有用的研究来说，这(可能)是太少的试验。对于元参数，如鉴别器和生成器的学习率，我们需要更多的试验。类似地，对于GANs性能的精确评估，历元的数量可能不够大。这些仅作为例子给出。</p><p id="a2fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为机器学习工程师，能够自动化元参数优化令人兴奋，因此我们可以花更多时间探索优化它们对所提供的合成数据集的影响。我们已经将Optuna代码应用到模型中，这将节省我们所有人的时间。</p></div></div>    
</body>
</html>