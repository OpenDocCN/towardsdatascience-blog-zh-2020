<html>
<head>
<title>Value Iteration to solve OpenAI Gym’s FrozenLake</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">价值迭代求解OpenAI健身房的FrozenLake</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/value-iteration-to-solve-openai-gyms-frozenlake-6c5e7bf0a64d?source=collection_archive---------10-----------------------#2020-06-14">https://towardsdatascience.com/value-iteration-to-solve-openai-gyms-frozenlake-6c5e7bf0a64d?source=collection_archive---------10-----------------------#2020-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="52e8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">素君的承担…</h2><div class=""/><div class=""><h2 id="0f90" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从零开始理解和实现价值迭代…</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0cb5e74310a61b0e7719090132128b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Vw-P7yAQYcgvx5pxBpUlA.jpeg"/></div></div></figure><p id="f870" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我的叙述下，我们将制定<strong class="lc ja"> <em class="lw">值迭代</em> </strong>并实现它来解决<strong class="lc ja"> <em class="lw"> FrozenLake8x8-v0 </em> </strong>环境来自<strong class="lc ja"> <em class="lw"> OpenAI的健身房。</em>T11】</strong></p><p id="7a6d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个故事有助于<em class="lw"> </em> <strong class="lc ja"> <em class="lw">强化学习的初学者</em> </strong>理解<strong class="lc ja"> <em class="lw">值迭代</em> </strong>从零开始实现，并了解<strong class="lc ja"> <em class="lw"> OpenAI Gym的</em> </strong>环境。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><blockquote class="me mf mg"><p id="1a73" class="la lb lw lc b ld le ka lf lg lh kd li mh lk ll lm mi lo lp lq mj ls lt lu lv ij bi translated"><strong class="lc ja">简介:FrozenLake8x8-v0环境下，</strong>是一个<strong class="lc ja">离散有限MDP。</strong></p></blockquote><p id="b4b1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们将计算出<strong class="lc ja"> <em class="lw">最优策略</em> </strong>对于一个<strong class="lc ja"><em class="lw"/></strong>(最佳可能<strong class="lc ja"> <em class="lw">行动</em></strong><strong class="lc ja"><em class="lw"/></strong>给定<strong class="lc ja"> <em class="lw">状态</em> </strong>)在给定<strong class="lc ja"> <em class="lw">环境下达到目标，</em> </strong>因此得到最大的<strong class="lc ja"> <em class="lw">预期报酬</em></strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ad38c0a12b7f455fb8e62aeae0f1f99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/1*ur_42c7MLhbi6q2L3JtSqg.gif"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">哑代理使用随机策略</strong></p></figure><blockquote class="me mf mg"><p id="c8f5" class="la lb lw lc b ld le ka lf lg lh kd li mh lk ll lm mi lo lp lq mj ls lt lu lv ij bi translated">代理控制一个角色在<strong class="lc ja">【8x8】</strong>网格世界中的移动。格子的有些瓷砖是可走的<strong class="lc ja">【F】</strong>，有些导致代理人落水<strong class="lc ja">【H】</strong>。此外，智能体的运动方向是不确定的<strong class="lc ja">(未知策略)</strong>，并且仅部分取决于所选择的方向<strong class="lc ja">(环境动力学)</strong>。代理人每走<strong class="lc ja"> (0) </strong>步找到一条通往目标方块的可行走路径，将获得<strong class="lc ja"> (1) </strong>奖励。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9eb514b85a1856bc817ae8046edbfc8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*2wKtCBQpvvD8e7-hYVuMPw.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">冰冻湖-v0</p></figure><p id="8ecd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"><em class="lw">action _ space:</em></strong>离散(4)，agent可以采取4个离散动作:左(0)，下(1)，上(2)，右(3)。</p><p id="c6a9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">状态_空间:</em> </strong>离散(64)，离散64个网格单元。</p><p id="c3d7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">转移概率</em> </strong>:由于环境的不确定性，以转移概率为例，给定<strong class="lc ja"> <em class="lw">状态(0)动作(1) </em> </strong>将…</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/404b2e60c66bbcd7259fceb87b59903a.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*fPAooBw66x8shDPaQHinXg.png"/></div></figure><p id="caf7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">环境的属性</em> </strong> : ' <strong class="lc ja"> env.env.nA </strong>'，'<strong class="lc ja"> env.env.nS </strong>'给出了可能的动作和状态的总数。</p><p id="9daa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> P[s|a] = P[s']，s '，r，done </strong></p><p id="ee6b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">到达后继状态的概率(s ')及其回报(r)。</p><p id="8526" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">更多环境详情，<a class="ae ms" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja"> FrozenLake8x8-v0 </strong> </a>。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="63c1" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">让我们来理解策略迭代:</h1><p id="1061" class="pw-post-body-paragraph la lb iq lc b ld nl ka lf lg nm kd li lj nn ll lm ln no lp lq lr np lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">预测和控制</em> </strong></p><blockquote class="me mf mg"><p id="0f3f" class="la lb lw lc b ld le ka lf lg lh kd li mh lk ll lm mi lo lp lq mj ls lt lu lv ij bi translated"><strong class="lc ja">策略评估:对于给定的策略(π)，确定状态值函数Vπ(s)。</strong></p></blockquote><p id="0707" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于给定的策略<strong class="lc ja"> (π) </strong>，初始近似值<strong class="lc ja"> v0 </strong>被任意选择，对于最终状态为‘0’，值函数的逐次近似值使用贝尔曼方程作为更新规则。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1835ff27bd425e9493ed3ff88a3c2a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*8whQjfViO9onKpUEVtN_AA.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">贝尔曼期望方程</strong>作为更新规则</p></figure><p id="042f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中，<strong class="lc ja">策略:</strong> <strong class="lc ja"> π(a|s) </strong>是策略<strong class="lc ja"> (π) </strong>下状态<strong class="lc ja"> (s) </strong>下采取行动<strong class="lc ja"> (a) </strong>的概率。<strong class="lc ja">环境</strong> <strong class="lc ja">跃迁动力学:</strong> <strong class="lc ja"> P(s '，r|s，a) </strong>是到达后继状态<strong class="lc ja"> (s') </strong>并从状态<strong class="lc ja"> (s) </strong>采取行动<strong class="lc ja"> (a) </strong>，<strong class="lc ja">γ</strong>是一个折扣因子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/dc92193b557cc159e6b37c72e83a1765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDgI-hjckIZhQ_x3is8Mgg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">花一点时间来理解<strong class="bd mp">迭代策略评估</strong>的<strong class="bd mp">伪代码</strong></p></figure><p id="4c39" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们迭代更新规则，直到迭代<strong class="lc ja">中的<strong class="lc ja">值估计</strong>变化变得可以忽略</strong>。</p><blockquote class="me mf mg"><p id="a201" class="la lb lw lc b ld le ka lf lg lh kd li mh lk ll lm mi lo lp lq mj ls lt lu lv ij bi translated"><strong class="lc ja">策略控制:改进现有策略(π) </strong></p></blockquote><p id="75d1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我们的例子中，我们贪婪地对待期望值函数，这给了我们确定性的策略。从状态<strong class="lc ja"> (s) </strong>中采取具有最高值的动作<strong class="lc ja"> (a) </strong>，简单。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e6720071b31c2d591cadd378d057148f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*4xus7RdnKVe4sQsIbAWwJg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp"> argmax() </strong>函数返回<strong class="bd mp">动作</strong>，该动作可以将我们带到<strong class="bd mp">更高的值状态</strong>。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/f791be8f4603343bd99a9e36fdfb959e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A9nKRWVYfTRxLaHct1c5Bw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">arg max()的实现</strong></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="2cc3" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">策略迭代:</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9a656b074ae43463c136d087a2700800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*ilt4JYda72oe9Eit6KnHwg.png"/></div></figure><p id="f091" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">策略评估和策略改进将针对每个状态迭代执行，以改进策略和值函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/303528786612e0b91bf15a99e878ae6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07AboseYfdjqknCq0kG8Ow.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">快速浏览一下<strong class="bd mp">策略迭代</strong>的<strong class="bd mp">伪代码</strong></p></figure><p id="2e82" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">哥们……</strong></p><p id="44e8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">谁使用这种多步多扫的同步DPs，让我们用“<strong class="lc ja"> <em class="lw">【贝尔曼最优性方程】</em> </strong>把这两步结合起来，称之为<strong class="lc ja"> <em class="lw">【值迭代】</em> </strong>，并说它属于<strong class="lc ja"> <em class="lw">【广义策略迭代】</em> </strong>。</p><p id="6443" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">酷……</strong></p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="d0e1" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">价值迭代:</h1><p id="10ac" class="pw-post-body-paragraph la lb iq lc b ld nl ka lf lg nm kd li lj nn ll lm ln no lp lq lr np lt lu lv ij bi translated">所以忘了一切，来个<strong class="lc ja"> <em class="lw">值迭代</em> </strong>(不，我只是开玩笑……别忘了什么)。</p><p id="d03f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在价值迭代中，我们不运行策略评估直到完成。我们只对所有状态进行一次扫描，并贪婪地使用当前值函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0c197bf246eb57c41db2640ca9f14cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*P4vnOpTbmmG3TuijB7DUSA.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">实现<strong class="bd mp">贝尔曼最优方程</strong>的</strong></p></figure><p id="67a4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当估计值的变化变得可以忽略不计时(<theta the="" given="" policy="" will="" strictly="" converge="" into="" optimal="" class="lc ja">伙计，我没这么说，贝尔曼说了。</theta></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/eb6e2f64e8ce6117e47c896c9ccc33ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Gi8h7WzP4-vfyMzZnLuqw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">贝尔曼最优性方程</strong>作为更新规则</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8239886503e1ba7ff90195fb135dcbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*UF6u-Oi0eNN4xq0bwyrC_Q.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">实现<strong class="bd mp">值函数</strong>的</strong></p></figure><p id="aeb4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，<strong class="lc ja"> <em class="lw">更新规则</em> </strong>不是指任何特定的策略<strong class="lc ja"> <em class="lw">而是最大化当前值估计</em> </strong>的动作。</p><p id="a696" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">唯一等待的事情就是<em class="lw"> C </em> <strong class="lc ja"> <em class="lw">所有的价值函数</em> </strong>和评估<strong class="lc ja"> <em class="lw">应用策略</em> </strong> <em class="lw"> : </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/4cdad1d5443039e5c7325aec7acbfc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIW0Gu02D7KJH5bsyTX4EQ.png"/></div></div></figure><h2 id="c0e2" class="oa mu iq bd mv ob oc dn mz od oe dp nd lj of og nf ln oh oi nh lr oj ok nj iw bi translated">解决方案:</h2><p id="ef46" class="pw-post-body-paragraph la lb iq lc b ld nl ka lf lg nm kd li lj nn ll lm ln no lp lq lr np lt lu lv ij bi translated">从策略中，我们从给定的状态中提取价值和要采取的行动。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/55311f81a6de797e5c83993ba363d99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*rVTjG4ybSNOrn6FQD8c3kg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">确定性策略</strong></p></figure><p id="8c71" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们将策略应用到环境中，并运行100集:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cdcb8cc3c74d869871ed134e0e5a377f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*GY6K--oDkQ4iof6BfGzaTg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">执行<strong class="bd mp">政策</strong>到<strong class="bd mp"> FrozenLake8x8-v0 </strong></strong></p></figure><h2 id="3ed7" class="oa mu iq bd mv ob oc dn mz od oe dp nd lj of og nf ln oh oi nh lr oj ok nj iw bi translated">结论:</h2><p id="4ff5" class="pw-post-body-paragraph la lb iq lc b ld nl ka lf lg nm kd li lj nn ll lm ln no lp lq lr np lt lu lv ij bi translated">我们看到了价值迭代的公式，并提取了达到目标的最优策略，并在OpenAI的FrozenLake环境上实现了相同的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/5731313acac129b06a418c171e334c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/1*tdVBlvD0e8jk-MGa2cpjEA.gif"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><strong class="bd mp">代理使用最优策略</strong>并在<strong class="bd mp">的8个步骤</strong>中达到目标……但受到<strong class="bd mp">环境动态</strong>的影响</p></figure><h2 id="e825" class="oa mu iq bd mv ob oc dn mz od oe dp nd lj of og nf ln oh oi nh lr oj ok nj iw bi translated">带走:</h2><p id="1a07" class="pw-post-body-paragraph la lb iq lc b ld nl ka lf lg nm kd li lj nn ll lm ln no lp lq lr np lt lu lv ij bi translated">即使我们得到了一个<strong class="lc ja"> <em class="lw">严格收敛的最优策略</em> </strong>，给定了<strong class="lc ja"> <em class="lw">环境动力学</em> </strong>，智能体总是达不到目标。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="56e0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">从下一篇文章开始，我们将使用能够更好地制定最优策略的算法。请继续收听更多…</p><p id="128c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">获取<a class="ae ms" href="https://github.com/SaiSugunSegu/Reinforcement-Learning-Journey" rel="noopener ugc nofollow" target="_blank">算法</a>的完整代码。</p><p id="1999" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">参考资料:</p><p id="114d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[1] <a class="ae ms" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=pd_sbs_14_t_0/147-7865363-3852556?_encoding=UTF8&amp;pd_rd_i=0262039249&amp;pd_rd_r=96f1b22e-1339-4f4f-93ff-26c787125b93&amp;pd_rd_w=vDJJ6&amp;pd_rd_wg=y1LXN&amp;pf_rd_p=5cfcfe89-300f-47d2-b1ad-a4e27203a02a&amp;pf_rd_r=ACR43W905JY9PYNWDRRS&amp;psc=1&amp;refRID=ACR43W905JY9PYNWDRRS" rel="noopener ugc nofollow" target="_blank">强化学习:导论|第二版，作者理查德·萨顿&amp;安德鲁·g·巴尔托</a>。</p><p id="1a09" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[2] <a class="ae ms" href="https://www.youtube.com/watch?v=2pWv7GOvuf0" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗的RL课程:DeepMind </a>。</p><p id="a0b5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[3] <a class="ae ms" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">开艾健身房</a>。</p><p id="4e76" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">来源:</p><p id="8cab" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[4] <a class="ae ms" href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">强化学习专业:阿尔伯塔大学</a>。</p><p id="b499" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[5] <a class="ae ms" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank">深度强化学习:加州大学伯克利分校</a></p></div></div>    
</body>
</html>