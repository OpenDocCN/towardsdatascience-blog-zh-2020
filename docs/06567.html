<html>
<head>
<title>Predicting Reddit Flairs using Machine Learning and Deploying the Model using Heroku — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习预测Reddit Flairs并使用Heroku部署模型—第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-reddit-flairs-using-machine-learning-and-deploying-the-model-on-heroku-part-1-574b69098d9a?source=collection_archive---------39-----------------------#2020-05-24">https://towardsdatascience.com/predicting-reddit-flairs-using-machine-learning-and-deploying-the-model-on-heroku-part-1-574b69098d9a?source=collection_archive---------39-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/12f38e0b5f1df573c402e9aaacee6c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aqbP1LCb4uguPK6y"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Kon Karampelas 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="6bae" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/reddit-flair-prediction" rel="noopener" target="_blank"> Reddit天赋预测系列</a></h2><div class=""/><div class=""><h2 id="9e84" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">问题定义和数据收集</h2></div><p id="7ebc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你被困在付费墙后面，点击<a class="ae jg" rel="noopener" target="_blank" href="/predicting-reddit-flairs-using-machine-learning-and-deploying-the-model-on-heroku-part-1-574b69098d9a?source=friends_link&amp;sk=4777eb0bfa202bd98305739c04534ade">这里</a>获取我的朋友链接并查看这篇文章。</p><p id="554d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Reddit是一个非常受欢迎的社交媒体网站，大约有3.3亿活跃用户，它产生了大量的<strong class="lj jt">用户生成内容</strong>，像我这样的数据科学家喜欢挖掘和分析这些内容。我最近完成了一个使用Reddit数据的项目，我打算谈谈我的经历以及我解决问题的过程。这将帮助任何正在寻找一个<strong class="lj jt">端到端机器学习项目</strong>的人。我将向您介绍收集数据、分析数据、构建模型、部署模型并最终使用Heroku将其上传到服务器的过程。到本系列结束时，您将已经使用了许多Python模块、API和方法，这将使您在自己的机器学习之旅中更加自信。</p><p id="0394" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我已经把这个项目分成了几个部分，希望我能够用3-4个部分来介绍它。欢迎来到本系列的第1部分，我将向您介绍这个问题的背景，并执行这项任务的数据收集方面。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="bb2c" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated"><strong class="ak">背景</strong></h1><p id="cdb1" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">在着手解决实际问题之前，这项任务需要一些领域知识。对于那些从来没有去过reddit网站的人，我强烈建议你去看看，因为它真的会帮助你分析你将要收集的数据。在这项任务之前，我从未使用过reddit，在我理解为什么某些东西对我的模型不起作用之前，我花了一些时间来适应这个网站。</p><h2 id="1b6a" class="nh ml jj bd mm ni nj dn mq nk nl dp mu lq nm nn mw lu no np my ly nq nr na jp bi translated">reddit和subreddit到底是什么？</h2><p id="a817" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">我经常发现自己试图回答这个问题，因此让我尽可能简单地为你解答。本质上，它是一个论坛的集合，人们可以在这里分享新闻和内容，或者对其他人的帖子发表评论。<a class="ae jg" href="https://www.digitaltrends.com /social-media/what-is-reddit/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>被分成超过100万个被称为“子Reddit”的社区，每个社区涵盖不同的主题。subreddit的名称以/r/开头，这是reddit使用的URL的一部分。比如<a class="ae jg" href="https://www.reddit.com/r/nba/" rel="noopener ugc nofollow" target="_blank"> /r/nba </a>是人们谈论国家篮球协会的子编辑，而<a class="ae jg" href="https://www.reddit.com/r/boardgames/" rel="noopener ugc nofollow" target="_blank"> /r/boardgames </a>是人们讨论桌游的子编辑。</p><p id="a9a5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">出于我们分析的目的，我们将使用'<a class="ae jg" href="https://www.reddit.com/r/india/" rel="noopener ugc nofollow" target="_blank"> India </a>'子编辑，因为我来自印度，这个帖子上有很多内容。你可以自由选择你喜欢的线。</p><h2 id="0302" class="nh ml jj bd mm ni nj dn mq nk nl dp mu lq nm nn mw lu no np my ly nq nr na jp bi translated">Reddit Flairs</h2><p id="832b" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">reddit上的另一个功能是<strong class="lj jt"> flair </strong>，我们将在接下来的分析和预测中使用它。flair是一个“标签”,可以添加到reddit网站的子reddit中的帖子中。它们帮助用户了解文章所属的类别，并帮助读者根据他们的偏好过滤特定类型的文章。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="d356" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">使用PRAW收集Reddit数据</h1><p id="915c" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">所以，让我们言归正传。任何机器学习任务都需要你给它输入数据。我们将通过编写一个脚本来从印度收集数据。这些数据将在问题的未来部分用于构建分类器。为此，我们将使用一个名为PRAW的专用库，它是Reddit API的Python包装器，使您能够从子编辑中抓取数据。</p><h2 id="6e1b" class="nh ml jj bd mm ni nj dn mq nk nl dp mu lq nm nn mw lu no np my ly nq nr na jp bi translated">安装PRAW</h2><p id="dfc6" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">我们可以在终端中使用pip或conda安装PRAW:</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="ff0e" class="nh ml jj nx b gy ob oc l od oe">pip install praw <br/>or <br/>conda install praw<br/>or<br/>pip3 install praw</span></pre><p id="8cb8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦安装了库，您就可以通过</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="4ab5" class="nh ml jj nx b gy ob oc l od oe">import pandas as pd # data manipulation <br/>import praw # python reddit API wrapper</span></pre><p id="716b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，在我们开始使用praw收集数据之前，我们必须对自己进行身份验证。为此，我们需要创建一个Reddit实例，并输入<code class="fe of og oh nx b">client_id</code>、<code class="fe of og oh nx b">client_secret</code>和<code class="fe of og oh nx b">user_agent</code>作为参数。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">创建Reddit实例</p></figure><p id="5c5d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">认证部分已经从<a class="ae jg" rel="noopener" target="_blank" href="/scraping-reddit-data-1c0af3040768">这篇文章</a>中引用，我想给作者<a class="ok ol ep" href="https://medium.com/u/b986eefd54ba?source=post_page-----574b69098d9a--------------------------------" rel="noopener" target="_blank">吉尔伯特·坦纳</a>满分，因为这是我从那里学到这项技术的原始文章。</p><p id="68bd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了获得认证信息，我们需要通过导航到<a class="ae jg" href="https://www.reddit.com/prefs/apps" rel="noopener ugc nofollow" target="_blank">该页面</a>并点击<strong class="lj jt">创建应用</strong>或<strong class="lj jt">创建另一个应用来创建reddit应用。</strong></p><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/8ab8c5ac02fb03abdeec60cf8ab1040f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m_SzE6w-GjnJ85mK.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图1: Reddit应用程序[1]</p></figure><p id="638f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这将打开一个表单，您需要在其中填写名称、描述和重定向uri。对于重定向uri，你应该选择<code class="fe of og oh nx b">http://localhost:8080</code>，正如优秀的<a class="ae jg" href="https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application" rel="noopener ugc nofollow" target="_blank"> PRAW文档</a>中所描述的。</p><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/fab2f6ca127aba4fa99f936f63864192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j-S73cy40hu1_F1R.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图2:创建新的Reddit应用程序[1]</p></figure><p id="d35e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">按下<strong class="lj jt">创建应用程序</strong>后，会出现一个新的应用程序。在这里，您可以找到创建<code class="fe of og oh nx b">praw.Reddit </code>实例所需的认证信息。</p><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/1cf44b4b627e5be05962e40cdc6f6759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*faispm8XoFGqOjpc.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图3:认证信息[1]</p></figure><p id="7ec9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦获得了这些信息，就可以将它添加到上面的代码段中，并创建一个<code class="fe of og oh nx b">praw.Reddit</code>的实例。Reddit类提供了对Reddit API的方便访问。该类的实例是通过PRAW与Reddit的API交互的门户。你可以在这里阅读更多关于这个类及其方法的内容。</p><h2 id="67d8" class="nh ml jj bd mm ni nj dn mq nk nl dp mu lq nm nn mw lu no np my ly nq nr na jp bi translated">获取子编辑数据</h2><p id="ad21" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">使用上面的实例，我们可以从我们想要的子编辑区(即印度)获得前1000个帖子或1000个热门帖子或最新帖子。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">收集热门帖子并展示它们</p></figure><p id="5194" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">结果将如下所示:</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="35f6" class="nh ml jj nx b gy ob oc l od oe">Will donate thrice the number of upvotes (amount in Rs.) i get for this thread in next 24 hours<br/>Indian reply to NYtimes cartoon on Paris climate accord by Satish Acharya.<br/>The essence of the Indian soap opera, distilled into one GIF.<br/>This looks legit..<br/>German exchange Student at IIT Madras is being sent back home by the Indian immigration department because he joined the protest.<br/>Tragedy of India<br/>Today's The Hindu<br/>Irrfan Khan dies at 54<br/>If you are not moved by this picture, I wish I had your heart. [NP]<br/>.... </span></pre><p id="712a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以类似的方式，我们可以使用下面的代码获得“最热”或“最新”的帖子。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="cc9f" class="nh ml jj nx b gy ob oc l od oe">hot_posts = reddit.subreddit(‘India’).hot(limit=num_of_posts)</span><span id="4e0d" class="nh ml jj nx b gy op oc l od oe">new_posts = reddit.subreddit(‘India’).new(limit=num_of_posts)</span></pre><p id="35c2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">物体<code class="fe of og oh nx b">hot_posts</code>、<code class="fe of og oh nx b">top_posts</code>和<code class="fe of og oh nx b">new_posts</code>属于<code class="fe of og oh nx b">ListingGenerator</code>类，更多可以在这里读到<a class="ae jg" href="https://praw.readthedocs.io/en/latest/code_overview/other/listinggenerator.html" rel="noopener ugc nofollow" target="_blank">。现在让我们将数据以结构化格式存储在一个<code class="fe of og oh nx b">pandas.DataFrame</code>和一个<code class="fe of og oh nx b">.csv</code>文件中，以便进一步分析。</a></p><p id="88e6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们需要为每个线程创建一个从reddit网站提取的特性列表。详细名单可以在<a class="ae jg" href="https://praw.readthedocs.io/en/latest/code_overview/models/submission.html#submission" rel="noopener ugc nofollow" target="_blank">这里</a>找到。每个特性都是<code class="fe of og oh nx b">Submission</code>类的一个属性。我已经描述了我收集的一些，但是你可以自由增减你的列表。</p><p id="718a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些是我们将从数据集中提取的数据列。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">被擦除数据的功能描述</p></figure><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">提取数据并转换为数据帧</p></figure><p id="e73b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是我们的数据框架:</p><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/08e4d7ebac2252fe32ce4f129671b32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLXIAbwbLKSPMQAgKcx2-w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">包含抓取数据的数据帧</p></figure><p id="9157" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下一步是修复我们的<code class="fe of og oh nx b">created_on</code>字段，因为它目前用<a class="ae jg" href="https://en.wikipedia.org/wiki/Unix_time" rel="noopener ugc nofollow" target="_blank"> UNIX时间</a>表示。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">将UNIX时间转换为人类可读的日期时间</p></figure><p id="4277" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出:</p><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/269fc8b08a058bc7c8edb43e2741283b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdLArHJLfcjBt5JVIAVMlQ.png"/></div></div></figure><p id="a7c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然从数据搜集的角度来看这个结果是可以接受的，但是从ML的角度来看，它有一些问题。在分组和value_counts等基本数据分析之后，人们会发现这些数据非常不平衡，并且偏向某些类。此外，我们还可以通过收集评论数据来收集更多信息。有时仅仅是标题和正文可能还不够，添加评论可以增加很多关于文章及其所属风格的有价值的信息。那么，让我们来看看一种即兴的数据收集方法。</p><h2 id="26d7" class="nh ml jj bd mm ni nj dn mq nk nl dp mu lq nm nn mw lu no np my ly nq nr na jp bi translated">数据收集:即兴</h2><p id="0126" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">我要做的第一件事是创建更少数量的分类目标，这样就不会有信息溢出，也不用花费几个小时来收集信息。这意味着我将有更多的数据为每个天赋，因此，我可以建立更强大的模型，并提高准确性。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="86ab" class="nh ml jj nx b gy ob oc l od oe">posts[‘Flair’].value_counts().sort_values(ascending=False)</span></pre><p id="4125" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我按降序排列了这些文件，并挑选了最受欢迎的文件，以避免数据失真。我将收集热门帖子及其评论，以及一些与分析和模型相关的其他信息。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">相关标志</p></figure><p id="6ba3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">值得注意的是，这些标记在我写这篇文章时是相关的，在你进行分析时可能会改变，所以相应地创建这个列表。我保留了或多或少相同的功能，增加了一个评论字段。</p><figure class="ns nt nu nv gt iv"><div class="bz fp l di"><div class="oi oj l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">收集数据的第二种方法</p></figure><p id="62dc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的代码段中，<code class="fe of og oh nx b">replace_more()</code>方法有助于处理<code class="fe of og oh nx b"><a class="ae jg" href="https://praw.readthedocs.io/en/latest/tutorials/comments.html" rel="noopener ugc nofollow" target="_blank">MoreComments</a></code>对象，如果不检查的话会导致错误。该方法替换或删除了<code class="fe of og oh nx b">MoreComments</code>。您可以设置<code class="fe of og oh nx b">limit=None</code>获取所有评论，而设置<code class="fe of og oh nx b">limit=0</code>删除所有评论。由于时间限制，我选择了<code class="fe of og oh nx b">limit=0.</code>。最后，从ML的角度来看，还有一个非常重要的步骤。在这种方法中，数据被聚类成相似的flair类型，因为我们将它们一个接一个地追加到一个列表中。很有可能在分割训练和测试数据之后，在训练或测试数据中甚至没有一个特定天赋的例子。这可能会导致明显的性能问题。让我们通过对随机排列的数据进行采样来解决这个问题。通过设置<code class="fe of og oh nx b">frac=1</code>，我们得到全部数据。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="6fd1" class="nh ml jj nx b gy ob oc l od oe"># Data Shuffling<br/>data = posts.sample(frac=1).reset_index(drop=True)<br/>data.head()</span></pre><figure class="ns nt nu nv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/406a2ade68908aaf5ec76422c9557365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTesAdc3MjORXX_EB5wNFQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">最终数据帧</p></figure><p id="fd70" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，将数据帧保存为. csv格式，以便进一步分析。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="13ba" class="nh ml jj nx b gy ob oc l od oe"># Save Data in a CSV file<br/>data.to_csv('data.csv')</span></pre><p id="01f7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这让我结束了本教程。这里有一个<a class="ae jg" href="https://github.com/prakharrathi25/reddit-flair-predictor/blob/master/notebooks/Collecting%20'India'%20subreddit%20data.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>到我原来的笔记本。在下一部分，我将分析这些数据并建立模型。我还将创建NLP管道，以加快数据分析。在<a class="ae jg" rel="noopener" target="_blank" href="/predicting-reddit-flairs-using-machine-learning-and-deploying-the-model-using-heroku-part-2-d681e397f258"> <strong class="lj jt">第二部分</strong> </a> <strong class="lj jt">中继续阅读该项目。</strong>你可以在这里找到本系列<a class="ae jg" href="https://towardsdatascience.com/tagged/reddit-flair-prediction" rel="noopener" target="_blank">的所有文章。</a></p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="3e51" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">参考</h1><ol class=""><li id="346b" class="ot ou jj lj b lk nc ln nd lq ov lu ow ly ox mc oy oz pa pb bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/scraping-reddit-data-1c0af3040768">https://towards data science . com/scraping-Reddit-data-1c 0af 3040768</a></li><li id="b7cb" class="ot ou jj lj b lk pc ln pd lq pe lu pf ly pg mc oy oz pa pb bi translated"><a class="ae jg" href="https://praw.readthedocs.io/en/latest/code_overview/praw_models.html" rel="noopener ugc nofollow" target="_blank">https://praw . readthedocs . io/en/latest/code _ overview/praw _ models . html</a></li><li id="8021" class="ot ou jj lj b lk pc ln pd lq pe lu pf ly pg mc oy oz pa pb bi translated">【https://www.digitaltrends.com/web/what-is-reddit/】</li></ol></div></div>    
</body>
</html>