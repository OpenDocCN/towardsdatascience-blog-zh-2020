# 自然语言处理中的命名实体识别

> 原文：<https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=collection_archive---------3----------------------->

## 真实世界的用例、模型、方法:从简单到高级

![](img/5a203987f6e48fe4c1f92a5fea4541be.png)

玛丽安·龙在 [Unsplash](https://unsplash.com/s/photos/text-message?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

在自然语言处理中，命名实体识别(NER)是识别和提取文本中特定类型实体的问题。如*人*或*地名*。事实上，任何有名字的具体“事物”。任何程度的特异性。*职位名称*，*公立学校名称*，*体育名称*，*音乐专辑名称*，*音乐人名称*，*音乐流派*，…你明白了。

正如我们将在下面看到的，NER 既是 NLP 中一个有趣的问题，也有许多应用。首先，我们来看一个具体的例子。

**例子**

从

```
Wimbledon is a tennis tournament held in the UK in the first two weeks of July every year. In 2019, the men’s singles winner was Novak Djokovic who defeated Roger Federer in the longest singles final in Wimbledon history.
```

我们可能会提取实体

```
**Person name(s)**: Novak Djokovic, Roger Federer
**Sport name(s)**: tennis
**Country name**: UK
**Month**: July
**Year**: 2019
```

让我们思考一下如何使用上面例子中提取的实体(和想象中的扩展)。这将揭示一些将在 NER 的各种用例中发挥作用的想法。然后我们将讨论用例——在这些想法的“肩膀”上。

发现课文主题:这个例子是关于网球运动和两个运动员的。似乎少了些关于国家(*英国*)，或者月份(*七月*)，或者年份( *2019* )。从一整篇文章中，也就是说，远远不止两句话，我们也许能够强化(或反驳)这种评价。

例如，如果文章中频繁提到*诺瓦克·德约科维奇*，可能就是关于他的。另一方面，如果*网球*被频繁提及，而每个球员只被提及几次，那么这篇文章可能更多地是关于网球的。如果文章中还提到了其他人的名字，那就更是如此了，每个名字只出现几次。

如果 *2019* 在文章中被反复提及，或许文章讲的就是当年的锦标赛。

发现实体之间的关系:假设我们有一个大的语料库。比如罗杰·费德勒这个名字经常出现在网球运动附近。推断这两者有关联是合理的。事实上，这个推论只是该类型的一个具体实例:

```
“statistical co-occurrence” (usually with some qualification added) implies association. 
```

在我们的例子中，限定条件是我们把自己局限于实体，而不是文本中任意的单词或短语。另外，我们知道每个实体的类型。

我们应该给一个推断出的关联赋予什么样的力量？这里就不赘述了。好奇的话谷歌*关联规则*、*信心*、*解除*。

因此，从一个大的语料库中，通过高质量的 NER 方法，我们可以构建一个所谓的*实体图*。图形的节点是实体，带有附加的属性，如实体类型。该图是有向的。图上的每个弧都有一个属性，该属性捕捉有向关联的强度。弧可能有额外的元数据。

接下来，我们来讨论具体的用例。

**搜索**

一个重要的是在网络搜索中。通过理解查询中的实体，搜索引擎可以提供更好的结果。谷歌使用这种方法。

让我们更详细地讨论这个问题。传统上，搜索引擎使用一种叫做*关键词搜索*的方法。该文件是按其中出现的词来编索引的。通过计算哪些查询词出现在文档中并适当地加权每个命中，为文档与查询的相关性评分。命中，即出现在文档中的查询词，通过称为 TF-IDF 的因子来加权。这是基于该词在文档中的频率，并根据其在语料库中的稀有程度进行调整。因此，如果查询包含一个不常见的单词，并且该单词在文档中出现得非常频繁，则该文档与查询的相关性很高。有道理对吗？

关键词搜索肯定很有用。然而，可以通过增加实体识别来进一步改进它。此外，这还利用了在文档中找到的关于主要*实体*的知识，以及对它们在该文档中所扮演角色的重要性的判断。

除了提高相关性，这种方法还有另一个主要好处。如果查询包含已识别的实体，搜索引擎还可以返回关于它们的关键事实。读者可能已经在谷歌上看到了这一点。输入*巴拉克·奥巴马*作为搜索词。搜索引擎理解这个实体。它使用这种理解来返回关于奥巴马的其他关键事实。比如从他的维基百科页面上搜集到的信息。还有他的照片。

由实体*图*(除了实体识别之外)支持的搜索引擎可以进一步提高搜索相关性。他们可以提供更好的建议，比如人们所看到的“人们也在搜索”的那种。

**索引文件**

另一个重要的应用是对网页或在线文章等文本文档进行编目或索引。这包括提取文档中的重要实体，并使用这些实体对文档进行索引。在线杂志和新闻出版商经常这样做。当然还有搜索引擎。

下面是考虑这种相关性的一种方法。文档更可能是关于文档中频繁提及的实体，而不是频繁提及的任意单词。也就是说，实体检查充当质量过滤器。

**产品评论**

第三个用例是在零售环境中从在线评论中提取产品名称或其他重要实体。这些评论可能来自电子商务网站或社交媒体网站。很明显，了解特定评论中讨论的是哪些产品和功能是很有用的。作为一个例子，考虑审查

```
I like my new XYZ smartphone but the camera sucks
```

*XYZ 智能手机*和*相机* 是我们想要提取的实体。我们还想提取情感词*，如*和 *sucks* ，并将每个词与正确的实体相关联。情绪分析超出了本文的范围。除了说情感也可以被建模为实体。例如，我们可能有两种实体类型*正面情感*和*负面情感*，其中*类似于*前者的一个实例*吸收了*后者的一个实例。情感分析已经被广泛研究。还有更先进的方法。

然后，评论可以被自动分类到它们所应用的产品或其他突出的实体下。由此产生的分组也可以揭示新的见解。比如人们抱怨(或称赞)最多的是哪些产品或功能。

**电子邮件中的实体**

第四个用例是从电子邮件中检测和提取实体，并自动触发某些操作。许多读者可能已经在他们自己的智能设备上看到了这种情况。举个例子，假设你收到一封来自航空公司的电子邮件，确认你购买了一张机票。最先进的电子邮件引擎可以自动提取相关信息(航空公司、出发日期和时间等)，并从中在您的数字日历中创建一个条目。这里的实体是*航空公司名称*、*日期*和*时间*，其中*日期*和*时间*标记为*出发*。

**从语料库中构建结构化数据库**

第五个用例是通过从语料库中识别所需类型的实体来构建结构化数据库。例如，我们可以通过抓取 web 并识别这种类型的实体来建立医疗疾病名称的数据库。(顺便说一下，纯粹基于字典的识别方法是行不通的，因为我们要找的结果就是那个字典。)

这种专门的名单是有市场的。有一些利基供应商已经通过销售这样的列表赚了一段时间的钱。

**走向构建本体**

第六个用例实质上是从一个文档集(比如网页或公司内部的文档)构建一个实体图。它也可以被看作是第五种用例的增强版本，其中实体之间的关系也被推断出来。

节点是被识别的实体。弧来自足够频繁地足够接近的实体对。对于弧上更精细的点，如权重，请参见前面的讨论。

除了前面提到的实体图的用途，深度和丰富的特定领域本体作为数据产品提供也有市场，例如，在利基电子商务网站上改进浏览和搜索。自动实体识别和 arcs 推理与人工管理的某种结合具有很大的潜力。有利基公司在这个领域赚钱。

**实体识别算法**

接下来，我们讨论识别和提取实体的各种方法，从最简单的到更高级的。我们还讨论了它们的优点和局限性。我们还描述了如何以及为什么组合一些互补的简单方法来构建更高级的方法。

**基于字典的**

这是最简单的一种。对于要识别的每种实体类型，我们都有一个值字典。为了识别和提取实体，我们只需扫描文本并在各种字典中找到匹配项。命中还揭示了实体类型，因为我们知道被命中的字典。

这里有一个简单的例子。

假设我们有以下字典:

```
**Month** = January, February, …, July, …, December
**Country** = USA, United States, UK, United Kingdom, France, …
**Person First Name** = John, Roger, Jim, …
**Person Last Name** = Smith, Green, …
```

考虑我们之前看到的文本。

```
Wimbledon is a tennis tournament held in the UK in the first two weeks of July every year. In 2019, the men’s singles winner was Novak Djokovic who defeated Roger Federer in the longest singles final in Wimbledon history.
```

提取的实体是

```
**Month** = July, **Person First Name** = Roger, **Country** = UK
```

啊，令人惊讶——提取完全不完整！事实上，在较小的程度上，这甚至发生在现实世界的 NER。我们正在体验现实。

好吧，让我们更深入地挖掘…首先，实体识别只和字典一样好。诺瓦克在我们人的名字字典里是缺失的，所以它被遗漏了。字典缺乏完整性和维护它们的挑战无疑是这种方法中的一个主要问题。第二，我们选择为一个人的名和姓准备单独的字典，而不是为一个人的全名准备一个字典。由于人的名和姓在某种程度上是独立的，后一种类型的字典将会非常庞大。组装和维护更具挑战性。考虑到这种选择，我们需要某种机制将一个人的名和姓组合成全名。事实上，它可能是基于模式的。也就是说，我们已经看到了结合基于字典和基于模式的方法的效用。

基于字典的方法的另一个重要问题，尽管在我们的例子中没有出现，是它们不能很好地适应歧义。相同的值存在于多个字典中。考虑一下*卡特*。这通常是一个人的姓，但有时也可以是一个人的名。

当然，我们可以把它放在两个字典里。我们不会捕捉到它更有可能是姓而不是名的概率。如果没有捕捉到这样的概率，模糊值的数量越多，字典就会被污染得越严重。我们将无法区分模糊的成员和重要的成员。

**基于模式的**

假设我们希望在一些文本中提取美国电话号码。如*(123)456–7890*。使用基于字典的方法没有意义。这将需要在字典中存储所有可能的美国电话号码。包括所有格式变化，如 *123 456 7890* 。

通过模式匹配可以更好地识别这些实体。通常，这是使用正则表达式完成的。

**例子**

考虑文本

```
Recognize all these as US phone numbers: (123) 456 7890, 456 7890, 123–456–7890, +1 (123) 456 7890.
```

这个例子的目的是揭示要识别的模式确实开始变得错综复杂。现在，将全球各地的国际电话号码添加到此列表中。现在，这变得非常复杂。按照软件工程的说法，复杂场景中的模式匹配有三个涉及“软件”的问题，这里是正则表达式。

1.  首先，它们更难调试
2.  它们通常是不透明的(难以理解)。
3.  它们很难维护，也就是说，当需要更改时(例如，当发现错误或需要添加新的场景，如新国家的电话号码时)很难更新。

尽管有上面的 1-3，模式匹配是当实体被结构模式最好地描述时使用的方法。其他此类实体的示例有*电子邮件地址*、*URL*和*美国邮政编码*。

**概率词典**

前面我们提到过，基于字典的方法不能令人满意地适应歧义。我们的例子是卡特和 T21。可以是一个人的名，也可以是一个人的姓。因此，它应该被收入两部词典。然而这样做隐含地假设卡特同样可能是一个人的名或姓。这种不必要的推断会对实体识别结果的质量产生不利影响。

幸运的是，基于字典的方法很容易增强以适应歧义。对于字典中的每个值，我们都附加了一个属性，即它的频率。如果一个值的频率不可用，我们将它默认为 1。频率是正数，但也可以是分数。

这里有一个例子。比方说，我们有 10 个出现的*卡特*，其中 2 个是名，8 个是姓。(这些数字是虚构的。)我们可以将*卡特*放在两个字典中，并在此人的名字典中分配频率 0.2，在此人的姓字典中分配频率 0.8。(或者我们可以只使用实际计数 2 和 8 作为频率。)把这想象成 *Carter* 的默认频率 1 按比例分配给两个实体类型。

好的，我们给字典中的每个值都加上了频率。我们实际上如何*使用*来预测一个值的实体呢？我们选择对该值具有最高概率的字典。我们在下面正式描述这一点。

考虑值 *v* 。对*P*(*D*|*v*)的简单经验估计是 *D* 中 *v* 的频率除以 *v* 在所有字典上的频率之和。

让我们看一个数字例子。下面是三本字典。

```
D1      D2     D3
**a** 10    **a** 1    e 1
b 20    c 5    f 5
e 15
```

我们应该预测什么作为 *a* 的实体类型？在 11 次出现的 *a* 中，10 次在 D1，1 次在 D2，没有一次在 D3。所以 D1 的可能性最大，D2 次之，D3 最小。所以我们有把握在 2011 年 10 月 11 日预测 D1 是一个实体。(注意，我们互换使用术语*实体类型*和*字典*。)

**间接，通过贝叶斯法则**

有时，我们有一些在频率中没有捕捉到的关于字典的额外的先验知识。或者在我们的频率中某个特定值 *v* 的总频率太小，以至于我们不愿意相信我们的经验概率估计。

考虑这个例子。说我们有两本字典:*名*和*姓*。一个合理的先验是(1/2，1/2)。也就是说，在我们看到它之前，两者都同样可能是特定值的实体类型。考虑补偿值 *Xincal* 。说它只出现一次，在字典里*的第一个名字*。已知 *Xincal* 的*名字*的经验概率为 1。鉴于我们只见过 *Xincal* 一次，我们真的应该那么确定 *Xincal* 的实体吗？并且鉴于*名*和*姓*实体类型同样丰富。

贝叶斯规则让我们将数据证据与先验信念结合起来，以缓解这个问题。也就是说，对于给定的值，我们找到字典的概率不仅取决于我们的数据，还取决于我们先前的信念。

更详细地说，我们使用贝叶斯规则估计后验概率

*P*(*D*|*v*)=*P*(*v*|*D*)*P*(*D*/*P*(*v*)

这里的*P*(*v*|*D*)是字典 *D.* 中值 *v* 的似然性，从经验上讲，它就是 *D* 中 *v* 的频率除以 *D* 中所有值的频率之和。

接下来我们注意一下*P*(*v*)= sum _*D*'*P*(*v*|*D*')P(*D*')。所以我们可以在推断 *v* 的可能字典的过程中，实时计算 *P* ( *v* )。

**回到经验后验，添加伪计数**

在明确调用贝叶斯规则后，我们发现实际上并不需要使用它。有一种简单、直观的方法可以有效地产生相同的行为。

考虑我们的(*名*，*姓*)例子。假设我们有一个先验的信念，认为两种实体类型的可能性是相等的。我们可以通过为每一个值 v 向两个字典中的每一个添加一个伪计数 *n* 来表明这一信念。把 *n* 想象成字典中一个值的初始频率。然后将该值的实际观察频率加到 *n* 上。

一旦我们有了伪计数和字典中的计数，我们就可以像以前一样精确地计算字典给定值的经验概率。

什么是 *n* ？它影响了我们先前信念的强度。n 越大，我们先前的信念就越强。也就是说，我们需要越多的证据来修正这个信念。也就是说，即使是 *n* = 1 通常也是对 *n* = 0 的改进，因此不值得为选择而烦恼。

伪计数是有理论支持的。特别是，它们的行为就好像在模拟一个所谓的狄利克雷先验。这个话题我们在本帖就不多说了。我们建议您使用谷歌。

**例子**:考虑我们的*名*和*姓*的例子。考虑值 *Xincal* 。假设我们选择 *n* = 1。*名*和*姓*中的 *Xincal* 的伪频率各为 1。现在加上 *Xincal* 的实际频率，在*名*中为 1，在*姓*中为 0。所以组合频率是

```
Xincal                  first name (2)             last name (1) 
```

所以*名*名*名*的后验概率是 2/3。实体类型*的名字*仍然获胜，尽管由于先前的信念而具有比以前更低的置信度。

在这个例子中，我们对伪计数的选择可以被解释为获取了下面的先验信念。如果一个值在我们的数据集中只出现一次，我们就不要用概率 1 来预测它出现的字典了。

**复合实体**

我们所说的复合实体是指本身由其他实体组成的实体。这里有两种类型的例子:

```
**Person name**: John K Smith | Dr. John Smith | John Smith, jr | John Smith, PhD**Street Address**: 10 Main St, Suite 220 | Hannover Bldg, 20 Mary St
```

在每种情况下，竖线分隔实体值。

复合实体有一个重要的特例，我们称之为多令牌实体。在本节关于复合实体的(长)讨论之后，我们将单独讨论这种情况。我们以这种方式组织内容，因为深入复合实体将有助于我们在随后关于多令牌实体的讨论。对多标记实体感兴趣的读者应该在“看够了”当前的内容后，向下滚动到该部分。

当处理复合实体时，它有助于区分两个计算问题。

**解析又名分解**:将一个复合实体分解成它的组件实体。这里有一个例子。

```
Dr. John K Smith ⇒ {**salutation** =Dr., **first_name** = John, **middle_name** = K, **last_name** = Smith}
```

**识别**:这里复合实体嵌入在一些非结构化的文本中。任务是识别实体的边界以及它的类型。并且可能还将它解析(即分解)成它的成分。这里有一个例子。

```
I looked up Dr. John K Smith. His street address came up as 10 Main St, Suite 220 in San Francisco.
```

我们希望我们的认可产生以下结果:

```
Dr. John K Smith      ⇒ **person_name**
10 Main St, Suite 220 ⇒ **street_address**
```

区分解析和识别的一个原因是不同的用例映射到一个或另一个。例如，我们可能有一个完整的人名数据库，并试图将每个人的名字解析成各个部分(名、姓等)。这不涉及任何承认。只是解析。

另一个原因是“各个击破”。假设我们的任务是识别非结构化文本中的复合实体并解析它们。两阶段方法——首先识别实体边界及其类型，然后解析识别出的实体——是有意义的。

我们并不是说两阶段方法会比同时进行识别和解析的集成方法更准确，只是说当您想要快速获得合理有效的解决方案，并且有更简单的方法来分别解决这两个问题时，这种方法值得考虑。

在这篇文章的剩余部分，我们将重点解析复合实体。

**解析/分解的方法**

首先，我们想观察复合实体的解析可能被框架化为一个序列标记问题。复合实体被适当地标记化，以产生标记序列。与令牌相关联的标签是其组成实体。下面是复合实体人员姓名的示例。

```
**Token sequence**: Dr. John K Smith
**Label sequence**: salutation first_name middle_name last_name
```

我们假设我们有(*标记序列*，*标签序列*对形式的训练集。这类似于固定维度空间中监督学习的训练集，除了这里我们的输入和标签是(对齐的)*序列*。

为什么要以这种方式对复合实体进行模型解析？主要原因是标签序列中隐藏着可以有效利用的信息。

假设我们有一个丰富的(*标记序列*，*标签序列*)对数据集，可用于学习解析人名。有用的信号仅隐藏在标签序列中。*称呼*通常出现在*名字*之前，而不是之后。*名字*通常在*姓氏*之前，而不是之后。

**训练和推理**

好的，我们有一组训练序列(*标记序列*，*标签序列*)对。我们的任务是从这个数据集中学习一个机器学习模型，以便当一个令牌序列是一个输入时，可能是一个在训练期间从未见过的输入，该模型输出它的最佳标签序列。同样，这类似于向量空间中的监督学习，除了我们的输入和输出是*序列*。

**隐马尔可夫模型**

这是最广为人知的建模序列标记问题的方法。它非常适合我们的需要。话虽如此，它确实做了一个强有力的假设，马尔可夫假设。有时，这可能会过度限制。对更高级的方法感兴趣的读者，或者有更长距离交互的用例(即具有长令牌序列的复合实体，其中相距很远的令牌相互影响)的读者，也应该阅读*条件随机场*。

好吧，回到 HMMs。我们将说明如何设置它来解析人名实体。在结构上，HMM 是一个有向无环图，具有节点(也称为*状态*)和弧(也称为*转换*)。实际上还有更多。节点可以从一些字母表中发出可观察到的信号。

HMM 具有附加到各种节点和弧的参数。这些参数形成合适的概率分布。更详细地说，一个州的排放是由可观测字母表上的概率分布决定的。这种分布是特定于州的。从一个状态的转移由状态的概率分布来控制。这些分布特定于起始状态。简而言之，从一个州你必须去某个地方。状态转移的概率分布捕捉了我们对下一步应该去哪里(哪个状态)的偏好。

**用于解析人名**

这有点抽象。让我们把它具体化。这个练习也将揭示 HMM 中的“H”，它代表隐藏的*。*

*我们将描述一个 HMM 来解析一个人全名的简化版本。具体来说，我们将只解析与以下正则表达式格式匹配的名称。*

```
*[salutation_word] first_name_word [middle_name_word] last_name_word*
```

*让我们读出这个正则表达式的内容。全名可以选择以称呼开头，然后是名，再然后是中间名，最后是姓。所有这些实体都必须是单字实体。*

***HMM vs 基于模式的解析***

*第一个问题，为什么不直接用这个正则表达式解析？也就是说，为什么不应用基于模式的方法来解决这个解析问题。如果你的目标是快速开发一个轻量级的方法，你可以。*

*基于 HMM 的方法在以下方面更强大。*

***部分解析**:考虑输入*史密斯*。基于 HMM 的方法可能会将其解析为 last_name = *Smith* 。这是因为它也模拟实体概率。(详情如下。)基于模式的方法无法做到这一点。它无法知道史密斯更有可能是姓而不是名。*

***更好地解决歧义**:考虑输入*史密斯博士*。基于 HMM 的方法可能会将其解析为**称呼**=*T21 博士**姓氏** = *史密斯*。基于模式的方法可能会将其解析为**名字** = *医生*和**姓氏** = *史密斯*。再说一次，因为它没有对实体概率建模，所以它无法知道 *Dr.* 更可能是称呼而不是名字。**

***附上解析置信度得分**:考虑两个输入*约翰·史密斯*和*史密斯·约翰*。与基于模式的方法一样，基于 HMM 的方法可以很好地将这些分别解析为{ **名字** = *约翰*、**姓氏** = *史密斯* }和{ **名字** = *史密斯*、**姓氏** = *约翰* }。基于 HMM 的解析可能比第二个解析更信任第一个解析，因为它知道 John 更可能是名而不是姓，Smith 更可能是姓而不是名。*

*这样的置信度分数可以用来寻找低置信度的解析(也许可以区别对待它们)。它们也可能暴露数据问题。就像在这个例子中，名字和姓氏似乎颠倒了。*

***人名解析 HMM:结构***

*好的，让我们回到定义 HMM 匹配的正则表达式。*

```
*[salutation_word] first_name_word [middle_name_word] last_name_word*
```

*由此，我们将确定 HMM 的状态为*称呼字*、*名字字*、*中间名字*和*姓氏字*。对此，我们将增加两种状态:*开始*和*结束。**

*我们也将使用正则表达式来确定弧。下面是弧线*

```
*begin → salutation_word 
begin → first_name_wordfirst_name_word → middle_name_word
first_name_word → last_name_wordlast_name_word → end*
```

*计算机科学家认为 HMM 的结构定义了一个有限状态自动机。*

*在离开这个主题之前，我们想指出的是，可以直接从训练集推断 HMM 的结构(节点和转换)。这可以更加灵活。随着训练集的发展，结构也可以发展。*

*我们基于正则表达式预先确定结构的主要原因是教学上的。它允许我们将对 HMM 结构的讨论与对其学习(关于其发射和转换的概率参数)的描述分开。*

*我们还想指出的是，即使训练集原则上可以确定结构，在查看训练集之前，考虑用现有的领域知识初始化结构也是有帮助的。这使得人们可以将领域知识(对于 Bayesians 人来说，是先前的信念)用于建模。这可以弥补训练集中的弱点。*

***培训***

*接下来，我们将说明从训练集中训练 HMM 的参数。*

*正如我们前面提到的，训练集是(*标记序列*，*标签序列*)对的集合。从这样的训练集中，在 HMM 中学习发射和转移概率。这两种概率的训练分解了，我们很快就会看到。我们可以称这样的训练为*模块化*。按照培训说明，我们将触及模块化培训的一些好处。*

*让我们看一个例子。它有三个训练实例。注意，我们已经将*开始*和*结束*状态添加到侧翼。这些将在训练中发挥作用，我们很快就会看到。*

```
 *Dr.        John       K          Smith
begin salutation first_name middle_name last_name end John      Smith
begin first_name last_name end John       Kent       Smith
begin first_name middle_name last_name end*
```

**开始*和*结束*状态被称为静默，因为它们不发出任何令牌。*

***训练转移概率***

*为了训练这些，我们只需要看状态序列。就好像我们的训练集*

```
*begin salutation first_name middle_name last_name end
begin first_name last_name end
begin first_name middle_name last_name end*
```

*从状态 *a* 到状态 *b* 的转移概率，即在弧 *a* → *b* 上的转移概率，仅仅是状态 *a* (不包括任何作为状态序列中最后一个状态的状态)被状态 *b* 所接替的概率。*

*让我们来看看一些已知的转移概率。*

```
*begin → salutation       ⅓ 
first_name → middle_name ⅔ 
first_name → last_name   ⅓*
```

*状态*开始*的三个事件之一之后是状态*问候*。状态*名*的三次出现中有两次跟随状态*名*，一次跟随状态*名*。*

***训练发射概率***

*为了训练这些，我们丢弃序列信息，并且仅在每次令牌*令牌*从状态*状态发出时跟踪(*状态*，*令牌*)对。*就好像训练集是*

```
***state       emitted token**
salutation  Dr.
first_name  John
first_name  John
middle_name K
middle_name Kent
last_name   Smith
last_name   Smith*
```

*从状态 *s* 发出令牌 *t* 的概率正好是发出令牌 *t* 的状态 *s* 出现的分数。*

***模块化训练***

*一个国家可能发出的不同价值观的数量是相当大的。例如，state *first_name* 可以发出数百万个不同的值。每一个都需要被明确地建模，以便将名字从诸如 *xyz* 这样的无意义的字符串中区分出来。*

*期望(*记号序列*，*标签序列*)对的训练集足够丰富以覆盖所有可能的名字是不现实的。幸运的是，不一定是这样。可以独立于转换来训练发射。只要我们有每个实体(状态)值的丰富字典，最好是增加频率信息，我们就可以从中学习发射概率。*

*另请注意，相比之下，这种爆炸并不适用于学习转移概率。少量的精选状态序列可以充分地捕获大多数全名的序列结构。如果希望能够解析各种地区的人名,“小”可能不像人们想象的那么小。例如西班牙语和阿拉伯语。尽管如此，即使在这种情况下，手动管理训练集也是可行的。*

***推论***

*好了，现在我们有了经过训练的 HMM，或者至少我们可以想象它，让我们来看一个推理的例子。推理的特定目标是输入一系列标记并输出最佳解析。为了很好地定义这个过程，我们需要(I)一个评估令牌序列特定解析质量的核心函数和(ii)一个寻找最高分解析的*搜索过程*。我们需要一个搜索过程的原因是，穷举搜索的方法——对所有可能的解析进行评分，并选择一个得分最高的解析——可能太慢，即使只有几个标记。*

*黄金标准搜索程序是一种称为维特比算法的动态编程形式。它隐式最大化的得分函数是(*记号序列*，*标签序列*)对的联合概率。令牌序列是固定的；标签序列是可变的，每次解析一个。维特比算法操作*就好像*正在对给定标记序列的所有可能的标签序列进行穷举搜索，只是它要快得多。*

*这里我们将不正式描述维特比算法。对于这篇已经很长的文章来说，它太复杂了。然而，我们将通过一个例子来说明它工作的某些方面。不可否认的是以一种粗略的方式。我们的希望是，那些第一次阅读它的人会足够好奇，从其他地方了解更多(见下面的参考资料)，而那些已经阅读过它的人会从我们的插图中找到一些见解，以增加他们的理解。*

*我们将展示的输入是约翰·史密斯博士。我们想要描述的第一个关键思想是维特比算法通过维护一个矩阵来操作，该矩阵的行索引 HMM 的状态，其列索引输入中的记号。在我们的例子中，这个矩阵的结构如下*

```
 *Dr.           John        Smith
begin       1
salutation  0    P(b)P(s|b)P(Dr.|s)
first_name  0                         ?
middle_name 0
last_name   0
end         0          0*
```

*什么进入矩阵的每个单元，它是如何到达那里的？这太复杂了，在我们分配给它的空间里很难详细说明和解释。因此，我们在一些单元格中放入了值或公式，我们将在下面解释。*

*让我们从第一栏开始。那里的价值观很容易解读。为了解析输入，HMM 必须从状态*开始*开始。这在第一列中表示为“处于状态 *begin* 的概率是 1，处于任何其他状态的概率是 0”。*

*接下来，我们来看看细胞[ *称呼*，*博士*。这里的值是解析到第一个标记并以状态 *salutation* 结束的概率。(在我们的例子中，“在状态*称呼*中结束”意味着*博士*是从*称呼*中发出的。)*

*这个概率就是从状态*开始*，过渡到状态*称呼*，从*称呼*发出*T21 博士的概率。这是我们在那间牢房里看到的。**

*现在让我们看看什么进入了单元格[*first _ name*，*John*。我们在那里放了一个问号，因为这个表达太复杂了，无法以一种抓住所有正在发生的重要事情的方式放在那里。所以我们在这里只讨论它们。*

*首先，让我们提醒自己，我们希望*什么进入这个单元格。它是前两个记号的联合概率和这些记号的最佳解析。说什么？**

*让我们慢慢来。首先，从现在开始，让我们将对上述联合概率的引用缩写为“这些记号的最佳解析的概率”。这损失了一些精确性，但是可读性更好。根据检查，您认为最好的解析是什么？【*开头*，*称呼*，*名字*对吗？这个解析的概率就是进入这个单元格的内容。*

*接下来是一个关键的观察。我们不会证明的。这对理解这种算法是如何工作的至关重要。这个解析的概率是[*begin，salutation*是对[ *Dr.* ] 的解析的概率乘以扩展这个解析以覆盖从 *first_name* 发出的 *John* 的概率。我们已经计算了粗体部分，并将其结果存入矩阵的单元格[ *salutation* ，*dr*]中。所以我们只需要将这个单元格的值乘以上述扩展的概率，即从状态*称呼*转换到状态*名字*的概率乘以从*名字*发出*约翰*的概率。这种对先前计算的概率的重用是该算法相对于穷举搜索实现加速的关键。*

*还有一个问题。在前两段中，为了便于解释，我们假设我们知道[ *Dr.* ， *John* ]的最佳解析是[ *begin* ， *salutation* ， *first_name* ]。算法当然不会。我们该如何应对？*

*首先，让我们明确我们*不*知道什么。我们确实知道 *John* 必须从 *first_name* 中发出，因为我们正在尝试填写该单元格的值。我们不知道的是[ *Dr.* ]的最佳解析以及它的最终状态。姑且称这个未知的‘T42’为‘博士’的状态吧。对于*博士*状态的每个可能值，我们知道形式为*开始*、*博士*状态】的*博士*的最佳解析的**概率。对于其中的每一个，我们都乘以扩展解析成为[ *begin* ， ***Dr.* 的 state** ， *first_name* ]的概率。这里唯一的变量是博士的州，所以因为我们想要最佳解析的概率，我们只取*博士*的州的所有值的最大概率。请注意，在这些计算中，我们以粗体重复使用了概率，因为在此之前，它们已经被计算并存储在矩阵中。***

***恢复最佳解析***

*我们还没完。在我们填充了矩阵之后，我们所知道的就是完整输入的最佳解析的概率。它位于矩阵的右下角。我们不知道实际的解析。*

*为了恢复一个实际的(得分最高的)解析，首先，在构建得分矩阵的“自底向上”阶段，我们捕获一些额外的元数据。一旦构建了矩阵，我们就使用这个元数据来查找实际的解析。*

*让我们看看分数矩阵，其中一些单元格增加了上述元数据。让我们用它自己的矩阵来描述它，我们称之为*回指矩阵*。这个矩阵具有与分数矩阵相同的结构。唯一的区别是它的单元格不存储分数，而是存储其他单元格的索引。因此得名“反向指针”。*

*下面是填充了一些值的反向指针矩阵。来解释一个吧。单元格[2，2]包含值[1，1]。这捕获了[ *博士*、*约翰*的最佳解析的信息，其中状态*名字*被约束为从状态*称呼*发出*约翰*发出*博士*。该信息在计算分数矩阵中相应单元格的分数的过程中出现。特别是在计算各项的最大值的过程中。*

```
 *0        1        2        3
                        Dr.     John     Smith
0 begin
1 salutation           [0,0]
2 first_name                    [1,1]
3 middle_name
4 last_name                              [2,2] 
5 end                                    [4,3]*
```

*接下来，让我们看看如何使用这个矩阵来找到一个实际的最佳解析。我们从右下角的单元格开始，即单元格[5，3]。从这个单元格，我们沿着一条路径回到单元格[0，0]。在我们的例子中，路径是*

```
*[5,3] → [4,3] → [2,2] → [1,1] → [0,0]*
```

*该路径包含逆向恢复(最佳)解析的信息。这在下面描述。*

```
*[5,3]    →    [4,3]    →    [2,2]    →    [1,1]   →    [0,0]
                     Smith          John         Dr.
        end        last_name      first_name   salutation     begin*
```

*在每一个反向指针步骤中，我们揭示反转令牌序列中一个令牌的*状态*。在步骤[5，3] → [4，3]中，我们恢复[，end]。令牌丢失，因为*端*处于静默状态。在步骤[4，3] → [2，2]中，我们从状态 4 ( *姓氏*)移动到另一个状态(状态 2)，同时从令牌 3 ( *史密斯*)移动到令牌 2。这些移动产生了一对(*史密斯*，*姓氏*)。事情就是这样。*

*最后一步是反转解析，这在堆栈的帮助下很容易做到。*

***多令牌实体***

*多标记实体是复合实体的一种特殊情况，其中实体的值跨越多个标记，但实体类型本身不会分解为更精细的实体类型。(或者我们不在乎。)*

*这里有两个这样实体的例子:*国家公园名称*和*医疗疾病名称*。让我们来看一些例子:*

```
***national park names**: Yellowstone national park, Yosemite national park, Grand Canyon national park, ...**disease names**: Parkinson's disease, Type I diabetes, heart disease, alzheimer's disease*
```

*在多标记实体中，没有解析问题，只有识别问题。也就是说，目标是识别流动文本中的实体及其边界。就像*

```
*I traveled to **Yellowstone National Park** and after that went to Colorado.* 
```

***方法***

***基于字典的***

*这是最简单的一种。然而，当字典不完整时，它就不起作用了，当我们试图首先使用实体识别来构建一个丰富的字典时，情况就是这样。(参见本文前面描述的用例**从语料库**构建结构化数据库。)*

***监督二元分类***

*这是解决这个问题的一个有吸引力的方法。训练实例可以是一个短短语，标记为*正*(实体的实例)或*负*(不是实体的实例)。我们还会选择合适的特征从输入短语中提取出来。(稍后将详细介绍这一点。)*

*一个重要的考虑是如何为训练集选择负实例。负面例子的世界是巨大的——所有短语都有一定的长度(应该是什么？).从宇宙中随机取样可能会给我们微弱的否定。因此，相对于我们对标记数据集的训练测试分割的评估，学习的分类器可能比我们想象的更弱。重点是否定实例不是真实的——我们通过特定的过程构建了它们——而这个过程可能并不代表我们在这个领域中得到的实际否定，它可能是文本中足够短的短语，而不是这个实体的实例。*

*相比之下，添加表面上看起来像正面的人为负面信息可能会产生更准确的分类器。这里有一个关于实体*国家公园名称*的负面例子:*城堡石国家公园*。这是一个公园，但不是国家公园。*

*前面段落的讨论还意味着，无论我们如何仔细地挑选否定，分类器准确性的最终评估都应该基于它在实际中如何使用的实际文本上的实体识别准确性。*

*再一次，这是因为我们正在从一个巨大的空间中挑选负面因素，我们不能排除选择偏差。第二个重要原因是，领域中的类别比例，特别是在操作期间输入到分类器的短语的比例是负的，将严重依赖于它所应用的文本。考虑疾病名称。金融文档中作为疾病名称的短语的比例可能远低于 WebMD 文档中的比例。因此，两者的假阳性率可能大不相同。*

***特征工程***

*考虑识别国家公园的名称，即将它们与其他短语区分开来。我们已经看到的例子表明，一个有目的的启发式单词袋变体作为特征。具体来说，首先，我们将输入标记为单词，然后将最后两个单词组合成一个二元模型，并捕获它们是最后两个单词。这很容易用例子来说明。*

```
*Yellowstone National Park --> {yellowstone, 'national_park_end'}
Castle Rock State Park    --> {castle,rock,'state_park_end'}*
```

*为什么我们把最后两个词组合成一个二元结构？因为将它们分开会有更高的假阳性率。比如*黄石国家银行*(杜撰的例子)。*

*值得注意的是，这一计划提供了一些灵活性，如果我们在稍后阶段决定将国家纪念碑也视为国家公园，我们只需要增加一些积极因素。恐龙国家保护区就是一个例子。我们所选择的特征将选择这些。*

*这些特征利用了国家公园名称的结构。他们在其他多标记问题上的表现取决于它的结构是否与国家公园的名称相匹配。考虑疾病名称。当然，最后一个词通常是*积极*对*消极*的预测。(参见我们之前列出的疾病名称示例。)也就是说，将最后两个词组合成一个二元模型可能不够概括。这是因为在疾病名称中，只有最后一个词可能是提示词。(如*病*。)使用国家公园名称特征将不会检测到以*单词 disease* 结尾的疾病，除非训练集具有一些以*单词 disease* 结尾的阳性。*字*可能是一个很具体的字。这就是为什么我们说这些特征不能很好地概括疾病名称。*

***分类器算法***

*既然我们已经定义了我们的标记训练集和特征，我们原则上可以使用任何二进制分类器算法来解决这个问题。两个想到我们的逻辑回归和朴素贝叶斯。两者我们都不深究。*

***备选方案:HMM***

*解决这个问题的另一种方法是将其公式化为一个序列标记问题，尽管有一点扭曲。这值得考虑，因为多标记实体识别肯定具有顺序结构。也就是说，它确实需要一些关于原始实体是什么的想法。另外，我们如何将 HMM 转换成二进制分类器。*

*想想国家公园的名字。以黄石国家公园为例。假设我们将其对应的状态序列建模为*name _ word geo _ level _ word park _ type _ word*。也就是说，作为一个(令牌序列，标签序列)实例，这将被表示为*

```
 *Yellowstone  National        Park
begin    name_word    _national_      park_type_word     **positive***
```

*现在考虑一个负面的:*城堡石州立公园*。它的标签实例可能如下所示。*

```
 *Castle     Rock      State    
begin    name_word  name_word _state_                   **negative***
```

*再来看一个负面:*莫卡冰淇淋*。它的标签实例可能如下所示。*

```
 *Mocca    Ice      Cream
begin    word     word     word                        **negative***
```

*我们的 HMM 会有我们在这三个标签序列中看到的状态。这里的想法如下。对于任何特定的输入，我们将强制一条路径以状态*正*或*负*结束。如果在某些输入上从 *begin* 可以到达这两种状态，那么我们希望我们的 HMM 结构和训练将有利于通向输入的类(*正*或*负*)的路径。*

***讨论***

*上一节提供的是这种解决方案的关键要素。在实际环境中，这种方法可能需要进一步完善。*

*例如在状态(例如进一步细化的状态*词*，也许是词性:*专有名词*，*名词*，*动词*，*形容词*，*冠词*，…*

*或者放松一阶马尔可夫假设。最常见的情况是，条件随机场也是如此。*

***延伸阅读***

*[](https://medium.com/data-science-in-your-pocket/named-entity-recognition-ner-using-conditional-random-fields-in-nlp-3660df22e95c) [## 自然语言处理中使用条件随机场的命名实体识别(NER)

### 在深入讨论了自然语言处理中的词性标注算法之后，是时候开始研究自然语言处理中的信息抽取了

medium.com](https://medium.com/data-science-in-your-pocket/named-entity-recognition-ner-using-conditional-random-fields-in-nlp-3660df22e95c)*