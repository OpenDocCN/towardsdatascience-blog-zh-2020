# 事件表中的重叠时间段问题

> 原文：<https://towardsdatascience.com/overlapping-time-period-problem-b7f1719347db?source=collection_archive---------40----------------------->

在数据科学领域，人们往往认为清理数据很无聊，并希望更多的机器学习和建模挑战，但有时可能会出现一些问题，如有趣的脑筋急转弯或数据清理期间的算法难题。这是我最近在清理活动桌时遇到的一个有趣的例子。它不像疯狂的 ML 算法或新的很酷的 python 工具，而是一个值得花几分钟去解决的有趣的小问题。

当我们构建任何类型的数据管道或 ML 管道时，尤其是在处理预测问题时，事件表是非常常见的。事件表类似于事务表，通常具有开始时间、结束时间、人员 ID 和其他事件属性。每一行都是一个事件，一些事件可能会重叠，这可能是一个问题。现实世界中的事件表可能如下所示:

```
 person_id      start        end
0          1 2017-01-01 2017-06-01
1          1 2017-02-01 2017-03-01
2          1 2017-05-01 2017-08-01
3          1 2017-09-01 2017-10-01
4          1 2017-11-01 2018-02-01
5          1 2018-01-01 2018-03-01
```

有时我们可以忍受，但有时我们希望将那些重叠的行合并成一行，最早开始时间和最晚结束时间如下:

```
 person_id      start        end
0          1 2017-01-01 2017-08-01
1          1 2017-09-01 2017-10-01
2          1 2017-11-01 2018-03-01
```

也就是说，由于第二个事件和第三个事件都与第一个事件重叠，即使第三个事件不与第二个事件重叠，我们希望合并的事件具有第一个事件的开始时间和第三个事件的结束时间。这看起来很简单，但要找出一个好的解决方案并不容易。

在这里，我用 python 来解决这个问题。

**简单案例:**

在第一种解决方案中，我们可以将此视为图连通性问题。每个事件都是一个节点。如果两个事件重叠，那么它们用一条边连接。因此，前三个事件将形成一个连通子图，第三个事件是一个孤立节点类子图，第四个和第五个事件将形成另一个连通子图。我们在原始数据帧中给每个子图分配一个 group_id。现在，我们可以按 group_id 分组，并按 min(开始时间)和 max(结束时间)聚合，然后我们就有了答案。

为了实现这一点，我们将创建一个邻接矩阵来表示该图。首先，我们要定义“重叠”。这里的重叠是指:对于任意两个节点 A 和 B，如果节点 A 的开始日期在节点 B 的结束日期之前，节点 A 的结束日期在节点 B 的开始日期之后，那么 A 和 B 重叠。我们可以用`numpy`数组实现逻辑来创建一个稀疏矩阵。存在几种算法来解决连通图问题。我们可以使用一个现成的功能`scipy.sparse.csgraph.connected_components`来实现这一点。`scipy`用类似于 Tarjan 算法的东西实现函数。然后，我们只要做简单的熊猫`groupby`方法就能得到我们想要的结果。

就其数学方法而言，这是一个很好的解决方案。那只是一个人的。如果我们有数千或数百万人，这通常是真实世界的情况，图形将会变得非常大，空间复杂性将以 n 为单位增长。我们仍然可以通过在创建邻接矩阵的行中添加另一个“&”逻辑来实现这一点。

理论上，Tarjan 算法的时间复杂度是线性的。`n^2`空间的复杂性使其无法扩展。这让我思考如何和`pandas`一起用可扩展的方式解决。

老实说，我不是`pandas`的粉丝，因为它的查询方式不直观，但是如果我们有一个矢量化的解决方案，它确实会提供很好的性能。

矢量化的解是这样的。

让我们忘掉所有的图、节点和边，用一些纯粹简单的时间序列数据来重新思考这个问题。首先，我们打破开始时间和结束时间，通过排序创建一维时间序列。我们有一列`start_end`来表示是开始时间还是结束时间，其中`1`是开始时间，`-1`是结束时间。然后，我们对列`cumsum`中的序列执行累积求和。通过查看，我们注意到新的开始时间是其`cumsum`为`1\. A` 的开始时间，新的结束时间是其`cumsum`为`0`的结束时间。所以现在我们有了一个`new_start`列。接下来，我们对`new_start`列执行另一个累积求和来得到`group_id`。最后，我们做同样的汇总得到答案。

看一下`mergedf`可能更容易理解这里发生了什么。

```
 person_id       time  start_end  cumsum  new_start  group
0          1 2017-01-01          1       1       True    1.0
1          1 2017-02-01          1       2      False    1.0
1          1 2017-03-01         -1       1      False    1.0
2          1 2017-05-01          1       2      False    1.0
0          1 2017-06-01         -1       1      False    1.0
2          1 2017-08-01         -1       0      False    1.0
3          1 2017-09-01          1       1       True    2.0
3          1 2017-10-01         -1       0      False    2.0
4          1 2017-11-01          1       1       True    3.0
5          1 2018-01-01          1       2      False    3.0
4          1 2018-02-01         -1       1      False    3.0
5          1 2018-03-01         -1       0      False    3.0
```

**性能**

![](img/d98f253c8daf78d6fd5a7bedaf99588d.png)![](img/fca527dbaac15879aff1ae96299adb3d.png)

由于我不想太深入的研究 scipy 的`connected_component`的实现，分析它的复杂性，所以我只想用一种实证的方式比较一下可扩展性。我所做的只是将示例`n`重复几次，以创建一个大的事件表，并随着`n`数量级的增长来测量运行时间和峰值内存。

在 n 达到 10 之前，这两种方法在运行时和内存上都有点相似。在那之后，很明显，图表方法很快在运行时和内存中爆炸。我实际上是在一台 200GB RAM 的机器上用`n=100000`运行图形方法时遇到了内存泄漏。肯定有一些方法可以优化图形方法，这是我很想看到的，但我在这里试图指出的是，scipy 的`connected_component`背后的操作比 pandas 中的矢量化更加占用内存。我猜有时候丑陋的算法可能会打败美丽的算法。

```
 size    mem     runtime       approach
0        10     82    0.083125          graph
1        50     83    0.111390          graph
2       100     85    0.094990          graph
3       500    150    0.458318          graph
4      1000    305    1.544724          graph
5      5000   6964   27.119864          graph
6     10000  27576  113.710234          graph
7    100000    inf         inf          graph
8        10     82    0.092768  vectorization
9        50     83    0.125092  vectorization
10      100     83    0.121607  vectorization
11      500     86    0.091670  vectorization
12     1000     89    0.168066  vectorization
13     5000    101    0.154213  vectorization
14    10000    115    0.224327  vectorization
15    50000    216    1.523057  vectorization
16   100000    351    2.482687  vectorization
17   500000   1407    6.840897  vectorization
18  1000000   2707   12.607009  vectorization
```