<html>
<head>
<title>Understanding And Implementing Dropout In TensorFlow And Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow和Keras中理解和实现辍学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-implementing-dropout-in-tensorflow-and-keras-a8a3a02c1bfa?source=collection_archive---------14-----------------------#2020-05-18">https://towardsdatascience.com/understanding-and-implementing-dropout-in-tensorflow-and-keras-a8a3a02c1bfa?source=collection_archive---------14-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d970" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">技术的</h2><div class=""/><div class=""><h2 id="6ec5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Dropout是一种常见的正则化技术，在计算机视觉任务(如姿态估计、对象检测或语义分割)的最新解决方案中得到利用。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a79ee89c1cbc9b8b3c65dcd8b4cfde84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqEjms7CIOA34eSUW_OJNg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">约翰·马特丘克在<a class="ae lh" href="https://unsplash.com/s/photos/stop?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="445c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="0db2" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">本文介绍了dropout技术的概念，这是一种在深度神经网络(如递归神经网络和卷积神经网络)中使用的技术。</p><p id="f5b2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">丢弃技术包括在每个训练步骤中从神经网络中省略充当特征检测器的神经元。每个神经元的排除是随机确定的。</p><p id="8540" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">G.E Hinton在2012年发表的论文中提出了这个简单的技术:“<a class="ae lh" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nb">通过防止特征检测器</em></a><em class="nb"/>的共同适应来改善神经网络。</p><p id="1ae3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">在本文中，我们将深入揭示辍学的概念，并了解如何使用TensorFlow和Keras在神经网络中实施这一技术。</strong></p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="bd8d" class="li lj it bd lk ll nj ln lo lp nk lr ls ki nl kj lu kl nm km lw ko nn kp ly lz bi translated">了解辍学技巧</h1><p id="1bdc" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">神经网络在其输入和输出层之间有隐藏层，这些隐藏层中嵌入了神经元，神经元内的权重以及神经元之间的互连使神经网络系统能够模拟类似学习的过程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/8bc6e8f009963d83c2c3a8994a48e5aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2mxCvj2A3-SufMSLRGrwQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用<a class="ae lh" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://playground.tensorflow.org/</a>构建简单的神经网络</p></figure><p id="7d2b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">一般的想法是，神经网络架构中的神经元和层越多，其代表能力就越强。这种表示能力的提高意味着神经网络可以拟合更复杂的函数，并很好地概括训练数据。</p><p id="264a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">简单地说，在神经网络层中，神经元之间的互连有更多的配置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/169a92ce6b2c8a1334c541eb9fead34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRrI7xoXVZG0uBRU8u_YGQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用<a class="ae lh" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://playground.tensorflow.org/</a>构建的复杂神经网络</p></figure><p id="f418" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">利用更深层次的神经网络的缺点是它们非常容易过度拟合。</p><p id="b645" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">过度拟合是一个常见的问题，它被定义为经过训练的机器学习模型无法很好地推广到看不见的数据，但同一模型在它接受训练的数据上表现良好。</p><p id="4357" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">退出的主要目的是最小化训练网络中过度拟合的影响。</p><p id="03ba" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd"> Dropout技术通过随机减少神经网络中互连神经元的数量来工作。在每一个训练步骤中，每个神经元都有可能被遗漏，或者更确切地说，从连接的神经元的整理贡献中被遗漏。</strong></p><p id="b396" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种技术最小化了过度拟合，因为每个神经元变得独立地足够，在某种意义上，层内的神经元学习不基于其相邻神经元的合作的权重值。</p><p id="e686" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">因此，我们减少了对大量互连神经元的依赖，以从训练好的神经网络中产生像样的表示能力。</p><p id="58e3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">假设你训练了7，000个不同的神经网络架构，要选择最好的一个，你只需取所有7，000个训练过的神经网络的平均值。</p><p id="7e43" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">辍学技术实际上模拟了这个场景。</p><p id="0341" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果神经元在训练步骤中被遗漏的概率被设置为0.5；我们实际上是在每个训练步骤中训练各种不同的网络，因为在任何两个训练步骤中排除相同的神经元是非常不可能的。因此，利用退出技术训练的神经网络是在每个训练步骤中出现的所有不同神经元连接组合的平均值。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="0563" class="li lj it bd lk ll nj ln lo lp nk lr ls ki nl kj lu kl nm km lw ko nn kp ly lz bi translated">实际场景</h1><p id="50c7" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在实际场景中，或者当测试利用了对看不见的数据的丢弃的已训练神经网络的性能时，需要考虑某些项目。</p><p id="2b4d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">第一个事实是<strong class="mc jd">剔除技术实际上并没有在神经网络</strong>的每一层上实现；它通常在网络最后几层的神经元中被利用。</p><p id="f8c4" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在已发表的<a class="ae lh" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中进行的实验中，有报道称，在<a class="ae lh" href="https://en.wikipedia.org/wiki/CIFAR-10" rel="noopener ugc nofollow" target="_blank"> CIFAR-10数据集</a>上测试时，在最后一个隐藏层使用dropout时有15.6%的错误率。这比在相同的卷积神经网络上测试相同的数据集时报告的16.6%的错误率有所改善，但是在任何层中都没有包括丢弃技术。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4e362b35fe975374056cbbe1f164ef82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*f8u4llviQH1G71HZgNOoZw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank">TIMIT基准测试中有脱落模型和无脱落模型的错误率比较</a></p></figure><p id="f8c3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">第二点是，在实际场景中，当评估一个训练好的神经网络时，并不使用丢失。由于在评估或测试阶段没有使用丢弃，因此实现了神经网络的全部潜力。这意味着网络中的所有神经元都是活跃的，每个神经元的输入连接都比训练时多。</p><p id="a171" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">因此，期望将神经元的权重除以1，减去丢失超参数值(<em class="nb">训练期间使用的丢失率</em>)。因此，如果训练期间的辍学率为0.5，那么在测试时间内，每个神经元的权重结果减半。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="9d6d" class="li lj it bd lk ll nj ln lo lp nk lr ls ki nl kj lu kl nm km lw ko nn kp ly lz bi translated">实施辍学技术</h1><p id="fd70" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">使用TensorFlow和Keras，我们配备了工具来实现一个神经网络，该网络通过在神经网络架构中包含脱落层来利用脱落技术。</p><p id="38da" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们只需要添加一行来在更广泛的神经网络体系结构中包括一个脱落层。Dropout类有几个参数，但是现在，我们只关心“rate”参数。辍学率是一个超参数，表示在训练步骤中神经元激活被设置为零的可能性。rate参数可以取0到1之间的值。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="0ae2" class="nw lj it ns b gy nx ny l nz oa">keras.layers.Dropout(rate=0.2)</span></pre><p id="76ef" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">从这一点开始，我们将逐步实现、训练和评估一个神经网络。</p><ol class=""><li id="67d4" class="ob oc it mc b md mw mg mx mj od mn oe mr of mv og oh oi oj bi translated">利用加载工具和库，<a class="ae lh" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae lh" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a></li></ol><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="e3e1" class="nw lj it ns b gy nx ny l nz oa">import tensorflow as tf<br/>from tensorflow import keras</span></pre><p id="ca03" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">2.加载FashionMNIST数据集，归一化图像并将数据集划分为测试、训练和验证数据。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="dd50" class="nw lj it ns b gy nx ny l nz oa">(train_images, train_labels),(test_images, test_labels) = keras.datasets.fashion_mnist.load_data()<br/>train_images = train_images /  255.0<br/>test_images = test_images / 255.0<br/>validation_images = train_images[:5000]<br/>validation_labels = train_labels[:5000]</span></pre><p id="4603" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">3.使用Keras模型类API创建一个包含dropout层的自定义模型。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="b325" class="nw lj it ns b gy nx ny l nz oa">class CustomModel(keras.Model):<br/>    def __init__(self, **kwargs):<br/>        super().__init__(**kwargs)<br/>        self.input_layer = keras.layers.Flatten(input_shape=(28,28))<br/>        self.hidden1 = keras.layers.Dense(200, activation='relu')<br/>        self.hidden2 = keras.layers.Dense(100, activation='relu')<br/>        self.hidden3 = keras.layers.Dense(60, activation='relu')<br/>        self.output_layer = keras.layers.Dense(10, activation='softmax')<br/>        self.dropout_layer = keras.layers.Dropout(rate=0.2)<br/>    <br/>    def call(self, input, training=None):<br/>        input_layer = self.input_layer(input)<br/>        input_layer = self.dropout_layer(input_layer)<br/>        hidden1 = self.hidden1(input_layer)<br/>        hidden1 = self.dropout_layer(hidden1, training=training)<br/>        hidden2 = self.hidden2(hidden1)<br/>        hidden2 = self.dropout_layer(hidden2, training=training)<br/>        hidden3 = self.hidden3(hidden2)<br/>        hidden3 = self.dropout_layer(hidden3, training=training)<br/>        output_layer = self.output_layer(hidden3)<br/>        return output_layer</span></pre><p id="381a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">4.加载实现的模型并初始化优化器和超参数。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="5c01" class="nw lj it ns b gy nx ny l nz oa">model = CustomModel()<br/>sgd = keras.optimizers.SGD(lr=0.01)<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer=sgd, metrics=["accuracy"])</span></pre><p id="d714" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">5.训练总共60个时期的模型</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="19b3" class="nw lj it ns b gy nx ny l nz oa">model.fit(train_images, train_labels, epochs=60, validation_data=(validation_images, validation_labels))</span></pre><p id="f9b8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">6.在测试数据集上评估模型</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="a606" class="nw lj it ns b gy nx ny l nz oa">model.evaluate(test_images, test_labels)</span></pre><p id="aa12" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">评估结果将类似于下面的评估结果示例:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="d3d6" class="nw lj it ns b gy nx ny l nz oa">10000/10000 [==============================] - 0s 34us/sample - loss: 0.3230 - accuracy: 0.8812</span><span id="2d68" class="nw lj it ns b gy ok ny l nz oa">[0.32301584649085996, 0.8812]</span></pre><p id="d063" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">评估结果示例中显示的准确性对应于我们的模型的88%的准确性。</p><p id="c77c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过一些微调和使用更重要的历元数进行训练，精确度可以提高几个百分点。</p><p id="26da" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><a class="ae lh" href="https://github.com/RichmondAlake/tensorflow_2_tutorials/blob/master/10_dropout.ipynb" rel="noopener ugc nofollow" target="_blank">这里有一个GitHub存储库，用于存储本文中介绍的代码。</a></p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="0011" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Dropout是一种常见的正则化技术，在计算机视觉任务(如姿态估计、对象检测或语义分割)的最新解决方案中得到利用。该概念易于理解，并且通过将其包含在PyTorch、TensorFlow和Keras等许多标准机器/深度学习库中而更容易实现。</p><p id="8cf2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果您对其他正则化技术以及它们是如何实现的感兴趣，请阅读下面的文章。</p><p id="d538" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">感谢阅读。</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/how-to-implement-custom-regularization-in-tensorflow-keras-4e77be082918"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">如何在TensorFlow(Keras)中实现自定义正则化</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">了解如何使用TensorFlow和Keras相对轻松地实现自定义神经网络正则化技术。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc lb oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/batch-normalization-in-neural-networks-code-d7c9b88da9f5"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">神经网络中的批量标准化(代码)</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">通过TensorFlow (Keras)实施</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc lb oo"/></div></div></a></div></div></div>    
</body>
</html>