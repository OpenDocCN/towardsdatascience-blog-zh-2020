<html>
<head>
<title>Function Estimation with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流函数估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/function-estimation-with-tensorflow-2b2beea142b?source=collection_archive---------30-----------------------#2020-04-01">https://towardsdatascience.com/function-estimation-with-tensorflow-2b2beea142b?source=collection_archive---------30-----------------------#2020-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b30f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用优化器来估计函数变量和回归</h2></div><p id="d4e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降是微积分中的一种数学方法，它考虑空间中变量的导数，以实现值的最佳组合来最小化函数。在机器学习中，我们通常使用定义为<strong class="kh ir"> <em class="lb">预测-实际</em> </strong>的损失函数来最小化，从而给出预测问题的一组变量。有兴趣可以看这里的<a class="ae lc" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="5184" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将看到如何在 Tensorflow 中使用梯度下降来估计我们在日常研究生活中碰巧看到的函数变量。此外，我将展示一个例子，说明如何使用张量流优化器来寻找函数的最小值及其参数的最小值。</p><h1 id="e2c2" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">函数估计的示例曲线</h1><p id="745d" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">让我们考虑以下三种情况。前两个场景用于函数估计，最后一个场景将演示评估函数最小值的能力。我将使用下面的代码来生成数据。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="3738" class="mj le iq mf b gy mk ml l mm mn"># imports<br/>import tensorflow as tf<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits import mplot3d<br/>np.random.seed(50) # For reproducability</span></pre><h2 id="43f7" class="mj le iq bd lf mo mp dn lj mq mr dp ln ko ms mt lp ks mu mv lr kw mw mx lt my bi translated">集合 1</h2><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b846" class="mj le iq mf b gy mk ml l mm mn">x = np.arange(10)<br/>y = 4 * x + 3<br/>noise = np.random.normal(0,1,10)<br/>y = y + noise</span><span id="24cc" class="mj le iq mf b gy mz ml l mm mn">fig = plt.figure(figsize=(5,5))<br/>ax = sns.lineplot(x, y)<br/>plt.xlabel("X")<br/>plt.ylabel("Y")</span></pre><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ed317b475d4341e982bea61ca6337aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*iLvwpZqxj0cOsBOs9Vt18w.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">带噪声的线性函数</p></figure><h2 id="a1d5" class="mj le iq bd lf mo mp dn lj mq mr dp ln ko ms mt lp ks mu mv lr kw mw mx lt my bi translated">集合 2</h2><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="93d9" class="mj le iq mf b gy mk ml l mm mn">x = np.arange(-10, 10)<br/>y = x**3 + x**2 + 5<br/>noise = np.random.normal(0,20,20)<br/>y = y + noise</span><span id="6ca1" class="mj le iq mf b gy mz ml l mm mn">fig = plt.figure(figsize=(5,5))<br/>ax = sns.lineplot(x, y)<br/>plt.xlabel("X")<br/>plt.ylabel("Y")</span></pre><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c2201ec571173387177998584371018e.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*pHO7QfoFCBmDEmDZgqKn-g.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">带噪声的 3 次多项式</p></figure><h2 id="a27c" class="mj le iq bd lf mo mp dn lj mq mr dp ln ko ms mt lp ks mu mv lr kw mw mx lt my bi translated">第三组</h2><p id="ff0e" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在这种情况下，我们将尝试在函数最小时估计 X 和 Y 的值。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b6c8" class="mj le iq mf b gy mk ml l mm mn">def f(x, y):<br/>    return (np.sin(x) * np.cos(y)) ** 2</span><span id="750b" class="mj le iq mf b gy mz ml l mm mn">x = np.linspace(0, 3, 100)<br/>y = np.linspace(0, 3, 100)</span><span id="476c" class="mj le iq mf b gy mz ml l mm mn">X, Y = np.meshgrid(x, y)<br/>Z = f(X, Y)</span><span id="9e24" class="mj le iq mf b gy mz ml l mm mn">fig = plt.figure(figsize=(10, 10))<br/>ax = plt.axes(projection='3d')<br/>ax.contour3D(X, Y, Z, 100)<br/># ax.scatter(0,0,0, color='r', marker='^')<br/>ax.set_xlabel('x')<br/>ax.set_ylabel('y')<br/>ax.set_zlabel('z');<br/>plt.xlim(-1, 4)<br/>plt.ylim(-1, 5)<br/>ax.set_zlim(-0.5,1)<br/>ax.view_init(25, 20)</span></pre><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a3443767019519bbd05fe089759a8785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*xWL-3qCXySS3W0XGNBHeIA.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">三维绘图</p></figure><h1 id="3487" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">使用张量流优化器</h1><p id="0dd1" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">根据<a class="ae lc" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/" rel="noopener ugc nofollow" target="_blank">文档</a>，有几个优化器供我们使用。让我们来看看如何为<strong class="kh ir">集合 1 </strong>估计变量。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="59cd" class="mj le iq mf b gy mk ml l mm mn">def init():<br/>    M = tf.Variable(5.0) <br/>    C = tf.Variable(0.0) <br/>    <br/>    return M, C</span><span id="17fd" class="mj le iq mf b gy mz ml l mm mn">M, C = init()<br/>opt = tf.keras.optimizers.Adam(learning_rate=0.001)</span><span id="cd1d" class="mj le iq mf b gy mz ml l mm mn">for epoch in range(1000):<br/>    opt.minimize(lambda: sum(abs(y - M*x - C)), var_list=[M, C])<br/>    print(sum((y - M*x - C)**2).numpy(), M.numpy(), C.numpy(), end="\r")<br/>print()</span><span id="21b7" class="mj le iq mf b gy mz ml l mm mn">print(M.numpy())<br/>print(C.numpy())</span></pre><p id="0d6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的代码中，我将尝试估计函数<code class="fe nk nl nm mf b">Y = Mx + C</code>的<code class="fe nk nl nm mf b">M</code>和<code class="fe nk nl nm mf b">C</code>参数。因此，作为第一步，我创建初始化函数来返回<code class="fe nk nl nm mf b">M</code>和<code class="fe nk nl nm mf b">C</code>作为张量流变量。这意味着优化器将计算 GRAD(多变量梯度),并使用提供的学习率将其应用于张量流变量。</p><p id="bcbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意损失函数是<code class="fe nk nl nm mf b">lambda: sum(abs(y — M*x — C))</code>。这转化为平均绝对误差，与使用均方误差的回归模型非常相似。经过 1000 次迭代后，对于<strong class="kh ir"> M </strong>和<strong class="kh ir"> C </strong>，我们得到值<strong class="kh ir"> <em class="lb"> 4.4131455 </em> </strong>和<strong class="kh ir"> <em class="lb"> 0.29607454 </em> </strong>。同一平面上的两个图如下所示。</p><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi na"><img src="../Images/7b01e7df584d3d057657856d1fbe0cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*EvAhwkqwKKU2pzreOwjSAA.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">原始和估计</p></figure><p id="03ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们有一个非常类似于原始数据集的估计。接下来，我们将看看如何解决第二个问题。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="5ac9" class="mj le iq mf b gy mk ml l mm mn">def init():<br/>    A = tf.Variable(1.0) <br/>    B = tf.Variable(1.0) <br/>    C = tf.Variable(1.0) <br/>    D = tf.Variable(1.0) <br/>    <br/>    return A, B, C, D</span><span id="ec47" class="mj le iq mf b gy mz ml l mm mn">A, B, C, D = init()<br/>opt = tf.keras.optimizers.Adam(learning_rate=0.01)</span><span id="ce01" class="mj le iq mf b gy mz ml l mm mn">for epoch in range(1000):<br/>    opt.minimize(lambda: sum(abs(y - (A*x**3+ B*x**2+C*x+D))), var_list=[A, B, C, D])<br/>    print(sum(abs(y - (A*x**3+ B*x**2+C*x+D))).numpy(), A.numpy(), B.numpy(), C.numpy(), D.numpy(), end="\r")<br/>print()</span><span id="8d7b" class="mj le iq mf b gy mz ml l mm mn">print(A.numpy())<br/>print(B.numpy())<br/>print(C.numpy())<br/>print(D.numpy())</span></pre><p id="5690" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用估计量<code class="fe nk nl nm mf b">Y = Ax^3 + Bx^2 + Cx + D</code>作为目标函数。因此，我们将有四个张量变量。与前面的场景类似，我们将使用平均绝对误差进行函数估计。我们分别得到<strong class="kh ir"> A，B，C </strong>和<strong class="kh ir"> D </strong>的系数<strong class="kh ir"> <em class="lb"> 1.0356606，1.1082488，-2.7969947 </em> </strong>和<strong class="kh ir"> <em class="lb"> 8.258981 </em> </strong>。我们的图如下所示。</p><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e39cb4c04f2bd26d3fa17db4ab970546.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*OxhzpC1LpDgd7y1EZu2dhg.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">原始和估计</p></figure><h1 id="2983" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">计算最小值</h1><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="f43b" class="mj le iq mf b gy mk ml l mm mn">def init():<br/>    X = tf.Variable(1.0) <br/>    Y = tf.Variable(1.0) <br/>    <br/>    return X, Y</span><span id="67a8" class="mj le iq mf b gy mz ml l mm mn">X, Y = init()<br/>opt = tf.keras.optimizers.Adam(learning_rate=0.001)</span><span id="96f6" class="mj le iq mf b gy mz ml l mm mn">for epoch in range(1000):<br/>    opt.minimize(lambda: (tf.math.sin(X) * tf.math.cos(Y)) ** 2, var_list=[X, Y])<br/>    print(((tf.math.sin(X) * tf.math.cos(Y)) ** 2).numpy(), X.numpy(), Y.numpy(), end="\r")<br/>print()</span><span id="5e05" class="mj le iq mf b gy mz ml l mm mn">print(X.numpy())<br/>print(Y.numpy())</span><span id="fb49" class="mj le iq mf b gy mz ml l mm mn">minX = X.numpy()<br/>minY = Y.numpy()</span></pre><p id="42ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们直接使用优化器的函数值，并要求它最小化。经过 1000 次迭代后，我们分别得到<strong class="kh ir"> X </strong>和<strong class="kh ir"> Y </strong>的值<strong class="kh ir"> <em class="lb"> 0.5938998 </em> </strong>和<strong class="kh ir"> <em class="lb"> 1.5066066 </em> </strong>。再次绘图给了我们下面标有最小值的图。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="932c" class="mj le iq mf b gy mk ml l mm mn">def f(x, y):<br/>    return (np.sin(x) * np.cos(y)) ** 2</span><span id="9bc0" class="mj le iq mf b gy mz ml l mm mn">x = np.linspace(0, 3, 100)<br/>y = np.linspace(0, 3, 100)</span><span id="a695" class="mj le iq mf b gy mz ml l mm mn">X, Y = np.meshgrid(x, y)<br/>Z = f(X, Y)</span><span id="39db" class="mj le iq mf b gy mz ml l mm mn">fig = plt.figure(figsize=(10, 10))<br/>ax = plt.axes(projection='3d')<br/>ax.contour3D(X, Y, Z, 100)<br/>ax.scatter(minX, minY, f(minX, minY), color='r', marker='o', s=100)<br/>ax.set_xlabel('x')<br/>ax.set_ylabel('y')<br/>ax.set_zlabel('z');<br/>plt.xlim(-1, 4)<br/>plt.ylim(-1, 5)<br/>ax.set_zlim(-0.5,1)<br/>ax.view_init(25, 150) # Rotate to see minima better</span></pre><figure class="ma mb mc md gt nb gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a90c1d6edda6ca5284be1c18eefb64e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*mP22-_VfVUCR-udksiyPbg.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">标有最小值的图</p></figure><p id="7d37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这个图的最小值实际上是一个平面。然而，我们只计算点，因此，我们不能指望找到一个平面。一个数学上更合理的方法将为这项任务建立一个更好的近似模型。</p><h2 id="2191" class="mj le iq bd lf mo mp dn lj mq mr dp ln ko ms mt lp ks mu mv lr kw mw mx lt my bi translated">笔记</h2><ul class=""><li id="130e" class="nn no iq kh b ki lv kl lw ko np ks nq kw nr la ns nt nu nv bi translated">可能存在多个最小值，尽管梯度下降法只报告一个最小值/最大值，该值在起始点附近。随机梯度下降试图减轻这种影响。</li><li id="29e1" class="nn no iq kh b ki nw kl nx ko ny ks nz kw oa la ns nt nu nv bi translated">同样可以使用神经网络进行建模。参见<a class="ae lc" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD" rel="noopener ugc nofollow" target="_blank">文档</a>。</li></ul><p id="b8a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢阅读这篇关于梯度下降的实际应用的文章。这里，我们使用函数来获取数据进行演示。然而，这同样适用于只存在具有多个参数的数据的情况。</p><p id="f640" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">干杯！</p></div></div>    
</body>
</html>