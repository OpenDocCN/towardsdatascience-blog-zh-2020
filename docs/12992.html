<html>
<head>
<title>Custom Instance Segmentation Training With 7 Lines Of Code.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 7 行代码进行自定义实例分段训练。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-instance-segmentation-training-with-7-lines-of-code-ff340851e99b?source=collection_archive---------3-----------------------#2020-09-07">https://towardsdatascience.com/custom-instance-segmentation-training-with-7-lines-of-code-ff340851e99b?source=collection_archive---------3-----------------------#2020-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="88b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 7 行代码训练您的数据集，以实现实例分割和对象检测。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/de40bf3e7fd20e815c76c4fbfce61a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OZYuKOCiWN_ebw-QIfC2A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Eastern_Grey_Squirrel.jpg" rel="noopener ugc nofollow" target="_blank">Wikicommons.com</a>(CC0)</p></figure><p id="0bb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图像分割是计算机视觉的一个重要领域，它应用于生活的各个领域。PixelLib 是一个库，创建该库是为了方便将分段应用于现实生活中的问题。它支持 Coco 模型的对象实例分割。使用 coco 模型的分段是有限的，因为您不能执行超出 coco 中可用的 80 个类的分段。现在只需 7 行代码就可以用 PixelLib 库训练你的自定义对象分割模型。</p><p id="4e7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装 PixelLib 及其依赖项:</p><p id="57ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装 Tensorflow 与:(PixelLib 支持 tensorflow 2.0 及以上版本)</p><ul class=""><li id="9c5d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb"> pip3 安装张量流</em></li></ul><p id="52d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装 imgaug，包括:</p><ul class=""><li id="622a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb"> pip3 安装 imgaug </em></li></ul><p id="bfc2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装 PixelLib 与</p><ul class=""><li id="9fa4" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">pip3 安装 pixellib</li></ul><p id="3745" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果已安装，请使用以下工具升级至最新版本:</p><ul class=""><li id="ce8d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">pip3 安装 pixellib —升级</li></ul><h2 id="ffb0" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">训练自定义模型所需的步骤。</h2><p id="8b9b" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated"><em class="mb">第一步:</em></p><p id="924e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">准备您的数据集:</strong></p><p id="28b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的目标是创建一个可以对蝴蝶和松鼠执行实例分割和对象检测的模型。</p><p id="ddab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为要检测的对象收集图像，并为自定义训练标注数据集。<em class="mb"> Labelme </em>是用来对对象进行多边形标注的工具。创建一个根目录或文件夹，并在其中创建培训和测试文件夹。分离训练(至少 300 张)和测试所需的图像。将您要用于训练的图像放入 train 文件夹，将您要用于测试的图像放入 test 文件夹。您将注释 train 和 test 文件夹中的两个图像。下载<a class="ae kv" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.0.0/Nature.zip" rel="noopener ugc nofollow" target="_blank"> Nature 的数据集</a>作为本文中的样本数据集，解压到 images’文件夹。这个数据集将作为一个指南，让你知道如何组织你的图像。请确保数据集目录的格式与其相同。Nature 是一个数据集，有两个类<em class="mb"> butterfly </em>和<em class="mb"> squirrel。</em>每类有 300 幅图像用于训练，每类有 100 幅图像用于测试，即 600 幅图像用于训练，200 幅图像用于验证。自然是一个有 800 张图片的数据集。</p><p id="5746" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读这篇文章来学习如何使用<em class="mb"> labelme </em>来注释对象。</p><div class="na nb gp gr nc nd"><a href="https://medium.com/@olafenwaayoola/image-annotation-with-labelme-81687ac2d077" rel="noopener follow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd ir gy z fp ni fr fs nj fu fw ip bi translated">使用 Labelme 的图像注释</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">Labelme 是多边形注记最方便的注记工具之一。这篇文章解释了如何使用 labelme…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr kp nd"/></div></div></a></div><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="15bc" class="mc md iq nt b gy nx ny l nz oa">Nature &gt;&gt;train&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; image1.jpg<br/>                           image1.json<br/>                           image2.jpg<br/>                           image2.json</span><span id="3129" class="mc md iq nt b gy ob ny l nz oa">      <br/>       &gt;&gt;test&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  img1.jpg<br/>                           img1.json<br/>                           img2.jpg<br/>                           img2.json   </span></pre><p id="f1c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注释后的文件夹目录示例。</p><p id="b9b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong> PixelLib 支持用 Labelme 标注。如果您使用其他注释工具，它将与库不兼容。</p><p id="84e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">第二步:</em></p><p id="7611" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">可视化数据集</strong></p><p id="f97f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练之前可视化样本图像，以确认遮罩和边界框生成良好。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="59ba" class="mc md iq nt b gy nx ny l nz oa">import pixellib<br/>from pixellib.custom_train import instance_custom_training</span><span id="7f65" class="mc md iq nt b gy ob ny l nz oa">vis_img = instance_custom_training()</span></pre><p id="1992" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们导入了<em class="mb"> pixellib </em>，从<em class="mb"> pixellib </em>我们<em class="mb"> </em>导入了类<em class="mb">instance _ custom _ training</em>并创建了该类的一个实例。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="9457" class="mc md iq nt b gy nx ny l nz oa">vis_img.load_dataset("Nature”)</span></pre><p id="0da6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用<em class="mb"> load_dataset 函数加载数据集。</em> PixelLib 要求多边形标注为 coco 格式。当您调用<em class="mb"> load_dataset </em> <em class="mb">函数时，</em>train 和 test 文件夹中的单个 json 文件将分别转换成单个<em class="mb"> train.json </em>和<em class="mb"> test.json </em>。<em class="mb">训练</em>和测试 json 文件将作为训练和测试文件夹位于根目录中。新文件夹目录现在将如下所示:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="2c32" class="mc md iq nt b gy nx ny l nz oa">Nature &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;train&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; image1.jpg<br/>               train.json           image1.json<br/>                                    image2.jpg<br/>                                    image2.json</span><span id="6fe2" class="mc md iq nt b gy ob ny l nz oa">       &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;test&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; img1.jpg<br/>                  test.json           img1.json<br/>                                      img2.jpg<br/>                                      img2.json</span></pre><p id="d1fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<em class="mb"> load_dataset 函数</em>中，注释是从 jsons 的<em class="mb"> </em>文件中提取的。从注释的多边形点生成遮罩，并且从遮罩生成边界框。封装遮罩所有像素的最小框被用作边界框。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="b0c4" class="mc md iq nt b gy nx ny l nz oa">vis_img.visualize_sample()</span></pre><p id="a539" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当您调用此函数时，它会显示一个带有遮罩和边框的示例图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/3760667a25c2ab65678f6d848c1132be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCth8-752Z68lfpv-QrBPg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/02bf948c437474c7a1bfe3943b12abc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1qzkN0BWHAYXbgyuHIybA.png"/></div></div></figure><p id="9331" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太好了！数据集适合训练，<em class="mb"> load_dataset 函数</em>成功地为图像中的每个对象生成遮罩和边界框。在 HSV 空间中为遮罩生成随机颜色，然后将其转换为 RGB。</p><p id="b14a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">最后一步:</em></p><p id="10f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用数据集训练自定义模型</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="037a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是执行训练的代码。只需 7 行代码，您就可以训练数据集。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="f6f1" class="mc md iq nt b gy nx ny l nz oa">train_maskrcnn.modelConfig(network_backbone = "resnet101", num_classes= 2, batch_size = 4)                       </span></pre><p id="913c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们调用函数<em class="mb"> modelConfig，即</em>模型的配置。它采用以下参数:</p><ul class=""><li id="bd24" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> network_backbone: </strong>这是 CNN 网络，用作<em class="mb"> mask-rcnn </em>的特征提取器。使用的特征提取器是<em class="mb"> resnet101。</em></li><li id="0047" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><strong class="ky ir"> num_classes: </strong>我们将类的数量设置为数据集中对象的类别。在这种情况下，我们在自然界的数据集中有两个类(蝴蝶和松鼠)。</li><li id="65da" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><strong class="ky ir"> batch_size: </strong>这是训练模型的批量。它被设置为 4。</li></ul><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="0d9c" class="mc md iq nt b gy nx ny l nz oa">train_maskrcnn.load_pretrained_model("mask_rcnn_coco.h5")</span><span id="0caf" class="mc md iq nt b gy ob ny l nz oa">train_maskrcnn.load_dataset(“Nature”)</span></pre><p id="6ab7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用迁移学习技术来训练模型。Coco 模型已经在 80 个类别的对象上进行了训练，它已经学习了许多将有助于训练模型的特征。我们调用函数<em class="mb"> load_pretrained_model 函数</em>来加载<em class="mb"> mask-rcnn coco 模型</em>。我们<em class="mb">使用<em class="mb"> load_dataset 函数加载</em>数据集。</em></p><p id="4d6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从<a class="ae kv" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.2/mask_rcnn_coco.h5" rel="noopener ugc nofollow" target="_blank">这里</a>下载 coco 模型。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="6bea" class="mc md iq nt b gy nx ny l nz oa">train_maskrcnn.train_model(num_epochs = 300, augmentation=True,path_trained_models = “mask_rcnn_models”)</span></pre><p id="2ba8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们调用 train 函数来训练<em class="mb"> mask r-cnn </em>模型。我们称之为<em class="mb"> train_model 函数。</em>该函数采用以下参数:</p><ul class=""><li id="18f9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> num_epochs </strong>:训练模型所需的次数。它被设置为<em class="mb"> 300 </em>。</li><li id="f9a7" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><strong class="ky ir">扩充:</strong>数据扩充应用于数据集。这是因为我们希望模型学习对象的不同表示。</li><li id="010b" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><strong class="ky ir"> path_trained_models: </strong>这是在训练过程中保存训练好的模型的路径。具有最低验证损失的模型被保存。</li></ul><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="c2ee" class="mc md iq nt b gy nx ny l nz oa">Using resnet101 as network backbone For Mask R-CNN model</span><span id="36f6" class="mc md iq nt b gy ob ny l nz oa">Train 600 images <br/>Validate 200 images </span><span id="c9c8" class="mc md iq nt b gy ob ny l nz oa">Applying augmentation on dataset </span><span id="b694" class="mc md iq nt b gy ob ny l nz oa">Checkpoint Path: mask_rcnn_models</span><span id="efa5" class="mc md iq nt b gy ob ny l nz oa">Selecting layers to train</span><span id="9835" class="mc md iq nt b gy ob ny l nz oa">Epoch 1/200<br/>100/100 - 164s - loss: 2.2184 - rpn_class_loss: 0.0174 - rpn_bbox_loss: 0.8019 - mrcnn_class_loss: 0.1655 - mrcnn_bbox_loss: 0.7274 - mrcnn_mask_loss: 0.5062 - val_loss: 2.5806 - val_rpn_class_loss: 0.0221 - val_rpn_bbox_loss: 1.4358 - val_mrcnn_class_loss: 0.1574 - val_mrcnn_bbox_loss: 0.6080 - val_mrcnn_mask_loss: 0.3572<br/><br/>Epoch 2/200<br/>100/100 - 150s - loss: 1.4641 - rpn_class_loss: 0.0126 - rpn_bbox_loss: 0.5438 - mrcnn_class_loss: 0.1510 - mrcnn_bbox_loss: 0.4177 - mrcnn_mask_loss: 0.3390 - val_loss: 1.2217 - val_rpn_class_loss: 0.0115 - val_rpn_bbox_loss: 0.4896 - val_mrcnn_class_loss: 0.1542 - val_mrcnn_bbox_loss: 0.3111 - val_mrcnn_mask_loss: 0.2554<br/><br/>Epoch 3/200<br/>100/100 - 145s - loss: 1.0980 - rpn_class_loss: 0.0118 - rpn_bbox_loss: 0.4122 - mrcnn_class_loss: 0.1271 - mrcnn_bbox_loss: 0.2860 - mrcnn_mask_loss: 0.2609 - val_loss: 1.0708 - val_rpn_class_loss: 0.0149 - val_rpn_bbox_loss: 0.3645 - val_mrcnn_class_loss: 0.1360 - val_mrcnn_bbox_loss: 0.3059 - val_mrcnn_mask_loss: 0.2493<br/></span></pre><p id="ea17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是训练日志。它显示了用于训练<em class="mb"> mask-rcnn </em>的网络主干，即<em class="mb"> resnet101 </em>，用于训练的图像数量和用于验证的图像数量。在<em class="mb"> path_to_trained models </em>目录中，模型按照有效性损失减少的原则保存，典型的模型名称如下:<strong class="ky ir"><em class="mb">mask _ rcnn _ model _ 25–0.55678</em>，</strong> it <strong class="ky ir"> </strong>保存为<strong class="ky ir"> </strong>及其对应的有效性损失。</p><p id="d29d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">网络骨干:</strong></p><p id="18c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有两个网络骨干用于训练<em class="mb"> mask-rcnn </em></p><ul class=""><li id="832d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb"> Resnet101 </em></li><li id="6e5a" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><em class="mb"> Resnet50 </em></li></ul><p id="0193" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">谷歌 colab: </strong>谷歌 colab 提供单个 12GB 英伟达特斯拉 K80 GPU，可连续使用长达 12 小时。</p><p id="97a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用 Resnet101: </strong>训练 Mask-RCNN 消耗大量内存。在使用<em class="mb"> resnet101 </em>作为网络主干的 google colab 上，你将能够以 4 的批量进行训练。默认的网络主干是<em class="mb"> resnet101 </em>。Resnet101 被用作默认主干，因为它在训练期间似乎比 resnet50 更快地达到更低的验证损失。对于包含多个类和更多图像的数据集，这种方法也更有效。</p><p id="46de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用 resnet 50:</strong>resnet 50 的优点是占用内存较少。因此，根据 colab 随机分配 GPU 的方式，您可以在 google colab 上使用 6 或 8 的 batch_size。</p><p id="63fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">支持<em class="mb"> resnet50 </em>的修改代码会是这样的。</p><p id="670b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="b2f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与原代码的主要区别在于，在模型的配置函数中，我们将<em class="mb"> network_backbone </em>设置为<em class="mb"> resnet50 </em>，并将批量大小改为 6 <em class="mb">。</em></p><p id="e5b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训日志中的唯一区别是:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="0d39" class="mc md iq nt b gy nx ny l nz oa">Using resnet50 as network backbone For Mask R-CNN model</span></pre><p id="63ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">说明我们在用 resnet50 进行训练。</p><p id="57f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>给出的 batch _ sizes 是用于 google colab 的样本。如果您使用的是功能较弱的 GPU，请减少批量大小。例如，对于配有 4G RAM GPU 的 PC，resnet50 或 resnet101 的批处理大小都应该为 1。我使用批量大小 1 在我的 PC 的 GPU 上训练我的模型，训练了不到 100 个历元，它产生了 0.263 的验证损失。这是有利的，因为我的数据集并不大。配有更强大 GPU 的 PC 应该可以使用批量大小为 2。如果您有一个包含更多类和更多图像的大型数据集，您可以使用 google colab 免费访问 GPU。最重要的是，尝试使用更强大的 GPU，并为更多的纪元进行训练，以产生一个自定义模型，该模型将跨多个类高效地执行。</p><p id="3f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用更多图像进行训练来获得更好的结果。建议培训的最低要求是每个班级 300 张图片。</p><p id="7a52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">车型评测</strong></p><p id="7788" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们完成训练后，我们应该评估验证损失最低的模型。模型评估用于访问测试数据集上已训练模型的性能。从<a class="ae kv" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.0.0/Nature_model_resnet101.h5" rel="noopener ugc nofollow" target="_blank">这里</a>下载训练好的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="d0eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="a3f9" class="mc md iq nt b gy nx ny l nz oa">mask_rcnn_models/Nature_model_resnet101.h5 evaluation using iou_threshold 0.5 is 0.890000</span></pre><p id="bd7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型的 mAP(平均精度)为 0.89。</p><p id="749e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以一次评估多个模型，你只需要传入模型的文件夹目录。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="92a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出日志</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="e80b" class="mc md iq nt b gy nx ny l nz oa">mask_rcnn_models\Nature_model_resnet101.h5 evaluation using iou_threshold 0.5 is 0.890000</span><span id="f4fc" class="mc md iq nt b gy ob ny l nz oa">mask_rcnn_models\mask_rcnn_model_055.h5 evaluation using iou_threshold 0.5 is 0.867500</span><span id="0405" class="mc md iq nt b gy ob ny l nz oa">mask_rcnn_models\mask_rcnn_model_058.h5 evaluation using iou_threshold 0.5 is 0.8507500</span></pre><p id="8ad0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用 Resnet50 进行评估</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="f099" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:C </strong>如果您正在评估 resnet50 型号，请将 network_backbone 更改为 resnet50。</p><p id="f670" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用自定义模型进行推理</strong></p><p id="6cdf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们对模型进行了训练和评估。下一步是观察模型在未知图像上的表现。</p><p id="e6ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在我们训练过的班级上测试这个模型。</p><p id="b774" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">Sample1.jpg</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5dc93987aac3a008f07aa0b40f444d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*NFFCo4ouQV_gWUwK3OPQeQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:Wikicommons.com(CC0)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="e4de" class="mc md iq nt b gy nx ny l nz oa">import pixellib<br/>from pixellib.instance import custom_segmentation </span><span id="3320" class="mc md iq nt b gy ob ny l nz oa">segment_image =custom_segmentation()<br/>segment_image.inferConfig(num_classes= 2, class_names= ["BG", "butterfly", "squirrel"])</span></pre><p id="f9be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们导入了用于执行推理的类<em class="mb"> custom_segmentation </em>，并创建了该类的一个实例。我们调用了模型配置的函数并引入了一个额外的参数<em class="mb"> class_names。</em></p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="27c4" class="mc md iq nt b gy nx ny l nz oa">class_names= ["BG", "butterfly", "squirrel"])</span></pre><p id="fbb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> class_names: </strong>这是一个列表，包含模型被训练的类的名称。“背景”是指图像的背景。它是第一个类，并且必须与类名一起提供。</p><p id="e099" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>如果你有多个类，并且你不知道如何根据它们的类 id 来排列这些类的名字，在数据集的文件夹中的<em class="mb"> test.json </em>中，检查类别列表。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="b044" class="mc md iq nt b gy nx ny l nz oa">{</span><span id="e0ff" class="mc md iq nt b gy ob ny l nz oa">"images": [</span><span id="1b72" class="mc md iq nt b gy ob ny l nz oa">{<br/>"height": 205,</span><span id="8db5" class="mc md iq nt b gy ob ny l nz oa">"width": 246,</span><span id="2310" class="mc md iq nt b gy ob ny l nz oa">"id": 1,</span><span id="77db" class="mc md iq nt b gy ob ny l nz oa">"file_name": "C:\\Users\\olafe\\Documents\\Ayoola\\PIXELLIB\\Final\\Nature\\test\\butterfly (1).png"</span><span id="0c94" class="mc md iq nt b gy ob ny l nz oa">},</span><span id="9858" class="mc md iq nt b gy ob ny l nz oa">],<br/>"categories": [</span><span id="9ed4" class="mc md iq nt b gy ob ny l nz oa">{</span><span id="25a4" class="mc md iq nt b gy ob ny l nz oa">"supercategory": "butterfly",</span><span id="3e36" class="mc md iq nt b gy ob ny l nz oa">"id": 1,</span><span id="6f91" class="mc md iq nt b gy ob ny l nz oa">"name": "butterfly"</span><span id="4a60" class="mc md iq nt b gy ob ny l nz oa">},</span><span id="adbd" class="mc md iq nt b gy ob ny l nz oa">{</span><span id="96fc" class="mc md iq nt b gy ob ny l nz oa">"supercategory": "squirrel",</span><span id="5050" class="mc md iq nt b gy ob ny l nz oa">"id": 2,</span><span id="f5e6" class="mc md iq nt b gy ob ny l nz oa">"name": "squirrel"</span><span id="98ef" class="mc md iq nt b gy ob ny l nz oa">}</span><span id="1fe7" class="mc md iq nt b gy ob ny l nz oa">],</span></pre><p id="3380" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面<em class="mb"> test.json </em>目录的样本中可以观察到，test.json 中<em class="mb">图片列表</em>之后是<em class="mb">物体类别列表。类名和它们对应的类 id 都在那里。<em class="mb">蝴蝶</em>有一个类 id <em class="mb"> 1 </em>而<em class="mb">松鼠</em>有一个类 id <em class="mb"> 2 </em>。记住，第一个 id“0”是为背景保留的。</em></p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="11f6" class="mc md iq nt b gy nx ny l nz oa">segment_image.load_model("mask_rcnn_model/Nature_model_resnet101.h5)</span><span id="b386" class="mc md iq nt b gy ob ny l nz oa">segment_image.segmentImage("sample1.jpg", show_bboxes=True, output_image_name="sample_out.jpg")</span></pre><p id="326d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">定制模型被加载，我们调用函数来分割图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/40084a9895e325536af36d0faeebe477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*hbyGORTsW3QF4oF-cxR5Tg.jpeg"/></div></figure><p id="e3cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当参数<em class="mb"> show_bboxes </em>设置为<em class="mb"> True，</em>我们将能够执行带有对象检测的实例分割。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="7c3b" class="mc md iq nt b gy nx ny l nz oa">test_maskrcnn.segmentImage(“sample1.jpg”,show_bboxes = False, output_image_name=”sample_out.jpg”)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/fa80c0752bab15d901813779ba7f21ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*JbOvl-_aEoIYNcau8cnStA.jpeg"/></div></figure><p id="55df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当<em class="mb"> show_bboxes </em>被设置为<em class="mb"> False </em>时，我们仅获得分段屏蔽。</p><p id="c50f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">Sample2.jpg</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/65ed3340c38c4e4838e4f75722436383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HadF1gw_kCf3vbBSjaUi5A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:Wikicommons.com</p></figure><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="dd34" class="mc md iq nt b gy nx ny l nz oa">test_maskrcnn.segmentImage(“sample2.jpg”,show_bboxes = True, output_image_name=”sample_out.jpg”)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/7bb253fa4a175657a74dc4ae4f65447f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Phk1NQBkihpHRkJo79rvRQ.jpeg"/></div></div></figure><blockquote class="on"><p id="29b0" class="oo op iq bd oq or os ot ou ov ow lr dk translated">哇！我们已经成功地训练了一个定制模型，用于在蝴蝶和松鼠上执行实例分割和对象检测。</p></blockquote><h2 id="0fab" class="mc md iq bd me mf ox dn mh mi oy dp mk lf oz mm mn lj pa mp mq ln pb ms mt mu bi translated"><strong class="ak">用自定义模型进行视频分割。</strong></h2><p id="2751" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated"><strong class="ky ir">样本 _ 视频 1 </strong></p><p id="c75d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们想对视频中的蝴蝶进行分割。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc od l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="bfbc" class="mc md iq nt b gy nx ny l nz oa">test_video.process_video("video.mp4", show_bboxes = True,  output_video_name="video_out.mp4", frames_per_second=15)</span></pre><p id="8f31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">调用函数<em class="mb"> process_video </em>对视频中的对象进行分割。</p><p id="7b5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它采用以下参数:-</p><ul class=""><li id="97fa" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb"> video_path: </em>这是我们想要分割的视频文件的路径。</li><li id="07d3" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><em class="mb">每秒帧数:</em>该参数用于设置已保存视频文件的每秒帧数。在这种情况下，它被设置为 15，即保存的视频文件每秒将有 15 帧。</li><li id="2b21" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated"><em class="mb">output _ video _ name:T</em>his 是保存的<em class="mb"> </em>分段视频的名称<em class="mb">。</em>输出的视频将保存在您当前的工作目录中。</li></ul><p id="d069" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出 _ 视频</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc od l"/></div></figure><p id="49db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与我们的自定义模型的另一个分段视频样本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc od l"/></div></figure><p id="9801" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用以下代码使用您的自定义模型执行实时摄像机分段:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="d023" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将用<em class="mb"> process_camera 功能替换 process_video 功能。</em>在函数中，我们替换了要捕获的视频文件路径，即我们正在处理摄像机捕获的帧流，而不是视频文件。为了显示摄像机画面，我们添加了额外的参数:</p><ul class=""><li id="642f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb">显示帧:</em>该参数处理分段摄像机帧的显示。</li><li id="bbbb" class="ls lt iq ky b kz og lc oh lf oi lj oj ln ok lr lx ly lz ma bi translated">这是显示的摄像机框架的名称。</li></ul><p id="2568" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">访问 Google colab 为训练自定义数据集而设置的笔记本。</p><div class="na nb gp gr nc nd"><a href="https://colab.research.google.com/drive/1LIhBcxF6TUQUQCMEXCRBuF4a7ycUmjuw?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd ir gy z fp ni fr fs nj fu fw ip bi translated">谷歌联合实验室</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">编辑描述</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">colab.research.google.com</p></div></div><div class="nm l"><div class="pd l no np nq nm nr kp nd"/></div></div></a></div><blockquote class="on"><p id="767b" class="oo op iq bd oq or os ot ou ov ow lr dk translated"><a class="ae kv" href="https://github.com/ayoolaolafenwa/PixelLib" rel="noopener ugc nofollow" target="_blank">访问 Pixellib 的官方 Github 库</a></p><p id="3e26" class="oo op iq bd oq or pe pf pg ph pi lr dk translated"><a class="ae kv" href="https://pixellib.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">访问 Pixellib 的官方文档</a></p></blockquote><p id="9d23" class="pw-post-body-paragraph kw kx iq ky b kz pj jr lb lc pk ju le lf pl lh li lj pm ll lm ln pn lp lq lr ij bi translated">通过以下方式联系我:</p><p id="828a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">邮箱:【olafenwaayoola@gmail.com T2】</p><p id="2f8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">领英:<a class="ae kv" href="https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/" rel="noopener ugc nofollow" target="_blank">阿尤拉·奥拉芬娃</a></p><p id="8c68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推特:<a class="ae kv" href="https://twitter.com/AyoolaOlafenwa" rel="noopener ugc nofollow" target="_blank"> @AyoolaOlafenwa </a></p><p id="d7bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">脸书:<a class="ae kv" href="https://web.facebook.com/ayofen" rel="noopener ugc nofollow" target="_blank">阿尤拉·奥拉芬娃</a></p></div></div>    
</body>
</html>