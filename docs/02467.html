<html>
<head>
<title>Faster and Memory-Efficient PyTorch models using AMP and Tensor Cores</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 AMP 和 Tensor 内核的速度更快、内存效率高的 PyTorch 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-and-memory-efficient-pytorch-models-using-amp-50fd3c8dd7fe?source=collection_archive---------14-----------------------#2020-03-09">https://towardsdatascience.com/faster-and-memory-efficient-pytorch-models-using-amp-50fd3c8dd7fe?source=collection_archive---------14-----------------------#2020-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4ad8400e268226263dc839861ec4c9fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69jl-fi6WVV-mgQGXzYoiQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1283795" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/Pexels-2286921/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1283795" rel="noopener ugc nofollow" target="_blank">像素</a></p></figure><div class=""/><div class=""><h2 id="e0ff" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">只需添加几行代码</h2></div><p id="f1db" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你知道 1986 年 Geoffrey Hinton 在《自然》<a class="ae jg" href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf" rel="noopener ugc nofollow" target="_blank">杂志的论文</a>中给出了反向传播算法吗？</p><p id="1a1b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，1998 年 Yann le cun 首次提出了用于数字分类的卷积网络，其中他使用了单个卷积层。直到 2012 年晚些时候，Alexnet 才通过使用多个卷积层在 imagenet 上实现最先进的水平，从而普及了 Convnets。</p><p id="0dd8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么是什么让他们现在如此出名而不是以前呢？</p><p id="018f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只有在我们拥有大量计算资源的情况下，我们才能在最近的过去试验和利用深度学习的全部潜力。</p><p id="8d33" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，我们是否充分利用了我们的计算资源？我们能做得更好吗？</p><p id="eb09" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">这篇帖子是关于利用张量核和自动混合精度来更快地训练深度学习网络的。</em>T9】</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="da0f" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">什么是张量核？</h1><p id="643a" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">根据<a class="ae jg" href="https://www.nvidia.com/en-us/data-center/tensorcore/" rel="noopener ugc nofollow" target="_blank"> NVIDIA </a>网站:</p><blockquote class="mz na nb"><p id="1f97" class="ky kz lu la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated">NVIDIA Turing 和 Volta GPUs 由 Tensor Cores 提供支持，这是一项革命性的技术，可提供突破性的 AI 性能。张量核可以加速大型矩阵运算，这是人工智能的核心，并在单次运算中执行混合精度矩阵乘法和累加计算。随着数百个张量内核在一个 NVIDIA GPU 中并行运行，这使得吞吐量和效率大幅提高</p></blockquote><p id="9663" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单来说；它们<strong class="la jk"> <em class="lu">是专门的内核，非常适合特定类型的矩阵运算</em> </strong>。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/26de3d1863efd15dd3fed50bc9123897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gLR7majj44zugHxGreEjA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="14ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以将两个 FP16 矩阵相乘，并将其与 FP16/FP32 矩阵相加，从而得到一个 FP16/FP32 矩阵。张量核支持混合精度数学，即输入为半精度(FP16)，输出为全精度(FP32)。上述操作对于许多深度学习任务来说具有内在价值，张量核为这种操作提供了专门的硬件。</p><p id="0b0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，使用 FP16 相对于 FP32 主要有两个好处。</p><ol class=""><li id="dafc" class="nk nl jj la b lb lc le lf lh nm ll nn lp no lt np nq nr ns bi translated">FP16 需要更少的内存，因此更容易训练和部署大型神经网络。它还涉及更少的数据移动。</li><li id="7f09" class="nk nl jj la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated">张量核在降低精度的情况下，数学运算运行得更快。NVIDIA 给出的 Volta GPU 的准确数字是:FP16 为 125 TFlops，FP32 为 15.7 TFlops 倍加速)</li></ol><p id="89a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是也有缺点。当我们从 FP32 转到 FP16 时，必然会降低精度。</p><div class="ng nh ni nj gt ab cb"><figure class="ny iv nz oa ob oc od paragraph-image"><img src="../Images/46ce5e5f7ee7cb62c8074ca114a75365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/0*g1HgEvgdKwbvkAws"/></figure><figure class="ny iv oe oa ob oc od paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/2163e5b8ceec66984c470b28d602505f.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/0*tRv_gt99KSda37Pc.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk of di og oh translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" rel="noopener ugc nofollow" target="_blank"> FP32 </a> vs <a class="ae jg" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="noopener ugc nofollow" target="_blank"> FP16 </a> : FP32 有 8 个指数位和 23 个分数位，FP16 有 5 个指数位和 10 个分数位。</p></figure></div><p id="a2c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">但是需要 FP32 吗？</em>T9】</strong></p><p id="534e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">FP16 实际上可以很好地表示大多数权重和梯度。因此，存储和使用 FP32 需要所有这些额外的位只是浪费。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="fb16" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">那么，我们如何使用张量核呢？</h1><p id="43b7" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我检查了我的泰坦 RTX GPU 有 576 个张量核心和 4608 个 NVIDIA CUDA 核心。但是我如何使用这些张量核呢？</p><p id="ab84" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">老实说，NVIDIA 通过几行代码提供的自动混合精度使张量核的使用变得微不足道。我们需要在代码中做两件事:</p><ol class=""><li id="fc95" class="nk nl jj la b lb lc le lf lh nm ll nn lp no lt np nq nr ns bi translated">需要使用 FP32 进行的操作(如 Softmax)被分配到 FP32，而可以使用 FP16 完成的操作(如 Conv)被自动分配到 FP16。</li></ol><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/c21a88db301cf0ae499cddca2247a47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7FOnTfh6mVCPMTgQV1bZw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="dfd2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la jk"> <em class="lu">使用损失缩放</em> </strong>来保留小的渐变值。梯度值可能会超出 FP16 的范围。在这种情况下，梯度值会进行缩放，使其落在 FP16 范围内。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/c3cabb55a66e3a039a0a5c5db8b787d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGLDPqIEOxMd4B5ziUdjCg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2720" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">如果你还不了解背景细节也没关系。代码实现相对简单。</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a5b8" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">PyTorch 混合精确训练:</h1><p id="6d23" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们从 PyTorch 中的一个基本网络开始。</p><pre class="ng nh ni nj gt ok ol om on aw oo bi"><span id="e53a" class="op md jj ol b gy oq or l os ot">N, D_in, D_out = 64, 1024, 512<br/>x = torch.randn(N, D_in, device="cuda")<br/>y = torch.randn(N, D_out, device="cuda")<br/>model = torch.nn.Linear(D_in, D_out).cuda()<br/>optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)</span><span id="d039" class="op md jj ol b gy ou or l os ot">for to in range(500):<br/>   y_pred = model(x)<br/>   loss = torch.nn.functional.mse_loss(y_pred, y)<br/>   optimizer.zero_grad()<br/>   loss.backward()<br/>   optimizer.step()</span></pre><p id="a1c3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了利用自动混合精度训练，我们首先需要安装 apex 库。只需在您的终端中运行以下命令。</p><pre class="ng nh ni nj gt ok ol om on aw oo bi"><span id="e1d9" class="op md jj ol b gy oq or l os ot">$ git clone https://github.com/NVIDIA/apex<br/>$ cd apex<br/>$ pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./</span></pre><p id="a322" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们只需在神经网络代码中添加几行代码，就可以利用自动混合精度(AMP)的优势。我将下面添加的行加粗:</p><pre class="ng nh ni nj gt ok ol om on aw oo bi"><span id="b127" class="op md jj ol b gy oq or l os ot"><strong class="ol jk"><em class="lu">from apex import amp</em></strong><br/>N, D_in, D_out = 64, 1024, 512<br/>x = torch.randn(N, D_in, device="cuda")<br/>y = torch.randn(N, D_out, device="cuda")<br/>model = torch.nn.Linear(D_in, D_out).cuda()<br/>optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)<br/><strong class="ol jk">model, optimizer = amp.initialize(model, optimizer, opt_level="O1")</strong><br/>for to in range(500):<br/>   y_pred = model(x)<br/>   loss = torch.nn.functional.mse_loss(y_pred, y)<br/>   optimizer.zero_grad()<br/><em class="lu">   </em><strong class="ol jk"><em class="lu">with amp.scale_loss(loss, optimizer) as scaled_loss:<br/>      scaled_loss.backward()</em></strong><em class="lu"><br/></em>   optimizer.step()</span></pre><p id="718e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，您可以看到我们用<code class="fe ov ow ox ol b">amp.</code>初始化了我们的模型，我们还使用<code class="fe ov ow ox ol b">amp.scale_loss</code>指定了损耗比例</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f707" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">标杆管理</h1><p id="ae63" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以使用这个伟大的<a class="ae jg" href="https://github.com/znxlwm/pytorch-apex-experiment" rel="noopener ugc nofollow" target="_blank">存储库</a>对 amp 的性能进行基准测试，该存储库在 CIFAR 数据集上对 VGG16 模型进行基准测试。我只需要修改几行代码就可以让它为我们工作了。你可以在这里找到修改版的<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/amp" rel="noopener ugc nofollow" target="_blank"/>。要自己运行基准测试代码，您可能需要:</p><pre class="ng nh ni nj gt ok ol om on aw oo bi"><span id="1ca1" class="op md jj ol b gy oq or l os ot">git clone <a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs" rel="noopener ugc nofollow" target="_blank">https://github.com/MLWhiz/data_science_blogs</a></span><span id="51df" class="op md jj ol b gy ou or l os ot">cd data_science_blogs/amp/pytorch-apex-experiment/</span><span id="e4a3" class="op md jj ol b gy ou or l os ot">python run_benchmark.py</span><span id="fb31" class="op md jj ol b gy ou or l os ot">python make_plot.py --GPU 'RTX' --method 'FP32' 'FP16' 'amp' --batch 128 256 512 1024 2048</span></pre><p id="5b1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将在主目录中为您填充以下图形:</p><div class="ng nh ni nj gt ab cb"><figure class="ny iv oy oa ob oc od paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/753a7004aee49566c1514c78ceba404e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*USbmM7SHqE9n8TFSuyEP1A.png"/></div></figure><figure class="ny iv oy oa ob oc od paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/d23763f5bfaf31dcf3e87787031a40a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6snoMRCoqts7lT_r-ZLauw.png"/></div></figure><figure class="ny iv oy oa ob oc od paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/6ed3293d74912877e00b3389139b4e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*9m6aVJ2Cu8jGJWJTqHLKfQ.png"/></div></figure></div><p id="fd05" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我使用不同的精度和批量设置训练了同一个模型的多个实例。我们可以看到，从 FP32 到 amp，存储器需求降低了，而精度基本保持不变。时间也会减少，但不会减少太多。这可能归因于简单的数据集或简单的模型。</p><p id="6c86" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">根据 NVIDIA 给出的基准测试，自动混合精度比标准 FP32 型号快 3 倍左右，如下图所示。</em> </strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/7c5da85710228ba11e33bc2b94fefb3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wjflIkiBhC3mxHbi"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://medium.com/tensorflow/automatic-mixed-precision-in-tensorflow-for-faster-ai-training-on-nvidia-gpus-6033234b2540" rel="noopener">来源</a>:加速比是单精度和自动混合精度下固定历元数的训练时间比。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="57b7" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">继续学习</h1><p id="4bbc" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">如果您想了解更多关于实用数据科学的知识，请查看 Coursera 课程的<a class="ae jg" href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">“如何赢得数据科学竞赛”</strong> </a>。我从卡格勒教授的这门课程中学到了很多新东西。</p><p id="6ed9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系到我。</p><p id="3f46" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——在这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>