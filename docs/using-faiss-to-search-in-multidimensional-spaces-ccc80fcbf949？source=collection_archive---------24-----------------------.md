# 使用 faiss 在多维空间中搜索

> 原文：<https://towardsdatascience.com/using-faiss-to-search-in-multidimensional-spaces-ccc80fcbf949?source=collection_archive---------24----------------------->

## 如何围绕 faiss 构建服务并避免问题？

![](img/07edb186517d2f6b24300ed0ac09f0da.png)

2019 年秋天， [Avito 的](https://medium.com/avitotech)自动审核团队推出了一项基于[faiss 库](https://github.com/facebookresearch/faiss)的虚假广告检测服务。它帮助我们检测一个图像是否已经在另一个广告中使用过，即使它被以任何方式修改过——模糊、裁剪等。

我想谈谈我们在实施这项服务的过程中遇到的问题以及我们解决这些问题的方法。

在本文中，我将主要关注技术方面，假设您对多维空间中的搜索有所了解。

# 问题是

当我接到开发图像检索系统的任务时，我首先想到的是它的规格和局限性:

1.  条目的数量。从一开始，我们有大约 1.5 亿个载体，现在有超过 2.4 亿个。
2.  搜索时间限制。在我们的例子中，第 95 百分位为 300 毫秒。
3.  内存限制。该指数必须足够小，以适应普通服务器，包括未来两年的预期增长。
4.  可维护性。因为我们所有的服务都是在 Kubernetes 上实现的，所以我真的不想构建一个需要在物理硬件上运行的系统。
5.  可以添加图像，但不能删除或修改。

# 我们系统的架构

因为我们需要足够快地在数亿个向量中搜索，穷举搜索不是一个选项——我们需要一个在 RAM 中的索引。为了适应那里的向量，它们需要被压缩。

一旦索引在 RAM 中，为了防止丢失，我们偶尔会制作备份副本并存储在外部。备份应该不需要将内存大小增加一倍，即使是暂时的，因为我们谈论的是几十千兆字节。

剩下的就是提供水平缩放。在我们的情况下，最简单的解决方案是相同的最终一致的索引。那么服务实例不需要知道彼此的存在，数据库将是唯一的同步点。

我们的服务是用 Python 3.7 编写的，PostgreSQL 用于存储向量，并且选择了一个 MinIO 作为备份存储。Faiss 作为一个索引库起着关键作用。

![](img/75e854373e7f91019c9be7e141fbcbe4.png)

为了与外部世界进行交互，我们使用 avio http 框架，这是我们围绕 aiohttp 的内部包装器。

由于 asyncio [不能很好地](https://bugs.python.org/issue21998)处理我们备份所需的 fork 调用，并且在异步服务中调用长时间阻塞操作是不好的做法，所有与索引的交互都被转移到一个单独的进程中，我们使用[多处理与它进行交互。管道](https://bugs.python.org/issue21998)。

# 选择索引结构

为了压缩，我们决定使用[乘积量化](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/#exhaustive-search-with-approximate-distances)。这种方法允许我们将原始向量压缩到 64 字节。

![](img/6e71d8ba45539fd14da0a35191272c73.png)

*产品量化允许我们将向量分割成多个部分，并建立独立的聚类。这些数字来自克里斯·麦考密克的一篇文章*[](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/#exhaustive-search-with-approximate-distances)

*为了加快搜索过程，我们选择了基于[倒排文件](http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/)和 [HNSW](https://arxiv.org/pdf/1603.09320v4.pdf) 的混合方法。*

*![](img/1997812d12916e0c9a9f7c0ffd27ea75.png)*

*左图来自[红点游戏](http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/)，右图来自[于的文章。a .马尔科夫，D. A .亚舒宁](https://arxiv.org/pdf/1603.09320v4.pdf)*

*结果，我们得到了一个索引结构，用 faiss 的术语来说就是用神秘的线来描述:IVF262144_HNSW32，PQ64。这意味着我们为 262144 个簇创建一个反向文件，其中最接近的簇将使用具有 32 个邻居的 HNSW 来选择，并且所有向量都使用乘积量化方法压缩到 64 字节。*

*这里有链接到 faiss 的开发者提供的一个索引选择指南和一个基准测试。*

***看基准测试结果要注意的:***

1.  *对索引进行一次查询以搜索 10，000 个向量，并给出每个向量的计时。*
2.  *显示了 16 个 OpenMP 线程的时间。如果只使用一个线程进行搜索，时间会长 16 倍左右，因为 faiss 库内部的并行性是由#pragma omp parallel for 提供的。*

# *估计内存成本*

*用相对值来估计内存开销是最合适的——每个向量的字节数。通过这种方式，我们可以识别内存泄漏(如果发生的话),并轻松地将它们与插入导致的有机增长区分开来。*

*存储器主要用于存储向量，在 IVF262144_HNSW32，PQ64 的情况下，每个向量为 80 字节:*

*   *64 字节存储描述向量的代码。*
*   *8 个字节存储 id(存储为 int64)。*
*   *8 个字节来存储指向簇内向量的指针。*

*可以使用以下公式计算相对内存成本:*

```
*int(faiss.get_mem_usage_kb() * 1024 / index.ntotal)*
```

*预先计算的距离表会消耗一些内存，但是当估计的大小超过 2Gb 时，它不再自动计算。它的大小可以估计为 nlist × pq。M × pq.ksub × float。在我们的例子中，262，144 × 64 × 256 × 4 ≈ 17Gb，其中 pq。m 是乘积量化分量的数量，pq.ksub 是 256，因为可以用一个字节来描述这么多的簇。*

***估算内存成本要注意什么。**以上数据仅为静态索引:如果添加和删除条目，内存消耗至少要乘以 2。我怀疑这是由于倒排文件中向量的动态内存分配，但我还没有找到支持这一假设的证据。如果你知道答案，请在下面的评论中分享。*

*推出服务后每个矢量的字节数的收敛:*

*![](img/ce9302adbac38ed2111105d0478ca1e3.png)*

*如果您正在处理的问题不涉及删除和修改向量，您可以将它们中的大部分移动到静态索引归档中，只在较小的索引中插入新的向量，不时地将其与较大的索引合并，并向两个索引发送搜索查询。基本上，经典的信息检索方法。*

# *查询并行化*

*尽管在 faiss 中使用了 OpenMP，但是发送到大型集群的任何长查询都会在相当长的时间内阻塞索引。为了避免系统不规则，我们使用[ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)(faiss 是友好的，释放 GIL)。*

*![](img/84dc12f4bcc7aa9fd01190d856f60dd7.png)*

*nginx 原创图*

*为了让 faiss 在多线程环境中正确运行，需要做两件事。首先，不允许并行执行修改操作(添加、删除)和任何其他操作。其次，在执行读取查询期间限制 OpenMP 线程的数量，以使线程总数不超过指定的 CPU 内核限制。否则，性能损失是必然的。*

*把写负载和读负载分开最方便的方法是 [RWlock](https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock) ，它允许很多读者进入临界区，但只能有一个写者。对于 Python，我推荐[这个包](https://github.com/elarivie/pyReaderWriterLock)。使用 faiss.omp_set_num_threads 函数可以调整正在运行的 OpenMP 线程的数量。*

*为了获得最佳性能，以最大化每秒查询次数的方式选择每个线程的批处理大小是有意义的。我们使用的批量大小为 5。*

*值得注意的是，使用多线程时，内存消耗会略有增加。好像是个 bug(这里是[各自的问题](https://github.com/facebookresearch/faiss/issues/1108))。*

*有关多线程支持的更多信息，请查看[这篇 faiss wiki 文章](https://github.com/facebookresearch/faiss/wiki/Threads-and-asynchronous-calls)。*

# *带宽和操作时间*

*新的向量被添加到数据库中，每个实例每 5 秒钟从数据库中独立地读取这些向量，并以 10，000 个为一批(或者自上次更新以来累积的更小的一批)插入到索引中。索引中的插入速度高到足以让数据库成为瓶颈:大约每分钟 80 万个向量。*

*当我们优化延迟时，一个 20 核实例能够处理单个图像的 20 个最近邻居的多达 150 rps 的搜索，如果我们优化吞吐量，则大约 550 qps。负载伸缩非常简单:只需添加更多的实例，因为它们之间互不了解。*

# *索引备份*

*因为索引在 RAM 中，所以如果机器停机，它可能会丢失。为避免此问题，请定期将其转储到外部存储。为此，我们使用 MinIO。*

*最方便的方法是创建一个 fork 并使用 Copy-on-Write 来获得一个索引的副本，该副本在内存消耗和对操作时间的影响方面几乎是免费的。然后将该索引保存到磁盘并上传到存储器。这里，我建议从静态索引大小开始——在我们的例子中，每个向量大约 80 字节。*

*因此，在启动应用程序时，您只需要从存储库中加载最新的副本，并从数据库中插入缺失的数据。*

# *使用 GPU*

*我们还没有在产品中使用 GPU 搜索，但我做了一些测量来了解操作时间的比率。*

*数据: [SIFT1M](http://corpus-texmex.irisa.fr/) ，维度 128。
查询:我们正在为 10，000 个具有不同 nprobe 值的向量寻找 100 个最近邻。*

*![](img/66fe2efaa7a3e4fe0eb602c08528b8da.png)*

***你要注意的:***

1.  *索引必须完全适合 GPU 内存。虽然有很多方法可以使用多个 GPU 进行搜索，但我认为这不是最好的方法，因为这样的解决方案更难维护。*
2.  *平面索引在 GPU 上运行得更快，但它们仅适用于相对少量的数据(在 128 的维度上多达数千万个向量)。*
3.  *使用 PQ64 索引时，GPU 只有在轮询大量集群时才有优势。*

# *结论*

*Faiss 可能是当今最好的近似搜索开源工具，但像任何复杂的工具一样，它需要时间来适应。*

*尽管 faiss 文档非常好，但是最好还是通过不时地检查代码来学习它。您还可以通过 issues 向开发人员[发送问题。通常，他们的反应相当快。](https://github.com/facebookresearch/faiss/issues)*

*如果你正在考虑在预定的机器上部署这种系统，我建议看看 JD.com 的 vearch 系统[。他们已经完成了大部分的脏活累活，并把他们的解决方案开源，尽管文档仍然非常简陋。](https://github.com/vearch/vearch)*