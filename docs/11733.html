<html>
<head>
<title>T-distributed Stochastic Neighbor Embedding(t-SNE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">t 分布随机邻居嵌入(t-SNE)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/t-distributed-stochastic-neighbor-embedding-t-sne-bb60ff109561?source=collection_archive---------7-----------------------#2020-08-14">https://towardsdatascience.com/t-distributed-stochastic-neighbor-embedding-t-sne-bb60ff109561?source=collection_archive---------7-----------------------#2020-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b98" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解 t- SNE 的基础知识，它与主成分分析的区别，以及如何在 MNIST 数据集上应用 t-SNE</h2></div><p id="ff17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将了解到:</p><ul class=""><li id="593e" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">t-SNE 和 PCA(主成分分析)的区别</li><li id="93c9" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">简单易懂的解释 SNE 霸王龙是如何工作的</li><li id="a0e2" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">了解可用于 t-SNE 的不同参数</li><li id="f9ac" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">t-SNE 和主成分分析在 MNIST 的应用</li></ul><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a8b31133c15ca3ae01cbcea003c25483.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*fjMbCSIwwZmvnfDOEK-R-w.png"/></div></figure><p id="7792" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ma">如果一个数据集中有数百个特征或数据点，并且希望在二维或三维空间中表示它们，该怎么办？</em>T3】</strong></p><p id="653e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两种常见的<strong class="kk iu">技术在保留数据集中大部分信息的同时降低了数据集的维度</strong>是</p><ul class=""><li id="d6c7" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><a class="ae mb" href="https://medium.com/datadriveninvestor/principal-component-analysis-pca-a0c5715bc9a2" rel="noopener"> <strong class="kk iu">主成分分析</strong> </a></li><li id="6967" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu"> t 分布随机邻居嵌入(t-SNE) </strong></li></ul><h2 id="2658" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">降低数据维度的目标</h2><ul class=""><li id="7d2f" class="le lf it kk b kl mv ko mw kr mx kv my kz mz ld lj lk ll lm bi translated"><strong class="kk iu">在低维表示中尽可能多地保留高维数据中存在的数据的重要结构或信息。</strong></li><li id="80cf" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">提高低维数据的可解释性</strong></li><li id="75a7" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">最小化因降维导致的数据信息损失</strong></li></ul><p id="cf1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ma">什么是 PCA 和 t-SNE，两者有什么区别或相似之处？</em> </strong></p><p id="44bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">PCA 和 t-SNE 都是无监督的降维技术。这两种技术</strong>都用于将高维数据可视化到低维空间。</p><h2 id="ac6c" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">主成分分析</h2><ul class=""><li id="90b5" class="le lf it kk b kl mv ko mw kr mx kv my kz mz ld lj lk ll lm bi translated"><strong class="kk iu">用于特征提取和可视化的无监督确定性算法</strong></li><li id="562e" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">应用<strong class="kk iu">线性降维技术</strong>，其中<strong class="kk iu">的重点是在低维空间</strong>中保持不同点之间的距离。</li><li id="08f3" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">通过使用特征值</strong>保留数据中的方差，将原始数据转换为新数据。</li><li id="aa4b" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">离群值影响 PCA。</strong></li></ul><h2 id="36a0" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">t-分布随机邻域嵌入(t-SNE)</h2><ul class=""><li id="e709" class="le lf it kk b kl mv ko mw kr mx kv my kz mz ld lj lk ll lm bi translated">一种<strong class="kk iu">无监督的随机化算法，仅用于可视化</strong></li><li id="06fc" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">应用一种非线性降维技术，重点是在低维空间中保持非常相似的数据点接近在一起。</li><li id="116c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">使用学生 t 分布</strong>来计算低维空间中两点之间的相似性，从而保留数据的局部结构。</li><li id="3e87" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">t-SNE 使用<strong class="kk iu">重尾 Student-t 分布来计算低维空间</strong>中两点之间的相似性，而不是高斯分布，这有助于<strong class="kk iu">解决拥挤和优化问题</strong>。</li><li id="1f62" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">异常值不会影响 t-SNE </strong></li></ul><p id="2ced" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mb" href="https://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">T-分布式随机邻居嵌入(t-SNE) </strong> </a>是由<a class="ae mb" href="https://en.wikipedia.org/w/index.php?title=Laurens_van_der_Maaten&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank"> Laurens van der Maaten </a>和<a class="ae mb" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank"> Geoffrey Hinton </a>开发的一种用于可视化的无监督机器学习算法。</p><p id="687d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t-SNE 是如何工作的？ </p><p id="63bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第一步:在高维空间中寻找相邻点之间的成对相似性。</strong></p><p id="b523" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t-SNE 将数据点 xᵢ和 xⱼ之间的高维欧几里德距离转换成条件概率 P(j|i)。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi na"><img src="../Images/df2bafc5443ab0fda12f9958c04782c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*nzh6fNOteHA147WifeRzbQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><a class="ae mb" href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank"> D </a>高维空间中的数据(图片由作者提供)</p></figure><p id="5b59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">xᵢ会根据以 xᵢ.点为中心的高斯分布下 xⱼ的概率密度比例来选择它作为邻居</p><p id="8379" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">σᵢ是以数据点 xᵢ为中心的高斯方差</p><p id="a5a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">一对点的概率密度与其相似度成正比。对于附近的数据点，p(j|i)将相对较高，而对于相距较远的点，p(j|i)将非常小。</strong></p><p id="bf47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将高维空间中的条件概率对称化，得到高维空间中的最终相似度。</p><p id="9722" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">条件概率通过对两个概率进行平均来对称化，如下所示。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/37a022abe53966bc4b7b832742b5eaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*xRKtStzh1on-tDw0m0-cqA.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">对称化条件概率</p></figure><p id="406c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">步骤 2:基于高维空间中的点的成对相似性，将高维空间中的每个点映射到低维映射。</strong></p><p id="ee7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">低维图将是二维或三维图</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/de6967d64dddeb9b00c6e33a0c3e472f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*XNdTIW4-Dwmn5LD8t2ocXw.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><a class="ae mb" href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank"> D </a>低维空间中的数据(图片由作者提供)</p></figure><p id="26f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">yᵢ和 yⱼ是高维数据点 xᵢ和 xⱼ.的低维对应物</p><p id="7c20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们计算条件概率 q(j|i ),类似于以点 yᵢ为中心的高斯分布下的 p(j[I ],然后对称化该概率。</p><p id="4512" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">步骤 3:使用基于 Kullback-Leibler 散度(KL 散度)的梯度下降，找到最小化 Pᵢⱼ和 qᵢⱼ之间不匹配的低维数据表示</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1214b053d955df2d0815402be867bfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*hxyQBRD0TO6I5T-rSjoXcA.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><strong class="bd ni"> Pᵢ </strong>代表点 xᵢ.在所有其他数据点上的条件概率分布 Q <strong class="bd ni"> ᵢ </strong>表示给定地图点 yᵢ的所有其他地图点上的条件概率分布</p></figure><p id="ba18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t-SNE 使用梯度下降优化低维空间中的点。</p><p id="d3ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ma">为什么用 KL 发散？</em>T9】</strong></p><p id="4ca8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">当我们最小化 KL 散度时，它使得 qᵢⱼ在物理上等同于 Pᵢⱼ，因此高维空间中的数据结构将类似于低维空间中的数据结构。</strong></p><p id="7084" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于 KL 散度方程，</p><ul class=""><li id="4c77" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">如果 Pᵢⱼ较大，那么我们需要一个较大的 qᵢⱼ值来表示相似性较高的局部点。</strong></li><li id="f9de" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">如果 Pᵢⱼ很小，那么我们需要一个更小的 qᵢⱼ值来表示相距很远的局部点。</strong></li></ul><p id="43bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第四步:使用 Student-t 分布计算低维空间中两点之间的相似度。</strong></p><p id="226d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t-SNE 使用一个自由度的<strong class="kk iu">重尾 Student-t 分布来计算低维空间</strong>中两点之间的相似性，而不是高斯分布。</p><p id="474a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t 分布在低维空间中创建点的概率分布，这有助于减少拥挤问题。</p><p id="f6aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如何在数据集上应用 t-SNE？ </p><p id="56f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们用 python 写代码之前，让我们了解一些我们可以使用的 TSNE 的关键参数</p><p id="1dba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="ma">n _ components</em></strong><em class="ma">:</em><strong class="kk iu">嵌入空间的维度，这是我们希望高维数据转换成</strong>的较低维度。二维空间的<strong class="kk iu">默认值为 2 </strong>。</p><p id="b190" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">困惑度:</strong>困惑度与 t-SNE 算法中使用的最近邻的数量有关。较大的数据集通常需要较大的困惑度。困惑度的值可以在 5 到 50 之间。 <strong class="kk iu">默认值为 30。</strong></p><p id="381b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> n_iter:优化的最大迭代次数</strong>。<strong class="kk iu">至少应为 250，默认值为 1000 </strong></p><p id="8d90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">learning _ rate:t-SNE 的学习率通常在[10.0，1000.0]范围内，默认值为 200.0。</strong></p><h2 id="cd47" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">在 MNIST 数据集上实现主成分分析和 t-SNE</h2><p id="43fe" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr nj kt ku kv nk kx ky kz nl lb lc ld im bi translated">我们将使用 sklearn.decomposition.PCA 应用 PCA，并在 MNIST 数据集上使用<a class="ae mb" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> sklearn.manifold.TSNE </a>实现 t-SNE。</p><p id="684e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">加载 MNIST 数据</strong></p><p id="3e24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">导入所需的库</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="10b5" class="mc md it nn b gy nr ns l nt nu">import time<br/>import numpy as np<br/>import pandas as pd</span></pre><p id="d648" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获取 MNIST 训练和测试数据，并检查训练数据的形状</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="860f" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">(X_train, y_train) , (X_test, y_test) = mnist.load_data()<br/></strong>X_train.shape</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/da4f3b0e86191f5c5afd65749ab74647.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*e8FW9KXhiehADm5lLAn6lA.png"/></div></figure><p id="1597" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建一个包含大量图像和图像中像素计数的数组，并将 X_train 数据复制到 X</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="2f2c" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">X = np.zeros((X_train.shape[0], 784))<br/>for i in range(X_train.shape[0]):<br/>    X[i] = X_train[i].flatten()</strong></span></pre><p id="db1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混洗数据集，取 10%的 MNIST 训练数据并将其存储在数据框中。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="0992" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">X = pd.DataFrame(X)<br/>Y = pd.DataFrame(y_train)<br/>X = X.sample(frac=0.1, random_state=10).reset_index(drop=True)<br/>Y = Y.sample(frac=0.1, random_state=10).reset_index(drop=True)<br/>df = X</strong></span></pre><p id="a7af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在数据准备好之后，我们可以应用主成分分析和 t-SNE。</p><h2 id="2595" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">在 MNIST 数据集上应用主成分分析</h2><p id="e47d" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr nj kt ku kv nk kx ky kz nl lb lc ld im bi translated">使用来自<strong class="kk iu"><em class="ma">sk learn . decomposition .</em></strong>的 PCA 库应用 PCA</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="2cf7" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">from sklearn.decomposition import PCA</strong><br/>time_start = time.time()<br/><strong class="nn iu">pca = PCA(n_components=2)<br/>pca_results = pca.fit_transform(df.values)</strong></span><span id="ee71" class="mc md it nn b gy nw ns l nt nu">print ('PCA done! Time elapsed: {} seconds'.format(time.time()-time_start))</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/406d3acd5d9c00e5abce2dc47deb6cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*H0sN6OPbIyJgLkE_eTjFng.png"/></div></figure><p id="1d8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 生成两个维度，主成分 1 和主成分 2。将两个 PCA 成分连同标签一起添加到数据框中。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="722d" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">pca_df = pd.DataFrame(data = pca_results<br/>             , columns = ['pca_1', 'pca_2'])<br/>pca_df['label'] = Y</strong></span></pre><p id="721e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只有可视化时才需要标签。</p><p id="6ee2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">绘制 PCA 结果</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="baf2" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">fig = plt.figure(figsize = (8,8))<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel('Principal Component 1', fontsize = 15)<br/>ax.set_ylabel('Principal Component 2', fontsize = 15)<br/>ax.set_title('2 component PCA', fontsize = 20)<br/>targets = [0,1,2,3,4,5,6,7,8,9]<br/>colors=['yellow', 'black', 'cyan', 'green', 'blue', 'red', 'brown','crimson', 'gold', 'indigo']</strong></span><span id="0d8c" class="mc md it nn b gy nw ns l nt nu"><strong class="nn iu">for target, color in zip(targets,colors):<br/>    indicesToKeep = pca_df['label'] == target<br/>    ax.scatter(pca_df.loc[indicesToKeep, 'pca_1']<br/>               , pca_df.loc[indicesToKeep, 'pca_2']<br/>               , c = color<br/>               , s = 50)</strong></span><span id="e7b0" class="mc md it nn b gy nw ns l nt nu"><strong class="nn iu">ax.legend(targets)<br/>ax.grid()</strong></span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8dca59da083122e80d1f7b71fa4894f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*okqTBkstGAgkJ543HsQbqQ.png"/></div></figure><h2 id="3e53" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">在 MNIST 数据集上应用 t-SNE</h2><p id="a98c" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr nj kt ku kv nk kx ky kz nl lb lc ld im bi translated">导入 t-SNE 和可视化所需的库</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="f55b" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">import time<br/>from sklearn.manifold import TSNE<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>import seaborn as sns<br/>import matplotlib.patheffects as PathEffects</strong><br/><strong class="nn iu">%matplotlib inline</strong></span></pre><p id="9472" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先使用默认参数创建一个 TSNE 实例，然后将高维图像输入数据放入嵌入空间，并使用 fit_transform 返回转换后的输出。</p><p id="2ff1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像数据的维数应该是(<strong class="kk iu"><em class="ma">n _ 样本，n _ 特征)</em> </strong>的形状</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="97b8" class="mc md it nn b gy nr ns l nt nu">time_start = time.time()<br/><strong class="nn iu">tsne = TSNE(random=0)<br/>tsne_results = tsne.fit_transform(df.values)</strong></span><span id="3b7d" class="mc md it nn b gy nw ns l nt nu">print ('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))</span></pre><p id="9214" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将标注添加到数据框中，这将仅在绘制时使用，以便为聚类添加可视化标注。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="fe4a" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">df['label'] = Y</strong></span></pre><p id="4506" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">函数来可视化数据</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="790c" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">def plot_scatter(x, colors):</strong><br/>    # choose a color palette with seaborn.<br/>   <strong class="nn iu"> num_classes = len(np.unique(colors))<br/>    palette = np.array(sns.color_palette("hls", num_classes))<br/>    print(palette)</strong><br/>    <br/>    # create a scatter plot.<br/>  <strong class="nn iu">  f = plt.figure(figsize=(8, 8))<br/>    ax = plt.subplot(aspect='equal')<br/>    sc = ax.scatter(x[:,0], x[:,1],  c=palette[colors.astype(np.int)], cmap=plt.cm.get_cmap('Paired'))<br/>    plt.xlim(-25, 25)<br/>    plt.ylim(-25, 25)<br/>    ax.axis('off')<br/>    ax.axis('tight')</strong></span><span id="227b" class="mc md it nn b gy nw ns l nt nu"># add the labels for each digit corresponding to the label<br/>  <strong class="nn iu">  txts = []</strong></span><span id="1821" class="mc md it nn b gy nw ns l nt nu"><strong class="nn iu">for i in range(num_classes):</strong></span><span id="a918" class="mc md it nn b gy nw ns l nt nu"># Position of each label at median of data points.</span><span id="af27" class="mc md it nn b gy nw ns l nt nu"><strong class="nn iu">xtext, ytext = np.median(x[colors == i, :], axis=0)<br/>        txt = ax.text(xtext, ytext, str(i), fontsize=24)<br/>        txt.set_path_effects([<br/>            PathEffects.Stroke(linewidth=5, foreground="w"),<br/>            PathEffects.Normal()])<br/>        txts.append(txt)</strong></span><span id="6333" class="mc md it nn b gy nw ns l nt nu"><strong class="nn iu">return f, ax, sc, txts</strong></span></pre><p id="05ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可视化 MNIST 数据集的 SNE 结果</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="f216" class="mc md it nn b gy nr ns l nt nu"><strong class="nn iu">plot_scatter( tsne_results, df['label'])</strong></span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/697c0fbca1fb68490335a25cec26730f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*ul098l7okVnmoWA1Ns-nwQ.png"/></div></figure><p id="2dfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尝试不同的参数值并观察不同的图</p><p id="82f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">不同困惑值的可视化</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/119a1739cb6704cb3cde17732949e83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RxnwhvOiykIPc5Y8OMzGA.png"/></div></div></figure><p id="63e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">n _ ITER 不同值的可视化</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi of"><img src="../Images/1bd60c97db29fc424b61480977bebc48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PPFCnaftwU7oM64wALsyRw.png"/></div></div></figure><p id="90d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，从 t-SNE 图生成的聚类比使用 PCA 生成的聚类更加明确。</p><ul class=""><li id="b06d" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">PCA 是确定性的，而 t-SNE 不是确定性的，而是随机的。</li><li id="e2d2" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">t-SNE 试图只映射局部邻居，而 PCA 只是我们初始协方差矩阵的对角旋转，特征向量代表并保持全局属性</li></ul><p id="6296" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码在<a class="ae mb" href="https://github.com/arshren/TSNE" rel="noopener ugc nofollow" target="_blank">这里</a>可用</p><h2 id="1884" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">结论:</h2><p id="b924" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr nj kt ku kv nk kx ky kz nl lb lc ld im bi translated">主成分分析和 t-SNE 是两种常见的降维方法，它们使用不同的技术将高维数据降维为可以可视化的低维数据。</p><h2 id="67fa" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">参考资料:</h2><div class="og oh gp gr oi oj"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">sk learn . manifold . tsne—sci kit-learn 0 . 23 . 2 文档</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">t 分布随机邻居嵌入。t-SNE [1]是一个可视化高维数据的工具。它转换…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">scikit-learn.org</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ly oj"/></div></div></a></div><p id="a007" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mb" href="https://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank">由<a class="ae mb" href="https://en.wikipedia.org/w/index.php?title=Laurens_van_der_Maaten&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">劳伦斯·范·德·马滕</a>和<a class="ae mb" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank">杰弗里·辛顿</a>使用 t-SNE </a>可视化数据。</p><div class="og oh gp gr oi oj"><a href="https://lvdmaaten.github.io/tsne/" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">t-SNE</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">t 分布随机近邻嵌入(t-SNE)是一种降维技术，特别适用于</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">lvd maten . github . io</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ly oj"/></div></div></a></div></div></div>    
</body>
</html>