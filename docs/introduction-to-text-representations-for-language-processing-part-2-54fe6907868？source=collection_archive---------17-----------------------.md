# è¯­è¨€å¤„ç†çš„æ–‡æœ¬è¡¨ç¤ºæ³•ä»‹ç»â€”ç¬¬ 2 éƒ¨åˆ†

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868?source=collection_archive---------17----------------------->

![](img/365b86cca147ab6d21a7d8bed866cf2d.png)

åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šç”± [Jaredd Craig](https://unsplash.com/@jaredd_craig?utm_source=medium&utm_medium=referral) æ‹ç…§

## æ›´å¥½åœ°è¡¨ç°æ–‡æœ¬å’Œè¯­è¨€çš„é«˜çº§æŠ€æœ¯

åœ¨[ä¹‹å‰çš„æ–‡ç« ](https://medium.com/@sundareshchandran/introduction-to-text-representations-for-language-processing-part-1-dc6e8068b8a4)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å°†æ–‡æœ¬è¾“å…¥æœºå™¨å­¦ä¹ æˆ–äººå·¥æ™ºèƒ½ç®—æ³•çš„ç¦»æ•£æ–‡æœ¬è¡¨ç¤ºã€‚æˆ‘ä»¬å­¦ä¹ äº†ä¸€äº›æŠ€æœ¯ï¼Œå®ƒä»¬çš„å·¥ä½œåŸç†ï¼Œå®ƒä»¬å„è‡ªçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ç¦»æ•£æ–‡æœ¬è¡¨ç¤ºçš„ç¼ºç‚¹&å®ƒå¦‚ä½•å¿½ç•¥äº†å•è¯çš„å®šä½&æ²¡æœ‰è¯•å›¾è§£é‡Šå•è¯çš„ç›¸ä¼¼æ€§æˆ–å†…åœ¨å«ä¹‰ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶æ–‡æœ¬çš„åˆ†å¸ƒå¼æ–‡æœ¬è¡¨ç¤º&å®ƒå¦‚ä½•è§£å†³ç¦»æ•£è¡¨ç¤ºçš„ä¸€äº›ç¼ºç‚¹ã€‚

# åˆ†å¸ƒå¼æ–‡æœ¬è¡¨ç¤º:

åˆ†å¸ƒå¼æ–‡æœ¬è¡¨ç¤ºæ˜¯æŒ‡ä¸€ä¸ªå•è¯çš„è¡¨ç¤ºä¸å¦ä¸€ä¸ªå•è¯ä¸ç‹¬ç«‹æˆ–ä¸äº’æ–¥ï¼Œå¹¶ä¸”å®ƒä»¬çš„é…ç½®é€šå¸¸è¡¨ç¤ºæ•°æ®ä¸­çš„å„ç§åº¦é‡å’Œæ¦‚å¿µã€‚æ‰€ä»¥å…³äºä¸€ä¸ªå•è¯çš„ä¿¡æ¯æ²¿ç€å®ƒæ‰€ä»£è¡¨çš„å‘é‡åˆ†å¸ƒã€‚è¿™ä¸åŒäºç¦»æ•£è¡¨ç¤ºï¼Œåœ¨ç¦»æ•£è¡¨ç¤ºä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½è¢«è®¤ä¸ºæ˜¯å”¯ä¸€çš„&å½¼æ­¤ç‹¬ç«‹ã€‚

ä¸€äº›å¸¸ç”¨çš„åˆ†å¸ƒå¼æ–‡æœ¬è¡¨ç¤ºæœ‰:

*   å…±ç”ŸçŸ©é˜µ
*   Word2Vec
*   æ‰‹å¥—

# å…±ç”ŸçŸ©é˜µ:

åŒç°çŸ©é˜µï¼Œé¡¾åæ€ä¹‰ï¼Œè€ƒè™‘çš„æ˜¯å½¼æ­¤é‚»è¿‘çš„å®ä½“çš„åŒç°ã€‚ä½¿ç”¨çš„å®ä½“å¯ä»¥æ˜¯ä¸€ä¸ªå•è¯ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªåŒå­—æ¯ç»„åˆï¼Œç”šè‡³æ˜¯ä¸€ä¸ªçŸ­è¯­ã€‚ä¸»è¦åœ°ï¼Œå•ä¸ªå­—è¢«ç”¨äºè®¡ç®—çŸ©é˜µã€‚å®ƒå¸®åŠ©æˆ‘ä»¬ç†è§£è¯­æ–™åº“ä¸­ä¸åŒå•è¯ä¹‹é—´çš„å…³è”ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨å‰ä¸€ç¯‡æ–‡ç« ä¸­è®¨è®ºçš„ CountVectorizer çš„ä¾‹å­&å°†å…¶è½¬æ¢ä¸ºè¿ç»­è¡¨ç¤ºï¼Œ

```
from sklearn.feature_extraction.text import CountVectorizerdocs = ['product_x is awesome',
        'product_x is better than product_y',
        'product_x is dissapointing','product_y beats product_x by miles', 
'ill definitely recommend product_x over others']# Using in built english stop words to remove noise
count_vectorizer = CountVectorizer(stop_words = 'english')
vectorized_matrix = count_vectorizer.fit_transform(docs)# We can now simply do a matrix multiplication with the transposed image of the same matrix
co_occurrence_matrix = (vectorized_matrix.T * vectorized_matrix)
print(pd.DataFrame(co_occurrence_matrix.A, 
                   columns=count_vectorizer.get_feature_names(),
                   index=count_vectorizer.get_feature_names()))
```

# è¾“å‡º:

```
awesome  beats  better  definitely  dissapointing  ill  miles  \\
awesome              1      0       0           0              0    0      0   
beats                0      1       0           0              0    0      1   
better               0      0       1           0              0    0      0   
definitely           0      0       0           1              0    1      0   
dissapointing        0      0       0           0              1    0      0   
ill                  0      0       0           1              0    1      0   
miles                0      1       0           0              0    0      1   
product_x            1      1       1           1              1    1      1   
product_y            0      1       1           0              0    0      1   
recommend            0      0       0           1              0    1      0                  product_x  product_y  recommend  
awesome                1          0          0  
beats                  1          1          0  
better                 1          1          0  
definitely             1          0          1  
dissapointing          1          0          0  
ill                    1          0          1  
miles                  1          1          0  
product_x              5          2          1  
product_y              2          2          0  
recommend              1          0          1
```

æ¯ä¸ªå•è¯çš„è¡¨ç¤ºæ˜¯å®ƒåœ¨å…±ç°çŸ©é˜µä¸­å¯¹åº”çš„è¡Œ(æˆ–åˆ—)

å¦‚æœæˆ‘ä»¬æƒ³äº†è§£ä¸ product_x ç›¸å…³çš„å•è¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿‡æ»¤è¯¥åˆ—ï¼Œå¹¶åˆ†ææ­£åœ¨ä¸ *product_y* &è¿›è¡Œæ¯”è¾ƒçš„ *product_x* ä¸å®ƒç›¸å…³çš„æ­£é¢å½¢å®¹è¯æ¯”è´Ÿé¢å½¢å®¹è¯å¤šã€‚

# ä¼˜åŠ¿:

*   å¯»æ‰¾å•è¯å…³è”çš„ç®€å•è¡¨ç¤ºæ³•
*   ä¸ç¦»æ•£æŠ€æœ¯ä¸åŒï¼Œå®ƒè€ƒè™‘å¥å­ä¸­å•è¯çš„é¡ºåº
*   è¿™ç§æ–¹æ³•äº§ç”Ÿçš„è¡¨ç¤ºæ˜¯å…¨å±€è¡¨ç¤ºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä½¿ç”¨æ•´ä¸ªè¯­æ–™åº“æ¥ç”Ÿæˆè¡¨ç¤º

# ç¼ºç‚¹:

*   ç±»ä¼¼äº CountVectorizer & TF-IDF çŸ©é˜µï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µã€‚è¿™æ„å‘³ç€å®ƒçš„å­˜å‚¨æ•ˆç‡ä¸é«˜&åœ¨ä¸Šé¢è¿è¡Œè®¡ç®—æ•ˆç‡å¾ˆä½
*   è¯æ±‡è¡¨è¶Šå¤§ï¼ŒçŸ©é˜µè¶Šå¤§(ä¸å¯æ‰©å±•åˆ°å¤§è¯æ±‡è¡¨)
*   ä½¿ç”¨è¿™ç§æŠ€æœ¯å¹¶ä¸èƒ½ç†è§£æ‰€æœ‰çš„å•è¯è”æƒ³ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå¦‚æœä½ æŸ¥çœ‹ *product_x* åˆ—ï¼Œæœ‰ä¸€è¡Œåä¸º *beatsã€‚*åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡æŸ¥çœ‹çŸ©é˜µæ— æ³•ç¡®å®š beats çš„ä¸Šä¸‹æ–‡

# Word2Vec

Word2Vec æ˜¯ä¸€ä¸ªè‘—åçš„è¡¨ç¤ºå•è¯åµŒå…¥çš„ç®—æ³•ã€‚å®ƒæ˜¯ç”±æ‰˜é©¬æ–¯Â·ç±³å¡æ´›å¤«äº 2013 å¹´åœ¨ç ”ç©¶è®ºæ–‡ã€Šå‘é‡ç©ºé—´ä¸­å•è¯è¡¨ç¤ºçš„é«˜æ•ˆä¼°è®¡ã€‹ä¸­å¼€å‘çš„

è¿™æ˜¯ä¸€ç§åŸºäºé¢„æµ‹çš„å•è¯è¡¨ç¤ºæ–¹æ³•ï¼Œè€Œä¸æ˜¯åŸºäºè®¡æ•°çš„æŠ€æœ¯ï¼Œå¦‚å…±ç”ŸçŸ©é˜µ

> *å•è¯åµŒå…¥æ˜¯å•è¯çš„å‘é‡è¡¨ç¤ºã€‚æ¯ä¸ªå•è¯ç”±å›ºå®šçš„å‘é‡å¤§å°è¡¨ç¤ºï¼ŒåŒæ—¶æ•æ‰å…¶ä¸å…¶ä»–å•è¯çš„è¯­ä¹‰&å¥æ³•å…³ç³»*

word2vec çš„æ¶æ„æ˜¯ä¸€ä¸ªæµ…å±‚çš„å•éšå±‚ç½‘ç»œã€‚éšè—å±‚çš„æƒé‡æ˜¯å•è¯çš„åµŒå…¥&æˆ‘ä»¬é€šè¿‡æŸå¤±å‡½æ•°(æ­£å¸¸çš„åå‘æŠ•å½±)æ¥è°ƒæ•´å®ƒ

è¿™ç§æ¶æ„ç±»ä¼¼äºè‡ªåŠ¨ç¼–ç å™¨ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªç¼–ç å™¨å±‚å’Œä¸€ä¸ªè§£ç å™¨å±‚ï¼Œä¸­é—´éƒ¨åˆ†æ˜¯è¾“å…¥çš„å‹ç¼©è¡¨ç¤ºï¼Œå¯ç”¨äºé™ç»´æˆ–å¼‚å¸¸æ£€æµ‹ç”¨ä¾‹ã€‚

word2vec é€šè¿‡ä¸¤ç§æ–¹æ³•/æŠ€æœ¯æ„å»ºçŸ¢é‡è¡¨ç¤º:

*   **CBOW** â€”å°è¯•åœ¨å‘¨å›´å•è¯çš„ä¸Šä¸‹æ–‡ä¸­é¢„æµ‹ä¸­é—´çš„å•è¯ã€‚å› æ­¤ï¼Œç®€å•æ¥è¯´ï¼Œå®ƒè¯•å›¾å¡«è¡¥ç©ºç™½ï¼Œå³åœ¨ç»™å®šçš„ä¸Šä¸‹æ–‡/å‘¨å›´å•è¯çš„æƒ…å†µä¸‹ï¼Œä»€ä¹ˆå•è¯æ›´é€‚åˆã€‚æ•°æ®é›†è¶Šå°ï¼Œæ•ˆç‡è¶Šé«˜ã€‚ä¸ Skip-Gram ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´æ›´çŸ­
*   **Skip-Gram** â€”å°è¯•ä»ç›®æ ‡å•è¯é¢„æµ‹å‘¨å›´çš„ä¸Šä¸‹æ–‡å•è¯(ä¸ CBOW ç›¸å)ã€‚å€¾å‘äºåœ¨è¾ƒå¤§çš„æ•°æ®é›†ä¸­è¡¨ç°å¾—æ›´å¥½ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´

Word2vec èƒ½å¤Ÿä½¿ç”¨ç®€å•çš„çŸ¢é‡ç®—æ³•æ•è·å•è¯ä¹‹é—´çš„å¤šç§ç›¸ä¼¼åº¦ã€‚åƒâ€œç”·äººå¯¹å¥³äººå°±åƒå›½ç‹å¯¹ç‹åâ€è¿™æ ·çš„æ¨¡å¼å¯ä»¥é€šè¿‡åƒâ€œå›½ç‹â€è¿™æ ·çš„ç®—æœ¯è¿ç®—è·å¾—â€”â€”â€œç”·äººâ€+â€œå¥³äººâ€=â€œç‹åâ€ï¼Œå…¶ä¸­â€œç‹åâ€å°†æ˜¯å•è¯æœ¬èº«æœ€æ¥è¿‘çš„å‘é‡è¡¨ç¤ºã€‚å®ƒè¿˜èƒ½å¤Ÿå¤„ç†å¥æ³•å…³ç³»ï¼Œå¦‚ç°åœ¨æ—¶å’Œè¿‡å»æ—¶ï¼Œä»¥åŠè¯­ä¹‰å…³ç³»ï¼Œå¦‚å›½å®¶ä¸é¦–éƒ½çš„å…³ç³»

è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨ gensim çš„ word2vec å®ç°

```
# pip install --upgrade gensim or conda install -c conda-forge gensim# Word2Vec expects list of list representation of words, the outer list represents
# the sentence, while the inner list represents the individual words in a sentence
# Ex: ["I love NLP", "NLP is awesome"] -> [["I", "love", "NLP"], ["NLP", "is", "awesome"]]import gensim
sentences = ["ML is awesome", "ML is a branch of AI", "ML and AI are used interchangably nowadays", 
             "nlp is a branch and AI", "AI has fastforwarded nlp",
             "RL is also a branch of AI", "word2vec is a high dimensional vector space embedding",
            "word2vec falls under text representation for nlp"]# Preprocessing sentence to convert to format expected by w2v
sentece_list=[]
for i in sentences:
    li = list(i.split(" "))
    sentece_list.append(li)print(sentece_list)# Training Word2Vec with Skip-Gram (sg=1), 100 dimensional vector representation,
# with 1 as min word count for dropping noise terms, 4 parallel workers to run on
# Window of 4 for computing the neighbours & 100 iterations for the model to converge
model = gensim.models.Word2Vec(Bigger_list, min_count=1, 
				workers=4, size = 100, iter=100, sg=1, window=4)model.wv['word2vec']model.wv.most_similar(positive=['word2vec'])
```

# è¾“å‡º

```
# Sentence List
[['ML', 'is', 'awesome'],
 ['ML', 'is', 'a', 'branch', 'of', 'AI'],
 ['ML', 'and', 'AI', 'are', 'used', 'interchangably', 'nowadays'],
 ['nlp', 'is', 'a', 'branch', 'and', 'AI'],
 ['AI', 'has', 'fastforwarded', 'nlp'],
 ['RL', 'is', 'also', 'a', 'branch', 'of', 'AI'],
 ['word2vec',
  'is',
  'a',
  'high',
  'dimensional',
  'vector',
  'space',
  'embedding'],
 ['word2vec', 'falls', 'under', 'text', 'representation', 'for', 'nlp']]# 100-dimensional vector representation of the word - "word2vec"
array([-2.3901083e-03, -1.9926417e-03,  1.9080448e-03, -3.1678095e-03,
       -4.9522246e-04, -4.5374390e-03,  3.4716981e-03,  3.8659102e-03,
        9.2548935e-04,  5.1823643e-04,  3.4266592e-03,  3.7806653e-04,
       -2.6678396e-03, -3.2777642e-04,  1.3322923e-03, -3.0630219e-03,
        3.1524736e-03, -8.5508014e-04,  2.0837481e-03,  5.2613947e-03,
        3.7915679e-03,  5.4354439e-03,  1.6099468e-03, -4.0912461e-03,
        4.8913858e-03,  1.7630701e-03,  3.1557647e-03,  3.5352646e-03,
        1.8157288e-03, -4.0848055e-03,  6.5594626e-04, -2.7539986e-03,
        1.5574660e-03, -5.1965546e-03, -8.8450959e-04,  1.6077182e-03,
        1.5791818e-03, -6.2289328e-04,  4.5868102e-03,  2.6237629e-03,
       -2.6883748e-03,  2.6881986e-03,  4.0420778e-03,  2.3544163e-03,
        4.8873704e-03,  2.4868934e-03,  4.0510278e-03, -4.2424505e-03,
       -3.7380056e-03,  2.5551897e-03, -5.0872993e-03, -3.3367933e-03,
        1.9790635e-03,  5.7303126e-04,  3.9246562e-03, -2.4457059e-03,
        4.2443913e-03, -4.9923239e-03, -2.8107907e-03, -3.8890676e-03,
        1.5237951e-03, -1.4327581e-03, -8.9179957e-04,  3.8922462e-03,
        3.5140023e-03,  8.2534424e-04, -3.7862784e-03, -2.2930673e-03,
       -2.1645970e-05,  2.9765235e-04, -1.4117253e-03,  3.0826295e-03,
        8.1492326e-04,  2.5406217e-03,  3.3184432e-03, -3.5381948e-03,
       -3.1870278e-03, -2.7319558e-03,  3.0047926e-03, -3.9584241e-03,
        1.6430502e-03, -3.2808927e-03, -2.8428673e-03, -3.1900958e-03,
       -3.9418009e-03, -3.3188087e-03, -9.5077307e-04, -1.1602251e-03,
        3.4587954e-03,  2.6288461e-03,  3.1395135e-03,  4.0585222e-03,
       -3.5573558e-03, -1.9402980e-03, -8.6417084e-04, -4.5995312e-03,
        4.7944607e-03,  1.1922724e-03,  6.6742860e-04, -1.1188064e-04],
      dtype=float32)# Most similar terms according to the trained model to the word - "Word2Vec"
[('AI', 0.3094254434108734),
 ('fastforwarded', 0.17564082145690918),
 ('dimensional', 0.1452922821044922),
 ('under', 0.13094305992126465),
 ('for', 0.11973076313734055),
 ('of', 0.1085459440946579),
 ('embedding', 0.06551346182823181),
 ('are', 0.06285746395587921),
 ('also', 0.05645104497671127),
 ('nowadays', 0.0527990460395813)]
```

åœ¨å‡ è¡Œä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…èƒ½å¤Ÿå°†å•è¯è®­ç»ƒå’Œè¡¨ç¤ºä¸ºå‘é‡ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä¸€äº›å†…ç½®çš„å‡½æ•°æ¥ä½¿ç”¨å‘é‡è¿ç®—æ¥æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å•è¯ã€æœ€ä¸ç›¸ä¼¼çš„å•è¯ç­‰ã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥æ‰¾åˆ°å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¿™å–å†³äºå®ƒä»¬æ˜¯å¦è¢«å½’ä¸€åŒ–:

*   **å¦‚æœå½’ä¸€åŒ–:**æˆ‘ä»¬å¯ä»¥è®¡ç®—å‘é‡ä¹‹é—´çš„ç®€å•ç‚¹ç§¯ï¼Œä»¥ç¡®å®šå®ƒä»¬æœ‰å¤šç›¸ä¼¼
*   **å¦‚æœæ²¡æœ‰å½’ä¸€åŒ–:**æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å…¬å¼è®¡ç®—å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦

![](img/8006e5a557d6d6bc15b76f866c26e7df.png)

ä½™å¼¦ç›¸ä¼¼åº¦ä¸ä½™å¼¦è·ç¦»çš„å…³ç³»

å…³äºæ‰€æœ‰å¯èƒ½çš„å‚æ•°å’ŒåŠŸèƒ½ï¼Œæ‚¨å¯ä»¥å‚è€ƒä¸‹é¢çš„ gensim æ–‡æ¡£:

[](https://radimrehurek.com/gensim/models/word2vec.html) [## gensim:é¢å‘äººç±»çš„ä¸»é¢˜å»ºæ¨¡

### è¿™ä¸ªæ¨¡å—å®ç°äº† word2vec ç³»åˆ—ç®—æ³•ï¼Œä½¿ç”¨äº†é«˜åº¦ä¼˜åŒ–çš„ C ä¾‹ç¨‹ã€æ•°æ®æµå’Œâ€¦

radimrehurek.com](https://radimrehurek.com/gensim/models/word2vec.html) 

å…³äºä½™å¼¦ç›¸ä¼¼æ€§çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒä¸‹é¢çš„ç»´åŸºæ–‡ç« 

 [## ä½™å¼¦ç›¸ä¼¼æ€§

### ä½™å¼¦ç›¸ä¼¼æ€§æ˜¯å†…ç§¯ç©ºé—´çš„ä¸¤ä¸ªéé›¶å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§çš„åº¦é‡ã€‚å®ƒè¢«å®šä¹‰ä¸ºâ€¦

en.wikipedia.org](https://en.wikipedia.org/wiki/Cosine_similarity) 

è¯¥æ¶æ„çš„ç¡®åˆ‡å·¥ä½œæ–¹å¼&è®­ç»ƒç®—æ³•&å¦‚ä½•å‘ç°å•è¯ä¹‹é—´çš„å…³ç³»è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´&å€¼å¾—å•ç‹¬å†™ä¸€ç¯‡æ–‡ç« 

åŸæ–‡å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°:

 [## å‘é‡ç©ºé—´ä¸­å•è¯è¡¨ç¤ºçš„æœ‰æ•ˆä¼°è®¡

### æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„æ¨¡å‹æ¶æ„ï¼Œç”¨äºä»éå¸¸å¤§çš„æ•°æ®ä¸­è®¡ç®—å•è¯çš„è¿ç»­å‘é‡è¡¨ç¤ºâ€¦

arxiv.org](https://arxiv.org/abs/1301.3781) 

# ä¼˜åŠ¿:

*   èƒ½å¤Ÿæ•æ‰ä¸åŒå•è¯ä¹‹é—´çš„å…³ç³»ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„å¥æ³•å’Œè¯­ä¹‰å…³ç³»
*   åµŒå…¥å‘é‡çš„å¤§å°å¾ˆå°&å¾ˆçµæ´»ï¼Œä¸åƒå‰é¢è®¨è®ºçš„æ‰€æœ‰ç®—æ³•ï¼ŒåµŒå…¥çš„å¤§å°ä¸è¯æ±‡é‡æˆæ¯”ä¾‹
*   ç”±äºæ— äººç›‘ç®¡ï¼Œæ ‡è®°æ•°æ®çš„äººå·¥å·¥ä½œæ›´å°‘

# ç¼ºç‚¹:

*   Word2Vec ä¸èƒ½å¾ˆå¥½åœ°å¤„ç†è¯æ±‡å¤–çš„å•è¯ã€‚å®ƒä¸º OOV å•è¯åˆ†é…éšæœºå‘é‡è¡¨ç¤ºï¼Œè¿™å¯èƒ½æ˜¯æ¬¡ä¼˜çš„
*   å®ƒä¾èµ–äºè¯­è¨€å•è¯çš„å±€éƒ¨ä¿¡æ¯ã€‚ä¸€ä¸ªè¯çš„è¯­ä¹‰è¡¨ç¤ºåªä¾èµ–äºå®ƒçš„é‚»å±…&å¯èƒ½è¢«è¯æ˜æ˜¯æ¬¡ä¼˜çš„
*   æ–°è¯­è¨€åŸ¹è®­çš„å‚æ•°ä¸èƒ½å…±äº«ã€‚å¦‚æœä½ æƒ³ç”¨ä¸€ç§æ–°çš„è¯­è¨€è®­ç»ƒ word2vecï¼Œä½ å¿…é¡»ä»å¤´å¼€å§‹
*   éœ€è¦ç›¸å¯¹è¾ƒå¤§çš„è¯­æ–™åº“æ¥ä½¿ç½‘ç»œæ”¶æ•›(ç‰¹åˆ«æ˜¯å¦‚æœä½¿ç”¨ skip-gram

# æ‰‹å¥—

å•è¯è¡¨ç¤ºçš„å…¨å±€å‘é‡æ˜¯ NLP ä¸­ç»å¸¸ä½¿ç”¨çš„å¦ä¸€ç§è‘—åçš„åµŒå…¥æŠ€æœ¯ã€‚è¿™æ˜¯æ–¯å¦ç¦å¤§å­¦çš„ Jeffery Penningtonã€Richard Socher å’Œ Christopher D Manning åœ¨ 2014 å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡çš„ç»“æœã€‚

å®ƒè¯•å›¾å…‹æœä¸Šé¢æåˆ°çš„ word2vec çš„ç¬¬äºŒä¸ªç¼ºç‚¹ï¼Œé€šè¿‡å­¦ä¹ å•è¯çš„å±€éƒ¨å’Œå…¨å±€ç»Ÿè®¡æ¥è¡¨ç¤ºå®ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒè¯•å›¾åŒ…å«åŸºäºè®¡æ•°çš„æŠ€æœ¯(å…±ç°çŸ©é˜µ)å’ŒåŸºäºé¢„æµ‹çš„æŠ€æœ¯(Word2Vec)çš„ç²¾åï¼Œå› æ­¤ä¹Ÿè¢«ç§°ä¸ºç”¨äºè¿ç»­å•è¯è¡¨ç¤ºçš„**æ··åˆæŠ€æœ¯**

åœ¨æ‰‹å¥—ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾åŠ å¼ºä¸‹é¢çš„å…³ç³»

![](img/ac6e58589c54f2e171c174ff4ae60067.png)

å…¶å¯ä»¥è¢«é‡å†™ä¸ºï¼Œ

![](img/8b1be225201b7e1a11772a820bac9c9e.png)![](img/24858cfb00550d0ba6b1dbac632b122c.png)

å› æ­¤ï¼Œæœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬æ­£åœ¨æ„é€ å¿ å®äº P(i|j)çš„è¯å‘é‡ Vi å’Œ Vjï¼ŒP(I | j)æ˜¯ä»å…±ç°çŸ©é˜µå…¨å±€è®¡ç®—çš„ç»Ÿè®¡

GloVe çš„æ£˜æ‰‹éƒ¨åˆ†æ˜¯ç›®æ ‡å‡½æ•°çš„æ¨å¯¼ï¼Œè¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚ä½†æ˜¯æˆ‘é¼“åŠ±ä½ é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼Œå®ƒåŒ…å«äº†å®ƒçš„æ¨å¯¼è¿‡ç¨‹ï¼Œä»¥è¿›ä¸€æ­¥ç†è§£å®ƒæ˜¯å¦‚ä½•è¢«è½¬åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜çš„

[](https://www.semanticscholar.org/paper/Glove%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5) [## [PDF] Glove:å•è¯è¡¨ç¤ºçš„å…¨å±€å‘é‡|è¯­ä¹‰å­¦è€…

### æœ€è¿‘çš„å­¦ä¹ å•è¯å‘é‡ç©ºé—´è¡¨ç¤ºçš„æ–¹æ³•å·²ç»æˆåŠŸåœ°æ•è·äº†ç»†ç²’åº¦çš„è¯­ä¹‰å’Œè¯­ä¹‰

www.semanticscholar.org](https://www.semanticscholar.org/paper/Glove%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5) 

ä¸ºäº†æ”¹å˜ï¼Œè€Œä¸æ˜¯ä»é›¶å¼€å§‹æ„å»ºæ‰‹å¥—å‘é‡ï¼Œè®©æˆ‘ä»¬äº†è§£å¦‚ä½•åˆ©ç”¨åœ¨æ•°åäº¿æ¡è®°å½•ä¸Šè®­ç»ƒçš„ä»¤äººæ•¬ç•çš„é¢„è®­ç»ƒæ¨¡å‹

```
import gensim.downloader as api# Lets download a 25 dimensional GloVe representation of 2 Billion tweets
# Info on this & other embeddings : <https://nlp.stanford.edu/projects/glove/>
# Gensim provides an awesome interface to easily download pre-trained embeddings
# > 100MB to be downloaded
twitter_glove = api.load("glove-twitter-25")# To find most similar words
# Note : All outputs are lowercased. If you use upper case letters, it will throw out of vocab error
twitter_glove.most_similar("modi",topn=10)# To get the 25D vectors
twitter_glove['modi']twitter_glove.similarity("modi", "india")# This will throw an error
twitter_glove.similarity("modi", "India")
```

# è¾“å‡º:

```
# twitter_glove.most_similar("modi",topn=10)
[('kejriwal', 0.9501368999481201),
 ('bjp', 0.9385530948638916),
 ('arvind', 0.9274109601974487),
 ('narendra', 0.9249324798583984),
 ('nawaz', 0.9142388105392456),
 ('pmln', 0.9120966792106628),
 ('rahul', 0.9069461226463318),
 ('congress', 0.904523491859436),
 ('zardari', 0.8963413238525391),
 ('gujarat', 0.8910366892814636)]# twitter_glove['modi']
array([-0.56174 ,  0.69419 ,  0.16733 ,  0.055867, -0.26266 , -0.6303  ,
       -0.28311 , -0.88244 ,  0.57317 , -0.82376 ,  0.46728 ,  0.48607 ,
       -2.1942  , -0.41972 ,  0.31795 , -0.70063 ,  0.060693,  0.45279 ,
        0.6564  ,  0.20738 ,  0.84496 , -0.087537, -0.38856 , -0.97028 ,
       -0.40427 ], dtype=float32)# twitter_glove.similarity("modi", "india")
0.73462856# twitter_glove.similarity("modi", "India")
KeyError: "word 'India' not in vocabulary"
```

# ä¼˜åŠ¿

*   åœ¨ç±»æ¯”ä»»åŠ¡ä¸­ï¼Œå®ƒå¾€å¾€æ¯” word2vec è¡¨ç°å¾—æ›´å¥½
*   å®ƒåœ¨æ„é€ å‘é‡æ—¶è€ƒè™‘è¯å¯¹åˆ°è¯å¯¹çš„å…³ç³»&å› æ­¤ä¸ä»è¯-è¯å…³ç³»æ„é€ çš„å‘é‡ç›¸æ¯”ï¼Œå€¾å‘äºå‘å‘é‡æ·»åŠ æ›´å¤šçš„å«ä¹‰
*   ä¸ Word2Vec ç›¸æ¯”ï¼ŒGloVe æ›´å®¹æ˜“å¹³è¡ŒåŒ–ï¼Œå› æ­¤è®­ç»ƒæ—¶é—´æ›´çŸ­

# ä¸è¶³ä¹‹å¤„

*   å› ä¸ºå®ƒä½¿ç”¨äº†å…±ç°çŸ©é˜µ&å…¨å±€ä¿¡æ¯ï¼Œæ‰€ä»¥ä¸ word2vec ç›¸æ¯”ï¼Œå®ƒçš„å†…å­˜å¼€é”€æ›´å°
*   ç±»ä¼¼äº word2vecï¼Œå®ƒæ²¡æœ‰è§£å†³å¤šä¹‰è¯çš„é—®é¢˜ï¼Œå› ä¸ºè¯å’Œå‘é‡æ˜¯ä¸€å¯¹ä¸€çš„å…³ç³»

# è£èª‰æå:

ä¸‹é¢æ˜¯ä¸€äº›é«˜çº§è¯­è¨€æ¨¡å‹ï¼Œåœ¨æŒæ¡äº†ä¸Šè¿°è¡¨ç¤ºæ³•ä¹‹åï¼Œåº”è¯¥å¯¹å®ƒä»¬è¿›è¡Œæ¢ç´¢

# å·¥ç¨‹ä¸åå‹¤ç®¡ç†å±€

ä»è¯­è¨€æ¨¡å‹åµŒå…¥æ˜¯ Matthew E. Peters ç­‰äººåœ¨ 2018 å¹´ 3 æœˆä»¥æ·±åº¦è¯­å¢ƒåŒ–å•è¯è¡¨ç¤ºçš„åä¹‰å‘è¡¨çš„è®ºæ–‡ã€‚

å®ƒè¯•å›¾é€šè¿‡åœ¨å‘é‡è¡¨ç¤ºå’Œå®ƒæ‰€è¡¨ç¤ºçš„å•è¯ä¹‹é—´å»ºç«‹å¤šå¯¹ä¸€çš„å…³ç³»æ¥è§£å†³ word2vec & GloVe çš„ç¼ºç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒç»“åˆäº†ä¸Šä¸‹æ–‡å¹¶ç›¸åº”åœ°æ”¹å˜äº†å•è¯çš„å‘é‡è¡¨ç¤ºã€‚

å®ƒä½¿ç”¨å­—ç¬¦çº§ CNN å°†å•è¯è½¬æ¢ä¸ºåŸå§‹å•è¯å‘é‡ã€‚è¿™äº›è¿›ä¸€æ­¥è¾“å…¥åŒå‘ LSTMs è¿›è¡Œè®­ç»ƒã€‚å‘å‰å’Œå‘åè¿­ä»£çš„ç»„åˆåˆ›å»ºäº†åˆ†åˆ«è¡¨ç¤ºå•è¯å‰åçš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸­é—´å•è¯å‘é‡ã€‚

åŸå§‹å•è¯å‘é‡å’Œä¸¤ä¸ªä¸­é—´å•è¯å‘é‡çš„åŠ æƒå’Œç»™å‡ºäº†æœ€ç»ˆçš„è¡¨ç¤ºã€‚

åŸå§‹ ELMO çº¸

 [## æ·±å±‚è¯­å¢ƒåŒ–çš„è¯æ±‡è¡¨å¾

### æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ·±åº¦ä¸Šä¸‹æ–‡åŒ–çš„å•è¯è¡¨ç¤ºï¼Œå®ƒæ¨¡æ‹Ÿäº†(1)å•è¯çš„å¤æ‚ç‰¹å¾â€¦

arxiv.org](https://arxiv.org/abs/1802.05365) 

# ä¼¯ç‰¹

BERT æ˜¯è°·æ­Œ AI å›¢é˜Ÿä»¥ BERT çš„åä¹‰å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡:2019 å¹´ 5 æœˆå‡ºæ¥çš„è¯­è¨€ç†è§£æ·±åº¦åŒå‘è½¬æ¢å™¨çš„é¢„è®­ç»ƒã€‚è¿™æ˜¯ä¸€ç§æ–°çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œç”¨äºé¢„è®­ç»ƒå˜å‹å™¨ï¼Œä»¥ä¾¿é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡å¯¹å…¶è¿›è¡Œå¾®è°ƒ

BERT ä½¿ç”¨è¯­è¨€æ¨¡å‹çš„åŒå‘ä¸Šä¸‹æ–‡ï¼Œå³å®ƒè¯•å›¾å±è”½ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦ï¼Œä»¥åˆ›å»ºç”¨äºé¢„æµ‹ä»»åŠ¡çš„ä¸­é—´æ ‡è®°ï¼Œå› æ­¤æœ‰æœ¯è¯­åŒå‘ã€‚

BERT æ¨¡å‹çš„è¾“å…¥è¡¨ç¤ºæ˜¯æ ‡è®°åµŒå…¥ã€åˆ†æ®µåµŒå…¥å’Œä½ç½®åµŒå…¥çš„æ€»å’Œï¼Œéµå¾ªæ¨¡å‹çš„æ©è”½ç­–ç•¥æ¥é¢„æµ‹ä¸Šä¸‹æ–‡ä¸­çš„æ­£ç¡®å•è¯ã€‚

![](img/48e96b3fd3b9716cf603a876078549da.png)

å®ƒä½¿ç”¨ä¸€ç§è½¬æ¢ç½‘ç»œå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œå­¦ä¹ å•è¯ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œå¹¶è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰¿æ‹…å…¶ä»–ä»»åŠ¡ï¼Œå¦‚ NER å’Œé—®ç­”é…å¯¹ç­‰ã€‚

åŸæ–‡å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°:

 [## BERT:ç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘è½¬æ¢å™¨çš„é¢„è®­ç»ƒ

### æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼Œç§°ä¸º BERTï¼Œä»£è¡¨åŒå‘ç¼–ç å™¨è¡¨ç¤ºâ€¦

arxiv.org](https://arxiv.org/abs/1810.04805) 

# æ‘˜è¦

åˆ†å¸ƒå¼æ–‡æœ¬è¡¨ç¤ºæ˜¯èƒ½å¤Ÿå¤„ç† NLP ä¸­å¤æ‚é—®é¢˜é™ˆè¿°çš„å¼ºå¤§ç®—æ³•ã€‚

å•ç‹¬åœ°ï¼Œå®ƒä»¬å¯ä»¥ç”¨äºç†è§£å’Œæ¢ç´¢è¯­æ–™åº“ï¼Œä¾‹å¦‚ï¼Œæ¢ç´¢è¯­æ–™åº“ä¸­çš„å•è¯&å®ƒä»¬å¦‚ä½•å½¼æ­¤å…³è”ã€‚ä½†æ˜¯ï¼Œå½“ä¸ç”¨äºè§£å†³é—®é¢˜é™ˆè¿°çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ç»“åˆæ—¶ï¼Œå®ƒä»¬çš„ä¼˜åŠ¿å’Œé‡è¦æ€§æ‰çœŸæ­£æ˜¾ç°å‡ºæ¥ï¼Œä¾‹å¦‚é—®ç­”ã€æ–‡æ¡£åˆ†ç±»ã€èŠå¤©æœºå™¨äººã€å‘½åå®ä½“è¯†åˆ«ç­‰ç­‰ã€‚

å¦‚ä»Šï¼Œå®ƒä»¬åœ¨çŒœæƒ³ä¸­è¢«é¢‘ç¹åœ°ä½¿ç”¨ CNNs & LSTMs æ¥æ±‚è§£&æ˜¯è®¸å¤šæœ€æ–°æˆæœçš„ä¸€éƒ¨åˆ†ã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªç³»åˆ—ï¼

# å›è´­é“¾æ¥:

[](https://github.com/SundareshPrasanna/Introduction-to-text-representation-for-nlp/tree/master) [## SundareshPrasanna/è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬è¡¨ç¤ºä»‹ç»

### ä¸º SundareshPrasanna/Introduction-to-text-re presentation-for-NLP å¼€å‘åˆ›å»ºä¸€ä¸ªå¸æˆ·â€¦

github.com](https://github.com/SundareshPrasanna/Introduction-to-text-representation-for-nlp/tree/master) 

å–œæ¬¢æˆ‘çš„æ–‡ç« ï¼Ÿç»™æˆ‘ä¹°æ¯å’–å•¡

 [## sundaresh æ­£åœ¨åˆ›ä½œä¸æ•°æ®ç§‘å­¦ç›¸å…³çš„æ–‡ç« ï¼Œå¹¶ä¸”çƒ­çˆ±æ•™å­¦

### å˜¿ğŸ‘‹æˆ‘åˆšåˆšåœ¨è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªé¡µé¢ã€‚ä½ ç°åœ¨å¯ä»¥ç»™æˆ‘ä¹°æ¯å’–å•¡äº†ï¼

www.buymeacoffee.com](https://www.buymeacoffee.com/sundaresh)