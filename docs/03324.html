<html>
<head>
<title>Understanding Feature extraction using Correlation Matrix and Scatter Plots</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解使用相关矩阵和散点图进行特征提取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-feature-extraction-using-correlation-matrix-and-scatter-plots-6c19e968a60c?source=collection_archive---------13-----------------------#2020-03-30">https://towardsdatascience.com/understanding-feature-extraction-using-correlation-matrix-and-scatter-plots-6c19e968a60c?source=collection_archive---------13-----------------------#2020-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6be4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">世界上的数据非常庞大，需要非常有意识地进行处理，以获得我们希望通过新的数据科学方法实现的任何合理结果。</h2></div><p id="f51b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在处理给定数据集中的大量要素时，本文将讨论一个非常基本且重要的概念。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/0a009e99ec90c0868213bdacb6b0de6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w2ciftvnX-7Qrvy9"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lu" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Carlos Muza </a>拍摄的照片</p></figure><p id="9443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">任何典型的机器学习或深度学习模型都是为了从大量数据中提供单一输出，无论这些数据是结构化的还是非结构化的。这些因素可能在不同的系数和程度上有助于所需的结果。需要根据它们在确定输出时的<strong class="kk iu">显著性，并考虑这些因素中的<strong class="kk iu">冗余，过滤掉它们。</strong></strong></p><p id="d78f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在监督学习中，我们知道总有一个输出变量和 n 个输入变量。为了非常清楚地理解这个概念，让我们举一个简单的线性回归问题的例子。</p><p id="5dbd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在简单的线性回归模型中，我们最终会从模型中生成一个 y=mx+c 形式的方程，其中 x 是自变量，y 是因变量。由于只有一个变量，y 必须依赖于 x 的值。虽然在实时计算公交车从 A 到 b 的平均速度时，可能很少有其他被忽略的外部因素，如空气阻力。这些因素肯定会对输出产生影响，但意义最小。在这种情况下，我们的常识和经验帮助我们选择因素。因此，我们选择司机给汽车的加速度，忽略空气阻力。但是在我们不知道输入变量对输出的重要性的复杂情况下呢？数学能解决这个难题吗？</p><p id="6f40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是啊！这里就出现了<strong class="kk iu">关联的概念。</strong></p><p id="a00e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相关性是一种统计方法，表示两个或多个变量一起波动的程度。简单来说，它告诉我们一个变量对于另一个变量的微小变化有多大的变化。根据变化的方向，它可以取正值、负值和零值。因变量和自变量之间的高相关值表明自变量在确定输出时具有非常高的重要性。在有许多因素的多元回归设置中，必须找到因变量和所有自变量之间的相关性，以建立一个具有更高精度的更可行的模型。人们必须永远记住<strong class="kk iu">更多的特征并不意味着更好的准确性。</strong>如果更多的特征包含任何不相关的特征，从而在我们的模型中产生不必要的噪声，那么这些特征可能会导致精度下降。</p><p id="e352" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个变量之间的相关性可以通过各种度量来发现，如<strong class="kk iu"> <em class="lv">皮尔逊 r 相关、肯德尔等级相关、斯皮尔曼等级相关等。</em> </strong></p><p id="f497" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Pearson <em class="lv"> r </em>相关性是最广泛使用的相关统计量，用于测量线性相关变量之间的相关程度。任何两个变量 x，y 之间的皮尔逊相关性可通过下式得出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/7c73859a7e3d1e5366fc8940ce080a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*n04EWgjicn3fq6hPU4HNZw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">n-观察次数，I-表示第 I 次观察</p></figure><p id="5f7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑一下关于纽约、加利福尼亚和佛罗里达的新创业公司的数据集 50_Strartups。数据集中使用的变量是利润、R&amp;D 支出、管理支出和营销支出。这里利润是被预测的因变量。</p><p id="b2eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先对每个独立变量分别应用线性回归，以直观地显示与独立变量的相关性。</p><div class="lf lg lh li gt ab cb"><figure class="lx lj ly lz ma mb mc paragraph-image"><img src="../Images/b1b0abdbcb02ec19e3a3dea95f3f40cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*3Z9YqWr5gOKb7ej8HDwrQA.png"/></figure><figure class="lx lj ly lz ma mb mc paragraph-image"><img src="../Images/bd6b98c7afcb20ee24e4e6882834285e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*BImD3ZKrGYU8qbREBH2Qvw.png"/></figure></div><p id="b07e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从散点图中，我们可以看到，R&amp;D 支出和利润具有非常高的相关性，因此，与 R&amp;D 支出相比，预测产量和营销支出与利润的相关性较低，具有更大的意义。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi md"><img src="../Images/659d905941899a2241ddbabf5b2ce69d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*_R6mqna3fdLMvDbHU_B1ug.png"/></div></figure><p id="c5b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，管理和利润之间的分散表明，它们之间的相关性非常小，最终可能会在预测过程中产生噪声。因此，为了获得更好的结果，我们可以在模型中排除这个特性。</p><p id="7d64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个过程消除了我们模型中无关紧要和不相关的特性。但是，冗余功能怎么办？</p><p id="5c56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">冗余特征</strong>:虽然有些特征与我们的目标变量高度相关，但它们可能是冗余的。<strong class="kk iu">任何两个自变量如果高度相关，则被认为是冗余的。这造成了不必要的时间和空间浪费。甚至两个变量之间的冗余度也可以通过相关性找到。</strong></p><p id="623b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注:<strong class="kk iu">因变量和自变量之间的高相关性是理想的，而两个自变量之间的高相关性是不理想的。</strong></p><div class="lf lg lh li gt ab cb"><figure class="lx lj ly lz ma mb mc paragraph-image"><img src="../Images/358aaccfe9fc900c5c2422a34987ca6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*RUkRd6Xv8LbPjd3FFdVKjQ.png"/></figure><figure class="lx lj ly lz ma mb mc paragraph-image"><img src="../Images/d3aa3d6af2984fc314c6597908cf2ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8Gaxv8nZTQPHy0aW1A83hQ.png"/></figure></div><p id="4996" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面两张图显示了独立变量之间的相关性。我们可以在第一张图中看到较高的相关性，而在第二张图中看到非常低的相关性。这意味着我们可以排除第一张图中两个特征中的任何一个，因为两个独立变量之间的相关性会导致冗余。但是要去掉哪一个呢？答案很简单。与目标变量具有更高相关性的变量保留，而另一个被移除。</p><p id="f142" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确定变量之间的相关性:</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="7db4" class="mj mk it mf b gy ml mm l mn mo">df = pd.DataFrame(data,<br/>columns=['R&amp;D Spend','Administration','Marketing Spend','Profit'])<br/>corrMatrix = df.corr()<br/>print (corrMatrix)</span></pre><p id="c108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出:输出显示一个 2*2 矩阵，显示所有变量之间的 Pearson r 相关性。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mp"><img src="../Images/223e19b499ec6c89b184a94434c49aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k1aH2EsPClEnIs20Vz42Q.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">数据集中所有变量之间的相关性。</p></figure><p id="06e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，基于它们的 r2 分数比较各种多元回归模型。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/7a77d9a87e04ba5914a4cefbc97d811a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*k4dHQB6xA56b0sL32QMkWw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">通过选择特征的组合来评分。</p></figure><p id="b43c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从实验分数中，我们观察到:</p><p id="9d05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">-&gt;相关性低的自变量导致 r2 得分较低。(例如:仅管理)</p><p id="c55c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">-&gt;相关性较高的变量在我们的模型中为我们提供了较高的 r2 分数(例如:R&amp;D 支出和营销支出)</p><p id="5b0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">&gt;<strong class="kk iu">消除冗余变量或无关变量可能会/可能不会导致我们的精度损失可以忽略不计，但使它在许多约束条件下成为一个非常有效的模型。</strong></p><p id="a94b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">供进一步参考:</p><p id="3bde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集:【https://www.kaggle.com/farhanmd29/50-startups T4】</p><p id="08dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代号:<a class="ae lu" href="https://github.com/Tarun-Acharya/50_Startups_Regression" rel="noopener ugc nofollow" target="_blank">https://github.com/Tarun-Acharya/50_Startups_Regression</a></p></div></div>    
</body>
</html>