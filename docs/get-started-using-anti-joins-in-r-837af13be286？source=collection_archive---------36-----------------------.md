# 开始在 R 中使用反连接

> 原文：<https://towardsdatascience.com/get-started-using-anti-joins-in-r-837af13be286?source=collection_archive---------36----------------------->

## 什么是反联接&我如何使用它？

![](img/ffe2805a24002bab7562bd1d27956ca7.png)

图片由 [Free-Photos](https://pixabay.com/photos/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=690088) 来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=690088)

# 介绍

假设您已经对其他更常见的连接类型(内、左、右和外)有了一些了解；添加 semi 和 anti 可以证明非常有用，可以节省您原本可以采取的多个步骤。

在之前的一篇文章中，我概述了半连接的好处以及如何使用它们。在这里，我将用一个非常相似的反连接解释来跟进。

如果你想先复习一下半连接，你可以在这里找到。

# 过滤连接

反连接和半连接与我刚才强调的其他四个有很大的不同；最大的区别是它们实际上被归类为所谓的过滤连接。

从语法上看，它与任何其他连接都非常相似，但其目的不是用额外的列或行来增强数据集，而是使用这些连接来执行过滤。

过滤连接不是通过添加新的信息列来分类的，而是它有助于保持或减少给定数据集中的记录。

# 反加入者

使用反连接的目的是获取一个数据集，并根据某个公共标识符是否不在某个附加数据集中对其进行过滤。如果它出现在两个数据集中…它被排除。

一个很好的方法就是编写替代方案。

# 地区分析示例

假设您是一名数据分析师，正在帮助您的销售团队工作，该团队刚刚聘用了一名新的客户经理。这位新代表需要客户，销售主管希望确保这些客户目前没有被占用。对于这个问题，假设我们有两个数据集；一个包含所有帐户，另一个记录所有销售活动，这将有一个活动 id 和一个帐户 id。

第一个数据集

|帐户 id |当前所有者|帐户名称|收入

第二数据集

|活动标识|帐户标识|活动类型|

您需要做的是使用第二个数据集过滤第一个数据集。

# 在一个没有反联接的世界里

让我们首先尝试在不使用反连接的情况下解决这个问题

根据我们现在对左连接的了解，我们可以做以下事情:

```
accounts %>%
left_join(activity, by = 'account_id')%>%
filter(is.na(activity_type))
```

如您所见，我们可以将活动数据集连接到我们的帐户数据集。对于在该数据集中没有匹配的任何账户，活动类型将被填充为 NULL。因此，如果您想要一个不在第二个数据集中的帐户列表，您可以在 activity type 为 NULL 的地方进行筛选。

这很好，并且有效地执行了我上面解释的相同功能。两个讨厌的事情是 1。它为您提供了一个新字段 activity _ type2.如果同一个帐户在活动数据集中出现多次，那么当你加入时，它将为尽可能多的匹配创建一个新记录。

之后，我们还可以添加一条 select 语句来提取该字段，这只是增加了另一行代码，您可以编写这些代码来实现该功能。

```
accounts %>%
left_join(activity, by = 'account_id')%>%
filter(is.na(activity_type))%>%
select(-activity_type)
```

# 使用反连接进行简化

现在让我们用一个反连接来进一步简化事情。

```
accounts %>%
semi_join(activity, by = 'account_id')
```

这将使我们得到与上面每个例子完全相同的输出。它将过滤出出现在活动数据集中的客户记录。因此，在我们的场景中，只有未被处理的客户会被转移到新代表那里。这种方法不会向数据集中添加列或行。它专门存在，并用于过滤目的。

# 结论

现在，在短短的几分钟内，我们已经介绍了很多内容，并提供了一些 dplyr 功能，可以简化您的代码和工作流程。

我们了解到:

*   变异连接和过滤连接之间的区别
*   如何在没有反连接的情况下执行“过滤连接”
*   反连接的特定输出和意图
*   如何使用反连接

我希望这能对你作为数据专家的日常工作有所帮助。

祝数据科学快乐！