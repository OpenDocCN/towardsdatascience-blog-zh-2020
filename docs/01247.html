<html>
<head>
<title>💡Illustrating the Reformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">💡示出了重整器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0?source=collection_archive---------11-----------------------#2020-02-04">https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0?source=collection_archive---------11-----------------------#2020-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="66b0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">🚊高效变压器️</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d15877aeac0916eefd139cce1efd618a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LdrVO56qTiQHmyIJhBcT9A.png"/></div></div></figure><p id="3a73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">参见:🇫🇷 </em> <a class="ae lr" href="https://lbourdois.github.io/blog/nlp/Reformer/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">法文</em> </a>的翻译</p><p id="393c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🎥如果你一直在开发用于处理顺序数据的机器学习算法——如语言处理中的文本、语音信号或视频——你可能听说过或使用过<a class="ae lr" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">Transformer</strong></a><strong class="kw iu"/>model<strong class="kw iu">，</strong>，你可能知道它与twitter认为的不同:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ls"><img src="../Images/e5b82fb5672814fbf8f8746055b284b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ooz1uaw4EgcCyz5Q13yF_Q.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图一。破冰者，摘自克里斯·曼宁教授的推特</p></figure><p id="5c05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🔊🗞最近，谷歌推出了<a class="ae lr" href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">重整器</strong> </a> <strong class="kw iu"> </strong>架构，这是一个<em class="lq">转换器</em>模型，旨在高效处理<em class="lq">超长序列</em>的数据(例如一种语言处理多达100万个单词)。Reformer的执行需要低得多的内存消耗，即使只在一个GPU上运行也能获得令人印象深刻的性能。论文<em class="lq"> </em> <a class="ae lr" href="https://arxiv.org/pdf/2001.04451.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lq">改革者:高效的变革者</em> </a> <em class="lq"> r </em>将于2020年在ICLR亮相(并在评论中获得近乎完美的分数)。<em class="lq">改革家</em>模型有望通过超越语言应用(如音乐、语音、图像和视频生成)对该领域产生重大影响。</p><p id="fd04" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">💡在本帖中，我们将尝试深入到<strong class="kw iu"> <em class="lq">重整器模型</em> </strong>中，并尝试用一些视觉向导来理解它。准备好了吗？💪</p><h2 id="3a58" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">为什么是变形金刚？</h2><p id="9b17" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">🎬NLP中的一类任务(如机器翻译、文本生成、问题回答)可以公式化为<a class="ae lr" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">序列到序列学习问题</a>。长短期记忆(LSTM)神经网络，后来配备了一个<a class="ae lr" href="http://Neural machine translation by jointly learning to align and translate." rel="noopener ugc nofollow" target="_blank">注意力机制</a>，是一个用于建立此类问题预测模型的突出架构——例如在<a class="ae lr" href="https://arxiv.org/pdf/1609.08144.pdf" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统</a>。然而，LSTMs中递归固有的顺序性质是对数据序列进行并行计算的最大障碍(就速度和消失梯度而言)，因此，这些架构无法利用长序列的上下文。</p><p id="5fc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🚀最近的<a class="ae lr" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">变压器型号</a>——在论文<a class="ae lr" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">中介绍的“注意力就是你所需要的全部”</a>——通过消除递归并引入多头自我注意力机制，在许多任务中实现了最先进的性能。transformer的主要新颖之处在于它的并行处理能力，这使得能够处理长序列(具有数千个单词的上下文窗口)，从而以更少的训练时间产生卓越的模型，如著名的<a class="ae lr" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> Open AI的GPT2语言模型</a>。🤗Huggingface的<a class="ae lr" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚库</a>——拥有100多种语言的超过32个预训练模型以及TensorFlow和PyTorch之间的互操作性——是构建最先进的NLP系统的一项出色的开源工作。🎮<a class="ae lr" href="https://transformer.huggingface.co/" rel="noopener ugc nofollow" target="_blank">用变形金刚写作</a>和<a class="ae lr" href="https://talktotransformer.com/" rel="noopener ugc nofollow" target="_blank">与变形金刚对话</a>是一些有趣的演示。这个转换器已经被用于文本之外的应用，例如<a class="ae lr" href="https://magenta.tensorflow.org/music-transformer" rel="noopener ugc nofollow" target="_blank">生成音乐</a>和<a class="ae lr" href="https://ai.google/research/pubs/pub46840/" rel="noopener ugc nofollow" target="_blank">图像</a>。</p><h2 id="bb04" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">变形金刚少了什么？</h2><p id="1638" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">🚦在深入研究重整器之前，让我们回顾一下transformer模型的挑战性。这需要对transformer体系结构本身有所了解，这一点我们在本文中无法详述。然而，如果你还不知道，Jay Alamar的<a class="ae lr" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">The Illustrated Transformer</a>帖子是迄今为止最伟大的视觉解释，我强烈建议在阅读这篇帖子的其余部分之前阅读他的帖子。</p><p id="cffa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🤔虽然transformer模型在越来越长的序列上使用时产生了很好的结果，例如(Liu等人，2018年)中的11K长的文本示例，但许多这样的大型模型只能在大型工业计算平台上训练，甚至不能在单个GPU上进行微调，即使是针对单个训练步骤也是如此，因为它们需要内存。例如，完整的GPT-2模型包含大约1.5亿个参数。(Shazeer等人，2018年)中报告的最大配置中的参数数量超过每层0.5B，而层的数量高达64(Al-Rfou等人，2018年)。</p><p id="d65e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">💡让我们看一下变压器模型的简化概述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/cf0e26672a356d7f24d5a53c3b898d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOPx3TSpEF2faZB9_85ArQ.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图2:标准变压器模型的简化总结[图片灵感来自‘图解变压器’]</p></figure><p id="d3ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">😕如果这个模型看起来不熟悉或似乎难以理解，我敦促你在这里停下来，并审查➡️ <a class="ae lr" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">的插图变压器</a>职位。</p><p id="8450" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能会注意到有一些👓在图表中用三种不同的颜色。每个都是独一无二的👓s代表了Transformer模型的一部分，Reformer作者将其视为计算和内存问题的来源:</p><h2 id="9ed4" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">👀问题1(红色👓):注意力计算</h2><p id="e145" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">计算长度为<em class="lq"> L </em>的序列上的注意力是<em class="lq"> O(L </em>)(时间和内存都有)。想象一下，如果我们有一个长度为64K的序列，会发生什么。</p><h2 id="1478" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">👀问题2(黑色👓):大量的层</h2><p id="a730" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">具有<em class="lq"> N </em>层的模型消耗的内存是单层模型的<em class="lq"> N </em>倍，因为每一层中的激活都需要存储用于反向传播。</p><h2 id="30ac" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">👀问题3(绿色👓):前馈层的深度</h2><p id="cbb5" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">中间前馈层的深度通常比注意激活的深度大得多。</p><p id="b5aa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重整器模型解决了上述<em class="lq">转换器中内存消耗的三个</em>主要来源，并对它们进行了改进，使得重整器模型可以处理高达<em class="lq">100万字</em>的上下文窗口，所有这些都在<em class="lq">单个加速器</em>上，并且仅使用<em class="lq"> 16GB内存。</em></p><blockquote class="mw mx my"><p id="607f" class="ku kv lq kw b kx ky ju kz la lb jx lc mz le lf lg na li lj lk nb lm ln lo lp im bi translated">简而言之，重整器模型结合了两种技术来解决注意力和内存分配的问题:<a class="ae lr" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" rel="noopener ugc nofollow" target="_blank">位置敏感哈希</a> (LSH)来降低处理长序列的复杂性，以及<a class="ae lr" href="https://arxiv.org/abs/1707.04585" rel="noopener ugc nofollow" target="_blank">可逆剩余层</a>来更有效地使用可用内存。</p></blockquote><p id="510f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们将进一步探讨细节。</p><h1 id="2c6a" class="nc ly it bd lz nd ne nf mc ng nh ni mf jz nj ka mi kc nk kd ml kf nl kg mo nm bi translated">💥 1.区分位置哈希(LSH)注意</h1><h2 id="c120" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">💭注意和最近的邻居</h2><p id="b2b0" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">深度学习中的注意力是一种机制，它使网络能够根据上下文的不同部分与当前时间步长的相对性，专注于上下文的不同部分。变压器模型中存在以下三种类型的注意机制:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/48ebfe45c78194359c2d1b987d201fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWO5_viVU1nPt5bwkAXrXQ.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图3:变形金刚模型中的三种注意力</p></figure><p id="382c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">变压器中使用的标准注意力是缩放的点积，公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/f77a18ac50e2b4f1d532d335083c3608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EphJAS1hwU9NNmUQMxv92w.png"/></div></div></figure><p id="c5ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从上式和下图可以看出，乘法<em class="lq"> QKᵀ </em>(形状为[ <em class="lq"> L，L </em>)的计算和内存开销都在<em class="lq"> O(L)，</em>这是主要的内存瓶颈。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/6989125546b077c5e2602d2405a2d084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gP3LvhmH9fV5qpAPfy4H0w.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图4:(左):点积注意力的主要计算，(右)一个标记(‘它’)注意序列中其他标记的子集(‘the’，‘animal’，‘street’，‘it’，’.’)</p></figure><p id="9ae4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">❓But是否有必要计算和存储全矩阵<em class="lq"> QKᵀ </em>？<em class="lq"> </em>答案是<em class="lq">不，</em>因为我们只对<em class="lq"> softmax </em> ( <em class="lq"> QKᵀ </em>)感兴趣，它由典型稀疏矩阵中的最大元素支配。因此，正如你在上面的例子中看到的，对于每个查询<em class="lq"> q </em>，我们只需要注意最接近<em class="lq"> q </em>的键<em class="lq"> k </em>。例如，如果<em class="lq"> K </em>的长度为64K，对于每个<em class="lq"> q </em>，我们只能考虑32或64个最接近的键的一个小子集。因此，注意力机制找到了查询的最近邻关键字，但是效率很低。💡这是否让你想起了<em class="lq">最近邻搜索</em>？</p><p id="8fc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重整器的第一个新颖之处来自于用<em class="lq">局部敏感散列(LSH) </em>代替点积关注，将复杂度从<em class="lq"> O </em> ( <em class="lq"> L </em>)变为O( <em class="lq"> L log L </em>)。</p><h2 id="d059" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">最近邻搜索的🗒 LSH</h2><p id="711e" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">LSH是在<em class="lq">高维</em>数据集中<em class="lq">最近邻搜索</em>的一种<em class="lq">高效</em>和<em class="lq">近似</em>方式的著名算法。LSH背后的主要思想是选择<em class="lq">散列</em>函数，使得对于两点'<em class="lq"> p' </em>和'<em class="lq"> q' </em>，如果'<em class="lq"> q' </em>接近'<em class="lq"> p' </em>，那么以足够好的概率我们得到'<em class="lq"> hash(q) == hash(p)'。</em></p><p id="0223" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实现这一点的最简单的方法是通过随机超平面保持切割空间，并附加<em class="lq"> sign(pᵀH) </em>作为每个点的散列码。让我们看下面的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7e6e3394252c16f9b8f20d89602157da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/1*fN4ck7Jd0gDilFeAZhowpA.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图5:用于最近邻居搜索的位置敏感散列的简化动画</p></figure><p id="ffc3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们找到所需长度的哈希码，我们就根据它们的哈希码将这些点划分到桶中——在上面的例子中，<em class="lq">‘a’</em>和<em class="lq">‘b’</em>属于同一个桶，因为<em class="lq"> hash(a) == hash(b) </em>。现在，用于查找每个点的最近邻居的搜索空间从整个数据集到其所属的桶中急剧减少。</p><p id="da86" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 🗒角度LSH: </strong>普通LSH算法的一种变体，称为<em class="lq">角度LSH，</em>将点投影到单位球上，该单位球已被分成预定义的区域，每个区域具有不同的代码。然后，点的一系列随机旋转定义了点所属的桶。让我们通过一个简化的2D例子来说明这一点，这个例子摘自《改革家》一文:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c5e3f6cecf92bfe009039150deaf3cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*bj8D4K05Gz8OR-AQMhyyvA.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图6:最近邻搜索的角度LSH的简化动画:两个点是分开的[基于论文中的例子创建的动画]</p></figure><p id="3555" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我们有两个点被投影到一个单位圆上，并以不同的角度随机旋转3次。我们可以观察到它们不太可能共享同一个哈希桶。然而，在下一个示例中，我们看到彼此非常接近的两个点将在3次随机旋转后共享相同的哈希桶:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ca86ccd9e78acdd9c7e575f57482734e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*aArg6a26KqbIlEkT43fxlw.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图7:最近邻居搜索的角度LSH的简化动画:两个点接近[基于论文中的例子创建的动画]</p></figure><h2 id="fe0e" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">🚀LSH注意了</h2><p id="1179" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">现在LSH关注<em class="lq">背后的基本想法</em>如下。回过头来看上面的标准注意力公式，不是计算<em class="lq"> Q </em>和<em class="lq"> K </em>矩阵中所有向量的注意力，而是进行以下操作:</p><ul class=""><li id="e89f" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated">找到矩阵<em class="lq"> Q </em>和<em class="lq"> K </em>的LSH散列。</li><li id="b3ef" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated">仅为相同哈希桶内的<em class="lq"> k </em>和<em class="lq"> q </em>向量计算标准注意力。</li></ul><p id="e602" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">多轮LSH注意:</strong>重复上述步骤几次，以增加相似物品不落入不同桶中的概率。</p><p id="7cb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的动画根据论文中的图片展示了简化版的LSH注意力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/ec08fb5b5ca3b2c2e079d3c095b64fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*cW8irlZJytFfDkSQCPXQxA.gif"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图6:LSH注意力机制的简化示意图[根据论文中的例子制作的动画]</p></figure><h1 id="382f" class="nc ly it bd lz nd ne nf mc ng nh ni mf jz nj ka mi kc nk kd ml kf nl kg mo nm bi translated">💥 2.可逆变压器和组块</h1><p id="3a3f" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">现在我们准备解决变压器中的第二和第三个问题，即大量(<em class="lq"> N </em>)编码器和解码器层以及前馈层的深度。</p><h2 id="f8c3" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">🗒可逆残差网络</h2><p id="efd8" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">密切关注图2中的编码器和解码器块，我们意识到每个关注层和前馈层都被包装到一个<em class="lq">残差块</em>(类似于我们在图6(左)中看到的)。<em class="lq">残差网络</em>(<em class="lq">ResNet</em>s)——在这篇<a class="ae lr" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中介绍——是NN架构中使用的强大组件，用于帮助解决深度网络(具有许多层)中的<a class="ae lr" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>。然而，ResNets中的内存消耗是一个瓶颈，因为需要<em class="lq">将每层中的激活存储在内存</em>中，以便<em class="lq"> </em>在反向传播期间计算梯度。存储成本与网络中单元的数量成正比。</p><p id="3921" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了解决这个问题，由一系列<em class="lq">可逆模块</em>组成的<a class="ae lr" href="https://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagation-without-storing-activations.pdf" rel="noopener ugc nofollow" target="_blank">可逆残差网络(RevNet </a>)。在Revnet中，每一层的激活都可以从后续层的激活中精确重建，这使我们能够执行反向传播，而无需将激活存储在内存中。图6。说明了残差块和可逆残差块。请注意，我们如何从模块的输出(<em class="lq"> Y₁，Y₂ </em>)计算模块的输入(<em class="lq"> X₁，X₂ </em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/4e82ccc468dd03256f6bd81e8ace54c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifCm7OLNDi5liHo87ECEzA.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图6:<em class="oi">残差网络</em>块(左)和可逆残差块(右)示意图</p></figure><h2 id="eb3a" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">🚀可逆变压器</h2><p id="1e57" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">回到我们的第二个问题，这个问题是处理<em class="lq"> N </em>层变压器网络的内存需求——N可能相当大。</p><p id="7489" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Reformer通过将RevNet块内的<em class="lq">注意</em>和<em class="lq">前馈</em>层结合起来，将RevNet思想应用于变压器。在图6中，现在<em class="lq"> F </em>成为关注层，<em class="lq"> G </em>成为前馈层:</p><p id="34df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">y₁=x₁+attention(x₂)<br/>y₂= x₂+feedforward(y₁)</em></p><p id="a007" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🎉现在使用可逆残差层代替标准残差使得在训练过程中只存储一次激活而不是N次<em class="lq">T42。</em></p><h2 id="d2e2" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">🚀组块</h2><p id="6a5d" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">重整器中效率改进的最后一部分处理第三个问题，即前馈层的高维中间向量，其可以达到4K或更高维。</p><p id="1814" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于前馈层中的计算独立于序列中的各个位置，因此前向和后向传递的计算以及反向计算都可以分成块。例如，对于向前传球，我们将有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/9eb0c2b4e1f3bd85efe0d5a59334800d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mi0G7knwLpgRFU8uBDaNlg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">前向传递计算中的分块[图片取自Reformer论文]</p></figure><h1 id="760b" class="nc ly it bd lz nd ne nf mc ng nh ni mf jz nj ka mi kc nk kd ml kf nl kg mo nm bi translated">🚀实验结果</h1><p id="94f6" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">作者对两个任务进行了实验:图像生成任务<em class="lq"> imagenet64 </em>(序列长度为12K)和文本任务<em class="lq"> enwik8 </em>(序列长度为64K)，并评估了可逆变换和LSH哈希对内存、准确性和速度的影响。</p><p id="8b40" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🎉可逆变压器匹配基线:他们的实验结果表明，可逆变压器节省内存而不牺牲准确性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/c4635f1976e00fb12976e76a5ec1c68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_04eSe7erHNunTOQvpsiw.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">可逆性对enwik8和imagenet64训练表现的影响[图片和说明取自Reformer论文]。</p></figure><p id="a7ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🎉LSH关注度与基线相符:📔注意，由于LSH注意力是完全注意力的近似，其准确性随着哈希值的增加而提高。当哈希值为8时，LSH注意力几乎等同于完全注意力:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/4d9665d0c9895fa9fb6dd764844b4961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eN8m_1C22SYQE--E5ht9zg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">LSH注意力在imagenet64上作为散列回合的函数的效果[图像和说明取自Reformer论文]。</p></figure><p id="6e7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">🎉他们还证明，随着序列长度的增加，常规注意力会变慢，而LSH注意力速度保持稳定，它在8GB GPUs上以通常的速度运行于长度约为100K的序列上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/7786ba547c79d9599d2a7a9d54aaa94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20Vz_MjyEtjPVYK8GUfrEQ.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">注意力评估的速度是完全注意力和LSH注意力输入长度的函数。</p></figure><blockquote class="mw mx my"><p id="7b62" class="ku kv lq kw b kx ky ju kz la lb jx lc mz le lf lg na li lj lk nb lm ln lo lp im bi translated">与变压器模型相比，最终的重整器模型表现类似，但是在长序列上显示出更高的存储效率和更快的速度。</p></blockquote><h1 id="9be3" class="nc ly it bd lz nd ne nf mc ng nh ni mf jz nj ka mi kc nk kd ml kf nl kg mo nm bi translated">💻Trax:代码和示例</h1><p id="f5d1" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">🤖重整器的<a class="ae lr" href="https://github.com/google/trax/tree/master/trax/models/reformer" rel="noopener ugc nofollow" target="_blank">代码</a>已经作为新的<a class="ae lr" href="https://github.com/google/trax/tree/master/trax/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="lq"> Trax </em> </strong> </a>库的一部分发布。Trax是一个模块化的深度学习训练和推理库，旨在让您从头开始理解深度学习。重整器代码包括几个例子，您可以在<a class="ae lr" href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" rel="noopener ugc nofollow" target="_blank">图像生成</a>和<a class="ae lr" href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" rel="noopener ugc nofollow" target="_blank">文本生成</a>任务中训练和推断。</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><h1 id="f60e" class="nc ly it bd lz nd ou nf mc ng ov ni mf jz ow ka mi kc ox kd ml kf oy kg mo nm bi translated">🙏承认</h1><p id="5a26" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">我要感谢祖卡斯·凯泽生动地介绍了改革者并提供了补充材料。我还要感谢<a class="oz pa ep" href="https://medium.com/u/cbc32bf7cff7?source=post_page-----393575ac6ba0--------------------------------" rel="noopener" target="_blank"> Abraham Kang </a>的深刻回顾和建设性反馈。</p><h1 id="12a1" class="nc ly it bd lz nd ne nf mc ng nh ni mf jz nj ka mi kc nk kd ml kf nl kg mo nm bi translated">📚参考资料和相关链接:</h1><div class="pb pc gp gr pd pe"><a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">改革家:高效的变压器</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">理解连续数据——如语言、音乐或视频——是一项具有挑战性的任务，尤其是当有…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">ai.googleblog.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ks pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">Transformer:一种用于语言理解的新型神经网络结构</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">神经网络，特别是递归神经网络(RNNs)，现在处于领先方法的核心…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">ai.googleblog.com</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps ks pe"/></div></div></a></div><ul class=""><li id="2c50" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated">改革家:高效的改革家</li><li id="1672" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://github.com/google/trax/tree/master/trax/" rel="noopener ugc nofollow" target="_blank"> Google/Trax深度学习库</a></li><li id="512a" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="a0a2" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸/变形金刚NLP库</a></li><li id="89f5" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated">你所需要的只是注意力</li><li id="3dd9" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">开放AI的GPT2语言模型</a></li><li id="fe9b" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://transformer.huggingface.co/" rel="noopener ugc nofollow" target="_blank">用变压器写字</a></li><li id="0935" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://talktotransformer.com/" rel="noopener ugc nofollow" target="_blank">与变形金刚对话</a></li><li id="4682" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1609.08144.pdf" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统</a></li></ul></div></div>    
</body>
</html>