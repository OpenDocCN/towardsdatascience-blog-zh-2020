<html>
<head>
<title>PyTorch [Basics] — Tensors and Autograd</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch【基础】—张量和亲笔签名</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-your-neural-net-tensors-and-autograd-941f2c4cc77c?source=collection_archive---------21-----------------------#2020-01-06">https://towardsdatascience.com/how-to-train-your-neural-net-tensors-and-autograd-941f2c4cc77c?source=collection_archive---------21-----------------------#2020-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="61a3" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="c2ac" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带您了解一些最常用的张量运算，并演示PyTorch中的自动签名功能。</h2></div><p id="f2f2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">随着PyTorch的大肆宣传，我决定冒险在2019年底学习它。开始有点困难，因为没有太多适合初学者的教程(在我看来)。有一些博客文章只是浏览了一个模型实现，其中到处都有一些深度学习理论，或者有一个巨大的Github repo，其中有几个python脚本，实现了一个具有多个依赖关系的超级复杂的模型。</p><p id="b0f7" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这两种情况下，你都会想——“<em class="lz">呃……什么？”</em></p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/0f17cfb5668e72875b66f09aa4f98ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*LJvxtR1CgTh7Md7vTyJw2w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">安德烈·卡帕西为PyTorch发的推文[图片[1]]</p></figure><p id="f727" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在使用PyTorch一段时间后，我发现它是最好的深度学习框架。因此，在2020年，我决定每两周(希望是:P)发表一篇博文，介绍我在<strong class="lf jp"> PyTorch 1.0+ </strong>中在时间序列预测、NLP和计算机视觉领域实现的一些东西。</p><blockquote class="mf"><p id="4f15" class="mg mh jf bd mi mj mk ml mm mn mo ly dk translated">这4大类是——py torch[基础]、py torch[表格]、PyTorch[NLP]和py torch[远景]。</p></blockquote></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="e140" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这篇博文中，我们将实现一些最常用的张量运算，并谈一谈PyTorch中的自动签名功能。</p><h1 id="243e" class="mw mx jf bd my mz na nb nc nd ne nf ng ku nh kv ni kx nj ky nk la nl lb nm nn bi translated">导入库</h1><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="b94a" class="nt mx jf np b gy nu nv l nw nx">import numpy as np<br/><br/>import torch<br/>from torch.autograd import grad</span></pre><h1 id="ee15" class="mw mx jf bd my mz na nb nc nd ne nf ng ku nh kv ni kx nj ky nk la nl lb nm nn bi translated">张量运算</h1><h2 id="0299" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">创建一个单位化张量</h2><p id="1e09" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated">长张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="6738" class="nt mx jf np b gy nu nv l nw nx">x = torch.LongTensor(3, 4)<br/>print(x)<br/></span><span id="be6a" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="b8c4" class="nt mx jf np b gy on nv l nw nx">tensor([[140124855070912, 140124855070984, 140124855071056, 140124855071128],<br/>        [140124855071200, 140124855071272, 140124855068720, 140125080614480],<br/>        [140125080521392, 140124855066736, 140124855066800, 140124855066864]])</span></pre><p id="5e2a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">浮点张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="75c7" class="nt mx jf np b gy nu nv l nw nx">x = torch.FloatTensor(3, 4)<br/>print(x)</span><span id="74d5" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="100c" class="nt mx jf np b gy on nv l nw nx">tensor([[1.7288e+25, 4.5717e-41, 1.7288e+25, 4.5717e-41],<br/>        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],<br/>        [0.0000e+00, 2.7523e+23, 1.8788e+31, 1.7220e+22]])</span></pre><h2 id="93fd" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">手动创建张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="5efc" class="nt mx jf np b gy nu nv l nw nx">x1 = torch.tensor([[1., 2., 3.], [4., 5., 6.]])<br/>x1</span><span id="df5c" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="4478" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span></pre><h2 id="a77a" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">从列表中创建张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="537d" class="nt mx jf np b gy nu nv l nw nx">py_list = [[1, 2, 3], [4, 5, 6]]<br/>print('List: \n', py_list, '\n')</span><span id="7c11" class="nt mx jf np b gy on nv l nw nx">print('Tensor:')<br/>print(torch.tensor(py_list))</span><span id="c2c0" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="3ed2" class="nt mx jf np b gy on nv l nw nx">List: <br/> [[1, 2, 3], [4, 5, 6]] </span><span id="3c38" class="nt mx jf np b gy on nv l nw nx">Tensor:<br/>tensor([[1, 2, 3],<br/>        [4, 5, 6]])</span></pre><h2 id="9ea4" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">从numpy数组创建张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="b4ff" class="nt mx jf np b gy nu nv l nw nx">numpy_array = np.random.random((3, 4)).astype(float)<br/>print('Float numpy array: \n', numpy_array, '\n')</span><span id="5526" class="nt mx jf np b gy on nv l nw nx">print('Float Tensor:')<br/>print(torch.FloatTensor(numpy_array))</span><span id="1e2b" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="d02f" class="nt mx jf np b gy on nv l nw nx">Float numpy array: <br/> [[0.03161911 0.52214984 0.06134578 0.03143846]<br/> [0.72253513 0.1396692  0.14399402 0.02879052]<br/> [0.01618331 0.41498778 0.60040201 0.95173069]] </span><span id="3d34" class="nt mx jf np b gy on nv l nw nx">Float Tensor:<br/>tensor([[0.0316, 0.5221, 0.0613, 0.0314],<br/>        [0.7225, 0.1397, 0.1440, 0.0288],<br/>        [0.0162, 0.4150, 0.6004, 0.9517]])</span></pre><h2 id="52d6" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">将张量转换为numpy数组</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="9eb9" class="nt mx jf np b gy nu nv l nw nx">print('Tensor:')<br/>print(x1)</span><span id="ec9b" class="nt mx jf np b gy on nv l nw nx">print('\nNumpy array:')<br/>print(x1.numpy())</span><span id="bf6c" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="68d1" class="nt mx jf np b gy on nv l nw nx">Tensor:<br/>tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span><span id="c22a" class="nt mx jf np b gy on nv l nw nx">Numpy array:<br/>[[1. 2. 3.]<br/> [4. 5. 6.]]</span></pre><h2 id="ad01" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">在一个范围内创建张量</h2><p id="15e0" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated">长型张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="0e80" class="nt mx jf np b gy nu nv l nw nx"># Create tensor from 0 to 10.<br/>torch.arange(10, dtype=torch.long)</span><span id="0abb" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="d8a5" class="nt mx jf np b gy on nv l nw nx">tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></pre><p id="c7cf" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">浮点型张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="3e49" class="nt mx jf np b gy nu nv l nw nx">torch.arange(10, dtype=torch.float)</span><span id="bdca" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="670e" class="nt mx jf np b gy on nv l nw nx">tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])</span></pre><h2 id="a432" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">用随机值创建张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="18b8" class="nt mx jf np b gy nu nv l nw nx">torch.randn(3, 4)</span><span id="9e6f" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="ad00" class="nt mx jf np b gy on nv l nw nx">tensor([[ 0.6797,  1.3238, -0.5602, -0.6977],<br/>        [ 1.1281, -0.8198, -0.2968, -0.1166],<br/>        [ 0.8521, -1.0367,  0.5664, -0.7052]])</span></pre><p id="ec03" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">只有正数。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="d585" class="nt mx jf np b gy nu nv l nw nx">torch.rand(3, 4)</span><span id="1ec1" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="43c7" class="nt mx jf np b gy on nv l nw nx">tensor([[0.8505, 0.8817, 0.2612, 0.8152],<br/>        [0.6652, 0.3061, 0.0246, 0.4880],<br/>        [0.7158, 0.6929, 0.4464, 0.0923]])</span></pre><h2 id="1018" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">创建张量作为单位矩阵</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="4b8c" class="nt mx jf np b gy nu nv l nw nx">torch.eye(3, 3)</span><span id="339f" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="4bc3" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 0., 0.],<br/>        [0., 1., 0.],<br/>        [0., 0., 1.]])</span></pre><h2 id="621b" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">创建全零张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="8803" class="nt mx jf np b gy nu nv l nw nx">torch.zeros(3, 4)</span><span id="0569" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="fe95" class="nt mx jf np b gy on nv l nw nx">tensor([[0., 0., 0., 0.],<br/>        [0., 0., 0., 0.],<br/>        [0., 0., 0., 0.]])</span></pre><h2 id="00a8" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">创建全1张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="ae95" class="nt mx jf np b gy nu nv l nw nx">torch.ones(3, 4)</span><span id="ebfe" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="ea13" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 1., 1., 1.],<br/>        [1., 1., 1., 1.],<br/>        [1., 1., 1., 1.]])</span></pre><h2 id="2406" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">索引张量</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="7c3d" class="nt mx jf np b gy nu nv l nw nx">print(x1, '\n')<br/>print(f"Tensor at x1[0,0] = {x1[0, 0]}")<br/>print(f"Tensor at x1[1,2] = {x1[1, 2]}")<br/>print(f"0th column of x1 = {x1[:, 0]}")<br/>print(f"1st row of x1 = {x1[1, :]}")<br/></span><span id="13a9" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="0e08" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]]) </span><span id="478b" class="nt mx jf np b gy on nv l nw nx">Tensor at x1[0,0] = 1.0<br/>Tensor at x1[1,2] = 6.0<br/>0th column of x1 = tensor([1., 4.])<br/>1st row of x1 = tensor([4., 5., 6.])</span></pre><h2 id="2cab" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">张量的形状</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="6882" class="nt mx jf np b gy nu nv l nw nx">print(x1, '\n')<br/>print(x1.shape)<br/></span><span id="f136" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="82f0" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]]) </span><span id="7200" class="nt mx jf np b gy on nv l nw nx">torch.Size([2, 3])</span></pre><h2 id="d965" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">重塑张量</h2><p id="03f6" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated">创建一个张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="bc51" class="nt mx jf np b gy nu nv l nw nx">x2 = torch.arange(10, dtype=torch.float)<br/>x2<br/></span><span id="fe13" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="cbcf" class="nt mx jf np b gy on nv l nw nx">tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])</span></pre><p id="72f0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">使用<code class="fe oo op oq np b">.view</code>重塑张量。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="fcec" class="nt mx jf np b gy nu nv l nw nx">x2.view(2, 5)<br/></span><span id="37f0" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="8745" class="nt mx jf np b gy on nv l nw nx">tensor([[0., 1., 2., 3., 4.],<br/>        [5., 6., 7., 8., 9.]])</span></pre><p id="cefe" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe oo op oq np b">-1</code>根据张量的大小自动识别维度。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="3b20" class="nt mx jf np b gy nu nv l nw nx">x2.view(5, -1)<br/></span><span id="2b6a" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="17d5" class="nt mx jf np b gy on nv l nw nx">tensor([[0., 1.],<br/>        [2., 3.],<br/>        [4., 5.],<br/>        [6., 7.],<br/>        [8., 9.]])</span></pre><p id="e51d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">使用<code class="fe oo op oq np b">.reshape</code>整形。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="8f87" class="nt mx jf np b gy nu nv l nw nx">x2.reshape(5, -1)<br/></span><span id="64a4" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="24a6" class="nt mx jf np b gy on nv l nw nx">tensor([[0., 1.],<br/>        [2., 3.],<br/>        [4., 5.],<br/>        [6., 7.],<br/>        [8., 9.]])</span></pre><h2 id="e6be" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">改变张量轴</h2><p id="fc17" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated"><code class="fe oo op oq np b">view</code>和<code class="fe oo op oq np b">permute</code>略有不同。<code class="fe oo op oq np b">view</code>改变张量的顺序，而<code class="fe oo op oq np b">permute</code>仅改变轴。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="e233" class="nt mx jf np b gy nu nv l nw nx">print("x1: \n", x1)<br/>print("\nx1.shape: \n", x1.shape)<br/>print("\nx1.view(3, -1): \n", x1.view(3, -1))<br/>print("\nx1.permute(1, 0): \n", x1.permute(1, 0))</span><span id="39c8" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="6f96" class="nt mx jf np b gy on nv l nw nx">x1: <br/> tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span><span id="3d7c" class="nt mx jf np b gy on nv l nw nx">x1.shape: <br/> torch.Size([2, 3])</span><span id="7fe6" class="nt mx jf np b gy on nv l nw nx">x1.view(3, -1): <br/> tensor([[1., 2.],<br/>        [3., 4.],<br/>        [5., 6.]])</span><span id="f3cf" class="nt mx jf np b gy on nv l nw nx">x1.permute(1, 0): <br/> tensor([[1., 4.],<br/>        [2., 5.],<br/>        [3., 6.]])</span></pre><h2 id="f418" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">获取张量的内容</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="51d1" class="nt mx jf np b gy nu nv l nw nx">print(x2)<br/>print(x2[3])<br/>print(x2[3].item())<br/></span><span id="b7de" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="fa77" class="nt mx jf np b gy on nv l nw nx">tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])<br/>tensor(3.)<br/>3.0</span></pre><h2 id="cfa2" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">得到张量的平均值</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="22dc" class="nt mx jf np b gy nu nv l nw nx">print(x1)<br/>torch.mean(x1)<br/></span><span id="1963" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="9b39" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span><span id="19a4" class="nt mx jf np b gy on nv l nw nx">tensor(3.5000)</span></pre><h2 id="48c8" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">获取张量中值的和</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="afc2" class="nt mx jf np b gy nu nv l nw nx">print(x1)<br/>torch.sum(x1)<br/></span><span id="bb91" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="cf97" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span><span id="c0b8" class="nt mx jf np b gy on nv l nw nx">tensor(21.)</span></pre><h2 id="0500" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">获取张量的e^x值</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="bdd1" class="nt mx jf np b gy nu nv l nw nx">print(x1)<br/>torch.exp(x1)</span><span id="46c4" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="02cb" class="nt mx jf np b gy on nv l nw nx">tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])</span><span id="c0c4" class="nt mx jf np b gy on nv l nw nx">tensor([[  2.7183,   7.3891,  20.0855],<br/>        [ 54.5981, 148.4132, 403.4288]])</span></pre><h2 id="2688" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">张量算术运算</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="6ccc" class="nt mx jf np b gy nu nv l nw nx">x3 = torch.randn(5, 2)<br/>x4 = torch.randn(5, 2)</span><span id="b263" class="nt mx jf np b gy on nv l nw nx">print(f"x3 + x4 = \n{x3 + x4}\n")<br/>print(f"x3 - x4 = \n{x3 - x4}\n")<br/>print(f"x3 * x4  (element wise)= \n{x3 * x4}\n")<br/># You can also do [x3 @ x4.t()] for matrix multiplication.<br/>print(f"x3 @ x4  (matrix mul)= \n{torch.matmul(x3, x4.t())}\n")</span><span id="43f6" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="6f25" class="nt mx jf np b gy on nv l nw nx">x3 + x4 = <br/>tensor([[-0.1989,  1.9295],<br/>        [-0.1405, -0.8919],<br/>        [-0.6190, -3.6546],<br/>        [-1.4263, -0.1889],<br/>        [ 0.7664, -0.6130]])</span><span id="698c" class="nt mx jf np b gy on nv l nw nx">x3 - x4 = <br/>tensor([[ 1.5613, -1.8650],<br/>        [-2.2483, -1.9581],<br/>        [ 0.4915,  0.6334],<br/>        [ 2.0189, -2.8248],<br/>        [-2.5310, -4.7337]])</span><span id="2434" class="nt mx jf np b gy on nv l nw nx">x3 * x4  (element wise)= <br/>tensor([[-0.5995,  0.0612],<br/>        [-1.2588, -0.7597],<br/>        [ 0.0354,  3.2387],<br/>        [-0.5104, -1.9860],<br/>        [-1.4547, -5.5080]])</span><span id="71f7" class="nt mx jf np b gy on nv l nw nx">x3 @ x4  (matrix mul)= <br/>tensor([[-0.5384,  0.7351, -0.4474, -1.1310,  1.1895],<br/>        [-1.6525, -2.0185,  3.7184,  0.1793, -4.9052],<br/>        [-2.8099, -0.8725,  3.2741, -1.8812, -3.2174],<br/>        [-3.1196, -0.4</span></pre><h2 id="d31a" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">连接张量</h2><p id="b977" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated">连接行。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="3ef5" class="nt mx jf np b gy nu nv l nw nx">x5, x6 = torch.randn(4, 7), torch.randn(3, 7)<br/>print('X5:', x5.shape)<br/>print(x5)<br/>print('\nX6:', x6.shape)<br/>print(x6)</span><span id="6cc3" class="nt mx jf np b gy on nv l nw nx">print('\nConcatenated tensor', torch.cat((x5, x6)).shape)<br/>print(torch.cat((x5, x6)))</span><span id="83d5" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="849a" class="nt mx jf np b gy on nv l nw nx">X5: torch.Size([4, 7])<br/>tensor([[ 0.9009, -0.7907,  0.2602, -1.4544,  1.0479, -1.1085,  0.9261],<br/>        [ 0.2041, -1.2046, -0.8984,  1.6531,  0.2567, -0.0466,  0.1195],<br/>        [-0.7890, -0.0156,  0.2190, -1.5073, -0.2212,  0.4541,  0.7874],<br/>        [ 0.7968, -0.1711, -1.0618, -0.1209, -0.4825, -0.7380, -2.1153]])</span><span id="03a9" class="nt mx jf np b gy on nv l nw nx">X6: torch.Size([3, 7])<br/>tensor([[-1.1890,  0.1310,  1.7379,  1.8666,  1.4759,  1.9887,  1.1943],<br/>        [ 0.1609, -0.3116,  0.5274,  1.3037, -1.2190, -1.6068,  0.5382],<br/>        [ 1.0021,  0.5119,  1.7237, -0.3709, -0.1801, -0.3868, -1.0468]])</span><span id="2099" class="nt mx jf np b gy on nv l nw nx">Concatenated tensor torch.Size([7, 7])<br/>tensor([[ 0.9009, -0.7907,  0.2602, -1.4544,  1.0479, -1.1085,  0.9261],<br/>        [ 0.2041, -1.2046, -0.8984,  1.6531,  0.2567, -0.0466,  0.1195],<br/>        [-0.7890, -0.0156,  0.2190, -1.5073, -0.2212,  0.4541,  0.7874],<br/>        [ 0.7968, -0.1711, -1.0618, -0.1209, -0.4825, -0.7380, -2.1153],<br/>        [-1.1890,  0.1310,  1.7379,  1.8666,  1.4759,  1.9887,  1.1943],<br/>        [ 0.1609, -0.3116,  0.5274,  1.3037, -1.2190, -1.6068,  0.5382],<br/>        [ 1.0021,  0.5119,  1.7237, -0.3709, -0.1801, -0.3868, -1.0468]])</span></pre><p id="2b4f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">串联列。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="4517" class="nt mx jf np b gy nu nv l nw nx">x7, x8 = torch.randn(3, 3), torch.randn(3, 5)<br/>print('X7:', x7.shape)<br/>print(x7)<br/>print('\nX8:', x8.shape)<br/>print(x8)</span><span id="1120" class="nt mx jf np b gy on nv l nw nx">print('\nConcatenated tensor', torch.cat((x7, x8), 1).shape)<br/>print(torch.cat((x7, x7), 1))</span><span id="92a0" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="5474" class="nt mx jf np b gy on nv l nw nx">X7: torch.Size([3, 3])<br/>tensor([[-1.1606,  1.5264,  1.4718],<br/>        [ 0.9691, -0.1215, -0.1852],<br/>        [-0.5792, -1.2848, -0.8485]])</span><span id="0011" class="nt mx jf np b gy on nv l nw nx">X8: torch.Size([3, 5])<br/>tensor([[ 0.3518, -0.2296,  0.5546,  1.5093,  1.6241],<br/>        [ 0.8275, -1.1022, -0.1441, -0.3518,  0.4338],<br/>        [-0.1501, -1.1083,  0.3815, -0.5076,  0.5819]])</span><span id="0063" class="nt mx jf np b gy on nv l nw nx">Concatenated tensor torch.Size([3, 8])<br/>tensor([[-1.1606,  1.5264,  1.4718, -1.1606,  1.5264,  1.4718],<br/>        [ 0.9691, -0.1215, -0.1852,  0.9691, -0.1215, -0.1852],<br/>        [-0.5792, -1.2848, -0.8485, -0.5792, -1.2848, -0.8485]])</span></pre><h2 id="ffe9" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">查找张量中的最小值/最大值</h2><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="959d" class="nt mx jf np b gy nu nv l nw nx">print('Tensor x1:')<br/>print(x1)</span><span id="1593" class="nt mx jf np b gy on nv l nw nx">print('\nMax value in the tensor:')<br/>print(torch.max(x1))</span><span id="626b" class="nt mx jf np b gy on nv l nw nx">print('\nIndex of the max value in the tensor')<br/>print(torch.argmax(x1))</span><span id="914a" class="nt mx jf np b gy on nv l nw nx">print('\nMin value in the tensor:')<br/>print(torch.min(x1))</span><span id="a978" class="nt mx jf np b gy on nv l nw nx">print('\nIndex of the min value in the tensor')<br/>print(torch.argmin(x1))</span><span id="ad4a" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="19e6" class="nt mx jf np b gy on nv l nw nx">Tensor x1:<br/>tensor([[1., 2., 3.],<br/>        [4., 5., 6.]])<br/></span><span id="66b5" class="nt mx jf np b gy on nv l nw nx">Max value in the tensor:<br/>tensor(6.)<br/></span><span id="e0fc" class="nt mx jf np b gy on nv l nw nx">Index of the max value in the tensor<br/>tensor(5)<br/></span><span id="a29c" class="nt mx jf np b gy on nv l nw nx">Min value in the tensor:<br/>tensor(1.)<br/></span><span id="abdd" class="nt mx jf np b gy on nv l nw nx">Index of the min value in the tensor<br/>tensor(0)</span></pre><h2 id="f40e" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">从张量中删除维度</h2><p id="86b0" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated"><code class="fe oo op oq np b">torch.squeeze()</code>从张量中删除所有一维部分。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="57e6" class="nt mx jf np b gy nu nv l nw nx">x9 = torch.zeros(2, 1, 3, 1, 5)<br/>print('Tensor:')<br/>print(x9)</span><span id="b486" class="nt mx jf np b gy on nv l nw nx">print('\nTensor shape:')<br/>print(x9.shape)</span><span id="e376" class="nt mx jf np b gy on nv l nw nx">print('\nTensor after torch.squeeze():')<br/>print(x9.squeeze())</span><span id="3d64" class="nt mx jf np b gy on nv l nw nx">print('\nTensor shape after torch.squeeze():')<br/>print(x9.squeeze().shape)</span><span id="e9e4" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="d95f" class="nt mx jf np b gy on nv l nw nx">Tensor:<br/>tensor([[[[[0., 0., 0., 0., 0.]],</span><span id="d6b4" class="nt mx jf np b gy on nv l nw nx">          [[0., 0., 0., 0., 0.]],</span><span id="8785" class="nt mx jf np b gy on nv l nw nx">          [[0., 0., 0., 0., 0.]]]],</span><span id="0ca9" class="nt mx jf np b gy on nv l nw nx">        [[[[0., 0., 0., 0., 0.]],</span><span id="7615" class="nt mx jf np b gy on nv l nw nx">          [[0., 0., 0., 0., 0.]],</span><span id="7447" class="nt mx jf np b gy on nv l nw nx">          [[0., 0., 0., 0., 0.]]]]])</span><span id="0b38" class="nt mx jf np b gy on nv l nw nx">Tensor shape:<br/>torch.Size([2, 1, 3, 1, 5])</span><span id="8a91" class="nt mx jf np b gy on nv l nw nx">Tensor after torch.squeeze():<br/>tensor([[[0., 0., 0., 0., 0.],<br/>         [0., 0., 0., 0., 0.],<br/>         [0., 0., 0., 0., 0.]],</span><span id="a926" class="nt mx jf np b gy on nv l nw nx">        [[0., 0., 0., 0., 0.],<br/>         [0., 0., 0., 0., 0.],<br/>         [0., 0., 0., 0., 0.]]])</span><span id="82b1" class="nt mx jf np b gy on nv l nw nx">Tensor shape after torch.squeeze():<br/>torch.Size([2, 3, 5])</span></pre><p id="1d47" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">另一个例子来阐明它是如何工作的。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="1993" class="nt mx jf np b gy nu nv l nw nx">x10 = torch.zeros(3, 1)<br/>print('Tensor:')<br/>print(x10)</span><span id="32f6" class="nt mx jf np b gy on nv l nw nx">print('\nTensor shape:')<br/>print(x10.shape)</span><span id="bbdc" class="nt mx jf np b gy on nv l nw nx">print('\nTensor shape after torch.squeeze(0):')<br/>print(x10.squeeze(0).shape)</span><span id="0633" class="nt mx jf np b gy on nv l nw nx">print('\nTensor shape after torch.squeeze(1):')<br/>print(x10.squeeze(1).shape)</span><span id="05fe" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="5c09" class="nt mx jf np b gy on nv l nw nx">Tensor:<br/>tensor([[0.],<br/>        [0.],<br/>        [0.]])</span><span id="ae9b" class="nt mx jf np b gy on nv l nw nx">Tensor shape:<br/>torch.Size([3, 1])</span><span id="4f83" class="nt mx jf np b gy on nv l nw nx">Tensor shape after torch.squeeze(0):<br/>torch.Size([3, 1])</span><span id="036d" class="nt mx jf np b gy on nv l nw nx">Tensor shape after torch.squeeze(1):<br/>torch.Size([3])</span></pre><h2 id="951b" class="nt mx jf bd my ny nz dn nc oa ob dp ng lm oc od ni lq oe of nk lu og oh nm jl bi translated">使用自动签名的反向传播</h2><p id="8cf0" class="pw-post-body-paragraph ld le jf lf b lg oi kp li lj oj ks ll lm ok lo lp lq ol ls lt lu om lw lx ly ij bi translated">如果<code class="fe oo op oq np b">requires_grad=True</code>，张量对象保持跟踪它是如何被创建的。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="f05c" class="nt mx jf np b gy nu nv l nw nx">x = torch.tensor([1., 2., 3.], requires_grad = True)<br/>print('x: ', x)</span><span id="b24a" class="nt mx jf np b gy on nv l nw nx">y = torch.tensor([10., 20., 30.], requires_grad = True)<br/>print('y: ', y)</span><span id="b9b4" class="nt mx jf np b gy on nv l nw nx">z = x + y <br/>print('\nz = x + y')<br/>print('z:', z)</span><span id="5da8" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="a395" class="nt mx jf np b gy on nv l nw nx">x:  tensor([1., 2., 3.], requires_grad=True)<br/>y:  tensor([10., 20., 30.], requires_grad=True)</span><span id="ca41" class="nt mx jf np b gy on nv l nw nx">z = x + y<br/>z: tensor([11., 22., 33.], grad_fn=&lt;AddBackward0&gt;)</span></pre><p id="4ccb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">既然，<code class="fe oo op oq np b">requires_grad=True</code>，<code class="fe oo op oq np b">z</code>知道它是由两个张量<code class="fe oo op oq np b">z = x + y</code>相加而产生的。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="4557" class="nt mx jf np b gy nu nv l nw nx">s = z.sum()<br/>print(s)</span><span id="8916" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="a103" class="nt mx jf np b gy on nv l nw nx">tensor(66., grad_fn=&lt;SumBackward0&gt;)</span></pre><p id="392f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe oo op oq np b">s</code>知道它是由它的数字之和创造出来的。当我们在<code class="fe oo op oq np b">s</code>上调用<code class="fe oo op oq np b">.backward()</code>时，backprop从<code class="fe oo op oq np b">s</code>开始运行。然后我们可以计算梯度。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="b490" class="nt mx jf np b gy nu nv l nw nx">s.backward()<br/>print('x.grad: ', x.grad)<br/>print('y.grad: ', y.grad)</span><span id="31e6" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="8b40" class="nt mx jf np b gy on nv l nw nx">x.grad:  tensor([1., 1., 1.])<br/>y.grad:  tensor([1., 1., 1.])</span></pre><h1 id="9d20" class="mw mx jf bd my mz na nb nc nd ne nf ng ku nh kv ni kx nj ky nk la nl lb nm nn bi translated">亲笔签名示例</h1><blockquote class="mf"><p id="5d87" class="mg mh jf bd mi mj mk ml mm mn mo ly dk translated">我在这里引用了一些很好的资源来研究PyTorch上的亲笔签名</p><p id="dfaa" class="mg mh jf bd mi mj mk ml mm mn mo ly dk translated"><a class="ae or" href="https://www.uio.no/studier/emner/matnat/ifi/IN5400/v19/material/week4/IN5400_2019_week4_intro_to_pytorch.pdf#page=39&amp;zoom=100,-87,505" rel="noopener ugc nofollow" target="_blank">奥斯陆大学的幻灯片</a></p><p id="bb05" class="mg mh jf bd mi mj mk ml mm mn mo ly dk translated"><a class="ae or" href="https://www.youtube.com/watch?v=MswxJw-8PvE" rel="noopener ugc nofollow" target="_blank">埃利奥特·瓦特——YouTube</a></p></blockquote><pre class="os ot ou ov ow no np nq nr aw ns bi"><span id="ce54" class="nt mx jf np b gy nu nv l nw nx">x1 = torch.tensor(2, dtype = torch.float32, requires_grad = True)<br/>x2 = torch.tensor(3, dtype = torch.float32, requires_grad = True)<br/>x3 = torch.tensor(1, dtype = torch.float32, requires_grad = True)<br/>x4 = torch.tensor(4, dtype = torch.float32, requires_grad = True)</span><span id="940d" class="nt mx jf np b gy on nv l nw nx">z1 = x1 * x2 <br/>z2 = x3 * x4<br/>f = z1 + z2</span><span id="4840" class="nt mx jf np b gy on nv l nw nx">gradients = grad(outputs=f, inputs = [x1, x2, x3, x4, z1, z2])<br/>gradients<br/></span><span id="080d" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="6ab0" class="nt mx jf np b gy on nv l nw nx">(tensor(3.), tensor(2.), tensor(4.), tensor(1.), tensor(1.), tensor(1.))</span></pre><p id="7b0b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们打印出所有的梯度。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="e2c4" class="nt mx jf np b gy nu nv l nw nx">print(f"Gradient of x1 = {gradients[0]}")<br/>print(f"Gradient of x2 = {gradients[1]}")<br/>print(f"Gradient of x3 = {gradients[2]}")<br/>print(f"Gradient of x4 = {gradients[3]}")<br/>print(f"Gradient of z1 = {gradients[4]}")<br/>print(f"Gradient of z2 = {gradients[5]}")</span><span id="f5b1" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="94b5" class="nt mx jf np b gy on nv l nw nx">Gradient of x1 = 3.0<br/>Gradient of x2 = 2.0<br/>Gradient of x3 = 4.0<br/>Gradient of x4 = 1.0<br/>Gradient of z1 = 1.0<br/>Gradient of z2 = 1.0</span></pre><p id="892e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf jp">叶张量</strong>是直接创建的张量，不是任何算术运算的结果。<br/>在上面的例子中，<em class="lz"> x1，x2，x3，x4 </em>是叶张量，而<em class="lz"> z1 </em>和<em class="lz"> z2 </em>不是。</p><p id="3ff0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们可以使用<code class="fe oo op oq np b">tensor.backward()</code>自动计算所有的梯度，而不是使用<code class="fe oo op oq np b">grad(outputs=f, inputs = [x1, x2, x3, x4, z1, z2])</code>指定所有的输入来计算梯度。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="03f0" class="nt mx jf np b gy nu nv l nw nx">x1 = torch.tensor(2, dtype = torch.float32, requires_grad = True)<br/>x2 = torch.tensor(3, dtype = torch.float32, requires_grad = True)<br/>x3 = torch.tensor(1, dtype = torch.float32, requires_grad = True)<br/>x4 = torch.tensor(4, dtype = torch.float32, requires_grad = True)</span><span id="1d8c" class="nt mx jf np b gy on nv l nw nx">z1 = x1 * x2 <br/>z2 = x3 * x4<br/>f = z1 + z2</span><span id="0014" class="nt mx jf np b gy on nv l nw nx">f.backward()</span><span id="228d" class="nt mx jf np b gy on nv l nw nx">print(f"Gradient of x1 = {x1.grad}")<br/>print(f"Gradient of x2 = {x2.grad}")<br/>print(f"Gradient of x3 = {x3.grad}")<br/>print(f"Gradient of x4 = {x4.grad}")<br/>print(f"Gradient of z1 = {z1.grad}")<br/>print(f"Gradient of z2 = {z2.grad}")</span><span id="2e6b" class="nt mx jf np b gy on nv l nw nx"><br/>################## OUTPUT ##################</span><span id="aac4" class="nt mx jf np b gy on nv l nw nx">Gradient of x1 = 3.0<br/>Gradient of x2 = 2.0<br/>Gradient of x3 = 4.0<br/>Gradient of x4 = 1.0<br/>Gradient of z1 = None<br/>Gradient of z2 = None</span></pre><p id="9616" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们可以使用<code class="fe oo op oq np b">x.grad.zero_</code>使渐变为0。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="b7f6" class="nt mx jf np b gy nu nv l nw nx">print(x1.grad.zero_())</span><span id="fcb5" class="nt mx jf np b gy on nv l nw nx">################## OUTPUT ##################</span><span id="1b67" class="nt mx jf np b gy on nv l nw nx">tensor(0.)</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="6deb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)<em class="lz"> </em>你可以在<a class="ae or" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae or" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a> <em class="lz">找到我。</em></p><p id="f0f0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你也可以在这里查看我的其他博客文章。</p><figure class="mb mc md me gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi ox"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>