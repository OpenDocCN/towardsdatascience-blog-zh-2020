<html>
<head>
<title>Introduction to Nash Equilibria: Friend or Foe Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纳什均衡介绍:朋友还是敌人Q学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e?source=collection_archive---------28-----------------------#2020-04-21">https://towardsdatascience.com/multi-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e?source=collection_archive---------28-----------------------#2020-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="af2e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让机器人打破平衡</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/43073a57f307f7ed09c438920d9ba524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GJiIQLvzsEZr_o33"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@heftiba?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Toa Heftiba </a>拍摄的照片</p></figure><p id="28f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不管出于什么原因，人类天生拥有合作的能力。它变得如此平常，以至于它的细微差别就在我们眼皮底下溜走了。我们如何<em class="lv">知道</em>在移动一个沉重的沙发时如何协调？我们如何<em class="lv">推理</em>在杂货店分头行动以尽量减少时间？我们如何能够<em class="lv">观察</em>他人的行动并且<em class="lv">理解</em>如何最好地<em class="lv">回应</em>？</p><p id="17f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个解释:我们达到了平衡。一个<strong class="lb iu">均衡。每个人采取的行动不仅能最好地补充他人的行动，而且能最有效地完成手头的任务。这种均衡的应用在博弈论中经常出现，并扩展到多代理RL (MARL)。在本文中，我们探索了两种算法，Nash Q-Learning和Friend or Foe Q-Learning，这两种算法都试图找到满足这种“平衡”思想的多主体策略。我们假设单代理公式和Q学习的基本知识。要了解更多关于MARL的背景知识和与之相关的基本原则，请查看我以前的文章:</strong></p><div class="lw lx gp gr ly lz"><a href="https://medium.com/swlh/the-gist-multi-agent-reinforcement-learning-767b367b395f" rel="noopener follow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">多主体强化学习:要点</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">好像一个机器人学习一切还不够</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">medium.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/dd8f76fab4b84c4467861a10eaeed838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IeBIWoccN_96G3fW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·麦克林在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="08f1" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">什么使得一个最优的政策…最优？</h1><p id="fbfc" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">多主体学习环境通常用随机博弈来表示。每个代理人的目标是找到一个政策，最大化他们自己的预期折扣奖励。总的目标是找到一个<strong class="lb iu">联合</strong>的政策，为<strong class="lb iu">每个代理</strong>聚集最多的报酬。这种联合奖励以价值函数的形式定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/05918aa25e08f43b55b22335ae721df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*SfDijj7Ws0IU1F8HF43lIA.png"/></div></figure><p id="db9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个目标适用于<em class="lv">竞争</em>和<em class="lv">协作</em>两种情况。代理可以找到最好的<em class="lv">反</em>或<em class="lv">补充</em>其他策略。我们称这个最优策略为<strong class="lb iu">纳什均衡。</strong>更正式地说，它是一种具有以下属性的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/cf856e267eb5ce581aef77e8dd25e239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39oI_qWgN50fOjEiJRNH2g.png"/></div></div></figure><p id="7397" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">起初，我们似乎是在白费力气。最好的政策聚集了最多的回报，那又怎样？</p><p id="f5f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有花哨的希腊字母和符号之下，纳什均衡告诉我们更多。它认为纳什均衡中每个代理人的策略是对其他代理人最优策略的最佳回应。没有代理人会因为任何调整带来的回报减少而改变他们的政策。换句话说，所有的代理都处于静止状态。内陆。在某种意义上被困住了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/fc30d9c4c41a0868de6b22bb1ad79be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RpUWH6TTs_ChpCBn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NeONBRAND </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="db29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，想象两个小机器人:C3PO和Wall-E进行一场<em class="lv">竞技</em>游戏，在每一轮中，他们各自选择一个一到十的数字，谁选择的数字高谁就赢。不出所料，两个机器人每次都选择数字10，因为两个机器人都不想冒输的风险。如果C3PO选择任何其他数字，他可能会输给Wall-E的最优策略，即总是选择10，反之亦然。换句话说，两者处于均衡状态。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="aa30" class="mp mq it bd mr ms nw mu mv mw nx my mz jz ny ka nb kc nz kd nd kf oa kg nf ng bi translated">纳什Q学习</h1><p id="f784" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">因此，我们定义了一个术语叫做<strong class="lb iu">纳什Q值:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/5b0d708781f6c5c71640536a24ed2a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-f59TnUKkn8DzrgNTfbZw.png"/></div></div></figure><p id="a7c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与它的单代理对应物非常相似，纳什Q值表示当选择特定的联合行动后，<em class="lv">所有代理遵循联合纳什均衡政策</em>时，代理的预期未来累积报酬。这可以被视为一个国家行动对的“最佳情况”的奖励收益。这个定义有助于Q学习的多代理版本:Nash Q学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/f60c8b05250fba26c8e1616db749d481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4WN0RrioA0QfjFJF"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔恩·弗洛布兰特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c2e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在单代理Q学习中，我们使用时间差(TD)更新Q值，公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/dbe5adb6e700215e28fb3b19d3f853de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8q0aHUi1pwIOxBewTWIkOA.png"/></div></div></figure><p id="3023" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，gamma表示折扣系数，alpha表示学习率。下面是有点无聊的部分:Nash Q-learning简化为Q-learning。更明确地说，纳什Q学习的更新方程是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/e06ac432e5e431a8652130b245d93d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehcOdYNseRUaXWof0zQxmg.png"/></div></div></figure><p id="b3b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过在行动空间内最大化来计算纳什Q值。交换几个术语，你会得到一个几乎相同的更新方程，唯一的区别是一个联合行动空间。那么，这一切的目的是什么？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/204848f0801eb57f93d5c8ccc4b4653d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*93wyIu0vls1oiSJJ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@rihok?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Riho Kroll </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="27f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要点是:我们不能总是假设单代理逻辑适用于多代理设置。换句话说，适用于MDPs(单代理RL)的算法并不总是适用于随机博弈(多代理RL)。纳什Q-Learning恰好是一个有点特殊的案例。胡和Wellman [1]在论文中证明了纳什Q学习总是收敛的。它的更新方程看起来与Q-learning相同的事实是直观的，但并不总是可以预先假定的。</p><p id="be51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个纳什Q学习算法类似于单代理Q学习，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b2f0d45cb0166644e02c07cde21bac0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TWv_OmtIVMuzTR8yQbPsQ.png"/></div></div></figure></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="ec78" class="mp mq it bd mr ms nw mu mv mw nx my mz jz ny ka nb kc nz kd nd kf oa kg nf ng bi translated">朋友或敌人Q-学习</h1><p id="153d" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">q值有一个自然的解释。他们代表了一个国家行动对的预期累积贴现回报，但这如何激励我们的更新方程？我们再来看一下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/e06ac432e5e431a8652130b245d93d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehcOdYNseRUaXWof0zQxmg.png"/></div></div></figure><p id="2f1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个加权和，其中alpha是学习率，是区间(0，1)中的一个值。第一项是前一时间步的Q值乘以某个分数。让我们剖析一下第二项，我们的学习率乘以其他两项之和。</p><p id="ca3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二项的第一部分是我们在执行一个行动后得到的奖励，而另一部分是下一个状态行动的贴现纳什Q值最大化。记住，这个纳什Q值等于给定状态<em class="lv">后的预期、最佳情况、累积回报。</em>把这两部分放在一起，我们有一个<strong class="lb iu">预期回报总和</strong>，其中我们假设在下一个状态<strong class="lb iu">的<em class="lv">最佳情况</em>。</strong>这是对当前Q值的另一种定义，只是一个更新的版本！更正式地说，这个总和代表实际Q值的更接近的估计值。缩小并查看整个更新方程，我们可以将其解释为“推动”当前Q值更接近这个更好的估计值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/23296fbbeeb286360b942568ebc93b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cyoB8-_jLptScGJV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@hudsonhintze?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">哈德逊·辛慈</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="8c81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在《非友即敌Q-Learning 》( FFQ)中，Littman [2]使用这种解释为多智能体系统创建了一种直观的算法。与纳什Q学习相反，FFQ首先让每个代理保持他们自己的Q函数。我们假设代理知道其他人的行动空间，但不知道其他人在任何给定时刻会选择什么行动。</p><p id="5d2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，Littman将系统概括为合作者和对手的组合，因此命名为朋友或敌人。我们再以C3PO(特工1)和Wall-E(特工2)为例，站在C3PO的角度看待事情。如果两者一起工作，C3PO的Nash-Q值将替换为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/690c111813bcb088eb0f2254d3abc277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*y_k-yDuz9c8bAZ-W4ibGtA.png"/></div></figure><p id="fa9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如预期的那样，两个代理都试图最大化他们的“奖励流”,类似于之前的Q更新功能。然而，如果两个机器人相互竞争，C3PO的Nash-Q将被替换为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d0e855c899e2b5baa87a8e87c910f1ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axe8r9g29sSvwVfWXFYDxQ.png"/></div></div></figure><p id="f538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在最大化C3PO的动作之前，我们如何首先最小化over Wall-E的动作。换句话说，当Wall-E试图最小化C3PO的回报流时，C3PO学习如何对Wall-E的反作用作出最佳反应，相应地更新他的Q值。这是一个纳什均衡显示其色彩的例子:代理人学习最佳的方式来互补。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/9de35f44140b714b57da8c9ec895a407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EmiX2GPMG4dWkYAY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@ptrikutam?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕万·特里库塔姆</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更一般地，这可以适用于更大的代理系统。纳什-Q值变成如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9568b81f40e71f9d09e19f8d35bed999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*qt9v_giRfFjN58vZ8sk2SQ.png"/></div></figure><p id="6289" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中X代表所有合作者的集合，而Y代表所有对手的集合。与Nash Q-learning类似，Littman[2]表明朋友或敌人Q-learning也会收敛。通过适当替换这些纳什Q值，一个<em class="lv">分布式</em>代理系统可以学习一个达到均衡的策略，如果存在的话。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="d805" class="mp mq it bd mr ms nw mu mv mw nx my mz jz ny ka nb kc nz kd nd kf oa kg nf ng bi translated">第一步</h1><p id="3959" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">一些人认为这些算法是进入多智能体学习场景的“第一步”。虽然这些想法相对简单，但结果却是强有力的。下一次，我们将研究更新颖的现代方法来解决复杂的多代理环境！要更深入地了解纳什Q学习和朋友或敌人Q学习，请随意看看下面列出的两篇论文。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h2 id="6e77" class="om mq it bd mr on oo dn mv op oq dp mz li or os nb lm ot ou nd lq ov ow nf ox bi translated">参考</h2><p id="8025" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">[1]胡军，M. Wellman，<a class="ae ky" href="http://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf" rel="noopener ugc nofollow" target="_blank">一般和随机博弈的Nash Q-学习</a> (2003)，机器学习研究杂志4</p><p id="35b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] M. Littman，<a class="ae ky" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.589.8571&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">普通和博弈中的敌友Q学习</a> (2001)，ICML</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><blockquote class="oy oz pa"><p id="54d1" class="kz la lv lb b lc ld ju le lf lg jx lh pb lj lk ll pc ln lo lp pd lr ls lt lu im bi translated">从经典到最新，这里有讨论多代理和单代理强化学习的相关文章:</p></blockquote><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/stigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">解开Stigmergic独立强化学习及其重要性</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">因为1+1=3</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="pe l mk ml mm mi mn ks lz"/></div></div></a></div><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">分层强化学习:封建网络</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">让电脑看到更大的画面</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="pf l mk ml mm mi mn ks lz"/></div></div></a></div></div></div>    
</body>
</html>