<html>
<head>
<title>Day 126 of #NLP365: NLP Papers Summary — Neural News Recommendation with Topic-Aware News Representation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第126天:NLP论文摘要-具有话题感知新闻表示的神经新闻推荐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-126-of-nlp365-nlp-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb9604330bb?source=collection_archive---------66-----------------------#2020-05-05">https://towardsdatascience.com/day-126-of-nlp365-nlp-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb9604330bb?source=collection_archive---------66-----------------------#2020-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="d0cc" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="148b" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="39ba" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的273天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="885d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz">带话题感知新闻表示的神经新闻推荐</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="dd97" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="71b4" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了TANR，一个具有话题感知新闻嵌入的神经新闻推荐系统。这包括一个主题感知新闻编码器和一个用户编码器。新闻编码器使用CNN网络和注意力机制来选择使用新闻标题的重要单词。我们联合训练新闻编码器和辅助主题分类任务。对于用户编码器，我们通过用户阅读过的历史新闻来学习表征，并使用注意机制为用户选择信息丰富的新闻。实验结果表明，该方法提高了新闻推荐的性能。</p><h1 id="5cde" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">方法学</h1><h2 id="1718" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">模型架构</h2><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1892afc3bc06a7b3eefef454175b1e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*l8RP5mbglr4Pyo4p.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">整体模型架构[1]</p></figure><p id="bfb5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">模型架构由三个主要模块组成:</p><ol class=""><li id="c3d7" class="nz oa jf lf b lg lh lj lk lm ob lq oc lu od ly oe of og oh bi translated">新闻编码器</li><li id="c350" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated">用户编码器</li><li id="015b" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated">点击预测器</li></ol><p id="f91d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">新闻编码器的目标是从标题中学习新闻表示。有三层。第一层是单词嵌入层，它将标题的单词转换成单词嵌入。第二层是CNN层，它接收单词嵌入，并通过捕获本地上下文信息输出上下文单词嵌入。最后一层是关注层，让模型关注标题中更重要的词。这一层生成最终的新闻表示，它是所有上下文单词嵌入的加权和。</p><p id="b79d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">用户编码器的目标是从历史浏览新闻中学习用户的表现。这个想法是，历史浏览新闻允许我们捕捉关于特定用户的不同信息/偏好。我们使用新闻编码器对所有浏览过的历史新闻进行编码，获得新闻表示。用户编码器接收这些新闻表示，并对其应用关注机制，以选择给我们提供关于用户的更好信息的关键新闻。最终的用户表征是所有用户历史浏览新闻表征的加权和。</p><p id="b00e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">点击预测器的目标是预测用户点击候选新闻的概率。点击预测器采用候选新闻表示和用户表示，并通过采用两个表示之间的内积来计算点击概率分数。</p><h2 id="f090" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">话题感知新闻编码器</h2><p id="a4f8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">新闻文章的主题对于新闻推荐很重要，因此包含主题信息将改善新闻和用户的表现。然而，我们有有限的主题信息，所以我们决定联合训练我们的新闻编码器与新闻主题分类模型，如下所示。这给了我们一个话题感知新闻编码器。新闻主题分类模型由新闻编码器和主题预测器模块组成。新闻编码器与新闻推荐模型共享，主题预测器模块用于根据新闻表示预测主题分布(使用softmax)。使用共享的新闻编码器，新闻编码器将对主题信息进行编码，并由新闻推荐模型使用。联合训练新闻推荐和主题分类任务意味着我们有两个损失要优化。总损失是这两个损失的总和。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1345ba0ea6dd2eebac8886ee8708bc26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/0*j12Byv1J0Y5LpYqM.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">话题感知新闻编码器的总体框架[1]</p></figure><h1 id="3873" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验设置和结果</h1><p id="8172" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">真实世界的数据集是一个月的MSN新闻。数据集和主题分布的统计如下所示。评估指标为AUC、MRR、nDCG@5和nDCG@10。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9c70c0fa9983a7be39425d1f04352f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/0*A7L5ryeKQ-z2Z4ak.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">我们数据集的描述性统计[1]</p></figure><h2 id="c86e" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">模型比较</h2><ol class=""><li id="eda0" class="nz oa jf lf b lg mz lj na lm op lq oq lu or ly oe of og oh bi translated"><em class="lz"> LibFM </em>。用于推荐的矩阵分解技术</li><li id="9e44" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz"> DSSM </em>。使用历史浏览新闻作为查询来检索候选新闻</li><li id="a806" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz">宽&amp;深</em>。宽线性通道+深度神经网络</li><li id="3098" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz"> DeepFM </em>。使用不同的分解机器和神经网络</li><li id="e824" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz"> DFM </em>。组合不同等级密集层并使用注意机制</li><li id="3fbc" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz"> DKN </em>。使用知识图中的实体信息</li><li id="8739" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly oe of og oh bi translated"><em class="lz">TANR-基本</em>。没有话题感知新闻编码器的TANR</li></ol><h1 id="7bac" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">结果</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/671f6513882e5d63f85a37642e052ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/0*KFDZNfOYBcuuThCF.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">使用不同评估指标的总体结果[1]</p></figure><p id="ce64" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">神经网络模型比传统的矩阵分解技术更好，因为神经网络可以学习更好的新闻和用户表示。TANR基础和TANR都超过了所有的基线模型。TANR的表现一直优于TANR-basic，展示了整合新闻主题进行新闻推荐的好处，以及我们联合训练模型的策略的有效性。</p><p id="7089" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">就我们的主题分类器的性能而言，F1结果如下所示。不同主题的分类都很好，除了“孩子”类。这可能是因为“孩子”类的训练数据有限。总的来说，结果表明我们的新闻编码器已经编码了主题信息，这改进了我们的新闻推荐模型的结果。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/b27dc1487bf51c76900761556efa1a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*NR42IvUn832jU2EH.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">话题分析和不同注意网络的有效性[1]</p></figure><p id="c9ce" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在图6中，我们展示了使用不同注意力网络的结果。结果表明，新闻级和单词级注意都是有用的，因为它们都优于无注意网络的基线。这又回到了一个假设，即不同的新闻包含关于用户的不同信息，不同的词在表示新闻时具有不同的重要性，我们的注意力网络允许我们挑选最有信息量的新闻和重要的词。结合这两种注意力网络会产生更高的结果。</p><p id="5395" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，我们研究了超参数λ的影响。该超参数控制主题分类任务的相对重要性，因为它决定了模型在多大程度上关注主题分类损失函数的优化。下面显示的结果告诉我们，如果lambda太低，我们的模型的性能不是最佳的，因为新闻编码器没有学习到足够的主题信息。如果lambda太高，模型会过于关注主题分类任务，而忽略了新闻推荐任务。最佳λ似乎是0.2。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/894c7091975818b6854a99bf1c8f4cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*l60i-Z9sluwQPGJf.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">λ超参数的影响[1]</p></figure><h2 id="9305" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">来源:</h2><p id="26a1" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]吴，c，吴，f，安，m，黄，y，谢，x，2019，7月。具有话题感知新闻表示的神经新闻推荐。在<em class="lz">计算语言学协会第57届年会的会议录</em>(第1154-1159页)。</p><p id="1139" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年5月5日</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/05/day-126-nlp-papers-summary-neural-news-recommendation-with-topic-aware-news-representation/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="bec0" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="054f" class="nz oa jf lf b lg mz lj na lm op lq oq lu or ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="7f31" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sensation-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="b83f" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="bb2f" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="b5fc" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towardsdatascience . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="22d8" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="9532" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="a5a6" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towards data science . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="0d19" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="f976" class="nz oa jf lf b lg mz lj na lm op lq oq lu or ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="5ebf" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="6ace" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="d031" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="fdfb" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61 AE 6 CDF 32 f</a></li><li id="3e04" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="c608" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="8c9e" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-cd55e 577 F6 de</a></li><li id="e11f" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3">https://towards data science . com/day-124-NLP-papers-summary-tldr-extreme-summary-of-scientific-documents-106 CD 915 F9 a 3</a></li></ul><h1 id="c35f" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="0b96" class="nz oa jf lf b lg mz lj na lm op lq oq lu or ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="d322" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="f448" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="4bbe" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="ab12" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li><li id="f851" class="nz oa jf lf b lg oi lj oj lm ok lq ol lu om ly ou of og oh bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-125-of-nlp365-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305c3aebe2">https://towards data science . com/day-125-of-NLP 365-NLP-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305 C3 aebe 2</a></li></ul></div></div>    
</body>
</html>