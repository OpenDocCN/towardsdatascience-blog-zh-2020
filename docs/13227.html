<html>
<head>
<title>Hyperparameter tuning with Keras and Ray Tune</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 和光线调节进行超参数调节</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda?source=collection_archive---------10-----------------------#2020-09-11">https://towardsdatascience.com/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda?source=collection_archive---------10-----------------------#2020-09-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a27b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 HyperOpt 的贝叶斯优化和超带调度器为机器学习模型选择最佳超参数</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/05ad14895da04b25fef9583f78fee5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FWpS7wF_MFfXEXJu"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Alexis Baydoun 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8ee1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我之前的<a class="ae kv" rel="noopener" target="_blank" href="/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b">文章</a>中，我解释了如何构建一个小巧灵活的图像分类器，以及在卷积神经网络中拥有可变输入维度的优势。然而，在经历了模型构建代码和训练例程之后，人们可以问这样的问题:</p><ol class=""><li id="4823" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如何选择一个神经网络的层数？</li><li id="e42b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何选择各层单元/滤波器的最优数量？</li><li id="781c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我的数据集的最佳数据扩充策略是什么？</li><li id="8f41" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">什么样的批量和学习率是合适的？</li></ol><p id="486c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建立或训练神经网络包括找出上述问题的答案。例如，您可能对 CNN 有一种直觉，随着我们越来越深入，每一层中的过滤器数量应该增加，因为神经网络学习提取越来越复杂的特征，这些特征建立在早期层中提取的简单特征的基础上。但是，可能有一个更优的模型(对于您的数据集而言)具有更少的参数，其性能可能优于您根据直觉设计的模型。</p><p id="3028" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将解释这些参数是什么，以及它们如何影响机器学习模型的训练。我将解释机器学习工程师如何选择这些参数，以及我们如何使用一个简单的数学概念来自动化这个过程。我将从我以前的<a class="ae kv" rel="noopener" target="_blank" href="/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b">文章</a>中的相同模型架构开始，并对其进行修改，以使大多数训练和架构参数可调。</p><h1 id="62f8" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">什么是超参数？</h1><p id="b65b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">超参数是机器学习工程师在训练模型之前设置的训练参数。<strong class="ky ir">在训练过程中，机器学习模型不会学习这些参数</strong>。例子包括批量大小、学习速率、层数和相应的单元等。机器学习模型在训练过程中从数据中学习的参数称为模型参数。</p><p id="e3ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">为什么超参数很重要？</strong></p><p id="fbec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当训练机器学习模型时，主要目标是获得在验证集上具有最佳性能的最佳性能模型。我们关注验证集，因为它代表了模型的泛化能力(在看不见的数据上的性能)。<strong class="ky ir">超参数构成了训练过程</strong>的前提。例如，如果<em class="nd">学习速率</em>设置得太高，那么模型可能永远不会收敛到最小值，因为它在每次迭代后将采取太大的步骤。另一方面，如果<em class="nd">学习率</em>设置得太低，模型将需要很长时间才能达到最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/5846ad58b25323414b280cafd309a1ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_aNcUbA17zbd5QfDYONZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">超参数调整前后的机器学习管道</p></figure><p id="f072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">为什么超参数很难选择？</strong></p><p id="589c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">找到正确的<em class="nd">学习率</em>包括选择一个值，训练一个模型，评估它，然后再次尝试。每个数据集都是独一无二的，有这么多参数可供选择，初学者很容易感到困惑。经历了多次失败训练尝试的机器学习工程师最终会对超参数如何影响给定的训练过程产生直觉。然而，这种直觉并不能推广到所有的数据集，一个新的用例通常需要一些实验才能确定令人信服的超参数。然而，有可能错过最佳或最优参数。</p><p id="1d6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们希望选择超参数，以便在训练过程完成后，我们有一个既精确又通用的模型</strong>。当处理神经网络时，评估目标函数可能非常昂贵，因为训练需要很长时间，并且手动尝试不同的超参数可能需要几天。这变成了手工完成的困难任务。</p><h1 id="1b45" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">超参数调整/优化</h1><p id="920a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir">超参数调整可被视为一个黑盒优化问题，我们试图在不知道其解析形式的情况下找到函数 f(x)的最小值</strong>。它也被称为无导数优化，因为我们不知道它的解析形式，也不能计算导数来最小化 f(x)，因此像梯度下降这样的技术不能使用。</p><p id="2a42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些著名的超参数调整技术包括网格搜索、随机搜索、差分进化和贝叶斯优化。网格搜索和随机搜索的性能略好于手动调整，因为我们建立了一个超参数网格，并对分别从网格中系统或随机选择的参数运行训练和评估周期。</p><p id="c802" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，网格和随机搜索相对低效，因为它们不基于先前的结果选择下一组超参数。另一方面，<strong class="ky ir">差分进化是一种进化算法，其中最佳执行超参数配置的初始集合(其是随机初始化的个体之一)被选择来产生更多的超参数</strong>。新一代的超参数(后代)更有可能表现得更好，因为他们继承了父母的良好特征，并且群体随着时间的推移而改善(一代又一代)。在这个美丽而实用的教程<a class="ae kv" href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/" rel="noopener ugc nofollow" target="_blank">中阅读更多关于这个概念的内容。</a></p><p id="9988" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管差异进化起作用，但它需要很长时间，并且仍然没有采取明智的步骤，或者它不知道我们试图实现/优化什么。<strong class="ky ir">贝叶斯优化方法跟踪过去的评估结果，并使用它来创建待优化的实际训练目标函数的概率模型</strong>。这个概率模型被称为目标函数的“<em class="nd">替代物“</em>”，其形成了超参数到目标函数表现如何的概率分数的映射。要评估的下一组超参数是基于它们在代理上的表现来选择的。这使得贝叶斯优化有效，因为它以<em class="nd">知情的方式</em>选择下一组超参数。在这篇详细的文章<a class="ae kv" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">中阅读更多关于这个概念的内容。这篇文章解释了 Tree Parzen Estimators (TPE)代理模型，它将在我们下面的实现中内部使用。</a></p><h1 id="fa53" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">去拿圣经</h1><p id="4672" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">一如既往，你可以在<a class="ae kv" href="https://github.com/himanshurawlani/hyper_fcn" rel="noopener ugc nofollow" target="_blank">这个 GitHub 链接</a>中获得本教程使用的所有代码。我建议读者克隆这个项目，并按照教程一步一步来更好地理解。<strong class="ky ir">注意</strong>:本文中的代码片段只突出了实际脚本的一部分，完整代码请参考 GitHub 链接。</p><div class="nf ng gp gr nh ni"><a href="https://github.com/himanshurawlani/hyper_fcn" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">himanshurawlani/hyper_fcn</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">这个项目使用 HyperOpt 的贝叶斯优化和光线调整来执行简单图像的超参数调整…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div><h1 id="a56d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">什么是雷调？</h1><p id="cfcf" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><a class="ae kv" href="https://ray.readthedocs.io/en/latest/tune.html" rel="noopener ugc nofollow" target="_blank"> Ray Tune </a>是一个 Python 库，通过允许您大规模利用尖端优化算法来加速超参数调整。它建立在<a class="ae kv" href="https://github.com/ray-project/ray" rel="noopener ugc nofollow" target="_blank">射线</a>之上，旨在消除缩放和设置实验执行过程中的摩擦。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/1d7305bd74c42f82e92fb41e15303f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wHaEb0ajg0MBHfsC"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf" rel="noopener">来源</a></p></figure><p id="354f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Ray Tune 与 MLFlow、TensorBoard、weights and biases 等实验管理工具无缝集成。并为<a class="ae kv" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> HyperOpt </a>(以下实现)<a class="ae kv" href="http://ax.dev/" rel="noopener ugc nofollow" target="_blank"> Ax </a>等众多前沿优化算法和库提供了灵活的接口。</p><h1 id="0f37" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">建立一个超级模型</h1><p id="c9ec" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">超模型是一种模型，其超参数可以使用优化算法进行优化，以便在某个指标上提供最佳性能(在这种情况下为验证损失)。这些超参数包括层数、每层中的单元数、要使用的层的类型、激活函数的类型等。让我们创建一个简单的超级模型来执行图像分类任务。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="f855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的模型构建代码中，我们传递了一个<code class="fe oa ob oc od b">config</code>字典，其中包含过滤器数量、辍学率、是否使用特定的数据扩充层等值。每次运行超参数调整都会创建一个新的配置字典。对应于最佳运行的<code class="fe oa ob oc od b">config</code>将被选为最佳配置，包括数据扩充、模型和训练程序的最佳参数。下面的超参数搜索空间部分解释了一个<code class="fe oa ob oc od b">config</code>的例子。</p><h1 id="7c07" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">选择最佳数据扩充</h1><p id="8e73" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">选择数据扩充对模型被训练的应用的性质非常敏感。人脸识别系统可能会遇到不同亮度、方向、部分裁剪等的人脸。然而，用于从系统生成的 PDF 中提取文本的基于 OCR 的文本提取系统肯定会遇到方向和亮度变化非常小的文本。</p><p id="29ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们有庞大的数据集，可能无法检查每一张图像来决定要使用的数据扩充。我们可以将这个任务作为超参数之一留给我们的优化算法。<strong class="ky ir">在 TensorFlow 2 中，使用 Keras 预处理层作为模型代码的一部分来添加数据扩充变得比以往任何时候都容易</strong>。这些预处理层仅在训练模式下是活动的，并且在推断或评估期间是禁用的。点击了解更多信息<a class="ae kv" href="https://keras.io/guides/preprocessing_layers/" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="dd9a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">定义超参数搜索空间</h1><p id="91b6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">为了定义超参数搜索空间，我们首先需要了解哪些可能的有效配置可以用来创建我们的模型。让我们考虑下面一个有效的<code class="fe oa ob oc od b">config</code>字典:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="f7b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们考虑<code class="fe oa ob oc od b">batch_size</code>，那么我们可以选择 1 到 100 之间的任意值，甚至更高。然而，最常见的批量大小是 2 的幂，介于 8 和 64 之间。因此，我们可以将搜索空间定义为 1 到 100 之间的任意整数值，或者我们可以通过提供一个最常见值的列表来减轻优化算法的负担，比如<code class="fe oa ob oc od b">[8, 16, 32, 64]</code>。同样，如果考虑学习率(<code class="fe oa ob oc od b">lr</code>)，可以选择 0.0001 到 0.1 之间的任意浮点值。我们可以更低或更高，但这通常是不必要的。我们可以指定最常见的值，这些值通常是 10 的幂，就像<code class="fe oa ob oc od b">[0.1, 0.01, 0.001, 0.0001]</code>一样，而不是去寻找 0.0001 到 0.1 的穷尽搜索空间。</p><p id="055e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在 HyperOpt 中，搜索空间由嵌套的函数表达式组成，包括随机表达式</strong>。随机表达式是超参数，优化算法通过用自适应探索策略替换正常的“采样”逻辑来对其进行工作。点击了解更多信息<a class="ae kv" href="https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space" rel="noopener ugc nofollow" target="_blank">。我们可以定义一个随机表达式，它由一列<code class="fe oa ob oc od b">batch_size</code>值组成，称为<code class="fe oa ob oc od b">hp.choice(‘batch_size’, [8, 16, 32, 64])</code>。同样，对于学习率，我们可以定义一个表达式为<code class="fe oa ob oc od b">hp.choice(‘lr’, [0.0001, 0.001, 0.01, 0.1])</code>。如果您想定义一个由双边区间约束的连续空间，我们可以将表达式修改为<code class="fe oa ob oc od b">hp.uniform(‘lr’, 0.0001, 0.1)</code>。这里</a>可以参考参数表达式<a class="ae kv" href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions" rel="noopener ugc nofollow" target="_blank">的完整列表。我们最终的超参数搜索空间将如下所示:</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="3e45" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">指定试验调度程序和搜索算法</h1><p id="155f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir">搜索算法是一种“<em class="nd">优化算法”</em>，它通过在每次后续试验中建议更好的超参数来优化训练过程的超参数。</strong> Tune 的搜索算法是围绕开源优化库的包装器，用于高效的超参数选择。每个库都有特定的方式定义搜索空间，就像上面的搜索空间是为<a class="ae kv" href="https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#hyperopt-tune-suggest-hyperopt-hyperoptsearch" rel="noopener ugc nofollow" target="_blank"> HyperOpt </a>定义的。要使用这个搜索算法，我们需要使用<code class="fe oa ob oc od b">pip install -U hyperopt</code>单独安装它。</p><p id="74e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">试验调度程序也是一种优化算法，作为“<em class="nd">调度算法”</em>实现，使超参数调整过程更加有效</strong>。试验调度程序可以提前终止不良试验、暂停试验、克隆试验，并更改正在运行的试验的超参数，从而加快超参数调整过程。<strong class="ky ir">注意</strong>:与搜索算法不同，试验调度程序不会为每次运行选择要评估的超参数配置。我们将使用<a class="ae kv" href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler" rel="noopener ugc nofollow" target="_blank">asynccessivehalvingalgorithm</a>(ASHA)调度器，它提供了与<a class="ae kv" href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#hyperband-tune-schedulers-hyperbandscheduler" rel="noopener ugc nofollow" target="_blank"> HyperBand </a> (SHA)类似的理论性能，但提供了更好的并行性，并避免了消除期间的掉队问题。我们不需要单独安装 AsyncSuccessiveHalvingAlgorithm 调度程序。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/bf2d3d9c350b02fbfdfc4c9e34e30ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GRLVG3moDewZ6d_STSS7tg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过随机采样 n 个配置开始连续减半。在每次迭代中，它丢弃最差的一半，并将剩余部分的资源加倍，直到达到最大资源。每条线对应一个配置，每个梯级对应连续的一半。(<a class="ae kv" href="https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="143c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有的试验调度程序和搜索算法都接受根据<code class="fe oa ob oc od b">mode</code>最大化或最小化的<code class="fe oa ob oc od b">metric</code>。试验调度程序也接收<code class="fe oa ob oc od b">grace_period</code>，类似于 Keras 中<em class="nd">提前停止</em>回调中使用的<code class="fe oa ob oc od b">patience</code>。对于搜索算法，我们可以提供一个初始配置(搜索空间),这通常是通过手动调整找到的最佳配置，或者如果没有，我们可以跳过它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="de74" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">定义超参数调整的目标</h1><p id="a8c7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">要开始超参数调整，我们需要指定一个目标函数进行优化，并将其传递给<code class="fe oa ob oc od b">tune.run()</code>。训练(更新模型参数)我们的图像分类器的损失函数将是分类交叉熵。训练和验证损失更准确地反映了我们的模型的表现。然而，在过度拟合期间，我们的训练损失将减少，但我们的验证损失将增加。因此，验证损失将向右度量，以监控超参数调整。</p><p id="df1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练图像分类器是一个漫长的过程，等待训练完成，然后报告验证损失度量不是一个好主意，因为我们的试验调度程序不知道训练进展如何，也不知道是否需要提前停止它。为了克服这个问题，<strong class="ky ir">我们将利用 Keras 回调，在每个时期结束时计算验证损失，因此我们可以使用</strong> <code class="fe oa ob oc od b"><strong class="ky ir">tune.report()</strong></code>将分数发送给 Tune。下面给出了 Keras 回调的最小代码，请参考 GitHub 上的完整<a class="ae kv" href="https://github.com/himanshurawlani/hyper_fcn/blob/master/callbacks.py#L8" rel="noopener ugc nofollow" target="_blank">代码。</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="1156" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 Ray Tune 中，我们可以使用基于<a class="ae kv" href="https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-function-api" rel="noopener ugc nofollow" target="_blank">函数的</a> API 或基于<a class="ae kv" href="https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-class-api" rel="noopener ugc nofollow" target="_blank">类的</a> API 来指定目标函数，在本教程中，我们将使用基于函数的 API。在超参数调整完成后，我们获得了最佳配置，用于训练我们的最终模型，该模型将保存在磁盘上。我们将把我们的目标函数封装在一个类中，以存储几个目录路径和一个布尔变量，该变量告诉我们给定的运行是否是最终的运行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="4f4a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">可视化结果</h1><p id="ba60" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">如果安装了 TensorBoard，Tune 会在<code class="fe oa ob oc od b">tune.run()</code>期间自动输出 Tensorboard 文件。运行实验后，您可以通过指定结果的输出目录来使用 TensorBoard 可视化您的结果:<code class="fe oa ob oc od b">$ tensorboard --logdir=~/ray_results/my_experiment</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/b605b87b88a2299e561bf67539430b36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sbddk0AEnbneFVknKoNMIQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">张量板标量视图</p></figure><p id="77d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 TF2，Tune 也自动生成<a class="ae kv" href="https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams" rel="noopener ugc nofollow" target="_blank"> TensorBoard HParams </a>输出，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/43ac5111b9869d1b4d10dfb3465d33b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sqs5Kbd540AoIw2ds8vpZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TensorBoard HParams 平行坐标视图</p></figure><h1 id="41c7" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">关于超参数调整的更多信息</h1><p id="5897" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在许多用例中，我们使用标准架构(如 ResNet50)而不是从头开始构建。这些架构非常庞大，执行超参数调整可能不切实际，或者您可能希望利用预先训练的 ImageNet 权重，因此改变模型架构不是一个选项。在这种情况下，我们可以在模型架构之外寻找超参数，例如数据扩充、批量大小、学习速率、优化器等。</p><p id="4ce2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测中的锚框</strong></p><p id="5e96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们考虑对象检测作为一个这样的用例，其中我们利用锚框进行边界框预测，这在训练过程中没有被学习到。每个对象检测数据集都有要检测的对象的唯一纵横比，默认锚点配置可能不适合检测数据集中的对象。例如，如果您的对象小于最小锚点的大小，或者您的对象具有较高的纵横比。在这种情况下，修改锚配置可能是合适的。这可以通过将锚参数设置为要调整的超参数来自动完成。</p><h1 id="b17c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="f45c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我希望这篇博文让你对机器学习模型训练中涉及的不同超参数有所了解。手动调整这些参数是乏味且不直观的，但在贝叶斯优化的帮助下，我们可以跟踪过去的评估结果，并使用它来创建实际训练目标函数的概率模型。这不仅自动化了调优过程，还产生了一个我们可能无法通过手动调优找到的最佳模型。</p><p id="a68f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将 HyperOpt 搜索算法与 HyperBand 试验调度程序相结合，可以显著减少我们的超参数调谐搜索时间和计算资源。此外，能够找到给定数据集的最佳数据扩充步骤只是锦上添花。在讨论本文中的各种主题时，我已经链接了一些很棒的资源，但我将在下面重新链接它们，以便您不会错过任何内容。继续学习！</p><h2 id="1913" class="oh mh iq bd mi oi oj dn mm ok ol dp mq lf om on ms lj oo op mu ln oq or mw os bi translated">参考资料和资源</h2><ol class=""><li id="d3da" class="ls lt iq ky b kz my lc mz lf ot lj ou ln ov lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">机器学习的贝叶斯超参数优化的概念解释</a></li><li id="8d3d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/" rel="noopener ugc nofollow" target="_blank">大规模超参数优化新手指南</a></li><li id="23da" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.automl.org/blog_bohb/" rel="noopener ugc nofollow" target="_blank"> BOHB:大规模稳健高效的超参数优化</a></li><li id="f5a5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization" rel="noopener ugc nofollow" target="_blank">使用贝叶斯优化的云机器学习引擎中超参数调整</a></li><li id="54f1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯优化初级读本</a></li><li id="2328" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/" rel="noopener ugc nofollow" target="_blank">Python 差分进化教程</a></li><li id="592b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020" rel="noopener ugc nofollow" target="_blank">Python 中超参数调优:2020 年完整指南</a></li></ol><p id="8cb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我很乐意听到你对这篇文章和<a class="ae kv" href="https://github.com/himanshurawlani/hyper_fcn" rel="noopener ugc nofollow" target="_blank"> GitHub 项目</a>的反馈和改进。你可以在推特(<a class="ae kv" href="https://twitter.com/raw_himanshu" rel="noopener ugc nofollow" target="_blank"> @raw_himanshu </a>)和 LinkedIn(<a class="ae kv" href="https://www.linkedin.com/in/himanshurawlani/" rel="noopener ugc nofollow" target="_blank">himanshurawlani</a>)上找到我</p></div></div>    
</body>
</html>