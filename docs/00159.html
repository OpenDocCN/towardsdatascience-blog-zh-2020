<html>
<head>
<title>Evaluating performance of an object detection model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估对象检测模型的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b?source=collection_archive---------0-----------------------#2020-01-06">https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b?source=collection_archive---------0-----------------------#2020-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5632" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是地图？如何评价一个物体检测模型的性能？</h2></div><p id="6ea9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将了解如何使用 mAP 来评估对象检测模型的性能。什么是地图？如何用 11 点插值计算 mAP？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/2d63826e19e6f15dff329e122d0e97fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkSR8uDsNuokljGe2lo_Mg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">对象检测和实例分割</p></figure><p id="66a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用机器学习和深度学习来解决回归或分类问题。</p><p id="eec4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用均方根(RMS)或平均百分比误差(MAPE)等。评估回归模型的性能。</p><p id="93f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用准确度、精确度、召回率或 F1 分数来评估分类模型。</p><p id="1f09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">是物体检测、分类还是回归问题？</em> </strong></p><p id="89e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">存在多种深度学习算法用于对象检测，如 RCNN:快速 RCNN、更快 RCNN、YOLO、掩模 RCNN 等。</p><p id="7bba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">目标物体检测模型是为了</strong></p><ul class=""><li id="59d5" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><strong class="kk iu">分类</strong>:识别图像中是否存在物体以及物体的类别</li><li id="9cb1" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">定位</strong>:当图像中存在物体时，预测物体周围边界框的坐标。这里我们比较地面真实和预测边界框的坐标</li></ul><p id="ca84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要评估在图像中使用包围盒的分类和定位的性能</p><p id="42e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我们如何衡量物体检测模型的性能？</em>T15】</strong></p><p id="9ec1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于对象检测，我们使用并集上的交集(IoU)的概念。IoU 计算两个边界框的并集上的交集；地面真实的边界框和预测的边界框</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b18223e9ae8dad0eba50a5ed160bbbe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*h0fLABTVPnIRgNrabuVVnw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">红色是真实边界框，绿色是预测边界框</p></figure><p id="0249" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">IoU 为 1 意味着预测边界框和实际边界框完全重叠。</strong></p><p id="a1de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以为 IoU 设置一个阈值，以确定对象检测是否有效。</p><p id="7e17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您将 IoU 设置为 0.5，在这种情况下</p><ul class=""><li id="54e5" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><strong class="kk iu">如果 IoU ≥0.5，</strong>将物体检测分类为<strong class="kk iu">真阳性(TP) </strong></li><li id="049c" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">如果 Iou &lt;为 0.5 </strong>，那么这是一次错误检测，将其归类为<strong class="kk iu">假阳性(FP) </strong></li><li id="5dfe" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">当图像中存在地面真实且模型未能检测到物体时，</strong>将其归类为<strong class="kk iu">假阴性(FN)。</strong></li><li id="0a91" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">真阴性(TN </strong> ): TN 是图像中我们没有预测到物体的每个部分。这个度量对于对象检测没有用，因此我们忽略 TN。</li></ul><p id="fa96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将 IoU 阈值设置为 0.5 或更大。可以设置为 0.5，0.75。0.9 或 0.95 等。</p><p id="ffcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用精确度和召回率作为评估性能的指标。使用真阳性(TP)、假阳性(FP)和假阴性(FN)来计算精确度和召回率。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e96151c525df0f47e6fdd3fa96d6f7fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*pt50d5q2KyrjA6BQhdNPHg.png"/></div></figure><p id="be0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算图像中所有对象的精度和召回率。</p><p id="f47d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还需要考虑图像中模型检测到的每个对象的置信度得分。考虑置信度得分高于某个阈值的所有预测边界框。高于阈值的边界框被认为是正边界框，低于阈值的所有预测边界框被认为是负边界框。</p><h2 id="4960" class="ml mm it bd mn mo mp dn mq mr ms dp mt kr mu mv mw kv mx my mz kz na nb nc nd bi translated"><strong class="ak">使用 11 点插值平均精度计算平均平均精度(mAP) </strong></h2><p id="0a10" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何使用 11 点插值计算地图？</em> </strong></p><p id="2dfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第一步:绘图精度和召回</strong></p><p id="299a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在精确度召回(PR)图上绘制精确度和召回值。PR 图是单调递减的，在精确度和召回率之间总是有一个权衡。增加一个会减少另一个。有时，由于某些例外和/或缺乏数据，PR 图并不总是单调递减的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/97199b501feb84e2075471b49b2d6589.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*7oGMy1fbcWw_jQttX0MNdw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:<a class="ae nj" href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html" rel="noopener ugc nofollow" target="_blank">https://NLP . Stanford . edu/IR-book/html/html edition/evaluation-of-ranked-retrieval-results-1 . html</a></p></figure><p id="48fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第二步:计算平均精度(mAP)，使用 11 点插值技术。</strong></p><p id="582b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上图所示，插值精度是在 0.0、0.1、0.2、0.3…0.9、1.0 等 11 个等距召回级别上测得的平均精度。</p><p id="7603" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PR 图有时可能不是单调递减的，为了解决这个问题，我们为 recall 值设置了最大精度。从图形上看，在每个召回级别，我们用该召回级别右侧的最大精度值替换每个精度值，即:我们取所有未来点的最大值</p><p id="2b02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本原理是，如果精度和召回率都变得更好，人们愿意考虑更高的精度值。</p><p id="212e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后计算测试集中每个信息在每个召回级别的插值精度的算术平均值。</p><p id="b923" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">mAP 总是在整个数据集上进行计算。</p><p id="c4be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们以如下所示的例子来理解，召回值被排序，以便我们绘制 PR 图</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2950b723b5faf6ef46eeddbab621f562.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*zK5kjPKTpixYh8uCXbytFw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">样本精度和召回值</p></figure><p id="c7ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">11 点插值将使用召回值精度的最高值。</p><p id="7877" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创造了 11 个等间隔的回忆水平，分别是 0.0，0.1，0.2，0.3…0.9，1.0。</p><p id="7ca5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">0.2 的召回具有最高精度值 1.00。0.4 的召回值具有不同的精度值 0.4、0.67、0.5。在这种情况下，我们使用最高精度值 0.67。当精度值为 0.6 时，我们的精度值为 0.5，但是对于 0.8 的召回，我们看到更高的精度值 0.57。基于 11 点插值的基本原理，我们取所有未来点的最大值，因此我们需要考虑的精度是 0.57，而不是 0.5。最后，对于 1.0 的召回，我们取最大精度 0.5。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4e6f2929701bdbfd9308eb25e1b3f908.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*XbniH9-QKmdEqOcHnWmTNQ.png"/></div></figure><p id="0f06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在绘制精度召回和插值精度。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nm"><img src="../Images/f6283a52aabfa343e92762d6655f3e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nRcIiyxBgyJKteWMr-pzLw.png"/></div></div></figure><p id="30cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们最后应用平均精度公式</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/cc2376d2b2a4b54ce43cf9482191bd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*Fq-d46tDerMSRJ8wFoKoBQ.png"/></div></figure><p id="ec5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AP = 1/11(4 * 1.0+2 * 0.67+4 * 0.57+1 * 0.5)= 0.74</p><p id="c1c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这给出了使用 11 点插值的平均精度</p><h2 id="a218" class="ml mm it bd mn mo mp dn mq mr ms dp mt kr mu mv mw kv mx my mz kz na nb nc nd bi translated">用于计算 Pascal VOC 数据格式地图的 Python 代码</h2><p id="2027" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated"><strong class="kk iu"> Pascal VOC 边界框由(<em class="le">x-左上，y-左上，x-右下，y-右下</em> ) </strong>定义</p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="9bd0" class="ml mm it np b gy nt nu l nv nw">#GT Boxes<br/>gt_boxes= {"img_00285.png": [[480, 457, 515, 529], [637, 435, 676, 536]]}</span><span id="0a62" class="ml mm it np b gy nx nu l nv nw">#Pred Boxes<br/>pred_boxs={"img_00285.png": {"boxes": [[330, 463, 387, 505], [356, 456, 391, 521], [420, 433, 451, 498], [328, 465, 403, 540], [480, 477, 508, 522], [357, 460, 417, 537], [344, 459, 389, 493], [485, 459, 503, 511], [336, 463, 362, 496], [468, 435, 520, 521], [357, 458, 382, 485], [649, 479, 670, 531], [484, 455, 514, 519], [641, 439, 670, 532]], "scores": [0.0739, 0.0843, 0.091, 0.1008, 0.1012, 0.1058, 0.1243, 0.1266, 0.1342, 0.1618, 0.2452, 0.8505, 0.9113, 0.972]}}</span></pre><p id="eb8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">导入所需的库</strong></p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="2655" class="ml mm it np b gy nt nu l nv nw">import numpy as np<br/>from copy import deepcopy<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span></pre><p id="653c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">创建图像 id 和置信度得分的字典</strong></p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="444e" class="ml mm it np b gy nt nu l nv nw"><strong class="np iu">def get_model_scores(pred_boxes):</strong><br/>    """Creates a dictionary of from model_scores to image ids.<br/>    Args:<br/>        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'<br/>    Returns:<br/>        dict: keys are model_scores and values are image ids (usually filenames)<br/>    """<br/>    model_score={}<br/>    for img_id, val in pred_boxes.items():<br/>        for score in val['scores']:<br/>            if score not in model_score.keys():<br/>                model_score[score]=[img_id]<br/>            else:<br/>                model_score[score].append(img_id)<br/>    return model_score</span></pre><p id="1cff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">计算 Pascal VOC 格式边界框的 IoU</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5eb0db3068705d4e149030d1dbf58d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*LS7YhdAut4K_0XZPD2sH2Q.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Pascal VOC 边界框</p></figure><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="c51c" class="ml mm it np b gy nt nu l nv nw"><strong class="np iu">def calc_iou( gt_bbox, pred_bbox)</strong>:<br/>    '''<br/>    This function takes the predicted bounding box and ground truth bounding box and <br/>    return the IoU ratio<br/>    '''<br/>    <strong class="np iu">x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt= gt_bbox</strong><br/>    <strong class="np iu">x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p= pred_bbox</strong><br/>    <br/>    if (x_topleft_gt &gt; x_bottomright_gt) or (y_topleft_gt&gt; y_bottomright_gt):<br/>        raise AssertionError("Ground Truth Bounding Box is not correct")<br/>    if (x_topleft_p &gt; x_bottomright_p) or (y_topleft_p&gt; y_bottomright_p):<br/>        raise AssertionError("Predicted Bounding Box is not correct",x_topleft_p, x_bottomright_p,y_topleft_p,y_bottomright_gt)<br/>        <br/>         <br/>    #if the GT bbox and predcited BBox do not overlap then iou=0<br/>    if(x_bottomright_gt&lt; x_topleft_p):<br/>        # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox<br/>        <br/>        return 0.0<br/>    if(y_bottomright_gt&lt; y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox<br/>        <br/>        return 0.0<br/>    if(x_topleft_gt&gt; x_bottomright_p): # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox<br/>        <br/>        return 0.0<br/>    if(y_topleft_gt&gt; y_bottomright_p): # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox<br/>        <br/>        return 0.0<br/>    <br/>    <br/>    <strong class="np iu">GT_bbox_area = (x_bottomright_gt -  x_topleft_gt + 1) * (  y_bottomright_gt -y_topleft_gt + 1)</strong><br/>   <strong class="np iu"> Pred_bbox_area =(x_bottomright_p - x_topleft_p + 1 ) * ( y_bottomright_p -y_topleft_p + 1)</strong><br/>    <br/>    <strong class="np iu">x_top_left =np.max([x_topleft_gt, x_topleft_p])<br/>    y_top_left = np.max([y_topleft_gt, y_topleft_p])<br/>    x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])<br/>    y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])</strong><br/>    <br/>    <strong class="np iu">intersection_area = (x_bottom_right- x_top_left + 1) * (y_bottom_right-y_top_left  + 1)</strong><br/>    <br/>    <strong class="np iu">union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)</strong><br/>   <br/>    <strong class="np iu">return intersection_area/union_area</strong></span></pre><p id="04bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">计算精度和召回</strong></p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="da59" class="ml mm it np b gy nt nu l nv nw"><strong class="np iu">def calc_precision_recall(image_results):</strong><br/>    """Calculates precision and recall from the set of images<br/>    Args:<br/>        img_results (dict): dictionary formatted like:<br/>            {<br/>                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},<br/>                'img_id2': ...<br/>                ...<br/>            }<br/>    Returns:<br/>        tuple: of floats of (precision, recall)<br/>    """<br/>    true_positive=0<br/>    false_positive=0<br/>    false_negative=0<br/>    <strong class="np iu">for img_id, res in image_results.items():</strong><br/>        <strong class="np iu">true_positive +=res['true_positive']<br/>        false_positive += res['false_positive']<br/>        false_negative += res['false_negative']</strong><br/>        try:<br/>            <strong class="np iu">precision = true_positive/(true_positive+ false_positive)</strong><br/>        except ZeroDivisionError:<br/>            precision=0.0<br/>        try:<br/>            <strong class="np iu">recall = true_positive/(true_positive + false_negative)</strong><br/>        except ZeroDivisionError:<br/>            recall=0.0<br/>    return (precision, recall)</span></pre><p id="665e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">返回单个图像的一批边界框的真阳性、假阳性和假阴性。</strong></p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="655b" class="ml mm it np b gy nt nu l nv nw"><strong class="np iu">def get_single_image_results(gt_boxes, pred_boxes, iou_thr):</strong><br/>    """Calculates number of true_pos, false_pos, false_neg from single batch of boxes.<br/>    Args:<br/>        gt_boxes (list of list of floats): list of locations of ground truth<br/>            objects as [xmin, ymin, xmax, ymax]<br/>        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)<br/>            and 'scores'<br/>        iou_thr (float): value of IoU to consider as threshold for a<br/>            true prediction.<br/>    Returns:<br/>        dict: true positives (int), false positives (int), false negatives (int)<br/>    """<br/>    all_pred_indices= range(len(pred_boxes))<br/>    all_gt_indices=range(len(gt_boxes))<br/>    if len(all_pred_indices)==0:<br/>        tp=0<br/>        fp=0<br/>        fn=0<br/>        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}<br/>    if len(all_gt_indices)==0:<br/>        tp=0<br/>        fp=0<br/>        fn=0<br/>        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}<br/>    <br/>    gt_idx_thr=[]<br/>    pred_idx_thr=[]<br/>    ious=[]<br/>    <strong class="np iu">for ipb, pred_box in enumerate(pred_boxes):<br/>        for igb, gt_box in enumerate(gt_boxes):<br/>            iou= calc_iou(gt_box, pred_box)</strong><br/>            <br/>           <strong class="np iu"> if iou &gt;iou_thr</strong>:<br/>                gt_idx_thr.append(igb)<br/>                pred_idx_thr.append(ipb)<br/>                ious.append(iou)<br/>    <strong class="np iu">iou_sort = np.argsort(ious)[::1]</strong><br/>    if len(iou_sort)==0:<br/>        tp=0<br/>        fp=0<br/>        fn=0<br/>        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}<br/>    else:<br/>        gt_match_idx=[]<br/>        pred_match_idx=[]<br/>        <strong class="np iu">for idx in iou_sort:<br/>            gt_idx=gt_idx_thr[idx]<br/>            pr_idx= pred_idx_thr[idx]</strong><br/>            # If the boxes are unmatched, add them to matches<br/>            <strong class="np iu">if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx)</strong>:<br/>                <strong class="np iu">gt_match_idx.append(gt_idx)<br/>                pred_match_idx.append(pr_idx)</strong><br/>        <strong class="np iu">tp= len(gt_match_idx)<br/>        fp= len(pred_boxes) - len(pred_match_idx)<br/>        fn = len(gt_boxes) - len(gt_match_idx)</strong><br/>    <strong class="np iu">return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}</strong></span></pre><p id="3010" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后使用 11 点插值技术计算地图。</strong>您可以在此指定 IoU 阈值，否则将使用默认值 0.5</p><pre class="lg lh li lj gt no np nq nr aw ns bi"><span id="09ee" class="ml mm it np b gy nt nu l nv nw"><strong class="np iu">def  get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):</strong><br/>    <br/>  <strong class="np iu">  model_scores = get_model_scores(pred_bb)</strong><br/>    sorted_model_scores= sorted(model_scores.keys())</span><span id="0d84" class="ml mm it np b gy nx nu l nv nw"># Sort the predicted boxes in descending order (lowest scoring boxes first):<br/>    for img_id in pred_bb.keys():<br/>        <br/>        arg_sort = np.argsort(pred_bb[img_id]['scores'])<br/>        pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()<br/>        pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()</span><span id="481f" class="ml mm it np b gy nx nu l nv nw">pred_boxes_pruned = deepcopy(pred_bb)<br/>    <br/>    precisions = []<br/>    recalls = []<br/>    model_thrs = []<br/>    img_results = {}</span><span id="a2a0" class="ml mm it np b gy nx nu l nv nw"># Loop over model score thresholds and calculate precision, recall<br/>    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):<br/>            # On first iteration, define img_results for the first time:<br/>        print("Mode score : ", model_score_thr)<br/>        img_ids = gt_boxes.keys() if ithr == 0 else model_scores[model_score_thr]</span><span id="fb80" class="ml mm it np b gy nx nu l nv nw">for img_id in img_ids:<br/>               <br/>            gt_boxes_img = gt_boxes[img_id]<br/>            box_scores = pred_boxes_pruned[img_id]['scores']<br/>            start_idx = 0<br/>            for score in box_scores:<br/>                if score &lt;= model_score_thr:<br/>                    pred_boxes_pruned[img_id]<br/>                    start_idx += 1<br/>                else:<br/>                    break <br/>            # Remove boxes, scores of lower than threshold scores:<br/>            pred_boxes_pruned[img_id]['scores']= pred_boxes_pruned[img_id]['scores'][start_idx:]<br/>            pred_boxes_pruned[img_id]['boxes']= pred_boxes_pruned[img_id]['boxes'][start_idx:]</span><span id="1172" class="ml mm it np b gy nx nu l nv nw"># Recalculate image results for this image<br/>            print(img_id)<br/>            img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr=0.5)</span><span id="d4d7" class="ml mm it np b gy nx nu l nv nw"># calculate precision and recall<br/>        prec, rec = calc_precision_recall(img_results)<br/>        precisions.append(prec)<br/>        recalls.append(rec)<br/>        model_thrs.append(model_score_thr)</span><span id="6e6c" class="ml mm it np b gy nx nu l nv nw">precisions = np.array(precisions)<br/>    recalls = np.array(recalls)<br/>    prec_at_rec = []<br/>    for recall_level in np.linspace(0.0, 1.0, 11):<br/>        try:<br/>            args= np.argwhere(recalls&gt;recall_level).flatten()<br/>            prec= max(precisions[args])<br/>            print(recalls,"Recall")<br/>            print(      recall_level,"Recall Level")<br/>            print(       args, "Args")<br/>            print(       prec, "precision")<br/>        except ValueError:<br/>            prec=0.0<br/>        prec_at_rec.append(prec)<br/>    avg_prec = np.mean(prec_at_rec) <br/>    return {<br/>        'avg_prec': avg_prec,<br/>        'precisions': precisions,<br/>        'recalls': recalls,<br/>        'model_thrs': model_thrs}</span></pre><h2 id="a19f" class="ml mm it bd mn mo mp dn mq mr ms dp mt kr mu mv mw kv mx my mz kz na nb nc nd bi translated"><strong class="ak">参考文献:</strong></h2><div class="nz oa gp gr ob oc"><a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html#tab:11-point" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">分级检索结果的评估</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">下一节:评估相关性上一节:信息检索中的评估上一节:未分级检索内容的评估…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">nlp.stanford.edu</p></div></div><div class="ol l"><div class="om l on oo op ol oq lp oc"/></div></div></a></div><p id="4409" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae nj" href="https://www.cl.cam.ac.uk/teaching/1415/InfoRtrv/lecture5.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cl . cam . AC . uk/teaching/1415/infor trv/lecture 5 . pdf</a></p><p id="34fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae nj" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">https://medium . com/@ Jonathan _ hui/map-mean-average-precision-for-object-detection-45c 121 a 31173</a></p></div></div>    
</body>
</html>