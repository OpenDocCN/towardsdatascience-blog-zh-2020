<html>
<head>
<title>Effortless Hyperparameters Tuning with Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark轻松调整超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/effortless-hyperparameters-tuning-with-apache-spark-20ff93019ef2?source=collection_archive---------6-----------------------#2020-02-18">https://towardsdatascience.com/effortless-hyperparameters-tuning-with-apache-spark-20ff93019ef2?source=collection_archive---------6-----------------------#2020-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9458" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在Spark上运行随机搜索而不用编写Spark代码？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/52784371fb7cbf7b469ff3fdd4b3e334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14hz1Nc8R5pviAnBM9Kk3Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1044105" rel="noopener ugc nofollow" target="_blank"> Gerd Altmann </a>从<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1044105" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="7263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我仍然无法决定<strong class="lb iu">超参数调优</strong>是我在处理机器学习管道时最喜欢还是最不喜欢的步骤之一；一方面，<strong class="lb iu">这是通常在项目</strong>接近尾声时发生的事情:那些定义问题的会议，花在挖掘数据上的所有时间，ETL脚本开发的数周… <strong class="lb iu">遥远的记忆，是时候看看事情是否联系在一起了！</strong>另一方面，有时候<strong class="lb iu">我觉得自己就像</strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Tantalus" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">坦塔罗斯，拿不到就挂在我面前的多汁水果！</strong>T19】</a></p><p id="1367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您曾经有过这种感觉，那么这篇文章可能是一个很好的起点，可以加速这个过程并简化您的生活，<strong class="lb iu">尤其是有了Apache Spark基础设施之后！</strong></p><h1 id="49f8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">超参数调谐101</h1><p id="1f9a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我会将学习算法的超参数定义为<strong class="lb iu">一条在训练过程之前嵌入模型中的信息，而不是在拟合过程中导出的信息</strong>。如果模型是一个随机森林，超参数的例子有:树的最大深度或在构建森林的每个元素时要考虑多少个特征。</p><p id="6da7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你曾经评估过一个ML模型的质量，<strong class="lb iu">你会知道没有一个放之四海而皆准的配置，因为当我们将同一个模型应用于两个不同的数据集时，它会表现出显著不同的性能</strong>；超参数调整是一个简单的过程，旨在优化配置，使我们为我们的问题选择的模型具有最佳性能。</p><p id="f05e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在非常高的水平上，我们希望使用机器学习算法实现的是最小化成本函数(或最大化质量度量)<code class="fe ms mt mu mv b">f(y_hat(x),y)</code>，其中<code class="fe ms mt mu mv b">y_hat</code>是模型预测的值(给定一组已知特征<code class="fe ms mt mu mv b">x</code>)<code class="fe ms mt mu mv b">y</code>是预期结果。</p><p id="c9e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑具有单个超参数的ML算法。<strong class="lb iu">如果我们在数据集上拟合算法并评估性能，我们将获得成本函数</strong> <code class="fe ms mt mu mv b"><strong class="lb iu">f</strong></code>的特定值。如果我们画出成本如何根据不同的超参数选择而变化，我们最终会得到如下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/a83ab427a4ae340a934586cd452e7755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mvx_0Y5AF5A1djOl9xa5nw.png"/></div></div></figure><p id="feb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，就成本函数而言，一个参数选择给了我们比其他参数更好的结果:<strong class="lb iu">我们可能应该选择那个来构建最终的模型</strong>。上面的例子中，我们有一个单一的超参数，因此，我们的搜索空间只是一条曲线。<strong class="lb iu">如果我们的算法支持</strong> <code class="fe ms mt mu mv b"><strong class="lb iu">2</strong></code> <strong class="lb iu">超参数，那么搜索空间变成一个曲面</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ee1cbda26f41f18e0f72f296661b1646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Zs3EhVMgnlcTNr2mI4Nieg.png"/></div></figure><p id="e8c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们将选择给出最佳成本函数的一对超参数<code class="fe ms mt mu mv b">(hp_1,hp_2)</code>。</p><p id="4e9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">当我们有</strong> <code class="fe ms mt mu mv b"><strong class="lb iu">k</strong></code> <strong class="lb iu">超参数时，搜索一定发生在一个</strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Hypersurface" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">超曲面</strong></a><strong class="lb iu"/><code class="fe ms mt mu mv b"><strong class="lb iu">k</strong></code><strong class="lb iu">维度</strong>上；参数越多，探索就越困难。</p><p id="d56f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此时我们需要的是一个在超参数空间中导航的策略。我们可以使用两种非常简单的方法:</p><ul class=""><li id="3b72" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated"><strong class="lb iu">网格搜索- </strong>这种方法非常简单:<strong class="lb iu">对于每组参数，我们将模型拟合到我们的数据集，并评估性能</strong>。最后，我们选择产生最佳结果的组合。请注意，搜索空间通常包含数百万个点，因此通常不可能进行广泛的测试；因此，网格搜索通常会超时运行。</li><li id="bfa3" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><strong class="lb iu">随机搜索- </strong>该方法非常类似于网格搜索，唯一的区别是搜索空间不是以“网格”方式导航，而是随机选择要测试的超参数元组。<strong class="lb iu">该策略通常优于网格搜索，因为后者对探测起点非常敏感</strong>(例如，考虑网格的第一个元素远离任何可接受的参数选择的情况)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/474ca3ef7ed4fd6e6a6f6c372e8a7c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tzi4d0XGRQuQEduzQzf5jQ.png"/></div></div></figure><h1 id="d26b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">冰山</h1><p id="f5fb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正如我之前所写的，超参数优化只是ML过程的冰山一角。对于这个演示，<strong class="lb iu">我们实际上需要定义问题并收集数据。</strong>幸运的是，一些可爱的人已经这样做了。我们将使用2017年举行的Kaggle <a class="ae ky" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing" rel="noopener ugc nofollow" target="_blank">梅赛德斯-奔驰绿色制造竞赛</a>。<br/>用梅赛德斯-奔驰自己的话说:<em class="nn">在这场比赛中，戴姆勒向卡格勒挑战，以解决维数灾难，减少汽车在测试台上花费的时间。竞争对手将使用代表梅赛德斯-奔驰汽车功能不同排列的数据集来预测通过测试所需的时间。获胜的算法将有助于加快测试速度，从而在不降低戴姆勒标准的情况下降低二氧化碳排放。</em></p><p id="ee8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我选择这个数据集/问题的原因是:</p><ol class=""><li id="f92e" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu no ne nf ng bi translated">任务很简单:我们有特性和一个要预测的。句号。本指南的目的不是为了在模型质量方面实现出色的性能，而是为了展示如何使用Spark来调整参数。<strong class="lb iu">我想从表格中删除所有与数据预处理相关的复杂内容</strong>(例如，我避免了图像、音频、时间序列等。).</li><li id="1a0f" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu no ne nf ng bi translated"><strong class="lb iu">数据集很小</strong>:一个(相当)大的数据集对于超参数优化本身来说不是问题(我们可以在数据集的样本上运行模型)，但是处理大量的样本是一个挑战，我不想在本文中介绍(可以在另一篇文章中介绍！).</li><li id="5aaf" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu no ne nf ng bi translated"><strong class="lb iu">我想要一个真正的问题</strong>:我不想使用Iris、IMDB或Titanic数据集。我相信，如果例子与现实生活中的情况相似，会更有效。</li><li id="d89c" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu no ne nf ng bi translated"><strong class="lb iu">预处理越少，代码越简单</strong>:重点要放在火花部分；在Python中，没有人需要数百行预处理！</li></ol><p id="1f55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">介绍够了，让我们跳到有趣的东西。</p><h1 id="3e3a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">准备好；设置；XGBoost！</h1><p id="03ab" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">数据集由几个分类变量、相当多的二元变量组成，并且相关变量是连续的</strong>。奔驰混淆了表格，所以功能只知道<code class="fe ms mt mu mv b">X_i</code>。如果你想要一个广泛的数据分析，我建议你去比赛页面，看看由社区制作的笔记本！<strong class="lb iu">为评估建议的度量是</strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">决定系数</strong> </a> <strong class="lb iu">，R，我们将坚持使用它来比较模型。我们将使用</strong><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">XGBoost</strong></a><strong class="lb iu">来做预测，</strong> <em class="nn">一个优化的分布式梯度增强库，在</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank"> <em class="nn">梯度增强</em> </a> <em class="nn">框架下实现机器学习算法</em><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><em class="nn">【1】</em></a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梅赛德斯-奔驰数据集示例</p></figure><p id="6639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们试图实现的是使用Spark基础设施运行并行网格/随机搜索。<strong class="lb iu"> Apache Spark自带一些机器学习工具(</strong><a class="ae ky" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">ml lib</strong></a><strong class="lb iu">)。</strong> <strong class="lb iu">别提了！</strong></p><p id="0ba2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我可以听到你的反对意见:是的，XGBoost网格搜索<strong class="lb iu">可以使用MLLib管道</strong>来实现(<a class="ae ky" href="https://www.slideshare.net/databricks/building-a-unified-data-pipeline-with-apache-spark-and-xgboost-with-nan-zhu" rel="noopener ugc nofollow" target="_blank">有一个Nan Zhu提供的很好的平台，解释了如何做到这一点</a>)，但是<strong class="lb iu">我认为MLLib没有为参数调整提供足够的灵活性</strong>:归根结底，你可能想要使用一个更智能的算法，纯Python将使实现更容易；有了下面的想法，你应该能够运行多种类型的搜索(贝叶斯，遗传)而不用忙于处理Spark APIs:所有的东西大部分都是用普通的Python编写的！</p><h2 id="e36e" class="nr lw it bd lx ns nt dn mb nu nv dp mf li nw nx mh lm ny nz mj lq oa ob ml oc bi translated">方法学</h2><p id="345a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们再做一步，定义什么是高级方法论。对于本文的范围来说，我们需要做一个最小的预处理:唯一的目的是让数据集对XGBoost可读。</p><p id="f8ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们要做的事情很简单:</p><ol class=""><li id="dccf" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu no ne nf ng bi translated"><strong class="lb iu">删除ID列</strong>:由于显而易见的原因，我们不想将ID提供给XGBoost。</li><li id="7e2a" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu no ne nf ng bi translated"><strong class="lb iu">对分类变量</strong>进行编码:我们将使用简单的均值编码(即<em class="nn"> </em> <a class="ae ky" href="https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study" rel="noopener ugc nofollow" target="_blank"> <em class="nn">对于每个类别，我们将其标签设置为一个训练数据</em> </a>上目标变量的均值)。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理功能</p></figure><p id="d63b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我在上面指出的，比赛是基于<strong class="lb iu"> R的，所以我们将继续使用这个指标来探究模型的性能；更准确地说，评估算法如下:</strong></p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="beaf" class="nr lw it mv b gy oh oi l oj ok">1. Pick a set of hyperparameters</span><span id="5e8e" class="nr lw it mv b gy ol oi l oj ok">2. Perform 4-folds <a class="ae ky" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">Cross-Validation</a></span><span id="04fa" class="nr lw it mv b gy ol oi l oj ok">3. Get the average <strong class="mv iu">R² score for the 4 runs and store it<br/> <br/></strong>4. Goto 1. until timeout</span><span id="41e7" class="nr lw it mv b gy ol oi l oj ok">5. Select the parameters with the <strong class="mv iu">highest</strong> average score (<strong class="mv iu">R²=1 is the perfect model</strong>)</span></pre><p id="ef31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">厉害！现在我们终于准备好实现算法了！</p><h2 id="729e" class="nr lw it bd lx ns nt dn mb nu nv dp mf li nw nx mh lm ny nz mj lq oa ob ml oc bi translated">定义搜索空间</h2><p id="041e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">运行网格或随机搜索的第一步是定义搜索空间。XGBoost有<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">很多很多参数</a>可以在装配前设置。对于我们的问题，我们将使用tree booster(库也提供了其他选项)，我们将只关注以下参数:</p><p id="59de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> learning_rate </strong> : <em class="nn">更新中使用的步长收缩，防止过拟合。在每个增强步骤之后，我们可以直接获得新特征的权重，并且</em> <code class="fe ms mt mu mv b"><em class="nn">eta</em></code> <em class="nn">缩小特征权重以使增强过程更加保守。【</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="nn"> xgboost参数</em></a><em class="nn">】</em></p><p id="e20c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> colsample_bytree </strong> : <em class="nn">是构造每棵树时列的子样率。对于每个构建的树，进行一次子采样。【</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="nn"> xgboost参数</em></a><em class="nn">】</em></p><p id="7d89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">子样本</strong> : <em class="nn">训练实例的子样本比率。将其设置为0.5意味着XGBoost会在生成树之前随机采样一半的训练数据。这将防止过度拟合。子采样将在每个增强迭代中出现一次</em>。<em class="nn"/><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"><em class="nn">xgboost参数</em></a><em class="nn"/></p><p id="295a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> n_estimators </strong>:算法使用的树的数量。</p><p id="135e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> reg_alpha </strong> : <em class="nn">权重上的L1正则项。增加该值将使模型更加保守。【</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="nn"> xgboost参数</em></a><em class="nn">】</em></p><p id="362e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最大深度</strong> : <em class="nn">一棵树的最大深度。增加该值将使模型更加复杂，并且更有可能过度拟合。【</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="nn"> xgboost参数</em></a><em class="nn">】</em></p><p id="3dc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> gamma </strong> : <em class="nn">在树的叶子节点上做进一步划分所需的最小损失减少。</em> <code class="fe ms mt mu mv b"><em class="nn">gamma</em></code> <em class="nn">越大，算法就越保守。【</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="nn"> xgboost参数</em></a><em class="nn">】</em></p><p id="b301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以想象，以上是一个相当大的搜索空间！<strong class="lb iu">然而，当我们进行超参数调整时，我们并不是完全在黑暗中摸索。许多ML方法(以及XGBoost)都有指导原则，可以帮助为网格的所有参数定义有意义的范围。</strong>比如，即使知道<code class="fe ms mt mu mv b">learning_rate</code>可以在<code class="fe ms mt mu mv b">[0,1]</code>，接近<code class="fe ms mt mu mv b">1</code>的值也不太可能有好结果。以下Python字典代表了我为Mercedes-Benz问题选择的搜索空间:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="b9a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的字典将导致6，890，400个点进行探测。注意，<strong class="lb iu">即使每个测试需要1秒钟完成，我们也需要大约79天18小时的连续计算时间来探索整个网格！</strong>网格搜索永远不会尝试所有那些组合。</p><p id="08f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下函数将网格作为Python生成器返回(以避免在内存中有大数据结构):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><h1 id="73d3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">局部网格和随机搜索</h1><p id="61f4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们有了搜索空间，让我们试着在本地实现网格搜索。</p><p id="958a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要一个方法来评估<strong class="lb iu"> R : </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="cfb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们用下面的签名定义一个函数<code class="fe ms mt mu mv b">grid_search</code>:</p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="803c" class="nr lw it mv b gy oh oi l oj ok">grid_search(timeout_seconds, cv_splits, boosting_rounds)</span></pre><p id="5db0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">搜索算法将在</strong>T5之后终止。交叉验证将使用<code class="fe ms mt mu mv b">cv_splits</code>数量的分割，XGBoost将使用<code class="fe ms mt mu mv b">boosting_rounds</code>迭代:</p><p id="3143" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们将收集熊猫数据框架中的结果进行分析:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="e42f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太棒了。让我们运行上面的代码一个小时，喝杯咖啡，把结果留到以后。</p><h2 id="8c3a" class="nr lw it bd lx ns nt dn mb nu nv dp mf li nw nx mh lm ny nz mj lq oa ob ml oc bi translated">随机搜索</h2><p id="a39f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们可以回收上面的大部分代码来实现随机搜索，<strong class="lb iu">唯一的区别是我们不需要预定义网格</strong>:我们可以只声明一个函数，返回探索面的一个随机点:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5c39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以稍微修改一下网格搜索循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="31ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再次:跑步，咖啡，等待结果。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="7650" class="lv lw it bd lx ly ot ma mb mc ou me mf jz ov ka mh kc ow kd mj kf ox kg ml mm bi translated">让我们随机搜索火花吧！</h1><p id="2e81" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">本文的要点是解释如何使用Spark基础设施来并行化上述算法。</strong>我们将只关注随机搜索。</p><p id="763a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用各种策略来解决这个问题:</p><ul class=""><li id="5ace" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated">用户定义函数(UDF):我们可以实现一个Spark UDF。例如，我们可以创建一个dataframe，在一列中包含网格的所有元素，然后对该列应用UDF，使用这些值作为XGBoost的输入参数。<strong class="lb iu">这个解决方案有几个缺点</strong>:首先<strong class="lb iu">我承诺过你会在没有火花知识的情况下运行这个算法</strong>，UDF并不是一个很好的维持这个承诺的起点。第二点是，在各种UDF之间共享训练数据集可能是不安全的(可能有共享变量？欢迎评论中的建议！).</li><li id="86bf" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><strong class="lb iu"> MLLib </strong>:同样，这是Spark的一个相当高级的用例，我认为它不够灵活。</li><li id="9670" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><strong class="lb iu">带有Spark后端的Joblib</strong>:我们要用这个！你们中的许多人，可能已经知道Joblib，<a class="ae ky" href="https://joblib.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> <em class="nn">一套工具提供了</em><strong class="lb iu"><em class="nn">Python中的轻量级流水线操作</em> </strong> <em class="nn">(函数的透明磁盘缓存和惰性重求值(memoize模式)，轻松简单的并行计算)。Joblib特别针对大数据优化为</em> <strong class="lb iu"> <em class="nn">快速</em></strong><em class="nn"/><strong class="lb iu"><em class="nn">健壮</em> </strong> <em class="nn">，并针对numpy数组</em> </a> <em class="nn">进行了具体优化。对你来说可能听起来很新鲜的是，Joblib 有一个方便的<a class="ae ky" href="https://github.com/joblib/joblib-spark" rel="noopener ugc nofollow" target="_blank"> Spark后端。<strong class="lb iu">通过几行代码，我们将直接在Spark上运行并行循环，而无需编写Spark代码！</strong></a></em></li></ul><p id="9f40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在查看实现之前，<strong class="lb iu">我们需要指定我们将使用的架构</strong>。<strong class="lb iu">特别是，一旦算法有了交叉验证的结果，它将需要在“某个地方”把它们吐出来</strong>(供我们收集和分析)。我们将使用一个MongoDB实例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/6f5f99ab0eb619d3d7935d834c0c3759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD7R5LRppI9HkRAiYBCvxA.png"/></div></div></figure><p id="05f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图显示我们将在Jupyter笔记本中有一个入口点。通过其中一个单元，我们将使用Joblib-Spark将执行发送到不同的节点；此外，<strong class="lb iu">在每个节点上，我们将使用普通Joblib来启用多线程</strong>。最后，每个执行器将把结果写在位于另一个服务器上的MongoDB实例中。</p><p id="f57a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预处理代码将保持不变，我们可以重用该函数来生成随机组合，所以让我们编写一个神奇的循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="0e82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Joblib魔术发生在下面一行</strong>:</p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="a5b3" class="nr lw it mv b gy oh oi l oj ok">Parallel(backend="spark", n_jobs=NODES)(delayed(evaluate)(X, y, kf, boosting_rounds) for p in range(0, NODES))</span></pre><p id="b490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来分解一下:</p><ul class=""><li id="2025" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated"><code class="fe ms mt mu mv b">Parallel</code>初始化Joblib环境。在这种情况下，我们需要指定后端(即“spark”)。</li><li id="6fcd" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">涉及到一些“全局”变量；一个重要的参数是<code class="fe ms mt mu mv b">NODES</code>，即集群中的节点数量。我们正在做的是将网格分成 <code class="fe ms mt mu mv b"><strong class="lb iu">NODES=3</strong></code> <strong class="lb iu">块，并将每个块发送给一个Spark执行器。</strong></li><li id="9ea8" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><code class="fe ms mt mu mv b">delayed</code>只是一个<em class="nn">装饰器，用来捕获函数</em>的参数(在这里是<code class="fe ms mt mu mv b">evaluate</code>，我们将在后面定义它)。需要<code class="fe ms mt mu mv b">for p in range(0, NODES)</code>来告诉<code class="fe ms mt mu mv b">Parallel</code>执行<code class="fe ms mt mu mv b">delayed</code>函数多少次(在这种情况下<strong class="lb iu">只执行</strong> <code class="fe ms mt mu mv b"><strong class="lb iu">NODES</strong></code> <strong class="lb iu">次，因为执行程序将运行测试，直到超时发生</strong></li><li id="30a8" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><code class="fe ms mt mu mv b">X</code>、<code class="fe ms mt mu mv b">y</code>、<code class="fe ms mt mu mv b">kf</code>和<code class="fe ms mt mu mv b">boosting_rounds</code>是<code class="fe ms mt mu mv b">evaluate</code>功能的参数。</li></ul><p id="be38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">引擎盖下发生了什么？<strong class="lb iu"> Joblib-Spark将通过网络发送函数(使用</strong><a class="ae ky" href="https://github.com/cloudpipe/cloudpickle" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">cloud-pickle</strong></a><strong class="lb iu">进行序列化)</strong>。然后通过<code class="fe ms mt mu mv b">parallelize</code> Spark方法，每个执行将被发送到单个节点。</p><p id="324f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要构建将在每个Spark机器上执行的<code class="fe ms mt mu mv b">evaluate</code>函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="fcb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，我们在<code class="fe ms mt mu mv b">evaluate</code>中定义了一个内部函数。<strong class="lb iu">这是必要的，因为除了在Spark上并行化网格搜索，我们还想在单个执行器上使用多线程。下面一行显示了默认后端的(普通)Joblib的用法:</strong></p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="7af7" class="nr lw it mv b gy oh oi l oj ok">Parallel()(delayed(evaluate_inner)(keys, g, X, y, kf, boosting_rounds) for g in df_grid)</span></pre><p id="f2fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们运行上面的<code class="fe ms mt mu mv b">random_search_spark</code>函数一个小时，并将结果保存在我们的MongoDB实例中。<strong class="lb iu">我们希望在Spark UI上看到的是三个阶段，每个阶段将运行一个小时</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/459d37a0b2a05cdb5f34b491b2040545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zz3DAu0YQ09QOLdWGTROhg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">35分钟后触发用户界面</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/ac05446df57c17bb1456d01d99c8748b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BP3cxzOHSFF30ocfZ8Jeuw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">60分钟后的Spark UI任务失败，因为Spark上下文在超时后终止</p></figure><h1 id="8216" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">比较和结论</h1><p id="346a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">旅程即将结束。我们已经收集了所有的结果，我们只需要分析它们！</p><p id="4521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Spark测试一直在由3台机器(1个驱动程序+ 2个执行器)组成的集群上运行，每台机器有4个内核</strong>，而“本地”搜索算法一直在单台机器上执行。我们对所有的方法都使用了下面的常数。</p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="9818" class="nr lw it mv b gy oh oi l oj ok">CV_SPLITS = 4 # Number of CV splits<br/>TIMEOUT_SECONDS = 3600 # Timeout is 1 hour<br/>BOOSTING_ROUNDS = 500 # XGBoost boosting rounds</span></pre><p id="9ae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个有趣的发现是简单地比较每种方法执行了多少测试:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/09ffeba15610a39db0eb4d80785151bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R05-bvtSTc9wiG4jKwHD0g.png"/></div></div></figure><p id="cd29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在这种情况下，我们获得了3倍的提升</strong>，这是给定节点数量的线性增长(注意:“本地”版本仍然有4个内核可用)。</p><p id="0613" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了了解方法“如何”探索超曲面，让我们画出算法获得的<strong class="lb iu">R:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/6ad9c330358f69f66d820ec8be0a3982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkHraY65qSfedeEnSKIq3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在单台机器上使用网格搜索通过4倍CV获得的平均R</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/56eb63b6d6043b2f3d39f091d76bda59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qIU5SmFCVux4MhcR7da3Dg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在单台机器上使用随机搜索通过4倍CV获得的平均R</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/5ec485bae64b1ad56120db46bb06909d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WnYDWgq-YfC2Fzlj5MGJQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在火花(3个节点)上使用网格搜索通过4倍CV获得的平均R</p></figure><p id="4d67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的图表中，至少可以看出两点:</p><ol class=""><li id="c5da" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu no ne nf ng bi translated"><strong class="lb iu">网格搜索探索似乎没那么有效</strong>。请注意曲线中的水平部分:每个部分中变化的<strong class="lb iu">参数对分数</strong>的影响非常小，因此可能不值得深入探究(当然，为了简单起见，我没有考虑参数之间的相互依赖性)。</li><li id="03ba" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu no ne nf ng bi translated">Spark并行化是可取的，因为它可以荒谬地增加尝试的次数。</li></ol><p id="88d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这些方法找到的最佳参数:</p><pre class="kj kk kl km gt od mv oe of aw og bi"><span id="dc6d" class="nr lw it mv b gy oh oi l oj ok"><strong class="mv iu">Grid Search (Local)</strong><br/>Best <strong class="mv iu">R²</strong>: 0.5656</span><span id="e883" class="nr lw it mv b gy ol oi l oj ok">Params:<br/>   learning_rate: 0.01,<br/>   colsample_bytree: 0.8,<br/>   subsample: 0.5,<br/>   n_estimators: 100,<br/>   reg_alpha: 0.01,<br/>   max_depth: 3,<br/>   gamma:0</span><span id="0323" class="nr lw it mv b gy ol oi l oj ok"><strong class="mv iu">Random Search (Local)</strong><br/>Best <strong class="mv iu">R²</strong>: 0.5648</span><span id="0513" class="nr lw it mv b gy ol oi l oj ok">Params:<br/>   learning_rate: 0.0354,<br/>   colsample_bytree: 0.88,<br/>   subsample: 0.8,<br/>   n_estimators: 1410,<br/>   reg_alpha: 0.10,<br/>   max_depth: 3,<br/>   gamma:0</span><span id="dd34" class="nr lw it mv b gy ol oi l oj ok"><strong class="mv iu">Random Search (Spark)</strong><br/>Best <strong class="mv iu">R²</strong>: 0.5643</span><span id="3426" class="nr lw it mv b gy ol oi l oj ok">Params:<br/>   learning_rate:0.0524,<br/>   colsample_bytree:0.93,<br/>   subsample:0.88,<br/>   n_estimators:1413,<br/>   reg_alpha:0.409,<br/>   max_depth:3,<br/>   gamma:7</span></pre><p id="aa02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意网格搜索赢了，即使它只能做32次尝试！</strong>以上结果的结果是<strong class="lb iu">我们可能应该重新考虑调整哪些超参数以及如何定义网格/范围</strong>。例如，我们可以看到，所有的方法都同意这样的事实，即<code class="fe ms mt mu mv b">learning_rate</code>应该在<code class="fe ms mt mu mv b">[0.01,0.05]</code>中，而在我们的网格中，我们将最大值设置为<code class="fe ms mt mu mv b">0.25</code> : <strong class="lb iu">我们可能浪费了大量的计算时间来寻找超曲面的错误区域！</strong> <em class="nn">(旁注:</em> <code class="fe ms mt mu mv b"><em class="nn">max_depth=3</em></code> <em class="nn">让我想到，也许，很多变量可以从数据集中剔除，但那是另一回事)。</em></p><p id="4312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为上面的结果可以引出下面的问题:<strong class="lb iu">我们是否可以在探索算法中加入一些智能，做一些比这更聪明的事情？</strong>答案是“<strong class="lb iu">是的，我们可以</strong>”，但那是另一篇文章的主题！离开之前，如果你读到这里，<strong class="lb iu">你值得拥有</strong> <a class="ae ky" href="https://gist.github.com/aialenti/acad08b609642c47a5abd14e78207173" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">链接到最后的笔记本</strong> </a> <strong class="lb iu">！</strong></p><h1 id="8aed" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">外卖食品</h1><ul class=""><li id="24db" class="my mz it lb b lc mn lf mo li pc lm pd lq pe lu nd ne nf ng bi translated">Joblib-Spark可以成为在Spark基础设施上扩展算法的强大工具。其中一个主要优势是<strong class="lb iu">它不需要使用Spark APIs进行开发:</strong>要使用它，您不需要对Apache的分布式计算系统有很深的理解。</li><li id="ad06" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">Joblib可能允许你用相对较小的努力提升和移动你现有的管道。</li><li id="f25f" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><strong class="lb iu">网格搜索和随机搜索</strong>是两种非常简单的方法，可用于进行超参数调整，并且<strong class="lb iu">易于实现</strong>。</li><li id="8a73" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">随机搜索通常优于网格搜索。</li><li id="c308" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated"><strong class="lb iu">网格和随机搜索在超参数调优方面并不是最好的</strong>，<strong class="lb iu">因为它们没有利用已经执行的测试来理解探索过程中的方向。</strong></li></ul><p id="cf0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢！<strong class="lb iu">让我知道你的想法</strong>，如果你愿意，<strong class="lb iu">看看这些其他的文章！</strong></p><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/the-art-of-joining-in-spark-dcbd33d693c"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd iu gy z fp pn fr fs po fu fw is bi translated">火花中加入的艺术</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">Spark中加速连接的实用技巧</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw ks pi"/></div></div></a></div><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/clustering-pollock-1ec24c9cf447"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd iu gy z fp pn fr fs po fu fw is bi translated">聚类波洛克</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">杰森·布拉克绘画的聚类分析——如何利用k-means进行色彩分组</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="px l pt pu pv pr pw ks pi"/></div></div></a></div></div></div>    
</body>
</html>