<html>
<head>
<title>Deep Learning: Introduction to Tensors &amp; TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:张量和张量流简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-introduction-to-tensors-tensorflow-36ce3663528f?source=collection_archive---------10-----------------------#2020-01-19">https://towardsdatascience.com/deep-learning-introduction-to-tensors-tensorflow-36ce3663528f?source=collection_archive---------10-----------------------#2020-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8f11" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解最受欢迎的深度学习库！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7d10b39bc8d7be47995fce560e521e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rydzd7Zbsnw-sG_4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/AAMldegB0x8" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="1dba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e795" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文的目标是涵盖以下主题:</p><ul class=""><li id="2f57" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">张量介绍</li><li id="3180" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">图表、变量和操作</li><li id="3a0d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">使用TensorFlow解决问题</li></ul><h1 id="f8dd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是张量流</h1><p id="85ee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TensorFlow是由Google开发和维护的一个框架，它使数学运算能够在CPU或GPU上以优化的方式执行。我们将专注于GPU，因为这是我们训练深度神经网络的最快方法。</p><p id="9aa8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">为什么是Tensorflow？</strong></p><ul class=""><li id="37d2" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">因为它的灵活性和可伸缩性</li><li id="b23e" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">因为它受欢迎</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d20882752d83f790f472a262bfacb420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*L1EUrKLkWWC8wklS.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="1e93" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">使TensorFlow成为最受欢迎的深度学习库的关键功能是:</p><ul class=""><li id="84bc" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">TensorFlow使用张量来执行操作。</li><li id="19fe" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">在TensorFlow中，首先定义要执行的活动(构建图)，然后执行它们(执行图)。这允许该过程被优化到手边的任务，大大减少了计算时间。</li><li id="9474" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">TensorFlow支持代码并行运行或在一个或多个GPU上运行。</li></ul><p id="2e6a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">好吧，但是张量是什么？</p><p id="12e1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">虽然张量是物理学家发明的，用来描述相互作用，但在人工智能领域，张量可以简单地理解为数字的容器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/f9be3653593680d9698f5ea39e9cbb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*WArDf9h6Dtbo-4H5P4lguQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="6f94" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们现在就把这一切付诸实践。我们将使用python编写一些张量代码，以便更好地理解它们是什么以及它们是如何工作的。</p><h1 id="e069" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">实践中的张量</h1><p id="ad47" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">假设我们想存储一个学生的平均成绩。我们将使用一个0D张量，它只是一个简单的数或标量。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="d2f8" class="nn la it nj b gy no np l nq nr">import numpy as np</span><span id="8338" class="nn la it nj b gy ns np l nq nr">tensor_0D = np.array(5)<br/>print("Average grade: \n{}".format(tensor_0D))<br/>print("Tensor dimensions: \n{}".format(tensor_0D.ndim))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3bc046fcf823884d0ebda2aa2d8f1372.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*4LhB89octgd8EYjLtNE6bg.png"/></div></figure><p id="742d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在让我们尝试存储该学生所学的每门课程的分数。我们可以用1D张量来做到这一点:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="8ece" class="nn la it nj b gy no np l nq nr">tensor_1D = np.array([4,6,8])<br/>print("Subject grades: \n{}".format(tensor_1D))<br/>print("Tensor dimensions: \n{}".format(tensor_0D.ndim))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b479e50dd6d7f752ba794ac4be8e3891.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*w96hPGNT1PqUNOyiTEJIOw.png"/></div></figure><p id="166c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">但是，等等……如果我们想存储学生每门学科的每次考试的分数，该怎么办呢？如果每个科目有3次考试，我们该怎么做呢？</p><p id="973a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">通过使用2D张量！正如我们之前看到的，它是一个矩阵。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="b308" class="nn la it nj b gy no np l nq nr"><strong class="nj iu"># 2D Tensor (matrix)</strong><br/>tensor_2D = np.array([[0, 1, 1],  # Subject 1<br/>                      [2, 3, 3],  # Subject 2<br/>                      [1, 3, 2]])  # Subject 3<br/>print("Exam grades are:\n{}".format(tensor_2D))</span><span id="8cf6" class="nn la it nj b gy ns np l nq nr">print("Subject 1:\n{}".format(tensor_2D[0]))</span><span id="797a" class="nn la it nj b gy ns np l nq nr">print("Subject 2:\n{}".format(tensor_2D[1]))</span><span id="375c" class="nn la it nj b gy ns np l nq nr">print("Subject 3:\n{}".format(tensor_2D[2]))</span><span id="de7e" class="nn la it nj b gy ns np l nq nr">print("Tensor dimensions: \n{}".format(tensor_2D.ndim))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/77d9f8a9ddbe4058f6b5067833b4755e.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*qOPnFt6EMZw0wFgSYjdirg.png"/></div></figure><p id="ba8c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们现在希望存储四个季度的科目成绩(每年一次)，这样将来需要时就可以更容易地访问它们，你不这样认为吗？你认为我们该如何组织他们？</p><p id="ad2e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果我们给2D张量增加一个维度来表示四分之一会怎么样？</p><p id="011f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们会得到一个三维张量(三维矩阵或立方体)。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="04be" class="nn la it nj b gy no np l nq nr">tensor_3D = np.array([[[0, 1, 1],  # First quarter<br/>                      [2, 3, 3],<br/>                      [1, 3, 2]],<br/>                     [[1, 3, 2],  # Second quarter<br/>                      [2, 4, 2],<br/>                      [0, 1, 1]]])<br/>print("Exam grades per quarter are:\n{}".format(tensor_3D))</span><span id="1d99" class="nn la it nj b gy ns np l nq nr">print("First quarter:\n{}".format(tensor_3D[0]))</span><span id="bfe6" class="nn la it nj b gy ns np l nq nr">print("Second quarter:\n{}".format(tensor_3D[1]))</span><span id="8043" class="nn la it nj b gy ns np l nq nr">print("Tensor dimensions: \n{}".format(tensor_3D.ndim))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/274b0cdc749900660e8ed56bb72a2937.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*2NLM05hFv3I00bwIf7pZ3w.png"/></div></figure><p id="0fdb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果我们在张量中增加一个维度，这样我们就可以得到每个学生每门学科每学期的成绩，会怎么样？</p><p id="635b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">它将是一个4D张量(3D矩阵向量或立方体向量)。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2780" class="nn la it nj b gy no np l nq nr">tensor_4D = np.array([[[[0, 1, 1], # Jacob<br/>                      [2, 3, 3],<br/>                      [1, 3, 2]],<br/>                     [[1, 3, 2],<br/>                      [2, 4, 2],<br/>                      [0, 1, 1]]],<br/>                      [[[0, 3, 1], # Christian<br/>                      [2, 4, 1],<br/>                      [1, 3, 2]],<br/>                     [[1, 1, 1],<br/>                      [2, 3, 4],<br/>                      [1, 3, 2]]],<br/>                     [[[2, 2, 4], # Sofia<br/>                      [2, 1, 3],<br/>                      [0, 4, 2]],<br/>                     [[2, 4, 1],<br/>                      [2, 3, 0],<br/>                      [1, 3, 3]]]])<br/>print("The grades of each student are:\n{}".format(tensor_4D))</span><span id="9da6" class="nn la it nj b gy ns np l nq nr">print("Jacob's grades:\n{}".format(tensor_4D[0]))</span><span id="fc58" class="nn la it nj b gy ns np l nq nr">print("Christian's grades:\n{}".format(tensor_4D[1]))</span><span id="a46c" class="nn la it nj b gy ns np l nq nr">print("Sofia's grades:\n{}".format(tensor_4D[2]))</span><span id="9ccf" class="nn la it nj b gy ns np l nq nr">print("Tensor dimensions: \n{}".format(tensor_4D.ndim))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4764c4032737baf28a1f05ad59156193.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*yuj9dwIAZ0yr095FX-bK2g.png"/></div></figure><p id="b726" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">所以我们可以无限地增加张量的维数，来存储更多的数据。</p><p id="fbf8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了让您了解张量在深度学习领域的使用频率，最常见的张量类型有:</p><ul class=""><li id="8108" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">三维张量</strong>:用于时间序列。</li><li id="196b" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> 4D张量</strong>:用于图像。</li><li id="53d5" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> 5D张紧器</strong>:与视频一起使用。</li></ul><p id="dfd4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">通常，其中一个维度将用于存储每种数据类型的样本。例如，对于图像:</p><p id="c302" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果我们想要存储224x224像素的64幅RGB图像，我们将需要一个3D矩阵向量，或者同样的，一个4D张量。我们需要多少维度？</p><p id="82eb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们有64幅224像素x 224像素x 3通道(R、G、B)的图像。</p><p id="f025" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">因此:(64，224，224，3)</p><p id="65f2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果你想更深入地了解张量或更多的例子，这里有一个非常好的资源:<a class="ae ky" href="https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32" rel="noopener ugc nofollow" target="_blank">用猫说明的张量</a></p><p id="8195" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们之前说过，在TensorFlow中，首先定义要执行的操作，然后执行它们。为了做到这一点，你使用一个图表。</p><p id="fe97" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">什么是图形？</strong></p><p id="25c7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">a + b求和的一个简单例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e217cd2a3bace8e9cc8055f9256db6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*dWHoDYkM-avZGG_s.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="f0fd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这是一个更复杂的例子，现在，你不需要完全理解，但这只是其中的一小部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/22905acf81c566d8d40aa36ea9c3f2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/0*ECi050BVTS3jpd0k.gif"/></div></figure><h1 id="14e6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">张量流付诸实践</h1><p id="659d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先我们需要定义和理解TensorFlow的一些基本概念(从现在开始TF):</p><ul class=""><li id="b957" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">tf。图:表示一组tf。操作</li><li id="1e21" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">tf。运算:运算是由我们定义的方程式决定的吗</li><li id="7c89" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">tf。张量:我们存储tf结果的地方。操作</li></ul><p id="d610" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">一开始，tf。图形对我们来说是透明的，因为有一个默认的图形，其中添加了我们定义的所有操作。它被称为tf.get_default_graph()。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="3a28" class="nn la it nj b gy no np l nq nr"><strong class="nj iu"># We import the Tensorflow package and matplotlib for the charts<br/></strong>import tensorflow as tf<br/>import matplotlib.pyplot as plt <br/>%matplotlib inline</span></pre><p id="0dcd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">先说一些很简单的，仅仅是Tensorflow中的一个简单乘法。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="4d66" class="nn la it nj b gy no np l nq nr"><strong class="nj iu"># Variables definition</strong><br/>x = tf.constant(6)  <br/>y = tf.constant(8)</span><span id="3e65" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu"># Operations definition</strong><br/>result = tf.multiply(x, y)<br/>print(result)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/63fc91197d690c78bb972d4706da470e.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*hB7rKvrkQPWRpEmNqsbn0A.png"/></div></figure><p id="059c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如你所见，它没有给我们返回结果。到目前为止，它所做的就是创建网络。</p><p id="6d08" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">举个例子，就像坐车一样。现在我们已经把它组装好了，但是它仍然没有完成它的设计，移动。为此，我们应该打开它。为此，我们将使用tf。会话()。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0577" class="nn la it nj b gy no np l nq nr">sess = tf.Session() <br/>output = sess.run(result)  <br/>print(output)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2448fea8cf563ad745deba3f19c760d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:98/format:webp/1*A8KbBRSz0xPcqCXPeWch0w.png"/></div></figure><p id="6dba" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了能够可视化该图，我们应该定义几个函数。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2985" class="nn la it nj b gy no np l nq nr">from IPython.display import clear_output, Image, display, HTML</span><span id="229c" class="nn la it nj b gy ns np l nq nr">def strip_consts(graph_def, max_const_size=32):<br/>    """Strip large constant values from graph_def."""<br/>    strip_def = tf.GraphDef()<br/>    for n0 in graph_def.node:<br/>        n = strip_def.node.add() <br/>        n.MergeFrom(n0)<br/>        if n.op == 'Const':<br/>            tensor = n.attr['value'].tensor<br/>            size = len(tensor.tensor_content)<br/>            if size &gt; max_const_size:<br/>                tensor.tensor_content = "&lt;stripped %d bytes&gt;"%size<br/>    return strip_def</span><span id="1d86" class="nn la it nj b gy ns np l nq nr">def show_graph(graph_def, max_const_size=32):<br/>    """Visualize TensorFlow graph."""<br/>    if hasattr(graph_def, 'as_graph_def'):<br/>        graph_def = graph_def.as_graph_def()<br/>    strip_def = strip_consts(graph_def, max_const_size=max_const_size)<br/>    code = """<br/>        &lt;script&gt;<br/>          function load() {{<br/>            document.getElementById("{id}").pbtxt = {data};<br/>          }}<br/>        &lt;/script&gt;<br/>        &lt;link rel="import" href="<a class="ae ky" href="https://tensorboard.appspot.com/tf-graph-basic.build.html" rel="noopener ugc nofollow" target="_blank">https://tensorboard.appspot.com/tf-graph-basic.build.html</a>" onload=load()&gt;<br/>        &lt;div style="height:600px"&gt;<br/>          &lt;tf-graph-basic id="{id}"&gt;&lt;/tf-graph-basic&gt;<br/>        &lt;/div&gt;<br/>    """.format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))</span><span id="9076" class="nn la it nj b gy ns np l nq nr">iframe = """<br/>        &lt;iframe seamless style="width:1200px;height:620px;border:0" srcdoc="{}"&gt;&lt;/iframe&gt;<br/>    """.format(code.replace('"', '&amp;quot;'))<br/>    display(HTML(iframe)</span></pre><p id="cb74" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在我们将展示之前定义的图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e843ebb73614f350c59aa620fcba6417.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*bnLoh9uWYijxfBqdjEzGCA.png"/></div></figure><p id="6793" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">正如我们所见，该图由两个常量类型的节点和一个运算符类型(乘法)的节点组成。然而，他们的名字并不十分具有指示性。让我们改变这一点:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="59b8" class="nn la it nj b gy no np l nq nr">x2 = tf.constant(5.0, name='x2')<br/>y2 = tf.constant(6.0, name='y2')</span><span id="578a" class="nn la it nj b gy ns np l nq nr">result = tf.multiply(x2, y2)</span><span id="2f66" class="nn la it nj b gy ns np l nq nr"># Let's see what it looks like now:<br/>show_graph(tf.get_default_graph().as_graph_def())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/fe39457d7b5f61bf57c46012b7585d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*RKB-opIxPTNTIvxjd6oVcw.png"/></div></figure><p id="888a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">一旦我们执行了操作，我们应该关闭会话，用sess.close()释放资源。</p><p id="962f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最后，我们还可以向the GPU表明我们希望它执行操作。为此，我们可以打印可用设备的列表:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="b355" class="nn la it nj b gy no np l nq nr">from tensorflow.python.client import device_lib</span><span id="362c" class="nn la it nj b gy ns np l nq nr">def get_available_devices():<br/>    local_device_protos = device_lib.list_local_devices()<br/>    return [x.name for x in local_device_protos]</span><span id="f656" class="nn la it nj b gy ns np l nq nr">print(get_available_devices())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ce8bba740046128738cf5e9bd6f8a5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8eKIKUm7lyrHsh-3utG_w.png"/></div></div></figure><p id="e149" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们将选择GPU:0，并执行[3 ^ 3]乘以[2 ^ 2]的乘法运算，结果可能是3x3 +2x2 = 12。让我们来看看:</p><p id="f2e5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">通过使用with ___ as __:我们让python自己释放TF会话的资源:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2bec" class="nn la it nj b gy no np l nq nr">with tf.Session() as sess:<br/>  with tf.device("/GPU:0"):<br/>    matrix1 = tf.constant([[3., 3.]])<br/>    matrix2 = tf.constant([[2.],[2.]])<br/>    product = tf.matmul(matrix1, matrix2)<br/>    <br/>  output = sess.run(product)<br/>  print(output)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1cb7766dc45f564c95489b2240bc7974.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*V7rhNIl5CQ4aLfXSUuXJEA.png"/></div></figure><p id="6528" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在让我们创建一个在-5和5之间平均稀疏的32个值的1D张量:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="487f" class="nn la it nj b gy no np l nq nr">n_values = 64<br/>x = tf.linspace(-5.0, 5.0, n_values)</span><span id="48b7" class="nn la it nj b gy ns np l nq nr">sess = tf.Session()<br/>result = sess.run(x)</span><span id="8a39" class="nn la it nj b gy ns np l nq nr">print(result)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b3664db8ac276255f632aac7ab0748f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IlcXL0qKMw5e5fk71zAzgw.png"/></div></div></figure><p id="5610" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">除了sess.run(_)之外，还有其他评价张量的方法</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2432" class="nn la it nj b gy no np l nq nr">x.eval(session=sess)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/38f073b02f5b5c1d945479dcc17703cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QjggzDPsZhpikPOCWOYb_A.png"/></div></div></figure><p id="8d63" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们需要始终记住结束会议:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="db92" class="nn la it nj b gy no np l nq nr">sess.close()</span></pre><p id="e714" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们还可以使用交互式会话，这可以帮助我们不必不断地调用。运行()以执行结果:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="a78c" class="nn la it nj b gy no np l nq nr">sess = tf.InteractiveSession()<br/>x.eval()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/efa9093d177530290c8672071f3e89bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Ix3re1ZwS4x4IupO5LkVA.png"/></div></div></figure><p id="9a5b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在我们来看一个tf。操作，为此，我们将使用“x”来创建和可视化高斯分布。它的公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a521f6c513789d56af55def552a789e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*Qs-sAF7B5jjJqFnuPas1Kw.png"/></div></figure><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2d29" class="nn la it nj b gy no np l nq nr">sigma = 1.0<br/>mean = 0</span><span id="08f9" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu"># To implement the gaussian distributio formula</strong>:<br/>g1d = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) / (2.0 * tf.pow(sigma, 2.0)))) *<br/>     (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))</span><span id="0c20" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu"># to check that this operation has been succesuflly included in our tf.Graph</strong><br/>if g1d.graph is tf.get_default_graph():<br/>  print('All good')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/346a3f3b397acfe34c9f614a4aa86636.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*9f4NdpEItwAJy0SqSCI47A.png"/></div></figure><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="aff6" class="nn la it nj b gy no np l nq nr">plt.plot(g1d.eval())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38d00867cde43be17e3c9db05a27dfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*VroxWcBSjMkDNay_qahxHA.png"/></div></figure><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f8d5" class="nn la it nj b gy no np l nq nr"># to see the dimensions<br/>print(g1d.get_shape())<br/>print(type(g1d.get_shape()))</span><span id="314d" class="nn la it nj b gy ns np l nq nr">print(g1d.get_shape().as_list())</span><span id="9bb0" class="nn la it nj b gy ns np l nq nr">print(type(g1d.get_shape().as_list()))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c12185dcc7d65d9280e40374f6368eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*LOiwam-r0mdeT-OwBPB17w.png"/></div></figure><p id="a320" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">有时候，在执行返回变量值的操作之前，我们不知道变量的维数。对于这些情况，我们可以使用tf.shape(variable ),它返回一个张量，在运行时计算结果的维数。</p><p id="d226" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这被称为“静态形状”和“动态形状”，其中静态形状的计算考虑了张量的维度和所涉及的操作，以及执行时的动态。</p><p id="22c7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果我们将x定义为“占位符”会发生什么？一个占位符就像一个储备，它表明那里会有一个张量，但没有必要在那个时刻定义它。例如，通过定义:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ed5c" class="nn la it nj b gy no np l nq nr">x = tf.placeholder(tf.int32, shape=[5])</span></pre><p id="54eb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们知道x将持有一个1D五维张量，这一点由x.get_shape()所证实:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="761e" class="nn la it nj b gy no np l nq nr">print(x.get_shape())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/54fee8a5f6d990f040fc85bf975f698b.png" data-original-src="https://miro.medium.com/v2/resize:fit:114/format:webp/1*nse5FfKSfsj-Q9OU6r3d8w.png"/></div></figure><p id="cc1d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">但是我们不知道什么价值观会形成它，直到我们告诉它。</p><blockquote class="oo op oq"><p id="bdb4" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated"><strong class="lt iu">占位符和变量的区别:</strong></p><p id="cf23" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated"><strong class="lt iu">变数</strong></p><p id="f7e5" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated">它们用于容纳在培训过程中学习到的参数。</p><p id="4d3f" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated">因此，这些值是从训练中获得的-它们需要分配一个初始值(可以是随机的)</p><p id="f2c6" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated"><strong class="lt iu">占位符</strong></p><p id="f809" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated">为数据保留空间(例如，为图像中的像素)</p><p id="cd5b" class="lr ls or lt b lu mp ju lw lx mq jx lz os nd mc md ot ne mg mh ou nf mk ml mm im bi translated">它们不需要为start赋值(尽管它们可以)</p></blockquote><p id="e190" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">获得的值5是x维度的静态值。但是，如果我们对x应用一个tf.unique()会发生什么呢？</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="8e49" class="nn la it nj b gy no np l nq nr">y, _ = tf.unique(x)<br/>print(y.get_shape())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a93fbf0b70f5434f57d3804375e1dd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:124/format:webp/1*8kg4m6j4G36uBOrYYRuoog.png"/></div></figure><p id="5dcd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">所发生的是tf.unique()返回x的唯一值，这些值最初是未知的，因为x被定义为一个占位符，占位符直到执行时才需要定义，正如我们之前所说的。事实上，让我们看看如果我们给“x”输入两个不同的值会发生什么:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="fe35" class="nn la it nj b gy no np l nq nr">with tf.Session() as sess:<br/> print(sess.run(y, feed_dict={x: [0, 1, 2, 3, 4]}).shape)<br/> print(sess.run(y, feed_dict={x: [0, 0, 0, 0, 1]}).shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/a8b9fb1c59e4081170385f01d17b7558.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/format:webp/1*VY8-qxZuxh3RWfWXCVdUnQ.png"/></div></figure><p id="e7ab" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">看那个！y的大小根据tf.unique()返回的内容而变化。这叫做“动态形状”，它总是被定义的，它永远不会通过回答返回一个问题。因此，TensorFlow支持类似tf.unique()的操作，这些操作可能会产生大小可变的结果。</p><p id="5558" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在你知道了，每次你使用输出变量的运算，你都需要使用tf.shape(变量)来计算张量的动态形状。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="fedd" class="nn la it nj b gy no np l nq nr">sy = tf.shape(y)</span><span id="4a3e" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu"># Returns a list with the dimensions</strong><br/>print(sy.eval(feed_dict={x: [0, 1, 2, 3, 4]}))<br/>print(sy.eval(feed_dict={x: [0, 0, 0, 0, 1]}))</span><span id="29db" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu"># We access the dimension of interest</strong><br/>print(sy.eval(feed_dict={x: [0, 1, 2, 3, 4]})[0])<br/>print(sy.eval(feed_dict={x: [0, 0, 0, 0, 1]})[0])</span></pre><p id="f073" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在，我们可以在考虑操作的输出大小之后执行操作，我们在第一个实例中并不知道输出的大小。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="5956" class="nn la it nj b gy no np l nq nr">print(tf.shape(y).eval(feed_dict={x: [0, 1, 4, 1, 0]}))<br/>print(type(tf.shape(y).eval(feed_dict={x: [0, 1, 4, 1, 0]})))</span><span id="c71f" class="nn la it nj b gy ns np l nq nr">print(tf.stack([y, y[::-1], tf.range(tf.shape(y)[0])]).eval(feed_dict={x: [0, 1, 4, 1, 0]}))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a1434615a6813d8d0ef7924da4bf85d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*T4gGSkmxnDX06XRs1vnF5w.png"/></div></figure><p id="3e10" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在让我们看看2D的高斯分布</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="3ebf" class="nn la it nj b gy no np l nq nr">g1d_r = tf.reshape(g1d, [n_values, 1])<br/>print(g1d.get_shape().as_list())<br/>print(g1d_r.get_shape().as_list())</span><span id="bcbd" class="nn la it nj b gy ns np l nq nr"># We multiply the row vector of the 1D Gaussian by the column to obtain the 2D version<br/>g2d = tf.matmul(tf.reshape(g1d, [n_values, 1]), tf.reshape(g1d, [1, n_values]))</span><span id="7d13" class="nn la it nj b gy ns np l nq nr"># To visualize it<br/>plt.imshow(g2d.eval())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2ec0644fae9117d4f7352f2ec0141ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*TtAMXR_mpYXntzXwSnycXw.png"/></div></figure><p id="bf73" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">查看我们的tf中包含的操作列表。图表</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f067" class="nn la it nj b gy no np l nq nr">ops = tf.get_default_graph().get_operations()<br/>print([op.name for op in ops])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/8ff8c5a1f1741addf82a14a67755ccea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FUz3SxSgIC38UNaWxbVKTQ.png"/></div></div></figure><h1 id="a979" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="849d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一如既往，我希望你<strong class="lt iu"> </strong>喜欢这篇文章，并且你已经学习了张量和张量流的基础知识以及它们是如何使用的<strong class="lt iu">。</strong></p><p id="12f0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><em class="or">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae ky" href="https://medium.com/@rromanss23" rel="noopener"> <em class="or">这里</em> </a> <em class="or">。</em></p><p id="8188" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><em class="or">如果你想了解更多关于机器学习和人工智能的知识</em> <strong class="lt iu"> <em class="or">请在Medium </em> </strong> <em class="or">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>