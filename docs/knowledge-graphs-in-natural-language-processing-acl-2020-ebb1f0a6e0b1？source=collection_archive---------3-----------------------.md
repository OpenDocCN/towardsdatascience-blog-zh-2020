# 自然语言处理中的知识图@ ACL 2020

> 原文：<https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1?source=collection_archive---------3----------------------->

## 2020 年中期的技术水平

这篇文章是为了纪念我们研究 NLP 和 Graph ML 在知识图驱动下的进步的系列文章发表一周年！🎂1️⃣
观众的反馈驱使我继续，所以系好安全带(也许在这一集酿造一些☕️):，我们正在看 KG 相关的 ACL 2020 进程！

![](img/7649cf83e6c71a8398b2da4f5cc70e86.png)

ACL 2020 今年实现了完全虚拟化，我无法想象主席们组织如此庞大的在线活动以满足多个时区和 700 多份已接受论文的需求有多困难。感谢所有参与者以及发言者和与会者，鉴于会议的规模，会议进行得很顺利👏

那么与 ACL 2019 相比，KG & NLP 领域有什么变化吗？是啊！
我将今年的贡献总结为:

> 知识图展示了在非结构化数据中揭示高阶相关性的更好能力

今天我们的议程是:

1.  [结构化数据问答](#1e21)
2.  [KG 嵌入:双曲线和超关系](#ce67)
3.  [数据转文本 NLG:准备好你的变压器](#3bf6)
4.  [对话式人工智能:改进面向目标的机器人](#b183)
5.  [信息提取:OpenIE 和链接预测](#c012)
6.  [结论](#cf78)

# 结构化数据上的问题回答

在这种情况下，问题是针对结构化数据源发送的，比如基于 SPARQL 的 kg 或 SQL 数据库(其他查询语言并不那么突出)。
今年，我们可以观察到越来越多的人加入了复杂(也称为多跳)问题。

例如， [**Saxena 等人**](https://www.aclweb.org/anthology/2020.acl-main.412.pdf) 在他们的 *EmbedKGQA 中解决了复杂 KGQA 耦合 KG 嵌入与问题嵌入向量的问题。* 1️⃣首先，一个底层的 KG 嵌入了某种算法(作者选择 [ComplEx](http://proceedings.mlr.press/v48/trouillon16.pdf) )，这样每个实体和关系都与一个特定的向量相关联。在某些情况下，作者冻结它们，或者根据 KG 大小保持微调。2️⃣输入的问题通过 RoBERTA(来自最后一层的[CLS]令牌)编码，并通过 4 个 FC 层(如果你问为什么是 4，我没有答案，看起来像一个幻数🧙‍♂️)传递，这些层应该将问题投影到复空间。3️⃣关键部分发生在评分中，作者采用 KG 嵌入框架并构建了一个*三元组* **(头实体、问题、候选实体)**。评分功能与 ComplEx 中的相同。这里，头部是问题中的主要实体，问题本身被认为是一个关系(虽然看起来有点牵强)，候选实体要么是 KG 中的所有实体(如果很小)，要么是头部周围的 2 跳子图(当需要修剪时)。是的，它确实类似于用于训练 KGE 算法的典型的 1-N 评分机制。✂️通过计算问题嵌入`h_q`和每个关系嵌入`h_r`之间的点积`(h_q, h_r)`并对其进行阈值处理，可以进一步修剪候选空间。

🧪在 MetaQA 和 WebQuestionsSP 上进行的实验中，作者探索了一个不完整 KG 随机删除 50%边的附加场景，因此系统必须学会推断这种丢失的链接。在完整的场景中， *EmbedKGQA* 的表现与 PullNet 不相上下(在 3 跳问题上略胜一筹),当不使用额外的文本来增加 KG 时，绝对命中@1 分数比基线高 10–40%。👏
尽管如此，检查 EmbedKGQA 如何处理需要聚合或有几个基础实体的问题还是很有趣的🤔。

![](img/7e35b5bd0cb1cae3c97f6cb66864dcf7.png)

EmbedKGQA。来源: **Saxena 等人**

另一方面， [**兰等人**](https://www.aclweb.org/anthology/2020.acl-main.91.pdf) 提出使用迭代的基于 RL 的查询生成方法(KG-free😉 ).基于主题实体(通过一些实体链接器获得，作者借助 Google KG API 链接到 Freebase)，有一组 3 个操作，即**扩展**、**连接**和**聚集**，它们被应用于构建查询模式的种子实体。自然，这些操作允许具有最小/最大聚合的复杂多跳模式。🔦在每一步，作者使用波束搜索来保留 K 个最佳模式，并通过导出一个 7d 特征向量，然后是一个带有 softmax 的前馈网络来对它们进行排序。🤨你会问:“等等，伯特在哪里？现在每个人都在用伯特！”。好了，不要惊慌，开始吧:参与查询图的实体和关系的表面形式被线性化，与输入问题连接，并馈入 BERT，以获得最后一层的[CLS]表示(这是 7d 特性之一)。

🌡作者在 ComplexWebQuestions、WebQuestionsSP 和 ComplexQuestions(看起来有点像是为 Freebase 量身定制的，不是吗？)，并发现比基线有明显的改进。进一步的消融显示了 3 个选择的操作者的重要性。这里有一个悬念:这是一篇简短的论文！👀我推荐这篇论文作为一篇短文的范例，它传达了主要思想，展示了实验，并证明了消融方法的有效性👍

![](img/f6280812878de1fc22772f03c576df91.png)

来源: [**兰等人**](https://www.aclweb.org/anthology/2020.acl-main.91.pdf)

📑结构化问答还包括对 SQL 表的语义解析，许多新的复杂数据集推动了 SQLandia 的研究。

![](img/5b408b1c8d5a40553c95f2bfea0b54ad.png)

来源: [**王等人**](https://www.aclweb.org/anthology/2020.acl-main.677.pdf)

在其他人当中，我将概述 [**王等人**](https://www.aclweb.org/anthology/2020.acl-main.677.pdf) 和他们的 RAT-SQL(关系感知转换器，不🐀)中，它们定义了列和表之间的显式边缘，以对数据库模式进行编码。作者还定义了一个初始模式和值链接来获得候选列和表。此外，列、表和问题标记通过修改后的自我关注层共同传递。➡️最后，树形结构的解码器构建一个 SQL 查询。
RAT-SQL 在 [Spider](https://yale-lily.github.io/spider) 中表现出了巨大的改进，在使用 BERT 进行令牌的初始嵌入时获得了更大的收益📈

![](img/df4bf38f2e368529ee936af692a53c8d.png)

来源: [**埃尔戈哈里等人**](https://www.aclweb.org/anthology/2020.acl-main.187.pdf)

通常，在与语义解析系统交互时，您会希望即时解决一些小问题🔴解析器出错了。 [**Elgohary 等人**](https://www.aclweb.org/anthology/2020.acl-main.187.pdf) 正是针对这个问题，提出了 [SPLASH](https://github.com/MSR-LIT/Splash) ，一个用自然语言反馈纠正 SQL 语法分析的数据集。纠正场景不同于对话式 text2SQL 任务，因此，即使是最近的 SOTA 模型，如 EditSQL，与人工注释者相比，在纠正任务中也存在很大差距，即 25%对 81%。那是*相当大的差距*👀。
[**曾等人**](https://www.aclweb.org/anthology/2020.acl-demos.24.pdf) 开发了 [Photon](http://www.naturalsql.com/) 这是一个成熟的文本到 SQL 的系统，也能够执行一些查询修正😉

# KG 嵌入:双曲线和超关系

双曲空间是 ML 最近的热门话题之一。克制🤯简单来说，在双曲空间中🔮(由于它的属性)您可以更有效地表示层次结构和树状结构，同时使用更少的维度！

![](img/f0181ece0e5b26ac938d41aac7d4f7d8.png)

来源: [**横山雅美等人**](https://www.aclweb.org/anthology/2020.acl-main.617.pdf)

出于这种动机， [**横山雅美等人**](https://www.aclweb.org/anthology/2020.acl-main.617.pdf) 提出了 *AttH* ，一种双曲 KG 嵌入算法，使用旋转、反射和平移来模拟 KG 中的逻辑和层次模式。 *Att* 来自应用于旋转和反射向量的双曲线注意。🎩绕过不稳定的黎曼优化的诀窍是使用切空间，可以将 *d* 维庞加莱球的每个点映射到切空间。在这个明显不平凡的设置中，每个关系不仅与一个向量相关，还与描述特定于关系的反射和旋转的参数相关。尽管如此，在现实世界的 KGs `R << V`中，开销并不是很大。

⚖️在实验中， *AttH* 在 WN18RR 和 yago 3–10 上表现特别好，表现出一些层次结构，在 FB15k-237 上的余量较小。更重要的是，仅仅 **32 维** *AttH* 在真实和复杂的平面中显示出与 32-d 模型相比的巨大余量。此外，在 WN18RR 和 FB15k-237 上，32-d 分数仅比 SOTA 500-d 嵌入模型小 0.02-0.03 个 MRR 点。消融研究证明了具有可学习曲率的重要性，而其最接近的匹配 [MurP](https://papers.nips.cc/paper/8696-multi-relational-poincare-graph-embeddings.pdf) 将曲率固定。

![](img/ed95b44a5c0c60478a8b4cda055a9538.png)

来源:[罗索等人](https://exascale.info/assets/pdf/rosso2020www.pdf)。发布于 WebConf 2020。

图表示学习中的另一个增长趋势是超越由三元组组成的简单 kg，学习更复杂的、*超关系*kg(如 [**Rosso 等人**](https://exascale.info/assets/pdf/rosso2020www.pdf) 的工作中所创造的)的表示，此时每个三元组可能有一组键-值属性对，这些属性对给出了关于三元组在各种上下文中的有效性的细粒度细节。事实上，Wikidata 在其 Wikidata 语句模型中采用了超关系模型，其中属性被称为*限定符*。重要的是**而不是**将模型与 n 元事实(生成冗余谓词)和超图混合在一起。也就是说，如果你只在三个层次上使用维基数据，你会丢失一半的内容😃

[**关等人**](https://www.aclweb.org/anthology/2020.acl-main.546.pdf) 不想失去大半个维基数据，提出了 *NeuInfer* ，一种学习超关系 kg 嵌入的方法(他们之前的工作 [NaLP](https://www.researchgate.net/profile/Saiping_Guan/publication/333060086_Link_Prediction_on_N-ary_Relational_Data/links/5cdda342299bf14d959f3863/Link-Prediction-on-N-ary-Relational-Data.pdf) ，更适合 n 元事实)。

![](img/f6f8b3cb291d37531badcf6826ca9a52.png)

来源: [**关等人**](https://www.aclweb.org/anthology/2020.acl-main.546.pdf)

*NeuInfer* 的想法是计算一个超相关事实的**有效性**和**兼容性**得分(参见图示)。首先，`(h,r,t)`嵌入被输入到一个完全连接的网络(FCN)中，以估计这个三元组的可能性(有效性)。其次，对于每个键值对，构建一个五元组`(h,r,t,k,v)`,并通过另一组 fcn 传递。有了 *m 个*对， *m 个*向量被最小池化，结果表示兼容性得分，即那些限定词与主三元组相处得有多好。最后，作者使用两个分数的加权和得到最终分数。

作者在标准基准 JF17K(提取自 Freebase)和 WikiPeople(来自 Wikidata)上评估了 *NeuInfer* ，并报告了 JF17K 与 NaLP 相比在预测头部、尾部和属性值时的显著改进。📈我鼓励作者将他们的数字与 HINGE(来自 [**Rosso 等人**](https://exascale.info/assets/pdf/rosso2020www.pdf) )进行比较，因为这两种方法在概念上是相似的。

💡现在我们需要谈谈。我们需要谈谈甚至在 ACL 2019 这样的顶级会议上发表的 KG 嵌入算法的**再现性**。[此外，他们表明，他们的性能指标得分(如命中@K 和 MRR)取决于有效三元组在采样否定中的位置(这实际上不应该发生)。另一方面，现有的强基线表现完全一样，不管任何位置。要点是使用评估协议，将有效的三元组放在否定中的随机位置。](https://www.aclweb.org/anthology/2020.acl-main.489.pdf)

![](img/5f34793f2cbe1d3b1eb46d5beb02f831.png)

来源: [**孙、瓦希思特、桑亚尔等人**](https://www.aclweb.org/anthology/2020.acl-main.489.pdf)

[开始一场无耻的自我推销😊]嗯，我们的团队对这个问题也有话要说:在我们的新论文 [*“将光明带入黑暗:统一框架下知识图嵌入模型的大规模评估”*](https://arxiv.org/pdf/2006.13365) 中，我们进行了 65K+实验，并花费了 21K+ GPU 小时评估了 19 个模型，从 2011 年首次发表的 RESCAL 到 2019 年末的 RotatE 和 TuckER，5 个损失函数，各种带/不带负采样的训练策略，以及更多的超参数我们还为你们和我们亲爱的社区发布了所有模型的最佳超参数🤗。此外，我们正在发布 [PyKEEN 1.0](https://github.com/pykeen/pykeen) ，这是一个 PyTorch 库，用于训练和基准测试 KG 嵌入模型！[自我推销结束]

🔥我鼓励你通读其他几部作品: [**萨昌**](https://www.aclweb.org/anthology/2020.acl-main.238.pdf) 通过离散化研究 KG 实体嵌入的*压缩*问题，例如，“巴拉克·奥巴马”而不是 200d float32 向量将被编码为“2–1–3–3”，而“米歇尔·奥巴马”将被编码为“2–1–3–2”。

![](img/561ab85aff5c892d24c5ea63732edb76.png)

使用(液压)压缩节省 CD 空间的示例

也就是你只需要一个 K 值的 D 长向量(这里 D=4，K=3)。对于离散化，发现回火的 softmax 效果更好。作为从 KD 代码到 N 维浮点向量的反函数，作者建议使用简单的双 LSTM。实验结果令人吃惊👀—**FB15k-237 和 WN18RR 的压缩率**达到**100–1000 x**，推理时(当 KD 代码必须被解码回来时)性能下降和计算开销可以忽略不计(最大 2% MRR)。🤔我建议我们都坐下来，重新思考我们的 KGE 管道(特别是在生产场景中)。例如，通过 [PyTorch-BigGraph](https://github.com/facebookresearch/PyTorch-BigGraph) 获得的 7800 万维基数据实体的 200d 嵌入需要👉110 GB👈空间。想象一下，轻轻的 100 倍压缩会是什么样子😏。

还有一系列工作改进了流行的 KGE 模型:
* [**唐等人**](https://www.aclweb.org/anthology/2020.acl-main.241.pdf) 将[旋转](https://arxiv.org/pdf/1902.10197.pdf)从 2D 旋转推广到高维空间，其中正交关系变换对于 1-N 和 N-N 关系更有效。
* [**徐等**](https://www.aclweb.org/anthology/2020.acl-main.358.pdf) 通过将密集向量分块为 K 个部分，将双线性模型推广为多线性。然后表明，如果 K=1，该方法等于 [DistMult](https://arxiv.org/pdf/1412.6575.pdf) ，如果 K=2，该方法简化为[复](https://arxiv.org/pdf/1606.06357.pdf)和[孔](https://arxiv.org/pdf/1510.04935.pdf)，作者对 K=4 和 K=8 进行了实验。谢等人通过用计算机视觉领域著名的 [Inception](https://arxiv.org/pdf/1512.00567.pdf) 网络中的滤波器替换标准的 conv 滤波器来扩展 ConvE。
* [**Nguyen 等人**](https://www.aclweb.org/anthology/2020.acl-main.313.pdf) 应用自关注式编码器和 CNN 解码器进行三重分类和搜索个性化任务。

# 数据到文本 NLG:准备好你的变压器

![](img/38f56e77c53005fc3529c00fbd4386f4.png)

WebNLG 2017 的 RDF 转文本任务示例。来源: [**赵等人**](https://www.aclweb.org/anthology/2020.acl-main.224.pdf)

随着 KGs(以及一般的结构化数据)在 NLP 中被广泛采用，在 2020 年，我们可以观察到自然语言生成(NLG)方法的激增，这些方法采用一组 RDF 三元组/一个 AMR 图/一组表格单元，并产生连贯的人类可读文本，如描述或问题。

顺便说一下，目前的 RDF-to-text 方法仅在 WebNLG 2017 数据上进行评估，但有一个新的挑战， [WebNLG 2020](https://webnlg-challenge.loria.fr/challenge_2020/) ！🎉如果你喜欢 NLG，一定要参加😉

一条推文详尽总结了今年的 NLG 趋势:

自然语言处理中的 SOTA 算法综述

老练的策划者和执行者？一些结构排列？不😅。只要旋转你最喜欢的预先训练的 LM。

🤔事实上，插入一个预先训练好的 LM 并给它一些例子确实有效。陈等人在配有 2 解码器的实验台上演示了这一现象。也就是说，表格单元首先通过可学习的 LSTM 编码器来获得复制机制的隐藏状态。另一方面，文本以冻结的重量进入 GPT-2。顶部的✍️The 复制机制有助于保留表格单元格中的稀有标记。WikiBio 上的实验表明，少至 **200 个**训练样本就足以生成比复杂的强基线好得多的文本。猜猜 GPT 3 号需要多少😏

![](img/6ba98ecab705b39e5c6afae202861817.png)

来源: [**陈等人**](https://www.aclweb.org/anthology/2020.acl-main.18.pdf)

![](img/3920f26b0961dc842cb116efa28fd484.png)

来源: [**陈等人**](https://www.aclweb.org/anthology/2020.acl-main.708.pdf)

继续使用表格，[陈等人](https://www.aclweb.org/anthology/2020.acl-main.708.pdf) 构建一个新的数据集， *LogicNLG* ，这需要在标准文本生成的基础上使用额外的逻辑。例如(参见插图)，需要一些比较和计数操作来包含像“多一枚金牌”或“最多金牌”这样的部分，使文本更加自然和生动🌼。数据集的基线模型使用预训练的 GPT-2 和 BERT，但看起来 LMs 仍有一些改进的空间。

在图形到文本的领域中， [**宋等人**](https://www.aclweb.org/anthology/2020.acl-main.712.pdf) 应用了稍微修改的变换器编码器，该编码器显式地处理关系表面形式。输入只是一个线性化的图(可以用 DFS 来构建)。但是解码器保持不变。🎩该方法的关键部分是添加(连同标准 LM 损失)两个自动编码损失，这两个损失被设计成更好地捕捉动词化图形的结构。第一个损失重建三重关系，而另一个损失重建线性化输入图的节点和边标签。在 AMR 和 RDF 图(WebNLG)上进行的🧪实验表明，仅仅将这两个损失相加就可以产生大约 2 个 BLEU 点。

![](img/30be1cce68b703cdcffe3c9c4a34b5cb.png)

辅助损失试图重建视图 1 和视图 2。来源: [**宋等人**](https://www.aclweb.org/anthology/2020.acl-main.712.pdf)

![](img/45f63cfca682a7689a325c75c704a260.png)

不要隐藏你的痛苦，退休吧！

🗒在这一点上，我应该做一个小小的评论[每个人都应该停止使用 BLEU 来评估 NLG 质量](https://www.aclweb.org/anthology/2020.acl-main.448.pdf)(ACL 20 篇论文的最佳提名之一，我相信他们)。WebNLG 2020 的组织者非常赞同这一观点，因为他们除了经典(或者我们应该说过时了？)度量。此外，在 ACL'20 上，提出了一种新的度量标准， [BLEURT](https://www.aclweb.org/anthology/2020.acl-main.704.pdf) ，并证明它与人类的判断更相关。让我们投资于这些新的评估指标，让 ol' BLEU 休息一下吧🏖

🌳尽管如此，没有(或者至少没有那么多)变形金刚，世界上还是有生动的生活！[、【赵】等人、 *DualEnc* 提出了一种编码-规划-解码模型，应用于图形到文本的任务。1️⃣首先，输入图被预处理以将关系转换成显式节点。因此，有几个诱导标记边缘`s->p, p->s, p->o, o->p`。然后，通过 R-GCN 对图进行编码，以获得实体和关系嵌入。2️⃣相同的图通过另一个 R-GCN 编码，具有显示关系是否已经被使用的附加特征。计划按以下方式构建:当存在未访问的关系时，softmax 选择最可能的关系，然后将该关系附加到计划中。一旦序列准备好了，它就扩展了与主体和客体的那些关系。最后，产生的序列通过 LSTM 编码。图形编码和平面编码都被送入解码器以生成输出。
📏实验表明:1)*DualEnc*在规划构建中的不可见测试集上表现出很好的泛化能力，2)文本生成质量优于直接应用的 transformers，3)规划阶段的大加速，即 2019 年 SOTA 需要 250 秒来解决**一个** 7-triple 实例，而 *DualEnc* 在 10 秒内解决所有 4928 个实例🚀](https://www.aclweb.org/anthology/2020.acl-main.224.pdf)

![](img/df9e099eb7271eccdee8478d3e292894.png)

来源: [**赵等人**](https://www.aclweb.org/anthology/2020.acl-main.224.pdf)

最后，让我们从数据到文本再到总结。在抽象概括领域 [**黄等人****在他们的 *ASGARD* 方法中采用从文档构建的 kg 来增强生成过程。**](https://www.aclweb.org/anthology/2020.acl-main.457.pdf)

**![](img/40d4da1507b95b406c52a3d620d44798.png)**

**具体来说，编码器由两部分组成。1️⃣·罗伯塔用于对输入段落进行编码。最终的层嵌入被馈入 BiLSTM 以获得隐藏状态。2️⃣ OpenIE 用于从输入文档中提取三元组并归纳出一个图。关系的令牌被转换成类似于 *DualEnc* 的显式节点，初始节点状态取自步骤 1 的 BiLSTM。然后，使用图注意网(GAT)通过一个读出函数来更新节点状态，从而得到一个图上下文向量。3️⃣:生成过程受步骤 1 和 2 中获得的两个向量的制约。**

**🧙‍♂️Some 魔法在训练中发生: *ASGARD* 求助于强化学习，其中奖励函数取决于胭脂和完形填空分数。完形填空部分包括从人类书写的摘要中提取开放图形，并基于它们生成完形填空式问题，以便系统更好地学习摘要文档的含义。📦所以你在里面有一个质量保证模型！作者为 CNN 和 NYT 数据集生成了 1M+的完形填空题。📏实验报告比以前的基线有所改进！然而，一致的赢家是一个预先训练好的 BART，它在目标数据集上进行了微调😅好吧，看起来“TPUs go brrr”策略在这里也行得通。**

# **对话式人工智能:改进面向目标的机器人**

**在 ConvAI 领域，我有点偏向于面向目标的系统，因为 KGs 和结构化数据自然地扩展了它们的功能。**

**![](img/75ae789325f346edf4bb039bd7d9802e.png)**

**来源: [**坎帕尼亚等人**](https://www.aclweb.org/anthology/2020.acl-main.12.pdf)**

**🔥首先， [**Campagna 等人**](https://www.aclweb.org/anthology/2020.acl-main.12.pdf) 提出了一种合成面向目标的对话的方法，作为对话状态跟踪(DST)任务的附加训练数据。作者创建了一个抽象模型(人们也可以将其命名为**本体**，它定义了基本状态、动作和转换。为什么这很酷:1️⃣该模型可以应用于各种领域，如餐厅预订或火车连接搜索与任何插槽和价值；2️⃣合成数据允许在监督数据非常有限的领域进行零发射传输。3️⃣事实上，实验表明，仅使用*和*合成的语料库进行训练(并在真实的 MultiWoz 2.1 测试上进行评估)达到了原始完整训练集的大约 2/3 的准确度💪
我相信这种方法可以作为一种通用的数据扩充实践，用于开发特定领域的对话系统，或者用于有限的带注释的训练数据。**

**![](img/28e15b76fd9c384a8b65270b594b8e54.png)**

**当新的基于朋友的数据集到达时**

**针对对话中的关系抽取， [**于等人**](https://www.aclweb.org/anthology/2020.acl-main.444.pdf) 开发了*dialoge*，一个新的数据集，由来自朋友的约 2k 个对话中的 36 个关系组成。虽然这些关系没有用维基百科或 URIs 数据库进行注释，但即使对伯特来说，数据集仍然是一个相当大的挑战。此外，作者提出了一个新的度量标准，它显示了一个系统需要多少回合来提取某个关系。**

**[OpenDialKG](https://pdfs.semanticscholar.org/0d3c/68c207fc83fb402b7217811af22066300fc9.pdf) 是 ACL 2019 年最佳论文奖提名者之一，因为他在新的数据集中促进了对话系统中基于图形的推理。 [**周等人**](https://www.aclweb.org/anthology/2020.acl-main.635.pdf) 在他们新的适合中国人的 *KdConv* 数据集上很好地采用了 OpenDialKG 的主要思想👏**

**还有一系列研究如何将外部知识融入端到端对话系统的作品。如果你的背景知识表示为文本三元组或表格单元格(甚至是纯文本)，那么 [**林等人**](https://www.aclweb.org/anthology/2020.acl-main.6.pdf) 建议使用 Transformer 作为知识编码器，而 [**秦等人**](https://www.aclweb.org/anthology/2020.acl-main.565.pdf) 则依靠记忆网络式编码器。如果你有像 ConceptNet 这样的常识 KG， [**张等人**](https://www.aclweb.org/anthology/2020.acl-main.184.pdf) 从话语中提取概念构建局部图，然后采用编码器对对话的“中心概念”进行编码，这将影响解码器。如果你对更近期的康威产品感兴趣，一定要查看康威工厂的 [NLP 会议记录，该工厂与 ACL 位于同一地点(实际上)!](https://sites.google.com/view/2ndnlp4convai/home)**

# **信息抽取:OpenIE 和链接预测**

**如果您的工作恰好是从原始文本文档构建 kg，那么您可能已经知道 OpenIE 是一个事实上的标准。正如我们在前面章节中看到的，像 OpenIE4 或 OpenIE 5 这样的基于规则的框架仍然在被积极地使用。也就是说，提高 OpenIE 提取的质量可以缓解 KG 构建中的许多问题。小注意:打开 IE 后获得的 kg 也叫*打开 kg*。**

**[**Kolluru 等人**](https://www.aclweb.org/anthology/2020.acl-main.521.pdf) 提出生成式 OpenIE 方法， *IMoJIE(基于迭代记忆的联合信息提取)*。受 CopyAttention 范式的启发，作者提出了一种迭代生成 seq2seq IE 算法:在每次迭代中，原始序列与先前提取的序列连接，并通过 BERT 获得最终嵌入。然后，具有复制和注意机制的 LSTM 解码器负责生成新的提取(例如，包含三元组的记号)。🤖为了进一步改进训练集，作者将 OpenIE 3、OpenIE 4 和其他系统的输出汇总并排序为“银标签”以供生成。

虽然架构看起来非常简单，但是*确实带来了显著的改进📈与现有基线相比。消融研究报告称，BERT 对整体质量至关重要，因此我假设可以通过插入更大的变压器或使用特定领域的预训练 LM 来进一步提高质量，例如，如果您的文本来自法律或生物医学领域。***

**![](img/e587e620a50a9a5eef3e152ae4c962b1.png)**

**来源: [**Kolluru 等人**](https://www.aclweb.org/anthology/2020.acl-main.521.pdf)**

**虽然在类似 RDF 的 kg 上的链接预测(LP)已经有了良好的记录和几个里程碑，但我们不能说开放 kg 上的 LP 也是如此。**

**![](img/31f05e5713a9379db1af565040e77d2a.png)**

**来源:**

**但是现在我们可以了！🤩 [**Broscheit 等人**](https://www.aclweb.org/anthology/2020.acl-main.209.pdf) 定义了开放链接预测的任务，给出了开放 kg 的挑战:**

*   **给定一个*('主题文本'，'关系文本'，？)*查询，系统必须预测真实的、新的、不能被平凡解释的事实。**
*   **然而，没有实体或关系 URIs 可用，将许多表面形式绑定到一个表示。**
*   **尽管如此，同一实体或关系的各种表面形式可能构成测试集泄漏，因此必须仔细构建和清理测试集。**

**作者提出了如何构建和清理数据集的方法、评估协议和基准本身。 *OLPBench* 是具有 KG 嵌入的 LP 的最大数据集之一:它包含 3000 万个三元组，100 万个不同的开放关系，以及 80 万个唯一实体的 250 万个提及👀。在实验中，作者使用了 ComplEx，其中多标记提及通过 LSTM 进行聚合。公开 LP 任务结果是*非常*困难😯:即使强大的 768d 复合体也仅产生 3.6 个 MRR，1 时 2 次命中，10 时 6.6 次命中。
显然，这是一个非常具有挑战性的数据集:非常有趣的是，看到这些方法不仅可以扩展到如此大的图形，还可以将性能提高到类似 FB15k-237 的数字(仅供参考，目前大约是 35 个 MRR 点和 55 次点击@10)。**

**顺便说一句，如果你对从文本中构建知识更感兴趣，我鼓励你查看最近的 [AKBC 2020](https://www.akbc.ws/2020/papers/) 会议的记录，这次会议吸引了一大批演讲者和出版物👏**

# **结论**

**今年在 ACL'20 上，我们看到了更少的 KG 增强语言模型(但我们确实看到了设计用于表格的 [TaPas](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html) 和 [TABERT](https://ai.facebook.com/blog/tabert-a-new-model-for-understanding-queries-over-tabular-data/) ),可能 NER 也少了一些。另一方面，图形到文本的 NLG 正在崛起！亲爱的读者，你还是坚持到了最后，你应该得到一些掌声:)**

**![](img/e539eb59c8fa96fdd581a769d7776aa7.png)**

**请在评论中让我知道你喜欢什么，哪些地方需要改进。感谢阅读，敬请关注更多出版物😌**