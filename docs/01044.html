<html>
<head>
<title>An Introduction to Unity ML-Agents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Unity ML-agent 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-unity-ml-agents-6238452fcf4c?source=collection_archive---------0-----------------------#2020-01-30">https://towardsdatascience.com/an-introduction-to-unity-ml-agents-6238452fcf4c?source=collection_archive---------0-----------------------#2020-01-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="bc35" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="http://www.simoninithomas.com/unitymlagentscourse/" rel="noopener ugc nofollow" target="_blank"> Unity-ML 代理课程</a></h2><div class=""/><div class=""><h2 id="8307" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">训练一个强化学习代理跳过墙。</h2></div><p id="7442" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们推出了新的免费、更新、</strong> <a class="ae lk" href="https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">深度强化学习课程从初学者到专家，用拥抱面对🤗</strong> </a></p><p id="88e2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉新版教程:<a class="ae lk" href="https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/deep-rl-course/unit0/introduction</a></p><p id="cbeb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">下面的章节是以前的版本</strong>，新版本在这里👉<a class="ae lk" href="https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt </a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/aff3ceb9b288992e83cc328b2c818610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ad4wAl9tdou_twfI"/></div></div></figure><p id="c14b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们推出了新的免费、更新、</strong> <a class="ae lk" href="https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">从初学者到专家的深度强化学习课程，用拥抱面对🤗</strong>T19】</a></p><p id="8d60" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉新版教程:<a class="ae lk" href="https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/deep-rl-course/unit0/introduction</a></p><p id="5b31" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面的章节是以前的版本，新版本在这里👉<a class="ae lk" href="https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt </a></p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="26a2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">过去的几年见证了强化学习的突破。从 2013 年深度学习模型首次成功使用<a class="ae lk" href="https://deepmind.com/blog/article/deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> RL 从像素输入学习策略</a>到 2019 年<a class="ae lk" href="https://openai.com/blog/learning-dexterity/" rel="noopener ugc nofollow" target="_blank"> OpenAI 灵巧程序</a>，我们生活在 RL 研究的一个激动人心的时刻。</p><p id="d16e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，作为 RL 研究人员，我们需要<strong class="kq ja">创造越来越复杂的环境</strong>，Unity 帮助我们做到这一点。Unity ML-Agents toolkit 是一个基于游戏引擎 Unity 的新插件，允许我们使用 Unity 游戏引擎作为环境构建器来训练代理。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi me"><img src="../Images/fa1a360d12118928d458b03d82ee2fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RFg2Owe3izLATItE"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mj">来源:Unity ML-Agents Toolkit Github 存储库</em></p></figure><p id="f360" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从踢足球、学习走路、跳过高墙，到训练一只可爱的小狗去抓棍子，Unity ML-Agents Toolkit 提供了大量令人惊叹的预制环境。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="583d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，在这个免费课程中，我们还将创造新的学习环境。</p><p id="e8b0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将学习 Unity ML-Agents 是如何工作的，在文章的最后，您将训练一个代理学习翻墙。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/fb2bb0ee137e6a0ca9739f6b2d62bc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*E_iRSaQQnUmLEqzfeHm9sw.gif"/></div></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="9556" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是首先，有一些要求:</p><ul class=""><li id="06a6" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated"><strong class="kq ja">这不是强化学习入门课程</strong>，如果你之前没有深度强化学习的技巧，需要查看免费课程【Tensorflow 深度强化学习。</li><li id="5d0c" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">而且这不是一门关于 Unity 的课程，所以<strong class="kq ja">你需要有一些 Unity 基本功。如果不是这样，你绝对应该看看他们为初学者开设的令人惊叹的课程:<a class="ae lk" href="https://learn.unity.com/course/create-with-code" rel="noopener ugc nofollow" target="_blank">用代码创造。</a></strong></li></ul><p id="1f8a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以让我们开始吧！</p><h1 id="71f4" class="nb nc iq bd nd ne nf ng nh ni nj nk nl kf nm kg nn ki no kj np kl nq km nr ns bi translated">Unity ML-Agents 如何工作？</h1><h2 id="5d85" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">什么是 Unity ML-Agents？</h2><p id="0b42" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">Unity ML-Agents 是游戏引擎 Unity 的一个新插件，它允许我们<strong class="kq ja">创建或使用预先制作的环境来训练我们的代理。</strong></p><p id="001a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它是由 Unity Technologies 开发的，Unity 是有史以来最好的游戏引擎之一。这是 Firewatch，Gone Home，Cuphead 和许多 AAA 游戏的创作者使用的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/561b27e5c401998f7abe24b6f115fcb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1onn3DyZjtBqaFH9"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mj"> Firewatch 是用 Unity 做的</em></p></figure><h2 id="a68a" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">三个组成部分</h2><p id="4f89" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">使用 Unity ML-Agents，您有三个重要的组件。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/d5e2b50be983f42a9473e74b2182e5bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b0SOTtuAqC8Frdcn"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mj">来源:Unity ML-Agents 文档</em></p></figure><p id="ca1a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一个是<em class="oj">学习组件(关于 Unity)，</em>即<strong class="kq ja">包含 Unity 场景和环境元素。</strong></p><p id="ef62" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第二个是<em class="oj"> Python API </em>包含<strong class="kq ja"> RL 算法</strong>(比如 PPO 和 SAC)。我们使用这个 API 来启动培训、测试等。它通过<em class="oj">外部通信器与学习环境通信。</em></p><h2 id="3715" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">在学习组件内部</h2><p id="902e" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">在学习组件中，我们有不同的元素:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/70ccf2a5ff9bb982e0dbfa1b105a2821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZipQQrAR1-4f_-jW"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mj">来源:Unity ML-Agents 文档</em></p></figure><p id="12c1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先是经纪人，<strong class="kq ja">现场的演员。他就是我们要通过优化他的策略(这将告诉我们在每种状态下采取什么行动)来训练的人。</strong></p><p id="b0a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，还有学院，这个元素<strong class="kq ja">编排代理和他们的决策过程。把这个学院想象成处理来自 python API 的请求的大师。</strong></p><p id="a972" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了更好地理解它的作用，让我们回忆一下 RL 过程。这可以建模为一个循环，其工作方式如下:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ok"><img src="../Images/e2d6bb5deda3d76fe5d42f1fdca12d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TNVqhOP3Ak2OL1Vc"/></div></div></figure><p id="52a6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">来源:萨顿的书</p><p id="e002" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们想象一个代理学习玩一个平台游戏。RL 流程如下所示:</p><ul class=""><li id="3943" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">我们的代理从环境接收状态 S0—我们接收游戏的第一帧(环境)。</li><li id="f9c0" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">基于状态 S0，代理采取行动 A0 —我们的代理将向右移动。</li><li id="3f92" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">环境过渡到一个新的状态 S1。</li><li id="f251" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">给代理一个奖励 R1——我们没死<em class="oj">(正奖励+1) </em>。</li></ul><p id="b0e6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个 RL 循环输出状态、动作和奖励的序列。代理人的目标是<strong class="kq ja">最大化期望的累积报酬。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/18012ab67c8b89385abf30997b25c385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2qCWkFMw9JJv0TIF"/></div></div></figure><p id="0dc1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实上，学院<strong class="kq ja">将向我们的代理发送订单，并确保代理同步</strong>:</p><ul class=""><li id="9d90" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">收集观察结果</li><li id="d804" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">使用您的策略选择您的操作</li><li id="a50a" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">采取行动</li><li id="8e9b" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">如果达到最大步数或完成，则重置。</li></ul><h1 id="2a8f" class="nb nc iq bd nd ne nf ng nh ni nj nk nl kf nm kg nn ki no kj np kl nq km nr ns bi translated">训练一名特工翻墙</h1><p id="2f67" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">现在我们已经了解了 Unity ML-Agents 的工作原理，让我们来训练一个代理从墙上跳下来。</p><p id="8e11" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="oj">我们在 github 上发布了我们训练过的模型，你可以在这里下载</em> <a class="ae lk" href="https://github.com/simoninithomas/unity_ml_agents_course" rel="noopener ugc nofollow" target="_blank"> <em class="oj">。</em>T11】</a></p><h2 id="3862" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">跳墙环境</h2><p id="22e9" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">在这种环境下，我们的目标是训练我们的代理人走上绿区。</p><p id="be8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是，有 3 种情况:</p><ul class=""><li id="53ba" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">首先，你没有墙，<strong class="kq ja">我们的代理只需要走上绿色瓷砖。</strong></li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/6166296f548223cbb82d34819e3cfe34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KmPbgziBkWiH-vJ8"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">无墙情况</p></figure><ul class=""><li id="992f" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">在第二种情况下，<strong class="kq ja">代理需要学习跳跃</strong>以到达绿色瓷砖。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/e6a1fa51a9dc18092c98ad3d7cbff83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VxYDlRiRm0KOQ7qr"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">小墙情况</p></figure><ul class=""><li id="077c" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">最后，在最困难的情况下，我们的代理人将无法跳得和墙一样高，所以他需要推动白色方块以便跳到它上面来跳过墙。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/1417d99ee28c79717747a3c0f007981b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wRnLx721Rpc_WKQu"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">大墙情况</p></figure><p id="db43" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们将根据墙的高度学习两种不同的政策</strong>:</p><ul class=""><li id="ac75" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">第一个<em class="oj">小墙跳</em>将在无墙和矮墙情况下学习。</li><li id="9810" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">第二个，<em class="oj">大墙跳跃</em>，将在高墙情况下学习。</li></ul><p id="1eb7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">奖励制度是:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/2493c814962c43aab90a6da483ab12d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NLEfHq4gwL4T-6Fd"/></div></div></figure><p id="53a3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在观察方面，<strong class="kq ja">我们没有使用正常视觉</strong>(帧)，而是 14 个光线投射，每个光线投射可以探测 4 个可能的物体。把光线投射<strong class="kq ja">想象成激光，如果它穿过物体就会被探测到。</strong></p><p id="0734" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还使用代理的全球位置以及是否接地。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ol"><img src="../Images/38629d1a36b5aa58f3484c585786e85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GAohH0cfAscYgSSA"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mj">来源:Unity ML-Agents 文档</em></p></figure><p id="fbae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">动作空间是离散的，有 4 个分支:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/672ce6809286b788ae75b69c957f38fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LuP9CNzd5gNAhJCy"/></div></div></figure><p id="5c39" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们的目标是以 0.8 的平均回报达到基准<strong class="kq ja">。</strong></p><h2 id="d9ba" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">我们跳吧！</h2><p id="b462" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">首先我们打开<em class="oj"> UnitySDK 项目。</em></p><p id="9a90" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在示例中搜索<em class="oj"> WallJump </em>并打开场景。</p><p id="1039" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可以在场景中看到，许多代理，他们每个人都来自同一个预置，他们都共享同一个大脑。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/b5726cd9cfd48ffac3ccce7c0ed7f30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c6BuO99jjvRczeGk"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">同一个代理预置的多个副本。</p></figure><p id="0beb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实上，正如我们在经典的深度强化学习中所做的那样，当我们启动一个游戏的多个实例(例如 128 个并行环境)时，我们在此复制并粘贴代理，以便有更多不同的状态。</p><p id="d8d9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，首先，因为我们想从头开始训练我们的特工，<strong class="kq ja">我们需要从特工身上移除大脑。</strong>我们需要进入预置文件夹并打开预置。</p><p id="d741" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在在预设层级中，选择代理并进入检查器。</p><p id="b733" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，在行为参数中，我们需要移除模型。<strong class="kq ja">如果你有一些图形处理器，你可以把推理机从 CPU 改成 GPU。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/f66186ccdb36cb4375a11a00576de055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7GXjJ-fUxd1GKBZf"/></div></div></figure><p id="a621" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后在跳墙代理组件中，<strong class="kq ja">我们需要移除无墙脑、小墙脑和大墙脑情况下的大脑。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/0768b0dee983a529c2d83c09fb6728bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*euXX1sTjAJFvATm-"/></div></div></figure><p id="c129" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在你已经完成了，你可以从头开始训练你的代理了。</p><p id="8445" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于第一次训练，我们将只修改两个策略(SmallWallJump 和 BigWallJump) <strong class="kq ja">的总训练步骤，因为我们只需要 300k 的训练步骤就可以达到基准。</strong></p><p id="9ec3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为此，我们转到，您可以在<em class="oj">config/trainer _ config . YAML</em>中将 SmallWallJump 和 BigWallJump 情况下的这些步骤修改为<strong class="kq ja"> max_steps 至 3e5 </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/cf2b6236e23e8dbc39f57da9bdbc2bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z9DadBDCWJB0K8w3"/></div></div></figure><p id="d827" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了训练这个代理，<strong class="kq ja">我们将使用 PPO </strong>(近似策略优化)如果你不知道或者你需要刷新你的知识，<a class="ae lk" href="https://www.google.com/search?client=firefox-b-d&amp;q=ppo+simonini" rel="noopener ugc nofollow" target="_blank">查看我的文章。</a></p><p id="2143" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们看到，为了训练这个代理，我们需要使用 Python API 调用我们的外部通信器。该外部通信器<strong class="kq ja">将要求学院启动代理。</strong></p><p id="8bf1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，您需要打开您的终端，进入 ml-agents-master 所在的位置并键入以下内容:</p><pre class="lm ln lo lp gt om on oo op aw oq bi"><span id="4105" class="nt nc iq on b gy or os l ot ou">mlagents-learn config/trainer_config.yaml — run-id=”WallJump_FirstTrain” — train</span></pre><p id="c248" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它会要求你运行 Unity 场景，</p><p id="b5c9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">按下编辑器顶部的▶️按钮。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/1cba2e86437cbc656e63cde10e65039a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2S_sG1Ho9Z4ZgrLy"/></div></div></figure><p id="2e3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可以使用以下命令启动 Tensorboard 来监控您的训练:</p><pre class="lm ln lo lp gt om on oo op aw oq bi"><span id="c44e" class="nt nc iq on b gy or os l ot ou">tensorboard — logdir=summaries</span></pre><h2 id="2743" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">看着你的特工翻墙</h2><p id="1436" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">你可以在训练的时候通过观看游戏窗口来观察你的经纪人。</p><p id="9cc5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">培训结束后，您需要将保存在<em class="oj">ML-Agents-master/models</em>中的模型文件移动到<em class="oj">unity SDK/Assets/ML-Agents/Examples/wall jump/TF models</em>中。</p><p id="2ba2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">再次打开 Unity 编辑器，选择<em class="oj"> WallJump 场景</em>。</p><p id="15e0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">选择<em class="oj"> WallJumpArea </em>预设对象并打开它。</p><p id="1786" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">选择<em class="oj">代理</em>。</p><p id="518a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在代理<em class="oj">行为参数</em>中，拖动<em class="oj"> SmallWallJump.nn </em>文件到模型占位符中。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/dc2a1c38c9ebe250dff553e1abb37ddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F5wrmYDcuygMOLlN"/></div></div></figure><p id="1fcb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将<em class="oj"> SmallWallJump.nn </em>文件拖到无墙脑占位符。</p><p id="5387" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将<em class="oj"> SmallWallJump.n </em> n 文件拖到小墙脑占位符中。</p><p id="e272" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将<em class="oj"> BigWallJump.nn </em>文件拖到无墙脑占位符。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/4e74699134a6a0f2e1a3cc1a4441b422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mAQsn4naWV4H__3T"/></div></div></figure><p id="a7b3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，按下编辑器顶部的▶️按钮，瞧！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/fb2bb0ee137e6a0ca9739f6b2d62bc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*E_iRSaQQnUmLEqzfeHm9sw.gif"/></div><p class="mf mg gj gh gi mh mi bd b be z dk">🎉</p></figure><h1 id="d4f7" class="nb nc iq bd nd ne nf ng nh ni nj nk nl kf nm kg nn ki no kj np kl nq km nr ns bi translated">是时候做些实验了</h1><p id="1d99" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">我们刚刚训练我们的特工学会了翻墙。既然我们有了好的结果，我们可以做一些实验。</p><p id="d462" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">记住,<strong class="kq ja">最好的学习方法是通过实验保持活跃。</strong>所以你要试着做一些假设，并验证它们。</p><h2 id="7269" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">将贴现率降至 0.95</h2><p id="67c0" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">我们知道:</p><ul class=""><li id="8019" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated"><strong class="kq ja">γ越大，折扣越小。</strong>这意味着学习代理更关心长期回报。</li><li id="4de5" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">另一方面，<strong class="kq ja">伽玛越小，折扣越大。这意味着我们的代理更关心短期回报。</strong></li></ul><p id="5fcb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个实验背后的想法是，如果我们通过将 gamma 从 0.99 降低到 0.95 来增加折扣，我们的代理将会更关心短期回报，也许这将<strong class="kq ja">帮助他更快地收敛到最优策略。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/1135b7719ee1f664ed96437db8758901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6qBdBFeID_a3mgqu"/></div></div></figure><p id="a780" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有趣的是，我们的代理人<strong class="kq ja">在小墙跳跃的情况下表现完全相同，</strong>我们可以解释说，这种情况很容易，如果有小墙，他只需要向绿色网格移动并跳跃。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/3d14ec53f092e3fc8e10ea8e9c84fa47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rPmcHnqpCI39Kw-5"/></div></div></figure><p id="dd88" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，<strong class="kq ja">在大墙跳的情况下表现真的很差。</strong>我们可以解释说，因为我们的新代理<strong class="kq ja">更关心短期回报，他无法进行长期思考</strong>，因此没有真正理解他需要推动白砖才能跳过墙。</p><h2 id="df52" class="nt nc iq bd nd nu nv dn nh nw nx dp nl kx ny nz nn lb oa ob np lf oc od nr iw bi translated">增加了神经网络的复杂性</h2><p id="6442" class="pw-post-body-paragraph ko kp iq kq b kr oe ka kt ku of kd kw kx og kz la lb oh ld le lf oi lh li lj ij bi translated">对于第三次也是最后一次培训，假设是<em class="oj">如果我们增加网络复杂性，我们的代理会变得更聪明吗？</em></p><p id="5d97" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们所做的是将隐藏单元的大小从 256 增加到 512。</p><p id="fbfb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是我们发现这个新代理<strong class="kq ja">的表现比我们的第一个代理差。</strong></p><p id="4781" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这意味着当我们遇到这种简单的问题时，我们不需要增加网络的复杂性，因为这增加了收敛之前的训练时间。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/ae3f15e6e069b2b3d8db57930faea81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pa0ngx8YmPKD0y-Z"/></div></div></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="34d9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">今天就到这里吧！你刚刚训练了一个学会翻墙的特工。厉害！</p><p id="7d3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">别忘了<strong class="kq ja">去实验，换一些超参数试试新闻的东西。玩得开心！</strong></p><p id="efe9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="oj">如果你想和我们的实验进行比较，我们在这里公布了我们训练过的模型</em> <a class="ae lk" href="https://github.com/simoninithomas/unity_ml_agents_course" rel="noopener ugc nofollow" target="_blank"> <em class="oj">。</em>T29】</a></p><p id="1857" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在下一篇文章中，我们将训练一个<strong class="kq ja">智能代理，它需要按下一个按钮来生成一个金字塔，然后导航到金字塔，撞倒它，并移动到顶部的金砖。为了做到这一点，除了外在奖励，我们还将好奇心作为内在奖励。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/84bfe27f741af9329fe38edac34bbbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*yZ2JXbtge9gPE4Axz9jv4w.gif"/></div></figure><p id="3e02" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你有任何想法，评论，问题，欢迎在下面评论或者给我发邮件:hello@simoninithomas.com，或者发推特给我。</p><p id="2941" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">不断学习，保持牛逼！</p><p id="1d14" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第 2 章:<a class="ae lk" rel="noopener" target="_blank" href="/diving-deeper-into-unity-ml-agents-e1667f869dc3">深入探究 Unity-ML 代理</a></p><p id="e614" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第三章:<a class="ae lk" rel="noopener" target="_blank" href="/unity-ml-agents-the-mayan-adventure-2e15510d653b">玛雅奇遇</a></p></div></div>    
</body>
</html>