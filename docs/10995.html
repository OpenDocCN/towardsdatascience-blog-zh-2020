<html>
<head>
<title>Web scraping with Scrapy: Practical Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Scrapy 抓取网页:实践理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-with-scrapy-practical-understanding-2fbdae337a3b?source=collection_archive---------6-----------------------#2020-07-31">https://towardsdatascience.com/web-scraping-with-scrapy-practical-understanding-2fbdae337a3b?source=collection_archive---------6-----------------------#2020-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="bbc1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">网刮系列</h2><div class=""/><div class=""><h2 id="0a22" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">与 Scrapy 一起动手</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/d2a621ddbb16e26afc861c6d83198cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tH0dc9iLgfd2k9cWnG6ADQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@ilyapavlov?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">伊利亚·巴甫洛夫</a>在<a class="ae le" href="https://unsplash.com/s/photos/web-scraping?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9b7d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在<a class="ae le" href="https://medium.com/@karthikn2789/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd" rel="noopener"> <strong class="lh ja"> <em class="mb">第 1 部分</em> </strong> </a>中讨论了使用 Scrapy 的所有理论方面，现在是时候给出一些实际例子了。我将把这些理论方面放到越来越复杂的例子中。有 3 个例子，</p><ul class=""><li id="a579" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">一个通过从天气站点提取城市天气来演示单个请求和响应的例子</li><li id="4dd0" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">一个通过从虚拟在线书店提取图书详细信息来演示多个请求和响应的示例</li><li id="d497" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">一个演示图像抓取的例子</li></ul><p id="b29a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以从我的<a class="ae le" href="https://github.com/karthikn2789/Scrapy-Projects" rel="noopener ugc nofollow" target="_blank"> GitHub 页面</a>下载这些例子。这是关于使用 Scrapy 和 Selenium 进行网络抓取的 4 部分教程系列的第二部分。其他部分可在以下网址找到</p><p id="0d4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd">第 1 部分:用刮刀刮网:理论理解</a></p><p id="7281" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/web-scraping-with-selenium-d7b6d8d3265a">第 3 部分:用硒刮网</a></p><p id="bcc3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/swlh/web-scraping-with-selenium-scrapy-9d9c2e9d83b1" rel="noopener">第 4 部分:用硒刮网&amp;刮屑</a></p><p id="babd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">重要提示:</em> </strong> <br/>在你尝试抓取任何网站之前，请先通读其<em class="mb"> robots.txt </em>文件。可以像<a class="ae le" href="http://www.google.com/robots.txt" rel="noopener ugc nofollow" target="_blank">www.google.com/robots.txt</a>一样访问。在那里，你会看到一个允许和不允许抓取谷歌网站的页面列表。您只能访问属于<code class="fe mq mr ms mt b">User-agent: *</code>的页面和<code class="fe mq mr ms mt b">Allow:</code>之后的页面。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="dfcc" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lo nk nl nm ls nn no np lw nq nr ns iw bi translated">示例 1 —通过从天气站点提取城市天气来处理单个请求和响应</h2><p id="21fa" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">我们这个例子的目标是从<a class="ae le" href="https://weather.com" rel="noopener ugc nofollow" target="_blank">weather.com</a>提取今天的“Chennai”城市天气预报。提取的数据必须包含温度、空气质量和条件/描述。你可以自由选择你的城市。只需在蜘蛛代码中提供您所在城市的 URL。如前所述，该网站允许抓取数据，前提是抓取延迟不少于 10 秒，也就是说，在从 weather.com 请求另一个 URL 之前，你必须等待至少 10 秒。这个可以在网站的<a class="ae le" href="https://weather.com/robots.txt" rel="noopener ugc nofollow" target="_blank"> robots.txt </a>中找到。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="79fc" class="nb nc iq mt b gy oc od l oe of">User-agent: *<br/># Crawl-delay: 10</span></pre><p id="3a92" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我用<code class="fe mq mr ms mt b">scrapy startproject</code>命令创建了一个新的 Scrapy 项目，并用</p><p id="44af" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">scrapy genspider -t basic weather_spider weather.com</code></p><p id="4589" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">开始编码的第一个任务是遵守网站的政策。为了遵守 weather.com 的爬行延迟政策，我们需要在 scrapy 项目的<code class="fe mq mr ms mt b">settings.py</code>文件中添加下面一行。</p><p id="98aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">DOWNLOAD_DELAY = 10</code></p><p id="e92a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这一行让我们项目中的蜘蛛在发出新的 URL 请求之前等待 10 秒钟。我们现在可以开始编码我们的蜘蛛。</p><p id="c69b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如前所示，生成了模板代码。我对代码做了一些修改。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="7106" class="nb nc iq mt b gy oc od l oe of">import scrapy<br/>import re<br/>from ..items import WeatherItem</span><span id="ad4d" class="nb nc iq mt b gy og od l oe of">class WeatherSpiderSpider(scrapy.Spider):<br/>    name = "weather_spider"<br/>    allowed_domains = ["weather.com"]</span><span id="912e" class="nb nc iq mt b gy og od l oe of">def start_requests(self):<br/>        # Weather.com URL for Chennai's weather<br/>        urls = [<br/>            "https://weather.com/en-IN/weather/today/l/bf01d09009561812f3f95abece23d16e123d8c08fd0b8ec7ffc9215c0154913c"<br/>        ]<br/>        for url in urls:<br/>            yield scrapy.Request(url=url, callback=self.parse_url)</span><span id="569b" class="nb nc iq mt b gy og od l oe of">def parse_url(self, response):</span><span id="1124" class="nb nc iq mt b gy og od l oe of"># Extracting city, temperature, air quality and condition from the response using XPath<br/>        city = response.xpath('//h1[contains(@class,"location")]/text()').get()<br/>        temp = response.xpath('//span[@data-testid="TemperatureValue"]/text()').get()<br/>        air_quality = response.xpath('//span[@data-testid="AirQualityCategory"]/text()').get()<br/>        cond = response.xpath('//div[@data-testid="wxPhrase"]/text()').get()</span><span id="fcd9" class="nb nc iq mt b gy og od l oe of">temp = re.match(r"(\d+)", temp).group(1) + " C"  # Removing the degree symbol and adding C<br/>        city = re.match(r"^(.*)(?: Weather)", city).group(1)  # Removing 'Weather' from location</span><span id="9709" class="nb nc iq mt b gy og od l oe of"># Yielding the extracted data as Item object. You may also extract as Dictionary<br/>        item = WeatherItem()<br/>        item["city"] = city<br/>        item["temp"] = temp<br/>        item["air_quality"] = air_quality<br/>        item["cond"] = cond<br/>        yield item</span></pre><p id="9b5b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我认为这个例子的代码是不言自明的。然而，我将解释流程。希望你能从<a class="ae le" href="https://medium.com/@karthikn2789/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd" rel="noopener"> <strong class="lh ja"> <em class="mb">最后一部分</em> </strong> </a>记住 Scrapy 的整体流程图。我希望能够控制请求，所以我使用<code class="fe mq mr ms mt b">start_requests()</code>而不是<code class="fe mq mr ms mt b">start_urls</code>。在<code class="fe mq mr ms mt b">start_requests()</code>中，指定了钦奈天气页面的 URL。如果你想把它改成你喜欢的城市或者增加更多的城市，请随意。对于 URL 列表中的每个 URL，生成一个请求并产生它。所有这些请求都将到达调度程序，调度程序将在引擎请求时分派这些请求。在对应于该请求的网页被下载器下载之后，响应被发送回引擎，引擎将其定向到相应的蜘蛛。在这种情况下，WeatherSpider 接收响应并调用回调函数<code class="fe mq mr ms mt b">parse_url()</code>。在这个函数中，我使用 XPath 从响应中提取所需的数据。</p><p id="8210" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可能理解到这一部分，代码的下一部分对你来说是新的，因为它还没有被解释。我使用了刺儿头<strong class="lh ja"> <em class="mb">物品</em> </strong>。这些是定义键值对的 Python 对象。您可以参考此<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/items.html#module-scrapy.item" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多关于物品的信息。如果您不希望使用条目，您可以创建一个字典并放弃它。<br/>可能会出现一个问题，在哪里定义这些所谓的项目。请允许我提醒你一下。在创建一个新项目时，我们看到 Scrapy 正在创建一些文件。记得吗？</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="a70e" class="nb nc iq mt b gy oc od l oe of">weather/<br/>├── scrapy.cfg<br/>└── weather<br/>    ├── __init__.py<br/>    ├── items.py<br/>    ├── middlewares.py<br/>    ├── pipelines.py<br/>    ├── __pycache__<br/>    ├── settings.py<br/>    └── spiders<br/>        ├── WeatherSpider.py<br/>        ├── __init__.py<br/>        └── __pycache__</span></pre><p id="4eed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您耐心地沿着这棵树看，您可能会注意到一个名为<code class="fe mq mr ms mt b">items.py</code>的文件。在这个文件中，你需要定义项目对象。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="f857" class="nb nc iq mt b gy oc od l oe of"># -*- coding: utf-8 -*-</span><span id="7aab" class="nb nc iq mt b gy og od l oe of"># Define here the models for your scraped items<br/>#<br/># See documentation in:<br/># <a class="ae le" href="https://docs.scrapy.org/en/latest/topics/items.html" rel="noopener ugc nofollow" target="_blank">https://docs.scrapy.org/en/latest/topics/items.html</a></span><span id="4cfe" class="nb nc iq mt b gy og od l oe of">import scrapy</span><span id="6d16" class="nb nc iq mt b gy og od l oe of">class WeatherItem(scrapy.Item):<br/>    city = scrapy.Field()<br/>    temp = scrapy.Field()<br/>    air_quality = scrapy.Field()<br/>    cond = scrapy.Field()</span></pre><p id="1a29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Scrapy 已经创建了这个类，你需要做的就是定义键值对。在这个例子中，因为我们需要城市名称、温度、空气质量和条件，所以我创建了 4 项。您可以根据项目需要创建任意数量的项目。</p><p id="2162" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当您使用下面的命令运行项目时，将会创建一个包含抓取项目的 JSON 文件。</p><p id="b72a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">scrapy crawl weather_spider -o output.json</code></p><p id="0c00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">里面的东西看起来会像，</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="9e7d" class="nb nc iq mt b gy oc od l oe of">output.json<br/>------------</span><span id="d1c0" class="nb nc iq mt b gy og od l oe of">[<br/>{"city": "Chennai, Tamil Nadu", "temp": "31 C", "air_quality": "Good", "cond": "Cloudy"}<br/>]</span></pre><p id="816c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">万岁！！。你已经成功地执行了一个简单的 Scrapy 项目，处理一个请求和响应。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="7ef3" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lo nk nl nm ls nn no np lw nq nr ns iw bi translated">示例 2 —通过从虚拟在线书店提取图书详细信息来处理多个请求和响应</h2><p id="f2ff" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">本例中我们的目标是从网站<a class="ae le" href="http://books.toscrape.com" rel="noopener ugc nofollow" target="_blank">books.toscrape.com</a>中搜集所有书籍(确切地说是 1000 本)的详细信息。不要担心 robots.txt。这个网站是专门为练习网络抓取而设计和托管的。所以，你是清白的。这个网站是这样设计的，它有 50 页，每页列出 20 本书。您无法从列表页面提取图书详细信息。你必须导航到单本书的网页，以提取所需的细节。这是一个需要抓取多个网页的场景，所以我将使用<em class="mb">爬行蜘蛛</em>。像前面的例子一样，我已经创建了一个新项目和一个爬行蜘蛛，使用了<code class="fe mq mr ms mt b">scrapy startproject</code>和</p><p id="6ef7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">scrapy genspider -t crawl crawl_spider books.toscrape.com</code></p><p id="70b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于这个例子，我将提取书名，它的价格，评级和可用性。这个<code class="fe mq mr ms mt b">items.py</code>文件应该是这样的。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="285b" class="nb nc iq mt b gy oc od l oe of">class BookstoscrapeItem(scrapy.Item):<br/>    title = scrapy.Field()<br/>    price = scrapy.Field()<br/>    rating = scrapy.Field()<br/>    availability = scrapy.Field()</span></pre><p id="b12c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在项目所需的一切都准备好了，让我们看看<code class="fe mq mr ms mt b">crawl_spider.py</code>。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="6dd5" class="nb nc iq mt b gy oc od l oe of">class CrawlSpiderSpider(CrawlSpider):<br/>    name = "crawl_spider"<br/>    allowed_domains = ["books.toscrape.com"]<br/>    # start_urls = ["http://books.toscrape.com/"] # when trying to use this, comment start_requests()</span><span id="5d63" class="nb nc iq mt b gy og od l oe of">rules = (Rule(LinkExtractor(allow=r"catalogue/"), callback="parse_books", follow=True),)</span><span id="5550" class="nb nc iq mt b gy og od l oe of">def start_requests(self):<br/>        url = "http://books.toscrape.com/"<br/>        yield scrapy.Request(url)</span><span id="d498" class="nb nc iq mt b gy og od l oe of">def parse_books(self, response):<br/>        """ Filtering out pages other than books' pages to avoid getting "NotFound" error.<br/>        Because, other pages would not have any 'div' tag with attribute 'class="col-sm-6 product_main"'<br/>        """<br/>        if response.xpath('//div[@class="col-sm-6 product_main"]').get() is not None:<br/>            title = response.xpath('//div[@class="col-sm-6 product_main"]/h1/text()').get()<br/>            price = response.xpath('//div[@class="col-sm-6 product_main"]/p[@class="price_color"]/text()').get()<br/>            stock = (<br/>                response.xpath('//div[@class="col-sm-6 product_main"]/p[@class="instock availability"]/text()')<br/>                .getall()[-1]<br/>                .strip()<br/>            )<br/>            rating = response.xpath('//div[@class="col-sm-6 product_main"]/p[3]/@class').get()</span><span id="6585" class="nb nc iq mt b gy og od l oe of"># Yielding the extracted data as Item object.<br/>            item = BookstoscrapeItem()<br/>            item["title"] = title<br/>            item["price"] = price<br/>            item["rating"] = rating<br/>            item["availability"] = stock<br/>            yield item</span></pre><p id="00d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你注意到<code class="fe mq mr ms mt b">start_requests()</code>的变化了吗？为什么我生成一个没有回调的请求？是我在最后一部分说每个请求都必须有相应的回调吗？如果你有这些问题，我赞赏你对细节和批判性推理的关注。向你致敬！！拐弯抹角说够了，让我继续回答你的问题。我没有在初始请求中包含回调，因为<code class="fe mq mr ms mt b">rules</code>中指定了回调和 URL，后续请求将使用该 URL。</p><p id="938f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">流程将从我用<a class="ae le" href="http://books.toscrape.com." rel="noopener ugc nofollow" target="_blank">http://books.toscrape.com 显式生成一个请求开始。</a>紧随其后的是 LinkExtractor 用模式<a class="ae le" href="http://books.toscrape.com/catalogue/." rel="noopener ugc nofollow" target="_blank">http://books.toscrape.com/catalogue/.</a>提取链接，爬行蜘蛛开始用 LinkExtractor 用<code class="fe mq mr ms mt b">parse_books</code>作为回调函数创建的所有 URL 生成请求。这些请求被发送到调度程序，当引擎发出请求时，调度程序依次调度请求。像以前一样，通常的流程继续进行，直到调度程序中不再有请求。当您使用 JSON 输出运行这个蜘蛛时，您将获得 1000 本书的详细信息。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="6fdf" class="nb nc iq mt b gy oc od l oe of">scrapy crawl crawl_spider -o crawl_spider_output.json</span></pre><p id="55ab" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">示例输出如下所示。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="523c" class="nb nc iq mt b gy oc od l oe of">[<br/>  {<br/>    "title": "A Light in the Attic",<br/>    "price": "\u00a351.77",<br/>    "rating": "star-rating Three",<br/>    "availability": "In stock (22 available)"<br/>  },<br/>  {<br/>    "title": "Libertarianism for Beginners",<br/>    "price": "\u00a351.33",<br/>    "rating": "star-rating Two",<br/>    "availability": "In stock (19 available)"<br/>  },<br/>  ...<br/>]</span><span id="c905" class="nb nc iq mt b gy og od l oe of">#Note: /u00a3 is the unicode representation of £</span></pre><p id="c0a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如前所述，这不是提取所有 1000 本书的细节的唯一方法。<em class="mb">一个基本的蜘蛛也可以用来</em>提取精确的细节。我已经用一个基本的蜘蛛程序包含了代码。使用以下命令创建一个基本的蜘蛛。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="b20c" class="nb nc iq mt b gy oc od l oe of">scrapy genspider -t basic book_spider books.toscrape.com</span></pre><p id="e2d4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基本的蜘蛛包含以下代码。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="286d" class="nb nc iq mt b gy oc od l oe of">class BookSpiderSpider(scrapy.Spider):<br/>    name = "book_spider"<br/>    allowed_domains = ["books.toscrape.com"]</span><span id="c254" class="nb nc iq mt b gy og od l oe of">def start_requests(self):<br/>        urls = ["http://books.toscrape.com/"]<br/>        for url in urls:<br/>            yield scrapy.Request(url=url, callback=self.parse_pages)</span><span id="9c87" class="nb nc iq mt b gy og od l oe of">def parse_pages(self, response):<br/>        """<br/>        The purpose of this method is to look for books listing and the link for next page.<br/>        - When it sees books listing, it generates requests with individual book's URL with parse_books() as its callback function.<br/>        - When it sees a next page URL, it generates a request for the next page by calling itself as the callback function.<br/>        """</span><span id="90ee" class="nb nc iq mt b gy og od l oe of">books = response.xpath("//h3")</span><span id="3a5a" class="nb nc iq mt b gy og od l oe of">""" Using response.urljoin() to get individual book page """<br/>        """<br/>        for book in books:<br/>            book_url = response.urljoin(book.xpath(".//a/@href").get())<br/>            yield scrapy.Request(url=book_url, callback=self.parse_books)<br/>        """</span><span id="f5cc" class="nb nc iq mt b gy og od l oe of">""" Using response.follow() to get individual book page """<br/>        for book in books:<br/>            yield response.follow(url=book.xpath(".//a/@href").get(), callback=self.parse_books)</span><span id="605e" class="nb nc iq mt b gy og od l oe of">""" Using response. urljoin() to get next page """<br/>        """<br/>        next_page_url = response.xpath('//li[@class="next"]/a/@href').get()<br/>        if next_page_url is not None:<br/>            next_page = response.urljoin(next_page_url)<br/>            yield scrapy.Request(url=next_page, callback=self.parse_pages)<br/>        """</span><span id="f3e4" class="nb nc iq mt b gy og od l oe of">""" Using response.follow() to get next page """<br/>        next_page_url = response.xpath('//li[@class="next"]/a/@href').get()<br/>        if next_page_url is not None:<br/>            yield response.follow(url=next_page_url, callback=self.parse_pages)</span><span id="69b1" class="nb nc iq mt b gy og od l oe of">def parse_books(self, response):<br/>        """<br/>        Method to extract book details and yield it as Item object<br/>        """</span><span id="22e0" class="nb nc iq mt b gy og od l oe of">title = response.xpath('//div[@class="col-sm-6 product_main"]/h1/text()').get()<br/>        price = response.xpath('//div[@class="col-sm-6 product_main"]/p[@class="price_color"]/text()').get()<br/>        stock = (<br/>            response.xpath('//div[@class="col-sm-6 product_main"]/p[@class="instock availability"]/text()')<br/>            .getall()[-1]<br/>            .strip()<br/>        )<br/>        rating = response.xpath('//div[@class="col-sm-6 product_main"]/p[3]/@class').get()</span><span id="6bd6" class="nb nc iq mt b gy og od l oe of">item = BookstoscrapeItem()<br/>        item["title"] = title<br/>        item["price"] = price<br/>        item["rating"] = rating<br/>        item["availability"] = stock<br/>        yield item</span></pre><p id="5df5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你有没有注意到两只蜘蛛都用了同样的<code class="fe mq mr ms mt b">parse_books()</code>方法？提取图书详细信息的方法是相同的。唯一不同的是，我在基本蜘蛛中用一个专用的长函数<code class="fe mq mr ms mt b">parse_pages()</code>替换了爬行蜘蛛中的<code class="fe mq mr ms mt b">rules</code>。希望这能让你看到爬行蜘蛛和基础蜘蛛的区别。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="c273" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lo nk nl nm ls nn no np lw nq nr ns iw bi translated">示例 3 —图像刮擦</h2><p id="1d85" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">在开始这个例子之前，让我们看一下 Scrapy 如何抓取和处理文件和图像的简要概述。要从网页中抓取文件或图像，您需要使用内置管道，具体来说就是<code class="fe mq mr ms mt b">FilesPipeline</code>或<code class="fe mq mr ms mt b">ImagesPipeline</code>，分别用于各自的目的。我将解释使用<code class="fe mq mr ms mt b">FilesPipeline</code>时的典型工作流程。</p><ol class=""><li id="8654" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma oh mi mj mk bi translated">您必须使用蜘蛛抓取一个项目，并将所需文件的 URL 放入一个<code class="fe mq mr ms mt b">file_urls</code>字段。</li><li id="87b1" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oh mi mj mk bi translated">然后，您返回该项目，该项目将进入项目管道。</li><li id="8501" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oh mi mj mk bi translated">当项目到达<code class="fe mq mr ms mt b">FilesPipeline</code>时，<code class="fe mq mr ms mt b">file_urls</code>中的 URL 被发送到调度器，由下载器下载。唯一的区别是这些<code class="fe mq mr ms mt b">file_urls</code>被赋予了更高的优先级，并在处理任何其他请求之前被下载。</li><li id="23e1" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oh mi mj mk bi translated">当文件被下载后，另一个字段<code class="fe mq mr ms mt b">files</code>将被结果填充。它将包括实际的下载网址，一个相对的路径，在那里存储，其校验和和状态。</li></ol><p id="5128" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">FilesPipeline</code>可用于抓取不同类型的文件(图片、pdf、文本等。).<code class="fe mq mr ms mt b">ImagesPipeline</code>专门用于图像的抓取和处理。除了<code class="fe mq mr ms mt b">FilesPipeline</code>的功能外，它还具有以下功能:</p><ul class=""><li id="80cf" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">将所有下载的图像转换为 JPG 格式和 RGB 模式</li><li id="daa0" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">生成缩略图</li><li id="e40f" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">检查图像宽度/高度，确保它们满足最小限制</li></ul><p id="f543" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，文件名也不同。使用<code class="fe mq mr ms mt b">ImagesPipeline</code>时，请用<code class="fe mq mr ms mt b">image_urls</code>和<code class="fe mq mr ms mt b">images</code>代替<code class="fe mq mr ms mt b">file_urls</code>和<code class="fe mq mr ms mt b">files</code>。如果您希望了解更多关于文件和图像处理的信息，您可以随时点击此<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/media-pipeline.html#downloading-and-processing-files-and-images" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="68b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们这个例子的目标是从网站<a class="ae le" href="http://books.toscrape.com" rel="noopener ugc nofollow" target="_blank">books.toscrape.com</a>上抓取所有书籍的封面图片。为了实现我们的目标，我将重新利用前面例子中的<em class="mb">爬行蜘蛛</em>。在开始编写代码之前，有一个重要的步骤需要完成。您需要设置<code class="fe mq mr ms mt b">ImagesPipeline</code>。为此，将以下两行添加到项目文件夹中的<code class="fe mq mr ms mt b">settings.py</code>文件中。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="1518" class="nb nc iq mt b gy oc od l oe of">ITEM_PIPELINES = {"scrapy.pipelines.images.ImagesPipeline": 1}<br/>IMAGES_STORE = "path/to/store/images"</span></pre><p id="45f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在您已经准备好编码了。因为我重用了爬行蜘蛛，所以爬行蜘蛛的代码不会有太大的不同。唯一的区别是，您需要创建包含<code class="fe mq mr ms mt b">images</code>、<code class="fe mq mr ms mt b">image_urls</code>的 Item 对象，并从 spider 中产生它。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="8182" class="nb nc iq mt b gy oc od l oe of"># -*- coding: utf-8 -*-<br/>import scrapy<br/>from scrapy.linkextractors import LinkExtractor<br/>from scrapy.spiders import CrawlSpider, Rule<br/>from ..items import ImagescraperItem<br/>import re</span><span id="f87a" class="nb nc iq mt b gy og od l oe of">class ImageCrawlSpiderSpider(CrawlSpider):<br/>    name = "image_crawl_spider"<br/>    allowed_domains = ["books.toscrape.com"]<br/>    # start_urls = ["http://books.toscrape.com/"]</span><span id="7bf5" class="nb nc iq mt b gy og od l oe of">def start_requests(self):<br/>        url = "http://books.toscrape.com/"<br/>        yield scrapy.Request(url=url)</span><span id="275b" class="nb nc iq mt b gy og od l oe of">rules = (Rule(LinkExtractor(allow=r"catalogue/"), callback="parse_image", follow=True),)</span><span id="fca9" class="nb nc iq mt b gy og od l oe of">def parse_image(self, response):<br/>        if response.xpath('//div[@class="item active"]/img').get() is not None:<br/>            img = response.xpath('//div[@class="item active"]/img/@src').get()</span><span id="c5c6" class="nb nc iq mt b gy og od l oe of">"""<br/>            Computing the Absolute path of the image file.<br/>            "image_urls" require absolute path, not relative path<br/>            """<br/>            m = re.match(r"^(?:../../)(.*)$", img).group(1)<br/>            url = "http://books.toscrape.com/"<br/>            img_url = "".join([url, m])</span><span id="8f06" class="nb nc iq mt b gy og od l oe of">image = ImagescraperItem()<br/>            image["image_urls"] = [img_url]  # "image_urls" must be a list</span><span id="7347" class="nb nc iq mt b gy og od l oe of">yield image</span></pre><p id="cf08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">items.py</code>文件看起来像这样。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="fdc5" class="nb nc iq mt b gy oc od l oe of">import scrapy</span><span id="27a9" class="nb nc iq mt b gy og od l oe of">class ImagescraperItem(scrapy.Item):<br/>    images = scrapy.Field()<br/>    image_urls = scrapy.Field()</span></pre><p id="2e5e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当您使用输出文件运行蜘蛛程序时，蜘蛛程序将抓取<a class="ae le" href="http://books.toscrape.com," rel="noopener ugc nofollow" target="_blank">http://books.toscrape.com 的所有网页，</a>抓取书籍封面的 URL 并将其作为<code class="fe mq mr ms mt b">image_urls</code>输出，然后将其发送给调度程序，工作流将继续进行，如本例开头所述。</p><p id="49ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mq mr ms mt b">scrapy crawl image_crawl_spider -o output.json</code></p><p id="b6dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下载的图像将被存储在由<code class="fe mq mr ms mt b">IMAGES_STORE</code>指定的位置，并且<code class="fe mq mr ms mt b">output.json</code>将看起来像这样。</p><pre class="kp kq kr ks gt ny mt nz oa aw ob bi"><span id="f8fd" class="nb nc iq mt b gy oc od l oe of">[<br/>  {<br/>    "image_urls": [<br/>      "http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg"<br/>    ],<br/>    "images": [<br/>      {<br/>        "url": "http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg",<br/>        "path": "full/59d0249d6ae2eeb367e72b04740583bc70f81558.jpg",<br/>        "checksum": "693caff3d97645e73bd28da8e5974946",<br/>        "status": "downloaded"<br/>      }<br/>    ]<br/>  },<br/>  {<br/>    "image_urls": [<br/>      "http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg"<br/>    ],<br/>    "images": [<br/>      {<br/>        "url": "http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg",<br/>        "path": "full/1c1a130c161d186db9973e70558b6ec221ce7c4e.jpg",<br/>        "checksum": "e3953238c2ff7ac507a4bed4485c8622",<br/>        "status": "downloaded"<br/>      }<br/>    ]<br/>  },<br/>  ...<br/>]</span></pre><p id="990d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想抓取其他不同格式的文件，你可以使用<code class="fe mq mr ms mt b">FilesPipeline</code>来代替。我将把这个留给你的好奇心。你可以从这个<a class="ae le" href="https://github.com/karthikn2789/Scrapy-Projects" rel="noopener ugc nofollow" target="_blank">链接</a>下载这 3 个例子。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="d51f" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lo nk nl nm ls nn no np lw nq nr ns iw bi translated">避免被禁止</h2><p id="9d0e" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">热衷于网络抓取的初学者可能会走极端，以更快的速度抓取网站，这可能会导致他们的 IP 被网站禁止/列入黑名单。一些网站实施了特定的措施来防止机器人爬取它们，这些措施的复杂程度各不相同。</p><p id="6514" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是在处理这类网站时需要记住的一些技巧，摘自<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/practices.html?highlight=DOWNLOAD_DELAY#avoiding-getting-banned" rel="noopener ugc nofollow" target="_blank"> Scrapy Common Practices </a>:</p><ul class=""><li id="224c" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">从浏览器中众所周知的用户代理中轮换你的用户代理(谷歌一下可以得到他们的列表)。</li><li id="23d4" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">禁用 cookie(参见 COOKIES_ENABLED ),因为一些网站可能会使用 cookie 来发现机器人行为。</li><li id="3372" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">使用下载延迟(2 或更高)。参见 DOWNLOAD_DELAY 设置。</li><li id="5294" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">如果可能的话，使用谷歌缓存获取页面，而不是直接访问网站</li><li id="c7d3" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">使用轮换 IP 池。比如免费的 Tor 项目或者像 ProxyMesh 这样的付费服务。一个开源的选择是 scrapoxy，一个超级代理，你可以附加你自己的代理。</li><li id="a35c" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">使用高度分布式的下载器，在内部绕过禁令，这样你就可以专注于解析干净的页面。这种下载程序的一个例子是 Crawlera</li></ul><h2 id="3e8f" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lo nk nl nm ls nn no np lw nq nr ns iw bi translated">结束语</h2><p id="63f8" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">因为我的目标是让你在读完这篇教程后自信地与 Scrapy 一起工作，所以我克制自己不去深入 Scrapy 的各种错综复杂的方面。但是，我希望我已经向您介绍了与 Scrapy 一起工作的概念和实践，明确区分了基本蜘蛛和爬行蜘蛛。如果你有兴趣游到这个池子的更深的一端，请随意接受通过点击<a class="ae le" href="https://docs.scrapy.org/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">这里</a>可以到达的零碎的官方文件的指导。</p><p id="0996" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本网刮系列的<a class="ae le" rel="noopener" target="_blank" href="/web-scraping-with-selenium-d7b6d8d3265a"> <strong class="lh ja"> <em class="mb">下一部分</em> </strong> </a>中，我们将着眼于硒。</p><p id="1533" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在那之前，祝你好运。保持安全和快乐的学习。！</p></div></div>    
</body>
</html>