<html>
<head>
<title>Dropout Intuition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">辍学直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dropout-intuition-87df344fce43?source=collection_archive---------29-----------------------#2020-01-18">https://towardsdatascience.com/dropout-intuition-87df344fce43?source=collection_archive---------29-----------------------#2020-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/8e3f45457f98a867be8efa1603169281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*zMb2B3jmyp1r8cbRu1bLzg.jpeg"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated"><a class="ae kb" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiNz5_ws4znAhVXVH0KHXOsDUYQjRx6BAgBEAQ&amp;url=https%3A%2F%2Fwww.learningmachines101.com%2Flm101-030-how-to-improve-deep-learning-performance-with-artificial-brain-damage-dropout-and-model-averaging%2F&amp;psig=AOvVaw1VuJRI12uFt901oQyaJq1F&amp;ust=1579410681263607" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="85cd" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">这篇文章的目的是提供一个非常简单的介绍背后的神经网络辍学的基本直觉。</p><p id="2b52" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">当神经网络(NN)完全连接时，NN中的所有神经元都投入工作，而不管它们在特定任务中是否有用。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/91330282086ff8ceecc27b39f5012aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*iEZAMIms2rjT7hUMJ_y3IQ.png"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">全连接图层(图片来源— <a class="ae kb" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="25b4" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">假设我们将100个神经元放入一个神经网络来完成一个分类任务，但实际上，只有50个神经元足以完成同样的任务。现在，因为我们已经让所有的神经元开始工作，它们将会做它们的工作来提取它们认为对分类是必要的特征。</p><p id="3b08" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">这些特征可以分为3种类型:</p><ol class=""><li id="a889" class="lf lg it ke b kf kg kj kk kn lh kr li kv lj kz lk ll lm ln bi translated">最重要的功能(MSF)</li><li id="c4cd" class="lf lg it ke b kf lo kj lp kn lq kr lr kv ls kz lk ll lm ln bi translated">次要特征(LSF)</li><li id="5acc" class="lf lg it ke b kf lo kj lp kn lq kr lr kv ls kz lk ll lm ln bi translated">噪声特征(NF)</li></ol><h2 id="0756" class="lt lu it bd lv lw lx dn ly lz ma dp mb kn mc md me kr mf mg mh kv mi mj mk ml bi translated">最重要的特征</h2><p id="eb03" class="pw-post-body-paragraph kc kd it ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz im bi translated">这些是数据中真正重要的部分，它们将极大地帮助我们对数据进行分类和归纳。有了它们，我们应该能够体面地进行分类。我们的神经网络需要提取这些特征。</p><h2 id="3d3d" class="lt lu it bd lv lw lx dn ly lz ma dp mb kn mc md me kr mf mg mh kv mi mj mk ml bi translated">不太重要的功能</h2><p id="fc62" class="pw-post-body-paragraph kc kd it ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz im bi translated">这些是有好处的，但在分类中不起主要作用的。光靠他们，我们可能做不好分类。如果我们忽略了这些特征，也没关系。</p><h2 id="6e5d" class="lt lu it bd lv lw lx dn ly lz ma dp mb kn mc md me kr mf mg mh kv mi mj mk ml bi translated">噪声特征</h2><p id="af98" class="pw-post-body-paragraph kc kd it ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz im bi translated">这些是不好的特征，也不应该被神经网络拾取。这些特征对分类没有任何帮助。通过选取这些特征，神经网络将使数据过度拟合，而不会泛化。</p><p id="68c4" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">因为我们已经让所有的神经元工作，并使用了梯度下降、随机梯度下降等方法来确保所有的神经元都工作，所以我们的神经元没有选择，只能从数据中选择所有类型的特征(MSF、LSF、NF)。这将导致过度拟合，并且神经网络不会泛化。</p><p id="677f" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">为了减少神经网络的过拟合和泛化，我们可以执行以下任一操作:</p><ol class=""><li id="c5db" class="lf lg it ke b kf kg kj kk kn lh kr li kv lj kz lk ll lm ln bi translated">找到分类所需的最佳神经元数量</li><li id="0fd0" class="lf lg it ke b kf lo kj lp kn lq kr lr kv ls kz lk ll lm ln bi translated">使用辍学技术</li></ol><p id="631f" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">在我们的例子中，分类所需的最佳神经元数量是50。但是，如果数据集发生变化，所需神经元的最佳数量也会发生变化。每次数据集改变时，寻找所需神经元的最佳数量是非常繁琐和不切实际的。因此，我们需要使用退出技术。</p><p id="7a78" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">在Dropout中，我们在训练阶段随机丢弃NN中的一些神经元。假设我们随机丢弃神经网络中50%的神经元。现在，分类的工作必须由剩下的50%的神经元来完成。并且这些神经元不能懈怠并拾取不太重要和有噪声的特征，因为它们不会帮助我们分类，并且如果拾取的特征对分类没有帮助，梯度下降之类的东西会更多地惩罚NN。因此，这些剩余的神经元被迫选择最重要的特征，这将减少过度拟合并更好地概括。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/61e652daa1984b03495352e3fc20d8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*irZq-zbdnqJcBtVX4UuU9Q.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">Dropout(图像源— <a class="ae kb" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)</p></figure><p id="9e28" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">因此，在深度神经网络中，过度拟合的可能性非常高。因此，辍学作为一个正则化神经网络。要丢弃的神经元百分比是一个超参数，可以根据数据的过度拟合量进行调整。</p><p id="44c1" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">这大致就是漏失如何在模型的泛化中发挥作用并提高其性能的。</p><h1 id="c370" class="mw lu it bd lv mx my mz ly na nb nc mb nd ne nf me ng nh ni mh nj nk nl mk nm bi translated">参考</h1><ol class=""><li id="fc54" class="lf lg it ke b kf mm kj mn kn nn kr no kv np kz lk ll lm ln bi translated"><a class="ae kb" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">http://jmlr . org/papers/volume 15/srivastava 14 a . old/srivastava 14 a . pdf</a></li></ol></div></div>    
</body>
</html>