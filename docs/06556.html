<html>
<head>
<title>Zipping and Submitting PySpark Jobs in EMR Through Lambda Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过 Lambda 函数在 EMR 中压缩和提交 PySpark 作业</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/zipping-and-submitting-pyspark-jobs-in-emr-through-lambda-functions-46a58a496d9e?source=collection_archive---------28-----------------------#2020-05-24">https://towardsdatascience.com/zipping-and-submitting-pyspark-jobs-in-emr-through-lambda-functions-46a58a496d9e?source=collection_archive---------28-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9bb8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">端到端的项目结构和部署</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fe8028f8e30eb877993c0b8648251853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vnF6er2qr-jJYUxc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Denys Nevozhai 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="8ca2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">语境</h1><p id="974f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我假设你已经熟悉 AWS 云平台，尤其是 Lambda 和 EMR 服务。我假设您已经运行了一个 EMR 集群，并且知道如何设置 lambda 函数。<em class="mn"> </em>本文涵盖了创建 PySpark 项目并提交给 EMR 的端到端主题。最后，本文产生了一个名为<em class="mn"> pyspark-seed </em>的 PySpark 项目，它已经准备好被克隆并在它的基础上进一步开发。</p><h1 id="158b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">重要的事情先来</h1><p id="5a91" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我的 Python 项目的 go to IDE 是<em class="mn"> PyCharm </em>。Jupyter 笔记本总是在那里快速检查。为了构建 Python 虚拟环境，我将使用<em class="mn"> venv </em>。Venv 很简单，并且预装了 Python 解释器。对于 Python，我会选择 3.7 版本；对于 PySpark，我会选择 2.4.5 版本。提供了一个持续集成配置文件，可以通过 GitLab Runner 执行。由于社区更大，我也在 GitHub 中托管这个<em class="mn"> pyspark-seed </em>项目的源代码。</p><h2 id="03f2" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">部署模式</h2><p id="c774" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在创建项目之前，让我们讨论一下部署！是的，部署发生在最后，但是它的重要性应该在项目开始时讨论。为什么？因为大多数时候，你部署项目的方式会影响项目结构和代码组织。</p><p id="5e13" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在 PySpark 项目中，您可以部署<em class="mn">独立脚本</em>或者部署<em class="mn">打包/压缩的</em>项目。如果你有几个简单的工作，但彼此之间不共享功能，就部署独立脚本。当您有多个彼此共享功能的作业时，部署一个<em class="mn">打包/压缩的</em>项目。</p><p id="522e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当打包用 Java 或 Scala 编写的 spark 作业时，您会创建一个单独的<em class="mn"> jar </em>文件。如果打包正确，在 EMR 中提交这个 jar 文件将会成功地运行作业。要在 EMR 中提交 PySpark 项目，您需要具备以下两点:</p><ul class=""><li id="7578" class="nf ng it lt b lu na lx nb ma nh me ni mi nj mm nk nl nm nn bi translated">您的项目的一个<em class="mn"> zip 文件</em>。</li><li id="027f" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">这个 zip 文件之外的一个<em class="mn"> main.py </em>文件，它将这个 zip 文件作为一个模块导入。您不必指定<em class="mn">。导入模块时压缩</em>。</li></ul><h2 id="a192" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">构建项目</h2><p id="de65" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我创建了一个名为<em class="mn"> pyspark-seed </em>的 Python 项目。该项目的结构如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/6b14bc5475e51b03c956b5232866a177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACqKQk8fwd5tL8_TMBVwkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark-seed 项目结构</p></figure><ul class=""><li id="846f" class="nf ng it lt b lu na lx nb ma nh me ni mi nj mm nk nl nm nn bi translated"><em class="mn">模块 _ 种子</em>:包含<em class="mn">作业</em>、<em class="mn">服务</em>、<em class="mn">实用程序</em>和<em class="mn">测试</em>的模块。这可以被认为是一个独立的模块，内部有一个<em class="mn"> run.py </em>文件。该文件被导入到<em class="mn"> main.py. </em>中</li><li id="b0db" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn">作业</em>:我存储所有作业的包。</li><li id="4caa" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn">服务</em>:自定义编写的函数，在作业内部调用，处理数据。这些函数抽象了作业中使用的共享功能。例子可能是:<em class="mn">模式服务、日期服务</em>等。</li><li id="d7f0" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn"> utils </em> : <em class="mn"> </em>一个包，我在里面存放了像<em class="mn"> s3_utils、spark_utils、log_utils </em>等助手。(是的 utils 的意思和 helpers 一样)。</li><li id="adee" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">测试:存储单元和集成测试的包。</li><li id="bb1f" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn">。gitignore </em>:通常由<a class="ae ky" href="http://gitignore.io/" rel="noopener ugc nofollow" target="_blank"> gitignore.io </a>生成。</li><li id="5fc4" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn"> main.py: </em>这是主函数，在部署时，它将被保存在 zip 文件之外。该文件从<em class="mn">模块种子</em>包中导入<em class="mn"> run.py </em>并运行它。</li><li id="1572" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><em class="mn"> setup.py: </em>安装这个项目。</li></ul><h2 id="7d98" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">虚拟环境</h2><p id="b01f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我使用<em class="mn"> venv </em>来创建隔离的 Python 环境。这个环境中的 Python 二进制文件与您用来创建这个环境的 Python 中的二进制文件是相同的。安装在这个虚拟环境中的模块独立于安装在本地/系统 Python 中的模块。要创建 Python 环境，请在项目根目录(即/pyspark-seed)。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b39f" class="mo la it nv b gy nz oa l ob oc">python3 -m venv venv</span><span id="bf1c" class="mo la it nv b gy od oa l ob oc">. ./venv/bin/activate </span><span id="b609" class="mo la it nv b gy od oa l ob oc">pip install -e .</span></pre><p id="fb47" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">第一个命令创建一个 Python 环境。这将在你的项目结构中创建一个名为<em class="mn"> venv </em>的目录。第二个命令将激活创建的 Python 环境。最后一个命令将运行<em class="mn"> setup.py </em>并安装/设置项目。</p><h2 id="ab06" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">安装文件</h2><p id="83fa" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">该文件的职责是正确设置您的 Python 项目。您可以指定项目的<em class="mn">名称</em>、版本<em class="mn">、</em>提供项目的<em class="mn">描述</em>、<em class="mn">作者</em>名称、<em class="mn">包</em>等等。一个简单的<em class="mn"> setup.py </em>如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="c6b5" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在<em class="mn"> __version__ </em>变量中指定项目版本，允许您在 CI 期间访问它，并使用它来生成一个路径，在那里您将在 s3 中存储工件(即<em class="mn"> seed_module </em>和<em class="mn"> main.py </em>)。访问这个变量非常简单:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="db65" class="mo la it nv b gy nz oa l ob oc">python setup.py --version</span></pre><h2 id="48f4" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">CI-管道</h2><p id="eec9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">PySpark 项目的 CI 管道通常有三个基本阶段:<em class="mn">构建</em>、<em class="mn">测试、</em>和<em class="mn">部署。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/6b325d5b384062b9aa36ca829bd8adbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ygm2eQIL_13Gv2sbx9pUXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark-seed 项目的线性 CI 渠道</p></figure><p id="7d75" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">Python <em class="mn"> 3.7-stretch </em>用作基础图像。如果您想在测试阶段安装 PySpark 并运行 PySpark 测试，那么这个版本是必需的。这条管道是线性的。每个阶段在前一个阶段成功完成后运行。阶段<em class="mn">构建</em>和<em class="mn">测试</em>自动运行，而<em class="mn">部署</em>应该手动触发以进行部署。AWS 凭证作为环境变量存储在 GitLab 中。当在 s3 中存储工件时，我们总是用最新的变更覆盖<em class="mn">最新的</em>路径内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/8610a827ee77bf14e45375b63b14e730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NDLwAk4FWMQcj1rrlJspQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">s3 中部署的工件</p></figure><h1 id="ef39" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">乔布斯</h1><p id="ac48" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每个作业都应该写在单独的 Python 文件中。为了简单起见，每个作业都应该有一个名为<em class="mn"> process </em>的函数，它至少接收<em class="mn"> spark_session </em>、<em class="mn"> input_path </em>和<em class="mn"> output_path </em>作为参数。这些参数，即我们想要运行的作业名和其他参数，在下一节讨论的 lambda 函数中指定。下面显示了一个简单的作业模板。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><h1 id="ca6d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">通过 Lambda 向 EMR 提交作业</h1><p id="c153" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">AWS Lambda 是一种无服务器服务。您可以通过 AWS CloudWatch 安排 lambda 代码运行，触发代码运行作为对事件的响应，甚至通过 API 调用按需触发 lambda 函数。下面展示了一个向 EMR 提交作业的 lambda 函数。这个 lambda 函数在 Python 3.7 中运行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="8fd7" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我已经定义了<em class="mn"> main_path </em>和<em class="mn"> modules_path </em>,它们默认指向产品的<em class="mn">最新</em>版本。特定于您的<em class="mn"> main </em>函数的参数在<em class="mn"> main_path 之后的<em class="mn"> spark-submit </em>中传递。</em>我更喜欢在字典中设置所有参数，将字典转换为字符串，并将整个字符串作为单个参数传递给<em class="mn"> main。</em>当收到这个字符串字典时，我使用<em class="mn"> ast </em>模块，从中提取字典。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="af18" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">然后，该参数字典被传递给<em class="mn"> run.py </em>函数，该函数使用提供的配置建立 spark 会话，并运行参数中规定的作业。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><h1 id="9434" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">场景工作流</h1><p id="72ed" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">考虑一个用例，其中每 20 分钟将一个数据文件(大小约 3GB)转储到 s3 原始数据桶中。考虑一个事件侦听器被设置到这个桶属性。该事件监听器监听<em class="mn">所有</em> <em class="mn">对象创建</em>事件。如果在这个桶中上传了一个对象，侦听器就会捕获它并触发目标 lambda 函数。这个 lambda 函数从事件消息中获取对象路径，提供作业参数，并在 EMR 中提交作业。下图显示了此工作流程的开始和结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/7acb99469a81e9e010a318d75e26dc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyqGcecSHf64BtkyZEVZFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过 lambda 函数在 EMR 中提交 PySpark 作业的简单场景</p></figure><h1 id="3a5a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的话</h1><p id="7c8d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我描述了启动 PySpark 项目、创建 CI 配置文件、在 S3 部署工件以及通过 Lambda 函数在 EMR 中提交作业的过程。提供的大部分建议都直接来自我在生产和研究中使用 PySpark 的个人经验。我在 Github 中托管了这个<em class="mn"> pyspark-seed </em>项目的源代码。通过亲自探索这个知识库，可以了解许多其他细节。随意克隆它，让它变得更好。</p><p id="330e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当然，这个项目许多方面可以用不同的方式来完成。我打算提供这个种子项目作为一个起点，可以进一步发展。欢迎所有问题、反馈和批评。我相信在这个世界上，批评推动变化。</p><div class="oi oj gp gr ok ol"><a href="https://github.com/dardanxhymshiti/pyspark-seed" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">dardanxhymshiti/pyspark-seed</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">在 GitHub 上创建一个帐户，为 dardanxhymshiti/pyspark-seed 的发展做出贡献。</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">github.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div></div></div>    
</body>
</html>