<html>
<head>
<title>Dense Video Captioning Using Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Pytorch 的密集视频字幕</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dense-video-captioning-using-pytorch-392ca0d6971a?source=collection_archive---------10-----------------------#2020-08-08">https://towardsdatascience.com/dense-video-captioning-using-pytorch-392ca0d6971a?source=collection_archive---------10-----------------------#2020-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0257" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在一个视频中为不同的时间段制作多个字幕</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/378c181a63e7d3bb6941d2865ca8c065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KSVOdarCrlvvmUk9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@onice?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">黄祖儿</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="8267" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="6319" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">深度学习任务<a class="ae ky" href="https://www.ijcai.org/Proceedings/2019/0877.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">视频字幕</strong> </a>，过去几年在计算机视觉和自然语言处理的交叉领域相当流行。特别是，<a class="ae ky" href="https://arxiv.org/pdf/1705.00754.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">密集视频字幕</strong> </a>，这是一个子领域，已经在研究人员中获得了一些牵引力。<strong class="lt iu">密集视频字幕</strong>的任务是从未经剪辑的视频中定位感兴趣的事件，并为每个事件生成单独的文本描述。</p><h1 id="a035" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">模型</h1><p id="6182" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">密集视频字幕具有挑战性，因为它需要视频的强上下文表示，并且能够检测局部事件。大多数模型通过将这个问题分解为两步来解决:从视频中检测事件提议，然后为每个事件生成句子。当前最先进的算法<a class="ae ky" href="https://arxiv.org/pdf/2005.08271.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="mn">具有建议生成器的双模式变换器</em> (BMT) </strong> </a>提出组合两个输入通道，以便进行密集视频字幕:视频和音频信息。它在<a class="ae ky" href="https://arxiv.org/pdf/1705.00754.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> ActivityNet 字幕数据集</strong> </a>上实现了最先进的性能，该数据集由数千个视频和与特定时间范围相关联的字幕组成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/7af3a5cc8b2a9b67153437c69b741439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iZZqVGowPa-bURWhFntOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BMT 建筑(<a class="ae ky" href="https://github.com/v-iashin/BMT" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="8228" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">BMT 架构包括三个主要组件:<strong class="lt iu">双模态编码器</strong>、<strong class="lt iu">双模态解码器</strong>，以及最后的<strong class="lt iu">建议生成器</strong>。</p><p id="b80a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">首先，视频的音频和视频分别使用<strong class="lt iu"/><a class="ae ky" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>和<a class="ae ky" href="https://arxiv.org/abs/1705.07750" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>进行编码。在特征提取之后，VGG 和 I3D 特征被传递到双模式编码器层，在那里音频和视觉特征被编码成本文所称的音频参与的视觉和视频参与的音频。这些特征然后被传递给提议生成器，该生成器接收来自两种设备的信息并生成事件提议。</p><p id="c6a0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">在事件提议生成之后，根据事件提议来修剪视频。从特征提取开始，每个短剪辑被再次传递到整个中。提取音频和视频特征，传递到双模式编码器，然后传递到双模式解码器层。这里，解码器层接受两个输入:来自双模式编码器的最后一层的输出，以及最后生成的字幕序列的<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">手套嵌入</strong> </a>。最后，解码器对内部表示进行解码，并基于概率分布生成下一个单词，该单词被添加到先前的字幕序列中。</p><h1 id="7164" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">履行</h1><p id="ec54" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们将向您展示如何使用预训练的 BMT 来执行密集的视频字幕给定的视频。不需要模型训练。</p><h2 id="20d4" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">步骤 1:下载 Repo 并设置环境</h2><p id="9f16" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通过以下方式下载<a class="ae ky" href="https://github.com/v-iashin/BMT" rel="noopener ugc nofollow" target="_blank">论文的官方知识库</a>:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6e74" class="mu la it nh b gy nl nm l nn no">git clone --recursive <a class="ae ky" href="https://github.com/v-iashin/BMT.git" rel="noopener ugc nofollow" target="_blank">https://github.com/v-iashin/BMT.git</a><br/>cd BMT/</span></pre><p id="8f97" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">下载 VGG 和 I3D 模型和手套嵌入。脚本会将它们保存在<code class="fe np nq nr nh b">./data</code>和<code class="fe np nq nr nh b">./.vector_cache</code>文件夹中。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c520" class="mu la it nh b gy nl nm l nn no">bash ./download_data.sh</span></pre><p id="1e1c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">用所有需要的库和依赖项设置一个<code class="fe np nq nr nh b">conda</code>环境:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="634f" class="mu la it nh b gy nl nm l nn no">conda env create -f ./conda_env.yml<br/>conda activate bmt<br/>python -m spacy download en</span></pre><h2 id="5024" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">第二步:下载视频</h2><p id="c7c3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，你可以得到你想要的视频。例如，我将从 Youtube 上获得一个关于最近全球冠状病毒疫情的短片。我得到了这个<a class="ae ky" href="https://www.youtube.com/watch?v=4XC2YWY9BLw&amp;t=18s" rel="noopener ugc nofollow" target="_blank">一个</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/0db42097ede8c6180162bd7dc0a13ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JIeUmd3p5IiCX9wbZkZEDA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">疫情 Youtube 视频(<a class="ae ky" href="https://www.youtube.com/watch?v=4XC2YWY9BLw&amp;t=18s" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="30c1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">您可以使用在线下载程序下载 Youtube 视频，但请谨慎使用，风险自担！下载后，你可以把它保存在你喜欢的地方。我在<code class="fe np nq nr nh b">BMT</code>项目文件夹下创建了一个<code class="fe np nq nr nh b">test</code>文件夹，将下载的视频复制到<code class="fe np nq nr nh b">test</code>文件夹。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="e0a4" class="mu la it nh b gy nl nm l nn no">mkdir test<br/># copied video to the test directory</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7742ae14726201fcbe70c52fc28ad549.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*eIGbFS7I04BZt6SjaaMTsg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下载视频后(图片由作者提供)</p></figure><h2 id="ac31" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">步骤 3:特征提取(I3D 和 VGGish)</h2><p id="401b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">获得视频后，现在是提取 I3D 特征的时候了，首先创建<code class="fe np nq nr nh b">conda</code>环境，然后运行 python 脚本:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="297a" class="mu la it nh b gy nl nm l nn no">cd ./submodules/video_features<br/>conda env create -f conda_env_i3d.yml<br/>conda activate i3d<br/>python main.py \<br/>    --feature_type i3d \<br/>    --on_extraction save_numpy \<br/>    --device_ids 0 \<br/>    --extraction_fps 25 \<br/>    --video_paths ../../test/pandemic.mp4 \<br/>    --output_path ../../test/</span></pre><p id="d2b7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">使用类似的过程提取 VGGish 特征:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="356d" class="mu la it nh b gy nl nm l nn no">conda env create -f conda_env_vggish.yml<br/>conda activate vggish<br/>wget https://storage.googleapis.com/audioset/vggish_model.ckpt -P ./models/vggish/checkpoints<br/>python main.py \<br/>    --feature_type vggish \<br/>    --on_extraction save_numpy \<br/>    --device_ids 0 \<br/>    --video_paths ../../test/pandemic.mp4 \<br/>    --output_path ../../test/</span></pre><p id="1246" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">运行上述脚本后，I3D 和 VGGish 特性将保存在<code class="fe np nq nr nh b">test</code>目录中。保存的特征包括 RGB 视觉特征(<code class="fe np nq nr nh b">pandemic_rgb.npy</code>)、光流特征(<code class="fe np nq nr nh b">pandemic_flow.npy</code>)和音频特征(<code class="fe np nq nr nh b">pandemic_vggish.npy</code>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f069e8ca1e8f4e00d72edf0ba48277ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*RnuyTEI6mmJ5ckFfgB7Ohw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征提取后(图片由作者提供)</p></figure><h2 id="f4b0" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">步骤 4:在视频上运行密集视频字幕</h2><p id="c26b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">导航回主项目文件夹，然后激活之前设置的<code class="fe np nq nr nh b">bmt</code>环境。最后，我们可以使用下面的命令运行视频字幕:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="e9d1" class="mu la it nh b gy nl nm l nn no">cd ../../<br/>conda activate bmt<br/>python ./sample/single_video_prediction.py \<br/>    --prop_generator_model_path ./sample/best_prop_model.pt \<br/>    --pretrained_cap_model_path ./sample/best_cap_model.pt \<br/>    --vggish_features_path ./test/pandemic_vggish.npy \<br/>    --rgb_features_path ./test/pandemic_rgb.npy \<br/>    --flow_features_path ./test/pandemic_flow.npy \<br/>    --duration_in_secs 99 \<br/>    --device_id 0 \<br/>    --max_prop_per_vid 100 \<br/>    --nms_tiou_thresh 0.4</span></pre><p id="272a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated"><code class="fe np nq nr nh b">prop_generator_model_path</code>和<code class="fe np nq nr nh b">pretrained_cap_model_path</code>指定建议生成器模型路径和字幕模型路径。因为我们正在使用两个预训练模型，所以我们可以将其直接链接到之前保存预训练模型的路径。<code class="fe np nq nr nh b">vggish_features_path</code>、<code class="fe np nq nr nh b">rgb_features_path</code>和<code class="fe np nq nr nh b">flow_features_path</code>是保存相应特征的路径，<code class="fe np nq nr nh b">duration_in_secs</code>是以秒为单位的视频持续时间，<code class="fe np nq nr nh b">device_id</code>是要使用的 GPU 号，<code class="fe np nq nr nh b">max_prop_per_vid</code>是要搜索的最大建议数(超参数)，最后<code class="fe np nq nr nh b">nms_tiou_thresh</code>是非最大抑制阈值参数。</p><h1 id="b193" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结果</h1><p id="a51f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">执行密集视频字幕后，以下是打印的结果:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="36b0" class="mu la it nh b gy nl nm l nn no">[<br/>  <!-- -->{'start': 2.9, 'end': 25.2, 'sentence': 'A person is seen sitting on a table and leads into a woman speaking to the camera'}<!-- -->,<br/>  <!-- -->{'start': 0.0, 'end': 61.9, 'sentence': 'A man is seen speaking to the camera and leads into several shots of people working on a building'}<!-- -->,<br/>  <!-- -->{'start': 0.0, 'end': 12.7, 'sentence': 'A woman is sitting in a chair talking to the camera'}<!-- -->,<br/>  <!-- -->{'start': 78.6, 'end': 100.0, 'sentence': 'The man then takes the cube and shows the cube off the table'}<!-- -->,<br/>  <!-- -->{'start': 0.2, 'end': 5.2, 'sentence': 'A man is sitting on a chair with a woman sitting on a chair and talking'}<!-- -->,<br/>  <!-- -->{'start': 38.8, 'end': 100.0, 'sentence': 'A man is seen speaking to the camera and leads into several shots of people working out and speaking to the camera'}<br/>]</span></pre><p id="d7e8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">结果给出了开始和结束时间的列表，以及描述该时间段内视频内容的句子。在观看并与视频本身进行比较后，我必须说，该模型在理解视频和考虑音频方面表现得相当好！</p><p id="a507" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">这可以应用于任何视频，所以请随意尝试~！这篇文章是基于<a class="ae ky" href="https://github.com/v-iashin/BMT" rel="noopener ugc nofollow" target="_blank">官方知识库</a>的，所以一定要去看看。</p><h1 id="a6bb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="4bb9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文向您展示了如何使用预先训练好的模型为单个视频中的不同时间段生成多个字幕。该模型的性能被证明是相当好的，并且有更多的改进可以在将来应用于该模型和密集视频字幕任务。</p><p id="d2ec" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">有关深度学习的更多教程，请随时查看:</p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">使用 Pytorch 的 BERT 文本分类</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">文本分类是自然语言处理中的一项常见任务。我们应用 BERT，一个流行的变压器模型，对假新闻检测使用…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">使用 Pytorch 微调用于文本生成的 GPT2</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">使用 Pytorch 和 Huggingface 微调用于文本生成的 GPT2。我们在 CMU 图书摘要数据集上进行训练，以生成…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="om l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/controlling-text-generation-from-language-models-6334935e80cf"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">控制语言模型的文本生成</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">控制机器生成文本的样式和内容的实际操作方法</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="on l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/lstm-text-classification-using-pytorch-2c6c657f8fc0"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">基于 Pytorch 的 LSTM 文本分类</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">一步一步的指导你如何在 Pytorch 中建立一个双向 LSTM！</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oo l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/nlp-preprocessing-with-nltk-3c04ee00edc0"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">用 NLTK 进行文本预处理</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">NLTK 教程 NLP 预处理:小写，删除标点，标记化，停用词过滤，词干，和…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="op l oi oj ok og ol ks nx"/></div></div></a></div><h1 id="e97f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考资料:</h1><div class="nu nv gp gr nw nx"><a href="https://arxiv.org/abs/2005.08271" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">更好地利用视听线索:使用双模式转换器的密集视频字幕</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">密集视频字幕旨在定位和描述未剪辑视频中的重要事件。现有的方法主要是…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://github.com/v-iashin/BMT" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">沃伊申/BMT</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">这是我们论文的 PyTorch 实现:更好地使用视听提示:具有双模式的密集视频字幕…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">github.com</p></div></div><div class="og l"><div class="oq l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://v-iashin.github.io/bmt" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">BMT -弗拉迪米尔·雅辛</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">密集视频字幕旨在定位和描述未剪辑视频中的重要事件。现有的方法主要是…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">v-iashin.github.io</p></div></div><div class="og l"><div class="or l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://arxiv.org/abs/1705.00754" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">视频中的密集字幕事件</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">大多数自然视频包含大量事件。例如，在一个“弹钢琴的人”的视频中，该视频也可能…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">视频中的密集字幕事件</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">A.添加上下文可以生成一致的标题。b .比较在线模型和完整模型。c .上下文可能会添加更多…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">cs.stanford.edu</p></div></div><div class="og l"><div class="os l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://www.ijcai.org/Proceedings/2019/877" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">视频字幕深度学习研究综述</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">陈，，蒋玉刚第二十八届国际人工智能联合会议论文集</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">www.ijcai.org</p></div></div><div class="og l"><div class="ot l oi oj ok og ol ks nx"/></div></div></a></div></div></div>    
</body>
</html>