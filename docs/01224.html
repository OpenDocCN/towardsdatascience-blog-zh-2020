<html>
<head>
<title>Preparing the data for Transformer pre-training — a write-up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为变压器预培训准备数据—书面报告</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a?source=collection_archive---------32-----------------------#2020-02-03">https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a?source=collection_archive---------32-----------------------#2020-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="88f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为 Transformer (BERT、RoBERTa、XLM、XLNet 等)收集、连接、重排和标记所需的数据。)预培训</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/192bf1791857555aaac18ed284c918e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMsopOR-eVsNayOatciECg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Devlin 等人的<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT 论文</a>的截图(2018)</p></figure><p id="ac6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅在一年多前，瓦斯瓦尼等人(2017)推出的 Transformer 模型的最知名化身，Devlin 等人(2018)推出的<strong class="lb iu">B</strong>I 方向<strong class="lb iu">E</strong>n 编码器<strong class="lb iu"> R </strong>表示来自<strong class="lb iu">T</strong>transformers(更好地称为<strong class="lb iu"> BERT </strong>)模型，此后在<strong class="lb iu">自然语言处理(NLP) </strong>社区中变得非常受欢迎。这不仅是因为它在一系列 NLP 任务上的最先进的结果，也是因为它可以通过他们自己的库公开获得，还因为越来越受欢迎的 T21 变形金刚库。</p><p id="2393" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特和其他变形金刚像<strong class="lb iu">罗伯塔、和 XLNet </strong>都是在<strong class="lb iu">多伦多图书语料库</strong>数据集(朱等人，2015)和<strong class="lb iu">英语维基百科</strong>的串联上(以及其他)被预训练的。由于前者不再公开提供，而后者需要大量的预处理，这篇文章展示了<strong class="lb iu">收集、连接、重排和标记</strong>自己试验这些变压器模型所需的数据所涉及的一切(和代码)。</p><h1 id="e446" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">收集(和预处理)数据</h1><h2 id="efe3" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">多伦多图书语料库数据集</h2><p id="db38" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">如前所述，朱等人(2015)的<strong class="lb iu">多伦多图书语料库(TBC) </strong>数据集已不再公开。然而，数据集的来源仍然可用。因此，您可以按照我之前的这篇文章自己复制 TBC 数据集:</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/replicating-the-toronto-bookcorpus-dataset-a-write-up-44ea7b87d091"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">复制多伦多图书语料库数据集——一篇综述</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">复制不再公开的多伦多图书语料库数据集，从其…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks nh"/></div></div></a></div><h2 id="4436" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">英语维基百科</h2><p id="cd85" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">虽然<a class="ae ky" href="https://dumps.wikimedia.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">维基百科转储</strong> </a>是免费提供的，但在准备用于变压器预训练之前，它们仍然需要被提取、清理和预处理。因此，类似于 TBC 数据集，您可以在我之前的另一篇文章之后这样做:</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">为 NLP 模型训练预处理 Wikipedia 转储—书面报告</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">下载、提取、清理和预处理 NLP 模型的维基百科转储(例如像 BERT 这样的变压器…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nw l ns nt nu nq nv ks nh"/></div></div></a></div><h1 id="96ad" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">➕🔀串联和混排数据</h1><p id="3184" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">现在，您已经收集了多伦多图书语料库数据集和英语维基百科，我们可以继续连接和重组这两者。这可以在 Bash 或类似的软件中使用下面的代码很简单地完成:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="1a84" class="mn lw it ny b gy oc od l oe of">cat tbc_replica.txt en_wiki.txt | shuf &gt;&gt; concat_shuffled.txt</span></pre><h1 id="b661" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">💥将数据符号化</h1><p id="b094" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">收集、连接和重组数据后，现在剩下的就是对数据进行标记。在理想情况下，我们会动态地对数据进行标记，但是出于性能原因，我们会对数据进行预标记。为此，我们使用最近发布的速度惊人的<a class="ae ky" href="https://github.com/huggingface/tokenizers" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">标记器库</strong> </a>，也是通过下面的代码由 HuggingFace 实现的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="a033" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要对数据进行令牌化，首先在这里下载 vocab 文件<a class="ae ky" href="https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt" rel="noopener ugc nofollow" target="_blank"/>，然后在您的终端中运行下面的命令:<code class="fe oi oj ok ny b">python3 tokenize_data.py<strong class="lb iu"> </strong>concat_shuffled.txt bert-base-uncased-vocab.txt</code></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="781a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">就这样，大功告成！</strong>现在，您可以使用您刚刚准备好的数据，自己开始试验这些 Transformer 模型。🤗</p><h1 id="1368" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="8d0e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">[1] J. Devlin 等人，<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向变换器的预训练</a> (2018)，<em class="os"> arXiv 预印本 arXiv:1810.04805 </em>。</p><p id="c9a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]瓦斯瓦尼等人，<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)，<em class="os">神经信息处理系统进展</em>(第 5998–6008 页)。</p><p id="a880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]朱等，【书与电影的对齐:通过看电影和看书走向故事化的视觉解释】 (2015)，<em class="os">IEEE 计算机视觉国际会议论文集</em>(第 19–27 页)。</p></div></div>    
</body>
</html>