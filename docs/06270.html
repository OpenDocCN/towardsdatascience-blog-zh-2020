<html>
<head>
<title>A Complete Guide On Serverless Data Lake Using AWS Glue, Athena and QuickSight</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于使用 AWS Glue、Athena 和 QuickSight 的无服务器数据湖的完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-guide-on-serverless-data-lake-using-aws-glue-athena-and-quicksight-3a8a24cfa4af?source=collection_archive---------9-----------------------#2020-05-20">https://towardsdatascience.com/a-complete-guide-on-serverless-data-lake-using-aws-glue-athena-and-quicksight-3a8a24cfa4af?source=collection_archive---------9-----------------------#2020-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a92c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/serverless-data-lake" rel="noopener" target="_blank">无服务器数据湖</a></h2><div class=""/><div class=""><h2 id="0a08" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">无服务器数据湖中 ETL 数据处理、查询和可视化的分步演练</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6f2ad5f07a6c1e7cc093534745ef910c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yW710idirYiMBpTMSFoyzg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@gioshakara?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">乔治·沙卡拉什维利</a>在<a class="ae lh" href="https://unsplash.com/s/photos/data-lake?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8f00" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上一篇<a class="ae lh" href="https://medium.com/swlh/serverless-data-lake-storing-and-analysing-streaming-data-using-aws-60cf3bfe1efd" rel="noopener"> <strong class="lk jd">文章</strong> </a>中，我们为流数据创建了一个无服务器的数据湖。我们处理流数据，使用 Kinesis Data Analytics 执行窗口功能，将其存储在 S3 上，使用 AWS Glue 创建目录，使用 AWS Athena 执行查询，最后在 QuickSight 上可视化。</p><blockquote class="me mf mg"><p id="6e62" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">现在让我们通过引入批量摄取来完成我们的 lambda 架构</p></blockquote><h1 id="9550" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">关于 Lambda 架构的一些背景知识</h1><p id="f019" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated"><a class="ae lh" href="https://d1.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Lambda 架构</strong> </a>是一种数据处理设计模式，用于处理海量数据，并将批处理和实时处理集成在一个框架内。Lambda 架构是一种混合批处理和流(实时)数据处理的方法，并通过服务层使组合数据可用于下游分析或查看。</p><h1 id="6161" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">待办事项</h1><p id="bc6f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在本教程中，我将通过自动化批处理摄取工作流来完成我们的 Lambda 架构。</p><p id="85d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文将涵盖以下内容:</p><ul class=""><li id="fca8" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">编写 Python Shell 作业以从本地文件服务器获取数据</li><li id="1e61" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">编写一个 Pyspark 作业来处理 upserts、到 parquet 的转换并创建分区</li><li id="3cb4" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">运行爬网程序并更新数据目录</li><li id="268d" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">通过粘合工作流程自动化粘合作业和爬虫</li><li id="70ef" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">在 AWS Athena 中编写分析查询</li><li id="db31" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">使用 Lambda 函数更新 Quicksight 的数据源</li><li id="9319" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">在 Quicksight 中创建视觉效果</li></ul><h1 id="7cd4" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">解决方案概述</h1><p id="2265" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这篇帖子是我上一篇帖子的后续:<a class="ae lh" href="https://medium.com/swlh/serverless-data-lake-storing-and-analysing-streaming-data-using-aws-60cf3bfe1efd" rel="noopener"> <strong class="lk jd">无服务器数据湖:使用 AWS </strong> </a> <strong class="lk jd">存储和分析流数据。</strong>我将使用相同的数据集<a class="ae lh" href="https://www.kaggle.com/currie32/crimes-in-chicago" rel="noopener ugc nofollow" target="_blank">芝加哥犯罪</a>。该数据集包含 6M 条记录，重 2 GB。我们将把文件放在本地文件服务器上。Python Shell Glue 作业将被安排为每天运行，它将执行批处理摄取。它还将处理 CDC(变更数据捕获)。文件的初始加载完成后，我们将运行 ETL 作业来进行转换和分区数据存储。一旦我们的粘合工作完成，我们将运行我们的爬行器，以注册新创建的分区和模式更改。目录更新后，Athena 将使用 Glue Catalog 对 S3 数据进行查询。我们将使用 lambda 函数来更新 Quicksight 数据源。然后我们的图表会在 Quicksight 上更新。</p><p id="92a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这里是高层架构:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/17fed0d80a089b872648fccacf993bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJ6kTJHwSxtejRGYSqRr-w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">体系结构</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="15e0" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">步骤 1-创建您的数据接收作业</h1><p id="dba4" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">首先，我们需要一个数据转储自动化作业，从本地文件服务器的文件夹中取出文件，并将其转储到 S3 存储桶中。这一过程可以通过 Python-Shell Glue job 实现自动化，因为不需要对数据进行转换，而且它是简单的 I/O。我们将使用 python-shell job，该 job 在第一次运行时将执行满负荷，在后续运行时将仅转储新创建或更新的文件。</p><p id="09bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是 Glue Python-Shell 代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="b49e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们深入代码，看看那里发生了什么。首先，我们正在导入 boto3 SDK。我们将使用 SSM 参数存储来存储文件服务器的凭证、存储桶路径、文件夹路径和上次 ETL 执行时间。要在参数存储中创建参数，只需登录 AWS 控制台，转到系统管理器，创建一个标准参数并保存其值。我已经分别创建了所有的参数，你可以做同样的事情或者用逗号分隔的值存储一个参数，然后在你的代码中分割它们。</p><p id="bf8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是创建参数的方法:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/be38dabb99a787297bf3869e3f2d1833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uW4LRuCrmduSWWydptKvKw.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">参数存储 Gif</p></figure><p id="b7d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我只是加载所有的参数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="85b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们使用给定的凭证创建一个与 FTP 服务器的安全 ssh 连接，然后继续打开 SFTP 连接。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7623" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们正在设置<strong class="lk jd"> TransferConfig </strong>参数。当上传、下载或复制文件或 S3 对象时，您可以将配置设置存储在<a class="ae lh" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig" rel="noopener ugc nofollow" target="_blank">boto 3 . S3 . transfer . transfer config</a>对象中。对象被传递给一个<strong class="lk jd">传输方法</strong>(上传文件、下载文件等)。)在<strong class="lk jd"> Config=参数中。</strong>通常默认设置非常适合一般情况，但是您可以更改此配置来处理您的特殊情况。我正在设置以下参数:</p><ul class=""><li id="5762" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated"><strong class="lk jd"> multipart_threshold: </strong>当文件大小超过 multipart_threshold 属性值时，会发生多部分传输。我已经设置为<strong class="lk jd"> 100MB </strong>。</li><li id="04ed" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd"> multipart_chunksize: </strong>多部分传输的每个部分的分区大小。我将它设置为 20MB，因为我的文件是 200MB(最大值),默认情况下<strong class="lk jd"> max_concurrency </strong>是 10，我的线程得到了适当的利用。你可以根据你的上网带宽定制，增加并发。</li></ul><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="9cad" class="or mm it on b gy os ot l ou ov">config = TransferConfig(multipart_threshold=100 * MB, multipart_chunksize=10 * MB)</span></pre><p id="732c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我将遍历文件服务器中的文件，并将它们的最后修改日期与我的<strong class="lk jd"> last_etl_execution_time 进行比较。</strong>每次我的作业成功完成时，它都会将 last_etl_execution_time 存储在参数存储中，以便下次我只加载 last_etl_execution_time 之后更新/创建的文件。</p><h2 id="2878" class="or mm it bd mn ow ox dn mr oy oz dp mv lr pa pb mx lv pc pd mz lz pe pf nb iz bi translated"><strong class="ak">重要的一点</strong></h2><blockquote class="me mf mg"><p id="8a66" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">这里需要注意的重要一点是，我使用≥来比较时间，而不是更长，这有一个重要的原因:当我们在 SSM 注册我们的 last_etl_execution_time 时，完全有可能一些新文件上传到我们的文件服务器中，并与我们的 last_etl_execution_time 进行了相同的修改，因此我们需要使用≥来上传我们的 S3 桶中的那些文件，以便任何新文件也得到上传，同时，任何重复的文件都将被自动覆盖(默认的 S3 行为)。</p></blockquote><p id="a565" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以总结一下，</p><ul class=""><li id="ae09" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">我们仅在 S3 上使用 last_etl_execution_time 上传新建或更新的文件。</li><li id="4e8c" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">使用≥以便与我们的 last_etl_execution_time 具有相同修改日期的任何文件都不会丢失。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="0e93" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们正在更新我们在 SSM 的<strong class="lk jd">last _ ETL _ execution _ time</strong>。请记住，第一次执行这个作业时，它将执行满负荷，因此您需要将这个 last_etl_execution_time 设置为一个更早的日期。在我的场景中，我将其设置为 2000–01–01 00:00:00</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="8ed9" class="or mm it on b gy os ot l ou ov">ssm.put_parameter(Name='Last_ETL_Execution_Time', Value=str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')),<br/>                  Type='String', Overwrite=True)</span></pre><p id="18b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嗯，就是这样。到目前为止，我们已经完成了 Python-Shell 的工作，并将文件转储到了 S3 bucket 上。现在让我们转到数据的转换和分区。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="9e04" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">步骤 2-创建 Pyspark ETL 作业</h1><p id="3052" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">现在我们将创建一个 Pyspark 作业，该作业将获取更新的或新的文件，根据某个列对它们进行重新分区，并保存为<strong class="lk jd">bucket/year =&lt;year&gt;/month =&lt;month&gt;/</strong>格式和<strong class="lk jd"> parquet 格式</strong>。我们需要一种方法来跟踪着陆桶中已处理的数据。</p><blockquote class="pg"><p id="4592" class="ph pi it bd pj pk pl pm pn po pp md dk translated"><a class="ae lh" href="https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html" rel="noopener ugc nofollow" target="_blank">使用工作书签跟踪已处理的数据</a></p></blockquote><p id="f2e5" class="pw-post-body-paragraph li lj it lk b ll pq kd ln lo pr kg lq lr ps lt lu lv pt lx ly lz pu mb mc md im bi translated">AWS Glue 通过保存作业运行的状态信息来跟踪在 ETL 作业的上一次运行中已经处理过的数据。该持久状态信息被称为<em class="mh">作业书签</em>。我们将为 Glue Pyspark 作业启用书签。</p><blockquote class="me mf mg"><p id="5c6b" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">让我们去 AWS 控制台</p></blockquote><p id="835a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">步骤 1: </strong>在 AWS Glue 控制台中创建一个新的 Pyspark 作业。</p><p id="a12f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">步骤 2: </strong>附加一个具有所需权限的 IAM 角色。在我的例子中，我附加了<a class="ae lh" href="https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonS3FullAccess" rel="noopener ugc nofollow" target="_blank"> AmazonS3FullAccess </a>、<a class="ae lh" href="https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2Fservice-role%2FAWSGlueServiceRole" rel="noopener ugc nofollow" target="_blank"> AWSGlueServiceRole </a>、<a class="ae lh" href="https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FCloudWatchLogsFullAccess" rel="noopener ugc nofollow" target="_blank">CloudWatchLogsFullAccess</a>、<a class="ae lh" href="https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonSSMFullAccess" rel="noopener ugc nofollow" target="_blank">amazonsmsmfullaccess</a>策略</p><p id="f92c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第三步:选择一个由你创作的新剧本。</p><p id="1659" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第四步:</strong>在类型中选择<strong class="lk jd"> Spark </strong>，在 ETL 语言中选择<strong class="lk jd"> Python </strong>。</p><p id="a76e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">步骤 5: </strong>在高级属性中，<strong class="lk jd">启用作业书签。</strong></p><p id="1c11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">步骤 6: </strong>其余配置可以保留默认配置，在运行和测试胶合作业时，您可以以较低的成本将最大容量更改为 2 个 dpu。</p><p id="4951" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里是我的配置的一瞥</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/34fea0364f291710cd30f5a4ea85adad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wYU5ZfgAIRQGdEqMTM-CbA.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">粘附配置</p></figure><h1 id="5dff" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">现在让我们深入研究 ETL 脚本</h1><p id="1692" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这里是 Github 上传的代码。请关注或给一颗星:)</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="0c61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们一部分一部分地分解它:</p><ul class=""><li id="85ae" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">首先，我导入 Glue 库并创建 Glue 上下文。</li><li id="5b40" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">从目录表创建动态框架。如果您还没有创建表，您需要转到<strong class="lk jd">表&gt;添加新表&gt;手动添加列并定义文件的模式。</strong></li><li id="bc84" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">将动态帧转换为数据帧以利用 spark SQL。</li><li id="2d5a" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">然后，我创建分区键(月、日)并追加到现有的数据框中(不需要创建年份，因为它已经存在于数据集中)。</li><li id="2a9c" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">在我们的<strong class="lk jd">bucket/year =&lt;year&gt;/month =&lt;month&gt;/data</strong>文件夹中，我们将存储按天分区的数据文件(每天单独的文件)。</li><li id="3ca2" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">如果在源文件(本地文件服务器)中更新了文件，那么各个 S3 分区文件夹中的数据将被最新的数据覆盖<strong class="lk jd">(处理的更新)</strong>。</li><li id="c172" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">重新划分的数据帧被转换回动态帧并存储在 S3 桶中，其中提到了分区键和拼花格式。</li></ul><blockquote class="pg"><p id="4eee" class="ph pi it bd pj pk pw px py pz qa md dk translated">这是我们的 Glue Pyspark 作业的输出</p></blockquote><figure class="qc qd qe qf qg kw gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/0ff79d394729bfcd0d4245f54c8bbb43.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/1*CqcLgP7ZzWXKt9Z4cwKMpg.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">S3 桶数据</p></figure><p id="e840" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看 S3 桶中的输出文件。数据以拼花格式按年和月进行分区。每个月文件夹中的每个文件都包含按天划分的犯罪数据。</p><h2 id="e741" class="or mm it bd mn ow ox dn mr oy oz dp mv lr pa pb mx lv pc pd mz lz pe pf nb iz bi translated">创建你的爬虫</h2><p id="c643" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">一旦您的工作完成，您需要在 S3 桶中注册新创建的分区。我们将使用这项工作的爬虫。如果您的模式从不改变，您可以使用<code class="fe qh qi qj on b">batch_create_partition()</code> glue api 来注册新的分区。更多信息，请参考我的博客<a class="ae lh" href="https://medium.com/swlh/serverless-data-lake-storing-and-analysing-streaming-data-using-aws-60cf3bfe1efd" rel="noopener">这里</a>。在这里，因为我们需要检测任何模式更改，所以我们将为这项工作运行一个 crawler。以下是爬虫的配置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/c448ecb3af0f28ab3b179ff74a81e56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*P8H6B9vdjTDhCn6yzi9tVw.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">爬虫配置</p></figure><p id="0251" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行此 crawler 来注册分区。接下来，我们将在工作流程中使用这个爬虫。</p><h1 id="17f3" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">步骤 3:让我们用粘合工作流来编排粘合作业和触发器</h1><p id="e18d" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">工作流通过在粘合实体(触发器、爬行器和作业)之间建立依赖关系来实现 ETL 工作负载的编排。您可以在控制台上直观地跟踪工作流中不同节点的状态，从而更容易监控进度和解决问题。此外，您可以在工作流程中的实体之间共享参数。</p><blockquote class="me mf mg"><p id="5b00" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">通过使用工作流，您不必手动设置不同触发器、爬行器和粘合作业之间的流程</p></blockquote><p id="22b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">所以是时候了！我们应该创建一个工作流程。但是首先，我们需要决定执行流程。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ql"><img src="../Images/36aa289b2074175a5148ea61c1455e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vl7zXIhAhGMl3ovWnTqJHw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ETL 执行流程</p></figure><p id="5cbd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以上是我们的 ETL 执行流程。我们可以创建一个工作流，轻松监控进度并排除错误，而不是配置 Cloudwatch 事件并自行管理。</p><blockquote class="pg"><p id="e500" class="ph pi it bd pj pk pl pm pn po pp md dk translated"><strong class="ak">现在让我们创建第一个工作流程！！！</strong></p></blockquote><ul class=""><li id="d0d7" class="ni nj it lk b ll pq lo pr lr qm lv qn lz qo md nn no np nq bi translated">转到 AWS Glue 控制台，选择<strong class="lk jd">工作流程。</strong></li><li id="9e05" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">给它起个<strong class="lk jd">名字</strong>和<strong class="lk jd">描述</strong>。点击<strong class="lk jd">添加</strong>。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qp"><img src="../Images/35e84ec9d5c2d2ae6ae711e9c5176c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXVMRleon5GWp4TGOHOyjw.png"/></div></div></figure><ul class=""><li id="18f1" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">选择您的工作流程并移至<strong class="lk jd">图表</strong>选项卡。</li><li id="8469" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">选择<strong class="lk jd">添加触发器。</strong></li><li id="0746" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">选择<strong class="lk jd">添加新的，</strong>给它一个名称和描述。</li><li id="7f46" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">选择触发类型为<strong class="lk jd">计划。</strong></li><li id="3e8c" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">选择运行触发器的时间，然后单击<strong class="lk jd">添加。</strong></li><li id="0ad7" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">现在您可以添加一个将被触发的<strong class="lk jd">节点</strong>。添加您的 Python-Shell 作业。</li><li id="0705" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">选择 Python-Shell 作业后，在它之后添加另一个触发器节点，并继续执行流程。最后，您的工作流将如下所示:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qq"><img src="../Images/ed5c19de67944733ae0dc3a386695224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hv-TTx2qlaPJdDkgPB19JQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">胶水工作流程</p></figure><p id="2448" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">完成粘合工作流程后，您就可以运行您的工作流程了。</p><blockquote class="me mf mg"><p id="92a7" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><strong class="lk jd">重要提示:</strong>完成教程后，不要忘记删除您的工作流，否则它将被安排在您设定的时间运行。</p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="657e" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">用亚马逊雅典娜分析数据</h1><p id="5ebe" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Amazon Athena 是一种交互式查询服务，它使得使用标准 SQL 分析亚马逊 S3 中的数据变得很容易。Athena 能够查询 CSV 数据。然而，Parquet 文件格式大大减少了查询数据的时间和成本。</p><blockquote class="me mf mg"><p id="001d" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><strong class="lk jd">现在来查询一些数据吧！！！</strong></p></blockquote><p id="bc83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">打开<strong class="lk jd"> Athena </strong>的 AWS 管理控制台，连接你的<strong class="lk jd">数据库</strong>和<strong class="lk jd">表</strong>。</p><p id="3128" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看看芝加哥每年的犯罪率是多少。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/4ad57102e99d253a7bfdde287462da99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4JRk68FvDf0dyp271FOmw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每年的犯罪率</p></figure><p id="a8e3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将在 Athena 中创建<strong class="lk jd">视图</strong>，稍后将由<strong class="lk jd"> Quicksight 导入。</strong>您只需在查询请求中添加以下行。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="d248" class="or mm it on b gy os ot l ou ov">CREATE OR REPLACE VIEW chicago_crimes_usecase1 AS</span></pre><p id="3bc2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来看看每月的犯罪率</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="4fa3" class="or mm it on b gy os ot l ou ov">CREATE OR REPLACE VIEW chicago_crimes_usecase3 AS<br/>SELECT<br/>  "date_format"("incident_date", '%M') "month"<br/>, "primary" "primary_type"<br/>, "count"(*) "count"<br/>FROM<br/>  chicago_crimes_batch_bucket<br/>WHERE ("primary" IS NOT NULL)<br/>GROUP BY "primary", "date_format"("incident_date", '%M')<br/>ORDER BY "month" ASC</span></pre><p id="93e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是查询结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qs"><img src="../Images/e5a6f5618b145943d8ae5429ac6b54b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwRlJD2wtEIBWI2JJrCLvA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每月犯罪率</p></figure><p id="c7cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来看看芝加哥排名前五的犯罪趋势。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="8edb" class="or mm it on b gy os ot l ou ov">CREATE OR REPLACE VIEW chicago_crimes_usecase6 AS<br/>SELECT<br/>  "primary"<br/>, "incident_year"<br/>, "count"<br/>FROM<br/>  (<br/>   SELECT<br/>     "primary"<br/>   , "incident_year"<br/>   , "count"("primary") "count"<br/>   , "row_number"() OVER (PARTITION BY "incident_year" ORDER BY "count"("primary") DESC) "rn"<br/>   FROM<br/>     chicago_crimes_batch_bucket<br/>   WHERE ("incident_year" BETWEEN '2001' AND '2070')<br/>   GROUP BY "primary", "incident_year"<br/>)  tmp<br/>WHERE ("rn" &lt;= 5)<br/>GROUP BY "primary", "incident_year", "count"<br/>ORDER BY "incident_year" ASC</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/76a95cb6eea3282243860b7ff86e87e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIkRGTJJod8vHviwF6jPmw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">热门犯罪</p></figure><p id="96dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们也有位置数据，所以我们可以显示前 5000 个位置点，并确定哪些地方是<strong class="lk jd">热点犯罪区域。</strong></p><blockquote class="me mf mg"><p id="7ec1" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><strong class="lk jd"> QuickSight 在一张地理图上最多只能显示 5000 个位置点</strong></p></blockquote><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="8814" class="or mm it on b gy os ot l ou ov">CREATE OR REPLACE VIEW chicago_crimes_usecase8 AS<br/>SELECT<br/>  TRY(CAST("col20" AS double)) "latitude"<br/>, TRY(CAST("col21" AS double)) "logitude"<br/>FROM<br/>  chicago_crimes_batch_bucket<br/>WHERE (("col20" &lt;&gt; 'null') AND ("col21" &lt;&gt; 'null'))<br/>LIMIT 5000</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/3a109a065342ccd92a267be10d61058d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SmIjx82PNk_JWATmH0jGw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">位置数据</p></figure><p id="f526" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以创建更多类似的查询并分析您的数据。将它们存储在视图中，接下来我们将在 QuickSight 中导入它们。</p><blockquote class="pg"><p id="2e8c" class="ph pi it bd pj pk pl pm pn po pp md dk translated"><a class="ae lh" href="https://github.com/ShafiqaIqbal/AWS-Athena-Queries" rel="noopener ugc nofollow" target="_blank">在我的 Github 上上传了其余的查询</a></p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="3222" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">是时候使用 QuickSight 进行一些数据可视化了</h1><p id="4be4" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated"><strong class="lk jd">第一步:</strong>打开<a class="ae lh" href="https://us-east-1.quicksight.aws.amazon.com/sn/start" rel="noopener ugc nofollow" target="_blank">亚马逊 QuickSight 控制台</a>如【1】所述。</p><p id="3631" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第二步:</strong>第二步:设置<a class="ae lh" href="https://us-east-1.quicksight.aws.amazon.com/sn/console/resources?" rel="noopener ugc nofollow" target="_blank">亚马逊 QuickSight 账号设置</a>到<a class="ae lh" href="https://docs.aws.amazon.com/quicksight/latest/user/managing-permissions.html" rel="noopener ugc nofollow" target="_blank">访问雅典娜和你的 S3 桶</a>。您可以查看[1]了解更多信息。</p><p id="0912" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第三步:</strong>选择<strong class="lk jd">管理数据。</strong></p><p id="dc5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第四步:</strong>选择<strong class="lk jd">新数据集</strong>然后选择<strong class="lk jd">雅典娜</strong>。</p><p id="be50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">步骤 5: </strong>输入您的数据源名称并选择您的视图。</p><p id="fb76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第七步:</strong>这里你可以选择<strong class="lk jd">香料</strong>(缓存)。</p><p id="634d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第 8 步:</strong>你可以添加一个新的可视化，字段和指标。</p><p id="7853" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有一些我用 Athena 的<strong class="lk jd">视图</strong>创建的样本图表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/da7489d0be933f80aec6ac1564c3b617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAYX9GgDoY3MXBSu2F8pJA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">快速浏览图表</p></figure><p id="21c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您还可以创建报表可视化，就像这里的这个。它将为您提供雅典娜风格的查询输出外观。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/cca30f7b96edba12d2aa912e660ae4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmXE4tbvArF7lZipFEmGtA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Quicksight 报告可视化</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="160f" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">QuickSight 仪表板中的计划数据刷新</h1><p id="1467" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">您可以使用时间表自动刷新<strong class="lk jd"> SPICE </strong>数据集。标准版或企业版可以选择<strong class="lk jd">每日</strong>、<strong class="lk jd">每周</strong>或<strong class="lk jd">每月</strong>。</p><ul class=""><li id="ac94" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated"><strong class="lk jd">每日</strong>:每天重复</li><li id="5a4b" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd">每周</strong>:每周的同一天重复</li><li id="244f" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd">每月</strong>:每月的同一天重复。要在每月的 29 日、30 日或 31 日刷新数据，从列表中选择<strong class="lk jd">每月的最后一天</strong>。</li></ul><p id="88c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">仅限企业版，可以选择<strong class="lk jd">小时</strong>。此设置从您选择的时间开始，每小时刷新您的数据集。</p><p id="a650" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如果您想要维护数据集刷新的自定义计划，可以使用 AWS SDK Boto3 来实现。在 lambda 函数中运行下面的代码，并调度您的 lambda。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="f97d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面的代码中得到的关键信息:</p><ul class=""><li id="dd99" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">过滤掉带有前缀“chicago_crimes”的数据集</li><li id="614a" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">在第一个循环中为每个数据集启动数据集刷新过程</li><li id="7e0d" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">检查第二个循环中每个数据集的状态</li></ul></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="e1e7" class="ml mm it bd mn mo oe mq mr ms of mu mv ki og kj mx kl oh km mz ko oi kp nb nc bi translated">就是这样！</h1><blockquote class="pg"><p id="e9b5" class="ph pi it bd pj pk pl pm pn po pp md dk translated">很棒吧？</p></blockquote><p id="f3da" class="pw-post-body-paragraph li lj it lk b ll pq kd ln lo pr kg lq lr ps lt lu lv pt lx ly lz pu mb mc md im bi translated">在这个逐步演练中，我们使用 AWS Glue、AWS Athena 和 AWS QuickSight 开发了一个无服务器数据湖。我们执行了从摄取和治疗到可视化的每个步骤。</p><p id="7ef8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于即将到来的故事，你应该关注我的简介。</p><p id="92f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当你点击并按住<strong class="lk jd">拍手</strong>按钮时，你想看到一些魔法吗？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://www.buymeacoffee.com/shafiqaiqbal"><div class="gh gi qx"><img src="../Images/ca6772c8f1ad47c0455f600513ff6f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*z9uVZt1KxrHXGRdj.jpeg"/></div></a></figure><ul class=""><li id="ed59" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">每当我发表一篇文章，就会收到一封<a class="ae lh" href="http://Get an email whenever I publish an article" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">电子邮件</strong> </a>，如果你喜欢这个故事，可以考虑成为<a class="ae lh" href="https://medium.com/@shafiqa_iqbal/membership" rel="noopener"> <strong class="lk jd">会员</strong> </a>。</li><li id="f9d2" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">在 insta gram<a class="ae lh" href="https://www.instagram.com/__shafiqaiqbal__/" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/></a>上与<strong class="lk jd"> 45，000+ </strong>其他关注者一起关注我，了解技术更新、谷歌的工作文化以及更多信息。</li><li id="4500" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd"> 48，000+ </strong>人们在<a class="ae lh" href="https://www.linkedin.com/in/shafiqa-iqbal/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Linkedin </strong> </a>上关注职业建议、技术建议和日常激励。</li></ul><h2 id="d912" class="or mm it bd mn ow ox dn mr oy oz dp mv lr pa pb mx lv pc pd mz lz pe pf nb iz bi translated">参考资料:</h2><p id="01cf" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">[1]<a class="ae lh" href="https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/blogs/big-data/create-real-time-click stream-sessions-and-run-analytics-with-Amazon-kinesis-data-analytics-AWS-glue-and-Amazon-Athena/</a></p></div></div>    
</body>
</html>