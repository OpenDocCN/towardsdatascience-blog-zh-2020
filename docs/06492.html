<html>
<head>
<title>Keras, Tell Me The Genre Of My Book</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras，告诉我我的书的类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/keras-tell-me-the-genre-of-my-book-a417d213e3a1?source=collection_archive---------25-----------------------#2020-05-23">https://towardsdatascience.com/keras-tell-me-the-genre-of-my-book-a417d213e3a1?source=collection_archive---------25-----------------------#2020-05-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8aac" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用递归神经网络的能力进行分类。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d7143588f7c4ed8da23ad984ca84ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oijjXa0hM3yWul4gqvfNIw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Shutterstock 上的十亿张照片</p></figure><p id="f87d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于任何希望深入了解递归神经网络(RNN)如何工作的人来说，我希望这个简单的教程是一个很好的读物！</p><p id="091b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用的数据集包括我从 GoodReads 收集的书籍描述和流派分类，这是一个使用 RNN 的分类方法解决典型分类问题的很好的例子。对于这个项目，我们将把我们的问题简化为一个二元分类问题，我们将使用 RNN 对全文图书描述进行情感分析！</p><p id="50ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想想这有多神奇。我们将训练一个人工神经网络如何“阅读”书籍描述并猜测其流派。</p><p id="c383" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于理解书面语言需要跟踪一个句子中的所有单词，我们需要一个递归神经网络来保存以前出现过的单词的“记忆”，因为它随着时间的推移“阅读”句子。</p><p id="05b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别是，我们将使用 LSTM(长短期记忆)细胞，因为我们真的不想太快“忘记”单词——句子中早期的单词会显著影响句子的意思。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="f60f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GitHub 资源库<a class="ae kv" href="https://github.com/ernestng11/predicting-book-genre-with-lstm-model/blob/master/book%20genre%20prediction.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="6b56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是我们这个项目需要的一些重要的库——Numpy、Pandas、matplotlib、Plotly 和 Tensorflow。我们将使用 Keras——运行在 TensorFlow(或 CNTK 或 Theano)之上的东西。Keras 允许我们更少地考虑原始模型拓扑，直接投入到简单快速的原型开发中。实验越快，结果越好:)</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="6566" class="me mf iq ma b gy mg mh l mi mj">import numpy as np<br/>import pandas as pd</span><span id="531f" class="me mf iq ma b gy mk mh l mi mj">import matplotlib.pyplot as plt<br/>import tensorflow<br/>import plotly.offline as pyoff<br/>import plotly.graph_objs as go<br/>pyoff.init_notebook_mode()</span></pre><p id="f490" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先导入我从好的阅读中收集的数据，看看我们需要处理什么。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="72a3" class="me mf iq ma b gy mg mh l mi mj">bookdata_path = 'book_data.csv'<br/>testdata_path = 'book_data18.csv'<br/>book = pd.read_csv(bookdata_path)<br/>test = pd.read_csv(testdata_path)</span><span id="4700" class="me mf iq ma b gy mk mh l mi mj">book.columns</span></pre><p id="184a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_authors ':作者姓名字符串</p><p id="48f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' desc 图书':描述字符串</p><p id="d282" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_edition ':图书字符串的不同版本</p><p id="7a99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_format ':精装/平装字符串</p><p id="55c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_pages ':一本书的页数</p><p id="3287" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“图书评级”:图书评级浮动</p><p id="0570" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_rating_count ':评分数 int</p><p id="ee5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_review_count ':评论数 int</p><p id="c36c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' book_title ':图书字符串标题</p><p id="a717" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“体裁”:书串的体裁</p><p id="90cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">' image_url ':图书图像 url 字符串</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><blockquote class="ml mm mn"><p id="c229" class="kw kx mo ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi translated"><strong class="ky ir">数据清理和数据探索</strong></p></blockquote><p id="2c40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一步占用了每个数据科学家的大部分时间。我们查看数据框中的每一列，找出我们可能面临的任何潜在问题。</p><p id="bcb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些常见问题包括:</p><ol class=""><li id="9c2d" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">缺少值</li><li id="3d06" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">涉及不同的语言</li><li id="7599" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">非 Ascii 字符</li><li id="ed0a" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">无效描述</li><li id="17b1" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">描述中缺少空格，例如 HelloILike toEat</li></ol><p id="58df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我建议写下你的数据清理步骤的所有发现，这样你就可以不断地查阅你的笔记，确保你不会错过任何一个步骤！</p><p id="ea70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事不宜迟，以下是我的发现。</p><ol class=""><li id="37b6" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">语料库中存在多种语言——我是想保留所有语言还是只保留英语描述？我的数据集中的整体语言分布如何？</li><li id="e39f" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">每本书至少有一个用户定义的流派，我的数据集中有多少流派？流派分布是怎样的？有多少独特的流派？</li></ol><p id="c024" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1。删除无效格式的描述</strong></p><p id="57e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们在预测类型，类型将是我们的标签，而特征将来自每本书的描述。我发现在一些条目中有格式错误——这就是<a class="ae kv" href="https://pypi.org/project/langdetect/" rel="noopener ugc nofollow" target="_blank"> langdetect </a>出现的地方。我们将实现一个函数来删除任何具有无效描述格式的行。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="847b" class="me mf iq ma b gy mg mh l mi mj">from langdetect import detect</span><span id="d3e8" class="me mf iq ma b gy mk mh l mi mj">def remove_invalid_lang(df):<br/>    invalid_desc_idxs=[]<br/>    for i in df.index:<br/>        try:<br/>            a=detect(df.at[i,'book_desc'])<br/>        except:<br/>            invalid_desc_idxs.append(i)<br/>    <br/>    df=df.drop(index=invalid_desc_idxs)<br/>    return df</span><span id="4f45" class="me mf iq ma b gy mk mh l mi mj">book = remove_invalid_lang(book)<br/>test = remove_invalid_lang(test)</span></pre><h2 id="70b9" class="me mf iq bd ng nh ni dn nj nk nl dp nm lf nn no np lj nq nr ns ln nt nu nv nw bi translated">2.仅获取英文描述</h2><p id="077b" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated">我注意到在我的数据集中涉及到许多语言。为了简单起见，我只想要英文的书籍描述。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="4572" class="me mf iq ma b gy mg mh l mi mj">book[‘lang’]=book[‘book_desc’].map(lambda desc: detect(desc))<br/>test['lang']=test['book_desc'].map(lambda desc: detect(desc))</span></pre><p id="01d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">langdetect 允许我们将每个描述映射到一个 ISO 639-1 值，以使我们在过滤掉英文描述时更加轻松。用它！然后，我将从维基百科中检索语言列表及其各自的 ISO 值。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="fbd2" class="me mf iq ma b gy mg mh l mi mj">lang_lookup = pd.read_html('<a class="ae kv" href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes')[1" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes')[1</a>]<br/>langpd = lang_lookup[['ISO language name','639-1']]<br/>langpd.columns = ['language','iso']</span><span id="77b1" class="me mf iq ma b gy mk mh l mi mj">def desc_lang(x):<br/>    if x in list(langpd['iso']):<br/>        return langpd[langpd['iso'] == x]['language'].values[0]<br/>    else:<br/>        return 'nil'</span><span id="006b" class="me mf iq ma b gy mk mh l mi mj">book['language'] = book['lang'].apply(desc_lang)<br/>test['language'] = test['lang'].apply(desc_lang)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/a2d2031f8225eb43836cfe647cb35cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4SJrag1dRpj1hrf8_tyr_Q.png"/></div></div></figure><p id="9f4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图中可以清楚地看到，绝大多数描述都是英文的。让我们仔细看看其他语言的分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/a479ca54df3b912cfb38be177caec381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gn_eQ8SqswqK1rPWthwgyw.png"/></div></div></figure><p id="60c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用这些一行程序从我们的测试和训练数据集中检索所有的英语书！</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="88a8" class="me mf iq ma b gy mg mh l mi mj">book = book[book['language']=='English']<br/>test = test[test['language']=='English']</span></pre><h2 id="8f0c" class="me mf iq bd ng nh ni dn nj nk nl dp nm lf nn no np lj nq nr ns ln nt nu nv nw bi translated">3.查看所有可用的流派</h2><p id="2ee2" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated">这就是我们的流派专栏的样子。我们有许多由“|”分隔的用户定义的风格，所以我们必须清理它。</p><p id="4ff0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幻想|年轻人|小说</p><p id="443c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每个数据科学项目中，了解数据的分布非常重要，最好的方法是绘制图表！我真的很喜欢使用 Plotly 进行数据可视化，但是 matplotlib 和 seaborn 也可以完成这项工作。</p><p id="620e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的功能，获取每本书的所有流派，并绘制成图表。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="65f6" class="me mf iq ma b gy mg mh l mi mj">def genre_count(x):<br/>    try:<br/>        return len(x.split('|'))<br/>    except:<br/>        return 0</span><span id="a456" class="me mf iq ma b gy mk mh l mi mj">book['genre_count'] = book['genres'].map(lambda x: genre_count(x))</span><span id="922b" class="me mf iq ma b gy mk mh l mi mj">plot_data = [<br/>    go.Histogram(<br/>        x=book['genre_count']<br/>    )<br/>]<br/>plot_layout = go.Layout(<br/>        title='Genre distribution',<br/>        yaxis= {'title': "Frequency"},<br/>        xaxis= {'title': "Number of Genres"}<br/>    )<br/>fig = go.Figure(data=plot_data, layout=plot_layout)<br/>pyoff.iplot(fig)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/133c1b14e1b9cbf4a13e5db86e6988ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ISpI669fBFAj8BzwKOk0A.png"/></div></div></figure><p id="7869" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我必须说，大多数书都有大约 5-6 种类型，而且分布非常均匀。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="8b8f" class="me mf iq ma b gy mg mh l mi mj">def genre_listing(x):<br/>    try:<br/>        lst = [genre for genre in x.split("|")]<br/>        return lst<br/>    except: <br/>        return []</span><span id="c352" class="me mf iq ma b gy mk mh l mi mj">book['genre_list'] = book['genres'].map(lambda x: genre_listing(x))</span><span id="e883" class="me mf iq ma b gy mk mh l mi mj">genre_dict = defaultdict(int)<br/>for idx in book.index:<br/>    g = book.at[idx, 'genre_list']<br/>    if type(g) == list:<br/>        for genre in g:<br/>            genre_dict[genre] += 1</span><span id="7732" class="me mf iq ma b gy mk mh l mi mj">genre_pd = pd.DataFrame.from_records(sorted(genre_dict.items(), key=lambda x:x[1], reverse=True), columns=['genre', 'count'])</span></pre><p id="c6a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码给了我一个所有流派的字典，以及它们在整个语料库中的总数。让我们进入剧情。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="bc9d" class="me mf iq ma b gy mg mh l mi mj">plot_data = [<br/> go.Bar(<br/> x=genre_pd[‘genre’],<br/> y=genre_pd[‘count’]<br/> )<br/>]<br/>plot_layout = go.Layout(<br/> title=’Distribution for all Genres’,<br/> yaxis= {‘title’: “Count”},<br/> xaxis= {‘title’: “Genre”}<br/> )<br/>fig = go.Figure(data=plot_data, layout=plot_layout)<br/>pyoff.iplot(fig)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/724885f9577c33f0185fd782578662f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Emakhy0gsMxc_yAIrQ9y3g.png"/></div></div></figure><p id="aa3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看那些数量很少的类型是不实际的，因为它对我们来说没有什么价值。我们只想查看代表数据集的顶级独特流派，因此让我们挑选 50 个顶级流派来查看！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/1fc30bd6ea92ace87cf67a4358bb3217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hy-0tZuuEK8hmJcFK25f1w.png"/></div></div></figure><p id="785d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们看一下 genre_list 这一栏，如果“小说”至少被列为一种类型，那么这本书就被归类为小说。通过观察，如果一本书在其流派列表中至少有小说，同一列表中的所有其他流派也将与小说密切相关。由此，我可以比较我的数据集中小说和非小说书籍的数量，并将其转化为二元分类问题！</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="9fe2" class="me mf iq ma b gy mg mh l mi mj">def determine_fiction(x):<br/>    lower_list = [genre.lower() for genre in x]<br/>    if 'fiction' in lower_list:<br/>        return 'fiction'<br/>    elif 'nonfiction' in lower_list:<br/>        return 'nonfiction'<br/>    else:<br/>        return 'others'<br/>book['label'] = book['genre_list'].apply(determine_fiction)<br/>test['label'] = test['genre_list'].apply(determine_fiction)</span></pre><h2 id="a68f" class="me mf iq bd ng nh ni dn nj nk nl dp nm lf nn no np lj nq nr ns ln nt nu nv nw bi translated">4.清理文本</h2><p id="3af6" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated">下面是我的函数，可以删除任何非 Ascii 字符和标点符号。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="109b" class="me mf iq ma b gy mg mh l mi mj">def _removeNonAscii(s): <br/>    return "".join(i for i in s if ord(i)&lt;128)</span><span id="01c9" class="me mf iq ma b gy mk mh l mi mj">def clean_text(text):<br/>    text = text.lower()<br/>    text = re.sub(r"what's", "what is ", text)<br/>    text = text.replace('(ap)', '')<br/>    text = re.sub(r"\'s", " is ", text)<br/>    text = re.sub(r"\'ve", " have ", text)<br/>    text = re.sub(r"can't", "cannot ", text)<br/>    text = re.sub(r"n't", " not ", text)<br/>    text = re.sub(r"i'm", "i am ", text)<br/>    text = re.sub(r"\'re", " are ", text)<br/>    text = re.sub(r"\'d", " would ", text)<br/>    text = re.sub(r"\'ll", " will ", text)<br/>    text = re.sub(r'\W+', ' ', text)<br/>    text = re.sub(r'\s+', ' ', text)<br/>    text = re.sub(r"\\", "", text)<br/>    text = re.sub(r"\'", "", text)    <br/>    text = re.sub(r"\"", "", text)<br/>    text = re.sub('[^a-zA-Z ?!]+', '', text)<br/>    text = _removeNonAscii(text)<br/>    text = text.strip()<br/>    return text</span><span id="c386" class="me mf iq ma b gy mk mh l mi mj">def cleaner(df):<br/>    df = df[df['label'] != 'others']<br/>    df = df[df['language'] != 'nil']<br/>    df['clean_desc'] = df['book_desc'].apply(clean_text)</span><span id="c9ea" class="me mf iq ma b gy mk mh l mi mj">return df</span></pre><p id="0307" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">只需拨打:</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="7711" class="me mf iq ma b gy mg mh l mi mj">clean_book = cleaner(book)<br/>clean_test = cleaner(test)</span></pre><p id="72af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在对每本书都有了一个“干净”的描述！</p><p id="167d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“胜利会让你出名。输了就意味着必死无疑。”成为</p><p id="035a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“胜利会让你出名，失败意味着死亡”</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><blockquote class="ml mm mn"><p id="f34d" class="kw kx mo ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi translated">为模型准备我们的数据</p></blockquote><p id="13be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在有趣的部分来了。书籍描述是我们的预测器，所以我们必须特别注意！我们需要确保每个描述都是相同的格式和长度。</p><p id="e0a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用固定输入长度可以提高模型训练期间的性能，因为这样可以创建固定形状的张量，从而获得更稳定的权重。因此，我们将进行裁剪和填充——如果原始描述长度比最佳长度短，则将描述裁剪到最佳长度并用空值填充的过程。</p><p id="c1ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">我们如何确定最佳长度？</em></p><p id="2131" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绘制描述长度的分布图，并观察最“常见”的描述长度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/a247048fa6cffee9b51fa62f4f03edeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYQpWwHfARZu0zNjC-Fc0w.png"/></div></div></figure><p id="d325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇，这是一个非常倾斜的分布！但是我们知道大多数书的描述长度小于 500。我想画出累积分布函数(CDF ),来观察每一级描述长度的“图书数量”。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="5ccd" class="me mf iq ma b gy mg mh l mi mj">len_df_bins=clean_book.desc_len.value_counts(bins=100, normalize=True).reset_index().sort_values(by=['index'])</span><span id="40b8" class="me mf iq ma b gy mk mh l mi mj">len_df_bins['cumulative']=len_df_bins.desc_len.cumsum()</span><span id="a59a" class="me mf iq ma b gy mk mh l mi mj">len_df_bins['index']=len_df_bins['index'].astype('str')</span><span id="121f" class="me mf iq ma b gy mk mh l mi mj">len_df_bins.iplot(kind='bar', x='index', y='cumulative')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/cc14f0a87f4b17750edab853d93074c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSEsYqN7RWpbSGlCxbC-cg.png"/></div></div></figure><p id="9a0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">约 92.7%的记录字数在 277 字以下。因此，我决定将我的最大阈值设置为 250 个单词。我们还需要一个最小阈值，我将把它设置为 6，因为任何少于 5 个单词的描述都不太可能足以确定类型。</p><h2 id="3590" class="me mf iq bd ng nh ni dn nj nk nl dp nm lf nn no np lj nq nr ns ln nt nu nv nw bi translated">1.剪辑和填充</h2><p id="bfbc" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated">对于描述少于 250 个单词的记录，我们将用空值填充它们，而对于描述多于 250 个单词的记录，我们将对它们进行裁剪，只包含前 250 个单词。</p><p id="4e91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RNN 从左到右读取令牌序列，并输出一个预测书是小说还是非小说。这些标记的内存被一个接一个地传递给最终的标记，因此，对序列进行预填充而不是后填充是很重要的。这意味着零添加在令牌序列之前，而不是之后。存在后填充可能更有效的情况，例如在双向网络中。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="691f" class="me mf iq ma b gy mg mh l mi mj">min_desc_length=6<br/>max_desc_length=250</span><span id="f982" class="me mf iq ma b gy mk mh l mi mj">clean_book=clean_book[(clean_book.clean_desc.str.split().apply(len)&gt;min_desc_length)].reset_index(drop=True)</span></pre><p id="3456" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码过滤掉了所有少于 6 个单词的描述。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="d2d8" class="me mf iq ma b gy mg mh l mi mj">vocabulary=set() #unique list of all words from all description</span><span id="3726" class="me mf iq ma b gy mk mh l mi mj">def add_to_vocab(df, vocabulary):<br/>    for i in df.clean_desc:<br/>        for word in i.split():<br/>            vocabulary.add(word)<br/>    return vocabulary</span><span id="4eb2" class="me mf iq ma b gy mk mh l mi mj">vocabulary=add_to_vocab(clean_book, vocabulary)</span><span id="1b0c" class="me mf iq ma b gy mk mh l mi mj">#This dictionary represents the mapping from word to token. Using token+1 to skip 0, since 0 will be used for padding descriptions with less than 200 words<br/>vocab_dict={word: token+1 for token, word in enumerate(list(vocabulary))}</span><span id="fedb" class="me mf iq ma b gy mk mh l mi mj">#This dictionary represents the mapping from token to word<br/>token_dict={token+1: word for token, word in enumerate(list(vocabulary))}</span><span id="8434" class="me mf iq ma b gy mk mh l mi mj">assert token_dict[1]==token_dict[vocab_dict[token_dict[1]]]</span><span id="5048" class="me mf iq ma b gy mk mh l mi mj">def tokenizer(desc, vocab_dict, max_desc_length):<br/>    '''<br/>    Function to tokenize descriptions<br/>    Inputs:<br/>    - desc, description<br/>    - vocab_dict, dictionary mapping words to their corresponding tokens<br/>    - max_desc_length, used for pre-padding the descriptions where the no. of words is less than this number<br/>    Returns:<br/>    List of length max_desc_length, pre-padded with zeroes if the desc length was less than max_desc_length<br/>    '''<br/>    a=[vocab_dict[i] if i in vocab_dict else 0 for i in desc.split()]<br/>    b=[0] * max_desc_length<br/>    if len(a)&lt;max_desc_length:<br/>        return np.asarray(b[:max_desc_length-len(a)]+a).squeeze()<br/>    else:<br/>        return np.asarray(a[:max_desc_length]).squeeze()</span><span id="b61a" class="me mf iq ma b gy mk mh l mi mj">len(vocabulary)<br/>85616</span></pre><p id="12e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有 85616 个独特的单词。最后，裁剪和填充步骤的最后一步，标记每个描述。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="b8e8" class="me mf iq ma b gy mg mh l mi mj">clean_test['desc_tokens']=clean_test['clean_desc'].apply(tokenizer, args=(vocab_dict, max_desc_length))</span></pre><h2 id="ca17" class="me mf iq bd ng nh ni dn nj nk nl dp nm lf nn no np lj nq nr ns ln nt nu nv nw bi translated">2.列车测试分离</h2><p id="0155" class="pw-post-body-paragraph kw kx iq ky b kz nx jr lb lc ny ju le lf nz lh li lj oa ll lm ln ob lp lq lr ij bi translated">当数据集不平衡时，即目标变量(虚构/非虚构)的分布不均匀时，我们应该确保训练-验证分裂是分层的。这确保了目标变量的分布在训练和验证数据集中得到保留。</p><p id="a80d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也可以尝试随机欠采样来减少虚构样本的数量，但是，在这种情况下，我将使用分层采样。原因如下。</p><p id="a9ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分层随机样本用于可以很容易分成不同的子群或子集的人群，在我们的例子中，是虚构或非虚构的。我将从每个标签中随机选择与群体规模和人口数量成比例的记录。每个记录必须只属于一个阶层(标签),我确信每个记录是互斥的，因为一本书只能是小说或非小说。重叠的地层会增加某些数据被包括在内的可能性，从而扭曲样本。</p><p id="ebfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分层抽样优于随机欠抽样的一个优点是，因为它使用特定的特征，所以它可以根据用于划分不同子集的内容来提供更准确的图书表示，而且我们不必删除任何可能对我们的模型有用的记录。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="4174" class="me mf iq ma b gy mg mh l mi mj">def stratified_split(df, target, val_percent=0.2):<br/>    '''<br/>    Function to split a dataframe into train and validation sets, while preserving the ratio of the labels in the target variable<br/>    Inputs:<br/>    - df, the dataframe<br/>    - target, the target variable<br/>    - val_percent, the percentage of validation samples, default 0.2<br/>    Outputs:<br/>    - train_idxs, the indices of the training dataset<br/>    - val_idxs, the indices of the validation dataset<br/>    '''<br/>    classes=list(df[target].unique())<br/>    train_idxs, val_idxs = [], []<br/>    for c in classes:<br/>        idx=list(df[df[target]==c].index)<br/>        np.random.shuffle(idx)<br/>        val_size=int(len(idx)*val_percent)<br/>        val_idxs+=idx[:val_size]<br/>        train_idxs+=idx[val_size:]<br/>    return train_idxs, val_idxs</span><span id="8c46" class="me mf iq ma b gy mk mh l mi mj">_, sample_idxs = stratified_split(clean_book, 'label', 0.1)</span><span id="2c8d" class="me mf iq ma b gy mk mh l mi mj">train_idxs, val_idxs = stratified_split(clean_book, 'label', val_percent=0.2)<br/>sample_train_idxs, sample_val_idxs = stratified_split(clean_book[clean_book.index.isin(sample_idxs)], 'label', val_percent=0.2)</span><span id="ed0c" class="me mf iq ma b gy mk mh l mi mj">classes=list(clean_book.label.unique())</span><span id="5b3e" class="me mf iq ma b gy mk mh l mi mj">sampling=False</span><span id="7520" class="me mf iq ma b gy mk mh l mi mj">x_train=np.stack(clean_book[clean_book.index.isin(sample_train_idxs if sampling else train_idxs)]['desc_tokens'])<br/>y_train=clean_book[clean_book.index.isin(sample_train_idxs if sampling else train_idxs)]['label'].apply(lambda x:classes.index(x))</span><span id="d979" class="me mf iq ma b gy mk mh l mi mj">x_val=np.stack(clean_book[clean_book.index.isin(sample_val_idxs if sampling else val_idxs)]['desc_tokens'])<br/>y_val=clean_book[clean_book.index.isin(sample_val_idxs if sampling else val_idxs)]['label'].apply(lambda x:classes.index(x))</span><span id="ec2f" class="me mf iq ma b gy mk mh l mi mj">x_test=np.stack(clean_test['desc_tokens'])<br/>y_test=clean_test['label'].apply(lambda x:classes.index(x))</span></pre><p id="f1a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">x_train 和 y_train 将用于训练我们的模型，而 x_val 和 y_val 用于检查我们模型的验证准确性。我们的目标是适度的高训练精度和高验证精度，以确保我们不会过度拟合。</p><p id="b981" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过度拟合是指我们的模型在对他们接受训练的数据进行预测时表现良好，但未能对看不见的数据(验证数据)进行归纳。另一方面，当我们的模型甚至在训练数据上表现糟糕时，就会出现欠拟合。</p><p id="a26b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过度拟合的模型将具有高方差和低偏差，而欠拟合的模型将具有高偏差和低方差。</p><p id="14d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">误差=偏差+方差</em></p><p id="807b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的主要目标是减少误差，而不是偏差或方差，因此最佳复杂度是中间值。</p><blockquote class="ml mm mn"><p id="e257" class="kw kx mo ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi translated">模型结构</p></blockquote><p id="fae1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一步，我们的训练数据只是一个数字矩阵，它是我们模型的必要输入。至于我们的 y 标签，现在是 1(虚构)或者 0(非虚构)。</p><p id="c577" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">重述</em></p><p id="a6ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">概括一下，我们有一堆书籍描述，它们被转换成由整数表示的单词向量，还有一个二元情感分类可供学习。RNN 病会很快发作，为了让事情在我们的小电脑上易于管理，我们将描述限制在它们的前 250 个单词。不要忘记，这也有助于提高我们的模型训练的性能！</p><p id="2c20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">初始化我们的模型</em></p><p id="29f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们建立我们的神经网络模型！考虑到 LSTM 递归神经网络是多么复杂，用 Keras 做到这一点是多么容易，这真的令人惊讶。</p><p id="b7ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从嵌入层开始，这只是将输入数据转换为更适合神经网络的固定大小的密集向量的一个步骤。您通常会看到这与我们这里的基于索引的文本数据结合在一起。嵌入层帮助我们降低问题的维度。如果我们对词汇表中的单词进行一次性编码，每个单词将由一个向量表示，这个向量的大小等于词汇表本身的大小，在本例中是 85643。由于每个样本将是一个大小为(词汇×记号数量)的张量，即(85643×250)，该层的大小对于 LSTM 来说将太大而不能消耗，并且对于训练过程来说将是非常资源密集和耗时的。如果我使用嵌入，我的张量大小将只有 250 x 250！WAYYY 小一点！</p><p id="d317" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个热编码将导致一个巨大的稀疏矩阵，而嵌入给我们一个密集的矩阵表示。嵌入长度越高，我们的模型可以学习的表示就越复杂，因为我们的嵌入层学习每个单词的固定长度的“表示”。</p><p id="84ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们只需为 RNN 本身设置一个 LSTM 图层。就这么简单。我们指定 200 来匹配嵌入层的输出大小，并删除项以避免过度拟合，这是 RNN 特别容易发生的。顺便说一下，你可以选择 200 以外的任何数字，它只指定了该层中隐藏神经元的数量。</p><p id="7e01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们只需要用 sigmoid 激活函数将它归结为最后一层中的单个神经元，以选择我们的二元情感分类 0 或 1。</p><p id="1858" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将解释为什么我为模型选择了超参数，但是如果你想跳到代码，请随意跳过这一部分！</p><p id="b2d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该使用多少 LSTM 层？<em class="mo">速度-复杂度权衡</em> </p><p id="8704" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常 1 层足以发现简单问题的趋势，2 层足以发现相当复杂的特征。对于一系列选择(层数)，我们可以在固定的时期数之后比较我们的模型的准确性，如果我们发现即使在添加更多层之后验证准确性也没有显著变化，我们可以选择最小层数。</p><p id="27a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们应该添加多少隐藏节点到我们的 LSTM 层？</strong> <br/> Ns:训练数据中样本的数量<br/> Ni:输入神经元的数量<br/> No:输出神经元的数量<br/> alpha:缩放因子(指示您希望您的模型有多通用，或者您希望防止过度拟合的程度)</p><p id="db5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">通式:Ns / [alpha * (Ni + No)] </em></p><p id="9e89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">添加脱落层。<em class="mo">精度-防过拟合权衡</em> </strong></p><p id="988d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过在训练期间忽略随机选择的神经元来防止过度拟合，并降低对单个神经元的特定权重的敏感性。这迫使我们的模型展开 it 学习。</p><p id="bde9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">添加密集层</strong></p><p id="8ae2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们有 1 个输出标签来表示小说或非小说，所以我们将有 1 维输出。</p><p id="9d39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">添加激活层</strong></p><p id="bd29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多激活功能可供选择，所以这取决于我们的目标。</p><p id="2b89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，我们希望输出是虚构/非虚构的，因此 Softmax 或 Sigmoid 函数会很好。</p><p id="daed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Sigmoid 函数基本上输出概率，我们通常会使用 sigmoid 进行二值分类。</p><p id="c99c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Softmax 函数输出介于 0 和 1 之间的值，使得所有输出值的总和等于 1。基本上你得到了每一类的概率，它们的和必然是 1。这使得 Softmax 非常适合多类问题。</p><p id="489a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用 Sigmoid 和 Softmax 函数调整激活层超参数，并比较验证精度！我们也可以尝试 reLU，它被广泛使用，因为它计算速度快，效果好。</p><p id="a5c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">选择损失函数、优化器和判断标准</strong></p><p id="a837" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们面临的是二进制分类问题，二进制交叉熵将与 sigmoid 一起很好地工作，因为交叉熵函数抵消了 Sigmoid 函数两端的平稳段，因此加快了学习过程。</p><p id="c794" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于优化器，自适应矩估计(adam)已被证明在大多数实际应用中工作良好，并且仅在超参数变化很小的情况下工作良好。r</p><p id="ea2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从整体准确性的角度来判断模型的性能是最容易解释最终模型性能的。</p><p id="8a0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们将实际训练我们的模型。像 CNN 一样，RNN 的资源非常丰富。保持相对较小的批量是让它在你的电脑上运行的关键。当然，在现实世界中，您可以利用安装在集群上的许多计算机上的 GPU 来大大提高这一规模。</p><p id="b4af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mo">代号</em></p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="5a50" class="me mf iq ma b gy mg mh l mi mj">parameters = {'vocab': vocabulary,<br/>              'eval_batch_size': 30,<br/>              'batch_size': 20,<br/>              'epochs': 5,<br/>              'dropout': 0.2,<br/>              'optimizer': 'Adam',<br/>              'loss': 'binary_crossentropy',<br/>              'activation':'sigmoid'}<br/><br/><strong class="ma ir">def</strong> bookLSTM(x_train, y_train, x_val, y_val, params):<br/>    model = Sequential()<br/>    model.name="Book Model"<br/>    model.add(Embedding(len(params['vocab'])+1, output_dim=x_train.shape[1], input_length=x_train.shape[1]))<br/>    model.add(LSTM(200, return_sequences=<strong class="ma ir">True</strong>))<br/>    model.add(Dropout(params['dropout']))<br/>    model.add(LSTM(200))<br/>    model.add(Dense(1, activation=params['activation']))<br/>    model.compile(loss=params['loss'],<br/>              optimizer=params['optimizer'],<br/>              metrics=['accuracy'])<br/>    print(model.summary())<br/>    model.fit(x_train, <br/>          y_train,<br/>          validation_data=(x_val, y_val),<br/>          batch_size=params['batch_size'], <br/>          epochs=params['epochs'])<br/>    results = model.evaluate(x_test, y_test, batch_size=params['eval_batch_size'])<br/>    <strong class="ma ir">return</strong> model<br/><br/>BookModel1 = bookLSTM(x_train, y_train, x_val, y_val, parameters)</span><span id="93f8" class="me mf iq ma b gy mk mh l mi mj">------ Model Summary ------</span><span id="dfd1" class="me mf iq ma b gy mk mh l mi mj">Model: "Book Model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_2 (Embedding)      (None, 200, 200)          17123400  <br/>_________________________________________________________________<br/>lstm_1 (LSTM)                (None, 200, 200)          320800    <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 200, 200)          0         <br/>_________________________________________________________________<br/>lstm_2 (LSTM)                (None, 200)               320800    <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 201       <br/>=================================================================<br/>Total params: 17,765,201<br/>Trainable params: 17,765,201<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="c407" class="me mf iq ma b gy mk mh l mi mj">Train on 23387 samples, validate on 5845 samples<br/>Epoch 1/5<br/>23387/23387 [==============================] - 270s 12ms/step - loss: 0.3686 - accuracy: 0.8447 - val_loss: 0.2129 - val_accuracy: 0.9129<br/>Epoch 2/5<br/>23387/23387 [==============================] - 282s 12ms/step - loss: 0.1535 - accuracy: 0.9476 - val_loss: 0.2410 - val_accuracy: 0.9013<br/>Epoch 3/5<br/>23387/23387 [==============================] - 279s 12ms/step - loss: 0.0735 - accuracy: 0.9771 - val_loss: 0.2077 - val_accuracy: 0.9357<br/>Epoch 4/5<br/>23387/23387 [==============================] - 280s 12ms/step - loss: 0.0284 - accuracy: 0.9924 - val_loss: 0.2512 - val_accuracy: 0.9334<br/>Epoch 5/5<br/>23387/23387 [==============================] - 293s 13ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.2815 - val_accuracy: 0.9290<br/>657/657 [==============================] - 3s 5ms/step</span></pre><p id="2491" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我注意到，随着我的纪元从 3 增加到 5，测试精度增加了，但是验证精度降低了。这意味着模型更好地拟合了训练集，但它失去了对新数据进行预测的能力，这表明我的模型开始拟合噪声，并开始过度拟合。让我们改变参数！</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="2f6b" class="me mf iq ma b gy mg mh l mi mj">parameters = {'vocab': vocabulary,<br/>              'eval_batch_size': 30,<br/>              'batch_size': 20,<br/>              'epochs': 2,<br/>              'dropout': 0.2,<br/>              'optimizer': 'Adam',<br/>              'loss': 'binary_crossentropy',<br/>              'activation':'sigmoid'}<br/><br/><strong class="ma ir">def</strong> bookLSTM(x_train, y_train, x_val, y_val, params):<br/>    model = Sequential()<br/>    model.name="Book Model2"<br/>    model.add(Embedding(len(params['vocab'])+1, output_dim=x_train.shape[1], input_length=x_train.shape[1]))<br/>    model.add(LSTM(200, return_sequences=<strong class="ma ir">True</strong>))<br/>    model.add(Dropout(params['dropout']))<br/>    model.add(LSTM(200))<br/>    model.add(Dense(1, activation=params['activation']))<br/>    model.compile(loss=params['loss'],<br/>              optimizer=params['optimizer'],<br/>              metrics=['accuracy'])<br/>    print(model.summary())<br/>    model.fit(x_train, <br/>          y_train,<br/>          validation_data=(x_val, y_val),<br/>          batch_size=params['batch_size'], <br/>          epochs=params['epochs'])<br/>    results = model.evaluate(x_test, y_test, batch_size=params['eval_batch_size'])<br/>    <strong class="ma ir">return</strong> model<br/><br/>BookModel2 = bookLSTM(x_train, y_train, x_val, y_val, parameters)</span><span id="bbde" class="me mf iq ma b gy mk mh l mi mj">------ Model Summary ------</span><span id="5e4b" class="me mf iq ma b gy mk mh l mi mj">Model: "Book Model2"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_3 (Embedding)      (None, 200, 200)          17123400  <br/>_________________________________________________________________<br/>lstm_3 (LSTM)                (None, 200, 200)          320800    <br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 200, 200)          0         <br/>_________________________________________________________________<br/>lstm_4 (LSTM)                (None, 200)               320800    <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 1)                 201       <br/>=================================================================<br/>Total params: 17,765,201<br/>Trainable params: 17,765,201<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="d6ba" class="me mf iq ma b gy mk mh l mi mj">Train on 23387 samples, validate on 5845 samples<br/>Epoch 1/2<br/>23387/23387 [==============================] - 337s 14ms/step - loss: 0.3136 - accuracy: 0.8690 - val_loss: 0.1937 - val_accuracy: 0.9305<br/>Epoch 2/2<br/>23387/23387 [==============================] - 332s 14ms/step - loss: 0.1099 - accuracy: 0.9636 - val_loss: 0.1774 - val_accuracy: 0.9341<br/>657/657 [==============================] - 3s 4ms/step</span></pre><p id="746e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们仅在两个时期内就实现了 96.4%的训练准确率和 93.4%的验证准确率！对于 2 次运行来说，这已经很不错了。我尝试了许多不同的超参数，仅运行 10 个纪元就花了我一个小时，所以…</p><p id="7101" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你自己试试，让我知道你选择了什么参数来获得比我更高的验证准确率！</p><blockquote class="ml mm mn"><p id="7019" class="kw kx mo ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi translated">测试我们模型的简单函数</p></blockquote><p id="03ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这部引人入胜的《纽约时报》畅销书《纸与火的女孩》续集中，雷和雷恩逃离了他们在隐秘宫殿中的压抑生活，但很快发现自由需要付出可怕的代价。雷，一个天真的乡下女孩，成为了一名皇家妓女，现在被称为月嫂，一个成功地做了别人做不到的事情的平民。但是杀死残忍的魔王并不是计划的结束——这只是开始。现在，雷和她的战士爱人雷恩必须前往王国，以获得来自遥远的反叛氏族的支持。旅途变得更加险恶，因为有人悬赏要得到雷的人头，还有阴险的怀疑威胁着要把雷和雷恩从内部分开。与此同时，在黑魔法和复仇的推动下，一个消灭叛军起义的邪恶阴谋正在成形。雷会成功地推翻帝制并保护她对任的爱情吗？还是她会成为邪恶魔法的牺牲品？</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="398e" class="me mf iq ma b gy mg mh l mi mj">def reviewBook(model,text):<br/>  labels = [‘fiction’, ‘nonfiction’]<br/>  a = clean_text(fantasy)<br/>  a = tokenizer(a, vocab_dict, max_desc_length)<br/>  a = np.reshape(a, (1,max_desc_length))<br/>  output = model.predict(a, batch_size=1)<br/>  score = (output&gt;0.5)*1<br/>  pred = score.item()<br/>  return labels[pred]</span></pre><p id="7119" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们传入我们的最终模型和样本文本，看看我们的模型是否能够仅仅根据一本书的描述准确地预测它的类型。你觉得上面这本书属于哪种流派？</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="2853" class="me mf iq ma b gy mg mh l mi mj">reviewBook(BookModel2,fantasy)</span></pre><p id="3855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">小说</p><p id="952b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显在描述中有“恶魔之王”和“黑魔法”!</p><blockquote class="ml mm mn"><p id="053e" class="kw kx mo ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi translated">结论</p></blockquote><p id="75df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，在第二或第三个时期之后，我们训练时的验证准确性从未真正提高；我们可能只是过度适应了。在这种情况下，早期停止将是有益的。</p><p id="a1a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是再一次——停下来想想我们刚刚在这里做了什么！一种神经网络，可以“阅读”描述，并根据文本推断这本书是否是小说。它考虑了每个单词的上下文及其在评论中的位置——建立模型本身只需要几行代码！你可以用 Keras 做的事情真是不可思议。</p><p id="2f97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">伙计们，希望你们在阅读我的第一篇媒体文章时过得愉快。cheerrrrrrssssssssssssss</p></div></div>    
</body>
</html>