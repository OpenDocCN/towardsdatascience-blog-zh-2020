# 偏见悄悄进入技术领域

> 原文：<https://towardsdatascience.com/bias-creeps-into-technology-e221dd7eae76?source=collection_archive---------57----------------------->

## 我们无法避免将我们自己的观点带到我们构建的产品中

![](img/ada1035146545fbb7fb13a7ca3b513fc.png)

编写计算机代码——图片来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1245714) 的免费照片

大多数开发技术的人都不想有偏见。然而，我们对世界都有自己独特的视角，我们忍不住会将这种视角带入到我们的工作中。我们根据自己的观点和经验做出决定。这些决定可能看起来都很小，但是它们会累积起来。因此，技术往往反映了它的创造者的观点。

这里是我见过的偏见渗入技术的几个地方。

## **我们构建的数据集**

随着机器学习(ML)和人工智能算法最近的成功，数据变得越来越重要。ML 算法从数据集学习它们的行为。这些数据集中包含的内容变得非常重要，因为它会直接影响产品的性能。

以自然语言理解领域(NLU)为例，大型预训练模型最近开始流行。预先训练的模型建立起来很昂贵，但是一旦建立起来，它们可以被不同的人在不同的任务中重用。 [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) 是使用最广泛的预训练模型之一，它是根据[维基百科](https://www.wikipedia.org/)文本构建的。维基百科作为数据来源也有自己的问题。在网站上的人物传记中，只有 18%是女性而[绝大多数内容是由欧洲和北美的编辑撰写的](https://www.theguardian.com/technology/2018/jul/29/the-five-wikipedia-biases-pro-western-male-dominated)。在维基百科[中产生的偏见被 BERT 模型](https://www.aclweb.org/anthology/W19-3823.pdf)学习并传播。

在另一个领域，计算机视觉，数据集在组成上同样存在问题。一类数据集是面部数据集，面部识别系统就是从这些数据集中训练出来的。他们通常是压倒性的白人，在两个流行的数据集中，分别有 79.6%和 86.2%的人肤色较浅。像这样的数据集导致 ML 模型对于深色皮肤的人表现不佳。

## **我们决定解决还是不解决的问题**

在 [CogX](https://cogx.co/) ，我主持了一场会议，会上[海蒂·克里斯滕森](http://staffwww.dcs.shef.ac.uk/people/H.Christensen/)博士讲述了她为那些患有言语障碍&临床语音障碍的人研究语音技术的工作。语音技术可能会对那些有语音障碍的人的生活产生巨大影响，因为影响他们声音的相同条件也会影响其他动作，使他们难以执行许多简单的任务。获得独立意味着更好的结果。然而，主流语音技术关注的是健康的说话者，而不是那些说话方式不标准的人。

其他时候，任务的框架是有问题的，并且有延续刻板印象的风险。接[性别分类](https://www.pewresearch.org/internet/2019/09/05/the-challenges-of-using-machine-learning-to-identify-gender-in-images/)的任务。我相信，把我识别为女性的系统不会带来任何好处。举例来说，我可能会看到[低薪工作的广告](https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study)或者被指向[更贵的](https://en.wikipedia.org/wiki/Pink_tax)产品。

关于做什么工作的决定通常是由财务问题指导的——谁将资助一个产品，谁将付费使用它——但也包括那些建设的个人经验和引起他们共鸣的问题。

## **我们分配给任务的优先级**

我身兼数职，但我的众多工作之一是对团队工作的技术任务进行优先排序。在一个理想的世界里，我们有足够的时间和金钱去做任何事情。但事实上，我们的时间有限。我们必须挑选工作内容。每个优先级的决定可能看起来很小，无关紧要，但它们加在一起会对产品的方向产生很大的影响。

这里有一个受真实事件启发的假设示例—在构建语音识别系统时，我们可能会努力构建一个平衡的训练集，但仍然会评估该系统，发现它对于特定的人口统计数据表现不佳。例如，我们的语音识别系统对英国人的表现可能更差，因为我们的一些发音和单词选择与系统预期的非常不同。现在，在花费团队的努力来调查并使系统对英国用户更可靠，与试验一种新的模型架构，看起来有希望使整个系统对每个人都有更好的表现之间，有一个选择。这个选择并不总是容易做出的。第二种选择可能最终也会提高英国用户的性能，但它不会解决这种不平衡。

## **我们听取的意见**

我们倾听的人会影响我们的想法和我们对谁应该担任这些有影响力的职位的看法。

众所周知，科技公司的人口统计数据——远见者和决策者——在[是倾斜的](https://www.wired.com/story/five-years-tech-diversity-reports-little-progress/)。在最高层，白人和亚裔男性占据了主导地位。除了科技之外，高层的构成也好不到哪里去。富时 100 指数的首席执行官中，叫史蒂夫的 T4 男性比少数族裔男性多，只有 6 名女性。不提拔更多的人进入这些行列，我们就没有听取他们的意见和观点。

2016 年的一项调查显示，英国媒体 94%是白人，女性薪酬明显低于男性。虽然传统媒体的构成有所扭曲，但社交媒体为许多未被充分代表的声音提供了一个平台。然而，即使是社交媒体也存在性别问题。最近的一项研究关注了学术医学领域，发现“[女性学者在该平台上的 Twitter 关注者、喜欢者和转发者也比男性同行少得多，无论她们的 Twitter 活动水平或专业级别如何](https://www.insidehighered.com/news/2019/10/15/women-have-about-half-followers-men-twitter-and-otherwise-diminished-influence)”。

## **我们通过**自我评估的指标

我们经常通过平均错误率来评估最大似然系统。计算这一点并在不同系统间进行比较很容易。也许有 100 个人使用我们的系统，每个人都有 95%的错误率。对于大多数系统来说，这是完全可用的。相反，假设这些用户中的 90 个具有 98%的错误率，10 个具有 68%的错误率。现在，这两组之间有很大的差异，但平均错误率仍然是 95%。发现错误率为 68%的用户可能会发现系统不可用，但这并没有显示在平均指标中。如果不衡量不同人群的表现，我们就无法发现我们建立的模型中的偏差。

在其他产品中，我们测量并优化[参与度](https://www.thedrum.com/opinion/2019/10/09/goodbye-likes-what-should-the-new-engagement-metric-be)——一篇帖子的点击数&赞数，或在网站上停留的时间。但是，参与可能不是用户福祉的最佳衡量标准。参与不仅可以由喜欢某人的页面或帖子引起，也可以由对内容的失望引起。据显示，标题中较高的情感水平会让读者[更有可能点击](https://www.engadget.com/2016-11-21-clickbait-fake-news-and-the-power-of-feeling.html)，但也会分化观点并导致[回音室](https://en.wikipedia.org/wiki/Echo_chamber_(media))，随着时间的推移[会强化极端观点](https://onlinelibrary.wiley.com/doi/abs/10.1002/poi3.88)，而不是挑战它们。

## **我们对顾客的看法**

我怀孕的时候，我的任天堂 Wii 痛斥我长胖了。没有办法告诉它我的体重增加只是暂时的，最终我把它扔进了垃圾箱。我们设计系统，想象我们的客户将如何使用它们。但是，客户在某些方面与我们不同，这是我们无法预料的。任天堂 Wii 的设计者似乎没有预料到用户可能会经历怀孕。任天堂 Wii 是我可以停止使用的东西，但怀孕歧视是一个非常现实的问题。

另一次，我在一个关于家庭安全系统的演讲的接收端，连接到云上。当你不在家时，你不仅可以在线查看录像，还可以随时查看谁在房子里，并在特定事件发生时发出警报。设计这个的团队想象他们的用户和他们一样——骄傲的父亲和慈爱的丈夫，他们只是想做好保护家庭安全的工作。但是，技术也以新的方式使滥用成为可能。设计师没有想到他们的一些客户可能会以不同的意图利用这样的家庭安全系统，所以安全是事后才想到的。

我们对客户的看法可能会限制我们为他们构建的东西，通常是在我们有盲点的领域，并强化我们持有的偏见。

## **我们选择的商业模式**

在科技行业，许多工程类工作报酬丰厚且有保障。相比之下，数据注释员、[司机](https://www.theverge.com/2020/6/24/21302140/california-ag-uber-lyft-drivers-classify-employees-preliminary-injunction)和[内容管理员](https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona)等低收入、不稳定的工作，许多科技公司没有这些工作就无法运营。对这些角色的依赖对许多科技公司的商业模式至关重要，这些员工的人口统计数据与工程人员非常不同。

最近一项关于打车公司算法定价的研究发现，种族、教育和年龄等因素都会影响乘车价格，尽管这些因素并不是模型的明确组成部分。这是因为它们与模型考虑的因素相关，如位置。

另一种常见的商业模式依赖于免费提供服务，并从广告中获得收入。这是一把双刃剑。一方面，免费提供产品扩大了产品的覆盖范围，让原本可能买不起的人也能使用它们。另一方面，伴随这种商业模式而来的[定向广告](https://hbr.org/2019/11/how-targeted-ads-and-dynamic-pricing-can-perpetuate-bias)是偏见被强化的另一种方式，例如允许广告针对[种族和性别](https://www.wired.com/story/are-facebook-ads-discriminatory-its-complicated/)。

技术对世界产生了巨大的影响。但是这个世界是有偏见的，我们这些创造技术的人有我们甚至不知道的盲点。即使有最好的意图，也很难将偏见排除在我们构建的产品之外。

*你可以雇佣我！如果我能帮助你的组织使用 AI &机器学习，请* [*取得联系*](https://www.catherinebreslin.co.uk/consulting) *。*