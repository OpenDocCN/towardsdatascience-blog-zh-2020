<html>
<head>
<title>Hierarchical Clustering — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分层聚类—已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hierarchical-clustering-explained-e58d2f936323?source=collection_archive---------4-----------------------#2020-04-03">https://towardsdatascience.com/hierarchical-clustering-explained-e58d2f936323?source=collection_archive---------4-----------------------#2020-04-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e2a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理论解释和科学学习范例</h2></div><p id="2245" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">聚类算法是无监督的机器学习算法，因此没有与数据点相关联的标签。聚类算法寻找数据点之间的相似性或不相似性，以便可以将相似的数据点分组在一起。有许多不同的方法和算法来执行集群任务。在这篇文章中，我将介绍一种常见的方法，即<strong class="kk iu">层次聚类</strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/b613eeea37c265a2faf1361efced8db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZQNqWWnTseKKfaYvLH6vQ.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">由<a class="ae lu" href="https://unsplash.com/@melipoole?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">梅尔·普尔</a>在<a class="ae lu" href="https://unsplash.com/s/photos/clusters?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c959" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">聚类简单地说就是将相似的事物组合在一起。然而，这并不像听起来那么简单。与聚类相关的挑战之一是，我们几乎总是不知道数据集内的聚类(或组)数量。</p><p id="57a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分层聚类的优点之一是我们不必指定聚类的数量(但是我们可以)。简短的介绍之后，让我们深入细节。</p><p id="8ee4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分层聚类意味着通过迭代分组或分离数据点来创建聚类树。有两种类型的分层聚类:</p><ul class=""><li id="991d" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">凝聚聚类</li><li id="0f7b" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">分裂聚类</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mj"><img src="../Images/eef6d9ce293621614d7e706fb82a08cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICdqpcL62G1q_kIlVszrrg.png"/></div></div></figure><h1 id="69ba" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">凝聚聚类</strong></h1><p id="0f57" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">聚集聚类是一种自下而上的方法。每个数据点首先被假定为一个单独的聚类。然后迭代地组合相似的聚类。让我们看一个例子来解释清楚这个概念。</p><p id="88db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有一个由 9 个样本组成的数据集。我选择与这些样本相关的数字来演示相似性的概念。在每次迭代(或级别)中，最接近的数字(即样本)被组合在一起。如下图所示，我们从 9 个集群开始。最接近的在第一级被组合，然后我们有 7 个集群。与蓝线相交的黑线的数量代表簇的数量。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3da31a120c60138e15d966ce2e7dc36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*8q47aniCkl21w9R4H0OCYQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">系统树图</p></figure><p id="dbd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图称为<strong class="kk iu">树状图</strong>，这是一个表示基于树的方法的图表。在层次聚类中，树状图用于可视化聚类之间的关系。</p><p id="d4f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着我们往上走，随着更多样本的合并，聚类的数量会减少。在级别 6 之后，所有样本被合并到一个大的聚类下。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ni"><img src="../Images/51bb8e8dbab98dab2454320a873ce0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4ccdI8ATjKEznH28ZQMkA.png"/></div></div></figure><p id="23e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个非常简单的数据集来说明目的，但现实生活中的数据集显然更复杂。我们提到“最近的数据点(或聚类)”被组合在一起。但是算法如何识别最接近的呢？在<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中实现了 4 种不同的方法来测量相似性:</p><ol class=""><li id="1830" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld nj mb mc md bi translated"><strong class="kk iu">沃德连锁</strong>:最小化被合并的聚类的方差。目标是簇形心周围的总方差增加最少。</li><li id="f465" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld nj mb mc md bi translated"><strong class="kk iu">平均连锁:</strong>两个聚类中每个数据点的平均距离。</li><li id="b6ba" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld nj mb mc md bi translated"><strong class="kk iu">完全(最大)连锁:</strong>两个聚类中所有数据点之间的最大距离。</li><li id="e628" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld nj mb mc md bi translated"><strong class="kk iu">单一(最小)连锁:</strong>两个聚类中所有数据点之间的最大距离。</li></ol><blockquote class="nk nl nm"><p id="9758" class="ki kj nn kk b kl km ju kn ko kp jx kq no ks kt ku np kw kx ky nq la lb lc ld im bi translated">默认选择是 ward's linkage，它适用于大多数数据集。</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/65220ffef32dfcc88bf3be5ebf0cea5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*0DI9lfDtbkV_93FtBSxZ5g.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">完整(最大)和单一(最小)联动</p></figure><p id="0b77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">层次聚类的优点之一是我们不必预先指定聚类的数量。但是，将所有数据点合并到一个聚类中是不明智的。我们应该在某个时候停止组合集群。Scikit-learn 为此提供了两个选项:</p><ul class=""><li id="f80b" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">达到一定数量的簇后停止(<strong class="kk iu"> n_clusters </strong>)</li><li id="8071" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">设置联动阈值(<strong class="kk iu">距离 _ 阈值</strong>)。如果两个聚类之间的距离超过阈值，这些聚类将不会被合并。</li></ul><h1 id="a8b5" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">分裂聚类</strong></h1><p id="45a6" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">分裂聚类在现实生活中并不常用，所以我将简单地提到它。简单而清晰的解释是<strong class="kk iu">分裂聚类</strong>是凝聚聚类的反义词。我们从一个包含所有数据点的巨大集群开始。然后数据点被分成不同的簇。这是一种自上而下的方法。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="c7b0" class="mk ml it bd mm mn nz mp mq mr oa mt mu jz ob ka mw kc oc kd my kf od kg na nb bi translated"><strong class="ak">利弊</strong></h1><p id="3d65" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">我将尝试解释层次聚类的优点和缺点，以及与 k-means 聚类(另一种广泛使用的聚类技术)的比较。</p><p id="87c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">赞成者</strong></p><ul class=""><li id="f7a3" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">不需要预先指定集群的数量。必须为 k-means 算法指定聚类数。</li><li id="52c9" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">借助于树状图，它很容易实现和解释。</li><li id="eeb4" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">总是生成相同的聚类。k-均值聚类可能会产生不同的聚类，这取决于质心(聚类的中心)是如何初始化的。</li></ul><p id="35e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点</strong></p><ul class=""><li id="5850" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">与 k-means 相比，这是一种较慢的算法。分层聚类需要很长时间来运行，尤其是对于大型数据集。</li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="a2f9" class="mk ml it bd mm mn nz mp mq mr oa mt mu jz ob ka mw kc oc kd my kf od kg na nb bi translated"><strong class="ak">层次聚类应用</strong></h1><p id="69a5" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">如果基础数据具有某种层次结构，分层聚类是有用的，并且会给出更好的结果。</p><p id="912f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分层聚类的一些常见用例:</p><ul class=""><li id="f9f9" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">遗传或其他生物数据可以用来创建一个树状图，以代表突变或进化水平。<a class="ae lu" href="https://en.wikipedia.org/wiki/Phylogenetic_tree" rel="noopener ugc nofollow" target="_blank">种系发生树</a>用于显示基于相似性和差异性的进化关系。如维基百科所述:</li></ul><blockquote class="nk nl nm"><p id="408f" class="ki kj nn kk b kl km ju kn ko kp jx kq no ks kt ku np kw kx ky nq la lb lc ld im bi translated">一个<strong class="kk iu">进化树</strong>或<strong class="kk iu">进化树</strong>是一个分支<a class="ae lu" href="https://en.wikipedia.org/wiki/Diagram" rel="noopener ugc nofollow" target="_blank">图</a>或<a class="ae lu" href="https://en.wikipedia.org/wiki/Tree_(graph_theory)" rel="noopener ugc nofollow" target="_blank">树</a>，显示了各种生物<a class="ae lu" href="https://en.wikipedia.org/wiki/Species" rel="noopener ugc nofollow" target="_blank">物种</a>或其他实体之间的<a class="ae lu" href="https://en.wikipedia.org/wiki/Evolution" rel="noopener ugc nofollow" target="_blank">进化</a>关系，基于它们的物理或遗传特征的相似性和差异性。</p></blockquote><p id="ccd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些树也被用来区分不同类型的病毒。</p><ul class=""><li id="09d1" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">层次聚类也用于对文本文档进行分组。然而，由于数据的高维性，这是一项非常复杂的任务。</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/277e2f043db7ead6e5aafb635b8ce6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*8NyMPa9xZ6rrCVOd4BtUGA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html" rel="noopener ugc nofollow" target="_blank">使用层次聚类对文档进行聚类</a></p></figure><ul class=""><li id="442d" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">层次聚类的另一个常见用例是<strong class="kk iu">社交网络分析。</strong></li><li id="217e" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">分层聚类也用于<strong class="kk iu">异常值检测。</strong></li></ul><h1 id="6e88" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak"> Scikit 学实现</strong></h1><p id="b416" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">我将使用在 scikit learn 的数据集模块下可用的 iris 数据集。让我们从导入数据集开始:</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="2247" class="ok ml it og b gy ol om l on oo">import pandas as pd<br/>import numpy as np<br/>from sklearn.datasets import load_iris</span><span id="be65" class="ok ml it og b gy op om l on oo">iris = load_iris()<br/>X = iris.data</span></pre><p id="f8f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虹膜数据集包括 150 个数据点。我将只使用前 50 个数据点，以便树状图看起来更清楚。</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="72d3" class="ok ml it og b gy ol om l on oo">X = X[:50, :]</span><span id="7510" class="ok ml it og b gy op om l on oo">X.shape<br/>(50, 4)</span></pre><p id="d835" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后引入凝聚聚类类，建立模型。</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="4ae5" class="ok ml it og b gy ol om l on oo">from sklearn.cluster import AgglomerativeClustering</span><span id="7f5e" class="ok ml it og b gy op om l on oo">model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)</span></pre><p id="090f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请记住，如果 distance_threshold 参数不为 None，则 n_cluster 参数必须为 None。我不设置任何条件，只是为了可视化一个完整的树。</p><p id="e2e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步是使模型符合数据:</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="a12e" class="ok ml it og b gy ol om l on oo">model = model.fit(X)</span></pre><p id="b7c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在绘制树状图之前，我们可以使用可用的方法检查模型的细节:</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="b562" class="ok ml it og b gy ol om l on oo"># Number of clusters<br/>model.n_clusters_<br/>50</span><span id="43d0" class="ok ml it og b gy op om l on oo"># Distances between clusters<br/>distances = model.distances_</span><span id="8277" class="ok ml it og b gy op om l on oo">distances.min()<br/>0.09999999999999964</span><span id="a6c3" class="ok ml it og b gy op om l on oo">distances.max()<br/>3.828052620290243</span></pre><p id="84b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Scikit learn 不提供树状图，所以我们将使用<a class="ae lu" href="https://docs.scipy.org/doc/scipy/reference/index.html" rel="noopener ugc nofollow" target="_blank"> SciPy </a>包中的<a class="ae lu" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram" rel="noopener ugc nofollow" target="_blank">树状图</a>。</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="cbe0" class="ok ml it og b gy ol om l on oo">from scipy.cluster.hierarchy import dendrogram<br/>from scipy.cluster import hierarchy</span></pre><p id="6b7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先创建一个关联矩阵:</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="d408" class="ok ml it og b gy ol om l on oo">Z = hierarchy.linkage(model.children_, 'ward')</span></pre><p id="56b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用模型中的孩子和一个链接标准，我选择它作为“病房”链接。</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="ded8" class="ok ml it og b gy ol om l on oo">plt.figure(figsize=(20,10))</span><span id="b33b" class="ok ml it og b gy op om l on oo">dn = hierarchy.dendrogram(Z)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oq"><img src="../Images/a6157f62d055738ddcaaad1fcb2caaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPJZ5Kk-Ep6Ll6lOc6A4uw.png"/></div></div></figure><p id="489f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">叶子的标签是数据点的索引。</p><p id="8766" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过调整<strong class="kk iu">距离 _ 阈值</strong>或<strong class="kk iu">n _ 集群</strong>参数来控制集群的数量。让我们检查计算出的集群之间的距离:</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="4540" class="ok ml it og b gy ol om l on oo">model.distances_<br/>array([0.1       , 0.1       , 0.1       , 0.1       , 0.14142136,        0.14142136, 0.14142136, 0.14142136, 0.14142136, 0.14142136,        0.14142136, 0.17320508, 0.17320508, 0.18257419, 0.2       ,        0.2081666 , 0.21602469, 0.21602469, 0.25819889, 0.27568098,        0.28284271, 0.29439203, 0.29439203, 0.31358146, 0.31464265,        0.31622777, 0.33166248, 0.33665016, 0.34641016, 0.36968455,        0.40620192, 0.42229532, 0.43969687, 0.43969687, 0.46726153,        0.54772256, 0.59441848, 0.6244998 , 0.6363961 , 0.66269651,        0.77628542, 0.81873887, 0.85556999, 0.90998199, 1.10513951,        1.25399362, 1.37126983, 1.91875287, 3.82805262])</span></pre><p id="d3cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">距离按升序排列。如果我们可以将 distance _ thresold 设置为 0.8，则聚类数将为 9。有 8 个距离大于 0.8，因此，当组合时，将形成 9 个集群。</p><pre class="lf lg lh li gt of og oh oi aw oj bi"><span id="536d" class="ok ml it og b gy ol om l on oo">model = AgglomerativeClustering(distance_threshold=0.8, n_clusters=None)</span><span id="a9ed" class="ok ml it og b gy op om l on oo">model = model.fit(X)</span><span id="cb0e" class="ok ml it og b gy op om l on oo">model.n_clusters_<br/>9</span></pre></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="92ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="806c" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="ea86" class="lv lw it kk b kl nc ko nd kr or kv os kz ot ld ma mb mc md bi translated"><a class="ae lu" href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html" rel="noopener ugc nofollow" target="_blank">https://NLP . Stanford . edu/IR-book/html/html edition/hierarchical-agglomerate-clustering-1 . html</a></li><li id="7c03" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/Phylogenetic_tree" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Phylogenetic_tree</a></li></ul></div></div>    
</body>
</html>