# 人工智能是有缺陷的——原因如下

> 原文：<https://towardsdatascience.com/ai-is-flawed-heres-why-3a7e90c48878?source=collection_archive---------23----------------------->

## 人工智能中的公平和人工智能偏差

![](img/897d6e17e508c59901dfba179d9f4ce0.png)

比尔·牛津在 [Unsplash](https://unsplash.com/s/photos/criminal?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

人工智能已经成为每个人生活中不可或缺的一部分。从简单的任务，如 YouTube 推荐，到复杂的拯救生命的任务，如生产治疗疾病的药物，它已经无处不在。它正以比我们意识到的更多的方式影响着我们的生活。

> 但是，AI 公平吗？不，绝对不是。

很难定义一个公平的人工智能。这是我能想到的最好的解释。如果输出独立于敏感参数(例如，性别、种族、性、宗教信仰、残疾等)，则给定的 AI 模型是公平的。)对于已经受到社会歧视影响的特定任务。

在这篇博客中，我将写关于人工智能偏见，人工智能偏见的真实例子，以及修复它的方法。

## **问题**

人工智能偏见是由于用于训练模型的数据中固有的偏见造成的，导致了社会歧视。这导致缺乏平等的机会。

例如，假设我的任务是创建一个模型，以位置作为参数来计算一个人的信用评分。某些种族群体集中在某些地方。这将使我的模型在种族上偏向那些伦理群体，在获得信用卡和银行贷款的同时影响他们。

> 有偏见的人工智能模型加剧了当前的社会歧视，并为压迫铺平了道路。

## 人工智能偏见的真实例子

这里有一些人工智能偏见的真实例子

1.  **COMPAS:** COMPAS(惩教罪犯管理概况替代制裁)是美国法院用来判断被告(被指控犯罪的人)成为惯犯(重复先前所犯罪行的行为)的概率的软件。由于数据存在严重偏差，该模型预测黑人罪犯累犯的误报率是白人罪犯的两倍。
2.  **亚马逊的招聘**:2014 年，亚马逊开发了一个人工智能招聘系统，以简化他们的招聘流程。人们发现这是对女性的歧视，因为用于训练该模型的数据来自过去 10 年，由于男性在科技行业占主导地位，大多数被选中的申请人都是男性。亚马逊在 2018 年刮掉了这个系统。
3.  **美国医疗保健:**美国医疗保健系统使用了一种人工智能模型，该模型为黑人指定了比白人更低的患同样疾病的风险率。这是因为该模型针对成本进行了优化，而且由于黑人被认为支付能力较低，该模型对他们健康风险的排名低于白人。这导致黑人的医疗保健标准降低。
4.  **Twitter 图像裁剪:**2020 年 9 月，Twitter 用户发现图像裁剪算法更喜欢白人面孔而不是黑人面孔。即，当具有与预览窗口不同的纵横比的图像被发布在 Twitter 上时，该算法裁剪图像的部分，并且仅显示图像的特定部分作为预览。这个人工智能模型经常在一张黑白脸的图片中的预览窗口中显示白色的脸。
5.  **脸书的广告算法:**2019 年，脸书允许广告商根据种族、性别和宗教来锁定目标人群。这导致护士和秘书等工作面向女性，而看门人和出租车司机等工作面向男性，尤其是有色人种。该模型还发现，当展示给白人看时，房地产广告的点击率更高，这导致少数族裔缺乏房地产广告。

这只是人工智能偏见的几个常见例子。在开发者不知情的情况下，有很多不公平的人工智能实践的例子。

## 你能如何修理它？

![](img/55bd50fada12f87c871fbdd5466b54f2.png)

照片由 [Unsplash](https://unsplash.com/s/photos/code?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的 [Yancy Min](https://unsplash.com/@yancymin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 拍摄

迈向公平人工智能的第一步是承认问题。AI 是不完美的。数据是不完美的。我们的算法并不完美。我们的技术并不完美。当我们假装没有问题时，是不可能找到解决办法的。

其次，问问自己这个解决方案是否需要 AI。

> 不要害怕推出没有机器学习的产品——谷歌

有几个问题不依赖于数据。像寻找被告再犯的可能性这样的任务更多地依赖于情感而不是数据。

第三，遵循负责任的人工智能实践。我在下面添加了谷歌负责任的人工智能实践指南的要点。

**负责任的人工智能实践:**

1.  **使用以人为中心的设计方法:**设计模型，内置适当的信息披露，并在部署之前整合来自测试人员的反馈。
2.  **确定多个指标来评估培训和监控:**使用适合任务的不同指标来了解不同错误和体验之间的权衡。这些度量可以是来自消费者的反馈、假阳性和假阴性率等。
3.  **如果可能的话，检查你的原始数据:**人工智能模型反映了用于训练模型的数据。如果数据有错，模型也会有错。尽量拥有平衡的数据。
4.  **了解你的模型的局限性:**一个被训练来检测相关性的模型不一定有助于建立因果关系。例如，一个模型可能会了解到购买篮球鞋的人一般来说会更高，但这并不意味着购买篮球鞋的用户会因此变高。
5.  **测试:**进行严格的单元测试，以确定模型中的错误。
6.  **在部署后继续监控和更新您的模型:**考虑用户反馈，并在部署后定期更新您的模型。
7.  **设计一个具有公平和包容具体目标的模型:**与道德和社会研究领域的专家合作，了解并解释各种观点。尽量让你的模型尽可能的公平。
8.  **使用有代表性的数据集来训练和测试你的模型:**尝试评估你的数据的公平性。即寻找特征和标签之间的偏见或歧视性的相关性。
9.  **检查不公平的偏见:**从不同背景的测试人员那里获得单元测试输入。这有助于确定哪一组人可能会受到模型的影响。
10.  **分析性能:**考虑不同的指标。一个指标的改进可能会损害另一个指标的性能。

## 开发公平人工智能的工具

1.  **FATE:** 人工智能中的公平、问责、透明和道德(FATE)，由微软提供评估可视化仪表板和偏见缓解算法的工具。它主要用于比较系统的公平性和性能之间的折衷。
2.  **AI Fairness 360:**AI Fairness 360 是 IBM 提供的开源工具包，可以帮助你检查、报告、减轻机器学习模型中的歧视和偏见。
3.  **ML Fairness Gym:**ML Fairness Gym 是谷歌提供的一个工具，用于探索机器学习系统对人工智能偏见的长期影响。

## 结论

在过去的几年里，公司和政府已经开始认真对待人工智能偏见。许多公司开发了评估人工智能公平性的工具，并尽最大努力打击人工智能偏见。虽然人工智能具有巨大的潜力，但现在对我们来说，记住人工智能系统的潜在歧视性危险并帮助开发公平的人工智能模型比以往任何时候都更重要。

# 关于我

我喜欢写一些较少谈论的人工智能主题，如联邦学习，图形神经网络，人工智能中的公平，量子机器学习，TinyML 等。关注我，了解我未来所有博客的最新动态。

你可以在 [Twitter](https://twitter.com/CleanPegasus) 、 [LinkedIn](https://www.linkedin.com/in/arunkumar-l/) 和 [Github](https://github.com/CleanPegasus) 上找到我。