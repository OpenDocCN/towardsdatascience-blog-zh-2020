<html>
<head>
<title>Pytorch [Basics] — Intro to Dataloaders and Loss Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[基础知识] —数据加载器和损失函数简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047?source=collection_archive---------14-----------------------#2020-02-01">https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047?source=collection_archive---------14-----------------------#2020-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="86fb" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="c2ac" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带您了解 PyTorch 中的数据加载器和不同类型的损失函数。</h2></div><p id="4888" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这篇博文中，我们将看到自定义数据集和数据加载器的简短实现，以及一些常见的损失函数。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="abf8" class="mg mh jf bd mi mj mk ml mm mn mo mp mq ku mr kv ms kx mt ky mu la mv lb mw mx bi translated">数据集和数据加载器</h1><figure class="mz na nb nc gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6470b727327f514fe1985cdebbe7afd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*k44TAbQX915txv-HfwfuZw.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据加载器 meme [Image [1]]</p></figure><p id="1b02" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">使用 3 个主要组件创建一个定制的数据集类。</p><ul class=""><li id="0cf8" class="nd ne jf lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated"><code class="fe nm nn no np b">__init__</code></li><li id="8d7d" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated"><code class="fe nm nn no np b">__len__</code></li><li id="155a" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated"><code class="fe nm nn no np b">__getitem__</code></li></ul><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="cc6a" class="nz mh jf np b gy oa ob l oc od">class CustomDataset(Dataset):<br/>    def __init__(self):<br/>        pass</span><span id="6f1c" class="nz mh jf np b gy oe ob l oc od">    def __getitem__(self, index):<br/>        pass</span><span id="3098" class="nz mh jf np b gy oe ob l oc od">    def __len__(self):<br/>        pass</span></pre><p id="1a63" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">__init__</code>:用于执行读取数据、预处理等初始化操作。<br/> <code class="fe nm nn no np b">__len__</code>:返回输入数据的大小。<br/> <code class="fe nm nn no np b">__getitem__</code>:批量返回数据(输入输出)。</p><p id="32b9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后在这个<strong class="lf jp">数据集</strong>类上使用一个<strong class="lf jp">数据加载器</strong>来批量读取数据。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="cb71" class="nz mh jf np b gy oa ob l oc od">train_loader = DataLoader(custom_dataset_object, batch_size=32, shuffle=True)</span></pre><p id="68d8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们实现一个基本的 PyTorch 数据集和数据加载器。假设你有输入和输出数据-</p><p id="a75f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">X</code> : 1，2，3，4，5，6，7，8，9，10</p><p id="75d0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y</code> : 0，0，0，1，0，1，1，0，0，1</p><p id="1503" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们定义数据集类。我们将返回一个元组(输入，输出)。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="968d" class="nz mh jf np b gy oa ob l oc od">class CustomDataset(Dataset):<br/>    <br/>    def __init__(self, X_data, y_data):<br/>        self.X_data = X_data<br/>        self.y_data = y_data<br/>        <br/>    def __getitem__(self, index):<br/>        return self.X_data[index], self.y_data[index]<br/>        <br/>    def __len__ (self):<br/>        return len(self.X_data)</span></pre><p id="312a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">初始化数据集对象。输入必须是张量类型。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="3df8" class="nz mh jf np b gy oa ob l oc od">data = CustomDataset(torch.FloatTensor(X), torch.FloatTensor(y))</span></pre><p id="34c2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们使用方法<code class="fe nm nn no np b">__len__()</code>和<code class="fe nm nn no np b">__getitem__()</code>。<code class="fe nm nn no np b">__getitem__()</code>将索引作为输入。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="90b1" class="nz mh jf np b gy oa ob l oc od">data.__len__()</span><span id="454a" class="nz mh jf np b gy oe ob l oc od">################### OUTPUT #####################</span><span id="0ea1" class="nz mh jf np b gy oe ob l oc od"><br/>10</span></pre><p id="7668" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">从输出数据中打印出第 4 个元素(第 3 个索引)。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="5bf7" class="nz mh jf np b gy oa ob l oc od">data.__getitem__(3)</span><span id="942e" class="nz mh jf np b gy oe ob l oc od">################### OUTPUT #####################</span><span id="012d" class="nz mh jf np b gy oe ob l oc od"><br/>(tensor(4.), tensor(1.))</span></pre><p id="6fd9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们现在初始化我们的数据加载器。在这里，我们指定批量大小和洗牌。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="35d9" class="nz mh jf np b gy oa ob l oc od">data_loader = DataLoader(dataset=data, batch_size=2, shuffle=True)</span><span id="234c" class="nz mh jf np b gy oe ob l oc od">data_loader_iter = iter(data_loader)<br/>print(next(data_loader_iter))</span><span id="1943" class="nz mh jf np b gy oe ob l oc od">################### OUTPUT #####################</span><span id="3771" class="nz mh jf np b gy oe ob l oc od"><br/>[tensor([3., 6.]), tensor([0., 1.])]</span></pre><p id="bf41" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们使用带有 for 循环的数据加载器。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="d4fb" class="nz mh jf np b gy oa ob l oc od">for i,j in data_loader:<br/>    print(i,j)</span><span id="5ef2" class="nz mh jf np b gy oe ob l oc od">################### OUTPUT #####################</span><span id="cf79" class="nz mh jf np b gy oe ob l oc od">tensor([ 1., 10.]) tensor([0., 1.])<br/>tensor([4., 6.]) tensor([1., 1.])<br/>tensor([7., 5.]) tensor([1., 0.])<br/>tensor([9., 3.]) tensor([0., 0.])<br/>tensor([2., 8.]) tensor([0., 0.])</span></pre></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="2a21" class="mg mh jf bd mi mj mk ml mm mn mo mp mq ku mr kv ms kx mt ky mu la mv lb mw mx bi translated">损失函数</h1><figure class="mz na nb nc gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0bae239fd02c79c86438826270db97ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*CjOlwzjtWfoKeL6T4eZwwg.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">损失函数 meme [Image [2]]</p></figure><p id="3a40" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">以下是不同深度学习任务常用的损失函数。</p><p id="a6d4" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">回归:</p><ul class=""><li id="82db" class="nd ne jf lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated">平均绝对误差— <code class="fe nm nn no np b">torch.nn.L1Loss()</code></li><li id="7ca3" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated">均方差— <code class="fe nm nn no np b">torch.nn.MSELoss()</code></li></ul><p id="dd8e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">分类:</p><ul class=""><li id="d047" class="nd ne jf lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated">二元交叉熵损失— <code class="fe nm nn no np b">torch.nn.BCELoss()</code></li><li id="01fb" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated">具有罗吉斯损失的二元交叉熵— <code class="fe nm nn no np b">torch.nn.BCEWithLogitsLoss()</code></li><li id="7463" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated">负对数可能性— <code class="fe nm nn no np b">torch.nn.NLLLoss()</code></li><li id="284e" class="nd ne jf lf b lg nq lj nr lm ns lq nt lu nu ly ni nj nk nl bi translated">交叉斜视— <code class="fe nm nn no np b">torch.nn.CrossEntropyLoss()</code></li></ul><p id="17d4" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">从官方<a class="ae og" href="https://pytorch.org/docs/stable/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank"> PyTorch 文档</a>了解更多损失函数。</p><h1 id="5dd7" class="mg mh jf bd mi mj oh ml mm mn oi mp mq ku oj kv ms kx ok ky mu la ol lb mw mx bi translated">导入库</h1><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="358d" class="nz mh jf np b gy oa ob l oc od">import torch<br/>import torch.nn as nn</span></pre><h1 id="a6ae" class="mg mh jf bd mi mj oh ml mm mn oi mp mq ku oj kv ms kx ok ky mu la ol lb mw mx bi translated">回归</h1><p id="efde" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">为了计算损失，让我们从定义实际和预测输出张量开始。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="30cd" class="nz mh jf np b gy oa ob l oc od">y_pred = torch.tensor([[1.2, 2.3, 3.4], [4.5, 5.6, 6.7]], requires_grad=True)</span><span id="41bd" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)<br/>print("\nY Pred shape: ", y_pred.shape, "\n")</span><span id="e742" class="nz mh jf np b gy oe ob l oc od">print("=" * 50)</span><span id="87ac" class="nz mh jf np b gy oe ob l oc od">y_train = torch.tensor([[1.2, 2.3, 3.4], [7.8, 8.9, 9.1]])<br/>print("\nY Train: \n", y_train)<br/>print("\nY Train shape: ", y_train.shape)</span><span id="75d6" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="5f6e" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000]], requires_grad=True)</span><span id="fd0c" class="nz mh jf np b gy oe ob l oc od">Y Pred shape:  torch.Size([2, 3]) </span><span id="1d53" class="nz mh jf np b gy oe ob l oc od">==================================================</span><span id="7e43" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]])</span><span id="0065" class="nz mh jf np b gy oe ob l oc od">Y Train shape:  torch.Size([2, 3])</span></pre><h2 id="116e" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">平均绝对误差— <code class="fe nm nn no np b">torch.nn.L1Loss()</code></h2><p id="5234" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">输入和输出必须与<strong class="lf jp">尺寸相同</strong>，并且具有数据类型<strong class="lf jp">浮动</strong>。</p><p id="f7e2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, *)</code>和<code class="fe nm nn no np b">y_train = (batch_size, *)</code>。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="1158" class="nz mh jf np b gy oa ob l oc od">mae_loss = nn.L1Loss()</span><span id="a168" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="9c69" class="nz mh jf np b gy oe ob l oc od">print("Y Train: \n", y_train)</span><span id="6738" class="nz mh jf np b gy oe ob l oc od">output = mae_loss(y_pred, y_train)<br/>print("MAE Loss\n", output)</span><span id="0839" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="25cd" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="4806" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000]], requires_grad=True)<br/>Y Train: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]])<br/>MAE Loss<br/> tensor(1.5000, grad_fn=&lt;L1LossBackward&gt;)</span></pre><h2 id="edc9" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">均方差— <code class="fe nm nn no np b"> torch.nn.MSELoss()</code></h2><p id="29bc" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">输入和输出必须与<strong class="lf jp">尺寸相同</strong>，并且具有数据类型<strong class="lf jp">浮动</strong>。</p><p id="de78" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, *)</code>和<code class="fe nm nn no np b">y_train = (batch_size, *)</code>。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="311a" class="nz mh jf np b gy oa ob l oc od">mse_loss = nn.MSELoss()</span><span id="77df" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="b57a" class="nz mh jf np b gy oe ob l oc od">print("Y Train: \n", y_train)</span><span id="764c" class="nz mh jf np b gy oe ob l oc od">output = mse_loss(y_pred, y_train)<br/>print("MSE Loss\n", output)</span><span id="a97c" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="a420" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="e6cc" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000]], requires_grad=True)<br/>Y Train: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]])<br/>MSE Loss<br/> tensor(4.5900, grad_fn=&lt;MseLossBackward&gt;)</span></pre><h1 id="b750" class="mg mh jf bd mi mj oh ml mm mn oi mp mq ku oj kv ms kx ok ky mu la ol lb mw mx bi translated">二元分类</h1><p id="3a5c" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated"><code class="fe nm nn no np b">y_train</code>有两个类——0 和 1。当网络的最终输出是介于 0 和 1 之间的单个值(最终密集层的大小为 1)时，我们使用 BCE 损失函数。</p><p id="27ab" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果网络的输出是长度为 2 的张量(最终密集层的大小为 2 ),其中两个值都位于 0 和 1 之间，则二进制分类可以重新构造为使用<strong class="lf jp"> NLLLoss </strong>或<strong class="lf jp">交叉熵</strong>损失。</p><p id="6ae2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们定义实际和预测的输出张量，以便计算损失。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="4313" class="nz mh jf np b gy oa ob l oc od">y_pred = torch.tensor([[1.2, 2.3, 3.4], [7.8, 8.9, 9.1]], requires_grad = True)<br/>print("Y Pred: \n", y_pred)<br/>print("\nY Pred shape: ", y_pred.shape, "\n")</span><span id="ca46" class="nz mh jf np b gy oe ob l oc od">print("=" * 50)</span><span id="f011" class="nz mh jf np b gy oe ob l oc od">y_train = torch.tensor([[1, 0, 1], [0, 0, 1]])<br/>print("\nY Train: \n", y_train)<br/>print("\nY Train shape: ", y_train.shape)</span><span id="b5e6" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="5fa7" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="df93" class="nz mh jf np b gy oe ob l oc od">Y Pred shape:  torch.Size([2, 3]) </span><span id="8a20" class="nz mh jf np b gy oe ob l oc od">==================================================</span><span id="2834" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([[1, 0, 1],<br/>        [0, 0, 1]])</span><span id="3626" class="nz mh jf np b gy oe ob l oc od">Y Train shape:  torch.Size([2, 3])</span></pre><h2 id="f3f5" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">二元交叉熵损失— <code class="fe nm nn no np b">torch.nn.BCELoss()</code></h2><p id="4447" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">输入和输出必须与<strong class="lf jp">尺寸相同</strong>并具有浮动的数据类型。</p><p id="6d40" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, *)</code>，浮点型(值应通过一个 Sigmoid 函数传递，其值介于 0 和 1 之间)</p><p id="df8f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_train = (batch_size, *)</code>，浮动</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="6fef" class="nz mh jf np b gy oa ob l oc od">bce_loss = nn.BCELoss()</span><span id="7d69" class="nz mh jf np b gy oe ob l oc od">y_pred_sigmoid = torch.sigmoid(y_pred)</span><span id="8194" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="f006" class="nz mh jf np b gy oe ob l oc od">print("\nY Pred Sigmoid: \n", y_pred_sigmoid)</span><span id="d505" class="nz mh jf np b gy oe ob l oc od">print("\nY Train: \n", y_train.float())</span><span id="2e31" class="nz mh jf np b gy oe ob l oc od">output = bce_loss(y_pred_sigmoid, y_train.float())<br/>print("\nBCE Loss\n", output)</span><span id="2e7b" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="ee24" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="b670" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="b446" class="nz mh jf np b gy oe ob l oc od">Y Pred Sigmoid: <br/> tensor([[0.7685, 0.9089, 0.9677],<br/>        [0.9996, 0.9999, 0.9999]], grad_fn=&lt;SigmoidBackward&gt;)</span><span id="e79e" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([[1., 0., 1.],<br/>        [0., 0., 1.]])</span><span id="72e0" class="nz mh jf np b gy oe ob l oc od">BCE Loss<br/> tensor(3.2321, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)</span></pre><h2 id="5b0d" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">具有罗吉斯损失的二元交叉熵— <code class="fe nm nn no np b">torch.nn.BCEWithLogitsLoss()</code></h2><p id="678b" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">输入和输出必须是相同尺寸的<strong class="lf jp">和浮动的</strong>。该类将<strong class="lf jp">s 形</strong>和<strong class="lf jp">b 形</strong>组合成一个类。这个版本在数值上比单独使用 Sigmoid 和 BCELoss 更稳定。</p><p id="0a37" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, *)</code>，浮动</p><p id="a02f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_train = (batch_size, *)</code>，浮动</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="45bc" class="nz mh jf np b gy oa ob l oc od">bce_logits_loss = nn.BCEWithLogitsLoss()</span><span id="c5f2" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="194d" class="nz mh jf np b gy oe ob l oc od">print("\nY Train: \n", y_train.float())</span><span id="e52f" class="nz mh jf np b gy oe ob l oc od">output = bce_logits_loss(y_pred, y_train.float())<br/>print("\nBCE Loss\n", output)</span><span id="dee3" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="545d" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="a4c0" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="37da" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([[1., 0., 1.],<br/>        [0., 0., 1.]])</span><span id="14af" class="nz mh jf np b gy oe ob l oc od">BCE Loss<br/> tensor(3.2321, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)</span></pre><h1 id="a422" class="mg mh jf bd mi mj oh ml mm mn oi mp mq ku oj kv ms kx ok ky mu la ol lb mw mx bi translated">多类分类</h1><p id="d0bc" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">让我们定义实际和预测的输出张量，以便计算损失。</p><p id="25e9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_train</code>有 4 个等级——0、1、2 和 3。</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="4de9" class="nz mh jf np b gy oa ob l oc od">y_pred = torch.tensor([[1.2, 2.3, 3.4], [4.5, 5.6, 6.7], [7.8, 8.9, 9.1]], requires_grad = True)<br/>print("Y Pred: \n", y_pred)<br/>print("\nY Pred shape: ", y_pred.shape, "\n")</span><span id="224a" class="nz mh jf np b gy oe ob l oc od">print("=" * 50)</span><span id="d327" class="nz mh jf np b gy oe ob l oc od">y_train = torch.tensor([0, 1, 2])<br/>print("\nY Train: \n", y_train)<br/>print("\nY Train shape: ", y_train.shape)</span><span id="8e4f" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="6254" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="1b4f" class="nz mh jf np b gy oe ob l oc od">Y Pred shape:  torch.Size([3, 3]) </span><span id="d1c0" class="nz mh jf np b gy oe ob l oc od">==================================================</span><span id="3a10" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([0, 1, 2])</span><span id="5a6f" class="nz mh jf np b gy oe ob l oc od">Y Train shape:  torch.Size([3])</span></pre><h2 id="996a" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">负对数可能性— <code class="fe nm nn no np b">torch.nn.NLLLoss()</code></h2><p id="a2c7" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, num_classes)</code>，Float(值应传递使用 log_softmax 函数获得的对数概率。</p><p id="e6e3" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_train = (batch_size)</code>，长整型(取值范围= 0，num_classes-1)。类别必须从 0、1、2 开始，...</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="3e66" class="nz mh jf np b gy oa ob l oc od">nll_loss = nn.NLLLoss()</span><span id="d097" class="nz mh jf np b gy oe ob l oc od">y_pred_logsoftmax = torch.log_softmax(y_pred, dim = 1)</span><span id="17bb" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="9983" class="nz mh jf np b gy oe ob l oc od">print("\nY Pred LogSoftmax: \n", y_pred_logsoftmax)</span><span id="9f96" class="nz mh jf np b gy oe ob l oc od">print("\nY Train: \n", y_train)</span><span id="c9ae" class="nz mh jf np b gy oe ob l oc od">output = nll_loss(y_pred_logsoftmax, y_train)<br/>print("\nNLL Loss\n", output)</span><span id="8e3a" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="f241" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="1008" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="8bb7" class="nz mh jf np b gy oe ob l oc od">Y Pred LogSoftmax: <br/> tensor([[-2.5672, -1.4672, -0.3672],<br/>        [-2.5672, -1.4672, -0.3672],<br/>        [-2.0378, -0.9378, -0.7378]], grad_fn=&lt;LogSoftmaxBackward&gt;)</span><span id="647a" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([0, 1, 2])</span><span id="21a1" class="nz mh jf np b gy oe ob l oc od">NLL Loss<br/> tensor(1.5907, grad_fn=&lt;NllLossBackward&gt;)</span></pre><h2 id="4fe3" class="nz mh jf bd mi or os dn mm ot ou dp mq lm ov ow ms lq ox oy mu lu oz pa mw jl bi translated">交叉斜视— <code class="fe nm nn no np b">torch.nn.CrossEntropyLoss()</code></h2><p id="2a96" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">这个类将<strong class="lf jp"> LogSoftmax </strong>和<strong class="lf jp"> NLLLoss </strong>组合成一个类。</p><p id="eaaf" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nm nn no np b">y_pred = (batch_size, num_classes)</code>、Float <br/> <code class="fe nm nn no np b">y_train = (batch_size)</code>、Long(取值范围= 0，num_classes-1)。类别必须从 0、1、2 开始，...</p><pre class="mz na nb nc gt nv np nw nx aw ny bi"><span id="025a" class="nz mh jf np b gy oa ob l oc od">ce_loss = nn.CrossEntropyLoss()</span><span id="ae62" class="nz mh jf np b gy oe ob l oc od">print("Y Pred: \n", y_pred)</span><span id="3757" class="nz mh jf np b gy oe ob l oc od">print("\nY Train: \n", y_train)</span><span id="361e" class="nz mh jf np b gy oe ob l oc od">output = ce_loss(y_pred, y_train)<br/>print("\nNLL Loss\n", output)</span><span id="f3cb" class="nz mh jf np b gy oe ob l oc od">output.backward()</span><span id="d0fd" class="nz mh jf np b gy oe ob l oc od"><br/>###################### OUTPUT ######################</span><span id="1e03" class="nz mh jf np b gy oe ob l oc od">Y Pred: <br/> tensor([[1.2000, 2.3000, 3.4000],<br/>        [4.5000, 5.6000, 6.7000],<br/>        [7.8000, 8.9000, 9.1000]], requires_grad=True)</span><span id="f397" class="nz mh jf np b gy oe ob l oc od">Y Train: <br/> tensor([0, 1, 2])</span><span id="d2c0" class="nz mh jf np b gy oe ob l oc od">NLL Loss<br/> tensor(1.5907, grad_fn=&lt;NllLossBackward&gt;)</span></pre></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="ced5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)<em class="pb"> </em>你可以在<a class="ae og" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae og" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>找到我。如果你喜欢这个，看看我的其他<a class="ae og" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p></div></div>    
</body>
</html>