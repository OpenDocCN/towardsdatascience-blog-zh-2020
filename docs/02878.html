<html>
<head>
<title>Machine Learning: Model Selection and Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:模型选择和超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-model-selection-and-hyperparameter-tuning-736158357dc4?source=collection_archive---------30-----------------------#2020-03-19">https://towardsdatascience.com/machine-learning-model-selection-and-hyperparameter-tuning-736158357dc4?source=collection_archive---------30-----------------------#2020-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d75a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">网格搜索是发现更好的超参数的一种简单明了的方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c9d9fccf5b01b7e4cb91e0ece026a486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LXzvZfMXoHMX9ohhoA8bgA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://pixabay.com/users/andreas160578-2383079/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2138981" rel="noopener ugc nofollow" target="_blank"> andreas160578 </a>来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2138981" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="7ecd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的日常研究中，我经常面临的一个问题是选择一个合适的统计模型来拟合我的数据。大多数情况下，我会使用统计模型来消除DNA数据中的错误信号，我相信选择一个模型来解释数据的行为是数据科学爱好者的共同关注。我认为分享如何使用现成的库来解决这种情况是有用的。此外，选择一个模型是不够的，除非你知道最合适的超参数进展。让我们看一个简单的场景来理解它是如何实现的。</p><h1 id="986b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预测要求</h1><p id="aa5f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">预测要求可以有几种。主要我们可以看到两种<strong class="ky ir">插值</strong>和<strong class="ky ir">外推</strong>。在<strong class="ky ir">插值</strong>中，我们尝试预测缺失变量值的标签值。简而言之，我们知道周围的数据点，并尝试预测同一功能域内的不同数据点。相反，<strong class="ky ir">外推</strong>旨在预测稍微偏离现有数据点的值。例如，根据前两天和今天的降雨量，预测明天的降雨量。</p><h1 id="585b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一个示例场景</h1><p id="4ee6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">模拟这种场景的最简单的方法是使用一个已知的函数并检查它的行为。让我们以<code class="fe mp mq mr ms b">y = 5000*sin(x)</code>为例。如果我们看看生成的代码和图，它看起来像下面这样。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="5c87" class="mx lt iq ms b gy my mz l na nb">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="6f2e" class="mx lt iq ms b gy nc mz l na nb">x = np.arange(0,6,0.005)<br/>y = np.sin(x)*5000</span><span id="822e" class="mx lt iq ms b gy nc mz l na nb">fig = plt.figure(figsize=(10,5))<br/>plt.plot(x, y, label="y=sin(x)")<br/>plt.xlabel("X")<br/>plt.ylabel("sin(X)")<br/>plt.legend(loc="upper left")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/359cc267a1b569bbb5248cf11bd000ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roErvy3Ye-zytYV-9UHi2g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">y =正弦(x)曲线</p></figure><h1 id="437a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">可能的回归变量</h1><p id="1863" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，我们已经看到了图表或趋势，让我们假设我们只有数据点，我们需要开发一个回归变量，可以满足<code class="fe mp mq mr ms b">x=8</code>。</p><h2 id="2551" class="mx lt iq bd lu ne nf dn ly ng nh dp mc lf ni nj me lj nk nl mg ln nm nn mi no bi translated">随机森林回归</h2><p id="35fa" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">随机森林是一种流行的回归技术。然而，随机森林对于过度适应的问题(对于训练集来说过于专门化)是相当流行的。下面是使用随机森林回归的代码和预测。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="640f" class="mx lt iq ms b gy my mz l na nb">from sklearn.ensemble import RandomForestRegressor<br/>from sklearn.model_selection import train_test_split</span><span id="2428" class="mx lt iq ms b gy nc mz l na nb">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=10)</span><span id="6d4d" class="mx lt iq ms b gy nc mz l na nb">forest_reg = RandomForestRegressor(n_estimators=1, random_state=10)<br/>forest_reg.fit(x_train.reshape(-1, 1), y)<br/>y_hat = forest_reg.predict(x_test.reshape(-1, 1))</span><span id="fd3f" class="mx lt iq ms b gy nc mz l na nb">fig = plt.figure(figsize=(10,5))<br/>plt.scatter(x_test, y_test, label="Truth")<br/>plt.scatter(x_test, y_hat, label="Prediction")<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.legend(loc="upper left")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/d59a65bfe10f6b4e65524e19a5cf909f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvZsjLg4oCSYPq8QkeWHzw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对一组值的预测</p></figure><p id="9293" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以清楚地看到，模型非常接近预测值。这是好事。请注意，我们将训练集和测试集分开，以确保没有明显的过度拟合。然而，如果我们看看未来的预测，我们可以看到一些有趣的事情。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="cf10" class="mx lt iq ms b gy my mz l na nb">x_next = np.arange(0,7,0.005)<br/>y_next = np.sin(x_next)*5000</span><span id="50ca" class="mx lt iq ms b gy nc mz l na nb">y_next_hat = forest_reg.predict(x_next.reshape(-1, 1))<br/>fig = plt.figure(figsize=(10,5))<br/>plt.plot(x_next, y_next, label="y=sin(x)", alpha=0.5)<br/>plt.plot(x_next, y_next_hat, label="Predicted", alpha=0.5)<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.legend(loc="upper left")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/d1340dab39ecb8dad99e2c3702f50fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LKkYztiFLsnrDmMJHQBDw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">未来预测</p></figure><p id="1e73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哎呀！。我们可以看到，超出训练极限的数值是没有意义的。这是因为随机森林学会了只在训练范围内预测，或者只是插值。这就是多项式回归派上用场的地方。</p><h2 id="a811" class="mx lt iq bd lu ne nf dn ly ng nh dp mc lf ni nj me lj nk nl mg ln nm nn mi no bi translated">多项式回归</h2><p id="37e8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">多项式回归试图用多项式函数来表达数据。然而，多项式拟合的次数是一个无法学习的超参数。让我们来看看数据的多项式拟合。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="3be2" class="mx lt iq ms b gy my mz l na nb">from sklearn.linear_model import Ridge<br/>from sklearn.preprocessing import PolynomialFeatures<br/>from sklearn.pipeline import make_pipeline</span><span id="2d79" class="mx lt iq ms b gy nc mz l na nb">model = make_pipeline(PolynomialFeatures(3), Ridge())<br/>model.fit(x_train.reshape(-1, 1), y_train)<br/>y_next_hat = model.predict(x_next.reshape(-1, 1))</span><span id="8895" class="mx lt iq ms b gy nc mz l na nb">x_next = np.arange(0,7,0.005)<br/>y_next = np.sin(x_next)*5000</span><span id="64c2" class="mx lt iq ms b gy nc mz l na nb">fig = plt.figure(figsize=(10,5))<br/>plt.plot(x_next, y_next, label="y=sin(x)")<br/>plt.plot(x_next, y_next_hat, label="Predicted")<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.legend(loc="upper left")<br/>plt.savefig("poly_fit.png")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/47bcc3b5c1def38ca965aa73a83933b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53jrKFt6GS3wQQoywmnXVQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三次多项式拟合</p></figure><p id="7bcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，虽然我对多项式次数为3的猜测不是很合理。然而，我可以继续输入值并进行测试。但是sklearn有一个更聪明的方法。</p><h1 id="f064" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">超参数调谐的网格搜索</h1><p id="2434" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Sklearn库为我们提供了定义参数网格和选择最佳参数的功能。让我们看看如何用它来决定我们预测的适当程度。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="e004" class="mx lt iq ms b gy my mz l na nb">from sklearn.model_selection import GridSearchCV<br/>from sklearn.pipeline import Pipeline</span><span id="4df9" class="mx lt iq ms b gy nc mz l na nb">param_grid = [<br/>    {'poly__degree': [2, 3, 4, 5, 6, 7, 8, 9]}<br/>  ]</span><span id="e853" class="mx lt iq ms b gy nc mz l na nb">pipeline = Pipeline(steps=[('poly', PolynomialFeatures()), ('ridge', Ridge())])</span><span id="4577" class="mx lt iq ms b gy nc mz l na nb">grid_search = GridSearchCV(pipeline, param_grid, cv=5,<br/>                           scoring='neg_mean_squared_error',<br/>                           return_train_score=True)</span><span id="8dcc" class="mx lt iq ms b gy nc mz l na nb">grid_search.fit(x_train.reshape(-1, 1), y_train)</span></pre><p id="2a7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们选择2到8度进行实验。这种网格搜索可以用于任何其他模型，您可以拥有尽可能多的参数。在这种情况下，我们试图找到使<code class="fe mp mq mr ms b">neg_mean_squared_error</code>最小的一组参数，或者选择具有最小均方误差的点。然而，对于更大的参数空间，推荐使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">随机化搜索</a>。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="2a3d" class="mx lt iq ms b gy my mz l na nb">&gt;&gt; grid_search.best_params_<br/>{'poly__degree': 5}</span></pre><p id="07c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mp mq mr ms b">best_params_</code>将输出最佳参数集作为python字典。就这么简单。</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="c044" class="mx lt iq ms b gy my mz l na nb">y_next_hat = grid_search.predict(x_next.reshape(-1, 1))</span><span id="fad5" class="mx lt iq ms b gy nc mz l na nb">x_next = np.arange(0,7,0.005)<br/>y_next = np.sin(x_next)*5000</span><span id="ae0f" class="mx lt iq ms b gy nc mz l na nb">fig = plt.figure(figsize=(10,5))<br/>plt.plot(x_next, y_next, label="y=sin(x)")<br/>plt.plot(x_next, y_next_hat, label="Predicted")<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.legend(loc="upper left")</span></pre><p id="f13a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们使用<code class="fe mp mq mr ms b">grid_search</code>进行预测，我们可以看到我们对潜在的未来X值有一个更好的估计。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/138a55c7b3b1fd53b1eb8e47b059f219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98kk6Ceyez725lLWS0ucdg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">五次多项式拟合</p></figure><p id="0b7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，预测的多项式仅在远离训练集的较小范围内成立。这是因为我们的函数<code class="fe mp mq mr ms b">sin(x)</code>是一个周期函数，除非我们提供所有可能的实数值，否则不存在完美的多项式拟合。</p><p id="6cc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章能够帮助您使用python内置的网格搜索功能进行超参数调优。这也可以用于更复杂的场景，例如具有预定义集群大小的集群、用于优化的可变ε值等。</p><p id="3131" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您正在寻找一个支持参数调整的ML工具，请查看下面的链接；</p><div class="np nq gp gr nr ns"><a href="https://neptune.ai/blog/optuna-vs-hyperopt" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd ir gy z fp nx fr fs ny fu fw ip bi translated">Optuna vs Hyperopt:应该选择哪个超参数优化库？- neptune.ai</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">思考应该选择哪个库进行超参数优化？使用远视有一段时间了，感觉像…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">海王星. ai</p></div></div><div class="ob l"><div class="oc l od oe of ob og kp ns"/></div></div></a></div><p id="0fa5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你好。</p></div></div>    
</body>
</html>