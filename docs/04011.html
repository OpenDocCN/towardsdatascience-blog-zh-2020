<html>
<head>
<title>Latent Dirichlet Allocation(LDA): A guide to probabilistic modelling approach for topic discovery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在狄利克雷分配(LDA):主题发现的概率建模方法指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-a-guide-to-probabilistic-modeling-approach-for-topic-discovery-8cb97c08da3c?source=collection_archive---------29-----------------------#2020-04-13">https://towardsdatascience.com/latent-dirichlet-allocation-lda-a-guide-to-probabilistic-modeling-approach-for-topic-discovery-8cb97c08da3c?source=collection_archive---------29-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="13cf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">潜在狄利克雷分配在python中的实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/aad3ac15c5d0819edb9c4c39f8b72327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPrSqAvRexBnr-wqOIn2Sg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://www.widewallpapershd.info/preview/26253/3840x2160/newspapers.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="403e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">潜在狄利克雷分配(LDA)是主题建模中最常用的算法之一。LDA由J. K. Pritchard、M. Stephens和P. Donnelly于2000年提出，并由David M. Blei、Andrew Y. Ng和Michael I. Jordan于2003年重新发现。在这篇文章中，我将试着给你一个什么是主题建模的概念。我们将学习LDA如何工作，最后，我们将尝试实现我们的LDA模型。</p><h1 id="4d1c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是主题建模？</h1><p id="0e41" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">主题建模是机器学习和自然语言处理中最有趣的领域之一。主题建模意味着从文档集合中提取抽象的“主题”。自然语言处理的主要应用之一是在大量的文本文档中了解人们在谈论什么。通读所有这些文档并提取或编辑主题真的很难。在这些情况下，主题建模用于提取文档信息。为了理解主题建模的概念，让我们看一个例子。</p><p id="99cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设你正在读报纸上的一些文章，在这些文章中,“气候”这个词出现得最多。所以，从正常意义上来说，你可以说这些文章更有可能是关于气候的。主题建模以统计的方式做同样的事情。它通过聚集相似的单词来产生话题。这里有两个术语:一个是“主题建模”，另一个是“主题分类”。虽然它们看起来相似，但它们是完全不同的过程。第一种是非监督机器学习技术，第二种是监督技术。<br/>让我们详细阐述一下这个概念。</p><p id="9573" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主题分类通常涉及互斥的类别。这意味着每个文档都标有特定的类别。另一方面，主题建模并不相互排斥。同一份文件可能涉及许多主题。由于主题建模是基于概率分布工作的，所以同一文档可能具有跨越许多主题的概率分布。</p><p id="4ece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于主题建模，您可以使用几种现有的算法。<em class="ms">非负矩阵分解(NMF) </em>、<em class="ms">潜在语义分析或潜在语义索引(LSA或LSI) </em>和<em class="ms">潜在狄利克雷分配(LDA) </em>是这些算法中的一些。在本文中，我们将讨论潜在的狄利克雷分配，这是主题建模中最常见的算法之一。</p><h1 id="50be" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">潜在狄利克雷分配(LDA) </strong></h1><p id="3681" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">"<em class="ms"/><strong class="lb iu"><em class="ms">潜在狄利克雷分配</em></strong><em class="ms">(</em><strong class="lb iu"><em class="ms">LDA</em></strong><em class="ms">)是一个生成统计模型，它允许观察集由未观察到的组来解释，这解释了为什么数据的某些部分是相似的。例如，如果观察是收集到文档中的单词，它假设每个文档都是少量主题的混合物，并且每个单词的出现都归因于文档的一个主题</em>。—维基百科</p><p id="aabb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，让我们试着理解这个定义。</p><p id="11aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">L <strong class="lb iu"> <em class="ms"> atent Dirichlet分配</em></strong><em class="ms">(</em><strong class="lb iu"><em class="ms">LDA</em></strong><em class="ms">)</em>的基本思想是，文档被认为是各种主题的随机混合，主题被认为是不同单词的混合。现在，假设你需要一些与动物有关的文章，你面前有成千上万的文章，但你真的不知道这些文章是关于什么的。看完所有这些文章，要找出与动物相关的文章，真的很繁琐。让我们看一个例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/825d595b509d4c74f55cc59874969ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*wj27aJa--rlsMoNWGZq1mQ.png"/></div></figure><p id="bedf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，让我们考虑我们有四篇文章。第1条涉及动物，第2条涉及遗传类型，第3条涉及计算机类型，第4条是动物和遗传类型的组合。作为一个人类，你可以很容易地根据它包含的单词来区分这些主题。但是如果有几千条，每条有几千行，你会怎么做？答案会是这样的——“如果我们能在计算机的帮助下做到这一点，那么我们应该这样做”。是的，借助于<em class="ms">潜在的狄利克雷分配，计算机可以这样做。现在我们将试着理解LDA是如何工作的。首先，我们将看到LDA的图形表示，然后我们将看到概率计算公式。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d187406f5e4af91dc44583a12a4873e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*4fV2ELLFGftwBS97uLCHwQ.png"/></div></figure><p id="bddd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图是LDA的图示。在上图中，我们可以看到有六个参数-</p><p id="2d6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">α</strong>(α)和<strong class="lb iu">η</strong>(η)——代表<a class="ae ky" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>。高alpha值表示每个文档包含大多数主题，相反，较低的alpha值表示文档可能包含较少数量的主题。与alpha相同，较高的η值表示主题可能覆盖大多数单词，相反，较低的eta值表示主题可能包含较少数量的单词。</p><p id="047f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">β</strong>(β)和<strong class="lb iu">θ</strong>(θ)——代表多项式分布。</p><p id="4bd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ms"> z </em> </strong> —代表一堆话题</p><p id="6e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ms"> w </em> </strong> —代表一串单词</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7c2be13b664d787a7ffb8780ddcfc97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*xUscjIbAdyBZsPJ7rRvHQA.png"/></div></figure><p id="5128" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">公式的左边表示文档的概率。在公式的右边，有四个术语。公式的第一项和第三项将帮助我们找到主题。第二个和第四个将帮助我们找到文章中的单词。公式右侧的前两项表示狄利克雷分布，右侧的其余部分是多项式分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/3f071cef8eb89031a5378dce467af04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XNwBNJ8ipVhoCOexyFdVw.png"/></div></div></figure><p id="6d42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们假设，在上图中，左边的三角形中，蓝色的圆圈表示不同的文章。现在，如果我们将文章分布在不同的主题上，它将如直角三角形所示分布。蓝色圆圈将移动到三角形的角上，这取决于它在该主题中所占的百分比。这个过程是由公式右边的第一项完成的。现在，我们使用多项式分布根据第一个词的百分比生成主题。</p><p id="eb2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在得到主题后，我们会发现哪些单词与这些主题更相关。这是通过另一个狄利克雷分布来实现的。主题根据单词分布，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/ed7a7bc8fc5580fe5c8b17d2f328f7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBgOl5tNrRa7OVBmugqqCQ.png"/></div></div></figure><p id="8695" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将使用另一个多项式分布来查找与这些主题更相关的单词，并使用该狄利克雷分布生成具有概率的单词。这个过程进行多次。</p><p id="b25c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们将找到与主题更相关的词，并基于这些主题分发文章。</p><h1 id="2889" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">实施LDA </strong></h1><p id="4488" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你可以在<a class="ae ky" href="https://github.com/aawanRahman/latent-dirichlet-allocation/blob/master/topic_modelling_LDA.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到代码。要实现LDA，您可以使用gensim或sklearn。这里，我们将使用gensim。</p><h2 id="9ca2" class="my lw it bd lx mz na dn mb nb nc dp mf li nd ne mh lm nf ng mj lq nh ni ml nj bi translated">加载数据</h2><p id="0f4b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了实现的目的，我使用了Kaggle <a class="ae ky" href="https://www.kaggle.com/canggih/voted-kaggle-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>。该数据集由15列2150个数据集信息组成:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0e09" class="my lw it nl b gy np nq l nr ns">dataset = pd.read_csv('/content/drive/My Drive/topic modelling/voted-kaggle-dataset.csv')</span></pre><h2 id="d7ab" class="my lw it bd lx mz na dn mb nb nc dp mf li nd ne mh lm nf ng mj lq nh ni ml nj bi translated">数据预处理</h2><p id="94cd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了处理数据，首先，我们选择对这个过程有意义的列。然后删除包含任何缺失值的行。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="6e4d" class="my lw it nl b gy np nq l nr ns">modified_dataset = modified_dataset.dropna()</span></pre><p id="dd86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将计算标签列中唯一标签的数量，因为我们会将此视为模型的主题数量。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="4960" class="my lw it nl b gy np nq l nr ns">unique_tag = []<br/><strong class="nl iu">for</strong> i <strong class="nl iu">in</strong> range(len(tag_dataset)):<br/>  tag_string = str(tag_dataset[i])<br/>  <strong class="nl iu">if</strong> tag_string != "nan" :<br/>    tag_word=convert(tag_string)<br/>    <strong class="nl iu">for</strong> j <strong class="nl iu">in</strong> range(len(tag_word)):<br/>      <strong class="nl iu">if</strong> tag_word[j] <strong class="nl iu">not</strong> <strong class="nl iu">in</strong> unique_tag:<br/>        unique_tag.append(tag_word[j])<br/>print(len(unique_tag))</span></pre><p id="5739" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">删除标点符号并转换小写的整个文本使训练任务更容易，并提高了模型的效率。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="c7be" class="my lw it nl b gy np nq l nr ns">remove_digits = str.maketrans('', '', string.digits)<br/>exclude = '[!"#$%&amp;<strong class="nl iu">\'</strong>()*+,-./:;&lt;=&gt;?@[<strong class="nl iu">\\</strong>]^_`{|}~]'<br/><strong class="nl iu">for</strong> column <strong class="nl iu">in</strong> ['Title','Subtitle','Description']:<br/>  modified_dataset[column] = modified_dataset[column].map(<strong class="nl iu">lambda</strong> x : x.translate(remove_digits))<br/>  modified_dataset[column] = modified_dataset[column].map(<strong class="nl iu">lambda</strong> x : re.sub(str(exclude), '', x))</span></pre><p id="182c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要标记数据集并执行词干操作。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="4bd2" class="my lw it nl b gy np nq l nr ns"><strong class="nl iu">import</strong> <strong class="nl iu">nltk</strong><br/>nltk.download('punkt')<br/>tokenized_dataframe =  modified_dataset.apply(<strong class="nl iu">lambda</strong> row: nltk.word_tokenize(row['Description']), axis=1)<br/>print(type(tokenized_dataframe))</span><span id="4d4a" class="my lw it nl b gy nt nq l nr ns"><strong class="nl iu">def</strong> lemmatize_text(text):<br/>    <strong class="nl iu">return</strong> [ps.stem(w)  <strong class="nl iu">for</strong> w <strong class="nl iu">in</strong> text <strong class="nl iu">if</strong> len(w)&gt;5]</span><span id="7573" class="my lw it nl b gy nt nq l nr ns">ps = PorterStemmer() <br/>stemmed_dataset = tokenized_dataframe.apply(lemmatize_text)</span></pre><h2 id="67f3" class="my lw it bd lx mz na dn mb nb nc dp mf li nd ne mh lm nf ng mj lq nh ni ml nj bi translated">探索性数据分析</h2><p id="04c9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通过使用<a class="ae ky" href="https://github.com/amueller/word_cloud" rel="noopener ugc nofollow" target="_blank"> WordCloud </a>，我们可以验证我们的预处理是否正确完成。单词云是由单词组合而成的图像，看起来像一个云状。它向我们展示了一个词在文本中出现的频率——它的频率。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="a0f3" class="my lw it nl b gy np nq l nr ns"><strong class="nl iu">from</strong> <strong class="nl iu">wordcloud</strong> <strong class="nl iu">import</strong> WordCloud<br/><strong class="nl iu">import</strong> <strong class="nl iu">matplotlib.pyplot</strong> <strong class="nl iu">as</strong> <strong class="nl iu">plt</strong><br/><em class="ms">#dataset_words=''</em><br/><em class="ms">#for column in ['Title','Subtitle','Description']:</em><br/>dataset_words=''.join(list(str(stemmed_dataset.values)))<br/>print(type(dataset_words))<br/>wordcloud = WordCloud(width = 800, height = 500, <br/>                background_color ='white',  <br/>                min_font_size = 10).generate(dataset_words) <br/><br/>plt.figure(figsize = (5, 5), facecolor = <strong class="nl iu">None</strong>) <br/>plt.imshow(wordcloud) <br/>plt.axis("off") <br/>plt.tight_layout(pad = 0) <br/>  <br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2080f4cc67da541bf7ade1d48757cc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*iKoKWNEiE54ZlJPzRpLYig.png"/></div></figure><h2 id="4854" class="my lw it bd lx mz na dn mb nb nc dp mf li nd ne mh lm nf ng mj lq nh ni ml nj bi translated">建立模型</h2><p id="50d8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于LDA模型，我们首先需要构建一个单词字典，其中每个单词都有一个唯一的id。然后需要创建一个包含word_frequency — -&gt;(word_id，word_frequency)的单词id映射的语料库。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="740f" class="my lw it nl b gy np nq l nr ns">dictionary_of_words = gensim.corpora.Dictionary(stemmed_dataset)</span><span id="5198" class="my lw it nl b gy nt nq l nr ns">word_corpus = [dictionary_of_words.doc2bow(word) <strong class="nl iu">for</strong> word <strong class="nl iu">in</strong> stemmed_dataset]</span></pre><p id="1f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，训练模型。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="10c7" class="my lw it nl b gy np nq l nr ns">lda_model = gensim.models.ldamodel.LdaModel(corpus=word_corpus,<br/>                                                   id2word=dictionary_of_words,<br/>num_topics=329, <br/>random_state=101,<br/>update_every=1,<br/>chunksize=300,<br/>passes=50,<br/>alpha='auto',<br/>per_word_topics=<strong class="nl iu">True</strong>)</span></pre><p id="d267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">连贯性衡量一个主题中单词之间的相对距离。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0f77" class="my lw it nl b gy np nq l nr ns">coherence_val = CoherenceModel(model=lda_model, texts=stemmed_dataset, dictionary=dictionary_of_words, coherence='c_v').get_coherence()<br/><br/>print('Coherence Score: ', coherence_val)</span></pre><p id="aade" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一致性值:0.4</p><h2 id="8dab" class="my lw it bd lx mz na dn mb nb nc dp mf li nd ne mh lm nf ng mj lq nh ni ml nj bi translated">估价</h2><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="4d28" class="my lw it nl b gy np nq l nr ns"><strong class="nl iu">for</strong>  index,score <strong class="nl iu">in</strong> sorted(lda_model[word_corpus[2]][0], key=<strong class="nl iu">lambda</strong> tup: -1*tup[1]):<br/>    print("<strong class="nl iu">\n</strong>Score: <strong class="nl iu">{}\t</strong> <strong class="nl iu">\n</strong>Topic: <strong class="nl iu">{}</strong>".format(score, lda_model.print_topic(index, 10)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/85d0fd19670df176ee9ef6a7b6f1f774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ML289mZWvy4RVeoP5HLQw.png"/></div></div></figure><p id="867c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顶部的主题获得了最高的概率，并且它与类似经济的东西相关。</p><p id="32f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在<a class="ae ky" href="https://github.com/aawanRahman/latent-dirichlet-allocation/blob/master/topic_modelling_LDA.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> GITHUB </strong> </a>上找到所有代码。</p><h1 id="c7d3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">参考文献:</strong></h1><ol class=""><li id="9c83" class="nw nx it lb b lc mn lf mo li ny lm nz lq oa lu ob oc od oe bi translated"><a class="ae ky" href="https://ai.stanford.edu/~ang/papers/jair03-lda.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ms">潜伏的狄利克雷分配</em> </a>作者大卫·m·布雷，安德鲁·y·Ng&amp;迈克尔·乔丹。</li><li id="c15c" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://youtu.be/T05t-SqKArY" rel="noopener ugc nofollow" target="_blank">潜伏的狄利克雷分配</a>路易斯·塞拉诺。</li><li id="80be" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=VTweNS8GiWI" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配(算法)</a>由ML论文讲解— A.I .苏格拉底圈— AISC。</li></ol><h1 id="90c9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最后，感谢您的阅读。感谢任何反馈。</h1></div></div>    
</body>
</html>