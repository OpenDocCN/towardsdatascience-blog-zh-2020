<html>
<head>
<title>Preprocessing text in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Python 中预处理文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/preprocessing-text-in-python-923828c4114f?source=collection_archive---------20-----------------------#2020-09-01">https://towardsdatascience.com/preprocessing-text-in-python-923828c4114f?source=collection_archive---------20-----------------------#2020-09-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dccf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">建立情感分类器的一步</h2></div><p id="76ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章是关于建立情感分类器的三篇连续文章中的第二篇。在第一篇文章的<a class="ae le" rel="noopener" target="_blank" href="/exploratory-text-analysis-in-python-8cf42b758d9e">中我们进行了探索性的文本分析之后，是时候对我们的文本数据进行预处理了。简单地说，预处理文本数据就是做一系列的操作，将文本转换成表格形式的数值数据。在本帖中，我们将探讨三种不同复杂度的方法来预处理文本到<em class="lf"> tf-idf </em>矩阵，为模型做准备。如果你不确定什么是 tf-idf，</a><a class="ae le" rel="noopener" target="_blank" href="/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc">这篇文章</a>用一个简单的例子来解释。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/4ea7ab753a45347540d242f5dceff6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yFEKwME9R7nQZmYn"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@domenicoloia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Domenico Loia </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d294" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始之前，让我们后退一步，快速地看一下更大的画面。<a class="ae le" href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome" rel="noopener ugc nofollow" target="_blank"> CRISP-DM </a>方法概述了成功的数据科学项目的流程。<em class="lf">数据预处理</em>是<strong class="kk iu">数据准备</strong>阶段的关键任务之一。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*qvgzhs_qixhx0SRCDN5-jg.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">CRISP-DM 工艺流程摘录</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="6d7c" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">0.Python 设置</h1><p id="a298" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">这篇文章假设读者(👀是的，你！)可以访问并熟悉 Python，包括安装包、定义函数和其他基本任务。如果你是 Python 的新手，<a class="ae le" href="https://www.python.org/about/gettingstarted/" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的起点。</p><p id="fa10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在 Jupyter 笔记本里测试过 Python 3.7.1 的脚本。</p><p id="a567" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在开始之前确保您已经安装了以下库:<br/> ◼️ <strong class="kk iu">数据操作/分析:</strong> <em class="lf"> numpy，pandas <br/> </em> ◼️ <strong class="kk iu">数据分区:</strong><em class="lf">sk learn<br/></em>◼️<strong class="kk iu">文本预处理/分析:</strong> <em class="lf"> nltk <br/> </em> ◼️ <strong class="kk iu">拼写检查器:</strong> <em class="lf">拼写检查器(pyspellchecker </em>安装时</p><p id="657e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你安装了<em class="lf"> nltk </em>，请确保你已经从<em class="lf"> nltk </em>下载了<em class="lf">【停用词】</em>和<em class="lf">【wordnet】</em>语料库，脚本如下:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="926f" class="ng mf it nc b gy nh ni l nj nk">import nltk<br/>nltk.download('stopwords') <br/>nltk.download('wordnet')</span></pre><p id="fe3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你已经下载了，运行这个会通知你。</p><p id="ece1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们准备导入所有的包:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="ee2a" class="ng mf it nc b gy nh ni l nj nk"># Setting random seed<br/>seed = 123</span><span id="04c3" class="ng mf it nc b gy nl ni l nj nk"># Measuring run time<br/>from time import time</span><span id="4235" class="ng mf it nc b gy nl ni l nj nk"># Data manipulation/analysis<br/>import numpy as np<br/>import pandas as pd</span><span id="df42" class="ng mf it nc b gy nl ni l nj nk"># Data partitioning<br/>from sklearn.model_selection import train_test_split</span><span id="520c" class="ng mf it nc b gy nl ni l nj nk"># Text preprocessing/analysis<br/>import re, random<br/>from nltk import word_tokenize, sent_tokenize, pos_tag<br/>from nltk.util import ngrams<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from spellchecker import SpellChecker</span></pre><h1 id="b31f" class="me mf it bd mg mh nm mj mk ml nn mn mo jz no ka mq kc np kd ms kf nq kg mu mv bi translated">1.数据📦</h1><p id="ea4f" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">我们将使用 IMDB 电影评论数据集。您可以在这里下载数据集<a class="ae le" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">，并将其保存在您的工作目录中。保存后，让我们将其导入 Python:</a></p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="9c2e" class="ng mf it nc b gy nh ni l nj nk">sample = pd.read_csv('IMDB Dataset.csv')<br/>print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")<br/>sample.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6b23a671ee90c83029239e074f5dfca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*PHe-BswQXmJWcDyd9CHtLA.png"/></div></figure><p id="bd69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看情绪之间的分歧:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="8a02" class="ng mf it nc b gy nh ni l nj nk">sample['sentiment'].value_counts()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5e5c06fc380f7ac8acfe9701d65dedac.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*e3f7OlQu1JOKN3FCw-HBMA.png"/></div></figure><p id="87c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在样本数据中，情感是平均分配的。我们先把数据分成两组:<em class="lf">训练</em>和<em class="lf">测试</em>。我们将留出 5000 箱进行测试:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="2e91" class="ng mf it nc b gy nh ni l nj nk"># Split data into train &amp; test<br/>X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, <br/>                                                    stratify=sample['sentiment'])# Append sentiment back using indices<br/>train = pd.concat([X_train, y_train], axis=1)<br/>test = pd.concat([X_test, y_test], axis=1)# Check dimensions<br/>print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")<br/>print(f"{train['sentiment'].value_counts()}\n")print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")<br/>print(test['sentiment'].value_counts())</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/dd7a92b9923d4142d655cce22d3d6f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*aBoCBXmLQy8-P8WF2awz8w.png"/></div></figure><p id="2544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将使用<em class="lf">序列</em>进行预处理实验。只有当我们需要评估最终模型时，我们才会预处理<em class="lf">测试</em>。让我们检查一下训练数据集的头部:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="318d" class="ng mf it nc b gy nh ni l nj nk">train.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ff942e16a2a52ba3489fa71cb2246a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*OUD0YC45nPYbfoxA6LgSXQ.png"/></div></figure><p id="3cbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，我们开始预处理吧！✨</p><h1 id="5d60" class="me mf it bd mg mh nm mj mk ml nn mn mo jz no ka mq kc np kd ms kf nq kg mu mv bi translated">2.预处理文本</h1><p id="12a0" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">根据我们的处理方式，我们可以得到不同的 tf-idf 矩阵。在构建模型时，尝试不同的预处理方法是很好的。我们将研究以下 3 种方法:</p><ol class=""><li id="158b" class="nv nw it kk b kl km ko kp kr nx kv ny kz nz ld oa ob oc od bi translated"><em class="lf">更简单的方法</em></li><li id="c9fb" class="nv nw it kk b kl oe ko of kr og kv oh kz oi ld oa ob oc od bi translated"><em class="lf">简单方法</em></li><li id="3409" class="nv nw it kk b kl oe ko of kr og kv oh kz oi ld oa ob oc od bi translated"><em class="lf">不太简单的方法</em></li></ol><p id="8ea5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然在这篇文章中我们只讨论了将文本预处理成 tf-idf 矩阵，但是您可能还想探索其他的方法。</p><p id="261c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每一种方法，我们一定会衡量运行时性能，因为这是一个重要的考虑因素，特别是如果模型将被生产。我们将在下一篇文章中测试哪种方法更适合这个模型。在我们开始之前，为了使事情变得简单，让我们定义一个函数来帮助我们检查不同的方法:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="4ec5" class="ng mf it nc b gy nh ni l nj nk">def inspect(vectoriser, X):<br/>    # Fit and transform<br/>    start = time()<br/>    print(f"There are {vectoriser.fit_transform(X).shape[1]} columns.\n")<br/>    end = time()<br/>    print(f"Took {round((end-start),2)} seconds.\n")<br/>    <br/>    # Inspect tokens<br/>    tokens = list(vectoriser.vocabulary_.keys())<br/>    tokens.sort()<br/>    print(f"Example tokens: {tokens[:50]}\n")<br/>    <br/>    # Inspect ignored tokens<br/>    ignored = vectoriser.stop_words_<br/>    if len(ignored)==0:<br/>        print("No token is ignored.")<br/>    elif len(ignored)&gt;50:<br/>        print(f"Example ignored tokens: {random.sample(ignored, 50)}")<br/>    else:<br/>        print(f"Example ignored tokens: {ignored}")</span></pre><h2 id="8b9e" class="ng mf it bd mg oj ok dn mk ol om dp mo kr on oo mq kv op oq ms kz or os mu ot bi translated">2.1.更简单的方法 1️⃣</h2><p id="62bf" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">让我们从三个中最简单的开始。在这种方法中，我们将让<em class="lf"> sklearn 的 TfidfVectorizer </em>做所有的预处理，而不使用任何额外的定制函数。让我们使用默认参数来感受一下我们将获得多少列:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="8d0c" class="ng mf it nc b gy nh ni l nj nk">vectoriser = TfidfVectorizer()<br/>inspect(vectoriser, X_train)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi gj"><img src="../Images/173b52987bed04324c67c2ff7852aa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKGWDXo8plW-h5kG_XdHbg.png"/></div></div></figure><p id="f56e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相当快。输出超过 97，000 列，前 50 个标记大部分是数字。现在，让我们调整一些参数来进一步清理标记，并更好地控制预处理:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="6aef" class="ng mf it nc b gy nh ni l nj nk"># Simpler approach<br/>vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', stop_words='english', min_df=30, max_df=.7)<br/>inspect(vectoriser, X_train)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ou"><img src="../Images/619825cbe10166c408b0292ea499d588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vs-mvYGOznLu8yGLQNPDjQ.png"/></div></div></figure><p id="4773" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然上述两个版本花费的时间差不多，但后者的列数要少 7-8 倍。在这里，我们要求<em class="lf"> TfidfVectorizer </em>做一些事情:<br/> ◼ <code class="fe ov ow ox nc b">token_pattern=r'[a-z]+'</code>:标记成字母标记——这意味着我们丢弃数字和标点符号。如果您不熟悉<a class="ae le" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank">正则表达式</a> , <code class="fe ov ow ox nc b">[a-z]+</code>意味着令牌必须只由字母组成。<br/> ◼ <code class="fe ov ow ox nc b">stop_words='english’</code>:去掉停止字。<br/> ◼ <code class="fe ov ow ox nc b">min_df=30</code>:移除稀有令牌。当一个令牌出现在少于 30 条评论中时，我们认为它是稀有的。这将大大减少令牌的数量。尝试在没有该参数的情况下运行脚本，并查看令牌的数量。<br/> ◼ <code class="fe ov ow ox nc b">max_df=.7</code>:删除超过 70%文档中的令牌。这意味着如果一个令牌包含在超过 31.5K 个评论中，那么我们将忽略它们。实际上并没有很多单词因为这个而被排除在外。因此，我们甚至可以保留这个特性的默认值。</p><p id="1ad6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe ov ow ox nc b">max_df</code>和<code class="fe ov ow ox nc b">min_df</code>有助于令牌选择。换句话说，在这两者的帮助下，我们可以丢弃那些或者<strong class="kk iu">太频繁</strong>可能对情感分类没有用处或者<strong class="kk iu">太罕见</strong>可能导致过度拟合的令牌。</p><p id="f5b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">📍这里有一个提示:</strong>如果我们这样定义参数:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="be98" class="ng mf it nc b gy nh ni l nj nk">TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.5)</span></pre><p id="c2ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在匹配矢量器之后，您可以通过运行<code class="fe ov ow ox nc b">vectoriser.stop_words_</code>来找出哪些令牌由于<code class="fe ov ow ox nc b">max_df=.5</code>条件而被排除。当我们调用<code class="fe ov ow ox nc b">inspect</code>函数时，输出的'<em class="lf">示例被忽略的标记'</em>部分显示了那些被排除的单词的片段。现在，我鼓励你运行上面的脚本并检查<code class="fe ov ow ox nc b">vectoriser.stop_words_</code>。你看到了什么？你看到的大多是停用词吗？尝试将这些值更改为. 5、. 6、. 8 或. 9，然后重新运行以观察被排除的单词如何变化。这有助于理解调整某些参数如何影响预处理。现在，如果你热衷于添加<code class="fe ov ow ox nc b">stop_words='english’</code>或<code class="fe ov ow ox nc b">min_df=30</code>(不要同时添加两个，一次添加一个以了解单个参数)，并检查这次排除了哪些令牌。我们将在下一篇文章中构建模型时进一步调整这些参数。</p><p id="94e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🔗<em class="lf">如果您热衷于了解更多关于参数的信息，这里有</em> <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">文档</em> </a> <em class="lf">。</em></p><p id="0707" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法是一种🍰，是不是？我喜欢这种方法，尤其是因为它做一些基本的事情真的又好又快。在我看来，从简单开始总是好的，只有在提高性能的情况下才增加复杂性。</p><h2 id="c8b4" class="ng mf it bd mg oj ok dn mk ol om dp mo kr on oo mq kv op oq ms kz or os mu ot bi translated">2.2.简单方法 2️⃣</h2><p id="d17c" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">总有改进的余地。例如，在以前的方法中，单词“播放”、“播放”、“正在播放”和“已播放”被认为是 4 种不同的标记。如果我们去掉这些屈折变化的词尾，使这些符号正常化为一个唯一的符号“play”，这不是很好吗？这就是我们在这部分要做的事情！</p><p id="bb30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使屈折词尾正常化，我们将使用一种叫做引理满足的技术。另一种选择是词干。如果你想了解这两者的区别，我在这里简单解释了一下区别<a class="ae le" rel="noopener" target="_blank" href="/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc">。</a></p><p id="92c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">词条释义有助于将单词转换成词典形式。以我们之前的例子为例，让我们将它们进行比较，看看输出是什么样的:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="d87d" class="ng mf it nc b gy nh ni l nj nk">lemmatiser = WordNetLemmatizer()<br/>for word in ['play', 'plays', 'playing', 'played']:<br/>    print(lemmatiser.lemmatize(word, 'v'))</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/d74b5321507cecf4c01fd4875aec5472.png" data-original-src="https://miro.medium.com/v2/resize:fit:94/format:webp/1*5zTdzRaLiuHM2avF2dhAmA.png"/></div></figure><p id="6f77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">酷，所有的单词现在都转换成‘玩’了。注意我们如何在<code class="fe ov ow ox nc b">lemmatize</code>方法中传递 v 作为第二个参数？‘v’是一个<em class="lf">词性标签</em>。事实上，准确的词汇匹配依赖于我们随单词一起提供给词汇匹配器的词性(POS)标签。为了演示这一点，让我们重新运行前面的脚本，做一点小小的改动，将“v”变成“n”:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="265b" class="ng mf it nc b gy nh ni l nj nk">lemmatiser = WordNetLemmatizer()<br/>for word in ['plays', 'playing', 'played']:<br/>    print(lemmatiser.lemmatize(word, 'n'))</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/11b916e55d719312a846064cca8fcd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*sApXjSFno9xhR0icmXRMPQ.png"/></div></figure><p id="e213" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次，并不是所有的词都转换成了玩。这个简单的例子展示了词性标签如何影响引理满足的有效性。那么到底什么是词性标签呢？简单地说，它指的是这个词的语法范畴。单词“movie”是一个名词，而“watch”根据上下文可以是动词也可以是名词。名词和动词都是词类的例子。在第一次运行中，我们告诉 lemmatiser 提供的单词是动词(因此是‘v’)，在第二次运行中是名词(因此是‘n’)。使用<em class="lf"> nltk 的</em> <em class="lf">词性标注器:</em> <code class="fe ov ow ox nc b">pos_tag()</code>，我们可以用词性来标注每个单词。</p><p id="2191" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将很快创建一个自定义函数，根据标签进行词性标注和词汇匹配。我们将这个函数传递给<code class="fe ov ow ox nc b">TdidfVectoriser()</code> <em class="lf">的<code class="fe ov ow ox nc b">analyzer</code>参数。</em>当我们这样做时，以前使用的一些参数如<code class="fe ov ow ox nc b">token_pattern, stop_words</code>将不再受支持。因此，我们还必须在自定义函数中包含一个标记化步骤:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="27ef" class="ng mf it nc b gy nh ni l nj nk">def preprocess_text(text):<br/>    # 1. Tokenise to alphabetic tokens<br/>    tokeniser = RegexpTokenizer(r'[A-Za-z]+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # 2. POS tagging<br/>    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}<br/>    pos_tags = pos_tag(tokens)<br/>    <br/>    # 3. Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]</span><span id="4b96" class="ng mf it nc b gy nl ni l nj nk">return tokens</span></pre><p id="7606" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里需要注意的一点是，<code class="fe ov ow ox nc b">pos_tag()</code>和<code class="fe ov ow ox nc b">lemmatiser.lemmatize()</code>对词性标签使用不同的命名约定，所以我们必须将由<em class="lf"> nltk 标签器</em>生成的词性映射到 lemmatiser 能够理解的名称。这就是我们有<code class="fe ov ow ox nc b">pos_map</code>的原因。现在，让我们预处理数据并评估:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="9cdb" class="ng mf it nc b gy nh ni l nj nk">vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)<br/>inspect(vectoriser, X_train)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pa"><img src="../Images/aef2ba495c3923ef606450ef3ec78907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E3NUaVaI0PMJOkmtp4PVUg.png"/></div></div></figure><p id="353e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它将列数减少到 10，754。与<em class="lf">更简单的方法</em>中的 12，805 列相比，列数减少了约 16%。在我的电脑上花了大约 11 分钟，比这慢了 85 倍。如果我们仔细想想，词条满足不会改变评论中的每一个词。就拿“这部电影太棒了”来说吧。句子为例。唯一受益于引理满足的词是“was”。所以记住这一点，如果你需要更快地完成词条满足，有时对所有单词使用默认的词性是很好的。在这种情况下，我们的自定义函数简化为:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="7e80" class="ng mf it nc b gy nh ni l nj nk"># Simple approach<br/>def preprocess_text(text):<br/>    # 1. Tokenise to alphabetic tokens<br/>    tokeniser = RegexpTokenizer(r'[A-Za-z]+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # 2. Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') for t in tokens]</span><span id="0ce3" class="ng mf it nc b gy nl ni l nj nk">return tokens</span><span id="ec2f" class="ng mf it nc b gy nl ni l nj nk">vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)<br/>inspect(vectoriser, X_train)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pb"><img src="../Images/c15e92b07cc50e2534fac49624c27578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5iAKnrRZ3o3fpLcsbojItA.png"/></div></div></figure><p id="7b74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列数与我们使用的词性标注器非常接近，但只花了大约一分钟。因此，在这两个版本之间，我更喜欢对这个文本数据集使用默认的词性标签。值得注意的是，如果我们不给<code class="fe ov ow ox nc b">pos</code>参数提供值，lemmatiser 默认为‘n’。一般来说，我通常喜欢使用' v '作为默认，因为我发现它在一般情况下规范化更多的单词。但是，更合适的默认<code class="fe ov ow ox nc b">pos</code>将取决于数据。</p><h2 id="30fd" class="ng mf it bd mg oj ok dn mk ol om dp mo kr on oo mq kv op oq ms kz or os mu ot bi translated">2.3.不那么简单的方法 3️⃣</h2><p id="aa6d" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">在这种方法中，我们将进一步清理数据。基于从探索性数据分析中获得的知识和一般预处理思想，我们将做以下工作:</p><p id="6086" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">◼纠正错别字:<em class="lf">【chr acter】</em>到<em class="lf">【character】</em><br/>◼将英式拼法转换为美式拼法:<em class="lf">【realize】</em>到<em class="lf">【realize】</em><br/>◼去掉停用词</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="e98e" class="ng mf it nc b gy nh ni l nj nk">def convert_to_american(token):<br/>    # Copied from <a class="ae le" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank">here</a><br/>    token = re.sub(r"(...)our$", r"\1or", token)<br/>    token = re.sub(r"([bt])re$", r"\1er", token)<br/>    token = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", token)<br/>    token = re.sub(r"ogue$", "og", token)<br/>    return token</span><span id="4e3a" class="ng mf it nc b gy nl ni l nj nk">def correct_typo(tokens):<br/>    spell = SpellChecker()<br/>    return [spell.correction(t) if len(spell.unknown([t]))&gt;0 else t for t in tokens]<br/>        <br/>def preprocess_text(text):<br/>    # 1. Tokenise to alphabetic tokens<br/>    tokeniser = RegexpTokenizer(r'[A-Za-z]+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # 2. Lowercase and lemmatise<br/>    lemmatiser = WordNetLemmatizer()<br/>    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') for t in tokens]</span><span id="f0bc" class="ng mf it nc b gy nl ni l nj nk"># 3. Correct spelling (this won't convert 100% )<br/>    tokens = correct_typo(tokens)<br/>    <br/>    # 4. Convert British spelling to American spelling (this won't convert 100%)<br/>    tokens = [convert_to_american(t) for t in tokens]</span><span id="70df" class="ng mf it nc b gy nl ni l nj nk"># 5. Remove stopwords<br/>    stop_words = stopwords.words('english')<br/>    stop_words.extend(['cannot', 'could', 'done', 'let', 'may' 'mayn',  'might',  'must', 'need', 'ought', 'oughtn', <br/>                       'shall', 'would', 'br'])<br/>    tokens = [t for t in tokens if t not in stop_words]<br/>    <br/>    return tokens</span></pre><p id="d133" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了这些，我们可以继续添加其他层来纠正和清理。但是，每个额外的步骤都会增加复杂性和运行时间，而不能保证模型性能的提高。在预处理文本时，通常可以尝试以下一些方法，这些方法在我们的例子中不是特别有用，但在其他例子中可能有用:</p><p id="4fff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">◼清理垃圾，如 html 标签、电子邮件地址、网址<br/> ◼将数字转换成文字，而不是丢弃它们<br/> ◼将表情符号或表情符号转换成文字</p><p id="8dd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，让我们预处理并检查输出:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="3760" class="ng mf it nc b gy nh ni l nj nk">vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)<br/>inspect(vectoriser, X_train)</span></pre><p id="f0d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我开始运行这段代码的几个小时后，它仍然在运行。因为我认为它花费的时间太长，所以我不得不中断内核来停止查询。为了了解这种方法比前两种方法慢多少，我使用下面的脚本将数据集的大小减少到其大小的 1/9:</p><pre class="lh li lj lk gt nb nc nd ne aw nf bi"><span id="bb9e" class="ng mf it nc b gy nh ni l nj nk">train = train.sample(5000, random_state=seed)</span></pre><p id="0485" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这个更小的数据子集上运行了所有三种方法。下面是三种方法的比较:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c5c3db88ea39a0736edad89f60a3cef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*pejKZNItAd2pHhuaEsrj-g.png"/></div></figure><p id="abc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">不太简单的方法</em>比其他两种方法更标准化代币，但成本非常高。与其他方法相比，预处理至少要花费 1000 倍的时间。当数据集增长时，这个比率可能会变得更糟。因此，进一步追求<em class="lf">不那么简单的方法</em>是不实际的，除非它被优化为运行更快。</p><p id="dbcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👂如果你有一台比我更好的计算机，这个运行时间问题可能不一定会成为你的一个限制，在这种情况下，你可以自由地继续追求。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pd"><img src="../Images/7674b2ec8affa7bcc713f7ff6ccb74b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NACDE_ro1oJVWjDr"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@andyoneru?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">和</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的合影</p></figure><p id="f69c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae le" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="lf">我的推荐链接</em></a><em class="lf">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="7460" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你看我的帖子。希望您已经学会了一些不同的预处理文本的实用方法，可以应用到您的下一个 NLP 项目中。在下一篇文章中，我们将构建一个情感分类器。以下是该系列另外两篇文章的链接:<em class="lf"><br/></em>◼️<a class="ae le" rel="noopener" target="_blank" href="/exploratory-text-analysis-in-python-8cf42b758d9e">python 中的探索性文本分析</a><br/>◼️<a class="ae le" rel="noopener" target="_blank" href="/sentiment-classification-in-python-da31833da01b">python 中的情感分类</a></p><p id="6643" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是我的其他 NLP 相关帖子的链接:<br/>◼️<a class="ae le" rel="noopener" target="_blank" href="/simple-wordcloud-in-python-2ae54a9f58e5">Python 中的简单 word cloud</a><br/><em class="lf">(下面列出了一系列关于 NLP 介绍的帖子)</em> <br/> ◼️ <a class="ae le" rel="noopener" target="_blank" href="/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96">第一部分:Python 中的预处理文本</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">第二部分:词条满足和词干的区别</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">第三部分:TF-IDF 解释</a> <br/> ◼️ <a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener">第四部分:python 中的监督文本分类模型</a><a class="ae le" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener"/></p><p id="7874" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再见🏃💨</p></div></div>    
</body>
</html>