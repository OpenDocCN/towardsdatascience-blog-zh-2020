<html>
<head>
<title>Regularization in Deep Learning — L1, L2, and Dropout</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的正规化——L1、L2和辍学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=collection_archive---------0-----------------------#2020-02-19">https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=collection_archive---------0-----------------------#2020-02-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cc01" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习中最重要的正则化技术的理论和实践指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5255a98100a63c4d1b9156f75603c78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIqXS_0W-R0vthMzy0YMeg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.spacetelescope.org/images/heic0611b/" rel="noopener ugc nofollow" target="_blank">https://www.spacetelescope.org/images/heic0611b/</a></p></figure><p id="c7a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">正则化是一套技术，可以防止神经网络中的过拟合，从而提高深度学习模型在面对来自问题域的全新数据时的准确性。</strong> <strong class="lb iu">在本文中，我们将讨论最流行的正则化技术，它们被称为L1、L2和辍学。</strong></p><h1 id="e696" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目录</h1><ol class=""><li id="5048" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">重述:过度拟合</strong></li><li id="9a60" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">什么是正规化？</strong></li><li id="83f8" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu"> L2正规化</strong></li><li id="9a3b" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu"> L1正规化</strong></li><li id="0746" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">为什么L1和L2的正规化会奏效？</strong></li><li id="934b" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">辍学</strong></li><li id="6ef0" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">带回家的信息</strong></li></ol></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="f621" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">1.回顾:过度拟合</h1><p id="408b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">训练神经网络时最重要的一个方面是避免过度拟合。我们已经在这篇文章中详细讨论了<a class="ae ky" href="https://www.deeplearning-academy.com/p/ai-wiki-overfitting-underfitting" rel="noopener ugc nofollow" target="_blank">过度拟合的问题。</a></p><p id="df8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">然而，让我们快速回顾一下:</strong>过度拟合是指神经网络对训练数据建模非常好，但当它看到来自同一问题领域的新数据时失败的现象。过度拟合是由神经网络在训练期间拾取的训练数据中的噪声引起的，并将其作为数据的基本概念来学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/cf6030cf8cbd954dc134a9d84acb4a03.png" data-original-src="https://miro.medium.com/v2/0*BhV2aN2UkmVQXl4K"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">www.memegenerator.net</p></figure><p id="8150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这种习得的噪声对于每个训练集是唯一的。一旦模型看到来自同一问题域的新数据，但不包含这种噪声，神经网络的性能就会变得更差。</p><p id="95e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">“为什么神经网络首先会拾取噪音？”</em></p><p id="f2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之所以这样，是因为这个网络的复杂程度太高了。右边的图像显示了更高复杂度的神经网络拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/3e4bda731f7eecccee3c47e72ca276be.png" data-original-src="https://miro.medium.com/v2/0*seXiPgCCaHi19sJJ"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图表1。具有良好拟合和高方差的模型。来源:<a class="ae ky" href="https://www.researchgate.net/publication/332412613" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/publication/332412613</a></p></figure><p id="b9ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有较高复杂性的模型能够拾取和学习数据中由一些随机波动或误差引起的模式(噪声)。该网络将能够逐个模拟分布的每个数据样本，而不识别描述该分布的真实函数。</p><p id="4899" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用真函数生成的新的任意样本将具有与模型拟合的高距离。我们也说这个模型有很高的方差。</p><p id="f758" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，左侧复杂度较低的网络对分布的建模要好得多，因为它没有太努力地对每个数据模式进行单独建模。</p><blockquote class="nu nv nw"><p id="35c3" class="kz la ns lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">在实践中，过度拟合导致神经网络模型在训练期间表现得非常好，但是当面对全新的数据时，在推理时间期间性能变得更差。</p></blockquote><p id="5c3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简而言之:</strong>不太复杂的神经网络不太容易过度拟合。为了防止过度拟合或高方差，我们必须使用所谓的正则化。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="0459" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">2.什么是正规化？</h1><p id="2c9d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated"><strong class="lb iu">简单来说:</strong>正则化是指在训练过程中降低神经网络模型复杂性的一套不同技术，从而防止过拟合。</p><p id="31ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有三种非常流行和有效的正则化技术叫做<em class="ns">【L1】</em>、<em class="ns"/>和我们将在下面讨论的辍学。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="0cc5" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">3.L2正则化</h1><p id="31f9" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">L2正则化是所有正则化技术中最常见的类型，通常也被称为权重衰减或乘坐回归。</p><p id="822a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个正则化的数学推导，以及为什么这个方法在减少过拟合方面起作用的数学解释，是相当长且复杂的。由于这是一篇非常实用的文章，我不想过多地关注数学。相反，我想传达这项技术背后的直觉，最重要的是如何实现它，以便您可以在深度学习项目中解决过度拟合问题。</p><p id="19fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在L2正则化期间，神经网络的损失函数由所谓的正则化项扩展，这里称为<strong class="lb iu">ω</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/c862a41b9a0ff3b5d81505b3e49d2dfa.png" data-original-src="https://miro.medium.com/v2/0*XgoBhlCMJUJxza87"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。1正则项</p></figure><p id="9821" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则项<strong class="lb iu">ω</strong>被定义为权重矩阵的欧几里德范数(或L2范数)，其是权重矩阵的所有平方权重值的总和。正则项由标量α除以2进行加权，并添加到为当前任务选择的正则损失函数中。这导致损失函数的新表达式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/61069debfa1c8f77af619e76728a0806.png" data-original-src="https://miro.medium.com/v2/0*SY_r-Ltc9mB6pNBK"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Eq 2。L2正则化过程中的正则化损失。</p></figure><p id="c367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Alpha有时被称为<strong class="lb iu">正则化率</strong>，是我们引入神经网络的一个额外的超参数。简单地说，alpha决定了我们正则化模型的程度。</p><p id="50f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一步中，我们可以计算新损失函数的梯度，并将梯度放入权重的更新规则中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/53d86e49554aa21d70e1f78e29fbccfe.png" data-original-src="https://miro.medium.com/v2/0*ELhNz4fSZqhzmV_X"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/50a4171fcb6ce60f93f36a2bf0c45e37.png" data-original-src="https://miro.medium.com/v2/0*dD9nxonSPoWgcMRE"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。3 L2正则化过程中的梯度下降。</p></figure><p id="158e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新规则的一些重新表述导致了非常类似于规则梯度下降期间权重的更新规则的表达式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/040954aa085ed1f30f1727d5f5011dcd.png" data-original-src="https://miro.medium.com/v2/0*XT2dY6rysZLvX2xj"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方程4 L2正则化期间的梯度下降。</p></figure><p id="a41f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唯一的区别是，通过添加正则化项，我们从当前权重中引入了额外的减法(等式中的第一项)。</p><p id="0b76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，独立于损失函数的梯度，我们在每次执行更新时使我们的权重变小一点。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="d414" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">4.L1正则化</h1><p id="cb47" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在L1正则化(也称为Lasso回归)的情况下，我们简单地使用另一个正则化项<strong class="lb iu">ω</strong>。该项是权重矩阵中权重参数绝对值的总和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/6bec9562a9082ca9a654cf84cf03dbdd.png" data-original-src="https://miro.medium.com/v2/0*0D62sWAtjjkBSCt_"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。5 L1正则化的正则化项。</p></figure><p id="ca07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与前一种情况一样，我们将正则项乘以α，然后将全部内容添加到损失函数中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/5be4e9a295b665e4e40fd96f27158bcb.png" data-original-src="https://miro.medium.com/v2/0*kQqNJUFFBBElXvYt"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。6 L1正则化过程中的损失函数。</p></figure><p id="8e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新损失函数的导数导致以下表达式，旧损失函数的梯度和权重值的符号乘以α。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/af0ba1b039c57684cf4beb60d569775a.png" data-original-src="https://miro.medium.com/v2/0*emmfKB8aMi9hskcB"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。7 L1正则化期间损失函数的梯度。</p></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="11bf" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">5.为什么L1和L2的正规化会奏效？</h1><p id="156a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">你现在可能会问自己的问题是:</p><p id="0864" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">“为什么所有这些有助于减少过度拟合问题？”</em></p><p id="c1c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来解决这个问题。</p><p id="65f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请考虑和函数的图，其中表示在L1期间执行的运算和在L2正则化期间执行的运算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/7e3d08af031289df73463e8982b5dfd6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*V5j0opbBKXTysXrKiRYBhw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图表。2 L1函数(红色)，L2函数(蓝色)。来源:自制。</p></figure><p id="c940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在L2正则化的情况下，我们的权重参数减小，但不一定变为零，因为曲线在零附近变得平坦。另一方面，在L1正则化期间，权重总是被强制向零。</p><p id="6db6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以对此采取不同的、更加数学化的观点。</p><p id="f872" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">在L2的情况下，你可以考虑解一个方程，其中权重值的平方和等于或小于一个值</em> <strong class="lb iu"> <em class="ns"> s </em> </strong>。<strong class="lb iu"> s </strong>是正则项<strong class="lb iu"> α </strong>的每个可能值的常数。对于仅仅两个权重值<strong class="lb iu"> W1 </strong>和<strong class="lb iu"> W2 </strong>，该等式看起来如下:<strong class="lb iu"> W1 <em class="ns"> + W ≤ s </em> </strong></p><p id="820c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，<em class="ns"> L1正则化可以被认为是一个等式，其中权重值的模之和小于或等于值</em> <strong class="lb iu"> <em class="ns"> s </em> </strong>。这看起来像下面的表达式:<strong class="lb iu"> |W1| <em class="ns"> + |W2| ≤ s </em> </strong></p><p id="e8e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，引入的L1和L2正则化方程是约束函数，我们可以将其可视化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/90d40e8b2a43694173445db4fddfe0f9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*mh-QVwSKbe2kYCBh9mwA0Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:统计学习导论，作者:加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼</p></figure><p id="2e93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左图显示了L1正则化的约束函数(绿色区域)，右图显示了L2正则化的约束函数。红色椭圆是梯度下降过程中使用的损失函数的轮廓。在轮廓的中心，有一组最优权重，对于这些权重，损失函数具有全局最小值。</p><p id="2362" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在L1和L2正则化的情况下，W1和W2的估计值由椭圆与绿色约束区域相交的第一个点给出。</p><p id="3b77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于L2正则化具有圆形约束区域，所以相交通常不会出现在轴上，并且这对于W1和W2的估计将完全是非零的。</p><p id="7f96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在L1的例子中，约束区域是带角的菱形。因此损失函数的轮廓通常会在轴上与约束区域相交。那么这种情况发生时，估计值之一(W1或W2)将为零。</p><p id="eaa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在高维空间中，许多权重参数将同时等于零。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="f5b3" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">5.1正规化实现了什么？</h1><ul class=""><li id="6750" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu oa mv mw mx bi translated">执行L2正则化促使权重值趋向于零(但不完全为零)</li><li id="8633" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">执行L1正则化促使权重值为零</li></ul><p id="bbbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观地说，较小的权重减少了隐藏神经元的影响。在这种情况下，这些隐藏的神经元变得可以忽略，神经网络的整体复杂性降低。</p><p id="00bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如前所述:</strong>不太复杂的模型通常会避免对数据中的噪声进行建模，因此不存在过度拟合。</p><p id="194e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是你必须小心。当选择正则项<strong class="lb iu"> α时。</strong>我们的目标是在模型的低复杂性和准确性之间取得适当的平衡</p><ul class=""><li id="77c0" class="mn mo it lb b lc ld lf lg li ob lm oc lq od lu oa mv mw mx bi translated">如果你的阿尔法值太高，你的模型会很简单，但是你会冒<em class="ns">对数据</em>拟合不足的风险。您的模型无法从训练数据中获得足够的信息来做出有用的预测。</li><li id="7cbb" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">如果你的alpha值太低，你的模型将会更复杂，你将会面临数据过度拟合的风险。您的模型将会了解太多有关训练数据的特殊性，并且无法推广到新数据。</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="93ec" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">6.拒绝传统社会的人</h1><p id="c66f" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">除了L2正则化和L1正则化，另一个著名的和强大的正则化技术是所谓的辍学正则化。辍学调整背后的程序非常简单。</p><p id="c13e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，辍学意味着在训练期间，神经网络的一个神经元以某种概率在训练期间被关闭。让我们看一个直观的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/e73d15d606d7a046ed22391cd30b5841.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5t5lyeuTBLNV6Gl23cbZig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图表3。有脱失(右)和无脱失(左)的神经网络。来源:机器学习研究杂志15 (2014)</p></figure><p id="6ec2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设在左侧，我们有一个无丢失的前馈神经网络。假设一个随机神经元在训练过程中被关闭的概率为<strong class="lb iu"> P=0.5 </strong>，使用dropout将导致神经网络位于右侧。</p><p id="b40a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，您可以观察到大约一半的神经元是不活跃的，并且不被视为神经网络的一部分。如你所见，神经网络变得更简单了。</p><p id="ff06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络的更简单版本导致复杂性降低，从而可以减少过拟合。在每个前向传播和权重更新步骤应用具有一定概率<strong class="lb iu"> P </strong>的神经元的去激活。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="10ae" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">6.带回家的信息</h1><ul class=""><li id="6728" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu oa mv mw mx bi translated">过度拟合发生在更复杂的神经网络模型中(许多层、许多神经元)</li><li id="79b5" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">通过使用L1和L2正则化以及丢弃，可以降低神经网络的复杂度</li><li id="f67c" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">L1正则化迫使权重参数变为零</li><li id="c3b4" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">L2正则化迫使权重参数趋向于零(但从不精确为零)</li><li id="a2aa" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">较小的权重参数使一些神经元可以忽略→神经网络变得不太复杂→较少过拟合</li><li id="acc7" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu oa mv mw mx bi translated">在退出期间，一些神经元以随机概率<strong class="lb iu"> P </strong> →神经网络变得不那么复杂<strong class="lb iu"> </strong> →更少的过拟合</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="5975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">最初发表于</em><a class="ae ky" href="https://www.deeplearning-academy.com/p/ai-wiki-regularization" rel="noopener ugc nofollow" target="_blank"><em class="ns">https://www.deeplearning-academy.com</em></a><em class="ns">。</em></p></div></div>    
</body>
</html>