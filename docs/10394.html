<html>
<head>
<title>Identifying hidden trends in news stories using hierarchical clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用层次聚类识别新闻报道中的隐藏趋势</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/identifying-hidden-trends-in-news-stories-using-hierarchical-clustering-b6297df795af?source=collection_archive---------44-----------------------#2020-07-21">https://towardsdatascience.com/identifying-hidden-trends-in-news-stories-using-hierarchical-clustering-b6297df795af?source=collection_archive---------44-----------------------#2020-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5ede" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用称为凝聚聚类的通用层次聚类算法在最近的新闻文章中查找新的主题聚类</h2></div><p id="5b67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为数据科学家，新闻报道的文本分析从学习和实践的角度来看都非常重要，因为它为我们提供了大量的数据语料库来训练文本分类、情感分析、命名实体识别等。模特。</p><p id="ccec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总的来说，这些模型中的大多数都是在历史新闻语料库上训练的，该语料库使用了过去 1-3 年的新闻报道数据。这在平时很有效，但是，在新冠肺炎疫情，这给我们带来了严重的问题，因为新闻报道现在有更快的周转周期。</p><p id="61b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解决这个问题的一种方法是对在短时间内收集的新闻故事运行文本聚类算法，并在它们开始过多影响我们的预训练情绪分析模型之前识别新兴趋势。</p><p id="ebd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，自从 Covid 疫情开始以来，我们开始看到情绪分析模型的性能下降。我们通过运行文本聚类和主题模型来缓解这一问题，并发现一个新的主题/聚类正在围绕一些令牌出现，如“工资保护计划”、“锁定”、“口罩”、“疫苗”、“空降”。我们不断保存来自这些“新”聚类的数据，直到我们有足够的数据点来重新训练我们的监督模型。</p><p id="8274" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">新闻 API </strong></p><p id="cc90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以从 Specrom Analytics 的<a class="ae le" href="http://www.specrom.com/products-services/" rel="noopener ugc nofollow" target="_blank">媒体监控</a>数据库获得最近的(&lt;24 h) news articles in structured format by accessing a <a class="ae le" href="https://algorithmia.com/algorithms/specrom/LatestNewsAPI/docs" rel="noopener ugc nofollow" target="_blank">公共新闻数据 API </a>)公开数据。你必须在 Algorithmia 注册一个免费账户。当你注册的时候，你每个月会得到 10，000 点积分，这对于一个月 500 次 API 调用来说足够了。</p><p id="4510" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要获取您的 API 密钥，请转到仪表板，然后单击我的 API 密钥，如下所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/670fee1cab25e1657c46dc0b61fa6ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8hFdZn2EjCu1kGd6.PNG"/></div></div></figure><p id="631e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Algorithmia 拥有所有流行软件语言的客户端库，所以让我们通过在 bash 或命令行上键入下面的文本来获得 python 客户端。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="6433" class="lw lx it ls b gy ly lz l ma mb">pip install algorithmia`</span></pre><p id="fe44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个 LatestNewsAPI 调用最多给我们 100 个结果，所以如果我们想要获取 1000 个新闻故事，我们将不得不分页大约 10 次，这对于本教程中的集群演示来说应该足够了。除了为文章的主要内容和标题指定关键字查询之外，API 本身还允许您按主题进行过滤。您还可以通过域名地址列表指定过滤，因为该数据库目前索引了超过 60，000 个新闻源。</p><p id="1576" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在这种情况下，我只通过在主题中指定“业务”来过滤它。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="ad5f" class="lw lx it ls b gy ly lz l ma mb">clean_response_list = []<br/>for i in range(1,11):<br/>    print("fetching page ", str(i))<br/>    input = {<br/>  "domains": "",<br/>  "topic": "business",<br/>  "q": "",<br/>  "qInTitle": "",<br/>  "page": str(i),<br/>        "content": "true"<br/>    }<br/>    client = Algorithmia.client(YOUR_ALGO_KEY)<br/>    algo = client.algo('specrom/LatestNewsAPI/0.1.4')<br/>    response_dict = algo.pipe(input).result<br/>    #print("Total results found: ", response_dict["totalResults"])<br/>    clean_response_list = clean_response_list + response_dict["Article"]<br/>len(clean_response_list)<br/></span></pre><p id="3dca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将结果加载到一个熊猫数据框架中，并创建一个单词云来查看前 1000 个新闻故事中单词的初始分布。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="12af" class="lw lx it ls b gy ly lz l ma mb">import numpy as np<br/>import pandas as pd<br/>df = pd.DataFrame(clean_response_list)<br/>df.head()</span><span id="3736" class="lw lx it ls b gy mc lz l ma mb">from wordcloud import WordCloud, STOPWORDS<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>wordcloud_generated_bywd = WordCloud(stopwords = set(STOPWORDS),background_color="white").generate(' '.join(df['content']))<br/># Generate plot<br/>plt.imshow(wordcloud_generated_bywd)<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi md"><img src="../Images/6247134a7a84dcd900d81bdfaa0cbd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*pOHO2Q0EiABTxjfThNY35Q.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">来自 specrom News API 的 1000 个新闻故事的词云</p></figure><p id="ad0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用 tf-idf 向量对文档进行矢量化，并通过奇异向量分解来降低维度。关于如何预处理文本的完整讨论请参考之前的博客文章。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="8880" class="lw lx it ls b gy ly lz l ma mb">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="b14e" class="lw lx it ls b gy mc lz l ma mb">tfidf_transformer = TfidfVectorizer(stop_words=’english’, <br/>ngram_range=(1, 3),max_df=0.99, min_df = 0.01, lowercase=True, max_features=2500)</span><span id="1506" class="lw lx it ls b gy mc lz l ma mb">X_train_text = tfidf_transformer.fit_transform(df[“content”])<br/>df_dtm = pd.DataFrame(X_train_text.toarray(), columns=tfidf_transformer.get_feature_names())<br/>#df_dtm.head()<br/>from sklearn.decomposition import TruncatedSVD</span><span id="04b0" class="lw lx it ls b gy mc lz l ma mb">svd = TruncatedSVD(n_components=50, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)<br/>x_svd = svd.fit_transform(X_train_text)<br/>#len(x_svd)</span></pre><p id="9879" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此时，我们已经准备好使用凝聚聚类来创建文本聚类。所有文本聚类算法的一个问题是需要预先指定文本聚类的数量作为超参数。有经验方法可用，如 k 均值聚类的“肘方法”,但它们需要太多的计算资源，无法每天运行。</p><p id="d38c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，凝聚聚类非常快，它允许我们通过新闻数据迭代大量可能的聚类值，并以相当低的成本检查聚类成员数量的增长。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="4b0d" class="lw lx it ls b gy ly lz l ma mb">from sklearn.cluster import AgglomerativeClustering</span><span id="1a86" class="lw lx it ls b gy mc lz l ma mb">cluster_list = range(2,10)<br/>def get_optimum_ag_clusters(input_array, cluster_list):<br/>    return_list = []<br/>    for cluster_n in cluster_list:<br/>        temp_dict = {}<br/>        AG = AgglomerativeClustering(n_clusters=cluster_n, affinity='euclidean', memory=None, connectivity=None, compute_full_tree=True,linkage='ward', pooling_func='deprecated')<br/>        pred_labels = AG.fit_predict(input_array)<br/>        valcount_series = pd.Series(pred_labels).value_counts()<br/>        temp_dict["cluster_n"] = cluster_n<br/>        temp_dict["cluster_values"] = valcount_series.tolist()<br/>        return_list.append(temp_dict)<br/>    return return_list</span><span id="321b" class="lw lx it ls b gy mc lz l ma mb">#return_list = get_optimum_ag_clusters(X_train_text.toarray(), cluster_list)<br/>return_list = get_optimum_ag_clusters(x_svd, cluster_list)</span><span id="18cb" class="lw lx it ls b gy mc lz l ma mb">return_list<br/># Output<br/>[{'cluster_n': 2, 'cluster_values': [788, 212]},<br/> {'cluster_n': 3, 'cluster_values': [774, 212, 14]},<br/> {'cluster_n': 4, 'cluster_values': [774, 199, 14, 13]},<br/> {'cluster_n': 5, 'cluster_values': [761, 199, 14, 13, 13]},<br/> {'cluster_n': 6, 'cluster_values': [751, 199, 14, 13, 13, 10]},<br/> {'cluster_n': 7, 'cluster_values': [730, 199, 21, 14, 13, 13, 10]},<br/> {'cluster_n': 8, 'cluster_values': [718, 199, 21, 14, 13, 13, 12, 10]},<br/> {'cluster_n': 9, 'cluster_values': [607, 199, 111, 21, 14, 13, 13, 12, 10]}]</span></pre><p id="8dbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不一定对这里确定的最大集群感兴趣；这可能会被我们通过现有的文本分析渠道或经验已经知道的东西所主导。更有趣的是，当天隐藏的新闻故事可能会在多个渠道受到关注。</p><p id="388f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们打印 6 个 n_clusters 的每个集群的顶部术语。</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="3d68" class="lw lx it ls b gy ly lz l ma mb">AG = AgglomerativeClustering(n_clusters=6, affinity='euclidean', memory=None, connectivity=None, compute_full_tree=True,linkage='ward', pooling_func='deprecated')<br/>#y_km = AG.fit_predict(X_train_text.toarray())<br/>y_km = AG.fit_predict(x_svd)</span><span id="705a" class="lw lx it ls b gy mc lz l ma mb">df_dtm["cluster_name"] = y_km<br/>df_dtm.head()</span><span id="7e4d" class="lw lx it ls b gy mc lz l ma mb">cluster_list = len(df_dtm['cluster_name'].unique())<br/>for cluster_number in range(cluster_list):<br/>    print("*"*20)<br/>    print("Cluster %d: " % cluster_number)<br/>    df_cl = df_dtm[df_dtm['cluster_name'] == cluster_number]<br/>    df_cl = df_cl.drop(columns = 'cluster_name')<br/>    print("Total documents in cluster: ", len(df_cl))<br/>    print()<br/>    df_sum = df_cl.agg(['sum'])<br/>    df_sum = df_sum.transpose()<br/>    df_sum_transpose_sort_descending= df_sum.sort_values(by = 'sum', ascending = False)<br/>    df_sum_transpose_sort_descending.index.name = 'words'<br/>    df_sum_transpose_sort_descending.reset_index(inplace=True)<br/>    print(','.join(df_sum_transpose_sort_descending.words.iloc[:20].tolist()))</span></pre><p id="0a9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最上面的两个集群代表了关于新冠肺炎困境和全球经济影响的可预测新闻。然而，其他集群似乎有一个有趣的关键字组合，如下所示:</p><pre class="lg lh li lj gt lr ls lt lu aw lv bi"><span id="0c0f" class="lw lx it ls b gy ly lz l ma mb">Cluster 2: <br/>Total documents in cluster:  14<br/><br/>huawei,networks,5g,british,britain,nikkei,5g networks,japan,nikkei said,tokyo,wireless,suppliers,british government,equipment,citing,citing sources,potential alternative suppliers,alternative suppliers,nec corp,potential alternative<br/>********************<br/>Cluster 4: <br/>Total documents in cluster:  13<br/><br/>trader,joe,trader joe,petition,products,josé trader,trader josé,trader josé trader,josé,food,ethnic,san,process,chain,ethnic sounding,sounding,racist,labeled,retail giant,signed online petition<br/>********************<br/>Cluster 5: <br/>Total documents in cluster:  10<br/><br/>freitas,congressional,state del,del,richmond,convention,gop,paperwork time,paperwork,republicans,state,campaign,house,spanberger,won,va,nick,marquee,picked,write</span></pre><p id="87c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，看起来集群 2 正在与华为讨论一些与 5G 相关的问题，这些问题与英国政府有关。我们可以过滤我们的数据框架以查看集群 2 中的成员，我们可以立即看到这是一个新兴的新闻趋势，它是由几天前的一篇<a class="ae le" href="https://www.reuters.com/article/us-britain-huawei-japan/uk-asks-japan-for-huawei-alternatives-in-5g-networks-nikkei-idUSKCN24K01I" rel="noopener ugc nofollow" target="_blank">路透社文章</a>引发的，当时他们报道了英国政府要求日本在 5G 无线网络上替代华为。</p><p id="a087" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">集群 4 是关于 Trader Joe 的品牌被认为是种族主义的，这产生了大量关于该品牌的负面报道。</p><p id="e453" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确定新的集群后，您可以运行名称实体关系模型来确定集群中的顶级实体，运行情感分析来查看其影响，并检查新闻传播流，以确定集群是否会在未来几天作为企业社交监听应用的一部分进行扩展。</p><p id="5e0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的几个月里，我们使用相同的技术发现了许多有趣的新兴话题，这使得我们的文本分类和社交听力模型对于我们今天所处的快速变化的世界更加强大。</p></div></div>    
</body>
</html>