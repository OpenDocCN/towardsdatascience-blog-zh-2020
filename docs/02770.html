<html>
<head>
<title>Convolutional Neural Networks in Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践中的卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolutional-neural-networks-in-practice-406426c6c19a?source=collection_archive---------14-----------------------#2020-03-17">https://towardsdatascience.com/convolutional-neural-networks-in-practice-406426c6c19a?source=collection_archive---------14-----------------------#2020-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Keras开发和实现你的第一个CNN！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d53b9ab8a3f5f91aeef3eab67687d772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*lLdcaV2vQnhEZsCg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><h1 id="ce4e" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="3334" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">本文的目标是成为如何开发卷积神经网络模型的教程。如果你想探索它们的理论基础，我鼓励你去看看这篇文章。</p><h1 id="e1f5" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">CIFAR-10数据集</h1><p id="e6ad" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在第一个例子中，我们将实现一个可以区分10种对象的网络。为此，我们将使用CIFAR-10数据集。该数据集由60，000张彩色图片组成，分辨率为32x32像素，分为10个不同的类别，可在下图中查看。数据集被分成50.000个训练图片和10.000个用于测试的图片。</p><p id="efc9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为了开发这个实现，我们将不使用TensorFlow，而是使用Keras。Keras是一个工作在TF之上的框架，它带来了灵活性、快速性和易用性。这些是它最近在深度学习开发者中受欢迎程度上升的主要原因。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="463a" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Original Dataset: </strong><a class="ae mi" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="mp iu">https://www.cs.toronto.edu/~kriz/cifar.html</strong></a><strong class="mp iu"> for more information</strong></span><span id="0819" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Load of necessary libraries</strong><br/>import numpy as np<br/>from keras.datasets import cifar10<br/>from keras.models import Sequential<br/>from keras.layers.core import Dense, Flatten<br/>from keras.layers.convolutional import Conv2D<br/>from keras.optimizers import Adam<br/>from keras.layers.pooling import MaxPooling2D<br/>from keras.utils import to_categorical</span><span id="c404" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># to make the example replicable</strong><br/>np.random.seed(42)</span><span id="2acd" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Load of the dataset</strong><br/>(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/65bee7c114e995f7cf4a02ccd7b2585d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4uJiXeFwaCYIuPxtIiS4A.png"/></div></div></figure><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="7728" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">i</strong><br/>import matplotlib.pyplot as plt<br/>class_names = ['airplane','automobile','bird','cat','deer',<br/>               'dog','frog','horse','ship','truck']<br/>fig = plt.figure(figsize=(8,3))<br/>for i in range(len(class_names)):<br/>  ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])<br/>  idx = np.where(Y_train[:]==i)[0]<br/>  features_idx = X_train[idx,::]<br/>  img_num = np.random.randint(features_idx.shape[0])<br/>  im = features_idx[img_num,::]<br/>  ax.set_title(class_names[i])<br/>  #im = np.transpose(features_idx[img_num,::], (1, 2, 0))<br/>  plt.imshow(im)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2e38841b9555e60153a63c4e877acd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*kB1ABVa633yt3CjJDjQ8tA.png"/></div></figure><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="3339" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Initializing the model</strong><br/>model = Sequential()</span><span id="0db0" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="862f" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="672b" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="134b" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># We add our classificator</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</span><span id="6d0a" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="e6c1" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Training of the model</strong><br/>model.fit(X_train, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test, to_categorical(Y_test)))</span><span id="7ae5" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluation of the model</strong><br/>scores = model.evaluate(X_test, to_categorical(Y_test))</span><span id="8c5d" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nf"><img src="../Images/f1df331758537e8448f6ba0cc1015eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LjCWLkjzFam6uNsdA99ZSA.png"/></div></div></figure><p id="1d7c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是怎么回事？只有10%的准确率？似乎我们的网络预测所有样本的类别相同。意味着某些东西正在失败。这肯定与我们的数据有关。在将它输入模型之前，我们没有对它进行预处理。</p><h2 id="8532" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">数据预处理</h2><p id="d90c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">第一件事是预处理数据，使我们的网络任务尽可能简单。如果我们做不到这一点，因为我们有从0到255的数据，网络将永远学不到任何东西。</p><p id="a3c1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为了进行这种预处理，通常要做两件事:</p><ul class=""><li id="ae0a" class="nr ns it lo b lp mj ls mk lv nt lz nu md nv mh nw nx ny nz bi translated"><strong class="lo iu">数据居中</strong>:计算数据集的平均值并减去。处理图像时，您可以计算数据集的整个平均值并直接减去它，也可以计算图像每个通道的平均值并从每个通道中减去它。</li><li id="afe1" class="nr ns it lo b lp oa ls ob lv oc lz od md oe mh nw nx ny nz bi translated"><strong class="lo iu">标准化数据</strong>:这样做是为了让所有的数据具有大致相同的比例。两种最常见的方法是:</li></ul><p id="caf1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">1)在数据居中(减去平均值)后，将每个维度除以其标准偏差</p><p id="b998" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">2)归一化，使每个维度的最小值和最大值分别为-1和1。只有当我们从不同尺度的数据开始，但我们知道它们应该是相似的，也就是说，它们对算法具有相似的重要性时，这才有意义。在图像的情况下，我们知道可以取的值是从0到255，因此没有必要进行严格的归一化，因为这些值已经处于类似的范围内。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi of"><img src="../Images/24b3bdfc5ff8fd947b44313ee5d834fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ox-p57oxfmaVSaJyJWyPg.png"/></div></div></figure><p id="8cf0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">重要提示！</strong></p><p id="41fc" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">必须只对训练集进行规范化。换句话说，我们应该计算训练集的平均值和标准偏差，并将这些值用于验证集和测试集。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="f61e" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Cenetering the data</strong><br/>X_train_mean = np.mean(X_train, axis = 0)<br/>X_train_cent = X_train - X_train_mean</span><span id="9fc7" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Normalization</strong><br/>X_train_std = np.std(X_train, axis = 0)<br/>X_train_norm = X_train_cent / X_train_std</span></pre><p id="f150" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，我们使用训练集的平均值和标准差来准备验证和测试数据。</p><p id="b5c8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">等等，但是我们没有验证数据！我们将以这种方式实现这个示例，但非常重要的是，当我们进行真正的开发时，我们有三个集合:</p><ol class=""><li id="2238" class="nr ns it lo b lp mj ls mk lv nt lz nu md nv mh og nx ny nz bi translated"><strong class="lo iu">训练集</strong>:更新每批的权重</li><li id="b466" class="nr ns it lo b lp oa ls ob lv oc lz od md oe mh og nx ny nz bi translated"><strong class="lo iu">验证集</strong>:检查网络在各个时期的泛化能力。它使用在训练期间未见过的样本来测试模型，它用于监控网络的训练以供参考，但是它不干预任何计算！它通常在您想要调整参数时使用，该设置是指示哪些参数最适合使用的设置。验证越准确，我们拥有的参数集就越好。由于这个原因，我们不能依靠这个结果来给出网络泛化能力的概念，因为我们选择了网络配置来给出更高的精度。因此，我们必须有一个额外的集合，让我们现在可以说，我们的网络是否适合我从未见过的样本:测试样本。</li><li id="c8e9" class="nr ns it lo b lp oa ls ob lv oc lz od md oe mh og nx ny nz bi translated"><strong class="lo iu">测试集</strong>:它通过对一个从未见过的集(比验证集大)进行归纳，给我们一种我们的网络有多好的直觉。</li></ol><p id="04b8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">好了，让我们准备好测试设备:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="5d4c" class="mt kv it mp b gy mu mv l mw mx">X_test_norm = (X_test - X_train_mean) / X_train_std</span></pre><p id="d7fc" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在我们准备用标准化数据再次测试我们的网络:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e46e" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Initializing the model</strong><br/>model = Sequential()</span><span id="5d37" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="e5c2" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="4a8b" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="aa83" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># We add our classificator</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</span><span id="67e2" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="e458" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Training of the model</strong><br/>model.fit(X_train, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test, to_categorical(Y_test)))</span><span id="e45c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluation of the model</strong><br/>scores = model.evaluate(X_test, to_categorical(Y_test))</span><span id="fc2b" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oh"><img src="../Images/9398cd469ef6e8fdc3f57c1af6acfbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7SPjfgooNbbv9Yt4C42QGQ.png"/></div></div></figure><p id="d32c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">众所周知，这个结果比前一个好。因此，我们可以自豪地说，我们已经训练了第一个CNN，训练精度约为0.99，测试精度约为0.7。</p><p id="422f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">下一个合乎逻辑的问题应该是:</p><p id="8121" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu"> <em class="oi">培训和测试怎么会有这么大的区别？</em>T9】</strong></p><p id="54e4" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">正如你们可能已经想到的，深度学习也存在过度拟合，事实上，比其他技术更加明显。</p><p id="fa8c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对于那些不记得什么是过度适配的人来说，想想这个:</p><p id="a444" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你有一个网络可以在任何给定的时间检测到哪个角色出现在老友记第4x08章。它的工作非常完美，它可以以99.3%的准确率分辨出哪些角色在舞台上。它非常好用，你可以用5x01试试。结果是只有71.2%的准确率。</p><p id="d76b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">嗯，这种现象被称为过度拟合，包括创建一个在我们的数据集中工作得很好的算法，但在推广方面非常糟糕。</p><p id="fd9b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在本文中，您可以找到对过度拟合的更深入的探索以及最小化过度拟合的技术。</p><p id="e858" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">请看代表基于时间的精度的图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/20cb882d42d2522e53d45015c0c08b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*q_4vrZJyeNWxpzEOPM-P-A.png"/></div></figure><p id="ee34" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">看看这个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ok"><img src="../Images/75fc5bf28724782ada4e449b4aeecb45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uspYtAclg_bmBUd1tpyU9w.png"/></div></div></figure><p id="36b1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你会选哪一个？</p><p id="afb6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">20层的比3层的好用吧？然而，我们通常寻求的是它具有良好的泛化能力(当发现新数据时，它工作得很好)。如果看到新的数据，你觉得哪个效果会更好？</p><p id="407b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">令人惊讶的是，左边的那个。</p><p id="0d0f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们回到我们的例子。在我们的例子中，我肯定我们都更喜欢99对70，而不是90对85，对吗？</p><p id="4d91" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如何才能实现这一点？标准化和规范化的技术。</p><p id="7f17" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">重要提示</strong>:实际上，通常对图像进行的唯一预处理是将它们的所有值除以255。这通常足以使网络正常工作，因此我们不依赖于与我们的训练集相关的任何参数。</p><h1 id="6895" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">处理过度拟合</h1><p id="7e54" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">有几种方法可以尽可能地减少过拟合，从而使算法能够更一般化。</p><h2 id="5904" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">批量标准化</h2><p id="0735" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">称为批量标准化的技术是由Ioffe和Szegedy开发的技术，旨在减少内部协变量的变化或内部协变量移位，这使得网络对不良初始化更加鲁棒。</p><p id="78e9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">内部协变量偏移被定义为由于小批量之间输入数据的不同分布而导致的网络激活分布的变化。小批量之间的这种差异越小，到达网络过滤器的数据就越相似，激活图就越相似，网络训练的效果就越好。</p><p id="fe9d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是通过在训练开始时强制网络激活具有单一高斯分布的选定值来实现的。这个过程是可能的，因为规范化是一个可区分的操作。</p><p id="f797" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它通常在激活功能执行之前插入:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="81a1" class="mt kv it mp b gy mu mv l mw mx">model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(32, 32, 3))</span><span id="570a" class="mt kv it mp b gy my mv l mw mx">model.add(BatchNormalization())</span><span id="15da" class="mt kv it mp b gy my mv l mw mx">model.add(Activation('relu'))</span></pre><p id="acf0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">用数学术语来说，我们所做的是用小批量计算的平均值和标准偏差对进入我们网络的每个小批量进行居中和标准化，然后用网络通过训练学习的参数重新调整和偏移数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/390c6a5fc8b20ac4332343772bbe18c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*wc9AwUMnx6CURk7_W6EGVA.png"/></div></figure><p id="ded4" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">此外，由于我们计算的是每个小批量的平均值和标准偏差，而不是整个数据集的平均值和标准偏差，因此批量定额还会引入一些噪声，这些噪声起到调节器的作用，有助于减少过度拟合。</p><p id="af0e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">事实证明，这种技术对于更快地训练网络非常有效。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4dea" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># We Import Batch Normalizarion layer</strong><br/>from keras.layers import BatchNormalization, Activation</span><span id="2b69" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializting the model</strong><br/>model = Sequential()</span><span id="f58d" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(32, 32, 3)))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))</span><span id="afa8" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))</span><span id="a469" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a thirdd convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))</span><span id="da7e" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># We include our classifier</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</span><span id="4b7c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="e655" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Training the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test))) # aquí deberíamos usar un conjunto distinto al de test!!!</span><span id="53cb" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="1f66" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi om"><img src="../Images/209acea8ef14d0355d2f9e4dcffd8da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvV2eI9R8szXpZ-QnHfoxA.png"/></div></div></figure><p id="4895" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们可以看到，精度提高了2%，一旦我们达到高数值，这是一个巨大的进步。但仍有改进的余地。</p><p id="08cf" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们来探讨一下正规化。</p><h2 id="5848" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">正规化</h2><p id="8598" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">正则化包括以某种方式惩罚我们的网络在训练期间做出的预测，以便它不认为训练集是绝对真实的，从而知道当它看到其他数据集时如何更好地进行概括。</p><p id="97dd" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">看一下这张图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/fa5d50657d298bfae88c80ab894acd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*nKa-F2MaTgnIzDsv.jpg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">https://commons.wikimedia.org/wiki/File:75hwQ.jpg<a class="ae mi" href="https://commons.wikimedia.org/wiki/File:75hwQ.jpg" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="ac07" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在这个图表中，我们可以看到一个过度拟合的例子，另一个欠拟合的例子，以及另一个可以正确概括的例子。</p><p id="96f1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">哪个是哪个？</p><ul class=""><li id="6c7e" class="nr ns it lo b lp mj ls mk lv nt lz nu md nv mh nw nx ny nz bi translated">蓝色:过度合身</li><li id="a343" class="nr ns it lo b lp oa ls ob lv oc lz od md oe mh nw nx ny nz bi translated">绿色:具有概括能力的好模型</li><li id="f490" class="nr ns it lo b lp oa ls ob lv oc lz od md oe mh nw nx ny nz bi translated">橙色:不合身</li></ul><p id="7fe7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，看看这个例子，在这个例子之后，有三个神经元数目不同的网络。我们现在看到的是20个神经元组成的网络，具有不同的正则化水平。</p><p id="b232" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">您可以在这里使用这些参数:</p><p id="48e1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><a class="ae mi" href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html" rel="noopener ugc nofollow" target="_blank">https://cs . Stanford . edu/people/kar pathy/convnetjs/demo/classify 2d . html</a></p><p id="bf8f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这里有一个更完整的:</p><p id="8e14" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">https://playground.tensorflow.org/<a class="ae mi" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"/></p><p id="fc8a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">最后，有许多层的网并应用正则化比有一个小的网以避免过度拟合要好得多。这是因为小网络是更简单的函数，具有更少的局部最小值，所以梯度下降达到一个或另一个很大程度上取决于初始化，所以实现的损耗通常具有很大的方差，这取决于初始化。</p><p id="bbcf" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">然而，具有许多层的网络是更复杂的函数，具有更多的局部最小值，尽管它们更难达到，但通常具有所有相似的和更好的损耗。</p><p id="a7a5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果你对这个话题感兴趣:【http://cs231n.github.io/neural-networks-1/#arch】T4。</p><p id="7f3a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">正则化的方法有很多。以下是最常见的几种:</p><h2 id="68fc" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">L2正则化(拉索正则化)</h2><p id="662d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">L2正则化可能是最常见的。</p><p id="b2b2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它包括通过为每个权重添加1/2 * λ* <em class="oi"> W**2 </em>项来惩罚损失函数，这导致:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b9de3b89be7ecfc5592856d4d713e5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*X9kVU_H5CktXJoJ7VABHiw.png"/></div></figure><p id="b4a2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">当计算导数时，1/2只是为了方便，因为这留下了λ* <em class="oi"> W </em>而不是2*λ* <em class="oi"> W </em>。</p><p id="1931" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这意味着我们惩罚非常高或不同的权重，并希望它们都是相似的量级。如果你还记得，权重意味着每个神经元在预测的最终计算中的重要性。因此，通过这样做，我们使所有的神经元或多或少地同等重要，也就是说，网络将使用其所有的神经元来进行预测。</p><p id="61ed" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">相反，如果某些神经元有非常高的权重，预测的计算就会更多地考虑它们，所以我们最终会得到一个没有用的死神经元网络。</p><p id="1e38" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">此外，在我们的损失函数中引入1/2 * λ* <em class="oi"> W**2 </em>项使得我们的权重在梯度下降期间接近于零。随着W+=-λ⋅W.的线性衰减</p><p id="34df" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们看看是否可以通过应用L2正则化来改进我们的网络:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="2baf" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># L2 Regularization</strong></span><span id="2eaf" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Regularizer layer import</strong><br/>from keras.regularizers import l2</span><span id="85ee" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="2a11" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="6ba2" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="c9df" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="439c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01)))<br/>model.add(Dense(10, activation='softmax'))</span><span id="f207" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="bd42" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="244c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="e60b" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi op"><img src="../Images/dc0169582b4010eddcabdc30079e0f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYvhcgqrXh1eHYoRtoeqDQ.png"/></div></div></figure><h2 id="b846" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">L1正则化(脊正则化)</h2><p id="92be" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">L1也很常见。这一次，我们在损失函数中加入了λ|w|项。</p><p id="1790" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们还可以在所谓的<strong class="lo iu">弹性网正则化</strong>中将L1正则化与L2结合起来:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/11ea2f20ed17abc8b8d077cd1e239b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*Xwq8onOfBx0GspqzKLFrNQ.png"/></div></figure><p id="606c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">L1正则化设法将W权重矩阵转换成稀疏权重矩阵(非常接近零，除了少数元素)。</p><p id="de18" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这意味着，与L2不同，它给予一些神经元比其他神经元更大的重要性，使得网络对可能的噪声更具鲁棒性。</p><p id="c915" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">一般来说，L2通常会给出更好的结果。如果您知道图像中有一定数量的特征可以很好地进行分类，并且您不希望网络被噪声扭曲，则可以使用L1。</p><p id="d7e8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们试试L1，然后是L1+L2:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="7a57" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># L1 Regularization</strong></span><span id="2f20" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Regularizer layer import</strong><br/>from keras.regularizers import l1</span><span id="7732" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="1480" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="5258" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="d111" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="3bcd" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu', kernel_regularizer=l1(0.01)))<br/>model.add(Dense(10, activation='softmax'))</span><span id="4fa5" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="03e6" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="448e" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="382b" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi or"><img src="../Images/205dfe9ce47b0009c3e9baa1d9c9fca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbhSmQbJsnYeMMmoFpSgDQ.png"/></div></div></figure><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="d4a4" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Elastic Net Regularization (L1 + L2)</strong></span><span id="9f0f" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Regularizer layer import</strong><br/>from keras.regularizers import l1_l2</span><span id="6f23" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="ad98" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="4f45" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="c7d3" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="7771" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01)))<br/>model.add(Dense(10, activation='softmax'))</span><span id="739e" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="3025" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="4e45" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="fa7d" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi os"><img src="../Images/eaa9c275f1192cb796eec7c8633f01f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ruy5WpFi5tgxoZpd5R-rEA.png"/></div></div></figure><h2 id="5dcd" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">最大范数约束</h2><p id="3aa7" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">另一种正规化是基于限制的正规化。例如，我们可以设置权重不能超过的最大阈值。</p><p id="4a5a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在实践中，这是通过使用下降梯度来计算新的权重值来实现的，正如我们通常所做的那样，但是然后为每个神经元计算每个权重向量的范数2，并将其作为不能超过<em class="oi"> C </em>的条件，即:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e82df00832e91b5a4078674c9e5a8ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*1ElCSpBFsZv2DvOMZRNSVw.png"/></div></figure><p id="9b58" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">正常情况下，<em class="oi"> C </em>等于3或4。</p><p id="50e9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们通过这种标准化实现的是网络不会“爆炸”，也就是说，权重不会过度增长。</p><p id="05b9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们看看这种正规化是如何进行的:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="6e34" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Elastic Net Regularization (L1 + L2)</strong></span><span id="5211" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Regularizer layer import</strong><br/>from keras.constraints import max_norm</span><span id="67cf" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="435c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))</span><span id="ab57" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="b7a0" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</span><span id="65a9" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu', kernel_costraint=max_norm(3.)))<br/>model.add(Dense(10, activation='softmax'))</span><span id="4123" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="5616" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="de61" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="d6e0" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ou"><img src="../Images/662c899e86b8fa2f732e07b83ef1be87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEEuHNqPckOW5lFTEDBMVg.png"/></div></div></figure><h2 id="77e2" class="mt kv it bd kw ng nh dn la ni nj dp le lv nk nl lg lz nm nn li md no np lk nq bi translated">辍学正规化</h2><p id="7098" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">退出正则化是Srivastava等人在他们的文章“<a class="ae mi" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">退出:防止神经网络过度拟合的简单方法</a>”中开发的一种技术，补充了其他类型的标准化(L1、L2、maxnorm)。</p><p id="8b48" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是一种非常有效和简单的技术，它包括在训练期间以概率<em class="oi"> p </em>保持神经元活跃或将其设置为0。</p><p id="33d3" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们实现的是在训练时改变网络的架构，这意味着将不会有单个神经元负责被激活到某个模式，但我们将有多个冗余神经元能够对该模式做出反应。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/19aa8a8cc2c0c0bb8231e8264f2c727c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*59fT8DXweVP2AmDVq-ASKg.png"/></div></figure><p id="0c25" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们看看应用辍学如何影响我们的结果:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="8a35" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Dropout</strong></span><span id="754b" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Dropout layer import</strong><br/>from keras.layers import Dropout</span><span id="a3bd" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="e149" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))<br/>model.add(Dropout(0.25))</span><span id="40a8" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(Dropout(0.25))</span><span id="ebe8" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(Dropout(0.25))</span><span id="505b" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(10, activation='softmax'))</span><span id="3746" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="7e92" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="1f4e" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="76d7" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ow"><img src="../Images/8eb527452ff26ad0b7b25c8c67423d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VbV0RhWWnPLedrdd1gq2sw.png"/></div></div></figure><p id="dc94" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，让我们看看Max norm + Dropout的影响:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="45cc" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu"># Dropout &amp; Max Norm</strong></span><span id="bd4c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Dropout &amp; Max Norm layers import</strong><br/>from keras.layers import Dropout<br/>from keras.constraints import max_norm</span><span id="8e69" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Inizializing the model</strong><br/>model = Sequential()</span><span id="d33f" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))<br/>model.add(Dropout(0.25))</span><span id="ddb7" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a second convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(Dropout(0.25))</span><span id="d97e" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Defining a third convolutional layer</strong><br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))<br/>model.add(Dropout(0.25))</span><span id="1cc7" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Classifier inclusion</strong><br/>model.add(Flatten())<br/>model.add(Dense(1024, activation='relu', kernel_constraint=max_norm(3.)))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(10, activation='softmax'))</span><span id="b65d" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Compiling the model</strong><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=Adam(lr=0.0001, decay=1e-6),<br/>              metrics=['accuracy'])</span><span id="e3f5" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Traning the model</strong><br/>model.fit(X_train_norm, to_categorical(Y_train),<br/>          batch_size=128,<br/>          shuffle=True,<br/>          epochs=10,<br/>          validation_data=(X_test_norm, to_categorical(Y_test)))</span><span id="ac7c" class="mt kv it mp b gy my mv l mw mx"><strong class="mp iu"># Evaluating the model</strong><br/>scores = model.evaluate(X_test_norm, to_categorical(Y_test))</span><span id="6cec" class="mt kv it mp b gy my mv l mw mx">print('Loss: %.3f' % scores[0])<br/>print('Accuracy: %.3f' % scores[1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ox"><img src="../Images/7470e8e76566b888ff18b3043a5e4c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57p6rPcmzhQ8449EO67zyg.png"/></div></div></figure><p id="e9c0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">有更多的技术来处理过度拟合，如最大池，改变步幅…等等。在实践中，最好的方法是应用其中的几种，并根据所面临的问题测试哪种组合能提供最好的结果。</p><h1 id="39d0" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">最后的话</h1><p id="1f97" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">一如既往，我希望你<strong class="lo iu"> </strong>喜欢这篇文章，并且获得了关于如何实现和开发卷积神经网络的直觉！</p><p id="33b7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="oi">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae mi" href="https://medium.com/@rromanss23" rel="noopener"> <em class="oi">这里</em> </a> <em class="oi">。</em></p><p id="779c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="oi">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="lo iu"> <em class="oi">请在Medium </em> </strong> <em class="oi">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>