<html>
<head>
<title>Understand the approximation power of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络的近似能力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-neural-networks-and-their-fascinating-effectiveness-81ebc054cb16?source=collection_archive---------37-----------------------#2020-07-17">https://towardsdatascience.com/exploring-neural-networks-and-their-fascinating-effectiveness-81ebc054cb16?source=collection_archive---------37-----------------------#2020-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a702" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener"> <strong class="ak">里面的艾</strong> </a></h2><div class=""/><div class=""><h2 id="b5bf" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">探索神经网络有效性的基本概念。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c3e79ec0d055b6abfa74578b7b023dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*erG-0nk0BIYczK1r"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@brookelark?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布鲁克·拉克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="d8d1" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">一切开始的地方。</h1><p id="69dd" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这一切都始于理解大脑实际上是如何工作的想法。早在 20 世纪 40 年代，麦卡洛克和皮茨就引入了神经元[1]，20 世纪 50 年代，弗兰克·罗森博尔特推出了第一台感知机[2]。自 20 世纪 40 年代以来，神经网络就伴随着我们，但是，由于缺乏实际的实施，该领域面临着起伏。最近涉及各种神经网络架构的深度学习技术实践的增长是因为两个主要进步，第一，<strong class="lz ja"><em class="mt"/></strong>(高性能 CPU 和 GPU)，第二，<strong class="lz ja"> <em class="mt">可用的数据量</em> </strong>。</p><p id="f427" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank"> Geoff Hinton </a>和他的两个研究生展示了如何使用深度学习将一个名为<a class="ae le" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja"> ImageNet </strong> </a>的非常大的数据集，包含 10000 个类别和 1000 万张图像，并将分类错误减少 20%。这发生在 2012 年的 NIPS 会议上，正如 Terrence Sejnowski 所说，</p><blockquote class="mz na nb"><p id="d766" class="lx ly mt lz b ma mu ka mc md mv kd mf nc mw mi mj nd mx mm mn ne my mq mr ms ij bi translated">传统上，在数据集(ImageNet)上，误差在一年内减少不到 1%。一年时间，绕过了 20 年的研究。这真的打开了闸门。——在《The Verge》的采访中。</p></blockquote><p id="e4de" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">这就是“<em class="mt">深度学习——嗡嗡声</em>”开始的时刻。人工智能就是从这一点发展起来的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c4b8c594933c3bb07c8dd6bef09a9116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Frss9MIfWfuir87VHL3kTw.png"/></div></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="5285" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">神经网络有多神奇？</h1><p id="35b4" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">人工神经网络由大量简单的相互连接的处理元件组成。这些元件并行操作，其功能由网络结构、连接强度以及在计算元件或节点处执行的处理决定。对深度学习兴趣的增长部分是由于传统编程技术在“硬”任务中的失败，如机器视觉、连续语音识别和机器学习。</p><p id="a103" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">神经网络在视觉、语音、信号处理和机器人方面的能力已经得到了显著的证明。神经网络解决的各种问题令人印象深刻。最近在诸如图像分类、对象检测、文本生成、图像字幕、语言翻译、GANs 等任务中有了突破。</p><p id="58bc" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">由于神经网络的有效性和高性能处理器(GPU、TPU)的可用性，神经网络的热门研究领域正在快速发展，并且范围很广。AWS、Google Cloud 等云技术的最新进展。也有助于更好的研究。但是深度学习往往面临<em class="mt">缩放、</em>和现实世界问题。(关于这些问题的讨论超出了本文的范围)</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="0ccf" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">函数逼近</h1><p id="5e2b" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">函数逼近是通过简单函数的集合来描述复杂函数的行为。这些方法包括通过<em class="mt">高斯</em>、<strong class="lz ja">级数展开</strong>的<strong class="lz ja">多项式逼近</strong>，以计算工作点附近函数的近似值，如<em class="mt">泰勒级数、</em>等等。</p><blockquote class="mz na nb"><p id="9a5f" class="lx ly mt lz b ma mu ka mc md mv kd mf nc mw mi mj nd mx mm mn ne my mq mr ms ij bi translated">神经网络是在统计上实现泛化的函数逼近机器。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/79728bc4f7ed3dc0fc4ac1dba34900da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*45quuyN7MoE8VuXynpSGGw.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">功能😋</p></figure><h1 id="2daa" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">神经网络是通用逼近器</h1><p id="d38d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">前馈神经网络提供了一个通用逼近框架，即<strong class="lz ja">通用逼近定理</strong>，</p><blockquote class="mz na nb"><p id="6fcf" class="lx ly mt lz b ma mu ka mc md mv kd mf nc mw mi mj nd mx mm mn ne my mq mr ms ij bi translated">通用逼近定理在其最一般的版本之一中说，如果我们仅考虑连续激活函数<strong class="lz ja"> σ </strong>，那么具有一个隐藏层的标准前馈神经网络能够将任何连续多元函数<strong class="lz ja"> f </strong>逼近到任何给定的逼近阈值<strong class="lz ja"> ε </strong>，当且仅当<strong class="lz ja"> σ </strong>是非多项式。[3]</p></blockquote><p id="fbe9" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">前馈网络提供了一个表示函数的通用系统，在这个意义上，给定一个函数，存在一个逼近该函数的前馈网络。这表明存在一个接近所考虑的函数的大网络，但它没有回答，到底有多大？</p><p id="80be" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">简而言之，具有单个层的前馈神经网络是代表任何函数的 suﬃcient，但是该层可能相当大，并且可能不能正确概括。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="3ebb" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">近似的例子</h1><p id="5afd" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">作为函数逼近的例子，我采用了众所周知的正弦、余弦函数和一个自定义函数。数据点的范围[-3，3]使得函数图看起来如下:</p><ol class=""><li id="eeb1" class="nt nu iq lz b ma mu md mv mg nv mk nw mo nx ms ny nz oa ob bi translated">余弦函数</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/e29509370d7bdf519c542e11884d42ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp6ezA-Ed_QAa8DCeP7uug.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">余弦图(来源:笔记本)</p></figure><p id="1139" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">2.正弦函数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/d17bf88c17d95a4b5b1c216bd3647630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BujkXNIij2kzMnUCHLT3qw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">正弦图(来源:笔记本)</p></figure><p id="02f8" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">用建筑的神经网络进一步逼近这些功能，</p><blockquote class="od"><p id="8b17" class="oe of iq bd og oh oi oj ok ol om ms dk translated">图层:输入(1)-隐藏(100)-输出(1)</p></blockquote><figure class="oo op oq or os kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/36f047a7b996e5a014d137dbc34ec1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dluic4e5ZPWGoU1gya0c2Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">神经网络体系结构[in-hidden-out，1–100–1]</p></figure><p id="3054" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">基于神经网络优化的近似函数的结果在函数图下面，</p><ol class=""><li id="5ed7" class="nt nu iq lz b ma mu md mv mg nv mk nw mo nx ms ny nz oa ob bi translated">余弦</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0a335934782f29592f1632a22732814c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*0sOF-WlWzQe4DD65PID7tg.jpeg"/></div></figure><p id="c27e" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">2.正弦</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3d0e4073adbccedb61ad8ea8fbedaabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*WfiTRba_ln8AALFYm1pxVw.jpeg"/></div></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="205d" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">问题</h1><p id="0115" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">上图显示，通过一组正确的参数，神经网络几乎符合原始函数。一切看起来都很完美，那么还会有什么地方出错呢？</p><blockquote class="od"><p id="967e" class="oe of iq bd og oh oi oj ok ol om ms dk translated">过度拟合</p></blockquote><p id="3b8b" class="pw-post-body-paragraph lx ly iq lz b ma ov ka mc md ow kd mf mg ox mi mj mk oy mm mn mo oz mq mr ms ij bi translated">是的，我们的神经网络过度拟合了！训练集上的误差被驱动到非常小的值，但是当新数据呈现给网络时，误差很大。网络已经记住了训练的例子，但是还没有学会推广到新的情况。这必须避免。我们可以看看另一个具有相同神经网络结构的自定义函数。</p><p id="5196" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">功能，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/b97751d877a8f8fd191c82e39b8e24e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*bhI7ytC981VmWyd6SQrFfQ.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">10*(a )*sin(a)*cos(s)</p></figure><p id="13fc" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">关于函数逼近，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/a6c2e9cfcdef15f415d716f3736ddd15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*RMSKEWqe_HDBeMFQyx1pSA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">接近</p></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="d465" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">结论</h1><p id="20ff" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这些是理解神经网络通用逼近能力的基础的一些尝试，这使得深度学习领域在广泛的任务上有效成为可能。最后，我们将上述内容总结如下:</p><ul class=""><li id="b3a6" class="nt nu iq lz b ma mu md mv mg nv mk nw mo nx ms pc nz oa ob bi translated">根据数据训练神经网络近似于从输入到输出的未知的基本映射函数。</li><li id="6ed3" class="nt nu iq lz b ma pd md pe mg pf mk pg mo ph ms pc nz oa ob bi translated">在训练神经网络时，过拟合等问题会阻碍新数据的结果(看不见)。</li></ul><p id="e3d1" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">人们可以通过在笔记本上用代码进行实验来观察改变神经架构和参数对结果的影响，如上图<a class="ae le" href="https://github.com/pr2tik1/universal-approximation" rel="noopener ugc nofollow" target="_blank">所示。</a></p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="377f" class="lf lg iq bd lh li nn lk ll lm no lo lp kf np kg lr ki nq kj lt kl nr km lv lw bi translated">参考</h1><p id="3660" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1]麦卡洛克，W. S .和皮茨，W. 1943。神经活动中固有观念的逻辑演算。<em class="mt">数学生物物理学通报</em>5:115–133。</p><p id="2dcd" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">[2]罗森布拉特，1957 年。感知机——一种感知和识别自动机。康奈尔航空实验室报告 85-460-1。</p><p id="ce24" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">[3]进一步了解神经网络的近似能力，Kai Fong Ernest Chong。</p><h1 id="ebb7" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">要签出的资源</h1><ul class=""><li id="eedf" class="nt nu iq lz b ma mb md me mg pi mk pj mo pk ms pc nz oa ob bi translated">【http://neuralnetworksanddeeplearning.com/chap4.html T4】</li><li id="7450" class="nt nu iq lz b ma pd md pe mg pf mk pg mo ph ms pc nz oa ob bi translated"><a class="ae le" rel="noopener" target="_blank" href="/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58">https://towards data science . com/the-approximation-power-of-neural-networks-with-python-codes-ddfc 250 BDB 58</a></li><li id="24ef" class="nt nu iq lz b ma pd md pe mg pf mk pg mo ph ms pc nz oa ob bi translated"><a class="ae le" href="https://machinelearningmastery.com/neural-networks-are-function-approximators/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/neural-networks-are-function-approximators/</a></li></ul><p id="a143" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">谢谢你！在 LinkedIn 和我联系，</p><div class="pl pm gp gr pn po"><a href="https://www.linkedin.com/in/pratik-kumar04/" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ja gy z fp pt fr fs pu fu fw iz bi translated">Pratik K. -作家-走向数据科学| LinkedIn</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">我是一名刚从泰国国立理工学院电子和通信工程专业毕业的学生</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">www.linkedin.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc ky po"/></div></div></a></div><p id="3e24" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">查看我的投资组合网页了解更多关于我的详细信息<a class="ae le" href="https://pr2tik1.github.io/" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p></div></div>    
</body>
</html>