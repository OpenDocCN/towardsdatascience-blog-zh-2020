<html>
<head>
<title>Implement SVM with Python .. in 2 minutes!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现SVM..两分钟后。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-svm-with-python-in-2-minutes-c4deb9650a02?source=collection_archive---------24-----------------------#2020-05-03">https://towardsdatascience.com/implement-svm-with-python-in-2-minutes-c4deb9650a02?source=collection_archive---------24-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7fb9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有可能在两分钟内编写一个简单的SVM版本吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5a78898ebceece1fbb02d26ec0c22f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UY0Lb4hg_n9689Mgj_ulgA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/photos/BXOXnQ26B7o" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/BXOXnQ26B7o</a></p></figure><p id="b2d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将使用Python来实现支持向量机分类器，它被认为是最复杂的基本机器学习算法之一。尽管如此，在我的代码中，与<a class="ae ky" href="https://bit.ly/3fgktMH" rel="noopener ugc nofollow" target="_blank">线性回归实现</a>唯一真正不同的是所使用的损失函数。为了更好地理解SVM使用的损失函数，我也推荐你看一下这个很棒的视频和解释。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【https://www.youtube.com/watch?v=VngCRWPrNNc T4】</p></figure><p id="a0e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从视频中你可以理解，SVM损失函数的核心是这个公式，它描述了点到超平面的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/c1ad1b04184282f33b370bb887052db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FpJCHTPrjMOMClAhH4tYug.png"/></div></div></figure><p id="1368" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过执行梯度下降和减少损失函数，SVM算法试图最大化决策边界和两类点之间的差距。SVM损失函数可以写成如下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/951ffa826bda8a788ffbb2b8f79db87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*2dQda-aRBmtXie4Lq6az1w.png"/></div></figure><p id="abd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们转到实现本身，只需要几分钟的时间来编码梯度下降，以最小化这个损失函数。</p><p id="8c35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用Python实现SVM</strong></p><ol class=""><li id="7f85" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">首先，我将创建数据集，使用sklearn.make_classification方法，我还将做一个训练测试拆分，以衡量模型的质量。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mi lw l"/></div></figure><p id="64c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.现在，我将实现上面描述的损失函数，以在训练模型时意识到损失在下降。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mi lw l"/></div></figure><p id="3a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我还创建了一个损失计算的小示例，位于函数本身的下方。</p><p id="9aed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.让我们写一个在随机梯度下降过程中，计算损失梯度的函数。这可以通过简单地将损失函数相对于W(超平面的坐标)进行微分来实现</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mi lw l"/></div></figure><p id="2ceb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.最后，让我们实现梯度下降本身(注意，我给X_train变量增加了一个额外的项，代表模型偏差)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/cfaf95cfb2f6bc5cd8b3eb1a9e8805c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5s-ukF_y6gXm2FIBS0hBPg.png"/></div></div></figure><p id="ba4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当训练完成时，可以通过简单地取权重和点坐标之间的点积来执行预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/6eb7cbf9eeda88e504b0575d43f3303a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nurtu_0BD3_5oqlEoqwr7w.png"/></div></div></figure><p id="5293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于比较，我也将安装一个sklearn SVM模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/c83c11a93814b3e22a9ccfc9ba738a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*65x00xEdfHiJm826zl_B0g.png"/></div></figure><p id="e5e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，sklearn实现的结果更好，但是对于一个非常简单的模型，我们的结果仍然很好。</p><p id="88f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">你可以在我的</strong> <a class="ae ky" href="http://artkulakov.com" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">网站</strong> </a>上查看其他帖子</p><h2 id="42d6" class="mm mn it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">你可能会对我的其他中型职位感兴趣。</h2><p id="baf4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/3bZqqLX" rel="noopener ugc nofollow" target="_blank">用Python在2分钟内实现朴素贝叶斯</a></p><p id="7b22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/3c1rejB" rel="noopener ugc nofollow" target="_blank">在2分钟内解释并实施PCA</a></p><p id="8fe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/3bZqStT" rel="noopener ugc nofollow" target="_blank">借助RapidsAI </a>，让您的机器学习速度提高300倍</p><p id="e468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk">如果这个故事对你有帮助，也许对别人也有帮助，别忘了分享一下:)</em></p></div></div>    
</body>
</html>