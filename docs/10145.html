<html>
<head>
<title>Controlling Text Generation for Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">控制语言模型的文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf?source=collection_archive---------23-----------------------#2020-07-17">https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf?source=collection_archive---------23-----------------------#2020-07-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a386" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">控制机器生成文本的样式和内容的实际操作方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/378cff9cfdb7114b5b2fc59b3d73bc5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OF_sJR0yqlaUO8S2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔恩·弗洛布兰特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="bc47" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="4388" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在看到机器能够生成任何种类的流畅文本已经很酷了。但是，如果我们可以告诉机器生成什么风格或形式，而不需要为每种风格训练单独的模型，会怎么样呢？这将耗费大量的计算能力和时间。</p><p id="0f8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我告诉你我们真的可以做到呢？介绍<a class="ae ky" href="https://arxiv.org/pdf/1912.02164.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> PPLM </strong> </a>，代表<strong class="lt iu">即插即用语言模型</strong>。多亏了<a class="ae ky" href="https://www.uber.com/us/en/uberai/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">优步人工智能</strong> </a>，PPLM 是一种将预训练的语言模型与一个或多个属性模型相结合的方法，这些模型将能够指导文本生成。PPLM 最好的一点是，它不需要语言模型训练或微调，并且对可以使用的属性模型没有限制。</p><h1 id="abb4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">它是如何工作的</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/420deb67dd80474c02e906fe64251996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38KL3unhLReIUXBD1Blu2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://eng.uber.com/pplm/" rel="noopener ugc nofollow" target="_blank">优步艾博文</a></p></figure><p id="93d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在官方的<a class="ae ky" href="https://eng.uber.com/pplm/" rel="noopener ugc nofollow" target="_blank">博客文章</a>中，作者将 GPT-2 这样的大型语言模型描绘成一只猛犸，而将属性模型描绘成一只老鼠。GPT-2 训练的计算成本更高，就像猛犸因其体积而难以移动一样。解决方案是使用一个较小的模型(像一只老鼠)，可以控制 GPT-2 的方向。</p><p id="e3a8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">PPLM 使用了两种属性模型:词袋(BoW)和鉴别器。在词袋中，给出一个包含特定主题的所有相关词的文件来训练模型。该模型像分类器一样附加在基本语言模型(GPT-2)之上。通过这样做，GPT-2 将比以前更频繁地产生这些词。作者包括以下 BoW 作为例子开始:法律，军事，怪物，政治，积极的话，宗教，科学，空间，技术。</p><p id="2006" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图显示了在给定相同输入标记“关注的问题”的情况下，基于不同单词包的不同输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/c2538d16535f9ede17d39d0fe220c28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cvoWtkuFTdVKzx70ylAF8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://eng.uber.com/pplm/" rel="noopener ugc nofollow" target="_blank">优步艾博文</a></p></figure><p id="8ad1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于 Discriminator，主题属性可以由包含与该属性相关的不同类的样本的数据集来表示。然后，PPLM 可以根据作为输入提供的类生成相应的文本。作者包括了以下鉴别器例子:积极情绪，消极情绪，点击诱饵，非点击诱饵。</p><p id="e5af" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图再次显示了基于<em class="mu">正</em>和<em class="mu">负</em>鉴别器类的相同输入令牌的不同输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/703d6b41cd41981800afbb03847e57bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qVE7S8ttz-PX0Zu_jMvHbw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://eng.uber.com/pplm/" rel="noopener ugc nofollow" target="_blank">优步 AI 博文</a></p></figure><p id="77f7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们已经了解了 PPLM 是如何工作的，让我们继续实现。由于时间关系，本文将主要关注 PPLM-鲍的实现。</p><h1 id="58df" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">履行</h1><h2 id="2636" class="mw la it bd lb mx my dn lf mz na dp lj ma nb nc ll me nd ne ln mi nf ng lp nh bi translated">第 1 部分:微调 GPT2(正如我在上一篇文章中提到的)</h2><p id="fe8f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我之前的<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7">帖子</a>中，我描述了如何微调 GPT2 以在您的自定义数据集中生成任何文本。这篇博客文章将使用那篇文章中的微调模型，该模型是根据书籍摘要训练的。如果你没有读过那篇文章，你可以先去看看那篇文章，或者用预先训练好的 GPT2 模型。拥有一个微调的模型将允许生成一个更具体的领域(例如，书籍摘要)，而不仅仅是一般的文本。</p><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">使用 Pytorch 微调用于文本生成的 GPT2</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">使用 Pytorch 和 Huggingface 微调用于文本生成的 GPT2。我们在 CMU 图书摘要数据集上进行训练，以生成…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ks nl"/></div></div></a></div><h2 id="076f" class="mw la it bd lb mx my dn lf mz na dp lj ma nb nc ll me nd ne ln mi nf ng lp nh bi translated">第 2 部分:下载回购</h2><p id="c190" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用优步研究所的官方回购，您可以通过以下方式下载他们的回购:</p><p id="da9b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oa ob oc od b">git clone https://github.com/uber-research/PPLM</code></p><p id="76ac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oa ob oc od b">cd PPLM/</code></p><h2 id="54bb" class="mw la it bd lb mx my dn lf mz na dp lj ma nb nc ll me nd ne ln mi nf ng lp nh bi translated">第三部分:构建 PPLM 弓</h2><p id="6fe1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">造一个<em class="mu">。txt </em>文件，包含与您希望生成的特定主题相关的单词。例如，如果我们想要生成浪漫书籍摘要，我们可以构建一个包含浪漫相关单词列表的浪漫 BoW:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><h2 id="4855" class="mw la it bd lb mx my dn lf mz na dp lj ma nb nc ll me nd ne ln mi nf ng lp nh bi translated">第 4 部分:在微调的 GPT2 上运行 PPLM</h2><p id="4be8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们已经构造了 BoW 文本文件，我们可以用它来运行 PPLM:</p><p id="0ebe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oa ob oc od b">CUDA_VISIBLE_DEVICES=$N python run_pplm.py -B /path/to/BoW/romance.txt --pretrained_model=/path/to/model/ --cond_text="The novel" —- num_samples=20 --length=150 --stepsize=0.03 --num_iterations=3 --window_length=5 --gamma=1.5 --gm_scale=0.95 --kl_scale=0.01 --colorama --verbosity='regular' --sample</code></p><p id="af90" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上面的命令将使用<code class="fe oa ob oc od b">/path/to/BoW/romance.txt</code>作为预训练模型<code class="fe oa ob oc od b">/path/to/model/</code>上的弓分类器来运行 PPLM。我们将条件文本(输入文本)设置为“小说”，看看它之后会生成什么。我们生成 20 个长度为 150 的样本。请注意，<code class="fe oa ob oc od b">--stepsize</code>可以用来控制主题的强度，因此增加它将导致生成的文本更频繁地包含在 BoW 中。<code class="fe oa ob oc od b">--colorama</code>将用红色对生成文本中的蝴蝶结进行颜色编码，<code class="fe oa ob oc od b">--sample</code>将确保模型用相同的种子生成不同的样本。您也可以使用预训练的 GPT-2，默认情况下没有指定任何模型将是“GPT 2-中等”。</p><p id="35f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您希望能够保存生成的输出，我已经添加了一个额外的命令行参数<code class="fe oa ob oc od b">--save_path</code> <em class="mu"> </em>，它将允许您保存(这里有<a class="ae ky" href="https://github.com/itsuncheng/PPLM" rel="noopener ugc nofollow" target="_blank">代码</a>)。您只需运行下面的命令，输出将保存到该文件夹中:</p><p id="6139" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oa ob oc od b">CUDA_VISIBLE_DEVICES=$N python run_pplm.py -B /path/to/BoW/romance.txt --pretrained_model=/path/to/model/ --cond_text="The novel" —- num_samples=20 --length=150 --stepsize=0.03 --num_iterations=3 --window_length=5 --gamma=1.5 --gm_scale=0.95 --kl_scale=0.01 --colorama --verbosity='regular' --sample --save_path="/path/to/save/romance.txt"</code></p><h2 id="6dd1" class="mw la it bd lb mx my dn lf mz na dp lj ma nb nc ll me nd ne ln mi nf ng lp nh bi translated">第五部分:瞧！您现在可以看到生成的样本</h2><p id="2dde" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">以下是使用 romance BoW 生成的一些示例，输入条件为文本“The novel…”:</p><blockquote class="og oh oi"><p id="6964" class="lr ls mu lt b lu mn ju lw lx mo jx lz oj mp mc md ok mq mg mh ol mr mk ml mm im bi translated">这部小说以两个年轻人的故事开始，他们是英国一位富有实业家的女儿，和他的妻子，一位富婆美丽女儿的女儿，嫁给了同一个男人。他们结婚是为了永远在一起，他们没有孩子。他们的女儿原来是一个美丽的女人，她的丈夫在爱了她多年后，决定给她一生的爱来结婚…</p><p id="efb4" class="lr ls mu lt b lu mn ju lw lx mo jx lz oj mp mc md ok mq mg mh ol mr mk ml mm im bi translated">这部小说讲述了年轻夫妇丽莎和大卫第二次结婚的故事。他们过着幸福的生活，但当大卫的父亲去世时，他们的婚姻注定要失败。这对夫妇与他们的鳏夫父亲和女婿住在一起，但他们有婚外情。</p><p id="e845" class="lr ls mu lt b lu mn ju lw lx mo jx lz oj mp mc md ok mq mg mh ol mr mk ml mm im bi translated">这部小说是一系列闪回故事，讲述了一个名叫苔莎的年轻孤儿的生活，她的母亲死于一场火灾，她住在自己工作的房子里。房子的主人是一个同名的女人。当苔莎去和她母亲住在附近的房子里几年时，她惊讶地看到一个英俊、迷人的年轻人。她爱上了他，但他不再爱她，她怀了她第一任丈夫的孩子…</p></blockquote><p id="6e47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更多例子可以在我的 Github <a class="ae ky" href="https://github.com/itsuncheng/PPLM/blob/master/generated_romance.txt" rel="noopener ugc nofollow" target="_blank"> repo </a>上查看。</p><h1 id="23df" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="e25f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文通过使用强大的语言模型 GPT-2 实现 PPLM，深入探讨了如何控制文本生成。PPLM 的强大之处在于它允许不同的属性模型组合来产生不同样式的文本。就是这样！希望你们能从这篇文章中学到一些东西，并期待在下一篇文章中见到你们！</p><p id="7aab" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果感兴趣，这里有更多我写的文章😊</p><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">使用 Pytorch 的 BERT 文本分类</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">文本分类是自然语言处理中的一项常见任务。我们应用 BERT，一个流行的变压器模型，对假新闻检测使用…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="om l nw nx ny nu nz ks nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/lstm-text-classification-using-pytorch-2c6c657f8fc0"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">基于 Pytorch 的 LSTM 文本分类</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">一步一步的指导你如何在 Pytorch 中建立一个双向 LSTM！</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="on l nw nx ny nu nz ks nl"/></div></div></a></div><h1 id="47df" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考资料:</h1><p id="0595" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] S. Dathathri，A. Madotto，J. Lan 等。，<a class="ae ky" href="https://arxiv.org/pdf/1912.02164.pdf" rel="noopener ugc nofollow" target="_blank">即插即用语言模型:控制文本生成的简单方法</a> (2020)，2020 年学习表征国际会议</p><p id="8b59" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] R .刘、s .达塔瑟里、a .马多托等。、<a class="ae ky" href="https://eng.uber.com/pplm/" rel="noopener ugc nofollow" target="_blank">用即插即用语言模型控制文本生成</a>、AI 博客</p><p id="a69f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3] S. Dathathri，A. Madotto，J. Lan 等。，<a class="ae ky" href="https://github.com/uber-research/PPLM" rel="noopener ugc nofollow" target="_blank"> PPLM 代码</a>，Github</p><p id="db92" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4] <a class="ae ky" href="https://transformer.huggingface.co/doc/pplm" rel="noopener ugc nofollow" target="_blank">用变形金刚</a>，拥抱脸和优步 AI 写</p></div></div>    
</body>
</html>