# 统计语言模型

> 原文：<https://towardsdatascience.com/statistical-language-models-4e539d57bcaa?source=collection_archive---------23----------------------->

## 从简单到++包含用例、示例和代码片段

![](img/423c9e422852012c8dc4f82f33f7d22a.png)

凯利·西克玛在 [Unsplash](https://unsplash.com/s/photos/language?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

在 NLP 中，语言模型是字母表上字符串的*概率分布*。在形式语言理论中，语言是字母表上的一组字符串。NLP 版本是形式语言理论的一个软变体。

NLP 版本更适合建模自然语言，如英语或法语。没有硬性的规则规定语言中应该包含哪些字符串，不包含哪些字符串。相反，我们有观察工作。人们写作。人们议论纷纷。他们的话语是这种语言的特征。

重要的是，NLP 统计版本有利于*从示例中的字符串学习*语言。考虑学习一个识别产品名称的模型。训练集可能包含 iPhone 4 和 iPhone 5，但不包含 iPhone 12。它应该会将 iPhone 12 识别为产品名称。

在这篇文章中，我们将从简单到详细地介绍统计语言模型。涵盖的模型包括:独立模型、一阶马尔可夫模型、k 阶马尔可夫模型、隐马尔可夫模型、条件马尔可夫模型和条件随机场。每个都用真实的例子和用例来说明。

**下一个令牌概率**

我们将语言定义为字符串的概率分布。在许多用例中，我们真正想要的是给定当前字符串下一个符号的概率。事实证明，这两者是等价的，如[1]中所述。

考虑字母表上的一个字符串。(字母表可以由字符或单词或其他记号组成。)表示这个字符串 *x* 1、 *x* 2、…、 *x* n .我们可以把 *P* ( *x* 1、 *x* 2、…、 *xn* )写成*P*(*x*1)**P*(*x【T33*

**用例**

我们能用语言模型做什么？相当多。

**建议自动完成**。智能手机会在我们打字时自动给出建议。当我们开始输入查询时，网络搜索引擎会提供自动完成的建议。在引擎盖下，这些是由语言模型驱动的。

**辨认笔迹。**想象一下，把你的智能手机指向你手写的笔记，并要求它们被识别。也许是数字化和可搜索的。或者至少让它们清晰可辨！

手写识别具有挑战性。甚至人们经常会弄错。有时甚至是他们自己写的！

考虑尝试识别书写糟糕的文本中的单词。纯粹的图像处理方法会产生很多错误。增加一个语言模型可以减少很多。语言模型提供了一个有用的上下文。

**例子 1** :让我们用一个简单的例子来看一下。想象一下 OCR 认为下一个单词是*数据库*。 *e* 被误认为是 *c* 。他们看起来很相似。

现在让我们添加一个根据英语单词训练的语言模型。特别是将高概率分配给看起来像英语单词的字符串，而将低概率分配给不像英语单词的字符串。 *bath* 会获得大概率，但 *axbs* 不会。

这个语言模型会知道*数据库*比 *databasc* 更有可能。因此添加它将有助于检测和纠正 OCR 的错误。

通过添加多词语言模型，我们可以进一步提高错误检测和纠正的准确性。这个模型的字母表是词汇。它的高概率字符串模拟高概率单词序列。字母表变得越来越大。字母表里有每一个不同的单词。因此，在避免成本(模型变得过于复杂)的同时，需要小心谨慎地获取其好处。稍后将详细介绍。

多词语言模型还可以帮助填充书面文本中缺失的词。

**检测并纠正拼写错误**。我们将重新解释同一个例子*数据库*。最后一个字符， *c* ，现在是拼写错误。

**识别语音。类似的推理在这里也成立。除了模态不同。所表达的基本语言是相同的。这并不是说准确的手写或语音识别很容易。只是添加语言模型会有所帮助。**

**识别多标记命名实体**。多标记命名实体通常具有语言结构。例如，在一个人的名字中，名字通常出现在姓字之前。夹在两者之间的可能是中间名词。作为另一个例子，在美国街道地址中，街道号码通常出现在街道名称之前。正如我们将在后面看到的，这样的实体通常是通过潜在的语言模型来识别的，比如隐马尔可夫模型。

现在我们已经看到了一些用例，让我们深入了解

**型号**

我们将字符串表示为*x* 1， *x* 2，…， *x* n。

**独立。**这是最简单的方法。它假设字符串中的所有字符都是独立生成的。

*P* ( *x* 1、 *x* 2、…、*xn*)=*Q*(*x*1)**Q*(*x*2)*…**Q*(*xn*)。

这里 *Q* 是字母表上的概率分布。

这种方法通常对大型字母表中的长字符串有效。比如有很多单词的文档。文本被看作是单词的序列。字母表是词汇。

在这种情况下，更复杂的方法会很快变得复杂，因此它们需要令人信服地工作，并显著地更好地证明它们增加的复杂性。

这种方法可以有效地检测文档的语言。对于每种语言，我们将训练一个单独的 Q，它是在该语言的训练集中看到的单词的分布。对于表示为单词序列 W1 W2 … Wn 的新文档 W，我们将为我们已经建模的每种语言 L 计算 PL(W)= QL(W1)* QL[W2]*…QL[Wn]。我们认为 L* = argmax_L PL(W)是 W 的语言。

**举例 I1:** 按语序训练*狗在摇猫尾巴*。我们得到 Q(the)=2/7，Q(is) = 1/7，等等。

**Python 代码 I1** :没有测试，甚至没有运行。另外，需要一个传令兵。

```
from collections import Counter
import mathclass IndependentModel:

    **def __init__(self):**
        self.ctr = Counter() **def train(self,words):**
        for word in words:
            self.ctr[word] += 1 **def Q(self,word):**
        return float(self.ctr[word])/self.ctr.values() **def P(self, words):**
        return math.prod(map(lambda word: self.Q(word), words))
```

**练习 I1** :将这段代码片段发展成一个语言识别器。说英语 vs 法语。

**一阶马尔可夫模型。**这里，一个符号允许依赖于前一个符号。

*P* ( *x* 1、 *x* 2、…、*xn*)=*Q*(*x*1)**Q*(*x*2 | x1)*…**Q*(*xn | xn-1*)

一阶马尔可夫模型由状态转移矩阵 *P* 描述。 *pij* 是从状态 *i* 到状态 *j* 的概率。表示 *xt* 为 *i* ， *xt* +1 为 *j* ， *Q* ( *xt* +1， *xt* )等于 *pij* 。

这个马尔可夫模型也可以表示为有向图。如果值 *i* 后面可以跟一个值 *j* ，则有一个弧 *i* → *j* 。这条弧线上有一个概率*p*(*j*|*I*)。图形表示仅仅明确了哪些状态转移概率为 0。

**例 1M1:** 一阶马尔可夫模型对短短语建模有效。假设我们想从大量的英语文档中发现突出的短语(每个短语包含 2 到 4 个单词)。一阶马尔可夫模型在足够富裕以至于能够做合理的工作，而不会变得太复杂之间取得了良好的平衡。

*考虑文本数据挖掘是一个短语。数据挖掘是一种从大型数据集中提取有意义信息的方法*。

从这个文本，我们将建立马尔可夫图。该图的节点是出现在文本中的不同单词。如果在文本中单词 *u* 后面至少有一次单词 *v* ，则从节点 *u* 到节点 *v* 有一条弧线。下面显示了一些弧线。

```
data → mining   mining → is    method → of  extracting → meaningful
```

我们看到 *P* ( *数据*，*挖掘* ) = *Q* ( *数据* )* *P* ( *挖掘* | *数据* ) = (2/19)*1。相比之下， *P* ( *a* ，*法*)=*q*(*a*)**p*(*法*|*a*)=(3/19)*(⅓)= 1/19<*p*(*数据*，*挖掘*可惜， *P* ( *是*，a) = *P* ( *数据*，*挖掘*)。所以(*是*，*是*)会是一个误报，因为它不是一个显著短语。

因此，一阶马尔可夫模型本身虽然有些用处，但也有明显的假阳性。向这种方法添加语法信息，特别是单词的词性标签，可以显著提高其质量。也就是说，(大部分)减少了假阳性而(几乎)没有遗漏真阳性。直觉告诉我们，突出的短语是由有利于某些词类的词组成的，尤其是名词。考虑(*数据*，*挖掘*)。这两个词都是名词。考虑一下(*是*，*一个*)。‘is’是动词，*是冠词。*

我们不会详细讨论如何将这两种方法结合起来。主要是马尔可夫模型还是有用的。

**Python 代码 1M1** :未测试，甚至未运行。另外，需要一个传令兵。

```
from collections import defaultdict, Counter
import mathclass FirstOrderMarkovModel: **def __init__(selfs):**
       self.ctr2 = defaultdict(lambda:defaultdict(int))
       self.ctr1 = Counter() **def train(self,words):**
       words_with_sentinel = [‘_B_’] + words
       for i in range(len(words_with_sentinel)-1):
          self.ctr2[words[i][words[i+1] += 1
          self.ctr1[words[i]] += 1 **def Q(self,word,previous_word):**
       return float(self.ctr2[previous_word[word])/
       self.ctr1[previous_word]) **def P(words):**
       p = self.Q(words[0],’_B’)
       for i in range(len(words)-1):
          p *= self.Q(words[i+1],words[i])
       return p
```

**k 阶马尔可夫模型**。在这个模型中，一个字符被允许依赖于前面的 *K* 个字符。

*P* ( *x* 1、 *x* 2、…、*xn*)=*Q*(*x*1)**Q*(*x*2 | x1)*…**Q*(*xn | xn-1、xn-1*

通过将 *K* 设置为 2 或 3，这种方法可以发现比一阶马尔可夫模型更好的显著短语，而不会导致复杂性的巨大增加。我们说的“更好”是什么意思？考虑一个长度为 3 的短语。称之为 *x* 1、 *x* 2、 *x* 3。一阶马尔可夫模型丢失了信息，因为它假设 *x3* 独立于给定 *x2* 的 *x1* 。二阶模型没有。*P*(*x*1)**P*(*x*2 |*x*1)**P*(*x*3 |*x*2， *x* 1)等于 *P* ( *x* 1， *x*

和以前一样，仍然需要把统计方法和语法方法结合起来。

**示例 KM1:** 我们想对三个单词的产品名称建立一个二阶马尔可夫模型。为什么是二阶？考虑这些编造的例子: *4k 电视三星*， *3k 电视索尼*，…想象一下索尼不提供 4k 选项。如果我们使用一阶马尔可夫模型，我们将失去第一个单词(4k 或 3k 或…)和品牌名称之间的相互作用。二阶模型将捕捉这种相互作用。换句话说，产品名称不仅会受到第二个词(在我们的例子中是 tv)的影响，还会受到第一个词(4k 或 3k 或……)的影响。

**隐马尔可夫模型**

这里，字符串是从具有潜在变量(即隐藏状态)的模型中概率性地生成的。

用符号表示，让 *X* = *x* 1、…、 *xn* 表示一串长度为 *n* 和 *S* = *s1* 、…、 *sn* 表示一串与 *X* 相关的状态。符号 *xi* 由状态 *si* 产生。

*X* 称为观测序列；产生 *X* 的(隐藏)状态序列。

在结构上，HMM 是状态集上的一阶马尔可夫模型。一些州通过弧线与其他州相连。此外，每个状态在可观察的字母表上都有一个概率分布。

该模型生成一对( *X* ， *S* )如下。首先，它生成 s1。然后它以一定概率从 *s* 1 发出 *x* 1。接下来，它以一定的概率移动到状态 *s* 2。这个概率只取决于它来自哪里，即 *s* 1。发射的是什么，即 *x* 1，是无关紧要的。接下来，它以一定的概率从 *s* 2 生成 *x* 2。事情就是这样。

总之，HMM 生成器在生成状态和从这些状态生成可观测量之间交替。状态转移概率只取决于前一个状态。发射跃迁概率只取决于当前状态，即可观测物发射的状态。

例子 HMM1 :假设我们想要生成与真实句子相似的英语句子。特别考虑具有两种结构之一的句子

```
Article Noun Verb Adverb
Pronoun Verb
```

第一个结构的例句是*男孩跑得快*。第二个结构的例句是*她唱*。

一个真实的句子生成器可以容纳更多的结构。我们选择两个，因为最大的洞察力来自于从一个到两个。

模拟这两种结构的 HMM 看起来像什么？首先，它有助于添加一个*开始*状态和一个*结束*状态。HMM 总是从*开始*状态开始，并在*结束*状态停止。*开始*和*结束*状态称为静音。他们不发射任何令牌。

好了，现在来看结构，也就是连接各州的弧线。

```
begin → article → noun → verb → adverb → end
begin → pronoun → verb → end
```

请注意，有些状态会显示两次。这仅仅是由于呈现的限制。举个例子。由于 HMM 具有弧线 *begin* → *冠词*和 *begin* → *代词*，这实际上意味着 HMM 可以从 *begin* 状态以某个概率移动到*冠词*状态，并以另一个概率移动到*代词*状态。两个概率之和为 1。从状态*开始*HMM 必须移动到某个地方。

类似地，从状态*动词*我们可以移动到状态*advor*b*或者移动到状态 *end* 。注意，HMM 没有跟踪我们如何到达状态*动词*，这意味着状态序列*开始*文章*名词 动词 结束也是可能的，尽管我们没有要求这样做。如果我们希望 HMM 能够生成诸如男孩吃了之类的句子，这种类型的概括是好的，否则就是坏的。

**训练**:我们可以很容易的训练出这个车型的排放参数。我们可以利用这样一个事实，即我们的状态是命名的实体，它们的训练集很容易获得。很容易收集构成冠词的词，构成名词的词等等。

接下来，我们研究训练状态转移概率。例如，我们需要估计从*开始*状态到*结束*状态的概率。这是小于 1 的 as 从状态*开始*我们也可以用状态*代词*来代替。

为了训练转换，我们假设我们可以访问单词附有词性标签的句子。这种类型的丰富训练集很容易组合。很容易得到大量的句子。通过在每个句子上运行合适的词性标注器，也很容易得到这些句子的词性标注序列。

HMM 的转移概率很容易从这样的训练集中训练出来。事实上，我们只需要每个句子的词性标签序列。让我们来说明这一点。弧线 *begin* → *article* 上的概率估计为第一个标记为 *article* 的词性标签序列数除以第一个标记为 *article* 或*代词*的词性标签序列数。

**两个生成的句子**

来看看几个。第一个在下面。

```
 The     boy     is     fast
           ^       ^      ^       ^
           |       |      |       |
begin → article → noun → verb → adverb → end
```

我们从状态*开始*开始，走到*条*，从它发出*条*，走到*名词*，从它发出*男孩*，等等。t

第二个在下面。

```
 She     sings
          ^        ^
          |        |
begin → pronoun → verb → end
```

我们从*开始*，走到*代词*，从中发出*她*，移到*动词*，从中发出*唱*，最后走到*结束*并停留在那里。

**与独立**:相比之下，想象一下独立模型生成的句子。单词会根据它们的概率被吐出来，而不考虑想要的结构。

**特定位置独立模型 aka 链 HMM**

在某些用例中，生成的符号会受到它们在字符串中的位置的显著影响。举个例子，考虑产品名称，比如 *iPhone 11* 、*佳能相机*和*索尼电视*。显然有一些连续的结构。在这些例子中，品牌名称(*佳能*、*索尼*)先于产品类型(*相机*、*电视*)。

独立模型的位置特定的一般化适合于这种建模。

*P* ( *x* 1、 *x* 2、…、*xn*)=*Q1*(*x*1)**Q2*(*x*2)*…**Qn*(*xn*

这里 *Qi* 是字母表上特定于位置的概率分布。所以对于 *n* 个位置，我们有 *n* 个分布 *Q* 1，…， *Qn* 。

特定于位置的独立模型可以被视为 HMM，其状态 *1* 、 *2* 、…、 *n* 分别表示令牌位置 1 至 *n* 。这样一个 HMM 的图是一个单向路径 1 → 2 → 3 → … → *n* 。因此，所有弧上的转移概率都是 1。HMM 的参数是特定位置的发射概率。我们将称这样的模型为*链嗯*。

链式 HMM 几乎不是 HMM，因为它没有任何马尔可夫性。也就是说，将它作为 HMM 调用是有用的。它根据需要提供增强模型的途径。

这种增强版本的一个例子通常用于生物分子序列分析。它的名字叫做*简介嗯*【3】。简档 HMM 是一个链式 HMM，其中添加了一些明智选择的状态和转换，以在某些位置偏离链。迂回路径在某些其他位置合并回链。迂回路径模拟位置特异性突变。在生物分子序列中，这种突变经常发生。捕获它们的模型更准确地识别模型家族中生物分子序列的成员。

与位置特定链 HMM 不同的增强路径是进入所谓的条件马尔可夫模型(CMM)。我们将在本帖的后面讨论 CMM。在下面的例子中，我们还将说明从链式 HMM 迁移到 CMM 的好处。

**CHMM 公园名称示例**:假设我们想要模仿美国的国家公园或州立公园名称。比如黄石国家公园、约塞米蒂国家公园、城堡石国家公园……连锁嗯是个不错的选择。

这个 HMM 将从国家公园和州立公园的名称列表中训练出来。我们将设置 *n* 为列表中一个条目的最大字数。链式 HMM 的特定位置发射概率易于估计。公园名称的标记化版本揭示了其中每个单词的位置。比如*约塞米蒂国家公园*，*约塞米蒂*是第一个字，*国家*第二，*公园*第三。因此，从公园名称中的一个标记，我们可以知道要训练哪个州的排放概率。

上面的模型因其简单性和灵活性而吸引人。只要在一系列公园名称上训练它。随着列表变得更加丰富，模型会自行改进。

这样说，似乎有点不自然。我们心目中公园名称的自然模式是

```
word+ ( state | national) park
```

这只是意味着一个公园名称有一两个单词后跟*国家*或*州*后跟*公园*。

下面的模型更接近我们寻求的自然模型。

**例子 HMM-Park-Names** :这里我们会用到三种状态:*前缀 _ 字*、*地区 _ 字*和*公园*。state *regional_word* 会发出 *state* 和 *national* (暂时)，概率相等(暂时)。state *prefix_word* 会发出一个出现在 regional_word 之前的公园名称中的单词。状态 *park* 将以概率 1(目前)发出单词 *park* 。

状态序列的训练集很容易构造吗？我们只需要一些:

```
prefix_word regional_word park
prefix_word prefix_word regional_word park
…
prefix_word prefix_word prefix_word regional_word park
```

那为什么不使用正则表达式呢？HMM 更准确。考虑一下某些文本中的短语“*国家公园*”*。这个短语不是一个真正的国家公园。HMM 原则上可以通过分配从状态*前缀 _ 字*发出*的非常低的概率来对此建模。**

*其实这就把我们带到了如何训练*前缀 _ 单词*的发射概率这个话题上了？(目前，其他州的排放概率已经确定。)这里有一个简单的方法。列出国家公园和州公园的名称。去掉每个公园名称的最后两个单词。剩下的单词是前缀词。*

*国家*公园*可以扩展到放射*海滩*、*森林*、*纪念碑、*等。(在加州，州立海滩通常归入州立公园。)*

*为了更好地理解链式 HMM 和基于实体的 HMM 之间的权衡，让我们看另一个涉及两者的例子。*

***CHMM 产品示例:**考虑一个链式 HMM 来对产品名称进行建模。(我们所说的“产品名称”是指特定的产品和产品类别。)我们可以使用一个经过训练的版本来识别非结构化文本中的产品名称，有些甚至还没有经过训练。*

*这个 HMM 将从产品名称列表中训练出来。培训类似于公园连锁嗯。记住，我们只需要学习(I)状态的数量和(ii)不同状态的排放概率。*

*这种 HMM 结构简单，易于训练。也就是说，我们下面描述的基于实体的版本可能会更准确。*

***示例 HMM-产品**:*

*我们可以选择*品牌令牌*、*产品类型令牌*、*产品版本令牌*和*产品基本名称令牌*作为我们的实体。示例值有 *brand_token* = *佳能*，*product _ type _ token*=*相机*，*product _ version _ token*= 12，*product _ base _ name _ token*=*iphone*。我们的 HMM 的状态就是这些实体。*

*为什么每个实体的名字都以单词*结尾 token* ？因为实体适用于单个令牌。因此，与“*智能手机*”*相关联的状态序列将是[ *产品类型令牌*，*产品类型令牌* ]。**

**为了训练这个 HMM，我们需要各种实体的训练集。这些训练集可以来自品牌、产品类型等的列表。我们说“派生”是因为如果列表包含多单词条目，我们就不能照原样使用它们。考虑将*智能手机*列为产品类型。由此我们会衍生出两个例子:*智能* → *产品类型令牌*和*手机* → *产品类型令牌*。**

**我们还需要状态序列来捕获标记化产品名称中的实体序列。我们可以手动构建这样的训练集。这并不难，因为产品名称往往遵循一些常见的约定。例如，在许多双字产品名称中，第一个名称是品牌，第二个是产品类型。我们已经看到一个例子:*佳能相机*。因此[*品牌 _ 令牌 T23，T24 产品 _ 类型 _ 令牌 T25 应该在状态序列的训练集中。***

****从产品名称自动导出状态序列****

**手动构造状态序列会让我们走得很远。几个州序列涵盖了很多产品名称。也就是说，手动方法存在无法扩展到涵盖数百万种产品的强大工业强度模型的风险。产品名称可能跨越州序列的长尾。考虑状态序列[ *产品类型令牌*，*品牌令牌* ]。一些产品名称遵循这一惯例。**

**事实证明，我们可以从标记化的产品名称中自动构造状态序列。这样，当我们向训练集添加越来越多的产品时，状态序列会自动从中提取出来。**

****基本版本**:基本思想是获取标记化的产品名称，并为其中的每个单词找到最可能的实体。我们可以这样做，因为我们有各种实体的(单词，实体)对的训练集。**

**让我们看一个简单的例子。**

```
**canon camera ⇒ [canon,camera] ⇒ [brand_token,product_type_token]**
```

****精炼版**:我们可以把这个基本思路精炼如下。和以前一样，我们对产品名称进行标记。然后，我们通过 HMM 运行令牌序列，以找到最适合它的状态序列。**

**这种改进使用了来自实体训练集和 HMM 状态转换的信息。因此，它可以更准确。**

**我喜欢把这种方法称为“引导训练”。我们手动初始化 HMM。这包括播种我们选择的任何状态序列。然后我们可以通过它运行标记化的产品名称，希望发现更多的状态序列。**

**为了能够发现新的状态序列，在初始化 HMM 时，我们应该允许从所有状态到所有状态的转换。(除了从状态*开始*到状态*结束*以及从状态*结束*的任何外出。)我们可以自动初始化这些转换的训练集，因此这些转换的初始概率非常低，尽管不是零。我们称之为伪训练。**

**我们通过“引导训练”发现的一些新的状态序列可能存在错误。所以这些应该由人类来审核。尽管如此，尽管有人在循环中，我们可能还是受益了。手动发现新的状态序列比管理自动发现的状态序列要耗时得多。**

****条件马尔可夫模型****

**就像 HMM 一样，条件马尔可夫模型(CMM)对(*令牌序列*、*状态序列*)对进行操作。与 HMM 不同，CMM 被优化用于为给定的令牌序列寻找最佳状态序列。这到底是什么意思？这将在下面的例子中逐渐变得清晰。**

**也就是说，在本文中，我们不会讨论如何为给定的令牌序列找到最佳状态序列。我们将讨论如何对给定的令牌序列的特定状态序列进行评分。这种评分将暴露 CMM 中的“有条件的”部分。**

****示例 CMM1(第一部分)**:设想在(*记号化的完整人名*，*记号实体*)对上训练一个语言模型，目的是用它来解析人名。解析意味着把一个人的名字分解成各个组成部分。如名字和姓氏。**

**想想约翰·K·史密斯这个名字。模型应该可以推断出 *K* 是中间名词。即使训练集不包含从 *middle_name_word* 发出的 *K* 。**

**CMM 直接将其建模为**

```
**P(state[2] = middle_name_word | state[1] = first_name_word, token[2] = K)P(state[2] = middle_name_word | state[1] = first_name_word)*P(token[2] = K | state[2] = middle_name_word)**
```

**与 HMM 的建模方式相比，这有什么好处。**

**为了回答这个问题，首先让我们抽象出这个例子的细节。我们有 CMM 的*P*(*S*[*I*+1】|*S*[*I*]， *X* [ *i* +1])对*P*(*S*[*I*+1】|*S*]这里 *X* [ *i* ]是第 *i* 个令牌，而 *S* [ *i* 是它被发出的状态。**

**CMM 方法可以被视为多项式分类器，其输入是一对( *S* [ *i* ， *X* [ *i* +1])，其输出是各种值 *S* [ *i* +1】的概率分布。这个公式有助于从输入中提取我们认为合适的任何特征。可能重叠。可能考虑到 *S* [ *i* ]和 *X* [ *i* +1]之间的相互作用。**

**我们现在可以在这个问题上使用任何多项式分类器算法。例如(多项式)逻辑回归、决策树或随机森林。**

**相比之下，HMM 不能捕捉到 *S* [ *i* ]和 *X* [ *i* +1]之间的交互。它也不能适应任意的特征。它也不能利用复杂的机器学习分类算法。另一方面，CMM 不能利用实体训练集。我们不再学习排放概率。**

****例子 CMM 1(已完成)**:让我们完成这个例子。从特征开始。从输入( *S* [ *i* ]， *X* [ *i* +1])我们将提取特征*状态* = *S* [ *i* ，*令牌* = *X* [i+1】，*令牌长度* =在*中的字符数也就是我们有三个预测器:*状态*、*令牌*和*令牌长度*。每个都是绝对的。响应是 *S* [ *i* +1]的值。也是绝对的。***

*为什么有这些功能？特性*状态*让我们模拟当前状态对下一个状态的影响。特性*令牌*让我们使用令牌的实际值作为其状态的预测器。这是有用的。某些令牌有利于某些实体。例如，*约翰*更有可能是名字而不是姓字。作为额外的奖励，我们可以免费获得*状态*和*令牌*之间的交互，只要我们的学习算法能够利用它。(如果有互动那就是。)特性*令牌长度*很有用，因为我们知道令牌的长度有利于某些实体。名字或中间名通常只有一个字符。几乎没有姓。*

*我们来看几个训练的例子。我们将从(*令牌序列*、*状态序列*)对开始。*

```
*John   Smith                        John    K    Smith
first  last                         first middle last*
```

*从这些出发，让我们为 CMM 公式推导一些训练实例。(我们不全部展示。)前三列列出了预测值。最后一个列出了响应。*

```
***(previous)** **state**    **token**   **token's length**             **state**
       first          K             1                  middle
       begin         John           4                  first
       first         Smith          5                  last
         …             …            …                  …*
```

*上面的第一个训练实例读作“(*前一个状态*等于*第一个*，令牌等于 *K* ，令牌长度等于 1)”导致下一个状态是*中间的*”。*

***示例 CMM 2(草图)**:考虑解析美国街道地址。为简单起见，假设它们具有以下形式*

```
*street_num_word (street_name_word)+ street_suffix [unit_prefix unit_num_word]*
```

*下面是几对标记化的美国街道地址及其实体。实体名称是缩写的，因此示例可以放在分配的空间中。*

```
*123             Marine           Dr
street_num_word street_name_word street_suffix203             Lake             Forest           Ave
street_num_word street_name_word street_name_word street_suffix123             Main             St            Apt         22
street_num_word street_name_word street_suffix unit_prefix unit_num*
```

*对于我们特性的设计，让我们从关注从令牌中提取的特性开始。令牌的哪些特征区分不同的实体？token 的实际值可以预测是 *street_name_word* 还是 *street_suffix* 还是 *unit_prefix* 。数字的存在预示着它是一个 *street_num_word* 或者是一个 *unit_num_word* 。*

*鉴于此，我们将从 input ( *S* [ *i* ]， *X* [ *i* +1])中提取特征*state*=*S*[*I*]，*token*=*X*【I+1】，*proportion _ of _ digits _ in _ token【中**

***条件随机字段***

*就像 CMM 一样，CRF 对( *S* ，X)对进行操作，其中 *X* 表示令牌序列，S 表示其状态序列。两个模型*P*(*S*|*X*)，为给定 *X* 寻找高概率状态序列 *S* 的用例进行优化。*

*CMM 认为这是因为*

**P*(*S*|*X*)= product _*I**P*(*S*[*I*)| X【I-1】，*S*【I-1】)**(CMM 1)***

*即状态 *S* [ *i* ]的概率是以前一状态 *S* [ *i* -1】和当前令牌 *X* [ *i* ]为条件的。*

*通用报告格式限制较少。也就是说，它容纳了更一般的 P(S|X)。*

*CRF 在无向图上操作，其节点模拟状态，其边模拟状态对之间的直接影响。这些影响是对称的。*

*在这篇文章中，我们将局限于线性链 CRF。它有这样的结构*

**S*【1】—*S*【2】—*S*【3】—*S*【n】*

*这只是模拟了一个状态受到前一个状态和下一个状态的影响。即 S[i]直接受 S[i-1]和 S[i+1]的影响。*

*在通用报告格式领域，线性通用报告格式是最接近 CMM 的模拟。所以让我们更详细地研究一下。*

*线性通用报告格式模型 *P* ( *S* | *X* )为*

**P* ( *S* |X)正比于 product _*I*product _ f*p*(*f*)*e^*f*(*s*[*I*-1】， *S* [ *i* ， *X* )*

***如果我们通过记录日志来定义一个分数 *C* ( *S* ， *X* )，这种直觉会更容易传达。***

****C* ( *S* ，*X*)= sum _ I sum _ f w(f)* f(S[I-1]，S[i]，X) **(CRF 2)*****

***我们称之为 *C* 是因为我们认为它是一个兼容性函数。作为 S 的函数，C(S，X)在 P(S|X)中是单调的。因此，为了找到得分高的状态序列，C(S，X)同样适用。***

***什么是 *f* ？这是一个特征函数，对三元组(S[i-1]，S[i]和 X)在某个维度上的兼容性进行评分。正值越多，兼容性越好。 *w* ( *f* )是 *f* 的重量。它控制兼容性的强度和方向。***

***我们可以选择任意多的特征函数。***

***在线性条件随机场里，我们有一组可能重叠的特征函数。每个应用于每个边缘。也就是说，他们可以使用来自 X 中标记的任何子集的任何信息。相比之下，cmm 只对( *S* [ *i* -1】， *S* [ *i* ]， *X* [ *i* )三元组的兼容性进行评分。***

***在这一点上，最好看一个具体的例子，具体的特性函数。***

***示例 CRF 1 :考虑我们之前看到的一个示例的一般化版本。解析全局街道地址。概括地说，我们正在走向全球。***

***全球的街道地址有许多不同的格式。在某些情况下，街道编号在街道名称之前，在某些情况下在街道名称之后。有些以单位开始，有些以单位结束。(单元的一个例子是“Apt 30”。)这些只是其中的几个变种。***

***全球街道地址的质量也各不相同，取决于它们是否符合当地的格式。我们希望能够尽可能好地解析低质量的地址。***

***让我们看两个例子***

```
***123 St Francis St
Rue du Vivier 15***
```

***第一个是美国的，第二个是比利时的。除了街道编号和名称的位置之外，街道关键字的位置也各不相同。(在这些例子中，“St”和“Rue”是街道关键字。)***

***我们将使用线性链 CRF。***

***我们应该从标记化的街道地址中提取什么全局特征吗？想到两个:*国家*和*语言。****

***语言和国家都会影响地址格式。这些因素相互重叠，但也有互补的影响。比利时和法国的地址就是这种情况，都是用法语写的。在某些方面，他们是相似的。在其他方面，他们不是。这是国家的问题。***

***有鉴于此，我们应该两者兼而有之。***

***我们将围绕这些构建两个特定的特性函数。***

```
*****F1**: country-edge compatibilities: (S[i-1],S[i], country(X))
**F2**: language-edge compatibilities: (S[i-1],S[i], country(X))***
```

***我们将添加第三个特征函数来优先选择与其标记状态兼容的令牌。***

```
*****F3**: state-token compatibilities: (S[i], X[i])***
```

*****训练** : F1 和 F2 可以从标记有国家和语言的(*符号化街道地址*、*州序列*)对中训练。从 *S* [ *i* -1】过渡到 *S* [ *i* 兼容国家和语言分数高的，不兼容的不要。可以从训练集中的(*状态*，*令牌*)对中训练 F3。***

*****延伸阅读*****

1.  ***[https://www . cl . cam . AC . uk/teaching/1718/R228/lections/le C9 . pdf](https://www.cl.cam.ac.uk/teaching/1718/R228/lectures/lec9.pdf)***
2.  ***[最大熵马尔可夫模型](https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model)***
3.  ***[https://www . ebi . AC . uk/training-beta/online/courses/pfam-creating-protein-families/what-are-Profile-hidden-Markov-models-hmms/#:~:text = Profile % 20 hmms % 20 are % 20 probability % 20 models，the % 20 alignment % 2C % 20 see % 20 figure % 202](https://www.ebi.ac.uk/training-beta/online/courses/pfam-creating-protein-families/what-are-profile-hidden-markov-models-hmms/#:~:text=Profile%20HMMs%20are%20probabilistic%20models,the%20alignment%2C%20see%20Figure%202)。***
4.  ***[cs 838–1 高级 NLP:条件随机字段](http://pages.cs.wisc.edu/~jerryzhu/cs838/CRF.pdf)***
5.  ***[条件随机场:用于分割和标记序列数据的概率模型](https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&httpsredir=1&article=1162&context=cis_papers)***