# 如何建立对人工智能解决方案的信任

> 原文：<https://towardsdatascience.com/how-to-build-trust-in-artificial-intelligence-solutions-83ca20c39f0?source=collection_archive---------48----------------------->

![](img/79a6084c9766f41f3598bfd28b31e0c8.png)

来源: [Adobe 股票](https://stock.adobe.com/de/)

## 心理学家对人工智能中建立信任的观点，以及公司需要了解哪些机制来满足其客户和用户的需求。

> 我采访了[玛丽莎·斯乔普](https://www.linkedin.com/in/marisa-tschopp-0233a026/)，她是一名组织心理学家，从人文角度进行人工智能研究，重点关注心理和伦理问题。她还是位于苏黎士的科技和网络安全公司 [scip AG](https://www.scip.ch/en/?labs.20190411) 的企业研究员。她是驻瑞士大使。

![](img/e29fdd34f979522073115c35279efd70.png)

**请用两到三句话描述你是谁。**

目前，我专注于对人工智能、自主武器系统和我们的 AIQ 项目的信任，这是一种测量数字助理(对话式人工智能)技能的心理测量方法，如 Siri 或 Alexa。

所以，很明显，我是一名研究人员，但我也是两个蹒跚学步的孩子的母亲，一个妻子，一个女儿，一个姐姐，一个排球运动员，希望是一个有趣的朋友，一个活动家，一个理想主义者，一个合作者，一个半职业的夏尔巴人(我喜欢在瑞士阿尔卑斯山徒步旅行，因此必须背着我的孩子！).

让我们从更好地理解信任开始。信任是什么，为什么它很重要，尤其是在人工智能的背景下？

在人工智能的背景下，有一个关键的潜在假设:“没有信任，就没有使用”。由于人工智能有很大的前景(也有危险)，科技公司和人工智能爱好者特别关注如何建立对人工智能的信任，以促进采用或使用。

> 信任似乎是一种持久的、神秘的竞争优势。

没有信任，就没有家庭、房屋、市场、宗教、政治和火箭科学。

根据信托研究员雷切尔·博茨曼的说法，

> 信任是一种社会粘合剂，使人类能够通过相互之间以及与包括技术在内的环境的互动而进步。

信任可以被视为一种应对不确定性的心理机制，位于已知和未知之间。

![](img/1e38baa079b9def8e4ec845f7994df6d.png)

图片:[瑞秋·博茨曼](https://medium.com/@rachelbotsman/trust-thinkers-72ec78ec3b59)

信任在我们的个性中根深蒂固。我们基本上生来就有信任或不信任人(或动物或任何其他事物)的倾向。

以一个女人的随机照片为例:你信任她吗？请在下面评分。

![](img/1ca8bfba786b7a3e543beacb419ec55e.png)

来源:[聂难多斯](http://tonsoffacts.com/25-interesting-and-bizarre-facts-about-nannie-doss/)

![](img/83a1f3151884dbd665087409121ceb14.png)

我们人类有独特的能力在一瞬间判断出我们是否信任这个人。我们看面部表情、身体姿势或环境(背景、周围环境等)。).我们在分秒之间将其与记忆或过去的经历进行比较，比如“她让我想起了我的祖母。”

> 总的来说，我们知道的是，我们更倾向于相信那些和我们更相似的人。一个原因是，我们更容易预测相似人的未来行为或反应，这降低了我们受到伤害的情感风险。

我们不知道的是，我们的直觉有多准确。你相信上面这个女人吗？也许是的，因为她在微笑，放松。也许不是，因为你已经在这里期待某种诡计，因为我是一个心理学家。

这个女人不太值得信任。**她几年前死于狱中，是最著名的女性连环杀手之一。**

**在人工智能的背景下，如果你问我们能否信任人工智能作为一种技术的问题，**那么与其他技术相比，理解人工智能(例如机器学习，比如图像分类)经常不完全按照预期的方式行事，犯错误，或者表现得不道德是决定性的。比如当黑人被归类为大猩猩或者鸟类被归类为导弹的时候。

## 过程和结果很难解释，有时根本不知道，因此，不太好预测。信任这项技术会带来更高的风险。

到目前为止，研究已经就建立信任需要回答的三个主要支柱达成一致，

**1。)性能:**表现好吗？安全吗？建的对吗？

**2。)流程:**它是否按照我们预期的方式执行？我们能预测结果吗？

**3。)目的:**我对程序和提供者的意图有好感吗？它是否符合道德标准？值得信赖吗？

人们常说，人工智能积极地改变了从医学到城市规划的几乎每个领域，但非常重要的是，它也带来了令人质疑甚至危险的影响。从数据平台的超精确黑客攻击到监控状态和没有机会获得公众同意的隐私损失。因此，除了缺乏可预测性和可解释性等技术问题之外，负面结果的概念、炒作、复杂性以及定义和应用中的分歧都会导致怀疑和不信任。

**非专家和企业主等应该如何。走近这个话题？**

人工智能已经成为我们日常生活的一部分，它已经越来越多地用于教育、警察、司法、招聘或卫生等决策。

> 我也没有技术背景，我是一名心理学家，所以我从不同的角度看问题，对我来说，可能更容易与大多数人产生共鸣，他们不知道如何编码或什么是算法。

最让我着迷并推动我研究的是一个问题，信任首先是如何建立的。你并不真正了解这个人或这个产品，它的价值或能力。第一次瞥见说“好的，我要去”。

这种信任最初是如何发展的，仍然有点神秘。

我们怎样才能最好地应对它？我认为这一切都与教育、沟通和批判性思维有关。但有一些东西限制了这些技能或我们参与人工智能讨论的意愿。

> 从心理学角度来看，这是一个大问题:我们缺乏选择的认知自由。我担心的是，我们正走向与人工智能的生死攸关的关系。摆脱人工智能几乎是不可能的，就像我们无法摆脱气候变化一样。

事实上，我们被迫或受到威胁，就像威胁性的终结者图像或不断输给机器新闻的人，会导致抵抗、否认、愤世嫉俗和淡化。这叫逆反，一种心理现象。当反抗发生时——我们选择这些行为——即使它们是完全不理性的——**简单地恢复我们选择的认知自由，夺回我们的控制感。**

**这可能是一个很大的挑战，尤其是在消费者心理方面**当你的目标是说服客户购买你的产品时，无论是汽车还是机器人吸尘器。

## 像所有人一样，消费者想要选择的自由，我们需要找到方法，让人们想要自己探索人工智能，而不是因为他们被迫这样做。

这就是为什么管理层经常在公司内部采用自下而上的方法，而不是自上而下的决策。

通过这种参与式的决策方式，你的目标是让所有人都参与进来，分享你的愿景和目标。

目前，关键问题之一是改变我们谈论人工智能的方式。我认为我们必须大幅改变关于人工智能的对话基调。我们必须远离炒作、威胁和恐惧，走向清晰的事实、愿景和原因，以创造我们自己与人工智能的关系，从而达到新的信任水平。

这也是我作为人工智能中的女性网络大使的愿景，这是一个致力于性别包容的人工智能，造福全球社会的非营利组织。

假设一家公司正在开发一款基于机器学习的产品，并且刚刚开始原型制作。从建立信任的角度来看，你有什么建议？

我从一个哲学家那里学到的是总是问为什么，从开始到结束，在项目的所有里程碑持续不断。

**预期的后果是什么，并推测所有可能的意外后果？**

从设计的角度来看，这一切都是为了让你的设计至少符合最低的道德标准，以确保你正在建造一个值得信赖的产品。但是，请记住，技术性能(质量)、安全性和安全性都是不可或缺的先决条件。

回到最初，在头脑中不断记住三大支柱**流程、绩效、目的**。人工智能中的伦理是关于完整性和真实性的。

最终，任务是建造一个伟大的、安全的、道德上正确的产品。焦点自然是首先建立一个好的产品，然后才是安全和道德的东西。

首先关注技术需求是很自然的，然而与直觉相反的是，应该首先关注后者。两年前，当我们开始我们的信任研究时，我们的想法是有一个质量证明，向用户或客户表明这是一个值得信赖的产品。这就是为什么我们发明了 AIQ，一种心理测量方法来陈述、比较和跟踪数字助理的技能。然而，我们有点太快了，因为市场仍处于发展阶段，而不是真正改善现有的对话式人工智能。我们一开始也关注技术技能，而不是如何建立和发展信任的实际决定性软因素。

![](img/59ae412fcde244a3e2b73566636a24d6.png)

来源:[瑞士认知](https://swisscognitive.ch/2019/12/18/developing-an-artificial-intelligence-quotient-a-iq-for-conversational-ai/)

这里有一集播客更详细地讨论了这个话题。

现在，我们后退一步，专注于在人工智能背景下影响信任建立的不太明显的因素。这些是微观感知水平上的良好影响因素，从个性特征到偏见，到过去的经历，到社会化和教养。我们只是在收集数据，通过关联、定性和定量的方法来探索人工智能中信任的这些前因。

![](img/2d6cfe4cbe58516279359c8ba907f5b5.png)

来源:[玛丽莎·特肖普](https://www.linkedin.com/in/marisa-tschopp-0233a026/)

**产品上市后有没有可能改变 AI 的形象或者影响消费者行为？**

如果你想探索你的信任形象，你需要从各种角度看问题和定义:你可能想看个人(像你的目标群体或员工的特征)，你可以从消费者的角度看建立、维持和发展信任的过程，以及破坏和重新获得信任。你必须清楚演员和角色(谁是可信的？)和情况:是像自动驾驶汽车这样的高风险情况，还是我们谈论的人工智能驱动的客服聊天机器人？

**最后，答案是肯定的**，然而，在两个方向上，无论是好是坏。当我们谈论人工智能时，我们必须非常敏感，中立，或者像汉斯·罗斯林所说的“实事求是”。关于做什么来维持一段关系或如何行动的研究非常清楚，如果你打破了信任关系，我不确定人工智能是否与其他技术有任何不同。违规就是违规，不管是脸书的数据违规还是导弹失误。

> 如果发生了信任违约，你必须立即、直接、清楚地沟通发生了什么，不要辩解地解释自己，要真实可信，并询问需要什么来获得另一次机会。

**你会为企业主或产品经理推荐哪些书籍和其他资源来学习人工智能中的信任建设？**

我建议检查一下[欧洲高级人工智能专家组](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence)。他们刚刚发布了一个构建[可信人工智能的框架。该框架有三个主要部分，包括合法人工智能、道德人工智能和健壮人工智能。报告讨论了后两者的要点。](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)

另一套全面的众包标准来自于 [IEEE 自主智能系统伦理全球倡议](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)，它被称为伦理一致设计。

雷切尔·博茨曼:你能相信谁？她写了在数字时代信任是如何建立、失去和恢复的，她也有几个被强烈推荐的 TED 演讲。

这次采访是由来自 Omdena 的迈克尔·布哈特完成的，Omdena 是一个创新平台，人工智能工程师和领域专家在这里合作构建现实世界问题的解决方案。