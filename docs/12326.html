<html>
<head>
<title>Action Masking with RLlib</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 RLlib 进行动作屏蔽</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505?source=collection_archive---------14-----------------------#2020-08-25">https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505?source=collection_archive---------14-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ef2a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">改善强化学习的参数动作</h2></div><p id="29a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RL 算法通过反复试验来学习。代理人在早期搜索状态空间，并采取随机行动来了解什么会带来好的回报。非常简单。</p><p id="fbc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，这并不是非常有效，特别是如果我们已经知道一些关于在某些州什么是好与坏的行为。幸运的是，我们可以使用<strong class="kk iu">动作屏蔽</strong>——一种将坏动作的概率设置为 0 的简单技术——来加速学习和改进我们的策略。</p><h1 id="fea5" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="3f6b" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我们通过对背包打包环境的动作屏蔽来实施约束，并向<a class="ae mb" href="https://www.datahubbs.com/action-masking-with-rllib/" rel="noopener ugc nofollow" target="_blank">展示如何使用 RLlib </a>来实现这一点。</p><h1 id="5800" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">强制约束</h1><p id="6c22" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">让我们用经典的<a class="ae mb" href="https://en.wikipedia.org/wiki/Knapsack_problem" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">背包问题</strong> </a>来开发一个具体的例子。</p><p id="5e6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">背包问题(KP)要求你打包一个背包，在不超载的情况下使包里的价值最大化。如果你收集了如下所示的物品，最佳包装将包含三个黄色盒子和三个灰色盒子，总共 36 美元和 15 公斤(这是<strong class="kk iu">无界背包问题</strong>，因为你可以选择的盒子数量没有限制)。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/e36089ee3ccc3ae9ab9f3fa32f346938.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/0*hy_pCLbWsu2f-Rnd"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">来源:维基百科</p></figure><p id="f024" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，这个问题是使用<a class="ae mb" href="https://www.datahubbs.com/what-is-dynamic-programming/" rel="noopener ugc nofollow" target="_blank">动态编程</a>或数学编程来解决的。如果我们按照数学程序建立它，我们可以写出如下模型:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/2903c6ddfce7d1363b9facb5d081f319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJa8UfktuxFg9ALuOSa1Ag.png"/></div></div></figure><p id="8985" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，<em class="mt"> x_i </em>可以是任何大于等于 0 的值，代表我们放入背包的物品数量。<em class="mt"> v_i </em>和<em class="mt"> w_i </em>，分别为项目的值和权重。</p><p id="cb7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用简单的语言来说，这个小模型是在说我们想要最大化背包中的价值(我们称之为<em class="mt"> z </em>)。我们通过找到最大数量的物品(<em class="mt"> x_i </em>)和它们的值(<em class="mt"> v_i </em>)而不超过背包的重量限制(<em class="mt"> W </em>)来做到这一点。这个公式被称为整数规划(IP ),因为我们有整数决策变量(我们不能打包项目的一部分，只能打包完整的整数值),并使用像 CPLEX、Gurobi 或 GLPK(最后一个是免费和开源的)这样的求解器来求解。</p><p id="dc07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强制实施这些约束是模型的一部分，但它并不是 RL 的一部分。RL 模型可能需要包装绿色的 12 公斤的盒子几次，然后才知道它不能包装那个和黄色的 4 公斤的盒子，通过受到几次大的负面奖励。过度打包的负面回报是一个“软约束”，因为我们没有明确禁止算法做出这些糟糕的决定。但是，如果我们使用动作屏蔽，我们可以确保模型不会做出愚蠢的选择，这也将有助于它更快地学习更好的策略。</p><h1 id="aa9a" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">金伯利进程环境</h1><p id="172b" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">让我们通过使用<code class="fe mu mv mw mx b"><a class="ae mb" href="https://github.com/hubbs5/or-gym" rel="noopener ugc nofollow" target="_blank">or-gym</a></code>库打包一个背包来实现这一点，该库包含一些来自运筹学领域的经典环境，我们可以用它们来训练 RL 代理。如果你熟悉 OpenAI Gym，你会以同样的方式使用它。可以用<code class="fe mu mv mw mx b">pip install or-gym</code>安装。</p><p id="de21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦安装完成，导入它并构建<code class="fe mu mv mw mx b">Knapsack-v0</code>环境，这就是我们上面描述的无界背包问题。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="475c" class="nc lf it mx b gy nd ne l nf ng">import or_gym<br/>import numpy as np<br/>env = or_gym.make('Knapsack-v0')</span></pre><p id="5b47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该环境的默认设置有 200 种不同的物品可供选择，最大承重为 200 公斤。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="5db4" class="nc lf it mx b gy nd ne l nf ng">print("Max weight capacity:\t{}kg".format(env.max_weight))<br/>print("Number of items:\t{}".format(env.N))</span><span id="eafe" class="nc lf it mx b gy nh ne l nf ng">[out]<br/>Max weight capacity: 200kg<br/>Number of items: 200</span></pre><p id="3bd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这很好，但是 200 个项目看得太清楚了，所以我们可以通过一个<code class="fe mu mv mw mx b">env_config</code>字典来更改其中的一些参数，以匹配上面的示例。此外，我们可以通过将<code class="fe mu mv mw mx b">mask: True</code>或<code class="fe mu mv mw mx b">mask: False</code>传递给配置字典来打开和关闭动作屏蔽。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="bfc8" class="nc lf it mx b gy nd ne l nf ng">env_config = {'N': 5,<br/>              'max_weight': 15,<br/>              'item_weights': np.array([1, 12, 2, 1, 4]),<br/>              'item_values': np.array([2, 4, 2, 1, 10]),<br/>              'mask': True}</span><span id="dcae" class="nc lf it mx b gy nh ne l nf ng">env = or_gym.make('Knapsack-v0', env_config=env_config)</span><span id="e08c" class="nc lf it mx b gy nh ne l nf ng">print("Max weight capacity:\t{}kg".format(env.max_weight))<br/>print("Number of items:\t{}".format(env.N))</span><span id="d8d9" class="nc lf it mx b gy nh ne l nf ng">[out]<br/>Max weight capacity: 15kg<br/>Number of items: 5</span></pre><p id="9277" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们的环境与上面的例子相匹配。简单看一下我们的状态。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="58d8" class="nc lf it mx b gy nd ne l nf ng">env.state</span><span id="86cc" class="nc lf it mx b gy nh ne l nf ng">[out]<br/>{'action_mask': array([1, 1, 1, 1, 1]),<br/> 'avail_actions': array([1., 1., 1., 1., 1.]),<br/> 'state': array([ 1, 12,  2,  1,  4,  2,  4,  2,  1, 10,  0])}</span></pre><p id="ee31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们将 action mask 选项设置为<code class="fe mu mv mw mx b">True</code>时，我们得到一个字典输出，作为包含三个条目<code class="fe mu mv mw mx b">action_mask</code>、<code class="fe mu mv mw mx b">avail_actions</code>和<code class="fe mu mv mw mx b">state</code>的状态。这对于<code class="fe mu mv mw mx b">or-gym</code>库中的所有环境都是相同的格式。掩码是一个二进制向量，其中 1 表示允许一个动作，0 表示它将打破某些约束。在这种情况下，我们唯一的约束是重量，所以如果一个给定的项目将推动模型超过重量，它将收到一个大的，负的惩罚。</p><p id="d056" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可用操作对应于代理可以选择打包的五个项目中的每一个。状态是传递给神经网络的输入。在这种情况下，我们有一个连接了项目权重和值的向量，并在末尾添加了当前权重(初始化环境时为 0)。</p><p id="41b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们继续并选择 12 公斤的物品进行包装，我们应该看到动作屏蔽更新，以消除包装任何其他物品，使模型超过重量限制。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="2649" class="nc lf it mx b gy nd ne l nf ng">state, reward, done, _ = env.step(1)<br/>state</span><span id="1b46" class="nc lf it mx b gy nh ne l nf ng">{'action_mask': array([1, 0, 1, 1, 0]),<br/> 'avail_actions': array([1., 1., 1., 1., 1.]),<br/> 'state': array([ 1, 12,  2,  1,  4,  2,  4,  2,  1, 10, 12])}</span></pre><p id="0850" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你看一下<code class="fe mu mv mw mx b">action_mask</code>，那正是我们所看到的。环境正在返回信息，我们可以使用这些信息来阻止代理选择 12 公斤或 4 公斤的物品，因为这将违反我们的约束。</p><p id="9bc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的概念很容易应用。在您完成通过策略网络的转发后，您可以使用掩码来更新非法操作的值，使它们成为较大的负数。这样，当你把它传递给 softmax 函数时，相关的概率将会是 0。</p><p id="b06e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们转向使用 RLlib 来训练模型以遵守这些约束。</p><h1 id="2c90" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">RLlib 中的动作屏蔽</h1><p id="abed" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">RLlib 中的动作屏蔽需要构建一个直接处理逻辑的定制模型。对于带有动作屏蔽的定制环境，这并不像我希望的那样简单，所以我将一步一步地指导您。</p><p id="0cc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要先进口很多东西。如果你熟悉这个库的话，<code class="fe mu mv mw mx b">ray</code>和我们的<code class="fe mu mv mw mx b">ray.rllib.agents</code>应该是显而易见的，但是我们还需要<code class="fe mu mv mw mx b">tune</code>、<code class="fe mu mv mw mx b">gym.spaces</code>、<code class="fe mu mv mw mx b">ModelCatalog</code>、一个 Tensorflow 或 PyTorch 模型(取决于你的偏好，对于这个我只坚持使用 TF)，以及一个我们编写的<code class="fe mu mv mw mx b">or_gym</code>库中名为<code class="fe mu mv mw mx b">create_env</code>的实用程序，以使这个更加平滑。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="ceec" class="nc lf it mx b gy nd ne l nf ng">import ray<br/>from ray.rllib import agents<br/>from ray import tune<br/>from ray.rllib.models import ModelCatalog<br/>from ray.rllib.models.tf.tf_modelv2 import TFModelV2<br/>from ray.rllib.models.tf.fcnet import FullyConnectedNetwork<br/>from ray.rllib.utils import try_import_tf<br/>from gym import spaces<br/>from or_gym.utils import create_env</span><span id="b4ec" class="nc lf it mx b gy nh ne l nf ng">tf = try_import_tf()</span></pre><h1 id="56e7" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">构建自定义模型</h1><p id="e15e" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我们需要明确地告诉神经网络如何处理状态字典中的不同值。为此，我们将基于 RLlib 的<code class="fe mu mv mw mx b">TFModelV2</code>模块构建一个定制模型。这将使我们能够构建一个定制的模型类，并向模型添加一个<code class="fe mu mv mw mx b">forward</code>方法以便使用它。在<code class="fe mu mv mw mx b">forward</code>方法中，我们应用如下所示的遮罩:</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="2928" class="nc lf it mx b gy nd ne l nf ng">class KP0ActionMaskModel(TFModelV2):<br/>    <br/>    def __init__(self, obs_space, action_space, num_outputs,<br/>        model_config, name, true_obs_shape=(11,),<br/>        action_embed_size=5, *args, **kwargs):<br/>        <br/>        super(KP0ActionMaskModel, self).__init__(obs_space,<br/>            action_space, num_outputs, model_config, name, <br/>            *args, **kwargs)<br/>        <br/>        self.action_embed_model = FullyConnectedNetwork(<br/>            spaces.Box(0, 1, shape=true_obs_shape), <br/>                action_space, action_embed_size,<br/>            model_config, name + "_action_embedding")<br/>        self.register_variables(self.action_embed_model.variables())</span><span id="36d5" class="nc lf it mx b gy nh ne l nf ng">    def forward(self, input_dict, state, seq_lens):<br/>        avail_actions = input_dict["obs"]["avail_actions"]<br/>        action_mask = input_dict["obs"]["action_mask"]<br/>        action_embedding, _ = self.action_embed_model({<br/>            "obs": input_dict["obs"]["state"]})<br/>        intent_vector = tf.expand_dims(action_embedding, 1)<br/>        action_logits = tf.reduce_sum(avail_actions * intent_vector,<br/>            axis=1)<br/>        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)<br/>        return action_logits + inf_mask, state</span><span id="be7f" class="nc lf it mx b gy nh ne l nf ng">    def value_function(self):<br/>        return self.action_embed_model.value_function()</span></pre><p id="b451" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了完成这个过程，我们首先初始化模型并传递我们的<code class="fe mu mv mw mx b">true_obs_shape</code>，它将匹配<code class="fe mu mv mw mx b">state</code>的大小。如果我们坚持使用简化的 KP，这将是一个有 11 个条目的向量。我们需要提供的另一个值是<code class="fe mu mv mw mx b">action_embed_size</code>，它将是我们动作空间的大小(5)。从这里，模型根据我们提供的输入值初始化一个<code class="fe mu mv mw mx b">FullyConnectedNetwork</code>并注册这些值。</p><p id="5190" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际的屏蔽发生在<code class="fe mu mv mw mx b">forward</code>方法中，我们从环境提供的观察字典中解包屏蔽、动作和状态。状态产生我们的动作嵌入，它与我们的掩码相结合，以提供具有我们所能提供的最小值的逻辑。这将传递给 softmax 输出，从而将选择这些操作的概率降低到 0，有效地阻止代理采取这些非法操作。</p><p id="9906" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了自己的模型，我们需要用<code class="fe mu mv mw mx b">ModelCatalog</code>注册它，这样 RLlib 就可以在训练中使用它。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="dcf8" class="nc lf it mx b gy nd ne l nf ng">ModelCatalog.register_custom_model('kp_mask', KP0ActionMaskModel)</span></pre><p id="f4e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们需要注册自定义环境，以便可以使用 RLlib 调用。下面，我有一个叫做<code class="fe mu mv mw mx b">register_env</code>的小助手函数，我们用它来包装我们的<code class="fe mu mv mw mx b">create_env</code>函数和 tune 的<code class="fe mu mv mw mx b">register_env</code>函数。Tune 需要基类，而不是像我们从<code class="fe mu mv mw mx b">or_gym.make(env_name)</code>获得的环境实例。所以我们需要使用如下所示的 lambda 函数将它传递给<code class="fe mu mv mw mx b">register_env</code>。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="b148" class="nc lf it mx b gy nd ne l nf ng">def register_env(env_name, env_config={}):<br/>    env = create_env(env_name)<br/>    tune.register_env(env_name, lambda env_name: env(env_name, env_config=env_config))<br/>    <br/>register_env('Knapsack-v0', env_config=env_config)</span></pre><p id="44d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以初始化 ray，并将模型和设置传递给我们的培训师。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="91f2" class="nc lf it mx b gy nd ne l nf ng">ray.init(ignore_reinit_error=True)</span><span id="65e5" class="nc lf it mx b gy nh ne l nf ng">trainer_config = {<br/>    "model": {<br/>        "custom_model": "kp_mask"<br/>        },<br/>    "env_config": env_config<br/>     }<br/>trainer = agents.ppo.PPOTrainer(env='Knapsack-v0', config=trainer_config)</span></pre><p id="b9d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了证明我们的约束有效，我们可以通过将其中一个值设置为 0 来屏蔽给定的动作。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="76f3" class="nc lf it mx b gy nd ne l nf ng">env = trainer.env_creator('Knapsack-v0')<br/>state = env.state<br/>state['action_mask'][0] = 0</span></pre><p id="84b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们屏蔽了动作 0，所以根本看不到代理选择 0。</p><pre class="md me mf mg gt my mx mz na aw nb bi"><span id="ed1c" class="nc lf it mx b gy nd ne l nf ng">actions = np.array([trainer.compute_action(state) for i in range(10000)])<br/>any(actions==0)</span><span id="594e" class="nc lf it mx b gy nh ne l nf ng">[out]<br/>False</span></pre><p id="35e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们做到了！我们已经成功地用 RLlib 中的自定义模型限制了我们的输出，以加强约束。您也可以使用与<code class="fe mu mv mw mx b">tune</code>相同的设置来约束动作空间并提供参数化动作。</p><h1 id="2e80" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">面具有用</h1><p id="7744" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">屏蔽可以非常有效地将代理从有害的局部最小值中释放出来。<a class="ae mb" href="https://arxiv.org/abs/2008.06319" rel="noopener ugc nofollow" target="_blank">在这里，我们为</a> <code class="fe mu mv mw mx b"><a class="ae mb" href="https://arxiv.org/abs/2008.06319" rel="noopener ugc nofollow" target="_blank">or-gym</a></code>构建了一个虚拟机分配环境，其中带有屏蔽的模型快速找到了一个优秀的策略，而没有屏蔽的模型则陷入了局部最优。我们尝试了很多奖励功能来摆脱这种困境，但是没有任何效果，直到我们应用了一个遮罩！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ni"><img src="../Images/cd1dfe1ce6235acf4d3aba645844759b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*swVii4TRqjuhRI3X.png"/></div></div></figure></div></div>    
</body>
</html>