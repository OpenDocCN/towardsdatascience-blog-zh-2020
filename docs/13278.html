<html>
<head>
<title>LiDAR point-cloud based 3D object detection implementation with colab {Part-1 of 2}</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 colab 实现基于激光雷达点云的三维目标检测(第 1 部分，共 2 部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lidar-point-cloud-based-3d-object-detection-implementation-with-colab-part-1-of-2-e3999ea8fdd4?source=collection_archive---------6-----------------------#2020-09-12">https://towardsdatascience.com/lidar-point-cloud-based-3d-object-detection-implementation-with-colab-part-1-of-2-e3999ea8fdd4?source=collection_archive---------6-----------------------#2020-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6fee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将了解使用 KITTI 激光雷达点云数据实现 3D 车辆检测的体素网算法所需的概念</p><p id="58e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">环境感知在构建自主车辆、自主导航机器人和其他现实世界应用中扮演着不可或缺的角色。相机、雷达和激光雷达等传感器用于感知环境的 360 度视图。从传感器获得的数据被解释为检测静态和动态物体，如车辆、树木和行人等。</p><h2 id="3617" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">3D 物体检测的需要</strong></h2><p id="77ce" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">计算机视觉中最先进的技术在 2D 数据如图像、视频(图像帧序列)上实时高精度地检测物体。但是使用相机传感器进行诸如定位、测量物体之间的距离以及计算深度信息之类的活动可能不是有效的，并且在计算上是昂贵的。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/74b5bfbb7243bfdcfd7d38b75c64ba16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Bl0jbKc5FUZTaPxJPfop_Q.gif"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图片来源:<a class="ae lv" href="https://github.com/enginBozkurt/LidarObstacleDetection" rel="noopener ugc nofollow" target="_blank"> engin Bozkurt </a>带 KITTI 点云浏览器</p></figure><p id="35ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">激光雷达是一种主要的传感器，它根据点云提供物体的 3D 信息，以定位物体并表征形状。</p><p id="9ba1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，许多先进的 3D 对象检测器，如 VeloFCN、3DOP、3D YOLO、PointNet、PointNet++等，被提出用于 3D 对象检测。但在本文中，我们将讨论<strong class="jp ir">体素网</strong>这是一种 3D 物体检测算法，其性能超过了上述所有最先进的模型*。</p><h2 id="985e" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">体素网:基于点云的三维物体检测的端到端学习。</h2><p id="1a41" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">作者:尹舟，Oncel Tuzel - Apple Inc</p><p id="4235" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文的 DF 可以在这里下载，发表日期:<em class="lw">2017 年 11 月 17 日</em></p><h2 id="46d7" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">体素网应对的挑战</h2><ul class=""><li id="2655" class="lx ly iq jp b jq le ju lf jy lz kc ma kg mb kk mc md me mf bi translated">代替手动特征提取:在手动特征提取中，将点云投影到俯视图中，然后应用基于图像的特征提取方法进行检测。但是这些技术造成了信息瓶颈，并且不能提取检测任务所需的 3D 信息。</li></ul><p id="08a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了有效地提取三维形状信息，本文引入了机器学习特征提取器——特征学习网络。</p><ul class=""><li id="7c7b" class="lx ly iq jp b jq jr ju jv jy mg kc mh kg mi kk mc md me mf bi translated">减少计算并关注存储器限制:体素分组和随机采样技术用于处理体素中包含多于 T 个点的体素。</li><li id="dbb0" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk mc md me mf bi translated">端到端 3D 检测架构:同时从原始点云数据中学习特征表示，并以端到端的方式预测精确的 3D 边界框。</li></ul></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><blockquote class="mv mw mx"><p id="2af0" class="jn jo lw jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">体素网简而言之是“将点云划分为等距的 3D 体素，通过堆叠的 VFE 层将每个体素编码为矢量，然后使用 3D 卷积层聚集(组合)局部体素特征，将点云转换为高维体积表示。最后，修改的 RPN 网络引入体积表示并提供检测结果”。</p></blockquote><h2 id="c0d9" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">体素网架构</h2><p id="4f16" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">体素网架构主要包含三个模块</p><ol class=""><li id="54c2" class="lx ly iq jp b jq jr ju jv jy mg kc mh kg mi kk nb md me mf bi translated">特征学习网络</li><li id="d06d" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated">卷积中间层</li><li id="6056" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated">区域提案网络</li></ol><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/ecdc6e3efadd70f1d683e12871076869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awNRB_nMO2n-uID1m7xwSQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">体素网架构。a)输入细分成等间距体素的点云数据 b)特征学习网络，将体素中的一组点转换成作为 3D 张量的新特征表示 c) 3D 卷积 d) RPN 网络以绘制 3D 边界框。<a class="ae lv" href="https://arxiv.org/abs/1711.06396" rel="noopener ugc nofollow" target="_blank"> P </a> ic 学分:【https://arxiv.org/abs/1711.06396 T2】T3</p></figure><h1 id="9914" class="nh km iq bd kn ni nj nk kq nl nm nn kt no np nq kw nr ns nt kz nu nv nw lc nx bi translated">特征学习网络:</h1><p id="51f0" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">特征学习网络用于通过处理体素中的单个点云来从体素网格中提取描述性特征，以获得逐点特征，然后将这些逐点特征与局部聚集特征聚集。特征学习网络被应用于包含多于 T 个数目的点的所有体素。</p><p id="2cc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">体素分割</strong>:将 3D 空间细分成等间距的体素</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ny"><img src="../Images/cea01254746c87df0491e09e8358f16e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8Lmxvg6iX2vrUrpVrA5zg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">体素分割:用体素分割 3D 空间。作者图片</p></figure><p id="9404" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是体素分区的代码片段:</p><blockquote class="mv mw mx"><p id="3cec" class="jn jo lw jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">注意:为体素网格定义的测量将根据对象的类别而变化</p></blockquote><ul class=""><li id="1b64" class="lx ly iq jp b jq jr ju jv jy mg kc mh kg mi kk mc md me mf bi translated">__cfg__。MAX_POINT_NUMBER 表示点云阈值，该阈值用于处理包含超过 35 个点云的选择性体素网格</li><li id="b674" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk mc md me mf bi translated">x，Y，Z 最小值，最大值以米为单位定义 3D 空间</li><li id="4dd0" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk mc md me mf bi translated">体素 X 尺寸表示体素网格尺寸(固定)</li></ul><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nz oa l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">体素分割</p></figure><p id="3328" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">随机抽样</strong></p><p id="b94f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点云被划分到体素网格中。因为处理所有点在计算上是昂贵的，并且增加了存储器的使用，这又增加了计算设备的负荷。为了应对这一挑战，对包含“T”个以上点云的体素网格进行采样。</p><p id="0fde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了这个策略，我们可以实现</p><ol class=""><li id="4753" class="lx ly iq jp b jq jr ju jv jy mg kc mh kg mi kk nb md me mf bi translated">计算节省</li><li id="1db6" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated">减少体素之间点的不平衡，从而减少采样偏差，并为训练增加更多变化。</li></ol><p id="9e73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">堆叠体素特征编码</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ob"><img src="../Images/645c07a0fb12c35129a5bd2e1011a5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKSwAJbZj8bnLku9d81wPQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">应用于单个体素的特征学习网络。<a class="ae lv" href="https://arxiv.org/abs/1711.06396" rel="noopener ugc nofollow" target="_blank"> pic 积分</a></p></figure><p id="6dd9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">全连接神经网络的逐点输入:</strong>我们考虑包含多于 T 个数目的点的体素网格。体素中的每个点云用 4 个坐标[x，y，z，r]表示；其中 x、y、z 表示坐标，r 表示反射率。我们计算局部平均值作为体素网格内所有点的质心(<strong class="jp ir"> V)。</strong>然后，我们用局部均值的偏移来增加体素中的每个点，以获得逐点输入特征集。</p><p id="f086" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">全连接神经网络(FCN 网):</strong>逐点输入特征集被馈送到全连接神经网络，以聚集所有逐点特征，从而对体素所包含的表面形状进行编码。</p><p id="c45e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">FCN 网络由线性层、批量归一化和递归组成。</p><p id="fe87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于元素的最大池:</strong>基于元素的最大池用于从基于点的输入特征中获取局部聚集的特征</p><p id="89ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，逐点连接用于聚合逐点特征和局部聚合特征。</p><p id="49e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用批量标准化和最大池定义逐点输入、聚合要素的 VFE 图层代码片段:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="7071" class="nh km iq bd kn ni nj nk kq nl nm nn kt no np nq kw nr ns nt kz nu nv nw lc nx bi translated">卷积中间层</h1><p id="29ab" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">该层将体素特征转换为密集的 4D 特征图，并使用卷积、批量归一化、ReLU 将特征图的大小减少到原始的四分之一。</p><p id="bfce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ConvMD(cin，cout，k，s，p)表示 M 维卷积运算符，其中 cin 和 cout 表示输入和输出通道的数量，k、s 和 p 分别表示内核大小、步长和填充大小。</p><p id="43d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显示 3D 卷积层的代码片段:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="c99c" class="nh km iq bd kn ni nj nk kq nl nm nn kt no np nq kw nr ns nt kz nu nv nw lc nx bi translated">区域提案网络</h1><p id="69be" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">改进的区域提议网络具有三个完全卷积层的块。每个块的第一层通过步长为 2 的卷积对特征图进行减半下采样，之后是步长为 1 的卷积序列(×q 表示滤波器的 q 次应用)。在每个卷积层之后，应用 BN 和 ReLU 操作。然后，我们将每个块的输出上采样到固定大小，并连接以构建高分辨率特征图。</p><p id="1ea2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，该特征图被映射到期望的学习目标:</p><p id="ece9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)概率得分图和</p><p id="d0b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(2)回归图。</p><p id="fe2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">定义卷积和反卷积层的代码片段:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ob"><img src="../Images/e04dfdf591f74ecb5f556b65dfbc5f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sD9ooZDbDi-nbHduQx-GmA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">修改后的 RPN 架构<a class="ae lv" href="https://arxiv.org/abs/1711.06396" rel="noopener ugc nofollow" target="_blank"> pic 信用</a></p></figure><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="a6d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！！！</p><p id="bf2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下一篇文章中，我们将实现三维物体检测的体素网代码</p><p id="00d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是在 colab 中实现该模型所获得的一些结果</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oc"><img src="../Images/adbbb88c64cb27271e8cb435702ce554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9wu8PgdKD6yAZP8AyGB2A.jpeg"/></div></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oc"><img src="../Images/5cc6b3d0cded044a6b0d3b220f5b85c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Z1Uxi_S8_1w0ejM6eYW5g.jpeg"/></div></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oc"><img src="../Images/2dde2e10acee1cacb1e9ff7ef08286b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dpP15YwsWv30h46z2mWYg.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">KITTI 验证数据集上的预测结果</p></figure><p id="66cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特别感谢:</strong></p><p id="7fed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">Uma K Mudenagudi 博士，KLE 理工大学，项目导师。</strong></p><p id="aecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考</p><ol class=""><li id="3b2f" class="lx ly iq jp b jq jr ju jv jy mg kc mh kg mi kk nb md me mf bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1711.06396.pdf" rel="noopener ugc nofollow" target="_blank">体素网:基于点云的三维物体检测的端到端学习</a></li><li id="ffcd" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated"><a class="ae lv" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank"> PointNet:用于三维分类和分割的点集深度学习</a></li><li id="6f86" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1907.09408" rel="noopener ugc nofollow" target="_blank">基于深度学习的物体检测综述</a></li><li id="1741" class="lx ly iq jp b jq mj ju mk jy ml kc mm kg mn kk nb md me mf bi translated">KITTI 原始数据集:@ARTICLE{ <a class="ae lv" href="http://www.cvlibs.net/publications/Geiger2013IJRR.pdf" rel="noopener ugc nofollow" target="_blank"> Geiger2013IJRR </a>，作者= { <a class="ae lv" href="http://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank"> Andreas Geiger </a>和<a class="ae lv" href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" rel="noopener ugc nofollow" target="_blank"> Philip Lenz </a>和<a class="ae lv" href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" rel="noopener ugc nofollow" target="_blank"> Christoph Stiller </a>和<a class="ae lv" href="http://ttic.uchicago.edu/~rurtasun" rel="noopener ugc nofollow" target="_blank"> Raquel Urtasun </a> }，标题= {视觉与机器人:KITTI 数据集}，期刊= {国际机器人研究期刊(IJRR)}，年份= {2013}</li></ol></div></div>    
</body>
</html>