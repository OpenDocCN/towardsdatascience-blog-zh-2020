<html>
<head>
<title>Classification of Hotel Cancellations Using KNN and SMOTE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 KNN 和 SMOTE 的酒店取消分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-of-hotel-cancellations-using-knn-and-smote-3290cc87e74d?source=collection_archive---------40-----------------------#2020-03-16">https://towardsdatascience.com/classification-of-hotel-cancellations-using-knn-and-smote-3290cc87e74d?source=collection_archive---------40-----------------------#2020-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1545" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">KNN(K-最近邻)是一种用于分类目的的常用方法。</h2></div><p id="c032" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此特定示例中，KNN 算法用于根据取消风险对酒店预订进行分类(1 =模型预测客户将取消预订，0 =预测客户不会取消预订)。</p><p id="5337" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设该数据集是不平衡的，即 0(未消除)比 1(消除)多，则使用合成少数过采样技术(SMOTE)来平衡类，以便应用 KNN 算法。</p><h1 id="28b9" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">SMOTE 过采样技术</h1><p id="7a61" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">鉴于数据集的不均匀性，有必要对次要类进行过采样(1 =取消)，以确保 KNN 结果不会偏向主要类。</p><p id="256f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可以通过 SMOTE 过采样技术来实现。</p><p id="dc71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 MinMaxScaler 导入和缩放数据后，可以从 imblearn 库中导入 SMOTE。计数器是为了汇总类分布而导入的。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="025a" class="mk lf it mg b gy ml mm l mn mo">import imblearn<br/>print(imblearn.__version__)<br/>from imblearn.over_sampling import SMOTE<br/>from collections import Counter</span></pre><p id="2605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，调用训练-测试分割将数据分成训练和验证数据。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="84b4" class="mk lf it mg b gy ml mm l mn mo">x1_train, x1_val, y1_train, y1_val = train_test_split(x_scaled, y1, random_state=0)</span></pre><p id="738a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原班级分布由<strong class="kk iu"> 0 </strong> : 21672，<strong class="kk iu"> 1 </strong> : 8373 组成。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4f1d" class="mk lf it mg b gy ml mm l mn mo">&gt;&gt;&gt; counter = Counter(y_train)<br/>&gt;&gt;&gt; print(counter)</span><span id="d4f5" class="mk lf it mg b gy mp mm l mn mo">Counter({0: 21672, 1: 8373})</span></pre><p id="25ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在应用 SMOTE 过采样技术之后，我们现在看到每一类中的观察值的数量是相等的。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="6bb2" class="mk lf it mg b gy ml mm l mn mo">&gt;&gt;&gt; oversample = SMOTE()<br/>&gt;&gt;&gt; x_train, y_train = oversample.fit_resample(x_train, y_train)<br/>&gt;&gt;&gt; counter = Counter(y_train)<br/>&gt;&gt;&gt; print(counter)</span><span id="c83d" class="mk lf it mg b gy mp mm l mn mo">Counter({1: 21672, 0: 21672})</span></pre><h1 id="8712" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">模型性能</h1><p id="cfc1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">该模型配置如下:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ae31" class="mk lf it mg b gy ml mm l mn mo"># KNN<br/>knn = KNeighborsClassifier(n_neighbors=10)<br/>model=knn.fit(x1_train, y1_train)<br/>pred = model.predict(x1_val)<br/>pred<br/>print("Training set score: {:.2f}".format(knn.score(x1_train, y1_train)))<br/>print("Validation set score: {:.2f}".format(knn.score(x1_val, y1_val)))</span><span id="db48" class="mk lf it mg b gy mp mm l mn mo"># KNN Plot<br/>mglearn.plots.plot_knn_classification(n_neighbors=10)<br/>plt.show()</span></pre><p id="9584" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">生成训练和测试集分数:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="7798" class="mk lf it mg b gy ml mm l mn mo">Training set score: 0.88<br/>Validation set score: 0.46</span></pre><p id="1ae2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是 KNN 模型所示的训练课程与测试预测的对比图:</p><figure class="mb mc md me gt mr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3bb39fa1f137266a3c74dd9a094dfa04.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*IPCO_sVs9vv7AKfE.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">来源:Jupyter 笔记本</p></figure><p id="a1de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是根据混淆矩阵对模型性能进行的细分:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="e93a" class="mk lf it mg b gy ml mm l mn mo">[[2286 4980]<br/> [ 440 2309]]<br/>              precision    recall  f1-score   support</span><span id="0a44" class="mk lf it mg b gy mp mm l mn mo">           0       0.84      0.31      0.46      7266<br/>           1       0.32      0.84      0.46      2749</span><span id="5ba0" class="mk lf it mg b gy mp mm l mn mo">    accuracy                           0.46     10015<br/>   macro avg       0.58      0.58      0.46     10015<br/>weighted avg       0.70      0.46      0.46     10015</span></pre><p id="e939" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然总体准确率很低，只有 46%，但根据 f1 评分，召回率相当不错，为 84%。</p><h1 id="81fa" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">精确度与召回率</h1><p id="8115" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">当处理分类数据时，还必须注意精度与召回读数，而不是简单的整体精度。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4d10" class="mk lf it mg b gy ml mm l mn mo">- Precision = ((True Positive)/(True Positive + False Positive))</span><span id="a3ea" class="mk lf it mg b gy mp mm l mn mo">- Recall = ((True Positive)/(True Positive + False Negative))</span></pre><p id="0477" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个读数经常相互矛盾，也就是说，通常不可能在不降低召回率的情况下提高精确度，反之亦然。</p><p id="247d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对理想指标的评估很大程度上取决于所分析的具体数据。例如，癌症检测筛查出现假阴性(即表明患者没有患癌症，而事实上他们患有癌症)是一大禁忌。在这种情况下，召回是理想的衡量标准。</p><p id="9385" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，对于电子邮件，人们可能更喜欢避免误报，例如，将一封重要的电子邮件发送到垃圾邮件文件夹，而实际上它是合法的。</p><p id="d0eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">f1 分数在设计一个更通用的分数时考虑了精确度和召回率。</p><p id="474d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">哪个因素对预测酒店取消更重要？</p><p id="7edd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从酒店的角度来看，他们可能希望更准确地识别出最终会取消预订的客户，这使得酒店能够更好地分配房间和资源。确定不打算取消预订的客户不一定会增加酒店分析的价值，因为酒店知道，无论如何，很大一部分客户最终都会坚持预订。</p><h1 id="ce8a" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">测试数据</h1><p id="361b" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">让我们看看当模型对 H2(测试集)进行预测时，结果是什么样的。</p><p id="3dde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以看到 f1 得分的准确性略微提高到了 52%。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="e492" class="mk lf it mg b gy ml mm l mn mo">[[12569 33659]<br/> [ 4591 28511]]<br/>              precision    recall  f1-score   support</span><span id="527e" class="mk lf it mg b gy mp mm l mn mo">           0       0.73      0.27      0.40     46228<br/>           1       0.46      0.86      0.60     33102</span><span id="d657" class="mk lf it mg b gy mp mm l mn mo">    accuracy                           0.52     79330<br/>   macro avg       0.60      0.57      0.50     79330<br/>weighted avg       0.62      0.52      0.48     79330</span></pre><p id="d2ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，取消类(1)的召回率为 86%。如前所述，精确度和召回率经常相互矛盾，仅仅是因为假阳性倾向于增加召回率，而假阴性倾向于增加精确度。</p><p id="ea42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设酒店希望最大化召回率(即容忍一定数量的误报，同时识别所有将取消预订的客户)，那么该模型符合该标准。</p><p id="284e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在所有取消预订的客户中，该模型正确识别了 86%的客户。</p><h1 id="b8c8" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">模型评估</h1><p id="9afd" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">虽然较高的召回率被认为是判断该模型的较好方式，但这不一定以较低的准确性为代价。</p><p id="e147" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果召回是惩罚假阴性，那么它也有利于假阳性——有太多的假阳性会破坏模型的目的——因为这本质上是假设所有客户都会取消，而事实并非如此。</p><p id="3475" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这方面，准确性和召回率将理想地最大化。例如，XGBoost 模型展示了 94%的召回率和 55%的 f1 分数准确度——两者都比这个例子中的略高。</p><h1 id="8241" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="3fa5" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在本例中，我们研究了如何使用 KNN 作为分类算法，以及在判断模型性能时精确度和召回率的重要性。</p><p id="ac63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非常感谢您的参与，这个例子的相关 GitHub 库可以在<a class="ae my" href="https://github.com/MGCodesandStats/hotel-modelling" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="765b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mz">免责声明:本文是在“原样”的基础上编写的，没有担保。本文旨在提供数据科学概念的概述，不应以任何方式解释为专业建议。</em></p><h1 id="d228" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">参考</h1><ul class=""><li id="cca0" class="na nb it kk b kl lw ko lx kr nc kv nd kz ne ld nf ng nh ni bi translated"><a class="ae my" href="https://www.sciencedirect.com/science/article/pii/S2352340918315191" rel="noopener ugc nofollow" target="_blank">安东尼奥、阿尔梅迪亚和努内斯(2019)。酒店预订需求数据集</a></li><li id="d7eb" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><a class="ae my" href="https://github.com/Msanjayds/Scikit-learn/blob/master/CrossValidation.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 仓库(Msanjayds):交叉验证计算</a></li><li id="1f48" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><a class="ae my" href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">机器学习掌握:用 Python 对不平衡分类进行 SMOTE 过采样</a></li></ul></div></div>    
</body>
</html>