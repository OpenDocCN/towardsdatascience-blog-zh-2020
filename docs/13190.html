<html>
<head>
<title>ECCV 2020 digest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ECCV 2020 文摘</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eccv-2020-digest-d1244737e68?source=collection_archive---------27-----------------------#2020-09-10">https://towardsdatascience.com/eccv-2020-digest-d1244737e68?source=collection_archive---------27-----------------------#2020-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9ab1770b21303f07c6fcd449950046a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mgrQbrQpYq8XL-oNzLBUAw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610749.pdf" rel="noopener ugc nofollow" target="_blank">的图像用于视频场景理解的概率未来预测</a></p></figure><div class=""/><div class=""><h2 id="2fe5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">专家最有趣的自驾研究。</h2></div><p id="d8b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两周前，数千名计算机视觉研究人员聚集在 ECCV<a class="ae jg" href="https://eccv2020.eu/" rel="noopener ugc nofollow" target="_blank">欧洲计算机视觉会议</a>展示他们的最新成果。想看一眼最新的自动驾驶技术吗？这份文摘是给你的。</p><p id="4cb8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了下面的论文，还可以查看我帮助组织的<a class="ae jg" href="https://sites.google.com/view/pad2020/" rel="noopener ugc nofollow" target="_blank"><em class="lu"/>【g】</a><a class="ae jg" href="https://sites.google.com/view/btfm2020" rel="noopener ugc nofollow" target="_blank"><em class="lu">标杆轨迹预测模型</em></a><em class="lu"/><a class="ae jg" href="https://vimeo.com/451293003" rel="noopener ugc nofollow" target="_blank"><em class="lu">5 级行业网络研讨会</em> </a> <em class="lu">和</em> <a class="ae jg" href="https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/" rel="noopener ugc nofollow" target="_blank"> Kaggle ML 竞赛</a>。</p><h1 id="5e09" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">感觉</h1><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/8b5ae94d5d36b8f6611cf24bed23ac12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4_8deRPNc55r8bo0"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710596.pdf" rel="noopener ugc nofollow" target="_blank">跟踪的图像通过环视静态场景出现，具有神经 3D 映射</a></p></figure><h2 id="4c78" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670018.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">基于柱子的自动驾驶物体检测</strong> </a></h2><p id="be32" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">麻省理工和谷歌</strong> <em class="lu">的论文——作者提出了一个完全基于支柱的无锚点物体检测模型。这项工作将圆柱投影纳入多视图特征学习，预测每个支柱的边界框参数，而不是每个点或每个锚，这简化了 3D 对象检测，同时显著改善了最先进的技术。</em></p><h2 id="d07b" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660511.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">用于自监督三维物体检测的单目可微分渲染</strong> </a></h2><p id="c9ee" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自丰田研究所的论文</strong> — <em class="lu">从图像中获取 3D 地面真实标签是一项艰巨而昂贵的任务。本文提出了一种自我监督的替代方案，该方案使用预训练的单目深度估计网络、差分渲染和定制的自我监督物镜来精确预测帧中所有对象的 3D 位置和网格。</em></p><h2 id="9199" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630477.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">雷达网:利用雷达对动态物体进行鲁棒感知</strong> </a></h2><p id="c0b5" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 ATG 优步的论文</strong> — <em class="lu">一种联合使用激光雷达和雷达传感器进行目标探测和速度估计的方法。他们提出的 DNN 可以分为 2 个步骤:首先，基于体素的激光雷达和雷达数据的表示由主干和检测网络进行融合和处理。然后，执行另一次后期融合，并通过注意机制，通过与雷达数据融合来细化对象的径向速度。</em></p><h2 id="55db" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580307.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">重新思考伪激光雷达表示</strong> </a></h2><p id="dd5a" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 SenseTime 的论文— </strong> <em class="lu">对于 2D 探测任务，通常使用中间表示和任务来提高性能:其中一种方法使用伪激光雷达表示，即通过深度估计将 2D 输入转换为类似激光雷达的输入，然后由针对激光雷达输入的 3D 探测而定制的网络对其进行处理。在这项工作中，作者研究了这一点的惊人用途，但得出的结论是，激光雷达表示对任务没有帮助:在这里，他们提出了图像空间的变换和相应任务的网络，并表明这产生了可比的结果。</em></p><h2 id="28be" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710596.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">跟踪通过环视静态场景，用神经 3D 映射出现</strong> </a></h2><p id="6dfd" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 CMU </strong> <em class="lu">的论文——一种新的无监督 3D 对象跟踪方法，通过利用静态点的多视图数据训练 3D 映射网络，然后可以使用这些数据来产生可用于搜索和对应 3D 对象的特征。</em></p><h2 id="f9ab" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470460.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">朝向流感知</strong> </a></h2><p id="8fba" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自宾夕法尼亚大学的论文</strong> — <em class="lu">在这项工作中，作者批评了视觉社区对“离线”方法的主要关注，并提出了一种结合多任务延迟和准确性的新评估设置:核心思想是将输入和输出(例如，对象检测任务)解释为流，并在评估期间将每个地面真实值与最近的预测进行比较。</em></p><h2 id="292f" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480154.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">福根:看到雨夜</strong> </a></h2><p id="6651" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 CMU 和阿尔戈艾的论文— </strong> <em class="lu">众所周知，数据偏差会极大地降低算法性能。为了解决这一问题，作者提出了一种新的基于 GAN 的图像到图像翻译架构，该架构解耦了域不变和域特定内容。提出的 ForkGAN 通过首先将夜间获取的图像转换为白天的图像，显著提高了在白天图像上训练的网络的性能。</em></p><h2 id="887f" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">用变形金刚进行端到端的物体检测</strong> </a></h2><p id="02d9" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自脸书的论文— </strong> <em class="lu">一种新的物体检测方法，不再需要许多手工制作的组件。主干用变换器编码器-解码器结构代替，整个任务作为直接集预测问题来计算成本。性能显示了这种新方法如何能够以少得多的操作和手动调整的参数产生与当前技术水平相似的性能。</em></p><h1 id="7b26" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">预测和规划</h1><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2f5609825f89229d57aaff28fe89fe93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/0*e5aSYg6vuW7Xha_n"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660154.pdf" rel="noopener ugc nofollow" target="_blank"> DSDNet:深度结构化自驾网络</a></p></figure><h2 id="846a" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660154.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> DSDNet:深度结构化自驾网络</strong> </a></h2><p id="0ee8" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">论文来自优步 ATG <em class="lu"> — </em> </strong> <em class="lu">深度神经网络，用于联合学习自动驾驶的感知、预测和规划。基于激光雷达和地图输入，CNN 中枢学习探测物体。随后是预测层，它使用能量公式来推理多个智能体的社会似是而非的轨迹，最后是规划层，它将此考虑在内来规划安全轨迹。</em></p><h2 id="ecd6" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660596.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> PiP:自主驾驶的规划告知轨迹预测</strong> </a></h2><p id="3c3d" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自香港大学的论文- </strong> <em class="lu">作者提出了一种管道，该管道不仅使用历史数据和当前位置，还将计划的自我轨迹考虑在内，从而引入了一种端到端的架构，该架构由交互的计划和预测模块组成。</em></p><h2 id="aa0a" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470528.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">学习车道图表示法进行运动预测</strong> </a></h2><p id="7eb4" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自优步 ATG 的论文— </strong> <em class="lu">一种图形神经网络变体，可以比传统的光栅化地图图像更简洁地编码地图信息。然后，他们使用融合网络将来自 LaneGCN 的地图信息与从当前和以前的位置信息中提取的演员特征相结合，以做出最先进的多模态轨迹预测。</em></p><h2 id="de51" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650698.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">从观察和交互中学习预测模型</strong> </a></h2><p id="c495" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 UPenn、Berkeley 和 Stanford 的论文— </strong> <em class="lu">注入无监督的观察数据，例如来自车辆仪表板摄像头的数据，可以显著提高模型的准确性。为了克服无监督的性质以及领域差异，他们提出了一个图形模型，该模型根据交互作用以及分布先验上的观察数据来推断作为潜在变量的真实行为。</em></p><h2 id="dad6" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610749.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">用于视频场景理解的概率未来预测</strong> </a></h2><p id="c204" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 Wayve 的论文— </strong> <em class="lu">作者从 RGB 视频中学习一种概率潜在表示，可用于对似是而非的未来进行采样。该框架首次联合预测自我轨迹、未来场景语义和几何以及其他智能体的动态。通过条件变分自动编码器方法学习潜在表示，该方法根据观察到的未来来调节所有可能的未来</em></p><h1 id="446a" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">猛击</h1><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/9882770e138b6a8b39889b36a4ef09f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HdL29m29uPOvtnWC"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730273.pdf" rel="noopener ugc nofollow" target="_blank"> DA4AD:面向自动驾驶的端到端深度注意力视觉定位</a></p></figure><h2 id="0ea4" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730273.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> DA4AD:基于端到端深度注意力的自动驾驶视觉定位</strong> </a></h2><p id="d003" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自百度的论文</strong> — <em class="lu">这项工作展示了单目视觉里程计的最新技术水平。这是通过将 VO 流水线的三个关键元素公式化为 ML 问题并采用神经网络对每个元素建模来实现的。</em></p><h2 id="3421" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> NeRF:神经辐射场</strong> </a></h2><p id="af5d" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">来自加州大学伯克利分校 <em class="lu">的论文——在今年最具标志性的简历论文之一中，作者训练了一个网络来构建 3D 模型，并从现有的图片集中推断出现实主义的小说观点。结果明显优于以前的结构从运动和 ML 渲染方法。</em></p><h2 id="83a6" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460222.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">深度 SFM:通过深度束调整从运动中构造</strong> </a></h2><p id="0e9a" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">复旦大学和 Nuro 的论文</strong> — <em class="lu">提出了一种新的基于深度光束法平差的运动结构(SFM)方法。通过引入分别用于深度和姿态估计的两个成本体，照片一致性和几何一致性在训练期间被共同加强，这提高了深度和姿态估计的性能，并具有优越的鲁棒性。</em></p><h1 id="3ead" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">模拟和安全</h1><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/cf0baa38b36798bc5d1e96cc577b1e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OgHWXtIoGzc5ppI1"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710307.pdf" rel="noopener ugc nofollow" target="_blank">通过模拟感知和预测测试自动驾驶车辆的安全性</a></p></figure><h2 id="c66e" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520528.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">交通事故因果关系认定基准</strong> </a></h2><p id="1b6a" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">首尔国立大学的论文</strong> — <em class="lu">本文提出了一个新的数据集，分析交通事故视频流中的因果关系。作者抓取 Youtube 以获得交通事故的剪辑，然后在时间上但也在语义上标记事故的影响和原因。基于此，不同的基准算法基于它们的分类性能被评估。</em></p><h2 id="0879" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710307.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">通过模拟感知和预测测试自动驾驶车辆的安全性</strong> </a></h2><p id="1222" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 ATG 优步的论文</strong> — <em class="lu">作者从模拟数据开始模拟感知-预测堆栈输出。虽然模拟整个堆栈需要像 CARLA 这样的 3D 模拟器，但这种方法完全跳过了原始数据模拟(激光雷达、相机)，只专注于模拟鸟瞰栅格。这大大减少了从模拟到现实世界的测试时间。</em></p><h1 id="0d47" class="lv lw jj bd lx ly lz ma mb mc md me mf kp mg kq mh ks mi kt mj kv mk kw ml mm bi translated">数据集</h1><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/bd208946eb067562394654cad6c0c100.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/0*R3t6do_CdUTgAReZ"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图像来自<a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680069.pdf" rel="noopener ugc nofollow" target="_blank">mapi pile 交通标志数据集，用于在全球范围内进行检测和分类</a></p></figure><h2 id="0664" class="ms lw jj bd lx mt mu dn mb mv mw dp mf lh mx my mh ll mz na mj lp nb nc ml nd bi translated"><a class="ae jg" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680069.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak"/></a>对全球范围内的交通标志数据集进行检测和分类</h2><p id="da45" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">来自 Mapillary 的数据集</strong> — <em class="lu">在这项工作中，作者收集了可用于交通标志检测(where)和分类(what)的最大数据集。该数据集涵盖多个国家，包括来自 300 多个不同标志类别的 52k 完全注释图像。</em></p></div></div>    
</body>
</html>