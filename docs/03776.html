<html>
<head>
<title>Demystifying Support Vector Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-support-vector-machine-b04d202bf11e?source=collection_archive---------35-----------------------#2020-04-08">https://towardsdatascience.com/demystifying-support-vector-machine-b04d202bf11e?source=collection_archive---------35-----------------------#2020-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9280" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解基础数学的快速指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67b01a9dcbc4132d61a8c636797cd723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEXD2C_Ig7_x99RkzeNJ7g.png"/></div></div></figure><h1 id="eb22" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="f236" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督学习</a>描述了一类涉及使用模型来学习输入示例和目标变量之间的映射的问题。对于分类问题，目标变量可以是类标签，对于回归问题，目标变量可以是回归变量值。有些模型既适用于回归也适用于分类。我们将在这个博客中讨论的一个这样的模型是支持向量机，缩写为SVM。我的目的是简单明了地向你提供SVM的内部运作。</p><p id="c2f1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">事不宜迟，让我们开始吧。🚗</p><p id="1f91" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们假设我们正在处理一个二元分类任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/0a2653701267817d61d37ab93e094279.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*d5hdGTUcwm1OwxGj224TsA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图1</p></figure><p id="f0eb" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">可能有无限多的超平面可以将这两类分开。你可以选择其中任何一个。但是这个超平面能很好地预测新查询点的类别吗？你不认为离其中一个班级很近的飞机偏向另一个班级吗？直观上，区分两个类别的最佳方法是选择一个超平面，该超平面与任一类别中的最近点等距。<br/>这就是SVM所做的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a297b15bb0bf8df688f32861b60720c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*vnyJIS_VjznGWQFDxhpPrQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图2</p></figure><p id="ab68" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">SVM</strong>的核心思想:</p><p id="bc3d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">选择将+ve点与-ve点尽可能分开的超平面π。</p><p id="a916" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">设π是分开这两类的超平面，π₊和π₋是平行于π的两个超平面，使得</p><p id="adc9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₊是当我们平行于π移动时得到的平面，它与π最近的+ve点接触</p><p id="6ab4" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₋是我们平行于π移动时得到的平面，它接触到π的最近点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f647887a2e23b5b4b6f84b3d3884d7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*paaf4zXZ6W2prZbTBqbEqQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图3</p></figure><p id="6bc1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">d =差额= dist(π₊,π₋)</p><ul class=""><li id="9bde" class="mv mw it lo b lp mj ls mk lv mx lz my md mz mh na nb nc nd bi translated">SVM试图找到一个使利润最大化的π。</li><li id="3413" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">随着边距的增加，泛化精度也会增加。</li></ul><h2 id="0106" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">支持向量:</h2><p id="a035" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">位于π₊或π₋上的点称为支持向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c97287e5729c24e610a1eec8e71dca88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*3cu24belDfhls29AzlncIA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图4</p></figure><h2 id="15d6" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM的数学公式</h2><p id="a3a9" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">π:边际最大化超平面</p><p id="1155" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π: wᵀx+ b =0</p><p id="9c80" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">比方说，</p><p id="e3f1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₊ : wᵀx + b =+1</p><p id="b960" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₊上或正方向上远离它的任何一点都标为正</p><p id="ad28" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₋ : wᵀx + b =-1</p><p id="f0a5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">π₋上或负方向远离它的任何一点都标为负</p><p id="0f47" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在这种情况下，差额为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/306a6ae80a21a34d9faeda1d2c0d7b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*TMbmN9-KGpKeOMdUfqNlGg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图5</p></figure><p id="ec62" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">所以，我们的优化问题变成了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/510b413469b32a52466a5bf8f3000657.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*8RG-HNO7-qMkJRx1t05MfA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图6:硬利润SVM</p></figure><p id="56ac" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，这可能看起来不错，但是，它只适用于我们的数据是线性可分的。如果不是，那么我们就不能解决上面的优化问题，我们就不能得到最优的w和b。</p><p id="413a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们设想一个场景，其中数据不是线性可分的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4af39f4155c1e09a934e32639c86fd4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*z2yW7IIzdmELopC-P4N75A.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图7</p></figure><p id="29fd" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">由于这四点，我们的优化问题将永远得不到解决，因为这些点yᵢ(wᵀx+b不≥ 1。</p><p id="31e9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是因为我们的优化问题过于严格，它只解决线性可分的数据。这种方法被称为硬利润。</p><p id="8f0a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">那么，我们能修改它吗？我们能不能让它宽松一点，这样它就可以处理几乎线性可分的数据？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c08fa022a062efdb632fbc597fd0b105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*uUlqTdvSWEyTHq15n9QnvQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图8</p></figure><p id="5f23" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们所做的是使松弛变量<strong class="lo iu"> ζ </strong> ᵢ (zeta)对应于每个数据点，使得对于位于+ve区域的+ve点和位于-ve区域的-ve点，<br/>t5】ζᵢ= 0。</p><p id="e8d9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这给我们留下了错误分类的点和边缘内的点。</p><p id="0a1f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu"> ζ </strong> ᵢ的意思</p><p id="dab5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">以f <em class="ny">图7中的点4为例，</em> <strong class="lo iu"> ζ </strong> ᵢ = 2.5 <br/>这意味着在这种情况下点4从其正确的hyperplane(π₊向相反的方向偏离了2.5个单位)。</p><p id="1e47" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">类似地，在这种情况下，点1距离其正确的hyperplane(π₋2.5个单位)在相反的方向。</p><p id="a4a6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="ny">随着</em><strong class="lo iu"><em class="ny">ζ</em></strong><em class="ny">ᵢ的增大，该点在</em> <strong class="lo iu"> <em class="ny">方向更加远离正确的超平面</em> </strong> <em class="ny">方向。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8220b170d73e1cf9003654a22263e99f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*3Oen2aJEylycS0n9M3yGBw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图9</p></figure><p id="eae2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">修正优化问题</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f05a97d2cff104fd8ddf52041912f704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*uaZgWbKLI-jfzp0vl6bxIA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图10:SVM的原始形态</p></figure><p id="d0d3" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们来分解一下</p><p id="d1b8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">首先，约束条件:</p><p id="8545" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们知道，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/32c0f9f42658a849c3165121d344e74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*3nBTOlPVsOm7Z1HOzRMuRA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图11</p></figure><p id="33a5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/040978dc2239e1af24d94ba4fd5b3690.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*DnaNg3PclVXG1tlhdLWzMA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图12</p></figure><p id="e01d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9a287995359462f084cb8116819c8a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*MYBkSTHLOtQqNHmwASFmdw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图13</p></figure><p id="1340" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">c是超参数。</p><p id="0721" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们可以直观地把优化问题想成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/012050ae5f7e5a9a0365a2a6aa931f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*pX6CDgj-Q1uJoAnW9UoLxA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图14</p></figure><ul class=""><li id="0681" class="mv mw it lo b lp mj ls mk lv mx lz my md mz mh na nb nc nd bi translated">如果c较高，损失项将被赋予更大权重，这将导致数据过度拟合。</li><li id="d400" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">如果c较低，将给予正则化项更大权重，这将导致数据拟合不足。</li></ul><h2 id="54c6" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM的双重形式</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e9980bef020a328c71679387ef9054aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*J9ogrVhWBfUb4WAt1UTFDw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图15</p></figure><p id="aaf8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">等等！我们是如何得到这种双重形式的呢？</p><p id="0d40" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">嗯，这都与优化有关，如果我们深入其中，我们将偏离我们的目标。让我们在另一个博客中继续这个双重形式。</p><p id="196a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为什么我们需要这种双重形式？</p><p id="7ac2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对偶形式有时更容易解决，如果对偶间隙很小，我们会得到类似的结果。特别是对SVM来说:对偶形式非常重要，因为它开启了一种通过内核函数解释支持向量机的新方法(我将在本博客的后半部分告诉你)。</p><blockquote class="og oh oi"><p id="86b0" class="lm ln ny lo b lp mj ju lr ls mk jx lu oj ml lx ly ok mm mb mc ol mn mf mg mh im bi translated">注意:在对偶形式中，所有的xᵢ都以点积的形式出现，不像原始形式中所有的xᵢ都以独立的点出现。</p></blockquote><p id="0b03" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">αᵢ可以被认为是拉格朗日乘数</p><p id="c09b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">从双重形式观察:</p><ul class=""><li id="3c61" class="mv mw it lo b lp mj ls mk lv mx lz my md mz mh na nb nc nd bi translated">对于每一个xᵢ，都有相应的αᵢ</li><li id="7a28" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">所有xᵢ都以点积的形式出现</li><li id="4c1d" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">我们对新点的分类方式发生了变化:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/85310094c32cfcb5939f93eaadb72fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*jOMol6WsE8rFbk_9-EQ2qA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图16</p></figure><ul class=""><li id="ed55" class="mv mw it lo b lp mj ls mk lv mx lz my md mz mh na nb nc nd bi translated">支持向量的αᵢ &gt; 0，非支持向量的αᵢ =0。</li></ul><p id="17eb" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这意味着只有点才是重要的支持向量，这就是将该模型命名为<em class="ny">支持向量机</em>的原因。</p><p id="7fde" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，以双重形式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a073e57dc0a474e8bb8b720caacc41ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*u9cW0BxZbzEFM2EzFhnE-g.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图17</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a4dbc3c106cac5233ae0b6c966d2c79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*7hKeme1HycoXj0uoR-_R6g.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图18</p></figure><p id="c23a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，如果给定一个相似矩阵，我们可以使用对偶形式而不是原始形式。这就是SVM的魅力所在。通常，这个similarity(xᵢ,xⱼ)被K(xᵢ,xⱼ)代替，其中k被认为是核函数。</p><h2 id="c0f9" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">内核技巧及其背后的直觉</h2><p id="1678" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">用内核函数K(xᵢ,xⱼ代替similarity(xᵢ,xⱼ)被称为<em class="ny">内核化</em>，或者应用<em class="ny">内核技巧</em>。</p><p id="423a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你看看，无非就是计算xᵢ和xⱼ.的点积那么，它有什么了不起的呢？</p><p id="2940" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们以下面的数据集为例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/4b912c9eaf151b08cf129d4ce556d05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IlJTPazPFd7DbrMoAJ9W6Q.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图19</p></figure><p id="94bf" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">很明显，这两个类别不能用线性模型来区分。</p><p id="a831" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，用特征<x>将数据集转换成3D，看看神奇之处。</x></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/060ad18ad1b88deaeb4a157bad7a064b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5mS3hpBIX_smZBx7DMuEQ.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图20</p></figure><p id="f996" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你看到了吗？应用适当的特征变换和增加维数使我们的数据线性可分。</p><p id="e292" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这就是内核SVM所做的。它将原始特征映射到一个更高维度的空间中，在该空间中找到边际最大化超平面，并将该超平面映射到原始维度空间中，以获得非线性决策表面<strong class="lo iu">，而无需实际访问</strong>该更高维度的空间。</p><p id="d205" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">所以，</p><p id="c932" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">线性支持向量机:在xᵢ's空间中寻找边缘最大化超平面</p><p id="44c0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">核SVM:在xᵢ's的<em class="ny">变换空间</em>中寻找边缘最大化超平面</p><p id="8fee" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，核SVM也能够解决非线性可分数据集。</p><p id="b7b6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们来看看SVM使用的一些玉米粒</p><h2 id="993c" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">多项式核</h2><p id="fa9a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/fa14531bf1cf34f19ae29044d19be9dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*CPynxYKWWKzOT0QThIpRfw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图21</p></figure><p id="e798" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在哪里</p><p id="e488" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">d=数据的维度</p><p id="19fe" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">c =常数</p><p id="fc8d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对于二次核，设c=1，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/cc6ebb5d1d3b8fd31df800bed15182a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*uv04nN01MGBFAoQWVV1bcw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图22</p></figure><p id="5578" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">维度= 2</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7c967cc10695727ade5c6fa4926834f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*MvUo4cnKkBU_kQVg8xg0gw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图23</p></figure><p id="fcea" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这可以被认为是两个向量x₁'和x₂'的乘积，其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c0f52fd8f61343320e75036430901dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*arS-7LmHKgnGQwdah6w0mg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图24</p></figure><p id="8926" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">新维度= d' = 6</p><p id="2a06" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，内核化与特征转换是相同的，但通常是在内部隐式完成的。</p><h2 id="8bfe" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">径向基函数(RBF核)</h2><p id="9d1d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">它是最受欢迎的内核，当你不知道选择哪个内核时，你可以选择这个😉</p><p id="79ec" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d9a73665f7896b031f4dc7b078088a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*ACHXSd7vwyL4OI0YTPcpVQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图25</p></figure><p id="45da" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="ny">d的效果:</em></p><p id="08a0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">当d增大时，指数部分的分子减小，或者换句话说，K值或相似度减小。</p><p id="5667" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">σ的影响:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/9140c342371f75c12b788b55c2a1f298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ob45sQNfLJ9EYgoP0bUXWw.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图26:来源Google</p></figure><p id="baad" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果您注意到，当σ = 0.3时，图表在x=2时接近0。当σ =1时，x=4时接近0，当σ=10时，x=11。这表明，当σ增加时，即使两个点相距很远，它们的相似性得分也会大于0。</p><p id="cca1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">没明白吗？</p><p id="bcdc" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">假设有两个点，x₁和x₂，距离为4个单位。如果我们应用σ = 0.3的RBF核，则核函数K值或相似性值将为0。如果σ = 1，K值将非常接近0，但如果σ = 10，K值将约为0.4左右。</p><p id="c004" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，RBF内核背后的直觉-</p><p id="6236" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">还记得我们在使用多项式内核时得到的6维映射函数吗(参见图24)？现在让我们试着找出一个RBF核的映射函数。</p><p id="ee78" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为了简化数学，假设数据的原始维数为2，指数部分的分母等于1。在这种情况下，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/6c2c743f780909a2f768acd472f892d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pENJJ4HzjUBW7tYxMAdJ_w.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图27</p></figure><p id="0a65" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果我们试图找出RBF核的映射函数，这将把我们引向一个无限向量。这意味着RBF核将我们的数据带入<strong class="lo iu">无限维空间</strong>，计算相似性得分并返回它，这就是为什么如果你无法确定选择哪个核，RBF核将是最安全的选择。😎</p><h2 id="01be" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM回归</h2><p id="734d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">到目前为止，我们已经了解了如何使用SVM执行分类任务。但是SVM不仅仅局限于此。它也可以用来执行回归任务。怎么会？我们去看看吧！</p><p id="1849" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">首先，让我们看看支持向量回归机(SVR)的数学公式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/ea5fd08b60d838ac975c2a9b3a6f1e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*81zf5Shs0GEFYVe-a358YA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图28</p></figure><p id="1074" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">放心吧！我会为你打破它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1df6b98e5988ccc6ef9a0634f8cd30df.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*RU0tv9MsW6TnVIV6gSnOVw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图29</p></figure><p id="415e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">SVR的工作方式是，它试图找到一个最适合数据点的超平面，同时保持容限= ε(超参数)，这意味着所有点都应该在超平面两侧的ε距离内。</p><p id="0cad" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这里最小化裕量意味着我们要找到一个超平面，它以低容限来拟合数据。</p><blockquote class="og oh oi"><p id="1d5f" class="lm ln ny lo b lp mj ju lr ls mk jx lu oj ml lx ly ok mm mb mc ol mn mf mg mh im bi translated">注意:平方是为了使函数变得可微，并适于优化。人们可以对SVC做同样的事情。</p></blockquote><p id="7643" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="ny">这个提法有什么问题？</em></p><p id="136c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这里的问题是我们太严格了，我们期望这些点位于超平面的ε距离内，这在现实世界中并不经常发生。在这种情况下，我们将无法找到所需的超平面。那么，我们该如何着手呢？</p><p id="8814" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">与我们在SVC中处理该问题的方式相同，我们将引入两个松弛变量<strong class="lo iu">ζ(ζ)</strong>用于法线方向的点，引入<strong class="lo iu">ζ*(ζ星)</strong>用于法线相反方向的点，以允许一些点位于容差范围之外，但会有损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ebe62dd3a5755bd44cc994997d6d197c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*Hk0OWEnMqu8faqVeLT1pBw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图30</p></figure><p id="3bf9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">所以数学函数现在变成了:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/0950bcf8dd9606f0d2648239737a353e.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*Waix1iPn9W-Hd2_9jp3Zyg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图31:SVR的原始形式</p></figure><p id="1189" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">其中C决定了所需的严格程度。C值越大，超出边界的点受到的惩罚越多，这可能导致数据过度拟合。C值越小，对超出边界的点的惩罚就越小，这可能会导致数据拟合不足。</p><p id="0bdf" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">就像在SVC中一样，图31显示了SVR的原始形式。已经观察到对偶形式更容易解决，并且我们也可以使用核技巧来寻找非线性超平面。</p><blockquote class="og oh oi"><p id="5f3e" class="lm ln ny lo b lp mj ju lr ls mk jx lu oj ml lx ly ok mm mb mc ol mn mf mg mh im bi translated">正如我已经说过的，对偶形式的表述有点棘手，涉及到解决约束优化问题的知识。我们不会谈论那么多细节，因为那会转移我们对SVM的注意力。</p></blockquote><h2 id="2ff4" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">支持向量回归机的对偶形式</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/cbd75ae5ff92e22b7799dd2850473790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgFQOAp6SwzcLN5w-nia8g.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图32:双重形式的SVR</p></figure><p id="aab0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">不好意思！我不是故意吓你的。这是通过使用拉格朗日乘子来解决优化问题而获得的。</p><p id="e372" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对于一个新的点，我们计算产值的方法是:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/9698fc05c12d27540289b02e748e3316.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*Vb6XWQGkxihOw5hcb_Fm9A.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图33:SVR的函数值</p></figure><p id="c776" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你注意到xᵢ以点积的形式出现了吗？是的，和我们在SVC中得到的一样。我们可以用我上面已经提到的相似度或核函数来代替这个点积。应用内核技巧也有助于我们拟合非线性数据。</p><h2 id="a792" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM的案例</h2><ol class=""><li id="b62a" class="mv mw it lo b lp lq ls lt lv pe lz pf md pg mh ph nb nc nd bi translated"><em class="ny">特征工程和特征转换</em></li></ol><p id="0b9f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是通过如上所述找到正确的内核来隐式完成的</p><p id="aa53" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">2.<em class="ny">决策面</em></p><p id="8131" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对于线性SVM:决策面只是一个超平面</p><p id="0310" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">对于内核SVM:这将是一个非线性的表面</p><p id="c208" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">3.<em class="ny">相似度函数/距离函数</em></p><p id="5cb5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">SVM的原始形式不处理相似性函数。然而，由于点积形式的xᵢ的存在，对偶形式可以容易地处理它。</p><p id="fc61" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">4.<em class="ny">特征重要性</em></p><p id="7695" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果特征不共线，则权重向量w中的特征权重确定特征重要性。在特征共线的情况下，可以使用前向特征选择或后向特征消除，这是确定任何模型的特征重要性的标准方式。</p><p id="8b91" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">5.<em class="ny">离群值</em></p><p id="836b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它们确实影响SVM，但是，与其他模型如逻辑回归相比，影响较小。</p><p id="5330" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">6.<em class="ny">偏差-方差</em></p><p id="d670" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它取决于SVM对偶形式中c的值。</p><p id="0d5f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果c较高，将给予误差项更大的权重，因此模型可能过拟合数据，如果c较小，将给予误差项更小的权重，并且模型可能欠拟合数据。</p><p id="d7d2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">7.高维度</p><p id="468c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">支持向量机被设计成即使在高维空间也能很好地工作。参考图14，该图显示了SVM的数学公式中已经存在一个<em class="ny">正则项</em>，这有助于处理高维度。你可能会说像KNN这样的其他模型在高维情况下不太适用，那么支持向量机有什么特别之处呢？这是因为SVM只关心找到最大限度的平面，它不关心点之间的相对距离。</p><h2 id="2e0f" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM的优势</h2><ul class=""><li id="6b1c" class="mv mw it lo b lp lq ls lt lv pe lz pf md pg mh na nb nc nd bi translated">它有一个正则项，有助于避免数据的过度拟合</li><li id="3dc6" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">它使用内核技巧，帮助处理非线性数据(在SVR的情况下)和非线性可分离数据(在SVC的情况下)</li><li id="0e4e" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">当我们不知道数据的时候，SVM是非常好的。</li><li id="79df" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">适用于文本、图像和树等非结构化和半结构化数据。</li><li id="47f8" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">即使当训练样本包含错误时，它也能工作，从这个意义上说，它是健壮的</li></ul><h2 id="75b3" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">SVM的局限性</h2><ul class=""><li id="f7b5" class="mv mw it lo b lp lq ls lt lv pe lz pf md pg mh na nb nc nd bi translated">选择正确的内核是一项艰巨的任务。</li><li id="fb07" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">SVM算法有几个需要正确设置的超参数，以实现任何给定问题的最佳分类结果。可能导致问题<em class="ny"> A </em>的分类精度极佳的参数，可能导致问题<em class="ny"> B </em>的分类精度不佳。</li><li id="be88" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">当数据点数量很大时，训练SVM需要很长时间。</li><li id="4020" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">对于核支持向量机，很难解释权重向量(w)。</li></ul><h2 id="f9cd" class="nj kv it bd kw nk nl dn la nm nn dp le lv no np lg lz nq nr li md ns nt lk nu bi translated">参考</h2><ul class=""><li id="020e" class="mv mw it lo b lp lq ls lt lv pe lz pf md pg mh na nb nc nd bi translated"><a class="ae mi" href="https://alex.smola.org/papers/2004/SmoSch04.pdf" rel="noopener ugc nofollow" target="_blank">https://alex.smola.org/papers/2004/SmoSch04.pdf</a></li><li id="2f0b" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">https://www.saedsayad.com/support_vector_machine_reg.htm<a class="ae mi" href="https://www.saedsayad.com/support_vector_machine_reg.htm" rel="noopener ugc nofollow" target="_blank"/></li><li id="55af" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" href="https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/" rel="noopener ugc nofollow" target="_blank">https://statin fer . com/204-6-8-SVM-优势-劣势-应用/ </a></li><li id="9c22" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" href="http://www.cs.uky.edu/~jzhang/CS689/PPDM-Chapter2.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.uky.edu/~jzhang/CS689/PPDM-Chapter2.pdf</a></li></ul></div></div>    
</body>
</html>