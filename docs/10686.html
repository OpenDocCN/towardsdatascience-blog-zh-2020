<html>
<head>
<title>Why Deep Learning Uses GPUs?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习为什么要用 GPU？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-deep-learning-uses-gpus-c61b399e93a0?source=collection_archive---------21-----------------------#2020-07-26">https://towardsdatascience.com/why-deep-learning-uses-gpus-c61b399e93a0?source=collection_archive---------21-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6795" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么你也应该…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5cec657e17f3b345d3760ea81e20fa1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c-RCBJPU7VzNVyTh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约瑟夫·格雷夫在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> T </span>这里有很多关于深度学习 GPU 的信息。你可能听说过这个领域需要一些巨大的计算机和难以置信的能力。也许你见过人们花几天甚至几周的时间训练他们的模型，然后发现他们的代码中有一个 bug。此外，我们在谈论游戏和图形设计时最常听到 GPU。这篇文章将帮助你了解这里实际发生了什么，以及为什么 Nvidia 是深度学习领域的巨大创新者。</p><h1 id="fa24" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">图形处理单元</h1><p id="5da2" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">GPU 是一种非常擅长处理专门计算的处理器。我们可以将其与中央处理器(CPU)进行对比，后者在处理一般的计算方面表现出色。CPU 为我们日常使用的设备上执行的大部分计算提供动力。</p><p id="44e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPU 可以比 CPU 更快地完成任务。然而，并不是每种情况都是如此。性能在很大程度上取决于所执行的计算类型。GPU 非常擅长可以并行运行的任务。</p><h1 id="7554" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">并行计算</h1><blockquote class="nc nd ne"><p id="78f5" class="kz la nb lb b lc ld ju le lf lg jx lh nf lj lk ll ng ln lo lp nh lr ls lt lu im bi translated"><strong class="lb iu">并行计算</strong>是一种计算架构，其中几个处理器同时执行多个较小的计算，这些计算是从一个整体较大的复杂问题中分解出来的。</p></blockquote><p id="f5ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们的处理单元中有多个内核，我们可以将任务分成多个更小的任务，并同时运行它们。这将利用我们现有的处理能力，更快地完成我们的任务。</p><p id="bc78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CPU 通常有四个、八个或十六个，而 GPU 可能有<strong class="lb iu">千个</strong>！从这里我们可以得出结论，GPU 最适合可以同时完成的任务。由于并行计算处理这样的任务，我们可以看到为什么在这种情况下会使用 GPU。</p><h1 id="5498" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">神经网络是并行的</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/7f2e85825575a964ff690f0bfde09476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQbtTgh78jnbZ6oTEeO1GA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由 Brighterion 提供)</p></figure><p id="c2d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经得出结论，当一个巨大的任务可以被分解成许多更小的任务时，GPU 是最好的选择，这就是 GPU 被用于并行计算的原因。如果我们看一看神经网络，我们可以注意到它们是令人尴尬的并行<em class="nb">。这意味着我们甚至不需要划分任务，也不需要决定哪个部分属于哪个核心。神经网络是专门为并行运行而设计的。由于它们是深度学习的基础，我们可以得出结论，GPU 非常适合这项任务。</em></p><p id="39e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，神经网络是并行的，因此它们不必依赖彼此的结果。一切都可以同时运行，而不必等待其他内核。这种高度独立的计算的一个例子是卷积。</p><h1 id="6e21" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">盘旋</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(动画由以色列维卡制作)</p></figure><p id="b50f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个没有数字的卷积的例子。左边是输入通道，右边是输出通道。在动画中，计算的过程是按顺序进行的，而现实生活中并非如此。实际上，所有的操作都可能同时发生，它们都不依赖于任何其他计算的结果。</p><p id="f0ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，计算可以在 GPU 上并行进行，并且可以产生结果。从例子中我们可以看到，并行计算和 GPU 可以大大加速卷积运算。相比之下，在 CPU 上运行相同的卷积将导致顺序执行，类似于动画中的情况。这个过程需要更多的时间。</p><h1 id="4825" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">Nvidia 硬件和软件</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/347ba7be23afd6bd6cbdec62b969d7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*XvSaFnJDAG4eqDdWdAi1Og.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片来自 Nvidia.com)</p></figure><p id="340d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们可以了解 CUDA 的地方。Nvidia 是一家生产 GPU 的公司，他们创造了 CUDA，这是一种可以很好地连接到他们生产的硬件的软件。该软件允许开发人员轻松利用 Nvidia GPUs 的并行计算能力。</p><p id="78d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单来说，<strong class="lb iu"> GPU 是硬件，CUDA 是软件</strong>。正如你可能已经猜到的，使用 CUDA 需要 Nvidia GPU，CUDA 可以从 Nvidia 的网站上完全免费下载。</p><h1 id="de4e" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">GPU vs CPU</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/964f5f6c0377cfb96c6d33c4d1e7e317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L59FWBdmJ0aGpxfGP6Q7rg@2x.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片来自 Nvidia.com)</p></figure><p id="f607" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CUDA 允许我们有选择地在 GPU 或 CPU 上运行计算。如果 GPU 好得多，为什么不把它用在所有事情上呢？</p><p id="6684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答案是 GPU 只对特定类型的任务更好。例如，如果我们的数据在 CPU 上，将其移动到 GPU 可能会很昂贵。所以在任务相当简单的情况下，使用 GPU 的成本会更高。我们可以得出结论，只有当任务足够大并且可以被分解为更小的任务时，GPU 才会表现得更好。对于小流程，只会让事情变得更慢。</p><p id="5c21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于这个原因，在刚开始时使用 CPU 通常是可以接受的，因为最初的任务会很短很简单。</p><h1 id="eab3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">GPGPU</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(英伟达中国 GTC)</p></figure><p id="071b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当 GPU 最初被创建时，它主要是为了处理计算机图形，这就是这个名字的由来。现在，越来越多的任务转移到 GPU，Nvidia 是这一领域的先驱。CUDA 是近 10 年前创建的，直到现在开发人员才开始使用它。</p><p id="94a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习和其他类型的并行计算导致了一个新领域的发展，称为<em class="nb"> GPGPU </em>或<em class="nb">通用 GPU 计算</em>。</p><p id="c9ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Nvidia 的 GTC 演讲是深度学习领域每个人的必看之作。当我们听到 GPU 计算栈的时候，我们应该把 GPU 想成是底层的硬件，中间是 CUDA 软件架构，顶层是 cuDNN 这样的库。</p><h1 id="0f85" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="fc29" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">GPU 在当前深度学习和并行计算的发展中发挥着巨大的作用。凭借所有这些发展，Nvidia 公司无疑是该领域的先锋和领导者。它为创作者提供硬件和软件。</p><p id="2fe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只用一个 CPU 就开始创建神经网络当然没问题。然而，现代 GPU 可以加快任务的速度，并使学习过程变得更加愉快。</p><h1 id="2ce4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">资源:</h1><p id="06e8" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">[1]以色列牧师。(2018 年 5 月 23 日)。<em class="nb">奥塔维奥·古德的卷积神经网络可视化</em>[视频]。YouTube。【https://youtu.be/f0t-OCG79-U T2】号</p><p id="e605" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]英伟达。(2016 年 9 月 26 日)。<em class="nb">中国 GTC:AI，深度学习与黄仁勋&amp;百度的吴恩达</em>【视频】。YouTube。<a class="ae ky" href="https://youtu.be/FPM3nmlaN00" rel="noopener ugc nofollow" target="_blank">https://youtu.be/zeSIXD6y3WQ</a></p><p id="6573" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]并行计算定义。(未注明)。检索自<a class="ae ky" href="https://www.omnisci.com/technical-glossary/parallel-computing" rel="noopener ugc nofollow" target="_blank">https://www . omni sci . com/technical-glossary/parallel-computing</a></p></div></div>    
</body>
</html>