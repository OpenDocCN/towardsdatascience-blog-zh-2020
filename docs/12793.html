<html>
<head>
<title>An Intuitive Guide to kNN with Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">kNN 及其实现的直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-guide-to-knn-with-implementation-fc100bf29a6f?source=collection_archive---------46-----------------------#2020-09-02">https://towardsdatascience.com/an-intuitive-guide-to-knn-with-implementation-fc100bf29a6f?source=collection_archive---------46-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="82af" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个载体被它所保持的公司所知</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f6bfe503114ba894a833832842b9f753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YLderfeKEoEwre5i"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:Unsplash(照片由<a class="ae ky" href="https://unsplash.com/@jontyson" rel="noopener ugc nofollow" target="_blank">乔恩·泰森</a>拍摄)</p></figure><p id="ae8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">kNN 是最简单的分类算法之一，因此仍然是社区的“宠儿”之一。有不少关于流行的机器学习算法的调查，kNN <strong class="lb iu">很少能被排除在前 10 名之外。你可以在 KNN 查看我们的视频<a class="ae ky" href="https://www.youtube.com/watch?v=xx7FIZrtnM0" rel="noopener ugc nofollow" target="_blank">这里</a>和我们的代码<a class="ae ky" href="https://www.kaggle.com/saptarsi/k-nn-sg" rel="noopener ugc nofollow" target="_blank">这里</a>。</strong></p><p id="5483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先来看直觉。在任何班级，当有多个学科的学生参加时，都有同一个学科的学生坐在一起的倾向。kNN 利用了这个想法。</p><p id="c9dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从一个例子开始，来理解它是如何工作的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/85111df434123c6c306eba1f45578474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*cblfzhXTrlKBFirdNKq_Tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:KNN 插图(来源:作者)</p></figure><blockquote class="lw lx ly"><p id="2184" class="kz la lz lb b lc ld ju le lf lg jx lh ma lj lk ll mb ln lo lp mc lr ls lt lu im bi translated">有三种鸟。任务是找到问号标记的新鸟的类型(类)。</p></blockquote><p id="8ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察它的三个最近的邻居(被圈起来的)，观察到大多数邻居是鸭子(3 个中的 2 个)，并且<strong class="lb iu">断定这个未标记的观察结果是鸭子</strong>。在这个例子中，我们已经查看了三个最近的邻居，因此 k 的值被取为 3。</p><blockquote class="lw lx ly"><p id="af68" class="kz la lz lb b lc ld ju le lf lg jx lh ma lj lk ll mb ln lo lp mc lr ls lt lu im bi translated">物以类聚</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0d3ed4d537f930933f5f55e7ee6abcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*971qs0LTYuawJYNd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:Unsplash Photo by<a class="ae ky" href="https://unsplash.com/@fungaifoto" rel="noopener ugc nofollow" target="_blank">Fungai Tichawangana</a></p></figure><p id="902d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">因此，有两个设计决策需要考虑:</strong></p><ul class=""><li id="39a2" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">需要考虑多少邻居？</li></ul><p id="6c63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们保持一个小值，它将识别局部模式，在某些情况下也可能拾取噪声。一个很高的 k 值，它会看全局格局。在最坏的情况下，它的行为类似于朴素贝叶斯的先验概率。<strong class="lb iu">无论数据位于何处，它都会将其分配给多数类。</strong></p><ul class=""><li id="1132" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">将使用哪种距离度量？</li></ul><p id="4c45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以是欧几里德、曼哈顿、汉明距离、余弦相似度的倒数等。有兴趣的读者可以看看参考文献 1，一本只讲距离度量的书。</p><p id="d67e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施:</strong></p><p id="f107" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码足以导入和实例化分类器。</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="b2da" class="mr ms it mn b gy mt mu l mv mw">from sklearn.neighbors import KNeighborsClassifier<br/><em class="lz">#Initalize the classifier</em><br/>knn = KNeighborsClassifier(n_neighbors=15)</span></pre><p id="a5ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">scikit 中 kNN 的重要参数有哪些？</strong></p><ul class=""><li id="6595" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated"><strong class="lb iu"> n_neighbors: </strong>与‘k’含义相同，默认值为 5</li><li id="3ea4" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated"><strong class="lb iu">权重:</strong>可能的值有均匀和距离。默认情况下，这是统一的，当您使用距离时，所有邻居都有相同的投票权重，这意味着较近的邻居比较远的邻居有更多的权重。</li><li id="7af3" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated"><strong class="lb iu">算法:</strong>最好的选择是使用“自动”，在这一步中计算距离矩阵，这是 kNN 中计算量最大的部分。</li><li id="a613" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated"><strong class="lb iu"> p </strong>:如果 p =2，则为欧几里德距离，如果 1，则为曼哈顿距离，这适用于度量为<strong class="lb iu"> <em class="lz">明可夫斯基的情况。</em> </strong> <em class="lz">方程给出了两个 p 维向量 x1 和 x2。</em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/bbadcd8faa904f86ffb10990024dc621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*J-rIzzYXmvWqAv5L6PxxQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nd">闵可夫斯基距离(来源:作者)</strong></p></figure><ul class=""><li id="e9ea" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated"><strong class="lb iu"> metric: </strong>默认为<strong class="lb iu"> <em class="lz"> Minkowski，</em> </strong> <em class="lz">基于两个向量的数据类型有很多选项，下表列出了其中一些。</em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/0ac69bfac87d9a357f2a8a33730d8bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*XGDBE-yLGHovXKQGdnBfUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:基于数据类型的不同距离度量(来源:作者)</p></figure><p id="5e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一个选项来创建您自己的距离度量，但那是一个单独的故事。</p><p id="9b45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如何确定‘k’的最优值？</strong></p><p id="9e7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最简单的解决方案是采用不同的“k”值并运行算法，检查测试集的准确性，当我们看到饱和时，我们可以停止。</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="a90d" class="mr ms it mn b gy mt mu l mv mw"><strong class="mn iu"># Initialize an array with diffrent choices of 'k'</strong><br/>nc=np.arange(1,100,2)<br/>#<strong class="mn iu">Creating an array to store the accuracy values</strong><br/>acc=np.empty(50)<br/>i=0<br/>for k in np.nditer(nc):<br/>    <strong class="mn iu">#Initializing kNN with diffrent values of K<br/>    </strong>knn = KNeighborsClassifier(n_neighbors=int(k))<br/>    knn.fit(x_train, y_train)<br/>    <strong class="mn iu">#Finding the testing set accuracy </strong>   <br/>    acc[i]== knn.score(x_test, y_test)<br/>    i = i + 1</span></pre><p id="253c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，绘制不同“k”值的精确度</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="2d95" class="mr ms it mn b gy mt mu l mv mw">x=pd.Series(acc,index=nc)<br/>x.plot()<br/><em class="lz"># Add title and axis names</em><br/>plt.title('Neighbor vs Accuracy')<br/>plt.xlabel('Count of Neighbor')<br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/26a65e928b39494917ddf9e2f59ea846.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/0*4ygNux9e0d6SvhHH.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:确定“k”的最佳值(来源:作者)</p></figure><p id="54a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以停在一个很小的“k”值上，如图 3 所示，可能在 6，7 左右。</p><p id="678f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">为什么基于距离的算法需要缩放？</strong></p><p id="70ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再举一个例子。假设，我们有三个客户的数据，我们有两个属性年龄和工资。三个向量，每个向量都是二维的。我们必须确定谁是 C1 最近的邻居。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/6d9c56643ac1df3eb1a5c2324a4db89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaBCJUphVYu-PmkG8MsDZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:缩放的需要(图片来源:作者)</p></figure><p id="6990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们计算一下，我们会发现两者的距离都是 1，因此它们是等距离的。然而，薪水的 1 Rs 的变化和年龄的 1 Rs 的变化是不等价的，并且由于薪水的范围与年龄相比会相当高，所以在欧几里德距离计算中，它会窒息或支配年龄。</p><p id="d3f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使它们达到相同范围的一种标准技术称为归一化或最小-最大缩放，由等式 2 给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/647441404a861820aa02e9315d2a6085.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*_uJGC7z2y8Qx-72fHg_iFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 2:标准化</p></figure><p id="fcf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面附上一张虚构的有年龄的插图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/be219e9de07a8dddefc0d91dc38a5929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*u93-AI5jpdK2Mv351aPIRg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:最小-最大缩放的图示(来源:作者)</p></figure><p id="4380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，所有值都映射在 0 到 1 之间，最小值映射到 0，最大值映射到 1。</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="9a9c" class="mr ms it mn b gy mt mu l mv mw"><strong class="mn iu"># Impoting minmaxscaler</strong><br/>from sklearn.preprocessing import MinMaxScaler <br/><strong class="mn iu">#Initilization</strong><br/>scaler = MinMaxScaler(feature_range=(0, 1)) <br/><strong class="mn iu">#Transforming the value</strong><br/>x_scaled = scaler.fit_transform(x)</span></pre><p id="81d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">关键备注:</strong></p><ul class=""><li id="fe1c" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">kNN 是一个非常简单的算法，并且没有对数据分布做任何假设，因此被称为<strong class="lb iu">非参数</strong>。</li><li id="c1f4" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated">确定“k”并不容易。</li><li id="d044" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated">随着维度数量的增加，许多距离度量都不再适用。</li><li id="5daa" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated">最后，<strong class="lb iu">邻域矩阵的计算非常昂贵</strong>。</li><li id="cff3" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated">可以将<strong class="lb iu">简单地扩展到回归</strong>，其中目标变量的值将是其 k 个最近邻居的平均值。</li><li id="a3a3" class="md me it lb b lc mx lf my li mz lm na lq nb lu mi mj mk ml bi translated">它还能容忍输入数据的变化，因为它只在被要求分类时才开始计算。它通常被称为懒惰的学习者。</li></ul><p id="c18d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考:</strong></p><p id="0142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]德扎 MM，德扎 e .距离百科。《距离百科全书 2009》(第 1-583 页)。斯普林格，柏林，海德堡。</p><p id="3e49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" rel="noopener" target="_blank" href="/knn-k-nearest-neighbors-1-a4707b24bd1d">https://towards data science . com/KNN-k-nearest-neighbors-1-a 4707 b 24 BD 1d</a></p></div></div>    
</body>
</html>