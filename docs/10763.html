<html>
<head>
<title>Understanding Text Vectorizations II: How TF-IDF Gives Your Simple Models the Power to Rival the Advanced Ones</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解文本矢量化 II:TF-IDF 如何让您的简单模型拥有与高级模型匹敌的能力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-text-vectorizations-ii-how-tf-idf-gives-your-simple-models-the-power-to-rival-the-79b6c975d7eb?source=collection_archive---------53-----------------------#2020-07-27">https://towardsdatascience.com/understanding-text-vectorizations-ii-how-tf-idf-gives-your-simple-models-the-power-to-rival-the-79b6c975d7eb?source=collection_archive---------53-----------------------#2020-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="983b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="e274" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Sklearn 管道、SHAP 和面向对象编程在情感分析中的应用</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0c10f5d4843283e86f07eeb8b8f674a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*peORPDTQ33QSV0NaBbT4DQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@itfeelslikefilm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">🇸🇮·扬科·菲利</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c3e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me"> L </em> </strong>让我们继续我们的情感分析之旅。</p><p id="e04f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">记得上次我们谈到使用单词袋模型来检测评论文本的情绪，我们已经有了一个相对较好的性能。今天，我们将在已有成果的基础上，用智能加权方案升级单词袋模型。在本帖中，我们将利用来自<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a">第 1 部分</a>的自定义管道<code class="fe mf mg mh mi b">StreamlinedModel</code>对象，并见证从逻辑回归等简单模型应用 TF-IDF 转换器所获得的惊人改进。</p><h2 id="01cb" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated"><strong class="ak">词频—逆文档频率(TF-IDF) </strong></h2><p id="becd" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">我们已经知道计算词频可以帮助我们判断评论文本的情感，但是这种方法忽略了单词在整个文档中的重要性。我们讨论了使用智能加权方案来解决这个问题，但是我们具体如何执行呢？</p><h2 id="95e0" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">大意</h2><p id="1480" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">答案是使用一种加权方案，该方案与该单词在所有文档中出现的频率成反比。如果这个词出现在几乎每一篇文档中(比如“<strong class="lk jd"> <em class="me">我</em> </strong>”、“<strong class="lk jd"> <em class="me">你</em> </strong>”或者“<strong class="lk jd"> <em class="me">他们</em> </strong>”)，那么如果我们对它们进行同等的加权，很可能会被视为非常重要。因此，我们想增加那些不经常出现的单词的权重。例如，使用相同的 3 篇评论</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="bfd3" class="mj mk it mi b gy nk nl l nm nn">I love dogs, I think they have adorable personalities.<br/>I don't like cats<br/>My favorite kind of pet is bird</span></pre><p id="7eb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到“<strong class="lk jd"> <em class="me">萌</em> </strong>”只出现在 3 个文档中的 1 个，文档频率为 3/(1+1)。我们在分母上加 1 以避免它变成 0。然后，我们将获取文档频率的自然对数值，这将为我们提供以下内容</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="988c" class="mj mk it mi b gy nk nl l nm nn">idf(adorable) = log(3/2) = 0.176</span></pre><p id="7cd8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与在 3 个文档中的 2 个中出现的类似于“<strong class="lk jd"> <em class="me"> I </em> </strong>的单词相比，我们将得到如下的逆文档频率</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="d1be" class="mj mk it mi b gy nk nl l nm nn">idf(I) = log(3/3) = 0</span></pre><p id="4376" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们将这袋字数乘以这个权重，得到 tf-idf 值。具体来说，</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="dbbb" class="mj mk it mi b gy nk nl l nm nn">tf-idf(t, d) = tf(t, d) x log(N/(df + 1))</span></pre><p id="92df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们了解了 TF-IDF 的大致思想，让我们看看我们应该如何将这个模型实现为一个转换器。</p><h2 id="5503" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">TF-IDF 实施</h2><p id="216d" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">我们将经历与<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a">第一部分</a>中概述的相同的文本预处理步骤。由于术语频率定义与单词袋模型相同，我们将关注如何有效地计算所有单词的逆文档频率。</p><p id="3a2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为我们必须检查每个单词是否存在于每个文档中，所以运行时间可能是 O(n)，这将相当慢。我们可以将预计算的 IDF 保存在属性化的类中，并在计算词频时引用它。</p><p id="7577" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们从获取所有单词的文档频率的辅助方法开始。在 pandas <code class="fe mf mg mh mi b">.str.contains</code>方法的帮助下，该方法将检测一列字符串中部分字符串的存在并返回一系列 0/1，我们可以对这些系列求和以获得单词的文档频率。然后，我们将重复相同的过程，并将结果存储在字典中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="af21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将在转换器的<code class="fe mf mg mh mi b">.fit</code>方法中使用<code class="fe mf mg mh mi b">get_document_frequency</code>方法，对每个单词的文档频率应用对数转换。得到的 idf 值将被保存为一个类属性，当我们计算词频时，它将被用作权重。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="56b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的<code class="fe mf mg mh mi b">.transform</code>方法将由两部分组成。除了计算词频(在<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a"> part 1 </a>中也有)，我们还会把它们乘以对应的 IDF 值。因为对于每个评论，我们将具有相同的 IDF 权重(因为单词索引顺序是固定的)，所以我们将重复 IDF 向量 N 次(等于评论的数量),以使 IDF 矩阵具有与单词频率矩阵相同的形状。最后，我们将在两个矩阵之间执行元素级乘法，以获得所有评论的 TF-IDF 值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="f8e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们现在准备将 tf-idf 用作变压器。完整的实现如下所示。正如你可能注意到的，<code class="fe mf mg mh mi b">TermFrequency_InvDocFrequency</code>变压器与<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a">第一部分</a>中的<code class="fe mf mg mh mi b">WordFrequecyVectorizer</code>变压器共用多个部分。最好的实现是类继承。实现每一个 helper 函数仅仅是为了清晰起见，以防您对将单词包转换器作为主类不感兴趣。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="d737" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们已经将 TF-IDF 实现为一个变形器，我们将做同样的事情，使用<code class="fe mf mg mh mi b">StreamlinedModel</code>构建 4 个不同版本的模型并比较性能。</p><h2 id="0028" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">绩效评估</h2><p id="d3dc" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">同<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a"> part 1 </a>，我们将使用以下函数原型生成 4 个<code class="fe mf mg mh mi b">StreamlinedModel</code>对象。</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="2d70" class="mj mk it mi b gy nk nl l nm nn"><strong class="mi jd">from</strong> models.feature <strong class="mi jd">import</strong> TermFrequency_InvDocFrequency<br/><strong class="mi jd">import</strong> lightgbm <strong class="mi jd">as</strong> lgb<br/><strong class="mi jd">from</strong> sentiment_analysis.models.model <strong class="mi jd">import</strong> StreamlinedModel</span><span id="f127" class="mj mk it mi b gy nq nl l nm nn">lgbm = StreamlinedModel(<br/>    transformer_description="TF-IDF",<br/>    transformer=TermFrequency_InvDocFrequency,<br/>    model_description="logisitc regression model",<br/>    model=lgb.LGBMClassifier,<br/>)</span></pre><p id="7919" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将尝试 4 种不同的模型，并收集它们的预测 AUC 分数，以制作柱状图。这一次我们将比较我们以前获得的 AUC 分数，看看我们是否实际上能够获得任何改进。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/53a483790fff0e711e02fd0d3ecf5887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gF8Lwo7BX4NbDyKcVmdYUg.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="e751" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">确实有所改善。</p><p id="aca4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用我们的 tf-idf 转换器，即使使用逻辑回归模型，我们也能够获得 0.896 的 AUC。这将给我们的模型解释带来显著的优势——因为单个词的逻辑回归系数可以直接解释为对评论情感的重要性。不再需要外部包装。</p><p id="231a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还看到多项式朴素贝叶斯模型的性能略有提高，其 AUC 从 0.883 增加到 0.898，现在略高于旧的 winner lightGBM 模型，后者几乎没有任何改进。</p><p id="7eab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还记得这篇博文的标题吗？TF-IDF 确实为逻辑回归这样的简单模型提供了击败 lightGBM 这样的高级模型的能力。这是一个完美的例子，使用正确的特性工程工具会给你带来难以置信的性能提升。</p><p id="1932" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来我们来看看，是不是因为换了变形金刚，才发现最错误的正/差评都变了。</p><h2 id="8a9b" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">错误分类的正面/负面评论</h2><p id="c771" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">我们将使用与<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a">第 1 部分</a>相同的行来获得最错误的正/负指数。</p><pre class="ks kt ku kv gt ng mi nh ni aw nj bi"><span id="4e41" class="mj mk it mi b gy nk nl l nm nn"># get the indices of the misclassified reviews<br/>wrong_positive_inds = np.where((y_test == 1) <br/>                             &amp; (y_pred != y_test))[0]<br/>wrong_negative_inds = np.where((y_test == 0) <br/>                             &amp; (y_pred != y_test))[0]</span><span id="5b26" class="mj mk it mi b gy nq nl l nm nn"># most wrong positive review<br/>most_wrong_positive_index = wrong_positive_inds[<br/>    y_prob_lgbm[:, 1][wrong_positive_inds].argmax()<br/>]</span><span id="b920" class="mj mk it mi b gy nq nl l nm nn"># most wrong negative review<br/>most_wrong_negative_index = wrong_negative_inds[<br/>    y_prob_lgbm[:, 1][wrong_negative_inds].argmin()<br/>]</span></pre><p id="977f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最错误的正面评论(来自多项朴素贝叶斯模型，因为它现在是表现最好的模型)是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/a0cf8f25530b1d286bfaf7fbd8f957ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zz1QR2soC5Kpf2ylOKcK3g.png"/></div></div></figure><p id="cca5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">老实说，我不知道这是一个积极的评论。它包含了强烈的否定词，如“<strong class="lk jd"><em class="me"/></strong>”和“<strong class="lk jd"> <em class="me">而不是</em> </strong>”。我们的模型出错是可以理解的。</p><p id="ce13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最错误的负面评论(来自多项朴素贝叶斯)是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/3809cc62e71f17b34d400ef73828cec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-4KEiAKdisiq7z0Ep3jtA.png"/></div></div></figure><p id="ddd5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我可以看出这篇评论是负面的，但是这篇评论没有任何强烈的负面词汇(见下一节)，犯这个错误也是可以理解的。</p><h2 id="c65d" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">特征重要性</h2><p id="993e" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">我们已经在<a class="ae lh" rel="noopener" target="_blank" href="/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a">第 1 部分</a>中看到了使用 SHAP 来可视化个人评论的例子，我们可以使用 SHAP 来可视化总体水平上的功能重要性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/c12c0449d3469b242a2d0ee49086b38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMQA9A2dwjyJaMBzBB6_Hg.png"/></div></div></figure><p id="29bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最重要的词其实是明显的情绪指示词。</p><p id="154f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">“<strong class="lk jd"> <em class="me">伟大的</em> </strong>”、“<strong class="lk jd"> <em class="me">容易的</em> </strong>”、“<strong class="lk jd"> <em class="me">爱情的</em> </strong>”作为强有力的肯定</p><p id="d1fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">“<strong class="lk jd"><em class="me"/></strong>”、“<strong class="lk jd"><em class="me"/></strong>”、“<strong class="lk jd"> <em class="me">废物</em> </strong>”为强负者</p><h2 id="f78f" class="mj mk it bd ml mm mn dn mo mp mq dp mr lr ms mt mu lv mv mw mx lz my mz na iz bi translated">结论</h2><p id="1e94" class="pw-post-body-paragraph li lj it lk b ll nb kd ln lo nc kg lq lr nd lt lu lv ne lx ly lz nf mb mc md im bi translated">我们使用 tf-idf 作为情感分析模型的特征工程工具，改进了我们的文本矢量器。事实上，tf-idf 也是许多更复杂的特征工程工具的基础，例如 word2vec。同样，GitHub 项目可以在这里找到<a class="ae lh" href="https://github.com/chen-bowen/Streamlined_Sentiment_Analysis" rel="noopener ugc nofollow" target="_blank"/>。现在，让我们总结一下我们在这篇博文中学到了什么。</p><ol class=""><li id="4d45" class="nv nw it lk b ll lm lo lp lr nx lv ny lz nz md oa ob oc od bi translated">获得正确的功能比使用更先进的型号更重要。使用 TF-IDF 将提高特征质量，甚至允许简单的模型胜过更高级的模型。</li><li id="cc43" class="nv nw it lk b ll oe lo of lr og lv oh lz oi md oa ob oc od bi translated"><strong class="lk jd">将值缓存为类属性将提高计算时间。保存每个字的 IDF 值使我们能够加快速度，否则这将是一个相当缓慢和昂贵的双 for 循环操作。</strong></li></ol><p id="0302" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是文本矢量化的全部内容！接下来，我们将了解一些使用深度学习的更高级的特征提取工具。下次见！</p></div></div>    
</body>
</html>