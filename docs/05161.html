<html>
<head>
<title>Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-networks-56e1ad215339?source=collection_archive---------18-----------------------#2020-05-03">https://towardsdatascience.com/recurrent-neural-networks-56e1ad215339?source=collection_archive---------18-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6687" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解 RNN 背后的直觉！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fe793e75ce31344ef8eb81e21913cb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZMRKlP2jvxFvEFSyUi9CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><h1 id="a02a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="9f35" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">本文的目标是深入探讨递归神经网络，这是一种具有不同于前几篇文章(链接)中所见的架构的神经网络。</p><p id="d9e5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">具体而言，文章分为以下几个部分:</p><ul class=""><li id="9ce7" class="mr ms it ls b lt mm lw mn lz mt md mu mh mv ml mw mx my mz bi translated">什么是 rnn</li><li id="9ec1" class="mr ms it ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">长短期记忆(LSTM)网络</li><li id="f0c5" class="mr ms it ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">时间序列的 RNNs 实现</li></ul><h1 id="fb7e" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">什么是 rnn？</h1><p id="e05c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正如我们在这里看到的，CNN 没有任何记忆，rnn 每次都可以超越“从头开始思考”的限制，因为它们有某种记忆。</p><p id="cf19" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们用一个非常直观的例子来看看它们是如何工作的:</p><h2 id="4abd" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">例子</h2><p id="7b33" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">假设我们住在公寓里，有一个完美的室友，他会根据天气做不同的饭菜，晴天还是雨天。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a5706fea6d43980ed347301a7e29b80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*3-fgESpp-x3UDMxR1KdIFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="d54b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">所以，如果我们用向量来编码这些食物:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c57d027a482b2e9a4c12e04fbc922d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*3pDuVhJxNg1Go00Oab98FQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="f35a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们的神经网络会做以下事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d10af8a23360a58c2c0980a4e59cac43.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*g1oOTWuaJPa7ZkM3e1ENzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="1c58" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果我们回想一下，神经网络学习一些可以用矩阵表示的权重，这些权重用于进行预测。我们的将如下:</p><p id="fbe0" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果是晴天:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c5c23b7852492cf61b8f5f57ed5ba299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*LSHijthiHsxBoQYO8ZsZLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="09ee" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果是雨天:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d386112bac532f31f480747c794b439b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*CllAKUgmS2dlhv8qTVhTZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="df6f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果我们看一下我们的权重矩阵，这次是一个图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/2cd9b15a116bdfbfafcff7b4a7fefa8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*dWHyifok4og8G-NTUuoyuQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="f5de" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在让我们看看在这个例子之后添加了什么 rnn:</p><h2 id="565b" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">递归神经网络</h2><p id="d240" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">比方说，现在我们亲爱的室友不仅根据天气来决定做什么，而且现在只看他昨天做了什么。</p><p id="da1a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">负责根据室友今天做的菜来预测她明天会做什么菜的网络是一个递归神经网络(RNN)。</p><p id="887d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个 RNN 可以表示为下面的矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/ad3d7e173cf92535bbf60959dd1f7250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZOLikJizP0R5QJU_xvYNA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="7c14" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">所以我们有一个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e4cb3bd455969f5b5376d84cdc5888b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*vmYK3sO-yud8Vk8NSEFhkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><h2 id="8892" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">让我们把它变得稍微复杂一点</h2><p id="4b3a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在想象一下，你的室友根据她昨天做的菜和天气决定做什么菜。</p><ul class=""><li id="eb83" class="mr ms it ls b lt mm lw mn lz mt md mu mh mv ml mw mx my mz bi translated">如果天气晴朗，她会手里拿着一瓶上好的啤酒在阳台上呆一天，所以她不做饭，所以我们吃和昨天一样的东西。但是</li><li id="f1a3" class="mr ms it ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">如果下雨，她就呆在家里做饭。</li></ul><p id="e049" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">大概是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/959da5cb479e78cb1b31991402346e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EDLyw0tI27FBMti4OK52Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="b682" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">因此，我们最终有了一个模型，它根据我们昨天吃的东西告诉我们将要吃什么，而另一个模型告诉我们我们的室友是否会做饭。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4b5cd9ee936250abd4695a3434beb2b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*VbgTyjeDz98zuEV7EsTKMw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="06a2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">添加和合并操作如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/f154ffbb6cb16b77cf41a1e7f7f80e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moYF_NxCo29ySZ-EIn7IFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/99bf95ae77102d78dc5632f1b1e6fa44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oqrFRxpRce4FJK4xtNCQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="ace1" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里你可以看到图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cd65ac15740e811508b8bbaaa66346ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*c83n86puDlNEpia-5wAaZA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="480a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这就是它的工作原理！</p><p id="4a29" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个例子来自一个很棒的视频，我建议你根据需要多次查看，深入理解前面的解释。你可以在这里找到视频:<a class="ae od" href="https://www.youtube.com/watch?v=UNmqTiOnRfg" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=UNmqTiOnRfg</a></p><h2 id="c878" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">rnn 是用来做什么的？</h2><p id="7034" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">有几种类型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/62e53cab9bd8ea8c31427ac1a90ea641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wBF8jCVH47XbzF3oYsUhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="8230" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">他们非常擅长做出预测，尤其是当我们的数据是连续的时候:</p><p id="b95b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">股市预测</strong></p><p id="aef9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一只股票的价值很大程度上取决于它以前的价值</p><p id="308f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">序列生成</strong> <br/>只要数据是序列，数据在一个瞬间<em class="of"> t </em>取决于数据在瞬间<em class="of"> t-1。</em></p><p id="d1bb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">文本生成</strong></p><p id="793a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">比如你手机提示单词的时候。它看着你写的最后一个单词，看着你正在写的字母，暗示下一个字母甚至单词。</p><p id="a8c2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">语音识别</strong></p><p id="d389" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这种情况下，我们可以识别出前一个单词，并在那一刻接收到音频。</p><h1 id="bf45" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">长短期记忆网络</h1><p id="df04" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在让我们研究一下最流行的 RNN 是如何工作的。它们是 LSTM 网络，其结构如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/5e29437559e6b65ed1fd7ab04b170f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxtFMa3EhshZeVLx5j_l-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="6f0c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是首先:<strong class="ls iu">为什么他们是最受欢迎的？</strong></p><p id="b747" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">原来常规的 rnn 都有内存问题。专门设计的记忆网络无法进行长期记忆。为什么这是一个问题？</p><p id="eddb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">好吧，回到我们室友的问题，对于这个例子，我们只需要知道我们昨天吃了什么，所以什么都不会发生。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d4080b73f53da8dd60588af7bbc94642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*0IFJ66ygBSiurrPMdigQYA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="205d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是想象一下，如果不是三道菜的菜单，而是 60 道菜。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/cd6bf89243cef392be907082bd6fbe14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcdrktpWJ11mdYnwOZECkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="b08c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">传统的 rnn 无法记住很久以前发生的事情。然而，LSTM 会！</p><p id="06c2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">为什么呢？</p><p id="a45d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们来看看 RNN 和 LSTM 的建筑:</p><h2 id="d899" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">RNN</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7482bee0a9dcb70101c17b5c05d60c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m9LBsmaCMIIIAB5e5KV2Vg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><h2 id="228f" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">LSTM</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fe793e75ce31344ef8eb81e21913cb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZMRKlP2jvxFvEFSyUi9CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="7237" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">事实证明，rnn 只有一层，而 LSTMs 有多层的组合，它们以一种非常特殊的方式相互作用。</p><p id="37d7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们试着理解这一点，但首先，让我解释一下术语:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b126933f86826f249cf8eff70c50e402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*ZUSwJXddL-7TZrr2TV48ag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="98c1" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在上图中:</p><ul class=""><li id="6ee1" class="mr ms it ls b lt mm lw mn lz mt md mu mh mv ml mw mx my mz bi translated">向量沿着每条线传播，从一个节点的输出到其他节点的输入。</li><li id="ab28" class="mr ms it ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">粉色圆圈表示元素到元素的运算，例如向量和，而黄色方框是通过训练学习的神经层。</li><li id="3a6f" class="mr ms it ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">连接的线表示连接，分隔的线表示相同的行内容到达两个不同的目的地。</li></ul><h2 id="294c" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">LSTMs 的核心思想</h2><p id="9531" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">关键是单元的状态，它在图中表示为穿过顶部的线:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f4c83658cd3ce34ed9091ae398f11bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*ifsNu6N59W_agzpQwov5iA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="fe95" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">细胞的状态就像一种传送带，它沿着网络的整个结构行进，很少交互作用(并且它们是线性的):这意味着信息只是简单地流动而没有被修改。</p><p id="153f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">巧妙之处在于，LSTM 的各层可以(或不可以)向这条传送带提供信息，而这个决定是由“门”做出的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/aaa0bde66af6302e9ed9e21382f8a31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*vBvOwBFVPTNCxDrT5SlTQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="127a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些门只不过是一种小心调节到达传送带的信息的方式。它们由具有 sigmoid 型激活和元素乘法的神经网络组成。</p><p id="2ffd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">因此，sigmoid 层输出一个介于 0 和 1 之间的数字，这意味着让信息传递到传送带有多重要。0 表示我不在乎，1 表示非常重要。</p><p id="7567" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如图所示，一台 LSTM 有 3 个这样的门，用于保护和控制传送带。</p><p id="5b81" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">关于这个操作的具体细节，在这里有很大的解释:<a class="ae od" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p id="8bb7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">而且这个博客也很有意思:【http://karpathy.github.io/2015/05/21/rnn-effectiveness/】T2</p><p id="65d2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">考虑到这一点，让我们看看循环网络可以做什么！</p><h1 id="0246" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">LSTM 实施</h1><h2 id="cf01" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">LSTM 图像分类</h2><p id="a7bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们将遵循一个可以在此找到的示例:</p><p id="820f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><a class="ae od" href="https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5" rel="noopener">https://medium . com/the-artificial-impostor/notes-understanding-tensor flow-part-2-f 7 e 5 ECE 849 f 5</a></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="7bcf" class="nf kz it on b gy or os l ot ou">from keras.models import Sequential<br/>from keras.layers import LSTM, Dense<br/>from keras.datasets import mnist<br/>from keras.utils import np_utils<br/>from keras import initializers<br/>  <br/><strong class="on iu"># Hyper parameters</strong><br/>batch_size = 128<br/>nb_epoch = 10</span><span id="4713" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># Parameters for MNIST dataset</strong><br/>img_rows, img_cols = 28, 28<br/>nb_classes = 10</span><span id="7644" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># Parameters for LSTM network</strong><br/>nb_lstm_outputs = 30<br/>nb_time_steps = img_rows<br/>dim_input_vector = img_cols</span><span id="bcac" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># Load MNIST dataset</strong><br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>print('X_train original shape:', X_train.shape)<br/>input_shape = (nb_time_steps, dim_input_vector)</span><span id="f6d3" class="nf kz it on b gy ov os l ot ou">X_train = X_train.astype('float32') / 255.<br/>X_test = X_test.astype('float32') / 255.</span><span id="3b90" class="nf kz it on b gy ov os l ot ou">Y_train = np_utils.to_categorical(y_train, nb_classes)<br/>Y_test = np_utils.to_categorical(y_test, nb_classes)</span><span id="0f1a" class="nf kz it on b gy ov os l ot ou">print('X_train shape:', X_train.shape)<br/>print(X_train.shape[0], 'train samples')<br/>print(X_test.shape[0], 'test samples')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d08c0dd357e367f2970fa74cdf1a6d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAE0Df5VlnWnWVqFsas9Yg.png"/></div></div></figure><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="6ab9" class="nf kz it on b gy or os l ot ou"><strong class="on iu"># LSTM Building</strong><br/>model = Sequential()<br/>model.add(LSTM(nb_lstm_outputs, input_shape=input_shape))<br/>model.add(Dense(nb_classes, activation='softmax'))<br/>model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])<br/>model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5c7d55e604aeafc58f3ef2a503944362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*lazr2wVzQChKidK3Oy-nEw.png"/></div></figure><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="ee22" class="nf kz it on b gy or os l ot ou"><strong class="on iu"># Training the model</strong><br/>history = model.fit(X_train, <br/>                    Y_train, <br/>                    nb_epoch=nb_epoch, <br/>                    batch_size=batch_size, <br/>                    shuffle=True,<br/>                    validation_data=(X_test, Y_test),<br/>                    verbose=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/e506fa01195c96a3657d1d7ded225c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8WfL1dy6HBpnLEGmkAlvQ.png"/></div></div></figure><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="154d" class="nf kz it on b gy or os l ot ou"><strong class="on iu"># Evaluation</strong><br/>evaluation = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1)<br/>print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/36e60f89547485231aa0fd88c600462f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*Z5sV2NziO5tRTmYVd_Bxeg.png"/></div></figure><h2 id="47df" class="nf kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">用 LSTM 进行时间序列预测</h2><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="e121" class="nf kz it on b gy or os l ot ou"># LSTM for international airline passengers problem with regression framing<br/># <a class="ae od" href="https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/</a><br/>!wget <a class="ae od" href="https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/airline/international-airline-passengers.csv" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/airline/international-airline-passengers.csv</a></span><span id="4414" class="nf kz it on b gy ov os l ot ou">import numpy<br/>import matplotlib.pyplot as plt<br/>from pandas import read_csv<br/>import math<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.metrics import mean_squared_error</span><span id="da0b" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># convert an array of values into a dataset matrix</strong><br/>def create_dataset(dataset, look_back=1):<br/> dataX, dataY = [], []<br/> for i in range(len(dataset)-look_back-1):<br/>  a = dataset[i:(i+look_back), 0]<br/>  dataX.append(a)<br/>  dataY.append(dataset[i + look_back, 0])<br/> return numpy.array(dataX), numpy.array(dataY)</span><span id="fa8e" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># fix random seed for reproducibility</strong><br/>numpy.random.seed(7)</span><span id="e1a4" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># load the dataset</strong><br/>dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)<br/>dataset = dataframe.values<br/>dataset = dataset.astype('float32')</span><span id="fd1a" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># normalize the dataset</strong><br/>scaler = MinMaxScaler(feature_range=(0, 1))<br/>dataset = scaler.fit_transform(dataset)</span><span id="d608" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># split into train and test sets</strong><br/>train_size = int(len(dataset) * 0.67)<br/>test_size = len(dataset) - train_size<br/>train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]</span><span id="130b" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># reshape into X=t and Y=t+1</strong><br/>look_back = 1<br/>trainX, trainY = create_dataset(train, look_back)<br/>testX, testY = create_dataset(test, look_back)</span><span id="1aae" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># reshape input to be [samples, time steps, features]</strong><br/>trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))<br/>testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))</span><span id="3391" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># create and fit the LSTM network</strong><br/>model = Sequential()<br/>model.add(LSTM(4, input_shape=(1, look_back)))<br/>model.add(Dense(1))<br/>model.compile(loss='mean_squared_error', optimizer='adam')<br/>model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)</span><span id="5093" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># make predictions</strong><br/>trainPredict = model.predict(trainX)<br/>testPredict = model.predict(testX)</span><span id="530d" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># invert predictions</strong><br/>trainPredict = scaler.inverse_transform(trainPredict)<br/>trainY = scaler.inverse_transform([trainY])<br/>testPredict = scaler.inverse_transform(testPredict)<br/>testY = scaler.inverse_transform([testY])</span><span id="d865" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># calculate root mean squared error</strong><br/>trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))<br/>print('Train Score: %.2f RMSE' % (trainScore))<br/>testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))<br/>print('Test Score: %.2f RMSE' % (testScore))</span><span id="c329" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># shift train predictions for plotting</strong><br/>trainPredictPlot = numpy.empty_like(dataset)<br/>trainPredictPlot[:, :] = numpy.nan<br/>trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict</span><span id="4dbb" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># shift test predictions for plotting</strong><br/>testPredictPlot = numpy.empty_like(dataset)<br/>testPredictPlot[:, :] = numpy.nan<br/>testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict</span><span id="de1f" class="nf kz it on b gy ov os l ot ou"><strong class="on iu"># plot baseline and predictions</strong><br/>plt.plot(scaler.inverse_transform(dataset))<br/>plt.plot(trainPredictPlot)<br/>plt.plot(testPredictPlot)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/05b607d4a8096cfcb716447cddad82c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*tqCVTZRz0NrYn1vSYlXX9g.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/85236cf2554b2951f3a302585032738a.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*bSAqvth0d_85pSbugx0FoA.png"/></div></figure><h1 id="6637" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">最后的话</h1><p id="4cf4" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一如既往，我希望你<strong class="ls iu"> </strong>喜欢这篇文章，并且对 rnn 以及如何实现它们有了一个直觉！</p><p id="c9ce" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="of">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae od" href="https://medium.com/@rromanss23" rel="noopener"> <em class="of">这里</em> </a> <em class="of">。</em></p><p id="cc0c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="of">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="ls iu"> <em class="of">请关注我的媒体</em> </strong> <em class="of">，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>