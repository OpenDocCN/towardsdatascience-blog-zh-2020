<html>
<head>
<title>Fine-tuning GPT2 for Text Generation Using Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Pytorch 微调用于文本生成的 GPT2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7?source=collection_archive---------5-----------------------#2020-06-27">https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7?source=collection_archive---------5-----------------------#2020-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6f1e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Huggingface 库提供的 GPT2 生成任何故事</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/eac7d5a2a2e7417a14e671e4b4d8188a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*93AkFkUxJNmuhRbE"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@agkdesign?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山大·奈特</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="ec26" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="282b" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在过去的几年里，NLP 的世界特别繁荣。这主要得益于 NLP 在现代十年最重要的突破之一——<a class="ae lf" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ma iu">变形金刚</strong> </a>。如果你没有看过我之前关于<a class="ae lf" rel="noopener" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"> <strong class="ma iu"> BERT 进行文本分类</strong> </a>的文章，那就去看看吧！我们今天要说的另一款热门变压器是<a class="ae lf" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu"> GPT2 </strong> </a>。GPT2 由 OpenAI 开发，是一个基于 transformer 的大规模语言模型，在一个大型文本语料库上进行预训练:800 万个高质量网页。它只使用预先训练的知识，而没有对它们进行明确的训练，从而在多种语言任务上产生竞争性的表现。GPT2 对于语言生成任务非常有用，因为它是一个自回归语言模型。</p><p id="59dc" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在今天的文章中，我们将深入探讨如何实现另一个流行的转换器 GPT2，以编写有趣和有创意的故事！具体来说，我们将使用<a class="ae lf" href="http://www.cs.cmu.edu/~dbamman/booksummaries.html" rel="noopener ugc nofollow" target="_blank"> CMU 图书摘要数据集</a>测试 GPT2 撰写有创意的图书摘要的能力。我们将使用<a class="ae lf" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>库来构建我们的模型并生成文本。</p><p id="67d2" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">本文的<strong class="ma iu">完整代码库</strong>可以在<a class="ae lf" href="https://github.com/itsuncheng/fine-tuning-GPT2" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p><h1 id="38f8" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 1:准备数据集</h1><p id="e09e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在构建模型之前，我们需要先下载并预处理数据集。</p><p id="2c96" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们使用的是 CMU 图书摘要数据集，其中包含从维基百科中提取的 16，559 本图书，以及元数据，包括书名、作者、出版日期、流派和情节摘要。点击下载数据集<a class="ae lf" href="http://www.cs.cmu.edu/~dbamman/booksummaries.html" rel="noopener ugc nofollow" target="_blank">。以下是数据集的外观:</a></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/c2792e37a4b43da5899b593ca63c1adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wAEzK-IWi0nePPwf"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="5cc1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于数据预处理，我们首先将整个数据集分成训练、验证和测试数据集，训练有效测试比率为 70–20–10。我们在每个摘要的开头添加了一个 bos 令牌<bos>，在每个摘要的结尾添加了一个 eos 令牌<eos>，以供以后培训使用。我们最终将摘要保存到。txt 文件，获取 train.txt，valid.txt，test.txt。</eos></bos></p><p id="aaab" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">你可以在这里获得预处理笔记本<a class="ae lf" href="https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/preprocessing.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="8450" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 2:下载库</h1><p id="4b0d" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了构建和训练 GPT2，我们需要安装 Huggingface 库，以及它的存储库。</p><p id="5628" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">安装 Huggingface 库:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="14b4" class="nf lh it nb b gy ng nh l ni nj">pip install transformers</span></pre><p id="3d88" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">克隆拥抱脸回购:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="c3d9" class="nf lh it nb b gy ng nh l ni nj">git clone github.com/huggingface/transformers</span></pre><p id="f001" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果您想在训练期间看到模型和超参数的可视化效果，也可以选择安装 tensorboard 或 wandb:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="1754" class="nf lh it nb b gy ng nh l ni nj">pip install tensorboard</span><span id="d2e6" class="nf lh it nb b gy nk nh l ni nj">pip install wandb; wandb login</span></pre><h1 id="3897" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第三步:微调 GPT2</h1><p id="7fe2" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在训练之前，我们应该按照之前在数据集中定义的那样设置 bos 令牌和 eos 令牌。</p><p id="d980" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们还应该设置 pad 令牌，因为我们将使用<em class="nl"> LineByLineDataset </em>，它将把数据集中的每一行都视为不同的示例。在<em class="nl">transformers/example/language-modeling/run-language-modeling . py</em>中，我们应该在训练之前为模型追加以下代码:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="33ab" class="nf lh it nb b gy ng nh l ni nj">special_tokens_dict = {'bos_token': '&lt;BOS&gt;', 'eos_token': '&lt;EOS&gt;', 'pad_token': '&lt;PAD&gt;'}</span><span id="0348" class="nf lh it nb b gy nk nh l ni nj">num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span><span id="d608" class="nf lh it nb b gy nk nh l ni nj">model.resize_token_embeddings(len(tokenizer))</span></pre><p id="3aa8" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">运行这段代码后，特殊的标记将被添加到标记器中，模型将调整其嵌入的大小，以适应修改后的标记器。</p><p id="60bf" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于训练，我们首先定义一些参数，然后运行语言建模脚本:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="d2f4" class="nf lh it nb b gy ng nh l ni nj">cd transformers/example/language-modeling</span><span id="bf40" class="nf lh it nb b gy nk nh l ni nj">N=gpu_num</span><span id="7a40" class="nf lh it nb b gy nk nh l ni nj">OUTPUT_DIR=/path/to/model</span><span id="cf40" class="nf lh it nb b gy nk nh l ni nj">TRAIN_FILE=/path/to/dataset/train.txt</span><span id="28b2" class="nf lh it nb b gy nk nh l ni nj">VALID_FILE=/path/to/dataset/valid.txt</span><span id="28a0" class="nf lh it nb b gy nk nh l ni nj">CUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \</span><span id="b58d" class="nf lh it nb b gy nk nh l ni nj">--output_dir=$OUTPUT_DIR \</span><span id="9ae0" class="nf lh it nb b gy nk nh l ni nj">--model_type=gpt2 \</span><span id="909c" class="nf lh it nb b gy nk nh l ni nj">--model_name_or_path=gpt2 \</span><span id="af8e" class="nf lh it nb b gy nk nh l ni nj">--do_train \</span><span id="4a27" class="nf lh it nb b gy nk nh l ni nj">--train_data_file=$TRAIN_FILE \</span><span id="1ad5" class="nf lh it nb b gy nk nh l ni nj">--do_eval \</span><span id="7886" class="nf lh it nb b gy nk nh l ni nj">--eval_data_file=$VALID_FILE \</span><span id="a28b" class="nf lh it nb b gy nk nh l ni nj">--per_device_train_batch_size=2 \</span><span id="c924" class="nf lh it nb b gy nk nh l ni nj">--per_device_eval_batch_size=2 \</span><span id="8f87" class="nf lh it nb b gy nk nh l ni nj">--line_by_line \</span><span id="3c85" class="nf lh it nb b gy nk nh l ni nj">--evaluate_during_training \</span><span id="c27a" class="nf lh it nb b gy nk nh l ni nj">--learning_rate 5e-5 \</span><span id="3c54" class="nf lh it nb b gy nk nh l ni nj">--num_train_epochs=5</span></pre><p id="8294" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">由于 GPU 的限制，我们设置 per_device_train_batch_size=2，per_device_eval_batch_size=2。请随意使用适合您的 GPU 的批量大小。我们使用 line_by_line，它告诉我们的模型将数据集中的每一行都视为一个单独的示例，如前所述。Evaluate_during_training 在每个<code class="fe nm nn no nb b">logging_steps</code>之后对评估数据集进行评估，默认为 500。</p><p id="9916" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果您想从最后一个检查点继续训练，您可以运行:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="9b61" class="nf lh it nb b gy ng nh l ni nj">CUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \</span><span id="fb41" class="nf lh it nb b gy nk nh l ni nj">--output_dir=$OUTPUT_DIR \</span><span id="e9c5" class="nf lh it nb b gy nk nh l ni nj">--model_type=gpt2 \</span><span id="46fc" class="nf lh it nb b gy nk nh l ni nj">--model_name_or_path=$OUTPUT_DIR \</span><span id="2ecf" class="nf lh it nb b gy nk nh l ni nj">--do_train \</span><span id="210f" class="nf lh it nb b gy nk nh l ni nj">--train_data_file=$TRAIN_FILE \</span><span id="3f4f" class="nf lh it nb b gy nk nh l ni nj">--do_eval \</span><span id="d7c4" class="nf lh it nb b gy nk nh l ni nj">--eval_data_file=$VALID_FILE \</span><span id="4abc" class="nf lh it nb b gy nk nh l ni nj">--per_device_train_batch_size=2 \</span><span id="1818" class="nf lh it nb b gy nk nh l ni nj">--per_device_eval_batch_size=2 \</span><span id="baf6" class="nf lh it nb b gy nk nh l ni nj">--line_by_line \</span><span id="46d5" class="nf lh it nb b gy nk nh l ni nj">--evaluate_during_training \</span><span id="4e91" class="nf lh it nb b gy nk nh l ni nj">--learning_rate 5e-5 \</span><span id="473d" class="nf lh it nb b gy nk nh l ni nj">--num_train_epochs=5 \</span><span id="6c38" class="nf lh it nb b gy nk nh l ni nj">--overwrite_output_dir</span></pre><h1 id="482a" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">(可选)步骤 4:评估测试数据集的困惑</h1><p id="532e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">这一步是可选的，取决于你是否想评估你训练过的 GPT2 的表现。您可以通过在测试数据集上评估困惑来做到这一点。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="40f3" class="nf lh it nb b gy ng nh l ni nj">TEST_FILE=/path/to/dataset/test.txt</span><span id="e9fe" class="nf lh it nb b gy nk nh l ni nj">CUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \</span><span id="5538" class="nf lh it nb b gy nk nh l ni nj">--output_dir=$OUTPUT_DIR \</span><span id="e667" class="nf lh it nb b gy nk nh l ni nj">--model_type=gpt2 \</span><span id="465e" class="nf lh it nb b gy nk nh l ni nj">--model_name_or_path=$OUTPUT_DIR \</span><span id="e4b6" class="nf lh it nb b gy nk nh l ni nj">--do_eval \</span><span id="9396" class="nf lh it nb b gy nk nh l ni nj">--eval_data_file=$TEST_FILE \</span><span id="24aa" class="nf lh it nb b gy nk nh l ni nj">--per_device_eval_batch_size=2 \</span><span id="b250" class="nf lh it nb b gy nk nh l ni nj">--line_by_line</span></pre><p id="a988" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这里，在我的例子中，在训练 5 个时期后，我们获得了 2.46 的损失和 11.70 的困惑度:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/43aacc97c40f8468428f717054f71b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5NDcfkSit80colx2"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><h1 id="28ff" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 5:生成文本</h1><p id="e01a" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在使用我们训练好的模型生成文本之前，我们首先通过在<em class="nl">transformers/examples/text-generation/run _ generation . py</em>中设置<code class="fe nm nn no nb b">add_special_tokens=True</code>来启用提示中的特殊标记:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="6d32" class="nf lh it nb b gy ng nh l ni nj">encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=”pt”)</span></pre><p id="2dba" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">然后，我们准备生成一些文本！开始生成方式:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="1dcc" class="nf lh it nb b gy ng nh l ni nj">cd transformers/examples/text-generation</span><span id="6e09" class="nf lh it nb b gy nk nh l ni nj">K=k_for_top-k_sampling_decoder</span><span id="84e6" class="nf lh it nb b gy nk nh l ni nj">CUDA_VISIBLE_DEVICES=$N python run_generation.py \</span><span id="ab50" class="nf lh it nb b gy nk nh l ni nj">--model_type gpt2 \</span><span id="e983" class="nf lh it nb b gy nk nh l ni nj">--model_name_or_path $OUTPUT_DIR \</span><span id="5f5d" class="nf lh it nb b gy nk nh l ni nj">--length 300 \</span><span id="bc46" class="nf lh it nb b gy nk nh l ni nj">--prompt "&lt;BOS&gt;" \</span><span id="8eae" class="nf lh it nb b gy nk nh l ni nj">--stop_token "&lt;EOS&gt;" \</span><span id="9fef" class="nf lh it nb b gy nk nh l ni nj">--k $K \</span><span id="e941" class="nf lh it nb b gy nk nh l ni nj">--num_return_sequences 5</span></pre><p id="190f" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们输入提示“<bos>”作为输入，它代表每个例子的开始，一旦生成了“<eos>”标记，就停止模型的生成。这样，我们的 GPT2 将学习从头到尾生成一个完整的摘要示例，利用它在培训期间从 bos 令牌和 eos 令牌中学到的知识。此外，我们正在使用 top-k 采样解码器，该解码器已被证明在生成非竞争性和更好的文本方面非常有效。k=50 是一个很好的开始值。Huggingface 还支持其他解码方法，包括贪婪搜索、波束搜索和 top-p 采样解码器。有关更多信息，请查看<code class="fe nm nn no nb b">model.generate</code>的<a class="ae lf" href="https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate" rel="noopener ugc nofollow" target="_blank">文档串</a>。</eos></bos></p><p id="e194" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">下面是几个 k=50 的生成文本的例子。</p><blockquote class="np nq nr"><p id="e444" class="ly lz nl ma b mb mu ju md me mv jx mg ns mw mj mk nt mx mn mo nu my mr ms mt im bi translated">主角是英国人威廉·拉克，他被英国政府派往北极执行任务，开始了一次冒险之旅。这部小说讲述了他的朋友和家人如何被卖到挪威小镇肖克当奴隶的故事…</p><p id="67d5" class="ly lz nl ma b mb mu ju md me mv jx mg ns mw mj mk nt mx mn mo nu my mr ms mt im bi translated">一个新的世界正在觉醒，沃尔塔星球的人类必须齐心协力拯救它免于毁灭。新地球现在居住着三个物种。第一个是年龄稍大的人类，第二个是沃尔塔人，第三个是有着深蓝色眼睛的人类…</p><p id="f6cc" class="ly lz nl ma b mb mu ju md me mv jx mg ns mw mj mk nt mx mn mo nu my mr ms mt im bi translated">这部小说开始于 2143 年，一群“地牢”或女巫决定通过消耗死者的灵魂来打破阻止死者力量的咒语。他们用尸体来帮助垂死的人，也用尸体来复活死者…</p></blockquote><p id="f7f2" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">你可以在这里看到更多生成的例子<a class="ae lf" href="https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/generated_summaries.txt" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="e68c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论</h1><p id="62c6" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在本文中，我们展示了如何实现最流行的 transformer 模型之一 GPT2 来创建有趣的文本。GPT2 的大规模预训练数据集和架构允许它产生连贯和流畅的写作片段。虽然 GPT2 的文本仍然可以与人类书写的文本区分开来，但这证明了机器的创造力只是从现在开始上升。想了解更多信息，你可以看看 GPT2 上的<a class="ae lf" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">官方论文</a>或者 OpenAI 的<a class="ae lf" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="3282" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">本文只展示了如何生成由人工智能决定的文本。如果您想知道是否有可能控制正在生成的文本(这是可能的！)，看看我写的下面这篇文章😊。</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/controlling-text-generation-from-language-models-6334935e80cf"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">控制语言模型的文本生成</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">控制机器生成文本的样式和内容的实际操作方法</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om kz ny"/></div></div></a></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="69bc" class="lg lh it bd li lj on ll lm ln oo lp lq jz op ka ls kc oq kd lu kf or kg lw lx bi translated">参考</h1><p id="762d" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">[1] A .瓦斯瓦尼，n .沙泽尔，n .帕尔马等。，<a class="ae lf" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你需要的全部</a> (2017)，第 31 届神经信息处理系统会议</p><p id="4cee" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[2] A .、j .吴、r .柴尔德等。，<a class="ae lf" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习器</a> (2019)，OpenAI</p></div></div>    
</body>
</html>