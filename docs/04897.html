<html>
<head>
<title>Support Vector Machines — Jack of all trades?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——万金油？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svms-jack-of-all-trades-fd6fe3b49580?source=collection_archive---------45-----------------------#2020-04-28">https://towardsdatascience.com/svms-jack-of-all-trades-fd6fe3b49580?source=collection_archive---------45-----------------------#2020-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8340" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">线性和非线性数据集的 SVM 解释</h2></div><p id="e24d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下解释假设您对监督机器学习以及线性判别函数有基本的了解。</p><p id="5191" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果你和我一样，拥有金鱼般的记忆力，让我们提醒自己以下几点:</p><p id="deee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">监督机器学习</strong> </a>需要创建一种算法，该算法能够基于示例输入-输出对将输入映射到输出。换句话说，输入神经网络的数据已经预先分类，实际上是在“监督”这个过程。</p><p id="9be1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在监督机器学习中，对我们的输入数据进行分类的一种原始而有效的方法是线性分类器，如<a class="ae lb" href="http://vision.psych.umn.edu/users/schrater/schrater_lab/courses/PattRecog09/Lec7PattRec09.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">线性判别函数</strong> </a>。这些函数通过超平面决策表面有效地划分了我们的特征空间，该超平面决策表面有助于对存在于两侧的数据进行分类。</p><p id="35d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">听着耳熟？让我们开始吧！</em></p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="li lj l"/></div></figure><h1 id="06fe" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">支持向量机(SVM)</h1><h2 id="51e2" class="mc ll iq bd lm md me dn lq mf mg dp lu ko mh mi lw ks mj mk ly kw ml mm ma mn bi translated"><strong class="ak">简介</strong></h2><p id="260d" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">作为线性判别函数的子集，SVM 也用 N 维超平面对每个预定义类别的标记训练数据进行分类。n 是所考虑的属性的数量。</p><p id="657e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑下面的红蓝圈数据集。在这种情况下，只有两个属性，x 和 y，导致超平面成为二维线。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2de323c932ca0f7a4bb736e3bb7c0328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*kBpTQ_-xMPsfflczMRxp9g.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">多种可能的超平面方向</p></figure><p id="a473" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图中可以清楚地看出，对于分隔两个类别的绿色超平面，存在多个可能的方向，这影响了模型的方差和偏差。</p><p id="ef7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，SVM 试图解决这样一个问题:哪一个超平面是最优选择？T15】</p><h1 id="5219" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">边缘和支持向量</h1><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/599a17d82e8468da8ef23889e66b3354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*UFMIxpc4XtKF7kDdgzLzrg.png"/></div></figure><p id="c801" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，边缘被定义为超平面表面和数据集中最近点之间的距离，称为<strong class="kh ir">支持向量</strong>。事实上，根据所选的支持向量，边距的宽度和方向可能会有所不同。因此，相应地选择它们以最大化分类的裕度。</p><p id="edcb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">位于边缘两侧的数据点被归类为属于相应的数据集。</p><h1 id="64db" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">边距选择</h1><p id="b70f" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">线性判别函数的基本形式如下，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f20b8d8a0bb202fb867bacd172f7563b.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*buQQbv-_SRw4XxMipQIEIQ.png"/></div></figure><p id="4300" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> w </strong>是与数据点 x 相关联的权重向量，w0 是偏差或阈值权重。</p><p id="7990" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在超平面 g(x) = 0 上，然后根据 g(x)&gt;0 还是 g(x) &lt;0, points are classified accordingly. If we plot g(x) for our dataset, we expect the final outcome to look as follows,</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi gj"><img src="../Images/4216be73c7d83c8b827e11fde4ebea9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzWVDZ_DippF4M4mOCroZQ.png"/></div></div></figure><p id="58d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Now, let us define the vector b, which consists of -1s and 1s depending on which side of the hyperplane each data point x, is situated.</p><p id="b45f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">If we multiply g(x) with b(x), we expect all the data points to be translated on the right-hand side of the previous diagram, as all g(x)&lt;0 points will be multiplied by -1.</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ng"><img src="../Images/0ef2936b31619c96d9450f8f4fc0603e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aS8VurvGXGWS3HAI19_jyQ.png"/></div></div></figure><p id="c4c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">As a result, the distance M, between g(x) =0 and the leftmost data point is defined as the margin.</p><p id="4516" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">However, if a support vector happens to be an outlier, an erroneous point in the dataset, it can result in poor classification as it skews the hyperplane.</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c87d9112c320f914b7695a650d03d140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*EIFVIQaWASmkdhbQNRCweQ.png"/></div></figure><p id="d2d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">As evident from the diagram, the rightmost red data point skews the hyperplane as it overfits to compensate for the anomaly. In fact, the optimum hyperplane remains the one positioned at a steeper gradient located equally between the two datasets.</p><p id="efa2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">如果有一种方法可以使 SVM 对支持向量不那么敏感，从而得到一个更一般化的结果呢？</em></p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ni lj l"/></div></figure><h1 id="7f14" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated"><strong class="ak">软边缘分类器</strong></h1><p id="bfa3" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们可以定义一个软度值 C，它定义了数据点可以超过边界准则的程度，从而错误的支持向量不会危及我们的模型的准确性。C 的值可以从交叉验证技术中选择。</p><p id="a572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果 C 的值太大，我们允许许多点超出确定的边界。然而，如果 C 的值太小，我们就定义了一个硬边界，并冒着过度拟合数据的风险。</p><p id="ad29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随后，可以用新定义的与软度 C 相关的参数ε来更新余量最大化方程，如下所示，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/2da0d678c98e3dbebf56d7ad06c1ccdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OWGQoCSj6lCO9Jm_mCZtw.png"/></div></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cfa6379d4bf7a2eae6cf6f7a76d4b924.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*kGedswT6AdpxmF0V9O_jOw.png"/></div></figure><ul class=""><li id="6124" class="nl nm iq kh b ki kj kl km ko nn ks no kw np la nq nr ns nt bi translated">如果ε = 0 <br/>，则数据点位于页边距的正确一侧</li><li id="58ed" class="nl nm iq kh b ki nu kl nv ko nw ks nx kw ny la nq nr ns nt bi translated">如果ε &gt; 0 <br/>，数据点位于页边距的错误一侧</li><li id="7160" class="nl nm iq kh b ki nu kl nv ko nw ks nx kw ny la nq nr ns nt bi translated">如果ε&gt;1 <br/>数据点位于超平面的错误一侧</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5b606df73cca6af21886e909ea4fe81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*Mssu-hhJMolZaIJ7HhpoQQ.png"/></div></figure><h1 id="b2d9" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated"><strong class="ak">非线性映射</strong></h1><p id="75de" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">在某些情况下，不可能线性分离数据集，相反，非线性映射能够将数据集分类到更准确的程度。常见的例子包括 XOR 数据集或循环数据集。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oa"><img src="../Images/28e7aa4fdffdfbde5f78dd92005c2ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*2KD7EHdRQu3tA02ummOjEg.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">循环数据集</p></figure><p id="aad5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，需要非线性映射，将数据集放入可以确定合适分离超平面的高阶空间。该映射由变换函数φ执行，并且线性支持向量点积现在可以用φ(x)重新表示。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0c1d4f2c671571a90e787ae6a38eb56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*KCW2UjnqHgEft3qGIFieHA.png"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/22443a140a0c428edff5155ad1dabb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*kJJ2vRev6EQ5SCf8aRJPlw.png"/></div></figure><p id="c75f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许一个例子会使这更直观，</p><p id="d03e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑上面的循环数据集。有可能用下面的变换函数将数据点从不可分的二维空间映射到可分的三维空间，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a5aba30b4527dadcb49f047067e16842.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*ycMMbfFpjQUQhITcqByBlg.png"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ffcc92f5a8587adb54a74b4ca84073ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*b4TIsSgCG7IaI6hcLrObfA.png"/></div></figure><p id="db2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用前述的φ(x ),判别函数更新如下，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi of"><img src="../Images/de351efbe0064d07ce30462f92a8a684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*wgjgO6edjdb043wDJIELVg.png"/></div></figure><p id="cc1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，可以<a class="ae lb" href="https://www.youtube.com/watch?v=JiM_LXpAtLc" rel="noopener ugc nofollow" target="_blank">示出</a>的是，<strong class="kh ir"> w </strong>总是 x(或φ(x ),如果在变换空间中操作的话)的线性组合，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bbd29d546fe1c587a32a6437bd14e113.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*F4N33f1bgLTDkQbVmN3kBQ.png"/></div></figure><p id="5f41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果，为了确定 SVM 判定边界，需要确定以下更高维度的点积[1]，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/eb325ef475830f345cb966db3022b11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*kYGdCVSlqdAjBcjP_Frl1Q.png"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/bb3849a571c5ac4ae0d9c399f6efbe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*mV_ubadnJ_4WQF81eUhnsg.png"/></div></figure><p id="b954" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管解决了问题，但是这种方法具有更高数量的计算操作。有 3×2 个操作来转换到 3 维空间，另外还有 3 个操作来执行点积。另外，计算复杂度为 O(n)。</p><h1 id="badb" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">核</h1><p id="9389" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">为了简化过程，降低计算复杂度和成本，存在被称为<strong class="kh ir">内核</strong>的函数。</p><blockquote class="oj ok ol"><p id="9dde" class="kf kg lc kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated">内核将线性不可分的数据转换为线性可分的数据，而无需将数据转换到更高维的空间并计算点积。</p></blockquote><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7eb20bfb7b9d28aa55bb7b5eb3bd6717.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*gsHJNQOwMteIFbM4iSFL7w.png"/></div></figure><p id="0356" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，可以使用如下内核来解决前面的示例，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oq"><img src="../Images/6c28149c10823f442b43f10b5c8798c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AS_Ws76yBfG8ogBpzU2DIg.png"/></div></div></figure><p id="da79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与前面的方法相比，核方法使用 2 次计算操作，计算复杂度为 O(n)。</p><p id="109a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是线性内核的示例，其他常见内核包括:</p><p id="8727" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lc">多项式内核</em> </strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0532f80f58b39b66e70c56fc65c27985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*0fJSUlHgnTFVMTA6Y3MjAw.png"/></div></figure><p id="98dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 c 和 d 表示多项式次数，由用户定义，以优化分类。</p><p id="091f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性分类器有 c=0 和 d=1。</p><p id="590b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">径向内核</strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4ccfcba5451419b5bdda8e3746b89cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*wBiVuxOOKe4zkb4_5iHOrA.png"/></div></figure><p id="5021" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 d 是多项式次数，γ定义所选支持向量的接近度。高伽玛值选择最近的支持向量，而低伽玛值选择更远的支持向量。</p><p id="3727" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更多内核类型可以在<a class="ae lb" href="https://data-flair.training/blogs/svm-kernel-functions/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="dc37" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">结论</h1><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ni lj l"/></div></figure><h1 id="872e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">参考</h1><p id="537e" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">[1] <a class="ae lb" href="https://svivek.com/teaching/lectures/slides/svm/kernels.pdf" rel="noopener ugc nofollow" target="_blank">内核和内核技巧:讲座(犹他大学)</a></p></div></div>    
</body>
</html>