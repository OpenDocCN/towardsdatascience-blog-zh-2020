# èšç±»åœ°ç†ç©ºé—´æ•°æ®

> åŸæ–‡ï¼š<https://towardsdatascience.com/clustering-geospatial-data-f0584f0b04ec?source=collection_archive---------1----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## ä½¿ç”¨äº¤äº’å¼åœ°å›¾ç»˜åˆ¶æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ èšç±»

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨æ•°æ®ç§‘å­¦å’Œ Pythonï¼Œå±•ç¤ºå¦‚ä½•å°†ä¸åŒçš„èšç±»ç®—æ³•åº”ç”¨äºåœ°ç†ç©ºé—´æ•°æ®ï¼Œä»¥è§£å†³é›¶å”®åˆç†åŒ–ä¸šåŠ¡æ¡ˆä¾‹ã€‚

![](img/06845dad056d8611e7147ea2b90b7751.png)

[**é—¨åº—åˆç†åŒ–**](https://www.investopedia.com/terms/r/rationalization.asp) æ˜¯å…¬å¸ä¸ºäº†æé«˜ç»è¥æ•ˆç‡ï¼Œé™ä½æˆæœ¬è€Œè¿›è¡Œçš„é‡ç»„ã€‚ç”±äº T4 å’Œæ–°å† è‚ºç‚çš„å±æœºï¼Œä¸–ç•Œå„åœ°çš„å‡ å®¶é›¶å”®ä¼ä¸šæ­£åœ¨å…³é—­åº—é“ºã€‚è¿™å¹¶ä¸ä»…ä»…æ˜¯é‡‘èå±æœºçš„ç—‡çŠ¶ï¼Œäº‹å®ä¸Šè®¸å¤šå…¬å¸å·²ç»å°†æŠ•èµ„é›†ä¸­åœ¨ä½¿ä»–ä»¬çš„ä¸šåŠ¡æ›´åŠ æ•°å­—åŒ–ä¸Šã€‚

[**èšç±»**](https://en.wikipedia.org/wiki/Cluster_analysis) æ˜¯å¯¹ä¸€ç»„å¯¹è±¡è¿›è¡Œåˆ†ç»„çš„ä»»åŠ¡ï¼Œä½¿åŒä¸€ç»„ä¸­çš„è§‚å¯Ÿå€¼å½¼æ­¤ä¹‹é—´æ¯”å…¶ä»–ç»„ä¸­çš„è§‚å¯Ÿå€¼æ›´ç›¸ä¼¼ã€‚è¿™æ˜¯[æ— ç›‘ç£å­¦ä¹ ](https://en.wikipedia.org/wiki/Unsupervised_learning)(æ²¡æœ‰ç›®æ ‡å˜é‡æ—¶çš„æœºå™¨å­¦ä¹ )æœ€å—æ¬¢è¿çš„åº”ç”¨ä¹‹ä¸€ã€‚

[**åœ°ç†ç©ºé—´åˆ†æ**](https://en.wikipedia.org/wiki/Spatial_analysis) æ˜¯å¤„ç†å«æ˜Ÿå›¾åƒã€GPS åæ ‡å’Œè¡—é“åœ°å€ä»¥åº”ç”¨äºåœ°ç†æ¨¡å‹çš„æ•°æ®ç§‘å­¦é¢†åŸŸã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨åœ°ç†æ•°æ®èšç±»æ¥è§£å†³é›¶å”®åˆç†åŒ–é—®é¢˜ã€‚æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_clustering.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_clustering.ipynb) 

æˆ‘å°†ä½¿ç”¨" **Starbucks Stores dataset** "æä¾›æ‰€æœ‰æ­£åœ¨è¿è¥çš„å•†åº—çš„ä½ç½®(ä¸‹é¢çš„é“¾æ¥)ã€‚æˆ‘å°†é€‰æ‹©ä¸€ä¸ªç‰¹å®šçš„åœ°ç†åŒºåŸŸï¼Œé™¤äº†æ‰€æä¾›çš„çº¬åº¦å’Œç»åº¦ä¹‹å¤–ï¼Œæˆ‘å°†æ¨¡æ‹Ÿæ•°æ®é›†ä¸­æ¯ä¸ªå•†åº—çš„ä¸€äº›ä¸šåŠ¡ä¿¡æ¯(æˆæœ¬ã€å®¹é‡ã€å‘˜å·¥)ã€‚

[](https://www.kaggle.com/starbucks/store-locations) [## æ˜Ÿå·´å…‹åœ¨å…¨çƒçš„ä½ç½®

### è¿è¥ä¸­çš„æ¯å®¶æ˜Ÿå·´å…‹åº—çš„åç§°ã€æ‰€æœ‰æƒç±»å‹å’Œä½ç½®

www.kaggle.com](https://www.kaggle.com/starbucks/store-locations) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   è®¾ç½®:å¯¼å…¥åŒ…ï¼Œè¯»å–åœ°ç†æ•°æ®ï¼Œåˆ›å»ºä¸šåŠ¡åŠŸèƒ½ã€‚
*   æ•°æ®åˆ†æ:ç”¨*å¶å­*å’Œ *geopy* åœ¨åœ°å›¾ä¸Šå±•ç¤ºå•†ä¸šæ¡ˆä¾‹ã€‚
*   èšç±»:ç”¨ *scikit-learn* çš„æœºå™¨å­¦ä¹ (K-Means / Affinity Propagation)ï¼Œç”¨ *minisom* çš„æ·±åº¦å­¦ä¹ (è‡ªç»„ç»‡æ˜ å°„)ã€‚
*   å•†åº—åˆç†åŒ–:æ„å»ºç¡®å®šæ€§ç®—æ³•æ¥è§£å†³ä¸šåŠ¡æ¡ˆä¾‹ã€‚

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åŒ…ã€‚

```
**## for data**
import **numpy** as np
import **pandas** as pd**## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for geospatial**
import **folium**
import **geopy****## for machine learning**
from **sklearn** import preprocessing, cluster
import **scipy****## for deep learning**
import **minisom**
```

ç„¶åæˆ‘å°†æŠŠæ•°æ®è¯»å…¥ä¸€ä¸ªç†ŠçŒ«æ•°æ®å¸§ã€‚

```
dtf = pd.read_csv('data_stores.csv')
```

åŸå§‹æ•°æ®é›†åŒ…å«è¶…è¿‡ 5ï¼Œ000 ä¸ªåŸå¸‚å’Œ 25ï¼Œ000 å®¶å•†åº—ï¼Œä½†æ˜¯å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å°†åªå¤„ç†ä¸€ä¸ªåŸå¸‚ã€‚

```
filter = **"Las Vegas"**dtf = dtf[dtf["City"]==filter][["City","Street Address","Longitude","Latitude"]].reset_index(drop=True)dtf = dtf.reset_index().rename(columns={"index":"id"})
dtf.head()
```

![](img/42ef6d1ef8b11a8af25397d23d83c175.png)

åœ¨é‚£ä¸ªåœ°åŒºï¼Œæœ‰ 156 å®¶å•†åº—ã€‚ä¸ºäº†ç»§ç»­è¿›è¡Œä¸šåŠ¡æ¡ˆä¾‹ï¼Œæˆ‘å°†**æ¨¡æ‹Ÿæ¯ä¸ªå•†åº—çš„ä¸€äº›ä¿¡æ¯**:

*   *æ½œåœ¨èƒ½åŠ›*:å‘˜å·¥çš„æ€»èƒ½åŠ›(ä¾‹å¦‚ï¼Œ10 è¡¨ç¤ºå•†åº—æœ€å¤šå¯å®¹çº³ 10 åå‘˜å·¥)
*   *å‘˜å·¥*:å½“å‰å‘˜å·¥çº§åˆ«(å¦‚ 7 è¡¨ç¤ºè¯¥åº—ç›®å‰æœ‰ 7 åå‘˜å·¥åœ¨è¿è¥)
*   *å®¹é‡*:å½“å‰å‰©ä½™å®¹é‡(å¦‚ 10â€“7 = 3ï¼Œå•†åº—ä»å¯å®¹çº³ 3 åå‘˜å·¥)
*   *æˆæœ¬*:å…¬å¸ç»´æŒåº—é“ºè¿è¥çš„å¹´åº¦æˆæœ¬(*ä½*ã€*ä¸­*ã€*é«˜*)

```
dtf["**Potential**"] = np.random.randint(low=3, high=10+1, size=len(dtf))dtf["**Staff**"] = dtf["**Potential**"].apply(lambda x: int(np.random.rand()*x)+1)dtf["**Capacity**"] = dtf["**Potential**"] - dtf["**Staff**"]dtf["**Cost**"] = np.random.choice(["high","medium","low"], size=len(dtf), p=[0.4,0.5,0.1])dtf.head()
```

![](img/267f82732ea0a0a4bf3379c317c1301e.png)

è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿï¼Œè¿™äº›æ•°å­—æ˜¯éšæœºäº§ç”Ÿçš„ï¼Œå¹¶ä¸çœŸæ­£åæ˜ æ˜Ÿå·´å…‹(æˆ–ä»»ä½•å…¶ä»–å…¬å¸)çš„ä¸šåŠ¡ã€‚

ç°åœ¨ä¸€åˆ‡éƒ½å‡†å¤‡å¥½äº†ï¼Œæˆ‘å°†ä»åˆ†æä¸šåŠ¡æ¡ˆä¾‹å¼€å§‹ï¼Œç„¶åæ„å»ºä¸€ä¸ªèšç±»æ¨¡å‹å’Œä¸€ä¸ªåˆç†åŒ–ç®—æ³•ã€‚

æˆ‘ä»¬å¼€å§‹å§ï¼Œå¥½å—ï¼Ÿ

## æ•°æ®åˆ†æ

å‡è®¾æˆ‘ä»¬æ‹¥æœ‰ä¸€å®¶é›¶å”®ä¼ä¸šï¼Œæˆ‘ä»¬ä¸å¾—ä¸å…³é—­ä¸€äº›å•†åº—ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨ä¸è£å‘˜çš„æƒ…å†µä¸‹å®ç°åˆ©æ¶¦æœ€å¤§åŒ–(é€šè¿‡æˆæœ¬æœ€å°åŒ–)ã€‚

è´¹ç”¨åˆ†é…å¦‚ä¸‹:

```
**x = "Cost"**ax = dtf[x].value_counts().sort_values().plot(kind="barh")
totals = []
for i in ax.patches:
    totals.append(i.get_width())
total = sum(totals)
for i in ax.patches:
     ax.text(i.get_width()+.3, i.get_y()+.20, 
     str(round((i.get_width()/total)*100, 2))+'%', 
     fontsize=10, color='black')
ax.grid(axis="x")
plt.suptitle(x, fontsize=20)
plt.show()
```

![](img/1cb427ea76c7b2d1046923ce09d1a743.png)

ç›®å‰ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†å•†åº—åœ¨æ»¡è´Ÿè·è¿è½¬(å‰©ä½™å®¹é‡= 0)ï¼Œè¿™æ„å‘³ç€æœ‰äº›å•†åº—çš„å‘˜å·¥æ•°é‡å¾ˆå°‘(å‰©ä½™å®¹é‡å¾ˆé«˜):

![](img/2426d6b3aaf3c8ae160d684fc812d8a0.png)

è®©æˆ‘ä»¬æŠŠè¿™äº›ä¿¡æ¯åœ¨åœ°å›¾ä¸Šå½¢è±¡åŒ–ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦è·å¾—åœ°ç†åŒºåŸŸçš„åæ ‡æ¥å¯åŠ¨åœ°å›¾ã€‚æˆ‘ä¼šç”¨ [*geopy*](https://geopy.readthedocs.io/en/stable/) æ¥åš:

```
city = **"Las Vegas"****## get location**
locator = geopy.**geocoders**.**Nominatim**(user_agent="MyCoder")
location = locator.geocode(city)
print(location)**## keep latitude and longitude only**
location = [location.latitude, location.longitude]
print("[lat, long]:", location)
```

![](img/e08c0c0acda6ef8cddbeac90b0eca1b5.png)

æˆ‘å°†ä½¿ç”¨[](https://python-visualization.github.io/folium/quickstart.html#Getting-Started)**åˆ›å»ºåœ°å›¾ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åŒ…ï¼Œå…è®¸æˆ‘ä»¬ç»˜åˆ¶äº¤äº’å¼åœ°å›¾ï¼Œè€Œæ— éœ€åŠ è½½ [shapefile](https://en.wikipedia.org/wiki/Shapefile) ã€‚æ¯ä¸ªå•†åº—åº”é€šè¿‡ä¸€ä¸ªç‚¹æ¥è¯†åˆ«ï¼Œè¯¥ç‚¹çš„å¤§å°ä¸å…¶å½“å‰çš„å‘˜å·¥äººæ•°æˆæ¯”ä¾‹ï¼Œé¢œè‰²åŸºäºå…¶æˆæœ¬ã€‚æˆ‘è¿˜å°†å‘é»˜è®¤åœ°å›¾æ·»åŠ ä¸€å°æ®µ HTML ä»£ç æ¥æ˜¾ç¤ºå›¾ä¾‹ã€‚**

```
**x, y = **"Latitude", "Longitude"**
color = **"Cost"** size = **"Staff"** popup = **"Street Address"**
data = dtf.copy() **## create color column**
lst_colors=["red","green","orange"]
lst_elements = sorted(list(dtf[color].unique()))
data["color"] = data[color].apply(lambda x: 
                lst_colors[lst_elements.index(x)])**## create size column (scaled)**
scaler = preprocessing.MinMaxScaler(feature_range=(3,15))
data["size"] = scaler.fit_transform(
               data[size].values.reshape(-1,1)).reshape(-1) **## initialize the map with the starting location** map_ = folium.**Map**(location=location, tiles="cartodbpositron",
                  zoom_start=11)**## add points**
data.apply(lambda row: folium.**CircleMarker**(
           location=[row[x],row[y]], popup=row[popup],
           color=row["color"], fill=True,
           radius=row["size"]).add_to(map_), axis=1)**## add html legend** legend_html = """<div style="position:fixed; bottom:10px; left:10px; border:2px solid black; z-index:9999; font-size:14px;">&nbsp;<b>"""+color+""":</b><br>"""
for i in lst_elements:
     legend_html = legend_html+"""&nbsp;<i class="fa fa-circle 
     fa-1x" style="color:"""+**lst_colors[lst_elements.index(i)]**+"""">
     </i>&nbsp;"""+str(i)+"""<br>"""
legend_html = legend_html+"""</div>"""map_.get_root().**html.add_child**(folium.**Element**(legend_html)) **## plot the map**
map_**
```

**![](img/a8e452147b98d2eb2ce8bd92862417a8.png)**

**æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å¤šåœ°å…³é—­é«˜æˆæœ¬å•†åº—(çº¢ç‚¹),å°†å‘˜å·¥è½¬ç§»åˆ°ä½äºåŒä¸€ç¤¾åŒºçš„ä½æˆæœ¬å•†åº—(ç»¿ç‚¹)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æœ€å¤§åŒ–åˆ©æ¶¦(é€šè¿‡å…³é—­é«˜æˆæœ¬å•†åº—)å’Œæ•ˆç‡(é€šè¿‡è®©ä½æˆæœ¬å•†åº—æ»¡è´Ÿè·å·¥ä½œ)ã€‚**

**æˆ‘ä»¬å¦‚ä½•åœ¨ä¸é€‰æ‹©è·ç¦»é˜ˆå€¼å’Œåœ°ç†è¾¹ç•Œçš„æƒ…å†µä¸‹å®šä¹‰é‚»åŸŸï¼Ÿå—¯ï¼Œç­”æ¡ˆæ˜¯â€¦â€¦é›†ç¾¤ã€‚**

## **ä½¿èšé›†**

**æœ‰å‡ ç§ç®—æ³•å¯ä»¥ä½¿ç”¨ï¼Œè¿™é‡Œåˆ—å‡ºäº†ä¸»è¦çš„[å’Œ](https://scikit-learn.org/stable/modules/clustering.html)ã€‚æˆ‘å°†å°è¯• K-Meansã€ç›¸ä¼¼æ€§ä¼ æ’­ã€è‡ªç»„ç»‡æ˜ å°„ã€‚**

**[**K-Means**](https://en.wikipedia.org/wiki/K-means_clustering) æ—¨åœ¨å°†è§‚å¯Ÿå€¼åˆ’åˆ†ä¸ºé¢„å®šä¹‰æ•°é‡çš„èšç±»( *k* )ï¼Œå…¶ä¸­æ¯ä¸ªç‚¹å±äºå…·æœ‰æœ€è¿‘å‡å€¼çš„èšç±»ã€‚å®ƒé¦–å…ˆéšæœºé€‰æ‹© k ä¸ªè´¨å¿ƒå¹¶å°†è¿™äº›ç‚¹åˆ†é…ç»™æœ€è¿‘çš„èšç±»ï¼Œç„¶åç”¨èšç±»ä¸­æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼æ›´æ–°æ¯ä¸ªè´¨å¿ƒã€‚å½“æ‚¨éœ€è¦è·å¾—ç²¾ç¡®çš„ç»„æ•°æ—¶ï¼Œè¿™ç§ç®—æ³•å¾ˆæ–¹ä¾¿(ä¾‹å¦‚ï¼Œä¿æŒæœ€å°æ•°é‡çš„è¿è¥å•†åº—)ï¼Œå¹¶ä¸”å®ƒæ›´é€‚åˆäºå°‘é‡çš„å¶æ•°èšç±»ã€‚**

**è¿™é‡Œï¼Œä¸ºäº†å®šä¹‰æ­£ç¡®çš„ kï¼Œæˆ‘å°†ä½¿ç”¨[è‚˜æ–¹æ³•](https://en.wikipedia.org/wiki/Elbow_method_(clustering)):ç»˜åˆ¶æ–¹å·®ä½œä¸ºé›†ç¾¤æ•°é‡çš„å‡½æ•°ï¼Œå¹¶é€‰æ‹©ä½¿æ›²çº¿å˜å¹³çš„ *k* ã€‚**

```
**X = dtf[[**"Latitude","Longitude"**]]max_k = **10****## iterations**
distortions = [] 
for i in range(1, max_k+1):
    if len(X) >= i:
       model = cluster.KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
       model.fit(X)
       distortions.append(model.inertia_)**## best k: the lowest derivative**
k = [i*100 for i in np.diff(distortions,2)].index(min([i*100 for i 
     in np.diff(distortions,2)]))**## plot**
fig, ax = plt.subplots()
ax.plot(range(1, len(distortions)+1), distortions)
ax.axvline(k, ls='--', color="red", label="k = "+str(k))
ax.set(title='The Elbow Method', xlabel='Number of clusters', 
       ylabel="Distortion")
ax.legend()
ax.grid(True)
plt.show()**
```

**![](img/76abaf213da6cf5f5d46e91c012819b7.png)**

**æˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨ *k = 5* ï¼Œè¿™æ · K-Means ç®—æ³•å°†æ‰¾åˆ° 5 ä¸ªç†è®ºè´¨å¿ƒã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å°†ç¡®å®šçœŸæ­£çš„è´¨å¿ƒ(ç¦»èšç±»ä¸­å¿ƒæœ€è¿‘çš„è§‚æµ‹å€¼)ã€‚**

```
**k = 5
model = cluster.**KMeans**(n_clusters=k, init='k-means++')
X = dtf[[**"Latitude","Longitude"**]]**## clustering**
dtf_X = X.copy()
dtf_X["cluster"] = model.fit_predict(X)**## find real centroids**
closest, distances = scipy.**cluster.vq.vq**(model.cluster_centers_, 
                     dtf_X.drop("cluster", axis=1).values)
dtf_X["centroids"] = 0
for i in closest:
    dtf_X["centroids"].iloc[i] = 1**## add clustering info to the original dataset** dtf[["**cluster**","**centroids**"]] = dtf_X[["**cluster**","**centroids**"]]
dtf.sample(5)**
```

**![](img/fdb7804b8ba623291ae3136c999fb151.png)**

**æˆ‘å‘æ•°æ®é›†æ·»åŠ äº†ä¸¤åˆ—:" *cluster* "è¡¨ç¤ºè§‚å¯Ÿå€¼å±äºå“ªä¸ªç°‡ï¼Œä»¥åŠ" *centroids* "å¦‚æœè§‚å¯Ÿå€¼ä¹Ÿæ˜¯è´¨å¿ƒ(æœ€é è¿‘ä¸­å¿ƒ)ï¼Œåˆ™ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚è®©æˆ‘ä»¬è®¡åˆ’ä¸€ä¸‹:**

```
****## plot**
fig, ax = plt.subplots()
sns.**scatterplot**(x="Latitude", y="Longitude", data=dtf, 
                palette=sns.color_palette("bright",k),
                hue='cluster', size="centroids", size_order=[1,0],
                legend="brief", ax=ax).set_title('Clustering 
                (k='+str(k)+')')th_centroids = model.cluster_centers_
ax.scatter(th_centroids[:,0], th_centroids[:,1], s=50, c='black', 
           marker="x")**
```

**![](img/f0d1e59050b55f194d3028fa663a709a.png)**

**[**ç›¸ä¼¼æ€§ä¼ æ’­**](https://en.wikipedia.org/wiki/Affinity_propagation) æ˜¯ä¸€ç§åŸºäºå›¾å½¢çš„ç®—æ³•ï¼Œå°†æ¯ä¸ªè§‚å¯Ÿå€¼åˆ†é…ç»™å…¶æœ€è¿‘çš„æ ·æœ¬ã€‚åŸºæœ¬ä¸Šï¼Œæ‰€æœ‰çš„è§‚å¯Ÿâ€œæŠ•ç¥¨â€ç»™å®ƒä»¬æƒ³è¦å…³è”çš„å…¶ä»–è§‚å¯Ÿï¼Œè¿™å¯¼è‡´å°†æ•´ä¸ªæ•°æ®é›†åˆ’åˆ†æˆå¤§é‡ä¸å‡åŒ€çš„èšç±»ã€‚å½“æ‚¨ä¸èƒ½æŒ‡å®šç°‡çš„æ•°é‡æ—¶ï¼Œè¿™æ˜¯éå¸¸æ–¹ä¾¿çš„ï¼Œå¹¶ä¸”å®ƒé€‚ç”¨äºåœ°ç†ç©ºé—´æ•°æ®ï¼Œå› ä¸ºå®ƒå¯ä»¥å¾ˆå¥½åœ°å¤„ç†éå¹³é¢å‡ ä½•ã€‚**

```
**model = cluster.**AffinityPropagation**()**
```

**ä½¿ç”¨ä¹‹å‰çš„ç›¸åŒä»£ç ï¼Œæ‚¨å¯ä»¥æ‹Ÿåˆæ¨¡å‹(æ‰¾åˆ° 12 ä¸ªç°‡)ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥ç»˜å›¾(ä¸åŒä¹‹å¤„åœ¨äº *k* åœ¨å¼€å§‹æ—¶æ²¡æœ‰å£°æ˜ï¼Œå¹¶ä¸”æ²¡æœ‰ç†è®ºä¸Šçš„è´¨å¿ƒ):**

```
**k = dtf["cluster"].nunique()sns.**scatterplot**(x="Latitude", y="Longitude", data=dtf, 
                palette=sns.color_palette("bright",k),
                hue='cluster', size="centroids", size_order=[1,0],
                legend="brief").set_title('Clustering 
                (k='+str(k)+')')**
```

**![](img/f8f33cec2b0349d1e14c450ab26b1ef7.png)**

**[**è‡ªç»„ç»‡åœ°å›¾**](https://en.wikipedia.org/wiki/Self-organizing_map) (SOMs)ç”±äºä½¿ç”¨æ·±åº¦å­¦ä¹ è€Œå¤§ä¸ç›¸åŒã€‚äº‹å®ä¸Šï¼ŒSOM æ˜¯ä¸€ç§äººå·¥ç¥ç»ç½‘ç»œï¼Œå®ƒä½¿ç”¨æ— ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥äº§ç”Ÿè¾“å…¥ç©ºé—´çš„ä½ç»´è¡¨ç¤ºï¼Œç§°ä¸ºâ€œåœ°å›¾â€(ä¹Ÿç§°ä¸º [Kohonen å±‚](https://en.wikipedia.org/wiki/Hybrid_Kohonen_self-organizing_map))ã€‚åŸºæœ¬ä¸Šï¼Œè¾“å…¥è¿æ¥åˆ°å½¢æˆæ˜ å°„çš„ *n x m* ç¥ç»å…ƒï¼Œç„¶åä¸ºæ¯ä¸ªè§‚å¯Ÿè®¡ç®—â€œè·èƒœâ€ç¥ç»å…ƒ(æœ€è¿‘çš„)ï¼Œå¹¶ä½¿ç”¨æ¨ªå‘è·ç¦»å°†ç¥ç»å…ƒèšé›†åœ¨ä¸€èµ·ã€‚è¿™é‡Œï¼Œæˆ‘å°†å°è¯•ä½¿ç”¨ 4x4 SOM:**

**![](img/5acf999105970a46b2ea0c2175bd7c20.png)**

```
**X = dtf[[**"Latitude","Longitude"**]]
map_shape = **(4,4)****## scale data**
scaler = preprocessing.**StandardScaler**()
X_preprocessed = scaler.fit_transform(X.values)**## clustering** model = minisom.**MiniSom**(x=map_shape[0], y=map_shape[1], 
                        input_len=X.shape[1])
model.train_batch(X_preprocessed, num_iteration=100, verbose=False)**## build output dataframe**
dtf_X = X.copy()
dtf_X["cluster"] = np.**ravel_multi_index**(np.array(
      [model.winner(x) for x in X_preprocessed]).T, dims=map_shape)**## find real centroids** cluster_centers = np.array([vec for center in model.get_weights() 
                            for vec in center])closest, distances = scipy.**cluster.vq.vq**(model.cluster_centers_, 
                                         X_preprocessed)
dtf_X["centroids"] = 0
for i in closest:
    dtf_X["centroids"].iloc[i] = 1**## add clustering info to the original dataset** dtf[["cluster","centroids"]] = dtf_X[["cluster","centroids"]]**## plot** k = dtf["cluster"].nunique()fig, ax = plt.subplots()
sns.**scatterplot**(x="Latitude", y="Longitude", data=dtf, 
                palette=sns.color_palette("bright",k),
                hue='cluster', size="centroids", size_order=[1,0],
                legend="brief", ax=ax).set_title('Clustering 
                (k='+str(k)+')')th_centroids = scaler.inverse_transform(cluster_centers)
ax.scatter(th_centroids[:,0], th_centroids[:,1], s=50, c='black', 
           marker="x")**
```

**![](img/0044c3d7e7fa929b4edbaf449e7ca658.png)**

**ç‹¬ç«‹äºæ‚¨ç”¨æ¥å¯¹æ•°æ®è¿›è¡Œèšç±»çš„ç®—æ³•ï¼Œç°åœ¨æ‚¨æœ‰äº†ä¸€ä¸ªå¤šäº†ä¸¤åˆ—çš„æ•°æ®é›†(" *cluster* "ï¼Œ" *centroids* ")ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¯è§†åŒ–åœ°å›¾ä¸Šçš„æ˜Ÿå›¢ï¼Œè¿™æ¬¡æˆ‘ä¹Ÿè¦ç”¨ä¸€ä¸ªæ ‡è®°æ¥æ˜¾ç¤ºè´¨å¿ƒã€‚**

```
**x, y = **"Latitude", "Longitude"**
color = **"cluster"** size = **"Staff"** popup = **"Street Address"** marker = "**centroids**"
data = dtf.copy()**## create color column**
lst_elements = sorted(list(dtf[color].unique()))
lst_colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for i in 
              range(len(lst_elements))]
data["color"] = data[color].apply(lambda x: 
                lst_colors[lst_elements.index(x)])**## create size column (scaled)**
scaler = preprocessing.MinMaxScaler(feature_range=(3,15))
data["size"] = scaler.fit_transform(
               data[size].values.reshape(-1,1)).reshape(-1)**## initialize the map with the starting location** map_ = folium.**Map**(location=location, tiles="cartodbpositron",
                  zoom_start=11)**## add points**
data.apply(lambda row: folium.**CircleMarker**(
           location=[row[x],row[y]], popup=row[popup],
           color=row["color"], fill=True,
           radius=row["size"]).add_to(map_), axis=1)**## add html legend** legend_html = """<div style="position:fixed; bottom:10px; left:10px; border:2px solid black; z-index:9999; font-size:14px;">&nbsp;<b>"""+color+""":</b><br>"""
for i in lst_elements:
     legend_html = legend_html+"""&nbsp;<i class="fa fa-circle 
     fa-1x" style="color:"""+**lst_colors[lst_elements.index(i)]**+"""">
     </i>&nbsp;"""+str(i)+"""<br>"""
legend_html = legend_html+"""</div>"""map_.get_root().**html.add_child**(folium.**Element**(legend_html))**## add centroids marker**
lst_elements = sorted(list(dtf[marker].unique()))
data[data[marker]==1].apply(lambda row: 
           folium.**Marker**(location=[row[x],row[y]], 
           popup=row[marker], draggable=False,          
           icon=folium.**Icon**(color="black")).add_to(map_), axis=1)**## plot the map**
map_**
```

**![](img/87a8a8388c8031ea99495052fa3e3aa9.png)**

**ç°åœ¨æˆ‘ä»¬æœ‰äº†é›†ç¾¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªé›†ç¾¤å†…éƒ¨å¼€å§‹å•†åº—åˆç†åŒ–ã€‚**

## **å•†åº—åˆç†åŒ–**

**å› ä¸ºæœ¬æ–‡çš„ä¸»è¦ç„¦ç‚¹æ˜¯åœ°ç†ç©ºé—´æ•°æ®çš„èšç±»ï¼Œæ‰€ä»¥æˆ‘å°†ä¿æŒè¿™ä¸€éƒ¨åˆ†éå¸¸ç®€å•ã€‚åœ¨æ¯ä¸ªé›†ç¾¤ä¸­ï¼Œæˆ‘å°†é€‰æ‹©æ½œåœ¨ç›®æ ‡(é«˜æˆæœ¬å•†åº—)å’Œä¸­å¿ƒ(ä½æˆæœ¬å•†åº—)ï¼Œå¹¶å°†ç›®æ ‡çš„å‘˜å·¥é‡æ–°å®‰ç½®åœ¨ä¸­å¿ƒï¼Œç›´åˆ°åè€…è¾¾åˆ°æ»¡è´Ÿè·ã€‚å½“ä¸€ä¸ªç›®æ ‡çš„æ‰€æœ‰å‘˜å·¥éƒ½è¢«è½¬ç§»æ—¶ï¼Œå•†åº—å¯ä»¥å…³é—­ã€‚**

**![](img/b7d0f448d7f9cfc71a0abdccdd58c675.png)**

**é›†ç¾¤å†…çš„è¿­ä»£**

```
**dtf_new = pd.DataFrame()for c in sorted(dtf["cluster"].unique()):
    dtf_cluster = dtf[dtf["cluster"]==c]

   ** ## hubs and targets**
    lst_hubs = dtf_cluster[dtf_cluster["Cost"]=="low"
               ].sort_values("Capacity").to_dict("records")
    lst_targets = dtf_cluster[dtf_cluster["Cost"]=="high"
               ].sort_values("Staff").to_dict("records") **## move targets**
    for target in lst_targets:
         for hub in lst_hubs:
             **### if hub has space**
             if hub["Capacity"] > 0:
                residuals = hub["Capacity"] - target["Staff"] **#### case of hub has still capacity: do next target**
                if residuals >= 0:
                   hub["Staff"] += target["Staff"]
                   hub["Capacity"] = hub["Potential"] - hub["Staff"]
                   target["Capacity"] = target["Potential"]
                   target["Staff"] = 0
                   break **#### case of hub is full: do next hub**
                else:
                   hub["Capacity"] = 0
                   hub["Staff"] = hub["Potential"]
                   target["Staff"] = -residuals
                   target["Capacity"] = target["Potential"] - 
                                          target["Staff"] dtf_new = dtf_new.append(pd.DataFrame(lst_hubs)
                 ).append(pd.DataFrame(lst_targets))dtf_new = dtf_new.append(dtf[dtf["Cost"]=="medium"]
                 ).reset_index(drop=True).sort_values(
                 ["cluster","Staff"])
dtf_new.head()**
```

**![](img/3fca5254ac03f375b0857aba0347292d.png)**

**è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç®—æ³•ï¼Œå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œæ”¹è¿›:ä¾‹å¦‚ï¼Œå°†ä¸­ç­‰æˆæœ¬çš„å•†åº—çº³å…¥ç­‰å¼ï¼Œå¹¶åœ¨ä½æˆæœ¬å•†åº—å…¨éƒ¨æ»¡å‘˜æ—¶é‡å¤è¿™ä¸€è¿‡ç¨‹ã€‚**

**è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬ç”¨è¿™ä¸ªåŸºæœ¬æµç¨‹å…³é—­äº†å¤šå°‘å®¶é«˜æˆæœ¬å•†åº—:**

```
**dtf_new["**closed**"] = dtf_new["**Staff**"].apply(lambda x: 1 
                                           if x==0 else 0)
print("closed:", dtf_new["**closed**"].sum())**
```

**æˆ‘ä»¬æˆåŠŸå…³é—­äº† 19 å®¶å•†åº—ï¼Œä½†æˆ‘ä»¬æ˜¯å¦ä¹Ÿä¿æŒäº†è¯¥åœ°åŒºçš„å‡åŒ€è¦†ç›–ï¼Œè¿™æ ·é¡¾å®¢å°±ä¸éœ€è¦å»å¦ä¸€ä¸ªç¤¾åŒºé€›å•†åº—äº†ï¼Ÿè®©æˆ‘ä»¬åœ¨åœ°å›¾ä¸Šæ ‡å‡ºå…³é—­çš„å•†åº—( *marker = "closed"* )æ¥æƒ³è±¡ä¸€ä¸‹åæœ:**

**![](img/a093709b08bb0b7d28b83de653912d50.png)**

## **ç»“è®º**

**è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ç¯‡å…³äº**å¦‚ä½•åœ¨é›¶å”®å•†ä¸šæ¡ˆä¾‹**ä¸­ä½¿ç”¨èšç±»å’Œåœ°ç†ç©ºé—´åˆ†æçš„æ•™ç¨‹ã€‚æˆ‘ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†æ¥æ¯”è¾ƒæµè¡Œçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åœ¨äº¤äº’å¼åœ°å›¾ä¸Šç»˜åˆ¶è¾“å‡ºã€‚æˆ‘è¿˜å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¡®å®šæ€§ç®—æ³•æ¥æä¾›ä¸šåŠ¡æ¡ˆä¾‹çš„è§£å†³æ–¹æ¡ˆã€‚**

**æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚**

> **ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ**

> **æœ¬æ–‡æ˜¯ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ ç³»åˆ—**çš„ä¸€éƒ¨åˆ†**ï¼Œå‚è§:**

**[](/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec) [## ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ :åˆ†ç±»(å®Œæ•´æ•™ç¨‹)

### æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

towardsdatascience.com](/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec) [](/machine-learning-with-python-regression-complete-tutorial-47268e546cea) [## Python æœºå™¨å­¦ä¹ :å›å½’(å®Œæ•´æ•™ç¨‹)

### æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

towardsdatascience.com](/machine-learning-with-python-regression-complete-tutorial-47268e546cea) [](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [## Python æ·±åº¦å­¦ä¹ :ç¥ç»ç½‘ç»œ(å®Œæ•´æ•™ç¨‹)

### ç”¨ TensorFlow å»ºç«‹ã€ç»˜åˆ¶å’Œè§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ

towardsdatascience.com](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c) [## åŸºäºç¥ç»ç½‘ç»œçš„ç°ä»£æ¨èç³»ç»Ÿ

### ä½¿ç”¨ Python å’Œ TensorFlow æ„å»ºæ··åˆæ¨¡å‹

towardsdatascience.com](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c)**