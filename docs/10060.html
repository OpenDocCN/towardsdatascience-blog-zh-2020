<html>
<head>
<title>An Intuitive Introduction to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的直观介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-ef8f004da55c?source=collection_archive---------52-----------------------#2020-07-15">https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-ef8f004da55c?source=collection_archive---------52-----------------------#2020-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="25f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">欢迎来到人工智能的未来</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f1203602b5f24637ad001c6aef439327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-rmY4Pif4cVc4H3z"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</p></figure><blockquote class="kz"><p id="0837" class="la lb it bd lc ld le lf lg lh li lj dk translated">强化学习是最接近人类学习方式的学习类型。</p></blockquote><p id="de95" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi translated">强化学习与监督和非监督学习技术相反，是一种面向目标的学习技术。它基于在一个环境中操作，在该环境中，一个假设的人(代理人)被期望从一组可能的决策中做出一个决策(行动),并通过反复学习选择一个导致期望目标的决策(基本上是试错法)来最大化通过做出该决策获得的利润(回报)。当我们继续阅读这篇文章时，我会更详细地解释这一点。</p><p id="1fe4" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">在这篇文章中，我将讨论强化学习(RL)的基础知识(尽可能用例子)。</p><h1 id="7e95" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">被监督？无人监管？强化学习！</h1><p id="7943" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">重要的事情先来！在开始谈论 RL 之前，我们先来看看它与监督和非监督学习技术到底有什么不同。</p><p id="e80d" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">让我们考虑一个正在学骑自行车的孩子的例子。我们将会看到，如果孩子在有人监督、无人监督或强化学习的情况下学习，这个问题将如何解决:</p><ol class=""><li id="1608" class="nh ni it lm b ln mf lq mg lt nj lx nk mb nl lj nm nn no np bi translated"><strong class="lm iu">监督学习:</strong>现在，如果孩子开始计算他需要在踏板上施加的力，或者他需要与地面保持平衡的角度；他开始在每次骑自行车时优化这些计算，以完善他的骑行技术，那么可以说他是在监督下学习。</li><li id="3862" class="nh ni it lm b ln nq lq nr lt ns lx nt mb nu lj nm nn no np bi translated">无监督学习:然而，如果孩子开始观察成千上万骑自行车的人，并基于这些知识，如果他开始弄清楚骑自行车到底要做什么，那么可以说他是以无监督的方式学习的。</li><li id="4a2a" class="nh ni it lm b ln nq lq nr lt ns lx nt mb nu lj nm nn no np bi translated"><strong class="lm iu">强化学习:</strong>最后，如果给他几个选项，比如踩踏板、向左或向右转动手柄、踩刹车等等。以及在这些选项中尝试任何他想要成功骑自行车的自由，他首先会做错并失败(可能会摔下来)；但最终，在几次失败的尝试后，他会想出如何去做，并最终成功。这个案例是强化学习的一个例子。</li></ol><p id="1420" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">好了，现在你知道为什么说它是最接近人类学习的方式了吧！现在，随着我们继续深入，你可以预期话题会变得有点正式。</p><h1 id="47ef" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">勘探开发</h1><p id="f280" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">让我们继续这个孩子的例子，他知道他骑自行车时可以做的一系列动作。所以，考虑这样一个场景，他最终发现持续踩下踏板可以驱动自行车。然而，他没有意识到骑行之后，他必须在某个点停下来(即在正确的时间踩刹车是骑自行车的一个组成部分)。但是，他很高兴现在他知道如何骑自行车，并且不关心未来的事情。我们姑且把他的快乐称之为'<strong class="lm iu">奖励</strong>'，意思是他因为踩踏板的动作而得到奖励。由于他得到了奖励，他纯粹是“<strong class="lm iu">利用</strong>”当前的行动，即踩踏板，不知道也许最终他可能会在某个地方撞车，这将使他远离实现他的最终目标；正确地骑自行车。</p><p id="0415" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">现在，他可以从一系列可用的动作中探索其他选项，而不仅仅是踩踏板。最终，他将能够随时停下自行车。以类似的方式，他将学习如何转弯，这样，他将是一个好骑手。</p><p id="d1e2" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">但是，任何东西太多都是不好的！我们看到，过度开发会导致失败。同理，过度探索也是不好的。例如，如果他只是在每一个情况下随机改变他的动作，他就不会骑自行车了，不是吗？所以基本上，这是一种权衡，它被称为<strong class="lm iu">勘探开发困境</strong>，是解决 RL 问题时要考虑的主要参数之一。</p><p id="7df0" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated"><strong class="lm iu">注意</strong>孩子会根据他对环境的当前状态<strong class="lm iu"/>【w r t】来决定他在给定情况下的行动，即他在骑车时的当前运动/位置以及从先前尝试中获得的奖励(这种决策机制就是 RL 的全部内容)。</p><h1 id="1200" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">RL 问题的构建模块</h1><ol class=""><li id="da5f" class="nh ni it lm b ln nc lq nd lt nv lx nw mb nx lj nm nn no np bi translated"><strong class="lm iu">策略:</strong><strong class="lm iu"/>策略定义了 RL 代理的行为。在我们的例子中，策略是孩子思考在可用的动作中选择什么动作的方式(孩子是代理)。</li><li id="d068" class="nh ni it lm b ln nq lq nr lt ns lx nt mb nu lj nm nn no np bi translated"><strong class="lm iu">奖励:</strong>这些定义了一个问题的目标。每走一步，环境都会给代理发送一个奖励。在我们的例子中，骑自行车的乐趣，或者从自行车上摔下来的痛苦，就是奖励(第二种情况可以称为惩罚)。</li><li id="4588" class="nh ni it lm b ln nq lq nr lt ns lx nt mb nu lj nm nn no np bi translated"><strong class="lm iu">价值函数:</strong>奖励是环境对主体的即时反应。然而，我们感兴趣的是长期回报的最大化。这是使用值函数计算的。从形式上来说，一个状态的价值是一个代理在未来可以期望积累的总回报，从那个状态开始(<a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">萨顿&amp;巴尔托</a>)。如果孩子仔细考虑了未来可能发生的事情，如果他选择了一个特定的动作，比如说几百米，那么这可以被称为<em class="ny">值。</em></li><li id="f3c2" class="nh ni it lm b ln nq lq nr lt ns lx nt mb nu lj nm nn no np bi translated">模型:环境的模型是计划的工具。它模拟实际环境，因此可用于推断环境的行为方式。例如，给定一个状态和一个动作，模型可能会预测下一个状态和下一个奖励(<a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">萨顿&amp;巴尔托</a>)。当然，RL 机制可以分为基于模型的方法和无模型的方法。</li></ol><h1 id="07ff" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><p id="e432" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">我们对 RL 问题是什么样子以及如何解决它有一种直觉。此外，我们将 RL 与监督学习和非监督学习区分开来。然而，强化学习比本文中的概述要复杂得多；但这足以明确基本概念。</p><h1 id="7c6c" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">参考</h1><p id="4cd1" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">萨顿和巴尔托:<a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf</a></p><p id="b48c" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">NPTEL RL 课程:<a class="ae ky" href="https://www.youtube.com/watch?v=YaPSPu7K9S0&amp;list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9&amp;index=5" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=YaPSPu7K9S0&amp;list = plyqspqzte 6m _ fwzhfayf 4 lskz _ ijmyjd 9&amp;index = 5</a></p></div></div>    
</body>
</html>