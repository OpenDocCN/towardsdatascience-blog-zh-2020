<html>
<head>
<title>Algebra behind PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA背后的代数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algebra-behind-pca-7d801226f4e6?source=collection_archive---------49-----------------------#2020-04-24">https://towardsdatascience.com/algebra-behind-pca-7d801226f4e6?source=collection_archive---------49-----------------------#2020-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e6eb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让PCA变得伟大的代数概要！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ec2c124296fdeff9a974176f195e6803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IW0IjV_e9VOzOMEg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">罗马法师在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="67c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我进行的一些采访中，我们最终会谈到降维技术。PCA绝对是最受考生欢迎的一个。我个人不喜欢深入挖掘与角色不相关的领域，因为我自己可能也不知道答案！但是我喜欢问他们是否知道PCA是如何工作的。虽然我并不期望应聘者知道确切的代数，但我喜欢看他们是否有一些直觉，他们是否只是不知道(这对我来说完全没问题)，或者他们是否试图假装他们知道，这就是我的大脑用黄色旗帜提醒我的地方。</p><p id="f7a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些候选人开始谈论<em class="ls"> PCA如何使用特征向量来计算最终特征</em>。这里是我喜欢问的地方，<strong class="ky ir">什么是特征向量，PCA计算</strong>的特征向量是什么？这是大多数候选人开始动摇的地方。同样，即使这不是成为一名数据科学家必须知道的事情，我个人也更愿意得到诸如<em class="ls">我不确定</em>之类的答案，而不是候选人不知道的事情。</p><p id="8a28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我希望能帮助人们对PCA的实际工作原理有一些基本的直觉，并且在面试中有信心回答这个问题。</p><h1 id="44ca" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">特征分解:特征值和特征向量</h1><p id="b076" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Eigen来自德语，意思是<em class="ls">拥有</em>，因此可以将特征向量和特征值转换为矩阵的自向量和自值。但这实际上意味着什么呢？</p><p id="e393" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们后退一步，把本征态的定义正式化。我们将矩阵<strong class="ky ir"> <em class="ls"> A </em> </strong>的特征值<strong class="ky ir"> <em class="ls"> a </em> </strong>定义为:</p><p id="7f29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">一个v =一个v </em> </strong></p><p id="b234" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，对向量<strong class="ky ir"><em class="ls"/></strong>实施线性变换(保持和与乘的变换)，通过标量<strong class="ky ir"><em class="ls"/></strong>改变<strong class="ky ir"><em class="ls"/></strong>。因此，当且仅当<strong class="ky ir"> <em class="ls"> v </em> </strong>是<strong class="ky ir"> <em class="ls"> A </em> </strong>的特征向量时，对<strong class="ky ir"> <em class="ls"> v </em> </strong>应用矩阵<strong class="ky ir"> <em class="ls"> A </em> </strong>的点积只会改变向量的长度而不会改变其方向！注意<strong class="ky ir"> <em class="ls"> A </em> </strong>是一个<em class="ls">n×n</em>矩阵，在做特征分解时会有<em class="ls"> n个</em>特征向量和<em class="ls"> n个</em>特征值。</p><p id="8439" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">只是澄清一下，不是所有矩阵和向量之间的点积都满足这个条件。</em></p><p id="63be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"><em class="ls"/>必须是一个方阵(行数与列数相同)才能进行特征分解。除此之外，为了很好地定义特征向量，所有特征值必须互不相同。</strong></p><p id="3b79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，同样重要的是要注意，所有的特征向量都是相互垂直的，每个特征向量都有自己的特征值。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="2646" class="lt lu iq bd lv lw mx ly lz ma my mc md jw mz jx mf jz na ka mh kc nb kd mj mk bi translated"><strong class="ak"> PCA的代数</strong></h1><p id="8411" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了理解PCA的代数是如何工作的，我们先来思考一下PCA的目标是什么。使用PCA，我们希望减少完整数据集中的特征数量，尽可能多地保留原始数据中的信息，并删除最高的相关性。</p><p id="1419" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是等等！我们如何衡量这些相关性来确定方向的优先级？嗯，我们确实有一个矩阵来跟踪不同特征之间的相关性，这就是<a class="ae kv" href="https://en.wikipedia.org/wiki/Covariance_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">协方差矩阵</strong> </a>，所以让我们使用它吧！</p><p id="e83d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">关于这一点，有一个快速说明:有其他矩阵可用于PCA，但协方差矩阵是最流行和直观的一种。</em></p><p id="1960" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们可以对协方差矩阵进行特征分解，看看哪些是具有更高特征值的特征向量。对应于最高特征值(<em class="ls"> x1 </em>)的特征向量将是指示数据中最高方差的特征向量，因此是特征之间相关性最小的方向。第二个(<em class="ls"> x2 </em>)将是垂直于<em class="ls"> x1 </em>的第二高方差方向，以此类推。</p><p id="f7dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，一旦计算出协方差矩阵，如果想将n维从<em class="ls"> n </em>维减少到<em class="ls"> m </em>维，就很简单:</p><ul class=""><li id="f45e" class="nc nd iq ky b kz la lc ld lf ne lj nf ln ng lr nh ni nj nk bi translated"><strong class="ky ir">第一步</strong>:计算协方差矩阵的特征向量。</li><li id="aec2" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated"><strong class="ky ir">第二步</strong>:设置特征值最高的前<em class="ls"> m </em>个特征向量。这将是你的旋转矩阵<strong class="ky ir"> <em class="ls"> W，</em> </strong>其中<strong class="ky ir"> <em class="ls"> W </em> </strong>中的每一行对应于排序后的顶部<em class="ls"> m </em>特征向量。<strong class="ky ir"> <em class="ls"> W </em> </strong>是一个<em class="ls"> m </em> x <em class="ls"> n </em>矩阵。</li><li id="bd17" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated"><strong class="ky ir">步骤3 </strong>:给定一个数据点<strong class="ky ir"> <em class="ls"> x，</em> </strong>可以通过执行线性变换<strong class="ky ir"><em class="ls">x’= W x .</em></strong>找到在<em class="ls"> m </em>维空间中的投影，给定<strong class="ky ir"> <em class="ls"> W </em> </strong>是一个<em class="ls"> m </em> x <em class="ls"> n </em>矩阵，<strong class="ky ir"><em class="ls">x’</em></strong></li></ul><h2 id="0baa" class="nq lu iq bd lv nr ns dn lz nt nu dp md lf nv nw mf lj nx ny mh ln nz oa mj ob bi translated">例子</h2><p id="503d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们看看下面的例子，以便更清楚地了解发生了什么:</p><p id="4512" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们有两个特征:给定城市中一套公寓的大小和该套公寓的价格。我们的任务是将数据缩减到1D空间。给定的数据集如下左图所示。</p><p id="347b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个人应该采取的步骤是(<em class="ls">)同样，有多种方法可以做到这一点，我们只是举一个例子！</em>):</p><ul class=""><li id="98a7" class="nc nd iq ky b kz la lc ld lf ne lj nf ln ng lr nh ni nj nk bi translated"><strong class="ky ir">第一步:</strong>根据你的数据定义协方差矩阵，计算你的协方差矩阵的特征向量。</li><li id="64cf" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated"><strong class="ky ir">第二步:</strong>特征向量在中间的图片中显示为绿色。最长的特征向量是对应于最高特征值的特征向量(特征向量通常被归一化为1，这只是为了可视化的目的)。很明显，它是指向最高方差的方向。这是一个人想要保持的方向，因为它包含了最多的信息。</li><li id="30c7" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated"><strong class="ky ir">第三步:</strong>一旦确定了最高特征值对应的特征向量，就要把数据点投影到后者定义的直线上。如右图所示。</li></ul><div class="kg kh ki kj gt ab cb"><figure class="oc kk od oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/370d137a8cd6f5414caee3eae7dc0964.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*mNkXuZONUa3BbEDuJJ1OEA.jpeg"/></div></figure><figure class="oc kk oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/35c187da73312167132ed09efd847f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*Yb4pmgLRKDpqAzXyonfDcw.jpeg"/></div></figure><figure class="oc kk oj oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/95c743b7de4024423cf43368296f6971.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*60Q3qNaUTYPRnegq4FkJ3Q.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk ok di ol om translated">图1:(从左到右)a)原始数据集b)原始数据集连同协方差矩阵特征向量c)原始数据集在最高特征值对应的特征向量所定义的直线上的投影。</p></figure></div><p id="e720" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这能让你对使用PCA时发生的事情有一个基本的概念！</p></div></div>    
</body>
</html>