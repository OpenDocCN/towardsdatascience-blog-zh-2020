<html>
<head>
<title>Tutorial: Scrape 100 Headlines in Seconds With 23 Lines of Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教程:用 23 行 Python 在几秒钟内刮出 100 个标题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutorial-scrape-100-headlines-in-seconds-with-23-lines-of-python-14047deb1a98?source=collection_archive---------32-----------------------#2020-05-13">https://towardsdatascience.com/tutorial-scrape-100-headlines-in-seconds-with-23-lines-of-python-14047deb1a98?source=collection_archive---------32-----------------------#2020-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ce13" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Scrapy 库的网页抓取快速，简单，而且非常强大。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5fb83e5f0fff82e32eef49da810c6635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6kcsRES_lhsGEZcS7QQxQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@benballaschottner?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">本斯·巴拉-肖特纳</a>在<a class="ae kv" href="https://unsplash.com/s/photos/spider-web?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="f56a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你需要做任何类型的网页抓取，Scrapy 几乎是不可能错过的。借助并行请求、用户代理欺骗、robots.txt 策略等内置特性，您只需几行代码就可以构建一个强大的 web scraper。</p><p id="cc13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我将展示如何创建一个基本的 Scrapy 蜘蛛，收集新闻文章的标题。在大多数网站上，你可以做到这一点，而不用担心付费墙、僵尸检测或奇怪的解析技巧，所以我将把这些担心留给另一个教程。</p><h2 id="1811" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">数据</h2><p id="a810" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://gist.github.com/jackbandy/208028b404d8c6a6f822397e306a5a34" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个有 100 个随机 URL 的文件，来自我从<a class="ae kv" href="https://newsapi.org" rel="noopener ugc nofollow" target="_blank"> NewsAPI </a>收集的数据集。前几行如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h2 id="3ac5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">Python 库</h2><p id="d21c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们将使用三个开源库(及其依赖项):<a class="ae kv" href="https://github.com/pandas-dev/pandas" rel="noopener ugc nofollow" target="_blank"> Pandas </a>、<a class="ae kv" href="https://github.com/scrapy/scrapy" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>和<a class="ae kv" href="https://github.com/buriy/python-readability" rel="noopener ugc nofollow" target="_blank"> Readability </a>。假设您已经安装了<a class="ae kv" href="https://pip.pypa.io/en/stable/installing/" rel="noopener ugc nofollow" target="_blank">pip</a>，您可以通过在终端中运行以下命令来确保您的计算机已经安装了这些:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="2627" class="ls lt iq mt b gy mx my l mz na">pip install scrapy pandas readability-lxml</span></pre><p id="25f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，创建要组织项目的文件夹，并导航到该文件夹:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="9aeb" class="ls lt iq mt b gy mx my l mz na">mkdir headline_scraper<br/>cd headline_scraper</span></pre><p id="0e47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以创建蜘蛛。Scrapy 有一个<code class="fe nb nc nd mt b">startproject</code>命令可以设置整个项目，但是我发现它可能会很臃肿。我们将通过制作我们自己的蜘蛛来保持简单，正如所承诺的，它只有 23 行(包括注释和空白)🙂).以下是一个名为<code class="fe nb nc nd mt b">headline_scraper.py</code>的文件的内容:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="1914" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与普通的 python 脚本不同，我们需要使用 scrapy 的<code class="fe nb nc nd mt b">runspider</code>命令来运行文件。使用<code class="fe nb nc nd mt b">-o</code>标志选择保存输出的位置:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="9daa" class="ls lt iq mt b gy mx my l mz na">scrapy runspider headline_scraper.py -o scraped_headlines.csv</span></pre><p id="7eb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这些了！下面是对代码中发生的事情的解释。</p><h1 id="4ab1" class="ne lt iq bd lu nf ng nh lx ni nj nk ma jw nl jx md jz nm ka mg kc nn kd mj no bi translated">遍历代码</h1><h2 id="bfa2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">初始化</h2><p id="a45a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">除了导入库和声明<code class="fe nb nc nd mt b">PATH_TO_DATA</code>(指向存储 100 个 URL 的<a class="ae kv" href="https://gist.github.com/jackbandy/208028b404d8c6a6f822397e306a5a34" rel="noopener ugc nofollow" target="_blank">要点</a>的链接)之外，下面是最初几行要做的事情:</p><ul class=""><li id="595d" class="np nq iq ky b kz la lc ld lf nr lj ns ln nt lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">class HeadlineSpider(scrapy.Spider)</code>创建了一个新的类“HeadlineSpider”，它继承了 scrapy <a class="ae kv" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank">蜘蛛</a>的基本功能</li><li id="ae3f" class="np nq iq ky b kz ny lc nz lf oa lj ob ln oc lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">name="headline_spider"</code>将新蜘蛛命名为“头条 _ 蜘蛛”，供 Scrapy 参考</li><li id="062c" class="np nq iq ky b kz ny lc nz lf oa lj ob ln oc lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">start_urls=read_csv(PATH_TO_DATA).url.tolist()</code>使用 pandas 的 csv 阅读器下载包含 100 个 url 的文件，然后获取“URL”列并将其转换为 python 列表。<code class="fe nb nc nd mt b">start_urls</code>是任何蜘蛛都需要的，但在这种情况下，它们是唯一会被访问的网址。</li></ul><p id="f688" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(如果您想使用具有自己的 URL 的不同文件，只需用您文件的位置替换<code class="fe nb nc nd mt b">PATH_TO_DATA</code>。然后，确保 url 列被标记为“URL”，或者将<code class="fe nb nc nd mt b">.url</code>替换为<code class="fe nb nc nd mt b">.name_of_your_column</code>，它可以在您的机器上，即<code class="fe nb nc nd mt b">PATH_TO_DATA=/Users/Jack/Desktop/file.csv</code></p><p id="e161" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经初始化了蜘蛛，我们需要告诉它当它到达一个网页时实际要做什么。</p><h2 id="741b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">从语法上分析</h2><p id="3b73" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">要真正提取标题，我们必须实现<code class="fe nb nc nd mt b">parse</code>方法。每当 Scrapy 从列表<code class="fe nb nc nd mt b">start_urls</code>中检索到一个 url 时，它会自动向这个方法发送 http 响应，然后完全由您决定如何处理它。这个版本的功能如下:</p><ul class=""><li id="1434" class="np nq iq ky b kz la lc ld lf nr lj ns ln nt lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">doc=Document(response.text)</code>这一行使用<a class="ae kv" href="https://github.com/buriy/python-readability" rel="noopener ugc nofollow" target="_blank">可读性的</a>功能来解析 html。你也可以使用其他的库，比如<a class="ae kv" href="https://www.crummy.com/software/BeautifulSoup/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>或者<a class="ae kv" href="https://github.com/codelucas/newspaper/" rel="noopener ugc nofollow" target="_blank">报纸</a>来获取这个部分。</li><li id="fca5" class="np nq iq ky b kz ny lc nz lf oa lj ob ln oc lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">yield</code>本质上是一个创建输出的 return 方法，您可以把它想象成生成的电子表格中的一行。</li><li id="35bb" class="np nq iq ky b kz ny lc nz lf oa lj ob ln oc lr nu nv nw nx bi translated"><code class="fe nb nc nd mt b">'short_title':doc.short_title()</code>而它下面的几行只是简单的设置了返回对象的不同属性。您可以将<code class="fe nb nc nd mt b">'short_title'</code>视为结果电子表格中的一列，内容将是<code class="fe nb nc nd mt b">doc.short_title()</code>的输出(例如，“冠状病毒爆发如何影响全球经济”)</li></ul><p id="bb90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，这部分是非常可定制的。如果您想尝试获得文章的全文，只需更改 yield 行以包含如下的<code class="fe nb nc nd mt b">doc.summary()</code>。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="63a1" class="ls lt iq mt b gy mx my l mz na">yield {<br/>    'full_text': doc.summary(),<br/>    'short_title': doc.short_title(),<br/>    'full_title': doc.title(),<br/>    'url': response.url<br/>}</span></pre><p id="8706" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将在结果中增加一个名为<code class="fe nb nc nd mt b">full_text</code>的列。csv 文件，可读性最好的提取文章文本的尝试。</p><h1 id="e0d4" class="ne lt iq bd lu nf ng nh lx ni nj nk ma jw nl jx md jz nm ka mg kc nn kd mj no bi translated">结果</h1><p id="c268" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">下面是运行<code class="fe nb nc nd mt b">scrapy runspider headline_scraper.py -o scraped_headlines.csv:</code>的输出文件</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="c6ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嘣！至少大部分情况下。Scrapy 是非常容错的，所以只有 95/100 的 URL 返回。没有回来的 5 遇到了某种 http 错误，就像你有时在浏览网页时看到的 404 错误。此外，一些行(我数了 7 行)写着“警告”和“你是机器人吗？”而不是提供真实的标题。(彭博似乎有非常有效的机器人检测。)</p><p id="31d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多策略来处理 bot 检测，其中一些已经在 Scrapy 中实现。如果您感兴趣，请告诉我，我可以在以后的教程中介绍这些策略。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5f311e9cd8ca64c586bde09a320da3f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iBK-iQJdv9N0MgMLVm2IrA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@lincon_street?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Divyadarshi Acharya </a>在<a class="ae kv" href="https://unsplash.com/s/photos/web-scrape?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="af3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ol">如果您对代码有任何疑问、意见或问题，欢迎回复本文或在</em><a class="ae kv" href="https://twitter.com/jackbandy" rel="noopener ugc nofollow" target="_blank"><em class="ol">Twitter</em></a><em class="ol">上给我发消息。谢谢！</em></p></div></div>    
</body>
</html>