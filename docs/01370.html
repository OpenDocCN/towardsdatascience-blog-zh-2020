<html>
<head>
<title>SVM From Scratch — Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的SVM——Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2?source=collection_archive---------0-----------------------#2020-02-07">https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2?source=collection_archive---------0-----------------------#2020-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c891" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">重要概念总结</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ecfee175a47b8e5fbaaa5b44bdf67c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkWTK4BydcVe3d334SKsjg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如果你明白了，干杯😉</p></figure><h2 id="4bed" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">本博客将涵盖的内容:</h2><p id="52d9" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">1.<a class="ae mn" href="#c22e" rel="noopener ugc nofollow"> SVM简介</a>T2 2。<a class="ae mn" href="#d7d8" rel="noopener ugc nofollow">读取数据集</a> <br/> 3。<a class="ae mn" href="#9b54" rel="noopener ugc nofollow">特色工程</a> <br/> 4。<a class="ae mn" href="#1001" rel="noopener ugc nofollow">拆分数据集</a> <br/> 5。<a class="ae mn" href="#66a2" rel="noopener ugc nofollow">成本函数</a> <br/> 6。<a class="ae mn" href="#72a3" rel="noopener ugc nofollow">成本函数的梯度</a> <br/> 7。<a class="ae mn" href="#163f" rel="noopener ugc nofollow">列车型号使用SGD</a>T20】8。<a class="ae mn" href="#80a3" rel="noopener ugc nofollow">停机判据为SGD</a>9<br/>。<a class="ae mn" href="#b46d" rel="noopener ugc nofollow">测试型号</a> <br/> 10。<a class="ae mn" href="#6a0c" rel="noopener ugc nofollow">具有相关性的特征选择&amp; P值</a> <br/> 11。给我代码</p><p id="1e24" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在深入代码或技术细节之前，我想提一下，虽然有许多库/框架可用于实现SVM(支持向量机)算法，而无需编写大量代码，但我决定用尽可能少的高级库来编写代码，以便您和我可以很好地掌握训练SVM模型(准确率为99%，召回率为0.98%，精确度为0.98%)所涉及的重要组件。如果你正在寻找SVM的快速实现，那么你最好使用像<a class="ae mn" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>、<a class="ae mn" href="https://cvxopt.org/" rel="noopener ugc nofollow" target="_blank"> cvxopt </a>等这样的包。要不，我们开始吧！</p><h1 id="c22e" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">SVM简介</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/adfc6f10cc218c5b8e01d7c1f57d0610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ala8WX2z47WYpn932hUkhA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图1: </strong> SVM用一张图表概括— <a class="ae mn" href="http://ireneli.eu" rel="noopener ugc nofollow" target="_blank"> Ireneli.eu </a></p></figure><p id="f773" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu"> SVM </strong>(支持向量机)是一种<strong class="lw iu">监督的</strong> <strong class="lw iu">机器学习</strong>算法，通常用于<strong class="lw iu">二元分类问题</strong>。它通过输入带有<em class="ng">标签示例的<em class="ng">数据集</em>来训练(xᵢ，yᵢ).</em>例如，如果您的示例是电子邮件，而您的问题是垃圾邮件检测，那么:</p><ul class=""><li id="fc38" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm nm nn no np bi translated">示例电子邮件消息<strong class="lw iu"> <em class="ng"> xᵢ </em> </strong>被定义为能够在n维空间上绘制的<em class="ng"> n维特征向量</em>。</li><li id="2892" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated">特征向量，顾名思义，包含特征(如字数、链接数等。)以数字形式显示</li><li id="b8ce" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated">每个特征向量都标有一个类别<strong class="lw iu"><em class="ng"/></strong></li><li id="a3c8" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated">类别<strong class="lw iu"><em class="ng"/></strong>可以是+ve或-ve(例如，spam=1，而非-spam=-1)</li></ul><p id="1dd2" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">使用这个<em class="ng">数据集</em>，算法<strong class="lw iu"> </strong> <em class="ng">找到一个超平面</em>(或决策边界)，该超平面理想地应该具有以下属性:</p><ul class=""><li id="ece0" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm nm nn no np bi translated">它以最大的裕度在两个类的示例之间创建了分离</li><li id="5ba8" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated">它的等式<em class="ng"> (w.x + b = 0) </em>对于来自+ve类的例子产生≥ 1的值，对于来自-ve类的例子产生≤-1的值</li></ul><p id="e38a" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">它是怎么找到这个超平面的？</strong>通过找到定义该超平面的最优值<em class="ng"> w*(权重/法线)</em>和<em class="ng">b *</em><strong class="lw iu"><em class="ng"/></strong><em class="ng">(截距)</em>。通过<em class="ng">最小化成本函数找到最佳值。</em>一旦算法识别出这些最优值，则<em class="ng"> SVM模型f(x) </em>被定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a30db037ba57829d938f259e47a0a9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*pzms4wJY9_TeMFvAiTvjVA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数学表达的SVM模型</p></figure><p id="1d20" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在我们继续之前，让我们导入本教程所需的包，并创建我们程序的框架<em class="ng"> svm.py </em>:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="f413" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># svm.py</em></strong><br/>import numpy as np  <em class="ng"># for handling multi-dimensional array operation</em><br/>import pandas as pd  <em class="ng"># for reading data from csv </em><br/>import statsmodels.api as sm  <em class="ng"># for finding the p-value</em><br/>from sklearn.preprocessing import MinMaxScaler  <em class="ng"># for normalization</em><br/>from sklearn.model_selection import train_test_split as tts<br/>from sklearn.metrics import accuracy_score <br/>from sklearn.utils import shuffle</span><span id="d032" class="ky kz it nx b gy of oc l od oe"><em class="ng"># &gt;&gt; FEATURE SELECTION &lt;&lt; #</em><br/>def remove_correlated_features(X):<br/>def remove_less_significant_features(X, Y):</span><span id="480c" class="ky kz it nx b gy of oc l od oe"><em class="ng"># &gt;&gt; MODEL TRAINING &lt;&lt; #</em><br/>def compute_cost(W, X, Y):<br/>def calculate_cost_gradient(W, X_batch, Y_batch):<br/>def sgd(features, outputs):</span><span id="c940" class="ky kz it nx b gy of oc l od oe">def init():</span></pre><h1 id="d7d8" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">读取数据集</h1><p id="5468" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我们将使用<a class="ae mn" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上的乳腺癌数据集。数据集中的特征是根据乳腺肿块细针抽吸(FNA)的数字化图像计算得出的。它们描述了图像中出现的细胞核的特征。基于这些特征，我们将训练我们的SVM模型来检测肿块是良性的<strong class="lw iu"> B </strong>(一般无害)还是恶性的<strong class="lw iu"> M </strong>(癌变)。</p><p id="4c0a" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">下载数据集并将<em class="ng"> data.csv </em>文件放在与<em class="ng"> svm.py </em>相同的文件夹中。<em class="ng"> </em>然后将这段代码添加到<em class="ng"> init() </em>函数中:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="7388" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu">def init():<br/></strong>    data = pd.read_csv('./data.csv')</span><span id="582e" class="ky kz it nx b gy of oc l od oe"><em class="ng">    # SVM only accepts numerical values. <br/>    # Therefore, we will transform the categories M and B into<br/>    # values 1 and -1 (or -1 and 1), respectively.</em><br/>    diagnosis_map = {'M':1, 'B':-1}<br/>    data['diagnosis'] = data['diagnosis'].map(diagnosis_map)</span><span id="f355" class="ky kz it nx b gy of oc l od oe">    # drop last column (extra column added by pd)<br/>    # and unnecessary first column (id)<br/>    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)</span></pre><p id="61aa" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><a class="ae mn" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" rel="noopener ugc nofollow" target="_blank"><em class="ng">read _ CSV()</em></a>Pandas<a class="ae mn" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">包的函数从。csv文件并将其存储在<em class="ng">数据帧</em>和<strong class="lw iu">中。</strong>把<strong class="lw iu"> </strong> DataFrame想象成一个数据结构的实现，它看起来像一个带有标记的列和行的表格。下面是从<em class="ng"> data.csv </em>中读取的数据在DataFrame中的样子:</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/4c97672ab2dc0a3b5e8949253a7346e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqTeQ9nl04K_3Gu89PBz2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图2: </strong>在PyCharm IDE中查看的数据帧中乳腺癌数据集的一部分</p></figure><p id="7b94" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">如您所见，我们的数据集的标签和结构得以保留，这使得DataFrame变得直观。</p><h1 id="9b54" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">特征工程</h1><p id="66c5" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">机器学习算法在一个数据集上操作，该数据集是一个由<a class="ae mn" href="https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#features" rel="noopener ugc nofollow" target="_blank">特征</a>和<a class="ae mn" href="https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#labels" rel="noopener ugc nofollow" target="_blank">标签</a>组成的带标签的例子的集合，即在我们的情况下<em class="ng">诊断</em>是一个标签，【<em class="ng">半径_平均</em>，<em class="ng">结构_平均</em>，<em class="ng">纹理_平均……</em>特征，并且每一行都是一个例子。</p><p id="f89e" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在大多数情况下，您最初收集的数据可能是原始的；它要么与你的模型不兼容，要么妨碍它的性能。这时<strong class="lw iu">特征工程</strong>来拯救。它包括预处理技术，通过从原始数据中提取特征来编译数据集。这些技术有两个共同的特点:</p><ul class=""><li id="c6dd" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm nm nn no np bi translated">准备与模型兼容的数据</li><li id="e316" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated">提高机器学习算法的性能</li></ul><p id="94e3" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">标准化</strong>是我们将要使用的众多特征工程技术之一。归一化是将数值范围转换为标准数值范围的过程，通常在区间[1，1]或[0，1]内。这不是一个严格的要求，但它提高了学习的速度(例如，在梯度下降中更快的收敛)并防止数值溢出。在<em class="ng"> init() </em>函数中添加以下代码，以规范化您的所有功能:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="939c" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># inside init()</em><br/></strong><em class="ng"># put features &amp; outputs in different DataFrames for convenience<br/></em>Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis' <br/>X = data.iloc[:, 1:]  # all rows of column 1 and ahead (features)</span><span id="88b4" class="ky kz it nx b gy of oc l od oe"><em class="ng"># normalize the features using MinMaxScalar from<br/># sklearn.preprocessing<br/></em>X_normalized = MinMaxScaler().fit_transform(X.values)<br/>X = pd.DataFrame(X_normalized)</span></pre><h1 id="1001" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">分割数据集</h1><p id="a27c" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我们将使用<em class="ng">sk learn . model _ selection</em>中的<em class="ng"> train_test_split() </em>函数将数据集分成训练集和测试集。我们需要一个单独的数据集来进行测试，因为我们需要看看我们的模型在看不见的观察上表现如何。将此代码添加到<em class="ng"> init() </em>中:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a411" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># inside init()</em></strong></span><span id="94a7" class="ky kz it nx b gy of oc l od oe"><em class="ng"># first insert 1 in every row for intercept b</em><br/>X.insert(loc=len(X.columns), column='intercept', value=1)</span><span id="4247" class="ky kz it nx b gy of oc l od oe"># test_size is the portion of data that will go into test set<br/># random_state is the seed used by the random number generator<br/>print("splitting dataset into train and test sets...")<br/>X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)</span></pre><p id="c0ca" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">如果你对我们为什么在每一行加1感到困惑，那么不要担心。你会在下一节得到答案。</p><h1 id="66a2" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">价值函数</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/405d429b161a65d712f8e376f33d1c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vn2HDrdqBsKN5rYw7rjO5w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图3: </strong> H1并没有把阶级分开。H2有，但幅度很小。H3以最大的差距将他们分开— <a class="ae mn" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="428f" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">也称为目标函数。每个机器学习算法的构建模块之一，它是我们试图<em class="ng">最小化或最大化</em>以实现我们目标的函数。</p><p id="ad5e" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们在SVM的目标是什么？我们的目标是找到一个超平面，它以最大的裕度分隔+ve和-ve示例，同时保持尽可能低的错误分类(见图3)。</p><p id="04a6" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们如何实现这个目标？我们将最小化成本/目标函数，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f744e7b912c07d83bcfb5011098ab593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JAS6rUTO7TDlrv4XZSMbsA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在训练阶段，较大的C导致较窄的裕度(对于无限大的C，SVM变成硬裕度)，而较小的C导致较宽的裕度。</p></figure><p id="aa11" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">您可能见过另一个版本的成本函数，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a0971dbbe8abf3283c75bff8386bfd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*6w_B_DjhGvaqCnvhzhhkDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">较大的λ给出较宽的余量，较小的λ导致较窄的余量(对于无限小的λ，SVM变成硬余量)。</p></figure><p id="aec2" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在这个成本函数中，λ基本上等于<em class="ng"> 1/C </em>，并且具有相反的效果，即较大的λ给出较宽的余量，反之亦然。我们可以使用任何上述成本函数，记住每个正则化参数(C和λ)的作用，然后相应地调整它们。让我们看看如何计算(1)中给出的总成本，然后我们将继续讨论其梯度，该梯度将用于训练阶段以使其最小化:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="d04c" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu">def compute_cost(W, X, Y):</strong><br/>    # calculate hinge loss<br/>    N = X.shape[0]<br/>    distances = 1 - Y * (np.dot(X, W))<br/>    distances[distances &lt; 0] = 0  # equivalent to max(0, distance)<br/>    hinge_loss = reg_strength * (np.sum(distances) / N)<br/>    <br/>    # calculate cost<br/>    cost = 1 / 2 * np.dot(W, W) + hinge_loss<br/>    return cost</span></pre><p id="00b3" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">您可能已经注意到，截距术语<em class="ng"> b </em>不见了。这是因为我们把它推进了重量向量，就像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/df8e5e192f3f9a2d865ec830fd501363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*jZJuvalHKsr18E5NUsemvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将截距项推入权重向量。</p></figure><p id="9886" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">这就是为什么我们在拆分数据集之前添加了一个全1的额外列。在本教程的剩余部分，请记住这一点。</p><h1 id="72a3" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">成本函数的梯度</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/17b422535e4b39f9e56e6342379f78ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*ww3F21VMVGp2NKhm0VTesA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">成本函数的梯度</p></figure><p id="7bd7" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">您可能已经注意到，等式(4)中的成本函数有一些变化。不要担心，如果你解析地解决它，它是相同的。现在让我们使用等式(5)实现<em class="ng">calculate _ cost _ gradient()</em>函数:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ce58" class="ky kz it nx b gy ob oc l od oe"># I haven't tested it but this same function should work for<br/># vanilla and mini-batch gradient descent as well<br/><strong class="nx iu">def calculate_cost_gradient(W, X_batch, Y_batch):</strong><br/>    # if only one example is passed (eg. in case of SGD)<br/>    if type(Y_batch) == np.float64:<br/>        Y_batch = np.array([Y_batch])<br/>        X_batch = np.array([X_batch])</span><span id="e7f6" class="ky kz it nx b gy of oc l od oe">    distance = 1 - (Y_batch * np.dot(X_batch, W))<br/>    dw = np.zeros(len(W))</span><span id="735a" class="ky kz it nx b gy of oc l od oe">    for ind, d in enumerate(distance):<br/>        if max(0, d) == 0:<br/>            di = W<br/>        else:<br/>            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])<br/>        dw += di</span><span id="6e37" class="ky kz it nx b gy of oc l od oe">    dw = dw/len(Y_batch)  # average<br/>    return dw</span></pre><h1 id="163f" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">使用SGD训练模型</h1><p id="be42" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">记得我在上面说过，“为了实现我们的目标，我们试图<em class="ng">最小化或最大化成本函数”</em>。在SVM算法中，我们最小化成本函数。</p><p id="5e13" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">我们为什么要最小化成本函数？因为成本函数本质上是衡量我们的模型在实现目标方面做得有多差的指标。如果你仔细观察J(w)，为了找到它的最小值，我们必须:</strong></p><ol class=""><li id="7468" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm om nn no np bi translated"><em class="ng">最小化</em>∣∣w∣∣<em class="ng">t13】哪个<em class="ng">最大化边距</em> (2/∣∣w∣∣)</em></li><li id="a1b1" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm om nn no np bi translated"><em class="ng">最小化铰链损耗的总和</em>，其中<em class="ng">最小化错误分类</em>。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5ef4983416d2c16d5a71decf97f7ef1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*oKqlv9dGoyEhmvM3zCbsJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">铰链损失函数</p></figure><p id="2584" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">因为我们的两个SVM目标都是通过最小化成本函数来实现的，这就是为什么我们要最小化它。</p><p id="6eb7" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们如何将它最小化？嗯，有多种方法，但我们将使用一种叫做随机梯度下降或SGD的方法。在深入SGD之前，我首先简单解释一下梯度下降是如何工作的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/3261cd0d993540a48da243bc3b8162e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykRTMpIdFqmyvTY6aEVoVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图4: </strong>梯度下降</p></figure><p id="0924" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">梯度下降算法的工作原理如下:</p><ol class=""><li id="cdc6" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm om nn no np bi translated">找到成本函数的梯度，即∇J(w')</li><li id="3da9" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm om nn no np bi translated">以一定的速率向梯度的相反方向移动，即w' = w' — ∝(∇J(w'))</li><li id="d806" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm om nn no np bi translated">重复步骤1-3，直到收敛，即我们发现w '其中J(w)最小</li></ol><p id="1190" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">为什么它的运动方向与梯度方向相反？因为梯度是函数增长最快的方向。我们需要向相反的方向移动，使函数J(w)最小。因此，在梯度下降中使用“下降”一词。</p><p id="0ca4" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在典型的梯度下降(又名普通梯度下降)中，使用所有示例(1…N)计算上述步骤1。然而，在SGD中，一次只使用一个示例。我不会在这里讨论SGD的好处，但是你可以在这个博客的末尾找到一些有用的链接。下面是如何用代码实现SGD:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="bc3e" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu">def sgd(features, outputs):<br/></strong>    max_epochs = 5000<br/>    weights = np.zeros(features.shape[1])<br/>    <em class="ng"># stochastic gradient descent</em><br/>    for epoch in range(1, max_epochs): <br/>        <em class="ng"># shuffle to prevent repeating update cycles</em><br/>        X, Y = shuffle(features, outputs)<br/>        for ind, x in enumerate(X):<br/>            ascent = calculate_cost_gradient(weights, x, Y[ind])<br/>            weights = weights - (learning_rate * ascent)<br/>            <br/>    return weights</span></pre><p id="c47b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">让我们通过添加以下代码在<em class="ng"> init() </em>函数内部调用它:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="c4d4" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># inside init()</em></strong></span><span id="4542" class="ky kz it nx b gy of oc l od oe"><em class="ng"># train the model</em><br/>print("training started...")<br/>W = sgd(X_train.to_numpy(), y_train.to_numpy())<br/>print("training finished.")<br/>print("weights are: {}".format(W))</span></pre><h1 id="80a3" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">SGD的停止标准</h1><p id="8558" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在上面的<em class="ng"> sgd() </em>实现中，我们运行了5000次循环(可能是任何次数)。每次迭代都花费我们时间和额外的计算。我们不需要完成所有的迭代。当满足中断标准时，我们可以终止循环。</p><p id="a9a9" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">停工标准应该是什么？</strong>有多个选项，但我们将使用最简单的一个。与之前的成本相比，当当前的成本没有降低很多时，我们将停止培训。下面是我们如何用停止标准定义<em class="ng"> sgd() </em>:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9c85" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu">def sgd(features, outputs):</strong><br/>    max_epochs = 5000<br/>    weights = np.zeros(features.shape[1])<br/>    nth = 0<br/>    prev_cost = float("inf")<br/>    cost_threshold = 0.01  # in percent<br/>    # stochastic gradient descent<br/>    for epoch in range(1, max_epochs):<br/>        # shuffle to prevent repeating update cycles<br/>        X, Y = shuffle(features, outputs)<br/>        for ind, x in enumerate(X):<br/>            ascent = calculate_cost_gradient(weights, x, Y[ind])<br/>            weights = weights - (learning_rate * ascent)</span><span id="806a" class="ky kz it nx b gy of oc l od oe">        # convergence check on 2^nth epoch<br/>        if epoch == 2 ** nth or epoch == max_epochs - 1:<br/>            cost = compute_cost(weights, features, outputs)<br/>            print("Epoch is:{} and Cost is: {}".format(epoch, cost))<br/>            # stoppage criterion<br/>            if abs(prev_cost - cost) &lt; cost_threshold * prev_cost:<br/>                return weights<br/>            prev_cost = cost<br/>            nth += 1<br/>    return weights</span></pre><h1 id="b46d" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">测试模型</h1><p id="f6e9" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在使用SGD训练模型之后，我们最终获得了最优权重<strong class="lw iu"> w* </strong>，其定义了分离两个类的最佳可能超平面。让我们用这个超平面来测试我们的模型。将此代码添加到<em class="ng"> init() </em>函数中:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="d446" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># inside init()</em></strong></span><span id="f710" class="ky kz it nx b gy of oc l od oe"><em class="ng"># testing the model on test set</em><br/>y_test_predicted = np.array([])<br/>for i in range(X_test.shape[0]):<br/>    yp = np.sign(np.dot(W, X_test.to_numpy()[i])) #model<br/>    y_test_predicted = np.append(y_test_predicted, yp)</span><span id="71fc" class="ky kz it nx b gy of oc l od oe">print("accuracy on test dataset: {}".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))<br/>print("recall on test dataset: {}".format(recall_score(y_test.to_numpy(), y_test_predicted)))<br/>print("precision on test dataset: {}".format(recall_score(y_test.to_numpy(), y_test_predicted)))</span></pre><p id="8f0d" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">现在让我们调用<em class="ng"> init() </em>函数:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="04ad" class="ky kz it nx b gy ob oc l od oe"><em class="ng"># set hyper-parameters and call init<br/># hyper-parameters are normally tuned using cross-validation<br/># but following work good enough</em><br/>reg_strength = 10000 # regularization strength<br/>learning_rate = 0.000001<br/>init()</span></pre><p id="060f" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">以下是输出结果:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="dda2" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"># OUTPUT</strong><br/>reading dataset...<br/>applying feature engineering...<br/>splitting dataset into train and test sets...<br/>training started...<br/>Epoch is: 1 and Cost is: 5333.266133501857<br/>Epoch is: 2 and Cost is: 3421.9128432834573<br/>Epoch is: 4 and Cost is: 2437.2790231100216<br/>Epoch is: 8 and Cost is: 1880.2998267933792<br/>Epoch is: 16 and Cost is: 1519.5578612139725<br/>Epoch is: 32 and Cost is: 1234.642324549297<br/>Epoch is: 64 and Cost is: 977.3285621274708<br/>Epoch is: 128 and Cost is: 804.8893546235923<br/>Epoch is: 256 and Cost is: 703.407799431284<br/>Epoch is: 512 and Cost is: 645.8275191300031<br/>Epoch is: 1024 and Cost is: 631.6024252740094<br/>Epoch is: 2048 and Cost is: 615.8378582171482<br/>Epoch is: 4096 and Cost is: 605.0990964730645<br/>Epoch is: 4999 and Cost is: 606.8186618758745<br/>training finished.<br/>weights are: [ 1.32516553  0.83500639  1.12489803  2.16072054 -1.24845441 -3.24246498<br/>  3.27876342  6.83028706 -0.46562238  0.10063844  5.68881254 -1.93421932<br/>  3.27394523  3.77331751  1.67333278 -2.43170464 -1.7683188   0.84065607<br/> -1.9612766  -1.84996828  2.69876618  5.32541102  1.0380137   3.0787769<br/>  2.2140083  -0.61998182  2.66514199  0.02348447  4.64797917  2.17249278<br/> -9.27401088]<br/>testing the model...<br/>accuracy on test dataset: 0.9736842105263158<br/>recall on test dataset: 0.9534883720930233<br/>precision on test dataset: 0.9534883720930233</span></pre><p id="ad03" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">请注意模型的准确性、召回率和精确度分数，以及完成模型所需的时期。现在让我们尝试使用特征选择来改进它。</p><h1 id="6a0c" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">具有相关性和P值的特征选择</h1><p id="1cfd" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">特征选择包括帮助过滤不相关或冗余特征的统计技术。相关性和p值就是这些统计技术中的一种。使用它们，我们将从我们的原始特征集中选择一个相关和重要的特征子集</p><p id="b2db" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">什么是相关性？</strong>相关性是两个变量之间线性依赖(或线性关系)的程度。如果一个特征的值可以用第二个特征的某种线性关系来解释，则称两个特征相关。这种关系的程度由<strong class="lw iu">相关系数</strong>(或“r”)给出。范围从-1.0到+1.0。r越接近+1或-1，这两个变量的关系就越密切。</p><p id="315e" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">为什么我们要删除其中一个相关特征？有多种原因，但最简单的原因是相关特征对因变量的影响几乎相同。此外，相关特征不会改善我们的模型，而且很可能会恶化它，因此我们最好只使用其中的一个。毕竟特征越少，学习速度越快，模型越简单(特征越少的模型)。</p><p id="d293" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">什么是p值？</strong>这个博客涉及的范围太广了。但是，在特征选择的背景下，p值帮助我们<em class="ng">找到在解释因变量(y) </em>的变化中最重要的特征。</p><p id="480d" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在解释这种变化时，p值低的要素意义更大，而p值高的要素意义较小。通常，我们设置一个<strong class="lw iu">显著性水平SL </strong>(阈值)，如果一个特征的p值高于这个水平，它将被丢弃。我会在这篇博客的最后留下一些链接，对p值进行深入研究。</p><p id="6c41" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">为什么我们要移除高p值的特征？因为它们不能说明因变量的行为。因此，当它们不能帮助我们预测结果时，为什么要保留它们，并不必要地增加我们模型的复杂性。</strong></p><p id="bb5a" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们有两个名为<em class="ng">remove _ correlated _ features()remove _ less _ significant _ features()</em>的函数，分别用于移除高度相关的特征和不太重要的特征(使用p值和<a class="ae mn" href="https://www.javatpoint.com/backward-elimination-in-machine-learning" rel="noopener ugc nofollow" target="_blank">向后消除</a>):</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a53e" class="ky kz it nx b gy ob oc l od oe"># &gt;&gt; FEATURE SELECTION &lt;&lt; #<br/><strong class="nx iu">def remove_correlated_features(X):</strong><br/>    corr_threshold = 0.9<br/>    corr = X.corr()<br/>    drop_columns = np.full(corr.shape[0], False, dtype=bool)<br/>    for i in range(corr.shape[0]):<br/>        for j in range(i + 1, corr.shape[0]):<br/>            if corr.iloc[i, j] &gt;= corr_threshold:<br/>                drop_columns[j] = True<br/>    columns_dropped = X.columns[drop_columns]<br/>    X.drop(columns_dropped, axis=1, inplace=True)<br/>    return columns_dropped</span><span id="cf51" class="ky kz it nx b gy of oc l od oe"><strong class="nx iu">def remove_less_significant_features(X, Y):</strong><br/>    sl = 0.05<br/>    regression_ols = None<br/>    columns_dropped = np.array([])<br/>    for itr in range(0, len(X.columns)):<br/>        regression_ols = sm.OLS(Y, X).fit()<br/>        max_col = regression_ols.pvalues.idxmax()<br/>        max_val = regression_ols.pvalues.max()<br/>        if max_val &gt; sl:<br/>            X.drop(max_col, axis='columns', inplace=True)<br/>            columns_dropped = np.append(columns_dropped, [max_col])<br/>        else:<br/>            break<br/>    regression_ols.summary()<br/>    return columns_dropped</span></pre><p id="e0f7" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在应用<em class="ng">规范化</em>之前，让我们在<em class="ng"> init() </em>中调用这些函数:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="47d1" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"><em class="ng"># inside init()</em></strong></span><span id="4b66" class="ky kz it nx b gy of oc l od oe"># filter features<br/>remove_correlated_features(X)<br/>remove_less_significant_features(X, Y)</span></pre><p id="96b7" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">重新运行代码并检查输出:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="803d" class="ky kz it nx b gy ob oc l od oe"><strong class="nx iu"># OUTPUT WITH FEATURE SELECTION<br/></strong>reading dataset...<br/>applying feature engineering...<br/>splitting dataset into train and test sets...<br/>training started...<br/>Epoch is: 1 and Cost is: 7198.889722245353<br/>Epoch is: 2 and Cost is: 6546.424590270085<br/>Epoch is: 4 and Cost is: 5448.724593530262<br/>Epoch is: 8 and Cost is: 3839.8660601754004<br/>Epoch is: 16 and Cost is: 2643.2493061396613<br/>Epoch is: 32 and Cost is: 2003.9830891013514<br/>Epoch is: 64 and Cost is: 1595.2499320295813<br/>Epoch is: 128 and Cost is: 1325.7502330505054<br/>Epoch is: 256 and Cost is: 1159.7928936478063<br/>Epoch is: 512 and Cost is: 1077.5846940303365<br/>Epoch is: 1024 and Cost is: 1047.208390340501<br/>Epoch is: 2048 and Cost is: 1040.2241600540974<br/>training finished.<br/>weights are: [ 3.53520254 11.03169318 -2.31444264 -7.89186867 10.14516174 -1.28905488<br/> -6.4397589   2.26113987 -3.87318997  3.23075732  4.94607957  4.81819288<br/> -4.72111236]<br/>testing the model...<br/>accuracy on test dataset: 0.9912280701754386<br/>recall on test dataset: 0.9767441860465116<br/>precision on test dataset: 0.9767441860465116</span></pre><p id="57b2" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">可以看到<em class="ng">准确率(99%) </em>，<em class="ng">精度(0.98) </em>，以及<em class="ng">召回(0.98) </em>的分数都有所提高。而且，新币收敛得更快；训练在2048个纪元内结束，这与之前的一次(5000个纪元)相比要少得多</p><h1 id="7944" class="mt kz it bd la mu mv mw ld mx my mz lg jz na ka lk kc nb kd lo kf nc kg ls nd bi translated">完全码</h1><p id="04c7" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">您可以在这个Github资源库中获得完整的代码:</p><div class="op oq gp gr or os"><a href="https://github.com/qandeelabbassi/python-svm-sgd" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">qandeelabbassi/python-svm-sgd</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">SVM随机次梯度下降算法的Python实现- qandeelabbassi/python-svm-sgd</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">github.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ks os"/></div></div></a></div></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><p id="c89b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">有用链接</strong></p><ul class=""><li id="403f" class="nh ni it lw b lx mo ma mp lh nj ll nk lp nl mm nm nn no np bi translated"><a class="ae mn" rel="noopener" target="_blank" href="/https-towardsdatascience-com-why-stochastic-gradient-descent-works-9af5b9de09b8">SGD为什么会起作用？</a></li><li id="f7fa" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated"><a class="ae mn" href="https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent" rel="noopener ugc nofollow" target="_blank">新币的好处</a></li><li id="f5cf" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated"><a class="ae mn" rel="noopener" target="_blank" href="/p-values-explained-by-data-scientist-f40a746cfc8">数据科学家解释的P值</a></li><li id="8680" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated"><a class="ae mn" href="https://www.youtube.com/watch?v=nk2CQITm_eo" rel="noopener ugc nofollow" target="_blank">线性回归和p值</a></li><li id="b2f5" class="nh ni it lw b lx nq ma nr lh ns ll nt lp nu mm nm nn no np bi translated"><a class="ae mn" href="https://www.javatpoint.com/backward-elimination-in-machine-learning" rel="noopener ugc nofollow" target="_blank">落后淘汰</a></li></ul></div></div>    
</body>
</html>