<html>
<head>
<title>The Recurrent Neural Network (RNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-recurring-neural-networks-rnns-b2c4c088eaf?source=collection_archive---------35-----------------------#2020-05-23">https://towardsdatascience.com/the-recurring-neural-networks-rnns-b2c4c088eaf?source=collection_archive---------35-----------------------#2020-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="39da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">递归神经网络(RNN)是一个输入节点(隐藏层),用于激活sigmoid。</h2></div><p id="4e7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNN实现这一点的方式是获取一个神经元的输出，并将其作为输入返回给另一个神经元，或者将当前时间步长的输入馈送给较早时间步长的输出。在这里，您将以前时间的输入一步一步地输入到当前时间的输入中，反之亦然。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/bf2a53e7cee14c97a4b56ebc91ff3b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WuseWCX9LQFgzWiqb_cjdQ.jpeg"/></div></div></figure><p id="a632" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可以以多种方式使用，例如通过具有已知变化的学习门或sigmoid激活和许多其他类型的神经网络的组合。</p><p id="48a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNNs的一些应用包括预测能源需求、预测股票价格和预测人类行为。rnn是根据基于时间和基于序列的数据建模的，但它们在各种其他应用中也是有用的。</p><p id="e837" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">递归神经网络是一种用于深度学习、机器学习和其他形式的人工智能(AI)的人工神经网络。它们有许多属性，这些属性使它们对于需要顺序处理数据的任务非常有用。</p><p id="0aa0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">说得更专业一点，递归神经网络被设计成通过从序列的一个步骤到下一个步骤遍历隐藏状态，结合输入，并在输入之间来回路由来学习数据序列。RNN是为有效处理顺序数据而设计的神经网络，但也适用于非顺序数据。</p><p id="986f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些类型的数据包括可被视为一系列单词的文本文档或音频文件，其中您可以看到一系列声音频率和时间。输出图层的可用信息越多，读取和排序的速度就越快，其性能也就越好。</p><p id="61a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">rnn旨在识别具有序列特征的数据，并预测下一个可能的场景。它们被用于模拟人脑中神经元活动的模型中，如深度学习和机器学习。</p><p id="537b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种类型的RNN有一种记忆力，使它能够记住过去发生过多次的重要事件(步骤)。rnn是可以分解成一系列小块并作为序列处理的图像。通过使用学习的输入数据的时间相关性，我们能够将我们学习的序列与其他回归和分类任务区分开来。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lq"><img src="../Images/6d48d239341af0f543c382f4564c56ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmutJFU7gam_FnvTShImIQ.jpeg"/></div></div></figure><p id="ad71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">处理顺序数据(文本、语音、视频等)。)，我们可以将数据向量输入常规的神经网络。RNNs可用于各种应用，例如语音识别、图像分类和图像识别。</p><p id="7229" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前馈神经网络中，决策基于当前输入，并且独立于先前输入(例如，文本、视频等)。).rnn可以通过接受先前接收的输入并线性处理它来处理顺序数据。神经网络中的前馈使得信息能够从一个隐藏层流向下一个隐藏层，而不需要单独的处理层。基于这种学习序列，我们能够通过其对输入数据的时间依赖性将其与其他回归和分类任务区分开来。</p><p id="0c2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上，RNN是一个上下文循环，允许在上下文中处理数据——换句话说，它应该允许递归神经网络有意义地处理数据。神经网络的循环连接与特定上下文中的输入和输出数据形成受控循环。</p><p id="6b36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于理解上下文对于任何类型的信息的感知都是至关重要的，这使得递归神经网络能够基于放置在特定上下文中的模式来识别和生成数据。与其他类型的直接处理数据并且独立处理每个元素的神经网络不同，递归神经网络关注输入和输出数据的上下文。</p><p id="3d3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于它们的内部循环，rnn具有动态组合经验的能力。像记忆细胞一样，这些网络能够有效地关联遥远时间的记忆输入，并随着时间的推移以高度可预测性动态捕获数据结构。已经证明，RNNs能够比传统的神经网络(例如，以线性回归模型的形式)更快地处理序列数据。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/95f101edaf0bef4b48898e7dc0dbf928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xjZeqhX6lbTr_r5SrIPxA.jpeg"/></div></div></figure><p id="9ec3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTM(长短期记忆)引入了一个隐藏层网络，其中传统的人工神经元被计算单元取代。</p><p id="c4ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与其他传统的rnn不同，LSTM可以处理梯度和消失问题，特别是在处理长期时间序列数据时，每个存储单元(LSTM单元)保留关于给定上下文(即输入和输出)的相同信息。</p><p id="d0c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">研究表明，与其他传统的rnn相比，神经LSTM网络在处理长期时间序列数据时表现更好。由于理解上下文对于任何种类的信息的感知都是至关重要的，这允许递归神经网络基于放置在特定上下文中的模式来识别和生成数据。</p><h2 id="5c21" class="lr ls it bd lt lu lv dn lw lx ly dp lz kr ma mb mc kv md me mf kz mg mh mi mj bi translated"><strong class="ak">引用来源</strong></h2><ul class=""><li id="bcc7" class="mk ml it kk b kl mm ko mn kr mo kv mp kz mq ld mr ms mt mu bi translated"><a class="ae mv" href="https://blog.usejournal.com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68" rel="noopener ugc nofollow" target="_blank">https://blog . use journal . com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de 700 BFF 68</a></li><li id="2fd3" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://pathmind.com/wiki/lstm" rel="noopener ugc nofollow" target="_blank">https://pathmind.com/wiki/lstm</a></li><li id="0507" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://developer.nvidia.com/discover/recurrent-neural-network" rel="noopener ugc nofollow" target="_blank">https://developer . NVIDIA . com/discover/recurrent-neural-network</a></li><li id="9879" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3131-8" rel="noopener ugc nofollow" target="_blank">https://bmcbioinformatics . biomed central . com/articles/10.1186/s 12859-019-3131-8</a></li><li id="6a5b" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://theappsolutions.com/blog/development/recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://theapp solutions . com/blog/development/recurrent-neural-networks/</a></li><li id="99ec" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://www.mlq.ai/guide-to-recurrent-neural-networks-lstms/" rel="noopener ugc nofollow" target="_blank">https://www . mlq . ai/guide-to-recurrent-neural-networks-lst ms/</a></li><li id="2e19" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f" rel="noopener ugc nofollow" target="_blank">https://blog . statsbot . co/time-series-prediction-using-recurrent-neural-networks-lst ms-807 fa 6 ca 7 f</a></li><li id="bd0d" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://www.simplilearn.com/recurrent-neural-network-tutorial-article" rel="noopener ugc nofollow" target="_blank">https://www . simpli learn . com/recurrent-neural-network-tutorial-article</a></li><li id="7bdf" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://mc.ai/rnn-recurrent-neural-networks-lstm/" rel="noopener ugc nofollow" target="_blank">https://mc.ai/rnn-recurrent-neural-networks-lstm/</a></li><li id="4248" class="mk ml it kk b kl mw ko mx kr my kv mz kz na ld mr ms mt mu bi translated"><a class="ae mv" href="https://victorzhou.com/blog/intro-to-rnns/" rel="noopener ugc nofollow" target="_blank">https://victorzhou.com/blog/intro-to-rnns/</a></li></ul></div></div>    
</body>
</html>