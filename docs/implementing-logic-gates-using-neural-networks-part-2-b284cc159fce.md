# 用神经网络实现逻辑门(下)

> 原文：<https://towardsdatascience.com/implementing-logic-gates-using-neural-networks-part-2-b284cc159fce?source=collection_archive---------3----------------------->

## 与、与非和异或门

大家好！！在开始使用神经网络实现逻辑门的第 2 部分之前，您可能想先浏览一下[第 1 部分](https://medium.com/@vedantk.0704/implementing-logic-gates-using-neural-networks-part-1-f8c0d3c48332)。

从第 1 部分中，我们已经知道我们有两个输入神经元或 x 向量，值为 x1 和 x2，1 是偏差值。输入值，即 x1、x2 和 1，乘以它们各自的权重矩阵，即 W1、W2 和 W0。相应的值然后被馈送到求和神经元，在那里我们得到求和值，即

![](img/52bb760d68a7c2bf35cf9d3ac1e22085.png)

作者图片

现在，这个值被馈送到一个神经元，该神经元具有一个非线性函数(在我们的例子中为 sigmoid ),用于将输出缩放到期望的范围。如果输出小于 0.5，则 sigmoid 的比例输出为 0，如果输出大于 0.5，则为 1。我们的主要目的是找到权重值或权重向量，这将使系统能够充当特定的门。

# 实现与门

与门运算是输入之间的简单乘法运算。如果任何输入为 0，则输出为 0。为了实现 1 作为输出，两个输入都应该是 1。下面的真值表传达了同样的信息。

![](img/d43f268a3ae99976d59600b70aae45f7.png)

“与”门的真值表和使系统充当“与”和“与非”门的权值，由作者形象

由于我们有 4 个输入选择，权重必须满足所有输入点的与门条件。

# (0，0)案例

考虑输入或 x 向量为(0，0)的情况。在这种情况下，Z 的值就是 W0。现在，W0 必须小于 0，因此 z 小于 0.5，输出或ŷ为 0，满足与门的定义。如果大于 0，则 Z 通过 sigmoid 函数后的值将为 1，这违反了与门条件。因此，我们可以说，对于分辨率，W0 必须是负值。但是 W0 值多少？继续阅读…

# (0，1)案例

现在，考虑输入或 x 向量为(0，1)的情况。这里 Z 的值将是 W0+0+W2*1。这是 sigmoid 函数的输入，应该具有小于 0 的值，使得输出小于 0.5 并被分类为 0。从今以后，W0+W2<0。如果我们把 W0 的值取为-3(记住 W0 的值必须是负的)，把 W2 的值取为+2，那么结果出来就是-3+2，也就是-1，这似乎满足上面的不等式，与 and 门的条件相当。

# (1，0)案例

类似地，对于(1，0)的情况，W0 的值将是-3，W1 的值可以是+2。请记住，只要不等式不变，您可以采用权重 W0、W1 和 W2 的任何值。

# (1，1)案例

在这种情况下，输入或 x 向量是(1，1)。在这种情况下，Z 的值就是 W0+W1+W2。现在，总输出必须大于 0，以便输出为 1，满足与门的定义。从前面的场景中，我们发现 W0，W1，W2 的值分别为-3，2，2。将这些值放入 Z 等式中会产生输出-3+2+2，它是 1 且大于 0。因此，在通过 sigmoid 函数后，这将被分类为 1。

# 关于“与”和“与非”实现的最后一点说明

因此，分隔上述四点的直线是一个等式 W0+W1*x1+W2*x2=0，其中 W0 是-3，W1 和 W2 都是+2。因此，四点分隔线的等式为 x1+x2=3/2。因此，或非门的实现将类似于将权重改变为 W0 等于 3，以及 W1 和 W2 等于-2

# 继续讨论异或门

对于 XOR 门，下图左侧的真值表显示，如果有两个补码输入，则只有输出为 1。如果输入相同(0，0 或 1，1)，则输出将为 0。绘制在右侧 x-y 平面上的点给我们的信息是，它们不像 OR 和 and 门那样是线性可分的(至少在二维上)。

![](img/8f6010cb464d0f69fb50558ecb692bf6.png)

XOR 门真值表和 x-y 平面上数值的绘制，图片由作者提供

# 两种可能解决方案

为了解决上述可分离性问题，可以采用两种技术，即添加非线性特征(也称为核心技巧)或添加额外的层(也称为深度网络)

XOR(x1，x2)可以被认为是 NOR(或非(x1，x2)和(x1，x2))

![](img/b2a3dd0c3de48d70db04d8cb49adc95d.png)

实现异或门的解决方案

# 异或网络的权重

在这里，我们可以看到该层从 2 增加到 3，因为我们添加了一个正在计算 AND 和 NOR 运算的层。输入保持不变，外加偏置输入 1。右下方的表格显示了作为输入的 4 个输入的输出。这里需要注意的一个有趣的事情是，权重的总数增加到了 9。从输入层到第二层共有 6 个权重，从第二层到输出层共有 3 个权重。第二层也称为隐藏层。

![](img/e2b6db11e2bae3e6cb569d49fa7b8706.png)

网络的权重使其充当异或门，图片由作者提供

谈到整个网络的权重，从上面和第 1 部分的内容中，我们已经推导出系统作为与门和或非门的权重。我们将使用这些权重来实现 XOR 门。对于层 1，总共 6 个权重中的 3 个将与或非门的权重相同，剩余的 3 个将与与门的权重相同。因此，NOR 门的输入权重将是[1，-2，-2]，and 门的输入权重将是[-3，2，2]。现在，从第 2 层到最后一层的权重将与或非门的权重相同，即[1，-2，-2]。

# 通用逼近定理

它指出，任何函数都可以表示为具有一个隐藏层的神经网络，以达到期望的精度

# 几何解释

![](img/e0601f34093cfb7b6ed243600ae7a1b4.png)

两类 3D 图像的线性可分性

有了这个，我们可以把增加额外的层想象成增加额外的维度。在三维可视化后，X 和 O 现在看起来是可分的。红色平面现在可以分隔这两个点或类。这样的平面叫做超平面。综上所述，以上几点在更高维度上是线性可分的。