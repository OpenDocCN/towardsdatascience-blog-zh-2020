<html>
<head>
<title>Hyperparameters of Decision Trees Explained with Visualizations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用可视化解释决策树的超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf?source=collection_archive---------5-----------------------#2020-07-28">https://towardsdatascience.com/hyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf?source=collection_archive---------5-----------------------#2020-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b64" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">超参数在建立稳健模型中的重要性。</h2></div><p id="9929" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树是一种广泛使用的监督学习算法，适用于分类和回归任务。决策树是一些著名的集成学习算法的构建模块，如随机森林、GBDT 和 XGBOOST。</p><p id="a337" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树建立在反复询问问题以划分数据的基础上。例如，下图显示了一个用作预测客户流失模型的决策树。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/5ea7c97c67580dfadc5a5c11405d7b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HURnefMczvAiCIXt.png"/></div></div></figure><p id="0005" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树在机器学习领域很流行，因为它们既成功又简单。使它们非常高效的一些特性:</p><ul class=""><li id="a9e9" class="lq lr it kk b kl km ko kp kr ls kv lt kz lu ld lv lw lx ly bi translated">易于理解和解释</li><li id="a621" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated">可以处理数字和分类数据</li><li id="52ba" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated">需要很少或不需要预处理，如标准化或虚拟编码</li></ul><p id="fdbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不利的一面是，决策树容易过度拟合。它们很容易变得过于复杂，这使得它们无法很好地推广到数据集中的结构。在这种情况下，模型很可能以<strong class="kk iu">过度拟合</strong>而告终，这是机器学习中的一个严重问题。</p><p id="1363" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了克服这个问题，我们需要仔细调整决策树的超参数。在本帖中，我们将尝试使用树形可视化来全面理解这些超参数。</p><p id="69eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 scikit-learn 的一个内置数据集。葡萄酒数据集包含三个不同葡萄酒类别的 13 个特征(即列)。数据集中有 178 个样本(即行)。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="ae9f" class="mj mk it mf b gy ml mm l mn mo">import numpy as np<br/>import pandas as pd</span><span id="fbdc" class="mj mk it mf b gy mp mm l mn mo">from sklearn.datasets import load_wine<br/>X, y = load_wine(return_X_y = True)</span></pre><p id="5975" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从没有任何超参数调整的决策树分类器开始。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="5564" class="mj mk it mf b gy ml mm l mn mo">from sklearn import tree</span><span id="b525" class="mj mk it mf b gy mp mm l mn mo">clf = tree.DecisionTreeClassifier()<br/>clf.fit(X, y)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/c2895b5f93033ca43752b90f3d845b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*BiJNw7l9ctKUEzTGW1ilNg.png"/></div></figure><p id="c310" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有超参数都设置为默认设置。我们可以使用<strong class="kk iu"> plot_tree </strong>函数绘制我们的模型。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="a8bf" class="mj mk it mf b gy ml mm l mn mo">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="6037" class="mj mk it mf b gy mp mm l mn mo">plt.figure(figsize=(24,14))<br/>tree.plot_tree(clf, filled=True, fontsize=14)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mr"><img src="../Images/88d79e4695f9d7e1ba5a82b75aa2fa75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOyCxoMVOGskgIFWAw3neQ.png"/></div></div></figure><p id="8158" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型不断分割节点，直到所有节点都是纯的(即包含仅来自一个类别的样本)。让我们先了解一下 a 中的信息</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8b51386fcaa190d2df5dcde35f59d1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*G7ZTu1bP6HzgqgjD38aTBg.png"/></div></figure><p id="9e3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">盒子告诉我们。第一行表示特征的名称(即列)。因为我们没有命名列，所以显示了列的索引。<strong class="kk iu">样本</strong>表示观察值的数量(即行数)<strong class="kk iu">值</strong>表示这些样本根据目标变量的分布。</p><p id="bcd3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基尼系数是衡量杂质的一个标准。正如在<a class="ae mt" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">维基百科</a>上所述，“基尼不纯度是对从集合中随机选择的元素被错误标记的频率的度量，如果它是根据标签在子集中的分布随机标记的话”。基本上就是杂质随着随机性增加。例如，假设我们有一个盒子，里面有十个球。如果所有的球都是同样的颜色，我们就没有随机性，杂质为零。然而，如果我们有 5 个蓝色球和 5 个红色球，杂质是 1。如果你看一下叶节点(树末端的节点)，你会发现基尼等于零。</p><p id="6fd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">评估分裂质量的另一个函数是<strong class="kk iu">熵</strong>，它是不确定性或随机性的度量。一个变量的随机性越大，熵就越大。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mu"><img src="../Images/eb84f8c0a4e36dca6a6840007cc19f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*h6maUGCe3G9O7sB3.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">熵 vs 随机性</p></figure><p id="26f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用<strong class="kk iu">标准</strong>参数选择基尼或杂质。默认值是基尼。</p><blockquote class="mz na nb"><p id="1eaa" class="ki kj nc kk b kl km ju kn ko kp jx kq nd ks kt ku ne kw kx ky nf la lb lc ld im bi translated">我们通常不希望一棵树只有纯叶节点。它会太具体，可能会过拟合。</p></blockquote><p id="068c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当算法执行拆分时，主要目标是尽可能减少杂质。杂质减少得越多，分裂获得的信息量就越大。随着树变得更深，杂质减少的量变得更低。我们可以利用这一点来防止树做进一步的分裂。该任务的超参数是<strong class="kk iu">最小杂质减少</strong>。默认情况下，它被设置为零。让我们改变它，看看有什么不同。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="af8c" class="mj mk it mf b gy ml mm l mn mo">clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.2)<br/>clf.fit(X, y)<br/>plt.figure(figsize=(18,10))<br/>tree.plot_tree(clf, filled=True, fontsize=14)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/682f2dd5d593a9d78804b0f25aa8e6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6XfxjDRdNSq3l1hWLEh5uQ.png"/></div></div></figure><p id="6b99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有了一棵更小的树。考虑底部的绿色节点。它包含 65 个样本，其中 61 个属于一个类。没有必要进一步分割该节点，因为我们可以承受 65 个样本中的 4 个错误分类。如果我们继续分割那个节点，模型可能会过拟合。<strong class="kk iu">最小杂质分割</strong>参数可用于根据杂质值控制树。它设定了基尼系数的门槛。例如，如果 min _ infinity _ split 设置为 0.3，则节点需要具有大于 0.3 的基尼值才能进一步分裂。</p><p id="65d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个控制树深度的超参数是<strong class="kk iu"> max_depth </strong>。它不进行任何关于杂质或样品比率的计算。当达到 max_depth 时，模型停止分割。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="85da" class="mj mk it mf b gy ml mm l mn mo">clf = tree.DecisionTreeClassifier(max_depth=3)<br/>clf.fit(X, y)<br/>plt.figure(figsize=(20,10))<br/>tree.plot_tree(clf, filled=True, fontsize=14)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nh"><img src="../Images/e92e9fd7a6d2f45c3c6b52dd2ed053cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FegovieOf799i1adJZ7-Iw.png"/></div></div></figure><p id="5ee2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与最小杂质减少相比，最大深度灵活性较差。举个例子，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c699f0c47f103813da49f942c06a65ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*xM5wnoR1C18KJLyY5mETjg.png"/></div></figure><p id="5164" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也许不应该在左边分开。它只能区分 2 个样品，杂质减少不到 0.1。这实际上带给我们另一个超参数，即<strong class="kk iu"> min_samples_leaf </strong>。它表示叶节点上所需的最小样本数。在一起使用超参数时，我们需要小心。例如，如果我们在前一个树中将 min_samples_leaf 设置为 3，那么将不会发生分隔 2 个样本的拆分，因为我们不能拥有少于 3 个样本的叶节点。嗯，不太对。让我们看看到底会发生什么。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="7af3" class="mj mk it mf b gy ml mm l mn mo">clf = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=3)<br/>clf.fit(X, y)<br/>plt.figure(figsize=(20,10))<br/>tree.plot_tree(clf, filled=True, fontsize=14)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b71bd1cc0a913ee8db6e9f67233fd657.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*nzwkS9HV2x1jraQS9HWvXA.png"/></div></figure><p id="223d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，min_samples_leaf 实际上对模型是有害的。这并不妨碍模型进行最后的分割。此外，它还造成了额外的错误分类。因此，以这种方式使用 min_samples_leaf 是不明智的。</p><p id="584d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还可以使用<strong class="kk iu"> max_leaf_nodes </strong>参数限制叶节点的数量，该参数以最佳优先的方式增长树，直到达到 max_leaf_nodes。最佳分割是基于杂质减少来决定的。</p><pre class="lf lg lh li gt me mf mg mh aw mi bi"><span id="af4c" class="mj mk it mf b gy ml mm l mn mo">clf = tree.DecisionTreeClassifier(max_leaf_nodes=5)<br/>clf.fit(X, y)<br/>plt.figure(figsize=(20,10))<br/>tree.plot_tree(clf, filled=True, fontsize=14)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nh"><img src="../Images/6fb9923ac03b598fe820154484a5f611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VSkmd3_SY0RhRfBTCQJ5g.png"/></div></div></figure><p id="b9ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们最终得到一个有 5 个叶节点的树。</p><p id="c495" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树的另一个重要超参数是<strong class="kk iu"> max_features </strong>，它是寻找最佳分割时要考虑的特征数量。如果未指定，模型将考虑所有特征。我们的数据集中有 13 个要素。如果我们将 max_features 设置为 5，则模型随机选择 5 个特征来决定下一次分割。Max_features 参数也有助于防止模型过度拟合，但仅仅使用 max_features 是不够的。如果我们让模型变得太深，它最终会使用所有的功能。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="ab1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要仔细调整超参数，以获得具有高样本外精度的健壮决策树。我们不需要使用所有的方法。根据任务和数据集的不同，几个就足够了。如果同时使用多个超参数，请格外注意，因为其中一个可能会对另一个产生负面影响。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="7f58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>