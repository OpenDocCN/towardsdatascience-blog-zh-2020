<html>
<head>
<title>How to Build Neural Network from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从零开始构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-neural-network-from-scratch-d202b13d52c1?source=collection_archive---------35-----------------------#2020-05-25">https://towardsdatascience.com/how-to-build-neural-network-from-scratch-d202b13d52c1?source=collection_archive---------35-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/24e1fa24cca6f6ba587debff6ab58c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0r20l5q8PN3tlYayh4W64w.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">安妮·斯普拉特的照片</p></figure><p id="edc0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如何从头开始建立一个简单的神经网络的分步教程</p><h1 id="ad78" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="669e" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在这篇文章中，我们将用一个隐藏层和一个sigmoid激活函数从头开始构建我们自己的神经网络。我们将仔细研究导数和链式法则，以便对反向传播的实现有一个清晰的了解。我们的网络将能够以与Keras模拟相同的精度解决线性回归任务。</p><p id="697a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这个<a class="ae kc" href="https://github.com/arseniyturin/neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到这个项目的代码。</p><h1 id="4f2f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是神经网络？</h1><p id="4041" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们曾经将神经网络视为神经元的互连层，左侧是输入层，中间是隐藏层，右侧是输出层。视觉上更容易消化，但最终，神经网络只是一个将其他函数作为输入的大函数；并且取决于网络的深度，这些内部函数也可以将其他函数作为输入，等等。那些内部功能实际上是“层”。让我们看一下我们将要构建的网络图:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/d47cb4f53b45cda6031e2bd6f533df37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aroEBBT6eKjARQEdY5dlQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">神经网络体系结构</p></figure><p id="04ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它有一个具有两个特征的输入层，一个具有三个神经元的隐藏层和一个输出层。隐藏层中的每个神经元是一个sigmoid激活函数，它将输入值(x1，x2)、权重(w1，…，w6)和偏差(b1，b2，b3)作为输入，并产生范围从0到1的值作为输出。</p><p id="4e47" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">开始时，我们给权重和偏差分配从0到1的随机值。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/9d7116d20d858f8fb884dd617392b6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7sRCoU_pkEmOfC3g6X12g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">隐藏层中的第一个神经元</p></figure><p id="ee20" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出层仅包含一个神经元，其工作与隐藏层的神经元相似。你可能已经猜到<strong class="kf ir"> ŷ </strong>实际上是我之前提到的那个大函数。</p><p id="b269" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们上面描述的网络类型被称为<strong class="kf ir"> Dence Network </strong>，因为神经元与来自前一层的元素完全连接，在我们的情况下是<strong class="kf ir">输入</strong>。</p><h2 id="9cec" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">我们如何训练一个神经网络？</h2><p id="d6a5" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">需要理解的最关键部分是，神经网络仅通过调整权重和偏差来训练<strong class="kf ir">以最小化输出误差。训练过程由<strong class="kf ir">前馈</strong>和<strong class="kf ir">反向传播</strong>组成。前馈预测输出，反向传播调整权重和偏差以最小化输出误差，即预测值和真实值之间的差异。</strong></p><h1 id="62a6" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">前馈</h1><p id="9344" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">当我们想预测产量时，我们使用前馈函数。该函数采用输入x1和x2，这些输入值与权重和偏差一起进入隐藏层中的神经元，每个神经元返回值[0–1]；然后输出层获取这些值并产生输出。</p><p id="25e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们仔细看看隐藏层中的第一个神经元，了解它实际上在做什么。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/0b48e37fe5b63f23ad665b239ad6ae5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apCX2G5cvPTw7rbOOS9aAA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">前馈过程</p></figure><p id="3ed6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我前面提到的，每个神经元只是一个sigmoid函数:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/4e6d354a386c1123dd5e6335373d5065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALOkdaJe7YnnKI7id8mNyA.png"/></div></div></figure><p id="a993" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于线性回归，我们有斜率和截距等参数来进行预测，在神经网络中，我们有权重和偏差。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/9d7116d20d858f8fb884dd617392b6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7sRCoU_pkEmOfC3g6X12g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">隐藏层中的第一个神经元</p></figure><p id="2427" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个神经元将产生一个值[0-1],我们将用它作为输出层的输入。在一些神经网络中，没有必要将输出层作为sigmoid或任何其他激活函数，它可以只是来自前一层的值的总和。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/0dc07cc63be22936032ed14a01bc3d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXLgl30C_tFAZPjcHT5ulw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">输出层中的神经元</p></figure><p id="fed1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数的代码如下所示:</p><pre class="mf mg mh mi gt mz na nb nc aw nd bi"><span id="9f2e" class="mk lc iq na b gy ne nf l ng nh">def sigmoid(x): return 1 / (1 + np.e**-x)</span><span id="5176" class="mk lc iq na b gy ni nf l ng nh">def feedforward(x1, x2):</span><span id="3e11" class="mk lc iq na b gy ni nf l ng nh">    n1 = sigmoid(x1 * w1 + x2 * w2 + b1)<br/>    n2 = sigmoid(x1 * w3 + x2 * w4 + b2)<br/>    n3 = sigmoid(x1 * w5 + x2 * w6 + b3)<br/>    y_hat = sigmoid(n1 * w7 + n2 * w8 + n3 * w9 + b4)</span><span id="4cff" class="mk lc iq na b gy ni nf l ng nh">    return y_hat</span></pre><p id="c2a2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测完值后，我们可以使用均方差(MSE)将其与真实值进行比较。</p><h2 id="ea57" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">为什么使用偏见？</h2><p id="e3a6" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">偏置的作用是为神经元提供一个不受前几层影响的额外参数，因为它与前几层没有任何联系。</p><h1 id="15e1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">反向传播</h1><p id="0380" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">起初，我们的网络在预测方面做得很糟糕，因为权重和偏差只是随机数。现在反向传播开始帮助我们训练这些参数。</p><p id="ce07" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每次网络做出预测时，我们用MSE来与真实值进行比较，然后我们回过头来调整每个权重和偏差，以稍微减少误差。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/b4980408fe6b39be7eb228760e0786c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQLLJJYfFy4Co16ynHdckQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">均方误差(mean square error)</p></figure><p id="22c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解反向传播，我们应该知道几个要素:</p><ol class=""><li id="7b55" class="nk nl iq kf b kg kh kk kl ko nm ks nn kw no la np nq nr ns bi translated">导数——改变每个权重和偏差的方向</li><li id="16dd" class="nk nl iq kf b kg nt kk nu ko nv ks nw kw nx la np nq nr ns bi translated">链式法则——如何获取每个重量</li><li id="cd24" class="nk nl iq kf b kg nt kk nu ko nv ks nw kw nx la np nq nr ns bi translated">梯度下降——调整权重和偏差的迭代过程</li></ol><h2 id="810f" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">派生物</h2><p id="5abd" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">感谢导数，我们总是知道我们在哪个方向改变每个参数(使它稍微变大或变小)。假设我们想要调整一个权重<strong class="kf ir"> (w = 0.75) </strong>，以便使MSE稍微小一点。为了做到这一点，我们应该对这个重量取一个函数的偏导数。然后我们把数字代入导出函数，得到一个数字(<strong class="kf ir"> 0.05 </strong>)，正数或者负数。然后我们从我们的重量中减去这个数字(<strong class="kf ir"> w -= 0.05 </strong>)。调整就是这么做的。这应该发生在网络中的每个权重上。</p><p id="0770" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为网络最终是非常复杂的函数，很难找到隐藏在无数其他函数中的参数的导数。幸运的是，衍生品有一个链式法则，它简化了我们的这一过程。</p><h2 id="606b" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">链式法则</h2><p id="14cb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果我们需要找到一个函数包含另一个函数的导数，我们使用链式法则。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/fc72cdaa63f04e40e1e14a1e0f753043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lm78vLno4CLJezjHtpSznA.png"/></div></div></figure><p id="8571" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这条规则说，我们对外部函数求导，保持内部函数不变，然后我们用内部函数的导数乘以所有的东西。</p><h2 id="cd47" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">梯度下降</h2><p id="6e0f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">一旦我们知道了所有的导数，我们就可以在每次反向传播时逐渐调整每个权重和偏差。这里需要知道的重要一点是，梯度下降有一个学习率参数，它通常是一个很小的数字，我们将其添加到导数的结果中，以减慢或加快训练。</p><h2 id="b3fc" class="mk lc iq bd ld ml mm dn lh mn mo dp ll ko mp mq lp ks mr ms lt kw mt mu lx mv bi translated">训练重量1的示例</h2><p id="94f3" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">让我们来练习一下如何对<strong class="kf ir"> w1 </strong>求偏导数。一旦我们理解了它对一种重量的工作原理，对其他重量的实现就很容易了，因为在大多数情况下，这几乎是完全相同的过程。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/31c411df1076397a094838bc16e00cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kb1EuIprfXINUWXrA7zcxQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">反向传播</p></figure><p id="bf49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对<strong class="kf ir"> w1 </strong>求偏导数，我们应该从MSE函数开始。它是包含网络中所有其他函数的根函数。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/8d10275f6805f46bc28762fe402332ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*54RXNXh6qmGCFFN9VA34ew.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">举重训练</p></figure><p id="8f7b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">众所周知，MSE是真实值和预测值的平方差<strong class="kf ir"> (y-ŷ) </strong>。如果我们展开整个网络，看看<strong class="kf ir"> w1 </strong>在里面的位置，它看起来像这样:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/767c5f5575a22552194847aeef3685cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0q1SIl44WjlavZB28T019Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">神经网络的Matryoshka</p></figure><p id="0754" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在应该记住，网络是一个包含其他函数的函数，为了调整该函数中的每个参数，我们需要使用链式法则:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/dcb417f24af9834a2ab1da6ffec35dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfyYC4SlaS3ACoLq5XReug.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">链式规则表示</p></figure><p id="4076" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">剩下的就是对每个函数求导。对于<strong class="kf ir"> w1 </strong>它有四个导数，但是对于<strong class="kf ir"> w8 </strong>它只有三个，因为它位于<strong class="kf ir"> ŷ </strong>并且我们不需要去那么深的地方。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/71a51e66177fee0fd98f2e2f3e425209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-0CEQxjy8zYZOL0dw-EGw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">关于x1的导数</p></figure><p id="d46f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们得到了这个看起来可怕的方程，它解释了<strong class="kf ir"> w1 </strong>如何影响MSE。让我们详细说明每一步发生了什么。</p><p id="eaa3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们对MSE (y-ŷ)求导，也就是2(y-ŷ，因为x的导数是2x，同样的规则也适用于这里。我们没有像链式法则要求的那样，接触平方函数内部的内容。</p><p id="2fc7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们对(y-ŷ)对ŷ求偏导数，得到(0-sigmoid'(…))。记住ŷ是一个sigmoid函数。因为它是通向重量w1的主要路径。接下来，我们取另一个sigmoid函数关于<strong class="kf ir"> n1 </strong>的偏导数，最后是x1*w1的最后一个导数，也就是<strong class="kf ir"> x1 </strong>，因为<strong class="kf ir"> w1 </strong>的导数是1，系数x1保持不变。然后我们乘以每个导数，我们就可以开始了。</p><p id="f827" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">综上所述，我们得到:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/5e45b8303cd93a40c9a65bd31418693a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQAhkMzesyRmcLDURJiXLg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">权重1的反向传播</p></figure><p id="ed6b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中学习率是一个小数字，通常范围从0.01到0.05，但也可以更大。同样的逻辑，我们适用于寻找所有其他的重量和偏见。</p><p id="22dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以想象，如果我们有数百个神经元，我们将有数千个权重和偏差，因此为了说明的目的，我们将神经元的数量保持得非常少。其余代码请看<a class="ae kc" href="https://github.com/arseniyturin/neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><h1 id="fe18" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">在真实数据上测试网络</h1><p id="4837" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们的网络在根据两个参数预测房价方面做得非常好:收入中位数和平均房间数。数据取自sklearn图书馆的“california_housing”数据集。该网络收敛相当快，仅超过6个历元，结果MSE = 0.028，与我从Keras analog得到的结果完全相同。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/63918383fc74e1b71017e6f19d02b475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mh-YbKR-TEqz7YvoMm4arA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">学习过程</p></figure><h1 id="3244" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="7e8d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们的网络非常适合教育目的，但它有局限性:我们不能改变隐藏层中神经元的数量，也不能向网络中添加另一层。我们只有一个激活函数，我们的网络只能解决简单的任务，如线性回归。如果我们想用它来解决分类问题，我们需要找到交叉熵或Softmax损失函数的导数。所有这些变化都可以在当前的设置中完成。</p><p id="ba65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有任何问题或难以理解的地方，请在评论中告诉我。</p><p id="9434" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>