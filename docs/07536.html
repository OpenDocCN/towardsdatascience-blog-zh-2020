<html>
<head>
<title>8 Simple Techniques to Prevent Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">防止过度拟合的 8 个简单技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d?source=collection_archive---------5-----------------------#2020-06-07">https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d?source=collection_archive---------5-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2385" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当模型对训练数据表现良好，但对看不见的数据概括较差时，就会发生过度拟合。过拟合是机器学习中非常普遍的问题，并且已经有大量的文献致力于研究防止过拟合的方法。在下文中，我将描述八种简单的方法，通过在每种方法中只引入一种对数据、模型或学习算法的改变来减轻过度拟合。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/13abe73f362a55f968a210609a0584ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H377j9pbSHLQhkNd.png"/></div></div></figure><h1 id="50b1" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">目录</h1><p id="ccb6" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated"><a class="ae md" href="#c287" rel="noopener ugc nofollow"> 1。坚守</a> <br/> <a class="ae md" href="#d2d6" rel="noopener ugc nofollow"> 2。交叉验证</a> <br/> <a class="ae md" href="#f80d" rel="noopener ugc nofollow"> 3。数据扩充</a> <br/> <a class="ae md" href="#253a" rel="noopener ugc nofollow"> 4。特征选择</a> <br/> <a class="ae md" href="#d178" rel="noopener ugc nofollow"> 5。L1 / L2 正规化</a> <br/> <a class="ae md" href="#87f3" rel="noopener ugc nofollow"> 6。移除层数/每层单元数</a> <br/> <a class="ae md" href="#6f6a" rel="noopener ugc nofollow"> 7。辍学</a> <br/> <a class="ae md" href="#98ac" rel="noopener ugc nofollow"> 8。提前停止</a></p><h1 id="c287" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">1.拒绝(数据)</h1><p id="bc3a" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们可以简单地将数据集分成两组:训练和测试，而不是使用所有的数据进行训练。常见的拆分比例是 80%用于培训，20%用于测试。我们训练我们的模型，直到它不仅在训练集上表现良好，而且在测试集上也表现良好。这表明了良好的泛化能力，因为测试集表示未用于训练的看不见的数据。然而，这种方法需要足够大的数据集来训练，即使在分割之后。</p><h1 id="d2d6" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">2.交叉验证(数据)</h1><p id="262c" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们可以将数据集分成 k 个组(k 重交叉验证)。我们让其中一个组作为测试集(请参见保留说明)，其他组作为训练集，重复这个过程，直到每个单独的组都被用作测试集(例如，<em class="me"> k </em>重复)。与保留不同，交叉验证允许所有数据最终用于训练，但在计算上也比保留更昂贵。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mf"><img src="../Images/f0d5b3cc702f5d5f61454731cd6eea42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jLUbzKXfGhZWG25A.png"/></div></div></figure><h1 id="f80d" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">3.数据扩充(数据)</h1><p id="3996" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">更大的数据集将减少过度拟合。如果我们无法收集更多数据，并且受限于当前数据集中的数据，我们可以应用数据扩充来人为增加数据集的大小。例如，如果我们正在为图像分类任务进行训练，我们可以对我们的图像数据集执行各种图像变换(例如，翻转、旋转、重新缩放、移位)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/ff0a834435ed69c8da1bd8372a0f8c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JgP_DG16kisBAdpS.png"/></div></div></figure><h1 id="253a" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">4.特征选择(数据)</h1><p id="5121" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">如果我们只有有限数量的训练样本，每个样本都有大量的特征，我们应该只选择最重要的特征进行训练，这样我们的模型就不需要学习这么多的特征，最终会过度拟合。我们可以简单地测试不同的特征，为这些特征训练单独的模型，并评估泛化能力，或者使用各种广泛使用的特征选择方法之一。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mh"><img src="../Images/78a81fb6c4bbbc759a34c25246d23361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N3paES6IzJ8oyh9p"/></div></div></figure><h1 id="d178" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">5.L1 / L2 正则化(学习算法)</h1><p id="4a8d" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">正则化是一种技术，用于限制我们的网络学习过于复杂的模型，这可能因此而过度拟合。在 L1 或 L2 正则化中，我们可以在成本函数上添加惩罚项，以将估计的系数推向零(并且不取更多的极值)。L2 正则化允许权重向零衰减，但不衰减到零，而 L1 正则化允许权重衰减到零。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/67161ab42cc4d6208d1625a95ad8f630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*69Jgv2gwAPtOIwNh.png"/></div></div></figure><h1 id="87f3" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">6.移除层数/每层的单位数(模型)</h1><p id="e9ff" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">正如在 L1 或 L2 正则化中提到的，一个过于复杂的模型更有可能过度拟合。因此，我们可以通过删除层来直接降低模型的复杂性，并减小模型的大小。我们可以通过减少全连接层中神经元的数量来进一步降低复杂性。对于我们的任务，我们应该有一个复杂的模型来充分平衡欠适应和过适应。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mj"><img src="../Images/2424a67bf5aa0f15c52e1b51d82b4dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9RwXpjZWOrcEdhQ7.png"/></div></div></figure><h1 id="6f6a" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">7.辍学(模型)</h1><p id="71e2" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">通过对我们的层应用 dropout(一种正则化形式),我们以设定的概率忽略了网络单元的子集。使用 dropout，我们可以减少单元之间的相互依赖学习，这可能会导致过度适应。然而，对于 dropout，我们需要更多的纪元来使我们的模型收敛。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mk"><img src="../Images/74e7fc6408aeb2d895cffc01f5a848b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YCofAkhSErYvlpRT.png"/></div></div></figure><h1 id="98ac" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">8.提前停止(模型)</h1><p id="0b59" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们可以首先为任意大量的时期训练我们的模型，并绘制验证损失图(例如，使用排除)。一旦验证损失开始降低(例如，停止降低而是开始增加)，我们停止训练并保存当前模型。我们可以通过监控损失图或设置早期停止触发器来实现这一点。所保存的模型将是在不同训练时期值之间进行概括的最佳模型。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/f972d8023593b9504a3f6759ad2a0453.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/0*b4lf4K0PswVYZXdI.png"/></div></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><p id="006a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">您已经到达文章末尾！希望你现在有一个工具箱的方法来对抗过度适应⚔️.</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mt"><img src="../Images/6b5b0e57c9e9f34d4c9d72564d3f600a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3FfFlJ3-1deBbhpo.jpeg"/></div></div></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><p id="c1ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="me">为你完成这篇文章向你致敬，我希望你喜欢它</em>🎩<em class="me">。如果你对更多与 ML 相关的话题感兴趣，也可以查看一些由</em> <a class="ae md" href="https://taying-cheng.medium.com/" rel="noopener"> <em class="me"> Tim </em> </a> <em class="me">:)撰写的有趣文章。</em></p><p id="ea83" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae md" href="https://chuanenlin.medium.com/membership" rel="noopener"> <em class="me">订阅？</em>🙂</a></p></div></div>    
</body>
</html>