<html>
<head>
<title>Just Used Machine Learning in My Workout!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">刚刚在我的锻炼中使用了机器学习！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/just-used-machine-learning-in-my-workout-ff079b8e1939?source=collection_archive---------10-----------------------#2020-03-27">https://towardsdatascience.com/just-used-machine-learning-in-my-workout-ff079b8e1939?source=collection_archive---------10-----------------------#2020-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b92f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">计算机视觉在健身中的应用演示</h2></div><p id="721f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我是体重法的忠实粉丝，通常也是锻炼的忠实粉丝，但我不太喜欢去健身房。</p><p id="d560" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，在这个由于冠状病毒而被迫封锁的时期，尝试不同的方式来进行健身和训练可能是有用的。</p><p id="6ef2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">于是我问自己:有没有办法把机器学习用在这方面？我能把两种激情结合起来做点有用的东西吗？</p><p id="d245" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个主要的问题是有一种方法来验证一个练习的正确性，所以我做了一些实验并尝试了一种方法，发现…</p><p id="c00e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，不想剧透什么，就继续看下去了解一下吧！</p><h1 id="c06f" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">框定问题</h1><p id="0489" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">和往常一样，让我们从框定问题开始。我们想要实现的是使用视频作为输入，有一种方法来评估练习的正确性。</p><p id="3408" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最佳方案应该是使用实时流，但是现在让我们简单地使用一个文件，因为毕竟我们想在构建之前验证一种方法。</p><p id="b350" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，第一步应该是有一个(希望)正确执行的视频，因为它可以用作比较其他视频的基线。</p><p id="d3ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是第一个视频，在我的地下疲劳室拍摄的:)</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">基线 ok 执行</p></figure><p id="6736" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的第一个想法是使用 CNN 来建立一个分类器，但是，除了所需的例子数量之外，我不确定图像序列像素是否有助于训练一个关于练习执行中什么是错什么是对的模型。</p><p id="8134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我做了一些研究，看看是否有可能使用视频作为输入来获得不同的特征，并找到了一个很棒的库，<a class="ae mj" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose </a>，一个“用于身体、面部、手和脚估计的实时多人关键点检测库”。</p><p id="9d47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看到演示视频，我明白这可能非常有用，所以我试图将它应用到我的问题中，并有了这个…</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用 OpenPose</p></figure><p id="1a23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(我将在后面的附录中写下所有必要的设置步骤)</p><p id="cb87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你在视频中看到的，该库可以很好地跟踪不同的身体部位(使用带有 18 个关键点的 COCO 配置)</p><p id="251b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很酷的一点是，也可以输出一个 json 文件，一帧一帧地显示所有的位置，所以应该可以用数字来表示一个练习。</p><p id="28d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，做一些辅助功能，并使用 Plotly，这是这个练习看起来如何考虑 y 轴移动-跳过 x 轴，因为给定相机位置不太有用。</p><p id="874a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">姑且称之为<strong class="kh ir">“ok1”</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/f13ecf317129b3ea3325c9889175c856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qcyzXs-Tjfv1cM1PMavCA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">好习题 ok1 的分解分析</p></figure><p id="1f73" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很好，下一步是找到一种方法来比较两个不同的执行，看看是否有显著的差异。</p><p id="0b16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们首先基于这些指标进行一个视觉比较，我们称这个执行为<strong class="kh ir">“失败 1”</strong></p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">故障 1</p></figure><p id="797b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们比较一下运动的图表</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/da6c6e451c1049dac93f95560cd8652c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pm7Qls1UIebOaMtWeSvDeg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">ok1 和 fail1 的比较</p></figure><p id="98df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">存在明显的差异。</p><p id="21df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们尝试另一个失败的表现(<strong class="kh ir">“fail 2”</strong>)</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">故障 2</p></figure><p id="d549" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们与基线正确执行 ok1 进行比较</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/bce03fd5a9f9156d5a113e20c4474475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhG-Dktug3slkJ839ERP7g.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">ok1 和 fail2 之间的比较</p></figure><p id="d112" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们来比较两个好的表现(让我们称之为第二个<strong class="kh ir">“ok 2”</strong>)</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">ok2</p></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/dca22a46bde4155cfa6d9817324cd058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sX1Mm-v9Sq1wGQUG8qFRqg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">ok1 和 ok2 的比较</p></figure><p id="ede0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">曲线看起来非常相似，所以我们根据经验测试了这种方法。</p><p id="8762" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在的问题是:考虑到可能有不同的时间尺度，有没有办法评估这些单变量时间序列曲线之间的相似性？</p><p id="3b7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">原来有一种叫做<strong class="kh ir">动态时间扭曲</strong>的东西可以用来“测量两个时间序列之间的相似性”。更多<a class="ae mj" href="https://en.wikipedia.org/wiki/Dynamic_time_warping" rel="noopener ugc nofollow" target="_blank">此处</a></p><p id="3ac0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Python 中有实现吗？当然，使用<em class="mr"> tslearn.metrics </em></p><p id="b1a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以让我们分析一些数字</p><p id="3949" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先比较“ok1”和它自己</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="69eb" class="mx lc iq mt b gy my mz l na nb">dtw_value for feature nose_y is 0.0 <br/>dtw_value for feature right_shoulder_y is 0.0 <br/>dtw_value for feature right_elbow_y is 0.0 <br/>dtw_value for feature right_wrist_y is 0.0 <br/>dtw_value for feature left_shoulder_y is 0.0 <br/>dtw_value for feature left_elbow_y is 0.0 <br/>dtw_value for feature left_wrist_y is 0.0 <br/>dtw_value for feature right_hip_y is 0.0 <br/>dtw_value for feature right_knee_y is 0.0 <br/>dtw_value for feature right_ankle_y is 0.0 <br/>dtw_value for feature left_hip_y is 0.0 <br/>dtw_value for feature left_knee_y is 0.0 <br/>dtw_value for feature left_ankle_y is 0.0 <br/>dtw_value for feature right_eye_y is 0.0 <br/>dtw_value for feature left_eye_y is 0.0 <br/>dtw_value for feature right_ear_y is 0.0 <br/>dtw_value for feature left_ear_y is 0.0 <br/>dtw_value for feature background_y is 0.0</span></pre><p id="d096" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以 0 值是最大相似度，分数越低表示相似度越高</p><p id="ee5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们现在尝试测量 ok1 和 fail1</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="7ee1" class="mx lc iq mt b gy my mz l na nb">dtw_value for feature nose_y is 188.00378744123748<br/>dtw_value for feature right_shoulder_y is 155.97642562435527<br/>dtw_value for feature right_elbow_y is 156.39925059973916<br/>dtw_value for feature right_wrist_y is 17.982641407757672<br/>dtw_value for feature left_shoulder_y is 13.5329438534267<br/>dtw_value for feature left_elbow_y is 158.0005797757085<br/>dtw_value for feature left_wrist_y is 27.544745106825722<br/>dtw_value for feature right_hip_y is 12.151614599714703<br/>dtw_value for feature right_knee_y is 191.94638493339747<br/>dtw_value for feature right_ankle_y is 223.23781654997444<br/>dtw_value for feature left_hip_y is 263.0165952996121<br/>dtw_value for feature left_knee_y is 195.8379463587177<br/>dtw_value for feature left_ankle_y is 227.95958454954243<br/>dtw_value for feature right_eye_y is 288.64055642788685<br/>dtw_value for feature left_eye_y is 192.9321060365538<br/>dtw_value for feature right_ear_y is 192.15753964939807<br/>dtw_value for feature left_ear_y is 190.20149442225735<br/>dtw_value for feature background_y is 189.09276308989186</span></pre><p id="a3b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我发现采用一个整体值来获得更简洁的信息很有用，比如中位值</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="99b5" class="mx lc iq mt b gy my mz l na nb">dtw_median : 189.6471287560746</span></pre><p id="d344" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ok1 和 fail2 之间的比较</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="10d1" class="mx lc iq mt b gy my mz l na nb">dtw_value for feature nose_y is 65.28319682858675<br/>dtw_value for feature right_shoulder_y is 38.87442004120449<br/>dtw_value for feature right_elbow_y is 37.75683113715981<br/>dtw_value for feature right_wrist_y is 18.907807197028447<br/>dtw_value for feature left_shoulder_y is 19.50736795264806<br/>dtw_value for feature left_elbow_y is 45.031636992674414<br/>dtw_value for feature left_wrist_y is 36.101698713495466<br/>dtw_value for feature right_hip_y is 13.248353503737741<br/>dtw_value for feature right_knee_y is 39.45295418596681<br/>dtw_value for feature right_ankle_y is 49.27277845829276<br/>dtw_value for feature left_hip_y is 65.78598402395453<br/>dtw_value for feature left_knee_y is 38.59586190254078<br/>dtw_value for feature left_ankle_y is 44.54850474482842<br/>dtw_value for feature right_eye_y is 64.17832564035923<br/>dtw_value for feature left_eye_y is 50.02819053653649<br/>dtw_value for feature right_ear_y is 50.233695101993064<br/>dtw_value for feature left_ear_y is 45.21480605000976<br/>dtw_value for feature background_y is 42.15576012017812</span><span id="9d57" class="mx lc iq mt b gy nc mz l na nb">dtw_median : 43.35213243250327</span></pre><p id="2aea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ok1 和 ok2 的比较</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="2795" class="mx lc iq mt b gy my mz l na nb">dtw_value for feature nose_y is 16.023831603583467<br/>dtw_value for feature right_shoulder_y is 11.24889546622242<br/>dtw_value for feature right_elbow_y is 11.94796246520719<br/>dtw_value for feature right_wrist_y is 20.509653605070962<br/>dtw_value for feature left_shoulder_y is 19.65007578484111<br/>dtw_value for feature left_elbow_y is 14.486468134089847<br/>dtw_value for feature left_wrist_y is 7.208783392501132<br/>dtw_value for feature right_hip_y is 14.17544715061928<br/>dtw_value for feature right_knee_y is 25.759515076957445<br/>dtw_value for feature right_ankle_y is 43.123581089700735<br/>dtw_value for feature left_hip_y is 83.91171946754521<br/>dtw_value for feature left_knee_y is 23.860467116131673<br/>dtw_value for feature left_ankle_y is 44.80603683656928<br/>dtw_value for feature right_eye_y is 91.27560108813313<br/>dtw_value for feature left_eye_y is 31.263050533657154<br/>dtw_value for feature right_ear_y is 25.735729785455852<br/>dtw_value for feature left_ear_y is 12.39151408383979<br/>dtw_value for feature background_y is 11.887661376402017</span><span id="cda8" class="mx lc iq mt b gy nc mz l na nb">dtw_median : 20.079864694956036</span></pre><p id="95f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，该值似乎可以用作一个指标，根据要找到的阈值来比较两次执行的正确性。</p><p id="6dc0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个经验性的反检验，让我们从这个值开始尝试其他例子</p><p id="0ed3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ok1 和 check1 -&gt;中位值 82。46860 . 66666666666</p><p id="f6b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ok2 和 check 2--&gt;中位值 19660 . 666666666616</p><p id="669f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确定并检查 3 -&gt;中位值 25。48860 . 88888888861</p><p id="a4ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">似乎低于 30 的中位值可能是起始阈值</p><p id="ea74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们在视频上看到他们</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">不允许跳跃！</p></figure><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">不完全的</p></figure><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">好的。</p></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="688a" class="lb lc iq bd ld le nk lg lh li nl lk ll jw nm jx ln jz nn ka lp kc no kd lr ls bi translated">结论</h1><p id="2404" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">这只是这个实验的开始:假设这是正确的方法，有很多开放点，例如:</p><ul class=""><li id="c0a2" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated">不同身高的人怎么样？他们也需要一个个人基线，或者可以概括？</li><li id="4996" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">不同的相机位置怎么样？</li><li id="569c" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">如何推断阈值？</li><li id="9088" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">如何对执行中的错误给出更详细的建议？</li><li id="e29f" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">如何在连续视频流中处理练习的相关部分？</li><li id="4bf2" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">可以跟踪使用哑铃等工具进行的练习吗？(提示:是的，但是也有特定的对象检测库)</li></ul><p id="5be0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我有一些想法要检查，我会在未来做，即使因为可能性是美妙的。</p><p id="02bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新:我从这开始增加了其他功能。在这里<a class="ae mj" rel="noopener" target="_blank" href="/used-again-machine-learning-in-my-workout-this-time-building-a-personal-trainer-3dfae9730c2b">查看</a></p><p id="ea3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象一个带有摄像头的工作站</p><ul class=""><li id="4025" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated">当您用面部识别输入时，识别您</li><li id="eb61" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">加载您的“wod”(当天锻炼)</li><li id="b838" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">检查给出提示的练习的正确性</li><li id="3296" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">向在场或可能与几十人一起参加远程课程的培训师发出执行不当的信号，允许他/她采取纠正措施。</li></ul><p id="2eb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">甚至培训也可以根据之前的课程和个人的整体情况进行定制。</p><p id="fcdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一如既往，我对这些技术可能实现的目标和想象感到惊讶，使用它们非常有趣。</p><p id="ffc7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，祝你锻炼愉快，注意安全。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="afc4" class="lb lc iq bd ld le nk lg lh li nl lk ll jw nm jx ln jz nn ka lp kc no kd lr ls bi translated">附录</h1><h2 id="2fc7" class="mx lc iq bd ld od oe dn lh of og dp ll ko oh oi ln ks oj ok lp kw ol om lr on bi translated">Docker+OpenPose</h2><p id="c93f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我没有直接安装 OpenPose 和所有必要的依赖项，而是选择了 Docker 方法。你可以在这里找到图片:<a class="ae mj" href="https://hub.docker.com/r/garyfeng/docker-openpose/" rel="noopener ugc nofollow" target="_blank">https://hub.docker.com/r/garyfeng/docker-openpose/</a></p><p id="9c07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，对于实时方法来说，使用容器可能不是正确的解决方案，因为有很多延迟，但我没有尝试过其他解决方案，所以我不能肯定地说。</p><p id="5892" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是在运行之前，需要使用 GPU 运行容器，否则 OpenPose 不会启动。这里是所有做这件事的指令(用 Invidia GPU):【https://github.com/NVIDIA/nvidia-docker<a class="ae mj" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank"/></p><p id="cca2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您将在命令中看到“privileged”和-e DISPLAY = $ DISPLAY-v/tmp/. X11-UNIX:/tmp/. X11-UNIX 部分，如果需要的话，它们用于访问容器内的摄像机。</p><p id="093c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在启动 docker 命令之前，请确保执行:</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="2aa1" class="mx lc iq mt b gy my mz l na nb">xhost +</span></pre><p id="5332" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以容器可以连接。</p><p id="5386" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，就发射</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="0d30" class="mx lc iq mt b gy my mz l na nb">docker run --privileged --gpus all -v &lt;host path to share&gt;:/<strong class="mt ir">data</strong>  -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -it garyfeng/docker-openpose:latest</span></pre><p id="cd95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">过一会儿，您将进入容器内的 bash shell</p><p id="9f43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你查看<a class="ae mj" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose 文档</a>，有很多参数，但是让我们看几个例子</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="ab48" class="mx lc iq mt b gy my mz l na nb">build/examples/openpose/openpose.bin --face</span></pre><p id="9230" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它应该打开相机，并开始检测你脸上的关键点。</p><p id="4638" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我之前用来创建数据的命令是:</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="31d9" class="mx lc iq mt b gy my mz l na nb">build/examples/openpose/openpose.bin --video /<strong class="mt ir">data</strong>/&lt;input file&gt;  --write_video /<strong class="mt ir">data</strong>/&lt;ouptut file&gt; --no_display --write_keypoint_json /<strong class="mt ir">data</strong>/&lt;folder with json output files&gt;</span></pre><p id="b20f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意启动容器时挂载的“data”文件夹。如果您更改了它，请确保根据命令进行相应的调整。</p><h2 id="2b86" class="mx lc iq bd ld od oe dn lh of og dp ll ko oh oi ln ks oj ok lp kw ol om lr on bi translated">Python 代码</h2><p id="fd83" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">现在让我们看一些 Python 代码来处理本文中使用的数据</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="6403" class="mx lc iq mt b gy my mz l na nb">import pandas as pd<br/>import os<br/>import numpy as np</span><span id="6aa6" class="mx lc iq mt b gy nc mz l na nb">def read_pose_values(path, file_name):<br/>    try:<br/>        path, dirs, files = next(os.walk(path))<br/>        df_output = pd.DataFrame()<br/>        for i in range(len(files)):<br/>            if i &lt;=9:<br/>                pose_sample = pd.read_json(path_or_buf=path+'/' +  file_name + '_00000000000' + str(i) + '_keypoints.json', typ='series')<br/>            elif i &lt;= 99:<br/>                pose_sample = pd.read_json(path_or_buf=path+'/' + file_name + '_0000000000' + str(i) + '_keypoints.json', typ='series')<br/>            else:<br/>                pose_sample = pd.read_json(path_or_buf=path+'/' + file_name + '_000000000' + str(i) + '_keypoints.json', typ='series')    <br/>            df_output = df_output.append(pose_sample, ignore_index = True)<br/>        return df_output<br/>    except Exception as e:<br/>        print(e)</span></pre><p id="ed4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这用于返回一个 DataFrame，其中包含在 OpenPose json 输出路径中找到的所有 json(注意，如果有 1000 个以上的文件，它会中断——肯定要修复:)</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="c8d4" class="mx lc iq mt b gy my mz l na nb">'''<br/>Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,<br/>Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,<br/>Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,<br/>LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16,<br/>Left Ear – 17, Background – 18<br/>'''<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="2e1a" class="mx lc iq mt b gy nc mz l na nb">def transform_and_transpose(pose_data, label):<br/>    output = pd.DataFrame()<br/>    for i in range(pose_data.shape[0] -1):<br/>        if len(pose_data.people[i]) &gt; 0: <br/>            output = output.append(pd.DataFrame(pose_data.people[i][0]['pose_keypoints']).T)</span><span id="f0ea" class="mx lc iq mt b gy nc mz l na nb"># drop confidence detection<br/>    for y in range(2,output.shape[1] ,3):<br/>        output.drop(columns=[y], inplace=True</span><span id="4faa" class="mx lc iq mt b gy nc mz l na nb"># rename columns<br/>    output.columns = ['nose_x', 'nose_y', 'right_shoulder_x', 'right_shoulder_y', 'right_elbow_x', 'right_elbow_y',<br/>                      'right_wrist_x', 'right_wrist_y', 'left_shoulder_x', 'left_shoulder_y', 'left_elbow_x', 'left_elbow_y',<br/>                      'left_wrist_x', 'left_wrist_y', 'right_hip_x', 'right_hip_y', 'right_knee_x', 'right_knee_y',<br/>                      'right_ankle_x', 'right_ankle_y', 'left_hip_x', 'left_hip_y', 'left_knee_x', 'left_knee_y',<br/>                      'left_ankle_x', 'left_ankle_y', 'right_eye_x', 'right_eye_y', 'left_eye_x', 'left_eye_y',<br/>                      'right_ear_x', 'right_ear_y', 'left_ear_x','left_ear_y','background_x', 'background_y']<br/> <br/>    # interpolate 0 values<br/>    output.replace(0, np.nan, inplace=True)<br/>    output.interpolate(method ='linear', limit_direction ='forward', inplace=True)</span><span id="34e2" class="mx lc iq mt b gy nc mz l na nb">return output</span></pre><p id="1378" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们基于 COCO 设置和基本插值对列进行重命名，如果值为 0(例如，当鼻子在引体向上栏后面时):</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="8977" class="mx lc iq mt b gy my mz l na nb">def model_exercise(json,name,label):<br/>    df_raw = read_pose_values(json,name)<br/>    return transform_and_transpose(df_raw,label)</span><span id="dac3" class="mx lc iq mt b gy nc mz l na nb">df_exercise_1 = model_exercise('&lt;path to json&gt;','&lt;file_name&gt;','&lt;label&gt;')</span></pre><p id="f6fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将所有这些放在一起，使用函数得到最终的数据帧。</p><p id="2a32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们看一些图表:</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="10ef" class="mx lc iq mt b gy my mz l na nb">import plotly.graph_objects as go<br/>from plotly.subplots import make_subplots<br/>def plot_y_features(df):<br/>    fig = make_subplots(rows=3, cols=6, start_cell="top-left")<br/>    r = 1<br/>    c = 1<br/>    X = pd.Series(range(df.shape[0]))<br/>    for feature in df.columns:<br/>        if '_y' in feature:<br/>            fig.add_trace(go.Scatter(x=X, y=df[feature], name=feature),<br/>            row=r, col=c)<br/>            fig.update_xaxes(title_text=feature, row=r, col=c)<br/>            if c &lt; 6:<br/>                c = c + 1<br/>            else:<br/>                c = 1<br/>                r = r + 1<br/>    fig.update_layout(title_text="Exercise y-axis movements breakdown", width=2000, height=1000)<br/>    fig.show()</span><span id="db22" class="mx lc iq mt b gy nc mz l na nb">plot_y_features(df_exercise_1)</span></pre><p id="6975" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">画出所有位置的支线剧情。</p><p id="86de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在比较两个练习:</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="3198" class="mx lc iq mt b gy my mz l na nb">def plot_comparison_y_features(df1,df2):<br/>    fig = make_subplots(rows=3, cols=6, start_cell="top-left")<br/>    r = 1<br/>    c = 1<br/>    X1 = pd.Series(range(df1.shape[0]))<br/>    X2 = pd.Series(range(df2.shape[0]))<br/>    for feature in df1.columns:<br/>        if '_y' in feature:<br/>            fig.add_trace(go.Scatter(x=X1, y=df1[feature], name=feature + '_ok'),row=r, col=c)<br/>            fig.add_trace(go.Scatter(x=X2, y=df2[feature], name=feature + '_fail'),row=r, col=c)<br/>            fig.update_xaxes(title_text=feature, row=r, col=c)<br/>            if c &lt; 6:<br/>                c = c + 1<br/>            else:<br/>                c = 1<br/>                r = r + 1<br/>    fig.update_layout(title_text="Exercise y-axis movements breakdown comparison", width=2000, height=1000)<br/>    fig.show()</span><span id="eee1" class="mx lc iq mt b gy nc mz l na nb">plot_comparison_y_features(df_exercise_1, df_ok2)</span></pre><p id="6c32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后是动态时间弯曲部分:</p><pre class="ly lz ma mb gt ms mt mu mv aw mw bi"><span id="e9b1" class="mx lc iq mt b gy my mz l na nb">def evaluate_dtw(df1,df2,feature, plot=False):<br/>    x1 = range(df1.shape[0])<br/>    y1 = df1[feature].values</span><span id="e1a8" class="mx lc iq mt b gy nc mz l na nb">    x2 = range(df2.shape[0])<br/>    y2 = df2[feature].values<br/>   <br/>    dtw_value = evaluate_dtw(df1[feature],df2[feature])<br/>      print("dtw_value for feature {} is {}".format(feature,     dtw_value))<br/>    return dtw_value</span><span id="4915" class="mx lc iq mt b gy nc mz l na nb">def evaluate_dtw_values(df1,df2,plot = False):<br/>    dtw_values = []<br/>    for feature in df1.columns:<br/>        if '_y' in feature:<br/>            dtw_values.append(dtw(df1,df2,feature,plot))<br/>    return pd.DataFrame(dtw_values)</span></pre><p id="2573" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅此而已！谢谢你。</p></div></div>    
</body>
</html>