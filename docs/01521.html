<html>
<head>
<title>Decision Trees and Random Forests — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林—解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-and-random-forest-explained-8d20ddabc9dd?source=collection_archive---------2-----------------------#2020-02-11">https://towardsdatascience.com/decision-tree-and-random-forest-explained-8d20ddabc9dd?source=collection_archive---------2-----------------------#2020-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cf1d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细的理论解释和代码示例</h2></div><p id="0e31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树和随机森林是用于分类和回归问题的监督学习算法。这两种算法最好放在一起解释，因为随机森林是一堆决策树的组合。当然，在创建和组合决策树时，需要考虑某些动态和参数。在这篇文章中，我将解释决策树和随机森林是如何工作的，以及使用这些模型时需要考虑的关键点。</p><p id="56cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树建立在反复询问问题以划分数据的基础上。用决策树的可视化表示来概念化分区数据更容易:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/36e3696b671015bb11c47ce0bd2e2594.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*tSkwQjyFHIKCIH1ZLMgeyg.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated"><a class="ae lq" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><p id="9a85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是给动物分类的决策树。第一次分割是基于动物的大小。虽然问题看起来是“动物有多大？”，以“动物大于 1m 吗？”因为我们想在每一步把数据点分成两组。随着树越来越深，问题也越来越具体。</p><p id="f843" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你在每一步问什么是最关键的部分，并极大地影响决策树的性能。例如，假设数据集的“要素 A”范围为 0 到 100，但大多数值都在 90 以上。在这种情况下，首先要问的问题是“特征 A 大于 90 吗？”。问“特性 A 大于 50 吗？”是没有意义的因为它不会给我们太多关于数据集的信息。</p><p id="6365" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的目标是在每次分区时尽可能增加模型的预测性，以便模型不断获得关于数据集的信息。随机分割要素通常不会给我们提供关于数据集的有价值的信息。增加节点纯度的分割更能提供信息。节点的纯度与该节点中不同类的分布成反比。要问的问题是以增加纯度或减少杂质的方式选择的。</p><p id="efdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有两种方法来衡量一个分裂的质量:<strong class="kk iu">基尼系数</strong>和<strong class="kk iu">熵。</strong>它们主要测量杂质，并给出相似的结果。Scikit-learn 默认使用基尼指数，但您可以使用<strong class="kk iu">标准</strong>参数将其更改为熵。</p><p id="dc7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基尼杂质</strong></p><p id="5fce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如在<a class="ae lq" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">维基百科</a>上所述，“基尼杂质是一种衡量从集合中随机选择的元素被错误标记的频率，如果它是根据标签在子集中的分布随机标记的话”。基本上就是杂质随着随机性增加。例如，假设我们有一个盒子，里面有十个球。如果所有的球都是同样的颜色，我们就没有随机性，杂质为零。然而，如果我们有 5 个蓝色球和 5 个红色球，杂质是 1。</p><p id="fea5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">熵和信息增益</strong></p><p id="c54d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">熵</strong>是不确定性或随机性的度量。一个变量的随机性越大，熵就越大。均匀分布的变量具有最高的熵。例如，掷一个公平的骰子有 6 个概率相等的可能结果，所以它有均匀的分布和高熵。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/3fdac90ad1babfe9c58c890a0b2343d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*FitdYVcN2GYOasuQrlWBIQ.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">熵 vs 随机性</p></figure><p id="3863" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择导致更纯节点的分裂。所有这些都表明“信息增益”，这基本上是分裂前后熵的差异。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/c579b68b00609de488115c92374189ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*DsKNneSrQuO0pdkwqHpuVg.png"/></div></figure><p id="9467" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当选择一个特征进行分割时，决策树算法试图实现</p><ul class=""><li id="2f40" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">更多的预测</li><li id="9bd6" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">杂质少</li><li id="2e6e" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">低熵</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="b80b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经了解了问题(或拆分)是如何选择的。下一个话题是题数。我们要问多少问题？我们什么时候停止？什么时候我们的树足以解决我们的分类问题？所有这些问题的答案将我们引向机器学习最重要的概念之一:<strong class="kk iu">过拟合</strong>。模型可以一直提问，直到所有节点都是纯的。纯节点只包括来自一个类的数据点。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b79f7564d03a6aa254af4aa531c9f7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9hasX4NiPmzxiJT_Fw0DpQ.png"/></div></figure><p id="a008" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型可以一直提问(或者拆分数据)，直到所有的叶子节点都是纯的。然而，这将是一个过于具体的模型，不能很好地概括。它在训练集上实现了高精度，但在新的、以前看不到的数据点上表现不佳。</p><p id="cda5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你在下面的可视化中看到的，在深度 5，模型明显过度拟合。类之间的狭窄区域可能是由异常值或噪声造成的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0c4e22c8b695b59ca61b115a2ece081f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*UxGvG5MWolhehlpz8fwAHA.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fb03f0370fd2929ddc611e08eb6a6e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*awD5E1vOA7u_kYDSlEc61A.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">深度= 5 ( <a class="ae lq" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html" rel="noopener ugc nofollow" target="_blank">图源</a>)</p></figure><p id="e69a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">控制或限制树的深度以防止过度拟合是非常重要的。Scikit-learn 提供超参数来控制决策树的结构:</p><p id="247d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> max_depth: </strong>一棵树的最大深度。树的深度从 0 开始(即根节点上的深度为零)。如果未指定，模型会一直分割，直到所有叶子都是纯的，或者直到所有叶子包含的样本数少于 min_samples_split。</p><p id="b612" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> min_samples_split: </strong>拆分内部节点所需的最小样本数。只要一个节点的样本(数据点)多于 min_samples_split 参数指定的数量，该算法就会一直拆分节点。</p><p id="2568" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最小杂质减少:</strong>进行拆分的目的是减少杂质(或不确定性)，但并不是所有的拆分都能同样达到这个目的。此参数设置进行分割的阈值。如果这种分裂导致杂质的减少大于或等于阈值，则节点将被分裂。</p><p id="d70f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里可以看到 DecisionTreeClassifier() <a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">的所有超参数列表。</a></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="c522" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated"><strong class="ak">随机森林</strong></h1><p id="06a1" class="pw-post-body-paragraph ki kj it kk b kl nn ju kn ko no jx kq kr np kt ku kv nq kx ky kz nr lb lc ld im bi translated">随机森林是许多决策树的集合。随机森林是使用一种叫做<strong class="kk iu"> bagging </strong>的方法构建的，其中每个决策树都被用作并行估计器。如果用于分类问题，结果基于从每个决策树接收的结果的多数投票。对于回归，叶节点的预测是该叶中目标值的平均值。随机森林回归取决策树结果的平均值。</p><p id="5ea6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林降低了过度拟合的风险，并且准确性比单个决策树高得多。此外，随机森林中的决策树并行运行，因此时间不会成为瓶颈。</p><p id="883c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林的成功高度依赖于使用不相关的决策树。如果我们使用相同或非常相似的树，总体结果将不会比单个决策树的结果有太大的不同。随机森林通过<strong class="kk iu">自举</strong>和<strong class="kk iu">特征随机性</strong>实现不相关的决策树。</p><ul class=""><li id="25e7" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">Bootsrapping 是从带有替换的训练数据中随机选择样本。它们被称为 bootstrap 样本。下图清楚地解释了这一过程:</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/92a6799e5adbf3518e53522ab03ecd7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*SfHes0xaGHVeUorVdkR8XQ.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated"><a class="ae lq" href="https://www.researchgate.net/figure/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some_fig2_322179244" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><ul class=""><li id="c223" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">通过为随机森林中的每个决策树随机选择特征来实现特征随机性。随机森林中每棵树使用的特征数量可通过<strong class="kk iu"> max_features </strong>参数控制。</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/afd1ecc7e9e8b4269b625dbba17c7e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*VmIM_umHWWEYCm00_B3ScQ.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">特征随机性</p></figure><p id="22df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自助样本和特征随机性为随机森林模型提供了不相关的树。</p><p id="34cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林引入了一个额外的参数:</p><p id="6937" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> n_estimators: </strong>代表一片森林中的树木数量。在一定程度上，随着森林中树木数量的增加，结果会变得更好。然而，在某个点之后，添加额外的树不会改进模型。请记住，添加额外的树总是意味着更多的计算时间。</p><h1 id="17b5" class="mv mw it bd mx my nu na nb nc nv ne nf jz nw ka nh kc nx kd nj kf ny kg nl nm bi translated"><strong class="ak">利弊</strong></h1><p id="7507" class="pw-post-body-paragraph ki kj it kk b kl nn ju kn ko no jx kq kr np kt ku kv nq kx ky kz nr lb lc ld im bi translated"><strong class="kk iu">决策树</strong></p><p id="4bfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优点:</p><ul class=""><li id="d6f8" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">通常不需要归一化或缩放要素</li><li id="3dd1" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">适合处理混合的特征数据类型(连续、分类、二进制)</li><li id="2273" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">容易理解</li></ul><p id="3298" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">缺点:</p><ul class=""><li id="8dbf" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">倾向于过度拟合，并且需要集合以便很好地概括</li></ul><p id="5f5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">随机森林</strong></p><p id="62e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优点:</p><ul class=""><li id="0b9d" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">针对许多不同问题的强大、高度精确的模型</li><li id="c737" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">像决策树一样，不需要标准化或缩放</li><li id="4a9a" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">像决策树一样，可以一起处理不同的特征类型</li><li id="1c5c" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">并行运行树，因此性能不会受到影响</li></ul><p id="e516" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">缺点:</p><ul class=""><li id="5d47" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">与快速线性模型(即朴素贝叶斯)相比，对于高维数据集(即文本分类)不是一个好的选择</li></ul><h1 id="54b2" class="mv mw it bd mx my nu na nb nc nv ne nf jz nw ka nh kc nx kd nj kf ny kg nl nm bi translated"><strong class="ak">使用 Scikit-Learn 的示例</strong></h1><p id="4c31" class="pw-post-body-paragraph ki kj it kk b kl nn ju kn ko no jx kq kr np kt ku kv nq kx ky kz nr lb lc ld im bi translated">决策树和随机森林也可以用于回归问题。我之前做过一个关于预测二手车价格的项目。我将跳过所有的预处理，数据清理和 EDA 部分，并显示模型部分。对于回归任务，我使用了线性回归和随机森林回归。毫不奇怪，随机森林回归器有更好的性能。</p><p id="0bf0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我从一个人们用来卖二手车的网站上搜集数据。经过探索性的数据分析，我决定用网站上 ad 上的车龄、公里数、发动机大小和时长。目标变量当然是汽车的价格。数据集包含 6708 个数据点。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e28ed1cd797a4dc0ce7d4616951538e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*U1PsstLkNaolWlem0pmogw.png"/></div></figure><p id="2a3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了一个 RandomForestRegressor()，max_depth 设置为 5。由于它不是一个非常大和复杂的数据集，我只使用了 10 个估计器(决策树):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/675e682a06fe5d570a55972ec7a227c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*-j3amZGSCi5GOt8d8oFHpQ.png"/></div></figure><p id="e1fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个简单的随机森林回归模型在训练和测试数据集上都达到了大约 90%的准确率:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8de9c92a8d76c94656ea7697b88ce600.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*IEN0wub7vZ8jPYAUjbLciQ.png"/></div></figure><p id="8c1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果不使用适当的超参数，随机森林也可能过度拟合。例如，如果我在 max_depth 设置为 20 的情况下运行同一个模型，该模型会过度拟合。它在训练数据集上取得了更好的准确性，但在测试数据集上表现不佳:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/dc59bb28dabab2493818a5101981c3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*EGRKPA5oj8ZcshA1atpROA.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">max_depth = 20 会导致过度拟合</p></figure><p id="f4aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="815a" class="mv mw it bd mx my nu na nb nc nv ne nf jz nw ka nh kc nx kd nj kf ny kg nl nm bi translated"><strong class="ak">我的其他帖子</strong></h1><ul class=""><li id="3f56" class="lx ly it kk b kl nn ko no kr od kv oe kz of ld mc md me mf bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd">支持向量机—解释</a></li><li id="a510" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/predicting-used-car-prices-with-machine-learning-fea53811b1ab">用机器学习预测二手车价格</a></li><li id="11c9" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/data-cleaning-and-analysis-with-a-bonus-story-36b3ae39564c">数据清理和分析，附带奖励故事</a></li></ul><h1 id="d6ed" class="mv mw it bd mx my nu na nb nc nv ne nf jz nw ka nh kc nx kd nj kf ny kg nl nm bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="3048" class="lx ly it kk b kl nn ko no kr od kv oe kz of ld mc md me mf bi translated"><a class="ae lq" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html" rel="noopener ugc nofollow" target="_blank">https://jakevdp . github . io/python datascience handbook/05.08-random-forests . html</a></li><li id="9613" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated"><a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree classifier . html</a></li><li id="2779" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Bootstrapping _(statistics)</a></li><li id="b5e0" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated"><a class="ae lq" href="https://www.researchgate.net/figure/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some_fig2_322179244" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/An-example-of-bootstrap-sampling-Since-objects-is-sub-sampled-with-replacement-some _ fig 2 _ 322179244</a></li></ul></div></div>    
</body>
</html>