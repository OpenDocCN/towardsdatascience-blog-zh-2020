<html>
<head>
<title>Machine Learning - Visualized</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习-可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-visualized-11965ecc645c?source=collection_archive---------20-----------------------#2020-08-08">https://towardsdatascience.com/machine-learning-visualized-11965ecc645c?source=collection_archive---------20-----------------------#2020-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2c70" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解机器学习的视觉方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b56b41b4850101dee1c7b84d706d0117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtfzpKK4zfVsCelY9ge-pw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">列宁·艾斯特拉达在<a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="204c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">机器学习导论</h1><p id="c17e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在传统的硬编码方法中，我们通过编程让计算机执行某项任务。当它收到某个输入时，我们准确地告诉它该做什么。用数学术语来说，这就像是我们编写了<strong class="lq ir"> <em class="mk"> f(x) </em> </strong> <em class="mk"> </em>这样当用户将输入<strong class="lq ir"> <em class="mk"> x </em> </strong>输入到<strong class="lq ir"> <em class="mk"> f(x) </em> </strong>中时，它会给出正确的输出<strong class="lq ir"> <em class="mk"> y </em> </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/aa30ef7a3b19cf6e633855c327fa16c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/0*xxYgrtvjoLhYB3BT"/></div></figure><p id="6d23" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">然而，在机器学习中，我们有一大组输入<strong class="lq ir"> <em class="mk"> x </em> </strong>和相应的输出<strong class="lq ir"> <em class="mk"> y </em> </strong>但没有函数<strong class="lq ir"> <em class="mk"> f(x) </em> </strong>。这里的目标是找到将输入<strong class="lq ir"><em class="mk"/></strong>转换为输出<strong class="lq ir"><em class="mk"/></strong>的<strong class="lq ir"> <em class="mk"> f(x) </em> </strong>嗯，这可不是件容易的事。在本文中，我们将了解这是如何发生的。</p><h1 id="5b8e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">资料组</h1><p id="5bad" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了可视化数据集，让我们制作我们的合成数据集，其中每个数据点(输入<strong class="lq ir"> <em class="mk"> x </em> </strong>)是三维的，使其适合绘制在 3D 图表上。我们将在以原点(0，0，0)为中心的簇中生成 250 个点<strong class="lq ir">(簇 0) </strong>。生成一个类似的 250 个点的群集<strong class="lq ir">(群集 1) </strong>，但不以原点为中心。两个集群都相对接近，但有一个清晰的分离，如下图所示。这两个聚类是两类数据点。大绿点代表整个数据集的质心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2161b098f5c10baa5c1c6ed7e7352856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*_Pm8fTurVldA-nH89fte-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fef8" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">生成数据集后，我们将通过减去平均值并除以标准差来对其进行归一化。这样做是为了使数据以零为中心，并将数据集中每个维度的值映射到一个公共比例。这加快了学习的速度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/bd404d65de69004372a10e87966ad65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XLmyh9Ql39EMLfL4"/></div></div></figure><p id="81c3" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">数据将保存在数组<strong class="lq ir"> X </strong>中，其中包含标准化点的 3D 坐标。我们还将生成一个数组<strong class="lq ir"> Y </strong>,根据 3D 点属于哪个簇，每个索引的值为 0 或 1。</p><h1 id="3e9b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">可学习功能</h1><p id="fa04" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在我们已经准备好了数据，我们可以说我们有了<strong class="lq ir"> <em class="mk"> x </em> </strong>和<strong class="lq ir"> <em class="mk"> y. </em> </strong>我们知道数据集是线性可分的，这意味着有一个平面可以将数据集分成两个聚类，但是我们不知道这样一个最优平面的方程是什么。现在，我们就随便坐一架飞机吧。</p><p id="0959" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">函数 f(x)应该将一个 3D 坐标作为输入，并输出一个介于 0 和 1 之间的数字。如果该数字小于 0.5，则该点属于聚类 0，否则，它属于聚类 1。让我们为这个任务定义一个简单的函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e58fc78d121d8d7e4748a560c3e93aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/0*PidUtOO4v1njEyNr"/></div></figure><p id="cb85" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk"> x </em> </strong>:形状的输入张量(num_points，3) <br/> W:随机选择的形状(3，1)的权重(参数)<br/> B:随机选择的形状(1，1)的偏差(参数)<br/> Sigmoid:映射 0 和 1 之间的值的函数</p><p id="7aa1" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">让我们花一点时间来理解这个函数的含义。在应用 sigmoid 函数之前，我们只是创建了一个从 3D 坐标(输入)到 1D 输出的线性映射。因此，<strong class="lq ir">该功能会将整个 3D 空间挤压到一条线上</strong>，这意味着原始 3D 空间中的每个点现在都会位于这条线上的某个位置。由于这条线将延伸到无穷大，我们使用<strong class="lq ir"> Sigmoid </strong>函数将其映射到<strong class="lq ir"> [0，1] </strong>。因此，对于每个给定的输入，<strong class="lq ir"> <em class="mk"> f(x) </em> </strong>将输出一个介于 0 和 1 之间的值。</p><p id="2e18" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">记住 W 和 B 是随机选择的，所以 3D 空间会被挤压到一条随机的线上。这个变换的判定边界是使<strong class="lq ir"> <em class="mk"> f(x) </em> = 0.5 </strong>的点集。想想为什么！当 3D 空间被挤压到 1D 线上时，整个平面被映射到线上的值 0.5。这个平面是 f(x)的判定边界。理想情况下，它应该将数据集分成两个集群，但是由于随机选择了<strong class="lq ir"> W </strong>和<strong class="lq ir"> B </strong>，因此该平面随机定向，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/33165b2d270a5a5cff844f196bb72b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*noEkeV1lecUexZIN-w9oDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="75c4" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">我们的目标是为<strong class="lq ir"> W </strong>和<strong class="lq ir"> B </strong>找到正确的值，以将数据集划分为两个集群的方式来定位该平面(决策边界)。完成后，生成如下所示的平面。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/eeeb8b5b285fd41ba0955af4f5c5c4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krCfnBRuBQCYN_qbVdB1_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="b330" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">失败</h1><p id="3696" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">所以，我们现在处于起点(随机决策边界)，我们已经定义了目标。我们需要一个指标来决定我们离目标有多远。分类器的输出是形状张量(num_points，1)，其中每个值在<strong class="lq ir">【0，1】</strong>之间。如果你仔细想想，这些值只是属于聚类 1 的点的概率。所以，我们可以说:</p><ul class=""><li id="3e0a" class="mw mx iq lq b lr mm lu mn lx my mb mz mf na mj nb nc nd ne bi translated">f(x) = P(x 属于簇 1)</li><li id="2de0" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">1-f(x) = P(x 属于群集 0)</li></ul><p id="76c4" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">说[ <strong class="lq ir"> <em class="mk"> 1-f(x)，f(x) </em> </strong> ]分别在簇 0 和簇 1 上形成概率分布也不会错。这是<strong class="lq ir">预测的概率分布</strong>。我们确切地知道数据集中的每个点属于哪个聚类(从<strong class="lq ir"> <em class="mk"> y </em> </strong>)。所以，我们也有了<strong class="lq ir">真实概率分布</strong>为:</p><ul class=""><li id="4f97" class="mw mx iq lq b lr mm lu mn lx my mb mz mf na mj nb nc nd ne bi translated">[0，1]当 x 属于簇 1 时</li><li id="7835" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">[1，0]当 x 属于群集 0 时</li></ul><p id="ecef" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">计算两个概率分布之间不一致的一个很好的度量是<strong class="lq ir">交叉熵</strong>函数。因为我们只处理两个类，所以我们可以使用<strong class="lq ir">二元交叉熵(BCE)。</strong>py torch 的<strong class="lq ir"> torch.nn </strong>模块中有此功能。如果预测的概率分布与真实的概率分布非常相似，则该函数将返回一个小值，反之亦然。我们可以对所有数据点的这个值进行平均，并将其用作测试分类器性能的参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8a8aa37bc43ed74c6482e02a2fa96e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/0*K-7_zNwqZtK39Vpi"/></div></figure><p id="f1cd" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这个值称为损耗，从数学上讲，我们现在的目标是最小化这个损耗。</p><h1 id="f16d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">培养</h1><p id="322d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">既然我们已经从数学上定义了我们的目标，那么我们如何实际地达到我们的目标呢？换句话说，我们如何找到<strong class="lq ir"> W </strong>和<strong class="lq ir"> B </strong>的最佳值？为了理解这一点，我们将看一看一些基本的微积分。回想一下，我们目前有随机值用于<strong class="lq ir"> W </strong>和<strong class="lq ir"> B </strong>。学习或训练或达到目标或减少损失的过程可以分为两步:</p><ol class=""><li id="d625" class="mw mx iq lq b lr mm lu mn lx my mb mz mf na mj nl nc nd ne bi translated"><strong class="lq ir">前向传播:</strong>我们通过分类器<strong class="lq ir"> <em class="mk"> f(x) </em> </strong>输入数据集，并使用<strong class="lq ir"> BCE </strong>找到<strong class="lq ir">损失</strong>。</li><li id="8831" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nl nc nd ne bi translated"><strong class="lq ir">反向传播:</strong>利用损失，调整<strong class="lq ir"> W </strong>和<strong class="lq ir"> B </strong>的值，使<strong class="lq ir">损失</strong>最小。</li></ol><p id="5dcc" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">以上两步会反复重复，直到损失停止减少。在这种情况下，我们说我们已经达到了目标！</p><h1 id="e61f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">反向传播</h1><p id="7d9a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">前向传播很简单，上面已经讨论过了。然而，有必要花点时间来理解<strong class="lq ir">反向传播</strong>，因为它是机器学习的关键。回想一下，我们在<strong class="lq ir"> W </strong>中有 3 个参数(变量),在<strong class="lq ir"> B </strong>中有 1 个。因此，我们总共有 4 个值要优化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/8a0a6d357a31cd4a3fdb7c7b8555d27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_L49mfURkizDPyqNvQ6qbg.png"/></div></div></figure><p id="0ba4" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">一旦我们有了前向传播的损失，我们将计算损失函数相对于分类器中每个变量的梯度。如果我们为每个参数的不同值绘制损失图，我们可以看到，在每个参数的特定值处，损失最小。我绘制了每个参数的<strong class="lq ir">损失与参数</strong>的关系图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d4c79251e0ce1ec3641943ccebc704ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*mZO1h0udlpJeyEgwR6kpyA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1472" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这里需要注意的一点是，对于这些参数中的每一个参数，在特定值时损耗最小，如红点所示。</p><p id="b89f" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">让我们考虑第一个情节，讨论 w1 将如何优化。对于其他参数，该过程保持不变。最初，W 和 B 的值是随机选择的，因此<strong class="lq ir"> (w1，loss) </strong>将被随机放置在这条曲线上，如绿点所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c8630685234894b6c5d0f0e8daf8d62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*hPUXPOlYb0WHUkQ3a8ecgg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b6ea" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">现在，目标是到达红点，从绿点开始。换言之，我们需要走下坡路。观察曲线在绿点处的斜率，我们可以看出，增加 w1(向右移动)会降低损耗，从而使绿点更靠近红点。在数学术语中，如果损失相对于 w1 的梯度为负，则增加 w1 以向下移动，反之亦然。因此，w1 应更新为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ea13e5c8948ad8105ba6944b3104ff5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/0*zdDUkbwkWArVMXs2"/></div></figure><p id="d6f4" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">上面的方程被称为<strong class="lq ir">梯度下降方程</strong>。这里，<strong class="lq ir"> learning_rate </strong>控制我们想要增加或减少 w1 的多少。如果 learning_rate 很大，更新也会很大。这可能导致 w1 越过红点，从而错过最佳值。如果该值太小，w1 将永远无法到达红点。你可以尝试不同的学习速率值，看看哪个效果最好。一般来说，像<strong class="lq ir"> 0.01 </strong>这样的小值在大多数情况下都适用。</p><p id="fb83" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在大多数情况下，单次更新不足以优化这些参数；因此，向前传播和向后传播的过程循环重复，直到损耗停止进一步降低。让我们来看看实际情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b9a002e2db9dd0baa80516c3a06ee383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*IUuaq1NgGWsxMOFItibC_A.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者 Gif</p></figure><p id="3e01" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">要做的一个重要观察是，最初绿点移动很快，当它逐渐接近最小值时慢下来。最初几个时期(当绿点远离最小值时)期间的大斜率(梯度)是对参数的大更新的原因。随着绿点接近最小值，梯度减小，因此更新变得缓慢。其他三个参数以完全相同的方式并行训练。另一个重要的观察结果是，曲线的形状随时代而变化。这是因为其他三个参数<strong class="lq ir"> (w2，w3，b) </strong>也在并行更新，并且每个参数都对损耗曲线的形状有贡献。</p><h1 id="835b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设想</h1><p id="fba2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们看看决策边界是如何随着参数的更新而实时更新的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ba82894c0f3d4cd82d38fc4fffc52f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Yn5VRkJfIx91tLjT1g8DoQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者 Gif</p></figure><h1 id="cea3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">那都是乡亲们！</h1><p id="7ebf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果你成功了，向你致敬！在这篇文章中，我们采用了一种视觉方法来理解机器学习是如何工作的。到目前为止，我们已经看到了如何使用简单的 3D 到 1D 映射，<strong class="lq ir"><em class="mk">【f(x)</em></strong>，来将决策边界(2D 平面)拟合到线性可分离数据集(3D)。我们讨论了如何使用前向传播来计算损耗，然后使用反向传播来计算损耗相对于参数的梯度，并在训练循环中重复更新参数。</p><p id="3f48" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">有什么建议请留言评论。我定期写文章，所以你应该考虑关注我，在你的订阅中获得更多这样的文章。</p><p id="c060" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">如果你喜欢这篇文章，你可能也会喜欢这些:</p><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/face-landmarks-detection-with-pytorch-4b4852f5e9c4"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">用 Pytorch 检测人脸标志点</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">想知道 Snapchat 或 Instagram 如何将惊人的滤镜应用到你的脸上吗？该软件检测你的关键点…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi kp nu"/></div></div></a></div><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/principal-component-analysis-visualized-17701e18f2fa"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">主成分分析-可视化</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">使用主成分分析(PCA)的数据压缩</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oj l of og oh od oi kp nu"/></div></div></a></div><p id="929f" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">访问我的<a class="ae kv" href="http://arkalim.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a>，了解更多关于我和我的工作。</p></div></div>    
</body>
</html>