# 人工智能中的公平

> 原文：<https://towardsdatascience.com/fairness-in-a-i-5d3ceaaf649?source=collection_archive---------42----------------------->

![](img/0caeaf2ac81e171766d35a60f1c85ecf.png)

来源:维基媒体([https://commons . Wikimedia . org/wiki/File:Domenico _ Becca fumi _ 012 . jpg](https://commons.wikimedia.org/wiki/File:Domenico_Beccafumi_012.jpg))

> 做好人容易，难的是做公正人。维克多·雨果

# **简介**

人工智能系统的普及不再是一个新事物，从产品、电影推荐到打车服务，它无处不在。随着时间的推移，它们的采用也越来越普及。公平是基于个人或群体的固有或后天特征而对他们没有任何偏见或偏袒，因此一个不太公平的制度会偏向某一类个人。

# **AI 不公平的问题**

有许多著名的案例强调了人工智能系统中公平的重要性。最近的一篇论文《不完美的图像化:GANs 加剧面部数据增强和 Snapchat 自拍镜头偏见的影响》强调了同样的事情。它说，“在这篇论文中，我们表明，在数据沿着某些轴(如性别、种族)表现出偏差的情况下，生成敌对网络(GANs)的故障模式加剧了生成数据中的偏差。”许多顶级研究人员一直在谈论偏见，在这个方向上有很多积极的研究。

COMPAS 预测一个犯罪是否可能再次犯罪。与其他种族相比，这一制度在某种程度上歧视非裔美国人。问题是数据是有偏见的，但它们也不是很透明，这加剧了情况。后来的一项研究表明，与非专家做出的预测相比，该系统的准确率仅为 65%左右。后来还发现，即使没有种族和性别等敏感特征，这种风险也是可以预测的。如果 COMPAS 考虑到公平的立场，这样的错误是可以避免的，尤其是在这样一个关键的领域

发现偏见的另一个非常有趣的领域是医疗保健。题为“剖析用于管理人口健康的算法中的种族偏见”的论文发现，一种特殊的算法可以帮助医院和保险机构根据训练数据确定哪个病人将从针对高风险个人的高端医疗服务中受益更多，其中黑人病人与白人的比例为 7T1。尽管这代表了现实，但这种不平衡的数据问题通常需要缓解。

# **偏置**

深入一点，简化一下我们手头的问题，有两类偏差，算法偏差和数据偏差。

与数据相关的常见偏差有历史偏差、表示偏差、测量偏差、评估偏差、聚合偏差、人口偏差、抽样偏差、内容生产偏差、时间偏差、流行性偏差、观察者偏差和资金偏差。显示偏差的一个很好的例子是辛普森悖论，在这个悖论中，子群的特征与聚合时相比是非常不同的。这意味着，当组中的组件之间有足够的相似性时，我们需要在一个级别上聚合数据，这通常很难实现。因此，我们需要确定何时聚合以及聚合多少，这不是基于我们的方便，而是取决于数据需要如何分离。

说到算法，我们可以把歧视分为直接歧视、间接歧视、系统歧视、统计歧视、可解释歧视和不可解释歧视。系统性歧视的一个很好的例子是亚马逊的人工智能招聘算法，它在本质上有点性别歧视。

检查这些系统中是否存在偏见和歧视行为的方法取决于具体情况。就优先顺序而言，发现一种偏向优于另一种偏向的重要性，以及首先解决哪种歧视纯粹是习惯性质的。但是，我们应该努力从算法和数据中排除尽可能多的偏见，并努力在有效性和公平性之间保持健康的平衡。

# **公平的工具**

领导者有许多有趣的方法来实现人工智能中的公平。我想提一下这方面非常有趣的想法:

ML 公平健身房依赖于通过使用模拟来理解 ML 决策系统的长期影响的基本思想，因此试图创建一个复制的社会动态系统。该仿真框架还可以扩展到多智能体交互环境。“公平机器学习的延迟影响”等论文告诉我们，考虑动态和时间因素是多么重要。

IBM 的 AI Fairness 360 是一个解决数据和算法公平性问题的开源工具。它实现了一些研究论文中提到的技术，并为我们提供了偏差检测、偏差缓解和偏差解释工具。

命运:人工智能中的公平、问责、透明和道德在微软的这一产品中，我们获得了极其有效的工具来评估可视化仪表板和偏差缓解算法。这是比较系统的性能和公平性之间的权衡的好工具。

就连欧盟委员会(EU commission)关于人工智能的白皮书也在事后强调了公平。但正如我们慢慢看到的那样，目前活跃的研究正在朝着更好理解的方向发展，并取得了巨大进展。随着这些算法解释能力的提高，追踪偏差并进行必要的干预以确保公平性将变得相对容易得多。诸如《概念主义:深入神经网络》、《可解释性的构建模块》、特征可视化等论文展示了在这些方向上的进展。当谈到可解释的人工智能时，现在有许多工具可供我们使用，我们可以用它们来理解甚至非常复杂的黑盒算法是如何工作的。工具是 Lime，Shap，使用代理更简单的模型和特征重要性图是非常有帮助的。对于高级，非结构化数据应用程序，如深度学习技术，如 GradCam 和注意力可视化，也因可解释性而变得流行。

# **确保 AI 公平实践的实践**

Google 还提供了一些公平的做法，这些做法基本上基于两个主要的理念。确保算法如何做出决策的透明度，并组建不同性质的团队。主要的想法是捕捉关于数据和算法的许多不同的观点，以确保偏见的问题可以从各个角度进行攻击。除了团队中有来自不同领域的人之外，开源社区也可以作为一个扩展团队。社区团体也有助于提高认识和确保透明度。还应该更加警惕地监控模型漂移以及系统的部署后性能。应该对数据的来源、数据收集方法、数据预处理、数据后处理、信息技术标记、可能存在的敏感领域(如种族、性别、宗教)以及数据是否足够多样和平衡(就所有存在的类别而言)进行深入研究。

在最近的论文“更广泛的影响”中有一个新的部分，它也涵盖了论文中算法使用的伦理方面。这表明研究人员越来越敏感，不仅要开发更精确的系统，还要开发更公平的系统。最著名的深度学习和机器学习会议 NeurIPS 2020 公布了更广泛影响部分的指导方针如下:

作者必须声明其作品的广泛影响，包括伦理方面和未来的社会后果。作者应该讨论积极和消极的结果，如果有的话。例如，作者应该讨论谁可能从这项研究中受益，谁可能从这项研究中处于不利地位，系统失败的后果是什么，任务/方法是否利用了数据中的偏差。如果作者认为这不适用于他们，作者可以简单地说明这一点。

# **结论**

总之，我们可以看到这项研究，以及工程界，现在正在认真对待人工智能中的不公平问题，我们可以看到好的工作正在出现。我觉得在未来，这将成为这些人工智能系统满足公平标准的先决条件，无论是训练数据的使用还是算法的使用。一个棘手的问题是，要在日益复杂的系统中追踪错误的偏差，以及对其进行训练的大量数据。一个很好的例子可以是 GPT-3 750 亿参数语言模型(深度学习系统),它是在大到 2000 GB 的语料库上训练的。如果对这些系统的理解和关于公平的研究的进展速度与新方法的发展一致，那么未来是安全的，我们可以看到一个更安全和公平的空间。在未来，我们可能会看到一个特定的机构来确保这些系统的公平性，该机构由来自不同领域的专家组成，类似于 FDA。这可能还需要开发标准化的程序来检查偏见和其他道德标准，以免为时过晚，这也应该可以很好地适应巨大的数据源。

# **参考文献**

[https://www . Reuters . com/article/us-Amazon-com-jobs-automation insight/Amazon-scraps-secret-ai-recruiting-tool-that-showed-biasagainst-women-iduskcn 1 MK 08g](https://www.reuters.com/article/us-amazon-com-jobs-automationinsight/amazon-scraps-secret-ai-recruiting-tool-that-showed-biasagainst-women-idUSKCN1MK08G)

[https://arxiv.org/pdf/1803.04383.pdf](https://arxiv.org/pdf/1803.04383.pdf)

[https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf)

[https://www . Microsoft . com/en-us/research/publication/fair learn-a-toolkit-for-assessing-and-improving-fair-in-ai/](https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/)

[https://blog . tensor flow . org/2019/12/fairness-indicators-fair-ml systems . html](https://blog.tensorflow.org/2019/12/fairness-indicators-fair-MLsystems.html)

https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/

[https://ai . Google blog . com/2020/02/ml-fairness-gym-tool-forexploring-long . html](https://ai.googleblog.com/2020/02/ml-fairness-gym-tool-forexploring-long.html)

[https://advances.sciencemag.org/content/4/1/eaao5580](https://advances.sciencemag.org/content/4/1/eaao5580)

[http://www . crj . org/assets/2017/07/9 _ Machine _ bias _ rejoinder . pdf](http://www.crj.org/assets/2017/07/9_Machine_bias_rejoinder.pdf)

[https://science.sciencemag.org/content/366/6464/447](https://science.sciencemag.org/content/366/6464/447)