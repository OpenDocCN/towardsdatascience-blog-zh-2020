<html>
<head>
<title>Regularization with Ridge, Lasso, and Elastic Net Regressions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用岭、套索和弹性网回归进行正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-regularization-and-how-do-i-use-it-f7008b5a68c6?source=collection_archive---------37-----------------------#2020-06-22">https://towardsdatascience.com/what-is-regularization-and-how-do-i-use-it-f7008b5a68c6?source=collection_archive---------37-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a1ac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概述3种常见正则化技术的差异-山脊、套索和弹性网。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f6a845b69e7ab1d416be9d81a0e74277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCmM-qnx00zndgvMfQV60Q.jpeg"/></div></div></figure><p id="da60" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">你已经训练了一个回归模型，你的R回来了，看起来很好——几乎好得不能再好了。当然，下一个合乎逻辑的步骤是看看测试数据集的进展如何。剧透警告:它不会有接近成功的训练集。</p><p id="4be1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是一种相当普遍的现象，被称为“过拟合”。过度拟合有一个相反的极性，叫做欠拟合。用专业术语来说，过度拟合意味着你建立的模型的参数比数据能够证明的要多。<a class="ae lz" href="https://github.com/bsamaha/Blog-Post-Ridge-and-Lasso" rel="noopener ugc nofollow" target="_blank">点击此链接查看包含这篇博文笔记本的GitHub资源库。</a></p><blockquote class="ma"><p id="905e" class="mb mc it bd md me mf mg mh mi mj lp dk translated">与计算机打交道有一种让我们脱离现实的方式，过度拟合/欠拟合模型就是证明。例如，在建筑中，计算机辅助设计(CAD)使绘图员能够画出极限公差，例如0.001英寸。如果你画一个平面图，告诉挖掘机挖一个34.963英寸深的沟渠，你真的能指望重型机械有那种精度吗？你能指望一个在野外工作的木匠把木材切割到0.001英寸吗？凭经验我可以告诉你，这是不会发生的。沟渠大约有36英寸(3英尺)深，木板可能会被修圆到最接近的1/8英寸，或者如果你幸运的话，1/16英寸。这是一个现实世界中经常发生的过度拟合计算机模型计算的例子。</p></blockquote><p id="bd67" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">一个真实世界的欠拟合例子是一个煤气炉，它只有两个输入设置，最大热和关。因为你产出的食物要么被烤焦了，要么还是生的，所以你几乎不可能把任何东西都煮熟。</p><p id="4751" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能会问<em class="mp">所有这些与正规化有什么关系？</em>答案是<strong class="kw iu">一切</strong>。通过降低模型对训练数据中某些噪声的敏感度，向模型中添加适量的偏差有助于做出更准确的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"><div class="gh gi mq"><img src="../Images/be8bc7bc0e06a7ad10ce07ce18ea3cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VLT3FrlV-CF-DH4M8XGkw.png"/></div></a><p class="mr ms gj gh gi mt mu bd b be z dk translated">点击图片阅读更多Sci-Kit Learn</p></figure><p id="2bf4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在蓝点上方的图中，是来自现实世界的样本数据点。这些样本与黄线的距离，即“真实函数”，被称为数据的“噪声”。样本点到蓝线的距离被称为我们模型的“误差”。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="bd78" class="na nb it mw b gy nc nd l ne nf"># Equation of True Function<br/><strong class="mw iu">def</strong> true_fun(X):<br/>    <strong class="mw iu">return</strong> <a class="ae lz" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.cos.html#numpy.cos" rel="noopener ugc nofollow" target="_blank">np.cos</a>(1.5 * <a class="ae lz" href="https://docs.scipy.org/doc/numpy/reference/constants.html#numpy.pi" rel="noopener ugc nofollow" target="_blank">np.pi</a> * X)</span></pre><p id="8b3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">欠拟合模型的预测根本不能解释真实函数的样本数据。真实函数比一个一次的简单线性方程更复杂。如果你需要复习<a class="ae lz" href="https://www.mathsisfun.com/algebra/degree-expression.html" rel="noopener ugc nofollow" target="_blank">如何确定多项式函数的次数，请参考这个网站</a>。</p><p id="d36f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个模型的一个很好的类比是，在多项选择测试中，每个问题的答案都是“C”。当然，你可能会在几个问题上走运，但你会答错绝大多数。</p><p id="fcc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">过度拟合模型的预测线几乎完美地描述了真实函数——甚至更多。该模型的预测输出是一个超级复杂的15次多项式函数。随着模型的复杂性(程度)增加，模型开始不太关心其预测的误差，而更关心预测它所获得的训练数据的值。花点时间沿着过度拟合模型的预测线追踪，看看它离真正的函数有多远。</p><p id="77b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">继续我们的考试类比，过度拟合模型找到了我们考试的答案并记住了那个答案。当我们给模型一个没有答案的考试会发生什么？很有可能它会悲惨地失败。</p><p id="a4e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">合适的模特在金发区。它既不太简单也不太复杂。作为一个8次多项式函数，该模型仍然具有复杂性，但该模型在看穿样本数据中的噪声方面做得很好。这允许模型极大地减少来自过度拟合模型的误差，并且更准确地预测真实函数值。这个模型拟合的黄金地带就是我们试图通过正则化来实现的。</p><h1 id="d113" class="ng nb it bd nh ni nj nk nl nm nn no np jz nq ka nr kc ns kd nt kf nu kg nv nw bi translated">正则化技术</h1><p id="bbf7" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">岭和套索回归技术扩展了简单的线性回归。在继续之前，让我们回顾一下OLS的倒退:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/676a7bb1c71141f80baeb64bf04f51cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*CDlqcUI-nZ-uYIjmjnEojg.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">使用RSS作为成本函数的多变量OLS回归</p></figure><p id="84db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">成本函数衡量模型输出与实际数据之间的差异。线性回归有多个成本函数，如<a class="ae lz" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank"> RSS </a>、<a class="ae lz" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>、<a class="ae lz" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank"> RMSE </a>。在上例中，成本函数是RSS(y _ actual-y _ predicted)。预测值可以用你的回归线方程代替。您的特征系数，也称为斜率，是(mⱼ).这些特征系数是我们用正则化技术处理的对象。</p><blockquote class="od oe of"><p id="2fc1" class="ku kv mp kw b kx ky ju kz la lb jx lc og le lf lg oh li lj lk oi lm ln lo lp im bi translated">注意:在这篇文章中，我将预测器和特征作为同义词使用。预测值是模型中使用的独立变量(x变量)。如果您正在构建一个身体质量指数估计器，高度很可能是您的预测因素之一。</p></blockquote><p id="40e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在构建模型时，无论是哪种类型，您都必须执行适当的数据清理和探索技术，以完全理解您的数据。本博客中的所有正则化技术都应用了基于预测系数的惩罚。这意味着你必须在应用这些算法之前标准化你的数据。</p><p id="5f58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些技术的假设取决于应用正则化技术的模型的类型。<a class="ae lz" href="https://medium.com/swlh/my-guide-to-understanding-the-assumptions-of-ordinary-least-squares-regressions-b180f81801a4" rel="noopener">如果你需要重温一下OLS模型的假设，请看看我之前的博文。</a></p><h2 id="0fb9" class="na nb it bd nh oj ok dn nl ol om dp np ld on oo nr lh op oq nt ll or os nv ot bi translated">里脊回归</h2><p id="9c5a" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">岭回归是OLS成本函数的一个小扩展，随着模型复杂性的增加，它会给模型增加一个惩罚。数据集中的predictors(mⱼ越多，r值就越高，模型过度拟合数据的可能性就越大。岭回归通常被称为L2范数正则化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5738fb12615c5f4d9a6b5af49de7272f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*iswpgU9v5YUAtMFyTO-AGA.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">岭成本函数-请注意λ(λ)乘以预测值的平方和</p></figure><p id="fa05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请记住，目标是最小化成本函数，因此惩罚项(λ * sum(mⱼ)越大，模型的性能就越差。此函数会因您的模型有太多或太大的预测值而惩罚您的模型。岭回归的目的是减少这些预测因子的影响，从而降低数据过度拟合的几率。如果我们设置λ = 0，那么这将是一个正常的OLS回归。</p><p id="0020" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">岭回归最常见的用途是<em class="mp">抢先</em>解决过度拟合问题。当您必须保留所有预测值时，岭回归是处理多重共线性的好工具。它通过缩小预测值的大小来解决共线性问题，但从未消除它们。</p><blockquote class="od oe of"><p id="6179" class="ku kv mp kw b kx ky ju kz la lb jx lc og le lf lg oh li lj lk oi lm ln lo lp im bi translated"><strong class="kw iu">如果有许多大约相同量级的预测值，岭回归效果很好。这意味着所有预测器预测目标值的能力相似。</strong></p></blockquote><h2 id="9419" class="na nb it bd nh oj ok dn nl ol om dp np ld on oo nr lh op oq nt ll or os nv ot bi translated">套索回归</h2><p id="ee87" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">当你看着下面的等式，心里想“这看起来几乎和岭回归一样”嗯，你大部分都是对的。Lasso不同于岭回归，它对预测值(mⱼ)的绝对值求和，而不是对平方值求和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/45644f26a3452c6629c3f5991e442cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*A2xY9OCX3meTjWUvHhmx_Q.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">套索成本函数-请注意λ(λ)乘以预测值的绝对值之和</p></figure><p id="5379" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Lasso是首字母缩略词，代表“最小绝对收缩和选择算子”<em class="mp">由于罚项不是平方，有些值可以达到0。当预测器系数(mⱼ)达到0时，该预测器不会影响模型。</em></p><p id="33e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于Lasso的要素选择属性，它非常适合多重共线性。解决两个预测变量的共线性的最有效方法之一是删除其中一个。然而，移除是有代价的。可能存在变量集合共线的情况，并且仍然提供进行估计的价值。Lasso将任意选择一个预测器进行预测，并放弃另一个。</p><p id="0f0b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果很少有重要的预测因素，并且其他因素的大小接近于零，那么Lasso往往表现良好。换句话说<strong class="kw iu">，</strong>一些变量是比其他预测值更好的目标值预测值。</p><h2 id="2dfc" class="na nb it bd nh oj ok dn nl ol om dp np ld on oo nr lh op oq nt ll or os nv ot bi translated">弹性净回归</h2><p id="ecd3" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">那么，如果我不想选呢？如果我不知道自己想要什么或需要什么怎么办？弹性净回归是作为对套索回归的批判而产生的。虽然它有助于功能选择，但有时您不希望主动删除功能。正如你可能已经猜到的，弹性网是套索和脊回归的结合。因为我们已经知道了岭和套索回归是如何起作用的，所以我就不赘述了。请参考<a class="ae lz" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elastic%20net#sklearn.linear_model.ElasticNet" rel="noopener ugc nofollow" target="_blank"> sci-kit learn的文档。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/f93df0b6514daf854839d3a447e791bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HnUpY4x_La-5yUFD_xc3g.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">λ₁是套索罚分(L1)，λ₂是岭回归罚分(L2)</p></figure><p id="4efe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如上图所示，现在有两个λ项。λ₁是回归套索部分的“alpha”值，λ₂是岭回归方程的“alpha”值。当使用sci-kit learn的弹性网络回归时，阿尔法项是λ₁:λ₂.的一个比率<strong class="kw iu">当设置比率= 0时，它作为岭回归，当比率= 1时，它作为套索回归。介于0和1之间的任何值都是脊线和套索回归的组合。</strong></p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h1 id="da6f" class="ng nb it bd nh ni pe nk nl nm pf no np jz pg ka nr kc ph kd nt kf pi kg nv nw bi translated">如何使用这些回归技术</h1><p id="b4ac" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">现在好戏开始了！我假设你已经知道如何建立一个基本的线性模型。如果没有，你会在这个博客的相应笔记本中看到。使用套索和岭回归非常相似-除了“阿尔法值”。<a class="ae lz" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearch#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"> Sci-kit learn的GridSearchCV </a>搜索一系列值，为您的超参数找到最佳值。</p><h2 id="7491" class="na nb it bd nh oj ok dn nl ol om dp np ld on oo nr lh op oq nt ll or os nv ot bi translated">套索回归</h2><p id="ce5f" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">下面是我用来构建下图结果图的代码。这个模型是使用Sci-Kit Learn的波士顿住房数据集创建的。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="b799" class="na nb it mw b gy nc nd l ne nf">from sklearn.model_selection import GridSearchCV</span><span id="b9cb" class="na nb it mw b gy pj nd l ne nf"># Create an array of alpha values to test<br/># Start np.linspace value is 10**-10 because a value of 0 throws warnings<br/>alphas = np.logspace(-10, 1, 1000,base=10)</span><span id="d07e" class="na nb it mw b gy pj nd l ne nf"># Create dictionary key,value pair of alpha values<br/>tuned_parameters = [{'alpha': alphas}]</span><span id="ad91" class="na nb it mw b gy pj nd l ne nf"># Specify number of folds for cross_validation<br/>n_folds = 5</span><span id="0cde" class="na nb it mw b gy pj nd l ne nf"># Create grid search instance using desired variables<br/>clf_lasso = GridSearchCV(lasso, tuned_parameters, cv=5, refit=True)<br/>clf_lasso.fit(x_train_scaled, y_train)<br/>lasso_scores = clf_lasso.cv_results_['mean_test_score']</span><span id="7ab2" class="na nb it mw b gy pj nd l ne nf"># Plot the results<br/>plt.figure().set_size_inches(8, 6)<br/>plt.plot(alphas, lasso_scores)<br/>plt.xlabel('Alpha Value')<br/>plt.ylabel('Model CV Score')<br/>plt.title('Lasso Regression Alpha Demonstration')<br/>plt.axvline(clf_lasso.best_params_['alpha'], color='black', linestyle='--')<br/>print(f'The optimal alpha value is :{clf_lasso.best_params_["alpha"]}')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/690ee88afaedc7a11a0a7c3f5adac5a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*FI-8W9Kx9Yy_99eX2eFJkQ.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">波士顿住房数据的优化Lasso回归</p></figure><p id="7d00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，最佳alpha值是0.0106，而不是0。这证明了添加这一点alpha比纯线性回归产生了更高的交叉验证分数。从这张图表中很容易看出，随着alpha值的增加，CV值接近并达到0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/02afed36160babafec7c4ccb506b7656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wW1YrAQqwFw7n9vnm9gegg.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">在alpha值为1时，几乎2/3的特征被移除，并且测试R仅降低了大约0.1！</p></figure><p id="de1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上图显示值为1时，套索回归移除了8个特征，在总共13个预测因子中只剩下5个。随着特征的去除，模型的预测强度下降，但是现在模型更简单了。这让我们对好的特性选择的力量有了一点了解。</p><h2 id="1994" class="na nb it bd nh oj ok dn nl ol om dp np ld on oo nr lh op oq nt ll or os nv ot bi translated">里脊回归</h2><p id="c54c" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">再次，像套索回归这里是代码和下图产生。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="0c6d" class="na nb it mw b gy nc nd l ne nf">from sklearn.model_selection import Ridge</span><span id="49b9" class="na nb it mw b gy pj nd l ne nf"># Create an array of alpha values to test<br/>alphas = np.logspace(-1, 1.5, 500,base=10)</span><span id="da30" class="na nb it mw b gy pj nd l ne nf"># Create a Ridge regression model instance<br/>ridge = Ridge(random_state=0, max_iter=10000,alpha=alphas)</span><span id="0df9" class="na nb it mw b gy pj nd l ne nf"># Create dictionary key,value pair of alpha values<br/>tuned_parameters = [{'alpha': alphas}]</span><span id="c9d9" class="na nb it mw b gy pj nd l ne nf"># Specify number of folds for cross_validation<br/>n_folds = 5</span><span id="1fde" class="na nb it mw b gy pj nd l ne nf"># Create grid search instance using desired variables<br/>clf_ridge = GridSearchCV(ridge, tuned_parameters, cv=5, refit=False)<br/>clf_ridge.fit(x_train_scaled, y_train)<br/>ridge_scores = clf_ridge.cv_results_['mean_test_score']</span><span id="ea4b" class="na nb it mw b gy pj nd l ne nf"># Plot the Figure<br/>plt.figure().set_size_inches(8, 6)<br/>plt.plot(alphas, ridge_scores)<br/>plt.xlabel('Alpha Value')<br/>plt.ylabel('Cross Validation Score')<br/>plt.title('Ridge Regression Alpha Demonstration')<br/>plt.axvline(clf_ridge.best_params_['alpha'], color='black', linestyle='--')<br/>print(f'The optimal alpha value is: {clf_ridge.best_params_["alpha"]}')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/3b9b74731b8c575d34060129177dcef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*H5rMu-2ZKPWK2tpaWL9Weg.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">Sci-Kit Learn波士顿住房数据的岭回归优化</p></figure><p id="a588" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与套索回归不同，我们可以在这里看到随着alpha值的增加，该值是如何接近0的。如果你愿意，你可以在GitHub笔记本上增加alpha值，看看它永远不会达到0。</p><p id="d10b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在岭回归图中，我们看到alpha值为2.97时，我们的模型比值为0时有更好的性能。请记住，alpha值为0等于正态回归。<strong class="kw iu">这证明了适量的偏差可以改善你的模型，防止过度拟合！</strong></p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h1 id="ad4d" class="ng nb it bd nh ni pe nk nl nm pf no np jz pg ka nr kc ph kd nt kf pi kg nv nw bi translated">结论</h1><p id="9387" class="pw-post-body-paragraph ku kv it kw b kx nx ju kz la ny jx lc ld nz lf lg lh oa lj lk ll ob ln lo lp im bi translated">就是这样！现在，您可以使用优化的alpha值来运行您选择的回归。下面是我们讨论内容的简要总结:</p><p id="c17e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mp">什么是正则化</em> </strong>:对抗过拟合和提高训练的技巧。</p><p id="f16f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><br/>弹性网——脊和套索的混合物</strong></p><p id="4787" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mp">如何使用正则化:</em> </strong></p><ol class=""><li id="dd95" class="pn po it kw b kx ky la lb ld pp lh pq ll pr lp ps pt pu pv bi translated">分割和标准化数据(只标准化模型输入，不标准化输出)</li><li id="dd19" class="pn po it kw b kx pw la px ld py lh pz ll qa lp ps pt pu pv bi translated">决定哪一个回归技术脊，套索，或弹性网，你希望执行。</li><li id="5697" class="pn po it kw b kx pw la px ld py lh pz ll qa lp ps pt pu pv bi translated">使用GridSearchCV优化超参数alpha</li><li id="7464" class="pn po it kw b kx pw la px ld py lh pz ll qa lp ps pt pu pv bi translated">用你现在优化的alpha构建你的模型并进行预测！</li></ol></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><blockquote class="od oe of"><p id="b7eb" class="ku kv mp kw b kx ky ju kz la lb jx lc og le lf lg oh li lj lk oi lm ln lo lp im bi translated">谢谢你花时间阅读我的博客。在我的GitHub库中可以找到该笔记本的全部细节。如果您有任何问题或有建设性的批评，请随时联系我！</p><p id="cc29" class="ku kv mp kw b kx ky ju kz la lb jx lc og le lf lg oh li lj lk oi lm ln lo lp im bi translated">领英:【www.linkedin.com/in/blake-samaha-54a9bbaa T21】</p><p id="5a79" class="ku kv mp kw b kx ky ju kz la lb jx lc og le lf lg oh li lj lk oi lm ln lo lp im bi translated">推特:@Mean_Agression</p></blockquote></div></div>    
</body>
</html>