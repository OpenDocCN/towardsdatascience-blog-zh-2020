<html>
<head>
<title>Normal Equation: A Matrix Approach to Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规方程:线性回归的矩阵方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normal-equation-a-matrix-approach-to-linear-regression-4162ee170243?source=collection_archive---------43-----------------------#2020-06-29">https://towardsdatascience.com/normal-equation-a-matrix-approach-to-linear-regression-4162ee170243?source=collection_archive---------43-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b96" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解 python 中使用法线方程进行线性回归的推导、实现和限制</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b55eea2d3e8ed36f777935b137dc1631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7uAR29i-hoBvZE7L"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae ky" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></p></figure><p id="9d7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression." rel="noopener ugc nofollow" target="_blank">线性回归</a>可能是数据科学和机器学习中最基础的统计模型，它假设输入变量<strong class="lb iu"> (x) </strong>和单个输出变量<strong class="lb iu"> (y) </strong>之间存在线性关系，并试图通过观察到的数据拟合直线<strong class="lb iu">。</strong>虽然您可能熟悉如何使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>作为选择的算法来最小化线性回归问题中的成本函数<strong class="lb iu"><em class="lv">【J(θ)</em></strong><strong class="lb iu"><em class="lv"/></strong>的成本函数<strong class="lb iu"><em class="lv"/></strong>，但是我们可以使用另一种方法来解析地获得最佳值，而无需通过梯度下降的多次迭代来获得全局最小值或选择 alpha(学习速率)。</p><h1 id="e620" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">什么是正规方程？</h1><p id="9720" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们首先推导正规方程，看看矩阵方法是如何用于线性回归的。</p><p id="d9ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定以下将输入映射到输出的假设函数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f131218a1ec3ad70d31987e993178af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*EV_XLMrTjdZ14fuRaBI8xg.png"/></div></figure><p id="2ba3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望最小化最小平方成本函数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/98ac9b30db42dbabb42c3d089c6aa479.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*T0FljJE0KLNj75VxOWa69Q.png"/></div></div></figure><p id="4c0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> m = </strong>训练样本数，<strong class="lb iu">x</strong>s =输入变量，<strong class="lb iu">y</strong>s =第<strong class="lb iu"> <em class="lv"> i 个</em> </strong>样本的输出变量</p><p id="2fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，回归系数<strong class="lb iu"> <em class="lv"> θ </em> </strong> <em class="lv"> </em>本质上是一个向量，每一个<strong class="lb iu"> m </strong>个输入样本也是一个<strong class="lb iu"> n+1 </strong>维的向量(为方便起见，设 x0 = 1)。因此，在矩阵符号中，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/445ab17f5010a3d92fd20cc6cfe9f46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*govKgX8b_Pn_yH8bX9VNNw.png"/></div></figure><p id="53b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重写最小平方成本函数并使用矩阵乘法，我们得到，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3126e6832c134d07b9d5ba47c9208898.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*BiL2DfDS4h-aw1OK6uZInQ.png"/></div></figure><p id="4cf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们应用一些矩阵转置恒等式并忽略<em class="lv"> (1/2m) </em>，因为我们将在计算的后面计算导数。我们最后得到一个简化的方程为<strong class="lb iu"><em class="lv">【J(θ)】</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2f0218e25b2bad1be4110d91e1769a5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*crPZhBV0C9BHqxDs4GI2jg.png"/></div></figure><p id="98ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了找到上述函数的最小值，我们将找到导数 wrt <strong class="lb iu"> <em class="lv"> θ，</em> </strong>并等于 0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/e68661a4d51a3491cbe9a4f79b477ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*RO_PAE7BBQpkfNJ7Pmm-MQ.png"/></div></div></figure><p id="4b68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们假设矩阵<strong class="lb iu"> ( <em class="lv"> X'X) </em> </strong> <em class="lv">(撇号代表转置)</em>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Invertible_matrix#:~:text=In%20linear%20algebra%2C%20an%20n,used%20is%20ordinary%20matrix%20multiplication." rel="noopener ugc nofollow" target="_blank">可逆</a>。两边乘以<strong class="lb iu"> <em class="lv"> (X'X)^(-1).</em> </strong>我们只剩下了法线方程，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f5876613108808f8d5a3707edd169088.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*QqqVvZ8yXfJ8fYXrewgSFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正态方程</p></figure><h1 id="7686" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">Python 实现:</h1><p id="b82e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">法线方程在 python 中的实现请参考 jupyter 笔记本<a class="ae ky" href="https://github.com/akshay-madar/linearRegresssion-using-normalEquations-from-scratch" rel="noopener ugc nofollow" target="_blank">这里</a>。我使用了来自 sklearn 库的<strong class="lb iu">波士顿房价数据集</strong>和<strong class="lb iu"> numpy </strong>包，使用上面推导的矩阵方法计算回归系数。</p><h1 id="e90e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">局限性:</h1><ol class=""><li id="d179" class="na nb it lb b lc mo lf mp li nc lm nd lq ne lu nf ng nh ni bi translated">计算<strong class="lb iu"><em class="lv">(X’X)</em></strong>然后将其求逆的过程是昂贵的，并且花费<strong class="lb iu"> <em class="lv"> O(N ) </em> </strong>其中<strong class="lb iu"> <em class="lv"> N </em> </strong>是<strong class="lb iu"> <em class="lv"> X </em> </strong>矩阵中的行数/观察数。根据斯坦福大学计算机科学兼职教授<a class="ae ky" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">吴恩达</a>的说法，法线方程应该避免用于超过 10K 的特征。</li><li id="37d0" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><strong class="lb iu"><em class="lv">【X ' X】</em></strong>可能是不可逆的，违背了我们的假设。在这种情况下，线性相关的冗余特征可能会被删除，或者像<a class="ae ky" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">套索</a>这样的正则化方法被用来减少特征的数量。</li></ol></div></div>    
</body>
</html>