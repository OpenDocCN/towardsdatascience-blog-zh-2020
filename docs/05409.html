<html>
<head>
<title>Model Complexity, Accuracy and Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模型的复杂性、准确性和可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-complexity-accuracy-and-interpretability-59888e69ab3d?source=collection_archive---------11-----------------------#2020-05-07">https://towardsdatascience.com/model-complexity-accuracy-and-interpretability-59888e69ab3d?source=collection_archive---------11-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a17c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">可解释的人工智能(XAI)</h2><div class=""/><div class=""><h2 id="03e0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">模型可解释性的重要性以及可解释性如何随着复杂性的增加而降低</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/afba792f68bb3de2f9780b29c1e6cb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p8nx3dB5TO9n9hJgeAXvWA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平衡之举:模型准确性与可解释性|鸣谢:kasiastock/Shutterstock</p></figure><h1 id="0a9c" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated"><strong class="ak">简介</strong></h1><p id="f4db" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">复杂的现实世界挑战需要建立复杂的模型，以最大的准确性给出预测。然而，它们最终并不具有高度的可解释性。在本文中，我们将探讨复杂性、准确性和可解释性之间的关系。</p><h1 id="a5b2" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">动机:</h1><p id="9667" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">我目前是一名机器学习研究员，正在从事一个关于可解释机器学习的项目。本系列基于我对克里斯托弗·莫尔纳尔的书的实现:<a class="ae mv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">“可解释的机器学习”</a>。</p><h1 id="b8a6" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated"><strong class="ak">系列:可解释的机器学习</strong></h1><p id="b95d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">第 1 部分:模型的复杂性、准确性和可解释性</strong></p><p id="626f" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我们将使用真实世界的数据集来展示机器学习模型的复杂性、准确性和可解释性之间的关系。我们将尝试越来越复杂的机器学习模型，看看准确性如何提高，可解释性如何降低。</p><p id="0cd1" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">第 2 部分:拆箱“黑盒”模型</strong></p><p id="cdbf" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我们将使用最复杂的模型作为我们的最终模型，因为它提供了非常高的准确性，并使用<a class="ae mv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">“可解释的机器学习”</a>实施模型不可知的方法来解释该模型，请参考此处:</p><p id="cecb" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><a class="ae mv" href="https://medium.com/@sajee.a/unboxing-the-black-box-models-23b4808a3be5" rel="noopener">https://medium . com/@ sajee . a/unboxing-the-black-box-models-23b 4808 a3 be 5</a></p><h1 id="67f5" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">内容:</h1><ol class=""><li id="abe9" class="nb nc it mb b mc md mf mg mi nd mm ne mq nf mu ng nh ni nj bi translated">准确性与可解释性</li><li id="94be" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">复杂性和准确性</li><li id="173e" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">可解释性的重要性</li><li id="88ef" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">实施——复杂性如何增加，可解释性如何降低</li></ol></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="011f" class="lh li it bd lj lk nw lm ln lo nx lq lr ki ny kj lt kl nz km lv ko oa kp lx ly bi translated">模型准确性与可解释性</h1><p id="c4b3" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在现实世界中，在处理任何问题时，理解模型准确性和模型可解释性之间的权衡是很重要的。业务用户希望数据科学家建立更高精度的模型，而数据科学家面临的问题是向他们解释这些模型如何做出预测。</p><p id="44bc" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">什么更重要？？</strong> —拥有一个对未知数据给出最佳精确度的模型，或者即使精确度很低也能理解预测。下面我们比较了传统模型的准确性和可解释性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d73236d2be1d1bf1dded22f08bc2e721.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*78t_O3MgIJ3yHs9dR8VELg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">准确性与可解释性</p></figure><p id="3737" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">该图显示了一些最常用的机器学习算法以及它们的可解释性。就机器学习模型在下面如何工作而言，复杂性增加了。它可以是参数模型(线性模型)或非参数模型(K-最近邻)、简单决策树(CART)或集成模型(Bagging 方法-随机森林或 Boosting 方法-梯度 Boosting 树)。复杂的模型通常能提供更准确的预测。然而，解释它们更困难。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="a814" class="lh li it bd lj lk nw lm ln lo nx lq lr ki ny kj lt kl nz km lv ko oa kp lx ly bi translated">模型复杂性和准确性</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/bf6aaf158f8ba252f691fbd228b5e2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXi_7HIL3FKETFdfegcEmA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">典型的精度-复杂度权衡</p></figure><p id="af8a" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">任何监督机器学习算法的目标都是实现低偏差和低方差。然而，这在现实生活中是不可能的，我们需要在偏差和方差之间进行权衡。</p><p id="c5ed" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">线性回归</strong>假设线性，但实际上关系相当复杂。这些简化的假设给了<strong class="mb jd">很高的偏差</strong>(训练和测试误差很高)，并且模型趋向于<strong class="mb jd">欠拟合</strong>。高偏差可以通过使用复杂函数或添加更多功能来降低。这时复杂性增加了，准确性也提高了。在某一点上，模型将变得过于复杂，并且倾向于<strong class="mb jd">过度拟合</strong>训练数据，即<strong class="mb jd">低偏差</strong>但<strong class="mb jd">高方差</strong>用于测试数据。<strong class="mb jd"> </strong>像<strong class="mb jd">决策树</strong>这样的复杂模型往往会过度拟合。</p><p id="a7d3" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">通常有过度拟合机器学习模型的趋势，因此，为了克服这一点，我们可以使用重采样技术(<strong class="mb jd">交叉验证</strong>)来提高对未知数据的性能。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="c7af" class="lh li it bd lj lk nw lm ln lo nx lq lr ki ny kj lt kl nz km lv ko oa kp lx ly bi translated">模型可解释性的重要性</h1><p id="d323" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在预测的影响较高的用例中，理解<strong class="mb jd">【为什么】</strong>做出某个预测是非常重要的。知道“为什么”可以帮助你更多地了解问题、数据和模型可能失败的原因。</p><p id="2cc8" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">了解可解释性的原因:</p><ol class=""><li id="0d81" class="nb nc it mb b mc mw mf mx mi od mm oe mq of mu ng nh ni nj bi translated"><strong class="mb jd">好奇心&amp;学习</strong></li><li id="46cc" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated"><strong class="mb jd">安全措施</strong> —确保学习无误</li><li id="430b" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">调试到<strong class="mb jd">检测模型训练中的偏差</strong></li><li id="4a20" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">可解释性增加<strong class="mb jd">社会接受度</strong></li><li id="54a7" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated"><strong class="mb jd">调试和审计</strong>机器学习模型</li></ol></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="64d3" class="lh li it bd lj lk nw lm ln lo nx lq lr ki ny kj lt kl nz km lv ko oa kp lx ly bi translated"><strong class="ak">实施:</strong></h1><p id="7065" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">数据集—自行车租赁预测</strong></p><p id="040d" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">自行车租赁数据集可以从 http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset 的 UCI 机器学习库<a class="ae mv" href="http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" rel="noopener ugc nofollow" target="_blank">找到。</a></p><p id="8506" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">该数据集包含华盛顿自行车租赁公司 Capital-Bikeshare 每天租赁自行车的数量，以及天气和季节信息。</p><p id="629f" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">目标:</strong> <strong class="mb jd">根据天气和当天的情况预测将会租出多少辆自行车。</strong></p><p id="f466" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">输入变量:</strong></p><ol class=""><li id="e113" class="nb nc it mb b mc mw mf mx mi od mm oe mq of mu ng nh ni nj bi translated">Total_count (target):租赁自行车总数，包括休闲自行车和注册自行车</li><li id="d57e" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">年:年(0: 2011，1:2012)</li><li id="08fe" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">月:月(1 到 12)</li><li id="2c67" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">小时:小时(0 到 23)</li><li id="f108" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">Temp:以摄氏度为单位的标准化温度。这些值是通过(t-t_min)/(t_max-t_min)，t_min=-8，t_max=+39(仅限于小时刻度)得出的</li><li id="5db3" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">Atemp:归一化的感觉温度，单位为摄氏度。这些值是通过(t-t_min)/(t_max-t_min)，t_min=-16，t_max=+50(仅在小时范围内)得出的</li><li id="c47e" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">湿度:标准化湿度。这些值除以 100(最大值)</li><li id="53a3" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">风速:归一化风速。这些值除以 67(最大值)</li><li id="ea3f" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">假日:一天是否是假日</li><li id="1427" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">工作日:一周中的某一天</li><li id="b03d" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">工作日:如果一天既不是周末也不是假日，则为 1，否则为 0</li><li id="b57d" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">季节:季节(1:冬天，2:春天，3:夏天，4:秋天)</li><li id="0740" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">天气:</li></ol><ul class=""><li id="e5e1" class="nb nc it mb b mc mw mf mx mi od mm oe mq of mu og nh ni nj bi translated">1:晴朗，少云，部分多云，部分多云</li><li id="ce2d" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu og nh ni nj bi translated">2:薄雾+多云，薄雾+碎云，薄雾+少云，薄雾</li><li id="dd16" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu og nh ni nj bi translated">3:小雪，小雨+雷雨+散云，小雨+散云</li><li id="058a" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu og nh ni nj bi translated">4:暴雨+冰托盘+雷雨+薄雾，雪+雾</li></ul><p id="ddff" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">功能:</strong></p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="70b2" class="om li it oi b gy on oo l op oq">Index(['month', 'hr', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed','total_count', 'season_Fall', 'season_Spring', 'season_Summer','season_Winter', 'weather_1', 'weather_2', 'weather_3', 'weather_4','weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4','weekday_5', 'weekday_6', 'holiday', 'year'],dtype='object')</span></pre><p id="26e4" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">探索性数据分析:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/4de0c0977c38435350a019f25429a79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abbFwkl9_cps23wlzNAlVQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">骑自行车的人在一段时间内会增加</p></figure><p id="a1ce" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">从 2011 年到 2012 年的两年间，骑自行车的次数增加了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/a151458e5e87447bc857068d6a335934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vs2mTpWpq3DZTTiv7N60Kw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">相关矩阵</p></figure><p id="4d41" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">风速和湿度略有负相关。Temp 和 atemp 携带相同的信息，因此高度正相关。因此，为了构建模型，我们可以使用 temp 或 atemp。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3e59363eb843da3c6b5276e67ad2a075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*Y0sUrwVcqyhnxZSqw25lFg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">目标直方图:大部分时间骑自行车的次数在 20-30 次/小时左右</p></figure><p id="305d" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">预处理:</strong></p><p id="7a94" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">删除像因果图这样的特征，因为它们与 total_count 相同。同样，对于与 temp 相同的 atemp 等要素，删除一个要素以减少多重共线性。对于分类特征，使用 OneHotEncoding 方法将其转换为更适合回归模型的格式。</p><p id="7863" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">模型实现:</strong></p><p id="d2c2" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我们将研究越来越复杂的模型，看看可解释性如何降低。</p><ol class=""><li id="19aa" class="nb nc it mb b mc mw mf mx mi od mm oe mq of mu ng nh ni nj bi translated">多元线性回归(线性，单一性)</li><li id="9705" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">决策树回归器</li><li id="477c" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">梯度推进回归器</li></ol><h2 id="3502" class="om li it bd lj ou ov dn ln ow ox dp lr mi oy oz lt mm pa pb lv mq pc pd lx iz bi translated"><strong class="ak">多元线性回归:</strong></h2><p id="82a3" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">涉及多个变量的线性回归称为“多元线性回归”或“多元线性回归”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/270eb3935053fdbede16309d7c8ce307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*r3aOsJoXHX7uC2nxn2lygQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae mv" href="https://hackernoon.com/an-intuitive-perspective-to-linear-regression-7dc566b2c14c" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="9611" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">多元线性回归(MLR)的目标是对解释变量(自变量)和响应变量(因变量)之间的线性关系进行建模。从本质上说，多元回归是普通最小二乘(OLS)回归的延伸，涉及一个以上的解释变量。</p><p id="254c" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">回归带有一些在真实世界数据集不实用的假设。</p><ol class=""><li id="cb18" class="nb nc it mb b mc mw mf mx mi od mm oe mq of mu ng nh ni nj bi translated">线性</li><li id="45ef" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">同方差(恒定方差)</li><li id="9276" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">独立性ˌ自立性</li><li id="e8ce" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">固定功能</li><li id="6d1f" class="nb nc it mb b mc nk mf nl mi nm mm nn mq no mu ng nh ni nj bi translated">多重共线性缺失</li></ol><p id="5f2c" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">线性回归实现:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/03d1f044e488a4c782a11ee4ccad0857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*uCKHaSl1fJqcTQEhOdrgew.png"/></div></figure><p id="6577" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">线性回归结果:</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="a931" class="om li it oi b gy on oo l op oq">Mean Squared Error: 19592.4703292543<br/>R score: 0.40700134640548247<br/>Mean Absolute Error: 103.67180228987019</span></pre><p id="8ce0" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">使用交叉验证:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/5e0c407f4d28b52b0bebf5c176cb73b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUNK1NX-Kg0rLg2R6d7SVQ.png"/></div></div></figure><p id="ce82" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">解读多元线性回归:</strong></p><p id="19e2" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">解释线性模型更容易，我们可以研究每个变量的系数，以了解它对预测的影响以及截距的斜率。</p><p id="4981" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">方程的截距(Bo): </strong></p><p id="96fd" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">截距</strong>表示当没有任何特征有任何影响(x=0)时 y(目标)的值。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="fc4b" class="om li it oi b gy on oo l op oq">18.01100142944577</span></pre><p id="53b8" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">对应于 x 列的系数有助于我们理解每个特征对目标结果的影响。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/09a93f9554ba6f3c22aec2858baa1de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*GI9ELnX3W3wDLqvaGnvnAQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">这意味着“温度”每增加一个单位，自行车骑行次数就会增加 211.05 个单位。这同样适用于 rest 特征</p></figure><h1 id="e44a" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">决策树回归器:</h1><p id="f1f6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">决策树的工作原理是以贪婪的方式将数据反复分割成不同的子集。对于回归树，它们被选择来最小化所有子集内的 MSE(均方误差)或 MAE(平均绝对误差)。</p><p id="8116" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd"> CART —分类和回归树:</strong></p><p id="6e85" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">CART 采用一个特征，并确定哪个分界点使回归任务的 y 方差最小。方差告诉我们一个节点中的 y 值围绕其平均值分布了多少。分割基于最小化基于决策树中使用的所有子集的平均值的方差的特征。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="ff83" class="om li it oi b gy on oo l op oq">DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,max_features=None, max_leaf_nodes=15,<br/>min_impurity_decrease=0.0, min_impurity_split=None,<br/>min_samples_leaf=1, min_samples_split=10,<br/>min_weight_fraction_leaf=0.0, presort='deprecated',<br/>random_state=None, splitter='best')</span></pre><p id="00f6" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">决策树结果:</strong></p><p id="0b2b" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">决策树回归结果更符合数据。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="244d" class="om li it oi b gy on oo l op oq">Mean Squared Error: 10880.635297455<br/>R score: 0.6706795022162286<br/>Mean Absolute Error: 73.76311613574498Decision tree split:</span></pre><p id="b7c7" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">决策树比线性回归更适合模型。<strong class="mb jd"> R 平方值约为 0.67 </strong>。</p><p id="86de" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">使用交叉验证:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c35025d46a14a6afd70d9abccb7af6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*3Q4O69-W8f7X3DsGs74TSQ.png"/></div></figure><p id="602e" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">决策树图:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/f8a16520fcb1f99658b1e79e322a6b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjeVRt1UgIVUkvma-hRv6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树回归器输出</p></figure><p id="0687" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">解释决策树:</strong></p><p id="1da7" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">特征重要性:</strong></p><p id="a816" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">特征重要性基于减少使用该特征的所有分割的最大方差的重要性。一个特征可以用于多个分割，或者根本不使用。我们可以将每个 p 特征的贡献相加，并得到每个特征对预测贡献的解释。</p><p id="6122" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我们可以看到特征:hr、temp、year、workingday、season_Spring 是用来分割决策树的特征。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/3060dde3ecb8eb7b670dc953e32aec6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*kAswthQRdgXcLym8Yvyrvw.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/7c011f8c3e368d4e2b6bdd2fc6c01521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cq1zIMmu2-mr6HTmmfTEBQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树回归器—特征重要性条形图</p></figure><h1 id="8292" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">梯度推进回归器:</h1><p id="2647" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">增强是一种集合技术，其中预测器不是独立产生的，而是顺序产生的。梯度提升使用决策树作为弱模型。</p><p id="4062" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">Boosting 是一种将弱学习者转化为强学习者的方法，通过以渐进、累加和顺序的方式训练许多模型，并在最终模型中<strong class="mb jd">最小化损失函数</strong>(即回归问题的<strong class="mb jd">平方误差</strong>)。</p><p id="7413" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">GBR 因其 Boosting 技术而比其他回归模型具有更高的精度。这是竞赛中最常用的回归算法。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="ad71" class="om li it oi b gy on oo l op oq">GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',init=None, learning_rate=0.1, loss='ls', max_depth=6,max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,<br/>min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, presort='deprecated',random_state=None, subsample=1.0, tol=0.0001,validation_fraction=0.1, verbose=0, warm_start=False)</span></pre><p id="70b5" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">来自 GBR 的结果如下:</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="19a8" class="om li it oi b gy on oo l op oq">Mean Squared Error: 1388.8979420780786<br/>R score: 0.9579626971080454<br/>Mean Absolute Error: 23.81293483364058</span></pre><p id="566c" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">梯度推进回归器给出了最佳的 R2 平方值 0.957 </strong>。然而，要解释这个模型是非常困难的。</p><p id="7936" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">解释集合模型:</strong></p><p id="5717" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">集合模型肯定属于“<strong class="mb jd">黑盒</strong>”模型的范畴，因为它们由许多潜在复杂的个体模型组成。</p><p id="433e" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">使用随机选择的特征对袋装数据按顺序训练每棵树，因此通过检查每棵树来获得对决策过程的全面理解是不可行的。</p><p id="23cf" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">第二部分:解释梯度推进回归器模型的模型不可知方法—</strong><a class="ae mv" href="https://medium.com/@sajee.a/unboxing-the-black-box-models-23b4808a3be5" rel="noopener">https://medium . com/@ sajee . a/unboxing-the-black-box-models-23b 4808 a3 be 5</a></p><p id="6662" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">参考文献:</strong></p><p id="eaf7" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><a class="ae mv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></p></div></div>    
</body>
</html>