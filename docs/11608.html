<html>
<head>
<title>Tree-Boosted Mixed Effects Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">树增强混合效应模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tree-boosted-mixed-effects-models-4df610b624cb?source=collection_archive---------2-----------------------#2020-08-12">https://towardsdatascience.com/tree-boosted-mixed-effects-models-4df610b624cb?source=collection_archive---------2-----------------------#2020-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1c71" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">GPBoost:结合树提升和混合效果模型</h2></div><p id="45f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文展示了如何使用<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"> GPBoost 算法</a>将树提升(有时也称为“梯度树提升”)与混合效果模型结合起来。提供了方法论以及如何使用 Python 应用<a class="ae le" href="https://github.com/fabsig/GPBoost" rel="noopener ugc nofollow" target="_blank"> GPBoost 库</a>的背景知识。我们展示了如何(I)训练模型，(ii)调整参数，(iii)解释模型，以及(iv)进行预测。此外，我们还比较了几种可供选择的方法。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/d6dcf4fa11d11f3671e91632c18011b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tilGsqK2zts4nBWLM0jjQw.jpeg"/></div></div></figure><h1 id="b194" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">介绍</h1><p id="8448" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated"><strong class="kk iu">树提升</strong>凭借其众所周知的实现，如 XGBoost、LightGBM 和 CatBoost，被广泛用于应用数据科学。除了最先进的预测准确性之外，树提升还具有以下优势:</p><ul class=""><li id="3f83" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">非线性、不连续性和复杂高阶相互作用的自动建模</li><li id="3ef1" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">对预测变量中的异常值和多重共线性具有稳健性</li><li id="244a" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">预测变量单调变换的尺度不变性</li><li id="4ac8" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">预测变量中缺失值的自动处理</li></ul><p id="e1d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">混合效应</strong>模型是一种用于聚类、分组、纵向或面板数据的建模方法。其中，它们的优势在于，它们允许更有效地学习为回归函数选择的模型(例如，线性模型或树集合)。</p><p id="b457" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"> Sigrist (2020)所述，</a> <strong class="kk iu">组合的梯度树提升和混合效果模型通常比(I)普通梯度提升、(ii)标准线性混合效果模型和(iii)将机器学习或统计模型与混合效果模型相结合的替代方法表现更好</strong>。</p><h1 id="a627" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">建模分组数据</h1><p id="c53b" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated"><strong class="kk iu">分组数据(也称为聚类数据、纵向数据、面板数据)</strong>在许多应用中，当对一个感兴趣变量的不同单位进行多次测量时，会自然出现分组数据。例子包括:</p><ul class=""><li id="5f69" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">人们希望研究某些因素的影响(例如，学习技巧、营养、睡眠等。)对学生的考试成绩，每个学生做几个测试。在这种情况下，单元(即分组变量)是学生，感兴趣的变量是考试分数。</li><li id="3e08" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">一家公司收集其客户的交易数据。对于每个客户，都有几笔交易。这些单元就是客户，而感兴趣的变量可以是交易的任何属性，比如价格。</li></ul><p id="25f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基本上，这种分组数据可以使用四种不同的方法建模:</strong></p><ol class=""><li id="a10e" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld nc mu mv mw bi translated"><strong class="kk iu">忽略分组结构</strong>。这很少是一个好主意，因为重要的信息被忽略了。</li><li id="d017" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld nc mu mv mw bi translated"><strong class="kk iu">分别为每个小组(即每个学生或每个顾客)建模</strong>。这也不是一个好主意，因为相对于不同组的数量，每组的测量数量通常很小。</li><li id="156d" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld nc mu mv mw bi translated"><strong class="kk iu">在您选择的模型中包含分组变量(如学生或客户 ID ),并将其视为分类变量。</strong>虽然这是一种可行的方法，但它有以下缺点。通常，每组的测量数量(例如，每个学生的测试数量、每个客户的交易数量)相对较少，而不同组的数量(例如，学生数量、客户数量等)相对较多。).在这种情况下，该模型需要基于相对少的数据学习许多参数(每个组一个)，这使得学习效率低下。此外，对于树，高基数分类变量可能会有问题。</li><li id="4a6a" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld nc mu mv mw bi translated"><strong class="kk iu">在混合效果模型中使用所谓的随机效果对分组变量进行建模。这通常是两种方法之间明智的妥协。第三。以上。具体而言，如下文和<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank">SIG rist(2020)</a>所示，在树提升的情况下，与其他方法相比，这是有益的。</strong></li></ol><h1 id="f343" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">方法论背景</h1><p id="088f" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">对于 GPBoost 算法，假设<strong class="kk iu">响应变量 y 是潜在的非线性均值函数 F(X)和所谓的随机效应 Zb </strong>的和:</p><blockquote class="nd"><p id="a5ed" class="ne nf it bd ng nh ni nj nk nl nm ld dk translated">y = F(X) + Zb + e</p></blockquote><p id="0464" class="pw-post-body-paragraph ki kj it kk b kl nn ju kn ko no jx kq kr np kt ku kv nq kx ky kz nr lb lc ld im bi translated">在哪里</p><ul class=""><li id="9151" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">y 是响应变量(又名标签)</li><li id="97fe" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">x 包含预测变量(即特征), F()是一个潜在的非线性函数。在线性混合效应模型中，这只是一个线性函数。在 GPBoost 算法中，这是树的集合。</li><li id="6f06" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">Zb 是假设遵循多元正态分布的随机效应</li><li id="b62b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">e 是一个误差项</li></ul><p id="3651" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<strong class="kk iu"> GPBoost 算法训练该模型，其中训练意味着使用树集合</strong>学习随机效应的(共)方差参数(又名超参数) <strong class="kk iu">和回归函数 F(X】。在模型被学习之后，随机效应 Zb 可以被估计(或预测，因为它经常被称为)。简而言之，GPBoost 算法是一种 boosting 算法，它迭代地学习(共)方差参数，并使用<a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/S0957417420308381" rel="noopener ugc nofollow" target="_blank">梯度和/或牛顿 boosting </a>步骤向树集合添加一棵树。与现有 boosting 算法的主要区别在于，首先，它考虑了由于聚类而导致的数据之间的依赖性，其次，它学习随机效应的(共)方差参数。有关该方法的更多详细信息，请参见<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"> Sigrist (2020) </a>。在 GPBoost 库中，可以使用(加速)梯度下降或 Fisher 评分来学习(共)方差参数，使用<a class="ae le" href="https://github.com/microsoft/LightGBM/" rel="noopener ugc nofollow" target="_blank"> LightGBM 库</a>来学习树。特别是，这意味着 LightGBM 的全部功能都是可用的。</strong></p><h1 id="5212" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">如何在 Python 中使用 GPBoost 库</strong></h1><p id="6d22" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在下文中，我们将展示如何使用 Python 中的 GPBoost 库来应用组合的树增强和混合效果模型。本文中使用的完整代码可以在这里作为 Python 脚本找到<a class="ae le" href="https://github.com/fabsig/GPBoost/blob/master/examples/python-guide/GPBoost_algorithm_blog_post_example.py" rel="noopener ugc nofollow" target="_blank">。注意，还有一个等价的 R 包。更多相关信息可在</a><a class="ae le" href="https://github.com/fabsig/GPBoost/tree/master/R-package" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="400b" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">装置</h2><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="47b0" class="ns ls it of b gy oj ok l ol om">pip install gpboost -U</span></pre><h2 id="e8c4" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">模拟数据</h2><p id="4eb4" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们在这里使用模拟数据。我们采用众所周知的非线性函数 F(X) 。为了简单起见，我们使用一个分组变量。但是同样可以使用几种随机效应，包括分层嵌套效应、交叉效应或随机斜率效应。样本的数量是 5000，不同组或聚类的数量是 500。我们还生成测试数据来评估预测的准确性。对于测试数据，我们既包括已知的观察组，也包括新的未观察组。</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="639f" class="ns ls it of b gy oj ok l ol om">import gpboost as gpb<br/>import numpy as np<br/>import sklearn.datasets as datasets<br/>import time<br/>import pandas as pd</span><span id="7b72" class="ns ls it of b gy on ok l ol om"># Simulate data<br/>ntrain = 5000 # number of samples for training<br/>n = 2 * ntrain # combined number of training and test data<br/>m = 500  # number of categories / levels for grouping variable<br/>sigma2_1 = 1  # random effect variance<br/>sigma2 = 1 ** 2  # error variance<br/># Simulate non-linear mean function<br/>np.random.seed(1)<br/>X, F = datasets.make_friedman3(n_samples=n)<br/>X = pd.DataFrame(X,columns=['variable_1','variable_2','variable_3','variable_4'])<br/>F = F * 10**0.5 # with this choice, the fixed-effects regression function has the same variance as the random effects<br/># Simulate random effects<br/>group_train = np.arange(ntrain)  # grouping variable<br/>for i in range(m):<br/>    group_train[int(i * ntrain / m):int((i + 1) * ntrain / m)] = i<br/>group_test = np.arange(ntrain) # grouping variable for test data. Some existing and some new groups<br/>m_test = 2 * m<br/>for i in range(m_test):<br/>    group_test[int(i * ntrain / m_test):int((i + 1) * ntrain / m_test)] = i<br/>group = np.concatenate((group_train,group_test))<br/>b = np.sqrt(sigma2_1) * np.random.normal(size=m_test)  # simulate random effects<br/>Zb = b[group]<br/># Put everything together<br/>xi = np.sqrt(sigma2) * np.random.normal(size=n)  # simulate error term<br/>y = F + Zb + xi  # observed data<br/># split train and test data<br/>y_train = y[0:ntrain]<br/>y_test = y[ntrain:n]<br/>X_train = X.iloc[0:ntrain,]<br/>X_test = X.iloc[ntrain:n,]</span></pre><h2 id="81a5" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">学习和预测</h2><p id="588e" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">下面的代码显示了如何训练模型并进行预测。如下所示，学习的方差参数接近真实值。注意，当进行预测时，可以对均值函数 F(X)和随机效应 Zb 进行单独的预测。</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="0f27" class="ns ls it of b gy oj ok l ol om"># Define and train GPModel<br/>gp_model = gpb.GPModel(group_data=group_train)<br/># create dataset for gpb.train function<br/>data_train = gpb.Dataset(X_train, y_train)<br/># specify tree-boosting parameters as a dict<br/>params = { 'objective': 'regression_l2', 'learning_rate': 0.1,<br/>    'max_depth': 6, 'min_data_in_leaf': 5, 'verbose': 0 }<br/># train model<br/>bst = gpb.train(params=params, train_set=data_train, gp_model=gp_model, num_boost_round=31)<br/>gp_model.summary() # estimated covariance parameters<br/>#Covariance parameters: <br/>#        Error_term   Group_1<br/>#Param.     0.92534  1.016069<br/><br/># Make predictions<br/>pred = bst.predict(data=X_test, group_data_pred=group_test)<br/>y_pred = pred['response_mean']<br/>np.sqrt(np.mean((y_test - y_pred) ** 2)) # root mean square error (RMSE) on test data. Approx. = 1.26</span></pre><h2 id="abf9" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">参数调谐</h2><p id="d324" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">仔细选择调整参数对于所有升压算法都很重要。可以说，最重要的调优参数是提升迭代的次数。太大的数值通常会导致回归问题中的过度拟合，而太小的数值则会导致“欠拟合”。在下文中，我们展示了如何使用交叉验证来选择提升迭代的次数。其他重要的调整参数包括学习速率、树深度和每片叶子的最小样本数。为了简单起见，我们不在这里对它们进行调优，而是使用一些默认值。</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="10ff" class="ns ls it of b gy oj ok l ol om"># Parameter tuning using cross-validation (only number of boosting iterations)<br/>gp_model = gpb.GPModel(group_data=group_train)<br/>cvbst = gpb.cv(params=params, train_set=data_train,<br/>               gp_model=gp_model, use_gp_model_for_validation=False,<br/>               num_boost_round=100, early_stopping_rounds=5,<br/>               nfold=4, verbose_eval=True, show_stdv=False, seed=1)<br/>best_iter = np.argmin(cvbst['l2-mean'])<br/>print("Best number of iterations: " + str(best_iter))<br/># Best number of iterations: 31</span></pre><p id="b4e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oo">更新</em> </strong> <em class="oo">:从版本 0.4.3 开始，GPBoost 现在有了一个函数(` grid_search_tune_parameters `)，可以使用随机或确定性网格搜索进行参数调整。详见本</em> <a class="ae le" href="https://github.com/fabsig/GPBoost/tree/master/examples/python-guide/parameter_tuning.py" rel="noopener ugc nofollow" target="_blank"> <em class="oo"> Python 参数调优演示</em> </a> <em class="oo">。</em></p><h2 id="be4d" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">特征重要性和部分相关性图</h2><p id="dcfb" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">特征重要性图和部分相关性图是用于解释机器学习模型的工具。这些可以如下使用。</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="066a" class="ns ls it of b gy oj ok l ol om"># Plotting feature importances<br/>gpb.plot_importance(bst)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi op"><img src="../Images/06d9fe659419dc0047102a33007ed98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCby9M4ObWquoFmx4veGKQ.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">特征重要性图</p></figure><p id="7f07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单变量部分相关图</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="5b4c" class="ns ls it of b gy oj ok l ol om">from pdpbox import pdp<br/># Single variable plots (takes a few seconds to compute)<br/>pdp_dist = pdp.pdp_isolate(model=bst, dataset=X_train,      <br/>                           model_features=X_train.columns,<br/>                           feature='variable_2', <br/>                           num_grid_points=50,<br/>                           predict_kwds={"ignore_gp_model": True})<br/>pdp.pdp_plot(pdp_dist, 'variable_2', plot_lines=True)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ou"><img src="../Images/77c1a01bfec5da85efa4fd44150668ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvemk78q67GbdtgldHh4lA.png"/></div></div></figure><p id="85cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多元部分相关图</p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="cc47" class="ns ls it of b gy oj ok l ol om"># Two variable interaction plot<br/>inter_rf = pdp.pdp_interact(model=bst, dataset=X_train,     <br/>                            model_features=X_train.columns,<br/>                            features=['variable_1','variable_2'],<br/>                            predict_kwds={"ignore_gp_model": True})<br/>pdp.pdp_interact_plot(inter_rf, ['variable_1','variable_2'], x_quantile=True, plot_type='contour', plot_pdp=True) <br/># ignore any error message</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ov"><img src="../Images/d5c9348e446921a4a2afc50108fa85df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tJPooCG8ezSA75Amap9oZw.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">用于可视化交互的二维部分相关图</p></figure><h2 id="0691" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">SHAP 价值观</h2><p id="8090" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">SHAP 值和依赖图是模型解释的另一个重要工具。这些可以按如下方式创建。<em class="oo">注意:为此需要 shap 版本&gt; =0.36.0。</em></p><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="559e" class="ns ls it of b gy oj ok l ol om">import shap<br/>shap_values = shap.TreeExplainer(bst).shap_values(X_test)<br/>shap.summary_plot(shap_values, X_test)<br/>shap.dependence_plot("variable_2", shap_values, X_test)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ow"><img src="../Images/e369a593d22f2d2472677a83cc4af747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3dlXK-Zy7N3h8kdstUxXw.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">SHAP 价值观</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ox"><img src="../Images/f11f19d9a11ecd87b04769d6f638831c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xqKolsU33n-Q0RZ_mm769g.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">变量 2 的 SHAP 相关图</p></figure><h1 id="ed97" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">与替代方法的比较</h1><p id="d868" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在下文中，我们使用上述模拟数据将 GPBoost 算法与几种现有方法进行比较。我们考虑以下替代方法:</p><ul class=""><li id="3e07" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu">线性混合效应模型(‘Linear _ ME’)</strong>其中 F(X)是线性函数</li><li id="052b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">忽略分组结构的标准梯度树提升(‘Boosting _ Ign’)</strong></li><li id="e534" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">标准梯度树提升，包括作为分类变量的分组变量(‘Boosting _ Cat’)</strong></li><li id="7886" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">混合效果随机森林(‘MERF’)</strong>(详见<a class="ae le" rel="noopener" target="_blank" href="/mixed-effects-random-forests-6ecbb85cb177">此处</a>和<a class="ae le" href="https://www.tandfonline.com/doi/full/10.1080/00949655.2012.741599" rel="noopener ugc nofollow" target="_blank"> Hajjem et al. (2014) </a>)</li></ul><p id="38e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们根据使用均方根误差(RMSE)和计算时间(以秒为单位的时钟时间)测量的预测准确性来比较这些算法。结果如下表所示。产生这些结果的代码可以在下面的附录中找到。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/5af38b78d56cfa1c946e38264ab56511.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*clc91k1FZ7JcufJZaWxQ2w.jpeg"/></div><p class="oq or gj gh gi os ot bd b be z dk translated">GPBoost 和替代方法的比较。</p></figure><p id="2939" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到 GPBoost 和 MERF 在预测准确性方面表现最好(几乎一样好)。此外，GPBoost 算法比 MERF 算法快大约 1000 倍。线性混合效应模型(' Linear_ME ')和忽略分组变量的树提升(' Boosting_Ign ')的预测精度明显较低。将分组变量作为分类变量(“Boosting_Cat”)的树提升也显示出比 GPBoost 或 MERF 更低的预测准确性。注意，在这个例子中，测试数据包含已经在训练数据中观察到的现有组和没有在训练数据中观察到的新组(各 50%)。如果测试数据仅包含现有组，Boosting_Cat 和 GPBoost / MERF 之间的差异更大；参见<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"> <em class="oo">中的实验 Sigrist (2020) </em> </a> <em class="oo">。</em></p><p id="1011" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oo">注意，为简单起见，我们只进行一次模拟运行(更详细的比较见</em><a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"><em class="oo">【SIG rist(2020)</em></a><em class="oo">)。除了 MERF，所有计算都是使用 gpboost Python 包 0.7.6 版完成的。此外，我们使用 MERF Python 包版本 0.3。</em></p><h1 id="2faa" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="8796" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">GPBoost 允许组合混合效果模型和树提升。<strong class="kk iu">如果应用线性混合效应模型，应调查线性假设是否确实合适。GPBoost 模型允许放宽这一假设。它可以帮助您找到非线性和相互作用，并实现更高的预测准确性。如果您是 XGBoost 和 LightGBM 等 boosting 算法的频繁用户，并且您有潜在高基数的分类变量，GPBoost(它扩展了 LightGBM)可以使学习更加高效</strong><a class="ae le" href="https://en.wikipedia.org/wiki/Efficiency_(statistics)" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/></a><strong class="kk iu">，并产生更高的预测准确性。</strong></p><p id="e425" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">据我们所知，GPBoost 库在计算速度和预测准确性方面目前是无与伦比的。其他优势是 GPBoost 支持一系列模型解释工具(可变重要性值、部分相关图、SHAP 值等)。).此外，除了分组或聚类随机效果外，<a class="ae le" rel="noopener" target="_blank" href="/tree-boosting-for-spatial-data-789145d6d97d">还支持其他类型的随机效果，如高斯过程</a>。</p><p id="9262" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望这篇文章对你有用。关于 GPBoost 的更多信息可以在配套文章<a class="ae le" href="https://arxiv.org/abs/2004.02653" rel="noopener ugc nofollow" target="_blank"> Sigrist (2020) </a>和<a class="ae le" href="https://github.com/fabsig/GPBoost" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。</p><h1 id="e528" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><p id="b073" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">Hajjem、f . Bella vance 和 d . la rocque(2014 年)。聚类数据的混合效果随机森林。<em class="oo">统计计算与模拟杂志</em>，<em class="oo"> 84 </em> (6)，1313–1328。</p><p id="3487" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">柯国光，孟，陈，王，陈，马，刘天元(2017)。Lightgbm:一种高效的梯度推进决策树。在<em class="oo">神经信息处理系统的进展</em>(第 3146–3154 页)。</p><p id="f672" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">皮涅罗和贝茨博士(2006 年)。<em class="oo">S 和 S-PLUS 中的混合效果车型</em>。斯普林格科学&amp;商业媒体。</p><p id="77c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">西格里斯特，F. (2020)。高斯过程增强。arXiv 预印本 arXiv:2004.02653 。</p><h1 id="2399" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">附录</h1><h2 id="3cbe" class="ns ls it bd lt nt nu dn lx nv nw dp mb kr nx ny md kv nz oa mf kz ob oc mh od bi translated">替代方法比较代码</h2><pre class="lg lh li lj gt oe of og oh aw oi bi"><span id="b77b" class="ns ls it of b gy oj ok l ol om">results = pd.DataFrame(columns = ["RMSE","Time"],<br/>                       index = ["GPBoost", "Linear_ME","Boosting_Ign","Boosting_Cat","MERF"])<br/># 1. GPBoost<br/>gp_model = gpb.GPModel(group_data=group_train)<br/>start_time = time.time() # measure time<br/>bst = gpb.train(params=params, train_set=data_train, gp_model=gp_model, num_boost_round=best_iter)<br/>results.loc["GPBoost","Time"] = time.time() - start_time<br/>pred = bst.predict(data=X_test, group_data_pred=group_test)<br/>y_pred = pred['response_mean']<br/>results.loc["GPBoost","RMSE"] = np.sqrt(np.mean((y_test - y_pred) ** 2))<br/><br/># 2. Linear mixed effects model ('Linear_ME')<br/>gp_model = gpb.GPModel(group_data=group_train)<br/>X_train_linear = np.column_stack((np.ones(ntrain),X_train))<br/>X_test_linear = np.column_stack((np.ones(ntrain),X_test))<br/>start_time = time.time() # measure time<br/>gp_model.fit(y=y_train, X=X_train_linear) # add a column of 1's for intercept<br/>results.loc["Linear_ME","Time"] = time.time() - start_time<br/>y_pred = gp_model.predict(group_data_pred=group_test, X_pred=X_test_linear)<br/>results.loc["Linear_ME","RMSE"] = np.sqrt(np.mean((y_test - y_pred['mu']) ** 2))<br/><br/># 3. Gradient tree-boosting ignoring the grouping variable ('Boosting_Ign')<br/>cvbst = gpb.cv(params=params, train_set=data_train,<br/>               num_boost_round=100, early_stopping_rounds=5,<br/>               nfold=4, verbose_eval=True, show_stdv=False, seed=1)<br/>best_iter = np.argmin(cvbst['l2-mean'])<br/>print("Best number of iterations: " + str(best_iter))<br/>start_time = time.time() # measure time<br/>bst = gpb.train(params=params, train_set=data_train, num_boost_round=best_iter)<br/>results.loc["Boosting_Ign","Time"] = time.time() - start_time<br/>y_pred = bst.predict(data=X_test)<br/>results.loc["Boosting_Ign","RMSE"] = np.sqrt(np.mean((y_test - y_pred) ** 2))<br/><br/># 4. Gradient tree-boosting including the grouping variable as a categorical variable ('Boosting_Cat')<br/>X_train_cat = np.column_stack((group_train,X_train))<br/>X_test_cat = np.column_stack((group_test,X_test))<br/>data_train_cat = gpb.Dataset(X_train_cat, y_train, categorical_feature=[0])<br/>cvbst = gpb.cv(params=params, train_set=data_train_cat,<br/>               num_boost_round=1000, early_stopping_rounds=5,<br/>               nfold=4, verbose_eval=True, show_stdv=False, seed=1)<br/>best_iter = np.argmin(cvbst['l2-mean'])<br/>print("Best number of iterations: " + str(best_iter))<br/>start_time = time.time() # measure time<br/>bst = gpb.train(params=params, train_set=data_train_cat, num_boost_round=best_iter)<br/>results.loc["Boosting_Cat","Time"] = time.time() - start_time<br/>y_pred = bst.predict(data=X_test_cat)<br/>results.loc["Boosting_Cat","RMSE"] = np.sqrt(np.mean((y_test - y_pred) ** 2))<br/><br/># 5. Mixed-effects random forest ('MERF')<br/>from merf import MERF<br/>rf_params={'max_depth': 6, 'n_estimators': 300}<br/>merf_model = MERF(max_iterations=100, rf_params=rf_params)<br/>print("Warning: the following takes a lot of time")<br/>start_time = time.time() # measure time<br/>merf_model.fit(pd.DataFrame(X_train), np.ones(shape=(ntrain,1)), pd.Series(group_train), y_train)<br/>results.loc["MERF","Time"] = time.time() - start_time<br/>y_pred = merf_model.predict(pd.DataFrame(X_test), np.ones(shape=(ntrain,1)), pd.Series(group_test))<br/>results.loc["MERF","RMSE"] = np.sqrt(np.mean((y_test - y_pred) ** 2))<br/><br/>print(results.apply(pd.to_numeric).round(3))</span></pre></div></div>    
</body>
</html>