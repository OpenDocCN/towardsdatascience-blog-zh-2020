<html>
<head>
<title>Convolutional Neural Network: Feature Map and Filter Visualization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络:特征映射和滤波器可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=collection_archive---------0-----------------------#2020-05-18">https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=collection_archive---------0-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e89f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解卷积神经网络如何理解图像。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5f562fe08dccdb4510c9b35b3c72ca7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*slzHFjXRkTVtFhRipPl8vw.png"/></div></div></figure><p id="3f3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="lq">在本文中，我们将可视化不同CNN层的中间特征表示，以了解CNN内部发生了什么来对图像进行分类。</em>T3】</strong></p><h2 id="dd45" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">先决条件:</h2><p id="1e29" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><a class="ae mp" href="https://medium.com/datadriveninvestor/convolutional-neural-network-cnn-simplified-ecafd4ee52c5" rel="noopener">卷积神经网络基础知识</a>，</p><p id="5680" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://medium.com/datadriveninvestor/building-powerful-image-classification-convolutional-neural-network-using-keras-a1839d0ff298" rel="noopener">使用Keras构建强大的图像分类卷积神经网络</a></p><p id="9615" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://medium.com/datadriveninvestor/building-powerful-image-classification-convolutional-neural-network-using-keras-a1839d0ff298" rel="noopener">使用Keras构建强大的图像分类CNN</a></p><h2 id="e3f0" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">CNN的快速概览</h2><p id="7fbe" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><strong class="kw iu">有监督的深度学习和机器学习在训练期间将数据和结果作为输入，以生成规则或数据模式</strong>。理解模型生成的数据模式或规则有助于我们理解结果是如何从输入数据中得出的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/05b9c261bbdcff5698241f8169413fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*QtLT_JqXC4j4ZR8bPmCyRg.png"/></div></figure><p id="461f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">训练:</strong>卷积神经网络以一幅二维图像和该图像的类别，如猫或狗作为输入。作为训练的结果，我们得到训练的权重，这是从图像中提取的数据模式或规则。</p><p id="9a2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">推理或预测:</strong>图像将是传递给训练好的模型的唯一输入，训练好的模型将输出图像的类别。图像的类别将基于训练期间学习的数据模式。</p><h2 id="6182" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">CNN架构</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/76582c10abd054f0c663d1f3ac8f9be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GPBRUVqDhQxDuW8r6X5n7g.png"/></div></div></figure><p id="c7c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">将过滤器</strong>或<strong class="kw iu">特征检测器应用于输入图像</strong>以使用Relu激活功能生成<strong class="kw iu">特征图或激活图。</strong>特征检测器或过滤器有助于识别图像中的不同特征，如边缘、垂直线、水平线、弯曲等。</p><p id="40d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">然后，为了平移不变性，在特征图上应用池化</strong>。<strong class="kw iu">汇集是基于这样一个概念:当我们少量改变输入时，汇集的输出不会改变</strong>。我们可以使用最小池、平均池或最大池。与最小或平均池相比，最大池提供了更好的性能。</p><p id="88ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">展平所有输入，并将这些展平的输入传递给深度神经网络，该网络输出对象的类别</strong></p><p id="cdfa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图像的类别可以是二进制的，如猫或狗，或者可以是多类别的分类，如识别数字或对不同的服装项目进行分类。</p><p id="9b17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">神经网络就像一个黑匣子，神经网络中学习到的特征是不可解释的。您传递一个输入图像，模型返回结果。</p><p id="6d9f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">如果你得到了一个不正确的预测，并想弄清楚为什么CNN会做出这样的决定，该怎么办？</em></p><blockquote class="ms"><p id="bed5" class="mt mu it bd mv mw mx my mz na nb lp dk translated">如果您能在CNN中可视化应用于不同卷积层的中间表示，以了解模型如何学习，那该多好。了解模型的工作将有助于了解错误预测的原因，从而更好地微调模型并解释决策</p></blockquote><p id="9def" class="pw-post-body-paragraph ku kv it kw b kx nc ju kz la nd jx lc ld ne lf lg lh nf lj lk ll ng ln lo lp im bi translated">这里使用的例子是一个深度CNN模型，用于<a class="ae mp" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank">对猫狗</a>进行分类。在开始学习可视化CNN生成的过滤器和要素地图之前，您需要了解卷积层和应用于它们的过滤器的一些关键点。</p><h2 id="75f7" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">关于卷积层和过滤器的要点</h2><ul class=""><li id="9562" class="nh ni it kw b kx mk la ml ld nj lh nk ll nl lp nm nn no np bi translated"><strong class="kw iu">CNN中滤镜的深度必须与输入图像的深度相匹配</strong>。滤镜中颜色通道的数量必须与输入图像保持一致。</li><li id="90a8" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">为彩色图像的三个通道</strong>创建不同的Conv2D滤镜。</li><li id="d706" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">基于正态或高斯分布随机初始化每层的过滤器。</strong></li><li id="bfdb" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">卷积网络的初始层从图像中提取高级特征，因此使用较少的滤波器。</strong>随着我们构建更深的层，我们将过滤器的数量增加到前一层过滤器大小的两倍或三倍。</li><li id="385c" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">更深层的过滤器学习更多的特征，但是计算非常密集</strong>。</li></ul><h2 id="234c" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">构建卷积神经网络</h2><p id="0b81" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们建立一个CNN来对狗和猫进行分类，然后将特征图或激活图以及用于在输入图像上生成它们的过滤器可视化</p><p id="b799" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">导入所需的库</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="0b26" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">import tensorflow as tf<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img</strong></span><span id="5de8" class="lr ls it nw b gy oe ob l oc od"><strong class="nw iu">import os<br/>import numpy as np<br/>import matplotlib.pyplot as plt</strong></span></pre><p id="ab5b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">为图像处理创建数据</strong></p><p id="6b22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据集可以从<a class="ae mp" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank">这里</a>下载</p><p id="5114" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将解压文件并创建如下所示的文件夹，然后将数据分成包含10，000只猫和10，000只狗图像的训练数据集以及包含2500只猫和2500只狗图像的验证数据集</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/009d8df86252d4c23372aa0c7068919f.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*DObDKu0zWSLDPX4AH18Bqw.png"/></div></figure><p id="dc45" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">设置关键参数</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="a6c2" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">batch_size = 64 <br/>epochs = 50<br/>IMG_HEIGHT = 150<br/>IMG_WIDTH = 150</strong></span></pre><p id="61af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">重新缩放并对训练图像应用不同的增强</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="8014" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">train_image_generator = ImageDataGenerator(                                                rescale=1./255,                                              rotation_range=45,                                                width_shift_range=.15,                                                height_shift_range=.15,                                                horizontal_flip=True,                                                zoom_range=0.3)</strong></span></pre><p id="60bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">重新调整验证数据</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="4bfd" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">validation_image_generator = ImageDataGenerator(rescale=1./255)</strong></span></pre><p id="0a2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">为训练和验证数据集生成批量归一化数据</strong></p><p id="c916" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您的数据存储在目录中，因此使用<strong class="kw iu"> flow_from_directory() </strong>方法。flow_from_directory()将从指定的路径获取数据，并生成批量的扩充规范化数据。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="73fc" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,                                                     directory=TRAIN_PATH,                                                     shuffle=True,                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),                                                     class_mode='binary')</strong></span><span id="8870" class="lr ls it nw b gy oe ob l oc od"><strong class="nw iu">val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,                                                              directory=VAL_PATH,                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),                                                              class_mode='binary')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0fc0d49465ef72bba21df3de10123be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*6qJF6KX5ZQVJDrpPT-YcMA.png"/></div></figure><p id="d91c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">创建深度卷积神经网络模型</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="763e" class="lr ls it nw b gy oa ob l oc od"><em class="lq">#Build the model</em><strong class="nw iu"><em class="lq"><br/></em>model = Sequential([<br/>    Conv2D(16, 3, padding='same', activation='relu', <br/>           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),<br/>    MaxPooling2D(),<br/>    Dropout(0.2),<br/>    Conv2D(32, 3, padding='same', activation='relu'),<br/>    MaxPooling2D(),<br/>    Conv2D(64, 3, padding='same', activation='relu'),<br/>    MaxPooling2D(),<br/>    Dropout(0.2),<br/>    Flatten(),<br/>    Dense(512, activation='relu'),<br/>    Dense(1)<br/>])</strong></span><span id="98da" class="lr ls it nw b gy oe ob l oc od"><em class="lq"># Compile the model</em><br/><strong class="nw iu">model.compile(optimizer='adam',              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),              metrics=['accuracy'])</strong></span><span id="3410" class="lr ls it nw b gy oe ob l oc od"><em class="lq"># print the model architecture</em><br/><strong class="nw iu">model.summary(<em class="lq">)</em></strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/68ba252812e61656bb042fc7656fba72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*Ww8UYLd_aWkRPkw0a9GUnA.png"/></div></figure><p id="6c1c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">训练模特</strong></p><p id="d4a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们训练模型50个时期</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="3f2f" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu"><em class="lq">history = model.fit_generator(<br/>    train_data_gen,<br/>    steps_per_epoch=1000,<br/>    epochs=epochs,<br/>    validation_data=val_data_gen,<br/>    validation_steps=1000<br/>)</em></strong></span></pre><h2 id="580a" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">特征可视化</h2><p id="9c49" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">特征可视化将图像中存在的内部特征转化为视觉上可感知或可识别的图像模式。特征可视化将帮助我们明确地理解所学的特征。</p><p id="4753" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，您将可视化应用于输入图像的不同过滤器或特征检测器，并在下一步中可视化生成的特征映射或激活映射。</p><h2 id="a0e1" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">CNN中的可视化过滤器或特征检测器</h2><p id="d822" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">CNN使用学习过的过滤器来卷积来自前一层的特征地图。过滤器是二维权重，并且这些权重彼此具有空间关系。</p><p id="a83f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">可视化过滤器的步骤。</strong></p><ol class=""><li id="2d8c" class="nh ni it kw b kx ky la lb ld oi lh oj ll ok lp ol nn no np bi translated">使用<strong class="kw iu"> <em class="lq"> model.layers </em> </strong>遍历模型的所有层</li><li id="43f7" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">如果该层是卷积层，则使用<strong class="kw iu"><em class="lq">get _ weights()</em></strong>提取该层的权重和偏差值。</li><li id="332e" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">将过滤器的权重归一化到0和1之间</li><li id="22a1" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">画出每个卷积层和所有通道的滤波器。对于彩色图像，RGB有三个通道。对于灰度图像，通道数为1</li></ol><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="d86c" class="lr ls it nw b gy oa ob l oc od"><em class="lq">#Iterate thru all the layers of the model</em><br/><strong class="nw iu">for layer in model.layers:<br/>    if 'conv' in layer.name:<br/>        weights, bias= layer.get_weights()<br/>        print(layer.name, filters.shape)</strong><br/>        <br/>        <em class="lq">#normalize filter values between  0 and 1 for visualization</em><br/>        <strong class="nw iu">f_min, f_max = weights.min(), weights.max()<br/>        filters = (weights - f_min) / (f_max - f_min)  <br/>        print(filters.shape[3])<br/>        filter_cnt=1</strong><br/>        <br/>       <em class="lq"> #plotting all the filters</em><br/>       <strong class="nw iu"> for i in range(filters.shape[3]):</strong><br/>            <em class="lq">#get the filters</em><br/>           <strong class="nw iu"> filt=filters[:,:,:, i]</strong><br/>            <em class="lq">#plotting each of the channel, color image RGB channels</em><br/>            <strong class="nw iu">for j in range(filters.shape[0]):<br/>                ax= plt.subplot(filters.shape[3], filters.shape[0], filter_cnt  )<br/>                ax.set_xticks([])<br/>                ax.set_yticks([])<br/>                plt.imshow(filt[:,:, j])<br/>                filter_cnt+=1<br/>        plt.show()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/a4173065aa14c8275671dd30fc3692ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*ok15pAQXNopN9mRz_yH3XQ.png"/></div><p class="on oo gj gh gi op oq bd b be z dk translated">应用于猫和狗的CNN模型的过滤器。</p></figure><h2 id="4a43" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">可视化CNN中生成的特征图或激活图</h2><p id="8d97" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">通过将过滤器或特征检测器应用于输入图像或先前层的特征图输出来生成特征图。要素图可视化将提供对模型中每个卷积层的特定输入的内部表示的深入了解。</p><p id="7367" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">可视化特征地图的步骤。</strong></p><ol class=""><li id="2479" class="nh ni it kw b kx ky la lb ld oi lh oj ll ok lp ol nn no np bi translated">定义一个新的模型，<strong class="kw iu"><em class="lq">visualization _ model</em></strong>，该模型将图像作为输入。模型的输出将是要素地图，它是第一个图层之后所有图层的中间表示。这是基于我们用于培训的模型。</li><li id="1a04" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">加载我们想要查看其特征地图的输入图像，以了解哪些特征对于图像分类是显著的。</li><li id="caa1" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">将图像转换为NumPy数组</li><li id="48c4" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">通过重新调整数组的大小来规范化数组</li><li id="573e" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">通过可视化模型运行输入图像，以获得输入图像的所有<br/>中间表示。</li><li id="6d19" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp ol nn no np bi translated">为所有卷积层和最大池层创建图，但不为全连接层创建图。要打印要素地图，请检索模型中每个图层的图层名称。</li></ol><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="cd4f" class="lr ls it nw b gy oa ob l oc od"><strong class="nw iu">img_path='\\dogs-vs-cats\\test1\\137.jpg' #dog</strong><br/><em class="lq"># Define a new Model, Input= image <br/># Output= intermediate representations for all layers in the  <br/># previous model after the first.</em><br/><strong class="nw iu">successive_outputs = [layer.output for layer in model.layers[1:]]</strong></span><span id="462e" class="lr ls it nw b gy oe ob l oc od"><em class="lq">#visualization_model = Model(img_input, successive_outputs)</em><br/><strong class="nw iu">visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)</strong></span><span id="dce7" class="lr ls it nw b gy oe ob l oc od"><em class="lq">#Load the input image</em><br/><strong class="nw iu">img = load_img(img_path, target_size=(150, 150))</strong></span><span id="99a5" class="lr ls it nw b gy oe ob l oc od"><em class="lq"># Convert ht image to Array of dimension (150,150,3)</em><strong class="nw iu"><br/>x   = img_to_array(img)                           <br/>x   = x.reshape((1,) + x.shape)</strong></span><span id="eb27" class="lr ls it nw b gy oe ob l oc od"><em class="lq"># Rescale by 1/255</em><br/><strong class="nw iu">x /= 255.0</strong></span><span id="3fbe" class="lr ls it nw b gy oe ob l oc od"># Let's run input image through our vislauization network<br/># to obtain all intermediate representations for the image.<br/><strong class="nw iu">successive_feature_maps = visualization_model.predict(x)</strong></span><span id="e01d" class="lr ls it nw b gy oe ob l oc od"># Retrieve are the names of the layers, so can have them as part of our plot<br/><strong class="nw iu">layer_names = [layer.name for layer in model.layers]<br/>for layer_name, feature_map in zip(layer_names, successive_feature_maps):<br/>  print(feature_map.shape)<br/>  if len(feature_map.shape) == 4:</strong><br/>    <br/>    <em class="lq"># Plot Feature maps for the conv / maxpool layers, not the fully-connected layers</em><br/>   <br/>    <strong class="nw iu">n_features = feature_map.shape[-1]</strong>  <em class="lq"># number of features in the feature map</em><br/>    <strong class="nw iu">size       = feature_map.shape[ 1]</strong>  <em class="lq"># feature map shape (1, size, size, n_features)</em><br/>    <br/>    <em class="lq"># We will tile our images in this matrix</em><br/>    <strong class="nw iu">display_grid = np.zeros((size, size * n_features))</strong><br/>    <br/>    # Postprocess the feature to be visually palatable<br/>    <strong class="nw iu">for i in range(n_features):<br/>      x  = feature_map[0, :, :, i]<br/>      x -= x.mean()<br/>      x /= x.std ()<br/>      x *=  64<br/>      x += 128<br/>      x  = np.clip(x, 0, 255).astype('uint8')<br/>      </strong># Tile each filter into a horizontal grid<br/><strong class="nw iu">      display_grid[:, i * size : (i + 1) * size] = x</strong></span><span id="a917" class="lr ls it nw b gy oe ob l oc od"><em class="lq"># Display the grid</em><br/>    <strong class="nw iu">scale = 20. / n_features<br/>    plt.figure( figsize=(scale * n_features, scale) )<br/>    plt.title ( layer_name )<br/>    plt.grid  ( False )<br/>    plt.imshow( display_grid, aspect='auto', cmap='viridis' )</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/bb855d8b305895f2c0dbf6b646919276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDFr4bAVsfcwlmN0uylekQ.png"/></div></div><p class="on oo gj gh gi op oq bd b be z dk translated">狗图像的特征图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/bd3972559053e5d4a7a45acea9db24fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quEdwD3xxBRyAgev1aQG5Q.png"/></div></div><p class="on oo gj gh gi op oq bd b be z dk translated">猫图像的特征图</p></figure><p id="7a9e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，对于狗的图像，鼻子和舌头是非常突出的特征，而对于猫的图像，耳朵和尾巴在特征图中非常突出。</p><p id="6cc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代码可用<a class="ae mp" href="https://github.com/arshren/Feature-Visualization" rel="noopener ugc nofollow" target="_blank">此处</a></p><h2 id="ecd7" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">结论:</h2><p id="ec3f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">可视化CNN如何学习识别图像中存在的不同特征的内部故事，提供了对该模型如何工作的更深入的了解。这也将有助于理解为什么模型可能无法正确地对一些图像进行分类，从而微调模型以获得更好的准确性和精确度。</p><h2 id="d66d" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">参考资料:</h2><div class="ot ou gp gr ov ow"><a href="https://keras.io/api/layers/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd iu gy z fp pb fr fs pc fu fw is bi translated">Keras文档:Keras层API</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">层是Keras中神经网络的基本构建块。一个层包括一个张量输入张量输出…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">keras.io</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk ks ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://keras.io/examples/conv_filter_visualization/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd iu gy z fp pb fr fs pc fu fw is bi translated">卷积滤波器可视化— Keras文档</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">从__future__ import print_function导入时间将numpy作为np从PIL导入图像作为pil_image从…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">keras.io</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk ks ow"/></div></div></a></div><p id="9f67" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://arxiv.org/pdf/1804.11191.pdf" rel="noopener ugc nofollow" target="_blank">卷积神经网络如何看待世界——卷积神经网络可视化方法综述</a></p><p id="8153" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></p><p id="2189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://ai.googleblog.com/2017/11/feature-visualization.html" rel="noopener ugc nofollow" target="_blank">特征可视化——谷歌人工智能博客</a></p><p id="8e9e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</a></p></div></div>    
</body>
</html>