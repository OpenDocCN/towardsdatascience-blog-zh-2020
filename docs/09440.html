<html>
<head>
<title>Text Classification with LSTMs in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中基于 LSTMs 的文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6?source=collection_archive---------15-----------------------#2020-07-06">https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6?source=collection_archive---------15-----------------------#2020-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f07c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">在 PyTorch 中使用 LSTMs 实现的文本分类基线模型</em></h2></div><blockquote class="kj"><p id="8476" class="kk kl it bd km kn ko kp kq kr ks kt dk translated">问题仍然悬而未决:如何学习语义学？什么是语义学？基于 DL 的模型能够学习语义吗？</p></blockquote><h1 id="39a7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="e867" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">这篇博客的目的是解释如何基于 LSTMs 构建一个文本分类器，以及如何使用 PyTorch 框架来构建它。</p><p id="bd7c" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">我想从下面这个问题开始:<em class="mm">如何对文本进行分类？</em>在不同的前提下，从不同的角度提出了几种方法，但是<em class="mm">哪一种是最合适的呢？</em>。停下来问自己一个有趣的问题:<em class="mm">作为人类，我们如何对文本进行分类？</em>，<em class="mm">我们的大脑在对文本进行分类时考虑了什么？</em>。这类问题很难回答。</p><p id="a430" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">目前，我们可以访问一系列不同的文本类型，如电子邮件、电影评论、社交媒体、书籍等。在这个意义上，文本分类问题将由想要分类的内容来确定(例如<em class="mm">是否想要对给定文本的极性进行分类？是不是打算把一套影评按类别分类？是否打算按主题对一组文本进行分类？</em>)。在这方面，文本分类的问题大部分时间被归类在以下任务下:</p><ul class=""><li id="46c2" class="mn mo it lo b lp mh ls mi lv mp lz mq md mr kt ms mt mu mv bi translated"><strong class="lo iu">情感分析</strong></li><li id="3a83" class="mn mo it lo b lp mw ls mx lv my lz mz md na kt ms mt mu mv bi translated"><strong class="lo iu">新闻分类</strong></li><li id="3fbc" class="mn mo it lo b lp mw ls mx lv my lz mz md na kt ms mt mu mv bi translated"><strong class="lo iu">话题分析</strong></li><li id="337f" class="mn mo it lo b lp mw ls mx lv my lz mz md na kt ms mt mu mv bi translated"><strong class="lo iu">问题解答</strong></li><li id="b149" class="mn mo it lo b lp mw ls mx lv my lz mz md na kt ms mt mu mv bi translated"><strong class="lo iu">自然语言推理</strong></li></ul><p id="72c5" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">为了深入这个热门话题，我真的推荐看一看这篇论文:<a class="ae nb" href="https://arxiv.org/pdf/2004.03705.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mm">基于深度学习的文本分类:综合复习</em> </a> <em class="mm">。</em></p><h1 id="c242" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated">方法学</h1><p id="8f70" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">该模型的两个关键是:标记化和递归神经网络。标记化是指将文本拆分成一组句子或单词(即<em class="mm">标记)</em>的过程。在这方面，<em class="mm">标记化技术</em>可以在序列级或<em class="mm">单词级</em>应用。为了理解<em class="mm">标记化</em>的基础，可以看一下:<a class="ae nb" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank"> <em class="mm">信息检索简介</em> </a> <em class="mm">。</em></p><p id="35d7" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">另一方面，RNNs(递归神经网络)是一种神经网络，众所周知，它能很好地处理序列数据，例如文本数据。在这种情况下，实现了一种特殊的 RNN，即 LSTMs(长短期记忆)。lstm 是 RNNs 的<em class="mm">改进</em>版本之一，本质上 lstm 在处理<em class="mm">长句时表现更好。</em>为了更深入的了解什么是 RNNs 和 LSTMs，可以看一下:<a class="ae nb" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> <em class="mm">了解 LSTMs 网络</em>。</a></p><p id="b9e2" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">因为这个博客的想法是提出一个文本分类的基线模型，所以文本预处理阶段是基于标记化技术的，这意味着每个<em class="mm">文本句子</em>将被<em class="mm">标记化</em>，然后每个<em class="mm">标记</em>将被转换成其基于索引的表示。然后，每个基于索引的<em class="mm">记号语句将顺序地通过嵌入层，该嵌入层将输出每个记号的嵌入表示，这些记号通过两层 LSTM 神经网络，然后最后的 LSTM 隐藏状态将通过双线层神经网络，该网络输出由 sigmoid 激活函数过滤的单个值。下图描述了模型架构:</em></p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nf"><img src="../Images/f4de2ba1c25b92f16f71a727367daec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hkR4Bqiq1MN6Mew8E9t1w.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图一。模型架构</p></figure><h1 id="f196" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated"><strong class="ak">预处理</strong></h1><p id="212e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">这个项目中使用的数据集取自一场<em class="mm"> k </em> <a class="ae nb" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank"> <em class="mm"> aggle 竞赛</em> </a> <em class="mm"> </em>，该竞赛旨在预测<em class="mm">哪些推文是关于真实灾难的，哪些不是。</em>本质上，数据集是关于一组用 1 和 0 标记的原始格式的推文(1 表示真正的灾难，0 表示不是真正的灾难)。看一下数据集的头部，它看起来像:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nv"><img src="../Images/af61854a2713286dc0dadbf33dcef901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9L8--ESS0hBxejec9UniVg.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图二。推文数据集的头</p></figure><p id="3b62" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">正如我们所看到的，有些列必须删除，因为它们毫无意义，因此删除不必要的列后，结果数据集将如下所示:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nw"><img src="../Images/8c6c735f868ea852de3ff0d7f05acb3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsSmvUXEEvrq_5GH97EsVA.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图 3。格式化数据集</p></figure><p id="b3d7" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">此时，我们已经可以应用标记化技术，并将每个标记转换成基于索引的表示；下面的代码片段解释了这个过程:</p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 1。预处理</p></figure><p id="8f3d" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">有一些固定的超参数值得一提。例如，<em class="mm"> max_len = 10 </em>指的是每个序列的最大长度，而<em class="mm"> max_words = 100 </em>指的是给定整个语料库要考虑的前 100 个频繁出现的单词。函数<em class="mm"> prepare_tokens() </em>将整个语料库转换成一组记号序列。函数<em class="mm"> sequence_to_token() </em>将每个标记转换成其索引表示。</p><h1 id="eb4c" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated">模型</h1><p id="973d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">作为输入层，它被实现为嵌入层。这个嵌入层获取每个令牌，并将其转换为嵌入的表示。这种嵌入的表示然后通过两个堆叠的 LSTM 层。最后，LSTM 的最后一个隐藏状态通过一个二线性层神经网络。下面的代码片段显示了用 PyTorch 编写的上述模型架构。</p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 2。模型架构</p></figure><p id="5298" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">因此，让我们分析一下展示的模型架构的一些重要部分。在第 16 行中，嵌入层被初始化，它接收参数:<em class="mm"> input_size </em>，它指的是词汇表的大小，<em class="mm"> hidden_dim </em>，它指的是输出向量的维数，以及<em class="mm"> padding_idx </em>，它用零来完成不满足所需序列长度的序列。</p><p id="d66b" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">在第 17 行中，LSTM 层被初始化，它接收参数:<em class="mm"> input_size </em>，它指的是嵌入令牌的维度，<em class="mm"> hidden_size </em>，它指的是隐藏和单元状态的维度，<em class="mm"> num_layers </em>，它指的是堆叠的 LSTM 层的数量，以及<em class="mm"> batch_first </em>，它指的是输入向量的第一维度，在这种情况下，它指的是批量大小。</p><p id="3289" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">在第 18 行和第 19 行，线性层被初始化，每个层接收参数:<em class="mm"> in_features </em>和<em class="mm"> out_features </em>，分别表示输入和输出尺寸。</p><h1 id="7de7" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated">培训阶段</h1><p id="ee4f" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">为了准备好训练阶段，首先，我们需要准备如何将序列输入到模型中。为此，PyTorch 提供了两个非常有用的类:<em class="mm"> Dataset </em>和<em class="mm"> DataLoader。</em>Dataset<em class="mm">class</em>class<em class="mm"/>的目的是提供一种简单的批量迭代数据集的方法。<em class="mm">数据加载器</em>的目的是创建<em class="mm">数据集</em>类的可迭代对象。下面的代码片段展示了这两个类的最小化实现。</p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 3。数据集和数据集加载器</p></figure><p id="2d1b" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">现在，是时候迭代训练集了。首先，让我们看看培训阶段是什么样子的:</p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 4。培训阶段</p></figure><p id="1327" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">第 2 行定义了优化器。从第 4 行开始，实现了历元上的循环。值得一提的是，在 PyTorch 中，我们需要"<em class="mm">打开</em>训练模式"，正如您在第 9 行中看到的，尤其是当我们必须从"<em class="mm">训练模式</em>切换到"<em class="mm">评估模式</em>(我们将在后面看到)时，这样做是很有必要的。本质上，训练模式允许对梯度的更新，而评估模式取消对梯度的更新。</p><p id="67b7" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">需要强调的是，在第 11 行，我们使用由<em class="mm"> DatasetLoader </em>创建的对象进行迭代。在 PyTorch 中，计算损失函数、计算梯度、通过实现一些优化器方法来更新参数以及使梯度为零是相对容易的。如我们所见，在第 20 行中，通过将<em class="mm"> binary_cross_entropy </em>实现为损失函数来计算损失，在第 24 行中，误差被向后传播(即梯度被计算)，在第 30 行中，通过将<em class="mm"> RMSprop </em>实现为优化器来更新每个参数，然后梯度被释放以开始新的时期。</p><h1 id="7fde" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated">估价</h1><p id="2848" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">评估部分与我们在培训阶段所做的非常相似，主要区别是从<em class="mm">培训模式</em>变为<em class="mm">评估模式。</em></p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 5。评估阶段</p></figure><p id="5665" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">正如我们所见，在第 6 行中，模型被更改为<em class="mm">评估模式</em>，同时跳过第 9 行中的渐变更新。然后，通过<em class="mm"> DatasetLoader </em>对象迭代测试集(第 12 行)，同样，预测值保存在第 21 行的<em class="mm">预测列表</em>中。</p><p id="3bac" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">最后，我们只需要计算精度。为了记住精度是如何计算的，让我们看一下公式:</p><pre class="ng nh ni nj gt nz oa ob oc aw od bi"><span id="31e5" class="oe kv it oa b gy of og l oh oi">Accuracy = (True Positives + True Negatives) / Number of samples</span></pre><p id="a607" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">在这方面，精度计算如下:</p><figure class="ng nh ni nj gt nk"><div class="bz fp l di"><div class="nx ny l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">代码片段 6。精确度计算</p></figure><h1 id="0765" class="ku kv it bd kw kx ky kz la lb lc ld le jz nc ka lg kc nd kd li kf ne kg lk ll bi translated">结论</h1><p id="592f" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">在这篇博客中，我们已经解释了文本分类的重要性，以及在不同观点下解决文本分类问题的不同方法。</p><p id="6594" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">以 LSTMs 神经网络为模型核心，实现了文本分类的基线模型，同时利用 PyTorch 作为深度学习模型框架的优势，对模型进行了编码。这个模型中使用的数据集取自一个 Kaggle 竞赛。这个数据集是由推文组成的。在预处理步骤中，展示了一种处理文本数据的特殊技术，即<em class="mm">标记化</em>。</p><h2 id="193f" class="oe kv it bd kw oj ok dn la ol om dp le lv on oo lg lz op oq li md or os lk ot bi translated">未来的工作</h2><p id="7d67" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg kt im bi translated">正如前面提到的，这个博客的目的是为文本分类任务提供一个基线模型。值得一提的是，文本分类的问题超出了两层 LSTM 体系结构，在该体系结构中，文本在基于标记的方法下进行预处理。最近的工作通过实施基于变压器的架构(例如<a class="ae nb" href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank"> BERT </a>)展示了令人印象深刻的结果。然而，通过遵循这一思路，可以通过移除基于标记的方法并代之以实现基于单词嵌入的模型(例如，<a class="ae nb" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> word2vec-gensim </a>)来改进所提出的模型。同样，可以应用双向 LSTMs 来捕捉更多的上下文(以向前和向后的方式)。</p><p id="a2bb" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">完整的代码可在<a class="ae nb" href="https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github . com/FernandoLpz/Text-Classification-lst ms-py torch</a>获得</p><p id="074e" class="pw-post-body-paragraph lm ln it lo b lp mh ju lr ls mi jx lu lv mj lx ly lz mk mb mc md ml mf mg kt im bi translated">随意克隆或者叉:)</p></div></div>    
</body>
</html>