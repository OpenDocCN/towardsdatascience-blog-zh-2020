<html>
<head>
<title>De-Mystifying XGBoost Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开XGBoost第二部分的神秘面纱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/de-mystifying-xgboost-part-ii-175252dcdbc5?source=collection_archive---------18-----------------------#2020-03-18">https://towardsdatascience.com/de-mystifying-xgboost-part-ii-175252dcdbc5?source=collection_archive---------18-----------------------#2020-03-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="82bd" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">揭开XGBoost的神秘面纱</h2><div class=""/><div class=""><h2 id="1647" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">挖掘超参数</h2></div><h1 id="5496" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">序幕</h1><p id="0932" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这是去神秘化XGBoost <a class="ae mc" href="https://medium.com/@b.debanjan/de-mystifying-xgboost-part-i-f37c5e64ec8e" rel="noopener">第一部分</a>的延续。如果可能的话，准备一份打印件，因为我会在整个博客中提到它。</p><p id="338c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在这篇博客中，我将讨论我们在训练XGBoost模型时使用的概念和各种参数背后的直觉。很多时候，我们盲目的训练这样的模型。这个算法是如此的符合逻辑，正如你从<a class="ae mc" href="https://medium.com/@b.debanjan/de-mystifying-xgboost-part-i-f37c5e64ec8e" rel="noopener">第一部分</a>中所理解的那样，即使盲目地使用所有的检查和平衡(正如我们将在下面讨论的)，也很难得出一个草率的模型。</p><p id="3907" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">或者是？你怎么想呢?</p><p id="c5bd" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在这篇博客的最后，我们应该能够更直观地看到参数，并在调整它们和推理为什么一个模型在看不见的数据上表现不佳时使用直觉。我将只讨论这个故事中的几个参数，其余的将在第五部分讨论。此外，你应该在<a class="ae mc" href="https://medium.com/@b.debanjan/de-mystifying-xgboost-part-i-f37c5e64ec8e" rel="noopener">第一部分</a>中找到所有测验问题的答案。</p><p id="09a5" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我们开始吧..</p><h1 id="4f79" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">直觉</h1><h2 id="6cb2" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">梯度</h2><p id="ffcf" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们从第一部分的数学中得到的最直观的东西是梯度的重要性。它是XGBoost如何创建和扩展树的基础。我们现在知道，叶分数=-G/(H+λ)，节点中的损失与-G/(H+λ)成比例。拉姆达，记得吗？叶分数的L2正则化参数。</p><p id="c8ba" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因此，为了减少从父节点到子节点的损失，我们需要减少子节点的损失，因此(带有负号)mod G的值需要更大。g = p-y的总和。因此，树函数需要找到列值条件，使得(p-y)的模和在每个子代中都很大。这发生在所有的(p-y)符号相同的时候，对吗？想想吧。</p><p id="332a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对，所以直觉上，一个好的分裂条件会试图将一个标签的样本分离到一个子节点中，而将相反标签的样本分离到其对应的兄弟节点中。它们混合得越多,( p-y)的和就越小。因此，良好的直观分割条件也将损耗降至最低。</p><p id="7643" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">再次直观地，权重或delta分数与梯度的负值成比例。这意味着<strong class="li ja"><em class="mt">δy hat</em></strong>将自身调整到梯度(或误差)的相反方向。这就是我们想要的！对吗？纠正错误。</p><p id="5e6f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">随着这些树不断地校正整体误差，对于一个样本来说，<strong class="li ja"><em class="mt">【pi】</em></strong>(其从<strong class="li ja"> <em class="mt"> y hat i) </em> </strong>不断地从一棵树变化到下一棵树。见<a class="ae mc" href="https://medium.com/@b.debanjan/de-mystifying-xgboost-part-i-f37c5e64ec8e" rel="noopener">第一部</a>最后一张图。对于一个样本，<strong class="li ja"> <em class="mt"> y hat </em> </strong>可能随着它从一棵树移动到下一棵树而增加或减少。在每棵树的末端，(<strong class="li ja"><em class="mt"/></strong>)或渐变，就是那个样本的误差<strong class="li ja"><em class="mt"/></strong>。或者是<strong class="li ja"> <em class="mt"> i </em> </strong>的最后一个最佳预测值与真实标号之差。和<strong class="li ja"><em class="mt">【π-yi】</em></strong>的总和或误差(在一片叶子或覆盖物中所有样本的平均值👇)是保持控制所有树中的分裂逻辑的东西。</p><p id="dcf7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这就是为什么我们说XgBoost从错误中不断学习。</p><h2 id="21f2" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">粗麻布/封面</h2><p id="5081" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们还在<a class="ae mc" href="https://medium.com/@b.debanjan/de-mystifying-xgboost-part-i-f37c5e64ec8e" rel="noopener">第一部分</a>中了解到，对于二元logistic损失函数，hessian是<strong class="li ja"> <em class="mt"> p(1-p) </em> </strong>。在一个节点上，hessian的和是<strong class="li ja"> <em class="mt"> pi(1-pi) </em> </strong>的和。当XGBoost为二进制分类指定最小子权重时，该值被视为最小允许值。让我们想一想这个问题。</p><p id="e14e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">想象一下，在某棵树上，向下一定深度，有一个节点，当在某种条件下分裂时，给出两个节点，其中一个有10个样本。比方说，这些样本的<strong class="li ja"> <em class="mt"> y hat </em> </strong>值，在进入树之前，就已经相当成熟，即<strong class="li ja"> <em class="mt"> y hat t-1 </em> </strong> s接近自己的预期得分或者完全错误。让我们说<strong class="li ja"> <em class="mt"> pi </em> </strong>因此，对于具有标签1的那些，已经在avg 0.95上，对于具有标签0的那些，已经在avg 0.95上。并且有一些错误分类的标签1样品具有<strong class="li ja"> <em class="mt"> pi </em> </strong> 0.05，反之亦然，标签0样品。</p><p id="7e41" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这片叶子值多少钱？就是~= 0.95*0.05*10 ~= 0.475。即使有一些类1样本的<strong class="li ja"> <em class="mt"> pi </em> </strong>在错误的一侧，反之亦然，对于类0样本，hessian的和仍然是~= 0.475。</p><p id="5d43" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果XGBoost的<strong class="li ja"> <em class="mt">最小子权重</em> </strong>设置为1，则不允许此拆分。而不管它是否从所有可能的子节点中给出最低损失，这些子节点是从该节点上所有可能的特征值分裂条件中导出的。</p><p id="a2cb" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">凭直觉，一个低总和的黑森，或低<strong class="li ja"> <em class="mt">盖</em> </strong>意味着，我们正在考虑一个非常渺茫和局部的条件。低的hessian或cover会导致更高的<strong class="li ja"> <em class="mt"> delta y hat </em> </strong> s(因为它在分母中)的-G/(H+lambda)。给定默认的λ为1，非常低的hessian几乎导致权重与梯度成比例。这种权重会对错误分类的样本造成大的错误分数移动(它们的<strong class="li ja"> <em class="mt"> y帽</em> </strong>将与它们的错误或梯度相反地移动，以及节点上多数类的样本，它们的<strong class="li ja"> <em class="mt"> y帽</em> </strong>将在期望的方向上移动，与整体梯度相反)。</p><p id="11f9" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果，在分割后，每个孩子的样本量&gt; 100；这样的分割是被允许的，因为黑森人的总数将远大于1。0.95*0.05*100 ~= 4.75.直觉上意味着。在如此大的样本量下，这种拆分更可能是有效的拆分，而不是过度拟合的拆分。我们也可以在看不见的数据中期待同样的特征。并且大的hessian将导致节点中所有样本的受控得分/权重(<strong class="li ja"><em class="mt">△y hat</em></strong>)。</p><p id="1b24" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">或者，也可能是反过来。概率平均值为0.85和/或0.15的10个样本。黑森州的总和应该等于1.275。在这种情况下，我们也没事。这是一个早期的分裂，大多数T21π还没有趋向于1或0。梯度很高，但会混淆，总的来说，最佳权重将导致受控的得分移动或增量。并且这种分叉不是过度配合的分叉。我们需要这样的分裂来开始校正梯度。</p><p id="9ea6" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我很快就有了电子表格，它会让你想象这些动作。</p><p id="8edb" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因此，对于二进制分类，节点上的hessians的覆盖或和对样本的数量以及这些样本的概率(计算到最后一棵树)接近1或0是敏感的。概率越成熟(正确或错误)，证明一个分裂所需的样本数就越高。</p><p id="03a2" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对于线性回归(实际上，这是一个很好的练习，如果你能再次计算线性回归的话)，损失函数是均方误差，hessian和为1*n，即样本数。因此，对于线性回归，cover就是一个节点上的样本数。</p><p id="2e01" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因此，为了形象化默认的<strong class="li ja"> <em class="mt"> min child weight </em> </strong>为1，认为需要至少10个样本在<strong class="li ja"> <em class="mt"> pi </em> </strong> 0.85和0.15附近，或者至少20个样本在<strong class="li ja"> <em class="mt"> pi </em> </strong> 0.95和0.05附近，至少50个样本在0.98、0.02附近，以此类推。如果您认为在您的用例中这是一个公平的节点纯度阈值，那么1是一个很好的数字。可以少去也可以多去。一般来说，不建议小于1，除非你的样本量很小，只有几百个。</p><p id="efcd" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">超过<strong class="li ja"> <em class="mt">的最小子体重</em> </strong>的1是安全的，保守的。然而，你爬得越高，矮树越难劈开。所以即使你有一个很高的最大深度，它也可能永远达不到那个深度。我们将很快讨论更多的深度。</p><h2 id="73b9" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">梯度黑森权衡</h2><p id="f647" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">你现在一定感觉到了，在gradient和hessian之间，有一场持续的拉锯战。渐变会尝试给出更高的mod leaf分数或<strong class="li ja"> <em class="mt"> delta y hat </em> </strong> s，而Hessian会降低分数或平均分数。</p><p id="6d71" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">当一个模型刚刚开始，我们有高梯度和高hessians了。随着我们不断添加树木，我们有较低的梯度，黑森也变小了。事实上，在许多树之后，当大多数样本具有低梯度或误差时，hessian几乎为零，唯一阻止我们爆炸权重或分数的是分母中的lambda。</p><p id="8691" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">总的来说，这是一个控制<strong class="li ja"><em class="mt"/></strong>的游戏，以便它永远不会太大，并根据每个节点上的误差在正确的方向上。</p><h2 id="891b" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">希腊字母的第11个</h2><p id="ec51" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">那么，对于lambda该怎么办呢？我会说，默认值1是一个非常好的值。它可以防止分数爆炸，同时也不会过多地降低梯度的权重。</p><p id="d8e8" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们增加λ，虽然没有明显的伤害。权重或叶分数将更小，因此模型将学习得更慢。将会有更多的树，更多的内存，需要更多的时间。但对模型没有伤害。但是，如果我们减少它，事情会反过来，我们可能会损害模型。怎么会？我们会在讨论<strong class="li ja"> <em class="mt"> eta </em> </strong>的时候讨论。</p><p id="0252" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">但是，想想看，如果lambda小于1，并且y hat(<strong class="li ja"><em class="mt"/></strong>) s非常接近1或0(导致hessian的和几乎为0 ),我们最终会得到分数或δy hat(<strong class="li ja"><em class="mt"/></strong>,这可能超过样本上的误差或梯度。对吗？因此，我们将来回超越梯度。听起来不太好，对吧？</p><h2 id="e8c3" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">Base_Score和零深度树</h2><p id="7381" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在我们知道了树木是如何生长的。如果我们有0深度的树呢？比如为什么要尽量把损耗从<strong class="li ja"> <em class="mt"> lp </em> </strong>降低到<strong class="li ja"> <em class="mt"> lc </em> </strong>。当我们在图12中求导时，L或叶片数也可能是1，对吗？完全正确，是的，我们可以在XgBoost中训练深度为0的树——试试吧。</p><p id="6523" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">但是会发生什么呢？既然，所有的样本都会在单个节点上，得到相同的分数，想一想，损失最小化的最好方法会是什么？纯直觉说，我们将需要根据类的分数改变<strong class="li ja"> <em class="mt"> y hat </em> </strong> s。假设我们有200个0类样本和20个1类样本。直觉上，我将开始从0到-5减少所有样本的<strong class="li ja"> <em class="mt"> y hat </em> </strong> s，因为类0样本在数量上更大。</p><p id="a1a2" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们选择w=-G/(H)，就会发生这种情况。如果我们将那个加到<strong class="li ja"> <em class="mt"> y hat </em> </strong> = 0，我们将得到所有样本的一个分数，这将导致最小的对数损失，并且考虑到上面的例子，δ或w将肯定是负数。只是给你那个数字，<strong class="li ja"> <em class="mt"> y hat </em> </strong> = -1.63将是所有220个样本的最佳<strong class="li ja"> <em class="mt"> y hat </em> </strong>，导致最小LogLoss。</p><p id="ecba" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">那么，XgBoost会训练一轮就停下来吗？(假设我们停止在最佳对数损失)。如果我多轮呢？是的，它会停在一轮，但是，只有当我们不使用lambda。如果我们使用<strong class="li ja"> <em class="mt"> lambda </em> </strong>正则化和<strong class="li ja"> <em class="mt"> eta </em> </strong>(学习速率)，XGBoost将在几轮中不断减少LogLoss，然后一旦无法再减少时，它将停止。为什么？因为我们知道λ和学习率都减慢了实际的<strong class="li ja"><em class="mt">△y帽子</em> </strong>或分数。</p><p id="a822" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">所以如果w = -G/H，它将在一轮后停止，但如果w = eta*-G/(H+1)，它将继续几轮。</p><p id="0aea" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">很酷的事实——对吗？😀。那么，我们为什么不用它呢？因为那只会给我们一个取决于阶级分布的概率。我们可以通过简单的数学计算发现。我们不需要任何功能。当我们考虑到某个事物属于一个类的概率取决于它的特征和价值时，我们需要在条件上进行分解。我们将立即拥有两个节点。L = 2。而我们以gain =<strong class="li ja"><em class="mt">lp-(lc1</em></strong>+<strong class="li ja"><em class="mt">【lc2】</em></strong>)回到LP和lc。当我们一棵接一棵地建立子树时，我们试图最大化收益。</p><p id="0090" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">事实上，给定200:20的类分布，从简单的数学和对数损失的最小化得出的分数(y hat = -1.63)可以用作<strong class="li ja"> <em class="mt"> base_score </em> </strong>而不是0。也就是说，只有当你认为阶级分布对最终概率有很大的发言权(偏差)时。但是请注意，这是假设您的训练样本类分布将始终与未知分布完全相同，并且每个预测都将以该偏差开始。所以——请小心使用。不推荐。</p><p id="cc3a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">使用级联模型时，基本分数有时也会得到不同于0的值。就像相同的样本依次经过不同的特征集和/或模型，其中第一个模型的输出变成了第二个模型的偏差，等等。</p><h2 id="69dd" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">最大增量步长</h2><p id="b4c3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们用一些真实的数据来理解这一点。我在一个节点中创建了一个示例数据集。它在类别0中具有200个样本的标签分布，在类别1中具有20个样本的标签分布。在一个条件下，节点分裂成两个节点。一个有185个样本，另一个有35个样本。假设这185人中，有180人来自0班，5人来自1班。35人中，0班20人，1班15人。见下图。这实际上看起来像一个很好的分裂，对不对？子1中的多数分数类0和子2中的多数分数类1。</p><p id="42da" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我已经给了样本一些先验概率值，假设父节点在一些树之后。因此，<strong class="li ja"> <em class="mt"> y hat </em> </strong> s已经远离全= 0。我将大多数0类样本(150个)的概率设为0.1，并保留不同程度的误差或梯度(下面的第2行和第3行)。类似地，我将大多数1类样本的概率设为0.9 (8)，并保持不同的梯度(下面的第2行和第4行)。然后我计算了所有的G、H、损耗和重量/分数，假设λ为1。对于父节点和两个子节点。见下文。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/89736a3e5e3e31a25e3297631c31fd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RO1qW1Ivzwuv5RCQFmKKog.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">节点根据某种标准分裂成可能的子节点。不平衡的数据。您可以在<a class="ae mc" href="https://github.com/run2/demystify-xgboost/blob/master/DeMystifyXGBoost-Example1.xlsx" rel="noopener ugc nofollow" target="_blank">https://github . com/run 2/demystify-xgboost/blob/master/DeMystifyXGBoost-example 1 . xlsx</a>访问该电子表格。你可以看到每个单元格上的公式，看看我是如何计算这些值的。玩弄样本数量和或概率。损失，重量，新的概率等将自动改变！:)</p></figure><p id="05e7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">让我们深入了解一下Gs和Hs。</p><p id="349c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对于p= 0.1的0级，H = 0.1 *(1–0.1)= 0.09。G =(0.1–0)= 0.1。</p><p id="e114" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对于p = 0.9的类1，H再次= 0.09，G =(0.9-1)=-0.1。</p><p id="cae3" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">所以G的mod比H大，对吧？因此，如果我们有许多样本是0类，很少样本是1类，那么具有大多数0类样本的叶子将倾向于具有比H或H + 1高得多的G。在示例中，我们可以看到在子节点1中，来自类0样本的G是32.5 (K20)。对于1类样品，只有1.6克的一点点抵消。h是21.71。因此，我们最终得到一个权重或分数或δ<strong class="li ja"><em class="mt">y hat</em></strong>=-1.36</p><p id="2271" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们允许这种分割，并且子节点以叶子结束，则子节点1中的所有类1样本将具有-1.36的<strong class="li ja"><em class="mt">δy帽</em> </strong>。这意味着，被错误分类的第1类样本的概率将突然以相反的方向跳跃(与它们的梯度相反)。例如，2个类别1样本将从概率0.9跳到0.7。看到红色标记。</p><p id="d417" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这就是最大增量步长的用武之地。如果最大增量步长设置为1，则不会发生上述分割，即使它导致最低<strong class="li ja"> <em class="mt"> lc </em> </strong>。这将迫使算法搜索其他特征值组合，其中<strong class="li ja"> <em class="mt"> lc </em> </strong>可能更高，但是分割将使得分数变化更小。</p><p id="adf1" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">*请注意，在上面的excel中，在损失母公司计算中，我没有使用我们在数学部分计算的1/2因子-1/2*G /(H+1)(图12第一部分)。XGBoost不应用这个1/2因子，因为它是父节点和子节点的常数乘数。</p><h2 id="904a" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">学习率</h2><p id="007a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们已经知道，eta是要乘以叶分数的分数，通过保持<strong class="li ja"><em class="mt">【delta y hat】</em></strong>为低，我们可以确保基于一些局部敏感的列值条件，不会由于某些叶中的局部损失最小化而导致样本概率突然跳跃。</p><p id="5017" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">从上面max delta step题目中的例子来看，如果我们使用eta为0.3，那么叶子分数和<strong class="li ja"> <em class="mt"> delta y hat </em> </strong> s就会降为-1.36*0.3 = -0.4和0.035*0.3=0.01。给定一个<strong class="li ja"> <em class="mt">最大增量步长</em> </strong> 1，这是一个允许的增量。</p><p id="0680" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因此，从子代1中的类别0误分类样本中，两个红色标记的样本将仅移动到2.2–0.4 = 1.8的<strong class="li ja"> <em class="mt"> y hat </em> </strong>，概率为0.85；相比于<strong class="li ja"> <em class="mt"> y hat </em> </strong>而言，没有eta的概率分别为0.84和0.7。因此，整体学习速度较慢(对于正确分割的样本)，但误差较小(对于错误分割的样本)。</p><p id="0845" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这里的关键问题是——eta仅仅控制学习的速度吗？也就是说，它是否只会影响所需树木的数量。较大的数字为较小的<strong class="li ja"><em class="mt">η</em></strong>反之亦然？答案是否定的。</p><p id="ada6" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">由于这种方式，每次分割查看梯度，并考虑其他因素，如最大增量步长、gamma(见下文)、最小子权重和其他树增强器参数，很可能高的eta或eta =1将导致算法进入不能再减少损失的位置(想想超过凹形损失曲线的谷值)，并且基本上停止在非最佳的评估度量值。当λ从1减小时，可以看到相同的效果。</p><p id="280b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">底线是，保持低eta没有坏处，除了学习速度慢。保持较高的eta可能会导致较差的模型。显然太低的eta，像0.01也是不推荐的，因为我们的模型将保持几个小时的训练。因此，默认值0.3是一个好的开始，我们可以稍微降低它，看看评估指标是否更好。在这个去神秘化系列的第三部分中会有一个关于这个的部分。</p><h2 id="a2e4" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">微克</h2><p id="ec20" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这就把我们带到了伽马射线。γ，再次，树增强参数是最小增益，或<strong class="li ja"> <em class="mt"> lp-(lc) </em> </strong>，或<strong class="li ja"> <em class="mt"> lp-(lc1+lc2) </em> </strong>，允许一个节点进一步分裂成两个以上的节点。让我们像下面这样改变上例中每个子节点中类0和类1的样本数。这是一个糟糕的分裂。我们在每个节点中有几乎50%的0类样本，1类样本也是如此。我再次计算了每个节点的G H和损耗。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/f61f1d668558a0b5bcd2d4aa0a19a279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roS3xzPwl-lVNChoxpjsyQ.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">节点根据某种标准分裂成可能节点。低增益。您可以在<a class="ae mc" href="https://github.com/run2/demystify-xgboost/blob/master/DeMystifyXGBoost-Example1.xlsx" rel="noopener ugc nofollow" target="_blank">https://github . com/run 2/demystify-xgboost/blob/master/DeMystifyXGBoost-example 2 . xlsx</a>访问该电子表格。你可以看到每个单元格上的公式，看看我是如何计算这些值的</p></figure><p id="69f1" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我们可以看到，增益或者说<strong class="li ja"> <em class="mt"> lp-lc </em> </strong>现在是= 0.966。上例中的增益约为7。直觉上，这种分裂看起来也不太好。如果gamma设置为1，这种分割将被排除。如果该增益是从所有列值组合中可获得的最佳增益，则该节点根本不会分裂。它将成为一个叶节点。</p><p id="6b22" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">总体来说，<strong class="li ja"> <em class="mt"> lp </em> </strong>和<strong class="li ja"> <em class="mt"> lc </em> </strong>依赖于-G/(H+λ)。因此，确保良好的损失减少基本上意味着获得一个分裂，其中在分裂的每个子代中有一个显著的G. Or，相同符号(+或-)的梯度。我们在讨论梯度的时候已经讨论过了。</p><p id="bfe6" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">找到允许的最佳伽玛或最小增益并不容易，低于最佳伽玛或最小增益，就不允许分裂。我们需要真正确定我们的数据大小和分布，才能开始使用它。但是，如果“最大深度”设置为较高的值，具有正的gamma值，将会自动修剪树，这样就不会有不必要的分割，并且损失减少非常少。</p><p id="faa6" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">即使我们不设置γ，算法的固有特性，即寻找具有最大增益的分裂，将避免低增益分裂。伽马射线是额外的保护，以确保树木是保守的。特别好使用，如果我们计划使用许多保守的模型，然后集合它们。</p><p id="feae" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">最小儿童体重和伽玛之间经常会出现混淆。如果你已经到达这里，希望没有这样的困惑。如果你注意到例2中的孩子2，hessian的和是9.6 (K42)。如果最小子体重为10，这将成为无效的分割。最小子重量确保一片叶子有足够的覆盖，从而控制重量或分数。Gamma确保我们不会分割节点，这样从父节点到子节点的整体增益会非常低。所以他们不一样。</p><h2 id="88c2" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">秤pos重量和AUC</h2><p id="efa8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">好吧，这个，我必须承认很难理解。我用了几次，但不知道它的头或尾，每次都有罪恶感。但是现在我们已经掌握了所有的数学知识，尤其是excel表格，这应该就像“吃米饭和扁豆汤”一样简单。</p><p id="4f12" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">首先，我们再回忆一下<strong class="li ja"> <em class="mt"> y hat </em> </strong>对概率的影响。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/633c6c3b2960071bb83406e8e774f29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*D-D6G6ORTPib1Eba0NQhNg.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">乙状结肠函数。图17。</p></figure><p id="dcae" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如我们所见，对于具有类别1或正标签的样本，在<strong class="li ja">T5处，y hatT7为~= 5，它们几乎被完全分类。所以对于所有这样的样本，树中的<strong class="li ja"><em class="mt">△y帽</em> </strong> s应该是把<strong class="li ja"> <em class="mt"> y帽</em> </strong> s从0推到5。因此δy hat应该是正的。对于类别0的样本，情况正好相反。</strong></p><p id="cac8" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">第二，逻辑损失函数的本质是惩罚错误，而不是欣赏正确的事情。查看ML备忘单中关于物流损失以及yln(p)或(1-y)ln(1-p)如何惩罚错误的参考资料。为了快速了解，请参见下面代表yln(p)的图片。(1-y)ln(1-p)看起来肯定和你想象的一样。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3fb6f741994e16de6a8a099458a7f021.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*jwLFCTNxqS7Jpu7TuFwTdg.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">yln(p)与p的关系。可以看出，当标签1样品的p较低时，损耗非常高。极端。</p></figure><p id="cb4e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这种惩罚和纠正在模型中根深蒂固。在每次分割时，XGBoost将尝试纠正错误分类的样本，而不是将已经做得很好的样本推到它们的期望值。</p><p id="7a0c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们有一个坏的分裂，这是反对这种惩罚和纠正的，就像上面的例子2，你会注意到，<strong class="li ja"><em class="mt">δy hat</em></strong>s有可能没有遵循期望的运动性质。两个子节点都有<strong class="li ja"><em class="mt">δy帽</em> </strong> s为负。然而，如果您注意到示例1，子1的<strong class="li ja"><em class="mt">δy hat</em></strong>具有更多的类0样本，具有负值，而叶2的相反情况具有更多的类1样本。这也说明我们的分裂很糟糕。</p><p id="96f9" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">现在，在一个不平衡的阶级情况下，有时，基于我们想要达到的目标，上述惩罚和纠正并没有很好地发挥作用，即使是在一个好的分裂中。如果你注意到，例子1中的孩子2只有0.03(没有eta和0.01有eta)正<strong class="li ja"><em class="mt">δy hat</em></strong>移动。而孩子1具有1.36的负运动。这种对少数类样本的受限移动是不期望的，特别是，如果我们更关心阈值处的分类准确度，而不是损失的总体减少。</p><p id="ba24" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">让我们稍微讨论一下。</p><p id="f596" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">第一，为什么在具有多数类1样本的子或可能的叶子上，<strong class="li ja"><em class="mt">δy帽</em> </strong>如此小？。这是因为几乎没有任何总体g。即使它是具有多数分数类别1样本的叶子，也总是会有一些错误分类的类别0样本。由于与类别1样本相比，类别0样本如此之多，即使错误分类的样本也可能与正确分类的类别1样本一样多——因此它们将抵消总体g的很多。因此小<strong class="li ja"><em class="mt">δ帽</em> </strong>。</p><p id="37cd" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">相反，分数类0样本占多数的孩子，拥有大量样本。总体梯度严重偏正(K33处为27.7)。错误分类的1类样本非常少，超过了这个梯度。并由此，<strong class="li ja"><em class="mt">δy帽子</em> </strong>相当高且为负(-1.27)。</p><p id="a637" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">所以XGBoost会因为样本数量而有偏差。它正在努力纠正错误。因为有如此多的类0样本，所以忽略将类1样本推向1。它更关心的是将类0样本推向0。</p><p id="1196" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这将降低对数损耗。但是分类正确或不正确的第1类样本将会与错误分类的第0类样本混合在一个小概率范围内。将不会有一个强有力的门槛将它们区分开来。</p><p id="5f73" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们更关心FPR的分类或TPR的准确性，上述行为就不太好。大多情况下，在不平衡的阶级情况下，积极的阶级是罕见的。比如预测地震。或者对恶意软件进行分类。这些模型需要在低FPR下运行，因为将一个干净的文件称为恶意软件并删除它，远比放过一个恶意软件要严重得多。</p><p id="965c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对于这种情况，更重要的是获得正确分类的阳性标记的样品，尽可能远离阴性标记的样品。因此，我们有一条分界线或阈值，在这条分界线上，较低的FPRs具有较高的TPR。</p><p id="e343" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这条线在哪里(在0和1之间)以及个体概率在哪里并不重要。即使所有0类样本都在0.7和0.91之间，所有1类样本都在0.85和0.95之间，我们也可以有一个良好的阈值，比如说0.9。但是对数损失会很严重(见上图),因为0类样本离0很远。但是在我们的用例中，这可能没问题。</p><p id="1e0a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">所以有时候，我们希望算法更偏向于在截止点给我们更好的分类，而不是总体上减少LogLoss。</p><p id="e529" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这就是<strong class="li ja"> <em class="mt"> scale_pos_weight </em> </strong>的用武之地。在梯度和hessian中给予少数类更多的权重，以便这些样本以更大的跳跃更快地远离多数类样本。</p><p id="3b21" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><strong class="li ja"> <em class="mt"> scale_pos_weight </em> </strong>定义为= s1 =(负样本总和*)/(正样本总和*)。但是没有硬性规定。这取决于与多数样本相比，你对少数样本的重视程度。有人说，对于高度不平衡的类，scale_pos_weight应该是s2 = sqrt(负例之和)/sqrt(正例之和)。我们可以选择值&gt; s1和&lt; s2，并尝试最符合我们评估指标的值。</p><p id="168f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">让我们看看下面的例子3。你不能真的看着它😀。从Github下载。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/c2c8d2e4cf65d06e13fdd3794ffab0bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqfu-BMwTJkl6XXeYBS_MQ.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><a class="ae mc" href="https://github.com/run2/demystify-xgboost/blob/master/DeMystifyXGBoost-Example1.xlsx" rel="noopener ugc nofollow" target="_blank">https://github . com/run 2/demystify-xgboost/blob/master/DeMystifyXGBoost-example 3 . xlsx</a>。</p></figure><p id="9b67" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这就是我所做的。</p><p id="a591" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我采取了同样的200:20分配。在左边，我将样本分成两个节点，没有使用比例位置权重。在右边，我已经将样本分成两个节点，每个子节点使用相同的样本，但是这次对阳性样本赋予了额外的权重。</p><p id="e4ca" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在左上角的父节点下，我首先展示了深度为零的树的效果。如果您注意到，我使用-G/H来调整先前的<strong class="li ja"> <em class="mt"> y hat </em> </strong>分数，以获得新的<strong class="li ja"> <em class="mt"> y hat </em> </strong>分数。我还计算了假设深度为零的树的先前测井曲线损失和新测井曲线损失。你会注意到原木损失减少了。分数调整为-1.15。由于梯度变化，它不同于前面讨论的200:20分布的-1.63。如果您将B3到JBOY3乐队的所有概率更改为0.5，您将看到-G/H值更改为-1.63。</p><p id="400f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">回到右边的两个子节点。秤位置权重用于计算G和h。对于1类样品，其在计算中的存在由秤位置权重4加权。</p><p id="1e20" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果你注意到，在没有秤重的情况下，分割后的原木损失从61减少到51。而用秤称重量，原木损失从61减少到55。因此，很明显，我们在使用称重法减少原木损失方面做得不是很好。但是实际的概率呢？</p><p id="ac3e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们注意到，在左边，对两个孩子来说，重量或δy帽子是负的。对于子节点1，我们有一个很好的正G。但是在子节点2中，即使它是一个1类的多数分数节点，总G也不可能是负的。因此，对于两个子节点，delta y hat仅帮助类0样本。</p><p id="8bb2" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在右边，这是一个不同的画面。我们在孩子1中仍然有一个负G，但是它不那么明显了。<strong class="li ja"><em class="mt">δy hat</em></strong>与-1.27相比为-1。但是奇迹发生在第二个孩子身上。现在我们的重力指数是负的-13.4。因为阳性标签样品上的额外重量。这导致正的<strong class="li ja"><em class="mt">δy hat</em></strong>，这将把正确分类的1个样本推向更高的<strong class="li ja"> <em class="mt"> y hat </em> </strong>值，从而在它们和其他样本之间产生空间。</p><p id="94fc" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">为了证明我的观点，我写了一些代码来找出有权重和无权重两种情况下的FPR/TPR/阈值，并绘制它们的ROC。(您将在本系列的第四部分找到代码)。对于这些计算，我需要样本的最终概率。我假设两个子节点都以叶子结束，因此它们的最终概率可以从它们之前的分数(B2到J2或N4到V4)加上<strong class="li ja"><em class="mt">δy hat</em></strong>s(K39 K72用于无权重的子节点，W41 W74用于有权重的子节点)中导出。</p><p id="77e7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这是中华民国的样子。带和不带秤的Pos重量。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/edd601dbd0bbdd517a4340ce20f4435e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*H9D6jMokimsaJvIB5L6ozA.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">3个样本的ROC图。有秤显示重量和无秤显示。</p></figure><p id="3552" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如您所见，在TPR与FPR的早期FPR数据中，带秤重的模型胜过不带秤重的模型。这只是一棵树。当在许多树的模型中使用比例位置权重时，这种差异更加明显。有秤重模型的AUC为0.956875，无秤重模型的AUC为0.941375。</p><p id="1049" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果不使用秤pos重量，而是对单个样品进行称重，则适用相同的概念。在这种情况下，每片叶子中每个样本的梯度和hessian乘以样本权重。因此，对于第I个样本，gi = wi(pi-yi)和hi = wi(pi)(1-pi)。对于多标签分类，我们可以根据需要调整所有样本的权重。</p><p id="727a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">此时可能会出现一个问题。如果当我们被AUC困扰时，我们为什么不将其最小化，而不是LogLoss。答案是，我们不能，因为AUC不是我们可以区分和等于零的东西。然后，可以在不同的操作点(即FPR值)获得AUC值。如果我们的目标是优化一个模型，仅仅是为了在一个操作FPR范围内增加AUC，那么这些树将会过度符合那个特定的标准，因此将会失去一般性。因此，通过使用加权，我们可以增加AUC，同时保持在优化日志损失的范围内。</p><h2 id="7138" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">培训课程分布与真实课程分布</h2><p id="3c16" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在我们结束这个话题之前，还有最后一点。当培训课程的分布与真实课程的分布不同时，我们所学的知识如何发挥作用？在上面的200:20训练课分布的例子中，如果实时未看见的分布是不同的呢？显著地？比如五五开。或者就此而言，在任何这样的训练中，训练集(验证集)和看不见的实时数据的类分布是显著不同的？</p><p id="f24b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">希望你现在能明白，(特别是如果你已经玩过excel表的话)数学和损失最小化对职业平衡很敏感。因此，如果类别分布明显不同，模型将不会如预期的那样在实时不可见分布上执行(和验证)。</p><p id="0d02" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们知道用于预测的看不见的数据是不平衡的，在我们开始使用体重秤之前，我们需要对它的不平衡程度有一个合理的认识。训练数据不必具有与未知数据相同的分布。更多的时候，我们有大量的回顾性训练数据，其中我们可以有更多的少数民族类样本。在这种情况下，我们可以继续使用整个数据并适当地调整权重或缩放pos权重，或者我们可以随机缩小类别以匹配未知的实时数据类别分布并创建多个模型和集合。</p><p id="c093" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">例如，如果我们在训练中有一个200:40的分布，但是我们知道在看不见的数据中有一个200:20的分布，我们可以创建两个200:20分布的模型并集成它们。或者，我们可以继续使用200:40的训练数据集，但是将体重秤位置权重更改为sqrt(200)/sqrt(20)*1/2。引入最后1/2因子是因为训练集中的阳性样本数量是真实数据中的两倍。</p><p id="4e99" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这意味着，当你在训练数据中获得越来越多的少数类样本时，你可以慢慢减少你的体重秤位置权重。您将获得验证集和维持集或训练数据的可比AUC。</p><p id="4440" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">要注意的主要事情是，维持集应该总是具有与真实的不可见数据相同的类分布。这样，在发布模型之前，您可以随时验证AUC或LogLoss(无论您的评估指标是什么)在维持集上的表现是否符合预期。</p><p id="fc5a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在我写作的时候，我觉得，也许我会在我的系列文章的第6部分中详细讨论如何在XGBoost中处理不平衡的数据。深入地。</p><h1 id="4566" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">谢谢</h1><p id="0697" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这就是伙计们，在这个职位。下一篇文章将展示如何快速构建一个docker容器，在其中你将能够运行几乎任何ML内容，并在Jupyter Lab中使用Jupyter和R。这将是第四部分代码的基础，在第四部分中，我将使用真实数据并构建模型来展示我们在第一部分和第二部分中讨论的所有内容。</p><h1 id="6c69" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考</h1><p id="af0c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">所有资源文件都可以在</p><p id="8dab" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><a class="ae mc" href="https://github.com/run2/demystify-xgboost/tree/master" rel="noopener ugc nofollow" target="_blank">https://github.com/run2/demystify-xgboost/tree/master</a></p><div class="no np gp gr nq nr"><a href="https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">sigmoid函数的导数$\sigma (x) = \frac{1}{1+e^{-x}}$</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">感谢为数学栈交换贡献一个答案！请务必回答问题。提供详细信息…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">math.stackexchange.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ne nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">对数损失的梯度和海森如何计算？(问题基于一个数字示例…</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">我想知道logloss函数的梯度和hessian是如何在xgboost中计算的…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">stats.stackexchange.com</p></div></div><div class="oa l"><div class="og l oc od oe oa of ne nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://www.quora.com/Logistic-Regression-Why-sigmoid-function" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">逻辑回归:为什么是乙状结肠函数？</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">回答(第1题，共12题):在二项式回归中，我们希望将响应变量建模为以下变量的线性组合</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">www.quora.com</p></div></div><div class="oa l"><div class="oh l oc od oe oa of ne nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">逻辑回归— ML词汇表文档</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">假设我们得到了学生考试成绩的数据，我们的目标是基于以下因素来预测学生是否会通过考试…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">ml-cheatsheet.readthedocs.io</p></div></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">理解二元交叉熵/对数损失:一个直观的解释</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">有没有想过用这个损失函数到底是什么意思？</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="oi l oc od oe oa of ne nr"/></div></div></a></div></div></div>    
</body>
</html>