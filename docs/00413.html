<html>
<head>
<title>Reinforcement Learning visualised with a predator prey ball game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习与捕食者猎物球游戏可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-visualised-with-a-predator-prey-ball-game-ab03518ac551?source=collection_archive---------35-----------------------#2020-01-12">https://towardsdatascience.com/reinforcement-learning-visualised-with-a-predator-prey-ball-game-ab03518ac551?source=collection_archive---------35-----------------------#2020-01-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/eca50bda1cf787d93b7bc09f9391267f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrIwkj1WTw98-vY-l98HTA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">米卡·鲍梅斯特在<a class="ae kf" href="https://unsplash.com/s/photos/reinforcement-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="73c9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是<a class="ae kf" rel="noopener" target="_blank" href="/reinforcement-learning-and-visualisation-with-a-simple-game-a1fe725f0509">上一篇文章</a>的后续，在这篇文章中，我们看了一个简单的强化学习(RL)游戏，其中一个绿色的球学会在 200 步内到达画布中心的一个小圆圈。我们编写了一个 Q-learning 算法，并使用基于 Tkinter 的 GUI 将其可视化。</p><p id="9074" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在给绿球一个稍微复杂一点的挑战。这一次的目标是学会在 200 步内到达中心，但现在有另一个球，一个红色的球，绿色的球必须避开。红球从圆圈附近开始，随机移动。如果绿色和红色的球相撞，游戏结束。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi le"><img src="../Images/65d5e4da22322b0ed8c1cc66ea30899f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/1*O-vCCWAoAxq0QGDlRSnJrw.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">绿色球学会到达圆</p></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi le"><img src="../Images/1282a560ddabff06afdc917c09790f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/1*tNhTSvelM5yEqrNBDdCU3A.gif"/></div></figure><p id="1b5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">红色的球就像一个喝醉了的掠食者，四处乱跑，希望抓住绿色的球。我当然意识到故事中喝醉的部分使我们的 RL 实现变得更容易些:)要记住的关键是绿球只是学习如何通过不碰到红球来避开它。然而，如果碰巧红球接住了绿球，绿球也无能为力，游戏就结束了。</p><p id="7817" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以现在绿球必须同时学习两件事。它必须明白到达中心是最有意义的事情，它必须明白如何远离红球。</p><p id="2753" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你可能已经想到的，这个 RL 的状态空间比单个绿球场景增长了 2 倍(上图)。最佳行动的决定(可能采取的步骤)不仅取决于绿球本身的位置，还取决于红球的位置。动作空间保持不变，因为绿球可以走的步数没有改变。</p><p id="916e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，让我们看看如何修改上一篇文章中的<a class="ae kf" href="https://github.com/bhattacharyya/reach_circle/blob/master/reach_circle_commandline.py" rel="noopener ugc nofollow" target="_blank">代码</a>来实现这一点。</p><p id="0ad0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可能的步骤现在被声明为</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="375d" class="lo lp it lk b gy lq lr l ls lt">mov_list = [-40,-20,20,40]</span></pre><p id="0303" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而不是我们之前用过的[-20，-10，10，20]。通过加倍步长，我们将单个球的搜索空间缩小了 4 倍。所以对于 2 球系统，我们的搜索空间小了 16 倍。即使在这之后，我们的搜索空间也是 202500，而不是之前的 1800。</p><p id="8436" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们介绍球碰撞的检查和碰撞的惩罚。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="abbc" class="lo lp it lk b gy lq lr l ls lt"># Check if green ball hit the red ball</span><span id="2460" class="lo lp it lk b gy lu lr l ls lt">if (pos_x1 == pos_x2) and (pos_y1 == pos_y2):<br/>   collision = 1<br/>   reward = -200<br/>   print("green collision", pos_x1, pos_y1, pos_x2, pos_y2)<br/>   green_collision_count += 1</span></pre><p id="4138" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到达圆圈的奖励是 100。我最初将冲突的惩罚设置为-25。没有任何学习，碰撞发生了 40%次。大约 20%是因为绿球踩到了红球(GBC，绿球碰撞)，而另外 20%是红球撞到了绿球(RBC)。对我们来说，绿色球击中红色球是重要的统计数据。即使在 100，000 场比赛后，gbc 仍约为 15%。为什么避免碰撞的学习如此缓慢？</p><p id="e461" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很快发现，绿色球的动机完全是因为它追求到达中心，这是因为与碰撞惩罚相比，奖励要高得多。啊哈！GBCs 从 20 下降到 15%的唯一原因是因为球快速到达中心，而不是随机移动，因此随机击中红球的机会较少。所以在下一步中，我们将冲突的惩罚设置为-200。</p><p id="50fc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于红细胞，没有处罚规定。<strong class="ki iu">随着新的罚分，GBCs 在 20，000 场比赛内降至 0% </strong>。它可能会更快完成，但我只在 20k 步后检查。我很乐观，但没有这么乐观！</p><p id="e3c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">整个模拟代码在<a class="ae kf" href="https://github.com/bhattacharyya/reach_circle/blob/master/strc_commandline.py" rel="noopener ugc nofollow" target="_blank">这里</a>可用，可视化代码在同一个库<a class="ae kf" href="https://github.com/bhattacharyya/reach_circle/blob/master/strc_commandline_test.py" rel="noopener ugc nofollow" target="_blank">这里</a>可用。</p><p id="ab3c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到了如何将简单的单球问题扩展到双球问题，从而大大增加了算法的复杂性。大多数真实生活场景要复杂得多，Q 表方法不再实用。</p><p id="fb67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在另一篇文章中，我们将尝试使用 DQN 来实现这个游戏。</p></div></div>    
</body>
</html>