# 人工智能信任危机:如何前进

> 原文：<https://towardsdatascience.com/the-ai-trust-crisis-how-to-move-forward-9b523ea0158d?source=collection_archive---------71----------------------->

## 在多年的大肆宣传和享受发现的独创性之后，世界进入了一场人工智能信任危机。

![](img/cebf98bfe440a5742a1e38e50ec0f2f1.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上 [NeONBRAND](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral) 拍摄的照片

数据和模型中的社会、种族和性别偏见已经成为机器学习行业和社会的一个主要问题。

最近，麻省理工学院取消了对一个流行的计算机视觉数据集的公开访问，因为一组研究人员发现它带有社会偏见，带有厌恶女性和种族主义的标签。

在微小图像中的这一发现，一个[8000 万图像数据集](https://people.csail.mit.edu/torralba/publications/80millionImages.pdf)，是社会偏见如何扩散和传播到机器学习数据集和项目的完美例子。1985 年，普林斯顿大学的语言学和心理学研究人员为英语制作了一个语义词典，名为 [WordNet](https://wordnet.princeton.edu/) ，它已被广泛用于自然语言处理任务。基于 WordNet，麻省理工学院的科学家在 2006 年发布了微型图像，这是一个图像数据集，由互联网上与 WordNet 单词相关的图像搜索汇编而成。由于 WordNet 最初是基于性别和种族偏见的，所以与收集的图像相关的微小图像标签也是如此。

就在几周前， [Pulse](https://arxiv.org/abs/2003.03808) ，一个自我监督照片上采样的生成模型，在人工智能模型中制造了许多噪音和对偏见的担忧，因为美国前总统巴拉克·奥巴马的像素化图像被 Pulse 转换成了一个白人的高分辨率图像。

![](img/57a72f96422d869d0bd182875696766d.png)

*PULSE 模型将像素化的人脸转换成高分辨率的图像，通过将巴拉克·奥巴马的脸变成白人的脸，带有种族偏见。*图片: [Twitter / @Chicken3gg](https://twitter.com/Chicken3gg/status/1274314622447820801)

这两个例子是人工智能中偏见问题的体现，并显示了这个问题的复杂性，远远超出了数据和模型。这是一个全球性问题，需要从多个角度加以解决，而不仅仅是技术上的修补。

# 如何解决偏见问题？

这项技术的发展很快，但可以肯定地说，可以通过多种方式将数据偏差的影响降至最低。没有神奇的药丸。

脸书的首席人工智能科学家 Yann LeCun 解释说,**处理偏差最直接的方法是意识到数据中的不平衡**,并通过平衡不同类别样本的频率来解决它。根据 LeCun 的说法，这是脸书在其人脸识别系统中使用的方法。为了说明这一点，LeCun 喜欢用这样的类比:

> “如果你去医学院，你不会花时间研究普通流感和一些罕见疾病在患者中的发生率。你在罕见疾病上花费了相对更多的时间，因为你需要为它们开发“功能”——扬·勒昆

深度学习系统中的偏差是这样一个问题，因为由于损失的非凸性，它们不会学习和创建罕见类别的特征，而这不会发生在例如逻辑回归中。

最近的一篇论文提出了一种新的方法，通过“T2 不变风险最小化”来识别和最小化偏倚。简而言之，这种方法旨在训练机器学习模型，以估计多个训练分布之间的不变相关性。

# 偏见不是唯一的担忧

这些最近的事件重新开启了关于可信人工智能的辩论:使用人工智能模型的机构和个人必须重新考虑他们制作、开发和部署人工智能的方式。无论是不是基于人工智能的系统，消费者和公民都需要相同级别的权利和安全。今天，人工智能并没有被人们自然地接受，大部分时间都有一种不信任在它周围徘徊。人工智能的道德使用成为信任的先决条件。

**是时候打破这些担忧，改变我们制造信息产业的方式了。**

在欧洲，监管机构正在研究这个问题，他们在 2019 年 4 月发布了一套针对可信人工智能的[道德准则](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)。

![](img/c54cb3a3a625a8f8ba1cd6b649b2339a.png)

2018 年，欧洲已经成立了一个关于 AI 的[高级专家组(AI HLEG)，发布可信 AI 的伦理准则。来源:](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence)[欧盟委员会](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)

[根据指南](https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60419)，可信人工智能应围绕三大支柱展开:

*   **道德**:尊重和拥护道德原则和价值观
*   **坚固性**:从技术和社会角度来看，安全可靠
*   **合法**:尊重所有适用的法律法规

![](img/d051f06063062fbf7071639bed83a4b1.png)

欧盟可信人工智能指南。

围绕这些支柱，欧盟监管机构定义了**七个** **实用的关键要求，任何人工智能系统都应该满足这些要求才能被贴上可信的标签。**

1.  人类监督:人工智能系统必须让人类保持在循环中，提供洞察力，让他们在保护他们的权利和尊重他们的价值观的同时做出明智的决定。
2.  **技术稳健性和加工安全性**:弹性和安全性也是一个主要问题。对可靠性、准确性和再现性提出了更高的要求。人工智能系统不能再独立存在了，它们必须伴随着一个备份系统，以防出现问题。
3.  **隐私和数据治理**:除了数据治理以保证数据质量、完整性和可信的数据访问之外，尊重隐私和数据保护也是实现信任的必要条件。
4.  **多样性、非歧视和公平**:正如本文开头部分所述，偏见会造成巨大影响，从歧视到对特定人群的偏见。值得信赖的人工智能必须避免设计偏见。欧盟监管机构还指出，对人工智能系统的访问必须是普遍的，没有歧视，无论残疾与否，并涉及人工智能系统生命周期的所有各方。
5.  **环境和社会影响:**可持续发展是这一关键要求的核心。随着深度学习模型所需的计算能力不断增加，对绿色人工智能的主要担忧在过去几年中有所增加。人工智能系统必须是“绿色”的，造福于所有人，包括后代，同时考虑到环境和其他生物和物种，并认真意识到它们的社会影响。
6.  **责任**:通过**人工智能审计**，重点关注**算法、数据和设计**，确保人工智能系统及其结果的责任和责任。
7.  透明性:可追溯性和可解释的人工智能有助于实现对人工智能系统决策的信任。必须清楚地解释和记录能力和限制。必须通知人类他们直接与人工智能系统连接，或者他们的数据正被人工智能系统使用。

![](img/f3f9262b4c7dcb6c12353ef3d833e94b.png)

在系统的整个生命周期中实施可信人工智能—来源:[可信人工智能道德准则，欧盟委员会](https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60419)

> “基于这项工作，我们现在可以向前推进，建立一个监管框架，让每个人都可以从值得信赖的人工智能中受益。”—内部市场专员 Thierry Breton

# 如何往前走，解决 AI 信任危机？

这 7 个关键原则可以作为审计人工智能系统价值链的切入点，从概念到部署和维护。

欧盟委员会发布的这份文件的主要附加价值在于，它提出了一系列实际问题，以落实我已经提到的关键原则。

这个值得信赖的人工智能评估列表，[阿尔泰](http://futurium.ec.europa.eu/en/content/altai-assessment-list-trustworthy-artificial-intelligence)，对于任何想要迈出第一步并向前迈进以解决我们今天面临的人工智能信任危机的公司或个人来说，都是一个伟大的举措。它将指导方针转换成一个可访问的和实用的清单，AI 的赞助者、开发者和部署者可以使用它来快速审计他们的 AI 系统。阿尔泰有一个[文档版本](https://ec.europa.eu/digital-single-market/news-redirect/682761)，还有一个基于网络的工具(原型版本)。

![](img/454eda607abfe268fb01c7f1b6924955.png)

迈向人工智能的可信赖性是下一个十年的挑战——Glenn Carstens-Peters 在 Unsplash 上的照片

建议采用技术和非技术方法，以确保在人工智能系统的设计、开发和运行阶段引入可信的人工智能。

任何公司的出发点都是定义其在使用人工智能方面的道德价值观和责任，并在行为准则中加以说明。这应该转化为治理原则，关键原则应该锚定在人工智能系统的架构设计中。一个关键点是通过人工智能风险管理工具和协议来识别、评估和管理风险。规则、过程和控制程序将建立可信的人工智能框架。随着时间的推移，对模型和 KPI 的监控有助于检测数据或性能漂移，并对潜在偏差发出警告。

生态系统也将发展，重点是人工智能系统的监管、标准化和认证，但也通过教育和意识培养道德思维，所有利益相关者参与和社会对话，多样性和包容性设计团队。

人工智能审计方法，将在未来几年蓬勃发展，当然，专门从事人工智能审计和认证的公司将出现在市场上。毫无疑问，值得信赖的人工智能有一个巨大的市场，各公司将蓬勃发展，将其作为一种差异化特征，从竞争对手中脱颖而出。

我真的希望这篇文章将提高人们对向前迈进的重要性和紧迫性的认识，并开始思考，设计和实施这些道德建议到我们日常的人工智能从业者的工作中。

你的，托马斯