<html>
<head>
<title>Batch Normalization In Neural Networks (Code Included)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的批量标准化(包括代码)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-code-d7c9b88da9f5?source=collection_archive---------26-----------------------#2020-04-24">https://towardsdatascience.com/batch-normalization-in-neural-networks-code-d7c9b88da9f5?source=collection_archive---------26-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b3ee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过 TensorFlow (Keras)实施</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c06e787dd13043efcbe34a7b9f6edf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipk6Xeowueq40fBcdf72vA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯托夫·高尔在<a class="ae ky" href="https://unsplash.com/s/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="fdd2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="aa13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">批量归一化(BN) </strong>是很多机器学习从业者遇到的一种技术。</p><p id="af06" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你还没有，这篇文章解释了 BN 背后的基本直觉，包括它的起源以及如何使用 TensorFlow 和 Keras 在神经网络中实现它。</p><p id="3186" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于那些熟悉 BN 技术并想专注于实现的人来说，你可以跳到下面的<strong class="lt iu">代码</strong>部分。</p><p id="c2d6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于那些可能对其背后的数学更感兴趣的人，请随意阅读下面的文章。</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/batch-normalization-explained-algorithm-breakdown-23d2794511c"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">解释了神经网络中的批量标准化(算法分解)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">理解深度神经网络中使用的一种常见转换技术</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="26bf" class="kz la it bd lb lc nr le lf lg ns li lj jz nt ka ll kc nu kd ln kf nv kg lp lq bi translated">定义</h1><p id="ab1c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">批次正常化是一种技术，它透过引入额外的层来减轻神经网路内不稳定梯度的影响，该层会对前一层的输入执行作业。这些操作对输入值进行标准化和规范化，然后通过缩放和移位操作转换输入值。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="1360" class="kz la it bd lb lc nr le lf lg ns li lj jz nt ka ll kc nu kd ln kf nv kg lp lq bi translated">密码</h1><p id="9f24" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一步是导入将用于实现或支持神经网络实现的工具和库。使用的工具如下:</p><ul class=""><li id="28be" class="nw nx it lt b lu mn lx mo ma ny me nz mi oa mm ob oc od oe bi translated"><a class="ae ky" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> TensorFlow </strong> </a>:一个开源平台，用于机器学习模型的实现、训练和部署。</li><li id="8f67" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated"><a class="ae ky" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Keras </strong> </a>:一个开源库，用于实现运行在 CPU 和 GPU 上的神经网络架构。</li></ul><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="6fad" class="op la it ol b gy oq or l os ot">import tensorflow as tf<br/>from tensorflow import keras</span></pre><p id="31e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将利用的数据集是普通的<a class="ae ky" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">时尚-MNIST 数据集</a>。</p><p id="e808" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">时尚-MNIST 数据集包含 70，000 幅服装图像。更具体地说，它包括 60，000 个训练样本和 10，000 个测试样本，这些样本都是尺寸为 28×28 的灰度图像，分为十类。</p><p id="5748" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据集的准备包括通过将每个像素值除以 255.0 来归一化训练图像和测试图像。这将像素值置于范围 0 和 1 之间。</p><p id="bfb1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据集的验证部分也在此阶段创建。这组数据集在训练期间被用来评估网络在各种迭代中的性能。</p><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="4fb2" class="op la it ol b gy oq or l os ot">(train_images, train_labels),  (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()<br/>train_images = train_images / 255.0<br/>test_images = test_images / 255.0<br/>validation_images = train_images[:5000]<br/>validation_labels = train_labels[:5000]</span></pre><p id="0bf7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Keras 提供了实现分类模型所需的工具。Keras 提出了一种顺序 API，用于以连续的方式堆叠神经网络的层。</p><p id="0386" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是一些层的信息，这些层将被实现以构成我们的神经网络。</p><ul class=""><li id="ba2e" class="nw nx it lt b lu mn lx mo ma ny me nz mi oa mm ob oc od oe bi translated"><strong class="lt iu">展平</strong>:取一个输入形状，将输入图像数据展平成一维数组。</li><li id="4917" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated"><strong class="lt iu">密集</strong>:密集层中嵌入了任意数量的单元/神经元。每个神经元都是一个感知器。</li><li id="6607" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated">感知器是人工神经网络的基本组成部分，由弗兰克·罗森布拉特于 1958 年发明。感知器利用基于阈值逻辑单元的操作。</li><li id="dc64" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated"><strong class="lt iu">批量规范化</strong>:批量规范化层通过对传入的输入数据执行一系列操作来工作。这组操作包括进入 BN 层的输入值的标准化、规范化、重缩放和偏移。</li><li id="94ed" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated"><strong class="lt iu">激活层</strong>:对神经网络内的输入执行指定的操作。这一层在网络中引入了非线性。本文中实现的模型将利用激活函数:<strong class="lt iu">整流线性单元(ReLU) </strong>和<strong class="lt iu"> softmax </strong>。</li><li id="6652" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated">由<strong class="lt iu"> ReLU </strong>对来自神经元的值施加的变换由公式 y=max(0，x)表示。ReLU 激活函数将来自神经元的任何负值钳制为 0，而正值保持不变。该数学变换的结果被用作当前层的激活，并作为下一层的输入。</li></ul><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="0627" class="op la it ol b gy oq or l os ot"># Placing batch normalization layer before the activation layers<br/>model = keras.models.Sequential([<br/>    keras.layers.Flatten(input_shape=[28,28]),<br/>    keras.layers.Dense(300, use_bias=False),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.Dense(200, use_bias=False),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.Dense(100, use_bias=False),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.Dense(10, activation=keras.activations.softmax)<br/>])</span></pre><p id="cfb2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们来看看 BN 层的内部组件</p><p id="2358" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">仅仅访问索引 2 处的层将向第一 BN 层内的变量及其内容提供信息，</p><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="7147" class="op la it ol b gy oq or l os ot">model.layers[2].variables</span></pre><p id="b1f0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我不会在这里进入太多的细节，但请注意变量名'伽马'和'贝塔'，这些变量中的值负责层内激活的重新缩放和偏移。</p><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="e1ab" class="op la it ol b gy oq or l os ot">for variable in model.layers[2].variables:<br/>    print(variable.name)</span><span id="cac4" class="op la it ol b gy ou or l os ot">&gt;&gt; batch_normalization/gamma:0<br/>&gt;&gt; batch_normalization/beta:0<br/>&gt;&gt; batch_normalization/moving_mean:0<br/>&gt;&gt; batch_normalization/moving_variance:0</span></pre><p id="a82d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这篇<a class="ae ky" rel="noopener" target="_blank" href="/batch-normalization-explained-algorithm-breakdown-23d2794511c">文章</a>详细介绍了 BN 层内的操作。</p><p id="11b1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在密集层中，偏置分量被设置为假。偏差的省略是由于在激活的标准化过程中由于均值相减而发生的常量值的抵消。</p><p id="635b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是特斯拉现任人工智能总监安德烈·卡帕西(Andrej Karpathy)的一篇 twitter 帖子的片段。他的推文是基于经常犯的神经网络错误的主题，而不是在使用 BN 时将 bias 设置为 false。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="93c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在下一段代码中，我们设置并指定了用于训练已实现的神经网络的优化算法，以及损失函数和超参数，如学习速率和时期数。</p><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="0566" class="op la it ol b gy oq or l os ot">sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer=sgd, metrics=["accuracy"])</span></pre><p id="51e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们使用模型的顺序 API 的'<em class="ox"> fit </em>方法来训练网络。我们将跳过关于如何训练神经网络模型的一些细节。有关神经网络的训练和实现的详细解释的进一步信息，请参考下面的链接。</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/in-depth-machine-learning-image-classification-with-tensorflow-2-0-a76526b32af8"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">使用 TensorFlow 2.0 进行(深入)机器学习图像分类</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">理解实现用于图像分类的神经网络的过程。</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="oy l ng nh ni ne nj ks mv"/></div></div></a></div><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="bd06" class="op la it ol b gy oq or l os ot">model.fit(train_images, train_labels, epochs=60, validation_data=(validation_images, validation_labels))</span></pre><p id="62fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用先前搁置的测试数据进行模型性能的评估。</p><p id="d0f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过评估结果，您可以在观察测试数据集评估的准确性后，决定微调网络超参数或继续生产。</p><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="d146" class="op la it ol b gy oq or l os ot">model.evaluate(test_images, test_labels)</span></pre><p id="69ca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在训练阶段，您可能会注意到，与没有批量归一化图层的训练网络相比，每个历元的训练时间更长。这是因为批量标准化增加了神经网络的复杂性，以及模型在训练期间学习所需的额外参数。</p><p id="a202" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管每个历元时间的增加与批量标准化减少了模型收敛到最优解所需的时间这一事实相平衡。</p><p id="82d9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">本文中实现的模型太浅，我们无法注意到在神经网络架构中使用批处理规范化的全部好处。通常情况下，批量归一化出现在更深层次的卷积神经网络中，如<a class="ae ky" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">异常</a>、<a class="ae ky" href="https://www.mathworks.com/help/deeplearning/ref/resnet50.html" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>和<a class="ae ky" href="https://uk.mathworks.com/help/deeplearning/ref/inceptionv3.html" rel="noopener ugc nofollow" target="_blank"> Inception V3 </a>。</p><h1 id="339f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外的</h1><ul class=""><li id="5e18" class="nw nx it lt b lu lv lx ly ma oz me pa mi pb mm ob oc od oe bi translated">上面实现的神经网络在激活层之前具有批量标准化层。但是在激活层之后添加 BN 层是完全可能的。</li></ul><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="5908" class="op la it ol b gy oq or l os ot"># Placing batch normalization layer after the activation layers<br/>model = keras.models.Sequential([<br/>    keras.layers.Flatten(input_shape=[28,28]),<br/>    keras.layers.Dense(300, use_bias=False),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dense(200, use_bias=False),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dense(100, use_bias=False),<br/>    keras.layers.Activation(keras.activations.relu),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dense(10, activation=keras.activations.softmax)<br/>])</span></pre><ul class=""><li id="ff38" class="nw nx it lt b lu mn lx mo ma ny me nz mi oa mm ob oc od oe bi translated">研究人员已经对批量标准化技术做了一些广泛的工作。例如<a class="ae ky" href="https://arxiv.org/pdf/1702.03275.pdf" rel="noopener ugc nofollow" target="_blank">批量重正化</a>和<a class="ae ky" href="https://arxiv.org/pdf/1706.02515.pdf" rel="noopener ugc nofollow" target="_blank">自归一化神经网络</a></li></ul></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="a336" class="kz la it bd lb lc nr le lf lg ns li lj jz nt ka ll kc nu kd ln kf nv kg lp lq bi translated">结论</h1><p id="5a28" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">BN 是神经网络中常用的技术，因此理解该技术如何工作以及如何实现将是有用的知识，尤其是在分析大多数神经网络架构时。</p><p id="bfb5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是 GitHub 到一个笔记本的链接，其中包含了本文中的代码片段。</p><div class="ms mt gp gr mu mv"><a href="https://github.com/RichmondAlake/tensorflow_2_tutorials/blob/master/04_batch_normalisation.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">Richmond alake/tensor flow _ 2 _ 教程</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">permalink dissolve GitHub 是 4000 多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">github.com</p></div></div><div class="ne l"><div class="pc l ng nh ni ne nj ks mv"/></div></div></a></div></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><div class="kj kk kl km gt mv"><a rel="noopener follow" target="_blank" href="/should-you-take-a-masters-msc-in-machine-learning-c01336120466"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">你应该读机器学习硕士吗？</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">包含来自理学硕士毕业生的想法和意见</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="pd l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/how-does-ai-detect-objects-technical-d8d63fc12881"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">AI 如何检测物体？(技术)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">了解如何使用机器和深度学习技术应用和实现对象检测</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="pe l ng nh ni ne nj ks mv"/></div></div></a></div></div></div>    
</body>
</html>