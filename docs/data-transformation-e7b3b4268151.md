# æ•°æ®è½¬æ¢

> åŸæ–‡ï¼š<https://towardsdatascience.com/data-transformation-e7b3b4268151?source=collection_archive---------31----------------------->

## ğŸ“ˆPython for finance ç³»åˆ—

## å¦‚ä½•å°†ç°ä»£æœºå™¨å­¦ä¹ åº”ç”¨äºä½“ç§¯æ‰©æ•£åˆ†æ(VSA)

![](img/c276200c7e5ec74675a94025295bc861.png)

[æ°ç‘ç±³Â·æ‰˜é©¬æ–¯](https://unsplash.com/@jeremythomasphoto?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/s/photos/leaves-wallpaper?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹ç…§

**è­¦å‘Š**:è¿™é‡Œæ²¡æœ‰ç¥å¥‡çš„å…¬å¼*æˆ–åœ£æ¯ï¼Œå°½ç®¡ä¸€ä¸ªæ–°çš„ä¸–ç•Œå¯èƒ½ä¼šä¸ºä½ æ‰“å¼€å¤§é—¨ã€‚*

## ğŸ“ˆPython For Finance ç³»åˆ—

1.  [è¯†åˆ«å¼‚å¸¸å€¼](https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa)
2.  [è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬äºŒéƒ¨åˆ†](https://medium.com/better-programming/identifying-outliers-part-two-4c00b2523362)
3.  [è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)
4.  [ç¨‹å¼åŒ–çš„äº‹å®](/data-whispering-eebb77a422da)
5.  [ç‰¹å¾å·¥ç¨‹&ç‰¹å¾é€‰æ‹©](https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2)
6.  [æ•°æ®è½¬æ¢](/data-transformation-e7b3b4268151)
7.  [ç»†å¾®å·®åˆ«ç‰¹å¾](https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55)
8.  [æ•°æ®æ ‡ç­¾](/the-triple-barrier-method-251268419dcd)
9.  [å…ƒæ ‡ç­¾å’Œå †å ](/meta-labeling-and-stacking-f17a7f9804ec)

åœ¨é¢„è§ˆæ–‡ç« ä¸­ï¼Œæˆ‘ç®€è¦ä»‹ç»äº†ä½“ç§¯æ‰©æ•£åˆ†æ(VSA)ã€‚åœ¨æˆ‘ä»¬å®Œæˆç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©åï¼Œæˆ‘ç«‹å³æ³¨æ„åˆ°ä¸¤ä»¶äº‹ï¼Œç¬¬ä¸€ä»¶æ˜¯æ•°æ®é›†ä¸­æœ‰å¼‚å¸¸å€¼ï¼Œç¬¬äºŒä»¶æ˜¯åˆ†å¸ƒä¸æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚é€šè¿‡ä½¿ç”¨è¿™é‡Œæè¿°çš„ã€è¿™é‡Œæè¿°çš„å’Œè¿™é‡Œæè¿°çš„çš„æ–¹æ³•ï¼Œæˆ‘å»é™¤äº†å¤§éƒ¨åˆ†çš„å¼‚å¸¸å€¼ã€‚ç°åœ¨æ˜¯æ—¶å€™é¢å¯¹æ›´å¤§çš„é—®é¢˜äº†ï¼Œå¸¸æ€ã€‚

æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ä¼ è¾“æ•°æ®ã€‚ä¼—æ‰€å‘¨çŸ¥çš„ä¸€ä¸ªä¾‹å­æ˜¯[ä¸€é”®ç¼–ç ](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)ï¼Œæ›´å¥½çš„ä¾‹å­æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­çš„[å•è¯åµŒå…¥](https://en.wikipedia.org/wiki/Word_embedding)ã€‚è€ƒè™‘åˆ°ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œå®ƒå®Œå…¨è‡ªåŠ¨åŒ–äº†è¿‡å»æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­æœ€å…³é”®çš„æ­¥éª¤:ç‰¹å¾å·¥ç¨‹ã€‚åœ¨åé¢çš„æ–‡ç« è¿›å…¥æ·±åº¦å­¦ä¹ ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ä¸€äº›ç®€å•çš„è½¬ç§»æ•°æ®çš„æ–¹æ³•ï¼Œçœ‹çœ‹èƒ½å¦è®©å®ƒæ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³å°è¯•å‡ ä»¶äº‹ã€‚ç¬¬ä¸€ç§æ˜¯å°†æ‰€æœ‰ç‰¹å¾è½¬æ¢æˆç®€å•çš„ç™¾åˆ†æ¯”å˜åŒ–ã€‚ç¬¬äºŒä¸ªæ˜¯åšç™¾åˆ†ä½æ•°æ’åã€‚æœ€åï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºï¼Œå¦‚æœæˆ‘åªé€‰æ‹©æ‰€æœ‰æ•°æ®çš„ç¬¦å·ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚åƒ Z-score è¿™æ ·çš„æ–¹æ³•ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„æ ‡å‡†é¢„å¤„ç†ï¼Œæˆ‘å®æ„¿æš‚æ—¶ä¸åšã€‚

# 1.æ•°æ®å‡†å¤‡

ä¸ºäº†ä¸€è‡´æ€§ï¼Œåœ¨æ‰€æœ‰çš„[ğŸ“ˆPython for finance ç³»åˆ—](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)ï¼Œæˆ‘ä¼šå°½é‡é‡ç”¨ç›¸åŒçš„æ•°æ®ã€‚å…³äºæ•°æ®å‡†å¤‡çš„æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨[è¿™é‡Œ](https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa)ï¼Œè¿™é‡Œ[è¿™é‡Œ](https://medium.com/@kegui/identifying-outliers-part-two-4c00b2523362)å’Œ[è¿™é‡Œ](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)æ‰¾åˆ°ï¼Œæˆ–è€…ä½ å¯ä»¥å‚è€ƒæˆ‘ä¹‹å‰çš„[æ–‡ç« ](https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2)ã€‚æˆ–è€…ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥å¿½ç•¥ä¸‹é¢çš„æ‰€æœ‰ä»£ç ï¼Œä½¿ç”¨ä½ æ‰‹å¤´ä¸Šä»»ä½•å¹²å‡€çš„æ•°æ®ï¼Œè¿™ä¸ä¼šå½±å“æˆ‘ä»¬å°†è¦ä¸€èµ·åšçš„äº‹æƒ…ã€‚

```
#import all the libraries
import pandas as pd
import numpy as np
import seaborn as sns 
import yfinance as yf  #the stock data from Yahoo Finance
import matplotlib.pyplot as plt #set the parameters for plotting
plt.style.use('seaborn')
plt.rcParams['figure.dpi'] = 300#define a function to get data
def get_data(symbols, begin_date=None,end_date=None):
    df = yf.download('AAPL', start = '2000-01-01',
                     auto_adjust=True,#only download adjusted data
                     end= '2010-12-31') 
    #my convention: always lowercase
    df.columns = ['open','high','low',
                  'close','volume'] 

    return dfprices = get_data('AAPL', '2000-01-01', '2010-12-31')#create some features
def create_HLCV(i):
#as we don't care open that much, that leaves volume, 
#high,low and close 
    df = pd.DataFrame(index=prices.index)
    df[f'high_{i}D'] = prices.high.rolling(i).max()
    df[f'low_{i}D'] = prices.low.rolling(i).min()
    df[f'close_{i}D'] = prices.close.rolling(i).\
                        apply(lambda x:x[-1]) 
    # close_2D = close as rolling backwards means today is 
    # literly the last day of the rolling window.
    df[f'volume_{i}D'] = prices.volume.rolling(i).sum()

    return df# create features at different rolling windows
def create_features_and_outcomes(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    outcomes = pd.DataFrame(index=prices.index)

    #as we already considered the different time span, 
    #here only day of simple percentage change used.

    features[f'volume_{i}D'] = volume.pct_change()
    features[f'price_spread_{i}D'] = (high - low).pct_change()
    #aligne the close location with the stock price change
    features[f'close_loc_{i}D'] = ((close - low) / \
                             (high -   low)).pct_change() #the future outcome is what we are going to predict
    outcomes[f'close_change_{i}D'] = close.pct_change(-i)

    return features, outcomesdef create_bunch_of_features_and_outcomes():
    '''
    the timespan that i would like to explore 
    are 1, 2, 3 days and 1 week, 1 month, 2 month, 3 month
    which roughly are [1,2,3,5,20,40,60]
    '''
    days = [1,2,3,5,20,40,60]
    bunch_of_features = pd.DataFrame(index=prices.index)
    bunch_of_outcomes = pd.DataFrame(index=prices.index)

    for day in days:
        f,o = create_features_and_outcomes(day)
        bunch_of_features = bunch_of_features.join(f)
        bunch_of_outcomes = bunch_of_outcomes .join(o)

    return bunch_of_features, bunch_of_outcomesbunch_of_features, bunch_of_outcomes = create_bunch_of_features_and_outcomes()#define the method to identify outliers
def get_outliers(df, i=4): 
    #i is number of sigma, which define the boundary along mean
    outliers = pd.DataFrame()
    stats = df.describe()

    for col in df.columns:
        mu = stats.loc['mean', col]
        sigma = stats.loc['std', col]
        condition = (df[col] > mu + sigma * i) | \
                  (df[col] < mu -   sigma * i) 
        outliers[f'{col}_outliers'] = df[col][condition]

    return outliers#remove all the outliers
features_outcomes = bunch_of_features.join(bunch_of_outcomes)
outliers = get_outliers(features_outcomes, i=1)features_outcomes_rmv_outliers = features_outcomes.drop(index = outliers.index).dropna()features = features_outcomes_rmv_outliers[bunch_of_features.columns]
outcomes = features_outcomes_rmv_outliers[bunch_of_outcomes.columns]
features.info(), outcomes.info()
```

![](img/27903ffff3bb6e8342604005c12809f2.png)

è¦ç´ æ•°æ®é›†çš„ä¿¡æ¯

![](img/b536e721faabf7e33b8e97f5ab6ff72b.png)

ç»“æœæ•°æ®é›†ä¿¡æ¯

æœ€åï¼Œæˆ‘ä»¬å°†å…·æœ‰åŸºäºä¸åŒæ—¶é—´å°ºåº¦çš„é‡å·®åˆ†æ(VSA)çš„åŸºæœ¬å››ä¸ªç‰¹å¾ï¼Œå¦‚ä¸‹æ‰€åˆ—ï¼Œå³ 1 å¤©ã€2 å¤©ã€3 å¤©ã€ä¸€å‘¨ã€ä¸€ä¸ªæœˆã€2 ä¸ªæœˆå’Œ 3 ä¸ªæœˆã€‚

*   éŸ³é‡:éå¸¸ç›´æ¥
*   èŒƒå›´/ä»·å·®:æœ€é«˜ä»·å’Œæ”¶ç›˜ä»·ä¹‹é—´çš„å·®å¼‚
*   æ”¶ç›˜ä»·ç›¸å¯¹äºåŒºé—´:æ”¶ç›˜ä»·æ˜¯æ¥è¿‘ä»·æ ¼æŸ±çš„é¡¶éƒ¨è¿˜æ˜¯åº•éƒ¨ï¼Ÿ
*   è‚¡ç¥¨ä»·æ ¼çš„å˜åŒ–:éå¸¸ç›´æ¥

# 2.ç™¾åˆ†æ¯”å›æŠ¥

æˆ‘çŸ¥é“ä¸Šé¢æœ‰å¾ˆå¤šä»£ç ã€‚æˆ‘ä»¬é€šè¿‡ä¸‹é¢çš„å‡½æ•°å°†æ‰€æœ‰ç‰¹å¾è½¬æ¢æˆç®€å•çš„ç™¾åˆ†æ¯”å˜åŒ–ã€‚

```
def create_features_and_outcomes(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    outcomes = pd.DataFrame(index=prices.index)

    #as we already considered the different time span, 
    #here only 1 day of simple percentage change used.

    features[f'volume_{i}D'] = volume.pct_change()
    features[f'price_spread_{i}D'] = (high - low).pct_change()
    #aligne the close location with the stock price change
    features[f'close_loc_{i}D'] = ((close - low) / \
    (high -    low)).pct_change()#the future outcome is what we are going to predict
    outcomes[f'close_change_{i}D'] = close.pct_change(-i)

    return features, outcomes
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨èšç±»å›¾æ¥çœ‹çœ‹å®ƒä»¬çš„ç›¸å…³æ€§ã€‚Seaborn çš„ `clustermap()`å±‚æ¬¡èšç±»ç®—æ³•å±•ç¤ºäº†ä¸€ç§å°†æœ€å¯†åˆ‡ç›¸å…³çš„ç‰¹å¾åˆ†ç»„çš„å¥½æ–¹æ³•ã€‚

```
corr_features = features.corr().sort_index()
sns.clustermap(corr_features, cmap='coolwarm', linewidth=1);
```

![](img/f34519f18e1e8cff5a1ef6f9afef6822.png)

åŸºäºæ­¤èšç±»å›¾ï¼Œä¸ºäº†æœ€å¤§é™åº¦åœ°å‡å°‘æ‰€é€‰è¦ç´ ä¸­çš„è¦ç´ é‡å é‡ï¼Œæˆ‘å°†ç§»é™¤é‚£äº›ä¸å…¶ä»–è¦ç´ ç´§å¯†é…å¯¹ä¸”ä¸ç»“æœç›®æ ‡ç›¸å…³æ€§è¾ƒå°çš„è¦ç´ ã€‚ä»ä¸Šé¢çš„èšç±»å›¾ä¸­ï¼Œå¾ˆå®¹æ˜“å‘ç°[40Dï¼Œ60D]å’Œ[2Dï¼Œ3D]ä¸Šçš„ç‰¹å¾æ˜¯æˆå¯¹å‡ºç°çš„ã€‚ä¸ºäº†äº†è§£è¿™äº›ç‰¹å¾ä¸ç»“æœä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ç»“æœä¹‹é—´çš„å…³ç³»ã€‚

```
corr_outcomes = outcomes.corr()
sns.clustermap(corr_outcomes, cmap='coolwarm', linewidth=2);
```

![](img/281625fcbd78c14547538a63f822b8c2.png)

ä»ä¸Šåˆ°ä¸‹ï¼Œ20 å¤©ã€40 å¤©å’Œ 60 å¤©çš„ä»·æ ¼ç™¾åˆ†æ¯”å˜åŒ–è¢«åˆ†ç»„åœ¨ä¸€èµ·ï¼Œ2 å¤©ã€3 å¤©å’Œ 5 å¤©ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œä¸€å¤©çš„è‚¡ä»·ç™¾åˆ†æ¯”å˜åŒ–ç›¸å¯¹ç‹¬ç«‹äºè¿™ä¸¤ç»„ã€‚å¦‚æœæˆ‘ä»¬é€‰æ‹©ç¬¬äºŒå¤©çš„ä»·æ ¼ç™¾åˆ†æ¯”å˜åŒ–ä½œä¸ºç»“æœç›®æ ‡ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›ç‰¹æ€§æ˜¯å¦‚ä½•ä¸ä¹‹ç›¸å…³çš„ã€‚

```
corr_features_outcomes = features.corrwith(outcomes. \
                                close_change_1D).sort_values()
corr_features_outcomes.dropna(inplace=True)
corr_features_outcomes.plot(kind='barh',title = 'Strength of Correlation');
```

![](img/328be316ac3efffc4f3ce4af96563987.png)

ç›¸å…³ç³»æ•°å¤ªå°ï¼Œæ— æ³•å¾—å‡ºå¯é çš„ç»“è®ºã€‚æˆ‘ä¼šæœŸæœ›æœ€è¿‘çš„æ•°æ®å…·æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ï¼Œä½†è¿™é‡Œçš„æƒ…å†µå¹¶éå¦‚æ­¤ã€‚

åŒäººæƒ…èŠ‚æ€ä¹ˆæ ·ï¼Ÿæˆ‘ä»¬åªé€‰æ‹©é‚£äº›åŸºäº 1 å¤©æ—¶é—´å°ºåº¦çš„ç‰¹å¾ä½œä¸ºæ¼”ç¤ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘å°†`close_change_1D`è½¬æ¢ä¸ºåŸºäºå®ƒæ˜¯è´Ÿæ•°è¿˜æ˜¯æ­£æ•°çš„ç¬¦å·ï¼Œä»¥å¢åŠ ç»˜å›¾çš„é¢å¤–ç»´åº¦ã€‚

```
selected_features_1D_list = ['volume_1D', 'price_spread_1D', 'close_loc_1D', 'close_change_1D']
features_outcomes_rmv_outliers['sign_of_close'] = features_outcomes_rmv_outliers['close_change_1D']. \
                                                  apply(np.sign)sns.pairplot(features_outcomes_rmv_outliers, 
             vars=selected_features_1D_list,
             diag_kind='kde',
             palette='husl', hue='sign_of_close',
             markers = ['*', '<', '+'], 
             plot_kws={'alpha':0.3});
```

![](img/e2481d015ee12c67417815251e794219.png)

é…å¯¹å›¾å»ºç«‹åœ¨ä¸¤ä¸ªåŸºæœ¬å›¾å½¢ä¸Šï¼Œç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾ã€‚å¯¹è§’çº¿ä¸Šçš„ç›´æ–¹å›¾å…è®¸æˆ‘ä»¬çœ‹åˆ°å•ä¸ªå˜é‡çš„åˆ†å¸ƒï¼Œè€Œä¸Šä¸‹ä¸‰è§’å½¢ä¸Šçš„æ•£ç‚¹å›¾æ˜¾ç¤ºäº†ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»(æˆ–ç¼ºä¹å…³ç³»)ã€‚ä»ä¸Šé¢çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€äº¤æ˜“é‡çš„å¢åŠ ï¼Œä»·å·®è¶Šæ¥è¶Šå¤§ã€‚å¤§éƒ¨åˆ†ä»·æ ¼å˜åŠ¨ä½äºç‹­çª„çš„ä»·å·®ï¼Œæ¢å¥è¯è¯´ï¼Œæ›´å¤§çš„ä»·å·®å¹¶ä¸æ€»æ˜¯ä¼´éšç€æ›´å¤§çš„ä»·æ ¼æ³¢åŠ¨ã€‚æ— è®ºæ˜¯ä½äº¤æ˜“é‡è¿˜æ˜¯é«˜äº¤æ˜“é‡éƒ½ä¼šå¯¼è‡´å‡ ä¹æ‰€æœ‰è§„æ¨¡çš„ä»·æ ¼å˜åŠ¨ã€‚æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰è¿™äº›ç»“è®ºåº”ç”¨åˆ°ä¸Šæ¶¨å’Œä¸‹è·Œçš„æ—¥å­é‡Œã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ¥è¿‘çš„é…’å§ä½ç½®ï¼Œä»¥å¢åŠ æ›´å¤šçš„ç»´åº¦ï¼Œç®€å•åœ°åº”ç”¨

```
features[â€˜sign_of_close_locâ€™] = np.where( \
 features[â€˜close_loc_1Dâ€™] > 0.5, \
 1, -1)
```

çœ‹æœ‰å¤šå°‘æ£’çº¿çš„æ”¶ç›˜ä»·é«˜äº 0.5 æˆ–ä½äº 0.5ã€‚

åœ¨ pair å›¾ä¸­ï¼Œæˆ‘ä¸å–œæ¬¢çš„ä¸€ç‚¹æ˜¯æ‰€æœ‰çš„å›¾éƒ½æµ“ç¼©äº†`close_loc_1D`,çœ‹èµ·æ¥åƒç¦»ç¾¤å€¼ä»ç„¶å­˜åœ¨ï¼Œå³ä½¿æˆ‘çŸ¥é“æˆ‘ä½¿ç”¨äº†ä¸€ä¸ªæ ‡å‡†åå·®ä½œä¸ºè¾¹ç•Œï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ä½çš„é˜ˆå€¼ï¼Œ338 ä¸ªç¦»ç¾¤å€¼è¢«åˆ é™¤ã€‚æˆ‘æ„è¯†åˆ°å› ä¸º close çš„ä½ç½®å·²ç»æ˜¯ç™¾åˆ†æ¯”å˜åŒ–äº†ï¼Œåœ¨ä¸Šé¢å†åŠ ä¸€ä¸ªç™¾åˆ†æ¯”å˜åŒ–æ²¡æœ‰å¤ªå¤§æ„ä¹‰ã€‚è®©æˆ‘ä»¬æ”¹å˜å®ƒã€‚

```
def create_features_and_outcomes(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    outcomes = pd.DataFrame(index=prices.index)

    #as we already considered the different time span, 
    #simple percentage change of 1 day used here.

    features[f'volume_{i}D'] = volume.pct_change()
    features[f'price_spread_{i}D'] = (high - low).pct_change()
    #remove pct_change() here
    features[f'close_loc_{i}D'] = ((close - low) / (high - low))
    #predict the future with -i
    outcomes[f'close_change_{i}D'] = close.pct_change(-i)

    return features, outcomes
```

å»æ‰äº†`pct_change()`ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç°åœ¨çš„æ˜Ÿå›¢å›¾æ˜¯ä»€ä¹ˆæ ·å­ã€‚

```
corr_features = features.corr().sort_index()
sns.clustermap(corr_features, cmap='coolwarm', linewidth=1);
```

![](img/3e7b636c0cb18988e6ffaf0ee233f843.png)

èšç±»å›¾ç°åœ¨æ›´æœ‰æ„ä¹‰äº†ã€‚æ‰€æœ‰å››ä¸ªåŸºæœ¬ç‰¹å¾éƒ½æœ‰éå¸¸ç›¸ä¼¼çš„æ¨¡å¼ã€‚[40Dï¼Œ60D]ï¼Œ[2Dï¼Œ3D]é…å¯¹åœ¨ä¸€èµ·ã€‚

ä»¥åŠä¸ç»“æœç›¸å…³çš„ç‰¹å¾ã€‚

```
corr_features_outcomes.plot(kind='barh',title = 'Strength of Correlation');
```

![](img/7126498c87557aceed901ca593b525f1.png)

é•¿æœŸæ—¶é—´å°ºåº¦ç‰¹å¾ä¸è‚¡ç¥¨ä»·æ ¼å›æŠ¥çš„ç›¸å…³æ€§è¾ƒå¼±ï¼Œè€Œè¿‘æœŸäº‹ä»¶å¯¹ä»·æ ¼å›æŠ¥çš„å½±å“æ›´å¤§ã€‚

é€šè¿‡å»æ‰`close_loc_1D`çš„`pct_change()`ï¼Œæœ€å¤§çš„åŒºåˆ«åœ¨äº`pairplot()`ã€‚

![](img/13d8baf95038efeb5c12329c5521feee.png)

æœ€åï¼Œ`close_loc_1D`å˜é‡åœ¨æ­£ç¡®çš„èŒƒå›´å†…ç»˜å›¾ã€‚è¿™è¯´æ˜æˆ‘ä»¬åº”è¯¥å°å¿ƒè¿‡åº¦è®¾è®¡ã€‚å®ƒå¯èƒ½ä¼šå¯¼è‡´ä¸€ä¸ªå®Œå…¨æ„æƒ³ä¸åˆ°çš„æ–¹å¼ã€‚

# 3.ç™¾åˆ†ä½æ•°æ’å

æ ¹æ®ç»´åŸºç™¾ç§‘ï¼Œç™¾åˆ†ä½æ’åæ˜¯

> åˆ†æ•°çš„ç™¾åˆ†ä½æ•°ç­‰çº§æ˜¯åˆ†æ•°åœ¨å…¶é¢‘ç‡åˆ†å¸ƒä¸­ç­‰äºæˆ–ä½äºå®ƒçš„ç™¾åˆ†æ¯”ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæµ‹è¯•åˆ†æ•°å¤§äºå‚åŠ æµ‹è¯•çš„äººçš„åˆ†æ•°çš„ 75%è¢«è®¤ä¸ºæ˜¯åœ¨ç¬¬**75**ç™¾åˆ†ä½ï¼Œå…¶ä¸­ 75 æ˜¯ç™¾åˆ†ä½ç­‰çº§ã€‚

ä»¥ä¸‹ç¤ºä¾‹è¿”å›ä¸è¿‡å» 60 å¤©æœŸé—´ç›¸æ¯”ï¼Œæ¯ä¸ªå€¼çš„äº¤æ˜“é‡çš„ç™¾åˆ†æ¯”ç­‰çº§(ä» 0.00 åˆ° 1.00)ã€‚

```
roll_rank = lambda x: pd.Series(x).rank(pct=True)[-1]
# you only pick the first value [0]
# of the 60 windows rank if you rolling forward.
# if you rolling backward, we should pick last one,[-1].features_rank = features.rolling(60, min_periods=60). \
                apply(roll_rank).dropna()
outcomes_rank = outcomes.rolling(60, min_periods=60). \
                apply(roll_rank).dropna()
```

## âœTipï¼

ç†ŠçŒ«`rolling()`ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œç»“æœè®¾ç½®ä¸ºçª—å£çš„å³è¾¹ç¼˜ã€‚è¿™æ„å‘³ç€è¯¥çª—å£æ˜¯å‘åçœ‹çš„çª—å£ï¼Œä»è¿‡å»æ»šåŠ¨åˆ°å½“å‰æ—¶é—´æˆ³ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¯¹äºé‚£ä¸ªçª—å£å¸§ä¸­çš„`rank()`ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€åä¸€ä¸ªå€¼`[-1]`ã€‚

æ›´å¤šå…³äº`rolling()`çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£ã€‚](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html)

é¦–å…ˆï¼Œæˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹ç»“æœçš„èšç±»å›¾ã€‚å®ƒå‡ ä¹ç­‰åŒäºé¡ºåºä¸åŒçš„ç™¾åˆ†æ¯”å˜åŒ–ã€‚

```
corr_outcomes_rank = outcomes_rank.corr().sort_index()
sns.clustermap(corr_outcomes_rank, cmap='coolwarm', linewidth=2);
```

![](img/48d35e7f8a8dac184f9b1000c6cc7ae0.png)

åŒæ ·çš„æ¨¡å¼ä¹Ÿé€‚ç”¨äºè¦ç´ çš„èšç±»å›¾ã€‚

```
corr_features_rank = features_rank.corr().sort_index()
sns.clustermap(corr_features_rank, cmap='coolwarm', linewidth=2);
```

![](img/3ad5b56fb81cdfa58b135dcf596db657.png)

å³ä½¿ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œ

```
# using 'ward' method
corr_features_rank = features_rank.corr().sort_index()
sns.clustermap(corr_features_rank, cmap='coolwarm', linewidth=2, method='ward');
```

![](img/82f913b587971c8af8ebe1204139725a.png)

å½“ç„¶ï¼Œç‰¹å¾å’Œç»“æœçš„ç›¸å…³æ€§ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚

```
corr_features_outcomes_rank = features_rank.corrwith( \
                              outcomes_rank. \
                              close_change_1D).sort_values()corr_features_outcomes_rank
```

![](img/1d4862580c67c53c56647633c443f4c1.png)

```
corr_features_outcomes_rank.plot(kind='barh',title = 'Strength of Correlation');
```

![](img/c3e589099bd1637641c80391d2b83d84.png)

æœ€åï¼Œä½ å¯èƒ½ä¼šçŒœåˆ°ç»“å¯¹å›¾ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚

```
selected_features_1D_list = ['volume_1D', 'price_spread_1D', 'close_loc_1D', 'close_change_1D']
features_outcomes_rank['sign_of_close'] = features_outcomes_rmv_outliers['close_change_1D']. \
                                                  apply(np.sign)sns.pairplot(features_outcomes_rank, 
             vars=selected_features_1D_list,
             diag_kind='kde',
             palette='husl', hue='sign_of_close',
             markers = ['*', '<', '+'], 
             plot_kws={'alpha':0.3});
```

![](img/a995ceb902f83d46eb0358f047fbad0b.png)

å› ä¸ºæˆ‘ä»¬åœ¨é›†åˆçª—å£ä¸­ä½¿ç”¨äº†ç™¾åˆ†ä½ç­‰çº§(ä» 0.00 åˆ° 1.00)ï¼Œæ‰€ä»¥æ–‘ç‚¹å‡åŒ€åœ°åˆ†å¸ƒåœ¨æ‰€æœ‰ç‰¹å¾ä¸Šã€‚ä¸æœªç»å˜æ¢çš„ç›¸åŒæ•°æ®ç›¸æ¯”ï¼Œæ‰€æœ‰ç‰¹å¾çš„åˆ†å¸ƒæˆ–å¤šæˆ–å°‘æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚

# 4.ç­¾ç½²

æœ€ååŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘æƒ³åˆ é™¤æ‰€æœ‰çš„æ•°æ®é¢—ç²’ï¼Œçœ‹çœ‹è¿™äº›åŠŸèƒ½åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å¦‚ä½•å…³è”çš„ã€‚

```
features_sign = features.apply(np.sign)
outcomes_sign = outcomes.apply(np.sign)
```

ç„¶åå†æ¬¡è®¡ç®—ç›¸å…³ç³»æ•°ã€‚

```
corr_features_outcomes_sign = features_sign.corrwith(
                              outcomes_sign. \
                              close_change_1D).sort_values(ascending=False)corr_features_outcomes_sign
```

![](img/92ea7f6b46e407d42a27aedb0bd6b028.png)

```
corr_features_outcomes_sign.plot(kind='barh',title = 'Strength of Correlation');
```

![](img/a9295335c2f39ca727d2573caa9fcdca.png)

ç°åœ¨çœ‹æ¥æœ‰ç‚¹æ€ªï¼Œåƒ`volume_1D`å’Œ`price_spread_1D`å’Œç°åœ¨çš„èƒœè´Ÿç›¸å…³æ€§å¾ˆå¼±ã€‚

å¹¸è¿çš„æ˜¯ï¼Œèšç±»å›¾åŸºæœ¬ä¿æŒä¸å˜ã€‚

```
corr_features_sign = features_sign.corr().sort_index()
sns.clustermap(corr_features_sign, cmap='coolwarm', linewidth=2);
```

![](img/f68380e79984c1181eadb7388a09b527.png)

ç»“æœä¹‹é—´çš„å…³ç³»ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

```
corr_outcomes_sign = outcomes_sign.corr().sort_index()
sns.clustermap(corr_outcomes_sign, cmap='coolwarm', linewidth=2);
```

![](img/a08e554127ca43ee367433c862437faf.png)

è‡³äº pair plotï¼Œç”±äºæ‰€æœ‰æ•°æ®éƒ½è¢«è½¬ç§»åˆ°-1 æˆ– 1ï¼Œå®ƒæ²¡æœ‰æ˜¾ç¤ºä»»ä½•æœ‰æ„ä¹‰çš„ä¸œè¥¿ã€‚

![](img/f23616dba66588f12eb1b07be342ceca.png)

æœ‰æ—¶å¯¹æ•°æ®è¿›è¡Œâ€œæ ‡å‡†åŒ–â€æˆ–â€œè§„èŒƒåŒ–â€æ˜¯è‡³å…³é‡è¦çš„ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½åœ¨ä¸åŒå°ºåº¦çš„ç‰¹å¾ä¹‹é—´è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚æˆ‘å¾ˆæƒ³ç”¨ Z-score æ¥æ ‡å‡†åŒ–æ•°æ®é›†ã€‚

![](img/ec8907ad6a787573e799e4772c1f078a.png)

Z å¾—åˆ†çš„å…¬å¼éœ€è¦å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼Œé€šè¿‡è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„è¿™ä¸¤ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬æœ‰æœºä¼šçœ‹åˆ°æœªæ¥ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡åˆ©ç”¨æ»šåŠ¨çª—å£ã€‚ä½†ä¸€èˆ¬æ¥è¯´ï¼Œäººä»¬ä¼šåœ¨å°†æ•°æ®æ³¨å…¥æ¨¡å‹ä¹‹å‰å¯¹å…¶è¿›è¡Œæ ‡å‡†åŒ–ã€‚

æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ 3 ç§ä¸åŒçš„æ•°æ®è½¬æ¢æ–¹æ³•ï¼Œç°åœ¨æˆ‘ä»¬å¾ˆæœ‰ä¿¡å¿ƒå¯ä»¥é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹æ€§å¹¶ä¸¢å¼ƒé‚£äº›å¤§é‡çš„ç‰¹æ€§ï¼Œå› ä¸ºæ‰€æœ‰ 3 ç§æ–¹æ³•å‡ ä¹å…±äº«ç›¸åŒçš„æ¨¡å¼ã€‚

# 5.å¹³ç¨³å’Œæ­£æ€æ€§æ£€éªŒ

æœ€åä¸€ä¸ªé—®é¢˜è½¬æ¢åçš„æ•°æ®èƒ½é€šè¿‡å¹³ç¨³æ€§/æ­£æ€æ€§æ£€éªŒå—ï¼Ÿè¿™é‡Œï¼Œæˆ‘å°†ä½¿ç”¨[æ‰©å±•çš„ Dickey-Fuller æ£€éªŒ](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)ï¼Œè¿™æ˜¯ä¸€ç§å«åš[å•ä½æ ¹æ£€éªŒ](https://en.wikipedia.org/wiki/Unit_root_test)çš„ç»Ÿè®¡æ£€éªŒã€‚åŒæ—¶ï¼Œæˆ‘è¿˜æƒ³çœ‹çœ‹åæ–œåº¦å’Œå³°åº¦ã€‚

```
import statsmodels.api as sm
import scipy.stats as scs

p_val = lambda s: sm.tsa.stattools.adfuller(s)[1]def build_stats(df):
    stats = pd.DataFrame({'skew':scs.skew(df),
                 'skew_test':scs.skewtest(df)[1],
                 'kurtosis': scs.kurtosis(df),
                 'kurtosis_test' : scs.kurtosistest(df)[1],
                 'normal_test' : scs.normaltest(df)[1]},
                  index = df.columns)
    return stats
```

æ£€éªŒçš„é›¶å‡è®¾æ˜¯æ—¶é—´åºåˆ—å¯ä»¥ç”¨ä¸€ä¸ªå•ä½æ ¹æ¥è¡¨ç¤ºï¼Œå®ƒä¸æ˜¯å¹³ç¨³çš„(å…·æœ‰ä¸€äº›ä¾èµ–äºæ—¶é—´çš„ç»“æ„)ã€‚å¦ä¸€ä¸ªå‡è®¾(æ‹’ç»é›¶å‡è®¾)æ˜¯æ—¶é—´åºåˆ—æ˜¯å¹³ç¨³çš„ã€‚

*   **é›¶å‡è®¾(H0)** :å¦‚æœæ²¡æœ‰è¢«æ‹’ç»ï¼Œè¯´æ˜æ—¶é—´åºåˆ—æœ‰å•ä½æ ¹ï¼Œæ„å‘³ç€å®ƒæ˜¯éå¹³ç¨³çš„ã€‚å®ƒæœ‰ä¸€äº›ä¾èµ–äºæ—¶é—´çš„ç»“æ„ã€‚
*   **å¤‡é€‰å‡è®¾(H1)** :æ‹’ç»é›¶å‡è®¾ï¼›è¿™è¡¨æ˜æ—¶é—´åºåˆ—æ²¡æœ‰å•ä½æ ¹ï¼Œè¿™æ„å‘³ç€å®ƒæ˜¯å¹³ç¨³çš„ã€‚å®ƒæ²¡æœ‰ä¾èµ–äºæ—¶é—´çš„ç»“æ„ã€‚

ä»¥ä¸‹æ˜¯æ¥è‡ª[å¢å¼ºè¿ªåŸº-å¯Œå‹’æµ‹è¯•](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)çš„ç»“æœ:

å¯¹äºåŠŸèƒ½å’Œç»“æœ:

```
features_p_val = features.apply(p_val)
outcomes_p_val = outcomes.apply(p_val)
outcomes_p_val,features_p_val
```

![](img/4245f9f2d643c40ba73f34906cd14fc1.png)![](img/d478fdaebab286f3a011ea41f0a765c0.png)

æµ‹è¯•å¯ä»¥é€šè¿‡ *p* å€¼æ¥è§£é‡Šã€‚ä½äºé˜ˆå€¼çš„*p*-å€¼(å¦‚ 5%æˆ– 1%)è¡¨æ˜æˆ‘ä»¬æ‹’ç»é›¶å‡è®¾(å¹³ç¨³)ï¼Œå¦åˆ™ï¼Œé«˜äºé˜ˆå€¼çš„*p*-å€¼è¡¨æ˜æˆ‘ä»¬ä¸èƒ½æ‹’ç»é›¶å‡è®¾(éå¹³ç¨³)ã€‚

*   ***p*-å€¼> 0.05** :ä¸èƒ½æ‹’ç»é›¶å‡è®¾(H0)ï¼Œæ•°æ®æœ‰å•ä½æ ¹ï¼Œéå¹³ç¨³ã€‚
*   ***p*-å€¼< = 0.05** :æ‹’ç»é›¶å‡è®¾(H0)ï¼Œæ•°æ®æ²¡æœ‰å•ä½æ ¹ï¼Œæ˜¯å¹³ç¨³çš„ã€‚

ä»è¿™ä¸ªæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„ç»“æœéƒ½è¿œä½äº 5%ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥æ‹’ç»é›¶å‡è®¾ï¼Œæ‰€æœ‰è½¬æ¢çš„æ•°æ®éƒ½æ˜¯å¹³ç¨³çš„ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥æµ‹è¯•æ­£æ€æ€§ã€‚

```
build_stats(features_outcomes_rmv_outliers)
```

![](img/9677f0b330ba4f97286b3b00d585cf4b.png)

å¯¹äºæ­£æ€åˆ†å¸ƒçš„æ•°æ®ï¼Œåæ–œåº¦åº”è¯¥å¤§çº¦ä¸ºé›¶ã€‚å¯¹äºå•å³°è¿ç»­åˆ†å¸ƒï¼Œå¤§äºé›¶çš„åæ–œå€¼æ„å‘³ç€åˆ†å¸ƒçš„å³å°¾æœ‰æ›´å¤šçš„æƒé‡ï¼Œåä¹‹äº¦ç„¶ã€‚

`scs.skewtest()`æ£€éªŒæ ·æœ¬æ€»ä½“çš„åæ–œåº¦ä¸ç›¸åº”æ­£æ€åˆ†å¸ƒçš„åæ–œåº¦ç›¸åŒçš„åŸå‡è®¾ã€‚ç”±äºæ‰€æœ‰çš„æ•°å­—éƒ½ä½äº 5%çš„é˜ˆå€¼ï¼Œæˆ‘ä»¬ä¸å¾—ä¸æ‹’ç»é›¶å‡è®¾ï¼Œå¹¶è¯´åæ–œåº¦ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚åŒæ ·çš„äº‹æƒ…å»`scs.kurtosistest()`ã€‚

`scs.normaltest()`æ£€éªŒæ ·æœ¬æ¥è‡ªæ­£æ€åˆ†å¸ƒçš„åŸå‡è®¾ã€‚å®ƒåŸºäº D'Agostino å’Œ Pearson çš„æµ‹è¯•ï¼Œç»“åˆäº†åæ–œåº¦å’Œå³°åº¦ï¼Œä»¥äº§ç”Ÿä¸€ä¸ªæ­£æ€æ€§çš„ç»¼åˆæµ‹è¯•ã€‚åŒæ ·ï¼Œæ‰€æœ‰æ•°å­—éƒ½ä½äº 5%é˜ˆå€¼ã€‚æˆ‘ä»¬å¿…é¡»æ‹’ç»é›¶å‡è®¾ï¼Œå¹¶è¯´ç”±ç™¾åˆ†æ¯”å˜åŒ–è½¬æ¢çš„æ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒã€‚

æˆ‘ä»¬å¯ä»¥å¯¹é€šè¿‡ç™¾åˆ†æ¯”æ’åå’Œç­¾åè½¬æ¢çš„æ•°æ®è¿›è¡ŒåŒæ ·çš„æµ‹è¯•ã€‚æˆ‘ä¸æƒ³è®©äº‹æƒ…å˜å¾—æ›´å¤æ‚ï¼ŒæŠŠäººä»¬å“è·‘ã€‚åœ¨è¿™ç¯‡æ–‡ç« å¤ªé•¿ä¹‹å‰ï¼Œæˆ‘æœ€å¥½åœ¨è¿™é‡Œç»“æŸã€‚

# å‚è€ƒ

1.  éº¦é‡‘å†œï¼ŒJ.G. 1994ã€‚å•ä½æ ¹å’Œåæ•´æ£€éªŒçš„è¿‘ä¼¼æ¸è¿‘åˆ†å¸ƒå‡½æ•°ã€‚ã€Šå•†ä¸šä¸ç»æµç»Ÿè®¡æ‚å¿—ã€‹12ï¼Œ167â€“76ã€‚
2.  è¾¾æˆˆæ–¯è’‚è¯ºï¼ŒR. B. (1971)ï¼Œâ€œä¸­ç­‰å’Œå¤§æ ·æœ¬é‡çš„æ­£æ€æ€§ç»¼åˆæ£€éªŒâ€ï¼Œã€Šç”Ÿç‰©è®¡é‡å­¦ã€‹ï¼Œ58ï¼Œ341â€“348
3.  è¾¾æˆˆæ–¯è’‚è¯ºï¼Œr .å’Œçš®å°”é€Šï¼ŒE. S. (1973)ï¼Œâ€œåç¦»æ­£æ€æ€§çš„æ£€éªŒâ€ï¼Œã€Šç”Ÿç‰©è®¡é‡å­¦ã€‹ï¼Œ60ï¼Œ613â€“622