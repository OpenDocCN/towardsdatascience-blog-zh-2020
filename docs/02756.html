<html>
<head>
<title>Boruta Explained Exactly How You Wished Someone Explained to You</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">博鲁塔准确地解释了你希望别人如何向你解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a?source=collection_archive---------0-----------------------#2020-03-17">https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a?source=collection_archive---------0-----------------------#2020-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="77b6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">看看最有效的特征选择算法之一的Boruta</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b011523a17e5a293f4d4682394170888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-7kkseitdupEm5adCfHmw.png"/></div></div></figure><p id="c22a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征选择是许多机器学习管道中的基本步骤。您丢弃了一堆特征，并且希望只选择相关的特征，而丢弃其他的特征。这样做的目的是通过去除会引入不必要噪声的无用特征来简化问题(听说过<a class="ae lq" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank"> Occam </a>？).</p><p id="afac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Boruta是一种非常智能的算法，可以追溯到2010年，旨在自动对数据集执行特征选择。它是作为R的一个包而诞生的(<a class="ae lq" href="https://www.jstatsoft.org/article/view/v036i11" rel="noopener ugc nofollow" target="_blank">这个</a>是最早描述它的文章)。Python的一个版本Boruta——叫做BorutaPy——已经存在，可以在<a class="ae lq" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="7b4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇文章中，我们将看到一些直接的Python代码来从头实现Boruta——我相信从头构建是真正理解它的最好方法——并且，在文章的最后，我们将看到如何使用borutay来使我们的生活变得更容易。</p><h1 id="e78a" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">1.一切都从X和y开始</h1><p id="df6c" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">为了看到Boruta的行动，让我们建立一个玩具数据集，它具有3个特征(<em class="mo">年龄</em>、<em class="mo">身高</em>和<em class="mo">体重</em>)、一个目标变量(<em class="mo">收入</em>)和5个观察值。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="cec4" class="mu ls it mq b gy mv mw l mx my">import pandas as pd</span><span id="a6bf" class="mu ls it mq b gy mz mw l mx my">### make X and y<br/>X = pd.DataFrame({'age': [25, 32, 47, 51, 62],<br/>                  'height': [182, 176, 174, 168, 181],<br/>                  'weight': [75, 71, 78, 72, 86]})<br/>y = pd.Series([20, 32, 45, 55, 61], name = 'income')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/c445f1bf6d5eac0ee14d562dec601aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgwA0J423hcJNpym8oM9_w.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">x和y</p></figure><p id="a239" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以目标是在知道一个人的年龄、身高和体重的情况下，预测他/她的收入。这可能看起来很荒谬(至少在理论上是这样:在实践中，<a class="ae lq" href="https://edition.cnn.com/2007/US/Careers/02/02/cb.tall.people/" rel="noopener ugc nofollow" target="_blank">身高已经被证明与薪水有关</a>)，但是总的来说，不要受个人偏见的影响是明智的。</p><p id="0c06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在现实生活中，我们必须处理3个以上的特征(通常从几百到几千个)。因此，逐一检查并决定是否保留它是不可行的。此外，有些关系(例如非线性关系和相互作用)不容易被人眼发现，即使通过精确的分析也不容易发现。</p><p id="0994" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">理想情况下，我们希望找到一种算法，能够自主决定<em class="mo"> X </em>的任何给定特征是否具有关于<em class="mo"> y. </em>的一些预测值</p><h1 id="4e26" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">2.为什么是博鲁塔？</h1><p id="0fcc" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">一个流行的特征选择算法是sklearn的<a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" rel="noopener ugc nofollow" target="_blank"> SelectFromModel </a>。基本上，你选择一个方便的模型——能够捕捉非线性关系和相互作用，例如随机森林——然后把它放在<em class="mo"> X </em>和<em class="mo"> y </em>上。然后，从这个模型中提取每个特性的重要性，只保留高于给定重要性阈值的特性。</p><p id="29ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这听起来很合理，但这种方法的弱点是不言而喻的:谁来决定阈值，以及如何决定？这里面有很大的随意性。</p><p id="9736" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当我第一次遇到这个问题时，我开始寻找一个更健壮的解决方案。直到我找到了博鲁塔。Boruta是一种基于统计的特征选择算法，即使没有用户的任何特定输入也能非常好地工作。这怎么可能呢？</p><p id="24c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Boruta基于两个绝妙的想法。</p><h2 id="c8dd" class="mu ls it bd lt nf ng dn lx nh ni dp mb ld nj nk md lh nl nm mf ll nn no mh np bi translated">2.1第一个理念:阴影功能</h2><p id="4089" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在Boruta中，特性之间并不竞争。相反——这是第一个绝妙的想法——他们与随机版本的他们竞争。</p><p id="a540" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实际上，从<em class="mo"> X </em>开始，另一个数据帧是通过随机洗牌创建的。这些置换的特征被称为<strong class="kw iu">阴影特征</strong>。此时，影子数据帧被附加到原始数据帧上，以获得一个新的数据帧(我们将称之为<em class="mo"> X_boruta </em>)，其列数是<em class="mo"> X </em>的两倍。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="4f83" class="mu ls it mq b gy mv mw l mx my">import numpy as np</span><span id="224c" class="mu ls it mq b gy mz mw l mx my">### make X_shadow by randomly permuting each column of X<br/>np.random.seed(42)<br/>X_shadow = X.apply(np.random.permutation)<br/>X_shadow.columns = ['shadow_' + feat for feat in X.columns]</span><span id="4c4d" class="mu ls it mq b gy mz mw l mx my">### make X_boruta by appending X_shadow to X<br/>X_boruta = pd.concat([X, X_shadow], axis = 1)</span></pre><p id="c25f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们的例子中，这就是<em class="mo"> X_boruta </em>的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/76ece4e8dd0d0dce34a460ebf31e9ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zStvS_9GpDEJJFZb0AvG4Q.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">x _博鲁塔</p></figure><p id="8c3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后在<em class="mo"> X_boruta </em>和<em class="mo"> y </em>上安装一个随机森林。</p><p id="df8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将每个原始特征的重要性与一个阈值进行比较。这次，<strong class="kw iu">阈值被定义为阴影特征</strong>中记录的最高特征重要性。当特征的重要性高于这个阈值时，这被称为“命中”。这个想法是，只有当一个特性能够比最好的随机特性做得更好时，它才是有用的。</p><p id="c357" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重现此过程的代码如下</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="9a28" class="mu ls it mq b gy mv mw l mx my">from sklearn.ensemble import RandomForestRegressor</span><span id="82f3" class="mu ls it mq b gy mz mw l mx my">### fit a random forest (suggested max_depth between 3 and 7)<br/>forest = RandomForestRegressor(max_depth = 5, random_state = 42)<br/>forest.fit(X_boruta, y)</span><span id="5311" class="mu ls it mq b gy mz mw l mx my">### store feature importances<br/>feat_imp_X = forest.feature_importances_[:len(X.columns)]<br/>feat_imp_shadow = forest.feature_importances_[len(X.columns):]</span><span id="6a9f" class="mu ls it mq b gy mz mw l mx my">### compute hits<br/>hits = feat_imp_X &gt; feat_imp_shadow.max()</span></pre><p id="c069" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于我们的玩具数据集，结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/bd65804686bb91749826ca8b3c1b6245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*btH1CNb6lQ62zakBxYECYg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">一次运行的结果</p></figure><p id="583b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">阈值为14%(最大值为11%、14%和8%)，因此有两个特征命中，即<em class="mo">年龄</em>和<em class="mo">身高</em>(分别为39%和19%)，而<em class="mo">体重</em> (8%)得分低于阈值。</p><p id="95d0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">显然，我们应该放下<em class="mo">体重</em>，继续关注<em class="mo">年龄</em>和<em class="mo">身高</em>。但是，我们应该相信这次跑步吗？如果只是一次不走运的减肥跑步呢？如果这只是年龄和身高的运气呢？</p><p id="a9f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是博鲁塔的第二个绝妙想法发挥作用的地方。</p><h2 id="7098" class="mu ls it bd lt nf ng dn lx nh ni dp mb ld nj nk md lh nl nm mf ll nn no mh np bi translated">2.2第二个想法:二项分布</h2><p id="27cd" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">就像机器学习中经常发生的那样(生活中？)，<strong class="kw iu">关键是迭代</strong>。毫不奇怪，20次试验比1次试验更可靠，100次试验比20次试验更可靠。</p><p id="a7d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，让我们重复上述过程20次。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="c912" class="mu ls it mq b gy mv mw l mx my">### initialize hits counter<br/>hits = np.zeros((len(X.columns)))</span><span id="9552" class="mu ls it mq b gy mz mw l mx my">### repeat 20 times<br/>for iter_ in range(20):</span><span id="6576" class="mu ls it mq b gy mz mw l mx my">   ### make X_shadow by randomly permuting each column of X<br/>   np.random.seed(iter_)<br/>   X_shadow = X.apply(np.random.permutation)<br/>   X_boruta = pd.concat([X, X_shadow], axis = 1)</span><span id="1f22" class="mu ls it mq b gy mz mw l mx my">   ### fit a random forest (suggested max_depth between 3 and 7)<br/>   forest = RandomForestRegressor(max_depth = 5, random_state = 42)<br/>   forest.fit(X_boruta, y)</span><span id="28af" class="mu ls it mq b gy mz mw l mx my">   ### store feature importance<br/>   feat_imp_X = forest.feature_importances_[:len(X.columns)]<br/>   feat_imp_shadow = forest.feature_importances_[len(X.columns):]</span><span id="b4d5" class="mu ls it mq b gy mz mw l mx my">   ### compute hits for this trial and add to counter<br/>   hits += (feat_imp_X &gt; feat_imp_shadow.max())</span></pre><p id="d901" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/2dd36d1bd356cae8a152e8bc839aa278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zly3ZYolDDsvn8x8_MeIeg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">20次运行的结果</p></figure><p id="91e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们如何设定决策标准？这是博鲁塔所包含的第二个高明的想法。</p><p id="6109" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就拿一个特征来说吧(不管<em class="mo">的年龄</em>、<em class="mo">的身高</em>还是<em class="mo">的体重</em>)，我们完全不知道它是否有用。我们保留它的可能性有多大？<strong class="kw iu">该特性的最大不确定性以50%的概率表示，就像扔硬币一样</strong>。由于每个独立的实验可以给出一个二元结果(命中或未命中)，一系列的<em class="mo"> n </em>试验遵循一个 <a class="ae lq" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">二项分布</strong> </a>。</p><p id="6f25" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在Python中，二项式分布的概率质量函数可计算如下:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="cb7a" class="mu ls it mq b gy mv mw l mx my">import scipy as sp</span><span id="27e1" class="mu ls it mq b gy mz mw l mx my">trials = 20<br/>pmf = [sp.stats.binom.pmf(x, trials, .5) for x in range(trials + 1)]</span></pre><p id="fc12" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些值看起来像一个铃铛:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/262561cc1df77992d55c8eaffe13d1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yqAUlMtPUiFyr8gYLFagTA.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">特征的二项式分布和定位</p></figure><p id="ea2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在Boruta案中，拒绝和接受之间没有硬性的界限。相反，有3个方面:</p><ul class=""><li id="1057" class="nr ns it kw b kx ky la lb ld nt lh nu ll nv lp nw nx ny nz bi translated">一个<strong class="kw iu">拒绝区域</strong>(红色区域):结束在这里的特征被认为是噪声，所以它们被丢弃；</li><li id="a430" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">犹豫不决的区域(蓝色区域):博鲁塔对这个区域的特征犹豫不决；</li><li id="bce7" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">一个<strong class="kw iu">接受区域</strong>(绿色区域):这里的特征被认为是预测性的，所以它们被保留。</li></ul><p id="ac62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过选择分布的两个最极端部分，即分布的<strong class="kw iu">尾部来定义面积(在我们的例子中，每个尾部占分布的0.5%)。</strong></p><p id="c965" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们对玩具数据进行了20次迭代，最终得出了一些有统计依据的结论:</p><ul class=""><li id="6f94" class="nr ns it kw b kx ky la lb ld nt lh nu ll nv lp nw nx ny nz bi translated">为了预测一个人的<em class="mo">收入</em>，<em class="mo">年龄</em>是预测性的，应该保留，<em class="mo">体重</em>只是噪音，应该去掉，</li><li id="7a50" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">博鲁塔对<em class="mo">身高</em>举棋不定:选择权在我们，但在保守的框架下，还是保持为宜。</li></ul><p id="160a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这一段中，我们已经实现了必要的代码，但是在Python中有一个很棒的(优化的)Boruta库。</p><h1 id="004e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">3.在Python中使用BorutaPy</h1><p id="2bb6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">Boruta可通过pip安装:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="7ea6" class="mu ls it mq b gy mv mw l mx my">!pip install boruta</span></pre><p id="a051" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是它的使用方法:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="2f23" class="mu ls it mq b gy mv mw l mx my">from boruta import BorutaPy<br/>from sklearn.ensemble import RandomForestRegressor<br/>import numpy as np</span><span id="168c" class="mu ls it mq b gy mz mw l mx my">###initialize Boruta<br/>forest = RandomForestRegressor(<br/>   n_jobs = -1, <br/>   max_depth = 5<br/>)<br/>boruta = BorutaPy(<br/>   estimator = forest, <br/>   n_estimators = 'auto',<br/>   max_iter = 100 # number of trials to perform<br/>)</span><span id="8742" class="mu ls it mq b gy mz mw l mx my">### fit Boruta (it accepts np.array, not pd.DataFrame)<br/>boruta.fit(np.array(X), np.array(y))</span><span id="fdbb" class="mu ls it mq b gy mz mw l mx my">### print results<br/>green_area = X.columns[boruta.support_].to_list()<br/>blue_area = X.columns[boruta.support_weak_].to_list()</span><span id="8860" class="mu ls it mq b gy mz mw l mx my">print('features in the green area:', green_area)<br/>print('features in the blue area:', blue_area)</span></pre><p id="4060" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如您所见，<strong class="kw iu">存储在<em class="mo"> boruta.support_ </em>中的特征是那些在某个点上结束于验收区域的特征，因此您应该将它们包含在您的模型</strong>中。存储在<em class="mo"> boruta.support_weak_ </em>中的特性是boruta无法接受或拒绝的特性(蓝色区域),这取决于数据科学家的选择:这些特性可能被接受，也可能不被接受，这取决于用例。</p><h1 id="4984" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">4.结论</h1><p id="cc5c" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">特征选择是机器学习管道中的决定性部分:过于保守意味着引入不必要的噪音，而过于激进意味着丢弃有用的信息。</p><p id="af01" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇文章中，我们看到了如何使用Boruta在数据集上执行稳健的、基于统计的要素选择。事实上，做出关于特性的实质性决策对于确保预测模型的成功至关重要。</p></div></div>    
</body>
</html>