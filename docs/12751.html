<html>
<head>
<title>Complete Guide to Adam Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Adam 优化完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complete-guide-to-adam-optimization-1e5f29532c3d?source=collection_archive---------4-----------------------#2020-09-02">https://towardsdatascience.com/complete-guide-to-adam-optimization-1e5f29532c3d?source=collection_archive---------4-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bad5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Adam 优化算法从定义到实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/783107b1c4d2fb0397ec50bbe2a7df50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VZBgp0S_7dPLQr6i"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拉斐尔·比斯卡尔迪在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="07aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 20 世纪 40 年代，数学规划是最优化的同义词。一个优化问题包括一个<strong class="lb iu">目标函数</strong>，通过从一组<strong class="lb iu">允许值</strong>【1】中选择<strong class="lb iu">输入值</strong>，该目标函数将被最大化或最小化。</p><p id="1985" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，优化是人工智能中一个非常熟悉的术语。具体来说，在深度学习问题上。而深度学习问题最值得推荐的优化算法之一就是<strong class="lb iu"> <em class="lv">亚当</em> </strong>。</p><p id="e23a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">免责声明:对神经网络优化的基本了解。如梯度下降和随机梯度下降是阅读前的首选。</em></p><h2 id="6326" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">在这篇文章中，我将强调以下几点:</h2><ol class=""><li id="759f" class="mp mq it lb b lc mr lf ms li mt lm mu lq mv lu mw mx my mz bi translated">Adam 优化的定义</li><li id="ec47" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">亚当之路</li><li id="bfda" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">随机优化的 Adam 算法</li><li id="a369" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">亚当和其他优化者之间的视觉比较</li><li id="7b9c" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">履行</li><li id="bbb4" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">亚当的优点和缺点</li><li id="6b92" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">结论和进一步阅读</li><li id="5feb" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">参考</li></ol><h1 id="6f1f" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">1.Adam 优化的定义</h1><p id="c87d" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">Adam 算法首先在 Diederik P. Kingma 和 Jimmy Ba 的论文<strong class="lb iu"> Adam:一种随机优化方法</strong>【2】中介绍。Adam 被定义为“一种高效<strong class="lb iu">随机优化</strong>的方法，其中<strong class="lb iu">只需要一阶梯度</strong>，几乎不需要内存”[2]。好，让我们把这个定义分解成两部分。</p><p id="f911" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，<strong class="lb iu">随机优化</strong>是在<em class="lv">随机性</em>存在的情况下，优化一个目标函数的过程。为了更好地理解这一点，让我们考虑随机梯度下降(SGD)。当我们有很多数据和参数时，SGD 是一个很好的优化器。因为在每一步，SGD 都从数据(小批量)的<em class="lv">随机子集计算梯度的估计值。不像梯度下降在每一步都考虑整个数据集。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/a153b46d23a4ff4c082335a2dc9054a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GNTUyQH47qvrlNsVCJMdQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与 GD 相比，SGD 导致许多振荡。然而，SGD 要快得多，因为它不会遍历整个批处理(来源:<a class="ae ky" href="https://www.deeplearning.ai/deep-learning-specialization/" rel="noopener ugc nofollow" target="_blank"> deeplearning.ai </a>)</p></figure><p id="5e05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二，Adam 只需要<strong class="lb iu">一阶梯度</strong>。也就是说，亚当只需要参数的一阶导数。</p><p id="ecfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，算法的名字<em class="lv">亚当</em>来源于<em class="lv">自适应矩估计</em>。随着我们对算法的研究，这一点将变得显而易见。</p><h1 id="de74" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">2.亚当之路</h1><p id="9b58" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">Adam 基于并结合了以前算法的优点。为了理解 Adam 算法，我们需要快速了解之前算法的背景。</p><h2 id="fbf3" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">我</strong>。<strong class="ak">带动量的新币</strong></h2><p id="e505" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">物理学中的动量是运动中的物体，比如一个球沿着斜坡加速下降。因此，<strong class="lb iu">带动量的 SGD</strong>【3】<strong class="lb iu"/>结合了之前更新步骤的梯度，以<strong class="lb iu">加速梯度下降</strong>。这是通过在相关方向上采取小而直接的步骤来实现的。</p><p id="69a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SGD 通过计算一个<strong class="lb iu"/><em class="lv"/><a class="ae ky" href="https://www.youtube.com/watch?v=lAq96T8FkTw" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a><em class="lv"/>的梯度移动平均值来实现，然后<strong class="lb iu">用它来更新你的参数</strong><em class="lv"/><strong class="lb iu"/>【权重，偏差】。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/8dfa8f06ec5689a6cb23a5931cb96eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgPQSyKq_nPi0B7ZybpWEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算指数加权平均值(移动平均值)，然后更新参数</p></figure><ul class=""><li id="b87c" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">贝塔项(<strong class="lb iu"> 𝛽 </strong>)控制着均线。β的值是[0，1]，一个常见的值是𝛽 = 0.9，这意味着我们对最后 10 次迭代的梯度进行平均，旧的梯度被丢弃或者被遗忘。因此，较大的β值(比如𝛽 = 0.98)意味着我们在更多的梯度上求平均值。</li><li id="5587" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated">Alpha ( <strong class="lb iu"> α </strong>)是决定每次迭代步长的学习率。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d5600ae8c0493de9a0f8c612fda63394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bUAUROYmPXJzagaeVPbmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oa">左</strong> : SGD，<strong class="bd oa">右</strong>:带动量的 SGD(来源:<a class="ae ky" href="https://www.willamette.edu/~gorr/classes/cs449/momrate.html" rel="noopener ugc nofollow" target="_blank">动量和学习率适应</a>)</p></figure><h2 id="2db0" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">二。相关工作(AdaGrad 和 RMSProp)</h2><p id="b121" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">好的，在我们讨论亚当之前，有两个算法需要了解。<strong class="lb iu"> AdaGrad </strong>(自适应梯度算法)【4】和<strong class="lb iu"> RMSProp </strong>(均方根传播)【5】都是 SGD 的扩展。这两种算法与 Adam 有一些相似之处。事实上，亚当结合了两种算法的优点。</p><h2 id="b815" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">三世。自适应学习率</strong></h2><p id="aec1" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">AdaGrad 和 RMSProp 也都是<em class="lv">自适应</em>梯度下降算法。意思是，<strong class="lb iu">对于参数</strong> (w，b)<strong class="lb iu">中的每一个，学习率</strong> (α) <strong class="lb iu">被适配</strong>。简而言之，学习率是按参数来维持的。</p><p id="2f68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地说明这一点，下面是对 AdaGrad 和 RMSProp 的解释:</p><ul class=""><li id="2741" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">阿达格勒</li></ul><p id="8bc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阿达格拉德的<strong class="lb iu">每参数学习率</strong>帮助<strong class="lb iu">增加稀疏参数的学习率</strong>。因此，<strong class="lb iu"> AdaGrad 适用于稀疏梯度</strong>，例如在自然语言处理和图像识别应用中【4】。</p><ul class=""><li id="5336" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated"><strong class="lb iu"> RMSProp </strong></li></ul><p id="74ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RMSProp 是由 Tielemen 和 Hinton 提出来加速小批量学习的。在 RMSProp 中，<strong class="lb iu">学习率基于 <strong class="lb iu">最近梯度</strong>的幅度的移动平均值来调整</strong> <em class="lv">。</em></p><p id="5cdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，RMSProp 保持最近梯度平方的移动平均值，用(v)表示。因此，给予最近的梯度更多的权重。</p><p id="3c2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，术语β(𝛽)被引入作为<em class="lv">遗忘因子</em>(就像在具有动量的 SGD 中一样)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/87a844d56547d79874f4ca224b4088a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fhTU7aKuhzihi10L3RWnw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算最近梯度平方的移动平均值</p></figure><p id="4921" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，在更新<strong class="lb iu"> <em class="lv"> θ </em> </strong>(比如<strong class="lb iu"> <em class="lv"> w </em> </strong>或<strong class="lb iu"> <em class="lv"> b </em> </strong>)时，将<strong class="lb iu"> <em class="lv"> θ </em> </strong> <em class="lv">的前一个值的梯度除以</em>该参数<strong class="lb iu"><em class="lv"/></strong>的最近梯度的平方的移动平均值，然后乘以<strong class="lb iu"> α </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/7c13319247c2904994a0086272192fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmbV8RsrlkGvxJBbrTTxpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">某些参数的更新步骤<em class="od"> θ </em></p></figure><p id="b758" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，RMSProp 在大的和冗余的数据集(例如噪声数据)上工作良好[5]。</p><p id="4696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> *术语(</em> <strong class="lb iu"> 𝜖 </strong> ) <em class="lv">用于数值稳定性(避免被零除)。</em></p><h2 id="53c8" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">以下是我们目前所学内容的视觉对比:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f2d8a2871edea2f680735b2137f80956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">围绕鞍点的不同优化器(来源:<a class="ae ky" href="https://imgur.com/a/Hqolp#2dKCQHh" rel="noopener ugc nofollow" target="_blank"> Imgur </a>亚历克·拉德福德</p></figure><p id="cf6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的 gif 中，你可以看到动量在找到正确的路径之前四处探索。至于 SGD，AdaGrad，和 RMSProp，都是走的差不多的路，但是 AdaGrad 和 RMSProp 显然更快。</p><h1 id="3b78" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">3.随机优化的 Adam 算法</h1><p id="b79e" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">好了，现在我们已经得到了算法所需的所有部分。</p><p id="0e68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如<a class="ae ky" href="https://www.youtube.com/watch?v=JXQT_vxqwIs" rel="noopener ugc nofollow" target="_blank">吴恩达</a>所解释的，亚当:自适应力矩估计是动量和 RMSProp 的简单组合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/258d293ad23258c2b4f6cb8fce84bcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0SzlwgD25QG1ERvc07V7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚当算法(来源:亚当:随机优化的方法[2])</p></figure><p id="29b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是优化目标函数<em class="lv"> f(θ) </em>的算法，参数<em class="lv"> θ </em>(权重和偏差)。</p><p id="84c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Adam 包括超参数:<em class="lv"> α </em>，𝛽 <em class="lv"> 1 </em>(来自动量)，𝛽 <em class="lv"> 2 </em>(来自 RMSProp)。</p><p id="f4f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始化:</p><ul class=""><li id="d1f0" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated"><em class="lv"> m </em> = 0，这是第一个力矩矢量，按动量处理</li><li id="2cd7" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated"><em class="lv"> v </em> = 0，这是第二个力矩矢量，在 RMSProp 中处理</li><li id="e150" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated"><em class="lv">t</em>:0</li></ul><p id="9fef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在迭代<em class="lv"> t </em>时:</p><ul class=""><li id="024f" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">更新<em class="lv"> t，t </em> := <em class="lv"> t </em> + 1</li><li id="464f" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated">得到<em class="lv"> g </em>相对于<em class="lv"> t 的梯度/导数，这里</em>的<em class="lv"> g </em>相当于<em class="lv">T25【分别为<em class="lv"> dw </em>和<em class="lv"> db </em></em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f26e48ac55678ac2a604ef161469ab87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*cacUh0tnsno11MUrWpfi-w.png"/></div></figure><ul class=""><li id="71b8" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">更新第一时刻<em class="lv"> mt </em></li><li id="be0a" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated">更新二阶矩<em class="lv"> vt </em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/e5996f6c354a33be738faff7af868530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAnfjvTr_gRouKMY_GzbCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分别更新 mt 和 vt</p></figure><ul class=""><li id="8103" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">计算偏差修正后的<em class="lv"> mt </em> ( <a class="ae ky" href="https://youtu.be/lWzo8CajF5s" rel="noopener ugc nofollow" target="_blank">偏差修正后的</a>对移动平均值给出了更好的估计)</li><li id="1f7b" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated">计算偏差校正后的 v  t</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/09eb5806bfa60bfadf79e24cd5629b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Z5rpAUj4hNnTewu_U2XEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分别为偏差校正的 mt 和 vt</p></figure><ul class=""><li id="2b04" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">更新参数θ</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/12c63362823da39b469d7e1374a4f0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-oG06hFf74Of6y7929rKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更新参数</p></figure><p id="4c7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">就这样！</strong>循环将继续，直到 Adam 收敛到一个解。</p><h1 id="974a" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">4.优化器之间的视觉比较</h1><p id="62ec" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">识别前面提到的优化算法之间的差异的更好的方法是查看它们的性能的视觉比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/cbd45e4dc1cf7da54e7c2028eea3cd7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEfp63w_JJWxVN-VgBXxFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同优化者的培训成本比较[2]</p></figure><p id="b422" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图来自亚当<a class="ae ky" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">的论文</a>。它展示了超过 45 个时期的训练成本，你可以看到 CNN 的 Adam 比 AdaGrad 收敛得更快。也许值得一提的是，AdaGrad 对应于一个版本的 Adam，其超参数(α，<em class="lv"> 𝛽1，𝛽2 </em>)为特定值[2]。为了避免混淆，我决定从这个帖子中删除 AdaGrad 的数学解释，但是如果你想了解更多，这里有 mxnet 的一个简单解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/1bb300446baadf28a0e0cc72a1df843a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*STiRp7PW5yIrvYZupZA6nw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="od"> Gif 作者使用【7】</em></p></figure><p id="47d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的 gif 中，你可以看到亚当和 RMSProp 以相似的速度收敛，而阿达格拉德似乎在努力收敛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f50509be52d02d1709a574dcf3913a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*U224pqhF4WUOZhfmDIWtxA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="od"> Gif 作者使用【7】</em></p></figure><p id="46c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时，在这张 gif 图中，你可以用动量将 Adam 和 SGD 收敛到一个解。而 SGD、AdaGrad 和 RMSProp 似乎陷入了局部最小值。</p><h1 id="b14e" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">5.履行</h1><p id="a716" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在这里，我将展示将 Adam 合并到您的模型中的三种不同方法，分别是 TensorFlow、PyTorch 和 NumPy 实现。</p><ul class=""><li id="7540" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank"> TensorFlow 实现</a>:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><ul class=""><li id="c506" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated"><a class="ae ky" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank"> PyTorch 实现</a>:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><ul class=""><li id="0cf3" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated">仅用 NumPy 实现:</li></ul><p id="23e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个实现可能不太实用，但它会让您更好地理解 Adam 算法。</p><p id="d9c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但正如你所猜测的，代码相当长，所以为了更好地查看，<a class="ae ky" href="https://gist.github.com/LayanCS/8bc7b0ec110487dd1e485c809954b044" rel="noopener ugc nofollow" target="_blank">给出了要点</a>。</p><h1 id="fee5" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">6.亚当的优点和缺点</h1><p id="5c17" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">与其他算法相比，Adam 是最好的优化器之一，但它也不是完美的。所以，下面是亚当的一些优点和缺点。</p><h2 id="5424" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">优势:</h2><ol class=""><li id="fd5e" class="mp mq it lb b lc mr lf ms li mt lm mu lq mv lu mw mx my mz bi translated">可以处理噪声数据集上的稀疏梯度。</li><li id="658f" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">默认的超参数值在大多数问题上都做得很好。</li><li id="6b57" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">计算效率高。</li><li id="8f84" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">需要很少的内存，因此内存效率高。</li><li id="2e26" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">适用于大型数据集。</li></ol><h2 id="80c3" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">缺点:</h2><ol class=""><li id="0616" class="mp mq it lb b lc mr lf ms li mt lm mu lq mv lu mw mx my mz bi translated">Adam 在某些领域没有收敛到最优解(这是 AMSGrad 的动机)。</li><li id="8285" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">Adam 可能会遇到体重下降的问题(这在 AdamW 中有所论述)。</li><li id="3aa7" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">最近的优化算法已经被证明更快更好[6]。</li></ol><h1 id="7f79" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">7.结论和进一步阅读</h1><p id="a497" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">这就是 Adam 的全部内容:自适应矩估计！</p><p id="54b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Adam </strong>是 SGD 的扩展，它结合了 AdaGrad 和 RMSProp 的优点。Adam 也是一种<strong class="lb iu">自适应</strong>梯度下降算法，因此它保持每个参数的学习速率。并且它跟踪梯度的第一和第二<strong class="lb iu">时刻</strong>的移动平均值。因此，使用第一和第二矩，Adam 可以给出参数更新的未缩放的直接估计<strong class="lb iu">。最后，尽管出现了更新的优化算法，Adam(和 SGD)仍然是一个稳定的优化器。</strong></p><p id="889b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进一步阅读(和观看)的绝佳资源:</p><ul class=""><li id="58b1" class="mp mq it lb b lc ld lf lg li nv lm nw lq nx lu ny mx my mz bi translated"><a class="ae ky" href="https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-b-chain-rule-gradient-and-directional-derivatives/session-35-gradient-definition-perpendicular-to-level-curves/" rel="noopener ugc nofollow" target="_blank">梯度的定义【Denis Auroux 教授(麻省理工学院)</a></li><li id="a80f" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/stochastic-gradient-descent-with-momentum-a84097641a5d">带有动量的随机梯度下降</a>作者 Vitaly Bushaev</li><li id="da3e" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated"><a class="ae ky" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习的 Adam 优化算法简介</a>作者 Jason Brownlee</li><li id="bf65" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated">吴恩达(deeplearning.ai)的深度学习专业化，也可以在<a class="ae ky" href="https://www.youtube.com/watch?v=1waHlpKiNyY&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上找到</li><li id="c36b" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu ny mx my mz bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">亚当——深度学习优化的最新趋势</a>维塔利·布沙耶夫</li></ul><h1 id="f04c" class="nf lx it bd ly ng nh ni mb nj nk nl me jz nm ka mh kc nn kd mk kf no kg mn np bi translated">8.参考</h1><ol class=""><li id="7f7c" class="mp mq it lb b lc mr lf ms li mt lm mu lq mv lu mw mx my mz bi translated">斯蒂芬·j·赖特，<a class="ae ky" href="https://www.britannica.com/science/optimization" rel="noopener ugc nofollow" target="_blank">优化</a> (2016)，大英百科全书</li><li id="d59e" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">Diederik P. Kingma，Jimmy Ba，<a class="ae ky" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Adam:一种随机优化方法</a> (2015)，arxiv</li><li id="83eb" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated"><a class="ae ky" href="https://dl.acm.org/doi/10.5555/104279.104293" rel="noopener ugc nofollow" target="_blank">通过错误传播学习内部表征</a> (1986)，鲁梅尔哈特，辛顿和威廉姆斯，美国计算机学会</li><li id="44a1" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">杜奇等人，<a class="ae ky" href="https://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf" rel="noopener ugc nofollow" target="_blank">在线学习和随机优化的自适应次梯度方法</a> (2011)，斯坦福</li><li id="9085" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">Geoffrey Hinton 与 Nitish Srivastava Kevin Swersky，<a class="ae ky" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank">用于机器学习的神经网络(讲座 6) </a> (2012)，UToronto 和 Coursera</li><li id="ea5a" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">陈志强，<a class="ae ky" href="https://johnchenresearch.github.io/demon/" rel="noopener ugc nofollow" target="_blank">最近梯度下降算法的最新概述</a> (2020)，GitHub</li><li id="39c6" class="mp mq it lb b lc na lf nb li nc lm nd lq ne lu mw mx my mz bi translated">kuroitu S，<a class="ae ky" href="https://qiita.com/kuroitu/items/6695e0c79e888543e150" rel="noopener ugc nofollow" target="_blank">优化方法比较</a> (2020)，Qiita</li></ol></div></div>    
</body>
</html>