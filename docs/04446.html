<html>
<head>
<title>Understanding ACGANs with code[PyTorch]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用代码理解 acgan[py torch]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4?source=collection_archive---------15-----------------------#2020-04-21">https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4?source=collection_archive---------15-----------------------#2020-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1c8e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">Python-PyTorch</h2><div class=""/><div class=""><h2 id="21c4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用 PyTorch 库和 Python 构建 ACGAN 模型并了解更多信息</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/edc76731c81a08bb25fd6ba0951afae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v65i8Sem-wGWl3sD"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">法比安·格罗斯在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1a72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">ACGAN 代表辅助分类器生成对抗网络。该网络由谷歌大脑的一组研究人员开发，并在澳大利亚悉尼举行的第 34 届国际机器学习会议上展示。本文简要描述了论文中所阐述的研究工作以及使用 PyTorch 实现的研究工作。</p><h2 id="fe0a" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">为什么是 ACGAN？</h2><p id="5525" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">ACGAN 是一种特殊的 GAN，可以在图像合成方面创造奇迹。你告诉它类别标签，它就能生成图像——全部来自<em class="nb">完全噪声</em>！ACGAN 的另一个重要特点是，与以前的方法相比，它生成的图像分辨率相当高。但是任何图像都可以用双线性插值来调整大小，并且它的大小可以增加。没错，它们可以，但它们仍然是低分辨率图像的模糊版本，不会比它们更难以分辨。ACGAN 是第一个提出使用预先训练的初始网络来检查图像可辨性的想法。</p><p id="d4fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[注意—此处构建的 PyTorch 模型是 ACGAN 论文的实现，仅包含生成器和鉴别器。自己动手对照预先训练好的初始网络来检查模型。请在评论中告诉我结果:)</p><h2 id="cd82" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">体系结构</h2><p id="4c43" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">与任何 GAN 网络一样，ACGAN 由一个发生器和一个鉴别器组成。然而，在 ACGAN 中，除了噪声<em class="nb"> z 之外，每个生成的样本都有相应的类别标签 c ~ <em class="nb"> C </em>(可用类别)，该类别标签帮助模型基于传递的标签来合成图像。生成器 G 使用类别标签和噪声 z 来生成图像。生成的图像可以表示为—</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9a88371684a19607a1e9e34ebed2f2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*V1oWOg4g07TlGQVDVvrB8Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">生成的图像(<strong class="bd nd"> <em class="ne">假</em> </strong>因为它必须被鉴别器标记为假)</p></figure><p id="921f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">发电机架构非常简单。它由一系列反卷积层组成，也称为转置卷积层。迷惑？让我来给你解释一下—</p><blockquote class="nf ng nh"><p id="76de" class="li lj nb lk b ll lm kd ln lo lp kg lq ni ls lt lu nj lw lx ly nk ma mb mc md im bi translated">转置卷积层与卷积层相同，但在原始输入中添加了填充。因此，当应用步长为 1 且无填充的卷积时，输出的高度和宽度大于输入的高度和宽度。步长为 2 时，我们可以对转置卷积层执行上采样，就像对步长为 2 的卷积层执行下采样一样。</p></blockquote><p id="1db8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">发生器的转置卷积层由 ReLU 非线性支持。</p><p id="9369" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鉴别器包含一组具有 leaky-ReLU 非线性的 2d 卷积模块，后面是线性层，以及用于其每个输出的 softmax 和 sigmoid 函数，用于检测模型的类别和来源。整个模型可以画为—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/071e802ffc1b033bbb2d1557abbaffd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*kXyQf76QUuso4OKC3DyXVA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ACGAN 模型</p></figure><p id="ccc9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们已经通过它们的架构模型定义了发生器和鉴别器，我们将得到损失函数。</p><h2 id="e5c3" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">损失函数</h2><p id="cb69" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">ACGAN 的损失函数分为两部分</p><ol class=""><li id="1de7" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated">被检查源的对数可能性—</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/3200f7a261bb89409f760e697a58bab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7USqtkHDR23UiGZyBwGjQw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">源损耗</p></figure><p id="cb0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.被检查类别的对数可能性—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/18bfd55bfc75d49751dac1823e02596d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*fGGPRSBBFniC3Rx1t05sZg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">阶级损失</p></figure><p id="238a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面的损失函数可以明显看出，发生器和鉴别器在这个损失函数上“斗争”。生成器和鉴别器都试图最大化类损失。然而，源损耗是一个极小极大问题。发生器试图<em class="nb">最小化</em>源损耗并欺骗鉴别器。另一方面，鉴别器试图<em class="nb">最大化</em>源损耗，并试图阻止发电机占上风。</p><h2 id="a32c" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">模特[PyTorch]</h2><p id="9a2a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">由于我们已经完成了对 ACGAN 论文的分析，现在我们将使用 CIFAR10 数据集构建模型。你可以从<a class="ae lh" href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集。事实上，我会将下载合并到培训本身，以摆脱麻烦！该数据集包含 60，000 张 32x32 尺寸的图像。有 10 个类，每个类有 6000 个图像。</p><p id="085e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">发电机，写成一个模块—</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ACGAN 的发生器模块</p></figure><p id="b60b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，在生成器中，卷积网络已经仔细选择了参数，使得输出张量与来自训练集的张量具有相同的维数。这是必要的，因为两者都进入鉴别器进行评估</p><p id="b135" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鉴别器，也写成一个模块—</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">鉴别器模型</p></figure><p id="62ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们开始训练！</p><p id="c99a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了进行训练，我将 epochs 的数量设置为 100。学习率设置为 0.0002，批量设置为 100。</p><p id="5afe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">理想情况下，为了正确的图像合成，历元的数量应该更多。例如，我已经将它设置为 100</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">培训模式</p></figure><h2 id="d25a" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">结果！</h2><p id="bc7e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们来看看这个小实验的结果</p><p id="0a4e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第一个时期的图像(噪声)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/f9b25a9264d37cf32ec08949137a0d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*JN-C2BpCGPpWClvFfm-IYw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第一历元图像</p></figure><p id="1e5f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上一个时代的图像—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/179042e20150b4c191cfd66309106c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*kaLOiKEYddwkIwyWRm4Svw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第 100 代图像</p></figure><p id="4323" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很大的进步，是吧？甘斯的妙处在于，你可以通过影像看到模特的训练。随着模型慢慢了解分布情况，您可以看到跨时代的结构正在形成！你还在等什么？为模型编写自己的代码；用你自己的方式解决问题。如果可以的话，即兴创作解决方案——我们会看看是否能以你的名字命名；)</p><p id="a139" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你卡住了，请在评论中告诉我！来帮忙了:)</p><p id="9629" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看我的博客以获得更快的更新，不要忘记订阅更多高质量的内容:D</p><p id="6974" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://www.theconvolvedblog.vision/" rel="noopener ugc nofollow" target="_blank">https://www.theconvolvedblog.vision/</a></p></div></div>    
</body>
</html>