<html>
<head>
<title>The Intuition Behind Gradient Boosting &amp; XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度增强背后的直觉&amp; XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-intuition-behind-gradient-boosting-xgboost-6d5eac844920?source=collection_archive---------24-----------------------#2020-05-24">https://towardsdatascience.com/the-intuition-behind-gradient-boosting-xgboost-6d5eac844920?source=collection_archive---------24-----------------------#2020-05-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d1b3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">定性理解梯度增强和XGBoost</h2></div><p id="9c60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们提出了一个非常有影响力和强大的算法，叫做<em class="lb">极限梯度提升</em>或<a class="ae lc" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank">XGBoost</a><a class="ae lc" href="#9f40" rel="noopener ugc nofollow">【1】</a>。它是梯度推进机器的实现，利用各种优化来非常快速地训练强大的预测模型。</p><p id="2e92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们将首先解释<a class="ae lc" href="#b18a" rel="noopener ugc nofollow"> <em class="lb">渐变增强</em></a><a class="ae lc" href="#9f40" rel="noopener ugc nofollow">【2】</a>，以将读者置于上下文中。然后，我们定性地浏览XGBoost 的<a class="ae lc" href="#9fc2" rel="noopener ugc nofollow">工作方式，必要时绘制与梯度增强概念的连接。最后，我们谈谈各种实施的</a><a class="ae lc" href="#4ca7" rel="noopener ugc nofollow">优化</a>及其背后的想法。</p><p id="59c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在写这篇文章时，我把尽可能定性作为自己的一个目标，只有在有助于解释的情况下才引入方程。目标是让读者直观地了解渐变增强和XGBoost是如何工作的。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="b18a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">梯度推进</h1><p id="0305" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">梯度推进包括建立弱学习者的集合。它基于两个关键的见解。这是洞察力一。</p><blockquote class="mh mi mj"><p id="22c8" class="kf kg lb kh b ki kj jr kk kl km ju kn mk kp kq kr ml kt ku kv mm kx ky kz la ij bi translated">如果我们能解释模型的错误，我们就能改善模型的性能。</p></blockquote><p id="d3a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用一个简单的例子来支持这个观点。假设我们有一个回归模型，它预测实际结果为1的测试用例为3。如果我们知道误差(在给定示例中为2)，我们可以通过从原始预测3中减去误差2来微调预测，并获得更准确的预测1。这就引出了一个问题，<em class="lb">“对于任何给定的输入，我们如何知道模型产生的误差？”</em>，这就引出了我们的第二个洞见。</p><blockquote class="mh mi mj"><p id="c3e5" class="kf kg lb kh b ki kj jr kk kl km ju kn mk kp kq kr ml kt ku kv mm kx ky kz la ij bi translated">我们可以训练一个新的预测器来预测原始模型产生的误差。</p></blockquote><p id="2eb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，给定任何预测模型，我们可以首先通过训练新的预测器来预测其当前误差，从而提高其准确性。然后，形成新的改进模型，其输出是原始预测的微调版本。改进的模型需要<em class="lb">原始预测器</em>和<em class="lb">误差预测器</em>的输出，现在被认为是两个预测器的集合。在梯度推进中，这被重复任意多次，以不断提高模型的准确性。这个重复的过程形成了梯度推进的症结。</p><h2 id="fc23" class="mn ll iq bd lm mo mp dn lq mq mr dp lu ko ms mt lw ks mu mv ly kw mw mx ma my bi translated">一群学习能力差的学生</h2><p id="7f74" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">当训练一个新的误差预测模型来预测模型的当前误差时，我们正则化其复杂性以防止<a class="ae lc" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank"> <em class="lb">过拟合</em> </a>。当预测原始模型的<em class="lb">‘误差’</em>时，该正则化模型将具有<em class="lb">‘误差’</em>。参考上面的<a class="ae lc" href="#d3a5" rel="noopener ugc nofollow">例子</a>，它可能不一定预测2。由于新的改进模型的预测依赖于新的误差预测模型的预测，它也将有误差，尽管比以前低。</p><p id="32f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了减轻这种情况，我们采取了两种措施。首先，我们通过对其输出应用小权重<em class="lb"> η </em>(通常在0到0.1之间)来减少对任何单个误差预测器的依赖或信任。然后，不是在1次改进迭代后停止，而是多次重复该过程，为新形成的改进模型学习新的误差预测器，直到精度或误差令人满意。这可以用下面的等式来总结，其中<em class="lb"> x </em>是一个输入。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="e10d" class="mn ll iq ne b gy ni nj l nk nl">i<em class="lb">mproved_model(x) = current_model(x) + η × error_prediction_model(x) </em></span><span id="dd15" class="mn ll iq ne b gy nm nj l nk nl"><em class="lb">current_model(x) = improved_model(x) </em></span><span id="e27a" class="mn ll iq ne b gy nm nj l nk nl"><em class="lb">Repeat above 2 steps till satisfactory</em></span></pre><p id="740b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，误差预测模型预测负梯度，因此，我们使用加法而不是减法。在每次迭代之后，将学习一个新的预测器来解释先前模型的误差，并将其添加到集合中。要执行的迭代次数和<em class="lb"> η </em>是超参数。</p><figure class="mz na nb nc gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nn"><img src="../Images/975352342c539d2bf4f8a7abb31392b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7EhjRtzxSj5whkHf-MxjLA.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">如果你的<strong class="bd nz">渐变增强</strong>的想法和这个图相似，你就在正确的轨道上。作者图片</p></figure><h2 id="1957" class="mn ll iq bd lm mo mp dn lq mq mr dp lu ko ms mt lw ks mu mv ly kw mw mx ma my bi translated">“梯度”推进</h2><p id="6cf4" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">在结束之前，我们先探讨一下为什么称之为<em class="lb">“渐变”</em>增强。原来，我们上面提到的误差是损失函数相对于模型预测的梯度，这可推广到任何可微分的损失函数。例如，当我们对平方误差损失函数<em class="lb">0.5(y _ true y _ pred)</em>求微分时，我们得到<em class="lb">y _ pred y _ true</em>，它恰好是我们训练新误差预测器的“误差”。类似地，其他类型的预测问题(例如分类问题)的误差可以通过梯度来表示。因为我们预测的是梯度，我们称之为梯度推进。</p><p id="a157" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从数学上讲，损失函数的导数∂loss/∂pred给出了调整预测以使损失最大化的方向。在梯度推进中，我们以相反的方向(负梯度)预测和调整我们的预测。这达到了相反的效果(使损失最小化)。因为模型的损失与其性能和准确性成反比，所以这样做可以提高其性能。</p><p id="818b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">凭直觉，我们正在朝着提高模型整体性能的方向逐步改变我们的模型预测。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="9fc2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">XGBoost</h1><p id="53d3" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">XGBoost是一种梯度增强机器，它使用梯度增强树(gbtree)作为误差预测器。它从一个简单的预测器开始，该预测器预测一个任意的数(通常为0.5)，而不考虑输入。不用说，预测器具有非常高误差率。然后，应用上述想法的<a class="ae lc" href="https://bobby-tan.github.io/bobby-tan.github.io/articles/20/The-Intuition-Behind-Gradient-Boosting-and-XGBoost#steps" rel="noopener ugc nofollow" target="_blank">，直到误差最小。在XGBoost中，误差预测模型的训练不是通过在<em class="lb">(特征，误差)</em>对上平凡地优化预测器来完成的。接下来，我们来看看它们是如何构建的。</a></p><h2 id="df09" class="mn ll iq bd lm mo mp dn lq mq mr dp lu ko ms mt lw ks mu mv ly kw mw mx ma my bi translated">梯度推进树</h2><p id="1db6" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">在XGBoost中，学习一个gbtree，使得新模型的整体损失最小化，同时记住不要<em class="lb">过度拟合</em>该模型。请注意，在本节中，我们将讨论上述想法的1次迭代。为了更好地理解它，让我们从最简单的可能的树开始，它不进行分割，并且预测相同的值，而不管输入是什么。这个树非常简单，独立于输入，并且绝对是欠满足的。尽管如此，它仍然可以帮助减少损失。提到的问题可以用下面的等式来表示。</p><figure class="mz na nb nc gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oa"><img src="../Images/cefb5edcb8720fe8e10767134a4b6dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LSU5MNM71_bOBxSNCNhjA.png"/></div></div></figure><p id="df36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由最后一项表示的所应用的<a class="ae lc" rel="noopener" target="_blank" href="/intuitions-on-l1-and-l2-regularisation-235f2db4c261"> <em class="lb"> L2正则化</em> </a>已经通过实验证明在防止过拟合方面是有效的。虽然在这个已经不足的模型中没有用，但随着我们增加树的复杂性，它将变得有意义。像这样的问题，可以通过对表达式相对于<em class="lb"> o </em>求导，设置导数为0，然后找到对应的<em class="lb"> o </em>来解决。不幸的是，我们上面看到的表达式很难区分。为了解决这个问题，我们使用<a class="ae lc" href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives#quadratic-approximations" rel="noopener ugc nofollow" target="_blank"> <em class="lb">二次近似</em> </a>用更简单的术语来近似这个表达式。</p><figure class="mz na nb nc gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ob"><img src="../Images/06290fab0311caeb78282b67855def48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ed8bP9cozMuXWDXIVEDrUw.png"/></div></div></figure><p id="c611" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个简化的表达式很容易求导，在导数设置为0后，我们可以求解并得到<em class="lb"> o </em>。原来，就是下面的<em class="lb"> o。</em></p><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3c6084c12194a83c03d516f99950d780.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*0yEY00GVssNTVWUzC3e3PQ.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">推导起来并不难。你可以把推导当作练习。</p></figure><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="gh gi od"><img src="../Images/13351e0517b8c9bda729384f64a24779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*Oh2bBE1wVtbeql9oJhM8hw.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">作者图片</p></figure><p id="63ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定场景的一个示例:我们找到一个单一的最佳调整<em class="lb"> o </em>,我们可以将其应用于数据集中的任何样本，以最小化总体损失。</p><p id="ec12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，现在，给定一个模型<em class="lb"> f </em>和一组样本，我们可以找到最佳改进我们模型的单个调整<em class="lb"> o </em>。<em class="lb">请注意，o也可以代入等式计算损失值。</em>本节的剩余部分将讨论我们如何通过增加简单模型的复杂性(增长树)来进一步改进(减少损失)。总体思路是这样的。</p><blockquote class="mh mi mj"><p id="47c6" class="kf kg lb kh b ki kj jr kk kl km ju kn mk kp kq kr ml kt ku kv mm kx ky kz la ij bi translated">通过巧妙地将样本分成子组，然后为每个子组找到oo(使用上面的方法)，可以进一步提高模型的性能(损失可以更低)。</p></blockquote><p id="bff0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以使用分割条件来分割样本。例如，如果分割条件是<em class="lb">“特征x小于10”</em>，特征x的值小于10的样本将进入一个子组，其余的样本进入另一个组。如果需要，每个子组可以进一步迭代划分。这些分裂将原始特征空间分成更小的子空间，并且每个子空间中的样本形成一个子组。对于每个子组，其最优的<em class="lb">o、</em>和损耗可以使用上述技术求解。总损失，<em class="lb"> Los </em> s，是每个子组(决策树中的叶子)损失的总和。</p><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/196d2f2c847a9a49a361348a86d9b3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*8EDoPxbT__OP5aJH.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">作者图片</p></figure><p id="85cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对所讨论的概念的说明:在这个例子中，特征空间被分成3段，具有分裂<em class="lb"> B &lt; 2 </em>和<em class="lb"> A &lt; 2.5 </em>。然后，使用所讨论的技术计算每个子组的最佳<em class="lb"> o </em>。</p><p id="6750" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个组或子组中，是否拆分以及如果拆分，使用哪个拆分的决定取决于拆分是否可以减少该组的损失以及每次拆分可以减少多少损失。我们总是选择使<em class="lb">损失</em>最小的分割，如果<em class="lb">损失</em>不能减少，我们就不分割。</p><p id="7221" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们直观地描述一下正在发生的事情。当前模型在特征空间的不同部分具有不同程度的误差。它对一些样本预测过高，对另一些样本预测过低，预测的幅度各不相同。<em class="lb">通过分割特征空间，使得每个子组中的误差相似、更具体，从而可以为每个子组计算更好的调整，增强整体模型性能。</em></p><h2 id="d5cd" class="mn ll iq bd lm mo mp dn lq mq mr dp lu ko ms mt lw ks mu mv ly kw mw mx ma my bi translated">过度拟合</h2><p id="5e2e" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">为了防止<em class="lb">过度配合</em>，实施了几项措施。我们在这里讨论两个重要的问题。首先，用户可以设定树木生长的最大高度。这有助于限制可以形成的子群(叶子)的数量。第二，分割造成的损失减少必须超过用户为XGBoost设置的某个阈值，XGBoost才会允许。这通过附加的正则化项γT被建模到<em class="lb">损失函数</em>中，其中T是叶子的数量。为了防止混淆，前面省略了这一点。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="4ca7" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">最佳化</h1><p id="8a68" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">下面是XGBoost用来提高训练速度和准确性的有趣优化。</p><p id="94b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">用于寻找近似最佳分割的加权分位数草图</strong>—在找到最佳分割之前，我们为每个特征形成一个直方图。直方图仓的边界然后被用作寻找最佳分割的候选点。在加权分位数草图中，根据数据点当前预测的“置信度”为其分配权重，构建直方图，使得每个条柱具有大致相同的总权重(与传统分位数草图中相同数量的点相反)。因此，在模型表现不佳的区域将存在更多的候选点，从而进行更详细的搜索。</p><p id="737b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">并行化</strong>加快树构建过程——当找到最佳分割时，候选点的尝试可以在特征/列级别并行化。例如，内核1可以为<em class="lb">特征A </em>找到最佳分割点及其相应的损耗，而内核2可以为<em class="lb">特征B </em>做同样的事情。最后，我们比较损失，并使用最好的一个作为分割点。</p><p id="6799" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">稀疏感知的分割查找</strong>用于处理稀疏数据——XGBoost通过在树中的每个节点为其指定一个默认方向来处理这种稀疏性，这种稀疏性可能是由单热编码的缺失值或频繁的零条目造成的。默认方向的选择基于哪个更能减少<em class="lb">损失</em>。除此之外，XGBoost确保在查找拆分过程中不会迭代稀疏数据，从而防止不必要的计算。</p><p id="8b07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">硬件优化</strong> — XGBoost将常用的gs和hs存储在缓存中，最大限度降低数据访问成本。当需要使用磁盘时(由于数据不适合内存)，数据会在存储前进行压缩，以一些压缩计算为代价降低IO成本。如果存在多个磁盘，可以对数据进行<a class="ae lc" href="https://en.wikipedia.org/wiki/Shard_(database_architecture)" rel="noopener ugc nofollow" target="_blank">分片</a>以增加磁盘读取吞吐量。</p><p id="b400" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">列和行子采样</strong> —为了减少训练时间，XGBoost提供了仅使用原始数据行的随机采样子集来训练每棵树的选项，其中该子集的大小由用户确定。这同样适用于数据集的列/特征。除了节省训练时间之外，在训练期间对列进行子采样还具有对树进行去相关的效果，这可以减少过拟合并提高模型性能。这个想法也用在了<a class="ae lc" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>算法中。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="ba76" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">尾注和参考文献</h1><p id="324f" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">干杯，我们已经到达终点。希望这对你有所帮助。随时给我发电子邮件(liangweitan300895@gmail.com)寻求反馈(我会喜欢他们)，问题，甚至聊天。</p><p id="9f40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] T. Chen和C. Guestrin，<a class="ae lc" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank">“XGBoost:一种可扩展的树提升系统”</a> (2016)，编号arXiv:1603.02754 [cs .LG] <br/> [2] J. H .弗里德曼，<a class="ae lc" href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf" rel="noopener ugc nofollow" target="_blank">“随机梯度推进”</a> (1999)</p></div></div>    
</body>
</html>