<html>
<head>
<title>High-Level History of NLP Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 模型的高级历史</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7?source=collection_archive---------20-----------------------#2020-01-13">https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7?source=collection_archive---------20-----------------------#2020-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="931c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们是如何达到目前 NLP 任务的基于注意力的变压器架构的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/12727d8ecd4f2d5a25cba3dcae1b36a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74dGEgtLHTfQgXpwhSeVZg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/photos/small-toy-figurine-cartoon-3871893/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="bebc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使计算机能够理解人类语言的自然语言处理(NLP)并不是一个新概念。然而，过去十年见证了 NLP 技术进步的前所未有的飞跃，其中大部分进步是由深度学习实现的。NLP 技术发展如此之快，以至于数据科学家必须<em class="lv">不断</em>学习新的机器学习技术和模型架构。令人欣慰的是，自从发展了当前最先进的 NLP 架构，即基于注意力的模型，NLP 领域的进展似乎暂时放缓了。数据科学家终于有机会赶上了！</p><p id="a1a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们是如何达到 NLP 的当前状态的呢？第一个重大进步出现在<strong class="lb iu"> 2013 </strong>对 Word2Vec 的突破性研究(详细内容见 Mikolov 的一篇<a class="ae ky" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">论文)。Mikolov 等人意识到，当他们在 NLP 任务中训练神经网络时，网络被迫学习单词之间的相似性。这些单词的矢量表示存储在神经网络的嵌入层中，它们的发现为 NLP 任务增加了一个全新的维度。多亏了 Word2Vec，我们现在有了一种更有效的方法来创建单词向量。我们不再需要依赖传统的单词稀疏表示和一个热编码。此外，利用单词嵌入需要更少的内存，减少计算时间，并已显示出极大地改善下游模型性能。其他单词表示模型，如 GloVe，也随之出现。不再有一个热编码！</a></p><p id="87b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于深度学习的进步和不断增加的计算能力，递归神经网络(RNN)和长短期记忆网络(LSTM)，一种 RNN 的版本，在 2014 年<strong class="lb iu">和 2015 年</strong>越来越受欢迎。安德烈<a class="ae ky" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">卡帕西的博客文章</a>题为“循环神经网络的不合理的有效性”是一封写给 RNN 的著名且被广泛引用的情书。RNN 和 LSTM 使文本序列数据的处理成为可能。数据的顺序与序列数据有关，在 RNN 之前没有好的方法来处理序列数据。LSTM 对 RNN 的改进在于，对于长序列，网络会记住更早的序列输入。这对于 RNN 氏症来说是一个重大问题，也称为消失梯度问题。LSTM 记住了序列中哪些信息是重要的，并防止早期输入的权重降低到零。还有一个额外版本的 RNN，称为门控循环单位(GRU)。它与 LSTM 非常相似，但不同之处在于它有保留长序列信息的特殊门。几年来，RNN 和 LSTM 是 NLP 任务的支柱——每个人都使用他们。但是没过多久，它们就被一个更好的架构取代了:注意力网络！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/9bf5d4c1db6eafc7c900bc0fe9e5e71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ufpqKFAL8mYD65rGrcY8tg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/illustrations/banner-header-attention-caution-1165973/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="72b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于注意力的网络在 2015 年<strong class="lb iu">到 2016 年</strong>期间变得流行起来。注意力网络是一种神经网络，允许关注数据输入的特定子集:你可以指定你希望网络关注什么。这些模型已经打破了许多 NLP 任务的性能记录，如神经机器翻译、语言建模和问答问题。注意力网络也更有效，需要更少的计算资源。这是一个重要的改进，因为它经常需要大量的 GPU 形式的计算能力(这并不总是可用的)来训练 RNN 氏症。</p><p id="aa99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu"> 2017 </strong>中引入的一种特定类型的基于注意力的网络，即 Transformer 模型，在现代 NLP 架构中尤其占主导地位。转换器与 RNN 的相似之处在于它处理序列数据，但是数据不需要以任何特定的顺序输入到模型中。因此，Transformer 模型可以使用并行化更快地训练更多的数据。Transformer 模型导致了我们 NLP 目前的状态:BERT、ERNIE 2.0 和 XLNet 的时代。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/83ca0d4bdd803b2f1f99535a72235a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkuwkSFE7pCgZWdzBKE6BQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/photos/universal-studios-singapore-2413365/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ae05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自变形金刚(BERT)模型的双向编码器表示由谷歌的研究人员在<strong class="lb iu"> 2018 </strong> 中<a class="ae ky" href="https://arxiv.org/abs/1810.04805v2" rel="noopener ugc nofollow" target="_blank">引入。BERT 版本是可用的最先进的 NLP 模型之一。BERT 是一个深度双向无监督模型，用于预先训练单词表示，以便稍后在 NLP 任务中使用。双向在神经网络中至关重要，因为它允许信息在模型训练时前后流动，从而提高模型性能。</a></p><p id="2406" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然 BERT 的概念类似于 Word2Vec 和 GloVe，但是 BERT 单词向量是上下文敏感的！使用 Word2Vec 和 GloVe，具有高度上下文多样性的单词(我感觉<em class="lv">蓝色</em>，<em class="lv">蓝色</em>是我最喜欢的颜色)由单个向量表示。您可以猜测这种类型的表示可能会导致下游的模型性能不佳，因为单词的含义严重依赖于上下文。有了 BERT，单词<em class="lv"> blue </em>的两个上下文将会用不同的向量来表示。</p><p id="a8d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT 只是基于注意力的架构的冰山一角。在<strong class="lb iu"> 2019 </strong>年，卡内基梅隆大学和谷歌<a class="ae ky" href="https://arxiv.org/pdf/1906.08237.pdf" rel="noopener ugc nofollow" target="_blank">的研究人员创建了 XLNet </a>。这篇论文声称 XLNet“在 20 个任务上超过了 BERT，而且经常是大幅度超过。”与 NLP 的其他最新进展不同，其架构并没有太大的不同。像伯特一样，XLNet 利用了一个基于注意力的网络。在<strong class="lb iu"> 2019 </strong>的夏天，一家中国科技公司在另一个基于注意力的网络 ERNIE 2.0 上发表了一篇<a class="ae ky" href="https://pdfs.semanticscholar.org/9025/1aa6225fcd5687542eab5819db18afb6a20f.pdf?_ga=2.197402415.1293825788.1578939927-1092130271.1578939927" rel="noopener ugc nofollow" target="_blank">论文</a>。该论文声称 ERNIE 2.0 在 16 个任务上优于 BERT 和 XLNet，包括中文任务。像 BERT 一样，ERNIE 2.0 和 XLNet 都是预训练模型，利用了 transformer 架构和注意机制。虽然最初的 BERT 模型不再是王者，但诸如<a class="ae ky" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa </a>之类的 BERT 版本在 NLP 领先技术领域仍然具有竞争力。</p><p id="5df6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，目前没有单一的、整体最佳的 NLP 模型。然而，基于注意力的变压器网络是占主导地位的架构。顶级模特在不同的任务上表现出色，每个都有自己独特的优点和缺点。面对所有这些相互竞争的模型，很难找出哪个模型最适合您的任务。我最喜欢的新资源之一是 paperswithcode.com。该网站根据特定的机器学习任务方便地组织研究论文，使您能够了解最新的模型和架构。</p><div class="lx ly gp gr lz ma"><a href="https://paperswithcode.com/area/natural-language-processing" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">浏览最先进的 ML</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">带代码的论文突出了 ML 研究的趋势和实现它的代码。</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">paperswithcode.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo ks ma"/></div></div></a></div><p id="b8f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是你要的 NLP 在过去十年中快速发展的简史。NLP 是一个不断变化和发展的领域，当然不适合喜欢模型稳定性的数据科学家。但这也是乐趣的一部分！我们将会看到基于注意力的网络时代会持续多久。</p></div></div>    
</body>
</html>