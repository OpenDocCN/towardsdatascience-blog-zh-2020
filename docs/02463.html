<html>
<head>
<title>How exactly does PCA work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA 到底是怎么工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-exactly-does-pca-work-5c342c3077fe?source=collection_archive---------10-----------------------#2020-03-09">https://towardsdatascience.com/how-exactly-does-pca-work-5c342c3077fe?source=collection_archive---------10-----------------------#2020-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b3f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最简单的 PCA 指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/16ac9b025db0dabf8cee04f02866ab02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Qw85l7pRnGYZ5k3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@pritesh557?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pritesh Sudra </a>拍摄</p></figure><blockquote class="kz"><p id="d10e" class="la lb it bd lc ld le lf lg lh li lj dk translated">主成分分析是将大量数据压缩成抓住数据本质的东西的过程。</p></blockquote></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h2 id="3092" class="lr ls it bd lt lu lv dn lw lx ly dp lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">直觉</h2><p id="a35e" class="pw-post-body-paragraph mn mo it mp b mq mr ju ms mt mu jx mv ma mw mx my me mz na nb mi nc nd ne lj im bi translated"><strong class="mp iu"> PCA </strong>(主成分分析)是一种在数据中寻找主要<em class="nf">模式</em>进行<em class="nf">降维</em>的技术。第一次读到这一行可能会引发几个问题:</p><p id="052c" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">这些图案是什么？</p><p id="de83" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">如何找到这些模式？</p><p id="298e" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">什么是降维？</p><p id="12d7" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">维度到底是什么？</p><p id="1fd2" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">为什么要减少它们？</p><p id="ca0c" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">让我们一个一个地去看看他们。</p><p id="c4cd" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">假设我们有一个数据集，比如说，有 300 列。所以我们的数据集有 300 个维度<strong class="mp iu"/>。干净利落。但是我们真的需要这么多维度吗？我们可以。但大多数时候，我们没有。因此，我们需要找到一种快速简单的方法，不仅删除一定数量的特征，而且在更少数量的<em class="nf">转换的</em>维度中捕获 300 个维度的数据的本质。</p><h2 id="5bf7" class="lr ls it bd lt lu lv dn lw lx ly dp lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">差异</h2><p id="b705" class="pw-post-body-paragraph mn mo it mp b mq mr ju ms mt mu jx mv ma mw mx my me mz na nb mi nc nd ne lj im bi translated">这 300 个特征中的每一个都有一定量的变化，即整个值的变化。如果一个特征描述了特定建筑物 200 天的楼层数，那么它的方差将为 0。因为它的值始终没有变化。方差为 0 的特征是没有用的，因为它们不提供洞察力。所以，方差的确是我们的朋友！这就是我之前提到的模式。</p><p id="ab9e" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">差异越大，该特性的重要性就越大。因为它包含更多的“信息”。方差为 0 的变量包含 0 个信息。不要混淆方差和相关性！方差与数据的目标变量无关。它只是陈述特定特性的值在整个数据中是如何变化的。</p><h2 id="7a02" class="lr ls it bd lt lu lv dn lw lx ly dp lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">主成分</h2><p id="c45f" class="pw-post-body-paragraph mn mo it mp b mq mr ju ms mt mu jx mv ma mw mx my me mz na nb mi nc nd ne lj im bi translated">现在我们知道了方差，我们需要找到一组新的转换特征集，它可以更好地解释方差。原始的 300 个特征被用来进行特征的线性组合，以便将所有的变化推入几个变换的特征中。这些变换后的特征称为主成分。</p><p id="a23c" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">主要部件现在与原始特征无关。我们将从 300 个特征中得到 300 个主成分。现在 PCA 的妙处来了——新形成的变换特征集或主成分将具有第一个 PC 中解释的最大方差。第二台 PC 将具有第二高的方差，依此类推。</p><p id="a163" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">例如，如果第一个 PC 解释了数据中总方差的 68%,第二个特征解释了总方差的 15%,接下来的 4 个特征总共包含 14%的方差。所以你有 97%的方差<em class="nf">可以用 6 个主成分来解释</em>！现在，假设接下来的 100 个特征总共解释了总方差的另外 1%。现在，仅仅为了增加一个百分比的方差而增加 100 个维度已经没有什么意义了。通过只取前 6 个主成分，我们将维数从 300 减少到仅仅 6！</p><h2 id="ef63" class="lr ls it bd lt lu lv dn lw lx ly dp lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">特征向量和特征值</h2><p id="2bda" class="pw-post-body-paragraph mn mo it mp b mq mr ju ms mt mu jx mv ma mw mx my me mz na nb mi nc nd ne lj im bi translated">现在让我们考虑一个更简单的例子，它只有两个特征，更容易理解。下图是我们用特征 2 绘制特征 1 的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/420f48c88f3c3d0ab1ce4bd0f4e3e4de.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*jmdAPilKzJconrU5KKtPtQ.png"/></div></figure><p id="8351" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">PCA(具有<a class="ae ky" rel="noopener" target="_blank" href="/svd-8c2f72e264f"> SVD </a>的)所做的是，它为这些数据点找到最佳拟合线，该最佳拟合线使<strong class="mp iu">最小化</strong>数据点与其在最佳拟合线上的投影之间的距离。现在，考虑特征 1 和特征 2 的数据点的平均值。它将在 a 附近的某个地方。因此，同样地，PCA 也可以<strong class="mp iu">最大化</strong>最佳拟合线上的投影点与点 a 的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/252b85274436f58a62565211246c4133.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*_5RR2lNBebEl8rZ9M3ZunA.png"/></div></figure><p id="e0d7" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">移动直线，使 A 点与原点重合，这样更容易观察。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/992f8a11d3f5ef2fb591c803e5084fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*I1q2EtpK84enxwJLcETP0A.png"/></div></figure><p id="4400" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">距离 d1 是点 1 相对于原点的距离。类似地，d2、d3、d4、d5、d6 将是投影点离原点的相应距离。最佳拟合线将具有最大的距离平方和。假设直线的斜率为 0.25。这意味着该线由要素 1 的 4 部分和要素 2 的 1 部分组成。这将类似于:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4b6472be88f34217c48bec15c4c1946e.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*z7ODXc_i6pktEp-h0LeDFw.png"/></div></figure><p id="b013" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">其中 B=4 &amp; C=1。因此，我们可以很容易地通过毕达哥拉斯定理找到 A，结果是 4.12。PCA 缩放这些值，使得向量 A 为单位长度。因此 A=1，B=4/4.12 = 0.97 &amp; C=1/4.12 = 0.242。这个单位向量 A 就是<strong class="mp iu">特征向量！</strong>距离 d1、d2、d3、d4、d5、d6 的平方和为<strong class="mp iu">特征值。</strong>相当直接！这就是我前面说的特征 1 和特征 2 的<strong class="mp iu">线性组合</strong>。这告诉我们，对于 PC1，功能 1 的重要性几乎是功能 2 的 4 倍，或者说，它包含的数据分布(变化)几乎是功能 2 的 4 倍。</p><p id="01c4" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">现在，主分量 2 将是与 PC1 正交的向量，因为主分量之间具有 0 相关性。这就像红线一样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/62301a79fb8aabbcf9e638fe1b62811f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*cBOKB7UpeBJ-iYbB7X5t4g.png"/></div></figure><p id="f401" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">根据类似的理解，PC2 将具有特征 1 的-0.242 份和特征 2 的 0.97 份。这告诉我们，对于 PC2 来说，特性 2 的重要性几乎是特性 1 的 4 倍。对于 PC2，可以类似地计算特征向量和特征值。所以我们终于找到了我们的主要成分！</p><h2 id="797d" class="lr ls it bd lt lu lv dn lw lx ly dp lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">解释方差</h2><p id="1307" class="pw-post-body-paragraph mn mo it mp b mq mr ju ms mt mu jx mv ma mw mx my me mz na nb mi nc nd ne lj im bi translated">我们计算了两个主成分的距离平方和。如果我们将这些值除以 n-1(其中 n 是样本大小)，我们将得到相应主成分的方差。</p><p id="1d8b" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">假设 PC1 的方差为 15，PC2 的方差为 3。因此，围绕两个主成分的总变化是 18。因此，PC1 占 15/18，相当于数据总方差的 0.83 或 83%。PC2 占 3/18，等于数据中总方差的 0.17 或 17%。这就是<strong class="mp iu">解释的方差比</strong>。这表明数据中有多少差异是由特定的主成分解释的。主成分按其解释的方差比排序。如果总的解释方差比率达到足够的值，我们可以选择前 m 个成分。</p><p id="97ce" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">主成分分析降低了维数以克服过拟合。您的模型可能不需要所有的功能来提供良好的性能。它可能会给出很高的训练分数，但测试分数却很低。换句话说，它可能<strong class="mp iu">过度适应</strong>。PCA 不是特征选择或特征消除技术。这更像是一种 T4 特征提取技术。您也可以将它归入特征工程范畴。</p><p id="1f72" class="pw-post-body-paragraph mn mo it mp b mq ng ju ms mt nh jx mv ma ni mx my me nj na nb mi nk nd ne lj im bi translated">这就是本文的全部内容。请参考以下优秀资源，了解更多信息:</p><ul class=""><li id="b732" class="nq nr it mp b mq ng mt nh ma ns me nt mi nu lj nv nw nx ny bi translated"><a class="ae ky" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2014 _ PCA _ step _ by _ step . html</a></li><li id="46a8" class="nq nr it mp b mq nz mt oa ma ob me oc mi od lj nv nw nx ny bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=FgakZw6K1QQ" rel="noopener ugc nofollow" target="_blank"> StatQuest:主成分分析(PCA)，逐步</a></li><li id="dd28" class="nq nr it mp b mq nz mt oa ma ob me oc mi od lj nv nw nx ny bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></li></ul></div></div>    
</body>
</html>