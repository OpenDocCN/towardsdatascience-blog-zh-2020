<html>
<head>
<title>Machine Learning with Python: Regression (complete tutorial)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 机器学习:回归(完整教程)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-python-regression-complete-tutorial-47268e546cea?source=collection_archive---------2-----------------------#2020-05-18">https://towardsdatascience.com/machine-learning-with-python-regression-complete-tutorial-47268e546cea?source=collection_archive---------2-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="4761" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">数据分析和可视化、特征工程和选择、模型设计和测试、评估和解释</h2></div></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="8ce8" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">摘要</h2><p id="d234" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在本文中，我将使用数据科学和 Python 解释回归用例的主要步骤，从数据分析到理解模型输出。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/29f211b020ad5f116090ac99e99bf171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zStUKzcnypSkwr5x2l_sXA.gif"/></div></div></figure><p id="0ad3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将展示一些有用的 Python 代码，这些代码可以很容易地用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，这样您就可以很容易地复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu mz"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">mdipietro 09/data science _ 人工智能 _ 实用工具</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">permalink dissolve GitHub 是 4000 多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">github.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ja mz"/></div></div></a></div><p id="8886" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将使用“<strong class="ly jf">房价数据集</strong>”(链接如下)，其中为您提供了描述一些住宅不同方面的多个解释变量，任务是预测每套住宅的最终价格。</p><div class="is it gp gr iu mz"><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">房价:高级回归技术</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">预测销售价格并实践特征工程、RFs 和梯度推进</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.kaggle.com</p></div></div><div class="ni l"><div class="no l nk nl nm ni nn ja mz"/></div></div></a></div><p id="4b11" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">特别是，我将经历:</p><ul class=""><li id="6a75" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">环境设置:导入库并读取数据</li><li id="bd95" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">数据分析:理解变量的意义和预测能力</li><li id="d836" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">特征工程:从原始数据中提取特征</li><li id="39c1" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">预处理:数据划分、处理缺失值、编码分类变量、缩放</li><li id="5527" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">特征选择:只保留最相关的变量</li><li id="bc1a" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">模型设计:基线、训练、验证、测试</li><li id="7e0b" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绩效评估:阅读指标</li><li id="b83c" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">可解释性:理解模型如何做出预测</li></ul></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="9ffd" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">设置</h2><p id="1cd4" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">首先，我需要导入以下库。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="2b7a" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## for data</strong><br/>import <strong class="oe jf">pandas </strong>as pd<br/>import <strong class="oe jf">numpy </strong>as np</span><span id="042c" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for plotting</strong><br/>import <strong class="oe jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="oe jf">seaborn </strong>as sns</span><span id="55ef" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for statistical tests</strong><br/>import <strong class="oe jf">scipy</strong><br/>import <strong class="oe jf">statsmodels</strong>.formula.api as smf<br/>import statsmodels.api as sm</span><span id="2214" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for machine learning</strong><br/>from <strong class="oe jf">sklearn </strong>import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition</span><span id="c8fd" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for explainer</strong><br/>from <strong class="oe jf">lime </strong>import lime_tabular</span></pre><p id="f5f9" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">然后我会把数据读入一个<em class="on">熊猫</em>数据帧。原始数据集包含 81 列，但是出于本教程的目的，我将使用 12 列的子集。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="6d85" class="la lb je oe b gy oi oj l ok ol">dtf = pd.read_csv("data_houses.csv")</span><span id="5f52" class="la lb je oe b gy om oj l ok ol">cols = ["OverallQual","GrLivArea","GarageCars", <br/>        "GarageArea","TotalBsmtSF","FullBath",<br/>        "YearBuilt","YearRemodAdd",<br/>        "LotFrontage","MSSubClass"]</span><span id="e450" class="la lb je oe b gy om oj l ok ol">dtf = dtf[["Id"]+cols+["SalePrice"]]<br/>dtf.head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/334b7aeb4f9175f7ec98e39a2132317e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VO8Ek9ZpXbgLC23NQ41HvA.png"/></div></div></figure><p id="41c3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有关列的详细信息可以在所提供的数据集链接中找到。</p><p id="60d4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">请注意，表格的每一行代表一个特定的房子(或观察)。如果您正在处理一个不同的数据集，它没有这样的结构，其中每一行代表一个观察，那么您需要汇总数据并转换它。</p><p id="c7c3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">现在一切都设置好了，我将从分析数据开始，然后选择特征，建立机器学习模型并进行预测。</p><p id="ddea" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我们开始吧，好吗？</p><h2 id="268c" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">数据分析</h2><p id="b497" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在统计学中，<a class="ae op" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank">探索性数据分析</a>是对数据集的主要特征进行总结的过程，以了解数据在正式建模或假设检验任务之外还能告诉我们什么。</p><p id="acd6" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我总是从获得整个数据集的概述开始，特别是，我想知道有多少个<strong class="ly jf">分类</strong>和<strong class="ly jf">数值</strong>变量以及<strong class="ly jf">缺失数据</strong>的比例。识别变量的类型有时会很棘手，因为类别可以用数字表示。为此，我将编写一个简单的函数来完成这项工作:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="86fd" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">'''<br/>Recognize whether a column is numerical or categorical.<br/>:parameter<br/>    :param dtf: dataframe - input data<br/>    :param col: str - name of the column to analyze<br/>    :param max_cat: num - max number of unique values to recognize a column as categorical<br/>:return<br/>    "cat" if the column is categorical or "num" otherwise<br/>'''</strong><br/>def <strong class="oe jf">utils_recognize_type</strong>(dtf, col, max_cat=20):<br/>    if (dtf[col].dtype == "O") | (dtf[col].nunique() &lt; max_cat):<br/>        return <strong class="oe jf">"cat"</strong><br/>    else:<br/>        return <strong class="oe jf">"num"</strong></span></pre><p id="ad1a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这个功能非常有用，可以用在很多场合。为了举例说明，我将绘制 dataframe 的<a class="ae op" href="http://Heat map" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">热图</strong> </a> <strong class="ly jf"> </strong>，并可视化列类型和缺失的数据。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="0200" class="la lb je oe b gy oi oj l ok ol">dic_cols = {col:<strong class="oe jf">utils_recognize_type</strong>(dtf, col, max_cat=20) for col in dtf.columns}</span><span id="3a0b" class="la lb je oe b gy om oj l ok ol">heatmap = dtf.isnull()<br/>for k,v in dic_cols.items():<br/> if v == "num":<br/>   heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)<br/> else:<br/>   heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)</span><span id="b6fd" class="la lb je oe b gy om oj l ok ol">sns.<strong class="oe jf">heatmap</strong>(heatmap, cbar=False).set_title('Dataset Overview')<br/>plt.show()</span><span id="12a0" class="la lb je oe b gy om oj l ok ol">print("\033[1;37;40m Categerocial ", "\033[1;30;41m Numeric ", "\033[1;30;47m NaN ")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/40d6b603311fa00d7a35c32e0597d8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njWVRZTq8telya98NhT5Pw.png"/></div></div></figure><p id="f7a8" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有 1460 行和 12 列:</p><ul class=""><li id="f0a0" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">表中的每一行都代表一个由<em class="on"> Id </em>标识的特定房屋(或观察)，所以我将它设置为索引(或者为 SQL 爱好者设置表的<a class="ae op" href="https://en.wikipedia.org/wiki/Primary_key" rel="noopener ugc nofollow" target="_blank">主键</a>)。</li><li id="1eb1" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated"><em class="on"> SalePrice </em>是我们想要了解和预测的因变量，所以我将该列重命名为“<em class="on">Y”</em>。</li><li id="7a54" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated"><em class="on">总体质量、车库、全浴</em>和<em class="on">ms 子类</em>是分类变量，其他是数值变量。</li><li id="0e93" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">只有<em class="on"> LotFrontage </em>包含缺失数据。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="ecac" class="la lb je oe b gy oi oj l ok ol">dtf = dtf.set_index("<strong class="oe jf">Id</strong>")</span><span id="5934" class="la lb je oe b gy om oj l ok ol">dtf = dtf.rename(columns={"<strong class="oe jf">SalePrice</strong>":"<strong class="oe jf">Y</strong>"})</span></pre><p id="dcb5" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我相信可视化是数据分析的最好工具，但是你需要知道什么样的图更适合不同类型的变量。因此，我将提供代码来为不同的示例绘制适当的可视化。</p><p id="2ead" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">首先，让我们看看单变量分布(只有一个变量的概率分布)。一个<a class="ae op" href="https://en.wikipedia.org/wiki/Histogram" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">直方图</strong> </a>完美地给出了单个<strong class="ly jf">数值</strong>数据的底层分布密度的粗略感觉。我推荐使用一个<a class="ae op" href="https://en.wikipedia.org/wiki/Box_plot" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">方框图</strong> </a> <strong class="ly jf"> </strong>来图形化地描绘数据组通过它们的四分位数。例如，让我们绘制目标变量:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="c425" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x = "Y"</strong></span><span id="01ae" class="la lb je oe b gy om oj l ok ol">fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)<br/>fig.suptitle(x, fontsize=20)</span><span id="4694" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### distribution</strong><br/>ax[0].title.set_text('distribution')<br/>variable = dtf[x].fillna(dtf[x].mean())<br/>breaks = np.quantile(variable, q=np.linspace(0, 1, 11))<br/>variable = variable[ (variable &gt; breaks[0]) &amp; (variable &lt; <br/>                    breaks[10]) ]<br/>sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])<br/>des = dtf[x].describe()<br/>ax[0].axvline(des["25%"], ls='--')<br/>ax[0].axvline(des["mean"], ls='--')<br/>ax[0].axvline(des["75%"], ls='--')<br/>ax[0].grid(True)<br/>des = round(des, 2).apply(lambda x: str(x))<br/>box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))<br/>ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))</span><span id="54bd" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### boxplot </strong><br/>ax[1].title.set_text('outliers (log scale)')<br/>tmp_dtf = pd.DataFrame(dtf[x])<br/>tmp_dtf[x] = np.log(tmp_dtf[x])<br/>tmp_dtf.boxplot(column=x, ax=ax[1])<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/f9aac63ecdb9248d46fc3e185b1d603c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xu2J6Pt9H0XuMNp6P-W-8g.png"/></div></div></figure><p id="6f5b" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这个群体的平均房价是 181，000 美元，分布是高度偏斜的，两边都有异常值。</p><p id="9450" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">此外，一个<a class="ae op" href="https://en.wikipedia.org/wiki/Bar_chart" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">条形图</strong> </a> <strong class="ly jf"> </strong>适于理解单个<strong class="ly jf">分类</strong>变量的标签频率。让我们以<em class="on">全浴室</em>(浴室数量)变量为例:它具有普通性(2 个浴室&gt; 1 个浴室)，但它不是连续的(一个家庭不可能有 1.5 个浴室)，所以它可以作为一个范畴来分析。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="577e" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x = "Y"</strong></span><span id="01c6" class="la lb je oe b gy om oj l ok ol">ax = dtf[x].value_counts().sort_values().plot(kind="barh")<br/>totals= []<br/>for i in ax.patches:<br/>    totals.append(i.get_width())<br/>total = sum(totals)<br/>for i in ax.patches:<br/>     ax.text(i.get_width()+.3, i.get_y()+.20, <br/>     str(round((i.get_width()/total)*100, 2))+'%', <br/>     fontsize=10, color='black')<br/>ax.grid(axis="x")<br/>plt.suptitle(x, fontsize=20)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/45933eb55f8567a4f718ff33b65b8399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*yU9Iq2pBFi2SdZeotypNjA.png"/></div></figure><p id="407a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">大多数房子有 1 个或 2 个浴室，也有一些离群值有 0 个和 3 个浴室。</p><p id="55b0" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将把分析带到下一个层次，并研究二元分布，以了解<em class="on"> FullBath </em>是否具有预测<em class="on"> Y </em>的预测能力。这就是<strong class="ly jf">分类(<em class="on">全浴</em>)对数字(<em class="on"> Y </em> ) </strong>的情况，因此我将这样进行:</p><ul class=""><li id="bf38" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">将人口(整套观察结果)分成 4 个样本:有 0 间浴室(<em class="on">全浴= 0)、</em> 1 间<em class="on"> </em>浴室(<em class="on">全浴= 1) </em>的房屋部分，以此类推…</li><li id="d1be" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绘制并比较 4 个样本的密度，如果分布不同，则变量是可预测的，因为 4 组具有不同的模式。</li><li id="69b1" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">将数值变量(<em class="on"> Y </em>)分组到箱(子样本)中，并绘制每个箱的组成，如果所有箱中类别的比例相似，则该变量不具有预测性。</li><li id="d186" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绘制并比较 4 个样本的箱线图，找出异常值的不同行为。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="d80c" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">cat, num = "FullBath", "Y"</strong></span><span id="b3a7" class="la lb je oe b gy om oj l ok ol">fig, ax = plt.subplots(nrows=1, ncols=3,  sharex=False, sharey=False)<br/>fig.suptitle(x+"   vs   "+y, fontsize=20)<br/>            <br/><strong class="oe jf">### distribution</strong><br/>ax[0].title.set_text('density')<br/>for i in dtf[cat].unique():<br/>    sns.distplot(dtf[dtf[cat]==i][num], hist=False, label=i, ax=ax[0])<br/>ax[0].grid(True)</span><span id="a546" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### stacked</strong><br/>ax[1].title.set_text('bins')<br/>breaks = np.quantile(dtf[num], q=np.linspace(0,1,11))<br/>tmp = dtf.groupby([cat, pd.cut(dtf[num], breaks, duplicates='drop')]).size().unstack().T<br/>tmp = tmp[dtf[cat].unique()]<br/>tmp["tot"] = tmp.sum(axis=1)<br/>for col in tmp.drop("tot", axis=1).columns:<br/>     tmp[col] = tmp[col] / tmp["tot"]<br/>tmp.drop("tot", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)</span><span id="5457" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### boxplot </strong>  <br/>ax[2].title.set_text('outliers')<br/>sns.catplot(x=cat, y=num, data=dtf, kind="box", ax=ax[2])<br/>ax[2].grid(True)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/66c639c586c9d7e96f59be1341f867f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfBZRuHdbZAnPWNGhefDXw.png"/></div></div></figure><p id="1383" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">FullBath 似乎具有预测性，因为 4 个样本的分布在价格水平和观察次数上有很大不同。房子里的浴室越多，价格就越高，但我想知道 0 浴室样本和 3 浴室样本中的观察值是否有统计学意义，因为它们包含的观察值很少。</p><p id="d8d4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">当“视觉直觉”无法说服你时，你可以求助于古老的统计数据来进行测试。在这种分类(<em class="on">full bath</em>vs 数值(<em class="on"> Y </em>)的情况下，我会使用一个<strong class="ly jf">o</strong><a class="ae op" href="http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example" rel="noopener ugc nofollow" target="_blank"><strong class="ly jf">n-way ANOVA 检验</strong> </a>。基本上是检验两个或两个以上独立样本的均值是否显著不同，所以如果 p 值足够小(&lt; 0.05)样本的零假设意味着相等可以被拒绝。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="4bb8" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">cat, num = "FullBath", "Y"</strong></span><span id="f28b" class="la lb je oe b gy om oj l ok ol">model = smf.<strong class="oe jf">ols</strong>(num+' ~ '+cat, data=dtf).fit()<br/>table = sm.stats.<strong class="oe jf">anova_lm</strong>(model)<br/>p = table["PR(&gt;F)"][0]<br/>coeff, p = None, round(p, 3)<br/>conclusion = "Correlated" if p &lt; 0.05 else "Non-Correlated"<br/>print("Anova F: the variables are", conclusion, "(p-value: "+str(p)+")")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ce15c45655e68aa8ccd66c50e471675d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*5oD8eoU4K_geb9J4ntsoQA.png"/></div></figure><p id="1ce4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我们可以得出结论，浴室的数量决定了房子的价格。这是有道理的，因为更多的浴室意味着更大的房子，而房子的大小是一个重要的价格因素。</p><p id="8a34" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">为了检查第一个结论的有效性，我必须分析目标变量相对于<em class="on"> GrLivArea </em>(以平方英尺为单位的地上居住面积)的行为。这是一个<strong class="ly jf">数值(<em class="on"> GrLivArea </em> ) vs 数值(<em class="on"> Y </em> ) </strong>的例子，所以我将生成两个图:</p><ul class=""><li id="70d6" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">首先，我将把<em class="on"> GrLivArea </em>值分组到箱中，并比较每个箱中<em class="on"> Y </em>的平均值(和中值),如果曲线不平坦，则变量是预测性的，因为箱具有不同的模式。</li><li id="890a" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">第二，我将使用散点图，图中两边是两个变量的分布。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="5e31" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x, y = "GrLivArea", "Y"</strong></span><span id="2bfc" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### bin plot<br/></strong>dtf_noNan = dtf[dtf[x].notnull()]<br/>breaks = np.quantile(dtf_noNan[x], q=np.linspace(0, 1, 11))<br/>groups = dtf_noNan.groupby([pd.cut(dtf_noNan[x], bins=breaks, <br/>           duplicates='drop')])[y].agg(['mean','median','size'])<br/>fig, ax = plt.subplots(figsize=figsize)<br/>fig.suptitle(x+"   vs   "+y, fontsize=20)<br/>groups[["mean", "median"]].plot(kind="line", ax=ax)<br/>groups["size"].plot(kind="bar", ax=ax, rot=45, secondary_y=True,<br/>                    color="grey", alpha=0.3, grid=True)<br/>ax.set(ylabel=y)<br/>ax.right_ax.set_ylabel("Observazions in each bin")<br/>plt.show()</span><span id="e55f" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### scatter plot</strong><br/>sns.jointplot(x=x, y=y, data=dtf, dropna=True, kind='reg', <br/>              height=int((figsize[0]+figsize[1])/2) )<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/4a5f81f0c2a71f05104378aa1b0bc5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MS7PhFMz3FZ7mqPf75CNYw.png"/></div></div></figure><p id="4b5f" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">GrLivArea 是预测性的，有一个清晰的模式:平均来说，房子越大，价格越高，即使有一些超出平均水平的异常值和相对较低的价格。</p><p id="8031" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">就像之前一样，我们可以测试这两个变量之间的相关性。既然都是数值，我就<strong class="ly jf"> t </strong> <a class="ae op" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf"> est 皮尔逊相关系数</strong> </a>:假设两个变量是独立的(零假设)，检验两个样本是否有线性关系。如果 p 值足够小(&lt; 0.05)，可以拒绝零假设，我们可以说这两个变量可能是相关的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="5a03" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x, y = "GrLivArea", "Y"</strong></span><span id="532e" class="la lb je oe b gy om oj l ok ol">dtf_noNan = dtf[dtf[x].notnull()]<br/>coeff, p = scipy.stats.<strong class="oe jf">pearsonr</strong>(dtf_noNan[x], dtf_noNan[y])<br/>coeff, p = round(coeff, 3), round(p, 3)<br/>conclusion = "Significant" if p &lt; 0.05 else "Non-Significant"<br/>print("Pearson Correlation:", coeff, conclusion, "(p-value: "+str(p)+")")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/d03964f25d450aa83f37b3701b9a870a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tx11lU1RgUrBV-sDVBZNlQ.png"/></div></div></figure><p id="fdb0" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><em class="on"> FullBath </em>和<em class="on"> GrLivArea </em>是预测特性的例子，因此我将保留它们用于建模。</p><p id="38cc" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">应该对数据集中的每个变量进行这种分析，以决定哪些应该作为潜在特征保留，哪些因为不具有预测性而可以放弃(查看完整代码的链接)。</p><h2 id="89ad" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">特征工程</h2><p id="1e57" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">是时候使用领域知识从原始数据创建新要素了。我将提供一个例子:<em class="on"> MSSubClass </em>列(building 类)包含 15 个类别，这是一个很大的数目，在建模过程中会导致维度问题。让我们看看:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="dfe9" class="la lb je oe b gy oi oj l ok ol">sns.<strong class="oe jf">catplot</strong>(x="MSSubClass", y="Y", data=dtf, kind="box")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/07caee9f3dea139f9410f1158e28a237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IP6Y5amXUbrGEj1kYfM8Dg.png"/></div></div></figure><p id="09e7" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有许多类别，很难理解每个类别中的分布情况。因此，我将把这些类别分组为簇:具有较高<em class="on"> Y </em>值的类(如<em class="on"> MSSubClass 60 和 120 </em>)将进入“最大”簇，价格较低的类(如<em class="on"> MSSubClass 30、45、180 </em>)将进入“最小”簇，其余的将进入“平均”簇。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="dc70" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## define clusters</strong><br/>MSSubClass_clusters = {"<strong class="oe jf">min</strong>":[30,45,180], "<strong class="oe jf">max</strong>":[60,120], "<strong class="oe jf">mean</strong>":[]}</span><span id="6ff2" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## create new columns</strong><br/>dic_flat = {v:k for k,lst in MSSubClass_clusters.items() for v in lst}<br/>for k,v in MSSubClass_clusters.items():<br/>    if len(v)==0:<br/>        residual_class = k <br/>dtf[x+"_cluster"] = dtf[x].apply(lambda x: dic_flat[x] if x in <br/>                          dic_flat.keys() else residual_class)</span><span id="c6f3" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## print</strong><br/>dtf[["MSSubClass","MSSubClass_cluster","Y"]].head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2e099d81b30f13b2f45c133be166b14c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*zelkHpu2eQ9TBm8grRIaQw.png"/></div></figure><p id="459d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这样，我将类别的数量从 15 个减少到 3 个，这样更便于分析:</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/e556fab1eaf8f886919f9649a88be2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qp6nBGhXpQtAyMfseqdgfg.png"/></div></div></figure><p id="9f5e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">新的分类特性更易于阅读，并且保持了原始数据中显示的模式，因此我将保留<em class="on"> MSSubClass_cluster </em>而不是列<em class="on"> MSSubClass </em>。</p><h2 id="92fe" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">预处理</h2><p id="6c19" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">数据预处理是准备原始数据以使其适合机器学习模型的阶段。特别是:</p><ol class=""><li id="452b" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo pa nv nw nx bi translated">每个观察必须用一行来表示，换句话说，不能用两行来描述同一个乘客，因为它们将被模型分别处理(数据集已经是这样的形式，所以✅).而且每一列都应该是一个特征，所以你不应该用<em class="on"> Id </em>作为预测器，这就是为什么这种表叫做“<strong class="ly jf">特征矩阵</strong>”。</li><li id="c24c" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo pa nv nw nx bi translated">数据集必须<strong class="ly jf">分割</strong>成至少两个集合:模型应该在你的数据集的一个重要部分上训练(所谓的“训练集”)，在一个较小的集合上测试(“测试集”)。</li><li id="23cd" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo pa nv nw nx bi translated"><strong class="ly jf">缺少的值</strong>应该用东西替换，否则，你的模型可能会出问题。</li><li id="ec21" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo pa nv nw nx bi translated"><strong class="ly jf">分类数据</strong>必须编码，这意味着将标签转换为整数，因为机器学习期望的是数字，而不是字符串。</li><li id="1498" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo pa nv nw nx bi translated">对数据进行<strong class="ly jf">缩放</strong>是一种很好的做法，这有助于在特定范围内对数据进行标准化，并加快算法中的计算速度。</li></ol><p id="603e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">好的，让我们从<strong class="ly jf">划分数据集</strong>开始。当把数据分成训练集和测试集时，你必须遵循一个基本规则:训练集中的行不应该出现在测试集中。这是因为模型在训练过程中会看到目标值，并使用它来理解现象。换句话说，模型已经知道训练观察的正确答案，在这些基础上测试就像作弊。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="da07" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## split data</strong><br/>dtf_train, dtf_test = <strong class="oe jf">model_selection</strong>.<strong class="oe jf">train_test_split</strong>(dtf, <br/>                      test_size=0.3)</span><span id="3726" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## print info</strong><br/>print("X_train shape:", dtf_train.drop("Y",axis=1).shape, "| X_test shape:", dtf_test.drop("Y",axis=1).shape)<br/>print("y_train mean:", round(np.mean(dtf_train["Y"]),2), "| y_test mean:", round(np.mean(dtf_test["Y"]),2))<br/>print(dtf_train.shape[1], "features:", dtf_train.drop("Y",axis=1).columns.to_list())</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/154543f6c0d5b567ea6e5cf868f78185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NjgQKEW3Oh00K5ApMJK7A.png"/></div></div></figure><p id="f00d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">下一步:<em class="on"> LotFrontage </em>列包含一些需要处理的<strong class="ly jf">缺失数据</strong> (17%)。从机器学习的角度来看，首先分成训练和测试，然后用训练集的平均值替换<em class="on"> NAs </em>是正确的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="2730" class="la lb je oe b gy oi oj l ok ol">dtf_train["<em class="on">LotFrontage</em>"] = dtf_train["<em class="on">LotFrontage</em>"].<strong class="oe jf">fillna</strong>(dtf_train["<em class="on">LotFrontage</em>"].<strong class="oe jf">mean</strong>())</span></pre><p id="b777" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我创建的新列<em class="on"> MSSubClass_cluster </em>包含应该被编码的<strong class="ly jf">分类数据</strong>。我将使用 One-Hot-Encoding 方法，将 1 个具有 n 个唯一值的分类列转换为 n-1 个虚拟列。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="dca3" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## create dummy</strong><br/>dummy = pd.<strong class="oe jf">get_dummies</strong>(dtf_train["<em class="on">MSSubClass_cluster</em>"], <br/>                       prefix="<em class="on">MSSubClass_cluster</em>",drop_first=True)<br/>dtf_train= pd.concat([dtf_train, dummy], axis=1)<br/>print( dtf_train.filter(like="<em class="on">MSSubClass_cluster</em>",axis=1).head() )</span><span id="6feb" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## drop the original categorical column</strong><br/>dtf_train = dtf_train.drop("<em class="on">MSSubClass_cluster</em>", axis=1)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/2c7b0fb052beeadeb07dffb538ca0f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWpCbfDrNJpithsunVIXDw.png"/></div></div></figure><p id="cb4d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">最后但同样重要的是，我将<strong class="ly jf">缩放特征</strong>。对于回归问题，通常需要转换输入变量和目标变量。我将使用<em class="on">鲁棒定标器</em>，它通过减去中值然后除以四分位数范围(75%值-25%值)来转换特征。这个定标器的优点是它受异常值的影响较小。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="ba55" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## scale X</strong><br/>scalerX = preprocessing.<strong class="oe jf">RobustScaler</strong>(quantile_range=(25.0, 75.0))<br/>X = scaler.fit_transform(dtf_train.drop("Y", axis=1))</span><span id="d15a" class="la lb je oe b gy om oj l ok ol">dtf_scaled= pd.DataFrame(X, columns=dtf_train.drop("Y", <br/>                        axis=1).columns, index=dtf_train.index)</span><span id="750c" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## scale Y<br/></strong>scalerY = preprocessing.<strong class="oe jf">RobustScaler</strong>(quantile_range=(25.0, 75.0))<br/>dtf_scaled[y] = scalerY.fit_transform(<br/>                    dtf_train[y].values.reshape(-1,1))</span><span id="9f91" class="la lb je oe b gy om oj l ok ol">dtf_scaled.head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/9db538c9b0c54c5321e016701da473f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7-DAD2b3RKoNtVJZ1uMBA.png"/></div></div></figure><h2 id="50c7" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">特征选择</h2><p id="cc32" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">特征选择是选择相关变量的子集来构建机器学习模型的过程。它使模型更容易解释，并减少过度拟合(当模型适应训练数据过多，并且在训练集之外表现不佳时)。</p><p id="3b86" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在数据分析期间，我已经通过排除不相关的列进行了第一次“手动”特征选择。现在会有一点不同，因为我们必须处理<strong class="ly jf">多重共线性</strong>问题，这是指多元回归模型中两个或更多解释变量高度线性相关的情况。</p><p id="a7ed" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我用一个例子来解释:<em class="on"> GarageCars </em>与<em class="on"> GarageArea </em>高度相关，因为它们给出的信息都是一样的(车库有多大，一个是根据能停多少车，另一个是根据平方英尺)。让我们计算相关矩阵来看看:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="b34b" class="la lb je oe b gy oi oj l ok ol">corr_matrix = dtf_train.<strong class="oe jf">corr</strong>(method="pearson")<br/>sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)<br/>plt.title("pearson correlation")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/6d7d72ef8b635a87a9a8eeda954e4d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rjv3Sz5YIIB9WfjYbSPZ0w.png"/></div></div></figure><p id="a2aa" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><em class="on"> GarageCars </em>和<em class="on"> GarageArea </em>中的一个可能是<em class="on"> </em>不必要的，我们可以决定丢弃它并保留最有用的一个(即具有最低 p 值的那个或最能降低熵的那个)。</p><p id="5679" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">线性回归</strong> </a>是一种对标量响应和一个或多个解释变量之间的关系进行建模的线性方法。单变量线性回归测试广泛用于测试许多回归变量中每一个的个体效应:首先，计算每个回归变量和目标之间的相关性，然后进行 ANOVA F-测试。</p><p id="8012" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/Tikhonov_regularization" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">岭正则化</strong> </a> <strong class="ly jf"> </strong>对于缓解线性回归中的多重共线性问题特别有用，这种问题通常出现在具有大量参数的模型中。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="d076" class="la lb je oe b gy oi oj l ok ol">X = dtf_train.drop("Y", axis=1).values<br/>y = dtf_train["Y"].values<br/>feature_names = dtf_train.drop("Y", axis=1).columns</span><span id="56db" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## p-value</strong><br/>selector = <strong class="oe jf">feature_selection.SelectKBest</strong>(score_func=  <br/>               feature_selection.f_regression, k=10).fit(X,y)<br/>pvalue_selected_features = feature_names[selector.get_support()]<br/><br/><strong class="oe jf">## regularization</strong><br/>selector = <strong class="oe jf">feature_selection.SelectFromModel</strong>(estimator= <br/>              linear_model.Ridge(alpha=1.0, fit_intercept=True), <br/>                                 max_features=10).fit(X,y)<br/>regularization_selected_features = feature_names[selector.get_support()]<br/> <br/><strong class="oe jf">## plot<br/></strong>dtf_features = pd.DataFrame({"features":feature_names})<br/>dtf_features["p_value"] = dtf_features["features"].apply(lambda x: "p_value" if x in pvalue_selected_features else "")<br/>dtf_features["num1"] = dtf_features["features"].apply(lambda x: 1 if x in pvalue_selected_features else 0)<br/>dtf_features["regularization"] = dtf_features["features"].apply(lambda x: "regularization" if x in regularization_selected_features else "")<br/>dtf_features["num2"] = dtf_features["features"].apply(lambda x: 1 if x in regularization_selected_features else 0)<br/>dtf_features["method"] = dtf_features[["p_value","regularization"]].apply(lambda x: (x[0]+" "+x[1]).strip(), axis=1)<br/>dtf_features["selection"] = dtf_features["num1"] + dtf_features["num2"]<br/>dtf_features["method"] = dtf_features["method"].apply(lambda x: "both" if len(x.split()) == 2 else x)</span><span id="7a70" class="la lb je oe b gy om oj l ok ol">sns.barplot(y="features", x="selection", hue="method", data=dtf_features.sort_values("selection", ascending=False), dodge=False)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/de6734a43cf8b4d0e35f99ea89a9d6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4MFCDfITNeuI2gO-riXzw.png"/></div></div></figure><p id="310e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">蓝色的特征是由方差分析和岭选择的，其他的是由第一种统计方法选择的。</p><p id="e8b8" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">或者，您可以使用集成方法来获得特征重要性。<a class="ae op" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">集成方法</strong> </a>使用多种学习算法来获得比单独使用任何一种成分学习算法所能获得的更好的预测性能。我将给出一个使用<a class="ae op" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">梯度推进</strong> </a> <strong class="ly jf"> </strong>算法的例子:它以向前逐级的方式建立一个加性模型，并在每个阶段在给定损失函数的负梯度上拟合一个回归树。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="2663" class="la lb je oe b gy oi oj l ok ol">X = dtf_train.drop("Y", axis=1).values<br/>y = dtf_train["Y"].values<br/>feature_names = dtf_train.drop("Y", axis=1).columns.tolist()</span><span id="b157" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## call model</strong><br/>model = ensemble.<strong class="oe jf">GradientBoostingRegressor</strong>()</span><span id="4669" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Importance</strong><br/>model.fit(X,y)<br/>importances = model.<strong class="oe jf">feature_importances_</strong></span><span id="d2ad" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Put in a pandas dtf</strong><br/>dtf_importances = pd.DataFrame({"IMPORTANCE":importances, <br/>            "VARIABLE":feature_names}).sort_values("IMPORTANCE", <br/>            ascending=False)<br/>dtf_importances['cumsum'] =  <br/>            dtf_importances['IMPORTANCE'].cumsum(axis=0)<br/>dtf_importances = dtf_importances.set_index("VARIABLE")<br/>    <br/><strong class="oe jf">##</strong> <strong class="oe jf">Plot</strong><br/>fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)<br/>fig.suptitle("Features Importance", fontsize=20)<br/>ax[0].title.set_text('variables')<br/>    dtf_importances[["IMPORTANCE"]].sort_values(by="IMPORTANCE").plot(<br/>                kind="barh", legend=False, ax=ax[0]).grid(axis="x")<br/>ax[0].set(ylabel="")<br/>ax[1].title.set_text('cumulative')<br/>dtf_importances[["cumsum"]].plot(kind="line", linewidth=4, <br/>                                 legend=False, ax=ax[1])<br/>ax[1].set(xlabel="", xticks=np.arange(len(dtf_importances)), <br/>          xticklabels=dtf_importances.index)<br/>plt.xticks(rotation=70)<br/>plt.grid(axis='both')<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/9697e956e25a8a62a7bc4d79b10c8592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkUUgLFAFQW6bqoU8su_UQ.png"/></div></div></figure><p id="d2f1" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">非常有趣的是，<em class="on"> OverallQual、GrLivArea </em>和<em class="on"> TotalBsmtSf </em>在所有呈现的方法中占主导地位。</p><p id="5c19" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">就我个人而言，我总是试图使用尽可能少的功能，因此在这里我选择以下功能，并继续设计、训练、测试和评估机器学习模型:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="eab7" class="la lb je oe b gy oi oj l ok ol">X_names = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', "GarageCars"]</span><span id="60f4" class="la lb je oe b gy om oj l ok ol">X_train = dtf_train[X_names].values<br/>y_train = dtf_train["Y"].values</span><span id="6415" class="la lb je oe b gy om oj l ok ol">X_test = dtf_test[X_names].values<br/>y_test = dtf_test["Y"].values</span></pre><p id="34be" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">请注意，在使用测试数据进行预测之前，您必须对其进行预处理，就像我们对训练数据所做的那样。</p><h2 id="47a4" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">模型设计</h2><p id="6213" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">最后，是时候建立机器学习模型了。我将首先运行一个简单的线性回归，并将其用作更复杂模型的基线，如梯度推进算法。</p><p id="9d63" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我通常使用的第一个指标是<a class="ae op" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf"> R 的平方</strong> </a>，它表示自变量中可预测的因变量方差的比例。</p><p id="0fc8" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将使用<strong class="ly jf"> k 倍交叉验证</strong>来比较线性回归 R 的平方与梯度推进的平方，这是一个将数据分成 k 次训练和验证集的过程，对于每次分割，模型都被训练和测试。它用于检查模型通过一些数据进行训练的能力，以及预测未知数据的能力。</p><p id="c593" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将通过绘制<strong class="ly jf">预测值与实际值<em class="on"> Y </em> </strong> <em class="on">来可视化验证的结果。</em>理想情况下，所有的点都应接近预测值=实际值的对角线。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="e64f" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## call model</strong><br/>model = linear_model.<strong class="oe jf">LinearRegression</strong>()</span><span id="357b" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## K fold validation</strong><br/>scores = []<br/>cv = model_selection.KFold(n_splits=5, shuffle=True)<br/>fig = plt.figure()<br/>i = 1<br/>for train, test in cv.split(X_train, y_train):<br/>    prediction = model.fit(X_train[train],<br/>                 y_train[train]).predict(X_train[test])<br/>    true = y_train[test]<br/>    score = metrics.r2_score(true, prediction)<br/>    scores.append(score)<br/>    plt.scatter(prediction, true, lw=2, alpha=0.3, <br/>                label='Fold %d (R2 = %0.2f)' % (i,score))<br/>    i = i+1<br/>plt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], <br/>         linestyle='--', lw=2, color='black')<br/>plt.xlabel('Predicted')<br/>plt.ylabel('True')<br/>plt.title('K-Fold Validation')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/dba4c2f4694ffd0e27b9a5aeeeea5bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LXrUHYqMVWtzsQKEyQIksw.png"/></div></div></figure><p id="1cc3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">线性回归的平均 R 平方为 0.77。让我们看看梯度推进验证是如何进行的:</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/080653e6de5764f9021f36bff3c0e42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ephiquqIx7Bjk3s4EF4jGA.png"/></div></div></figure><p id="6ab0" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">梯度推进模型表现出更好的性能(平均 R 平方为 0.83)，因此我将使用它来预测测试数据:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="b240" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## train</strong><br/>model.<strong class="oe jf">fit</strong>(X_train, y_train)</span><span id="cb21" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## test</strong><br/>predicted = model.<strong class="oe jf">predict</strong>(X_test)</span></pre><p id="f261" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">请记住，数据是经过缩放的，因此为了将预测值与测试集中的实际房价进行比较，它们必须是未经缩放的(使用逆变换函数):</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="92c7" class="la lb je oe b gy oi oj l ok ol">predicted = scalerY.<strong class="oe jf">inverse_transform</strong>( <br/>                  predicted.reshape(-1,1) ).reshape(-1)</span></pre><h2 id="ea76" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">估价</h2><p id="91aa" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">关键时刻到了，我们要看看所有这些努力是否值得。重点是研究模型能解释多少 Y 的方差，以及误差是如何分布的。</p><p id="fc83" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将使用以下常用度量来评估该模型:R 平方、<strong class="ly jf">平均绝对误差(MAE) </strong>和<strong class="ly jf">均方根误差(RMSD) </strong>。后两个是表达同一现象的成对观察值之间的误差度量。由于误差可能是正的(实际&gt;预测)也可能是负的(实际&lt;预测)，您可以测量每个误差的绝对值和平方值。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/dd879484429b98719eff0235f7c4caf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*NYsPOiuV0aUqVsEUl73a5Q.png"/></div></figure><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/93bc620789b7332e503713494bef6ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*zP_6VqR2aC61m1FQjDVXfQ.png"/></div></figure><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="4bc1" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## Kpi</strong><br/>print("R2 (explained variance):", round(metrics.r2_score(y_test, predicted), 2))<br/>print("Mean Absolute Perc Error (Σ(|y-pred|/y)/n):", round(np.mean(np.abs((y_test-predicted)/predicted)), 2))<br/>print("Mean Absolute Error (Σ|y-pred|/n):", "{:,.0f}".format(metrics.mean_absolute_error(y_test, predicted)))<br/>print("Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)):", "{:,.0f}".format(np.sqrt(metrics.mean_squared_error(y_test, predicted))))</span><span id="7581" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## residuals<br/></strong>residuals = y_test - predicted<br/>max_error = max(residuals) if abs(max(residuals)) &gt; abs(min(residuals)) else min(residuals)<br/>max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) &gt; abs(min(residuals)) else list(residuals).index(min(residuals))<br/>max_true, max_pred = y_test[max_idx], predicted[max_idx]<br/>print("Max Error:", "{:,.0f}".format(max_error))</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/a6e8b153f5afd27bc758fb1e483702ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNmr2hBWZiLotOpWBYhJwg.png"/></div></div></figure><p id="c8b0" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">该模型解释了目标变量 86%的方差。平均而言，预测误差为 2 万美元，或者说误差为 11%。测试集上的最大误差超过 17 万美元。我们可以通过绘制预测值与实际值和每个预测值的残差(误差)来可视化误差。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="f2be" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## Plot predicted vs true</strong><br/>fig, ax = plt.subplots(nrows=1, ncols=2)<br/>from statsmodels.graphics.api import abline_plot<br/>ax[0].scatter(predicted, y_test, color="black")<br/>abline_plot(intercept=0, slope=1, color="red", ax=ax[0])<br/>ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label="max error")<br/>ax[0].grid(True)<br/>ax[0].set(xlabel="Predicted", ylabel="True", title="Predicted vs True")<br/>ax[0].legend()<br/>    <br/><strong class="oe jf">## Plot predicted vs residuals</strong><br/>ax[1].scatter(predicted, residuals, color="red")<br/>ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label="max error")<br/>ax[1].grid(True)<br/>ax[1].set(xlabel="Predicted", ylabel="Residuals", title="Predicted vs Residuals")<br/>ax[1].hlines(y=0, xmin=np.min(predicted), xmax=np.max(predicted))<br/>ax[1].legend()<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/ac2daf957bc4739fee460f412bacd5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AifcxLBnHZFwgXk03PCAkA.png"/></div></div></figure><p id="e7fa" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这就是-170k 的最大误差:模型预测的误差约为 320k，而实际观测值约为 150k。似乎大多数误差位于 50k 和-50k 之间，让我们更好地看看残差的分布，看看它是否近似正态:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="26f7" class="la lb je oe b gy oi oj l ok ol">fig, ax = plt.subplots()<br/>sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)<br/>ax.grid(True)<br/>ax.set(yticks=[], yticklabels=[], title="Residuals distribution")<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/5ed3be723bec1d7222150e837c5cd608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*i_O0Ft4iWH997vlu7o0iKg.png"/></div></figure><h2 id="78cd" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">可解释性</h2><p id="2482" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">你分析并理解了数据，你训练了一个模型并测试了它，你甚至对性能感到满意。你可以多走一步，证明你的机器学习模型不是一个黑盒。</p><p id="8066" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这个<em class="on">石灰</em>包可以帮助我们建造一个<strong class="ly jf">讲解器</strong>。为了举例说明，我将从测试集中随机观察，看看模型预测了什么:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="f596" class="la lb je oe b gy oi oj l ok ol">print("True:", "{:,.0f}".format(y_test[1]), "--&gt; Pred:", "{:,.0f}".format(predicted[1]))</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/7a4eeef01a8c6c930f529e8234301683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*SUK7s1AyiGv7-3IBupmzgA.png"/></div></figure><p id="c88e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">该模型预测这栋房子的价格为 194，870 美元。为什么？让我们使用解释器:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="32fd" class="la lb je oe b gy oi oj l ok ol">explainer = lime_tabular.LimeTabularExplainer(training_data=X_train, feature_names=X_names, class_names="Y", mode="regression")<br/>explained = explainer.explain_instance(X_test[1], model.predict, num_features=10)<br/>explained.as_pyplot_figure()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/0c90c13127259e7175f7e8bc8d39b71b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NS2UyKohsRNCysKaM97HVw.png"/></div></div></figure><p id="844a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这个特殊预测的主要因素是房子有一个大的地下室(TotalBsmft &gt; 1.3k)，它是用高质量的材料建造的(总体质量&gt; 6)，而且是最近建造的(建造年份&gt; 2001)。</p><p id="a63b" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">预测值与实际值的对比图是显示测试进行情况的一个很好的工具，但是我也绘制了<strong class="ly jf">回归平面</strong>来直观地帮助观察模型没有正确预测的异常值。由于线性模型效果更好，我将使用线性回归来拟合二维数据。为了绘制二维数据，需要进行一定程度的降维(通过获得一组主要变量来减少特征数量的过程)。我将给出一个例子，使用<a class="ae op" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>算法将数据总结为两个变量，这些变量是通过特征的线性组合获得的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="0382" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## PCA</strong><br/>pca = decomposition.PCA(n_components=2)<br/>X_train_2d = pca.fit_transform(X_train)<br/>X_test_2d = pca.transform(X_test)</span><span id="1764" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## train 2d model</strong><br/>model_2d = linear_model.LinearRegression()<br/>model_2d.fit(X_train, y_train)</span><span id="62ec" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## plot regression plane</strong><br/>from mpl_toolkits.mplot3d import Axes3D<br/>ax = Axes3D(plt.figure())<br/>ax.scatter(X_test[:,0], X_test[:,1], y_test, color="black")<br/>X1 = np.array([[X_test.min(), X_test.min()], [X_test.max(), <br/>               X_test.max()]])<br/>X2 = np.array([[X_test.min(), X_test.max()], [X_test.min(), <br/>               X_test.max()]])<br/>Y = model_2d.predict(np.array([[X_test.min(), X_test.min(), <br/>                     X_test.max(), X_test.max()], <br/>                    [X_test.min(), X_test.max(), X_test.min(), <br/>                     X_test.max()]]).T).reshape((2,2))<br/>Y = scalerY.inverse_transform(Y)<br/>ax.plot_surface(X1, X2, Y, alpha=0.5)<br/>ax.set(zlabel="Y", title="Regression plane", xticklabels=[], <br/>       yticklabels=[])<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/29f211b020ad5f116090ac99e99bf171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zStUKzcnypSkwr5x2l_sXA.gif"/></div></div></figure><h2 id="daca" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="2580" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">这篇文章是一个教程，展示了<strong class="ly jf">如何用数据科学处理回归用例</strong>。我以房价数据集为例，经历了从数据分析到机器学习模型的每个步骤。</p><p id="5056" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在探索部分，我分析了单个分类变量、单个数值变量的情况，以及它们如何相互作用。我举了一个从原始数据中提取特征的特征工程的例子。关于预处理，我解释了如何处理缺失值和分类数据。我展示了选择正确特性的不同方法，如何使用它们来构建回归模型，以及如何评估性能。在最后一节，我就如何提高你的机器学习模型的可解释性给出了一些建议。</p><p id="3bdb" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">一个重要的注意事项是，我还没有介绍在您的模型被批准部署之后会发生什么。请记住，您需要构建一个管道来自动处理您将定期获得的新数据。</p><p id="b688" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">现在，您已经知道如何处理数据科学用例，您可以将这些代码和方法应用于任何类型的回归问题，执行您自己的分析，构建您自己的模型，甚至解释它。</p><p id="56e0" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="pq"><p id="b4c3" class="pr ps je bd pt pu pv pw px py pz mo dk translated">👉<a class="ae op" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><blockquote class="qa qb qc"><p id="432a" class="lw lx on ly b lz mu kf mb mc mv ki me qd mw mg mh qe mx mj mk qf my mm mn mo im bi translated">本文是用 Python 进行机器学习系列<strong class="ly jf">的一部分</strong>，参见:</p></blockquote><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">用 Python 进行机器学习:分类(完整教程)</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">数据分析和可视化、特征工程和选择、模型设计和测试、评估和解释</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qg l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/clustering-geospatial-data-f0584f0b04ec"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">聚类地理空间数据</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">使用交互式地图绘制机器学习和深度学习聚类</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qh l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">Python 深度学习:神经网络(完整教程)</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">用 TensorFlow 建立、绘制和解释人工神经网络</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qi l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">基于神经网络的现代推荐系统</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">使用 Python 和 TensorFlow 构建混合模型</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qj l nk nl nm ni nn ja mz"/></div></div></a></div></div></div>    
</body>
</html>