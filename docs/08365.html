<html>
<head>
<title>Generating cooking recipes using TensorFlow and LSTM Recurrent Neural Network: A step-by-step guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流和LSTM递归神经网络生成烹饪食谱:一步一步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-cooking-recipes-using-tensorflow-and-lstm-recurrent-neural-network-a7bf242acad3?source=collection_archive---------32-----------------------#2020-06-18">https://towardsdatascience.com/generating-cooking-recipes-using-tensorflow-and-lstm-recurrent-neural-network-a7bf242acad3?source=collection_archive---------32-----------------------#2020-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d3d0b291dbb98266cd7d509243995eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQ0yVrXpaq_k0e4jkt07WQ.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://www.instagram.com/home_full_of_recipes/" rel="noopener ugc nofollow" target="_blank"> home_full_of_recipes </a>拍摄(Instagram频道)</p></figure><h1 id="d9a1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="722c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我用TensorFlow在<em class="lz"> ~100k </em>食谱数据集上训练了一个人物级别的LSTM <em class="lz">(长短期记忆)</em><em class="lz"/>，它建议我做<em class="lz">“奶油苏打加洋葱”</em>、<em class="lz">、【松饼草莓汤】、</em>、<em class="lz">、</em>、<em class="lz">、【三文鱼牛肉慕斯和墨西哥胡椒斯蒂尔顿沙拉】</em></p><p id="27fb" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在这里，你可能会找到更多我最后得到的例子:</p><ul class=""><li id="0f4c" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">🎨<a class="ae kc" href="https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">烹饪食谱生成器演示</strong></a>——在你的浏览器中交互式地尝试该模型。</li><li id="e583" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">🏋🏻‍ <a class="ae kc" href="https://github.com/trekhleb/machine-learning-experiments/blob/master/experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir"> LSTM模型训练流程</strong></a>——看看模型是怎么训练出来的。</li><li id="92d5" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated"><a class="ae kc" href="https://github.com/trekhleb/machine-learning-experiments" rel="noopener ugc nofollow" target="_blank">T25】🤖交互式机器学习实验 </a>知识库——查看更多“物体检测”、“草图识别”、“图像分类”等实验。</li></ul><p id="36bb" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">本文包含了LSTM模型如何在Python上使用<a class="ae kc" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow 2 </a>和<a class="ae kc" href="https://www.tensorflow.org/guide/keras" rel="noopener ugc nofollow" target="_blank"> Keras API </a>进行实际训练的细节。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/159e198694de4956a567ee3b518a6b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*y7eXQBIbC_WGK0Kg.gif"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来自<a class="ae kc" href="https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN" rel="noopener ugc nofollow" target="_blank">机器学习实验</a>的屏幕记录</p></figure><h1 id="6422" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">我们的模型最终会学到什么</h1><p id="920b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">经过几个小时的训练，我们的角色级RNN模型将学习英语语法和标点符号的基本概念(我希望我能这么快学会英语！).它还将学习如何生成配方的不同部分，如<em class="lz">📗【配方名称】</em>，<em class="lz">🥕【配方成分】</em>和<em class="lz">📝【食谱说明】</em>。有时候食谱名称、配料和说明会很有趣，有时候很愚蠢，有时候很有趣。</p><p id="4a06" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">下面是几个生成的配方示例:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5f6e" class="nd ke iq mz b gy ne nf l ng nh">📗 [NAME]</span><span id="a10c" class="nd ke iq mz b gy ni nf l ng nh">Orange Club Tea Sandwich Cookies</span><span id="bb22" class="nd ke iq mz b gy ni nf l ng nh">🥕 [INGREDIENTS]</span><span id="4338" class="nd ke iq mz b gy ni nf l ng nh">• 1 cup (2 sticks) unsalted butter, softened<br/>• 1 cup confectioners' sugar<br/>• 1/2 cup flaxseed meal<br/>• 1/2 cup shelled pumpkin seeds (pecans, blanched and sliced)<br/>• 2 teaspoons vanilla extract</span><span id="c167" class="nd ke iq mz b gy ni nf l ng nh">📝 [INSTRUCTIONS]</span><span id="f006" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat oven to 350 degrees F.<br/>▪︎ Combine cake mix, milk, egg and sugar in a large bowl. Stir until combined and smooth but not sticky. Using a spatula, sprinkle the dough biscuits over the bottom of the pan. Sprinkle with sugar, and spread evenly. Bake for 20 minutes. Remove from the oven and cool on a rack. To serve, add the chocolate.</span></pre><p id="9265" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">或者另一个:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="46bf" class="nd ke iq mz b gy ne nf l ng nh">📗 [NAME]</span><span id="7e69" class="nd ke iq mz b gy ni nf l ng nh">Mushrooms with Lentil Stewed Shallots and Tomatoes</span><span id="83ed" class="nd ke iq mz b gy ni nf l ng nh">🥕 [INGREDIENTS]</span><span id="cfab" class="nd ke iq mz b gy ni nf l ng nh">• 1 tablespoon olive oil<br/>• 3 cloves garlic, smashed<br/>• Kosher salt<br/>• 1 1/2 pounds lean ground turkey<br/>• 1 cup coarsely peeled tart apples<br/>• 2 tablespoons chopped garlic<br/>• 1 teaspoon ground cumin<br/>• 1/2 teaspoon cayenne pepper<br/>• 1 teaspoon chopped fresh thyme<br/>• 3/4 cup chopped fresh basil<br/>• 1/2 small carrot, halved lengthwise and cut into 1/2-inch pieces<br/>• 1 roasted red pepper, halved and sliced vertically diced and separated into rough chops<br/>• 3 tablespoons unsalted butter<br/>• 2 cups shredded mozzarella<br/>• 1/4 cup grated parmesan cheese<br/>• 1/4 cup prepared basil pesto</span><span id="6550" class="nd ke iq mz b gy ni nf l ng nh">📝 [INSTRUCTIONS]</span><span id="91d9" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Stir the olive oil, garlic, thyme and 1 teaspoon salt in a saucepan; bring to a simmer over medium heat. Remove from the heat. Add the basil and toast the soup for 2 minutes.<br/>▪︎ Meanwhile, heat 4 to 4 inches vegetable oil in the skillet over medium-high heat. Add the olive oil, garlic, 1/2 teaspoon salt and 1/2 teaspoon pepper and cook, stirring often, until cooked through, a</span></pre><p id="207c" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">⚠️ <em class="lz">本文中的食谱只是为了娱乐和学习目的而制作的。食谱是</em> <strong class="ld ir"> <em class="lz">而不是</em> </strong> <em class="lz">用于实际烹饪！</em></p><h1 id="ca13" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">先验知识</h1><p id="03c1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">假设你已经熟悉<a class="ae kc" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>的概念，特别是<a class="ae kc" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM) </a>架构。</p><p id="14ce" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">ℹ️，如果这些概念对你来说是新的，我强烈推荐你参加Coursera的深度学习专业课程。浏览一下Andrej Karpathy的文章<em class="lz">中的<a class="ae kc" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">循环神经网络</a>的不合理的有效性可能也是有益的。</em></p><p id="21b4" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在高层次上，<strong class="ld ir">递归神经网络(RNN) </strong>是一类深度神经网络，最常用于基于序列的数据，如语音、声音、文本或音乐。它们用于机器翻译、语音识别、语音合成等。rnn的关键特征是它们是有状态的，并且它们具有内部存储器，其中可以存储序列的一些上下文。例如，如果序列的第一个单词是<code class="fe nj nk nl mz b">He</code>，RNN可能向<code class="fe nj nk nl mz b">speaks</code>建议下一个单词，而不仅仅是<code class="fe nj nk nl mz b">speak</code>(以形成<code class="fe nj nk nl mz b">He speaks</code>短语)，因为关于第一个单词<code class="fe nj nk nl mz b">He</code>的先验知识已经在内存中。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/7be41e544849fdb250828ae676bcb1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ku9eottpgMebXClDfcs85w.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来源:<a class="ae kc" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>维基百科上的文章</p></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/bb1ddaafa74637ddf04ca24c0b8c53cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IP_7FVkvzA68URHh.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来源:<a class="ae kc" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">LSTM和GRU</a>关于数据科学的图文并茂的文章</p></figure><p id="513e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">令人兴奋的是，RNN(尤其是LSTM)不仅能记住<em class="lz">单词对单词</em>的依存关系，还能记住<em class="lz">字符对字符</em>的依存关系！序列由什么组成并不重要:可能是单词，也可能是字符。重要的是它们形成了一个时间分布的序列。例如，我们有一个字符序列<code class="fe nj nk nl mz b">['H', 'e']</code>。如果我们问LSTM下一步可能会做什么，它可能会建议一个<code class="fe nj nk nl mz b">&lt;stop_word&gt;</code>(意思是，组成单词<code class="fe nj nk nl mz b">He</code>的序列已经完成，我们可以停止)，或者它也可能会建议一个字符<code class="fe nj nk nl mz b">l</code>(意思是，它试图为我们建立一个<code class="fe nj nk nl mz b">Hello</code>序列)。这种类型的rnn被称为<strong class="ld ir">字符级rnn</strong>(与<strong class="ld ir">单词级rnn</strong>相对)。</p><p id="c679" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在本教程中，我们将依靠RNN网络的记忆功能，我们将使用一个角色级别的LSTM版本来生成烹饪食谱。</p><h1 id="399b" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">探索数据集</h1><p id="483b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们浏览几个可用的数据集，并探讨它们的优缺点。我希望数据集满足的一个要求是，它不仅应该有一个配料列表，还应该有烹饪说明。我还希望它有一个措施和每种成分的数量。</p><p id="4f56" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">以下是我发现的几个烹饪食谱数据集:</p><ul class=""><li id="e99b" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">🤷<a class="ae kc" href="https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home" rel="noopener ugc nofollow" target="_blank">食谱配料数据集</a> <em class="lz">(没有配料比例)</em></li><li id="cb87" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">🤷<a class="ae kc" href="http://pic2recipe.csail.mit.edu/" rel="noopener ugc nofollow" target="_blank"> Recipe1M+ </a> <em class="lz">(食谱很多但需要注册才能下载)</em></li><li id="8a08" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">🤷<a class="ae kc" href="https://www.kaggle.com/hugodarwood/epirecipes?select=full_format_recipes.json" rel="noopener ugc nofollow" target="_blank">美食家——带评级和营养的食谱</a> <em class="lz">(仅约20k份食谱，最好能找到更多)</em></li><li id="16ab" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">👍🏻<a class="ae kc" href="https://eightportions.com/datasets/Recipes/" rel="noopener ugc nofollow" target="_blank">食谱框</a><em class="lz">(~ 12.5万份食谱搭配食材比例，不错)</em></li></ul><p id="ecfc" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们尝试使用“配方箱”数据集。菜谱的数量看起来足够多，而且它既包含配料又包含烹饪说明。看看RNN是否能够了解配料和说明之间的联系是很有趣的。</p><h1 id="0077" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为训练设置TensorFlow/Python沙盒</h1><p id="561e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在本教程中，您可以使用几个选项来试验代码:</p><ol class=""><li id="220f" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly no ml mm mn bi translated">你可以在你的浏览器 <em class="lz">(不需要本地设置)</em>中使用<a class="ae kc" href="https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb" rel="noopener ugc nofollow" target="_blank"> GoogleColab进行实验。</a></li><li id="a4f9" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly no ml mm mn bi translated">你可以在你的浏览器中使用<a class="ae kc" href="https://mybinder.org/v2/gh/trekhleb/machine-learning-experiments/master?filepath=experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a> <em class="lz">(不需要本地设置)</em>。</li><li id="85b3" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly no ml mm mn bi translated">你可以在本地建立一个Jupyter笔记本。</li></ol><p id="70da" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">我会建议使用GoogleColab选项，因为它不需要对您进行任何本地设置(您可以在您的浏览器中进行实验)，并且它还提供了强大的GPU支持，可以使模型训练更快。您也可以尝试训练参数。</p><h1 id="7345" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">导入依赖关系</h1><p id="28eb" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们从导入一些我们以后会用到的包开始。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="c22f" class="nd ke iq mz b gy ne nf l ng nh"><em class="lz"># Packages for training the model and working with the dataset.</em><br/><strong class="mz ir">import</strong> tensorflow <strong class="mz ir">as</strong> tf<br/><strong class="mz ir">import</strong> matplotlib.pyplot <strong class="mz ir">as</strong> plt<br/><strong class="mz ir">import</strong> numpy <strong class="mz ir">as</strong> np<br/><strong class="mz ir">import</strong> json</span><span id="776e" class="nd ke iq mz b gy ni nf l ng nh"><em class="lz"># Utility/helper packages.</em><br/><strong class="mz ir">import</strong> platform<br/><strong class="mz ir">import</strong> time<br/><strong class="mz ir">import</strong> pathlib<br/><strong class="mz ir">import</strong> os</span></pre><p id="caa3" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">首先，让我们确保我们的环境设置正确，并且我们使用的是Tensorflow的<em class="lz"> 2nd </em>版本。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="1cf0" class="nd ke iq mz b gy ne nf l ng nh">print('Python version:', platform.python_version())<br/>print('Tensorflow version:', tf.__version__)<br/>print('Keras version:', tf.keras.__version__)</span></pre><p id="bfb0" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="98d5" class="nd ke iq mz b gy ne nf l ng nh">Python version: 3.7.6<br/>Tensorflow version: 2.1.0<br/>Keras version: 2.2.4-tf</span></pre><h1 id="5944" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">加载数据集</h1><p id="a07a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file" rel="noopener ugc nofollow" target="_blank">TF . keras . utils . get _ file</a>加载数据集。使用<code class="fe nj nk nl mz b">get_file()</code>实用程序很方便，因为它为您处理现成的缓存。这意味着您将只下载一次数据集文件，即使您再次在笔记本中启动相同的代码块，它也会使用缓存，并且代码块的执行速度会更快。</p><p id="cc84" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">如果缓存文件夹不存在，则创建它:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="31b1" class="nd ke iq mz b gy ne nf l ng nh">CACHE_DIR = './tmp'<br/>pathlib.Path(CACHE_DIR).mkdir(exist_ok=<strong class="mz ir">True</strong>)</span></pre><p id="6e6e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">下载并解包数据集:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="1f2a" class="nd ke iq mz b gy ne nf l ng nh">dataset_file_name = 'recipes_raw.zip'<br/>dataset_file_origin = 'https://storage.googleapis.com/recipe-box/recipes_raw.zip'</span><span id="1563" class="nd ke iq mz b gy ni nf l ng nh">dataset_file_path = tf.keras.utils.get_file(<br/>    fname=dataset_file_name,<br/>    origin=dataset_file_origin,<br/>    cache_dir=CACHE_DIR,<br/>    extract=<strong class="mz ir">True</strong>,<br/>    archive_format='zip'<br/>)</span><span id="2b9d" class="nd ke iq mz b gy ni nf l ng nh">print(dataset_file_path)</span></pre><p id="77a7" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">以下是下载后数据集文件的路径:</p><p id="4f1e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="1a9d" class="nd ke iq mz b gy ne nf l ng nh">./tmp/datasets/recipes_raw.zip</span></pre><p id="f509" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们打印缓存文件夹，看看到底下载了什么:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e98c" class="nd ke iq mz b gy ne nf l ng nh">!ls -la ./tmp/datasets/</span></pre><p id="3d64" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="492e" class="nd ke iq mz b gy ne nf l ng nh">total 521128<br/>drwxr-xr-x  7        224 May 13 18:10 .<br/>drwxr-xr-x  4        128 May 18 18:00 ..<br/>-rw-r--r--  1      20437 May 20 06:46 LICENSE<br/>-rw-r--r--  1   53355492 May 13 18:10 recipes_raw.zip<br/>-rw-r--r--  1   49784325 May 20 06:46 recipes_raw_nosource_ar.json<br/>-rw-r--r--  1   61133971 May 20 06:46 recipes_raw_nosource_epi.json<br/>-rw-r--r--  1   93702755 May 20 06:46 recipes_raw_nosource_fn.json</span></pre><p id="92af" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">如您所见，数据集由3个文件组成。稍后，我们需要将这些文件中的信息合并成一个数据集。</p><p id="42e9" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们从<code class="fe nj nk nl mz b">json</code>文件中加载数据集数据，并从中预览示例。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b0d2" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">load_dataset</strong>(silent=False):<br/>    <em class="lz"># List of dataset files we want to merge.</em><br/>    dataset_file_names = [<br/>        'recipes_raw_nosource_ar.json',<br/>        'recipes_raw_nosource_epi.json',<br/>        'recipes_raw_nosource_fn.json',<br/>    ]<br/>    <br/>    dataset = []</span><span id="0d2e" class="nd ke iq mz b gy ni nf l ng nh">    <strong class="mz ir">for</strong> dataset_file_name <strong class="mz ir">in</strong> dataset_file_names:<br/>        dataset_file_path = f'{CACHE_DIR}/datasets/{dataset_file_name}'</span><span id="9c88" class="nd ke iq mz b gy ni nf l ng nh">        <strong class="mz ir">with</strong> open(dataset_file_path) <strong class="mz ir">as</strong> dataset_file:<br/>            json_data_dict = json.load(dataset_file)<br/>            json_data_list = list(json_data_dict.values())<br/>            dict_keys = [key <strong class="mz ir">for</strong> key <strong class="mz ir">in</strong> json_data_list[0]]<br/>            dict_keys.sort()<br/>            dataset += json_data_list</span><span id="ed94" class="nd ke iq mz b gy ni nf l ng nh">            <em class="lz"># This code block outputs the summary for each dataset.</em><br/>            <strong class="mz ir">if</strong> silent == <strong class="mz ir">False</strong>:<br/>                print(dataset_file_path)<br/>                print('===========================================')<br/>                print('Number of examples: ', len(json_data_list), '\n')<br/>                print('Example object keys:\n', dict_keys, '\n')<br/>                print('Example object:\n', json_data_list[0], '\n')<br/>                print('Required keys:\n')<br/>                print('  title: ', json_data_list[0]['title'], '\n')<br/>                print('  ingredients: ', json_data_list[0]['ingredients'], '\n')<br/>                print('  instructions: ', json_data_list[0]['instructions'])<br/>                print('\n\n')</span><span id="2938" class="nd ke iq mz b gy ni nf l ng nh">    <strong class="mz ir">return</strong> dataset  </span><span id="8acb" class="nd ke iq mz b gy ni nf l ng nh">dataset_raw = load_dataset()</span></pre><p id="f372" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="fbdc" class="nd ke iq mz b gy ne nf l ng nh">./tmp/datasets/recipes_raw_nosource_ar.json<br/>===========================================<br/>Number of examples:  39802 </span><span id="656d" class="nd ke iq mz b gy ni nf l ng nh">Example object keys:<br/> ['ingredients', 'instructions', 'picture_link', 'title'] </span><span id="e40e" class="nd ke iq mz b gy ni nf l ng nh">Example object:<br/> {'title': 'Slow Cooker Chicken and Dumplings', 'ingredients': ['4 skinless, boneless chicken breast halves ADVERTISEMENT', '2 tablespoons butter ADVERTISEMENT', '2 (10.75 ounce) cans condensed cream of chicken soup ADVERTISEMENT', '1 onion, finely diced ADVERTISEMENT', '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ADVERTISEMENT', 'ADVERTISEMENT'], 'instructions': 'Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.\nCover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.\n', 'picture_link': '55lznCYBbs2mT8BTx6BTkLhynGHzM.S'} </span><span id="a62f" class="nd ke iq mz b gy ni nf l ng nh">Required keys:</span><span id="9d0d" class="nd ke iq mz b gy ni nf l ng nh">  title:  Slow Cooker Chicken and Dumplings </span><span id="37cd" class="nd ke iq mz b gy ni nf l ng nh">  ingredients:  ['4 skinless, boneless chicken breast halves ADVERTISEMENT', '2 tablespoons butter ADVERTISEMENT', '2 (10.75 ounce) cans condensed cream of chicken soup ADVERTISEMENT', '1 onion, finely diced ADVERTISEMENT', '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ADVERTISEMENT', 'ADVERTISEMENT'] </span><span id="3a08" class="nd ke iq mz b gy ni nf l ng nh">  instructions:  Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.<br/>Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.<br/></span><span id="f000" class="nd ke iq mz b gy ni nf l ng nh">./tmp/datasets/recipes_raw_nosource_epi.json<br/>===========================================<br/>Number of examples:  25323 </span><span id="462f" class="nd ke iq mz b gy ni nf l ng nh">Example object keys:<br/> ['ingredients', 'instructions', 'picture_link', 'title'] </span><span id="d65c" class="nd ke iq mz b gy ni nf l ng nh">Example object:<br/> {'ingredients': ['12 egg whites', '12 egg yolks', '1 1/2 cups sugar', '3/4 cup rye whiskey', '12 egg whites', '3/4 cup brandy', '1/2 cup rum', '1 to 2 cups heavy cream, lightly whipped', 'Garnish: ground nutmeg'], 'picture_link': None, 'instructions': 'Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.\nBeat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.', 'title': 'Christmas Eggnog '} </span><span id="95c1" class="nd ke iq mz b gy ni nf l ng nh">Required keys:</span><span id="ce72" class="nd ke iq mz b gy ni nf l ng nh">  title:  Christmas Eggnog  </span><span id="51c9" class="nd ke iq mz b gy ni nf l ng nh">  ingredients:  ['12 egg whites', '12 egg yolks', '1 1/2 cups sugar', '3/4 cup rye whiskey', '12 egg whites', '3/4 cup brandy', '1/2 cup rum', '1 to 2 cups heavy cream, lightly whipped', 'Garnish: ground nutmeg'] </span><span id="03bb" class="nd ke iq mz b gy ni nf l ng nh">  instructions:  Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.<br/>Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.</span><span id="54c3" class="nd ke iq mz b gy ni nf l ng nh">./tmp/datasets/recipes_raw_nosource_fn.json<br/>===========================================<br/>Number of examples:  60039 </span><span id="1848" class="nd ke iq mz b gy ni nf l ng nh">Example object keys:<br/> ['ingredients', 'instructions', 'picture_link', 'title'] </span><span id="1b15" class="nd ke iq mz b gy ni nf l ng nh">Example object:<br/> {'instructions': 'Toss ingredients lightly and spoon into a buttered baking dish. Top with additional crushed cracker crumbs, and brush with melted butter. Bake in a preheated at 350 degrees oven for 25 to 30 minutes or until delicately browned.', 'ingredients': ['1/2 cup celery, finely chopped', '1 small green pepper finely chopped', '1/2 cup finely sliced green onions', '1/4 cup chopped parsley', '1 pound crabmeat', '1 1/4 cups coarsely crushed cracker crumbs', '1/2 teaspoon salt', '3/4 teaspoons dry mustard', 'Dash hot sauce', '1/4 cup heavy cream', '1/2 cup melted butter'], 'title': "Grammie Hamblet's Deviled Crab", 'picture_link': None} </span><span id="644c" class="nd ke iq mz b gy ni nf l ng nh">Required keys:</span><span id="aec1" class="nd ke iq mz b gy ni nf l ng nh">  title:  Grammie Hamblet's Deviled Crab </span><span id="22dd" class="nd ke iq mz b gy ni nf l ng nh">  ingredients:  ['1/2 cup celery, finely chopped', '1 small green pepper finely chopped', '1/2 cup finely sliced green onions', '1/4 cup chopped parsley', '1 pound crabmeat', '1 1/4 cups coarsely crushed cracker crumbs', '1/2 teaspoon salt', '3/4 teaspoons dry mustard', 'Dash hot sauce', '1/4 cup heavy cream', '1/2 cup melted butter'] </span><span id="2726" class="nd ke iq mz b gy ni nf l ng nh">  instructions:  Toss ingredients lightly and spoon into a buttered baking dish. Top with additional crushed cracker crumbs, and brush with melted butter. Bake in a preheated at 350 degrees oven for 25 to 30 minutes or until delicately browned.</span></pre><p id="7454" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们统计一下合并文件后的示例总数:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="2535" class="nd ke iq mz b gy ne nf l ng nh">print('Total number of raw examples: ', len(dataset_raw))</span></pre><p id="9122" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="9e3e" class="nd ke iq mz b gy ne nf l ng nh">Total number of raw examples:  125164</span></pre><h1 id="2253" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">预处理数据集</h1><h1 id="9150" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">过滤掉不完整的例子</h1><p id="2aa3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有可能有些菜谱没有一些必填字段(<em class="lz">名称</em>、<em class="lz">配料</em>或<em class="lz">说明</em>)。我们需要从这些不完整的例子中清理出我们的数据集。</p><p id="83ae" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">以下函数将帮助我们筛选出既没有标题也没有配料或说明的食谱:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e2bc" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">recipe_validate_required_fields</strong>(recipe):<br/>    required_keys = ['title', 'ingredients', 'instructions']<br/>    <br/>    <strong class="mz ir">if</strong> <strong class="mz ir">not</strong> recipe:<br/>        <strong class="mz ir">return</strong> <strong class="mz ir">False</strong><br/>    <br/>    <strong class="mz ir">for</strong> required_key <strong class="mz ir">in</strong> required_keys:<br/>        <strong class="mz ir">if</strong> <strong class="mz ir">not</strong> recipe[required_key]:<br/>            <strong class="mz ir">return</strong> <strong class="mz ir">False</strong><br/>        <br/>        <strong class="mz ir">if</strong> type(recipe[required_key]) == list <strong class="mz ir">and</strong> len(recipe[required_key]) == 0:<br/>            <strong class="mz ir">return</strong> <strong class="mz ir">False</strong><br/>    <br/>    <strong class="mz ir">return</strong> <strong class="mz ir">True</strong></span></pre><p id="8e79" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">现在让我们使用<code class="fe nj nk nl mz b">recipe_validate_required_fields()</code>函数进行过滤:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5365" class="nd ke iq mz b gy ne nf l ng nh">dataset_validated = [recipe <strong class="mz ir">for</strong> recipe <strong class="mz ir">in</strong> dataset_raw <strong class="mz ir">if</strong> recipe_validate_required_fields(recipe)]</span><span id="8789" class="nd ke iq mz b gy ni nf l ng nh">print('Dataset size BEFORE validation', len(dataset_raw))<br/>print('Dataset size AFTER validation', len(dataset_validated))<br/>print('Number of incomplete recipes', len(dataset_raw) - len(dataset_validated))</span></pre><p id="80e4" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="81dc" class="nd ke iq mz b gy ne nf l ng nh">Dataset size BEFORE validation 125164<br/>Dataset size AFTER validation 122938<br/>Number of incomplete recipes 2226</span></pre><p id="f3a1" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">正如你可能看到的，我们的<code class="fe nj nk nl mz b">2226</code>食谱有些不完整。</p><h1 id="e359" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">将配方对象转换为字符串</h1><p id="e683" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">RNN不理解物体。因此，我们需要将recipes对象转换为字符串，然后转换为数字(索引)。让我们从将recipes对象转换成字符串开始。</p><p id="6a5e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了帮助我们的RNN更快地学习文章的结构，让我们给它添加3个“地标”。我们将使用这些独特的“标题”、“配料”和“说明”标志来分隔每个食谱的逻辑部分。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e249" class="nd ke iq mz b gy ne nf l ng nh">STOP_WORD_TITLE = '📗 '<br/>STOP_WORD_INGREDIENTS = '\n🥕\n\n'<br/>STOP_WORD_INSTRUCTIONS = '\n📝\n\n'</span></pre><p id="7f29" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">下面的函数将recipe对象转换为字符串(字符序列),以便以后在RNN输入中使用。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="d16e" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">recipe_to_string</strong>(recipe):<br/>    <em class="lz"># This string is presented as a part of recipes so we need to clean it up.</em><br/>    noize_string = 'ADVERTISEMENT'<br/>    <br/>    title = recipe['title']<br/>    ingredients = recipe['ingredients']<br/>    instructions = recipe['instructions'].split('\n')<br/>    <br/>    ingredients_string = ''<br/>    <strong class="mz ir">for</strong> ingredient <strong class="mz ir">in</strong> ingredients:<br/>        ingredient = ingredient.replace(noize_string, '')<br/>        <strong class="mz ir">if</strong> ingredient:<br/>            ingredients_string += f'• {ingredient}\n'<br/>    <br/>    instructions_string = ''<br/>    <strong class="mz ir">for</strong> instruction <strong class="mz ir">in</strong> instructions:<br/>        instruction = instruction.replace(noize_string, '')<br/>        <strong class="mz ir">if</strong> instruction:<br/>            instructions_string += f'▪︎ {instruction}\n'<br/>    <br/>    <strong class="mz ir">return</strong> f'{STOP_WORD_TITLE}{title}\n{STOP_WORD_INGREDIENTS}{ingredients_string}{STOP_WORD_INSTRUCTIONS}{instructions_string}'</span></pre><p id="e4f8" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们将<code class="fe nj nk nl mz b">recipe_to_string()</code>函数应用于<code class="fe nj nk nl mz b">dataset_validated</code>:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b1a0" class="nd ke iq mz b gy ne nf l ng nh">dataset_stringified = [recipe_to_string(recipe) <strong class="mz ir">for</strong> recipe <strong class="mz ir">in</strong> dataset_validated]</span><span id="b846" class="nd ke iq mz b gy ni nf l ng nh">print('Stringified dataset size: ', len(dataset_stringified))</span></pre><p id="0767" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="9c3e" class="nd ke iq mz b gy ne nf l ng nh">Stringified dataset size:  122938</span></pre><p id="5ae9" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们先来预览几个食谱:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="ddcb" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> recipe_index, recipe_string <strong class="mz ir">in</strong> enumerate(dataset_stringified[:3]):<br/>    print('Recipe #{}\n---------'.format(recipe_index + 1))<br/>    print(recipe_string)<br/>    print('\n')</span></pre><p id="f5b8" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="43f7" class="nd ke iq mz b gy ne nf l ng nh">Recipe #1<br/>---------<br/>📗 Slow Cooker Chicken and Dumplings</span><span id="1640" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="6781" class="nd ke iq mz b gy ni nf l ng nh">• 4 skinless, boneless chicken breast halves <br/>• 2 tablespoons butter <br/>• 2 (10.75 ounce) cans condensed cream of chicken soup <br/>• 1 onion, finely diced <br/>• 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces </span><span id="f900" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="5354" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.<br/>▪︎ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.</span><span id="3427" class="nd ke iq mz b gy ni nf l ng nh">Recipe #2<br/>---------<br/>📗 Awesome Slow Cooker Pot Roast</span><span id="beac" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="9a9c" class="nd ke iq mz b gy ni nf l ng nh">• 2 (10.75 ounce) cans condensed cream of mushroom soup <br/>• 1 (1 ounce) package dry onion soup mix <br/>• 1 1/4 cups water <br/>• 5 1/2 pounds pot roast </span><span id="e207" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="bd0d" class="nd ke iq mz b gy ni nf l ng nh">▪︎ In a slow cooker, mix cream of mushroom soup, dry onion soup mix and water. Place pot roast in slow cooker and coat with soup mixture.<br/>▪︎ Cook on High setting for 3 to 4 hours, or on Low setting for 8 to 9 hours.</span><span id="54d9" class="nd ke iq mz b gy ni nf l ng nh">Recipe #3<br/>---------<br/>📗 Brown Sugar Meatloaf</span><span id="1555" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="ca4d" class="nd ke iq mz b gy ni nf l ng nh">• 1/2 cup packed brown sugar <br/>• 1/2 cup ketchup <br/>• 1 1/2 pounds lean ground beef <br/>• 3/4 cup milk <br/>• 2 eggs <br/>• 1 1/2 teaspoons salt <br/>• 1/4 teaspoon ground black pepper <br/>• 1 small onion, chopped <br/>• 1/4 teaspoon ground ginger <br/>• 3/4 cup finely crushed saltine cracker crumbs </span><span id="7217" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="f2ef" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat oven to 350 degrees F (175 degrees C). Lightly grease a 5x9 inch loaf pan.<br/>▪︎ Press the brown sugar in the bottom of the prepared loaf pan and spread the ketchup over the sugar.<br/>▪︎ In a mixing bowl, mix thoroughly all remaining ingredients and shape into a loaf. Place on top of the ketchup.<br/>▪︎ Bake in preheated oven for 1 hour or until juices are clear.</span></pre><p id="5c7c" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">出于好奇，让我们从数据集的中间预览一下菜谱，看看它是否具有预期的数据结构:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="76f9" class="nd ke iq mz b gy ne nf l ng nh">print(dataset_stringified[50000])</span></pre><p id="802c" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="d855" class="nd ke iq mz b gy ne nf l ng nh">📗 Herbed Bean Ragoût </span><span id="0b6e" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="525a" class="nd ke iq mz b gy ni nf l ng nh">• 6 ounces haricots verts (French thin green beans), trimmed and halved crosswise<br/>• 1 (1-pound) bag frozen edamame (soybeans in the pod) or 1 1/4 cups frozen shelled edamame, not thawed<br/>• 2/3 cup finely chopped onion<br/>• 2 garlic cloves, minced<br/>• 1 Turkish bay leaf or 1/2 California bay leaf<br/>• 2 (3-inch) fresh rosemary sprigs<br/>• 1/2 teaspoon salt<br/>• 1/4 teaspoon black pepper<br/>• 1 tablespoon olive oil<br/>• 1 medium carrot, cut into 1/8-inch dice<br/>• 1 medium celery rib, cut into 1/8-inch dice<br/>• 1 (15- to 16-ounces) can small white beans, rinsed and drained<br/>• 1 1/2 cups chicken stock or low-sodium broth<br/>• 2 tablespoons unsalted butter<br/>• 2 tablespoons finely chopped fresh flat-leaf parsley<br/>• 1 tablespoon finely chopped fresh chervil (optional)<br/>• Garnish: fresh chervil sprigs</span><span id="b4af" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="1106" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Cook haricots verts in a large pot of boiling salted water until just tender, 3 to 4 minutes. Transfer with a slotted spoon to a bowl of ice and cold water, then drain. Add edamame to boiling water and cook 4 minutes. Drain in a colander, then rinse under cold water. If using edamame in pods, shell them and discard pods. Cook onion, garlic, bay leaf, rosemary, salt, and pepper in oil in a 2- to 4-quart heavy saucepan over moderately low heat, stirring, until softened, about 3 minutes. Add carrot and celery and cook, stirring, until softened, about 3 minutes. Add white beans and stock and simmer, covered, stirring occasionally, 10 minutes. Add haricots verts and edamame and simmer, uncovered, until heated through, 2 to 3 minutes. Add butter, parsley, and chervil (if using) and stir gently until butter is melted. Discard bay leaf and rosemary sprigs.<br/>▪︎ Cook haricots verts in a large pot of boiling salted water until just tender, 3 to 4 minutes. Transfer with a slotted spoon to a bowl of ice and cold water, then drain.<br/>▪︎ Add edamame to boiling water and cook 4 minutes. Drain in a colander, then rinse under cold water. If using edamame in pods, shell them and discard pods.<br/>▪︎ Cook onion, garlic, bay leaf, rosemary, salt, and pepper in oil in a 2- to 4-quart heavy saucepan over moderately low heat, stirring, until softened, about 3 minutes. Add carrot and celery and cook, stirring, until softened, about 3 minutes.<br/>▪︎ Add white beans and stock and simmer, covered, stirring occasionally, 10 minutes. Add haricots verts and edamame and simmer, uncovered, until heated through, 2 to 3 minutes. Add butter, parsley, and chervil (if using) and stir gently until butter is melted. Discard bay leaf and rosemary sprigs.</span></pre><h1 id="aa12" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">过滤掉大量食谱</h1><p id="e1a9" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">食谱长短不一。在将配方序列输入RNN之前，我们需要有一个<em class="lz">硬编码序列长度</em>限制。我们需要找出什么样的配方长度可以覆盖大多数的配方用例，同时我们希望它尽可能的短，以加快训练过程。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b68e" class="nd ke iq mz b gy ne nf l ng nh">recipes_lengths = []<br/><strong class="mz ir">for</strong> recipe_text <strong class="mz ir">in</strong> dataset_stringified:<br/>    recipes_lengths.append(len(recipe_text))</span><span id="2fef" class="nd ke iq mz b gy ni nf l ng nh">plt.hist(recipes_lengths, bins=50)<br/>plt.show()</span></pre><p id="6fc1" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2a79c60926a6dd3c33e0e7d4b437da55.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*ELcb0J77sf0OHBJj.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">配方长度(代码生成的图像)</p></figure><p id="071f" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">大多数食谱的长度小于<code class="fe nj nk nl mz b">5000</code>个字符。让我们放大来看更详细的图片:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="6493" class="nd ke iq mz b gy ne nf l ng nh">plt.hist(recipes_lengths, range=(0, 8000), bins=50)<br/>plt.show()</span></pre><p id="03b0" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d08d98be936458b9d9e2c08b7923df1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*nJzs14LKEwTVywt_.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">配方长度(代码生成的图像)</p></figure><p id="4168" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">看起来食谱的字符限制将覆盖大多数情况。我们可以试着用这个最大食谱长度限制来训练RNN。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="c4b9" class="nd ke iq mz b gy ne nf l ng nh">MAX_RECIPE_LENGTH = 2000</span></pre><p id="1584" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">因此，我们来过滤掉所有长于<code class="fe nj nk nl mz b">MAX_RECIPE_LENGTH</code>的菜谱:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="0e81" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">filter_recipes_by_length</strong>(recipe_test):<br/>    <strong class="mz ir">return</strong> len(recipe_test) &lt;= MAX_RECIPE_LENGTH </span><span id="44bd" class="nd ke iq mz b gy ni nf l ng nh">dataset_filtered = [recipe_text <strong class="mz ir">for</strong> recipe_text <strong class="mz ir">in</strong> dataset_stringified <strong class="mz ir">if</strong> filter_recipes_by_length(recipe_text)]</span><span id="6e44" class="nd ke iq mz b gy ni nf l ng nh">print('Dataset size BEFORE filtering: ', len(dataset_stringified))<br/>print('Dataset size AFTER filtering: ', len(dataset_filtered))<br/>print('Number of eliminated recipes: ', len(dataset_stringified) - len(dataset_filtered))</span></pre><p id="30d9" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="0a82" class="nd ke iq mz b gy ne nf l ng nh">Dataset size BEFORE filtering:  122938<br/>Dataset size AFTER filtering:  100212<br/>Number of eliminated recipes:  22726</span></pre><p id="1836" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在过滤过程中，我们丢失了<code class="fe nj nk nl mz b">22726</code>配方，但现在配方的数据更加密集。</p><h1 id="c50a" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">汇总数据集参数</h1><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="9254" class="nd ke iq mz b gy ne nf l ng nh">TOTAL_RECIPES_NUM = len(dataset_filtered)</span><span id="3849" class="nd ke iq mz b gy ni nf l ng nh">print('MAX_RECIPE_LENGTH: ', MAX_RECIPE_LENGTH)<br/>print('TOTAL_RECIPES_NUM: ', TOTAL_RECIPES_NUM)</span></pre><p id="5e27" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="87b4" class="nd ke iq mz b gy ne nf l ng nh">MAX_RECIPE_LENGTH:  2000<br/>TOTAL_RECIPES_NUM:  100212</span></pre><p id="8a88" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">最后，我们以<code class="fe nj nk nl mz b">~100k</code>食谱告终。每个配方都有<code class="fe nj nk nl mz b">2000</code>字符长度。</p><h1 id="9119" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">创造词汇</h1><p id="1179" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">递归神经网络不理解字符或单词。相反，它理解数字。因此，我们需要将食谱文本转换成数字。</p><p id="4045" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在这个实验中，我们将使用基于多层LSTM(长短期记忆)网络的<strong class="ld ir">字符级</strong>语言模型(与<strong class="ld ir">单词级</strong>语言模型相对)。这意味着我们将为字符创建唯一的索引，而不是为单词创建唯一的索引。通过这样做，我们让网络预测序列中的下一个<em class="lz">字符</em>，而不是下一个<em class="lz">单词</em>。</p><p id="90bf" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">ℹ️你可以在Andrej Karpathy的文章<a class="ae kc" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">中找到更多关于字符级RNNs的解释:</a></p><p id="acdf" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了从食谱文本中创建词汇表，我们将使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">TF . keras . preprocessing . text . tokenizer</a>。</p><p id="f965" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">我们还需要来一些独特的字符，将被视为一个<em class="lz">停止字符</em>，将表明一个食谱的结束。我们需要它来生成菜谱，因为如果没有这个停止字符，我们就不知道正在生成的菜谱的结尾在哪里。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="8346" class="nd ke iq mz b gy ne nf l ng nh">STOP_SIGN = '␣'</span><span id="8707" class="nd ke iq mz b gy ni nf l ng nh">tokenizer = tf.keras.preprocessing.text.Tokenizer(<br/>    char_level=<strong class="mz ir">True</strong>,<br/>    filters='',<br/>    lower=<strong class="mz ir">False</strong>,<br/>    split=''<br/>)</span><span id="873a" class="nd ke iq mz b gy ni nf l ng nh"><em class="lz"># Stop word is not a part of recipes, but tokenizer must know about it as well.</em><br/>tokenizer.fit_on_texts([STOP_SIGN])</span><span id="174f" class="nd ke iq mz b gy ni nf l ng nh">tokenizer.fit_on_texts(dataset_filtered)</span><span id="1618" class="nd ke iq mz b gy ni nf l ng nh">tokenizer.get_config()</span></pre><p id="7170" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="c1ba" class="nd ke iq mz b gy ne nf l ng nh">{'num_words': None,<br/> 'filters': '',<br/> 'lower': False,<br/> 'split': '',<br/> 'char_level': True,<br/> 'oov_token': None,<br/> 'document_count': 100213,</span><span id="3b8a" class="nd ke iq mz b gy ni nf l ng nh"> 'word_counts': '{"\\u2423": 1, "\\ud83d\\udcd7": 100212, " ": 17527888, "S": 270259, "l": 3815150, "o": 5987496, "w": 964459, "C": 222831, "k": 890982, "e": 9296022, "r": 4760887, "h": 2922100, "i": 4911812, "c": 2883507, "n": 5304396, "a": 6067157, "d": 3099679, "D": 63999, "u": 2717050, "m": 1794411, "p": 2679164, "g": 1698670, "s": 4704222, "\\n": 1955281, "\\ud83e\\udd55": 100212, "\\u2022": 922813, "4": 232607, ",": 1130487, "b": 1394803, "t": 5997722, "v": 746785, "2": 493933, "(": 144985, "1": 853931, "0": 145119, ".": 1052548, "7": 31098, "5": 154071, ")": 144977, "f": 1042981, "y": 666553, "\\ud83d\\udcdd": 100212, "\\u25aa": 331058, "\\ufe0e": 331058, "P": 200597, "6": 51398, "H": 43936, "A": 134274, "3": 213519, "R": 101253, "x": 201286, "/": 345257, "I": 81591, "L": 46138, "8": 55352, "9": 17697, "B": 123813, "M": 78684, "F": 104359, "j": 110008, "-": 219160, "W": 61616, "\\u00ae": 10159, "N": 12808, "q": 69654, "T": 101371, ";": 72045, "\'": 26831, "Z": 2428, "z": 115883, "G": 52043, ":": 31318, "E": 18582, "K": 18421, "X": 385, "\\"": 6445, "O": 28971, "Y": 6064, "\\u2122": 538, "Q": 3904, "J": 10269, "!": 3014, "U": 14132, "V": 12172, "&amp;": 1039, "+": 87, "=": 113, "%": 993, "*": 3243, "\\u00a9": 99, "[": 30, "]": 31, "\\u00e9": 6727, "&lt;": 76, "&gt;": 86, "\\u00bd": 166, "#": 168, "\\u00f1": 891, "?": 327, "\\u2019": 111, "\\u00b0": 6808, "\\u201d": 6, "$": 84, "@": 5, "{": 8, "}": 9, "\\u2013": 1228, "\\u0096": 7, "\\u00e0": 26, "\\u00e2": 106, "\\u00e8": 846, "\\u00e1": 74, "\\u2014": 215, "\\u2044": 16, "\\u00ee": 415, "\\u00e7": 171, "_": 26, "\\u00fa": 48, "\\u00ef": 43, "\\u201a": 20, "\\u00fb": 36, "\\u00f3": 74, "\\u00ed": 130, "\\u25ca": 4, "\\u00f9": 12, "\\u00d7": 6, "\\u00ec": 8, "\\u00fc": 29, "\\u2031": 4, "\\u00ba": 19, "\\u201c": 4, "\\u00ad": 25, "\\u00ea": 27, "\\u00f6": 9, "\\u0301": 11, "\\u00f4": 8, "\\u00c1": 2, "\\u00be": 23, "\\u00bc": 95, "\\u00eb": 2, "\\u0097": 2, "\\u215b": 3, "\\u2027": 4, "\\u00e4": 15, "\\u001a": 2, "\\u00f8": 2, "\\ufffd": 20, "\\u02da": 6, "\\u00bf": 264, "\\u2153": 2, "|": 2, "\\u00e5": 3, "\\u00a4": 1, "\\u201f": 1, "\\u00a7": 5, "\\ufb02": 3, "\\u00a0": 1, "\\u01b0": 2, "\\u01a1": 1, "\\u0103": 1, "\\u0300": 1, "\\u00bb": 6, "`": 3, "\\u0092": 2, "\\u215e": 1, "\\u202d": 4, "\\u00b4": 2, "\\u2012": 2, "\\u00c9": 40, "\\u00da": 14, "\\u20ac": 1, "\\\\": 5, "~": 1, "\\u0095": 1, "\\u00c2": 2}',</span><span id="bb1d" class="nd ke iq mz b gy ni nf l ng nh"> 'word_docs': '{"\\u2423": 1, "k": 97316, "0": 61954, "o": 100205, "r": 100207, "d": 100194, "u": 100161, "S": 89250, "\\u25aa": 100212, "D": 40870, "1": 99320, "g": 99975, "n": 100198, "b": 99702, "t": 100202, ".": 100163, " ": 100212, "7": 24377, "3": 79135, "\\ud83d\\udcd7": 100212, "i": 100207, "5": 65486, "f": 98331, "c": 100190, "4": 82453, "a": 100205, "2": 96743, "v": 97848, "C": 83328, "s": 100204, "\\n": 100212, "6": 35206, "\\ud83d\\udcdd": 100212, ",": 98524, "\\ufe0e": 100212, "l": 100206, "e": 100212, "y": 96387, ")": 67614, "p": 100046, "H": 31908, "\\ud83e\\udd55": 100212, "m": 99988, "w": 99227, "(": 67627, "A": 60900, "h": 100161, "\\u2022": 100212, "P": 79364, "R": 54040, "9": 14114, "8": 37000, "L": 32101, "x": 72133, "I": 46675, "/": 89051, "j": 47438, "F": 57940, "B": 64278, "M": 48332, "-": 74711, "T": 53758, "\\u00ae": 5819, "N": 9981, "W": 38981, "q": 36538, ";": 33863, "G": 35355, "\'": 18120, "z": 42430, "Z": 2184, ":": 18214, "E": 12161, "K": 14834, "X": 321, "\\"": 2617, "O": 20103, "Y": 5148, "\\u2122": 448, "Q": 3142, "J": 8225, "!": 2428, "U": 10621, "V": 9710, "&amp;": 749, "+": 32, "=": 48, "%": 717, "*": 1780, "\\u00a9": 91, "]": 26, "[": 25, "\\u00e9": 2462, "&gt;": 33, "&lt;": 27, "\\u00bd": 81, "#": 139, "\\u00f1": 423, "?": 207, "\\u2019": 64, "\\u00b0": 3062, "\\u201d": 3, "@": 4, "$": 49, "{": 7, "}": 8, "\\u2013": 491, "\\u0096": 7, "\\u00e0": 22, "\\u00e2": 45, "\\u00e8": 335, "\\u00e1": 38, "\\u2014": 95, "\\u2044": 9, "\\u00ee": 122, "\\u00e7": 120, "_": 8, "\\u00fa": 25, "\\u00ef": 24, "\\u201a": 10, "\\u00fb": 29, "\\u00f3": 40, "\\u00ed": 52, "\\u25ca": 2, "\\u00f9": 6, "\\u00d7": 4, "\\u00ec": 4, "\\u00fc": 19, "\\u2031": 2, "\\u00ba": 9, "\\u201c": 2, "\\u00ad": 11, "\\u00ea": 4, "\\u00f6": 4, "\\u0301": 6, "\\u00f4": 5, "\\u00c1": 2, "\\u00be": 18, "\\u00bc": 55, "\\u00eb": 2, "\\u0097": 1, "\\u215b": 2, "\\u2027": 3, "\\u00e4": 8, "\\u001a": 1, "\\u00f8": 1, "\\ufffd": 4, "\\u02da": 3, "\\u00bf": 191, "\\u2153": 1, "|": 2, "\\u00e5": 1, "\\u00a4": 1, "\\u201f": 1, "\\u00a7": 3, "\\ufb02": 1, "\\u0300": 1, "\\u01a1": 1, "\\u00a0": 1, "\\u01b0": 1, "\\u0103": 1, "\\u00bb": 2, "`": 3, "\\u0092": 2, "\\u215e": 1, "\\u202d": 1, "\\u00b4": 1, "\\u2012": 1, "\\u00c9": 15, "\\u00da": 5, "\\u20ac": 1, "\\\\": 5, "~": 1, "\\u0095": 1, "\\u00c2": 1}',</span><span id="9851" class="nd ke iq mz b gy ni nf l ng nh"> 'index_docs': '{"1": 100212, "165": 1, "25": 97316, "41": 61954, "5": 100205, "8": 100207, "11": 100194, "14": 100161, "33": 89250, "31": 100212, "58": 40870, "26": 99320, "18": 99975, "6": 100198, "19": 99702, "4": 100202, "21": 100163, "66": 24377, "37": 79135, "51": 100212, "7": 100207, "40": 65486, "22": 98331, "13": 100190, "34": 82453, "3": 100205, "29": 96743, "27": 97848, "35": 83328, "9": 100204, "16": 100212, "62": 35206, "53": 100212, "20": 98524, "32": 100212, "10": 100206, "2": 100212, "28": 96387, "43": 67614, "15": 100046, "64": 31908, "52": 100212, "17": 99988, "23": 99227, "42": 67627, "44": 60900, "12": 100161, "24": 100212, "39": 79364, "50": 54040, "71": 14114, "60": 37000, "63": 32101, "38": 72133, "54": 46675, "30": 89051, "47": 47438, "48": 57940, "45": 64278, "55": 48332, "36": 74711, "49": 53758, "76": 5819, "73": 9981, "59": 38981, "57": 36538, "56": 33863, "61": 35355, "68": 18120, "46": 42430, "84": 2184, "65": 18214, "69": 12161, "70": 14834, "92": 321, "79": 2617, "67": 20103, "80": 5148, "90": 448, "81": 3142, "75": 8225, "83": 2428, "72": 10621, "74": 9710, "86": 749, "105": 32, "100": 48, "87": 717, "82": 1780, "103": 91, "115": 26, "116": 25, "78": 2462, "106": 33, "108": 27, "98": 81, "97": 139, "88": 423, "93": 207, "101": 64, "77": 3062, "137": 3, "141": 4, "107": 49, "133": 7, "131": 8, "85": 491, "136": 7, "119": 22, "102": 45, "89": 335, "109": 38, "95": 95, "126": 9, "91": 122, "96": 120, "120": 8, "111": 25, "112": 24, "123": 10, "114": 29, "110": 40, "99": 52, "144": 2, "129": 6, "138": 4, "134": 4, "117": 19, "145": 2, "125": 9, "146": 2, "121": 11, "118": 4, "132": 4, "130": 6, "135": 5, "153": 2, "122": 18, "104": 55, "154": 2, "155": 1, "149": 2, "147": 3, "127": 8, "156": 1, "157": 1, "124": 4, "139": 3, "94": 191, "158": 1, "159": 2, "150": 1, "166": 1, "167": 1, "142": 3, "151": 1, "171": 1, "169": 1, "168": 1, "160": 1, "170": 1, "140": 2, "152": 3, "161": 2, "172": 1, "148": 1, "162": 1, "163": 1, "113": 15, "128": 5, "173": 1, "143": 5, "174": 1, "175": 1, "164": 1}',</span><span id="ddd6" class="nd ke iq mz b gy ni nf l ng nh"> 'index_word': '{"1": " ", "2": "e", "3": "a", "4": "t", "5": "o", "6": "n", "7": "i", "8": "r", "9": "s", "10": "l", "11": "d", "12": "h", "13": "c", "14": "u", "15": "p", "16": "\\n", "17": "m", "18": "g", "19": "b", "20": ",", "21": ".", "22": "f", "23": "w", "24": "\\u2022", "25": "k", "26": "1", "27": "v", "28": "y", "29": "2", "30": "/", "31": "\\u25aa", "32": "\\ufe0e", "33": "S", "34": "4", "35": "C", "36": "-", "37": "3", "38": "x", "39": "P", "40": "5", "41": "0", "42": "(", "43": ")", "44": "A", "45": "B", "46": "z", "47": "j", "48": "F", "49": "T", "50": "R", "51": "\\ud83d\\udcd7", "52": "\\ud83e\\udd55", "53": "\\ud83d\\udcdd", "54": "I", "55": "M", "56": ";", "57": "q", "58": "D", "59": "W", "60": "8", "61": "G", "62": "6", "63": "L", "64": "H", "65": ":", "66": "7", "67": "O", "68": "\'", "69": "E", "70": "K", "71": "9", "72": "U", "73": "N", "74": "V", "75": "J", "76": "\\u00ae", "77": "\\u00b0", "78": "\\u00e9", "79": "\\"", "80": "Y", "81": "Q", "82": "*", "83": "!", "84": "Z", "85": "\\u2013", "86": "&amp;", "87": "%", "88": "\\u00f1", "89": "\\u00e8", "90": "\\u2122", "91": "\\u00ee", "92": "X", "93": "?", "94": "\\u00bf", "95": "\\u2014", "96": "\\u00e7", "97": "#", "98": "\\u00bd", "99": "\\u00ed", "100": "=", "101": "\\u2019", "102": "\\u00e2", "103": "\\u00a9", "104": "\\u00bc", "105": "+", "106": "&gt;", "107": "$", "108": "&lt;", "109": "\\u00e1", "110": "\\u00f3", "111": "\\u00fa", "112": "\\u00ef", "113": "\\u00c9", "114": "\\u00fb", "115": "]", "116": "[", "117": "\\u00fc", "118": "\\u00ea", "119": "\\u00e0", "120": "_", "121": "\\u00ad", "122": "\\u00be", "123": "\\u201a", "124": "\\ufffd", "125": "\\u00ba", "126": "\\u2044", "127": "\\u00e4", "128": "\\u00da", "129": "\\u00f9", "130": "\\u0301", "131": "}", "132": "\\u00f6", "133": "{", "134": "\\u00ec", "135": "\\u00f4", "136": "\\u0096", "137": "\\u201d", "138": "\\u00d7", "139": "\\u02da", "140": "\\u00bb", "141": "@", "142": "\\u00a7", "143": "\\\\", "144": "\\u25ca", "145": "\\u2031", "146": "\\u201c", "147": "\\u2027", "148": "\\u202d", "149": "\\u215b", "150": "\\u00e5", "151": "\\ufb02", "152": "`", "153": "\\u00c1", "154": "\\u00eb", "155": "\\u0097", "156": "\\u001a", "157": "\\u00f8", "158": "\\u2153", "159": "|", "160": "\\u01b0", "161": "\\u0092", "162": "\\u00b4", "163": "\\u2012", "164": "\\u00c2", "165": "\\u2423", "166": "\\u00a4", "167": "\\u201f", "168": "\\u00a0", "169": "\\u01a1", "170": "\\u0103", "171": "\\u0300", "172": "\\u215e", "173": "\\u20ac", "174": "~", "175": "\\u0095"}',</span><span id="c110" class="nd ke iq mz b gy ni nf l ng nh"> 'word_index': '{" ": 1, "e": 2, "a": 3, "t": 4, "o": 5, "n": 6, "i": 7, "r": 8, "s": 9, "l": 10, "d": 11, "h": 12, "c": 13, "u": 14, "p": 15, "\\n": 16, "m": 17, "g": 18, "b": 19, ",": 20, ".": 21, "f": 22, "w": 23, "\\u2022": 24, "k": 25, "1": 26, "v": 27, "y": 28, "2": 29, "/": 30, "\\u25aa": 31, "\\ufe0e": 32, "S": 33, "4": 34, "C": 35, "-": 36, "3": 37, "x": 38, "P": 39, "5": 40, "0": 41, "(": 42, ")": 43, "A": 44, "B": 45, "z": 46, "j": 47, "F": 48, "T": 49, "R": 50, "\\ud83d\\udcd7": 51, "\\ud83e\\udd55": 52, "\\ud83d\\udcdd": 53, "I": 54, "M": 55, ";": 56, "q": 57, "D": 58, "W": 59, "8": 60, "G": 61, "6": 62, "L": 63, "H": 64, ":": 65, "7": 66, "O": 67, "\'": 68, "E": 69, "K": 70, "9": 71, "U": 72, "N": 73, "V": 74, "J": 75, "\\u00ae": 76, "\\u00b0": 77, "\\u00e9": 78, "\\"": 79, "Y": 80, "Q": 81, "*": 82, "!": 83, "Z": 84, "\\u2013": 85, "&amp;": 86, "%": 87, "\\u00f1": 88, "\\u00e8": 89, "\\u2122": 90, "\\u00ee": 91, "X": 92, "?": 93, "\\u00bf": 94, "\\u2014": 95, "\\u00e7": 96, "#": 97, "\\u00bd": 98, "\\u00ed": 99, "=": 100, "\\u2019": 101, "\\u00e2": 102, "\\u00a9": 103, "\\u00bc": 104, "+": 105, "&gt;": 106, "$": 107, "&lt;": 108, "\\u00e1": 109, "\\u00f3": 110, "\\u00fa": 111, "\\u00ef": 112, "\\u00c9": 113, "\\u00fb": 114, "]": 115, "[": 116, "\\u00fc": 117, "\\u00ea": 118, "\\u00e0": 119, "_": 120, "\\u00ad": 121, "\\u00be": 122, "\\u201a": 123, "\\ufffd": 124, "\\u00ba": 125, "\\u2044": 126, "\\u00e4": 127, "\\u00da": 128, "\\u00f9": 129, "\\u0301": 130, "}": 131, "\\u00f6": 132, "{": 133, "\\u00ec": 134, "\\u00f4": 135, "\\u0096": 136, "\\u201d": 137, "\\u00d7": 138, "\\u02da": 139, "\\u00bb": 140, "@": 141, "\\u00a7": 142, "\\\\": 143, "\\u25ca": 144, "\\u2031": 145, "\\u201c": 146, "\\u2027": 147, "\\u202d": 148, "\\u215b": 149, "\\u00e5": 150, "\\ufb02": 151, "`": 152, "\\u00c1": 153, "\\u00eb": 154, "\\u0097": 155, "\\u001a": 156, "\\u00f8": 157, "\\u2153": 158, "|": 159, "\\u01b0": 160, "\\u0092": 161, "\\u00b4": 162, "\\u2012": 163, "\\u00c2": 164, "\\u2423": 165, "\\u00a4": 166, "\\u201f": 167, "\\u00a0": 168, "\\u01a1": 169, "\\u0103": 170, "\\u0300": 171, "\\u215e": 172, "\\u20ac": 173, "~": 174, "\\u0095": 175}'}</span></pre><p id="a3dd" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了得到一个完整的词汇表，我们需要将<code class="fe nj nk nl mz b">+1</code>加到已经注册的字符数上，因为<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">索引</a> <code class="fe nj nk nl mz b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">0</a></code> <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">是一个保留索引，不会分配给任何单词</a>。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="43d8" class="nd ke iq mz b gy ne nf l ng nh">VOCABULARY_SIZE = len(tokenizer.word_counts) + 1</span><span id="a931" class="nd ke iq mz b gy ni nf l ng nh">print('VOCABULARY_SIZE: ', VOCABULARY_SIZE)</span></pre><p id="189f" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="d0db" class="nd ke iq mz b gy ne nf l ng nh">VOCABULARY_SIZE:  176</span></pre><p id="ea7d" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们尝试一下记号化器字典，看看如何将字符转换成索引，反之亦然:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="dbf0" class="nd ke iq mz b gy ne nf l ng nh">print(tokenizer.index_word[5])<br/>print(tokenizer.index_word[20])</span></pre><p id="7fd8" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="f8e0" class="nd ke iq mz b gy ne nf l ng nh">o<br/>,</span></pre><p id="79f4" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们尝试将字符转换为索引:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="4705" class="nd ke iq mz b gy ne nf l ng nh">tokenizer.word_index['r']</span></pre><p id="e217" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="c2c8" class="nd ke iq mz b gy ne nf l ng nh">8</span></pre><p id="67e4" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了说明哪种字符构成了我们数据集中的所有食谱，我们可以将它们打印成一个数组:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="33be" class="nd ke iq mz b gy ne nf l ng nh">array_vocabulary = tokenizer.sequences_to_texts([[word_index] <strong class="mz ir">for</strong> word_index <strong class="mz ir">in</strong> range(VOCABULARY_SIZE)])<br/>print([char <strong class="mz ir">for</strong> char <strong class="mz ir">in</strong> array_vocabulary])</span></pre><p id="31c5" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="4a55" class="nd ke iq mz b gy ne nf l ng nh">['', ' ', 'e', 'a', 't', 'o', 'n', 'i', 'r', 's', 'l', 'd', 'h', 'c', 'u', 'p', '\n', 'm', 'g', 'b', ',', '.', 'f', 'w', '•', 'k', '1', 'v', 'y', '2', '/', '▪', '︎', 'S', '4', 'C', '-', '3', 'x', 'P', '5', '0', '(', ')', 'A', 'B', 'z', 'j', 'F', 'T', 'R', '📗', '🥕', '📝', 'I', 'M', ';', 'q', 'D', 'W', '8', 'G', '6', 'L', 'H', ':', '7', 'O', "'", 'E', 'K', '9', 'U', 'N', 'V', 'J', '®', '°', 'é', '"', 'Y', 'Q', '*', '!', 'Z', '–', '&amp;', '%', 'ñ', 'è', '™', 'î', 'X', '?', '¿', '—', 'ç', '#', '½', 'í', '=', '’', 'â', '©', '¼', '+', '&gt;', '$', '&lt;', 'á', 'ó', 'ú', 'ï', 'É', 'û', ']', '[', 'ü', 'ê', 'à', '_', '\xad', '¾', '‚', '�', 'º', '⁄', 'ä', 'Ú', 'ù', '́', '}', 'ö', '{', 'ì', 'ô', '\x96', '”', '×', '˚', '»', '@', '§', '\\', '◊', '‱', '“', '‧', '\u202d', '⅛', 'å', 'ﬂ', '`', 'Á', 'ë', '\x97', '\x1a', 'ø', '⅓', '|', 'ư', '\x92', '´', '‒', 'Â', '␣', '¤', '‟', '\xa0', 'ơ', 'ă', '̀', '⅞', '€', '~', '\x95']</span></pre><p id="6daa" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">这些都是我们的RNN模型将要处理的角色。它将尝试学习如何将这些字符组合成看起来像食谱的序列。</p><p id="0021" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们看看如何使用<code class="fe nj nk nl mz b">tokenizer</code>函数将文本转换成索引:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="6ee3" class="nd ke iq mz b gy ne nf l ng nh">tokenizer.texts_to_sequences(['📗 yes'])</span></pre><p id="15d6" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="76ae" class="nd ke iq mz b gy ne nf l ng nh">[[51, 1, 28, 2, 9]]</span></pre><h1 id="457c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">对数据集进行矢量化</h1><p id="1d3f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在，一旦我们有了词汇表(<code class="fe nj nk nl mz b">character --code</code>和<code class="fe nj nk nl mz b">code --character</code>关系)，我们就可以将食谱从文本转换成数字(RNN将数字作为输入，而不是文本)。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e66d" class="nd ke iq mz b gy ne nf l ng nh">dataset_vectorized = tokenizer.texts_to_sequences(dataset_filtered)</span><span id="bf41" class="nd ke iq mz b gy ni nf l ng nh">print('Vectorized dataset size', len(dataset_vectorized))</span></pre><p id="553b" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="2a6f" class="nd ke iq mz b gy ne nf l ng nh">Vectorized dataset size 100212</span></pre><p id="e59b" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">这是第一个矢量化食谱的开头:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="ea3a" class="nd ke iq mz b gy ne nf l ng nh">print(dataset_vectorized[0][:10], '...')</span></pre><p id="e156" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="058f" class="nd ke iq mz b gy ne nf l ng nh">[51, 1, 33, 10, 5, 23, 1, 35, 5, 5] ...</span></pre><p id="f813" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们看看如何将矢量化的食谱转换回文本表示:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="6eb8" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">recipe_sequence_to_string</strong>(recipe_sequence):<br/>    recipe_stringified = tokenizer.sequences_to_texts([recipe_sequence])[0]<br/>    print(recipe_stringified)</span><span id="2e8d" class="nd ke iq mz b gy ni nf l ng nh">recipe_sequence_to_string(dataset_vectorized[0])</span></pre><p id="c422" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="9c52" class="nd ke iq mz b gy ne nf l ng nh">📗 Slow Cooker Chicken and Dumplings</span><span id="226b" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="9811" class="nd ke iq mz b gy ni nf l ng nh">• 4 skinless, boneless chicken breast halves <br/>• 2 tablespoons butter <br/>• 2 (10.75 ounce) cans condensed cream of chicken soup <br/>• 1 onion, finely diced <br/>• 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces </span><span id="514e" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="0304" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.<br/>▪︎ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.</span></pre><h1 id="9927" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">给序列添加填充</h1><p id="19d4" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们需要所有的食谱都有相同的训练长度。为此，我们将使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences" rel="noopener ugc nofollow" target="_blank">TF . keras . preprocessing . sequence . pad _ sequences</a>实用程序在每个配方的末尾添加一个停止字，并使它们具有相同的长度。</p><p id="4706" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们检查一下食谱的长度:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b930" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> recipe_index, recipe <strong class="mz ir">in</strong> enumerate(dataset_vectorized[:10]):<br/>    print('Recipe #{} length: {}'.format(recipe_index + 1, len(recipe)))</span></pre><p id="a5da" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e1c8" class="nd ke iq mz b gy ne nf l ng nh">Recipe #1 length: 546<br/>Recipe #2 length: 401<br/>Recipe #3 length: 671<br/>Recipe #4 length: 736<br/>Recipe #5 length: 1518<br/>Recipe #6 length: 740<br/>Recipe #7 length: 839<br/>Recipe #8 length: 667<br/>Recipe #9 length: 1264<br/>Recipe #10 length: 854</span></pre><p id="617d" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们把所有的食谱都加上一个<code class="fe nj nk nl mz b">STOP_SIGN</code>:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5d95" class="nd ke iq mz b gy ne nf l ng nh">dataset_vectorized_padded_without_stops = tf.keras.preprocessing.sequence.pad_sequences(<br/>    dataset_vectorized,<br/>    padding='post',<br/>    truncating='post',<br/>    <em class="lz"># We use -1 here and +1 in the next step to make sure</em><br/>    <em class="lz"># that all recipes will have at least 1 stops sign at the end,</em><br/>    <em class="lz"># since each sequence will be shifted and truncated afterwards</em><br/>    <em class="lz"># (to generate X and Y sequences).</em><br/>    maxlen=MAX_RECIPE_LENGTH-1,<br/>    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]<br/>)</span><span id="0a44" class="nd ke iq mz b gy ni nf l ng nh">dataset_vectorized_padded = tf.keras.preprocessing.sequence.pad_sequences(<br/>    dataset_vectorized_padded_without_stops,<br/>    padding='post',<br/>    truncating='post',<br/>    maxlen=MAX_RECIPE_LENGTH+1,<br/>    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]<br/>)</span><span id="b532" class="nd ke iq mz b gy ni nf l ng nh"><strong class="mz ir">for</strong> recipe_index, recipe <strong class="mz ir">in</strong> enumerate(dataset_vectorized_padded[:10]):<br/>    print('Recipe #{} length: {}'.format(recipe_index, len(recipe)))</span></pre><p id="45de" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="43ee" class="nd ke iq mz b gy ne nf l ng nh">Recipe #0 length: 2001<br/>Recipe #1 length: 2001<br/>Recipe #2 length: 2001<br/>Recipe #3 length: 2001<br/>Recipe #4 length: 2001<br/>Recipe #5 length: 2001<br/>Recipe #6 length: 2001<br/>Recipe #7 length: 2001<br/>Recipe #8 length: 2001<br/>Recipe #9 length: 2001</span></pre><p id="a929" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">填充后，数据集中的所有食谱现在都具有相同的长度，RNN也将能够知道每个食谱在哪里停止(通过观察<code class="fe nj nk nl mz b">STOP_SIGN</code>的出现)。</p><p id="52b9" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">以下是填充后第一个配方的外观示例。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5657" class="nd ke iq mz b gy ne nf l ng nh">recipe_sequence_to_string(dataset_vectorized_padded[0])</span></pre><p id="4ba6" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="662a" class="nd ke iq mz b gy ne nf l ng nh">📗 Slow Cooker Chicken and Dumplings</span><span id="cf7f" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="e810" class="nd ke iq mz b gy ni nf l ng nh">• 4 skinless, boneless chicken breast halves <br/>• 2 tablespoons butter <br/>• 2 (10.75 ounce) cans condensed cream of chicken soup <br/>• 1 onion, finely diced <br/>• 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces </span><span id="354e" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="9b5a" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.<br/>▪︎ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span></pre><p id="4ec3" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">现在所有的食谱都以一个或多个<code class="fe nj nk nl mz b">␣</code>符号结尾。我们希望我们的LSTM模型知道，每当它看到<code class="fe nj nk nl mz b">␣</code>停止字符时，它就意味着食谱结束了。一旦网络学会了这个概念，它就会在每个新生成的菜谱的末尾加上停止字符。</p><h1 id="14cd" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">创建张量流数据集</h1><p id="942c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">到目前为止，我们是像处理NumPy数组一样处理数据集的。如果我们将数据集NumPy数组转换成<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据集</a>，在训练过程中会更方便。它将赋予我们使用<code class="fe nj nk nl mz b">batch()</code>、<code class="fe nj nk nl mz b">shuffle()</code>、<code class="fe nj nk nl mz b">repeat()</code>、<code class="fe nj nk nl mz b">prefecth()</code>等助手功能的能力。：</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5686" class="nd ke iq mz b gy ne nf l ng nh">dataset = tf.data.Dataset.from_tensor_slices(dataset_vectorized_padded)</span><span id="20ec" class="nd ke iq mz b gy ni nf l ng nh">print(dataset)</span></pre><p id="5906" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e372" class="nd ke iq mz b gy ne nf l ng nh">&lt;TensorSliceDataset shapes: (2001,), types: tf.int32&gt;</span></pre><p id="8a6e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们这次通过使用TensorFlow数据集API来看看数据集中的第一个配方是什么样的:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="9c49" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> recipe <strong class="mz ir">in</strong> dataset.take(1):<br/>    print('Raw recipe:\n', recipe.numpy(), '\n\n\n')<br/>    print('Stringified recipe:\n')<br/>    recipe_sequence_to_string(recipe.numpy())</span></pre><p id="ad36" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="37eb" class="nd ke iq mz b gy ne nf l ng nh">Raw recipe:<br/> [ 51   1  33 ... 165 165 165] </span><span id="ee6e" class="nd ke iq mz b gy ni nf l ng nh">Stringified recipe:</span><span id="454d" class="nd ke iq mz b gy ni nf l ng nh">📗 Slow Cooker Chicken and Dumplings</span><span id="9fda" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="92d7" class="nd ke iq mz b gy ni nf l ng nh">• 4 skinless, boneless chicken breast halves <br/>• 2 tablespoons butter <br/>• 2 (10.75 ounce) cans condensed cream of chicken soup <br/>• 1 onion, finely diced <br/>• 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces </span><span id="934a" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="0361" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.<br/>▪︎ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span></pre><h1 id="ae11" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><code class="fe nj nk nl mz b">input</code>和<code class="fe nj nk nl mz b">target</code>文本上的拆分示例</h1><p id="d3f1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于每个序列，我们需要复制并移动它，以形成<code class="fe nj nk nl mz b">input</code>和<code class="fe nj nk nl mz b">target</code>文本。例如，假设<code class="fe nj nk nl mz b">sequence_length</code>是<code class="fe nj nk nl mz b">4</code>，我们的文本是<code class="fe nj nk nl mz b">Hello</code>。输入序列是<code class="fe nj nk nl mz b">Hell</code>，目标序列是<code class="fe nj nk nl mz b">ello</code>。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="773c" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">split_input_target</strong>(recipe):<br/>    input_text = recipe[:-1]<br/>    target_text = recipe[1:]<br/>    <br/>    <strong class="mz ir">return</strong> input_text, target_text</span><span id="aae6" class="nd ke iq mz b gy ni nf l ng nh">dataset_targeted = dataset.map(split_input_target)</span><span id="870a" class="nd ke iq mz b gy ni nf l ng nh">print(dataset_targeted)</span></pre><p id="4d6a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="263b" class="nd ke iq mz b gy ne nf l ng nh">&lt;MapDataset shapes: ((2000,), (2000,)), types: (tf.int32, tf.int32)&gt;</span></pre><p id="a20e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">您可能会从上面的行中注意到，现在数据集中的每个示例都由两个元组组成:输入和目标。让我们打印一个例子:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="7e3a" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> input_example, target_example <strong class="mz ir">in</strong> dataset_targeted.take(1):<br/>    print('Input sequence size:', repr(len(input_example.numpy())))<br/>    print('Target sequence size:', repr(len(target_example.numpy())))<br/>    print()<br/>    <br/>    input_stringified = tokenizer.sequences_to_texts([input_example.numpy()[:50]])[0]<br/>    target_stringified = tokenizer.sequences_to_texts([target_example.numpy()[:50]])[0]<br/>    <br/>    print('Input:  ', repr(''.join(input_stringified)))<br/>    print('Target: ', repr(''.join(target_stringified)))</span></pre><p id="2321" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b956" class="nd ke iq mz b gy ne nf l ng nh">Input sequence size: 2000<br/>Target sequence size: 2000</span><span id="f33d" class="nd ke iq mz b gy ni nf l ng nh">Input:   '📗   S l o w   C o o k e r   C h i c k e n   a n d   D u m p l i n g s \n \n 🥕 \n \n •   4   s k i n l e'<br/>Target:  '  S l o w   C o o k e r   C h i c k e n   a n d   D u m p l i n g s \n \n 🥕 \n \n •   4   s k i n l e s'</span></pre><p id="37ca" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">RNN将这些向量的每个索引作为一个时间步长进行处理。对于时间步长<code class="fe nj nk nl mz b">0</code>的输入，模型接收<code class="fe nj nk nl mz b">📗</code>的索引，并尝试预测`` `(一个空格字符)的索引作为下一个字符。在下一个时间步，它做同样的事情，但是RNN除了考虑当前输入字符外，还考虑前一步的上下文。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="0f26" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> i, (input_idx, target_idx) <strong class="mz ir">in</strong> enumerate(zip(input_example[:10], target_example[:10])):<br/>    print('Step {:2d}'.format(i + 1))<br/>    print('  input: {} ({:s})'.format(input_idx, repr(tokenizer.sequences_to_texts([[input_idx.numpy()]])[0])))<br/>    print('  expected output: {} ({:s})'.format(target_idx, repr(tokenizer.sequences_to_texts([[target_idx.numpy()]])[0])))</span></pre><p id="9a11" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="dd30" class="nd ke iq mz b gy ne nf l ng nh">Step  1<br/>  input: 51 ('📗')<br/>  expected output: 1 (' ')<br/>Step  2<br/>  input: 1 (' ')<br/>  expected output: 33 ('S')<br/>Step  3<br/>  input: 33 ('S')<br/>  expected output: 10 ('l')<br/>Step  4<br/>  input: 10 ('l')<br/>  expected output: 5 ('o')<br/>Step  5<br/>  input: 5 ('o')<br/>  expected output: 23 ('w')<br/>Step  6<br/>  input: 23 ('w')<br/>  expected output: 1 (' ')<br/>Step  7<br/>  input: 1 (' ')<br/>  expected output: 35 ('C')<br/>Step  8<br/>  input: 35 ('C')<br/>  expected output: 5 ('o')<br/>Step  9<br/>  input: 5 ('o')<br/>  expected output: 5 ('o')<br/>Step 10<br/>  input: 5 ('o')<br/>  expected output: 25 ('k')</span></pre><h1 id="0ee6" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">将数据集分成几批</h1><p id="4202" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们在数据集中有<code class="fe nj nk nl mz b">~100k</code>个食谱，每个食谱有两个由<code class="fe nj nk nl mz b">2000</code>字符组成的元组。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="283b" class="nd ke iq mz b gy ne nf l ng nh">print(dataset_targeted)</span></pre><p id="00f3" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="a448" class="nd ke iq mz b gy ne nf l ng nh">&lt;MapDataset shapes: ((2000,), (2000,)), types: (tf.int32, tf.int32)&gt;</span></pre><p id="8da9" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们打印常量值:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="dd88" class="nd ke iq mz b gy ne nf l ng nh">print('TOTAL_RECIPES_NUM: ', TOTAL_RECIPES_NUM)<br/>print('MAX_RECIPE_LENGTH: ', MAX_RECIPE_LENGTH)<br/>print('VOCABULARY_SIZE: ', VOCABULARY_SIZE)</span></pre><p id="e54f" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b837" class="nd ke iq mz b gy ne nf l ng nh">TOTAL_RECIPES_NUM:  100212<br/>MAX_RECIPE_LENGTH:  2000<br/>VOCABULARY_SIZE:  176</span></pre><p id="58af" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">如果我们在训练过程中将完整的数据集提供给模型，然后尝试一次对所有示例进行反向传播，我们可能会耗尽内存，并且每个训练时期可能需要太长时间来执行。为了避免这种情况，我们需要将数据集分成几批。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="f99f" class="nd ke iq mz b gy ne nf l ng nh"><em class="lz"># Batch size.</em><br/>BATCH_SIZE = 64</span><span id="3e69" class="nd ke iq mz b gy ni nf l ng nh"><em class="lz"># Buffer size to shuffle the dataset (TF data is designed to work</em><br/><em class="lz"># with possibly infinite sequences, so it doesn't attempt to shuffle</em><br/><em class="lz"># the entire sequence in memory. Instead, it maintains a buffer in</em><br/><em class="lz"># which it shuffles elements).</em><br/>SHUFFLE_BUFFER_SIZE = 1000</span><span id="072a" class="nd ke iq mz b gy ni nf l ng nh">dataset_train = dataset_targeted \<br/>  <em class="lz"># Shuffling examples first.</em><br/>  .shuffle(SHUFFLE_BUFFER_SIZE) \<br/>  <em class="lz"># Splitting examples on batches.</em><br/>  .batch(BATCH_SIZE, drop_remainder=<strong class="mz ir">True</strong>) \<br/>  <em class="lz"># Making a dataset to be repeatable (it will never ends). </em><br/>  .repeat()</span><span id="7f4b" class="nd ke iq mz b gy ni nf l ng nh">print(dataset_train)</span></pre><p id="2c47" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="6592" class="nd ke iq mz b gy ne nf l ng nh">&lt;RepeatDataset shapes: ((64, 2000), (64, 2000)), types: (tf.int32, tf.int32)&gt;</span></pre><p id="71ae" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">从上面的行中，您可能会注意到我们的数据集现在由相同的两个<code class="fe nj nk nl mz b">2000</code>字符元组组成，但是现在它们被<code class="fe nj nk nl mz b">64</code>分组到批处理中。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="79fd" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> input_text, target_text <strong class="mz ir">in</strong> dataset_train.take(1):<br/>    print('1st batch: input_text:', input_text)<br/>    print()<br/>    print('1st batch: target_text:', target_text)</span></pre><p id="0e39" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="f719" class="nd ke iq mz b gy ne nf l ng nh">1st batch: input_text: tf.Tensor(<br/>[[ 51   1  54 ... 165 165 165]<br/> [ 51   1  64 ... 165 165 165]<br/> [ 51   1  44 ... 165 165 165]<br/> ...<br/> [ 51   1  69 ... 165 165 165]<br/> [ 51   1  55 ... 165 165 165]<br/> [ 51   1  70 ... 165 165 165]], shape=(64, 2000), dtype=int32)</span><span id="c0b6" class="nd ke iq mz b gy ni nf l ng nh">1st batch: target_text: tf.Tensor(<br/>[[  1  54   4 ... 165 165 165]<br/> [  1  64   5 ... 165 165 165]<br/> [  1  44   6 ... 165 165 165]<br/> ...<br/> [  1  69   3 ... 165 165 165]<br/> [  1  55   3 ... 165 165 165]<br/> [  1  70   2 ... 165 165 165]], shape=(64, 2000), dtype=int32)</span></pre><h1 id="4847" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">建立模型</h1><p id="80d2" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" rel="noopener ugc nofollow" target="_blank"> tf.keras.Sequential </a>来定义模型。在本实验中，我们将使用以下图层类型:</p><ul class=""><li id="2904" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding" rel="noopener ugc nofollow" target="_blank">TF . keras . layers . embedding</a>—输入层(一个可训练的查找表，将每个字符的数字映射到一个具有<code class="fe nj nk nl mz b">embedding_dim</code>维度的向量)，</li><li id="d806" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank"> tf.keras.layers.LSTM </a> —一种大小为<code class="fe nj nk nl mz b">units=rnn_units</code>的RNN(这里也可以使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU" rel="noopener ugc nofollow" target="_blank"> GRU </a>图层)，</li><li id="d629" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank">TF . keras . layers . dense</a>—输出层，带有<code class="fe nj nk nl mz b">VOCABULARY_SIZE</code>输出。</li></ul><h1 id="74a3" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">弄清楚嵌入层是如何工作的</h1><p id="c8e6" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们快速迂回一下，看看嵌入层是如何工作的。它接受几个字符索引序列(批处理)作为输入。它将每个序列的每个字符编码成一个长度为<code class="fe nj nk nl mz b">tmp_embedding_size</code>的向量。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="7fb5" class="nd ke iq mz b gy ne nf l ng nh">tmp_vocab_size = 10<br/>tmp_embedding_size = 5<br/>tmp_input_length = 8<br/>tmp_batch_size = 2</span><span id="d9df" class="nd ke iq mz b gy ni nf l ng nh">tmp_model = tf.keras.models.Sequential()<br/>tmp_model.add(tf.keras.layers.Embedding(<br/>  input_dim=tmp_vocab_size,<br/>  output_dim=tmp_embedding_size,<br/>  input_length=tmp_input_length<br/>))<br/><em class="lz"># The model will take as input an integer matrix of size (batch, input_length).</em><br/><em class="lz"># The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).</em><br/><em class="lz"># Now model.output_shape == (None, 10, 64), where None is the batch dimension.</em><br/>tmp_input_array = np.random.randint(<br/>  low=0,<br/>  high=tmp_vocab_size,<br/>  size=(tmp_batch_size, tmp_input_length)<br/>)<br/>tmp_model.compile('rmsprop', 'mse')<br/>tmp_output_array = tmp_model.predict(tmp_input_array)</span><span id="0b0d" class="nd ke iq mz b gy ni nf l ng nh">print('tmp_input_array shape:', tmp_input_array.shape)<br/>print('tmp_input_array:')<br/>print(tmp_input_array)<br/>print()<br/>print('tmp_output_array shape:', tmp_output_array.shape)<br/>print('tmp_output_array:')<br/>print(tmp_output_array)</span></pre><p id="22c5" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="edc0" class="nd ke iq mz b gy ne nf l ng nh">tmp_input_array shape: (2, 8)<br/>tmp_input_array:<br/>[[2 4 7 5 1 6 9 7]<br/> [3 6 8 1 4 0 1 2]]</span><span id="3dd2" class="nd ke iq mz b gy ni nf l ng nh">tmp_output_array shape: (2, 8, 5)<br/>tmp_output_array:<br/>[[[-0.02229502 -0.02800617 -0.0120693  -0.01681594 -0.00650246]<br/>  [-0.03046973 -0.03920818  0.04956308  0.04417323 -0.00446874]<br/>  [-0.0215276   0.01532575 -0.02229529  0.02834387  0.02725342]<br/>  [ 0.04567988  0.0141306   0.00877035 -0.02601192  0.00380837]<br/>  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]<br/>  [ 0.04598364 -0.00704358 -0.01386416  0.01195388 -0.00309662]<br/>  [-0.00137572  0.01275543 -0.02348721 -0.04825885  0.00527108]<br/>  [-0.0215276   0.01532575 -0.02229529  0.02834387  0.02725342]]</span><span id="0168" class="nd ke iq mz b gy ni nf l ng nh"> [[ 0.01082945  0.03824175 -0.00450991 -0.02865709  0.02502238]<br/>  [ 0.04598364 -0.00704358 -0.01386416  0.01195388 -0.00309662]<br/>  [ 0.02275398  0.03806095 -0.03491788  0.04705564  0.00167596]<br/>  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]<br/>  [-0.03046973 -0.03920818  0.04956308  0.04417323 -0.00446874]<br/>  [-0.02909902  0.04426369  0.00150937  0.04579213  0.02559013]<br/>  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]<br/>  [-0.02229502 -0.02800617 -0.0120693  -0.01681594 -0.00650246]]]</span></pre><h1 id="bf3c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">LSTM模型</h1><p id="0173" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们组装模型。</p><p id="123b" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">ℹ️您可以使用TensorFlow文档中的RNN 笔记本来检查<a class="ae kc" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">文本生成，以了解有关模型组件的更多详细信息。</a></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="dea4" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">build_model</strong>(vocab_size, embedding_dim, rnn_units, batch_size):<br/>    model = tf.keras.models.Sequential()</span><span id="4fce" class="nd ke iq mz b gy ni nf l ng nh">    model.add(tf.keras.layers.Embedding(<br/>        input_dim=vocab_size,<br/>        output_dim=embedding_dim,<br/>        batch_input_shape=[batch_size, <strong class="mz ir">None</strong>]<br/>    ))</span><span id="d1bf" class="nd ke iq mz b gy ni nf l ng nh">    model.add(tf.keras.layers.LSTM(<br/>        units=rnn_units,<br/>        return_sequences=<strong class="mz ir">True</strong>,<br/>        stateful=<strong class="mz ir">True</strong>,<br/>        recurrent_initializer=tf.keras.initializers.GlorotNormal()<br/>    ))</span><span id="4dcf" class="nd ke iq mz b gy ni nf l ng nh">    model.add(tf.keras.layers.Dense(vocab_size))<br/>    <br/>    <strong class="mz ir">return</strong> model</span><span id="7fba" class="nd ke iq mz b gy ni nf l ng nh">model = build_model(<br/>  vocab_size=VOCABULARY_SIZE,<br/>  embedding_dim=256,<br/>  rnn_units=1024,<br/>  batch_size=BATCH_SIZE<br/>)</span><span id="4ac5" class="nd ke iq mz b gy ni nf l ng nh">model.summary()</span></pre><p id="c665" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="bead" class="nd ke iq mz b gy ne nf l ng nh">Model: "sequential_13"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_13 (Embedding)     (64, None, 256)           45056     <br/>_________________________________________________________________<br/>lstm_9 (LSTM)                (64, None, 1024)          5246976   <br/>_________________________________________________________________<br/>dense_8 (Dense)              (64, None, 176)           180400    <br/>=================================================================<br/>Total params: 5,472,432<br/>Trainable params: 5,472,432<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="3775" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们来看看这个模型:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="8a5f" class="nd ke iq mz b gy ne nf l ng nh">tf.keras.utils.plot_model(<br/>    model,<br/>    show_shapes=<strong class="mz ir">True</strong>,<br/>    show_layer_names=<strong class="mz ir">True</strong>,<br/>    to_file='model.png'<br/>)</span></pre><p id="7140" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/7b0abb4c4acc4cc4996ef915380d2096.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/0*H9Q3pEXwB1_1hV6f.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">网络架构(代码生成的映像)</p></figure><p id="29fc" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">对于每个字符，模型查找嵌入，以嵌入作为输入运行LSTM一个时间步长，并应用密集层来生成预测下一个字符的对数似然的逻辑:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/ae79b16dbf18dd6b93dd1373dfd71d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GxVDsOcMjVp07Lrn.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来源:<a class="ae kc" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">用RNN </a>笔记本生成文本。</p></figure><p id="720a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">上图展示了GRU网络，但是你可以很容易地用LSTM代替GRU。</p><h1 id="5431" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">在训练前尝试模型</h1><p id="8b0f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们试验一下未经训练的模型，看看它的界面(我们需要什么样的输入，我们将有什么样的输出),并看看在训练之前模型预测了什么:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="20a2" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">for</strong> input_example_batch, target_example_batch <strong class="mz ir">in</strong> dataset_train.take(1):<br/>    example_batch_predictions = model(input_example_batch)<br/>    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")</span></pre><p id="57b4" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="8858" class="nd ke iq mz b gy ne nf l ng nh">(64, 2000, 176) # (batch_size, sequence_length, vocab_size)</span></pre><p id="95c0" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了从模型中获得实际的预测，我们需要从输出分布中取样，以获得实际的字符索引。这种分布是由字符词汇表上的逻辑定义的。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="adf7" class="nd ke iq mz b gy ne nf l ng nh">print('Prediction for the 1st letter of the batch 1st sequense:')<br/>print(example_batch_predictions[0, 0])</span></pre><p id="cd37" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="cada" class="nd ke iq mz b gy ne nf l ng nh">Prediction for the 1st letter of the batch 1st sequense:<br/>tf.Tensor(<br/>[-9.0643829e-03 -1.9503604e-03  9.3381782e-04  3.7442446e-03<br/> -2.0541784e-03 -7.4054599e-03 -7.1884273e-03  2.6014952e-03<br/>  4.8721582e-03  3.0045470e-04  2.6016519e-04 -4.1374690e-03<br/>  5.3856964e-03  2.6284808e-03 -5.6002503e-03  2.6019611e-03<br/> -1.9491187e-03 -3.1097094e-04  6.3465843e-03  1.4640498e-03<br/>  2.4560774e-03 -3.1256995e-03  1.4104056e-03  2.5478401e-04<br/>  5.4266443e-03 -4.1188141e-03  3.6904984e-03 -5.8337618e-03<br/>  3.6372752e-03 -3.1899021e-05  3.2178329e-03  1.5033322e-04<br/>  5.2770867e-04 -8.1920059e-04 -2.2364906e-03 -2.3271297e-03<br/>  4.4109682e-03  4.2381673e-04  1.0532180e-03 -1.4208974e-03<br/> -3.2446394e-03 -4.5869066e-03  4.3250201e-04 -4.3490473e-03<br/>  3.7889536e-03 -9.2122913e-04  7.8936084e-04 -9.7079907e-04<br/>  1.7070504e-03 -2.5260956e-03  6.7904620e-03  1.5470090e-03<br/> -9.4337866e-04 -1.5072266e-03  6.8939931e-04 -1.0795534e-03<br/> -3.1912089e-03  2.3665284e-03  1.7737487e-03 -2.3504677e-03<br/> -6.8649277e-04  9.6421910e-04 -4.1204207e-03 -3.8750230e-03<br/>  1.9077851e-03  4.7145790e-05 -2.9846188e-03  5.8050319e-03<br/> -5.6210475e-04 -2.5910907e-04  5.2890396e-03 -5.8653783e-03<br/> -6.0040038e-06  2.3905798e-03 -2.9405006e-03  2.0132761e-03<br/> -3.5594390e-03  4.0282350e-04  4.7719614e-03 -2.4438011e-03<br/> -1.1028582e-03  2.0007135e-03 -1.6961874e-03 -4.2196750e-03<br/> -3.5689408e-03 -4.1934610e-03 -8.5307617e-04  1.5773368e-04<br/> -1.4612130e-03  9.5826073e-04  4.0543079e-04 -2.3562380e-04<br/> -1.5394683e-03  3.6650903e-03  3.5997448e-03  2.2390878e-03<br/> -6.8982318e-04  1.4068574e-03 -2.0531749e-03 -1.5443334e-03<br/> -1.8235333e-03 -3.2099178e-03  1.6660831e-03  1.2230751e-03<br/>  3.8084832e-03  6.9559496e-03  5.7684043e-03  3.1751506e-03<br/>  7.4234616e-04  1.1971325e-04 -2.7798198e-03  2.1485630e-03<br/>  4.0362971e-03  6.4410735e-05  1.7432809e-03  3.2334479e-03<br/> -6.1469898e-03 -2.2205685e-03 -1.0864032e-03 -2.0876178e-07<br/>  2.3065242e-03 -1.5816523e-03 -2.1492387e-03 -4.4033155e-03<br/>  1.1003019e-03 -9.7132073e-04 -6.3941808e-04  3.0277157e-03<br/>  2.9096641e-03 -2.4778468e-03 -2.9532036e-03  7.7463314e-04<br/>  2.7473709e-03 -7.6333171e-04 -8.1811845e-03 -1.3959130e-03<br/>  3.2840301e-03  6.0461317e-03 -1.3022404e-04 -9.4000692e-04<br/> -2.0096730e-04  3.3895797e-03  2.9710699e-03  1.9046264e-03<br/>  2.5092331e-03 -2.0799250e-04 -2.2211851e-04 -3.4621451e-05<br/>  1.9962704e-03 -2.3159904e-03  2.9832027e-03  3.3852295e-03<br/>  3.4411502e-04 -1.9019389e-03 -3.6734296e-04 -1.4232489e-03<br/>  2.6938838e-03 -2.8015859e-03 -5.7366290e-03  8.0239226e-04<br/> -6.2909431e-04  1.1508183e-03 -1.5899434e-04 -5.9326587e-04<br/> -4.1618512e-04  5.2454891e-03  1.2823739e-03 -1.7550631e-03<br/> -3.0120560e-03 -3.8433261e-03 -9.6873334e-04  1.9963509e-03<br/>  1.8154597e-03  4.7434499e-03  1.7146189e-03  1.1544267e-03], shape=(176,), dtype=float32)</span></pre><p id="68ef" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">对于每个输入字符，<code class="fe nj nk nl mz b">example_batch_predictions</code>数组包含下一个字符可能是什么的概率向量。如果向量中位置<code class="fe nj nk nl mz b">15</code>处的概率是<code class="fe nj nk nl mz b">0.3</code>，位置<code class="fe nj nk nl mz b">25</code>处的概率是<code class="fe nj nk nl mz b">1.1</code>，这意味着我们最好选择索引为<code class="fe nj nk nl mz b">25</code>的字符作为下一个字符。</p><p id="19c6" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">由于我们希望我们的网络生成不同的配方(即使对于相同的输入)，我们不能只选择最大概率值。在这种情况下，我们将以网络一遍又一遍地预测相同的配方而告终。我们要做的是通过使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/random/categorical" rel="noopener ugc nofollow" target="_blank">TF . random . categorial()</a>函数从预测中提取<strong class="ld ir">样本</strong>(就像上面打印的一样)。会给网络带来一些模糊性。例如，假设我们有字符<code class="fe nj nk nl mz b">H</code>作为输入，然后，通过从分类分布中取样，我们的网络不仅可以预测单词<code class="fe nj nk nl mz b">He</code>，还可以预测单词<code class="fe nj nk nl mz b">Hello</code>和<code class="fe nj nk nl mz b">Hi</code>等。</p><h1 id="8f39" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">了解<code class="fe nj nk nl mz b">tf.random.categorical</code>如何运作</h1><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="3976" class="nd ke iq mz b gy ne nf l ng nh"><em class="lz"># logits is 2-D Tensor with shape [batch_size, num_classes].</em><br/><em class="lz"># Each slice [i, :] represents the unnormalized log-probabilities for all classes.</em><br/><em class="lz"># In the example below we say that the probability for class "0"</em><br/><em class="lz"># (element with index 0) is low but the probability for class "2" is much higher.</em><br/>tmp_logits = [<br/>  [-0.95, 0, 0.95],<br/>];</span><span id="d219" class="nd ke iq mz b gy ni nf l ng nh"><em class="lz"># Let's generate 5 samples. Each sample is a class index. Class probabilities </em><br/><em class="lz"># are being taken into account (we expect to see more samples of class "2").</em><br/>tmp_samples = tf.random.categorical(<br/>    logits=tmp_logits,<br/>    num_samples=5<br/>)</span><span id="aa5d" class="nd ke iq mz b gy ni nf l ng nh">print(tmp_samples)</span></pre><p id="0d7c" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5795" class="nd ke iq mz b gy ne nf l ng nh">tf.Tensor([[2 1 2 2 1]], shape=(1, 5), dtype=int64)</span></pre><h1 id="3326" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">从LSTM预测中取样</h1><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5fc2" class="nd ke iq mz b gy ne nf l ng nh">sampled_indices = tf.random.categorical(<br/>    logits=example_batch_predictions[0],<br/>    num_samples=1<br/>)</span><span id="c938" class="nd ke iq mz b gy ni nf l ng nh">sampled_indices = tf.squeeze(<br/>    input=sampled_indices,<br/>    axis=-1<br/>).numpy()</span><span id="9680" class="nd ke iq mz b gy ni nf l ng nh">sampled_indices.shape</span></pre><p id="9caa" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="bf2c" class="nd ke iq mz b gy ne nf l ng nh">(2000,)</span></pre><p id="1405" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们来看看菜谱前<code class="fe nj nk nl mz b">100</code>个字符的一些预测示例:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="48d2" class="nd ke iq mz b gy ne nf l ng nh">sampled_indices[:100]</span></pre><p id="4b1d" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="aff7" class="nd ke iq mz b gy ne nf l ng nh">array([ 64,  21,  91, 126, 170,  42, 146,  54, 125, 164,  60, 171,   9,<br/>        87, 129,  28, 146, 103,  41, 101, 147,   3, 134, 171,   8, 170,<br/>       105,   5,  44, 173,   5, 105,  17, 138, 165,  32,  88,  96, 145,<br/>        83,  33,  65, 172, 162,   8,  29, 147,  58,  81, 153, 150,  56,<br/>       156,  38, 144, 134,  13,  40,  17,  50,  27,  35,  39, 112,  63,<br/>       139, 151, 133,  68,  29,  91,   2,  70, 112, 135,  31,  26, 156,<br/>       118,  71,  49, 104,  75,  27, 164,  41, 117, 124,  18, 137,  59,<br/>       160, 158, 119, 173,  50,  78,  45, 121, 118])</span></pre><p id="c577" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">我们现在可以看到我们未经训练的模型实际上预测了什么:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="86f6" class="nd ke iq mz b gy ne nf l ng nh">print('Input:\n', repr(''.join(tokenizer.sequences_to_texts([input_example_batch[0].numpy()[:50]]))))<br/>print()<br/>print('Next char prediction:\n', repr(''.join(tokenizer.sequences_to_texts([sampled_indices[:50]]))))</span></pre><p id="308f" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="ef99" class="nd ke iq mz b gy ne nf l ng nh">Input:<br/> '📗   R e s t a u r a n t - S t y l e   C o l e s l a w   I \n \n 🥕 \n \n •   1   ( 1 6   o u n c e )   p'</span><span id="e78f" class="nd ke iq mz b gy ni nf l ng nh">Next char prediction:<br/> 'H . î ⁄ ă ( “ I º Â 8 ̀ s % ù y “ © 0 ’ ‧ a ì ̀ r ă + o A € o + m × ␣ ︎ ñ ç ‱ ! S : ⅞ ´ r 2 ‧ D Q Á'</span></pre><p id="4650" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">正如您可能看到的，该模型提出了一些无意义的预测，但这是因为它尚未经过训练。</p><h1 id="689e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">训练模型</h1><p id="ed69" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们希望训练我们的模型来生成尽可能与真实食谱相似的食谱。我们将使用数据集中的所有数据进行训练。在这种情况下，不需要提取测试或验证子集。</p><h1 id="46f3" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">附加一个优化器和一个损失函数</h1><p id="c64d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank">TF . keras . optimizer . Adam</a>优化器和<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy" rel="noopener ugc nofollow" target="_blank">TF . keras . loss . sparse _ categorial _ cross entropy()</a>损失函数来训练模型:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="fb87" class="nd ke iq mz b gy ne nf l ng nh"><em class="lz"># An objective function.</em><br/><em class="lz"># The function is any callable with the signature scalar_loss = fn(y_true, y_pred).</em><br/><strong class="mz ir">def</strong> <strong class="mz ir">loss</strong>(labels, logits):<br/>    entropy = tf.keras.losses.sparse_categorical_crossentropy(<br/>      y_true=labels,<br/>      y_pred=logits,<br/>      from_logits=<strong class="mz ir">True</strong><br/>    )<br/>    <br/>    <strong class="mz ir">return</strong> entropy</span><span id="6775" class="nd ke iq mz b gy ni nf l ng nh">example_batch_loss = loss(target_example_batch, example_batch_predictions)</span><span id="b451" class="nd ke iq mz b gy ni nf l ng nh">print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")<br/>print("scalar_loss.shape:      ", example_batch_loss.shape)<br/>print("scalar_loss:      ", example_batch_loss.numpy().mean())</span></pre><p id="e0cf" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="e8ff" class="nd ke iq mz b gy ne nf l ng nh">Prediction <strong class="mz ir">shape: </strong>   (64, 2000, 176)  <em class="lz"># (batch_size, sequence_length, vocab_size)</em><br/><strong class="mz ir">scalar_loss.shape: </strong>  (64, 2000)<br/><strong class="mz ir">scalar_loss: </strong>        5.1618285</span></pre><p id="0296" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们最后编译这个模型:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="b1ab" class="nd ke iq mz b gy ne nf l ng nh">adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)</span><span id="31fb" class="nd ke iq mz b gy ni nf l ng nh">model.compile(<br/>    optimizer=adam_optimizer,<br/>    loss=loss<br/>)</span></pre><h1 id="b497" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">配置回调</h1><h2 id="1708" class="nd ke iq bd kf nt nu dn kj nv nw dp kn lm nx ny kr lq nz oa kv lu ob oc kz od bi translated">提前停止回调</h2><p id="eb74" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于模型训练过程，我们可以配置一个<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank">TF . keras . callbacks . early stopping</a>回调。如果模型在几个时期内没有改善，它将自动停止训练:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="18ec" class="nd ke iq mz b gy ne nf l ng nh">early_stopping_callback = tf.keras.callbacks.EarlyStopping(<br/>    patience=5,<br/>    monitor='loss',<br/>    restore_best_weights=<strong class="mz ir">True</strong>,<br/>    verbose=1<br/>)</span></pre><h2 id="6d3b" class="nd ke iq bd kf nt nu dn kj nv nw dp kn lm nx ny kr lq nz oa kv lu ob oc kz od bi translated">模型检查点回调</h2><p id="6e6c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们还配置一个<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint" rel="noopener ugc nofollow" target="_blank">TF . keras . callbacks . model check point</a>检查点，它将允许我们定期将训练好的权重保存到文件中，以便我们可以在以后从权重中恢复模型。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5675" class="nd ke iq mz b gy ne nf l ng nh"><em class="lz"># Create a checkpoints directory.</em><br/>checkpoint_dir = 'tmp/checkpoints'<br/>os.makedirs(checkpoint_dir, exist_ok=<strong class="mz ir">True</strong>)</span><span id="a4e0" class="nd ke iq mz b gy ni nf l ng nh">checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')<br/>checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(<br/>    filepath=checkpoint_prefix,<br/>    save_weights_only=<strong class="mz ir">True</strong><br/>)</span></pre><h1 id="c5fc" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">执行培训</h1><p id="cc03" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们为<code class="fe nj nk nl mz b">500</code>个时期训练我们的模型，每个时期有<code class="fe nj nk nl mz b">1500</code>个步骤。对于每个历元步，将取出一批<code class="fe nj nk nl mz b">64</code>配方，并对那些长度为<code class="fe nj nk nl mz b">2000</code>的<code class="fe nj nk nl mz b">64</code>配方逐步执行梯度下降。</p><p id="41a0" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">如果您正在试验训练参数，可能有必要将历元数减少到，比如说<code class="fe nj nk nl mz b">20</code>以及每个历元的步数，然后看看模型在该条件下的表现。如果模型提高了性能，您可以向训练过程添加更多数据(步骤和时期)。当你调整参数时，它可能会节省你一些时间。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="cec5" class="nd ke iq mz b gy ne nf l ng nh">EPOCHS = 500<br/>INITIAL_EPOCH = 1<br/>STEPS_PER_EPOCH = 1500</span><span id="6a9d" class="nd ke iq mz b gy ni nf l ng nh">print('EPOCHS:          ', EPOCHS)<br/>print('INITIAL_EPOCH:   ', INITIAL_EPOCH)<br/>print('STEPS_PER_EPOCH: ', STEPS_PER_EPOCH)</span></pre><p id="9cfa" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="abd3" class="nd ke iq mz b gy ne nf l ng nh">EPOCHS:           500<br/>INITIAL_EPOCH:    1<br/>STEPS_PER_EPOCH:  1500</span></pre><p id="8499" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们开始培训:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="14d9" class="nd ke iq mz b gy ne nf l ng nh">history = model.fit(<br/>    x=dataset_train,<br/>    epochs=EPOCHS,<br/>    steps_per_epoch=STEPS_PER_EPOCH,<br/>    initial_epoch=INITIAL_EPOCH,<br/>    callbacks=[<br/>        checkpoint_callback,<br/>        early_stopping_callback<br/>    ]<br/>)</span><span id="64d9" class="nd ke iq mz b gy ni nf l ng nh"><em class="lz"># Saving the trained model to file (to be able to re-use it later).</em><br/>model_name = 'recipe_generation_rnn_raw.h5'<br/>model.save(model_name, save_format='h5')</span></pre><h1 id="59e6" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">可视化培训进度</h1><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="f989" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">render_training_history</strong>(training_history):<br/>    loss = training_history.history['loss']</span><span id="26cc" class="nd ke iq mz b gy ni nf l ng nh">    plt.title('Loss')<br/>    plt.xlabel('Epoch')<br/>    plt.ylabel('Loss')<br/>    plt.plot(loss, label='Training set')<br/>    plt.legend()<br/>    plt.grid(linestyle='--', linewidth=1, alpha=0.5)<br/>    plt.show()</span><span id="f8c9" class="nd ke iq mz b gy ni nf l ng nh">render_training_history(history)</span></pre><p id="dd72" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1057d593ae128705de7e46788ab414e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*e7dShhkFtiy9sfqI.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">培训流程(代码生成的图像)</p></figure><p id="cdf6" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">ℹ️ <em class="lz">在上面的图表中，只显示了前10个时期。</em></p><p id="a6c7" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">从图表中我们可以看出，在培训期间，模特的表现越来越好。这意味着该模型学习预测下一个字符，使得最终序列看起来类似于一些真实的食谱文本。</p><h1 id="003c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">生成配方</h1><h1 id="f260" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">从最新的检查点恢复模型</h1><p id="6fce" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了使这个预测步骤简单，我们将恢复保存的模型，并以批处理大小1重新构建它。由于RNN状态是从一个时间步长传递到另一个时间步长的，该模型一旦建立就只接受固定的批量大小。为了使用不同的<code class="fe nj nk nl mz b">batch_size</code>运行模型，我们需要重建模型并从检查点恢复权重。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="6da4" class="nd ke iq mz b gy ne nf l ng nh">tf.train.latest_checkpoint(checkpoint_dir)</span></pre><p id="ed4d" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="96e7" class="nd ke iq mz b gy ne nf l ng nh">'tmp/checkpoints/ckpt_1'</span></pre><p id="dc86" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们重建批量为<code class="fe nj nk nl mz b">1</code>的模型，并向其加载训练好的权重:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5e34" class="nd ke iq mz b gy ne nf l ng nh">simplified_batch_size = 1</span><span id="e85a" class="nd ke iq mz b gy ni nf l ng nh">model_simplified = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)<br/>model_simplified.load_weights(tf.train.latest_checkpoint(checkpoint_dir))<br/>model_simplified.build(tf.TensorShape([simplified_batch_size, <strong class="mz ir">None</strong>]))</span><span id="decc" class="nd ke iq mz b gy ni nf l ng nh">model_simplified.summary()</span></pre><p id="dcf1" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5ad4" class="nd ke iq mz b gy ne nf l ng nh">Model: "sequential_6"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_6 (Embedding)      (1, None, 256)            45056     <br/>_________________________________________________________________<br/>lstm_5 (LSTM)                (1, None, 1024)           5246976   <br/>_________________________________________________________________<br/>dense_5 (Dense)              (1, None, 176)            180400    <br/>=================================================================<br/>Total params: 5,472,432<br/>Trainable params: 5,472,432<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="464a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">让我们仔细检查输入形状是否简化了:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="64a5" class="nd ke iq mz b gy ne nf l ng nh">model_simplified.input_shape</span></pre><p id="8d6a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="ba77" class="nd ke iq mz b gy ne nf l ng nh">(1, None)</span></pre><h1 id="217f" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">预测循环</h1><p id="6a95" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了使用我们训练过的模型来生成配方，我们需要实现一个所谓的预测循环。下面的代码块使用循环生成文本:</p><ul class=""><li id="c081" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">它首先选择一个起始字符串，初始化RNN状态，并设置要生成的字符数。</li><li id="2ed4" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">它使用起始字符串和RNN状态获得下一个字符的预测分布。</li><li id="6e87" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">然后，它使用分类分布来计算预测字符的索引。它使用这个预测的字符作为模型的下一个输入。</li><li id="2454" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">模型返回的RNN状态被反馈到模型中，因此它现在有更多的上下文，而不是只有一个字符。预测下一个字符后，修改后的RNN状态再次反馈到模型中，这是它从先前预测的字符中获得更多上下文时的学习方式。</li></ul><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/8eab59200718d544ed8ab9d4dd03862e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*glH5Eo3QaA9x1_uF.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来源:<a class="ae kc" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">用RNN </a>笔记本生成文本。</p></figure><p id="15fb" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">这里的<code class="fe nj nk nl mz b">temperature</code>参数定义了生成的配方有多模糊或者有多出乎意料。温度越低，文本越容易预测。更高的温度导致更令人惊讶的文本。你需要尝试找到最佳设置。我们将在下面不同的温度下做一些实验。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="93b4" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">generate_text</strong>(model, start_string, num_generate = 1000, temperature=1.0):<br/>    <em class="lz"># Evaluation step (generating text using the learned model)</em><br/>    <br/>    padded_start_string = STOP_WORD_TITLE + start_string</span><span id="6867" class="nd ke iq mz b gy ni nf l ng nh">    <em class="lz"># Converting our start string to numbers (vectorizing).</em><br/>    input_indices = np.array(tokenizer.texts_to_sequences([padded_start_string]))</span><span id="493d" class="nd ke iq mz b gy ni nf l ng nh">    <em class="lz"># Empty string to store our results.</em><br/>    text_generated = []</span><span id="3c28" class="nd ke iq mz b gy ni nf l ng nh">    <em class="lz"># Here batch size == 1.</em><br/>    model.reset_states()<br/>    <strong class="mz ir">for</strong> char_index <strong class="mz ir">in</strong> range(num_generate):<br/>        predictions = model(input_indices)<br/>        <em class="lz"># remove the batch dimension</em><br/>        predictions = tf.squeeze(predictions, 0)</span><span id="2651" class="nd ke iq mz b gy ni nf l ng nh">        <em class="lz"># Using a categorical distribution to predict the character returned by the model.</em><br/>        predictions = predictions / temperature<br/>        predicted_id = tf.random.categorical(<br/>            predictions,<br/>            num_samples=1<br/>        )[-1, 0].numpy()</span><span id="ea22" class="nd ke iq mz b gy ni nf l ng nh">        <em class="lz"># We pass the predicted character as the next input to the model</em><br/>        <em class="lz"># along with the previous hidden state.</em><br/>        input_indices = tf.expand_dims([predicted_id], 0)<br/>        <br/>        next_character = tokenizer.sequences_to_texts(input_indices.numpy())[0]</span><span id="7a69" class="nd ke iq mz b gy ni nf l ng nh">        text_generated.append(next_character)</span><span id="fd55" class="nd ke iq mz b gy ni nf l ng nh">    <strong class="mz ir">return</strong> (padded_start_string + ''.join(text_generated))</span></pre><h1 id="e533" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为预测回路计算出合适的温度</h1><p id="9d41" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在，让我们使用<code class="fe nj nk nl mz b">generate_text()</code>来实际生成一些新的食谱。<code class="fe nj nk nl mz b">generate_combinations()</code>功能会检查第一个配方字母和温度的所有可能组合。它生成<code class="fe nj nk nl mz b">56</code>不同的组合来帮助我们弄清楚模型的表现以及使用什么温度更好。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="eadb" class="nd ke iq mz b gy ne nf l ng nh"><strong class="mz ir">def</strong> <strong class="mz ir">generate_combinations</strong>(model):<br/>    recipe_length = 1000<br/>    try_letters = ['', '\n', 'A', 'B', 'C', 'O', 'L', 'Mushroom', 'Apple', 'Slow', 'Christmass', 'The', 'Banana', 'Homemade']<br/>    try_temperature = [1.0, 0.8, 0.4, 0.2]</span><span id="5adb" class="nd ke iq mz b gy ni nf l ng nh">    <strong class="mz ir">for</strong> letter <strong class="mz ir">in</strong> try_letters:<br/>        <strong class="mz ir">for</strong> temperature <strong class="mz ir">in</strong> try_temperature:<br/>            generated_text = generate_text(<br/>                model,<br/>                start_string=letter,<br/>                num_generate = recipe_length,<br/>                temperature=temperature<br/>            )<br/>            print(f'Attempt: "{letter}" + {temperature}')<br/>            print('-----------------------------------')<br/>            print(generated_text)<br/>            print('\n\n')</span></pre><p id="079c" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了避免这篇文章太长，下面将只列出一些<code class="fe nj nk nl mz b">56</code>的组合。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="2ede" class="nd ke iq mz b gy ne nf l ng nh">generate_combinations(model_simplified)</span></pre><p id="16f2" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="lz"> ➔输出:</em></p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="05b6" class="nd ke iq mz b gy ne nf l ng nh">Attempt: "A" + 1.0<br/>-----------------------------------<br/>📗 Azzeric Sweet Potato Puree</span><span id="7f01" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="8ecc" class="nd ke iq mz b gy ni nf l ng nh">• 24 large baking potatoes, such as Carn or Marinara or 1 (14-ounce) can pot wine<br/>• 1/4 pound unsalted butter, cut into small pieces<br/>• 1/2 cup coarsely chopped scallions</span><span id="3ad7" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="e858" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Bring a large pot of water to a boil, place a large nonstick skillet over medium-high heat, add All Naucocal Volves. Reduce heat to medium and cook the potatoes until just cooked through, bubbles before adding the next layer, about 10 to 12 minutes. Remove ground beans and reserve. Reserve the crumb mixture for about 6 greased. Let cool 2 minutes. Strain soak into a glass pitcher. Let cool in ice. Add short-goodfish to the batter and stir to dissolve. Pour in the cheese mixture and whisk until smooth. Set aside for 20 seconds more. Remove dumplings and cheese curds. Spread 1/3 cup of the mixture on each circle for seal ballo. Transfer mixture into a greased 9-by-11-inch baking dish and chill for 20 minutes.<br/>▪︎ Bake, covered, for 30 minutes. Serve warm.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span><span id="f5f6" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "A" + 0.4<br/>-----------------------------------<br/>📗 Apricot "Cookie" Cakes</span><span id="332b" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="4789" class="nd ke iq mz b gy ni nf l ng nh">• 1 cup all-purpose flour<br/>• 1 cup corn flour<br/>• 1 cup sugar<br/>• 1 tablespoon baking powder<br/>• 1 teaspoon salt<br/>• 1 teaspoon ground cinnamon<br/>• 1 cup grated Parmesan<br/>• 1 cup pecans, chopped<br/>• 1/2 cup chopped pecans<br/>• 1/2 cup raisins</span><span id="ca8e" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="924d" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat oven to 350 degrees F.<br/>▪︎ Butter and flour a 9 by 13-inch baking dish. In a medium bowl, whisk together the flour, sugar, baking powder, baking soda and salt. In a small bowl, whisk together the eggs, sugar, and eggs. Add the flour mixture to the butter mixture and mix until just combined. Stir in the raisins and pecans and transfer to the prepared pan. Spread the batter over the top of the crust. Bake for 15 minutes. Reduce the oven temperature to 350 degrees F, and bake until the cupcakes are set and the top is golden brown, about 20 minutes more. Transfer the cake to a wire rack to cool to room temperature. Refrigerate until ready to serve.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span><span id="5361" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "A" + 0.2<br/>-----------------------------------<br/>📗 Alternative to the Fondant</span><span id="479a" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="0507" class="nd ke iq mz b gy ni nf l ng nh">• 1 cup sugar<br/>• 1 cup water<br/>• 1 cup heavy cream<br/>• 1 teaspoon vanilla extract<br/>• 1/2 cup heavy cream<br/>• 1/2 cup heavy cream<br/>• 1 teaspoon vanilla extract<br/>• 1/2 cup chopped pecans</span><span id="3b31" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="d50c" class="nd ke iq mz b gy ni nf l ng nh">▪︎ In a saucepan over medium heat, combine the sugar, sugar, and corn syrup. Cook over medium heat until the sugar is dissolved. Remove from the heat and stir in the vanilla. Refrigerate until cold. Stir in the chocolate chips and the chocolate chips. Serve immediately.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span><span id="7ff0" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "B" + 0.4<br/>-----------------------------------<br/>📗 Battered French Toast with Bacon, Bacon, and Caramelized Onions and Pecorino</span><span id="023d" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="969e" class="nd ke iq mz b gy ni nf l ng nh">• 1/2 pound squid (shredded carrots)<br/>• 1 small onion, diced<br/>• 1 small green pepper, seeded and cut into strips<br/>• 1 red bell pepper, stemmed, seeded and cut into 1/4-inch dice<br/>• 1 small onion, chopped<br/>• 1 green bell pepper, chopped<br/>• 1 cup chicken stock<br/>• 1 cup heavy cream<br/>• 1/2 cup shredded sharp Cheddar<br/>• 1 teaspoon ground cumin<br/>• 1 teaspoon salt<br/>• 1 teaspoon freshly ground black pepper</span><span id="fdc9" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="7549" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat the oven to 350 degrees F.<br/>▪︎ For the bacon mixture: In a large bowl, combine the cheese, sour cream, mustard, salt, pepper, and hot sauce. Stir together and mix well. Fold in the milk and set aside.<br/>▪︎ For the filling: In a large bowl, mix the flour and salt and pepper, to taste. Add the beaten eggs and mix to combine. Set aside.<br/>▪︎ For the topping: Mix the cream cheese with the mayonnaise, salt and pepper in a medium bowl. Add the chicken and toss to coat the other side. Transfer the mixture to the prepared</span><span id="757a" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "C" + 1.0<br/>-----------------------------------<br/>📗 Crema battered Salmon</span><span id="b21d" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="d8e7" class="nd ke iq mz b gy ni nf l ng nh">• 1 cup fresh cranberries (from 4 tablespoons left of 4 egg whites)<br/>• 3 teaspoons sugar<br/>• 1 tablespoon unsalted butter<br/>• 2 tablespoons truffle oil<br/>• Coarse salt<br/>• Freshly ground black pepper</span><span id="9afc" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="b3c6" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place cornmeal in a small serving bowl, and combine it. Drizzle milk over the plums and season with salt and pepper. Let stand for about 5 minutes, until firm. Serve immediately.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span><span id="2b3f" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "C" + 0.8<br/>-----------------------------------<br/>📗 Classic Iseasteroles</span><span id="e5c6" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="96ac" class="nd ke iq mz b gy ni nf l ng nh">• 3 cups milk<br/>• 3/4 cup coconut milk<br/>• 1/2 cup malted maple syrup<br/>• 1/2 teaspoon salt<br/>• 3 cups sugar<br/>• 4 1-inch strawberries, sliced into 1/4-inch pieces<br/>• 1/2 teaspoon ground cinnamon</span><span id="15c4" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="470c" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Place the cherries in a small saucepan; sprinkle with the sugar. Bring to a simmer over medium-low heat, then remove from the heat. Let stand until the coconut fluffy, about 15 to 20 minutes. Drain the coconut oil in a stream, whisking until combined. Add the cream, espresso and cocoa powder and stir to combine. Cover and refrigerate until ready to serve. Makes 10 to 12 small springs in the same fat from the surface of the bowl, which using paper colors, and freeze overnight.<br/>▪︎ Meanwhile, combine the cream, sugar, vanilla and salt in a medium saucepan. Cook over medium heat until the sugar dissolves and the sugar melts and begins to boil, about 5 minutes. Remove from the heat and stir in the vanilla.<br/>▪︎ To serve, carefully remove the pops from the casserole and put them in</span><span id="05e1" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "C" + 0.4<br/>-----------------------------------<br/>📗 Cinnamon Corn Cakes with Coconut Flour and Saffron Sauce</span><span id="bf0b" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="cac2" class="nd ke iq mz b gy ni nf l ng nh">• 3 cups shredded sharp Cheddar<br/>• 1 cup grated Parmesan<br/>• 2 cups shredded sharp Cheddar<br/>• 1 cup grated Parmesan<br/>• 1 cup shredded part-skim mozzarella cheese<br/>• 1 cup grated Parmesan<br/>• 1 cup grated Parmesan<br/>• 1 cup grated Parmesan<br/>• 1 teaspoon kosher salt<br/>• 1/2 teaspoon freshly ground black pepper</span><span id="1f0c" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="b228" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat the oven to 400 degrees F. Line a baking sheet with a silpat and preheat the oven to 350 degrees F.<br/>▪︎ In a large bowl, combine the masa harina, cumin, cayenne, and salt and pepper. Dredge the pasta in the flour and then dip in the egg mixture, then dip in the eggs, then dip in the egg mixture and then dredge in the breadcrumbs. Place the breaded cheese on a sheet tray. Bake until the crust is golden brown and the filling is bubbling, about 25 to 30 minutes. Remove from the oven and serve hot.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣<br/></span><span id="dd44" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "L" + 0.4<br/>-----------------------------------<br/>📗 Lighted Flan with Chocolate and Pecans</span><span id="b2ab" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="d099" class="nd ke iq mz b gy ni nf l ng nh">• 2 cups milk<br/>• 1 cup sugar<br/>• 1 teaspoon vanilla extract<br/>• 1 cup heavy cream<br/>• 1/2 cup heavy cream<br/>• 1 tablespoon powdered sugar<br/>• 1 teaspoon vanilla extract<br/>• 1/2 cup heavy cream<br/>• 1/2 teaspoon ground cinnamon<br/>• 1/2 teaspoon ground nutmeg<br/>• 1/2 cup chopped pecans</span><span id="6c69" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="1b4b" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Watch how to make this recipe.<br/>▪︎ In a small saucepan, combine the sugar, salt, and a pinch of salt. Cook over medium heat, stirring occasionally, until the sugar has dissolved. Remove from the heat and set aside to cool. Remove the cherries from the refrigerator and place in the freezer for 1 hour.<br/>▪︎ In a blender, combine the milk, sugar, vanilla, salt and water. Blend until smooth. Pour the mixture into a 9-by-13-inch glass baking dish and set aside.<br/>▪︎ In a small saucepan, combine the remaining 2 cups sugar, the vanilla, and 2 cups water. Bring the mixture to a boil, and then reduce the heat to low. Cook until the sugar is dissolved, about 5 minutes. Remove from the heat an</span><span id="0308" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "L" + 0.2<br/>-----------------------------------<br/>📗 Lighted Fondanta with Chocolate and Cream Cheese Frosting</span><span id="3261" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="85f8" class="nd ke iq mz b gy ni nf l ng nh">• 1 cup heavy cream<br/>• 1 tablespoon sugar<br/>• 1 tablespoon vanilla extract<br/>• 1 teaspoon vanilla extract<br/>• 1 cup heavy cream<br/>• 1 cup heavy cream<br/>• 1/2 cup sugar<br/>• 1 teaspoon vanilla extract<br/>• 1 teaspoon vanilla extract<br/>• 1/2 cup chopped pistachios</span><span id="63b6" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="6a4a" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat the oven to 350 degrees F.<br/>▪︎ In a large bowl, combine the cream cheese, sugar, eggs, vanilla, and salt. Stir until smooth. Pour the mixture into the prepared baking dish. Sprinkle with the remaining 1/2 cup sugar and bake for 15 minutes. Reduce the heat to 350 degrees F and bake until the crust is golden brown, about 15 minutes more. Remove from the oven and let cool completely. Spread the chocolate chips on the parchment paper and bake until the chocolate is melted and the top is golden brown, about 10 minutes. Set aside to cool.<br/>▪︎ In a medium bowl, whisk together the egg yolks, sugar, and vanilla until smooth. Stir in the cream and continue to beat until the chocolate</span><span id="5a7b" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "Mushroom" + 1.0<br/>-----------------------------------<br/>📗 Mushroom and Bacon Soup with Jumbo Sugar Coating</span><span id="4536" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="1ac3" class="nd ke iq mz b gy ni nf l ng nh">• 2 tablespoons vegetable oil<br/>• 1 2/3 pounds red cabbage, shredded, about 4 cups of excess pasted dark ends of fat, and pocked or firm<br/>• 2 red bell peppers, cored, seeded and diced<br/>• 1 poblano pepper, chopped<br/>• 3 medium carrots, finely chopped<br/>• 1/2 medium pinch saffron<br/>• 4 cups water<br/>• 2 cups mushrooms or 1/2 cup frozen Sojo Bean red<br/>• Salt and freshly ground black pepper<br/>• 1 pound andouille sausage<br/>• 1 gallon vegetable broth<br/>• Chopped fresh parsley, cilantro leaves, for garnish</span><span id="7782" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="1aae" class="nd ke iq mz b gy ni nf l ng nh">▪︎ In a large Dutch oven for gas burner, heat oil over moderate heat. Add the leeks to the pot, scraping the bottom of the skillet. Add the beans and sausage and sprinkle the reserved potatoes with some orange juice cooked sausage (such as The Sauce.) Add roasted vegetables and pinto beans, mozzarella, basil and bamboo shoots. Simmer rice until soup is absorbed, 15 to 20 minutes.<br/>▪︎ Bring another pan of water to a boil and cook shrimp for 5 minutes. While onions</span><span id="4b13" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "Mushroom" + 0.8<br/>-----------------------------------<br/>📗 Mushrooms with Lentil Stewed Shallots and Tomatoes</span><span id="6f8f" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="a5eb" class="nd ke iq mz b gy ni nf l ng nh">• 1 tablespoon olive oil<br/>• 3 cloves garlic, smashed<br/>• Kosher salt<br/>• 1 1/2 pounds lean ground turkey<br/>• 1 cup coarsely peeled tart apples<br/>• 2 tablespoons chopped garlic<br/>• 1 teaspoon ground cumin<br/>• 1/2 teaspoon cayenne pepper<br/>• 1 teaspoon chopped fresh thyme<br/>• 3/4 cup chopped fresh basil<br/>• 1/2 small carrot, halved lengthwise and cut into 1/2-inch pieces<br/>• 1 roasted red pepper, halved and sliced vertically diced and separated into rough chops<br/>• 3 tablespoons unsalted butter<br/>• 2 cups shredded mozzarella<br/>• 1/4 cup grated parmesan cheese<br/>• 1/4 cup prepared basil pesto</span><span id="7567" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="2b07" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Stir the olive oil, garlic, thyme and 1 teaspoon salt in a saucepan; bring to a simmer over medium heat. Remove from the heat. Add the basil and toast the soup for 2 minutes.<br/>▪︎ Meanwhile, heat 4 to 4 inches vegetable oil in the skillet over medium-high heat. Add the olive oil, garlic, 1/2 teaspoon salt and 1/2 teaspoon pepper and cook, stirring often, until cooked through, a</span><span id="fd57" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "Mushroom" + 0.4<br/>-----------------------------------<br/>📗 Mushroom Ravioli with Chickpeas and Shiitake Mushrooms and Sun-Dried Tomatoes</span><span id="2fd4" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="bfd8" class="nd ke iq mz b gy ni nf l ng nh">• 1 pound zucchini<br/>• 1 cup chicken broth<br/>• 1 cup fresh basil leaves<br/>• 1/2 cup chopped fresh basil leaves<br/>• 1/2 cup grated Parmesan<br/>• 1 teaspoon salt<br/>• 1/2 teaspoon freshly ground black pepper<br/>• 1 teaspoon chopped fresh thyme<br/>• 1 teaspoon fresh lemon juice<br/>• 2 cups chicken broth<br/>• 1/2 cup grated Parmesan<br/>• 1/2 cup grated Parmigiano-Reggiano</span><span id="0d5e" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="d6cc" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat oven to 450 degrees F.<br/>▪︎ Place the bread cubes in a large bowl. Add the basil, parsley, olive oil, parsley, thyme, basil, salt and pepper and toss to coat. Spread the mixture out on a baking sheet and bake until the sausages are cooked through, about 20 minutes. Serve immediately.<br/>▪︎ In a small saucepan, bring the chicken stock to a boil. Reduce the heat to low and cook the soup until the liquid is absorbed. Remove from the heat and stir in the parsley, shallots and season with salt and pepper. Serve immediately.<br/>␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣</span><span id="48d4" class="nd ke iq mz b gy ni nf l ng nh">Attempt: "Mushroom" + 0.2<br/>-----------------------------------<br/>📗 Mushroom and Spicy Sausage Stuffing</span><span id="5b2d" class="nd ke iq mz b gy ni nf l ng nh">🥕</span><span id="4ccd" class="nd ke iq mz b gy ni nf l ng nh">• 1 tablespoon olive oil<br/>• 1 medium onion, chopped<br/>• 2 cloves garlic, minced<br/>• 1 cup frozen peas<br/>• 1 cup frozen peas<br/>• 1/2 cup chopped fresh parsley<br/>• 1/2 cup grated Parmesan<br/>• 1/2 cup grated Parmesan<br/>• 1 teaspoon salt<br/>• 1/2 teaspoon freshly ground black pepper<br/>• 1 cup shredded mozzarella<br/>• 1/2 cup grated Parmesan<br/>• 1 cup shredded mozzarella<br/>• 1 cup shredded mozzarella cheese</span><span id="248d" class="nd ke iq mz b gy ni nf l ng nh">📝</span><span id="8ff1" class="nd ke iq mz b gy ni nf l ng nh">▪︎ Preheat the oven to 350 degrees F.<br/>▪︎ Bring a large pot of salted water to a boil. Add the pasta and cook until al dente, about 6 minutes. Drain and reserve.<br/>▪︎ Meanwhile, heat the olive oil in a large skillet over medium-high heat. Add the shallots and saute until tender, about 3 minutes. Add the garlic and cook for 1 minute. Add the sausage and cook until the shallots are tender, about 3 minutes. Add the sausage and cook until tender, about 2 minutes. Add the garlic and cook, stirring, until the garlic is lightly browned, about 1 minute. Add the sausage and cook until the s</span></pre><h1 id="6a80" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">交互式模型演示</h1><p id="74cb" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以使用🎨<a class="ae kc" href="https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">烹饪食谱生成器演示</strong> </a>使用这个模型，输入文本和温度参数，就可以在浏览器中生成一些随机的食谱。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2f3ff66a8e9cb41fa57a372f2538ad15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*uq71hvoe3yV0ixRH.gif"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来自<a class="ae kc" href="https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN" rel="noopener ugc nofollow" target="_blank">机器学习实验</a>的屏幕记录</p></figure><h1 id="6d5f" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">需要改进的地方</h1><p id="7ed7" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这超出了本文的范围，但是模型仍然有以下问题需要解决:</p><ul class=""><li id="c83b" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">我们需要删除配料部分的重复内容。</li><li id="5a75" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">食谱部分(名称、配料和烹饪步骤)大部分时间是不连贯的，这意味着我们可能会在配料部分看到<code class="fe nj nk nl mz b">mushrooms</code>，但它们不会在食谱名称或烹饪步骤中提及。</li></ul></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><blockquote class="on oo op"><p id="96cd" class="lb lc lz ld b le ma lg lh li mb lk ll oq mc lo lp or md ls lt os me lw lx ly ij bi translated">更多更新和新文章<a class="ae kc" href="https://twitter.com/Trekhleb" rel="noopener ugc nofollow" target="_blank">请在Twitter上关注我</a></p></blockquote></div></div>    
</body>
</html>