<html>
<head>
<title>The Most Complete Guide to PyTorch for Data Scientists</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向数据科学家的 PyTorch 最完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b?source=collection_archive---------9-----------------------#2020-09-07">https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b?source=collection_archive---------9-----------------------#2020-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/34ba7c6ccd67f72a6cc7901da45cb22d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHBMXuB-c2gNvZh2HLsu8A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://pixabay.com/users/Manuchi-1728328/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2462436" rel="noopener ugc nofollow" target="_blank">денисмарчук</a>来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2462436" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="e8fb" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">进行深度学习时需要的 PyTorch 的所有功能。从实验/研究的角度来看。</h2></div><p id="2e0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu"> PyTorch </em> </strong>现在已经成为创建神经网络的事实标准之一，我喜欢它的界面。然而，对于初学者来说，掌握它有点困难。</p><p id="c0ea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我记得几年前，在做了大量实验后，我选择了 PyTorch。说实话，我花了很多时间才学会，但我很高兴我从<a class="ae jg" rel="noopener" target="_blank" href="/moving-from-keras-to-pytorch-f0d4fff4ce79"> Keras 转到了 PyTorch </a>。<em class="lu">py torch 具有高度的可定制性和 pythonic 语法，使用起来非常愉快，我会向任何想用深度学习来做一些繁重工作的人推荐它。</em></p><p id="9555" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在本 PyTorch 指南中，<strong class="la jk"> <em class="lu">我将尝试用 PyTorch 减轻一些初学者的痛苦，并介绍一些在用 Pytorch 创建任何神经网络时需要的最重要的类和模块。</em></strong></p><p id="bb87" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，这并不是说这只是针对初学者的，因为<strong class="la jk"> <em class="lu">我还将谈到</em></strong><strong class="la jk"><em class="lu">py torch 提供的高度可定制性，并将谈到定制图层、数据集、数据加载器和损失函数</em> </strong>。</p><p id="e17a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想充分发挥 Pytorch 的能力，Exxact Corporation 有一系列基于人工智能的工作站和服务器，起价 3700 美元，配有几个英伟达 RTX 30 系列 GPU，3 年保修和深度学习软件堆栈。</p><p id="c6e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们去喝点咖啡吧，☕ ️and，开始吧。</p><p id="9e5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想看某一部分，这里有一个目录表。</p><p id="6dca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="#de86" rel="noopener ugc nofollow">张量</a>T2】∘<a class="ae jg" href="#22bf" rel="noopener ugc nofollow">1。创建一个张量</a>t5】∘<a class="ae jg" href="#136f" rel="noopener ugc nofollow">2。张量运算</a><br/><a class="ae jg" href="#ecb5" rel="noopener ugc nofollow">nn。模块</a> <br/> <a class="ae jg" href="#1a40" rel="noopener ugc nofollow">一言关于图层</a> <br/> <a class="ae jg" href="#b19e" rel="noopener ugc nofollow">数据集和数据加载器</a> <br/> ∘ <a class="ae jg" href="#6066" rel="noopener ugc nofollow">了解自定义数据集</a> <br/> ∘ <a class="ae jg" href="#8b4f" rel="noopener ugc nofollow">了解自定义数据加载器</a> <br/> <a class="ae jg" href="#f26b" rel="noopener ugc nofollow">训练神经网络</a> <br/> <a class="ae jg" href="#8913" rel="noopener ugc nofollow">损失函数</a> <br/> ∘ <a class="ae jg" href="#a312" rel="noopener ugc nofollow">自定义损失函数</a> <br/> <a class="ae jg" href="#23e8" rel="noopener ugc nofollow">优化器</a> <br/> <a class="ae jg" href="#f034" rel="noopener ugc nofollow">使用 GPU</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="de86" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">张量</h1><p id="f081" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">张量是 PyTorch 中的基本构建模块，简单来说，它们是 NumPy 数组，但在 GPU 上。在这一部分，我将列出一些我们在处理张量时最常用的运算。这绝不是你能用张量做的一个详尽的操作列表，但是在进入更令人兴奋的部分之前，理解什么是张量是有帮助的。</p><h2 id="22bf" class="mz md jj bd me na nb dn mi nc nd dp mm lh ne nf mo ll ng nh mq lp ni nj ms nk bi translated">1.创建一个张量</h2><p id="5dc8" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以用多种方法创建 PyTorch 张量。这包括从 NumPy 数组转换为张量。下面只是一个小要点和一些例子，但是你可以用张量做更多的事情，就像你可以用 NumPy 数组一样。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/571865b29a7960249346862229faee10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6YNEcv30WJGXnnuFi2BDow.png"/></div></div></figure><h2 id="136f" class="mz md jj bd me na nb dn mi nc nd dp mm lh ne nf mo ll ng nh mq lp ni nj ms nk bi translated">2.张量运算</h2><p id="835f" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">还是那句话，在这些张量上你可以做很多运算。完整的功能列表可在<a class="ae jg" href="https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/d21ec7c8fa18e36cc9229ff632253fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TJC7XpTdIfmgAmqNRcFYQ.png"/></div></div></figure><p id="0de5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">注:</strong>py torch 变量是什么？在 Pytorch 的早期版本中，张量和变量曾经是不同的，提供不同的功能，但现在变量 API<a class="ae jg" href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated" rel="noopener ugc nofollow" target="_blank">已弃用</a>，变量的所有方法都与张量一起工作。所以，如果你不知道他们，没关系，因为他们是不需要的，如果你知道他们，你可以忘记他们。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ecb5" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">nn。组件</h1><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/3696e0275796457f012c367f38e3321e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MSOVrrot5jMhA_Up"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Fernand De Canne 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="156e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的部分来了，我们现在要谈论一些在创建深度学习项目时 Pytorch 中最常用的构造。nn。模块可以让您将深度学习模型创建为一个类。你可以从<code class="fe nt nu nv nw b">nn.Module</code>继承，将任何模型定义为一个类。每个模型类必然包含一个<code class="fe nt nu nv nw b">__init__</code>程序块和一个<code class="fe nt nu nv nw b">forward</code>通道块。</p><ul class=""><li id="e113" class="nx ny jj la b lb lc le lf lh nz ll oa lp ob lt oc od oe of bi translated">在<code class="fe nt nu nv nw b">__init__</code>部分，用户可以定义网络将要拥有的所有层，但还没有定义这些层如何相互连接。</li><li id="4c0f" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">在<code class="fe nt nu nv nw b">forward</code>传递块中，用户定义数据如何在网络中从一层流向另一层。</li></ul><p id="5071" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，简单地说，我们定义的任何网络看起来都像:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ba1c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们定义了一个非常简单的网络，它接受大小为 784 的输入，并按顺序通过两个线性层。但是需要注意的是，在定义向前传递时，我们可以定义任何类型的计算，这使得 PyTorch 高度可定制以用于研究目的。例如，在我们疯狂的实验模式中，我们可能使用了下面的网络，在那里我们任意地附加我们的层。在这里，我们将第二个线性层的输出添加到第一个线性层(跳过连接)后再次发送回第一个线性层(老实说，我不知道这将做什么)。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="28d2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以检查神经网络前向传递是否有效。我通常这样做，首先创建一些随机输入，并通过我创建的网络传递。</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="be90" class="mz md jj nw b gy op oq l or os">x = torch.randn((100,784))<br/>model = myCrazyNeuralNet()<br/>model(x).size()<br/>--------------------------<br/>torch.Size([100, 10])</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1a40" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">关于层的一句话</h1><p id="6533" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Pytorch 非常强大，实际上您可以使用<code class="fe nt nu nv nw b">nn.Module</code>自己创建任何新的实验层。例如，我们可以创建自己的<strong class="la jk">自定义线性层</strong>，而不是使用上面 Pytorch 中的预定义线性层<code class="fe nt nu nv nw b">nn.Linear</code>。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6732" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以看到我们如何在<code class="fe nt nu nv nw b">nn.Parameter.</code>中包装我们的权重张量，这样做是为了使张量被认为是一个模型参数。来自 PyTorch <a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter" rel="noopener ugc nofollow" target="_blank">文档</a>:</p><blockquote class="ot ou ov"><p id="87df" class="ky kz lu la b lb lc kk ld le lf kn lg ow li lj lk ox lm ln lo oy lq lr ls lt im bi translated">参数是<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" rel="noopener ugc nofollow" target="_blank"><em class="jj">Tensor</em></a></code>的子类，当与<code class="fe nt nu nv nw b"><em class="jj">Module</em></code>一起使用时，它们有一个非常特殊的属性——当它们被指定为模块属性时，它们会被自动添加到参数列表中，并且会出现在<code class="fe nt nu nv nw b"><em class="jj">parameters()</em></code>迭代器中</p></blockquote><p id="1eb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您稍后将看到的，<code class="fe nt nu nv nw b">model.parameters()</code>迭代器将是优化器的一个输入。但稍后会详细介绍。</p><p id="334e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们可以在任何 PyTorch 网络中使用这个自定义层，就像任何其他层一样。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5cdc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是话又说回来，如果 Pytorch 没有提供许多在各种神经网络架构中经常使用的现成层，它就不会被如此广泛地使用。一些例子是:<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" rel="noopener ugc nofollow" target="_blank">nn.Linear</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" rel="noopener ugc nofollow" target="_blank">nn.Conv2d</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d" rel="noopener ugc nofollow" target="_blank">nn.MaxPool2d</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" rel="noopener ugc nofollow" target="_blank">nn.ReLU</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" rel="noopener ugc nofollow" target="_blank">nn.BatchNorm2d</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout" rel="noopener ugc nofollow" target="_blank">nn.Dropout</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" rel="noopener ugc nofollow" target="_blank">nn.Embedding</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU" rel="noopener ugc nofollow" target="_blank">nn.GRU</a>/<a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM" rel="noopener ugc nofollow" target="_blank">nn.LSTM</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax" rel="noopener ugc nofollow" target="_blank">nn.Softmax</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax" rel="noopener ugc nofollow" target="_blank">nn.LogSoftmax</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention" rel="noopener ugc nofollow" target="_blank">nn.MultiheadAttention</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder" rel="noopener ugc nofollow" target="_blank">nn.TransformerEncoder</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder" rel="noopener ugc nofollow" target="_blank">nn.TransformerDecoder</a></code></p><p id="ecf7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经将所有层链接到它们的来源，在那里您可以阅读关于它们的所有内容，但是为了显示我通常如何试图理解一个层和阅读文档，我将在这里尝试查看一个非常简单的卷积层。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/267f922e5952ccb4e8a6a1f633859f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ak5CX080dVtfTzwnBcQLiQ.png"/></div></div></figure><p id="9a36" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，Conv2d 层需要一个高度为 H、宽度为 W 的图像作为输入，并带有<code class="fe nt nu nv nw b">Cin</code>通道。现在，对于 convnet 中的第一层，<code class="fe nt nu nv nw b">in_channels</code>的数量是 3(RGB)，<code class="fe nt nu nv nw b">out_channels</code>的数量可以由用户定义。最常用的<code class="fe nt nu nv nw b">kernel_size</code>是 3x3，常用的<code class="fe nt nu nv nw b">stride</code>是 1。</p><p id="fbb7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了检查一个我不太了解的新图层，我通常会尝试查看该图层的输入和输出，如下图所示，我将首先初始化该图层:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="7bdc" class="mz md jj nw b gy op oq l or os">conv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1)</span></pre><p id="d37a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后通过它传递一些随机输入。这里 100 是批量大小。</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="364f" class="mz md jj nw b gy op oq l or os">x = torch.randn((100,3,24,24))<br/>conv_layer(x).size()<br/>--------------------------------<br/>torch.Size([100, 64, 24, 24])</span></pre><p id="c65c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们根据需要从卷积运算中获得输出，并且我有足够的信息来说明如何在我设计的任何神经网络中使用这一层。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b19e" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据集和数据加载器</h1><p id="dcf1" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在训练或测试时，我们如何将数据传递给我们的神经网络？我们肯定可以像上面那样传递张量，但 Pytorch 也为我们提供了预先构建的数据集，使我们更容易将数据传递给我们的神经网络。您可以查看<a class="ae jg" href="https://pytorch.org/docs/stable/torchvision/datasets.html" rel="noopener ugc nofollow" target="_blank"> torchvision.datasets </a>和<a class="ae jg" href="https://pytorch.org/text/datasets.html" rel="noopener ugc nofollow" target="_blank"> torchtext.datasets </a>提供的完整数据集列表。但是，为了给出数据集的具体示例，假设我们必须使用文件夹将图像传递到图像神经网络，该文件夹包含以下结构的图像:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="9511" class="mz md jj nw b gy op oq l or os">data<br/>    train<br/>        sailboat<br/>        kayak<br/>        .<br/>        .</span></pre><p id="e9c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用<code class="fe nt nu nv nw b">torchvision.datasets.ImageFolder</code>数据集获得如下示例图像:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/7b48d63bc08e0e076901005ee7109299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1tY8u-Ig_YycJ5kGTwsOg.png"/></div></div></figure><p id="06c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个数据集有 847 幅图像，我们可以使用索引来获取图像及其标签。现在我们可以使用 for 循环将图像逐个传递给任何图像神经网络:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="5543" class="mz md jj nw b gy op oq l or os">for i in range(0,len(train_dataset)):<br/>    image ,label = train_dataset[i]<br/>    pred = model(image)</span></pre><p id="ad4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">但那不是最优的。我们想做批处理。</em> </strong>我们其实可以多写一些代码，批量追加图片和标签，然后传递给神经网络。但是 Pytorch 为我们提供了一个实用的迭代器<code class="fe nt nu nv nw b">torch.utils.data.DataLoader</code>来实现这一点。现在我们可以简单地将我们的<code class="fe nt nu nv nw b">train_dataset</code>包装在数据加载器中，我们将获得批量而不是单个的例子。</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="8d05" class="mz md jj nw b gy op oq l or os">train_dataloader = <strong class="nw jk">DataLoader</strong>(train_dataset,batch_size = 64, shuffle=True, num_workers=10)</span></pre><p id="9038" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以简单地使用批处理进行迭代:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="c485" class="mz md jj nw b gy op oq l or os">for image_batch, label_batch in train_dataloader:<br/>    print(image_batch.size(),label_batch.size())<br/>    break<br/>------------------------------------------------------------------<br/>torch.Size([64, 3, 224, 224]) torch.Size([64])</span></pre><p id="1143" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以实际上，使用数据集和数据加载器的整个过程变成了:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d3fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我之前关于使用深度学习进行图像分类的博客文章中，你可以在这里看看这个具体的例子。</p><p id="37ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这很棒，Pytorch 确实提供了很多现成的功能。但是 Pytorch 的主要功能来自其巨大的定制性。如果 PyTorch 提供的数据集不适合我们的用例，我们也可以创建自己的自定义数据集。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="6066" class="mz md jj bd me na nb dn mi nc nd dp mm lh ne nf mo ll ng nh mq lp ni nj ms nk bi translated">了解自定义数据集</h2><p id="5093" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了编写自定义数据集，我们可以利用 Pytorch 提供的抽象类<code class="fe nt nu nv nw b">torch.utils.data.Dataset</code>。我们需要继承这个<code class="fe nt nu nv nw b">Dataset</code>类，并需要定义两个方法来创建自定义数据集。</p><ul class=""><li id="0871" class="nx ny jj la b lb lc le lf lh nz ll oa lp ob lt oc od oe of bi translated"><code class="fe nt nu nv nw b">__len__</code>:返回数据集大小的函数。在大多数情况下，这个代码很容易编写。</li><li id="f180" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated"><code class="fe nt nu nv nw b">__getitem__</code>:将索引<code class="fe nt nu nv nw b">i</code>作为输入并返回索引<code class="fe nt nu nv nw b">i</code>处的样本的函数。</li></ul><p id="80fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，我们可以创建一个简单的自定义数据集，从文件夹中返回一个图像和一个标签。请注意，大多数任务都发生在<code class="fe nt nu nv nw b">__init__</code>部分，我们使用<code class="fe nt nu nv nw b">glob.glob</code>来获取图像名称并进行一些常规预处理。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="e9fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另外，请注意，我们在<code class="fe nt nu nv nw b"> __getitem__</code>方法中一次打开一个图像，而不是在初始化时打开。在<code class="fe nt nu nv nw b">__init__</code>中没有这样做，因为我们不想把所有的图像都加载到内存中，只需要加载需要的图像。</p><p id="187e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以像以前一样使用这个数据集和实用程序<code class="fe nt nu nv nw b">Dataloader</code>。它就像 PyTorch 提供的前面的数据集一样工作，但是没有一些实用函数。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8b4f" class="mz md jj bd me na nb dn mi nc nd dp mm lh ne nf mo ll ng nh mq lp ni nj ms nk bi translated">了解自定义数据加载器</h2><p id="ff16" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated"><strong class="la jk">这一部分有点高深，在阅读这篇文章时可以跳过，因为在很多情况下都不需要。但是我在这里添加它是为了完整性。</strong></p><p id="a222" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设您希望向处理文本输入的网络提供批处理，只要批处理中的序列大小保持不变，网络就可以接收任意长度的序列。例如，我们可以有一个可以处理任意长度序列的 BiLSTM 网络。如果你现在不理解其中使用的图层，那也没关系；只知道它可以处理大小可变的序列。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="66ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该网络期望其输入为形状(<code class="fe nt nu nv nw b">batch_size</code>、<code class="fe nt nu nv nw b">seq_length</code>)，并与任何<code class="fe nt nu nv nw b">seq_length</code>一起工作。我们可以通过向模型传递两个不同序列长度(10 和 25)的随机批次来检查这一点。</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="8925" class="mz md jj nw b gy op oq l or os">model = BiLSTM()<br/>input_batch_1 = torch.randint(low = 0,high = 10000, size = (100,<strong class="nw jk">10</strong>))<br/>input_batch_2 = torch.randint(low = 0,high = 10000, size = (100,<strong class="nw jk">25</strong>))<br/>print(model(input_batch_1).size())<br/>print(model(input_batch_2).size())<br/>------------------------------------------------------------------<br/>torch.Size([100, 1])<br/>torch.Size([100, 1])</span></pre><p id="44ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们希望为这个模型提供紧密的批次，这样每个批次都具有基于批次中最大序列长度的相同序列长度，以最小化填充。这有一个额外的好处，使神经网络运行更快。事实上，这是在卡格尔举行的 Quora 无诚意挑战赛中获胜的方法之一，在那里运行时间至关重要。</p><p id="4932" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，我们该怎么做呢？让我们先编写一个非常简单的自定义数据集类。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6d54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，让我们生成一些随机数据，我们将使用这个自定义数据集。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/aedb690951bf337ba3c57637bef44c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZRHFvmuWHnXDXPn82Igp1g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个随机序列和标签的例子。序列中的每个整数对应于句子中的一个单词。</p></figure><p id="1b93" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以使用自定义数据集:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="6c1e" class="mz md jj nw b gy op oq l or os">train_dataset = CustomTextDataset(X,y)</span></pre><p id="008c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们现在尝试在这个数据集上使用带有<code class="fe nt nu nv nw b">batch_size</code> &gt; 1 的 Dataloader，我们将会得到一个错误。这是为什么呢？</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="142b" class="mz md jj nw b gy op oq l or os">train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10)<br/>for xb,yb in train_dataloader:<br/>    print(xb.size(),yb.size())</span></pre><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/a9d4eb0c404e50293ceaa242b83ceb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMHNSL0xQ_1Mfk8UVp4ZJg.png"/></div></div></figure><p id="57a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">发生这种情况是因为序列具有不同的长度，而我们的数据加载器期望我们的序列具有相同的长度。请记住，在前面的图像示例中，我们使用变换将所有图像的大小调整为 224，因此我们没有遇到这个错误。</p><p id="0ad4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">那么，我们如何遍历这个数据集，使得每一批都有长度相同的序列，但不同的批可能有不同的序列长度呢？</em>T13】</strong></p><p id="d9f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在 DataLoader 中使用<code class="fe nt nu nv nw b">collate_fn</code>参数，让我们定义如何在特定的批处理中堆叠序列。要使用它，我们需要定义一个函数，它将一个批处理作为输入，并根据批处理中的<code class="fe nt nu nv nw b">max_sequence_length</code>返回(<code class="fe nt nu nv nw b">x_batch</code>、<code class="fe nt nu nv nw b">y_batch</code>)填充的序列长度。我在下面的函数中使用的函数是简单的数字运算。此外，该函数被适当地注释，以便您可以理解发生了什么。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="8d92" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以将此<code class="fe nt nu nv nw b">collate_fn</code>与我们的数据加载器一起使用，如下所示:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="e43c" class="mz md jj nw b gy op oq l or os">train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,<strong class="nw jk">collate_fn</strong> <strong class="nw jk">=</strong> <strong class="nw jk">collate_text</strong>)</span><span id="a399" class="mz md jj nw b gy pd oq l or os">for xb,yb in train_dataloader:<br/>    print(xb.size(),yb.size())</span></pre><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/26c8f2b8a121241b59df9d2e50fd70ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3nw9Itcr779LCGTyMsPNQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">请注意，这些批次现在具有不同的序列长度</p></figure><p id="c4cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次它将工作，因为我们已经提供了一个定制的<code class="fe nt nu nv nw b">collate_fn.</code>,并看到现在这些批次具有不同的序列长度。因此，我们将能够像我们希望的那样，使用可变的输入大小来训练我们的 BiLSTM。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f26b" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">训练神经网络</h1><p id="954d" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们知道如何使用<code class="fe nt nu nv nw b">nn.Module.</code>创建神经网络，但是如何训练它呢？任何需要训练的神经网络都有一个类似于下图的训练循环:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="0de8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的代码中，我们运行五个时期，并且在每个时期中:</p><ol class=""><li id="6e20" class="nx ny jj la b lb lc le lf lh nz ll oa lp ob lt pf od oe of bi translated">我们使用数据加载器遍历数据集。</li><li id="a55e" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt pf od oe of bi translated">在每次迭代中，我们使用<code class="fe nt nu nv nw b">model(x_batch)</code>向前传递</li><li id="8e0a" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt pf od oe of bi translated">我们使用<code class="fe nt nu nv nw b">loss_criterion</code>计算损失</li><li id="c6c0" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt pf od oe of bi translated">我们使用<code class="fe nt nu nv nw b">loss.backward()</code>调用反向传播损失。我们根本不用担心梯度的计算，因为这个简单的调用为我们做了所有的事情。</li><li id="703f" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt pf od oe of bi translated">使用<code class="fe nt nu nv nw b">optimizer.step()</code>采取优化步骤来改变整个网络中的权重。这就是使用在<code class="fe nt nu nv nw b">loss.backward()</code>调用中计算的梯度来修改网络权重的地方。</li><li id="f392" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt pf od oe of bi translated">我们通过验证数据加载器来检查验证分数/指标。在进行验证之前，我们使用<code class="fe nt nu nv nw b">model.eval().</code>将模型设置为评估模式。请注意，我们不会在评估模式下反向传播损耗。</li></ol><p id="fb35" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，我们已经讨论了如何使用<code class="fe nt nu nv nw b">nn.Module</code>创建网络，以及如何通过 Pytorch 使用定制数据集和数据加载器。所以让我们来谈谈损失函数和优化器的各种可用选项。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8913" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">损失函数</h1><p id="6ecf" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Pytorch 为我们最常见的任务提供了各种损失函数，比如分类和回归。一些最常用的例子是<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">nn.CrossEntropyLoss</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" rel="noopener ugc nofollow" target="_blank">nn.NLLLoss</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss" rel="noopener ugc nofollow" target="_blank">nn.KLDivLoss</a></code>和<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss" rel="noopener ugc nofollow" target="_blank">nn.MSELoss</a>.</code>你可以阅读每个损失函数的文档，但是为了解释如何使用这些损失函数，我将通过<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" rel="noopener ugc nofollow" target="_blank">nn.NLLLoss</a></code>的例子</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/95069c88e36c0ca82e3b53ed290e11e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8BIZmDGA40xZ0w1OIM8QA.png"/></div></div></figure><p id="e03f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NLLLoss 的文档非常简洁。如中所示，此损失函数用于多类分类，并基于文档:</p><ul class=""><li id="7303" class="nx ny jj la b lb lc le lf lh nz ll oa lp ob lt oc od oe of bi translated">预期的输入需要大小为(<code class="fe nt nu nv nw b">batch_size</code> x <code class="fe nt nu nv nw b">Num_Classes</code> ) —这些是我们创建的神经网络的预测。</li><li id="fee7" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">我们需要输入中每个类的对数概率——为了从神经网络中获得对数概率，我们可以添加一个<code class="fe nt nu nv nw b">LogSoftmax</code>层作为网络的最后一层。</li><li id="cb90" class="nx ny jj la b lb og le oh lh oi ll oj lp ok lt oc od oe of bi translated">目标需要是类别数量在(0，C-1)范围内的类别的张量，其中 C 是类别的数量。</li></ul><p id="c8b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们可以尝试将这个损失函数用于简单的分类网络。请注意最后一个线性图层后的<code class="fe nt nu nv nw b">LogSoftmax</code>图层。如果你不想使用这个<code class="fe nt nu nv nw b">LogSoftmax</code>层，你可以直接使用<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">nn.CrossEntropyLoss</a></code></p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="bc27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们定义一个随机输入传递到我们的网络来测试它:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="8f42" class="mz md jj nw b gy op oq l or os"># some random input:</span><span id="5b90" class="mz md jj nw b gy pd oq l or os">X = torch.randn(100,784)<br/>y = torch.randint(low = 0,high = 10,size = (100,))</span></pre><p id="c0fb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并通过模型得到预测:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="9618" class="mz md jj nw b gy op oq l or os">model = myClassificationNet()<br/>preds = model(X)</span></pre><p id="b862" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以得到损失为:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="f407" class="mz md jj nw b gy op oq l or os">criterion = nn.NLLLoss()<br/>loss = criterion(preds,y)<br/>loss<br/>------------------------------------------<br/>tensor(2.4852, grad_fn=&lt;NllLossBackward&gt;)</span></pre><h2 id="a312" class="mz md jj bd me na nb dn mi nc nd dp mm lh ne nf mo ll ng nh mq lp ni nj ms nk bi translated">定制损失函数</h2><p id="47b2" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">定义你的自定义损失函数也是小菜一碟，只要你在损失函数中使用张量运算，你应该没问题。例如，这里是<code class="fe nt nu nv nw b">customMseLoss</code></p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="1d1a" class="mz md jj nw b gy op oq l or os">def customMseLoss(output,target):<br/>    loss = torch.mean((output - target)**2)     <br/>    <strong class="nw jk">return</strong> loss</span></pre><p id="0298" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以像以前一样使用这个自定义损耗。但是请注意，我们这次没有使用 criterion 实例化损失，因为我们已经将它定义为一个函数。</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="a559" class="mz md jj nw b gy op oq l or os">output = model(x)<br/>loss = customMseLoss(output, target)<br/>loss.backward()</span></pre><p id="da8a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们愿意，我们也可以用<code class="fe nt nu nv nw b">nn.Module</code>把它写成一个类，然后我们就可以把它当作一个对象来使用。下面是一个 NLLLoss 自定义示例:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="23e8" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">优化者</h1><p id="7c58" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">一旦我们使用<code class="fe nt nu nv nw b">loss.backward()</code>调用得到梯度，我们需要采取一个优化步骤来改变整个网络中的权重。Pytorch 使用<code class="fe nt nu nv nw b">torch.optim</code>模块提供了各种不同的现成优化器。比如:<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta" rel="noopener ugc nofollow" target="_blank">torch.optim.Adadelta</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad" rel="noopener ugc nofollow" target="_blank">torch.optim.Adagrad</a></code>、<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop" rel="noopener ugc nofollow" target="_blank">torch.optim.RMSprop</a></code>以及应用最广泛的<code class="fe nt nu nv nw b"><a class="ae jg" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" rel="noopener ugc nofollow" target="_blank">torch.optim.Adam</a>.</code></p><p id="7068" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要使用 PyTorch 中最常用的 Adam 优化器，我们可以简单地用以下代码实例化它:</p><pre class="nl nm nn no gt ol nw om on aw oo bi"><span id="432e" class="mz md jj nw b gy op oq l or os">optimizer <strong class="nw jk">=</strong> <!-- -->torch.<!-- -->optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))</span></pre><p id="db8e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后在训练模型时使用<code class="fe nt nu nv nw b">optimizer<strong class="la jk">.</strong>zero_grad()</code>和<code class="fe nt nu nv nw b">optimizer.step()</code>。</p><p id="7bc8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我不讨论如何编写定制优化器，因为这是一个不常见的用例，但如果你想有更多的优化器，请查看<a class="ae jg" href="https://pytorch-optimizer.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pytorch-optimizer </a>库，它提供了许多其他在研究论文中使用的优化器。此外，如果你想创建自己的优化器，你可以使用<a class="ae jg" href="https://github.com/pytorch/pytorch/tree/master/torch/optim" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>或<a class="ae jg" href="https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer" rel="noopener ugc nofollow" target="_blank">py torch-optimizer</a>中实现的优化器的源代码来获得灵感。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/b4d9233bda73c9f70ca32cbf0a4db08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llpKD4rsSDYcxXRIMmVppg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://github.com/jettify/pytorch-optimizer" rel="noopener ugc nofollow" target="_blank"> pytorch-optimizer </a>库的其他优化器</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f034" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">使用 GPU/多个 GPU</h1><p id="5f17" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">到目前为止，我们所做的一切都是在 CPU 上进行的。如果你想使用 GPU，你可以使用<code class="fe nt nu nv nw b">model.to('cuda')</code>将你的模型放到 GPU 中。或者想用多个 GPU，可以用<code class="fe nt nu nv nw b">nn.DataParallel</code>。这里有一个实用函数，它检查机器中的 GPU 数量，并在需要时使用<code class="fe nt nu nv nw b">DataParallel</code>自动设置并行训练。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c694" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们唯一需要改变的是，如果我们有 GPU，我们将在训练时将数据加载到 GPU。这就像在我们的训练循环中添加几行代码一样简单。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="42a8" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结论</h1><p id="5eb4" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Pytorch 用最少的代码提供了很多可定制性。虽然一开始可能很难理解整个生态系统是如何用类来构建的，但最终，它是简单的 Python。在这篇文章中，我试着分解了你在使用 Pytorch 时可能需要的大部分内容，我希望读完这篇文章后对你来说更有意义。</p><p id="2ac3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在我的<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo 上找到这篇文章的代码，我在这里保存了我所有博客的代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2475" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想学习更多关于 Pytorch 的基于课程的结构，看看 IBM 在 Coursera 上的<a class="ae jg" href="https://coursera.pxf.io/jWG2Db" rel="noopener ugc nofollow" target="_blank">深度神经网络与 PyTorch </a>课程。还有，如果你想了解更多关于深度学习的知识，我想推荐这门关于<a class="ae jg" href="https://coursera.pxf.io/NKERRq" rel="noopener ugc nofollow" target="_blank">计算机视觉中的深度学习</a>的优秀课程</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5cef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"><strong class="la jk"/></a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter<a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">@ mlwhiz</strong></a>联系</p><p id="e166" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p><div class="is it gp gr iu pi"><a href="https://mlwhiz.medium.com/membership" rel="noopener follow" target="_blank"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd jk gy z fp pn fr fs po fu fw ji bi translated">通过我的推荐链接加入 Medium-Rahul Agarwal</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">作为一个媒体会员，你的会员费的一部分给了你所阅读的作家，你可以在…上看到所有的故事</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">mlwhiz.medium.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw ja pi"/></div></div></a></div></div></div>    
</body>
</html>