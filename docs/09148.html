<html>
<head>
<title>Treating Linear Regression like More Than Just a Black Box</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将线性回归视为不仅仅是一个黑盒</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/treating-linear-regression-more-than-just-a-black-box-7053350f90e9?source=collection_archive---------45-----------------------#2020-06-30">https://towardsdatascience.com/treating-linear-regression-more-than-just-a-black-box-7053350f90e9?source=collection_archive---------45-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f5ff" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">超越线性回归()。拟合(x，y)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8f98fbc1fb653c59e9dba6e0036fd407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EeTpWg1ZCwPZaL9Aqltew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@fossy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Fab Lentz </a>在<a class="ae ky" href="https://unsplash.com/s/photos/arrow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="192b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是最基本的机器学习模型之一，但它的应用无处不在:从预测房价到预测电影收视率。即使当你坐出租车去上班时，你也有一个线性回归问题要处理。</p><p id="d3db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单来说，线性回归是指预测值和特征值之间的线性关系。线性回归的数学方程如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/89ae7dd7e8ae297040c27d71475d457e.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*SkqafAri7hmMnew1dWhIcg.png"/></div></figure><p id="f30b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lw"> Y </em> </strong>为预测值，<strong class="lb iu"> <em class="lw"> θ₀ </em> </strong>为偏差参数，<strong class="lb iu"> <em class="lw"> θ₁ </em> </strong>为参数值，<strong class="lb iu"> <em class="lw"> x </em> </strong>为特征值。</p><p id="469c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了说明这些参数的含义，假设您乘坐出租车去上班。你进入出租车后不久，出租车费就不会从 0 开始了。而是从一些值开始，比如说 5 美元。这 5 美元代表回归方程中的偏差项(<strong class="lb iu"> <em class="lw"> θ₀ </em> </strong>)。预测值<strong class="lb iu"> <em class="lw"> Y </em> </strong>是你应该支付的价格，特征值<strong class="lb iu"> <em class="lw"> x </em> </strong>是行驶距离，<strong class="lb iu"><em class="lw">【θ₁】</em></strong>是随着行驶距离的增加你需要多支付多少的斜率。</p><p id="60b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们在出租车费用和出行距离之间生成一个随机的线性外观数据，以形象化上面的图示。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="2d3c" class="mc md it ly b gy me mf l mg mh">import numpy as np</span><span id="2acb" class="mc md it ly b gy mi mf l mg mh">np.random.seed(1)<br/>x = 2 * np.random.rand(100,1)<br/>y = 4+ 2 * x + np.random.rand(100,1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/07ba2c339a1b9cd24004379f7acb8428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hyPJRK8GHdHbVHZxiXwK7Q.png"/></div></div></figure><p id="0f8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出租车费和旅行距离之间有一个线性趋势。因此，我们可以将出租车费和出行距离作为线性函数进行关联:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/255b0c3906e0231c8abaadc98edea76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*Nu2DOH6D3c8xLrB-kOSTyQ.png"/></div></figure><p id="7556" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在的问题是我们如何获得<strong class="lb iu"><em class="lw">【θ₀】</em></strong><strong class="lb iu"><em class="lw">【θ₁】</em></strong>的最优值，使得我们的线性回归模型表现最佳？为此我们可以使用 Scikit-learn。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="19d8" class="ms md it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">用 Scikit-learn 构建线性回归模型</h1><p id="981b" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">如果您有一个线性回归问题要解决，就像上面的研究案例一样，找到<strong class="lb iu"> <em class="lw"> θ₀ </em> </strong>和<strong class="lb iu"> <em class="lw"> θ₁ </em> </strong>的最优值的最简单方法是通过调用 Scikit-learn 库中的<code class="fe no np nq ly b">LinearRegression</code>函数。这个问题可以用四行代码解决，如下所示:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="23ce" class="mc md it ly b gy me mf l mg mh">from sklearn.linear_model import LinearRegression</span><span id="e4cc" class="mc md it ly b gy mi mf l mg mh">lm = LinearRegression()</span><span id="3b42" class="mc md it ly b gy mi mf l mg mh">#train the model<br/>lm.fit(x,y)</span><span id="2ebe" class="mc md it ly b gy mi mf l mg mh">print(lm.intercept_, lm.coef_)</span><span id="9c38" class="mc md it ly b gy mi mf l mg mh">## Output: [4.49781698] [[1.98664487]]</span></pre><p id="2c59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此我们可以得出出租车费和出行距离之间的关系如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/46b8fef16b2a337c38745c432379dbdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*gyW9F66f93jXhVOJjc1Oww.png"/></div></figure><p id="a0da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个关系，现在你可以估算出你的位置和目的地之间的距离，现在你可以预测你应该付多少钱的出租车费。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="2e64" class="ms md it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">线性回归不仅仅是一个黑箱</h1><p id="e428" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">机器学习库可以让我们的生活更容易解决不同的问题，这令人惊叹。仅用四行代码，您就已经优化了一个线性回归模型，而无需了解任何内幕。</p><p id="70aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这些机器学习库令人惊叹，但它们肯定为你隐藏了一些实现细节，例如，我们将线性回归问题视为黑盒。</p><p id="eee2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">知道引擎盖下是什么以及 Scikit-learn 如何提出解决方案<em class="lw">taxi _ fares = 4.49+1.98 * travel _ distance 岂不是太棒了？</em>他们怎么知道 4.49 是偏差项的最佳值，1.98 是斜率的最佳值？</p><p id="4d33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们知道在引擎盖下发生了什么，我们将对在任何给定问题中使用的适当模型有深刻的理解，并且我们可以更有效地调试错误。另外，对回归问题如何工作有一个很好的理解将是我们理解神经网络算法背后的数学和直觉的一个很好的基础。</p><p id="5a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要找到线性回归问题的最佳模型，通常有两种方法:</p><ul class=""><li id="c7b2" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">使用称为梯度下降的迭代优化方法。</li><li id="04b7" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">使用“封闭形式”的方程称为正规方程。</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="96e5" class="ms md it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">线性回归的梯度下降</h1><p id="dced" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">简而言之，梯度下降算法通过调整模型参数<strong class="lb iu"> <em class="lw"> θ </em> </strong>(偏差项和斜率)来迭代地最小化关于训练数据的误差或成本函数，从而找到线性回归问题的最优解。</p><p id="0410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们形象化地描述一下梯度下降是如何工作的，以使其更加清晰。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/de289b88185b062c6abfbef59b542b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqNhznquBz3Q5woP3pUtxQ.png"/></div></div></figure><p id="8de4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降从随机初始化参数值<strong class="lb iu"> <em class="lw"> θ </em> </strong>的初始值开始。然后，这个<strong class="lb iu"> <em class="lw"> θ </em> </strong>值将在每次迭代中被调整，使得误差或成本函数被最小化。一旦梯度下降算法找到<strong class="lb iu"> <em class="lw"> θ </em> </strong>最小值，代价函数就不会再变小。这意味着算法已经收敛。</p><p id="955e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，并非所有的成本函数都有一个完美的碗形，如上图所示。大多数情况下，成本函数有各种不规则的形状和平台，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/54b36b0ba11632ca1b7eb7def5c7367c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_v3Z6NxsIaYI3rJM8OHKTA.png"/></div></div></figure><p id="5cf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成本函数的形状不规则性使得梯度下降算法很难收敛，因为该算法很有可能会陷入局部最优而无法找到全局最优。</p><p id="05a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，线性回归问题中的成本函数总是凸的，这意味着没有局部最优解，只有一个全局最优解。因此，无论你如何初始化你的初始值，几乎可以保证梯度下降算法将在全局最小解中结束。</p><p id="e396" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但问题是，如果没有局部最优解，为什么“几乎”可以保证梯度下降最终会达到全局最小？</p><p id="2492" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为梯度下降中有一个重要的参数你需要提前定义，就是学习率。</p><p id="9987" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习率决定了算法在每次迭代中的步长。如果学习率太小，很有可能在迭代结束时达不到全局最小值。如果学习率太高，算法将在每次迭代中围绕谷值跳跃，并且算法将发散。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/ac33e3d083416ff5d3ab39dc9040551f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JlQB8Bm4Kta8E50iVJVcwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习率过低(左)和过高(右)</p></figure><p id="9f05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">梯度下降的实现</strong></p><p id="a00f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有关于梯度下降的解释之后，让我们使用研究案例“出租车费 vs 旅行距离”来实现它。</p><p id="2c1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于线性回归问题，我们应该最小化梯度下降算法的代价函数是均方误差(MSE)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8a2669b53901f800e1cbc89e8029b477.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*GF-p3yfZtarWWCsU_xR3xg.png"/></div></figure><p id="cf91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4d1fa7e78701481df25704707342ed9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*T9XUn86Ob6x2VCUtGQZ0jw.png"/></div></figure><p id="3204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的数学符号中，<strong class="lb iu"><em class="lw"/></strong>是代价函数，<strong class="lb iu"> <em class="lw"> m </em> </strong>是观测值或数据的个数，<strong class="lb iu"> <em class="lw"> hθ </em> </strong>是来自线性回归的预测值，<strong class="lb iu"> <em class="lw"> θ₀ </em> </strong>是偏差项，<strong class="lb iu"> <em class="lw"> θ₁ </em> </strong>是斜率，<strong class="lb iu"> <em class="lw"> x </em> </strong>是特征值我们可以用代码实现上述等式:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="7dd5" class="mc md it ly b gy me mf l mg mh">def computeCostFunction(x, y, theta, num_observations):<br/>    <br/>    linear_function= np.matmul(x,theta)<br/>    error = np.subtract(linear_function,y) <br/>    error_squared = error.transpose()**2 <br/>    cost_function = (0.5/num_observations)*(np.sum(error_squared))<br/>    <br/>    return cost_function</span></pre><p id="2374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个重要步骤是计算每个模型参数<strong class="lb iu"><em class="lw"/></strong>的成本函数的梯度。这种梯度背后的直觉是，如果你调整参数<strong class="lb iu"><em class="lw">【θⱼ】</em></strong>的值，成本函数将改变多少。</p><p id="8184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于均方误差，其偏导数有一个封闭形式的解。所以，如果你不是微积分专家，这不是问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a5b022c99fc23cc2d14fe61a2f1c5bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*9jg2te85t2ltH_PlQUDrTw.png"/></div></figure><p id="d4e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你知道了每个参数<strong class="lb iu"> <em class="lw"> θⱼ </em> </strong>的梯度，现在你可以更新每个参数<strong class="lb iu"> <em class="lw"> θⱼ </em> </strong>进行一次迭代。这就是学习率<strong class="lb iu"> α </strong>在梯度下降算法中发挥作用的地方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c60b4ab18bc414c478dd4fa0393f603f.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*Nx6Q5GWb3vUZapWwlfNd-g.png"/></div></figure><p id="52c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以用代码实现上述等式:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="8019" class="mc md it ly b gy me mf l mg mh">intercept_x = np.ones((len(x),1)) #add bias term<br/>x = np.column_stack((intercept_x,x))</span><span id="becd" class="mc md it ly b gy mi mf l mg mh">alpha = 0.05 #learning rate<br/>iterations = 25000<br/>theta = np.random.randn(2,1) #random initialization of parameters<br/>num_observations = 1000</span><span id="afaf" class="mc md it ly b gy mi mf l mg mh">def gradientDescent(x, y, theta, alpha, num_observations, iterations):<br/>    <br/>    cost_history = np.zeros((iterations,1))<br/>    <br/>    for i in range (iterations):<br/>        <br/>        linear_function= np.matmul(x,theta)<br/>        error = np.subtract(linear_function,y)</span><span id="d697" class="mc md it ly b gy mi mf l mg mh">        gradient = np.matmul(x.transpose(),error)<br/>        theta = theta - (alpha * (1/num_observations) * gradient)<br/>        <br/>        cost_history[i] = computeCostFunction(x, y, theta,              num_observations)<br/>        <br/>    return theta, cost_history</span><span id="b417" class="mc md it ly b gy mi mf l mg mh">theta, cost_history = gradientDescent(x, y, theta, alpha, num_observations, iterations)</span><span id="1199" class="mc md it ly b gy mi mf l mg mh">print(theta)</span><span id="3d60" class="mc md it ly b gy mi mf l mg mh">#Output: array([[4.49781697],[1.98664487]])</span></pre><p id="dc19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降的输出看起来与用 Scikit-learn 获得的输出相同。不同的是，你现在知道什么是引擎盖下。</p><p id="51d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一种更直接的方法可以找到线性回归问题的最优解，那就是正规方程。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="2f96" class="ms md it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">正规方程</h1><p id="6f5a" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在线性回归中，正规方程是找到参数最佳值的封闭解。使用这种方法，没有迭代方法，没有偏导数，也没有参数更新。一旦你计算了正规方程，你就完成了，你得到了线性回归模型的最优解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bdb07897f5f44e211b797e00bd39dd1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*csx7OgEBamKib5chdQiDoA.png"/></div></figure><p id="5c21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上式中，<strong class="lb iu"> <em class="lw"> θ </em> </strong>为回归参数的最优值，<strong class="lb iu"> <em class="lw"> x </em> </strong>为特征，<strong class="lb iu"> <em class="lw"> Y </em> </strong>为预测值。我们可以用代码实现上面的等式:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="64ef" class="mc md it ly b gy me mf l mg mh">def normalEquation(x, y):<br/>    <br/>    x_transpose = x.transpose()<br/>    x = np.matmul(x_transpose, x)<br/>    x_inverse = np.linalg.inv(x)<br/>    x_final = np.matmul(x_inverse,x_transpose)<br/>    <br/>    theta = np.matmul(x_final,y)<br/>    <br/>    return theta</span><span id="3e3d" class="mc md it ly b gy mi mf l mg mh">theta = normalEquation(x, y)</span><span id="505b" class="mc md it ly b gy mi mf l mg mh">print(theta)</span><span id="f7a5" class="mc md it ly b gy mi mf l mg mh">#Output: array([[4.49781698],[1.98664487]])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/16f6c8b291d89b9a391720fd7bc16d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lERQ1y781I3Cs5ikgBdf6g.png"/></div></div></figure><p id="9849" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，结果与我们从 Scikit-learn 库和梯度下降优化中得到的结果相匹配。现在你可能会问一个问题:如果法方程非常简单，那么我们为什么不应该用它来代替梯度下降呢？让我们讨论一下。</p><h1 id="f5f5" class="ms md it bd mt mu op mw mx my oq na nb jz or ka nd kc os kd nf kf ot kg nh ni bi translated">正规方程还是梯度下降？</h1><p id="4f96" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在解决线性回归问题时，正规方程和梯度下降都有各自的优点和缺点。</p><p id="4a37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用正规方程，可以直接求解线性回归问题，不需要初始化初始参数值<strong class="lb iu"> <em class="lw"> θⱼ </em> </strong>，也不需要计算偏导数。所有这些都可以通过矩阵矢量化来完成。这使得一切都很快，即使观察或训练数据的数量很大。此外，您不需要对法线方程的每个参数进行归一化来得出最优解。</p><p id="c0b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用正规方程的主要缺点是它的计算复杂性。这是因为我们需要用这种方法求矩阵的逆矩阵。如果你有一个多元线性回归问题，有大量的特征可供选择，法方程的计算成本将非常昂贵。</p><p id="fb9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用梯度下降，如果在多元线性回归问题中有大量的特征选择，计算复杂性就不是问题。</p><p id="4207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果你有大量的观察或训练数据，梯度下降会变得很慢。这是因为在每次迭代中，梯度下降使用所有的训练数据来计算成本函数。</p><p id="4cea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，您始终需要归一化或缩放每个要素，以使梯度下降收敛得更快。下图显示了归一化特征和忘记归一化特征的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/7397cd556fc5bc9b992f3aacfccb648b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nomd5BW2IUA4lsDSmFkgMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归中的归一化要素(左)和非归一化要素(右)</p></figure><p id="3245" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果对要素进行归一化，线性回归中的成本函数会形成一个完美的碗形，从而加快算法的收敛速度。同时，如果您忘记对要素进行归一化或它们不共享相同的比例，则会形成拉长的碗形。该算法最终仍会以全局最小解结束，但需要更长的时间来收敛。</p><p id="1923" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你知道使用法方程和梯度下降算法的优缺点了。作为提示，下表总结了这两种算法的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/e489b49f0cd98c0f9c685d6e82dcc370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU2m9X47uQHGXD2cxm8JvQ.png"/></div></div></figure><h1 id="d36d" class="ms md it bd mt mu op mw mx my oq na nb jz or ka nd kc os kd nf kf ot kg nh ni bi translated"><strong class="ak">如何提高梯度下降的性能？</strong></h1><p id="8085" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">梯度下降的主要缺点是，它使用全部训练数据来计算每次迭代的成本函数。如果我们有大量的观察或训练数据，这将是有问题的。为了解决这个问题，通常采用随机梯度下降(SGD)或小批量梯度下降。</p><p id="ca95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 SGD 算法中，该算法不是使用整个训练样本，而是在每一步的训练样本中随机选取一个实例。然后，将基于这个随机实例计算梯度。这使得计算时间更快，成本更低，因为在每一步中只有一个实例需要在内存中。</p><p id="45c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在小批量梯度下降中，不是使用整个训练示例(如梯度下降)或使用随机实例(如 SGD ),而是基于称为小批量的随机实例的小子集来计算梯度。将整个训练数据分成几个小的小批量使得矩阵运算和计算过程更快。</p><p id="4c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于简单或多元线性回归问题，通常使用梯度下降就足够了。然而，对于更复杂的应用，如神经网络算法，小批量梯度下降将是首选。</p></div></div>    
</body>
</html>