<html>
<head>
<title>Predict League of Legends Matches While Learning PyTorch (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习PyTorch的同时预测英雄联盟比赛(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-league-of-legends-matches-while-learning-pytorch-part-2-38b8e982c7ea?source=collection_archive---------40-----------------------#2020-06-11">https://towardsdatascience.com/predict-league-of-legends-matches-while-learning-pytorch-part-2-38b8e982c7ea?source=collection_archive---------40-----------------------#2020-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b2fc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习在PyTorch中实现一个简单的前馈网络，并使用GPU为一个合适的用例场景进行训练，同时学习一些理论</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/af5e778d8cd1fb342f1526e8ee4759af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDcJvV7ODN2pQV5A-NCO3g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这个小系列的第二部分，手绘版！</p></figure><blockquote class="kv kw kx"><p id="3299" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">读者朋友，你好！如果你还没有读过这个由2部分组成的“系列”的第一部分，我强烈推荐你在阅读之前阅读它。您可以在此处或下方<em class="iq">进行操作👇</em></p></blockquote><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/predict-matches-in-league-of-legends-while-learning-pytorch-basics-3dd43cf8d16f"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ir gy z fp me fr fs mf fu fw ip bi translated">在学习PyTorch基础知识的同时预测英雄联盟中的比赛</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">请跟我来，我将使用PyTorch实现一个逻辑回归模型来预测英雄联盟中的比赛</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn kp lz"/></div></div></a></div><p id="94c0" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">上一次，我们停止了用PyTorch做一个逻辑回归器来达到同样的目的。这一次，我们将事情推进了一步:创建一个前馈神经网络(只有完全连接的层)。如果你想知道更多关于这样做的意图，这个迷你项目将使用的数据集，和/或数据集的数据准备过程，那么你应该看看我的第一篇文章<a class="ae lv" rel="noopener" target="_blank" href="/predict-matches-in-league-of-legends-while-learning-pytorch-basics-3dd43cf8d16f">这里</a>或以上。</p><p id="8de4" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">《英雄联盟》是我一直以来最喜欢的游戏之一，尽管我真的很不擅长。LOL是一个极具竞争力的MOBA，两个由5人组成的队伍<em class="la">(蓝队和红队)</em>相互对抗，以摧毁对方的基地(nexus)。获胜通常需要大量的团队合作、协调，或者对于一个倾斜的玩家来说，“运气”。不管怎样，对于一个联盟玩家(即使他们是相当新的)来说，根据游戏记录的死亡人数和许多其他数据来判断哪个队可能会赢并不太难。神经网络可以预测的东西……</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="83dc" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">等等，什么是神经网络？</h1><blockquote class="kv kw kx"><p id="3969" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">嘶！如果你不想学习一些理论，可以跳过这一部分。你会错过一些我自己的画:(</p></blockquote><p id="c643" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">上次我们已经看到了逻辑回归模型如何在预测方面做得相当好(它在测试数据集上实现了高达74%的准确性)。事实上，逻辑回归变量几乎完全是一个线性回归变量，它本身就是一个T2，一个一批输入之间的矩阵点积，一个权重矩阵，外加一个偏差向量。 <em class="la">可变的权重和偏差使模型能够训练并更好地做它正在做的事情。</em>线性回归和逻辑回归之间的唯一区别是，对于涉及简单的是或否问题(就像比赛中的一支球队赢得了比赛)或分类问题的预测，存在一个将输出“挤压”成一系列值(通常从0到1)的函数。逻辑回归变量就是使用这种“挤压”函数的变量，它通常以<em class="la"> sigmoid </em>或<em class="la"> softmax </em>函数的形式出现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/779cc38d5f73d71002906c804c775058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qw2cSUHNhhDA0ZFYCOe4Fg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归的基本数学，其中线性回归的矩阵运算用sigmoid函数包装。我画的😬 🔥</p></figure><p id="2fda" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">那么，神经网络如何设置自己以获得进一步的成功呢？<strong class="lb ir">简单地说，一个普通的神经网络是多个线性回归器堆叠在一起。</strong> <em class="la">理论上，这应该允许神经网络拾取数据之间的更多关系/趋势，以帮助预测。</em>但不可能这么简单！不做任何额外的事情，链接矩阵乘法和加法只会让我们一无所获。看一看:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/18c1b57838b925d5675bbff75a3e1aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfsG5bHy7j2VPHVcc-Z5dg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这就是当你试图直接链接线性回归操作时会发生的情况。</p></figure><p id="39b5" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">您可以看到，将两个线性回归链接起来与仅一个线性回归同义，只是权重和偏差不同。那么，我们如何解决这个问题呢？<strong class="lb ir">我们引入一个非线性激活函数</strong>，它将环绕线性回归操作的每个实例。它不仅解决了上面普遍存在的问题，而且还模仿了(在某种意义上)生物神经元的工作方式。例如，神经元确定信号是否超过设定的<em class="la">阈值</em>，以将信号向前传递到下一个神经元。类似地，激活函数将决定并调整神经网络层的最终输出。我可以详细说明激活功能，但我们会潜水太深！</p><p id="4a93" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated"><em class="la">顺便说一下，为了使我们对词汇的用法更趋向于约定俗成，从现在开始让我们把输入和输出之间的线性回归的每一个实例都称为一个</em> <strong class="lb ir"> <em class="la">神经网络的隐藏层</em> </strong> <em class="la">，而每一个个体的权重和偏差都称为一个</em> <strong class="lb ir"> <em class="la">节点</em> </strong> <em class="la">。</em>考虑到这一点，这就是神经网络的“样子”:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/66a316a1c6e7c9c009df6af2bee073de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SiDWBcOsBLqCpO9SueOXwA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">或者，你可以谷歌搜索“神经网络”，你会看到更好的图像！</p></figure><p id="12e3" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">好了，现在让我们回到编码上来！如果你想要一个更直观的方法来研究神经网络&amp;更多，看看<a class="ae lv" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2&amp;t=0s" rel="noopener ugc nofollow" target="_blank"> 3blue1brown关于深度神经网络的视频系列</a>！</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="d29e" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">制作前馈神经网络</h1><p id="0934" class="pw-post-body-paragraph ky kz iq lb b lc nt jr le lf nu ju lh mo nv lk ll mp nw lo lp mq nx ls lt lu ij bi translated">TL；对于我们刚才所说的博士:<em class="la">一个神经网络基本上是多个线性回归操作(隐藏层)链接在一起，在每一层之后有一个</em> <strong class="lb ir"> <em class="la">激活函数</em> </strong> <em class="la">。</em>下面是定义模型时的样子:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个特性的输入大小将是29(参见第一篇文章)，输出大小将是2，每一个都是对团队输赢的预测。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">当我们初始化模型时，我们现在有多个“nn.Linear”实例，我们将通过每一层和“F.relu()”传递输入(稍后将详细介绍)。</p></figure><p id="68aa" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">酷，<em class="la">但是什么是</em> <code class="fe oa ob oc od b"><em class="la">F.relu()</em></code>？整流线性单元(ReLU)是深度学习中使用的许多激活函数之一，与其他替代方法(例如Sigmoid)相比，它的性能非常好。如果你想知道更多关于ReLU和其他激活功能的信息，去看看这篇文章。PyTorch在<code class="fe oa ob oc od b">torch.nn.functional</code>(通常作为<code class="fe oa ob oc od b">F</code>导入)中提供了过多的激活函数，所以一定要检查他们的<a class="ae lv" href="https://pytorch.org/docs/stable/nn.functional.html" rel="noopener ugc nofollow" target="_blank">文档</a>，看看你有哪些选项可以自己使用。</p><p id="047a" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">我们将使用SGD优化器和交叉熵损失函数来训练模型。我们将训练循环定义如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们在这里定义了很多函数来形成训练循环。这里提供了注释，向您展示大多数代码行的用途。</p></figure></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="bbfb" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">在GPU上训练</h1><p id="2fae" class="pw-post-body-paragraph ky kz iq lb b lc nt jr le lf nu ju lh mo nv lk ll mp nw lo lp mq nx ls lt lu ij bi translated">随着神经网络模型变得越来越复杂，训练这些模型的计算需求也急剧增加。图形处理单元，被称为GPU或显卡，是专门设计来进行大规模矩阵运算的。如果你还不知道，除非启用，PyTorch总是使用你的CPU进行计算，这肯定不如GPU有效。这一次，我们将发现如何利用GPU来为我们的神经网络处理数据。</p><blockquote class="kv kw kx"><p id="cd64" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在我们开始之前，只支持NVIDIA GPUs，对不起AMD粉丝<em class="iq">😢。</em></p></blockquote><p id="591d" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">PyTorch提供了一个函数<code class="fe oa ob oc od b">torch.cuda.is_available()</code>，它输出一个布尔值，表明安装了CUDA的兼容(NVIDIA) GPU的存在。如果你有一个受支持的GPU，你可以完成设置过程，或者你可以创建一个<a class="ae lv" href="http://kaggle.com" rel="noopener ugc nofollow" target="_blank"> kaggle </a>或<a class="ae lv" href="http://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> google colab </a>帐户，并访问免费的GPU以进行深度学习(当然有一些限制)。让我们使用<code class="fe oa ob oc od b">is_available()</code>函数来设置GPU的使用，但是如果没有GPU，就退回到CPU:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Torch.device(…)是指PyTorch中可用的硬件。</p></figure><p id="7481" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">使用PyTorch，您可以通过使用任何张量或模型的<code class="fe oa ob oc od b">.to()</code>方法将数据移入和移出GPU设备。因此，要开始使用GPU，<strong class="lb ir">您首先必须将您的模型移动到GPU上</strong>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们初始化模型“LOLModelmk2()”,并通过使用方法“to(device)”将其移动到GPU，其中device =“torch . device(“cuda”)”</p></figure><p id="7a18" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">现在，我们开始训练:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在训练之前用测试数据测试模型。损耗徘徊在16%左右，准确率50%。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">您可以看到验证损失急剧下降，准确性出现峰值。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这种趋势在很小的范围内继续</p></figure><p id="6506" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">下面是一些漂亮的图表😁：</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="5b49" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">这里是我们从测试数据集得到的结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">嗯嗯…</p></figure><p id="15d1" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">嗯… 与线性回归模型(74%)相比，我们的模型看起来表现完全相同。现在，我们对这个结果有一些可能性:</p><ol class=""><li id="e9f8" class="oe of iq lb b lc ld lf lg mo og mp oh mq oi lu oj ok ol om bi translated">某段代码不正确</li><li id="8d78" class="oe of iq lb b lc on lf oo mo op mp oq mq or lu oj ok ol om bi translated">神经网络通常比逻辑回归模型差</li><li id="0554" class="oe of iq lb b lc on lf oo mo op mp oq mq or lu oj ok ol om bi translated">神经网络<em class="la">过拟合</em></li><li id="b688" class="oe of iq lb b lc on lf oo mo op mp oq mq or lu oj ok ol om bi translated">逻辑回归模型在其训练中是幸运的(这是可能的，因为数据集被随机分为回归器和神经网络的训练集、验证集和测试集)</li><li id="2b85" class="oe of iq lb b lc on lf oo mo op mp oq mq or lu oj ok ol om bi translated">在这种情况下使用神经网络可能没有优势，我们正在经历收益递减。</li></ol><h2 id="21d6" class="os mz iq bd na ot ou dn ne ov ow dp ni mo ox oy nk mp oz pa nm mq pb pc no pd bi translated">好吧，我们用排除法，好吗？</h2><p id="60a1" class="pw-post-body-paragraph ky kz iq lb b lc nt jr le lf nu ju lh mo nv lk ll mp nw lo lp mq nx ls lt lu ij bi translated">经过长时间的调试，我在代码<em class="la">中没有发现任何错误(如果你发现了什么，请告诉我！！！)</em>，所以#1出局了。#2可能不是这种情况:我们之前建立了神经网络如何基于线性回归模型，线性回归模型基本上是没有<em class="la"> sigmoid/softmax </em>函数的逻辑回归。他们应该能够从数据中得出更多的关系，这需要更好的准确性，而不是相反。</p><p id="e788" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">#3比其他两个更有可能，因为神经网络比逻辑回归更复杂，因此更容易接受这类问题。通常，过度拟合可以通过使用<strong class="lb ir">丢弃</strong>来解决，这仅仅意味着在训练时禁用随机选取的模型节点的一部分。对于PyTorch来说，这意味着在<code class="fe oa ob oc od b">__init__()</code>中初始化一个<code class="fe oa ob oc od b">nn.Dropout()</code>层，并用ReLU把它放在层之间。下面是实现过程:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们只需初始化“nn.Dropout”的一个实例，因为它可以在模型类的forward函数中多次使用。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">尽管如此，该模型在测试数据集上的准确率仍保持在70%左右。</p></figure><p id="0257" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">令人惊讶的是，即使这样也不起作用，这意味着模型没有过度拟合训练数据。最后，为了测试我们的假设#4，我在逻辑回归器上重新查看了我的旧笔记本，并用这个模型进行了几次试验。<strong class="lb ir">事实证明，逻辑回归器上次74%的准确率是相当幸运的。</strong>事实上，让我们再来看看#个时期的精度图:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="dfc2" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh mo lj lk ll mp ln lo lp mq lr ls lt lu ij bi translated">准确性在很大程度上是相当不稳定的，但总体来说，它徘徊在70%左右，这更类似于我后来在逻辑回归和本文中的神经网络上运行的试验。</p><h1 id="de17" class="my mz iq bd na nb pe nd ne nf pf nh ni jw pg jx nk jz ph ka nm kc pi kd no np bi translated">结论</h1><p id="6e00" class="pw-post-body-paragraph ky kz iq lb b lc nt jr le lf nu ju lh mo nv lk ll mp nw lo lp mq nx ls lt lu ij bi translated">通过这个例子，深度学习学科可以学到很多东西。主要是，深度学习不是巫毒魔法；它不能神奇地解决你给它的每一个分类问题。它不能预测每一场英雄联盟的比赛；在许多情况下，比赛的前10分钟不足以决定哪支球队会赢(我可以通过我的经验证明)。尽管如此，从这种经历中还是有很多收获的，比如学习神经网络的概念并在PyTorch中实现它，利用GPU，以及在模型过拟合的情况下退出。在这一点上，我希望你喜欢和我一起为这个英雄联盟数据集构建PyTorch模型的旅程。编码快乐(还有继续打联赛)！</p><blockquote class="kv kw kx"><p id="3f34" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">如果你想知道这个迷你项目使用的jupyter笔记本的来源，请看这里:<a class="ae lv" href="https://jovian.ml/richardso21/lol-nn" rel="noopener ugc nofollow" target="_blank">https://jovian.ml/richardso21/lol-nn</a>。</p></blockquote></div></div>    
</body>
</html>