<html>
<head>
<title>Gesture Recognition for Beginners with CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN新手手势识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/artificial-neural-networks-for-gesture-recognition-for-beginners-7066b7d771b5?source=collection_archive---------16-----------------------#2020-04-30">https://towardsdatascience.com/artificial-neural-networks-for-gesture-recognition-for-beginners-7066b7d771b5?source=collection_archive---------16-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f3f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python和SqueezeNet的有趣实验</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/f70975f29566a49c86edc458fb27b567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jqJJmsHP3WW-EMi3"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">奥斯曼·拉纳在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c2a8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">CNN或卷积神经网络是图像分类问题中最常用的算法。图像分类器将照片或视频作为输入，并将其分类到它被训练识别的可能类别之一。它们在各个领域都有应用，如无人驾驶汽车、国防、医疗保健等。图像分类有许多算法，在这个实验中，我们将研究这种类型中最受欢迎的算法之一，DeepScale称之为<strong class="li iu"> <em class="mc"> SqueezeNet </em> </strong>。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="ddd5" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">目标是:</h1><p id="f599" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们的目标是设计一个应用程序，它将使用网络摄像头(或外部摄像头)作为输入设备，然后它将识别手势并将其分类到我们将要定义的类别中。在本文中，我们将看到如何使用SqueezeNet算法来设计一个应用程序，它采用不同的手势，然后触发某些动作。简而言之，我们可以向我们的计算机发送一些命令(无需触摸键盘或鼠标)。让我们戴上魔术师的帽子吧！🧙‍♂️</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi na"><img src="../Images/bcbe2b2c7c1d67731a8134c7a21fc002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2AXgSDYD5RAPci1f"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">照片由<a class="ae lf" href="https://unsplash.com/@alekssei199?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿列克谢·耶什金</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4385" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">实验包括下列四个阶段:</p><h1 id="458a" class="md me it bd mf mg nb mi mj mk nc mm mn jz nd ka mp kc ne kd mr kf nf kg mt mu bi translated">概述:</h1><ol class=""><li id="509d" class="ng nh it li b lj mv lm mw lp ni lt nj lx nk mb nl nm nn no bi translated">定义我们的分类类别</li><li id="3b34" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">收集训练图像</li><li id="1923" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">训练模型</li><li id="486e" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">测试我们的模型</li></ol></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="389c" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">1.定义我们的分类类别(手势)</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nu"><img src="../Images/050ebfd3391274f4bff1ee6bf14193d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V5Wsq7AZjjl-efT1"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@bady?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">巴迪qb </a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="ca56" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我们最后的节目中，我们将表演一些动作。这是我决定要做的一些事情的清单。根据您的具体要求，此列表中的项目数量会有所不同。接下来，为了方便起见，我们将使用类别名称来映射这些操作，以便稍后在我们的程序中使用。</p><ol class=""><li id="0f95" class="ng nh it li b lj lk lm ln lp nv lt nw lx nx mb nl nm nn no bi translated">增加扬声器音量(<em class="mc">类别= </em> <strong class="li iu"> <em class="mc">向上</em> </strong>)</li><li id="21a5" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">降低扬声器音量(<em class="mc">类别= </em> <strong class="li iu"> <em class="mc">调低</em> </strong>)</li><li id="be2f" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">静音/取消静音(<em class="mc">类别= </em> <strong class="li iu"> <em class="mc">静音</em> </strong>)</li><li id="b055" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">播放/暂停(<em class="mc">类别= </em> <strong class="li iu"> <em class="mc">播放</em> </strong>)</li><li id="5204" class="ng nh it li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">打开谷歌浏览器(<em class="mc">category =</em><strong class="li iu"><em class="mc">Chrome</em></strong>)</li></ol><p id="6d4f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">除了这五个类别，我们还应该有另一个类别(<em class="mc">category =</em><strong class="li iu"><em class="mc">nothing</em></strong><em class="mc">)</em>，当用户做出我们的模型无法识别的手势时，或者当用户没有输入手势时，我们将在最终的程序中使用这个类别。</p><p id="ae31" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在我们已经决定了我们的六个类别，让我们看看我的手势看起来怎么样。</p><p id="9cf0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我在下面创建了一个所有手势和它们各自类别名称(红色)的拼贴画。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ny"><img src="../Images/c646dbbe4eb0bb98b0082a1bf90f1963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wc7C-EO1onWr6DsNrEJFlw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">所有手势及其类别名称的拼贴画(使用网络摄像头拍摄)</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="0c34" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">2.收集用于训练的图像</h1><p id="2ecb" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">下一组是为每个类别准备我们的训练图像。为了收集我们的训练数据集(图像)，我们将使用网络摄像头。为了让事情变得简单，我确保使用背景简单整洁的图片。下面的python程序将使用OpenCV库来执行这个动作，并将这些图像存储到一个名为“<strong class="li iu"> <em class="mc"> training_images”的文件夹中。</em> </strong>程序需要两个输入参数:</p><pre class="kq kr ks kt gt nz oa ob oc aw od bi"><span id="e23f" class="oe me it oa b gy of og l oh oi">python get_training_images.py 50 up</span></pre><blockquote class="oj ok ol"><p id="2005" class="lg lh mc li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated"><strong class="li iu"> <em class="it"> a) </em> </strong>要拍摄的图像数量(例如:- 50)</p><p id="cf24" class="lg lh mc li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated"><strong class="li iu"> <em class="it"> b) </em> </strong>标签(或类别)名称(例如:-向上、向下、播放、铬等)</p></blockquote><p id="29c8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae lf" href="https://github.com/arindomjit/Gesture_Detection_CNN/blob/master/get_training_images.py" rel="noopener ugc nofollow" target="_blank"> <em class="mc">程序</em> </a>在捕捉到那么多图像后停止。第二个参数指示这些图像所属类别的名称。该程序在我们的'<strong class="li iu"> training_images </strong>'文件夹中根据类别名称创建一个子文件夹，并将所有图像存储在那里。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi op"><img src="../Images/60ebce39b5eacacb4924e4166a050bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*l9qqf4PYvvPW_v_FjelFPA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">“training_images”文件夹中所有子文件夹的屏幕截图</p></figure><p id="c477" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一旦你运行该程序，网络摄像头的饲料将打开。将手放在白色框的边界内，按下“<strong class="li iu"> <em class="mc"> s </em> </strong>”键到<strong class="li iu"> <em class="mc">开始</em> </strong>给手拍照。在此过程中，尝试移动您的手，在训练数据集中添加一些变化。我从这里开始，对每个类别运行程序两次，右手50张图片，左手50张图片。一旦程序结束，你应该能够看到你的手势名称的子文件夹中的图像。一旦这个过程完成，你将在每个文件夹中得到100张图片。你可能想选择一个适合你的号码，越多越好。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="533e" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak"> 3。训练模型</strong></h1><p id="e2c9" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在这个演示中，我们将使用SqueezeNet。<a class="ae lf" href="https://arxiv.org/abs/1602.07360" rel="noopener ugc nofollow" target="_blank"><strong class="li iu"><em class="mc">SqueezeNet</em></strong></a>是一个流行的用于图像分类问题的预训练模型，它非常轻量级，具有令人印象深刻的精确度。我们将在训练过程中使用<em class="mc"> Keras </em>库。<em class="mc"> Keras </em>是一个简单而强大的python库，广泛应用于深度学习。这使得训练神经网络模型变得非常容易。</p><p id="20da" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们看一下用于我们培训的一些参数。</p><p id="24f7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">顺序:</strong>我们将使用顺序模型，这意味着各层以线性堆栈(顺序)排列。模型中的层作为参数添加到此构造函数中。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oq"><img src="../Images/064656d6a736e7ad9f1a07e09fd4c923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VNyurY_vis0-jEwo"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@photoholgic?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">霍尔格连杆</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="6796" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">退出率:</strong>在较小数据集上训练的神经网络往往会过度拟合，因此在新数据上不太可能准确。从理论上讲，训练模型的最佳方法可能是尝试不同参数值的不同组合，然后取这些单独结果的平均值，以得出一个概括的结果。但是这将需要大量的时间和计算资源来用这些参数的多种组合多次训练模型。为了解决这个问题，引入了辍学率。这里，层中的一些单元/节点(来自输入层或隐藏层，但不来自输出层)被'<em class="mc">丢弃'</em>，这使得该节点的输入和输出连接消失。简而言之，当在训练期间多次这样做时，不同数量的节点从层中被丢弃，使得该层在节点数量及其与前一层的连接方面看起来不同(粗略地模拟具有不同层配置的多个模型)。一个图层的辍学率的较好值在0.5–0.8之间。就我而言，在尝试了几个不同的值之后，我发现0.5给了我最好的结果。请注意，脱扣是用来避免过度拟合。如果模型的精确度低，则可以避免该参数。</p><p id="e916" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">节点:</strong>我们输出层中神经元/节点的数量等于我们试图预测的类的数量。</p><p id="488d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">输入形状:</strong>SqueezeNet要求的输入形状至少是224 X 224(RGB有3个通道)。在这个程序中，我们使用了225 X 225。</p><p id="f6a0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">选择优化器&amp;损失:</strong>这里我选择了<strong class="li iu"> <em class="mc">亚当</em> </strong>优化器。并且基于问题的类型选择损失函数。例如:-对于二元分类问题，<em class="mc">[loss = ' binary _ cross entropy ']</em>更适合，对于多类分类问题，选择<em class="mc">[loss = ' categorial _ cross entropy ']</em>。而对于回归问题，可以选择<em class="mc"> [loss='mse'] </em>。由于我们的问题是一个多类分类问题，我们将使用<strong class="li iu"> <em class="mc">分类_交叉熵</em>。</strong></p><p id="0a65" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">历元数:</strong>历元数是整个数据集在训练过程中通过神经网络的次数。对此没有一个理想的数字，它取决于数据。在这种情况下，我从10开始，我用了15。数字越大表示训练时间越长。</p><p id="38d3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">激活函数:</strong>我们使用的是ReLU激活函数，这是神经网络中最常用的激活函数，因为它计算简单(还有其他优点)。该函数为负输入返回零，为正输入返回值本身。对于大多数现代神经网络，ReLU是默认的激活函数。还使用的其他激活函数是Sigmoid、Tanh等。</p><p id="503d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">池化:</strong>在CNN中，通常的做法是在卷积&amp;激活层之后添加一个池化层。输入图像被下采样或转换成低分辨率版本，以便仅保留重要的细节并去除较精细的不太重要的细节。</p><p id="3f5f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">soft max:</strong>soft max层用在输出层之前的多类分类中。它给出了输入图像属于特定类别的可能性。</p><pre class="kq kr ks kt gt nz oa ob oc aw od bi"><span id="e63b" class="oe me it oa b gy of og l oh oi">import numpy as np<br/>from keras_squeezenet import SqueezeNet<br/>from keras.optimizers import Adam<br/>from keras.utils import np_utils<br/>from keras.layers import Activation, Dropout, Convolution2D, GlobalAveragePooling2D<br/>from keras.models import Sequential<br/>import tensorflow as tf</span><span id="aa54" class="oe me it oa b gy or og l oh oi">GESTURE_CATEGORIES=6<br/>base_model = Sequential()<br/>base_model.add(SqueezeNet(input_shape=(225, 225, 3), include_top=False))<br/>base_model.add(Dropout(0.5))<br/>base_model.add(Convolution2D(GESTURE_CATEGORIES, (1, 1), padding='valid'))<br/>base_model.add(Activation('relu'))<br/>base_model.add(GlobalAveragePooling2D())<br/>base_model.add(Activation('softmax'))</span><span id="6ea0" class="oe me it oa b gy or og l oh oi">base_model.compile(<br/>    optimizer=Adam(lr=0.0001),<br/>    loss='categorical_crossentropy',<br/>    metrics=['accuracy']<br/>)</span></pre><p id="79c8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">训练后，我们将把训练好的模型参数存储到一个文件(手势-模型05_20.h5)中，我们将在稍后的模型测试中使用该文件。全部代码都在github <a class="ae lf" href="https://github.com/arindomjit/Gesture_Detection_CNN" rel="noopener ugc nofollow" target="_blank">库</a>中。</p><pre class="kq kr ks kt gt nz oa ob oc aw od bi"><span id="eff0" class="oe me it oa b gy of og l oh oi">model.save("gesture-model05_20.h5")</span></pre></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="f2ec" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak"> 4。测试型号</strong></h1><p id="659a" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">为了测试这个模型，我使用了我的网络摄像头再次捕捉到的一些手势图像。该程序加载模型文件“手势-模型05_20.h5 ”,并将输入图像作为参数，并预测其所属的类别。在传递图像之前，我们需要确保我们使用的尺寸与我们在训练阶段使用的尺寸相同。</p><pre class="kq kr ks kt gt nz oa ob oc aw od bi"><span id="1147" class="oe me it oa b gy of og l oh oi">from keras.models import load_model<br/>import cv2<br/>import numpy as np</span><span id="6d77" class="oe me it oa b gy or og l oh oi">img = cv2.imread(&lt;input image file path&gt;)<br/>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>img = cv2.resize(img, (225, 225))</span><span id="be6b" class="oe me it oa b gy or og l oh oi">model = load_model("gesture-base-rmsprop-model05_20.h5")<br/>prediction = model.predict(np.array([img]))</span></pre><p id="0245" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于以下测试图像，模型预测正确。然而，如果图像在背景中包含太多其他项目，它并不总是准确的。</p><div class="kq kr ks kt gt ab cb"><figure class="os ku ot ou ov ow ox paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><img src="../Images/f67ae4bda564592eae734d019a816905.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*JOVL0zHPiNs2zj4FXjiY5Q.jpeg"/></div></figure><figure class="os ku ot ou ov ow ox paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><img src="../Images/c0a80028a07c9072376dd8f16ffe529d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*B9S-AxXUqsHzcfAfxWwE9A.jpeg"/></div></figure><figure class="os ku ot ou ov ow ox paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><img src="../Images/4a7f0d71fd1ede58258c0846a4574fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EP9_dGUVGmxtehcO3-CywA.jpeg"/></div><p class="lb lc gj gh gi ld le bd b be z dk oy di oz pa translated">测试图像(使用网络摄像头拍摄)</p></figure></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="aaea" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，尽管这种模式可能无法每次都完美运行，并且我们可以做很多事情来改善其性能，但它为我们提供了一个起点，并从这里开始建立这些基础。感谢您抽出时间阅读这篇文章，我希望它能帮助您创建您的第一个CNN项目。我期待着看到这个项目你的版本。请与我分享您的想法:)</p><p id="af2c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我的下一篇<a class="ae lf" href="https://medium.com/@b.arindom/build-a-voice-controlled-mouse-keyboard-in-5-minutes-952bc8f101fc" rel="noopener"> <strong class="li iu"> <em class="mc">文章</em> </strong> </a>中，我们将看看如何使用Python基于已识别的类采取行动。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="ae9c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文使用Python OpenCV。如果您不熟悉Python中的OpenCV库，这里有一个链接，链接到我关于这个主题的介绍性文章。</p><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/face-detection-in-10-lines-for-beginners-1787aa1d9127"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">面向初学者的10行人脸检测</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">使用Python OpenCV在图像和视频中检测人脸的介绍。</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps kz pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a href="https://medium.com/@b.arindom/build-a-voice-controlled-mouse-keyboard-in-5-minutes-952bc8f101fc" rel="noopener follow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">在5分钟内制作一个声控鼠标</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">Python中语音识别和GUI自动化的初学者指南</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">medium.com</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps kz pe"/></div></div></a></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="d389" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">参考文献&amp;延伸阅读:</strong></p><p id="b115" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[1] SqueezeNet，<a class="ae lf" href="https://codelabs.developers.google.com/codelabs/keras-flowers-squeezenet/#4" rel="noopener ugc nofollow" target="_blank">https://codelabs . developers . Google . com/codelabs/keras-flowers-SqueezeNet/</a></p><p id="e6b2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[2] Pooling，2019，<a class="ae lf" href="https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/Pooling-layers-for-convolutionary-neural-networks/</a></p><p id="550d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[3] SqueezeNet Paper，2016，<a class="ae lf" href="https://arxiv.org/abs/1602.07360" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1602.07360</a></p><p id="8178" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[4]备选OpenCV方法，<a class="ae lf" href="https://www.youtube.com/watch?v=-_9WFzgI7ak" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=-_9WFzgI7ak</a></p><p id="2f62" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[5]辍学:防止神经网络过度拟合的简单方法，2014，<a class="ae lf" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">http://jmlr.org/papers/v15/srivastava14a.html</a></p></div></div>    
</body>
</html>