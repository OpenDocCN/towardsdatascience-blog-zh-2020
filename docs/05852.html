<html>
<head>
<title>Understanding the Building Blocks of Graph Neural Networks (Intro)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解图形神经网络的构建模块(简介)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-building-blocks-of-graph-neural-networks-intro-56627f0719d5?source=collection_archive---------14-----------------------#2020-05-14">https://towardsdatascience.com/understanding-the-building-blocks-of-graph-neural-networks-intro-56627f0719d5?source=collection_archive---------14-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="830a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/grl-series" rel="noopener" target="_blank"> GRL系列</a></h2><div class=""/><div class=""><h2 id="51ce" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">对用于分析和学习图形数据的神经框架的直觉(带有运行代码)</h2></div><p id="e45e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di"> T </span>他的帖子是关于<em class="lw">图形神经网络</em> (GNNs)的一系列文章的介绍。本系列的目标是通过直觉和例子提供GNNs构建模块的详细描述。</p><p id="6173" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个系列中，我还会分享运行代码，使用Numpy，Pytorch，以及这个领域采用的最突出的库，比如<a class="ae lx" href="https://www.dgl.ai/" rel="noopener ugc nofollow" target="_blank">深度图形库(DGL) </a>和<a class="ae lx" href="https://pytorch-geometric.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Pytorch几何</a>。在本系列的最后，您将能够组合这些构建块并创建一个神经架构来对图形数据执行分析和学习任务。</p><p id="09e9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本系列将分析建立GNN的主要组件，包括(I)输入层，(ii)GNN层，和(iii)多层感知器(MLP)预测层。</p><p id="c349" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">分析和分解标准GNN体系结构的框架基于最近发表的题为“基准图神经网络”的论文，其元数据如下:</p><blockquote class="ly lz ma"><p id="ecb1" class="kr ks lw kt b ku kv kd kw kx ky kg kz mb lb lc ld mc lf lg lh md lj lk ll lm im bi translated"><em class="it"> Dwivedi，V. P .，Joshi，C. K .，Laurent，t .，Bengio，y .，&amp; Bresson，X. (2020)。基准图神经网络。arXiv预印本arXiv:2003.00982 <em class="it">。<br/>来源:</em><a class="ae lx" href="https://arxiv.org/abs/2003.00982" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2003.00982</a></em></p></blockquote><p id="acbc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇文章不包括图论和神经网络的基础。对于这个主题的介绍，我建议阅读下面的文章:</p><div class="me mf gp gr mg mh"><a rel="noopener follow" target="_blank" href="/graph-theory-and-deep-learning-know-hows-6556b0e9891b"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">图论和深度学习知识</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">图形学习和几何深度学习—第0部分</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">towardsdatascience.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv mw mh"/></div></div></a></div><h2 id="312c" class="mx my it bd mz na nb dn nc nd ne dp nf la ng nh ni le nj nk nl li nm nn no iz bi translated">GNN建筑:主要组成部分概述</h2><p id="21bb" class="pw-post-body-paragraph kr ks it kt b ku np kd kw kx nq kg kz la nr lc ld le ns lg lh li nt lk ll lm im bi translated"><em class="lw">输入层</em>定义图形数据的初始表示，它成为GNN层的输入。基本上，其思想是为图的节点和边分配一个要素表示。</p><p id="f578" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lw"> GNN层</em>对图形结构的信息进行编码。然后，它利用这些信息来更新节点和边的初始表示。</p><p id="b9c3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lw"> MLP预测层</em>执行特定的学习任务，包括<em class="lw">节点分类</em>或<em class="lw">链路预测</em>，采用从GNN层输出获得的编码图表示。</p><p id="3599" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇文章介绍了输入层和GNN层背后的主要原理。接下来的文章将描述不同类型的GNN层，解释相关的功能，并显示它们之间的主要差异。同时，我将概述传统的MLP预测图层，以对图表数据执行特定的任务。</p><figure class="nv nw nx ny gt nz gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1c5abbcaf394225da2ba0d0212bc8948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*wt31DAeKTVeDWmPqKprycw.gif"/></div><p class="ob oc gj gh gi od oe bd b be z dk translated">这个动画gif描述了通过GNN网络传播更新的节点特性——原始图片来自GraphSAGE网站的<a class="ae lx" href="http://snap.stanford.edu/graphsage/" rel="noopener ugc nofollow" target="_blank">主页</a></p></figure><h2 id="b6e7" class="mx my it bd mz na nb dn nc nd ne dp nf la ng nh ni le nj nk nl li nm nn no iz bi translated">输入层</h2><p id="e3ac" class="pw-post-body-paragraph kr ks it kt b ku np kd kw kx nq kg kz la nr lc ld le ns lg lh li nt lk ll lm im bi translated">如前所述，输入层的目标是定义图形数据的初始表示，将要素分配给节点和边。为了简单起见，我目前只考虑节点特性。</p><p id="3ac8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在图中表示节点的最简单的方法是使用<em class="lw">单键</em>向量。这种表示通常被用来区分NLP任务词汇表中的不同单词。在我们的例子中，它被用来表示图的不同节点。表示每个节点的向量的长度等于节点的数量，并且对于每个向量，不同位置的元素被设置为1，而其他元素被设置为0。</p><p id="d13e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了阐明这种表示，下面的脚本创建了一个包含5个节点的图，用一个热点向量表示。</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="8fad" class="mx my it og b gy ok ol l om on">import numpy as np</span><span id="73ed" class="mx my it og b gy oo ol l om on">X = np.eye(5, 5)<br/>n = X.shape[0]<br/>np.random.shuffle(X)</span><span id="2e7c" class="mx my it og b gy oo ol l om on">print(X)</span><span id="2dd3" class="mx my it og b gy oo ol l om on">-- output:</span><span id="f689" class="mx my it og b gy oo ol l om on">[[0. 0. 1. 0. 0.]  # Node 1<br/> [0. 0. 0. 1. 0.]  # Node 2<br/> [1. 0. 0. 0. 0.]  # ..<br/> [0. 0. 0. 0. 1.]  # ..<br/> [0. 1. 0. 0. 0.]] # Node 5</span></pre><p id="df5c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这个矩阵的每一行代表图的一个节点。为了给每个节点分配初始特征，输入层将线性变换(也称为<em class="lw">投影</em>)应用于编码节点表示的一键矢量。简单回顾一下线性变换，定义如下:</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="4a50" class="mx my it og b gy ok ol l om on">Y = WX + b</span></pre><p id="8a95" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如Dwivedi等人所报道的，在独热向量的情况下，偏置值<em class="lw"> b </em>不用于线性变换。因此，以下脚本执行线性变换:</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="aa08" class="mx my it og b gy ok ol l om on"># Dimension of the node features (embedding)<br/>emb = 3</span><span id="700c" class="mx my it og b gy oo ol l om on"># Weight matrix (initialized according to Glorot &amp; Bengio (2010))</span><span id="0722" class="mx my it og b gy oo ol l om on">W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (n, emb))</span><span id="c629" class="mx my it og b gy oo ol l om on">print(W)</span><span id="5f34" class="mx my it og b gy oo ol l om on">-- output:<br/>[[-0.34857891 -0.5419972   0.43603217]<br/> [ 0.26261991  0.04720523 -0.42555547]<br/> [-0.09968833  0.3218483   0.09688095]<br/> [-0.36646565  0.37652735 -0.45564272]<br/> [-0.24990413 -0.50164433 -0.51217414]]<br/>--</span><span id="f4b7" class="mx my it og b gy oo ol l om on"># Linear projection<br/>L_0 = X.dot(W)</span><span id="0e22" class="mx my it og b gy oo ol l om on">print(L_0)</span><span id="e5a0" class="mx my it og b gy oo ol l om on">-- output:</span><span id="1fa8" class="mx my it og b gy oo ol l om on">[[-0.09968833  0.3218483   0.09688095]<br/> [-0.36646565  0.37652735 -0.45564272]<br/> [-0.34857891 -0.5419972   0.43603217]<br/> [-0.24990413 -0.50164433 -0.51217414]<br/> [ 0.26261991  0.04720523 -0.42555547]]</span></pre><p id="c78f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">投影步骤为图中的每个节点分配一个<em class="lw"> d </em>维向量表示。在这个例子中，表示节点的5长度的独热向量被映射(或投影)成3长度的密集特征向量。</p><p id="8562" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">转述Dwivedi等人的话:</p><blockquote class="op"><p id="bb95" class="oq or it bd os ot ou ov ow ox oy lm dk translated">输入层的目标是将节点(和边)的输入要素嵌入到隐藏要素的d维向量中。这种新的表示是通过简单的线性变换(也称为投影)获得的。</p></blockquote><p id="c6d0" class="pw-post-body-paragraph kr ks it kt b ku oz kd kw kx pa kg kz la pb lc ld le pc lg lh li pd lk ll lm im bi translated">为了阐明这一方面，可以分析以下块:</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="c7e3" class="mx my it og b gy ok ol l om on"># X: One-hot vectors representing the nodes<br/>[<strong class="og jd">[0. 0. 1. 0. 0.] </strong> <strong class="og jd"># Node 1 - 1 element in the <em class="lw">3rd</em> position </strong><br/> [0. 0. 0. 1. 0.]             <strong class="og jd">0 in the other positions</strong><br/> [1. 0. 0. 0. 0.]  <br/> [0. 0. 0. 0. 1.]  <br/> [0. 1. 0. 0. 0.]] </span><span id="d112" class="mx my it og b gy oo ol l om on"># W: Weight matrix<br/>[[-0.34857891 -0.5419972   0.43603217]<br/> [ 0.26261991  0.04720523 -0.42555547]<br/> <strong class="og jd">[-0.09968833  0.3218483   0.09688095] # Emphasis to the <em class="lw">3rd</em> row</strong><br/> [-0.36646565  0.37652735 -0.45564272]<br/> [-0.24990413 -0.50164433 -0.51217414]]</span><span id="6ce4" class="mx my it og b gy oo ol l om on"># L_0 (projection) = X.dot(W)<br/>[[<strong class="og jd">-0.09968833  0.3218483   0.09688095</strong>] # <strong class="og jd">Features of Node 1,</strong> [-0.36646565  0.37652735 -0.45564272]    <strong class="og jd">represented by the <em class="lw">3rd</em> row</strong><br/> [-0.34857891 -0.5419972   0.43603217]   <strong class="og jd">of the weight matrix</strong><br/> [-0.24990413 -0.50164433 -0.51217414]<br/> [ 0.26261991  0.04720523 -0.42555547]]</span></pre><p id="4107" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当前特征是随机生成的。因此，这些特征实际上并不传达关于节点的任何类型的信息。然而，节点的这些初始特征将通过两个不同的步骤来更新:</p><ul class=""><li id="9feb" class="pe pf it kt b ku kv kx ky la pg le ph li pi lm pj pk pl pm bi translated">通过GNN层聚合相邻节点的特征。</li><li id="9bd0" class="pe pf it kt b ku pn kx po la pp le pq li pr lm pj pk pl pm bi translated">通过MLP层为特定目的对神经结构进行的训练。</li></ul><p id="81fc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个双重过程的最后，我们将能够获得节点的<em class="lw">嵌入</em>表示，它将由传达特定信息的特征来表征。换句话说，节点的矢量表示将表达有意义的信息，作为人类，我们应该能够通过观察图形来识别这些信息。在最简单的情况下，相似的嵌入特征将被分配给图中相似的节点。</p><h2 id="74f6" class="mx my it bd mz na nb dn nc nd ne dp nf la ng nh ni le nj nk nl li nm nn no iz bi translated">GNN层</h2><p id="f279" class="pw-post-body-paragraph kr ks it kt b ku np kd kw kx nq kg kz la nr lc ld le ns lg lh li nt lk ll lm im bi translated">GNN层的目标是更新从输入层获得的节点的<em class="lw">维</em>维表示。这个目标是通过Dwivedi等人定义的计算来实现的。al，一个“递归邻域扩散”，通过所谓的“消息传递框架”。该框架背后的主要思想是每个结点要素都用其相邻结点的要素进行更新。邻居特征通过边作为消息被<em class="lw">传递</em>到目标节点。因此，节点的新表示编码并表示图形的局部结构。为了执行这个步骤，我们需要一个描述图中节点之间关系(边)的结构。描述图中节点之间关系的邻接矩阵在这个方向上帮助了我们。</p><p id="e224" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">考虑下面的脚本，它初始化一个5节点图中的随机邻接矩阵:</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="daee" class="mx my it og b gy ok ol l om on"># Randomly generated adjacency matrix<br/>A = np.random.randint(2, size=(n, n))<br/>np.fill_diagonal(A, 1)  # Include the self loop</span><span id="d58d" class="mx my it og b gy oo ol l om on"># The following lines are a trivial ack to create a symmetric<br/># Adj matrix that defines the edges of an undirected<br/># graph of 5 nodes<br/>A = (A + A.T)<br/>A[A &gt; 1] = 1</span><span id="19fa" class="mx my it og b gy oo ol l om on">print(A)</span><span id="a081" class="mx my it og b gy oo ol l om on">-- output:</span><span id="cd5f" class="mx my it og b gy oo ol l om on">[<strong class="og jd">[1 1 1 0 1]</strong> # Connections to Node 1<br/> [1 1 1 1 1]<br/> [1 1 1 1 0]<br/> [0 1 1 1 0]<br/> [1 1 0 0 1]]</span></pre><p id="8fd2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">邻接矩阵的每一行代表由1元素标识的到一个节点的连接。例如，第一行表示节点1连接到自身、节点2、节点3和节点5。另一方面，节点1没有连接到节点4，因为位置(1，4)中的值等于0。</p><p id="91b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们看看当我们将邻接矩阵与应用了投影的输入图层的输出相乘时会发生什么:</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="011a" class="mx my it og b gy ok ol l om on"># A: Adjacency matrix<br/>[<strong class="og jd">[1 1 1 0 1]</strong> <strong class="og jd"># Connections to Node 1</strong><br/> [1 1 1 1 1]<br/> [1 1 1 1 0]<br/> [0 1 1 1 0]<br/> [1 1 0 0 1]]</span><span id="d8fc" class="mx my it og b gy oo ol l om on"># L_0: Output from the input layer<br/>[<strong class="og jd">[-0.09968833  0.3218483   0.09688095] # Features of Node 1</strong><br/> [-0.36646565  0.37652735 -0.45564272]<br/> [-0.34857891 -0.5419972   0.43603217]<br/> [-0.24990413 -0.50164433 -0.51217414]<br/> [ 0.26261991  0.04720523 -0.42555547]]</span><span id="07c2" class="mx my it og b gy oo ol l om on"># L_1 = A.dot(L_0)<br/>[<strong class="og jd">[-0.55211298  0.20358368 -0.34828506] # What is this?</strong><br/> [-0.8020171  -0.29806065 -0.86045919]<br/> [-1.06463701 -0.34526588 -0.43490372]<br/> [-0.96494868 -0.66711419 -0.53178468]<br/> [-0.20353407  0.74558089 -0.78431723]]</span></pre><p id="3b72" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了更好地理解节点表示发生了什么，考虑下面的脚本，它将节点1的<em class="lw"> d </em>维表示求和为节点1、节点2、节点3和节点5的d维表示。</p><pre class="nv nw nx ny gt of og oh oi aw oj bi"><span id="7207" class="mx my it og b gy ok ol l om on">print(L_0[0, :] + L_0[1, :] + L_0[2, :] + L_0[4, :])</span><span id="f60a" class="mx my it og b gy oo ol l om on">-- output:<br/><strong class="og jd">[-0.55211298  0.20358368 -0.34828506]i</strong></span><span id="da51" class="mx my it og b gy oo ol l om on"># L_1 = A.dot(L_0)<br/>[<strong class="og jd">[-0.55211298  0.20358368 -0.34828506] # Features of Node 1,</strong><br/> [-0.8020171  -0.29806065 -0.86045919]   <strong class="og jd">obtained summing the</strong><br/> [-1.06463701 -0.34526588 -0.43490372]   <strong class="og jd">features of local neighbors</strong><br/> [-0.96494868 -0.66711419 -0.53178468]<br/> [-0.20353407  0.74558089 -0.78431723]]</span></pre><p id="8809" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如您所见，节点1的更新矢量表示对应于邻居特征的<em class="lw">聚合</em>(在这种情况下是求和运算)。换句话说，这种表示编码了图的局部结构。</p><p id="ce9f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种表示的关键思想之一是在神经架构中堆叠<em class="lw"> L </em>层，得到的目标节点表示聚集了节点的特征，其与目标节点的距离等于<em class="lw"> L </em>。这种行为是“递归邻域扩散”的结果。</p><p id="cb4d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如Dwivedi等人所强调的:</p><blockquote class="op"><p id="6787" class="oq or it bd os ot ou ov ow ox oy lm dk translated">“堆叠L GNN层允许网络从每个节点的L跳邻居构建节点表示。”</p></blockquote><p id="474d" class="pw-post-body-paragraph kr ks it kt b ku oz kd kw kx pa kg kz la pb lc ld le pc lg lh li pd lk ll lm im bi translated">不同GNN层之间的主要差异包括聚合类型，聚合是通过利用本地图结构来执行的。在最简单的GNN公式中，例如普通图卷积网络(GCNs)，聚集/更新是一种<em class="lw">各向同性</em>操作。这意味着以同样的方式考虑邻居节点的特征。更高级的神经架构，如图形注意力网络(GAT)，引入了<em class="lw">各向异性</em>运算，其中每个邻居节点在聚合中的贡献根据其重要性进行加权。</p><h2 id="0151" class="mx my it bd mz na nb dn nc nd ne dp nf la ng nh ni le nj nk nl li nm nn no iz bi translated">下一步是什么</h2><p id="c870" class="pw-post-body-paragraph kr ks it kt b ku np kd kw kx nq kg kz la nr lc ld le ns lg lh li nt lk ll lm im bi translated">在下一篇文章中，我将介绍GCN层，还将描述带标记边的图(知识图)的一个特定扩展，名为关系图卷积网络(R-GCN)。</p><div class="me mf gp gr mg mh"><a rel="noopener follow" target="_blank" href="/graph-neural-networks-for-multi-relational-data-27968a2ed143"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">多关系数据的图形神经网络</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">从GCNs到R-GCNs:用神经架构编码知识图的结构</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">towardsdatascience.com</p></div></div><div class="mq l"><div class="pt l ms mt mu mq mv mw mh"/></div></div></a></div><p id="2486" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">关于GAT层计算的各向异性操作的更多细节，我建议阅读下面的文章，它提供了“从math到NumPy”的详细解释。</p><div class="me mf gp gr mg mh"><a rel="noopener follow" target="_blank" href="/graph-attention-networks-under-the-hood-3bd70dc7a87"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">引擎盖下的图形注意力网络</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">从数学到数学的循序渐进指南</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">towardsdatascience.com</p></div></div><div class="mq l"><div class="pu l ms mt mu mq mv mw mh"/></div></div></a></div><p id="8e46" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">【https://towardsdatascience.com/tagged/grl-series】:<a class="ae lx" href="https://towardsdatascience.com/tagged/grl-series" rel="noopener" target="_blank"/><em class="lw">要进一步阅读关于图形表征学习的文章，可以点击以下链接关注我的系列文章。</em></p><p id="db2f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lw">如果你喜欢我的文章，你可以支持我使用这个链接</em><a class="ae lx" href="https://medium.com/@giuseppefutia/membership" rel="noopener"><em class="lw">https://medium.com/@giuseppefutia/membership</em></a><em class="lw">成为一个中等会员</em>。</p></div></div>    
</body>
</html>