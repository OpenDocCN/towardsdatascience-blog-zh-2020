<html>
<head>
<title>Machine Learning Algorithms and The Art of Hyperparameter Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法和超参数选择的艺术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-and-the-art-of-hyperparameter-selection-279d3b04c281?source=collection_archive---------13-----------------------#2020-02-11">https://towardsdatascience.com/machine-learning-algorithms-and-the-art-of-hyperparameter-selection-279d3b04c281?source=collection_archive---------13-----------------------#2020-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="52f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">回顾四种优化策略</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7a7508feb792aa1ef5df2fad3bf3ced8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MtS_MazIgI4n3cYlEVdfw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">网格搜索、随机搜索、爬山和贝叶斯优化的超参数搜索</p></figure><p id="0a01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从智能手机到航天器，机器学习算法无处不在。他们告诉你明天的天气预报，从一种语言翻译成另一种语言，并建议你可能喜欢网飞的下一部电视剧。</p><p id="8187" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些算法根据数据自动调整(学习)它们的内部参数。然而，有一个参数子集是不知道的，必须由专家来配置。这样的参数通常被称为“超参数”——随着人工智能的使用增加，它们对我们的生活产生了很大的影响。</p><p id="4a60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，决策树模型中的树深度和人工神经网络中的层数就是典型的超参数。模型的性能很大程度上取决于超参数的选择。对于中等深度的树，决策树可以产生良好的结果，而对于非常深的树，决策树的性能非常差。</p><p id="5f09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们想要手动运行，最佳超参数的选择更像是艺术而不是科学。事实上，超参数值的最佳选择取决于手头的问题。</p><p id="6e22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于算法、目标、数据类型和数据量从一个项目到另一个项目都有很大的变化，因此不存在适合所有模型和所有问题的单一最佳超参数值选择。相反，超参数必须在每个机器学习项目的上下文中进行优化。</p><p id="3bf7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们将首先回顾优化策略的威力，然后概述四种常用的优化策略:</p><ul class=""><li id="6891" class="le lf it js b jt ju jx jy kb lg kf lh kj li kn lj lk ll lm bi translated">网格搜索</li><li id="6bde" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">随机搜索</li><li id="4abb" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">爬山</li><li id="6483" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">贝叶斯优化</li></ul><p id="2330" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">优化策略</strong></p><p id="fe13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">即使专家具有深入的领域知识，手动优化模型超参数的任务也可能非常耗时。另一种方法是将专家放在一边，采用自动方法。根据一些性能度量来检测给定项目中给定模型的最优超参数集的自动过程被称为优化策略。</p><p id="63e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个典型的优化过程定义了可能的超参数集和对于特定问题要最大化或最小化的度量。因此，在实践中，任何优化程序都遵循这些经典步骤:</p><p id="bb3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">1)将手头的数据分成训练和测试子集</p><p id="e904" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2)重复优化循环固定次数或直到满足条件:</p><p id="2791" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a)选择一组新的模型超参数</p><p id="468b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">b)使用所选择的一组超参数在训练子集上训练模型</p><p id="87e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">c)将模型应用于测试子集并生成相应的预测</p><p id="6c00" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">d)使用适当的评分标准评估测试预测，如准确性或平均绝对误差。存储对应于所选超参数集的度量值</p><p id="d38e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3)比较所有度量值，并选择产生最佳度量值的超参数集</p><p id="1f28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">问题是如何从步骤2d回到步骤2a进行下一次迭代；也就是说，如何选择下一组超参数，确保它确实比前一组好。我们希望我们的优化循环朝着一个合理的好的解决方案前进，即使它可能不是最优的。换句话说，我们希望合理地确定下一组超参数是对前一组的改进。</p><p id="31a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">典型的优化过程将机器学习模型视为黑盒。这意味着在每一次迭代中，对于每一组选定的超参数，我们感兴趣的只是由选定的度量所度量的模型性能。我们不需要(想)知道黑盒里发生了什么样的魔法。我们只需要移动到下一个迭代，迭代下一个性能评估，等等。</p><p id="0e1d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有不同优化策略中的关键因素是如何在步骤2a中选择下一组超参数值，这取决于步骤2d中先前的度量输出。因此，对于一个简化的实验，我们省略了黑盒的训练和测试，我们专注于度量计算(一个数学函数)和选择下一组超参数的策略。此外，我们用任意数学函数代替了度量计算，用函数参数代替了模型超参数集。</p><p id="5fb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样，优化循环运行得更快，并尽可能保持通用性。进一步的简化是使用只有一个超参数的函数，以便于可视化。下面是我们用来演示四种优化策略的函数。我们想强调的是，任何其他的数学函数也可以。</p><p id="563b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">f(x)= sin(x/2)+0.5⋅sin(2⋅x)+0.25⋅cos(4.5⋅x)</p><p id="1c7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种简化的设置允许我们在简单的x-y图上可视化一个超参数的实验值和相应的函数值。x轴是超参数值，y轴是函数输出。然后，根据描述超参数序列生成中的点位置的白-红梯度，对(x，y)点着色。</p><p id="57be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">白色点对应于过程中较早生成的超参数值；较红的点对应于随后在该过程中产生的超参数值。这种渐变着色将有助于稍后说明优化策略之间的差异。</p><p id="d582" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个简化用例中，优化过程的目标是找到一个使函数值最大化的超参数。</p><p id="f7a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们开始回顾四种常见的优化策略，这些策略用于为优化循环的下一次迭代确定新的超参数值集。</p><p id="2976" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">网格搜索</strong></p><p id="7e3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个基本的<a class="ae ls" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search" rel="noopener ugc nofollow" target="_blank">蛮力策略</a>。如果你不知道要尝试哪些价值观，你可以尝试所有的价值观。具有固定步长的范围内的所有可能值都用于函数评估。</p><p id="5f2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果范围是[0，10]并且步长是0.1，那么我们将得到超参数值序列(0，0.1，0.2，0.3，… 9.5，9.6，9.7，9.8，9.9，10)。在网格搜索策略中，我们计算每个超参数值的函数输出。因此，网格越细，我们就越接近最优，但也需要更多的计算资源。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lt"><img src="../Images/f84c43a83edd456005fe5bb3b1196704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yHMNpbdX4LJqWHWq"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:使用步长0.1在[0，10]范围内对超参数值进行网格搜索。颜色梯度反映了生成的候选超参数序列中的位置。白色点对应于过程中早期生成的超参数值；红点对应于稍后生成的超参数值。</p></figure><p id="148c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如图1所示，超参数的范围是从小到大扫描的。</p><p id="abee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">网格搜索策略在单个参数的情况下可以很好地工作，但是当必须同时优化多个参数时，它变得非常低效。</p><p id="ae40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机搜索</strong></p><p id="58ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于<a class="ae ls" href="https://en.wikipedia.org/wiki/Random_search" rel="noopener ugc nofollow" target="_blank">随机搜索</a>策略，顾名思义，超参数的值是随机选择的。在多个超参数的情况下，这种策略通常是优选的，并且当一些超参数比其他超参数对最终度量的影响更大时，这种策略特别有效。</p><p id="2b7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，超参数值在范围[0，10]内生成。然后，随机生成固定数量N的超参数。要试验的预定义超参数的固定数量N允许您控制这种优化策略的持续时间和速度。N越大，达到最优的概率越高，但所需的计算资源也越高。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lt"><img src="../Images/394fb9a9e97b89008846e2b27dd5fb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wH3ronU1oyTKZGxU"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:在[0，10]范围内随机搜索超参数值。颜色梯度反映了生成的候选超参数序列中的位置。白色点对应于过程中早期生成的超参数值；红点对应于稍后生成的超参数值。</p></figure><p id="8f5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如所料，来自生成序列的超参数值以不递减或递增的顺序使用:白点和红点在图中随机混合(图2)。</p><p id="0c84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">爬山</strong></p><p id="c724" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每次迭代的<a class="ae ls" href="https://en.wikipedia.org/wiki/Hill_climbing" rel="noopener ugc nofollow" target="_blank">爬山</a>方法选择超参数空间中的最佳方向，以选择下一个超参数值。如果没有邻居改进最终度量，则优化循环停止。</p><p id="3310" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，这个过程在一个重要方面不同于网格和随机搜索:下一个超参数值的选择考虑了先前迭代的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lt"><img src="../Images/5a0fba61287a3479277e5000615b50ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*omAkfz9eeNr2wdsM"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:在[0，10]范围内对超参数值的爬山搜索。颜色梯度反映了生成的候选超参数序列中的位置。白色点对应于过程中早期生成的超参数值；红点对应于稍后生成的超参数值。</p></figure><p id="f90c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图3显示了应用于我们函数的爬山策略从随机超参数值x=8.4开始，然后在x=6.9时向函数最大值y=0.4移动。一旦达到最大值，在下一个邻居中没有观察到度量的进一步增加，并且搜索过程停止。</p><p id="9996" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个例子说明了与这个策略相关的一个警告:它可能会陷入第二个最大值。从其他图中，我们可以看到全局最大值位于x=4.0，对应的度量值为1.6。这种策略不会找到全局最大值，而是陷入局部最大值。这种方法的一个好的经验法则是用不同的初始值运行多次，并检查算法是否收敛到相同的最大值。</p><p id="ba2b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">贝叶斯优化</strong></p><p id="ca50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ls" href="https://en.wikipedia.org/wiki/Bayesian_optimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>策略基于先前迭代中的函数输出选择下一个超参数值，类似于爬山策略。与爬山不同，贝叶斯优化着眼于过去的全局迭代，而不仅仅是最后一次迭代。</p><p id="996a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该程序通常有两个阶段:</p><ul class=""><li id="d5db" class="le lf it js b jt ju jx jy kb lg kf lh kj li kn lj lk ll lm bi translated">在称为预热的第一阶段，随机生成超参数值。在用户定义数量N的这种随机生成的超参数之后，第二阶段开始。</li><li id="12f2" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">在第二阶段，在每次迭代中，估计P(输出|过去的超参数)类型的“替代”模型，以描述来自过去迭代的超参数值的输出值的条件概率。这个代理模型比原始函数更容易优化。因此，该算法优化了代理，并且建议代理模型的最大值处的超参数值也作为原始函数的最优值。第二阶段中的一部分迭代也用于探测最佳区域之外的区域。这是为了避免局部极大值的问题。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/29512b38d2d9fb11aa4d8ee921347aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/0*L2g_ZadGVYXT9rux"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4:超参数值在[0，10]范围内的贝叶斯优化。颜色梯度反映了生成的候选超参数序列中的位置。白色点对应于过程中早期生成的超参数值；红点对应于稍后生成的超参数值。灰点是在策略的第一个随机阶段生成的。</p></figure><p id="1a4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图4展示了贝叶斯优化策略使用预热阶段来定义最有希望的区域，然后为该区域中的超参数选择下一个值。</p><p id="15c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你还可以看到，强烈的红色点聚集在最接近最大值的地方，而淡红色和白色的点分散在各处。这表明最佳区域的定义随着第二阶段的每次迭代而提高。</p><p id="78a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">总结</strong></p><p id="2813" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们都知道在训练机器学习模型时超参数优化的重要性。由于手动优化耗时且需要专业知识，我们探索了四种常见的超参数优化自动程序。</p><p id="4f99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通常，自动优化程序遵循迭代程序，其中在每次迭代中，模型在一组新的超参数上被训练，并在测试集上被评估。最后，对应于最佳度量分数的超参数集合被选为最优集合。问题是如何选择下一组超参数，确保这实际上比前一组更好。</p><p id="d89f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们概述了四种常用的优化策略:网格搜索、随机搜索、爬山和贝叶斯优化。它们都有优点和缺点，我们通过在一个简单的玩具用例中说明它们是如何工作的来简要解释它们的区别。现在，您已经准备好在现实世界的机器学习问题中尝试它们了。</p><p id="3010" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由<a class="ae ls" href="https://www.linkedin.com/in/misha-lisovyi/" rel="noopener ugc nofollow" target="_blank">米莎·利索夫伊</a>和<a class="ae ls" href="https://www.linkedin.com/in/rosaria/" rel="noopener ugc nofollow" target="_blank">罗莎丽娅·西里波</a>克尼梅</p><p id="91a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lv">首发于</em> <a class="ae ls" href="https://thenextweb.com/podium/2019/11/11/machine-learning-algorithms-and-the-art-of-hyperparameter-selection/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">下期网页。</em>T11】</a></p></div></div>    
</body>
</html>