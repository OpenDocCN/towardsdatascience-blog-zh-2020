<html>
<head>
<title>Parallel Neural Networks and Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">并行神经网络与迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parallel-neural-networks-and-transfer-learning-dbce24f2cf58?source=collection_archive---------19-----------------------#2020-04-08">https://towardsdatascience.com/parallel-neural-networks-and-transfer-learning-dbce24f2cf58?source=collection_archive---------19-----------------------#2020-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b7ec" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">近距离观察迁移学习和一个有趣的用例</h2></div><h2 id="9b16" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h2><p id="0be4" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">你好。这是我第一篇关于媒介的文章。我已经计划写这篇文章有一段时间了。我的主要动机是帮助简化，甚至可能提供一个模板，用于下一步构建复杂的神经网络，除了您可能已经构建的特定架构中的神经元之外，还涉及并行神经元。这是因为尽管在理论上看起来很简单，但构建、保存、加载和精炼(以后再训练)神经网络确实会在我们实际开始编码它们时形成绊脚石。希望这篇文章能帮助你更好地理解与这些相关的问题，并帮助你解决在开始深度学习时不时遇到的小问题。就这样，让我们开始我们的旅程。</p><p id="0596" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">对于那些刚开始学习或正在练习神经网络的人来说，你可能已经观察到，我们使用的大多数神经网络架构通常是前馈的，即张量从一层按顺序流向下一层。假设我们想要探索，在我们构建并训练我们的架构达到一定程度的满意后，如果我们在一个层中增加神经元的数量，会发生什么。这通常意味着从头再来，可能会浪费几个小时的训练时间。仅仅为了看到在特定层中添加几个神经元的效果，从头开始似乎是不合理的。这就是迁移学习的用武之地。简单地说，它的意思是，你想把你在学习一个特定任务时学到的东西转移到另一个任务中去。在神经网络的范式中，我们学习的东西是用训练后得到的权重值来表示的。</p><p id="4579" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">当我们开始更多地了解如何利用迁移学习时，大多数内置函数都有固定的神经架构，并包含用于重新加载权重并在新的上下文中更新它们的代码。要想弄清楚将它应用到您的定制模型的细节，很可能需要几个小时或几天的时间，仔细琢磨网上的多个例子。我自己已经这样做了，我相信将我所学的东西都集中在一个地方，肯定会帮助任何想要更多地实践迁移学习以及探索增强神经网络的另一种替代方法的人。就这样，让我们开始我们的例子。</p><p id="fd2c" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我假设您熟悉 PyTorch 中关于构建神经网络的教程，可从以下链接获得:<a class="ae mc" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/beginner/blitz/cifar 10 _ tutorial . html</a>。要了解现有流行架构的迁移学习基础，请参考链接:<a class="ae mc" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/transfer _ learning _ tutorial . html</a>。我强烈建议您访问这些链接和其他可用的链接进行实践，并形成一个训练神经网络架构的全面的想法。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="89b1" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">我们的场景</h2><p id="27f2" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">假设我们现在想要创建 CIFAR 10 教程中描述的网络的副本，并将其与我们的训练版本并行放置，为图像分类的相同问题创建一个更大的网络。我们当前的模型在概念上是这样的:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/c76acfa5cf2e848f97cecadc822994d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*RT0sgUgKtWWZKLKckTbB1A.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">教程中简单的前馈神经网络。来源:这是我自己用 MS Paint 画的概念图。</p></figure><p id="eca7" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">该图的张量板描述揭示了以下内容:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/646d9f649bf04ece30556cb31405563e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*wR_WgsrovcHL_DPurKnxYA.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">我电脑上模型的张量板表示。</p></figure><p id="efc0" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我们现在的目标是构建一个如下所示的神经网络架构:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/01cdfbd8205ab922b0bf9f6cf1f7e731.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*BU32gtFPFlOQKTwoSXCeIg.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">并行前馈神经网络——本质上是并排放置的模型的核心。来源:这是我自己用 MS Paint 画的概念图。</p></figure><p id="ea64" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我们还希望这个新结构的上部子部分包含与通过执行教程获得的权重相同的权重。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="605d" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">把它编码</h2><p id="234c" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">让我们看看帮助我们实现这种结构的代码:</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="e07d" class="ki kj it mz b gy nd ne l nf ng">import torch<br/>import torchvision<br/>import torchvision.transforms as transforms</span><span id="8f8f" class="ki kj it mz b gy nh ne l nf ng">transform = transforms.Compose(<br/>    [transforms.ToTensor(),<br/>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><span id="6af4" class="ki kj it mz b gy nh ne l nf ng">trainset = torchvision.datasets.CIFAR10(root='./data', train=True,<br/>                                        download=True, transform=transform)</span><span id="4b13" class="ki kj it mz b gy nh ne l nf ng">trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,<br/>                                          shuffle=True, num_workers=0)</span><span id="d7e9" class="ki kj it mz b gy nh ne l nf ng">testset = torchvision.datasets.CIFAR10(root='./data', train=False,<br/>                                       download=True, transform=transform)</span><span id="0a14" class="ki kj it mz b gy nh ne l nf ng">testloader = torch.utils.data.DataLoader(testset, batch_size=4,<br/>                                         shuffle=False, num_workers=0)</span><span id="2f2e" class="ki kj it mz b gy nh ne l nf ng">classes = ('plane', 'car', 'bird', 'cat',<br/>           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')</span><span id="b185" class="ki kj it mz b gy nh ne l nf ng">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="2c0b" class="ki kj it mz b gy nh ne l nf ng"># functions to show an image<br/>def imshow(img):<br/>    img = img / 2 + 0.5     # unnormalize<br/>    npimg = img.numpy()<br/>    plt.imshow(np.transpose(npimg, (1, 2, 0)))<br/>    plt.show()</span><span id="1d26" class="ki kj it mz b gy nh ne l nf ng"># get some random training images<br/>dataiter = iter(trainloader)<br/>images, labels = dataiter.next()</span><span id="fd40" class="ki kj it mz b gy nh ne l nf ng"># show images<br/>imshow(torchvision.utils.make_grid(images))<br/># print labels<br/>print(' '.join('%5s' % classes[labels[j]] for j in range(4)))</span><span id="e31c" class="ki kj it mz b gy nh ne l nf ng">import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="fe9e" class="ki kj it mz b gy nh ne l nf ng">class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 6, 5)<br/>        self.pool = nn.MaxPool2d(2, 2)<br/>        self.conv2 = nn.Conv2d(6, 16, 5)<br/>        self.fc1 = nn.Linear(16 * 5 * 5, 120)<br/>        self.fc2 = nn.Linear(120, 84)<br/>        self.fc3 = nn.Linear(84, 10)</span><span id="cc95" class="ki kj it mz b gy nh ne l nf ng">def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = x.view(-1, 16 * 5 * 5)<br/>        x = F.relu(self.fc1(x))<br/>        x = F.relu(self.fc2(x))<br/>        x = self.fc3(x)<br/>        return x</span><span id="6ebb" class="ki kj it mz b gy nh ne l nf ng">net = Net()<br/>PATH = './cifar_net.pth'<br/>net.load_state_dict(torch.load(PATH))</span></pre><p id="5d9c" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">到目前为止，我们已经重新创建了从教程中学习到的模型，并为我们训练和加载了权重，以复制到我们的新网络中。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="5851" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">下面的代码构建了我们需要的架构。</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="e5d3" class="ki kj it mz b gy nd ne l nf ng">class SideNet(nn.Module):<br/>    def __init__(self):<br/>        super(SideNet, self).__init__()<br/>        self.pool = nn.MaxPool2d(2, 2)</span><span id="1dcc" class="ki kj it mz b gy nh ne l nf ng">        self.conv11 = nn.Conv2d(3, 6, 5)<br/>        self.conv12 = nn.Conv2d(6, 16, 5)<br/>        <br/>        self.conv11.weight.data.copy_( net.conv1.weight.data)<br/>        self.conv12.weight.data.copy_(net.conv2.weight.data)<br/>        <br/>        self.conv21 = nn.Conv2d(3, 6, 5)<br/>        self.conv22 = nn.Conv2d(6, 16, 5)<br/>        <br/>        self.fc11 = nn.Linear(16 * 5 * 5, 120)<br/>        self.fc12 = nn.Linear(120, 84)<br/>    <br/>        self.fc11.weight.data.copy_(net.fc1.weight.data)<br/>        self.fc12.weight.data.copy_(net.fc2.weight.data)<br/>        <br/>        self.fc21 = nn.Linear(16 * 5 * 5, 120)<br/>        self.fc22 = nn.Linear(120, 84)<br/>        <br/>        self.fc3 = nn.Linear(168,10)<br/>    def forward(self, x):<br/>        y = self.pool(F.relu(self.conv11(x)))<br/>        y = self.pool(F.relu(self.conv12(y)))<br/>        y = y.view(-1, 16 * 5 * 5)<br/>        y = F.relu(self.fc11(y))<br/>        y = F.relu(self.fc12(y))<br/>        <br/>        x = self.pool(F.relu(self.conv21(x)))<br/>        x = self.pool(F.relu(self.conv22(x)))<br/>        x = x.view(-1, 16 * 5 * 5)<br/>        x = F.relu(self.fc21(x))<br/>        x = F.relu(self.fc22(x))<br/>    <br/>        out = self.fc3(torch.cat((x,y),dim=1))<br/>        return out</span><span id="0f25" class="ki kj it mz b gy nh ne l nf ng"># create a new model<br/>net1 = SideNet()</span></pre><p id="234a" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我们现在已经创建了我们的网络架构，并在类 SideNet()的 forward 函数中定义了网络中的张量流。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="683d" class="ki kj it mz b gy nd ne l nf ng">self.conv11.weight.data.copy_( net.conv1.weight.data)</span><span id="8bc9" class="ki kj it mz b gy nh ne l nf ng">self.conv12.weight.data.copy_(net.conv2.weight.data)</span><span id="95e6" class="ki kj it mz b gy nh ne l nf ng">self.fc11.weight.data.copy_(net.fc1.weight.data)</span><span id="c340" class="ki kj it mz b gy nh ne l nf ng">self.fc12.weight.data.copy_(net.fc2.weight.data)</span></pre><p id="39d4" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">这段代码片段在很多方面都很关键。你可能会立刻意识到这里发生了什么，因为这是迁移学习中基本发生的事情。我们刚刚将训练好的网络的权重复制到新结构的上部子部分。瞧啊。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="eddc" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我们现在可以选择保持这些权重不变，通过设置 requires_grad = False(实质上是冻结那些层的权重，这些层的权重是从我们训练的网络中复制的)或者在训练我们的新架构时更新它们。因为我只训练了几个纪元的网络，所以我将训练我们架构的两个子部分的权重。最初只训练一个时期的原因将在以后变得清楚。</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="5be3" class="ki kj it mz b gy nd ne l nf ng">#check weights<br/>print(net.fc2.weight.data)<br/>print(net1.fc12.weight.data)<br/>print(net1.fc22.weight.data)</span><span id="6e35" class="ki kj it mz b gy nh ne l nf ng">#for param in net.parameters():<br/>#    param.requires_grad = False<br/>from torch.utils.tensorboard import SummaryWriter</span><span id="48ba" class="ki kj it mz b gy nh ne l nf ng"># default `log_dir` is "runs" - we'll be more specific here<br/>writer = SummaryWriter('runs/temp')</span><span id="f54c" class="ki kj it mz b gy nh ne l nf ng"># write model to tensorboard<br/>writer.add_graph(net1, images)</span><span id="a2d1" class="ki kj it mz b gy nh ne l nf ng">writer.close()</span><span id="5881" class="ki kj it mz b gy nh ne l nf ng"># train the new model<br/>import torch.optim as optim</span><span id="c447" class="ki kj it mz b gy nh ne l nf ng">criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.SGD(net1.parameters(), lr=0.01)</span><span id="caa7" class="ki kj it mz b gy nh ne l nf ng">for epoch in range(1):  # loop over the dataset multiple times</span><span id="a597" class="ki kj it mz b gy nh ne l nf ng">running_loss = 0.0<br/>    for i, data in enumerate(trainloader, 0):<br/>        # get the inputs; data is a list of [inputs, labels]<br/>        inputs, labels = data</span><span id="51f2" class="ki kj it mz b gy nh ne l nf ng"># zero the parameter gradients<br/>        optimizer.zero_grad()</span><span id="e306" class="ki kj it mz b gy nh ne l nf ng"># forward + backward + optimize<br/>        outputs = net1(inputs)<br/>        loss = criterion(outputs, labels)<br/>        loss.backward()<br/>        optimizer.step()</span><span id="b487" class="ki kj it mz b gy nh ne l nf ng"># print statistics<br/>        running_loss += loss.item()<br/>        if i % 2000 == 1999:    # print every 2000 mini-batches<br/>            print('[%d, %5d] loss: %.3f' %<br/>                  (epoch + 1, i + 1, running_loss / 2000))<br/>            running_loss = 0.0</span><span id="63a1" class="ki kj it mz b gy nh ne l nf ng">print('Finished Training')</span><span id="c057" class="ki kj it mz b gy nh ne l nf ng">correct = 0<br/>total = 0<br/>with torch.no_grad():<br/>    for data in testloader:<br/>        images, labels = data<br/>        outputs = net1(images)<br/>        _, predicted = torch.max(outputs.data, 1)<br/>        total += labels.size(0)<br/>        correct += (predicted == labels).sum().item()</span><span id="509b" class="ki kj it mz b gy nh ne l nf ng">print('Accuracy of the network on the 10000 test images: %d %%' % (<br/>    100 * correct / total))<br/>#check weights<br/>print(net.fc2.weight.data)<br/>print(net1.fc12.weight.data)<br/>print(net1.fc22.weight.data)</span></pre><p id="710e" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">这里，优化器的参数设置如下:学习率= 0.01，没有使用动量。我使用相同的参数来训练初始网络。这是因为当我们并排查看新网络和初始网络的权重时，我们需要检查旧网络的权重是否被复制，新网络的权重是否已被适当初始化，以及在训练新网络的权重时，初始网络的权重是否没有改变。最后一个短语是关键，有许多方法可以实现前一句中的前两个短语，但它们通常也会导致修改初始网络的权重！例如，如果使用以下代码片段:</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="45eb" class="ki kj it mz b gy nd ne l nf ng">self.conv11.weight.data = net.conv1.weight.data</span><span id="3a64" class="ki kj it mz b gy nh ne l nf ng">self.conv12.weight.data = net.conv2.weight.data</span><span id="fb3c" class="ki kj it mz b gy nh ne l nf ng">self.fc11.weight.data = net.fc1.weight.data</span><span id="2fb3" class="ki kj it mz b gy nh ne l nf ng">self.fc12.weight.data = net.fc2.weight.data</span></pre></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="57e3" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">让我们看看 TensorBoard 推断出的结构:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/8d30cd57420a0bb1d2ca4acf59f105c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*y9uQ_UUxZzQD-BZpVd13EQ.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">我电脑上模型的张量板表示。</p></figure><p id="62b4" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">它确实符合我们试图建立的东西。太好了！</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="c4cc" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">验证我们对实际情况的理解</h2><p id="f1d1" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">作为验证步骤，让我们打印两个网络的 FC2 层(全连接 2)的权重值:</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="b520" class="ki kj it mz b gy nd ne l nf ng">print(net.fc2.weight.data) # before train<br/>tensor([[-2.5432e-03, -6.9285e-02,  7.7019e-02,  ...,  2.8243e-02,<br/>          4.9399e-02, -8.7909e-05],<br/>        [-7.2035e-02, -1.2313e-03, -8.9993e-02,  ...,  1.8121e-02,<br/>         -6.1479e-02, -3.8699e-02],<br/>        [-6.3147e-02,  5.5815e-02, -6.0806e-02,  ...,  3.3566e-02,<br/>          7.6486e-02,  7.3699e-02],<br/>        ...,<br/>        [ 1.9772e-03, -1.8449e-02,  6.8946e-02,  ..., -2.1011e-02,<br/>          7.5202e-02,  4.1823e-02],<br/>        [ 2.9912e-02, -7.9396e-02, -8.7561e-02,  ...,  4.6011e-02,<br/>         -9.0685e-02,  4.1302e-02],<br/>        [-1.8297e-02, -7.3356e-02,  4.7250e-02,  ..., -7.5147e-02,<br/>         -6.4722e-02,  6.0243e-02]])</span><span id="77b8" class="ki kj it mz b gy nh ne l nf ng">print(net.fc2.weight.data) # after train<br/>tensor([[-0.0151, -0.0470,  0.1057,  ...,  0.0288,  0.0280,  0.0171],<br/>        [-0.0720, -0.0029, -0.0907,  ...,  0.0181, -0.0630, -0.0408],<br/>        [-0.0417,  0.0548, -0.1226,  ...,  0.0335,  0.0679,  0.0900],<br/>        ...,<br/>        [ 0.0074, -0.0028,  0.0292,  ..., -0.0218,  0.0754,  0.0473],<br/>        [ 0.0307, -0.0784, -0.0875,  ...,  0.0460, -0.0903,  0.0510],<br/>        [-0.0252, -0.0824,  0.0380,  ..., -0.0744, -0.0741,  0.1009]])</span><span id="2a56" class="ki kj it mz b gy nh ne l nf ng">In our new model code, before train,<br/>print(net.fc2.weight.data)<br/>print(net1.fc12.weight.data)<br/>print(net1.fc22.weight.data)</span><span id="eeed" class="ki kj it mz b gy nh ne l nf ng"><br/>tensor([[-0.0151, -0.0470,  0.1057,  ...,  0.0288,  0.0280,  0.0171],<br/>        [-0.0720, -0.0029, -0.0907,  ...,  0.0181, -0.0630, -0.0408],<br/>        [-0.0417,  0.0548, -0.1226,  ...,  0.0335,  0.0679,  0.0900],<br/>        ...,<br/>        [ 0.0074, -0.0028,  0.0292,  ..., -0.0218,  0.0754,  0.0473],<br/>        [ 0.0307, -0.0784, -0.0875,  ...,  0.0460, -0.0903,  0.0510],<br/>        [-0.0252, -0.0824,  0.0380,  ..., -0.0744, -0.0741,  0.1009]])<br/>tensor([[-0.0151, -0.0470,  0.1057,  ...,  0.0288,  0.0280,  0.0171],<br/>        [-0.0720, -0.0029, -0.0907,  ...,  0.0181, -0.0630, -0.0408],<br/>        [-0.0417,  0.0548, -0.1226,  ...,  0.0335,  0.0679,  0.0900],<br/>        ...,<br/>        [ 0.0074, -0.0028,  0.0292,  ..., -0.0218,  0.0754,  0.0473],<br/>        [ 0.0307, -0.0784, -0.0875,  ...,  0.0460, -0.0903,  0.0510],<br/>        [-0.0252, -0.0824,  0.0380,  ..., -0.0744, -0.0741,  0.1009]])</span><span id="22b0" class="ki kj it mz b gy nh ne l nf ng"><br/>tensor([[ 0.0864,  0.0843,  0.0060,  ...,  0.0325, -0.0519, -0.0048],<br/>        [ 0.0394, -0.0486, -0.0258,  ...,  0.0515,  0.0077, -0.0702],<br/>        [ 0.0570, -0.0178,  0.0411,  ..., -0.0026, -0.0385,  0.0893],<br/>        ...,<br/>        [-0.0760,  0.0237,  0.0782,  ...,  0.0338,  0.0055, -0.0830],<br/>        [-0.0755, -0.0767,  0.0308,  ..., -0.0234, -0.0403,  0.0812],<br/>        [ 0.0057, -0.0511, -0.0834,  ...,  0.0028,  0.0834, -0.0340]])</span><span id="c998" class="ki kj it mz b gy nh ne l nf ng">After training,<br/>print(net.fc2.weight.data)<br/>print(net1.fc12.weight.data)<br/>print(net1.fc22.weight.data)</span><span id="5da3" class="ki kj it mz b gy nh ne l nf ng">tensor([[-0.0151, -0.0470,  0.1057,  ...,  0.0288,  0.0280,  0.0171],<br/>        [-0.0720, -0.0029, -0.0907,  ...,  0.0181, -0.0630, -0.0408],<br/>        [-0.0417,  0.0548, -0.1226,  ...,  0.0335,  0.0679,  0.0900],<br/>        ...,<br/>        [ 0.0074, -0.0028,  0.0292,  ..., -0.0218,  0.0754,  0.0473],<br/>        [ 0.0307, -0.0784, -0.0875,  ...,  0.0460, -0.0903,  0.0510],<br/>        [-0.0252, -0.0824,  0.0380,  ..., -0.0744, -0.0741,  0.1009]])</span><span id="b149" class="ki kj it mz b gy nh ne l nf ng">tensor([[-0.0322, -0.0377,  0.0366,  ...,  0.0290,  0.0322,  0.0069],<br/>        [-0.0749, -0.0033, -0.0902,  ...,  0.0179, -0.0650, -0.0402],<br/>        [-0.0362,  0.0748, -0.1354,  ...,  0.0352,  0.0715,  0.1009],<br/>        ...,<br/>        [ 0.0244, -0.0192, -0.0326,  ..., -0.0220,  0.0661,  0.0834],<br/>        [ 0.0304, -0.0785, -0.0976,  ...,  0.0461, -0.0911,  0.0529],<br/>        [-0.0225, -0.0737,  0.0275,  ..., -0.0747, -0.0805,  0.1130]])</span><span id="3e08" class="ki kj it mz b gy nh ne l nf ng">tensor([[ 0.0864,  0.0843,  0.0060,  ...,  0.0325, -0.0519, -0.0048],<br/>        [ 0.0390, -0.0469, -0.0283,  ...,  0.0506,  0.0030, -0.0723],<br/>        [ 0.0571, -0.0178,  0.0411,  ..., -0.0027, -0.0389,  0.0893],<br/>        ...,<br/>        [-0.0763,  0.0230,  0.0792,  ...,  0.0337,  0.0065, -0.0802],<br/>        [-0.0756, -0.0769,  0.0306,  ..., -0.0235, -0.0413,  0.0810],<br/>        [ 0.0048, -0.0525, -0.0822,  ...,  0.0019,  0.0785, -0.0313]])</span></pre><p id="b8ed" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">因此，我们可以观察到，在训练新模型时，我们的初始模型的参数权重被复制，但没有改变。此外，复制了权重的层在训练后改变了权重，从而验证了我们的健全性检查。</p><p id="f506" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我们现在已经建立了一个更复杂的模型，并且能够并行地重用我们的权重。当然，如果想要中间并行，只需要改变类 SideNet()的 forward 函数中张量的流向即可。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="1329" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">另一个例子</h2><p id="96f3" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">例如，假设我们希望保留卷积层，但在此之后引入两条平行路线。我们想要:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8b716d77dc76b49ad2c2af677e5498dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*JDVxtfYIzWerrjj9V9yV9Q.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">在神经网络之间引入并行性。来源:这是我自己用 MS Paint 画的概念图。</p></figure><p id="2881" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">类 SideNet()现在看起来如下:</p><pre class="ml mm mn mo gt my mz na nb aw nc bi"><span id="b91d" class="ki kj it mz b gy nd ne l nf ng">class SideNet(nn.Module):<br/>    def __init__(self):<br/>        super(SideNet, self).__init__()<br/>        self.pool = nn.MaxPool2d(2, 2)</span><span id="6b7b" class="ki kj it mz b gy nh ne l nf ng">self.conv11 = nn.Conv2d(3, 6, 5)<br/>        self.conv12 = nn.Conv2d(6, 16, 5)<br/>        <br/>        self.conv11.weight.data.copy_(net.conv1.weight.data)<br/>        self.conv12.weight.data.copy_(net.conv2.weight.data)<br/>                <br/>        self.fc11 = nn.Linear(16 * 5 * 5, 120)<br/>        self.fc12 = nn.Linear(120, 84)<br/>    <br/>        self.fc11.weight.data.copy_(net.fc1.weight.data)<br/>        self.fc12.weight.data.copy_(net.fc2.weight.data)<br/>        <br/>        self.fc21 = nn.Linear(16 * 5 * 5, 120)<br/>        self.fc22 = nn.Linear(120, 84)<br/>        <br/>        self.fc3 = nn.Linear(168,10)</span><span id="74a3" class="ki kj it mz b gy nh ne l nf ng">def forward(self, x):<br/>        y = self.pool(F.relu(self.conv11(x)))<br/>        y = self.pool(F.relu(self.conv12(y)))<br/>        <br/>        z = y.view(-1, 16 * 5 * 5)<br/>        y = F.relu(self.fc11(z))<br/>        y = F.relu(self.fc12(y))<br/>        <br/>    <br/>        x = F.relu(self.fc21(z))<br/>        x = F.relu(self.fc22(x))<br/>    <br/>        out = self.fc3(torch.cat((x,y),dim=1))<br/>        return out<br/># create a new model<br/>net1 = SideNet()</span></pre><p id="c7bb" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">TensorBoard 的描绘证实了我们想要建造的东西:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/00b5d5ac8fb425b515d2178180fc5083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*ZHEvyzaDlg9A1mpXbVPXdQ.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">在我的电脑上对我们改进的并行神经网络的张量板描绘。</p></figure></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="72dd" class="pw-post-body-paragraph le lf it lg b lh lx ju lj lk ly jx lm kr lz lo lp kv ma lr ls kz mb lu lv lw im bi translated">我希望所有这些例子能让你对编写复杂的神经网络充满信心，并成为你机器学习之旅中的关键垫脚石。感谢阅读:)</p></div></div>    
</body>
</html>