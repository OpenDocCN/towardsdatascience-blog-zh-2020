<html>
<head>
<title>Ridge Regression Regularization from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始岭回归正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-regression-regularization-from-scratch-9149be19ff19?source=collection_archive---------62-----------------------#2020-08-25">https://towardsdatascience.com/ridge-regression-regularization-from-scratch-9149be19ff19?source=collection_archive---------62-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e7d3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">降低回归模型的方差。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fb79294d4da8f8a0549652f5bbe82f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ft3HgOHLFehX8oFRZ8DZBA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贾里德·阿兰戈在<a class="ae ky" href="https://unsplash.com/s/photos/abstract?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a42b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使机器学习模型有效，它必须在偏差和方差之间取得平衡。岭回归正则化旨在通过折衷偏差来减少方差。</p><h1 id="a393" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">偏差和方差:</h1><p id="77bf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">要理解岭回归正则化的使用及其实现，需要了解它试图解决的问题的一些背景。</p><p id="3336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">偏差本质上是模型的均方误差，当应用于它被训练的数据时。具有低均方误差的模型因此具有低偏差。</p><p id="ca06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方差是将数据与数据集的不同样本进行比较时，损失变化的度量。过度拟合的模型具有很高的方差，因为它们的预测对其他样本不起作用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/22c8b2057311db49a13b807db52d271b.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*g7bZegCyQkq7I_SDDs07Ww.png"/></div></figure><p id="78d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型有很高的偏差，因为它完美地符合每一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d904d65baee8fbcef3283d7fcdd1a875.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*DXX6WCW7kr8ewqDQK-RumQ.png"/></div></figure><p id="f57e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当对一个不同的样本进行拟合时，它没有被训练过，表现很差。</p><p id="543a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每种机器学习算法都有偏差和方差之间的权衡。一般的经验法则是，参数越多的模型偏差越小，但方差越大，而参数越少的模型偏差越大，但方差越小。</p><h1 id="3416" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">岭正则化如何减少方差？</h1><p id="fccd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与最小化均方误差函数的回归模型相反，岭正则化最小化添加到λ和斜率平方的乘积的均方误差函数。</p><h1 id="eb99" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码:</h1><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="893f" class="my lw it mu b gy mz na l nb nc">import numpy as np<br/>from matplotlib import pyplot as plt</span><span id="1fba" class="my lw it mu b gy nd na l nb nc">def sigmoid(x):<br/>    return 1/(1+np.exp(-x))</span><span id="bce2" class="my lw it mu b gy nd na l nb nc">def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))</span><span id="b72f" class="my lw it mu b gy nd na l nb nc">def relu(x):<br/>    return np.maximum(x, 0)</span><span id="8b5b" class="my lw it mu b gy nd na l nb nc">def relu_p(x):<br/>    return np.heaviside(x, 0)</span><span id="e0af" class="my lw it mu b gy nd na l nb nc">def tanh(x):<br/>    return np.tanh(x)</span><span id="0381" class="my lw it mu b gy nd na l nb nc">def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2</span><span id="3bf8" class="my lw it mu b gy nd na l nb nc">def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)</span><span id="3eca" class="my lw it mu b gy nd na l nb nc">class NeuralNetwork:<br/>    def __init__(self):<br/>        self.layers = []<br/>        self.weights = []<br/>        self.loss = []<br/>    def add(self,layer_function):<br/>        self.layers.append(layer_function)<br/>            <br/>    def propagate(self,X):<br/>        As,Zs = [],[]<br/>        input_data = X<br/>        for layer in self.layers:<br/>            index = self.layers.index(layer)<br/>            weights = layer.initialize_weights(self.layers,index)<br/>            if type(weights) != type(None) :<br/>                self.weights.append(weights)<br/>            a,z = layer.propagate(input_data)<br/>            As.append(a)<br/>            Zs.append(z)<br/>            input_data = a<br/>            print(a.shape)<br/>        return As,Zs<br/>    <br/>    def train(self,X,y,iterations):<br/>        loss = []<br/>        for i in range(iterations):<br/>            As,Zs = self.propagate(X)<br/>            loss.append(np.square(sum(y - As[-1])))<br/>            As.insert(0,X)<br/>            g_wm = [0] * len(self.layers)<br/>            for i in range(len(g_wm)):<br/>                pre_req = (y-As[-1])*2<br/>                a_1 = As[-(i+2)]<br/>                z_index = -1<br/>                w_index = -1<br/>                if i == 0:<br/>                    range_value = 1<br/>                else:<br/>                    range_value = 2*i<br/>                for j in range(range_value):<br/>                    if j% 2 == 0:<br/>                        pre_req = pre_req * sigmoid_p(Zs[z_index])<br/>                        z_index -= 1<br/>                    else:<br/>                        pre_req = np.dot(pre_req,self.weights[w_index].T)<br/>                        w_index -= 1<br/>                gradient = np.dot(a_1.T,pre_req)<br/>                g_wm[-(i+1)] = gradient<br/>                for i in range(len(self.layers)):<br/>                    if self.layers.trainable == True:<br/>                        self.layers[i].network_train(g_wm[i])<br/>        <br/>        return loss<br/>        <br/>    class Perceptron:<br/>        def __init__(self,nodes,input_shape= None,activation = None):<br/>            self.nodes = nodes<br/>            self.input_shape = input_shape<br/>            self.activation = activation<br/>            self.trainable = True<br/>        def initialize_weights(self,layers,index):<br/>            if self.input_shape:<br/>                self.weights = np.random.randn(self.input_shape[-1],self.nodes)<br/>            else:<br/>                self.weights = np.random.randn(layers[index-1].output_shape[-1],self.nodes)<br/>            return self.weights<br/>        def propagate(self,input_data):<br/>            z = np.dot(input_data,self.weights)<br/>            if self.activation:<br/>                a = self.activation(z)<br/>            else:<br/>                a = z<br/>            self.output_shape = a.shape<br/>            return a,z<br/>        def network_train(self,gradient):<br/>            self.weights += gradient<br/>                <br/>model = NeuralNetwork()</span><span id="aba1" class="my lw it mu b gy nd na l nb nc">Perceptron = model.Perceptron</span></pre><p id="06f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我创建的一个神经网络框架，执行基本的线性和逻辑回归。我在这里添加了完整的代码，这样当示例完成时，您可以自己实现它。让我们从培训功能中删除所有内容:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="4ae2" class="my lw it mu b gy mz na l nb nc">def train(self,X,y,iterations):<br/>        loss = []<br/>        for i in range(iterations):<br/>            As,Zs = self.propagate(X)<br/>            loss.append(np.square(sum(y - As[-1])))<br/>            As.insert(0,X)<br/>            g_wm = [0] * len(self.layers)<br/>            for i in range(len(g_wm)):<br/>                pre_req = (y-As[-1])*2<br/>                a_1 = As[-(i+2)]<br/>                z_index = -1<br/>                w_index = -1<br/>                if i == 0:<br/>                    range_value = 1<br/>                else:<br/>                    range_value = 2*i<br/>                for j in range(range_value):<br/>                    if j% 2 == 0:<br/>                        pre_req = pre_req * sigmoid_p(Zs[z_index])<br/>                        z_index -= 1<br/>                    else:<br/>                        pre_req = np.dot(pre_req,self.weights[w_index].T)<br/>                        w_index -= 1<br/>                gradient = np.dot(a_1.T,pre_req)<br/>                g_wm[-(i+1)] = gradient<br/>                for i in range(len(self.layers)):<br/>                    if self.layers.trainable == True:<br/>                        self.layers[i].network_train(g_wm[i])</span></pre><p id="4682" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个代码通过神经网络导航，将导数链接到它们各自的部分。您会注意到变量 pre_req 对于所有计算的梯度都是一致的。这是预测对损失函数的偏导数。我们需要更改的只是那行代码:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b493" class="my lw it mu b gy mz na l nb nc">pre_req = (y-As[-1])*2</span></pre><p id="0ff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目前，该变量包含均方误差函数的导数。让我们用岭回归损失函数的导数来代替它:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="24e7" class="my lw it mu b gy mz na l nb nc">#The loss function<br/>loss = np.square(y-As[-1]) + lam*self.weights[0]<br/>#the derivative<br/>pre_req = (y-As[-1])*2 + lam*self.weights[0]</span></pre><h1 id="80a1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论:</h1><p id="2ccb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">岭正则化的工作原理是对较高斜率值的模型进行惩罚，从而降低数据集中不同样本之间的方差</p><p id="7267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谢谢你看我的文章！</p></div></div>    
</body>
</html>