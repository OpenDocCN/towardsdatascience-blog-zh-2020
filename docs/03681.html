<html>
<head>
<title>Cross-Entropy, Log-Loss, And Intuition Behind It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵、对数损失及其背后的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514?source=collection_archive---------42-----------------------#2020-04-06">https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514?source=collection_archive---------42-----------------------#2020-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="46a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这篇博客中，你会对交叉熵和对数损失在机器学习中的应用有一个直观的了解。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0764fdb4a1e93ce2b859ed6c6ac021db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*603q5DF2svLM1U0T4dWULg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@freegraphictoday?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">absolute vision</a>在<a class="ae kv" href="https://unsplash.com/s/photos/compass?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="ac57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在浏览这个博客之前，你不需要知道任何事情，因为我将从基础开始。以下内容将在本博客中详细介绍和解释。</p><ol class=""><li id="a2f6" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">随机变量</strong></li><li id="ee10" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">信息内容</strong></li><li id="e778" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">熵</strong></li><li id="146f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">交叉熵和 K-L 散度</strong></li><li id="f1fb" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">日志丢失</strong></li></ol><h2 id="2ada" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">随机变量:</strong></h2><p id="7f9f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">这被定义为一个获取随机事件输出的变量。例如，我们可以定义一个变量 X，它取掷骰子的输出值。这里 X 可以取 1 到 6 的值。</p><p id="f0cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有时也会计算随机变量取特定值的概率。例如，P(X=1)是 1/6，这对于其他值也是一样的，因为它们同样可能发生。</p><h2 id="a410" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">信息内容(IC): </strong></h2><p id="0f9b" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">当我们讨论信息内容时，我们是在随机变量的背景下进行的。你可以这样想，如果你知道一个事件将要发生，而且这个事件经常发生，那么信息量就非常少。现在，如果你知道一个事件将要发生，而这个事件很少发生，那么信息量就很大。比如有两个随机变量 X 和 Y，这里 X 告诉太阳会不会升起，Y 告诉今天会不会有地震<strong class="ky ir">(惊喜元素)</strong>。你很容易得出 Y 包含更多信息的结论。</p><blockquote class="ne"><p id="814e" class="nf ng iq bd nh ni nj nk nl nm nn lr dk translated">信息含量与惊喜元素成正比。</p></blockquote><p id="9918" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf nq lh li lj nr ll lm ln ns lp lq lr ij bi translated">因此，我们可以得出结论，一个随机变量的信息含量取决于一个事件发生的概率。如果概率很低，信息量就很大。我们可以用数学方法写成如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/9c52f352fa1cf8c72c80529fb0e5bc47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wL6Zg1wlHWww5DZTK8Mzvg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">信息内容</p></figure><p id="bd25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此 IC 是概率的函数。这里 P(X=S)表示 X 取值 S 的概率，这将用于博客的其余部分。信息内容显示以下属性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/7ac0b40a06fda20450396df5ed6d67be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d72iFNeX1ajyz8vel1REeg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">如果 X 和 Y 是独立事件，IC 的性质。</p></figure><p id="f03f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图中的第二个性质见于对数函数族。这给了我们用对数作为函数来计算 IC 的直觉。因此，现在我们可以用数学方法定义 IC 如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/8ade17dffc96390f42ea11e4247cc885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6h7hmbSmN-EH-eez-dki6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">IC 公式。</p></figure><h2 id="43ab" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">熵:</strong></h2><p id="46f2" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">随机变量的熵被定义为随机变量的期望信息量。我们会用电子学分支的信息论来更直观的解释。</p><p id="9d6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在信息论中，发送值 X=某个值的比特数称为信息量。如果随机变量有 N 个值，那么我们说它是一个信号。事实上，我们将信号定义为取 N 个值的随机变量。一个信号在不同的时间间隔取不同的值，因此我们可以定义一个随机变量，它取信号可以取的 N 个可能的值。现在我们找到发送信号所需的比特数，这就是随机变量的预期信息内容。换句话说，我们称之为熵。下面的例子将使它更清楚。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/91d1145284ea6075d5084b036a5fc076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55HzsE6Ma-HxKqoMm7rRBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">举例说明如何计算熵。</p></figure><p id="40e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用以下公式计算熵/预期 IC</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/8c223cda0a14b1e25a4b6fb6e077c3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4gtDid839OsTJq2TtqxQA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">计算熵/预期 IC 的公式</p></figure><p id="ace3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用这个公式我们得到熵= (1/2*1)+(1/4*2)+(1/4*2) = 3/2。因此，平均而言，我们将使用 1.5 位来发送该信号。</p><h2 id="f988" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">交叉熵和 K-L 散度:</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/e5133f879d387fcbbf9f489ff05b53de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPBdv5i0pXVVXjTbKj65Wg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据传送</p></figure><p id="4706" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接收端，我们不知道随机变量的实际分布。我们将看到接收到 100 个信号，然后估计随机变量的分布。现在让我们假设发送方的实际分布是“y ”,估计是'y^'.在这里，分布意味着随机变量取特定值的概率。以下是发送数据所需的位数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3fdb1ec9aff04c33cc8e56c5d67031a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*iEw1JsVaCHGJ3TdPwTGUsQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">计算发送方和接收方位数的公式。</p></figure><p id="b369" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经知道位数和熵是一样的。发送方的熵称为熵，接收方的估计熵称为<strong class="ky ir">交叉熵</strong>。现在，这被称为交叉熵，因为我们使用实际分布和估计分布来计算接收端的估计熵。你一定会想到的另一个问题是，为什么我们在接收端使用实际分布(y)。答案是，我们正在估计接收到的随机变量(-log(yi^).)的每个值所需的比特数所使用的比特数将取决于接收机接收到的随机变量的分布。如果在这之后还不清楚，那么让我们举个例子。</p><p id="120e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们估计 P(X=A)是 1/4，而实际是 1/8，那么估计的比特数将是-(log(1/4)) = 2，但是对最终答案的贡献将是 1/8*(2) = 1/4，因为我们将在接收器接收这个值 1/8 次。现在我想这已经很清楚了。</p><p id="0e99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> K-L 散度</strong>等于交叉熵和熵之差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/62f0fe54239f0e2e4320909567b83341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNS97JW_MQkohakhg8LI1g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K-L 散度。</p></figure><h2 id="3366" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">日志丢失:</strong></h2><p id="1c84" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">现在我们将转到机器学习部分。我想我们知道 y^是什么意思。如果你不知道，那么 Y^就是给定数据点属于特定类别/标签的预测概率。例如，我们可以在机器学习中有一个模型，它将判断一个文本是否是滥用的。计算 y^的公式如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/423c8693c262bdbcc6f01d4985af8f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*bfl_8h7ky5f9Dzmm9FT7Pw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据点属于某一类的概率公式。</p></figure><p id="9ca4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里<strong class="ky ir">‘w’是权重向量</strong>，<strong class="ky ir">‘x’是<strong class="ky ir">数据点</strong>的 d 维表示</strong>，<strong class="ky ir">‘b’是偏差项</strong>。</p><blockquote class="ne"><p id="f0e4" class="nf ng iq bd nh ni nj nk nl nm nn lr dk translated"><strong class="ak">我们在所有机器学习中的主要目标是正确估计训练数据集中数据点的分布</strong>。</p></blockquote><p id="ce6c" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf nq lh li lj nr ll lm ln ns lp lq lr ij bi translated">在这里，训练集可以被视为发送者，而模型可以被视为试图估计分布的接收者。</p><p id="d989" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当 K-L 散度最小时，将出现最佳估计。因此我们将找到对应于最小 K-L 散度的(w，b)。在更新(w，b)时，我们忽略熵项，因为它是一个常数，只有交叉熵项变化。因此，我们的损失方程如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/379ecda421145560f8e0a95f2e55b949.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*fnG48WKO2oqLygMda9dTiQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">失败</p></figure><p id="617d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是损失项，我们通常称之为对数损失，因为它包含对数项。</p><p id="bbbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<strong class="ky ir">二进制分类</strong>，其中‘yi’可以是 0 或 1。这个损失看起来将类似于<strong class="ky ir">loss =-(y * log(y)+(1-y)* log(1-y))</strong>。这是我们大多数人都熟悉的。暂时就这些了。我希望你都记下了。</p><p id="1683" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这个内容，请在下面的评论中告诉我。</p><p id="a8d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考资料:</p><p id="5c2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cross_entropy</a></p><p id="5eec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Information_content" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Information_content</a></p><p id="0eea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Random_variable" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Random_variable</a></p><p id="f387" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想了解什么是校准，那就去看看这个<a class="ae kv" href="https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555" rel="noopener"> <strong class="ky ir">博客</strong> </a>。</p><div class="oa ob gp gr oc od"><a href="https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555" rel="noopener follow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">机器学习中的校准</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">在这篇博客中，我们将学习什么是校准，为什么以及何时应该使用它。</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">medium.com</p></div></div><div class="om l"><div class="on l oo op oq om or kp od"/></div></div></a></div><p id="beee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想学习如何防止自己的模型欠拟合、欠拟合，那就去翻翻这篇<a class="ae kv" rel="noopener" target="_blank" href="/overfitting-and-underfitting-in-machine-learning-89738c58f610"> <strong class="ky ir">博客</strong> </a>。</p><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/overfitting-and-underfitting-in-machine-learning-89738c58f610"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">机器学习中的过拟合和欠拟合</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">在这篇文章中，你将了解什么是过度拟合和欠拟合。您还将学习如何防止模型…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="os l oo op oq om or kp od"/></div></div></a></div></div></div>    
</body>
</html>