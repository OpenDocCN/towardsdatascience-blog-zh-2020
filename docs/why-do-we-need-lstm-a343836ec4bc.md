# 我们为什么需要 LSTM

> 原文：<https://towardsdatascience.com/why-do-we-need-lstm-a343836ec4bc?source=collection_archive---------33----------------------->

![](img/f38edb1eeefde644cf4faab960b558ca.png)

[https://images.unsplash.com/photo-1504639725590-](https://images.unsplash.com/photo-1504639725590-34d0984388bd?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1267&q=80)

## *深入分析消失梯度问题以及 LSTM 是如何解决的。*

在本帖中，我们将讨论渐变消失的问题，以及 LSTM 是如何解决的。

在深度学习中，会出现这样的情况，我们需要了解以前的数据，以便预测下一次的输出。例如，为了预测一个句子的下一个单词，我们应该知道它以前的单词，在这种情况下，我们通常的神经元不会有所帮助，因为它不知道以前的单词。RNN 在这种情况下发挥作用。RNN 是一种神经元细胞，具有保留序列信息的能力。RNN 的隐藏状态将信息从一个时间步骤传递到另一个时间步骤，因此记住了序列。

![](img/968fe32f8c3fc194cbbed7f4430a8c57.png)

RNN 的结构(作者图)

RNN 的主要优势在于它可以保留过去处理过的数据的信息。但是过去意味着多少时间步骤？为了回答这个问题，我们必须深入了解 RNN 细胞的功能。当一个输入 x 被传入 RNN 单元时，它将被乘以一个权重 W，并加上一个偏差 b。然后，计算值将被激活函数如 sigmoid、ReLu 等非线性化。

如果我们考虑 sigmoid 函数，对于一系列输入值，它将返回(0，1)范围内的输出。如果我们考虑时间步长为 4，sigmoid 的输出为 0.3。在第 4 个时间步长结束时，RNN 的输出 Y 将为 Y=0.3*0.3*0.3*0.3*x + b。

x 与一个非常小的数相乘，该数可以近似为零，这将导致反向传播过程中的梯度为零，因此权重不会得到优化。这个问题被称为消失梯度问题。

这将导致初始层没有被优化/训练。因为后面的层依赖于由初始层捕获的低级特征，所以模型的整体性能将会降低。如果激活函数是 ReLu，则在负输入的情况下，它将输出零，这将导致相同的问题，即使我们使用 Leaky-ReLu，负值的输出也太小，无法克服消失梯度问题。因此，改变激活函数不足以解决消失梯度问题。

使用 RNN 的主要原因是保留从以前的数据中获得的信息，但是如果我们面临梯度消失的问题，可以存储在内存中的数据量是非常有限的。

引入 LSTM 作为这个问题的解决方案。长短期记忆能够存储关于较长数据序列的处理过的信息。LSTM 如何解决渐变消失的问题？

![](img/260ccb5f461c893996b69fbaee088bcb.png)

LSTM 的结构(作者图)

在上图中，Ct 标注的水平线被称为细胞状态，LSTM 的记忆。它沿着所有的细胞携带信息，每个细胞根据它得到的输入，修改细胞状态中的信息。门用于修改单元状态。门是决定单元状态是否被修改的结构。例如，如果门产生 0，则没有新数据将被添加到单元状态，如果门产生 1，则全部数据将被添加到单元状态。改变单元状态中的数据只是将输入数据乘以门的输出，并将其添加到单元状态。

在 LSTM 单元中有三种类型的门，遗忘门、输入门和输出门。它们中的每一个将决定旧数据的哪一部分必须被忘记，新数据的哪一部分必须被记住，以及相应地存储器的哪一部分必须被给出。

现在我们将走过 LSTM。来自先前时间步长 Ct-1 的单元状态、来自先前时间步长 ht-1 的隐藏状态和输入特征被提供给 LSTM 单元。遗忘门将根据输入特征和隐藏状态给出一个输出。因为 sigmoid 将被用作激活函数，所以输出范围将从 0 到 1。遗忘门的输出将与单元状态相乘。如果输出为 0，乘以单元状态后，单元状态将为空。也就是说，该单元将会忘记到目前为止它已经学习的所有内容，并且如果输出为 1，那么该单元状态将保持不变。即该细胞将记住它所学习的一切。基于遗忘门的输出值，单元状态将改变。即被单元遗忘的信息量由遗忘门的输出决定，因此得名。这也解释了为什么我们使用 sigmoid 作为门的激活函数。由于它的输出在 0 和 1 之间变化，我们可以添加新的数据或一部分数据或不添加，这同样适用于遗忘。

![](img/5e9404d472faeded72622b10508ef1ac.png)

忘记门决定什么数据被忘记

现在，我们已经决定了需要对现有内存做什么，接下来我们必须决定新数据的哪一部分要存储在内存中。输入门在这里发挥作用，类似于遗忘门，输入门将被给予先前时间步长的隐藏状态和输入特征，基于这些，输入门将确定存储较新数据的哪一部分。然后，双曲正切函数创建一组新值，并存储到存储器中。新创建的值将与来自输入门的输出相乘，然后将被添加到单元状态。

![](img/b460c8eb3d97b85248ae5b589dfdace0.png)

输入门和 tanh 决定哪些数据将被添加到单元状态(作者的图表)

![](img/002b7df05f8f70cd8105d14092bc727f.png)

(作者图)

现在，来自输入特征的新的重要信息已经被存储到存储器中，接下来，我们必须决定来自存储器的数据的哪一部分应该作为输出给出。输出门和一个双曲正切函数扮演了这个角色。输出门将决定给出单元状态数据的哪一部分，tanh 函数在-1 到 1 的范围内转换数据。那么它将作为输出与单元状态一起提供给下一个单元。

![](img/f4d310e549d7110d7e3789588f31a92b.png)

决定输出数据的输出门(作者的图表)

这些是发生在 LSTM 细胞内的过程或步骤。单元状态或存储器像传送带一样传送重要信息，而门像网关一样允许新数据进入传送带或允许不必要的数据离开传送带。

那么，这个设置如何解决渐变消失的问题呢？这里，这些门控制着进入下一个时间步的数据量，同样，这些门也控制着权重的优化方式。如果梯度在反向传播期间消失，同样的事情也会发生在正向传播中。因此，该权重不会对预测产生影响，因此在反向传播期间不会对其进行优化。

虽然 LSTM 解决了消失梯度问题，但这让我们觉得 LSTM 很复杂。但就像深度学习中发生的所有其他奇迹一样，当足够靠近时，LSTM 也是在数学的帮助下玩的一个简单的把戏。