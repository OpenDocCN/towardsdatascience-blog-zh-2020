<html>
<head>
<title>Calculus Behind Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归背后的微积分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculus-behind-linear-regression-1396cfd0b4a9?source=collection_archive---------20-----------------------#2020-05-07">https://towardsdatascience.com/calculus-behind-linear-regression-1396cfd0b4a9?source=collection_archive---------20-----------------------#2020-05-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4757" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解线性回归的数学方面</h2></div><p id="0493" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归通常是任何机器学习课程的起点。目标是预测输入变量和目标变量之间的线性关系。</p><p id="1eb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最简单的例子是穿过空间原点的直线。在这里，我们被限制在二维空间，因此笛卡尔平面。让我们从头开始逐步开发，从<strong class="kh ir"> y=mx </strong>格式开始，然后是<strong class="kh ir"> y=mx+c </strong>回归。</p><h1 id="bced" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">y=mx的简化场景</h1><p id="073c" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在这种情况下，我们知道我们要对通过原点的数据拟合一条线。这是一个简单的例子，可能是一个很好的起点。让我们发展我们的损失函数，看看它如何表现。让我们把我们的估计改写如下。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/bd4556e61e467d5ea7821c90a2ecdcb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3y0Xt7vFAPZr-GupoaOcQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><strong class="bd mo"> h(x) </strong>是估计函数<strong class="bd mo"> J(θ) </strong>是损失函数</p></figure><p id="e4f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我们使用平方误差除以<strong class="kh ir"> 2m </strong>，其中<strong class="kh ir"> m </strong>是我们拥有的数据点的数量。因此，我们可以认为这是一个平均值。准确地说，这是均方误差的一半。当我们得到损失函数的导数时，除以2背后的直觉将是可见的。这有助于我们得到一个更简单的损失函数的导数。让我们考虑以下几点。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mp"><img src="../Images/a819193e2e98d47ad520bb30fcbf780d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljzHE3yGa47-8h-ReHHjfw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">2D平面上的点集</p></figure><p id="a25c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我使用添加了噪声的<strong class="kh ir"> y=5x </strong>图生成了这些点。因此，理想情况下，我们对<strong class="kh ir"> θ </strong>的估计值应该更接近<strong class="kh ir"> 5 </strong>的值。现在，如果我们绘制不同<strong class="kh ir"> θ </strong>值的损失函数，我们将得到如下结果。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mp"><img src="../Images/3f7ee191830e7ba6ccabe81045c422be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LVmJGQJ8AfshzN0TiQqmbg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">损失函数</p></figure><p id="27b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我们有一个更接近<strong class="kh ir"> θ=5 </strong>的损失函数的最小值。让我们看看如何通过计算得出这个特定的最小值。</p><p id="3a3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意橙色的切线和它的梯度。它是一个正的梯度，这给了我们一个想法，我们必须向相反的方向去寻找一个更低的值。</p><h1 id="3fa9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">梯度下降</h1><p id="38c0" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">梯度下降的概念，正如通常解释的那样，是仅使用周围斜坡的知识在未知的地形中穿过下坡。因此，在本例中，目标是使用坡度找到穿过的<strong class="kh ir">方向，并决定移动的<strong class="kh ir">步长</strong>。</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mq"><img src="../Images/2170e7b80bdbf267e34a65feffc20f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCSiPGDY7Ap6geld5Q2PvQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">寻找遍历方向</p></figure><p id="efe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很明显，我们必须向左走。步长呢？。这就是学习率发挥作用的地方。这被称为<strong class="kh ir"> ⍺ </strong>。步长越小，速度越慢。然而，迈出更大的一步可能会让你错过最小值(想象你从θ= 7°步进到θ= 1°，没有意义)。所以我们可以把<strong class="kh ir"> θ </strong>的变化公式化为<strong class="kh ir">。</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mr"><img src="../Images/134a31a41f6aafe37fb9a1c02f202e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lkjfi228txRyJyaCf9zVw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">θ计算的下一个值和损失函数的导数</p></figure><p id="606b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于这种线性回归的情况是独立于数据的，这可以用作计算损失和进行梯度下降的标准方程。这应该强调这样一个事实，即我们总是坚持标准损失函数，而没有建立自己的损失函数。用python实现这一点非常简单。我们用<strong class="kh ir"> len(x) </strong>表示<strong class="kh ir"> m </strong>，用<strong class="kh ir"> 0.001 </strong>表示<strong class="kh ir"> ⍺.x </strong>和<strong class="kh ir"> y </strong>是训练数据集或我们要估计的点集。</p><pre class="lz ma mb mc gt ms mt mu mv aw mw bi"><span id="ed46" class="mx lc iq mt b gy my mz l na nb">#Loss function</span><span id="cc09" class="mx lc iq mt b gy nc mz l na nb">(1/2* len(x)) * sum([(theta* x[i] - y[i])**2 for i in range(len(x))])</span><span id="956c" class="mx lc iq mt b gy nc mz l na nb">#Change of theta</span><span id="7822" class="mx lc iq mt b gy nc mz l na nb">theta - 0.0001 * (1/len(x)) * sum([x[i]*(theta*x[i] - y[i]) for i in range(len(x))])</span></pre><p id="6392" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以迭代地进行梯度下降，直到我们达到最小值或者直到损失函数低于某个阈值。我们也可以决定迭代的次数。因为我们保证在线性回归中有一个最小值，所以我们可以如下实现descend。</p><pre class="lz ma mb mc gt ms mt mu mv aw mw bi"><span id="241a" class="mx lc iq mt b gy my mz l na nb">loss = float('inf')<br/>theta = 8<br/><br/>while True:<br/>    theta = descend(theta)<br/>    <br/>    if loss &gt; loss_function(theta, x, y):<br/>        loss = loss_function(theta, x, y)<br/>    else:<br/>        break</span></pre><p id="8817" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">下降</strong>函数返回下一个<strong class="kh ir"> θ </strong>值，而<strong class="kh ir">损失_函数</strong>返回<strong class="kh ir"> θ </strong>值处的损失。我们可以将梯度遍历可视化如下。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nd"><img src="../Images/91674b5acf3520e00377fe7edb29dc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SS5f3qur9SDG_mv_k0seqQ.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">随着损失函数的下降</p></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nd"><img src="../Images/185245d1cb395bf6e648a3af5ce664f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*JcZbUmW0RG-sO88Qj3Tflw.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">梯度下降中每次迭代的估计变化</p></figure><h1 id="002c" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">完整场景y=mx+c</h1><p id="8617" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">这是前一种情况的扩展，我们可以对估计方程和损失函数建模如下。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ne"><img src="../Images/4ff2b58ab6f05bca2fc15c907b49d3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Si27xurIYgUZdVz4cikczQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><strong class="bd mo"> h(x) </strong>是估计函数<strong class="bd mo"> J(θ1，θ2) </strong>是损失函数</p></figure><p id="dc24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是我将尝试拟合的一组点。我使用函数<strong class="kh ir"> y=5 + 3x </strong>生成了这个图，带有一点噪声。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mp"><img src="../Images/12945204cb7f4875ff621ef2f14130f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJgz8SPKCs9Fsyo7uH7_mw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><strong class="bd mo"> y=5 + 3x </strong>带噪声</p></figure><p id="21c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了一个包含两个变量的损失函数，我们的损失函数将是一个3D图，第三个轴对应于损失值。用图解法，我们可以说明如下。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nf"><img src="../Images/3483a6a7c8e3479b5c5f12c1727ae0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPYeup4sd1x96jDi4ccQig.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">损失函数的变化。圆圈中间是最小损失点。</p></figure><p id="9a47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的梯度下降可以被类似地导出以达到下面的方程组。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ng"><img src="../Images/20251eb8e64280bb098e1b55ebba12bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trtuEAnFKnMK1MqpU1LXAw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">梯度下降函数的求导</p></figure><p id="b86f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，梯度必须同时更新，以便一个<strong class="kh ir"> θ </strong>值的更新不会影响另一个。在python中，该操作如下所示。</p><pre class="lz ma mb mc gt ms mt mu mv aw mw bi"><span id="4b8e" class="mx lc iq mt b gy my mz l na nb">theta1_new = theta1 - 0.01 * (1/len(x)) * sum([theta1 + theta2*x[i] - y[i] for i in range(len(x))])</span><span id="f6f0" class="mx lc iq mt b gy nc mz l na nb">theta2_new = theta2 - 0.01 * (1/len(x)) * sum([x[i]*(theta1 + theta2*x[i] - y[i]) for i in range(len(x))])</span><span id="a5ed" class="mx lc iq mt b gy nc mz l na nb">theta1, theta2 = theta1_new, theta2_new</span></pre><p id="f260" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失函数如下。对我们之前场景的唯一更新是，现在我们有了第二个变量<strong class="kh ir"> θ2 </strong>。</p><pre class="lz ma mb mc gt ms mt mu mv aw mw bi"><span id="6e45" class="mx lc iq mt b gy my mz l na nb">(1/2* len(x)) * sum([(theta1 + theta2* x[i] - y[i])**2 for i in range(len(x))])</span></pre><p id="e8b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我通过梯度下降更新了我的迭代，如下所示。除了前面检查下降损失的场景之外，我还考虑了下降损失的数量，以确保我不会迭代到非常接近最小值的点。</p><pre class="lz ma mb mc gt ms mt mu mv aw mw bi"><span id="021f" class="mx lc iq mt b gy my mz l na nb">loss = float('inf')<br/>theta1 = 10<br/>theta2 = -1<br/><br/>while True:<br/>    theta1, theta2 = descend(theta1, theta2)<br/>    <br/>    if loss &gt; loss_function(theta1, theta2, x, y) and \<br/>            abs(loss - loss_function(theta1, theta2, x, y)) &gt; 0.001:<br/>        loss = loss_function(theta1, theta2, x, y)<br/>    else:<br/>        break</span></pre><p id="1ea9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以将损失函数在梯度上的下降形象化如下。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nf"><img src="../Images/cf90e949c5031620bd9511847a2ad97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*p5mJgzPIvivtnyHM69G4vQ.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">随着损失地形移动</p></figure><p id="aa9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们的θ值变化时，我们的回归拟合与表上的值非常吻合。它看起来像下面这样。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nd"><img src="../Images/fa58fc6286bfe6bd270bc8b613c143dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qJ8SeENNNkXgdH2-Ewb9nw.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">梯度下降中每次迭代的估计变化</p></figure><p id="c4d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在最终拟合中分别得到截距和梯度的值<strong class="kh ir"> 6.5 </strong>和<strong class="kh ir"> 2.7 </strong>，考虑到数据中的噪声，这是合理的。</p><p id="fe9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢阅读。干杯！</p></div></div>    
</body>
</html>