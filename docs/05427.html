<html>
<head>
<title>A Tale of Model Quantization in TF Lite</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF Lite中模型量化的故事</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca?source=collection_archive---------29-----------------------#2020-05-07">https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca?source=collection_archive---------29-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9225" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/production-ml" rel="noopener" target="_blank">生产中的机器学习</a></h2><div class=""/><div class=""><h2 id="7014" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">模型优化策略和量化技术，帮助在资源受限的环境中部署机器学习模型。</h2></div><p id="67fb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在此与结果仪表板<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">互动。</a></p><p id="be0f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最新的机器学习模型通常体积庞大，这使得它们在资源受限的环境中部署时效率低下，如移动电话、树莓pi、微控制器等。即使您认为您可以通过在云上托管您的模型并使用API来提供结果来解决这个问题，也要考虑到互联网带宽可能并不总是很高，或者数据不能离开特定设备的受限环境。</p><p id="2c0a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们需要一套工具来无缝过渡到设备上的机器学习。在这篇报道中，我将向你展示<strong class="kt jd"> TensorFlow Lite (TF Lite) </strong>如何在这种情况下大放异彩。我们将讨论TensorFlow支持的模型优化策略和量化技术。</p><p id="7629" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ln" href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite" rel="noopener ugc nofollow" target="_blank">查看GitHub上的代码</a> →</p><p id="f308" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢<strong class="kt jd"> Arun </strong>、<strong class="kt jd"> Khanh </strong>和<strong class="kt jd"> Pulkit </strong>(谷歌)为这篇报道分享了非常有用的技巧。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/b3c5ad3c9b34526f580f150857ab991d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2PlqL4-F5LA3mo3AygT4g.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">不同量化实验之间的性能概述(此处<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">可用</a></p></figure><h2 id="0d96" class="me mf it bd mg mh mi dn mj mk ml dp mm la mn mo mp le mq mr ms li mt mu mv iz bi translated">概观</h2><p id="32c8" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">在本文中，我们将涵盖以下主题</p><ul class=""><li id="3b83" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">对设备上机器学习的需求</li><li id="7659" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">TensorFlow支持的模型优化策略</li><li id="e3bf" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">量化技术</li><li id="51bc" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">执行量化时需要记住的事项</li></ul><h1 id="839f" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">对设备上机器学习的需求</h1><p id="87d0" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">在他们的演讲<a class="ae ln" href="https://www.youtube.com/watch?v=27Zx-4GOQA8" rel="noopener ugc nofollow" target="_blank">tensor flow Lite:ML for mobile and IoT devices(TF Dev Summit ' 20)</a>中，Tim Davis和T.J. Alumbaugh强调了以下几点:</p><ul class=""><li id="8262" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd">更低的延迟&amp;紧密互动</strong>:在许多关键应用中，你可能希望预测的延迟为零，例如无人驾驶汽车。您可能还需要保持系统的所有内部交互非常紧凑，这样就不会引入额外的延迟。</li><li id="abc5" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">网络连接</strong>:正如我之前提到的，当你依赖云托管模式时，你实际上是将你的应用程序限制在一定的网络带宽水平上，而这可能并不总是能够实现的。</li><li id="ab09" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">隐私保护</strong>:对隐私有硬性要求，例如，数据不能离开设备。</li></ul><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="76e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了使大型ML模型能够部署在小型设备上，我们需要对它们进行优化，例如，将1.9GB的模型放入2GB的应用程序中。为了帮助ML开发者和移动应用开发者，TensorFlow团队提出了两个解决方案:</p><ul class=""><li id="66ff" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><a class="ae ln" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a></li><li id="2b41" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/" rel="noopener ugc nofollow" target="_blank"> TensorFlow模型优化工具包</a></li></ul><h1 id="aeb3" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">TensorFlow支持的模型优化策略</h1><p id="183f" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">通过TensorFlow Lite和模型优化工具包，TensorFlow目前支持以下模型优化策略-</p><ul class=""><li id="80dd" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd">量化</strong>你可以使用不同的低精度格式来减小模型的大小。</li><li id="f2eb" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">修剪</strong>在这里，您可以丢弃模型中对模型预测意义不大的参数。</li></ul><p id="085c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将重点讨论量子化。</p><h1 id="8df4" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">量化技术</h1><p id="52d5" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">通常，我们的机器学习模型以<code class="fe oc od oe of b">float32</code>精度格式运行。所有模型参数都以这种精度格式存储，这通常会导致模型更重。模型的重量与模型进行预测的速度直接相关。因此，你可能会自然而然地想到，如果我们能降低模型运行的精度，我们就能减少预测时间。这就是量化的作用——它将精度降低到更低的形式，如float16、int8等，来表示模型的参数。</p><p id="0049" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">量子化可以以两种方式应用于一个模型</p><ul class=""><li id="475d" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd">训练后量化</strong>在训练后应用于模型<em class="og">。</em></li><li id="0197" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">量化感知训练</strong>通常对模型进行训练，以补偿量化可能带来的精度损失。当您降低模型参数的精度时，可能会导致信息丢失，您可能会看到模型的精度有所降低。在这些情况下，感知量化的训练会非常有帮助。</li></ul><p id="6ae9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将在这份报告中看到这两种味道。我们开始吧！</p><h1 id="fe98" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">实验设置</h1><p id="1171" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">我们在这份报告中做的所有实验都是在<a class="ae ln" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Colab </a>上进行的。我使用<strong class="kt jd"> flowers </strong>数据集进行实验，并微调了一个预先训练好的MobileNetV2网络。这是定义网络架构的代码-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oh ob l"/></div></figure><p id="991a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对网络进行10个时期的训练，批次大小为32。</p><h1 id="fe09" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">正常微调下的性能</h1><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oi"><img src="../Images/4b583aebcecaad439e238feeb88acd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txeFnAnLRDPoEufNL8D-0A.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">运行页面可用<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization/" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oj"><img src="../Images/3c8c55959d7180ea6d80efd78b512ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MbwkdN7_2hUqXA2z.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">所有这些文件都可以在<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization/runs/normal-training-fine-crctd/files?workspace=user-sayakpaul" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></figure><p id="6659" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们看到网络运行良好，达到35.6 MB。</p><h1 id="c70d" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">量化微调后的模型</h1><p id="122f" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">当你在<code class="fe oc od oe of b">tf.keras</code>中训练好一个模型后，量化部分只是几行代码的事情。所以，你可以这样做-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oh ob l"/></div></figure><p id="8265" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先将您的模型加载到一个<code class="fe oc od oe of b">TFLiteConverter</code>转换器类中，然后指定一个优化策略，最后，您要求TFLite使用优化策略转换您的模型。序列化转换后的TF Lite文件非常简单</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oh ob l"/></div></figure><p id="f08b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种量化形式也被称为<strong class="kt jd">训练后动态范围量化</strong>。它将模型的<em class="og">权重</em>量化到8位精度。<a class="ae ln" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到关于这个和其他训练后量化方案的更多细节。</p><h1 id="a861" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">关于为转换设置配置选项的说明</h1><p id="54b8" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">TF Lite允许我们在转换模型时指定许多不同的配置。我们在前面提到的代码中看到过一次这样的配置，其中我们指定了优化策略。</p><p id="9c12" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">除了<code class="fe oc od oe of b">tf.lite.Optimize.DEFAULT</code>，还有另外两个政策可供选择- <code class="fe oc od oe of b">tf.lite.Optimize.OPTIMIZE_FOR_SIZE</code> &amp; <code class="fe oc od oe of b">tf.lite.Optimize.OPTIMIZE_FOR_LATENCY</code>。从名称中可以看出，基于策略的选择，TF Lite会相应地尝试优化模型。</p><p id="5cf3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以指定其他事情，比如-</p><ul class=""><li id="914f" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><code class="fe oc od oe of b">target_spec</code></li><li id="516a" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><code class="fe oc od oe of b">representative_dataset</code></li></ul><p id="a9b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">点击了解更多关于<code class="fe oc od oe of b">TFLiteConverter</code>类<a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter" rel="noopener ugc nofollow" target="_blank">的信息。值得注意的是，这些不同的配置选项允许我们在模型的预测速度和准确性之间保持平衡。</a><a class="ae ln" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">在这里</a>，你可以找到TF Lite中不同训练后量化方案的一些权衡。</p><p id="b404" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面我们可以看到这个转换模型的一些有用的统计数据。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ok"><img src="../Images/7f43223849333895ca2879740c22be2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2tVYhoNVgCrURTO5KzKiA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">餐桌可用<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><p id="0af0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们看到模型的规模大幅缩小，但这是以准确性为代价的。理想情况下，我们不希望转换后的模型精度损失这么大。这表明，我们需要探索其他量化方案，以进一步提高转换模型的准确性。</p><h1 id="b4ea" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">具有相同模型的量化感知训练(QAT)</h1><p id="ede7" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">这里一个好的第一个方法是训练你的模型，让它学会补偿可能由量化引起的信息损失。通过量子化感知训练，我们可以做到这一点。为了以量化感知的方式训练我们的网络，我们只需添加以下代码行-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oh ob l"/></div></figure><p id="1765" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，你可以像训练一个<code class="fe oc od oe of b">tf.keras</code>模型一样训练<code class="fe oc od oe of b">qat_model</code>。<a class="ae ln" href="https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到QAT的全面报道。</p><p id="6f96" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面，我们可以看到这个量化感知模型比我们之前的模型稍好一些。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ol"><img src="../Images/62dfc60474e0369594eb7e3dc1f68924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdSJG3Dnx2V2pkcFDBakKw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">运行可用的<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">这里</a></p></figure><h1 id="e056" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">QAT与非QAT模型的简要比较</h1><p id="dd01" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">就模型大小而言，QAT模型与非QAT模型相似:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi om"><img src="../Images/41ad3c8404cd147ecd3ecdc34ca918d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6bnggVanLTcmj15J.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">请记住，这些文件可以在任何运行的“文件”选项卡下找到</p></figure><p id="e06c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是在模型训练时间方面，我们看到QAT模型需要更多的时间。这是因为在QAT过程中，模型中引入了伪量化节点来补偿信息损失，这使得QAT模型需要更多的时间来收敛。</p><p id="866f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在优化收敛时间的情况下，记住这一点很重要。如果你的训练模型需要很长的训练时间，那么引入QAT会进一步增加这个时间。</p><p id="5368" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">量化QAT模型与我们在上一节中看到的完全相同(我们将使用相同的量化配置)。</p><p id="fc86" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们比较量化版本的QAT模型的性能。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d3b5b095839473fe7793dca86e90f7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*50LbTrfTerxWZnMAfYb7pw.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">模型训练时间的比较</p></figure><h1 id="ce57" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">评估量化的QAT模型</h1><p id="220d" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">在下表中，我们看到量化版本的QAT模型确实比以前的模型表现得更好。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oo"><img src="../Images/6c0f202cd4dde9e42fe9e2692651bfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpPsswsvmK5f6yT4YV56Tw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">此处可用的表<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">为</a></p></figure><p id="7583" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们清楚地看到，用QAT训练的模型没有引起任何精度下降。在下一节中，我们将把两个模型的参数都保持为浮点数，看看我们能在多大程度上推动模型大小和准确性之间的权衡。</p><h1 id="a854" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">量化为浮动模型</h1><p id="01a9" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">为了将我们的模型量化到浮点精度，我们只需要丢弃这条线— <code class="fe oc od oe of b">converter.optimizations = [tf.lite.Optimize.DEFAULT]</code>。如果你想利用<a class="ae ln" href="https://www.tensorflow.org/lite/performance/gpu" rel="noopener ugc nofollow" target="_blank"> GPU代理</a>，这个策略尤其有用。注意，<a class="ae ln" href="https://www.tensorflow.org/lite/performance/post_training_float16_quant" rel="noopener ugc nofollow" target="_blank"> float16量化</a>在TensorFlow Lite中也是支持的。在下表中，我们可以看到使用该方案量化的模型的大小和精度。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi op"><img src="../Images/6ece48e04e328914b78c83d0ba0f4ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YIRpPJIdQFVa-PRHM0NFog.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">此处可用的表<a class="ae ln" href="https://app.wandb.ai/sayakpaul/tale-of-quantization" rel="noopener ugc nofollow" target="_blank">为</a></p></figure><p id="0891" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然这些型号的尺寸有所增加，但我们看到这些型号的原始性能仍然很高。注意，不建议使用这种方案转换QAT模型，因为在QAT期间，插入的伪量化运算具有<code class="fe oc od oe of b">int</code>精度。因此，当我们使用这种方案量化QAT模型时，转换后的模型可能会出现不一致。</p><p id="2e8d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，硬件加速器，如边缘TPU USB加速器，将不支持浮动模型。</p><h1 id="3c99" class="np mf it bd mg nq nr ns mj nt nu nv mm ki nw kj mp kl nx km ms ko ny kp mv nz bi translated">探索其他量化方案和总结思路</h1><p id="cd52" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">还有其他训练后量化技术可用，如全整数量化、浮点量化等。这是你可以了解他们更多的地方。请记住，全整数量化方案可能并不总是与QAT模型兼容。</p><p id="05f2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有许多SoTA预训练的TF Lite模型可供开发人员在其应用中使用，它们可以在以下位置找到:</p><ul class=""><li id="7d9d" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><a class="ae ln" href="https://www.tensorflow.org/lite/guide/hosted_models" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/lite/guide/hosted_models</a></li><li id="0b4f" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><a class="ae ln" href="https://tfhub.dev/s?deployment-format=lite&amp;publisher=tensorflow&amp;q=lite" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/s?deployment-format=lite&amp;publisher = tensor flow&amp;q = lite</a></li></ul><p id="0cb4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于希望在应用程序中集成机器学习的移动开发人员来说，<a class="ae ln" href="https://www.tensorflow.org/lite/examples" rel="noopener ugc nofollow" target="_blank">TF Lite中有许多示例应用程序</a>值得一试。TensorFlow Lite还为嵌入式系统和微控制器提供工具，您可以从<a class="ae ln" href="https://www.tensorflow.org/lite/guide" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多信息。</p><p id="4b11" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果您想重现此分析的结果，您可以–</p><p id="641a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ln" href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite" rel="noopener ugc nofollow" target="_blank">查看GitHub上的代码→ </a></p></div></div>    
</body>
</html>