<html>
<head>
<title>Understanding the LightGBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解光 GBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-lightgbm-772ca08aabfa?source=collection_archive---------27-----------------------#2020-09-15">https://towardsdatascience.com/understanding-the-lightgbm-772ca08aabfa?source=collection_archive---------27-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f9bd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">是什么让它更快更高效</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/15addbf4c78386625f42956e20fd497c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S881GIDqcDMOnIFA9aJR0Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@aycai?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">艾伦蔡</a>在<a class="ae ky" href="https://unsplash.com/s/photos/light?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cf47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由微软研究人员创建的 LightGBM 是梯度增强决策树(GBDT)的一种实现，这是一种以串行方式(增强)组合决策树(作为弱学习器)的集成方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/2fe3e38b8df9bab06132b5a87bc7ed14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KMrB4kegUF4Sf3Ih.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度增强决策树</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="b89d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树以这样一种方式组合，即每个新的学习者拟合来自前一个树的残差，从而改进模型。最终的模型汇总了每一步的结果，从而形成了一个强学习者。</p><p id="6d8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GBDT 是如此准确，以至于它的实现一直主导着主要的机器学习竞赛。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="3c8b" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">灯光背后的动机 GBM </strong></h1><p id="5eb7" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">决策树是通过基于特征值拆分观察值(即数据实例)来构建的。决策树就是这样“学习”的。该算法寻找导致最高信息增益的最佳分割。</p><p id="0727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">信息增益</strong>基本就是分裂前后的熵差。熵是不确定性或随机性的度量。一个变量的随机性越大，熵就越大。因此，以减少随机性的方式进行分割。</p><p id="f0e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">寻找最佳分割是决策树学习过程中最耗时的部分。其他 GBDT 实现找到最佳分割的两种算法是:</p><ul class=""><li id="23e2" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">预先排序:特征值预先排序，并评估所有可能的分割点。</li><li id="82c3" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">基于直方图:连续特征被分成离散的箱，用于创建特征直方图。</li></ul><p id="3e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“Sklearn GBDT”和“gbm in R”使用预排序算法，而“pGRT”使用基于直方图的算法。“xgboost”两者都支持。</p><p id="a6c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于直方图的算法在内存消耗和训练速度方面更有效。但是，随着实例或特征数量的增加，预排序和基于直方图的速度都会变慢。LightGBM 旨在解决这个效率问题，尤其是对于大型数据集。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="736f" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">是什么让 LightGBM 更加高效</strong></h1><p id="26e0" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">LightGBM 的起点是基于直方图的算法，因为它比预先排序的算法性能更好。</p><p id="8328" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个特征，扫描所有数据实例以找到关于信息增益的最佳分割。因此，基于直方图的算法的复杂性取决于数据实例和特征的数量。</p><p id="328f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，LightGBM 使用了两种技术:</p><ul class=""><li id="287e" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">梯度单侧采样</li><li id="536b" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">EFB(独家功能捆绑)</li></ul><p id="70fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面就来详细说说这些技术是做什么的，是如何让 LightGBM“轻”起来的。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="e0e7" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">高斯(梯度单侧采样)</strong></h1><p id="24a7" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们已经提到，一般的 GBDT 实现会扫描所有数据实例，以找到最佳分割。这绝对不是一个最优的方式。</p><p id="db49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们能够根据信息增益对数据进行采样，算法将会更加有效。一种方法是根据权重对数据进行采样。但是，它不适用于 GBDT，因为在 GBDT 没有样品重量。</p><p id="ff89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS 通过使用梯度来解决这个问题，梯度给了我们对信息增益的有价值的洞察力。</p><ul class=""><li id="9efc" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">小梯度:算法已经在这个实例上被训练，并且与它相关的误差很小。</li><li id="2d15" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">大梯度:与该实例相关的误差很大，因此它将提供更多的信息增益。</li></ul><p id="a302" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以排除梯度小的实例，只关注梯度大的实例。但是，在这种情况下，数据分布将会改变。我们不希望这样，因为这将对学习模型的准确性产生负面影响。</p><p id="0176" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS 提供了一种基于梯度的数据采样方法，同时考虑了数据分布。</p><p id="1c48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它是这样工作的:</p><ol class=""><li id="a498" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu no ng nh ni bi translated">数据实例根据其梯度的绝对值进行排序</li><li id="bc54" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu no ng nh ni bi translated">选择了前 ax100%个实例</li><li id="b851" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu no ng nh ni bi translated">从剩余的实例中，选择大小为 bx100%的随机样本</li><li id="adbe" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu no ng nh ni bi translated">当计算信息增益时，小梯度的随机样本乘以等于(1-a) / b 的常数</li></ol><p id="3419" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS 最终实现的是，模型的重点倾向于导致更多损失(即训练不足)的数据实例，而不会对数据分布产生太大影响。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="98ab" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak"> EFB(独家功能捆绑)</strong></h1><p id="7713" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">具有大量要素的数据集可能具有稀疏要素(即大量零值)。这些稀疏特征通常是互斥的，这意味着它们不会同时具有非零值。考虑独热编码文本数据的情况。在特定的行中，只有一列指示特定的单词是非零的，所有其他行都是零。</p><p id="f443" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EFB 是一种使用贪婪算法将这些互斥组合(或捆绑)成单个特征(例如，互斥特征束)并因此降低维度的技术。EFB 减少了 GDBT 的训练时间，而不太影响准确性，因为创建特征直方图的复杂性现在与束的数量而不是特征的数量成比例(束的数量远小于特征的数量)。</p><p id="9f41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EFB 面临的挑战之一是找到最佳捆绑包。微软的研究人员设计了一种算法，将捆绑问题转化为图形着色问题。</p><p id="2bc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图着色问题中，将特征作为顶点，在不互斥的特征之间添加边。然后，使用贪婪算法来产生束。</p><p id="e47d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更进一步，该算法还允许捆绑很少同时具有非零值(即几乎互斥)的特征。</p><p id="920b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个挑战是以能够提取原始特征的值的方式将特征合并成束。考虑一组 3 个特性。我们需要能够使用捆绑特性的价值来确定这 3 个特性的价值。</p><p id="26bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下，基于直方图的算法为连续值创建离散的仓。为了克服合并特征的挑战，将束中特征的唯一值放入不同的箱中，这可以通过向原始特征值添加偏移来实现。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="4f0c" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">结论</strong></h1><p id="a5b1" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">LightGBM 背后的动机是解决在处理大型数据集时与 GBDTs 的传统实现相关的训练速度和内存消耗问题。</p><p id="4fc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标基本上是在尽可能保留信息的同时减少数据的大小(包括数据实例和特征)。戈斯和 EFB 技术是实现这一目标。</p><p id="86d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据 LightGBM 创造者的<a class="ae ky" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，“LightGBM 将传统 GBDT 的训练过程加快了 20 倍以上，同时达到了几乎相同的精度”。</p><p id="8412" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="7a12" class="md me it bd mf mg np mi mj mk nq mm mn jz nr ka mp kc ns kd mr kf nt kg mt mu bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="0104" class="na nb it lb b lc mv lf mw li nu lm nv lq nw lu nf ng nh ni bi translated">LightGBM:一个高效的梯度推进决策树</li></ul></div></div>    
</body>
</html>