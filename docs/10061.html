<html>
<head>
<title>Beyond Weisfeiler-Lehman</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越魏斯费勒-雷曼</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-weisfeiler-lehman-bc1f546d5164?source=collection_archive---------53-----------------------#2020-07-15">https://towardsdatascience.com/beyond-weisfeiler-lehman-bc1f546d5164?source=collection_archive---------53-----------------------#2020-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/7cc04fc12286b7cb41f6cc187a53b6eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2I4vS0IaOodJNUxjwP-B1Q.png"/></div></figure><h2 id="3ad5" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">超越 Weisfeiler-Lehman:近似同构和度量嵌入</h2><p id="e5bd" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由<a class="lm ln ep" href="https://medium.com/u/7b1129ddd572?source=post_page-----bc1f546d5164--------------------------------" rel="noopener" target="_blank">迈克尔布朗斯坦</a> — 6 分钟阅读</p><p id="bdc8" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">在这篇文章中，我认为图同构的设置对于分析图神经网络的表达能力来说太有限了，并建议基于度量嵌入的更广泛的设置。这是关于图形神经网络表达能力系列文章的第三篇。</p><figure class="lt lu lv lw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi jn"><img src="../Images/3b152a15bd087b2b3d54bd219607e7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEj8Y3sBf9kYxURtOdBYlA.jpeg"/></div></div></figure><h2 id="9fef" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/why-neural-nets-can-approximate-any-function-a878768502f0">为什么神经网络可以逼近任何函数</a></h2><p id="97e4" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">托马斯·汐宫光·克拉克——8 分钟阅读</p><p id="570c" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">在本文中，我将解释通用逼近定理，并用 PyTorch 代码展示两个快速示例，以演示神经网络学习逼近函数。如果你已经知道神经网络如何工作的基础知识，请随意直接跳到代码和可视化！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="lt lu lv lw gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/88d51db7119d75cbab96891ee1766c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTwMKSZGXY06t6jUxYqTXg.jpeg"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">图片来源:Unsplash</p></figure><h2 id="6cdc" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/intro-to-probabilistic-programming-b47c4e926ec5">概率编程简介</a></h2><p id="f32f" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由法比亚娜·克莱门特 — 6 分钟读完</p><p id="a995" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">概率编程背后的思想是将来自统计的推理算法和理论与来自编程语言的形式语义、编译器和其他工具结合起来，为来自机器学习的模型和应用程序构建高效的推理评估器。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="lt lu lv lw gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/fb06189bd6e1e622eb97a0f61f9d44ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnXA1HZXGBq647HiiMEsPA.png"/></div></figure><h2 id="2fae" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/a-comprehensive-survey-on-deep-learning-for-anomaly-detection-b1989b09ae38">深度学习异常检测:综述</a></h2><p id="daf0" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由<a class="lm ln ep" href="https://medium.com/u/1e6e5ed9fb39?source=post_page-----bc1f546d5164--------------------------------" rel="noopener" target="_blank">关松庞</a> — 7 分钟读完</p><p id="f953" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">异常检测，也称为异常值检测，几十年来一直是一个活跃的研究领域，因为它在许多关键领域都有广泛的应用，如风险管理、合规性、安全、金融监控、健康和医疗风险以及人工智能安全。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="lt lu lv lw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mm"><img src="../Images/8a981db58f21ba6c3ce3eaa1ba222b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cibWt5Nfq_COjO32"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">美国宇航局在 Unsplash 上拍摄的照片</p></figure><h2 id="00f8" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/a-beginners-guide-to-segmentation-in-satellite-images-9c00d2028d52">卫星图像分割初学者指南</a></h2><p id="c12d" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">汉娜·彼得森 — 15 分钟阅读</p><p id="406c" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">与根据标签对整个图像进行分类的图像分类相反，图像分割包括检测和分类图像中的单个对象。此外，分割不同于对象检测，因为它在像素级工作以确定图像中对象的轮廓。</p></div></div>    
</body>
</html>