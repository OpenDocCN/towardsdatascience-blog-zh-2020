<html>
<head>
<title>Why data normalization is important for non-linear classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么数据标准化对非线性分类器很重要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-data-normalization-is-important-for-svm-classifiers-49ca0d8e4930?source=collection_archive---------10-----------------------#2020-03-21">https://towardsdatascience.com/why-data-normalization-is-important-for-svm-classifiers-49ca0d8e4930?source=collection_archive---------10-----------------------#2020-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1601" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数据归一化对线性和非线性SVM分类器精度性能的影响</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c9084a09813e9373e1b217a42d3a3a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MN9uj41UW1ZxVAn1Nfv7pA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/photo/centimeters-length-measure-measurement-47040/" rel="noopener ugc nofollow" target="_blank">像素</a>上的<a class="ae ky" href="https://www.pexels.com/@pixabay" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="405d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">术语“<a class="ae ky" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">标准化</a>”通常指术语标准化和规模化。虽然<a class="ae ky" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank">标准化</a>通常旨在重新调整数据，使其均值为0，标准差为1，但<a class="ae ky" href="https://en.wikipedia.org/wiki/Scaling_(geometry)" rel="noopener ugc nofollow" target="_blank">调整</a>侧重于改变数据集值的<strong class="lb iu">范围</strong>。</p><p id="ad13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如在[1]和许多其他文章中提到的，当特征具有不同的范围时，需要数据标准化。例如，当我们有体重和身高特征时，归一化是很重要的，因为它们的取值范围有不同的尺度:例如，体重为[~ 45–130Kg]而身高为[~ 120–230cm]。</p><p id="8c46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，<strong class="lb iu">许多文章没有提到数据标准化也很重要的其他应用</strong>。通常，在这些应用中，由于所有特征的相似尺度或每个特征的相对信息，数据标准化可能不那么明显或容易实现。频域中的时间序列就是一个例子，因为，首先，原则上频率幅度的尺度是相同的，其次，因为每个频率(特征)的归一化可能导致相对信息的丢失。</p><p id="25ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，在什么情况下数据规范化是必要的呢？为了回答这个问题，<strong class="lb iu">本文展示了一个说明性的例子，该例子比较了线性分类器(具有线性核的SVM分类器)和非线性分类器(具有RBF核的SVM分类器)在实施数据归一化之前和之后的性能</strong>。</p><p id="26e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如将要观察到的，结果表明实现数据归一化不会影响线性分类器的精度，但是它会显著影响非线性分类器的精度。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="b445" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">文章结构:</strong></p><p id="7a54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文首先介绍Python代码，并简要描述它的用途。然后，给出了解释为什么数据的规模影响非线性分类器的数学方程。最后，它会根据数据集的规模显示超参数调整后非线性分类器的新结果。</p><p id="3c41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对SVM量词感兴趣，强烈推荐阅读:</p><blockquote class="mc md me"><p id="e339" class="kz la mf lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated">Drew Wilimitis，<a class="ae ky" rel="noopener" target="_blank" href="/the-kernel-trick-c98cdbcaeb3f">支持向量机中的内核绝招</a> (2018)，中</p></blockquote><p id="30e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于SVM分类器的进一步信息在本文的结尾有所说明(参考文献。[3]、[4]和[5])。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">导入Python库</strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">导入Python库</p></figure><p id="3380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">建立数据集</strong>:</p><p id="087c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集是使用<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html" rel="noopener ugc nofollow" target="_blank"> make_blobs() </a>函数创建的，该函数生成具有高斯分布的点的blob。生成了具有1000个数据的两个blobs数据集。数据集的中心在(100，100)和(200，200)上，它们的标准差是120。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成两个各向同性高斯斑点数据集</p></figure><p id="17e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实现数据标准化和缩放</strong>:</p><p id="56e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如[2]中所解释的，有不同的方法来标准化数据。对于本文，数据集使用<em class="mf">最小-最大归一化技术</em>进行归一化。该方法将每个特征的<strong class="lb iu">最小值</strong>调整为0，将<strong class="lb iu">最大值</strong>调整为1。根据以下公式，其余的<strong class="lb iu">值</strong>被转换成0到1之间的十进制数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/ce63dd8cee91dee64528a227d39f8975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RKAbAEpVFMbkKcjf0o8aHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最大-最小归一化:I =行(数据)，j =列(要素)</p></figure><p id="0af3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了研究尺度效应对分类器结果的影响，对原始数据进行了数据归一化，并对归一化数据进行了放大(50和300倍)。</p><blockquote class="mc md me"><p id="4596" class="kz la mf lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated">向上缩放数据集值=数据集值*缩放值</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标准化数据</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">放大数据</p></figure><p id="d7e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有生成数据的曲线图如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/818dc1b2432d5764201fbd3655ced7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*wKRfpApwqr8m4kiT82xBjA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0f91c58fd492699eb7ef9ae1395ec75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*JRu7-V8gCKMFSlTkCJAifA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:生成的数据图。</p></figure><p id="422d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">声明并运行线性核SVM分类器</strong>:</p><p id="cefa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Python上的<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sklearn库</a>创建了具有线性内核的SVM分类器。一半的数据用于训练，另一半用于测试。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对x_orig数据运行SVM线性核分类器的代码示例</p></figure><p id="5d2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性分类器的结果如图2所示。可以看出，规模并不影响其性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/24383964868b4cc33f7f6a0df26c8f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_r7Ckb3SZcKs_eECApJwPA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/69b6ddca01d34a957a93643f31936b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-PyxE_zFV_-PPIY5rZb2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:精度≈ 0.71</p></figure><p id="e662" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">声明并运行具有RBF内核的SVM分类器</strong>:</p><p id="6b91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有RBF核的SVM分类器的方法类似于上面提到的方法。首先，使用Python上的<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sklearn库</a>创建分类器。然后，一半的数据用于训练它，另一半用于测试它。</p><p id="ebea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与线性分类器相比，这个非线性分类器有两个超参数需要调整:<em class="mf"> gamma </em>和<em class="mf"> c </em>。当超参数<em class="mf"> gamma </em>被设置为常量值1时，分类器使用多个<em class="mf"> c </em>值迭代20次。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制径向基核SVM分类器的精度图</p></figure><p id="fd8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非线性分类器的结果如图3所示。该图的坐标轴是:</p><ul class=""><li id="82b2" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated">x轴:超参数c的值</li><li id="53e6" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">y轴:分类器的精确度</li></ul><p id="dc28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图3所示，<strong class="lb iu">标度确实影响精度性能</strong>，通过归一化数据获得更高的精度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/1539cf52af984a8ae6487a54a7ee6fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zZZfx72bG0ujFLsGWdRtqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:不同数据集的RBF核分类器的精度性能</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4b9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，线性核不受数据的放大和归一化的影响，而RBF核SVM分类器的精度高度依赖于数据集的规模。</p><p id="a121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个特定的分类器，这种依赖性来自于在实现内核技巧时使用的欧几里德距离:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/0ab9feb1a907cb9fc74f0b7fb0f3c38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uRzpfxbBFk0DKHwzVQM-Q.png"/></div></div></figure><p id="8f34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个特定的核，指数的值与欧几里德距离的平方成比例地增加。因此，为了提高具有较大比例的数据集的精度，当比例放大50倍和300倍时，伽马值应分别反向减小5倍和30倍。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制不同伽马值的RBF核SVM分类器的精度</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/35f71f1a11b24003ca7091285889038a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cm0tvW92fVAeoCnDVoBQOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:使用不同gamma值的RBF核分类器的精度性能</p></figure><p id="3642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图4所示，一旦超参数根据数据集的比例进行了调整，放大数据的精度与归一化数据的精度相同。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="0bf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="mf">如果你喜欢这篇文章，请考虑</em> </strong> <a class="ae ky" href="https://javiferfer.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="mf">订阅</em> </strong> </a> <strong class="lb iu"> <em class="mf">。你将获得我所有的内容+所有其他来自牛逼创作者的文章！</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="73b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong>:</p><p id="dcc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] Urvashi Jaitle，<a class="ae ky" href="https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029" rel="noopener">为什么数据规范化对于机器学习模型是必要的</a> (2018)，Medium</p><p id="c034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]张，<a class="ae ky" rel="noopener" target="_blank" href="/understand-data-normalization-in-machine-learning-8ff3062101f0">理解机器学习中的数据归一化</a> (2019)，中</p><p id="337e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://stats.stackexchange.com/questions/168051/kernel-svm-i-want-an-intuitive-understanding-of-mapping-to-a-higher-dimensional" rel="noopener ugc nofollow" target="_blank">内核SVM </a> (2015)，堆栈交换</p><p id="a213" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Brandon Rohrer，<a class="ae ky" href="https://www.youtube.com/watch?v=-Z4aojJ-pdg" rel="noopener ugc nofollow" target="_blank">支持向量机如何工作</a> (2017)，YouTube</p><p id="24b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] CodeEmporium，<a class="ae ky" href="https://www.youtube.com/watch?v=05VABNfa1ds" rel="noopener ugc nofollow" target="_blank">支持向量机</a> (2018)，YouTube</p></div></div>    
</body>
</html>