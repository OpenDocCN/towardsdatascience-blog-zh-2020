<html>
<head>
<title>Deep Neural Multilayer Perceptron (MLP) with Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有 Scikit-learn 的深度神经多层感知器(MLP)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-neural-multilayer-perceptron-mlp-with-scikit-learn-2698e77155e?source=collection_archive---------3-----------------------#2020-08-31">https://towardsdatascience.com/deep-neural-multilayer-perceptron-mlp-with-scikit-learn-2698e77155e?source=collection_archive---------3-----------------------#2020-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1c2d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">MLP 是一种人工神经网络。最简单的 MLP 至少由三层节点组成:输入层、隐藏层和输出层。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e6995e43b74b0e9ae7c50fc57afbf901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4qdwITDr-9R2ZQs1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Robina Weermeijer 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="dfc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深度学习的世界里，TensorFlow、Keras、微软认知工具包(CNTK)、PyTorch 都很受欢迎。我们大多数人可能没有意识到，非常流行的机器学习库<strong class="lb iu"> Scikit-learn </strong>也能够进行基本的深度学习建模。在这篇文章中，我将讨论 Scikit-learn 中深度学习建模的可行性和局限性。此外，我将通过两个例子讨论实际实现。</p><p id="8028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">sci kit-learn 中多层感知器(MLP)的显著点</em></p><ul class=""><li id="694e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">输出层没有激活功能。</li><li id="f753" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">对于回归场景，平方误差是损失函数，交叉熵是分类的损失函数</li><li id="64cd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">它可以处理单个以及多个目标值回归。</li><li id="6b05" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">与其他流行的软件包不同，像 Keras Scikit 中 MLP 的实现不支持 GPU。</li><li id="7f67" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们不能像不同的激活函数、权重初始化器等一样微调参数。对于每一层。</li></ul><p id="71af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">回归示例</em> </strong></p><p id="e321" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 1: </strong>在<strong class="lb iu"> Scikit-Learn </strong>包中，MLPRegressor 在 neural_network 模块中实现。我们将导入其他模块，如“train_test_split”来将数据集拆分为训练集和训练集以测试模型，“fetch_california_housing”来获取数据，以及“StandardScaler”来缩放数据，因为不同的特征(独立变量)具有较宽的值域范围。缩放用于训练模型的数据是非常重要的。</p><p id="5028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在文章<a class="ae ky" rel="noopener" target="_blank" href="/feature-scaling-effect-of-different-scikit-learn-scalers-deep-dive-8dec775d4946">功能缩放-不同 Scikit 的效果-了解缩放器:深入探讨</a>中了解更多关于不同缩放器的信息</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="8295" class="mp mq it ml b gy mr ms l mt mu">"""Import the required modules"""</span><span id="27f8" class="mp mq it ml b gy mv ms l mt mu"><strong class="ml iu">from sklearn.neural_network import MLPRegressor<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.datasets import fetch_california_housing<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import r2_score<br/>import pandas as pd</strong></span></pre><p id="052a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 2: </strong>我们将把数据集分成训练数据集和测试数据集。我们保留了 20%的数据集用于检查训练模型的准确性。独立的训练和测试数据集被进一步缩放，以确保输入数据是标准的正态分布，以零为中心，并且具有相同数量级的方差。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="935e" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">cal_housing = fetch_california_housing()<br/>X = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)<br/>y = cal_housing.target</strong></span><span id="9ddc" class="mp mq it ml b gy mv ms l mt mu"><strong class="ml iu">X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1, test_size=0.2)</strong></span></pre><p id="d6d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三步:我们就像上面的回归例子一样缩放数据，原因相同。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="a54b" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">sc_X = StandardScaler()<br/>X_trainscaled=sc_X.fit_transform(X_train)<br/>X_testscaled=sc_X.transform(X_test)</strong></span></pre><p id="33eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 4: </strong>在下面的代码中，建模了三个隐藏层，每层有 64 个神经元。考虑到输入和输出层，我们在模型中共有 5 层。如果没有提到任何优化器，那么“Adam”就是默认的优化器，它可以管理非常大的数据集。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="7b1e" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">reg = MLPRegressor(hidden_layer_sizes=(64,64,64),activation="relu" ,random_state=1, max_iter=2000).fit(X_trainscaled, y_train)</strong></span></pre><p id="66f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了“RELU”激活之外，MLPRegressor 还支持“sigmoid”和“双曲线 tan”功能。</p><p id="c266" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤 5:在下面的代码中，训练好的模型用于预测保留的测试数据集的目标值，该模型以前没有见过。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="dd64" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">y_pred=reg.predict(X_testscaled)<br/>print("The Score with ", (r2_score(y_pred, y_test))</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/68b2a7fe36d978012c9cb5da06b6f546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5LCN-K3-aJZUA09rTk4Ew.png"/></div></div></figure><p id="c988" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">分类举例</em> </strong></p><p id="31df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到了一个回归的例子。接下来，我们将看一个分类示例。在 Scikit-learn 中,“MLPClassifier”可用于多层感知器(MLP)分类场景。</p><p id="dd55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像往常一样，首先我们将导入我们将在示例中使用的模块。我们将使用 Iris 数据库和 MLPClassifierfrom 作为分类示例。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="299d" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from sklearn.datasets import load_iris<br/>from sklearn.neural_network import MLPClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler<br/>import pandas as pd<br/>from sklearn.metrics import plot_confusion_matrix<br/>import matplotlib.pyplot as plt</strong></span></pre><p id="979b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 2: </strong>在单独的数据帧“X”和“y”中，存储独立和从属特征的值。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="8edd" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">iris_data = load_iris()<br/>X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)<br/>y = iris_data.target</strong></span></pre><p id="ba58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 3: </strong>类似于上面的回归示例，我们将把数据集分成训练数据集和测试数据集。我们保留了 20%的数据集用于检查训练模型的准确性。独立的训练和测试数据集被进一步缩放，以确保输入数据是标准的正态分布，以零为中心，并且具有相同数量级的方差。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="75f3" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, test_size=0.2)<br/>sc_X = StandardScaler()<br/>X_trainscaled=sc_X.fit_transform(X_train)<br/>X_testscaled=sc_X.transform(X_test)</strong></span></pre><p id="e810" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 4: </strong>在下面的代码中，我们建立了四个隐藏层，每个层中有不同的神经元。考虑到输入和输出层，我们在模型中共有 6 层。如果没有提到任何优化器，那么“Adam”就是默认的优化器。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="d90b" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation="relu",random_state=1).fit(X_trainscaled, y_train)<br/>y_pred=clf.predict(X_testscaled)<br/>print(clf.score(X_testscaled, y_test))</strong></span></pre><p id="829b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分类器对测试数据显示出相当高的分数。重要的是要了解分类模型出错的领域，以充分了解模型的准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/2d70ebb7a0689ad6e36a3f2a23ebba1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bhbSUgPEicIv_-KtmHZ1Q.png"/></div></div></figure><p id="5223" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在“<a class="ae ky" rel="noopener" target="_blank" href="/accuracy-visualisation-supervised-machine-learning-classification-algorithms-af13d18fcc6c">准确性可视化:监督机器学习分类算法</a>”中了解更多关于我们应该使用混淆矩阵来判断分类模型准确性的原因。</p><p id="d47c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第五步:我们将绘制一个混淆矩阵来理解模型造成的错误分类。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="da77" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">fig=plot_confusion_matrix(clf, X_testscaled, y_test,display_labels=["Setosa","Versicolor","Virginica"])<br/>fig.figure_.suptitle("Confusion Matrix for Iris Dataset")<br/>plt.show()</strong></span></pre><p id="8e2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">似乎只有一种“云芝”被模型错误地识别为“海滨锦鸡儿”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/c6ba81c39c5d179593a118cbf37755d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vki8xciLH2Jg8KDBzIO6rw.png"/></div></div></figure><p id="7eeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong>:我们可以在 Scikit-learn 中做简单的深度学习回归和分类模型。在我看来，它不适合任何现实生活中的大规模建模，因为没有 GPU 的支持，调整参数的选项也非常有限。</p><p id="41f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在文章<a class="ae ky" rel="noopener" target="_blank" href="/accuracy-visualisation-in-deep-learning-part-1-b42d32b07913">深度学习中的准确性可视化</a>中了解更多关于深度学习可视化技术的信息</p><p id="8105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于开始机器学习之旅的人来说，你可以从文章<a class="ae ky" href="https://kaushikthoughts.medium.com/machine-learning-for-people-in-hurry-4a7540c0860d" rel="noopener">中的《为匆忙中的人学习机器</a>》开始。</p></div></div>    
</body>
</html>