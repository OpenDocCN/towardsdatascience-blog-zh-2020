<html>
<head>
<title>Machine Learning: Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-autoencoders-712337a07c71?source=collection_archive---------14-----------------------#2020-03-10">https://towardsdatascience.com/machine-learning-autoencoders-712337a07c71?source=collection_archive---------14-----------------------#2020-03-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f155" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用自动编码器将高维数据拟合到更密集的表示中</h2></div><p id="4974" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我通过维基百科找到了自动编码器最简单的定义，它将自己翻译为“一种学习数据低维编码的机器学习模型”。这是减少数据集维数的最聪明的方法之一，只需使用微分终止(Tensorflow，PyTorch等)的功能。事实上，我们应该有一个特定的神经网络架构，以实现这一点。不过，在开始之前，我们先简单概述一下降维。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/49e7e4b263982daffff9aab8fe2d2398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJs-8Pe5WqLiNVfeLrkDrw.png"/></div></div></figure><h1 id="5fb5" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">PCA与自动编码器</h1><p id="059f" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">PCA和自动编码器都打算学习低维表示，从而通过低维表示减少原始维度的重构误差。但是，正如您可能知道的，PCA将原始数据转换为一组较低的相互正交的维度。我们称之为到另一个空间的线性转换。问题来自于线性部分。信息损失可能会更高。与自动编码器相比，神经网络使用梯度下降来估计较低维度的隐藏层的最佳可能参数。</p><h1 id="1b7f" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">设计自动编码器</h1><p id="902b" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">自动编码器采用数据流通过瓶颈的架构。这个瓶颈代表了较低的维度。自动编码器被期望学习这个较低的维度，从而最小化在输入和输出之间定义的误差分数。</p><p id="8e62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来看看一个简单的自动编码器(来自维基百科)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/526f1b57c7a3c41cb5be7b545301067b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*tQs7G-n95UJVA264bH0SJA.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">自动编码器示意图</p></figure><p id="c738" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，我们有一个中间元素，它是一个狭窄的通道。训练的输入和输出通常是相同的，因为我们打算学习低维表示。</p><h1 id="7717" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">使用Keras构建自动编码器</h1><p id="1f12" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">我们通常的进口商品是:</p><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="ebe3" class="mu lo iq mq b gy mv mw l mx my">import tensorflow as tf<br/>import numpy as np<br/>import matplotlib.pyplot as plt # for plots</span></pre><p id="c0c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">处理数据；以MNIST手写数据集为例。</p><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="981f" class="mu lo iq mq b gy mv mw l mx my">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><span id="a234" class="mu lo iq mq b gy mz mw l mx my">x_train = x_train.astype('float32') / 255<br/>x_test = x_test.astype('float32') / 255</span><span id="54c1" class="mu lo iq mq b gy mz mw l mx my">x_train = np.round(x_train, 0)<br/>x_test = np.round(x_test, 0)</span><span id="efc5" class="mu lo iq mq b gy mz mw l mx my">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))</span><span id="c6d0" class="mu lo iq mq b gy mz mw l mx my">print(x_train.shape)</span></pre><p id="f49f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们对数据进行了四舍五入。这是因为这个数据集是灰度的，我们只考虑严格的黑色像素和严格的白色像素。这有助于我们形成一个<strong class="kh ir"> <em class="na">二元交叉熵</em> </strong>作为损失函数。</p><p id="2b4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们使用Keras functional API创建网络。</p><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="d3d1" class="mu lo iq mq b gy mv mw l mx my">inputs = tf.keras.layers.Input(784)<br/>encoded_1 = tf.keras.layers.Dense(128)(inputs)<br/>encoded = tf.keras.layers.Dense(64, activation='relu')(encoded_1)<br/>decoded_1 = tf.keras.layers.Dense(128)(encoded)<br/>decoded = tf.keras.layers.Dense(784, activation='sigmoid')(decoded_1)</span><span id="39d2" class="mu lo iq mq b gy mz mw l mx my">auto_encoder = tf.keras.Model(inputs, decoded)<br/>auto_encoder.compile(loss='binary_crossentropy', <br/>                     optimizer='adam', <br/>                     metrics=['accuracy'])</span><span id="394c" class="mu lo iq mq b gy mz mw l mx my">auto_encoder.summary()<br/>tf.keras.utils.plot_model(auto_encoder, show_shapes=True, to_file='autoenc.png', dpi=200)</span></pre><p id="7610" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我使用了<strong class="kh ir"><em class="na">binary _ cross entropy</em></strong>，因为我们的数据是二进制格式(0或1)。完整的层集是自动编码器(编码和解码以给出相同的输出)。让我们看看我们的编码器。我们可以使用从<strong class="kh ir"> <em class="na">输入</em> </strong>到<strong class="kh ir"> <em class="na">编码</em> </strong>的层来构建它，如下所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/2ba9aa5acc6481be409ccf2fd4e0c1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YgfzXQVJBHjmD86UTcrslg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">自动编码器的可视化</p></figure><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="d782" class="mu lo iq mq b gy mv mw l mx my">encoder = tf.keras.Model(inputs, encoded)</span><span id="4d6f" class="mu lo iq mq b gy mz mw l mx my">encoder.summary()<br/>tf.keras.utils.plot_model(encoder, show_shapes=True, to_file='enc.png', dpi=200)</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/52bd4affc34b9d634deebd5f0b12c52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yFOGbAjA8gMDcyx9ZMyYGg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">编码器的可视化</p></figure><p id="6cbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然您可以使用顺序Keras API来实现这一点，但我发现函数式API更优雅(只是个人意见，您可能会在以后的文章中看到原因)。</p><h1 id="f0a3" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">使用自动编码器</h1><p id="2aaf" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">既然我们已经构建了我们的自动编码器，我们可以拟合我们的训练数据。</p><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="2ff7" class="mu lo iq mq b gy mv mw l mx my">auto_encoder.fit(x_train, x_train, <br/>                 epochs=10,<br/>                 batch_size=256,<br/>                 shuffle=True)</span></pre><p id="3854" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">经过训练，我获得了0.98510.9851的准确度，这个准确度还不错！。因此，我们可以期待降维方面的良好表现。</p><p id="cf2c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看MNIST测试数据集的估计低维和重建图像。</p><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="4b06" class="mu lo iq mq b gy mv mw l mx my">predicted_2dim = encoder.predict(x_test)<br/>predicted_original = auto_encoder.predict(x_test)</span></pre><p id="71f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，对于解码后的值，我直接将原始数据提供给完整的自动编码器，后者在编码后直接进行重构。对于编码，我使用编码器组件。</p><h1 id="fb9d" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">看起来怎么样？</h1><p id="cba4" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">未经调整的输入数据如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/93e6bc51f19deaaff2583ff274b1afdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ht67EVIvHh1F0cDJQeGpvA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">原始字符</p></figure><p id="f648" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编码图像看起来像这样:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/75eae284275b92ed60a232c1e0255e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bTxxU1hrFgykMiMeIs6kg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">编码字符</p></figure><p id="d228" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重建看起来是这样的；</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/fe9f1af9496645ae85bbffc76a2c1f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WYxokaB7M8pwm-Xy9qiOg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">重构字符</p></figure><h1 id="102a" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">注意事项</h1><ul class=""><li id="50eb" class="ne nf iq kh b ki mf kl mg ko ng ks nh kw ni la nj nk nl nm bi translated">请注意，重建的图像看起来很模糊。这是因为我们的最后一层是由<strong class="kh ir"><em class="na">s形</em> </strong>函数激活的。该函数返回范围<strong class="kh ir"> <em class="na"> 0 </em> </strong>和<strong class="kh ir"> <em class="na"> 1 </em> </strong>内的值。</li><li id="3ac4" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">我们使用<strong class="kh ir"> <em class="na"> relu </em> </strong>激活函数，因为在中间层，当我们使用<strong class="kh ir"> <em class="na"> sigmoid </em> </strong>函数时，值变小的几率更高。</li><li id="8731" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">当我们减小隐藏维数时，重建的精度降低。重建看起来像这样。它只有人物的可辨别的特征，而不是确切的人物。</li></ul><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/8db7a62e5aef3ac4b9b93bd1cf8c5241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQDMQDBXwGN8pCr-Hb_bTQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">使用二维重建</p></figure><ul class=""><li id="371b" class="ne nf iq kh b ki kj kl km ko ns ks nt kw nu la nj nk nl nm bi translated">我们可以看到，在上面的图像9和4几乎是相似的。更多的信息丢失了。</li><li id="dc2a" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">我的字符绘图功能(可能会有用)</li></ul><pre class="lc ld le lf gt mp mq mr ms aw mt bi"><span id="b37f" class="mu lo iq mq b gy mv mw l mx my">shown = {}<br/>fig = plt.figure(figsize=(20, 10))<br/>fig.subplots_adjust(hspace=1, wspace=0.1)<br/>i = 0</span><span id="eadb" class="mu lo iq mq b gy mz mw l mx my">for data, y in zip(predicted_original, y_test):<br/>    if y not in shown and y==len(shown):<br/>        i += 1<br/>        ax = fig.add_subplot(1, 10, i)<br/>        ax.text(1, -1, str(y), fontsize=25, ha='center', c='g')<br/>        ax.imshow(np.array(data).reshape(28, 28), cmap='gray')</span><span id="687c" class="mu lo iq mq b gy mz mw l mx my">shown[y] = True<br/>    if len(shown) == 10:<br/>        break</span></pre><p id="b523" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随意更改代码并运行。希望这篇文章有用。我将在以后的文章中介绍变型自动编码器！</p><p id="48e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">干杯！</p></div></div>    
</body>
</html>