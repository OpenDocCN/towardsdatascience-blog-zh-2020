# 作为电影评论推荐系统的受限玻尔兹曼机(下)

> 原文：<https://towardsdatascience.com/restricted-boltzmann-machine-as-a-recommendation-system-for-movie-review-part-2-9a6cab91d85b?source=collection_archive---------28----------------------->

## 关于如何使用 Pytorch 创建用于电影分级预测的 Boltzmann 机器的技术演练

![](img/5b9e5dfeb6777fe2e2f4f9d77faefdd4.png)

通过[链接](https://unsplash.com/photos/CiUR8zISX60)改编自 unsplash 的 Img

本文是如何构建一个受限波尔兹曼机器(RBM)作为推荐系统的第 2 部分。

> 在[第一部分](/restricted-boltzmann-machine-how-to-create-a-recommendation-system-for-movie-review-45599a406deb)中，我们专注于数据处理，这里的重点是**模型创建**。你将学到的是**如何从头开始创建一个 RBM 模型**。它分为三个部分。

1.  模型结构
2.  模特培训
3.  模型检验

*📣📣这是一篇技术驱动的文章*。现在让我们开始旅程🏃‍♀️🏃‍♂️.

1.  **模型构建**

> 本质上，RBM 是一个概率图形模型。

为了构建模型架构，我们将为 RBM 创建一个类。在类中，定义 RBM 的所有参数，包括隐藏节点的数量、权重以及可见节点和隐藏节点的概率偏差。

> 有 4 个函数，第一个函数是初始化类，第二个函数是在给定可见节点的情况下采样隐藏节点的概率，第三个函数是在给定隐藏节点的情况下采样可见节点的概率，最后一个函数是训练模型。

1.1 *__init__* 函数

**在 *__init__* 函数中，我们将初始化所有需要优化的参数。**注意， *nv* 和 *nh* 分别是可见节点数和隐藏节点数。W 是可见节点和隐藏节点的权重。我们使用平均值为 0、方差为 1 的正态分布来初始化权重和偏差。 ***a*** 是给定可见节点时隐藏节点概率的偏差， ***b*** 是给定隐藏节点时可见节点概率的偏差。注意我们为批处理添加了一个维度，因为我们将在 *Pytorch* 中使用的函数不能接受只有一维的向量。

```
class RBM():
    def __init__(self, nv, nh):
        self.W = torch.randn(nh, nv)
        self.a = torch.randn(1, nh)
        self.b = torch.randn(1, nb)
```

1.2 隐藏节点采样功能

> **这个函数是关于在给定可见节点概率的情况下对隐藏节点进行采样**。我们为什么需要这个？因为我们需要概率来采样隐藏节点的激活。

假设我们有 100 个隐藏节点，这个函数将对隐藏节点的激活进行采样，即按照一定的概率 *p_h_given_v* 激活它们。 *p_h_given_v* 是给定 *v* 的值，隐藏节点等于 1(激活)的概率。

注意，该函数接受参数 *x* ，这是可见节点的值。我们用 *v* 来计算隐藏节点的概率。记住， *h* 给定*v*(*p _ h _ 给定 _v* )的概率就是 *v* 的 sigmoid 激活。因此，我们将可见节点的值乘以权重，再加上隐藏节点的偏差。我们扩展了偏置 *a* 的尺寸，使其与 *wx* 具有相同的尺寸，从而将偏置添加到 *wx* 的每一行。

```
def sample_h(self, x):
    wx = torch.mm(x, self.W.t())
    activation = wx + self.a.expand(wx)
    p_h_given_v =torch.sigmoid(activation)
    reutrn p_h_given_v, torch.bernoulli(p_h_given_v)
```

注意返回的是 *p_h_given_v* ，以及采样的隐藏节点。这里，我们做了一个伯努利 RBM，因为我们预测了一个二元结果，即用户喜欢或不喜欢一部电影。假设有 100 个隐藏节点， *p_h_given_v* 是 100 个元素的向量，每个元素为每个隐藏节点被激活的概率，给定可见节点的值(即用户对电影的评分)。但问题是如何激活隐藏节点？这里我们使用**伯努利采样**。假设，对于一个隐藏节点，它在 *p_h_given_v* 中的概率是 70%。我们取 0 到 1 之间的一个随机数。如果低于 70%，我们将不会激活隐藏节点。通过对 *p_h_given_v* 中的所有隐藏节点重复伯努利采样，我们得到 0 和 1 的向量，其中 1 对应于要激活的隐藏节点。

这是吉布斯采样✨✨.需要的第一个函数

1.3 可见节点采样功能

> **遵循同样的逻辑，我们创建了对可见节点进行采样的函数。**给定隐藏节点的值(1 或 0，激活与否)，我们估计可见节点的概率 *p_v_given_h* ，即每个可见节点等于 1(被激活)的概率。

因为有 1682 个电影和 1682 个可见节点，我们有 1682 个概率的向量，每个概率对应于等于 1 的可见节点，给定隐藏节点的激活。我们使用伯努利采样来决定是否对这个可见节点进行采样。最后，函数返回可见节点的概率 *p_v_given_h* ，以及一个 1 和 0 的向量，其中 1 对应于要激活的可见节点。

```
def sample_v(self, y):
    wy = torch.mm(y, self.W)
    activation = wy + self.b.expand(wy)
    p_v_given_h =torch.sigmoid(activation)
    reutrn p_v_given_h, torch.bernoulli(p_v_given_h)
```

1.4 对比散度函数

> **RBM 是一个基于能源的模型，这意味着我们需要最小化能源函数。**

> 能量函数取决于模型的权重，因此我们需要优化权重。另一方面，RBM 可以被视为一个概率图形模型，它需要最大化训练集的对数似然性。显然，对于任何神经网络，为了最小化能量或最大化对数似然，我们需要计算梯度。这里我们使用对比散度来近似似然梯度。
> 
> 对比分歧是关于近似对数似然梯度。代替需要大量计算资源的梯度的直接计算，我们近似梯度。在训练过程中，我们朝着能量最小化的方向调整重量。类似于通过梯度下降最小化损失函数，其中我们更新权重以最小化损失，唯一的区别是我们使用对比散度算法来近似梯度。

具体来说，我们从输入向量 *v0* 开始，基于 *p_h_given_v，*的概率，我们在第一次迭代时采样第一组隐藏节点，并使用这些采样的隐藏节点来采样可见节点 *v1* 与 *p_v_given_h.* 重复这个过程 *K* 次，这就是关于 K 步对比发散的全部内容。

在这个函数中，我们将使用本文概述的算法更新权重、可见节点的偏差和隐藏节点的偏差。*我强烈推荐这本*[*RBM pape*](https://link.springer.com/content/pdf/10.1007/978-3-642-33275-3_2.pdf)*r 如果你喜欢更深入的了解。*

在函数内部， *v0* 是包含用户对所有电影的评级的输入向量。 *vk* 是从可见节点到隐藏节点进行 *k* 采样后得到的可见节点。 *ph0* 是给定 *v0* 的第一次迭代时隐藏节点概率等于 1 的向量。 *phk* 是给定可见节点 *vk* 在第*k 次*迭代时隐藏节点的概率。

```
def train(self, v0, vk, ph0, phk):
    self.W += torch.mm(v0.t(), ph0) — torch.mm(vk.t(), phk)
    self.b += torch.sum((v0 — vk), 0)
    self.a += torch.sum((ph0 — phk), 0)
```

1.5 RBM 对象创建

为了初始化 RBM，我们创建一个 RBM 类的对象。首先我们需要可见节点数，也就是电影总数。隐藏节点的数量对应于我们想要从电影中检测的特征的数量。很难确定最佳的功能数量。但是这个参数是可调的，所以我们从 100 开始。我们还定义了批量大小，这是我们用来更新权重的一批中的观察次数。我们再次从 100 开始。

```
nv = len(training_set[0])
 nh = 100
 batch_size = 100
 rbm = RBM(nv, nh)
```

2.**模特培训**

祝贺你通过了第一部分，因为这是最难的部分👍👍。现在让我们来训练 RBM 模型。

我们首先将 *nb_epoch* 设置为 10。对于每个时期，所有观测值将进入网络，并在每批数据通过网络后更新权重。最后，我们得到最终的可见节点，这些节点对最初没有评级的电影有了新的评级。在每一批中，我们将进行 k 步对比散度来预测随机行走 *k* 步后的可见节点。因此，我们将有 3 个循环，一个用于历元迭代，一个用于批量迭代，最后一个用于对比发散。

对于损失函数，我们将测量训练集中预测评级和真实评级之间的差异。有几个选项，包括 RMSE，它是预测收视率和实际收视率之间的平方差的均值的根，以及预测收视率和实际收视率之间的绝对差。我们在这里取一个绝对的差值。

在**批处理**循环中，我们有输入向量 *vk* ，它将通过对比散度进行更新，并在随机行走 *k* 步后作为 Gibbs 采样的输出。但开始时， *vk* 是一批用户所有评分的输入批次。 *v0* 是将与预测值进行比较的目标值，预测值是该批用户已经评定的等级。 *ph* 0 是给定可见节点 *v0* 时隐藏节点的初始概率。

在对比发散循环中，我们将进行吉布斯采样。基本上，它包括建立 Gibbs 链，这是从可见节点到隐藏节点的几次往返。在每一轮中，可见节点被更新以获得良好的预测。从可见节点 *vk* 开始，我们用伯努利采样法对隐藏节点进行采样。在 10 次随机行走结束时，我们得到第 10 个采样的可见节点。请注意，我们不会对 RBM 进行等级为-1 的培训，这些等级在开始时并不作为真实等级。

利用 *v0* 、 *vk、ph0、phk* ，我们可以应用训练函数来更新权重和偏差。最终，与电影特征最相关的概率将获得最大的权重，从而导致正确的预测。在每一批结束时，我们记录训练损失。同样，我们只记录已经存在的评级损失。

```
nb_epoch = 10
for epoch in range(1, nb_epoch+1):
    train_loss = 0
    s = 0.
    for id_user in range(0, nb_users — batch_size, 100):
        vk = training_set[id_user: id_user+batch_size]
        v0 = training_set[id_user: id_user+batch_size]
        ph0, = rbm.sample_h(v0)
        for k in range(10):
            _, hk = rbm.sample_h(vk)
            _, vk = rbm.sample_v(hk)
            vk[v0<0] = v0[v0<0]
        phk, _ = rbm.sample_h(vk)
       rbm.train(v0, vk, ph0, phk)
       train_loss += torch.mean(torch.abs(v0[v0>0]-vk[v0>0]))
       s += 1
 print(‘epoch: ‘+str(epoch)+’ loss: ‘+str(train_loss/s))
```

经过 10 个历元迭代的训练，我们得到了一个 **0.15** 的损失。相当准确的✌✌.

3.**模型测试**

> 与训练循环相比，我们去除了历元迭代和批量迭代。我们将通过 RBM 循环每个观测值，并逐个进行预测，累积每次预测的损失。

注意下面，我们使用*训练集*作为输入来激活 RBM，同样的训练集用于训练 RBM。但不同的是，在测试阶段，我们没有删除用户最初没有评级的评级，因为这些是用于测试目的的模型的未知输入。还要注意，我们没有像在训练阶段那样进行 10 步随机行走。这是因为对于获得最佳预测的测试，1 步优于 10 次迭代。

```
test_loss = 0
s = 0.
for id_user in range(0, nb_users):
    v_input = training_set[id_user: id_user+1]
    v_target = test_set[id_user: id_user+1]
    if len(v_target(v_target>=0)):
        _, h = rbm.sample_h(v_input)
        _, v_input = rbm.sample_v(h)
        test_loss += torch.mean(torch.abs(v_target[v_target>0]-
                                          v_input[v_target>0])) 
        s += 1
print(‘test loss: ‘ +str(test_loss/s))
```

太好了。我们获得的损失**为 0.16** ，接近训练损失，表明轻微过度拟合。

**仅此而已。希望这能让你了解如何创建一个 RBM 作为推荐系统。如果需要源代码，请访问我的** [**Github**](https://github.com/luke4u/Movie-Rating-Prediction) **页面🤞🤞。**