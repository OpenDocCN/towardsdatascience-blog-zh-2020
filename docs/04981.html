<html>
<head>
<title>Get started with deep learning OCR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习OCR入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-started-with-deep-learning-ocr-136ac645db1d?source=collection_archive---------13-----------------------#2020-04-30">https://towardsdatascience.com/get-started-with-deep-learning-ocr-136ac645db1d?source=collection_archive---------13-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8402" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">现代OCR的实用介绍，包括培训和测试所需的一切。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/035a1c85873d6959630dacb53edc5b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCwHh5fxRdfSPIRAGrh_rw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一项人类不该做的字符识别任务。</p></figure><p id="8730" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不久前，我开始研究潜在的OCR实现，目标是仅使用生成的训练数据在tf.keras 2中创建一个清晰的卷积递归神经网络(CRNN)基线。另一个目标是创建一个存储库，允许以后使用相同的模板探索不同的体系结构和数据域。该存储库应该是模块化的，易于使用，并充满解释性评论，以便我的同事可以很容易地跟进。</p><p id="eab5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在高强度的谷歌搜索之后，我没能找到一个满足我要求的库。虽然有一些好的资源和实现，但我决定创建自己的[<a class="ae lu" href="https://keras.io/examples/image_ocr/" rel="noopener ugc nofollow" target="_blank">1</a>、<a class="ae lu" href="https://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/" rel="noopener ugc nofollow" target="_blank">2</a>、<a class="ae lu" rel="noopener" target="_blank" href="/a-gentle-introduction-to-ocr-ee1469a201aa">3</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/a68ed5fcf063d09ff2a82a35b8066964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*vZlI-6JYM821yqZv9FTVow.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在我的笔记本电脑GPU上训练下面的简单模型需要几分钟，但是没有GPU你也可以很容易地训练它。</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="fb60" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">传统的文档处理流水线通过应用OCR软件从文本图像中解析文本来工作。通常，图像由边界框模块预先准备，边界框模块为OCR模块隔离图像内的文本区域。管道的后续阶段将:解析文本，提取含义、关键字、值和表格，并以更易于编程的形式返回结果，如:数据库条目、CSV，或将输出与公司的内部系统集成。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="dc53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，重点是OCR阶段，以基于深度学习的CRNN架构为例。在<a class="ae lu" href="https://github.com/kutvonenaki/simple_ocr" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> GitHub </strong> </a>中共同发布了一个完整的功能实现，旨在作为一个端到端管道模板，包括数据生成和推理。重点是提供一个清晰的、有良好文档记录的管道，可以很容易地扩展到不同的架构，而不是神经网络(NN)架构本身。</p><p id="b3b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的目标是让人们能够在不访问GPU或任何训练数据的情况下，仅使用存储库中的指令来训练和测试简单的OCR模型。这篇文章将讨论一些实现细节，但是对于那些只想尝试代码的人来说，这些并不是真正需要的。</p><p id="1312" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，虽然在这个例子中为了简单起见我们只解析数字，但是在回购中使用的<a class="ae lu" href="https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> trdg </strong> </a> <strong class="la iu"> </strong>文本生成器可以从维基百科生成随机句子，用于训练更通用的OCR模型。也就是说，为了在更具挑战性的数据领域(如噪声图像、手写文本或自然场景中的文本)上实现良好的性能，需要调整模型和数据增强方法。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="cb7d" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">CRNN</h2><p id="599b" class="pw-post-body-paragraph ky kz it la b lb mw ju ld le mx jx lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">OCR中常用的神经网络架构很少，比如CRNN和各种基于注意力的模型[ <a class="ae lu" href="https://nanonets.com/blog/attention-ocr-for-text-recogntion/" rel="noopener ugc nofollow" target="_blank"> 4 </a>，<a class="ae lu" href="https://github.com/emedvedev/attention-ocr" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。CRNN模型使用卷积神经网络(CNN)来提取视觉特征，这些视觉特征被整形并馈入长短期记忆网络(LSTM)。然后，LSTM的输出被映射到具有密集层的字符标签空间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/511659ddc973fd53d80a91e0b3d1983b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UN_WqA_5RQkDAEdwUu7wXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CRNN OCR模型的基本构件。块内的层数和具体参数在不同的实现中有所不同。在推理时，不使用CTC丢失，而是将来自密集层的输出解码成相应的字符标签。详情见代码。</p></figure><h2 id="197d" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">有时LSTM层可以被去除</h2><p id="fa99" class="pw-post-body-paragraph ky kz it la b lb mw ju ld le mx jx lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">LSTM层的功能是学习文本的语言模型。例如，考虑一种情况，其中CNN特征提取层产生“tha_k you”的编码，但是place _处的字符没有被很好地识别。也许在那个地方有一个污垢点或者字符是以一种草率的方式手写的。但由于LSTM层通过数据了解到文本中有大量的“谢谢”输入，OCR可以预测正确的缺失字母。</p><p id="485a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样的推理对随机字符串不起作用，比如“1252 _ 5”；在这种情况下，没有“语言模型”的上下文历史，所以实际上我们可以删除这个任务的LSTM部分，使模型更快更轻。这正是我们的数字唯一的例子，但真实的话，LSTM层应该使用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在提供的示例存储库中，LSTM的使用由use_lstm布尔控制，它是models.make_standard_CRNN()的输入参数。</p></figure><h1 id="e07e" class="ne me it bd mf nf ng nh mi ni nj nk ml jz nl ka mo kc nm kd mr kf nn kg mu no bi translated"><em class="np">实施</em></h1><p id="0698" class="pw-post-body-paragraph ky kz it la b lb mw ju ld le mx jx lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">这里的重点是高层次的概述和tf.keras实现中几个稍微复杂的部分，尤其是与CTC损失相关的部分。鼓励有兴趣更详细了解CTC损失的人阅读<a class="ae lu" href="https://distill.pub/2017/ctc/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">这篇</strong> </a>的优秀文章。</p><p id="a561" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最高级别上，训练代码构建由图像高度和我们的字典中的字符总数参数化的神经网络，创建数据生成器并训练模型。注意，在我们的实现中，训练和推理可以接受任何长度的文本，图像宽度不需要预先定义。</p><p id="533d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单个批次中的图像必须具有相同的尺寸，但这由数据生成器(下面的datagen)负责，将图像填充到批次中最大图像的尺寸。训练数据完全由<a class="ae lu" href="https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> trdg </strong> </a>模块生成。</p><p id="08a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下代码行构造了回调函数，用于在每个时期后可视化批图像及其预测，并在训练期间保存已训练的权重。在此之后，我们编译模型，选择我们喜欢的优化器，并为选定数量的时期进行训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tf.keras中的高级函数</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="b607" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">破解tf.keras 2.1中的CTC丢失</h2><p id="9bf5" class="pw-post-body-paragraph ky kz it la b lb mw ju ld le mx jx lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">在tf.keras 2.1中为CRNN实现CTC loss可能会很有挑战性。这是由于来自NN模型的输出(最后一个密集层的输出)是形状的张量(batch_size、时间分布长度、数据中唯一字符的数量)，但是批量条目的实际预测目标是单词中的字符标签，例如[1，3，6，2，0，0，0]。因此，预测和目标将不会具有相同的形状。tf.keras 2.1中的一个限制是y_true和y_pred的维度必须匹配。在推理时，我们只需要模型输出和CTC解码函数来产生字符标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="6bcc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了批次的y_pred和true标签(batch_labels)，ctc_batch_cost还需要每个输入单词的字符数(label_length)。这需要忽略batch_labels中的“虚拟标签”,例如，在一个最长的字为7个字符长的批处理中，4个字符的字在位置[4:7]中的标签，因此batch_labels的形状为(batch_size，7)。还有，我们要馈入input_length，也就是y_pred (y_pred.shape[1])的时间维度。</p><p id="8691" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在某些keras实现中，损耗是在神经网络内部实现的，输出不是上面的y_pred。在这些实现中，模型已经返回计算的ctc批次损失，并且在model.compile()中使用虚拟损失来最小化来自NN的输出(CTC损失)。我不想在模型中实现loss，因为我认为这使得代码与标准的tf.keras格式不同，并可能在以后搜索不同的架构和参数时导致复杂性。</p><p id="2c1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我仍然需要依靠一些黑客技术来解决y_true和y_pred的不匹配维度。最后，我将标签、输入长度和batch_labels嵌入到一个形状为y_pred的数组中，并将其作为y_true返回。这种打包在数据生成器中完成，张量在损失函数中解包。</p><p id="1fdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数组的打包实际上也考虑了所需的参数。只有当发送到CTC loss的y_pred的时间维度大于批中最长文本的字符数时，打包才可能导致错误。但这是使用太多卷积的结果，或者是由于输入定义不当。一个6个字符长的单词不能通过建议5个字符来识别。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="313c" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">如何自己测试OCR</h2><p id="c4dc" class="pw-post-body-paragraph ky kz it la b lb mw ju ld le mx jx lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">要开始使用，下载或克隆<a class="ae lu" href="https://github.com/kutvonenaki/simple_ocr" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> github repo </strong> </a>并设置包含Tensorflow 2.1、<strong class="la iu"> trdg </strong> (pip install trdg)和Jupyter notebook的Python环境。自述文件包含如何使用Docker设置环境的说明。</p><p id="7691" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">设置好环境后，用jupyter笔记本打开<a class="ae lu" href="https://github.com/kutvonenaki/simple_ocr/blob/master/OCR_simple.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">笔记本</strong> </a> <strong class="la iu"> </strong>(点击查看示例输出)。执行第一个单元以创建模型、数据生成器和编译模型。第二个单元将训练模型。第三个单元格将调用推理函数来预测放置在inference_test_imgs/文件夹中的所有图像中的文本。训练也可以通过主文件夹中的“python train.py”来完成，但是你会错过训练过程中的可视化输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/c4ed371067af6ff444881d942cea139f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*hopW_Nfky4uGHAKP2imXsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从推理输出的例子来看，预测的准确性取决于训练参数、神经网络结构和所使用的数据生成参数。在这个例子中，我们将与第一和第三个例子相似的数字输入神经网络。人们可以通过探索不同的神经网络结构、训练参数、字体、背景和其他数据扩充参数来改进结果。</p></figure><p id="59c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望你能从这个简短的介绍开始！更多详情和评论请查看<a class="ae lu" href="https://github.com/kutvonenaki/simple_ocr" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> Github </strong> </a> <strong class="la iu">。</strong>我们也希望聘请有好奇心的、久经考验的软件工程师来开发人工智能/人工智能产品，请随时联系我们。感谢您的阅读！</p><p id="5de1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在Medium的其他帖子:</p><p id="aed5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://medium.com/swlh/exploring-quantum-computing-with-rigetti-pyquil-mid-2020-edition-70b28f917670" rel="noopener">使用Rigetti的量子计算&amp; pyQuil </a></p><p id="6dc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://medium.com/@akikutvonen/how-to-train-sentencepiece-tokenizers-for-any-language-with-large-data-pretrained-models-for-e84bb225ed4a" rel="noopener">https://medium . com/@ akikutvonen/how-to-train-sentence piece-tokenizers-for-any-language-with-large-data-pretrained-models-for-e84bb 225 ed4a</a></p></div></div>    
</body>
</html>