<html>
<head>
<title>Random Forest: what you need to know before starting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林:开始前你需要知道的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quick-intro-to-random-forest-3cb5006868d8?source=collection_archive---------22-----------------------#2020-03-21">https://towardsdatascience.com/quick-intro-to-random-forest-3cb5006868d8?source=collection_archive---------22-----------------------#2020-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7a11" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/ac17494d53ece05ffebaa990cc8f27a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bki7ZaaXf05wf6lHlDRtkA.jpeg"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">弗拉季斯拉夫·巴比延科在<a class="ae ko" href="https://unsplash.com/s/photos/choice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="d2a7" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是随机森林？</h1><p id="f1c5" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">根据官方<a class="ae ko" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>::<em class="ml">随机森林是一种元估计器，它在数据集的各个子样本上拟合多个决策树分类器，并使用平均来提高预测精度和控制过拟合。子样本大小始终与原始输入样本大小相同，但样本是用替换的</em>抽取的。</p><p id="4cc1" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di">换句话说，随机森林是一种强大但相对简单的数据挖掘和监督机器学习技术。<strong class="lp jd">它允许从极大的数据集中快速、自动地识别相关信息。</strong>算法最大的优点是依赖于很多预测(树)的集合，而不是信任单个。用随机森林你可以做到:<br/> - C <a class="ae ko" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">分类</a>(输入=分类数据)，每棵树为最终预测投下一票。<br/> - <a class="ae ko" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank">回归</a>(输入=连续数据)，树的结果被平均以创建最终预测。</span></p><p id="92ca" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><strong class="lp jd">我们举个现实生活中的例子来解释一下。假设你正在犹豫下次去哪里度假。你决定向你的朋友拉斯姆斯寻求建议。拉斯莫斯会问你，你已经去过哪里，你做了什么，你是否喜欢它，以了解你喜欢做或看到什么样的事情。根据你的回答和他自己的经历，他会给你一些建议。 <strong class="lp jd"> <em class="ml">在这里，拉斯姆斯正在做一个决策树</em> </strong> <em class="ml">来为你提供可能的最佳选择。</em>于是，为了做出最准确的决定，你和“n”个其他朋友重复操作。他们像拉斯姆斯一样问你一些随机的问题，并给你提供不同的去处。每一个回答都像是为去的地方投了一票。有朋友推荐的一些常见的地方。你收集所有的投票并汇总。你决定去票数最多的地方。 <strong class="lp jd"> <em class="ml">在这里，你做的是随机森林。</em> </strong></strong></p><blockquote class="na nb nc"><p id="a303" class="ln lo ml lp b lq mm ls lt lu mn lw lx nd mo ma mb ne mp me mf nf mq mi mj mk im bi translated">换句话说，在做决定之前，你是想向一个人征求建议，还是想向许多不同的人征求意见，然后把他们的答案汇总起来再做决定？</p></blockquote><p id="995c" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">这就是你在使用随机森林时所做的事情。通过累积各种树的预测，您正在授权您的分类/回归。</p><h1 id="58b2" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">如何建立一个森林？</h1><p id="2227" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">决策树是随机森林模型的基础。它们很容易理解，因为每个人都有意识或无意识地直观使用它们:</p><figure class="nh ni nj nk gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ng"><img src="../Images/7589b49c51ea1120389e8e9179bd39af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrTEdUwBcaVS4YUOtBaPQQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">来源:<a class="ae ko" href="https://www.canva.com/" rel="noopener ugc nofollow" target="_blank">https://www.canva.com/</a></p></figure><p id="ce1e" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">在数据科学的世界里，决策树一步一步地将数据集分解成更小的子集。最终的结果是一棵有<strong class="lp jd">个决策节点</strong>和<strong class="lp jd">个叶节点</strong>的树。</p><figure class="nh ni nj nk gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nl"><img src="../Images/e56e3b55c96676cf8dac9fc0d6081f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2iFa-lmvUCyq74TbudLlw.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">来源:<a class="ae ko" href="https://www.datacamp.com/community/tutorials/decision-tree-classification-python" rel="noopener ugc nofollow" target="_blank">“Python 中的决策树分类。”</a>，纳夫拉尼，阿维纳什。2018.</p></figure><h2 id="0fa9" class="nm kq it bd kr nn no dn kv np nq dp kz ly nr ns ld mc nt nu lh mg nv nw ll iz bi translated">决策树的优势</h2><ol class=""><li id="1eef" class="nx ny it lp b lq lr lu lv ly nz mc oa mg ob mk oc od oe of bi translated">因为直观所以容易理解。</li><li id="5b2c" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">易于解释和可视化，因为它产生了一组规则。</li><li id="1cd5" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">对数字和分类数据都开放</li><li id="e5f7" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">在大型数据集上表现相当好</li><li id="b713" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">与 KNN 和其他分类算法相比，以光速工作。</li></ol><h2 id="6d5a" class="nm kq it bd kr nn no dn kv np nq dp kz ly nr ns ld mc nt nu lh mg nv nw ll iz bi translated">决策树的极限</h2><p id="bcd5" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">决策树很简单，但相当不准确。选择下一个节点的一个失策，就可能把你推向完全不同的结果。此外，他们还面临着过度拟合的高风险。</p><blockquote class="na nb nc"><p id="a49d" class="ln lo ml lp b lq mm ls lt lu mn lw lx nd mo ma mb ne mp me mf nf mq mi mj mk im bi translated"><strong class="lp jd">记住:</strong> <a class="ae ko" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>发生在我们的模型捕捉到数据中的噪声和潜在模式时。当我们在嘈杂的数据集上训练我们的模型时，就会发生这种情况。这些模型具有低<strong class="lp jd">偏差</strong>和高<strong class="lp jd">方差</strong>。</p></blockquote><p id="dfcd" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><strong class="lp jd">所以基本上，树越深→对你的数据集越具体→过度拟合的风险越高。</strong></p><figure class="nh ni nj nk gt kd gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/bdb0b64c77e8f5824447bd1bd3c596f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*NMR6FGx3OhdiY7jSz9Rwng.jpeg"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">来源:<a class="ae ko" href="https://www.google.com/url?sa=i&amp;url=http%3A%2F%2Fdata-mining.philippe-fournier-viger.com%2Fsome-funny-pictures-related-to-data-mining%2Foverfitting%2F&amp;psig=AOvVaw14joteF4qLRXuv1taaF_JS&amp;ust=1585058265879000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCODo36ngsOgCFQAAAAAdAAAAABAc" rel="noopener ugc nofollow" target="_blank">数据挖掘博客——Philippe Fournier-Viger</a></p></figure><h1 id="cf81" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">团结则存，分裂则亡</h1><p id="506c" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">随机森林算法不是创建一个单独的决策树，而是从随机选择的数据集子集创建许多单独的树。这些单独的树中的每一个生成数据子集内的对象的分类/回归。</p><p id="723f" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><strong class="lp jd">它是如何工作的？</strong></p><p id="8f90" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">让我们考虑我们总共有<em class="ml"> m </em>个特性。</p><ol class=""><li id="8789" class="nx ny it lp b lq mm lu mn ly om mc on mg oo mk oc od oe of bi translated">随机选择<em class="ml"> k </em>特征，其中<em class="ml"> k </em> &lt; <em class="ml"> m. </em></li><li id="0e4a" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">在<em class="ml"> k </em>特征中，使用最佳分割点计算节点。</li><li id="a015" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">使用最佳分割方法将节点分割成子节点。</li><li id="222f" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">重复前面的步骤<em class="ml"> n 次</em>。</li><li id="2c70" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">你最终会拥有一片由 n 棵树组成的森林。</li><li id="787e" class="nx ny it lp b lq og lu oh ly oi mc oj mg ok mk oc od oe of bi translated">将决策树的结果组合在一起(<em class="ml">即自举</em>)。</li></ol><figure class="nh ni nj nk gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi op"><img src="../Images/754a5243dacda2e77c2c811e875077e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SS9c_Ihb8BRgkfiZz1Y7ig.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">随机森林=决策树的简单性*随机性带来的准确性</p></figure><h1 id="1b93" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论——为什么随机森林是这么酷的 ML 算法？</h1><blockquote class="na nb nc"><p id="7a9a" class="ln lo ml lp b lq mm ls lt lu mn lw lx nd mo ma mb ne mp me mf nf mq mi mj mk im bi translated"><strong class="lp jd">对异常值和非线性数据相当稳健</strong></p></blockquote><p id="2b91" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">随机森林通过本质上的宁滨处理离群值。它对非线性特征也漠不关心。</p><blockquote class="na nb nc"><p id="4abf" class="ln lo ml lp b lq mm ls lt lu mn lw lx nd mo ma mb ne mp me mf nf mq mi mj mk im bi translated"><strong class="lp jd">很好地处理不平衡数据</strong></p></blockquote><p id="1b3b" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">让我们弄清楚，我们离欠采样、过采样或 SMOTE 方法还很远。随机森林所提供的是，根据类在数据集中的存在情况对它们进行反比例加权的可能性。换句话说，如果您有一个少数民族类和一个多数民族类，则少数民族类的权重将比多数民族类大得多(错误率大),以重新平衡其在数据集中代表性不足的事实。</p><blockquote class="na nb nc"><p id="d8c5" class="ln lo ml lp b lq mm ls lt lu mn lw lx nd mo ma mb ne mp me mf nf mq mi mj mk im bi translated"><strong class="lp jd">降低过度拟合的风险</strong></p></blockquote><p id="7fe6" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated">每个独立的引导数据集的分类/回归都可能略有错误。然而，由于该算法依赖于所有自举集<strong class="lp jd">的集合，它极大地限制了过拟合，而不会由于偏差而显著增加误差。</strong>让我们更详细地了解如何:</p><p id="4050" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><strong class="lp jd"> <em class="ml"> →对数据集的子样本进行训练:</em> </strong>每棵决策树的方差都很高，但偏差很低。但是因为我们平均所有的树，我们也平均方差，所以我们有较低的偏差和一个更温和的方差模型。</p><p id="bb99" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><strong class="lp jd"> <em class="ml"> →对</em> </strong> <strong class="lp jd"> <em class="ml">特征的子样本进行训练</em> </strong> <br/>如果我们有 12 个特征，随机森林在每个模型中只会使用一定数量的那些特征。假设 4 乘树，那么 8 个潜在有用的特征被省略。但是由于 Random Forest 是决策树的集合，最终我们的许多或所有特征都会被表示出来。每个树中的特征是随机选择的，以便限制由偏差引起的误差和由方差引起的误差。</p><blockquote class="oq"><p id="2bb5" class="or os it bd ot ou ov ow ox oy oz mk dk translated">我们更有可能击中随机森林的目标</p></blockquote><figure class="pb pc pd pe pf kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pa"><img src="../Images/196aed991ee374cc4b7b9b002c2c9943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8z0UBjc_O00KZIkK5qMag.jpeg"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">来源:<a class="ae ko" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">“理解偏差-方差权衡”，</a>2012 年 6 月，Scott Fortmann-Roe</p></figure></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><p id="95b6" class="pw-post-body-paragraph ln lo it lp b lq mm ls lt lu mn lw lx ly mo ma mb mc mp me mf mg mq mi mj mk im bi translated"><em class="ml">感谢阅读！如果你喜欢这篇文章，就按住按钮鼓掌支持我的写作。我总是愿意聊天和喝杯“虚拟”咖啡，所以来关注我的</em><a class="ae ko" href="https://www.linkedin.com/in/aureliegiraud9000/" rel="noopener ugc nofollow" target="_blank"><em class="ml">Linkedin</em></a><em class="ml">。</em></p><div class="pn po gp gr pp pq"><a href="https://agiraud.medium.com/membership" rel="noopener follow" target="_blank"><div class="pr ab fo"><div class="ps ab pt cl cj pu"><h2 class="bd jd gy z fp pv fr fs pw fu fw jc bi translated">通过我的推荐链接加入 Medium-aurélie Giraud</h2><div class="px l"><h3 class="bd b gy z fp pv fr fs pw fu fw dk translated">不要错过我的下一篇文章，阅读 Medium 上成千上万的其他作者的文章！我写的是如何使用数据科学…</h3></div><div class="py l"><p class="bd b dl z fp pv fr fs pw fu fw dk translated">agiraud.medium.com</p></div></div><div class="pz l"><div class="qa l qb qc qd pz qe ki pq"/></div></div></a></div></div></div>    
</body>
</html>