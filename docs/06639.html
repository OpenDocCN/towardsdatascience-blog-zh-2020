<html>
<head>
<title>Linear Discriminant Analysis — An Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析——导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-an-introduction-50230ba7dadb?source=collection_archive---------38-----------------------#2020-05-25">https://towardsdatascience.com/linear-discriminant-analysis-an-introduction-50230ba7dadb?source=collection_archive---------38-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1942" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索LDA作为降维和分类技术</h2></div><p id="48db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA是一种降维算法，类似于PCA。然而，虽然PCA是一种无监督算法，其重点在于最大化数据集中的方差，但是LDA是一种监督算法，其最大化类别之间的可分性。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/7e828a51e98c56c6e2e202679b28913b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*lxDIprx-DuqOjzrP5pRwFA.jpeg"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">杰米·麦卡弗里拍摄，版权所有。</p></figure><p id="0c2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在建立分类问题时，目标是确保类别的最大可分性或区分度。</p><p id="d2a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个包含两列的数据集—一个解释变量和一个二进制目标变量(值为1和0)。二进制变量的分布如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/7d1e93db6267c7fb6e399706e78c3d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*BmV6Cq80YUU8V-ZyJtKATA.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">目标变量的分布(在draw.io上创建)</p></figure><p id="c5a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">绿点代表1，红点代表0。因为只有一个解释变量，所以用一个轴(X)表示。但是，如果我们试图放置一个线性分割线来划分数据点，我们将无法成功地做到这一点，因为这些点分散在整个轴上。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/d491eed221f08406d274459f3f0f279a.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*ks3NTpVXs8wo9x3hHSMvlQ.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">划分点的分隔线(在draw.io上创建)</p></figure><p id="edae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，似乎一个解释变量不足以预测二元结果。因此，我们将引入另一个特征X2，并检查点在2维空间的分布。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/4e5495fed6521bdb7732586c57a2a400.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*mNJ2SSwZFtoUE9m18JdqZw.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">二维空间中点的分布(在draw.io上创建)</p></figure><p id="aa40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在二维空间中，输出的分界似乎比以前更好。但是，在已经有多个要素的数据集中增加维度可能不是一个好主意。在这些情况下，LDA通过最小化维度来拯救我们。在下图中，目标类被投影到一个新轴上:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/efb5fd4c582ce25c01bd02bd16a63bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Zl5hb-KGalcnkSUqnLpfpg.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">目标值在变换轴上的投影(在draw.io上创建)</p></figure><p id="3c22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些阶层现在很容易划分。LDA将原始特征变换到一个新的轴上，称为线性判别式(LD ),从而降低维数并确保类的最大可分性。</p><p id="ec16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将这种可分性用数字表示，我们需要一个度量可分性的标准。计算这两个等级的平均值之间的差异可以是这样一种度量。差值越大，则表明两点之间的距离越大。然而，这种方法没有将数据的扩散考虑在内。因此，即使较高的平均值也不能确保某些类不会相互重叠。</p><p id="139e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二项措施是考虑类内的均值和方差。为了确保最大的可分性，我们将最大化均值之间的差异，同时最小化方差。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/4de8e8b0077ff3b8c881949f111fcbcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*iQADuE2rMHNNArKrrF-cfA.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">类内方差(S1和S2)和两个类的均值(M1和M2)</p></figure><p id="a009" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分数的计算方法是(M1-M2) /(S1 +S2)。</p><p id="15b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果有三个解释变量- X1、X2、X3，LDA将把它们转换成三个轴-LD1、LD2和LD3。这三个轴将根据计算出的分数排名第一、第二和第三。</p><p id="bdcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，LDA帮助我们降低维度和分类目标值。</p><p id="7cc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将通过一个例子来看看LDA是如何实现这两个目标的。下面的数据显示了IBM的一个虚构数据集，它记录了员工数据和减员情况。目标是根据不同的因素，如年龄、工作年限、出差性质、教育程度等，预测员工的流失情况。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/8e2c39fd2d32e7c6906621ec49b1d8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5q_sZQCmOE8x2a5sCSV_g.png"/></div></div></figure><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="698e" class="md me it lz b gy mf mg l mh mi">sns.countplot(train['Attrition'])<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/f0ccfbebb0ac6dfb7effab701fae29d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*IuTQ1c6gekXCpcT-_0TSOw.png"/></div></figure><p id="fbab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大约有1470份记录，其中237名员工离开了组织，1233名员工没有离开。是被编码为1，否被编码为0。</p><p id="0f79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果没有正确预测，员工流失会导致失去有价值的人，导致组织效率降低，团队成员士气低落等。因此，有必要正确预测哪个员工可能会离开。换句话说，如果我们预测某个员工会留下来，但实际上该员工离开了公司，那么漏报的数量就会增加。我们的目标是最小化假阴性，从而提高召回率(TP/(TP+FN))。</p><p id="22c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将尝试使用KNN对类别进行分类:</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="16c8" class="md me it lz b gy mf mg l mh mi">knn=KNeighborsClassifier(n_neighbors=10,weights='distance',algorithm='auto', p=3)<br/>start_time = time.time()<br/>knn.fit(X_train_sc,y_train)<br/>end_time = time.time()<br/>time_knn = end_time-start_time<br/>print(time_knn)<br/>pred = knn.predict(X_test_sc)<br/>print(confusion_matrix(y_test,pred))<br/>print(classification_report(y_test,pred))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ee32d0c7c19f5e1f1a595583a1abe9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*E6wnRW4lbmTe_Q8jtewbqQ.png"/></div></figure><p id="5ed1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">适应KNN所用的时间:0 . 46360 . 38386838661</p><p id="bfab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于0.05离开的员工来说，回忆是非常差的。</p><p id="85c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在将使用LDA作为分类算法并检查结果。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="9628" class="md me it lz b gy mf mg l mh mi">lda_0 = LDA()<br/>lda_0.fit(X_train_sc, y_train)<br/>y_test_pred_0 = lda_0.predict(X_test_sc)<br/>print(confusion_matrix(y_test, y_test_pred_0))<br/>print(classification_report(y_test, y_test_pred_0))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/46c9adf4db6e1c803b95e1b9f3c21561.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*TtomqHxHSmJF1XmgI3UKyg.png"/></div></figure><p id="c513" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">召回率急剧上升至0.36</p><p id="dc1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将使用LDA转换训练集，然后使用KNN。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="8ad5" class="md me it lz b gy mf mg l mh mi">#Transformation by LDA<br/>lda_1 = LDA(n_components = 1, solver='eigen', shrinkage='auto')<br/>X_train_lda = lda_1.fit_transform(X_train_lda, y_train_lda)<br/>X_test_lda = lda_1.transform(X_test_lda)</span></pre><p id="8e5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是标准缩放。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="cb12" class="md me it lz b gy mf mg l mh mi">sc3 = StandardScaler()<br/>X_train_lda_sc = sc3.fit_transform(X_train_lda)<br/>X_test_lda_sc = sc3.transform(X_test_lda)</span></pre><p id="6d19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们对转换后的数据应用KNN。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="8f33" class="md me it lz b gy mf mg l mh mi">knn=KNeighborsClassifier(n_neighbors=8,weights='distance',algorithm='auto', p=3)<br/>start_time = time.time()<br/>knn.fit(X_train_lda_sc,y_train_lda)<br/>end_time = time.time()<br/>time_lda = end_time-start_time<br/>print(time_lda)<br/>print(confusion_matrix(y_test_lda,pred))<br/>print(classification_report(y_test_lda,pred))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b888f1db078c59a61abb9dc7b8f394eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*LSShulOiPQ86lkbRJdQI4g.png"/></div></figure><p id="9f3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对转换后的数据运行KNN程序所用的时间:0 . 46866 . 46868686661</p><p id="2f25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">召回率现在提高了6%。</p><p id="5681" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，KNN拟合LDA转换数据所用的时间是KNN一个人所用时间的50%。</p><p id="039a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">全部代码可在<a class="ae lq" href="https://github.com/pritha21/Concepts/tree/master/LDA" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="f813" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望我已经演示了LDA的使用，包括分类和将数据转换成不同的轴！</p><p id="5324" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">敬请关注更多内容！一如既往，感谢您的任何反馈。</p></div></div>    
</body>
</html>