# è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ•°æ®æ¸…ç†å’Œç‰¹å¾æå–åˆå­¦è€…æŒ‡å—

> åŸæ–‡ï¼š<https://towardsdatascience.com/beginners-guide-for-data-cleaning-and-feature-extraction-in-nlp-756f311d8083?source=collection_archive---------24----------------------->

![](img/364e648afb51a17e16e5959c25fae0fe.png)

æ¥æº:[è±å¼—å‰å°¼äºšå·](https://www.shutterstock.com/g/ievgeniiya)é€”å¾„:[æ‘„å½±è®°è€…](https://www.shutterstock.com/image-illustration/painted-hand-shows-concept-hologram-data-1078657103)

æœ¬æ–‡å°†è§£é‡Šä½¿ç”¨ç¥ç»è¯­è¨€å¤„ç†(NLP)è¿›è¡Œæ–‡æœ¬åˆ†æçš„æ•°æ®æ¸…ç†å’Œæœªæ¥æå–çš„æ­¥éª¤ã€‚

åœ¨ç½‘ä¸Šï¼Œæœ‰å¾ˆå¤šå¾ˆæ£’çš„æ–‡å­—æ¸…ç†æŒ‡å—ã€‚ä¸€äº›æŒ‡å—åœ¨æ–‡æœ¬æ¸…ç†ä¹‹åè¿›è¡Œç‰¹å¾æå–ï¼Œè€Œä¸€äº›æŒ‡å—åœ¨æ–‡æœ¬æ¸…ç†ä¹‹å‰è¿›è¡Œç‰¹å¾æå–ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å¾ˆå¥½ã€‚ç„¶è€Œï¼Œ**è¿™é‡Œæœ‰ä¸€ä¸ªå¾ˆå°‘è¢«å…³æ³¨çš„é—®é¢˜**:åœ¨æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†ä¸€äº›å¯èƒ½çš„ç‰¹å¾(å˜é‡)ã€‚åœ¨æ•°æ®æ¸…æ´—ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œç‰¹å¾æå–ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ‰äº›ç‰¹å¾åªæœ‰åœ¨æ•°æ®æ¸…ç†åæå–æ—¶æ‰æœ‰æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åœ¨æ•°æ®æ¸…æ´—åè¿›è¡Œç‰¹å¾æå–ã€‚æœ¬ç ”ç©¶å…³æ³¨è¿™ä¸€ç‚¹**ã€**å’Œ**è¿™æ˜¯æœ¬ç ”ç©¶çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚**

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¾æ¬¡éµå¾ªä¸‰ä¸ªæ­¥éª¤:

1.  ç‰¹å¾æå–â€”ç¬¬ä¸€è½®
2.  æ•°æ®æ¸…ç†
3.  ç‰¹å¾æå–â€”ç¬¬äºŒè½®

è¿™ç¯‡ç ”ç©¶æ–‡ç« æ˜¯é‡‡ç”¨ NLP æ–¹æ³•çš„äºšé©¬é€Šç»¼è¿°åˆ†æçš„ä¸€éƒ¨åˆ†ã€‚è¿™é‡Œæ˜¯ä¸»ç ”ç©¶ä»£ç çš„ Colab ç¬”è®°æœ¬çš„ [my GitHub repo](https://github.com/EnesGokceDS/Amazon_Reviews_NLP_Capstone_Project) ï¼Œä»¥åŠæœ¬æ¬¡ç ”ç©¶çš„ [**ä»£ç **](https://github.com/EnesGokceDS/Amazon_Reviews_NLP_Capstone_Project/blob/master/1_Data_cleaning_and_feature_extraction.ipynb) ã€‚

**å…³äºæˆ‘ä½¿ç”¨çš„æ•°æ®çš„ç®€è¦ä¿¡æ¯:**æœ¬é¡¹ç›®ä½¿ç”¨çš„æ•°æ®æ˜¯ä» [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews) ä¸‹è½½çš„ã€‚æ˜¯æ–¯å¦ç¦ç½‘ç»œåˆ†æé¡¹ç›®ä¸Šä¼ çš„ã€‚åŸå§‹æ•°æ®æ¥è‡ª J. McAuley å’Œ J. Leskovec (2013)å¯¹â€œ[ä»ä¸šä½™çˆ±å¥½è€…åˆ°è¡Œå®¶:é€šè¿‡åœ¨çº¿è¯„è®º](http://i.stanford.edu/~julian/pdfs/www13.pdf)å¯¹ç”¨æˆ·ä¸“ä¸šçŸ¥è¯†çš„æ¼”å˜è¿›è¡Œå»ºæ¨¡â€çš„ç ”ç©¶ã€‚è¿™ä¸ªæ•°æ®é›†ç”±æ¥è‡ªäºšé©¬é€Šçš„ç¾é£Ÿè¯„è®ºç»„æˆã€‚è¯¥æ•°æ®åŒ…æ‹¬äº†ä» 1999 å¹´åˆ° 2012 å¹´çš„æ‰€æœ‰ 568ï¼Œ454 ç¯‡è¯„è®ºã€‚è¯„è®ºåŒ…æ‹¬äº§å“å’Œç”¨æˆ·ä¿¡æ¯ã€è¯„çº§å’Œçº¯æ–‡æœ¬è¯„è®ºã€‚

# **ç‰¹å¾æå–â€”ç¬¬ä¸€è½®**

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œå°†æå–æ•°æ®æ¸…æ´—åä¸å¯èƒ½è·å¾—çš„ç‰¹å¾ã€‚

1.  **åœç”¨è¯çš„æ•°é‡:**åœç”¨è¯æ˜¯æœç´¢å¼•æ“å·²ç»è¢«ç¼–ç¨‹å¿½ç•¥çš„å¸¸ç”¨è¯(ä¾‹å¦‚â€œtheâ€ã€â€œAâ€ã€â€œanâ€ã€â€œinâ€)ï¼Œæ— è®ºæ˜¯åœ¨ç´¢å¼•ç”¨äºæœç´¢çš„æ¡ç›®æ—¶è¿˜æ˜¯åœ¨ä½œä¸ºæœç´¢æŸ¥è¯¢çš„ç»“æœæ£€ç´¢å®ƒä»¬æ—¶ã€‚åœ¨ Python çš„ ***nltk*** åŒ…ä¸­ï¼Œæœ‰ 127 ä¸ªè‹±æ–‡åœç”¨è¯é»˜è®¤ã€‚é€šè¿‡åº”ç”¨åœç”¨è¯ï¼Œè¿™ 127 ä¸ªè¯è¢«å¿½ç•¥ã€‚åœ¨åˆ é™¤åœç”¨è¯ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°†â€œåœç”¨è¯æ•°é‡â€ä½œä¸ºä¸€ä¸ªå˜é‡ã€‚

```
df['stopwords'] = df['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))df[['Text','stopwords']].head()
```

![](img/49cd80fb70b054fbc5286f98f36048c9.png)

2.**æ ‡ç‚¹ç¬¦å·çš„æ•°é‡:**æ•°æ®æ¸…æ´—åæ— æ³•è·å¾—çš„å¦ä¸€ä¸ªç‰¹å¾æ˜¯å› ä¸ºå‘éŸ³ä¼šè¢«åˆ é™¤ã€‚

```
def count_punct(text):
    count = sum([1 for char in text if char in string.punctuation])
    return count#Apply the defined function on the text data
df['punctuation'] = df['Text'].apply(lambda x: count_punct(x))#Let's check the dataset
df[['Text','punctuation']].head()
```

![](img/d9f0dd6028c99e97167d9e509c2536ff.png)

3.æ ‡ç­¾å­—ç¬¦çš„æ•°é‡:æˆ‘ä»¬å¯ä»¥ä»æ–‡æœ¬æ•°æ®ä¸­æå–çš„ä¸€ä¸ªæ›´æœ‰è¶£çš„ç‰¹å¾æ˜¯æ ‡ç­¾æˆ–æåŠçš„æ•°é‡ã€‚åœ¨æ•°æ®æ¸…ç†æœŸé—´ï¼Œæ ‡ç­¾å°†è¢«åˆ é™¤ï¼Œæˆ‘ä»¬å°†æ— æ³•è®¿é—®è¿™äº›ä¿¡æ¯ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬åœ¨ä»ç„¶å¯ä»¥è®¿é—®å®ƒçš„æ—¶å€™æå–è¿™ä¸ªç‰¹æ€§ã€‚

```
df['hastags'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))df[['Text','hastags']].head()
```

![](img/031aeeedcabbfc32aa83af7bb16bb193.png)

4.**æ•°å­—å­—ç¬¦çš„æ•°é‡:**æ‹¥æœ‰è¯„è®ºä¸­å‡ºç°çš„æ•°å­—å­—ç¬¦çš„æ•°é‡å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚

```
df['numerics'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))df[['Text','numerics']].head()
```

![](img/5f6cdb4855c20ef0086e05bb143f74c2.png)

5.**å¤§å†™å•è¯çš„æ•°é‡:**æ„¤æ€’ã€æ„¤æ€’ç­‰æƒ…ç»ªç»å¸¸é€šè¿‡å¤§å†™å•è¯æ¥è¡¨è¾¾ï¼Œè¿™ä½¿å¾—è¿™æˆä¸ºè¯†åˆ«è¿™äº›å•è¯çš„å¿…è¦æ“ä½œã€‚åœ¨æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰å­—æ¯å°†è¢«è½¬æ¢æˆå°å†™ã€‚

```
df['upper'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))df[['Text','upper']].head()
```

![](img/fbb6d02d555c069f928d2a3ad126b606.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å®Œæˆäº†åªèƒ½åœ¨æ•°æ®æ¸…ç†ä¹‹å‰è·å¾—çš„ç‰¹å¾ã€‚æˆ‘ä»¬å‡†å¤‡æ¸…é™¤æ•°æ®ã€‚

# **æ–‡å­—æ¸…ç†æŠ€å·§**

åœ¨å¯¹æ•°æ®åº”ç”¨ NLP æŠ€æœ¯ä¹‹å‰ï¼Œé¦–å…ˆéœ€è¦æ¸…ç†æ•°æ®å¹¶ä¸ºåˆ†æå‡†å¤‡æ•°æ®ã€‚å¦‚æœè¿™ä¸ªè¿‡ç¨‹åšå¾—ä¸æ­£ç¡®ï¼Œå®ƒå¯èƒ½ä¼šå®Œå…¨ç ´ååˆ†æéƒ¨åˆ†ã€‚ä»¥ä¸‹æ˜¯åº”ç”¨äºæ•°æ®çš„æ­¥éª¤:

1.  **å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢æˆå°å†™:**ç¬¬ä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤æ˜¯å°†è¯„è®ºè½¬æ¢æˆå°å†™ã€‚è¿™é¿å…äº†ç›¸åŒå•è¯çš„å¤šä¸ªå‰¯æœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—å­—æ•°æ—¶ï¼Œå¦‚æœæˆ‘ä»¬å¿½ç•¥è¿™ç§è½¬æ¢ï¼Œåˆ™â€œç‹—â€å’Œâ€œç‹—â€å°†è¢«è§†ä¸ºä¸åŒçš„å•è¯ã€‚

```
df['Text'] = df['Text'].apply(lambda x: " ".join(x.lower() for x in x.split()))df['Text'].head()
```

**2)** ç›®å‰ï¼ŒNLP æ–¹æ³•è¿˜æ²¡æœ‰ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–¹æ³•æ¥åˆ†ææ ‡ç‚¹ç¬¦å·ã€‚å› æ­¤ï¼Œå®ƒä»¬è¢«ä»æ–‡æœ¬æ•°æ®ä¸­åˆ é™¤ã€‚é€šè¿‡è¿™ä¸€æ­¥ï¼Œè¿™äº›å­—ç¬¦è¢«åˆ é™¤:[ï¼" #$% & '()*+ï¼Œ-ã€‚/:;= >ï¼Ÿ@[\]^_`{|}~]

```
df['Text'] = df['Text'].apply(lambda x: " ".join(x.lower() for x in df['Text'] = df['Text'].str.replace('[^\w\s]','')
df['Text'].head()
```

**3)** **åœç”¨è¯çš„ç§»é™¤**:é€šè¿‡è¿™ä¸€æ­¥ï¼Œæˆ‘ç§»é™¤äº† *nltk* åŒ…ä¸­æ‰€æœ‰é»˜è®¤çš„è‹±æ–‡åœç”¨è¯ã€‚

```
from nltk.corpus import stopwords
stop = stopwords.words('english')df['Text'] = df['Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))df['Text'].sample(10)
```

***æ·»åŠ è‡ªå·±çš„åœç”¨è¯*** :æ­¤æ—¶ï¼Œä½ å¯èƒ½æƒ³æ·»åŠ è‡ªå·±çš„åœç”¨è¯ã€‚æˆ‘è¿™æ ·åšä¸»è¦æ˜¯åœ¨æ£€æŸ¥äº†æœ€å¸¸ç”¨çš„å•è¯ä¹‹åã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·æ£€æŸ¥æœ€å¸¸ç”¨çš„å•è¯:

```
import pandas as pd
freq = pd.Series(' '.join(df['Text']).split()).value_counts()[:20]
freq
```

![](img/30649be09dfd53e9b2eb433d8cf254f0.png)

æœ€å¸¸è§çš„ 20 ä¸ªå•è¯

ä»è¿™å‡ ä¸ªè¯ä¸­ï¼Œæˆ‘æƒ³å»æ‰' br 'ï¼Œ' get 'ï¼Œ' also 'ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å¤ªå¤§æ„ä¹‰ã€‚è®©æˆ‘ä»¬å°†å®ƒä»¬æ·»åŠ åˆ°åœç”¨è¯åˆ—è¡¨ä¸­:

```
# Adding common words from our document to stop_wordsadd_words = ["br", "get", "also"]
stop_words = set(stopwords.words("english"))
stop_added = stop_words.union(add_words)df['Text'] = df['Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop_added))df['Text'].sample(10)
```

æ³¨æ„:åœ¨å…¶ä»–æŒ‡å—ä¸­ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ° TF-IDF æ–¹æ³•ã€‚TF-IDF æ˜¯ä»æ–‡æœ¬æ•°æ®ä¸­å»é™¤æ²¡æœ‰è¯­ä¹‰ä»·å€¼çš„å•è¯çš„å¦ä¸€ç§æ–¹æ³•ã€‚å¦‚æœä½ ç”¨çš„æ˜¯ TF-IDFï¼Œå°±ä¸éœ€è¦åº”ç”¨åœç”¨è¯(ä½†æ˜¯ä¸¤ä¸ªéƒ½åº”ç”¨ä¹Ÿæ— å¦¨)ã€‚

**4)** **ç§»é™¤ URL:**URL æ˜¯è¢«ç§»é™¤çš„æ•°æ®ä¸­çš„å¦ä¸€ä¸ªå™ªéŸ³ã€‚

```
def remove_url(text): 
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', text)# remove all urls from df
import re
import string
df['Text'] = df['Text'].apply(lambda x: remove_url(x))
```

**5)** **å»é™¤ html HTML æ ‡ç­¾:** HTML åœ¨äº’è”ç½‘ä¸Šè¢«å¹¿æ³›ä½¿ç”¨ã€‚ä½†æ˜¯ HTML æ ‡ç­¾æœ¬èº«åœ¨å¤„ç†æ–‡æœ¬æ—¶å¹¶æ²¡æœ‰ä»€ä¹ˆå¸®åŠ©ã€‚å› æ­¤ï¼Œæ‰€æœ‰ä»¥ url å¼€å¤´çš„æ–‡æœ¬éƒ½å°†è¢«åˆ é™¤ã€‚

```
def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)# remove all html tags from df
df['Text'] = df['Text'].apply(lambda x: remove_html(x))
```

**6)** **åˆ é™¤è¡¨æƒ…ç¬¦å·:**è¡¨æƒ…ç¬¦å·å¯ä»¥æ˜¯ä¸å®¢æˆ·æ»¡æ„åº¦ç›¸å…³çš„ä¸€äº›æƒ…ç»ªçš„æŒ‡ç¤ºå™¨ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„æ–‡æœ¬åˆ†æä¸­åˆ é™¤è¡¨æƒ…ç¬¦å·ï¼Œå› ä¸ºç›®å‰è¿˜ä¸èƒ½ç”¨ NLP åˆ†æè¡¨æƒ…ç¬¦å·ã€‚

```
# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304bdef remove_emoji(text): 
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)#Example
remove_emoji("Omg another Earthquake ğŸ˜”ğŸ˜”")
```

![](img/6be3573c11f6a2fae518b45f5a0b053c.png)

```
# remove all emojis from df
df['Text'] = df['Text'].apply(lambda x: remove_emoji(x))
```

**7)** **ç§»é™¤è¡¨æƒ…ç¬¦å·:**è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

:-)æ˜¯è¡¨æƒ…ç¬¦å·

ğŸ˜œæ˜¯ä¸€ä¸ªâ†’è¡¨æƒ…ç¬¦å·ã€‚

```
!pip install emot #This may be required for the Colab notebookfrom emot.emo_unicode import UNICODE_EMO, EMOTICONS# Function for removing emoticons
def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in   EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text)#Example
remove_emoticons("Hello :-)")
```

![](img/6a3c6d1109f2269823be55b9da133f9e.png)

```
df['Text'] = df['Text'].apply(lambda x: remove_emoticons(x))
```

**8)** **æ‹¼å†™çº æ­£:**äºšé©¬é€Šè¯„è®ºä¸Šï¼Œæ‹¼å†™é”™è¯¯å¤šå¦‚ç‰›æ¯›ã€‚äº§å“è¯„è®ºæœ‰æ—¶å……æ»¡äº†åŒ†å¿™å‘é€çš„è¯„è®ºï¼Œæœ‰æ—¶å‡ ä¹æ— æ³•è¾¨è®¤ã€‚

åœ¨è¿™æ–¹é¢ï¼Œæ‹¼å†™çº æ­£æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå› ä¸ºè¿™ä¹Ÿå°†æœ‰åŠ©äºæˆ‘ä»¬å‡å°‘å•è¯çš„å¤šä¸ªå‰¯æœ¬ã€‚ä¾‹å¦‚ï¼Œâ€œåˆ†æâ€å’Œâ€œåˆ†æâ€å°†è¢«è§†ä¸ºä¸åŒçš„è¯ï¼Œå³ä½¿å®ƒä»¬åœ¨åŒä¸€æ„ä¹‰ä¸Šä½¿ç”¨ã€‚

```
from textblob import TextBlob
df['Text'][:5].apply(lambda x: str(TextBlob(x).correct()))
```

9.**è¯æ¡é‡Šä¹‰:** *è¯æ¡é‡Šä¹‰*æ˜¯å°†ä¸€ä¸ªå•è¯è½¬æ¢æˆå…¶åŸºæœ¬å½¢å¼çš„è¿‡ç¨‹ã€‚è¯æ±‡åŒ–è€ƒè™‘ä¸Šä¸‹æ–‡ï¼Œå°†å•è¯è½¬æ¢æˆæœ‰æ„ä¹‰çš„åŸºæœ¬å½¢å¼ã€‚ä¾‹å¦‚:

â€œå…³æ€€â€->â€œè¯åŒ–â€->â€œå…³æ€€â€

Python NLTK æä¾›äº† **WordNet è¯æ¡æ•´ç†å™¨**ï¼Œå®ƒä½¿ç”¨ WordNet æ•°æ®åº“æ¥æŸ¥æ‰¾å•è¯çš„è¯æ¡ã€‚

```
import nltk
from nltk.stem import WordNetLemmatizer 

# Init the Wordnet Lemmatizer
lemmatizer = WordNetLemmatizer()df['Text'] = df['Text'].apply(lambda x: lemmatizer(x))
```

å…³äºè¯æ±‡åŒ–çš„æ›´è¯¦ç»†çš„èƒŒæ™¯ï¼Œå¯ä»¥æŸ¥çœ‹ [Datacamp](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python) ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘å°†åœæ­¢æ¸…ç†æ•°æ®ã€‚ä½†æ˜¯ï¼Œä½œä¸ºä¸€åç ”ç©¶äººå‘˜ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®æ‚¨çš„æ•°æ®è¿›è¡Œæ›´å¤šçš„æ–‡æœ¬æ¸…ç†ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨:

âš«å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè¯å¹²åˆ†æ

âš«æ‹¼å†™çº æ­£çš„æ›¿ä»£æ–¹æ³•*:å­¤ç«‹è¯*çº æ­£å’Œ*ä¸Šä¸‹æ–‡ç›¸å…³*çº æ­£æ–¹æ³•

âš«ä¸åŒçš„åŒ…ä½¿ç”¨ä¸åŒæ•°é‡çš„åœç”¨è¯ã€‚ä½ å¯ä»¥è¯•è¯•å…¶ä»–çš„ NLP åŒ…ã€‚

# **ç‰¹å¾æå–-ç¬¬äºŒè½®**

ä¸€äº›ç‰¹å¾å°†åœ¨æ–‡æœ¬æ¸…ç†åæå–ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ­¤æ­¥éª¤è·å¾—æ›´æœ‰æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ•°æ®æ¸…ç†ä¹‹å‰æå–è¿™ä¸ªç‰¹å¾ï¼Œå­—ç¬¦çš„æ•°é‡ä¼šå—åˆ° URL é“¾æ¥çš„ä¸¥é‡å½±å“ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å°è¯•æå–å°½å¯èƒ½å¤šçš„ç‰¹å¾ï¼Œå› ä¸ºé¢å¤–çš„ç‰¹å¾æœ‰æœºä¼šåœ¨æ–‡æœ¬åˆ†ææœŸé—´æä¾›æœ‰ç”¨çš„ä¿¡æ¯ã€‚æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒè¿™äº›åŠŸèƒ½å°†æ¥æ˜¯å¦çœŸçš„æœ‰ç”¨ã€‚åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨å®ƒä»¬ã€‚

1.  **å­—æ•°:**æ­¤åŠŸèƒ½å‘ŠçŸ¥è¯„è®ºä¸­æœ‰å¤šå°‘å­—

```
df['word_count'] = df['Text'].apply(lambda x: len(str(x).split(" ")))df[['Text','word_count']].head()
```

2.**å­—æ•°:**è¯„è®ºä¸­åŒ…å«å¤šå°‘ä¸ªå­—æ¯ã€‚

```
df['char_count'] = df['Text'].str.len() ## this also includes spacesdf[['Text','char_count']].head()
```

3.**å¹³å‡å•è¯é•¿åº¦:**è¯„è®ºä¸­å•è¯çš„å¹³å‡å­—æ¯æ•°ã€‚

```
def avg_word(sentence):
    words = sentence.split()
    return (sum(len(word) for word in words)/(len(words)+0.000001))df['avg_word'] = df['Text'].apply(lambda x: avg_word(x)).round(1)
df[['Text','avg_word']].head()
```

![](img/d06207b74d0444454bf0aa959bb2ceb8.png)

è®©æˆ‘ä»¬æ£€æŸ¥æå–çš„è¦ç´ åœ¨æ•°æ®æ¡†ä¸­çš„æ ·å­:

```
df.sample(5)
```

![](img/129d820ebb7f101620d2e551d06ad149.png)

# **ç»“è®º**

è¿™é¡¹ç ”ç©¶è§£é‡Šäº†æ–‡æœ¬æ¸…æ´—çš„æ­¥éª¤ã€‚æ­¤å¤–ï¼Œ***æœ¬æŒ‡å—çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œåœ¨æ–‡æœ¬æ¸…ç†ä¹‹å‰å’Œæ–‡æœ¬æ¸…ç†ä¹‹åï¼Œé€šè¿‡ä¸¤è½®*** *:* ***æ¥å®Œæˆç‰¹å¾æå–ã€‚æˆ‘ä»¬éœ€è¦è®°ä½ï¼Œå¯¹äºå®é™…çš„ç ”ç©¶æ¥è¯´ï¼Œæ–‡æœ¬æ¸…ç†æ˜¯ä¸€ä¸ªé€’å½’è¿‡ç¨‹ã€‚ä¸€æ—¦æˆ‘ä»¬å‘ç°å¼‚å¸¸ï¼Œæˆ‘ä»¬ä¼šå›æ¥é€šè¿‡è§£å†³å¼‚å¸¸è¿›è¡Œæ›´å¤šçš„æ¸…ç†ã€‚***

> *ç‰¹åˆ«æ„Ÿè°¢æˆ‘çš„æœ‹å‹å¡”æ¯”ç‘ŸÂ·æ–¯è’‚å…‹å°”æ ¡å¯¹äº†è¿™ç¯‡æ–‡ç« ã€‚