# 源于我们偏见的算法

> 原文：<https://towardsdatascience.com/algorithms-born-of-our-prejudices-8be47ca6103c?source=collection_archive---------61----------------------->

## 算法有辨别能力吗？恐怕是的。让问题变得复杂的是，算法开发者很难被指控有恶意。那么，一个数学公式怎么会让个人和社区处于危险之中呢？

![](img/fc6aa2f5f23374097c6b955a0471659e.png)

马库斯·斯皮斯克在 [Unsplash](https://unsplash.com/s/photos/programming?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

尽管数学方程看起来遥远而冷漠，但它们通常也与可靠的硬科学联系在一起。然而，时不时地，事实证明一系列数字和符号隐藏着更不祥的潜在危险。是什么导致应用程序变坏，而这些应用程序原本是为好的事业服务的？可能有很多原因。首先浮现在脑海中的一个问题与人性有关。众所周知，人们遵循一种熟悉的机制，让刻板印象和偏见指导他们的生活。他们将它们应用于其他个人、社会团体和价值体系。这种认知模式很容易被缺乏想象力和不愿意给予事情适当的考虑所驱动。由此产生的爆炸性混合物会产生负面后果。盲目相信计算机数据的人看不到情况的复杂性，容易放弃对事件的主观评估。一旦发生这种情况，不幸的事情就会发生，给每个相关的人带来巨大的问题。我们的无知和算法越来越大的自主性(事实证明算法远非万无一失)产生了一个令人不安的混合体。

# 为警察服务的算法

警察非常适合测试智能技术。这种技术有其独特之处，业界非常清楚一个有用的算法有时会引发问题。但是让我们公平一点。智能数据处理允许警方计算机有效地将犯罪、历史数据和环境分组到类别和数据集。毫无疑问，应用程序有助于将地点、人物、心理特征、犯罪时间和使用的工具联系起来。[](http://www-07.ibm.com/innovation/my/exhibit/documents/pdf/IBM_SPSS_Memphis_Police_Department.pdf)**孟菲斯大学的犯罪学家和数据处理学者选择使用 IBM 设计的预测分析软件。该项目团队创建了一个分析机制，该机制考虑了气温、当地地理、人口分布、商店和餐馆的位置、居民偏好和犯罪统计等变量。底层算法使用这些变量来识别城市中的潜在爆发点。它们确实有效。对该系统的测试表明，确实有可能在一定程度上预测未来，尽管没有给出具体的程度。然而，这种确定性足以证明向以这种方式确定的“高风险”地区派遣警察是合理的。还有人声称，这有助于将警方从报告事件那一刻起的反应时间缩短三分之一。我只能想象在这样的地方仅仅有警察在场就能阻止犯罪活动。虽然这个例子对于外行人来说可能难以理解，但它证明了现代技术提供了具有产生惊人结果潜力的“爆炸性”创新。**

# **当计算机出错时**

**创业公司 Azavea 的 HunchLab 系统已经在美国推出，该系统筛选各种类型的海量数据(包括月相)，以帮助警方调查犯罪。和前面的例子一样，这个想法是创建一个犯罪发生概率特别高的地点的地图。该计划的重点是整个城市的酒吧，学校和公共汽车站的位置。事实证明这很有帮助。虽然它的一些发现是显而易见的，但其他发现可能会令人惊讶。很容易解释为什么天气越冷犯罪越少。然而，要找到停在费城学校附近的汽车更容易被偷的原因却相当困难。没有这种软件的警察会想到去调查学校和汽车盗窃之间的联系吗？以上都是正面情景。然而，很难接受的事实是，智能机器不仅在处理过程中出错，而且还会导致错误的解释。通常，他们无法理解情境背景。不是完全不像人。**

# **软件不可靠的可信度**

**2016 年，联合调查记者的独立新闻编辑室 [**ProPublica 发表了“机器偏见”**](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) 一文，内容是美国法院使用 Northpointe 的专业软件对罪犯进行侧写。文章指出，该软件旨在评估有前科的人再次犯罪的可能性，事实证明该软件很受美国法官的欢迎。Northpointe tool 估计黑人罪犯再次犯罪的可能性为 45%。与此同时，白人再次犯罪的风险为 24%。为了得出这些有趣的结论，算法假设黑人居住区比白人居住区有更高的犯罪行为风险。软件传播的假设受到了质疑，最终结束了 Northpointe 软件套件的分析生涯。问题的根本原因在于仅仅根据历史数据进行评估，以及缺乏认识，或者说未能设计出算法来说明最新的人口趋势。**

# **算法和白脸**

**凯茜·奥尼尔(Cathy O'Neil)在她 2016 年的著作《数学毁灭的武器》中，探索了一个有趣的假设，即算法极大地影响了人们生活的各个领域。她指出，人们倾向于过于相信数学模型。她声称，这导致了以多种方式和多种层次形成的偏见。她说，偏见很早就产生了，甚至在算法用于分析的数据被收集之前。亚马逊的经理们也发现了同样的机制。他们注意到他们使用的招聘程序经常歧视女性。在寻找有前途的人时，女性总是占少数。是什么导致了偏差？对历史数据的依赖表明有更多的男性申请特定的职位。这破坏了就业中的性别平等，使天平向有利于男性的方向倾斜，最终导致了有偏见的就业政策的制定。**

# **算法没有改变文化**

**上述评估软件是建立在基于性别的严重不平等困扰就业的时代开发的算法基础上的。那个特定时刻的特点是男性人数过多。在历史数据的基础上训练出来的算法致力于世界没有改变的“信念”。这意味着他们的假设和简化(如黑人意味着更高的犯罪概率，男性更有可能成为优秀的专业人士)是被误导的。**

# **令人不安的问题**

**如果你认为类似上述的机制在职业和个人生活中可能很普遍，你可能会有所发现。有多少情况是我们不知道的，在这些情况下，数据是根据错误的假设组织起来的？算法未能解释经济和文化变化的频率有多高？**

**黑匣子是一个术语，用来指人类面对人工智能“大脑”中发生的事情时的无助。我们的无知和算法越来越大的自主性(事实证明算法远非万无一失)产生了一个令人不安的混合体。算法的偏见不会随着魔杖的挥动而消失。因此，关键问题是，他们的开发人员(他们经常自己完成设计和培训工作)是否会承担起这项任务，并意识到人类的偏见和行为模式会多么容易影响软件。**

****作品引用**:**

**IBM，孟菲斯警察局，IBM SPSS:孟菲斯警察局，详细 ROI 案例研究， [**链接**](http://www-07.ibm.com/innovation/my/exhibit/documents/pdf/IBM_SPSS_Memphis_Police_Department.pdf) ，2015。**

**《边缘》,莫里斯·查马，马克·汉森补充报道,《维持未来》。弗格森事件后，圣路易斯警察采用犯罪预测软件， [**链接**](https://www.theverge.com/2016/2/3/10895804/st-louis-police-hunchlab-predictive-policing-marshall-project) ，2018。**

**ProPublica，机器偏见:全国各地都有用来预测未来罪犯的软件。而且是对黑人的偏见，作者茱莉亚·安格温(Julia Angwin)、杰夫·拉森(Jeff Larson)、苏亚·马特图(Surya Mattu)和劳伦·基什内尔(Lauren Kirchner)，ProPublica、 [**链接**](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) ，2018。**

****相关文章:****

**–[**像机器一样学习，如果不努力的话**](https://norbertbiedrzycki.pl/en/learn-like-a-machine-if-not-harder/)**

**–[**我们与机器对话的时间**](https://norbertbiedrzycki.pl/en/time-we-talked-to-our-machines/)**

**–[**算法会犯战争罪吗？**](https://norbertbiedrzycki.pl/en/will-algorithms-commit-war-crimes/)**

**–[**机器，你什么时候才能学会和我做爱？**](https://norbertbiedrzycki.pl/en/machine-when-will-you-learn-to-make-love-to-me/)**

**–[**你好。你还是人类吗？**](https://norbertbiedrzycki.pl/en/hello-are-you-still-human/)**

**–[**人工智能是新电**](https://norbertbiedrzycki.pl/en/artificial-intelligence-is-new-electricity/)**

**–[**机器如何思考**](https://norbertbiedrzycki.pl/en/how-machines-think-2/)**