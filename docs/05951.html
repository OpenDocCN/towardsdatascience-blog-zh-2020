<html>
<head>
<title>Boost Machine Learning Performance by 30% with Normalization and Standardization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过标准化和规范化将机器学习性能提高 30%</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boost-machine-learning-performance-by-30-with-normalization-and-standardization-156adfbf215b?source=collection_archive---------44-----------------------#2020-05-15">https://towardsdatascience.com/boost-machine-learning-performance-by-30-with-normalization-and-standardization-156adfbf215b?source=collection_archive---------44-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2b22" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如果您不扩展您的数据，就等于放弃了性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c3122c8a4da60e9b1cda4db5e68632f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyOqen1tq1cAWGZLdpStcw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/photo/london-new-york-tokyo-and-moscow-clocks-48770/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>的<a class="ae ky" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure><p id="aee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预处理是机器学习管道中最被低估的方面。</p><p id="3479" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">特征缩放</strong>，将特征值控制在相似的数字尺度上，是其中的关键组成部分。</p><p id="2521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当数据没有缩放时，计算距离或相似性的模型表现不佳。这包括 KNN 和 SVM。</p><p id="80de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将演示 KNN 的缩放数据如何将准确度从 72%提高到 96%。</p><p id="9cbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不扩展输入数据，您就放弃了轻松的性能提升。</p><p id="edd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我们将用 3 种方法解决一个简单的 ML 问题，比较结果，然后解释为什么缩放有效:</strong> <br/> 1)无缩放<br/> 2)缩放:标准化<br/> 3)缩放:标准化<br/> 4)为什么缩放可以提高性能</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9b72" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">基本情况</h1><p id="b1e8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在这里，我们将解决一个没有任何缩放的分类问题。</p><p id="e948" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<a class="ae ky" href="https://www.kaggle.com/brynja/wineuci" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载葡萄酒数据集。这是一个简单的数据集，我们在其中预测葡萄酒的品种(例如:cab sauv，merlot，riesling)。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="dfd8" class="ne md it na b gy nf ng l nh ni">import pandas as pd</span><span id="fb62" class="ne md it na b gy nj ng l nh ni">import numpy as np<br/>import pandas as pd</span><span id="c1a9" class="ne md it na b gy nj ng l nh ni">df = pd.read_csv('Wine.csv', names=[<br/>    'Class', 'Alcohol', 'Malic acid',<br/>    'Ash', 'Alcalinity of ash', 'Magnesium',<br/>    'Total phenols', 'Flavanoids',<br/>    'Nonflavanoid phenols', 'Proanthocyanins',<br/>    'Color intensity', 'Hue',<br/>    'OD280/OD315 of diluted wines', 'Proline'])</span><span id="93ea" class="ne md it na b gy nj ng l nh ni">df.iloc[np.r_[0:2, -2:0]]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/fadac4efa829c05c4a51f64378a43b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73J_h7uAGUEzX0A7r23ZTQ.png"/></div></div></figure><p id="9576" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速检查数据的形状。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8714" class="ne md it na b gy nf ng l nh ni">df.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/b973c23e2fb16f3ff4854d4f554a9ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EPdKreIzSkNt5xk5-DgFQ.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="af13" class="ne md it na b gy nf ng l nh ni">df.hist(figsize=(25, 15))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/fa06c7e8bcdd6efbe8f17086f1522dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vepsz-KPINlGEdR7f6qkpA.png"/></div></div></figure><p id="f289" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">观察特征在完全不同的比例上。<strong class="lb iu">酒精</strong>从<code class="fe nn no np na b">11.03</code>到<code class="fe nn no np na b">14.83</code>不等，而<strong class="lb iu">非黄酮类酚类</strong>从<code class="fe nn no np na b">0.13</code>到<code class="fe nn no np na b">0.66</code>不等。</p><p id="ed17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这应该会给你数据科学中的<a class="ae ky" href="https://en.wikipedia.org/wiki/Code_smell" rel="noopener ugc nofollow" target="_blank">代码味道</a>。不同的比例会混淆模型。</p><p id="d4e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是让我们用逻辑回归来分析一下。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e753" class="ne md it na b gy nf ng l nh ni"># helper functions</span><span id="af08" class="ne md it na b gy nj ng l nh ni">import numpy as np<br/>from sklearn.model_selection import StratifiedShuffleSplit<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="fc13" class="ne md it na b gy nj ng l nh ni">def dataframe_to_X_and_y(df):<br/>    X = np.asarray(<br/>        df[['Alcohol','Malic acid','Ash',<br/>            'Alcalinity of ash','Magnesium',<br/>            'Total phenols','Flavanoids',<br/>            'Nonflavanoid phenols',<br/>            'Proanthocyanins','Color intensity',<br/>            'Hue','OD280/OD315 of diluted wines',<br/>            'Proline']])<br/>    <br/>    y = np.asarray(df[['Class']])<br/>    <br/>    return X, y</span><span id="c4e5" class="ne md it na b gy nj ng l nh ni">def split_data(X, y):    <br/>    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)</span><span id="9e86" class="ne md it na b gy nj ng l nh ni">for train_index, test_index in splitter.split(X, y):<br/>        X_train, X_test = X[train_index], X[test_index]<br/>        y_train, y_test = y[train_index], y[test_index]</span><span id="752f" class="ne md it na b gy nj ng l nh ni">return X_train, X_test, y_train, y_test</span><span id="85f7" class="ne md it na b gy nj ng l nh ni">def fit_and_predict(X_train, y_train, X_test):<br/>    classifier = KNeighborsClassifier()<br/>    classifier.fit(X_train, y_train)<br/>    y_pred = classifier.predict(X_test)<br/>    <br/>    return y_pred</span></pre><p id="7f86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数据进行分类和评估。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="51df" class="ne md it na b gy nf ng l nh ni">X, y = dataframe_to_X_and_y(df)<br/>X_train, X_test, y_train, y_test = split_data(X, y)<br/>y_pred = fit_and_predict(X_train, y_train, X_test)</span><span id="f5e6" class="ne md it na b gy nj ng l nh ni">from sklearn.metrics import classification_report<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/b2f55fa5a546a66d63f6146a0e67c1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0jCLcUl0Nz9ReskAZxXgA.png"/></div></div></figure><p id="0ba2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致了 72%的准确率。不可怕，但我打赌我们可以通过扩展做得更好。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9f59" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">正常化</h1><h2 id="d7db" class="ne md it bd me nr ns dn mi nt nu dp mm li nv nw mo lm nx ny mq lq nz oa ms ob bi translated">什么是正常化</h2><p id="531d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">归一化也叫做“最小-最大缩放”，但我认为“挤压”这个术语更直观。</p><p id="7c7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它强制数据在 0 和 1 之间的范围内，最低特征值为 0，最高特征值为 1。</p><p id="5d7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归一化后，所有要素的值都将在此范围内。例如，比较标准化前后的<code class="fe nn no np na b">Total phenols</code>数据。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ba6d" class="ne md it na b gy nf ng l nh ni">attribute = 'Total phenols'<br/>df_copy = df.copy()<br/>df_copy = df_copy[[attribute]]<br/>df_copy.hist()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/d8f22384c1048076abdaf9f7e080114f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOE8pZeZsp6Y2Db54-H94Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以前</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="fc7a" class="ne md it na b gy nf ng l nh ni">from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>df_copy[[attribute]] = scaler.fit_transform(df_copy[[attribute]])<br/>df_copy.hist()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/988fc15835f2871ad54d8eb8d3acafe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wxAG367r4OFm9hcMUSeLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在...之后</p></figure><p id="6647" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在后者中，没有值低于 0 或高于 1。</p><p id="8ee8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种缩放算法的一个缺点是，一个巨大的正异常值将变为 1，并将所有其他值挤到接近于 0。</p><p id="717a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<code class="fe nn no np na b">[1,2,10]</code>会变成<code class="fe nn no np na b">[0.00, 0.11 ,1.00]</code>。</p><h2 id="20b5" class="ne md it bd me nr ns dn mi nt nu dp mm li nv nw mo lm nx ny mq lq nz oa ms ob bi translated">分类问题</h2><p id="78c0" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">回到我们上面解决的那个分类问题。这一次，我们将在进行预测之前对数据进行归一化处理。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="58f2" class="ne md it na b gy nf ng l nh ni">df_copy = df.copy()<br/>X, y = dataframe_to_X_and_y(df_copy)<br/>X_train, X_test, y_train, y_test = split_data(X, y)</span><span id="875e" class="ne md it na b gy nj ng l nh ni">from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span><span id="7fd7" class="ne md it na b gy nj ng l nh ni">y_pred = fit_and_predict(X_train, y_train, X_test)</span><span id="ab6e" class="ne md it na b gy nj ng l nh ni">from sklearn.metrics import classification_report<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/20dac71607762fc7ed398fa2ae9b368f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q06MHiBzXPJprCvx5Rba_Q.png"/></div></div></figure><p id="c480" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">96%.这是一个巨大的进步！这是规范化数据帮助模型解决问题的一个很好的例子。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="327f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">标准化</h1><h2 id="6ac0" class="ne md it bd me nr ns dn mi nt nu dp mm li nv nw mo lm nx ny mq lq nz oa ms ob bi translated">什么是标准化？</h2><p id="08a6" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">标准化减去平均值，然后除以样本方差，因此平均值为零，分布具有单位方差。</p><blockquote class="od oe of"><p id="61f2" class="kz la og lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">z = (x — u) / s</p><p id="d489" class="kz la og lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">其中<code class="fe nn no np na b">u</code>是训练样本的均值，如果<code class="fe nn no np na b">with_mean=False</code>为 0，<code class="fe nn no np na b">s</code>是训练样本的标准差，如果<code class="fe nn no np na b">with_std=False</code>为 1。</p><p id="7963" class="kz la og lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">- <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> Sklearn 文档</a></p></blockquote><p id="6f64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意下面应用标准化前后的比例差异。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="efff" class="ne md it na b gy nf ng l nh ni">attribute = 'Hue'<br/>df_copy = df.copy()<br/>df_copy = df_copy[[attribute]]<br/>df_copy.hist()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/44a266215dc6c256ce623b76161de632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-upTXsktK9t42dXutjeGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以前</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7f38" class="ne md it na b gy nf ng l nh ni">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>df_copy[[attribute]] = scaler.fit_transform(df_copy[[attribute]])<br/>df_copy.hist()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c1b563a85401d012cd202fa2c58c6b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qne2y9gaguGKHqOsa7lGnw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在...之后</p></figure><h2 id="9ee7" class="ne md it bd me nr ns dn mi nt nu dp mm li nv nw mo lm nx ny mq lq nz oa ms ob bi translated">分类问题</h2><p id="6a5b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们解决同样的分类问题，但这一次我们将在进行预测之前对值进行标准化。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="89d5" class="ne md it na b gy nf ng l nh ni">df_copy = df.copy()<br/>X, y = dataframe_to_X_and_y(df_copy)<br/>X_train, X_test, y_train, y_test = split_data(X, y)</span><span id="513b" class="ne md it na b gy nj ng l nh ni">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span><span id="1b1f" class="ne md it na b gy nj ng l nh ni">y_pred = fit_and_predict(X_train, y_train, X_test)</span><span id="2c69" class="ne md it na b gy nj ng l nh ni">from sklearn.metrics import classification_report<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/235f61244c15ae7bae01818eaf5d61bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wIEXsV_Ga72vWSSVk0lp8A.png"/></div></div></figure><p id="78ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也做到了！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ab47" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">为什么扩展可以提高性能？</h1><p id="c5ed" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">缩放不会改进所有模型的预测。但是对于那些利用距离/相似性计算的人来说，这可能会有很大的不同。</p><p id="eb43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们正在根据购买价格(1000 美元)和行驶距离(公里)来比较二手车。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="1fda" class="ne md it na b gy nf ng l nh ni">pd.DataFrame(<br/>  {'purchase_price': [20, 25, 80], <br/>   'distance_driven': [10000,15000,11000]}<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/67cdd3a5e0fb808d7ef4011fe8e493be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVa30wOvZIacnnRo8IBfYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虚构数据</p></figure><p id="7e3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以看到一些距离的计算(虽然不是全部)会被<code class="fe nn no np na b">distance_driven</code>严重扭曲，因为它更大，即使<code class="fe nn no np na b">purchase_price</code>可能是相似性的更强指标。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="acd3" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="4e50" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们介绍了一个例子，其中缩放将 KNN 分类器的性能提高了 30%以上。虽然我们可以尝试其他模型，但扩展应该是标准做法。</p><p id="1867" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使缩放不能提高性能(例如:逻辑回归)，它也能使系数更容易解释。</p></div></div>    
</body>
</html>