# 优惠券推荐中的机器学习

> 原文：<https://towardsdatascience.com/machine-learning-in-coupon-recommendation-2bdae281d840?source=collection_archive---------21----------------------->

![](img/1e716faab0357e9a5d3d147472993df6.png)

演职员表:模拟世界

**多臂强盗如何帮助星巴克向顾客发送个性化的优惠。**

> 你多久会等一次优惠券来购买一件商品？在一些无趣的交易之后，你是否想放弃使用这项服务？

优惠券系统已被广泛用于提高消费者对基于数字的平台的参与度。通过向用户提供挑战和相应的奖励，公司的服务不仅变得更有吸引力，而且最重要的是，它可以使用户成为常客，从而增强品牌对其客户的影响。然而，知道提供哪种优惠券可能是一项相当复杂的任务，因为每个客户对每个优惠的反应都不同，经常向他们提供糟糕的交易可能会将他们从您的业务中拖走。

为了克服这个问题，可以使用机器学习技术来建立数据驱动的客户档案，并开发更好的优惠券推荐。就此而言，**这篇文章展示了如何在星巴克移动奖励应用程序中使用 K-Means 聚类结合多臂强盗来构建优惠券推荐系统。**

这篇文章是提交给 Udacity 的机器学习工程师 Nanodegree 的 capstone 项目的结果，其源代码可以在这个资源库中找到[。有关更多技术信息，如参数扫描和数据集预处理，请参考关于存储库的详细报告。](https://github.com/guedes-joaofelipe/ml-engineer-capstone)

所以事不宜迟，言归正传！

![](img/bd94296fe9e6574a749b37c7c7734e22.png)

学分:免费

# **星巴克奖励手机 App**

每隔一天，星巴克都会向用户发送可过期的优惠，这些优惠有自己的难度和过期日期。每个报价可以通过多种营销渠道(电子邮件、手机、社交媒体和网页)发送，一旦完成报价挑战，他们将获得与难度级别一样高的奖励金额。

当用户查看优惠时，他们可以决定:

*   完成挑战:)
*   忽略挑战，但仍然在没有优惠券的情况下进行购买:|
*   忽略一切:(

以下是几周以来 app 营销漏斗中流量的一瞥:

![](img/c3fb9f72fd1df0f617b65f7fa8e0915a.png)

星巴克奖励应用中顾客流的桑基图(图片由作者提供)。

发送报价后，我们有兴趣根据每个营销渠道进行分析:

*   该要约未被转换；
*   这变成了购买；
*   这导致了未来的购买
*   这些都被忽略了，但用户仍然从应用程序中购买。

在样本数据集中，100%的报价都被浏览过——很可能是因为报价是通过多种渠道发送的。高比例的浏览报价带来了一个积极的特点，即让顾客了解品牌和他们提议的报价。当这些建议导致购买行为时，可以推断出顾客不仅知道该品牌的服务，而且也被它所吸引。

然而，相当多的购买是在没有使用优惠券的情况下进行的。这表明即使使用了适当的营销渠道，奖励提议对用户也没有吸引力，这更多地涉及产品的特性而不是营销策略本身。也许通过使用相同的渠道改变发送的报价(这将是我们的推荐系统的任务)，客户将接受报价并因此获得奖励。

在漏斗的末端，要约提议可以实现的最重要的任务之一是说服客户进行未来购买。这可以作为一种代理，吸引客户成为品牌的拥护者[2]。

> 当客户倾向于重复购买某个公司的产品时，他/她可能会说服他们的同行尝试某个产品，从而在市场上产生更多的品牌认知度和吸引力。

请注意，这是移动奖励应用程序的一个相当大的缺陷，因为只有来自 *111k* 的 *22.9k* 才会导致未来的购买。

请记住，在发送要约后，用户参与漏斗的各个步骤所需的时间与销售线索状态高度相关。**销售线索是潜在买家，其状态可以是冷淡、热情或合格[3]。**用户一看到报价，就可能被认为是潜在客户，因为他对此感兴趣。

例如，当客户打开电子邮件报价时，他可能会被其标题内容所吸引——因此我们认为他是在为热身**。然而，如果他/她不继续接受这份工作，他会变得越来越冷淡，这意味着他对接受这份工作的兴趣会逐渐减少。**

理想情况下，用户应该花很短的时间从查看和提供并完成它。这是我们数据集的运行时间分布:

![](img/2f2a4ab111eedbb6f09fd697e2d8f3d3.png)

从(a)发送和提供到观看它和(b)观看和提供并完成它(作者的图像)的时间推移分发。

请注意从查看要约到完成要约的时间分布(以小时计)大致遵循长尾分布。这意味着**大多数购买来自于迅速完成的报价**。为了分析分布的尾部有多长，我们可以将数据拟合到一个**幂律分布**，其概率分布函数定义如下

![](img/82ecdb2180e42b395e463414af6576a4.png)

其中参数 **α** 越低，尾巴越长。对于我们的数据集，在第一个时间间隔中α = 10.57，在第二个时间间隔中α = 7.01。自然地，这些经过的时间很大程度上受要约的属性影响，如难度和持续时间，这只会影响用户查看要约时的延时——因此解释了为什么我们在延时中有更长的尾巴(b)。

这个营销方面的概述帮助我们创建一个定制函数，该函数指示给定客户的报价有多合适:我们将把它称为 ***MAB 奖励函数*** ，其算法描述如下。

![](img/a164ac48b059f1c5ceeba649e18fb454.png)

(图片由作者提供)

# **多股武装匪徒(MAB)**

MAB 算法在统计学中已经被广泛研究了一段时间，用来解决老虎机问题。想象一下，你有一个代理人(强盗),他可以重复地从 k 台可用的机器中拉出一只手臂，并可能从中获得奖励。

当强盗与吃角子老虎机互动时，他注意到其中一个似乎给了更多的奖励，因此很想利用这个杠杆。然而，其他机器可能会给予更多奖励。在这种情况下，他需要决定是否

*   利用一台迄今为止累计回报最高的机器，或者
*   探索其他机器，从长远来看，它们可能会带来更好的累积回报。

> 理论上，代理人是在试图估计每台机器的报酬概率分布函数，而不是在那些期望报酬低的人身上浪费太多的尝试。

在其最简单的数学公式中，MAB 由 *k* 个机器组成，它们有自己的概率分布 *p* 、预期回报μ和方差σ——所有这些对代理人来说最初都是未知的。在每个时期*T∈【1，T* 】一个手臂 *a_i* 被拉动，并且获得奖励。然后，强盗遵循一个策略来选择下一条手臂:到目前为止给出最高回报的手臂或者另一条可能带来更好回报的手臂。第*轮 T* 的最高收益由下式给出

![](img/621439b64c3de53feb36a6f6500572e7.png)

其中μ*是最佳臂的预期回报。换句话说，**他需要做出开采-勘探权衡的决策**。

已经采用了几种技术来解决这个问题，其中之一是ϵ-greedy 方法[1]。在这个经典的方法中，每一轮强盗选择概率为 1 的具有最高经验平均值的手臂——ϵ，或者选择概率为ϵ.的随机手臂

可以推断，ϵ参数对开采-勘探权衡有很大影响。对于更高的ϵ，选择最高经验均值臂的贪婪行为被选中的概率更低，从而导致土匪探索更多的选项。相反，在ϵ较低的情况下，算法倾向于选择贪婪动作。

ϵ-greedy 是无数算法的基础。为了在最初的几轮中有更多的勘探和在以后的几轮中有更多的开发，可以用衰减ϵ-greedy 方法应用一个轮变β。在这种情况下，第 n 轮的ϵ定义为

![](img/e9c01a56ac6abf002e1fb1b92e4672b3.png)

其中β控制ϵ减少的速度， *n* 是当前回合。此外，可以定义阈值λ，以便在后面的回合中限制最小探索。

MAB 算法已经在许多需要进行顺序决策的应用中使用，例如推荐系统。在这种情况下，MAB 被用于建模消费简档，考虑向用户(代理)提供若干项目(老虎机),并且他们可以根据奖励概率分布函数来消费它。

对于当前的 Starbucks 奖励项目，它们将用于建立客户档案模型，将每只手臂视为一个要约和一个定制的奖励函数，该函数定义为掌握获得的优惠券奖励和营销成就，这将在后面看到。

# **在星巴克的奖励应用中应用 MAB**

将 MAB 应用于一个问题的一个关键步骤是定义它的回报函数。正如我们所见，客户进入营销漏斗的深度在很大程度上表明了优惠策略的好坏。在这个意义上，可以使用下面的奖励函数:

![](img/d7f84de377d55491c4932eb41883d28e.png)

其中所有变量都是二进制的，除了*提供奖励*。该等式的下限为 0，这发生在要约未被查看时(使得 *offerCompleted = 0* 和 *futurePurchase = 0* )。当给出一个高报价奖励，并且所有的二元变量都是 1，包括未来的购买，这是营销策略的圣杯时，最好的奖励就来了。

一旦我们建立了这个功能，我们就可以考虑以下变量来创建数据驱动的客户档案:

*   年龄，收入
*   订阅后的年数
*   每种产品的平均*mab 奖励*

在对用户进行聚类之后，每一组都被建模为一个 MAB，有要约作为可拉的手臂。下图描绘了 MAB 训练的 20 个实现的平均回报演变(所有参数决策在项目报告[中讨论，此处](https://github.com/guedes-joaofelipe/ml-engineer-capstone)):

![](img/7cf58b4e0562c809b2a267a133cfa206.png)

ϵ-greedy 衰减表示 20 次实现的平均奖励(图片由作者提供)。

注意平均报酬是如何上升的，直到一个收敛区域。这是勘探-开采权衡变化的结果:开始时，我们尽可能多地勘探，以便找到更好的武器。随着时代的推移，利用最好的武器变得更加频繁，直到没有观察到平均奖励的显著提高。

最后，用户群可以被预测，他们的报价推荐可以通过 MAB 的帮助来完成。由于每次迭代只能拉动一只手臂，自然会出现对产品多样性的关注。那么，在培训阶段选择报价的频率如何呢？我们用下图来核对一下:

![](img/85ce7aa790fed3cbe75ec4ee15314262.png)

按类别选择的武器数量(图片由作者提供)。

可以看出，10 个选项中有 3 个被大多数集群高度选中，这意味着，一般来说，30%的产品组合是被推荐的。这个数字受 MAB 收敛后勘探速度的影响很大。当探索度较低时，该模型倾向于仅推荐已经显示提供最高回报的报价。更高的探索可以为该推荐器带来更好的多样性，然而这可能在使用预测方面影响模型的性能。

# 包扎

我们已经看到优惠券如何成为一种强大的参与工具，让用户更接近公司的服务。通过瞄准正确的线索和客户，不仅可以提高转化率，而且最重要的是，可以与他们建立长期关系。然而，制定成功的目标战略可能需要建立数据驱动的客户档案。这就是公司依靠机器学习的力量来提供个性化建议的地方。

在无数的推荐算法中，多臂强盗因其使用手工制作的奖励函数的灵活性而被广泛探索，例如我们用于 graps 营销方面的函数。如果你想深入研究一种更强大的 MAB 方法，我强烈推荐阅读 Spotify 研究人员开发的关于 BaRT(作为治疗方法的替代物)[4]的文章。

[1]萨顿，理查德 s 和巴尔托，安德鲁 G. [强化学习:导论](https://www.bibsonomy.org/bibtex/ac6b144aaec1819919a2fba9f705c852) (2018)，麻省理工学院出版社。

[2] P. Kotler，H. Kartajaya 和 I. Setiawan，[营销 4.0:从传统走向数字](https://books.google.com.br/books?id=jN9mDQAAQBAJ) (2016)

【3】[冷、暖、合格引线的区别](https://www.successagency.com/growth/2014/06/26/the-difference-between-cold-warm-and-qualified-leads/)

[4] J. McInerney，B. Lacker，S. Hansen，K. Higley，H. Bouchard，A. Gruson，R. Mehrotra。[探索、利用和解释:用强盗个性化可解释的推荐](http://jamesmc.com/blog/2018/10/1/explore-exploit-explain) (2018)，ACM 推荐系统会议(RecSys) *。*