<html>
<head>
<title>How to Explain Each Machine Learning Model at an Interview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在面试时解释每个机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-explain-each-machine-learning-model-at-an-interview-499d82f91470?source=collection_archive---------7-----------------------#2020-06-20">https://towardsdatascience.com/how-to-explain-each-machine-learning-model-at-an-interview-499d82f91470?source=collection_archive---------7-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6900" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从回归到支持向量机再到XGBoost的模型综述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/62aeb89322937b206e254cef73e2ea13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c14OKEbBA4Pc-4wjRnZ1eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由卡特曼戈斯塔创作—<a class="ae ky" href="http://www.freepik.com" rel="noopener ugc nofollow" target="_blank">www.freepik.com</a></p></figure><ul class=""><li id="e675" class="kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><em class="lr">如果你喜欢这个，</em> <a class="ae ky" href="https://medium.com/@terenceshin" rel="noopener"> <em class="lr">跟我上Medium </em> </a> <em class="lr">了解更多</em></li><li id="f07c" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">关注我</em><a class="ae ky" href="https://www.kaggle.com/terenceshin" rel="noopener ugc nofollow" target="_blank"><em class="lr">Kaggle</em></a><em class="lr">了解更多内容！</em></li><li id="416e" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">我们连线上</em><a class="ae ky" href="https://www.linkedin.com/in/terenceshin/" rel="noopener ugc nofollow" target="_blank"><em class="lr">LinkedIn</em></a></li><li id="0881" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">有兴趣合作？查看我的</em> <a class="ae ky" href="http://Want to collaborate?" rel="noopener ugc nofollow" target="_blank"> <em class="lr">网站</em> </a> <em class="lr">。</em></li><li id="0667" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">查看</em> <a class="ae ky" href="https://docs.google.com/document/d/1UV6pvCi9du37cYAcKNtuj-2rkCfbt7kBJieYhSRuwHw/edit#heading=h.m63uwvt9w358" rel="noopener ugc nofollow" target="_blank"> <em class="lr">我的免费数据科学资源</em> </a> <em class="lr">每周都有新素材！</em></li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="fe32" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">在准备任何采访时，我想分享一个资源，提供每个机器学习模型的简明解释。它们并不意味着广泛，而是相反。希望通过阅读这篇文章，您会对如何以简单的方式交流复杂的模型有所了解。</p><h1 id="cfc6" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">涵盖的型号</h1><ol class=""><li id="3cb8" class="kz la it lb b lc nj le nk lg nl li nm lk nn lm no lo lp lq bi translated">线性回归</li><li id="98c7" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">里脊回归</li><li id="d765" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">套索回归</li><li id="fb19" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">逻辑回归</li><li id="7aa9" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">k最近邻</li><li id="c46f" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">朴素贝叶斯</li><li id="ca91" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">支持向量机</li><li id="a40b" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">决策树</li><li id="ba72" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">随机森林</li><li id="9ee2" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">adaboost算法</li><li id="2470" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">梯度增强</li><li id="b80b" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">XGBoost</li></ol><h1 id="25a2" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">线性回归</h1><p id="b735" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">线性回归包括使用最小二乘法找到代表数据集的“最佳拟合线”。最小二乘法包括寻找最小化残差平方和的线性方程。残差等于实际值减去预测值。</p><p id="b465" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">举个例子，红线比绿线更适合，因为它离点更近，因此残差更小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/44bbab84a394b5d24ee5b622dbfba9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GHApjgH_c6EeLIKo.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><h1 id="3187" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">里脊回归</h1><p id="0221" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">岭回归，也称为L2正则化，是一种引入少量偏差以减少过拟合的回归技术。这是通过最小化残差的平方和<strong class="lb iu">加上</strong>惩罚来实现的，其中惩罚等于λ乘以斜率的平方。λ指的是惩罚的严厉程度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/78d2ff61b3b4a5a70fb26d468a1f9aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HHMG3WHZZ2lMH9OvDWqmg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ef04accaf89f21949b87e3d3fd8e3d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vnHI8OyZqzPwgKM8nK7UcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><p id="7b6f" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">没有惩罚，最佳拟合的线具有更陡的斜率，这意味着它对x的小变化更敏感。通过引入惩罚，最佳拟合的线对x的小变化变得不那么敏感。这是岭回归背后的思想。</p><h1 id="96c2" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">套索回归</h1><p id="0367" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">套索回归，也称为L1正则化，类似于岭回归。唯一的区别是，惩罚是用斜率的绝对值来计算的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/5714ca1ecdb8d527f73175f897514014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNknsxFrZO8vGcwedTCiEA.png"/></div></div></figure><h1 id="b091" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">逻辑回归</h1><p id="aba1" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">逻辑回归是一种分类技术，也能找到“最佳拟合线”。但是，与使用最小二乘法找到最佳拟合线的线性回归不同，逻辑回归使用最大似然法找到最佳拟合线(逻辑曲线)。这样做是因为y值只能是1或0。<a class="ae ky" href="https://www.youtube.com/watch?v=BfKanl1aSG0" rel="noopener ugc nofollow" target="_blank"> <em class="lr">查看StatQuest的视频，看看最大似然是如何计算的</em> </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0bb149447f65d883274d745dd75e49e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*nBH02qlime9ali_frW63dA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><h1 id="36d9" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">k-最近邻</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/fdd3a75f3ce8bad09fc6e15f45b86411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*evDyspMuyttExyDFe6B4-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><p id="9d0f" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">K-最近邻是一种分类技术，其中通过查看最近的分类点对新样本进行分类，因此称为“K-最近”。在上面的例子中，如果k=1，那么未分类的点将被分类为蓝点。</p><p id="5729" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">如果k值太低，它可能会受到异常值的影响。但是，如果它太高，可能会忽略只有几个样本的类。</p><h1 id="ce96" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">朴素贝叶斯</h1><p id="d9c2" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">朴素贝叶斯分类器是一种受贝叶斯定理启发的分类技术，贝叶斯定理陈述了以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/1b24239828af600a394a6d2f50c5dd67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cJo6zolk5fZ4M45Q"/></div></div></figure><p id="df30" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">因为假设变量是独立的，我们可以将P(X|y)改写如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/a8a13d5b747a42f053c57d41a4b7cb60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MIu4cAWvYLXsjNEv"/></div></div></figure><p id="4d27" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">此外，由于我们求解y，P(X)是一个常数，这意味着我们可以将其从等式中删除，并引入一个比例。</p><p id="e0fa" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">因此，y的每个值的概率被计算为给定y的xn的条件概率的乘积。</p><h1 id="075a" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">支持向量机</h1><p id="c557" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">支持向量机是一种分类技术，它找到一个最佳边界，称为超平面，用于区分不同的类别。通过最大化类之间的间隔来找到超平面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d7339cea52ea2a572e21efa72c52b728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sE-c5O6pkAVofI74MTue4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><h1 id="0471" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">决策树</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/6ffd7745c6a62e52eca3a90150e505c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvPKnEVMKxXfomrm5Zm-kw.png"/></div></div></figure><p id="94db" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">决策树本质上是一系列条件语句，它们决定了样本在到达底部之前的路径。它们直观且易于构建，但往往不准确。</p><h1 id="1fef" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">随机森林</h1><p id="ec14" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">随机森林是一种集成技术，这意味着它将几个模型结合成一个模型，以提高其预测能力。具体来说，它使用自举数据集和随机变量子集(也称为bagging)构建了1000个较小的决策树。对于数千个较小的决策树，随机森林使用“多数获胜”模型来确定目标变量的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/50b4337b269644882f50b37a17e50fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dKt5G_W1NsL7Gv6R.png"/></div></div></figure><p id="97bb" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">例如，如果我们创建一个决策树，第三个，它会预测0。但是如果我们依赖所有4个决策树的模式，预测值将是1。这就是随机森林的力量。</p><h1 id="8285" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">adaboost算法</h1><p id="30fd" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">AdaBoost是一种增强算法，类似于随机森林，但有两个显著的区别:</p><ol class=""><li id="54b1" class="kz la it lb b lc ld le lf lg lh li lj lk ll lm no lo lp lq bi translated">AdaBoost通常制作树桩森林(树桩是只有一个节点和两片叶子的树)，而不是树木森林。</li><li id="ab01" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">每个树桩的决策在最终决策中的权重并不相等。总误差少(精度高)的树桩会有更高的话语权。</li><li id="7137" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm no lo lp lq bi translated">树桩创建的顺序很重要，因为每个后续树桩都强调在前一个树桩中被错误分类的样本的重要性。</li></ol><h1 id="3dce" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">梯度增强</h1><p id="16a0" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">梯度增强在某种意义上类似于AdaBoost，它构建多棵树，其中每棵树都是基于前一棵树构建的。与AdaBoost构建树桩不同，Gradient Boost构建的树通常有8到32片叶子。</p><p id="a1fb" class="pw-post-body-paragraph me mf it lb b lc ld ju mg le lf jx mh lg mi mj mk li ml mm mn lk mo mp mq lm im bi translated">更重要的是，Gradient与AdaBoost在决策树的构建方式上有所不同。梯度增强从初始预测开始，通常是平均值。然后，基于样本的残差构建决策树。通过采用初始预测+学习率乘以残差树的结果来进行新的预测，并且重复该过程。</p><h1 id="5f20" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">XGBoost</h1><p id="523f" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">XGBoost本质上与渐变增强是一样的，但是主要的区别是如何构建残差树。使用XGBoost，通过计算叶子和前面的节点之间的相似性分数来构建残差树，以确定哪些变量被用作根和节点。</p><h1 id="0d99" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">感谢阅读！</h1><p id="f103" class="pw-post-body-paragraph me mf it lb b lc nj ju mg le nk jx mh lg np mj mk li nq mm mn lk nr mp mq lm im bi translated">希望在读完这篇文章时，你会有一个想法，如何通过突出要点来总结各种机器学习模型。同样，这并不意味着是一篇深入的文章，解释每篇文章的复杂性。根据上面的总结，随意研究任何不完全有意义的模型！</p><h2 id="d22e" class="oc ms it bd mt od oe dn mx of og dp nb lg oh oi nd li oj ok nf lk ol om nh on bi translated">特伦斯·申</h2><ul class=""><li id="2d19" class="kz la it lb b lc nj le nk lg nl li nm lk nn lm ln lo lp lq bi translated"><em class="lr">如果你喜欢这个，</em> <a class="ae ky" href="https://medium.com/@terenceshin" rel="noopener"> <em class="lr">在Medium上关注我</em> </a> <em class="lr">获取更多</em></li><li id="cd25" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">关注我的</em><a class="ae ky" href="https://www.kaggle.com/terenceshin" rel="noopener ugc nofollow" target="_blank"><em class="lr">Kaggle</em></a><em class="lr">了解更多内容！</em></li><li id="7822" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">我们连线上</em><a class="ae ky" href="https://www.linkedin.com/in/terenceshin/" rel="noopener ugc nofollow" target="_blank"><em class="lr">LinkedIn</em></a></li><li id="4da8" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">有兴趣合作？查看我的</em> <a class="ae ky" href="http://Want to collaborate?" rel="noopener ugc nofollow" target="_blank"> <em class="lr">网站</em> </a> <em class="lr">。</em></li><li id="b3ac" class="kz la it lb b lc ls le lt lg lu li lv lk lw lm ln lo lp lq bi translated"><em class="lr">查看</em> <a class="ae ky" href="https://docs.google.com/document/d/1UV6pvCi9du37cYAcKNtuj-2rkCfbt7kBJieYhSRuwHw/edit#heading=h.m63uwvt9w358" rel="noopener ugc nofollow" target="_blank"> <em class="lr">我的免费数据科学资源</em> </a> <em class="lr">每周都有新素材！</em></li></ul></div></div>    
</body>
</html>