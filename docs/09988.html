<html>
<head>
<title>Sports Article Generation with HuggingFace’s GPT-2 module</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 HuggingFace 的 GPT-2 模块生成体育文章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sports-article-generation-with-huggingface-157314eadd9e?source=collection_archive---------40-----------------------#2020-07-14">https://towardsdatascience.com/sports-article-generation-with-huggingface-157314eadd9e?source=collection_archive---------40-----------------------#2020-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8b94" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自然语言处理(NLP)将机器学习的应用扩展到了一个全新的领域。谁会想到统计模型可以应用于我们每天看到的文本，以产生商业见解和预测？在过去的两年里，NLP 经历了前所未有的复兴。</p><p id="2e0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 NLP 的先进方法中，人们对语言生成模型产生了极大的兴趣。Open AI 的 GPT-2 模型中的自动回归功能使得新的文本序列能够非常接近地代表人类思维的想法、说法和文字。这些基于变压器的神经网络模型显示出在提出令人信服的人类长篇文本方面的前景。</p><p id="ac56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我们看看如何使用 HuggingFace 的 GPT-2 语言生成模型来生成体育文章。为了满足这个计算密集型任务，我们将使用来自<a class="ae ko" href="https://spell.ml/" rel="noopener ugc nofollow" target="_blank"> Spell.ml </a> MLOps 平台的 GPU 实例。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/f86cf14a29585e12a89b1d5b5290a50b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3SkF9i5B_0XPYrBC"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@tyleranderson_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">泰勒·安德森</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="65d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">法术入门</strong></p><p id="8d76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如上所述，语言生成模型在计算上可能变得很昂贵，并且对于具有 CPU 的普通机器来说不可能处理它们。为了解决这个问题，我们使用了 Spell 的 GPU 支持的 Jupyter 笔记本。Spell 是一个强大的机器学习和深度学习的 MLOps 平台。它负责基础设施，从而使开发人员和企业能够专注于简单、快速和有组织地执行他们的机器学习模型。</p><p id="c437" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 Spell 进行设置非常简单。只需访问<a class="ae ko" href="https://spell.ml/" rel="noopener ugc nofollow" target="_blank">https://spell.ml/</a>并创建一个新账户。Spell 的每个新用户都可以获得 10 美元的免费使用积分。</p><p id="b47a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本练习中，我们将使用 UCI 机器学习库中的体育文章数据集。它包含 1000 个文本文件，每个文件中都有一篇体育文章。要上传文本文件，请在您的终端或命令提示符窗口中登录到 Spell，并导航到您解压缩文件的父文件夹，然后键入:</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="9311" class="lk ll it lg b gy lm ln l lo lp">spell upload ‘Raw data’</span></pre><p id="88cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将把所需的文本文件上传到 Spell 的参考资料部分。<a class="ae ko" href="https://spell.ml/docs/resources/#uploading-resources" rel="noopener ugc nofollow" target="_blank">在这里了解更多关于上传文件到法术资源的信息</a>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lq"><img src="../Images/0a981a2df8a21475584dab0c7aa103bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N18d-Xo4hargKMgd"/></div></div></figure><p id="923c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要打开 Jupyter 笔记本，请登录到拼写 web 控制台，然后单击工作区&gt;创建工作区。</p><p id="a74c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给这个新工作区起一个您自己选择的名字，然后点击 Continue。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/060d6cba0db28417b3ed32eda4094eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/0*Ma40OezXNaCZ2hAE"/></div></figure><p id="cae8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下一个屏幕中，Spell 为您提供了多个选项来定义您想要在其中运行代码的环境。例如，在“机器类型”下，您可以根据您的用途和预算从各种 CPU 和 GPU 选项中进行选择。此外，您可以在设置 Jupyter 之前选择要安装的框架、环境变量和库。</p><p id="14ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就本项目而言，我们在机器类型下选择“V100 ”;朱庇特名下的笔记本。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/9f9ff36f1fbaaeae100dd1c5e2afe487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/0*8wTDdLbNH5DNURQI"/></div></figure><p id="2f7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下一个屏幕中，让我们单击“启动服务器”开始。完成后，我们会发现一个 Jupyter 基础架构，类似于我们在本地机器上的基础架构。点击新建&gt; Python3。</p><p id="9e6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">语言生成算法概述</strong></p><p id="1498" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们从 HuggingFace 安装“变形金刚”并加载“GPT-2”模型。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="588b" class="lk ll it lg b gy lm ln l lo lp">!pip install -q git+https://github.com/huggingface/transformers.git</span><span id="aba1" class="lk ll it lg b gy lt ln l lo lp">!pip install -q tensorflow==2.1</span><span id="1f8d" class="lk ll it lg b gy lt ln l lo lp">import tensorflow as tf<br/>from transformers import TFGPT2LMHeadModel, GPT2Tokenizer<br/><br/><br/>tokenizer = GPT2Tokenizer.from_pretrained("gpt2")<br/><br/># add the EOS token as PAD token to avoid warnings<br/>model = TFGPT2LMHeadModel.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id)</span></pre><p id="afcb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个对象让你使用预先训练的 GPT-2。一般来说，我们利用 GPT-2 或 transformers 的方式是指定一个输入字符串/短语，这基本上是您文章的开头。然后，算法预测下一组单词，使它们与给定的初始字符串一致。</p><p id="d167" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们试着理解不同类型的算法，以及 GPT-2 如何能够得出最像人类的文本段落。</p><p id="f62c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">先说贪婪搜索算法，这是单词预测的简单化方法之一。基于初始字符串，该算法贪婪地搜索最可能的下一个单词。使用这个新字符串，它基本上是初始字符串加上预测的单词，预测下一个单词。这个过程不断重复，直到我们得到想要的单词数。这种方法的缺点是，在文本的几行/几个单词之后，单词开始重复。这是因为它错过了隐藏在低概率单词后面的高概率单词。</p><p id="fd03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">波束搜索通过每次保持预定义数量的假设，并最终选择具有总体最高概率的假设来减轻这种情况。但是经过几次实验，发现它仍然存在可重复性的问题。</p><p id="c596" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">语言生成的最佳算法之一是采样算法。使用采样的语言生成是不确定的，因为它根据条件概率分布随机选取下一个单词。但是，据观察，考虑到随机性，采样有时会产生听起来不像人类的文本段落。</p><p id="ed7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解决这个问题的一个技巧是在给定前一个<em class="lu"> i </em>单词的情况下，锐化下一个单词的预测分布。在锐化的同时，我们还在绘制随机样本；但是另外，我们增加了高概率单词被选取的可能性，并且降低了低概率单词被选取的可能性。</p><p id="7e3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一个转变是引入 Top-K 采样，其中 K 个最可能的下一个单词被过滤，并且概率质量在这 K 个下一个单词中重新分配。这一简单而强大的概念被纳入 GPT-2 模型，是其成功的主要原因之一。GPT-2 模型的另一个补充是原子核取样。该模型不是只从最可能的 K 个单词中取样，而是从累积概率超过预定义概率<em class="lu"> p </em>的最小可能单词集中进行选择。这个特性的引入确保了单词集的大小可以根据下一个单词的概率分布动态地增加和减少。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="d983" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">使用 GPT-2 模型生成体育文本的示例</strong></p><p id="b6d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">了解了它的内部工作原理后，让我们深入了解 GPT-2 模型的工作原理和性能。请注意，在这一点上，我们使用的是 GPT-2 模型，而不是使用我们之前下载的体育数据。当我们在下一节中微调模型时，我们将更多地研究如何使用这些数据。</p><p id="d62b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的代码演示了 GPT-2 采用的采样技术。“input_ids”是指给予模型的初始字符串。通过将' do_sample '指定为 True，我们告诉模型使用采样技术。“最大长度”对应于物品的期望长度。‘top _ K’和‘top _ p’分别对应于 K 个单词和核概率 p。最后，我们将名为“num_return_sequences”的参数指定为 2。这使用相同的初始字符串生成了两个不同的段落，并给了我们一个选项来选择我们更喜欢的输出。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="586d" class="lk ll it lg b gy lm ln l lo lp">#initial string<br/>input_ids = tokenizer.encode('Manchester City agree deal to sell Leroy Sane', return_tensors='tf')<br/><br/># set seed to reproduce results. Feel free to change the seed though to get different results<br/>tf.random.set_seed(0)<br/><br/># set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3<br/>sample_outputs = model.generate(<br/>    input_ids,<br/>    do_sample=True, <br/>    max_length=100, <br/>    top_k=50, <br/>    top_p=0.45, <br/>    num_return_sequences=3 )<br/><br/>print("Output:\n" + 100 * '-')<br/>for i, sample_output in enumerate(sample_outputs):<br/>  print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))</span></pre><p id="251d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是输出结果:</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="b005" class="lk ll it lg b gy lm ln l lo lp">0: Manchester City agree deal to sell Leroy Sane<br/><br/>The Gunners are ready to sign Leroy Sane, who has been on loan at Tottenham for the past three seasons, from Chelsea.<br/><br/>The 21-year-old, who is in his first season at the club, has scored five goals in 14 games for the Blues this season.<br/><br/>The former Arsenal and Chelsea striker has been a target for Chelsea since he joined from Southampton in January 2013.<br/><br/>The deal is<br/><br/><br/>1: Manchester City agree deal to sell Leroy Sane<br/><br/>Manchester City have agreed a £30million deal to sell Leroy Sane to Manchester United for £30million.<br/><br/>The move was confirmed by City sources.<br/><br/>Sane, 24, has scored nine goals in 20 Premier League appearances for the club since joining from Manchester United in January 2014.<br/><br/>He has scored seven goals in 16 Premier League appearances for United since joining from Manchester United in January 2014.</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="bd1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定制 GPT-2:体育文章数据微调</strong></p><p id="9300" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看了输出，我们可以说 GPT-2 已经能够把一个有凝聚力的文本放在一起。然而，它产生的事实陈述缺乏准确性。还有，整个段落没有给我们很运动的感觉。为了解决这些问题，我们试图训练和微调 GPT-2 特别是体育文章，而不是使用它。我们将使用我们之前上传的运动数据集。</p><p id="265c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的第一项工作是将文章整理成一个文本文件。为了做到这一点，我们启动了一个新的 Jupyter 笔记本。单击“文件”选项卡，然后单击“添加装载”。转到上传并选择您刚刚上传的“原始数据”文件夹。</p><p id="9613" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回到笔记本，执行下面的代码来阅读文本文件并整理它们。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="52a8" class="lk ll it lg b gy lm ln l lo lp">mypath = 'Raw data/'<br/>import os<br/>from os import listdir<br/>from os.path import isfile, join<br/>onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]</span><span id="a503" class="lk ll it lg b gy lt ln l lo lp">article_list = ''<br/>for i in range(len(onlyfiles)):<br/>  if onlyfiles[i][-3:] == 'txt':<br/>    try:<br/>      with open('Raw data/' + onlyfiles[i], 'r') as file:<br/>        data = file.read()<br/>      article_list = article_list + '\n' + data<br/>    except:<br/>      pass</span></pre><p id="c809" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完成后，我们需要使用字符串包中的' printable '函数过滤掉所有不属于 ASCII 的字符。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="445b" class="lk ll it lg b gy lm ln l lo lp">import string<br/>article_list_str = ''.join(filter(lambda x: x in string.printable, article_list))</span></pre><p id="5044" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦数据达到所需的格式，让我们继续构建模型。我们安装“transformers”包并导入所需的包。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="876a" class="lk ll it lg b gy lm ln l lo lp">!pip install transformers</span><span id="9ba3" class="lk ll it lg b gy lt ln l lo lp">import logging<br/>import os<br/>import pickle<br/>import random<br/>import torch<br/>import torch.nn as nn<br/>import transformers<br/>from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler<br/>from transformers import (<br/>    GPT2Config,<br/>    GPT2LMHeadModel,<br/>    GPT2PreTrainedModel,<br/>    GPT2Tokenizer,<br/>    PreTrainedModel,<br/>    PreTrainedTokenizer,<br/>)<br/><br/>MODEL_CLASSES = {"gpt2": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)}<br/><br/>logger = logging.getLogger(__name__)</span></pre><p id="ea89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们定义一个类“SportsData”来微调体育数据集并获取其中的令牌。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="fb50" class="lk ll it lg b gy lm ln l lo lp">class SportsData(Dataset):<br/>    def __init__(<br/>        self,<br/>        tokenizer: PreTrainedTokenizer,<br/>        #file_path: str,<br/>        block_size=512,<br/>        overwrite_cache=False,<br/>    ):<br/>        #assert os.path.isfile(file_path)<br/><br/>        block_size = block_size - (<br/>            tokenizer.max_len - tokenizer.max_len_single_sentence<br/>        )<br/><br/>        # change if args are added at later point<br/>        cached_features_file = os.path.join(<br/>           "gpt2" + "_" + str(block_size) + "_file.txt" <br/>        )<br/><br/>        if os.path.exists(cached_features_file) and not overwrite_cache:<br/>            logger.info(<br/>                f"Loading features from your cached file {cached_features_file}"<br/>            )<br/>            with open(cached_features_file, "rb") as cache:<br/>                self.examples = pickle.load(cache)<br/>                logger.debug("Loaded examples from cache")<br/>        else:<br/>            logger.info(f"Creating features from file")<br/><br/>            self.examples = []<br/><br/>            text = article_list_str<br/>            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))<br/><br/>            for i in range(0, len(tokenized_text) - block_size + 1, block_size):<br/>                self.examples.append(<br/>                    tokenizer.build_inputs_with_special_tokens(<br/>                        tokenized_text[i : i + block_size]<br/>                    )<br/>                )<br/><br/>            logger.info(f"Saving features into cached file {cached_features_file}")<br/>            with open(cached_features_file, "wb") as cache:<br/>                <br/>                pickle.dump(self.examples, cache, protocol=pickle.HIGHEST_PROTOCOL)<br/><br/>    def __len__(self):<br/>        return len(self.examples)<br/><br/>    def __getitem__(self, item):<br/>        return torch.tensor(self.examples[item], dtype=torch.long)</span></pre><p id="d56d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们启动定制模型的训练并保存模型。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="2bc8" class="lk ll it lg b gy lm ln l lo lp">device = 'cpu'<br/>if torch.cuda.is_available():<br/>    device = 'cuda'<br/></span><span id="c9bc" class="lk ll it lg b gy lt ln l lo lp">tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')<br/>model = GPT2LMHeadModel.from_pretrained('gpt2-medium')<br/>model = model.to(device)<br/><br/>dataset = SportsData(tokenizer= tokenizer )<br/>article_loader = DataLoader(dataset,batch_size=1,shuffle=True)<br/><br/>BATCH_SIZE = 1<br/>EPOCHS = 1<br/>LEARNING_RATE = 0.0002<br/>WARMUP_STEPS = 5000<br/><br/>from transformers import AdamW, get_linear_schedule_with_warmup<br/><br/>model = model.to(device)<br/>model.train()<br/>optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)<br/>scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)<br/>script_count = 0<br/>sum_loss = 0.0<br/>batch_count = 0<br/><br/>for epoch in range(EPOCHS):<br/>    print(f"EPOCH {epoch} started" + '=' * 30)<br/>    for idx,script in enumerate(article_loader):<br/>        outputs = model(script.to(device), labels=script.to(device))<br/>        #outputs = torch.tensor(tokenizer.encode(script)).unsqueeze(0).to(device) <br/>        loss, logits = outputs[:2]                        <br/>        loss.backward()<br/>        sum_loss = sum_loss + loss.detach().data<br/>                       <br/>        script_count = script_count + 1<br/>        if script_count == BATCH_SIZE:<br/>            script_count = 0    <br/>            batch_count += 1<br/>            optimizer.step()<br/>            scheduler.step() <br/>            optimizer.zero_grad()<br/>            model.zero_grad()<br/>            <br/>        if batch_count == 200:<br/>            model.eval()<br/>            print(f"sum loss {sum_loss}")<br/>            sample_outputs = model.generate(<br/>                                    bos_token_id=random.randint(1,30000),<br/>                                    do_sample=True,   <br/>                                    top_k=50, <br/>                                    max_length = 1000,<br/>                                    top_p=0.95, <br/>                                    num_return_sequences=1<br/>                                )<br/><br/>            print("Output:\n" + 100 * '-')<br/>            for i, sample_output in enumerate(sample_outputs):<br/>                  print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))<br/>            <br/>            batch_count = 0<br/>            sum_loss = 0.0<br/>            model.train()</span><span id="cda4" class="lk ll it lg b gy lt ln l lo lp">output_dir = 'Raw data/'<br/><br/>from transformers import WEIGHTS_NAME, CONFIG_NAME<br/>output_model_file = os.path.join(output_dir, WEIGHTS_NAME)<br/>output_config_file = os.path.join(output_dir, CONFIG_NAME)<br/><br/>torch.save(model.state_dict(), output_model_file)<br/>model.config.to_json_file(output_config_file)<br/>tokenizer.save_vocabulary(output_dir)</span></pre><p id="4119" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们已经准备好了微调的模型，我们导入它并测试它如何针对我们之前使用的相同输入字符串工作。</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="38a9" class="lk ll it lg b gy lm ln l lo lp">model = GPT2LMHeadModel.from_pretrained(output_dir)<br/>tokenizer = GPT2Tokenizer.from_pretrained(output_dir)<br/><br/>input_ids = tokenizer.encode('Manchester City agree deal to sell Leroy Sane', return_tensors='pt')<br/><br/>sample_outputs = model.generate(<br/>                        input_ids= input_ids,<br/>                        do_sample = True,<br/>                        #num_beams= 5,<br/>                        max_length = 100,<br/>                        top_k = 50,<br/>                        top_p=0.85, <br/>                        num_return_sequences=1<br/>                    )<br/><br/>print("Output:\n" + 100 * '-')<br/>for i, sample_output in enumerate(sample_outputs):<br/>      print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))</span></pre><p id="f89b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是输出结果:</p><pre class="kq kr ks kt gt lf lg lh li aw lj bi"><span id="9f92" class="lk ll it lg b gy lm ln l lo lp">Output:<br/>----------------------------------------------------------------------------------------------------<br/>0: Manchester City agree deal to sell Leroy Sane to Liverpool<br/><br/>Leroy Sane was among three players who were sold this summer and Liverpool boss Brendan Rodgers admitted he felt the need to replace the former Manchester City winger.<br/><br/>"We sold four players last year and I know I had to get another player in to improve our squad," Rodgers told Sky Sports News HQ.<br/><br/>"We had to sell players and a few of them we did but it was Leroy Sane.</span></pre><p id="a716" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们注意到这段文字不仅听起来像人类，而且更流畅。经理的引语尤其给了这段文字一种更加真实的感觉，使它看起来非常类似于一篇真正的手写文章。请注意，增加“max_length”的值将允许我们获得更长的文本。</p><p id="ac36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，事实仍然不是完全正确的，但这些可以用我们的知识迅速纠正。重要的是，我们已经能够摆脱手工书写的过程。只要运行这个命令，编辑一些可能存在的事实差异，瞧，你就完成了。</p><p id="4fcb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要试用本教程，请使用<a class="ae ko" href="https://web.spell.ml/refer/sushrut17" rel="noopener ugc nofollow" target="_blank">此链接</a>登录 Spell 的 MLOps 平台，并在创建帐户后获得 10 美元的免费 GPU 点数。另外，请随时在这个<a class="ae ko" href="https://chat.spell.ml/" rel="noopener ugc nofollow" target="_blank">链接</a>上发布您的问题和疑问。</p><p id="1c85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献:</strong></p><p id="7959" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb#scrollTo=mQHuo911wfT-" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/hugging face/blog/blob/master/notebooks/02 _ how _ to _ generate . ipynb # scroll to = MQ huo 911 wft-</a></p><p id="5835" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/cdpierse/script_buddy_v2/tree/master/script_buddy" rel="noopener ugc nofollow" target="_blank">https://github . com/cdpierse/script _ buddy _ v2/tree/master/script _ buddy</a></p></div></div>    
</body>
</html>