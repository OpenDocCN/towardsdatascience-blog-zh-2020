<html>
<head>
<title>Overview of Human Pose Estimation Neural Networks — HRNet + HigherHRNet, Architectures and FAQ — 2d3d.ai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人体姿态估计神经网络概述—HRNet+higher rnet，架构和常见问题— 2d3d.ai</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-human-pose-estimation-neural-networks-hrnet-higherhrnet-architectures-and-faq-1954b2f8b249?source=collection_archive---------6-----------------------#2020-06-13">https://towardsdatascience.com/overview-of-human-pose-estimation-neural-networks-hrnet-higherhrnet-architectures-and-faq-1954b2f8b249?source=collection_archive---------6-----------------------#2020-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d54b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://arxiv.org/pdf/1902.09212.pdf" rel="noopener ugc nofollow" target="_blank">高分辨率网络(HRNet) </a>是一种用于人体姿势估计的最先进的神经网络，这是一种图像处理任务，可以在图像中找到对象的关节和身体部位的配置。该网络的新颖之处在于保持输入数据的高分辨率表示，并将其与高到低分辨率子网络并行组合，同时保持有效的计算复杂性和参数计数。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">演示视频HRNet姿势估计超过世界纪录侏儒发射！</p></figure><h1 id="29c8" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">在本帖中，我们将介绍:</h1><ul class=""><li id="a26d" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">为什么选择HRNet？</li><li id="b953" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">人力资源网与建筑</li><li id="41f3" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">HigherHRNet:用于自下而上人体姿态估计的尺度感知表征学习</li><li id="609f" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">演示视频</li><li id="51ff" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">代码常见问题</li></ul><h1 id="8f42" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么选择HRNet？</h1><ul class=""><li id="75b9" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">良好的文档化和维护的开源(<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" rel="noopener ugc nofollow" target="_blank">链接</a>)。github上的2490颗星——在所有人类姿势评估中排名最高。</li><li id="5a63" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">它被用作同一研究领域中最近的新架构的主干(例如在<a class="ae kl" href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" rel="noopener ugc nofollow" target="_blank">项目</a></li><li id="2fbb" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">许多姿势估计挑战中的顶级竞争者(<a class="ae kl" href="https://paperswithcode.com/paper/deep-high-resolution-representation-learning" rel="noopener ugc nofollow" target="_blank">参考</a>):</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/30ea8708579373f56364d48f8131cf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oErCAvKftg3HHo6a.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">可可排名第一</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ms"><img src="../Images/35bc39851d8d4f30c94e40b458b82a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Lguk07iVZrd06O85.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">可可排名第一</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/12694e9c80e65474b87fd36467d2eda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lDgB27UFpZu3GOC7.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">PoseTrack2017排名第二</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mt"><img src="../Images/4ee44ecf24a78d5c0801c7393e53cd96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IciEPBS6NpcHMic3heaaIw.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">MPII排名第六</p></figure><h1 id="9f5a" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">HRNet解释道</h1><p id="2540" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">当处理人体姿态估计时，我们需要能够检测图像中的人，并估计他的关节(或关键点)的配置。因此，存在两种可能的姿势估计方法:</p><h2 id="dd8e" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">自顶向下和自底向上的姿态估计</h2><p id="f189" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">自下而上的方法首先找到关键点，然后将它们映射到图像中的不同人物，而自上而下的方法首先使用一种机制来检测图像中的人物，在每个人物实例周围放置一个边界框区域，然后估计边界框内的关键点配置。</p><p id="b6a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自上而下的方法依赖于单独的人检测网络，并且需要为每个人单独估计关键点，因此它们通常是计算密集型的，因为它们不是真正的端到端系统。相比之下，自下而上的方法首先通过预测不同解剖关键点的热图来定位输入图像中所有人的无身份关键点，然后将他们分组到人实例中，这有效地使他们更快。</p><p id="3126" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自上而下的方法更为普遍，并且目前实现了更好的预测准确性，因为它将两项任务分开，以使用为每项任务训练的特定神经网络，并且因为自下而上的方法由于图像中不同人的比例变化而在预测关键点方面存在问题(也就是说，直到HigherHRNet出现——如下)。自顶向下方法中不存在这种比例变化，因为所有人员实例都被规范化为相同的比例。而自底向上的方法被认为更快，因为</p><p id="4c37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">HRNet使用自上而下的方法，该网络是基于由另一个网络(FasterRCNN)在推理\测试期间检测的人包围盒来估计关键点而构建的。在训练期间，HRNet使用给定数据集的带注释的边界框。</p><h2 id="37d0" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">两个数据集用于训练和评估网络</h2><ul class=""><li id="c4f5" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">COCO —超过20万张图片和25万个人实例，标注了17个关键点。COCO数据集评估还需要评估人的边界框，这是使用FasterRCNN网络完成的。评估度量是<a class="ae kl" href="http://cocodataset.org/#keypoints-eval" rel="noopener ugc nofollow" target="_blank">对象关键点相似性(OKS)</a>——标准的关键点检测准确度度量。</li><li id="fc3c" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">MPII人体姿势——大约25000张照片，40000个主题。MPII评估是使用数据集中带注释的边界框完成的。</li></ul><h1 id="bb6f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">体系结构</h1><p id="316d" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">以下是基于<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" rel="noopener ugc nofollow" target="_blank"> git项目</a>中代码的神经网络图，之后是研究论文中描述的网络图。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nj"><img src="../Images/e74003bb29d9a244e167fc0fb2cdb309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUuLUDIQ_9GJaZLMJFQcMQ.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">基于公开开源的HRNet网络架构</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nk"><img src="../Images/fa340c2847d19499204329d1beb25ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M5Reakrl_tapQKDz.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">HRNet网络体系结构如本文所述</p></figure><p id="89a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要注意的重要结构是，网络计算高分辨率子网络(分支1)与低分辨率子网络(分支2–4)并行。子网络通过融合层融合，使得每个高到低分辨率表示反复接收来自其他并行表示的信息，导致丰富的高分辨率表示。</p><p id="74c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入图像为256 x 192或384 x 288，对应的热图输出尺寸为64 x 48或96 x 72。前两个卷积根据预期的热图大小减小输入大小。网络输出热图大小和17个通道-热图中每个关键点(17个关键点)的每个像素的值。</p><p id="7bce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所描述的开源架构是针对32通道配置的。对于48个通道，从第一个过渡层开始改变每一层，直到48个通道，不同的乘数为2。</p><p id="23f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文中的交换块是开源中的一个模块，交换单元是开源中的保险丝层。在纸图中，过渡层看起来像子网络的独立融合，而在代码中，当创建较低分辨率(较高通道)的子网络时，过渡是基于与另一个卷积层的融合，该融合导致先前最低分辨率的子网络。此外，在开放源代码中，最后一层的融合仅针对高分辨率分支(分支1)进行计算，而不是针对纸图中看到的所有分支。</p><p id="3f00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下采样是在融合部分(或交换单元)从高分辨率分支转移到较低分辨率分支的跨距=2的卷积，对于双倍和三倍下采样，仅在最后一次下采样中扩大通道的数量。这要么是代码中的错误，要么是论文中没有明确解释。很可能是代码中的错误，因为信息没有从较大分辨率映射到较深通道中的第一个下采样，即git 中的<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/187" rel="noopener ugc nofollow" target="_blank">开放问题。</a></p><p id="b952" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nl">如果有疑问，使用基于开源的图表——这是运行训练有素的网络时使用的图表。</em></p><h2 id="c02d" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">网络培训</h2><ul class=""><li id="d6a9" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">对于权重初始化，作者使用ImageNet分类数据集上的不同输出层训练相同的网络，并使用权重值作为姿势估计训练的初始化值。</li><li id="e31b" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">在COCO数据集上训练HRNet-W32的210个历元需要大约50-60小时，使用4个P100 GPU—<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/3" rel="noopener ugc nofollow" target="_blank">参考</a>。</li></ul><h1 id="9c2b" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><a class="ae kl" href="https://arxiv.org/abs/1908.10357" rel="noopener ugc nofollow" target="_blank">higher rnet:自下而上人体姿态估计的尺度感知表示学习</a></h1><p id="bcf8" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">这是同一个研究小组使用HRNet作为主干的自下而上姿势跟踪的新网络。作者解决了自下而上姿态估计中的尺度变化问题(如上所述),并表示他们能够通过输出多分辨率热图和使用HRNet提供的高分辨率表示来解决该问题。</p><p id="e3fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">HigherHRNet在COCO数据集上的表现优于所有其他自下而上的方法，对于中等身材的人来说收益尤其大。HigherHRNet还在CrowdPose数据集上取得了最先进的结果。作者表示，这表明自下而上的方法比自上而下的方法对拥挤的场景更鲁棒，但在同一数据集上没有与常规的自上而下的HRNet结果进行比较。</p><p id="4285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该网络的主干是常规的HRNet，但在末端增加了一个部分，用于输出更高分辨率的热图:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nm"><img src="../Images/6326ad9d00b3c36ed1fd44d1d167e1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hcpC99eqys5LMeRI.png"/></div></div></figure><p id="c91c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">体系结构的右侧部分输出两个热图，一个用于低分辨率，一个用于高分辨率，分辨率分别为128 x 128和256 x 256。在推断期间，两个热图被平均聚合到更高的分辨率，并且最高值的点被选择用于关键点检测。梯形是一个反卷积层，它输出2倍高的分辨率，后面是4个剩余块。此外，对于每个关键点，计算输出标量标签，近标签值形成属于特定人的一组关键点，远标签值指示属于不同人的关键点组。标签是根据本文中<a class="ae kl" href="https://arxiv.org/abs/1611.05424" rel="noopener ugc nofollow" target="_blank">描述的“关联嵌入”方法计算的。仅针对最低分辨率的热图训练和预测标签值，因为作者发现，根据经验，较高分辨率的热图标签值无法很好地预测，甚至无法收敛。</a></p><p id="626f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练期间，损失函数是热图预测损失和标签值损失的加权平均值(根据关联嵌入方法，相同组的标签之间的较小距离导致较低的损失，不同组的标签之间的较大距离也是如此)。每个热图分辨率损失是根据地面实况独立计算的，它们是总和。</p><p id="afd5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">检查HigherHRNet的<a class="ae kl" href="https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation" rel="noopener ugc nofollow" target="_blank">开源代码</a>还没有推理代码可用于创建基于训练好的网络的演示姿势估计视频。</p><h1 id="17a7" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">演示视频</h1><p id="2a8f" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">演示视频基于HRNet中的推理脚本(这是一个修改过的脚本，在joins之间画棍子，运行时不打开pop图像— <a class="ae kl" href="https://drive.google.com/drive/folders/1KR462gnw05sB0nN9sicR8evyrsy1zwKs?usp=sharing" rel="noopener ugc nofollow" target="_blank">脚本链接</a>)。感谢罗斯·史密斯的Youtube频道。</p><h2 id="33ee" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">视频特征</h2><ul class=""><li id="f8ee" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">1920X1080像素，每秒25帧，56秒(1400帧)。</li><li id="4ccd" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">多人、挑战性场景的好例子——同质和异质背景、变化的背景、不同的摄像机角度，包括放大和缩小，以及姿势令人敬畏的侏儒。</li></ul><h2 id="7d33" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">运行时信息</h2><ul class=""><li id="dbcc" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">带有Resnet50的FasterRCNN用于人员检测</li><li id="be5c" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">使用具有48个通道和384 x 288分辨率输入图像的HRNet。</li><li id="3c74" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">戴尔笔记本电脑酷睿i5–7200，32GB内存，GeForce 940MX，使用Ubuntu 18.04。推断期间GPU达到100%利用率。</li><li id="25ab" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">跟踪一帧中所有边界框的平均时间:1.14秒</li><li id="f518" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">一帧中所有姿态估计的平均时间:0.43秒</li><li id="460d" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">一帧解析的平均总时间:1.62秒</li><li id="c9f8" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">代码在整个视频上运行推理的总时间:2586.09秒</li></ul><h2 id="f2b7" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">演示中的问题</h2><p id="75f6" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">当评估图像处理算法的结果时，重要的是要注意该算法哪里执行得不好，这给出了其固有问题的线索:</p><ul class=""><li id="ccb8" class="lv lw iq jp b jq jr ju jv jy nn kc no kg np kk mc md me mf bi translated">具有木制背景的赤膊者在faster CNN中未被很好地检测到-这可能是faster CNN网络的训练数据问题，没有足够的赤膊样本或没有足够的背景颜色类似于人的颜色的样本</li><li id="a85e" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">大黄色蹦床被检测为人(分钟00:11)-这可能显示了同质场景的快速CNN的固有问题。</li><li id="76c6" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">在边界框中检测到17个关键点，即使框中没有人或没有显示所有关节HRNet的构建方式是必须预测所有17个关节，即使它们不是可视的。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nq"><img src="../Images/0cdf02c1406bf6f6466590a441886262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cboPN6YlubpMvuYLxPSP6A.jpeg"/></div></div></figure><ul class=""><li id="d00c" class="lv lw iq jp b jq jr ju jv jy nn kc no kg np kk mc md me mf bi translated">值得注意的是，在视频的开始，即使有遮挡，也有很好的姿势估计。处理图像中由于模糊而丢失的信息是一件棘手的事情，HRNet能够很好地解决这个问题。</li><li id="2c2a" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">另外值得一提的是，矮人手持的棍子估计不是四肢之一，这也是一个积极的迹象。</li></ul><h1 id="2a2b" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">代码常见问题</h1><ol class=""><li id="561f" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk nr md me mf bi translated">姿势跟踪在RGB(<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/41" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/41</a>)中完成，而人检测基线训练网络在BGR(<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/15" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/15</a>)中完成</li><li id="33a6" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">使用coco数据集API pycocotools与python 3【https://github.com/cocodataset/cocoapi/issues/49 T4】不兼容。HRNet基本上可以工作，但是一旦你开始使用pycocotools，可能会有例外。</li><li id="fe03" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">必须使用numpy 1.17:<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/177" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/177</a></li><li id="171c" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">如何用自己的数据集训练网络:<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/68" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/68</a></li><li id="6425" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">在推理中，考虑使用model.no_grad来提高性能和降低内存使用量(我还没有测试过)</li><li id="36ae" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">第三个关节参数似乎总是为零，对于joints_3d_vis对象，前两个参数总是具有相同的生存能力标志，而第三个参数也是零，来自coco . py--&gt; _ load _ coco _ key point _ annotation _ kernal()。joins dataset-&gt;<strong class="jp ir">getitem</strong>()-&gt;affine _ transform中，关节的大小为3，作为仿射变换的准备，但第三个参数从未使用过(可能是遗留的，或者，它被放置在适当的位置，供以后用于HigherHRNet)。同样的事情似乎发生在MPII数据集上。</li><li id="f8ed" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">在验证\测试期间，不使用带注释的关节(即使它们保存在dataloader管道中)，因此测试运行期间打印的准确性结果不正确。试运行期间的整个精度计算流程是多余的。在运行结束时，他们使用coco api来计算正确的精度度量</li><li id="3355" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk nr md me mf bi translated">推理配置为384X288(但自述文件说使用256X192)</li></ol><h2 id="cb6f" class="mx ky iq bd kz my mz dn ld na nb dp lh jy nc nd ll kc ne nf lp kg ng nh lt ni bi translated">图像和关节变换</h2><ul class=""><li id="847b" class="lv lw iq jp b jq lx ju ly jy lz kc ma kg mb kk mc md me mf bi translated">演示/推断— box_to_center_scale()根据框缩放图像，但不清楚pixel_std=200做什么。关于它有几个公开的问题:<br/><a class="ae kl" href="https://github.com/microsoft/human-pose-estimation.pytorch/issues/26" rel="noopener ugc nofollow" target="_blank">https://github . com/Microsoft/human-pose-estimation . py torch/issues/26</a><br/><a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/23" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/23</a><br/><a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/9" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/9</a><br/><a class="ae kl" href="https://github.com/microsoft/human-pose-estimation.pytorch/issues/94" rel="noopener ugc nofollow" target="_blank">https://github . com别管它了。</a></li><li id="09c9" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">中心和比例是根据原始图像中检测到的注释bbox的位置。中心是原始图像上bbox的中心，比例应该是bbox相对于原始图像的大小—来自coco . py--&gt; _ load _ coco _ person _ detection _ results()。bbox由x，y，w，h = box[:4] (x，y，width，height)构成。计算比例时，会根据预先配置的pixel_std和1.25比例进行纵横比和归一化。</li><li id="eebb" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">推理-&gt; get _ pose _ estimation _ prediction返回原始图像上的坐标(没有旋转，只有每个边界框的中心和比例)</li><li id="0b95" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">JointsDataset-&gt; getitem-&gt; get _ affine _ transform获取一个变换，该变换根据原始图像比bbox大多少来放大原始图像的比例，然后将图像置于bbox的中心。</li><li id="457c" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">然后，warpAffine将原始图像转移到中心，并提供缩放比例，这意味着我们应该在输出图像中看到bbox的区域。输出图像被裁剪，其' 0，0点对应于原始图像上的点，该点在转换后落在0，0坐标上，裁剪是从该点向右下移动完成的。</li><li id="b053" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">在训练期间，仿射变换还具有随机旋转缩放和翻转类关节Dataset → __getitem__()</li><li id="d198" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">JointsDataset中self.db中的对象通过引用进行更改。self.db填充在coco dataset-&gt; _ load _ coco _ person _ detection _ results()类的第246行。</li><li id="30ee" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">变换计算是:x_new(of x_old)，y_new(of y_old)，z = T*(x_old，y_old，1) <br/>好地方看例子:<a class="ae kl" href="https://docs.opencv.org/master/dd/d52/tutorial_js_geometric_transformations.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/master/DD/d52/tutorial _ js _ geometric _ transformations . html</a></li><li id="03b2" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">变换后关节位置可以是负的，它们使用与图像相同的变换矩阵进行传递，并且由于存在向中心的变换和根据边界框的放大比例，一些关节可以在框外。</li><li id="5cd0" class="lv lw iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated">MPII的中心和比例尺标注不完全清楚—<a class="ae kl" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/issues/51" rel="noopener ugc nofollow" target="_blank">https://github . com/leoxiaobin/deep-high-resolution-net . py torch/issues/51</a></li></ul></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="f1d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nl">原载于2020年6月13日</em><a class="ae kl" href="https://2d3d.ai/index.php/2020/06/14/human-pose-estimation-hrnet/" rel="noopener ugc nofollow" target="_blank"><em class="nl">https://2d3d . ai</em></a><em class="nl">。</em></p></div></div>    
</body>
</html>