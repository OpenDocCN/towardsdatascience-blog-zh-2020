<html>
<head>
<title>Hands-on Transformers (Kaggle Google QUEST Q&amp;A Labeling).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动手变形金刚(Kaggle 谷歌 QUEST 问答标注)。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=collection_archive---------25-----------------------#2020-08-06">https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=collection_archive---------25-----------------------#2020-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7cec" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">《变形金刚》第 3/3 部 vs 谷歌 QUEST 问答标注(Kaggle top 5%)。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/88d158acc638450c10b0f75dc25820c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOohtF4J9u1updZhHNh9ow.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d7f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">这是一个由 3 部分组成的系列，我们将经历变形金刚、伯特和动手 Kaggle 挑战—</em><a class="ae lv" href="https://www.kaggle.com/c/google-quest-challenge/" rel="noopener ugc nofollow" target="_blank"><em class="lu">Google QUEST Q&amp;A Labeling</em></a><em class="lu">来观看变形金刚的行动(在排行榜上排名第 4.4%)。在这部分(3/3 ),我们将会看到谷歌在 Kaggle 上的一个实践项目。由于这是一个 NLP 挑战，我在这个项目中使用了变形金刚。我在这一部分没有详细介绍变形金刚，但是如果你愿意的话，你可以看看这个系列的 1/3 部分，在那里我详细讨论了变形金刚。</em></p><h1 id="4327" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">博客的鸟瞰图:</h1><p id="ff26" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">为了方便阅读，我把博客分成了不同的子主题-</p><ul class=""><li id="18df" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">问题陈述和评估标准。</li><li id="c9e9" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">关于数据。</li><li id="68a6" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">探索性数据分析(EDA)。</li><li id="eb9b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">建模(包括数据预处理)。</li><li id="6ee8" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">建模后分析。</li></ul><h1 id="779b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">问题陈述和评估指标:</h1><p id="5c57" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">计算机非常擅长用单一的、可验证的答案来回答问题。但是，人类通常更善于回答关于观点、建议或个人经历的问题。</p><p id="c3a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人类更擅长解决需要对背景有更深层次、多维度理解的主观问题。问题可以有多种形式——有些是多句子的阐述，有些可能是简单的好奇或者是一个完全成熟的问题。他们可以有多种意图，或者寻求建议和意见。有些可能是有帮助的，有些可能是有趣的。有些是简单的对错。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/39cc5e6b54ee85711bbdd696c244aec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKNxCI-qlr_YAhcKowSHkQ.png"/></div></div></figure><p id="f602" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，由于缺乏数据和预测模型，很难建立更好的主观问答算法。这就是为什么谷歌研究的<a class="ae lv" href="https://crowdsource.google.com/" rel="noopener ugc nofollow" target="_blank">众包</a>团队，一个致力于通过众包推进 NLP 和其他类型的 ML 科学的团队，已经收集了许多这些质量评分方面的数据。</p><p id="ee52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这场比赛中，我们面临的挑战是使用这个新的数据集为问答的不同主观方面构建预测算法。这些问答配对是从近 70 个不同的网站上以“常识”的方式收集来的。评分者接受的指导和培训很少，很大程度上依赖于他们对提示的主观理解。因此，每个提示都是以最直观的方式制作的，这样评分者可以简单地利用他们的常识来完成任务。</p><p id="ee30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">证明这些主观标签可以被可靠地预测，可以为这一研究领域带来新的曙光。这次比赛的结果将告知未来智能问答系统的构建方式，希望有助于它们变得更像人类。</p><p id="f5ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">评估指标:</strong>根据平均列相关系数<a class="ae lv" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Spearman 相关系数</a>对提交的内容进行评估。为每个目标列计算 Spearman 等级相关性，并为提交分数计算这些值的平均值。</p><h1 id="c15e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">关于数据:</h1><p id="d2b6" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">此竞赛的数据包括来自各种 StackExchange 属性的问题和答案。我们的任务是预测每个问答对的 30 个标签的目标值。<br/>30 个目标标签的列表与<code class="fe ni nj nk nl b">sample_submission.csv</code>文件中的列名相同。前缀为<code class="fe ni nj nk nl b">question_</code>的目标标签与数据中的<code class="fe ni nj nk nl b">question_title</code>和/或<code class="fe ni nj nk nl b">question_body</code>特征相关。带有前缀<code class="fe ni nj nk nl b">answer_</code>的目标标签与<code class="fe ni nj nk nl b">answer</code>特征相关。<br/>每一行都包含一个问题和该问题的一个答案，以及附加功能。训练数据包含带有一些重复问题(但答案不同)的行。测试数据不包含任何重复的问题。<br/>目标标签可以有范围<code class="fe ni nj nk nl b">[0,1]</code>内的连续值。因此，预测也必须在该范围内。<br/>提供的文件有:</p><ul class=""><li id="55d3" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">train.csv —训练数据(目标标签是最后 30 列)</li><li id="a357" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">test.csv —测试集(您必须为每个测试集行预测 30 个标签)</li><li id="49e3" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">sample_submission.csv —格式正确的示例提交文件；列名是 30 个目标标签</li></ul><p id="5f55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以使用<a class="ae lv" href="https://www.kaggle.com/c/google-quest-challenge/data" rel="noopener ugc nofollow" target="_blank">这个</a>链接来检查数据集。</p><h1 id="2874" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">探索性数据分析(EDA) </strong></h1><p id="459c" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu"> <em class="lu"> Check-out 深入 EDA +数据抓取的笔记本(</em> </strong> <a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="lu"> Kaggle 链接</em> </strong> </a> <strong class="la iu"> <em class="lu">)。</em> </strong></p><p id="9224" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练数据包含 6079 个列表，每个列表有 41 列。在这 41 列中，前 11 列/特征必须用作输入，后 30 列/特征是目标预测。<br/>让我们来看看输入和目标标签:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/a244781c29006cf2e417cd956da61618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5AIFvq5vvoWICyjktZtAg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7569" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出特征都是介于 0 和 1 之间的浮点类型。</p><p id="4235" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们一个一个地探索输入标签。</p><h2 id="f82c" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">qa_id</h2><p id="c9c6" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">问题答案 id 表示给定数据集中特定数据点的 ID。每个数据点都有一个唯一的 qa_id。此功能不用于培训，稍后将在向 Kaggle 提交输出时使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/88b6b8faec0f1feea3623bfcefed7736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AaPKUzHR6jKegZ7ku_INeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://anime.stackexchange.com/questions/56789/if-naruto-loses-the-ability-he-used-on-kakashi-and-guy-after-kaguyas-seal-what" rel="noopener ugc nofollow" target="_blank">https://anime . stack exchange . com/questions/56789/if-naruto-loss-the-ability-he-used-on-kakashi-and-guy-after-kaguyas-seal-what</a></p></figure><h2 id="2427" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">问题 _ 标题</h2><p id="373a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个字符串数据类型功能，保存所提问题的标题。<br/>为了分析 question_title，我将绘制这个特性的字数直方图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/3b427a9b6bf7b39868ae4ba8680b7722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bio022v9rxBg8mhGhLNOjA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/320c4c40af32b21dd7d93ad4701eefd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2VPrcBPqpLrTVOAXO8jMw.png"/></div></div></figure><p id="7f07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从分析来看，很明显:<br/>——大部分 question_title 特征的词长在 9 左右。<br/> -最小问题长度为 2。<br/> -最大问题长度为 28 个。<br/> - 50%的问题标题的长度在 6 到 11 之间。<br/> - 25%的问题标题长度在 2 到 6 之间。<br/> - 25%的问题标题长度在 11 到 28 之间。</p><h2 id="e8ca" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">问题 _ 正文</h2><p id="4868" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这也是一个字符串数据类型特性，保存所提问题的详细文本。<br/>为了分析 question_body，我将绘制这个特性的字数直方图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/96483f696831d07e7047c5b5a18b8885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vck8a5DcxH6JSQIwvW-COA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/4fcafdff65e3c027a278ba89f51383ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_p2iMsZ8wLfxFlC7acrfQ.png"/></div></div></figure><p id="bf03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从分析来看，很明显:<br/>——大部分 question_body 特征的词长在 93 左右。<br/> -最小问题长度为 1。<br/> -最大问题长度为 4666。<br/> - 50%的问题标题长度在 55 到 165 之间。<br/> - 25%的问题标题长度在 1 到 55 之间。<br/> - 25%的问题标题长度在 165 到 4666 之间。</p><p id="a7c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该分布看起来像幂律分布，可以使用 log 将其转换为高斯分布，然后用作工程特征。</p><h2 id="e068" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">问题 _ 用户名</h2><p id="b20a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个字符串数据类型功能，表示提出问题的用户的姓名。<br/>为了分析 question_answer，我将绘制这个特性中单词数量的直方图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/9f223b03877235653d35a7b3adb803a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2lMiIxsJae7PqDLCZLruw.png"/></div></div></figure><p id="8f8b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我没有发现这个特性有多大用处，所以我不会用它来建模。</p><h2 id="831d" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">问题 _ 用户 _ 页面</h2><p id="e4c0" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个字符串数据类型特性，它保存了提问用户的个人资料页面的 URL。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/9c275d4a4f04f8972391562b0cf8548d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEIgkWtuReLYpi4vO1jczQ.png"/></div></div></figure><p id="5fe7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在个人资料页面上，我注意到了 4 个有用的特性，它们可能会有助于做出正确的预测。特征有:<br/> -信誉:表示用户的信誉。<br/> - gold_score:颁发的金牌数。<br/> - silver_score:颁发的银牌数。<br/> - bronze_score:颁发的铜牌数量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><h2 id="e9e0" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">回答</h2><p id="47a3" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这也是一个字符串数据类型特性，它保存问题答案的详细文本。<br/>为了对<em class="lu">答案</em>进行分析，我将绘制这个特征的字数直方图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/2b72097de6a7063861db87d73d75cb96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoFuOPl3B9cNz1TSeHLzrA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/3183764d94badf523833962dc2073a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPMSqpz78qsCvfOmyD5icg.png"/></div></div></figure><p id="d752" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从分析来看，很明显:<br/> -大部分的 question_body 特征的词长在 143 左右。<br/> -最小问题长度为 2。<br/> -最大问题长度为 8158。<br/> - 50%的问题标题长度在 48 到 170 之间。<br/> - 25%的问题标题长度在 2 到 48 之间。<br/> - 25%的问题标题长度在 170 到 8158 之间。</p><p id="8747" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种分布看起来也像幂律分布，也可以使用对数转换为高斯分布，然后用作工程特征。</p><h2 id="a65c" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">答案用户名</h2><p id="c20a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个字符串数据类型功能，表示回答问题的用户的姓名。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/6e6a6fcb9d2442dd77b8439e494ad7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8am6v-oUIg_XKA1d6LpzBQ.png"/></div></div></figure><p id="d6b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我没有发现这个特性有多大用处，所以我不会用它来建模。</p><h2 id="2b3e" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">答案 _ 用户 _ 页面</h2><p id="d039" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个字符串数据类型特性，类似于特性“question_user_page ”,它保存了提问用户的个人资料页面的 URL。</p><p id="0041" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还使用这个特性中的 URL 从用户的个人资料页面中抓取外部数据，类似于我对特性‘question _ user _ page’所做的。</p><h2 id="9b01" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">全球资源定位器(Uniform Resource Locator)</h2><p id="f37d" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">此功能保存 StackExchange 或 StackOverflow 上的问答页面的 URL。下面我打印了来自 train.csv 的前 10 个<em class="lu"> url </em>数据点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/a5eca2ab403d1dd4cfcbea2e98157a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_RcKZAy01R7OxZ7BaVsCQ.png"/></div></div></figure><p id="b5bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">需要注意的一点是，这个功能会把我们带到问答页面，而这个页面通常会包含更多的数据，比如评论、投票、其他答案等等。如果模型由于训练中的数据较少而表现不佳，则可以使用它来生成更多的特征。csv <br/>让我们看看数据是否存在，以及可以从问答页面中获取哪些附加数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/4b9c049e959864e9ce4ab5cf7a3fcfc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HikC8L5q8zm8GFasu4gM1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://anime.stackexchange.com/questions/3281/whos-inside-the-third-coffin-that-orochimaru-tried-to-summon?rq=1" rel="noopener ugc nofollow" target="_blank">网页来源</a></p></figure><p id="e830" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面附加的快照中，<em class="lu">帖子 1 </em>和<em class="lu">帖子 2 </em>包含对所提问题的回答、支持投票和评论，按支持投票的降序排列。带有绿色勾号的帖子包含 train.csv 文件中提供的答案。</p><p id="eea3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个问题可能有不止一个答案。我们可以收集这些答案，并将其用作额外的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/e07e8304350fada64795f94fbccc8cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_qUPbojaSFLJ38d_LPzhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://anime.stackexchange.com/questions/3281/whos-inside-the-third-coffin-that-orochimaru-tried-to-summon?rq=1" rel="noopener ugc nofollow" target="_blank">网页来源</a></p></figure><p id="c12f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的快照定义了一篇文章的结构。我们可以收集有用的特征，如<em class="lu"> upvotes </em>和<em class="lu"> comments </em>并将其用作附加数据。</p><p id="9091" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是从 URL 页面抓取数据的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="1d11" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我刮到了 8 个新功能- <br/> - upvotes:所提供答案的 upvotes 数。<br/> - comments_0:对所提供答案的评论。<br/> -答案 1:除了提供的答案之外，投票最多的答案。<br/> -评论 _1:回答 _1 的置顶评论。<br/> -答案 2:第二多的投票答案。<br/> -评论 _2:回答 _2 的置顶评论。<br/> -答案 _3:票数第三的答案。<br/> -评论 _3:回答 _3 的置顶评论。</p><h2 id="e182" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">种类</h2><p id="ad55" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这是一个分类特征，它告诉问题和回答对的类别。下面我打印了来自 train.csv 的前 10 个<em class="lu">类别</em>数据点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/21235c630f4171ddc6a4ceedd81c1274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVyREoM-KJIonAMNurlWZQ.png"/></div></div></figure><p id="03df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是绘制类别饼图的代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/0a1dd38a73917ad9c5b9515d24d2c640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQeMNBXBG_9wjgL5RwMuKA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/72fc9ca39aee7022234eb509a3cd59ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raqsI9DfgAi-X9DEteDG-g.png"/></div></div></figure><p id="c3f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图表告诉我们，大部分分属于<em class="lu">技术类</em>，最少属于<em class="lu">生活 _ 艺术类</em>(6079 分中的 709 分)。</p><h2 id="b869" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">宿主</h2><p id="b291" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">此功能保存 StackExchange 或 StackOverflow 上的问答页面的主机或域。下面我打印了来自 train.csv 的前 10 个<em class="lu">主机</em>数据点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/f7743e5b847fe35e5546254d986176ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2qQIIdk3tS1TTPUjbjesQ.png"/></div></div></figure><p id="8269" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是绘制唯一主机条形图的代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/c22f72427c825f1f99c98bfaa662835b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEYLevz8Mb4jEDi45l3CHQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/99a44c8060b5e98ab30a1d5eeef3404d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AC4MzE30X4w-jOdZUKDWPw.png"/></div></div></figure><p id="2753" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来在训练数据中并不多，只有 63 个不同的子域。大多数数据点来自 StackOverflow.com，而最少来自 meta.math.stackexchange.com</p><h2 id="aeb2" class="nn lx it bd ly no np dn mc nq nr dp mg lh ns nt mi ll nu nv mk lp nw nx mm ny bi translated">目标值</h2><p id="f203" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们来分析一下需要预测的目标值。但是首先，为了更好的解释，请使用<a class="ae lv" href="https://www.kaggle.com/c/google-quest-challenge/data?select=train.csv" rel="noopener ugc nofollow" target="_blank">链接</a>查看 kaggle 上的完整数据集。</p><p id="3bb6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是显示目标值统计描述的代码块。这些只是全部 30 个特性中的前 6 个特性。<br/>所有特性的值都是浮点型的，并且在 0 和 1 之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/00c61cd25ffa2b54f25d178e0dd9142a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIH-FKw4qnebXL4HnYJuSw.png"/></div></div></figure><p id="7024" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意第二个代码块，它显示了数据集中存在的唯一值。0 和 1 之间只有 25 个唯一值。这在以后微调代码时会很有用。</p><p id="d7a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，让我们检查目标特性的分布及其相关性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/dbd728906be3e2036a171e759ef2836d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K33iaHeCjBKwQiAE6aW2Yw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/63d9f05390842789a9c2778ec510e03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6rTLOBC2RuXYBDZExY9Hw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标特征的直方图。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d0bcf92e669147e6a8cf4473efb0b81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*olpEN_FeEg1ViqLw_jJV7w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/e8bc8b8fea8d3c11466f79df81e8d366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJGjCI31Tu4yM3bi6vfeAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标特征之间相关性的热图。</p></figure></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><h1 id="23b3" class="lw lx it bd ly lz pf mb mc md pg mf mg jz ph ka mi kc pi kd mk kf pj kg mm mn bi translated">建模</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/aba60164fd31975f93555e67517b3297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdslcbSWBOdPju1zpqycgQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6639" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们通过 EDA 更好地了解了我们的数据，让我们从建模开始。下面是我们将在这一部分讨论的副主题-</p><ul class=""><li id="aed4" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">架构概述:</strong>总体架构及其不同组件的快速概述。</li><li id="2a92" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">基础学习者:</strong>集合中使用的基础学习者的概述。</li><li id="9653" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">准备数据:</strong>数据清理和建模准备。</li><li id="9860" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">集合:</strong>创建训练模型，并进行预测。将数据准备、模型训练和模型预测步骤流水线化。</li><li id="5bb6" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">从 Kaggle 获取分数:</strong>在 Kaggle 上提交测试数据的预测目标值，并生成排行榜分数，以查看整体表现如何。</li></ul><p id="1c53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我尝试了各种深度神经网络架构，包括 GRU、Conv1D、密集层，以及竞争对手的不同功能，但 8 个变压器的组合(如上所示)似乎效果最佳。<br/>在这一部分中，我们将关注所使用的整体的最终架构，对于我实验的其他基线模型，你可以查看我的 github repo。</p><blockquote class="pk pl pm"><p id="1d86" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">架构概述:</strong></p></blockquote><p id="46dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记住我们的任务是给定<strong class="la iu"> <em class="lu">问题标题、问题正文和答案</em> </strong> <em class="lu">、</em>，我们必须预测 30 个目标标签。<br/>现在出来的这 30 个目标标签中，前 21 个都与<strong class="la iu"> <em class="lu">问题 _ 标题</em> </strong>和<em class="lu">问题 _ 正文</em> 和<strong class="la iu"> <em class="lu">答案</em> </strong>没有关系，而最后 9 个目标标签都只与<strong class="la iu"> <em class="lu">答案</em> </strong>有关，但在这 9 个当中，有些还带<strong class="la iu"> <em class="lu">问题 _ 标题</em><br/>例如，像<em class="lu">答案 _ 相关性</em>和<em class="lu">答案 _ 满意度</em>这样的特征只能通过看问题和答案来评级。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/28b972bc51b0db797de3981f6c995be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JsrdmWw2Jwa9uNN8c5LdNw.png"/></div></div></figure><p id="7b39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过一些实验，我发现基础学习者(BERT_base)在预测前 21 个目标特征(仅与问题相关)方面表现得非常好，但是在预测后 9 个目标特征方面表现得不太好。注意到这一点，我构建了 3 个专门的基础学习者和 2 个不同的数据集来训练他们。</p><ol class=""><li id="7b43" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt pr mz na nb bi translated">第一个基础学习者只致力于预测与问题相关的特征(前 21 个)。用于训练该模型的数据集仅由特征<strong class="la iu"> <em class="lu">问题 _ 标题</em> </strong>和<strong class="la iu"> <em class="lu">问题 _ 正文</em> </strong>组成。</li><li id="6340" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt pr mz na nb bi translated">第二个基础学习者只致力于预测与答案相关的特征(最后 9 个)。用于训练该模型的数据集由特征<strong class="la iu"> <em class="lu">问题 _ 标题</em> </strong>、<strong class="la iu"> <em class="lu">问题 _ 正文、</em> </strong>和<strong class="la iu"> <em class="lu">答案</em> </strong>组成。</li><li id="e47f" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt pr mz na nb bi translated">第三个基础学习者致力于预测所有 30 个特征。用于训练该模型的数据集再次由特征<strong class="la iu"> <em class="lu">问题 _ 标题</em> </strong>、<strong class="la iu"> <em class="lu">问题 _ 正文、</em> </strong>和<strong class="la iu"> <em class="lu">答案</em> </strong>组成。</li></ol><p id="fb7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使架构更加健壮，我使用了 3 种不同类型的基础学习者——BERT、RoBERTa 和 XLNet。 <br/>我们将在本博客稍后讨论这些不同的变压器模型。</p><p id="e697" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的系综图中，我们可以看到—</p><ul class=""><li id="2654" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">由<strong class="la iu">【问题标题+问题正文】</strong>和<strong class="la iu">【问题标题+问题正文+答案】</strong>组成的 2 个数据集分别用于训练不同的基础学习者。</li><li id="32c6" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">然后，我们可以看到 3 个不同的基础学习者<strong class="la iu"> (BERT、RoBERTa 和 XLNet) </strong>致力于使用数据集<strong class="la iu">[question _ title+question _ body]</strong>预测仅<strong class="la iu">问题相关特征的蓝色</strong>(前 21 个)</li><li id="7371" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">接下来，我们可以看到 3 个不同的基础学习者<strong class="la iu"> (BERT、RoBERTa 和 XLNet) </strong>专门使用数据集<strong class="la iu">[question _ title+question _ body+answer]预测仅<strong class="la iu">答案相关的特征</strong>(最后 9 个)以绿色显示。</strong></li><li id="7def" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">最后，我们可以看到 2 个不同的基础学习器<strong class="la iu"> (BERT 和 RoBERTa) </strong>致力于使用数据集<strong class="la iu">[question _ title+question _ body+answer]预测<strong class="la iu">所有 30 个红色特征</strong>。</strong></li></ul><p id="3253" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一个步骤中，来自仅专用于预测<strong class="la iu">问题相关特征的模型的预测数据</strong>(表示为<strong class="la iu"> <em class="lu"> bert_pred_q，roberta_pred_q，xlnet _ pred _ q</em></strong>)<strong class="la iu"/>和来自仅专用于预测<strong class="la iu">答案相关特征的模型的预测数据</strong>(表示为<strong class="la iu"> <em class="lu"> bert_pred_a，Roberta _ a 这些连接的特征被表示为<strong class="la iu"> <em class="lu"> xlnet_concat、roberta_concat、</em> </strong>和<strong class="la iu"> <em class="lu"> bert_concat。</em>T48】</strong></em></strong></p><p id="2a40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，收集来自专用于预测所有 30 个特征<strong class="la iu"><strong class="la iu"/>模型的预测数据(表示为<strong class="la iu"> <em class="lu"> bert_qa，roberta_qa </em> </strong>)。请注意，我在这里没有使用 XLNet 模型来预测所有 30 个特性，因为分数没有达到标准。</strong></p><p id="099d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，在收集了所有不同的预测数据—<strong class="la iu"><em class="lu">【xlnet _ concat，roberta_concat，bert_concat，bert_qa，和 roberta_qa】，</em> </strong>之后，通过取所有不同预测值的平均值来计算最终值。</p><blockquote class="pk pl pm"><p id="667a" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">基础学习者</strong></p></blockquote><p id="15e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将看看作为基础学员使用的 3 种不同的变压器模型。</p><ol class=""><li id="fa51" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt pr mz na nb bi translated"><strong class="la iu"> bert_base_uncased: </strong></li></ol><p id="5226" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert </a>是由谷歌人工智能在 2018 年底提出的，从那时起，它已经成为广泛的自然语言处理任务的最先进技术。<br/>它使用一种源自 transformers 的架构，对大量未标记的文本数据进行预训练，以学习一种语言表示，可用于微调特定的机器学习任务。BERT 在几个具有挑战性的任务上超过了 NLP 的最新水平。BERT 的这种性能可以归因于 transformer 的编码器架构、非常规的训练方法，如掩蔽语言模型(MLM)、下一句预测(NSP)以及它所训练的海量文本数据(所有维基百科和书籍语料库)。BERT 有不同的尺寸，但在这个挑战中，我使用了<em class="lu"> bert_base_uncased。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/b5800233c7cdf8debe771a829a1ccd8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nbFb82C1avPQB6aH.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0db7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> bert_base_uncased </em>的架构由 12 个编码器单元组成，每个编码器单元中有 8 个注意头。默认情况下，它接受大小为 512 的输入并返回 2 个值，输出对应于第一个输入令牌[CLS]，其维度为 786，另一个输出对应于所有 512 个输入令牌，其维度为(512，768) aka pooled_output。<br/>但除此之外，我们还可以通过将<strong class="la iu"><em class="lu">output _ hidden _ States = True</em></strong>作为参数之一来访问 12 个编码器单元中的每一个返回的隐藏状态。<br/> BERT 接受几组输入，在这次挑战中，我将使用三种类型的输入:</p><ul class=""><li id="8730" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu"><em class="lu">input _ ids</em></strong><em class="lu">:</em>标记嵌入是输入句子中单词的数字表示。还有一种叫做子词标记化的东西，BERT 使用它首先将较大或复杂的单词分解为简单的单词，然后将它们转换为标记。例如，在上图中，在生成令牌嵌入之前，看看单词“playing”是如何被分解为“play”和“##ing”的。标记化中的这一调整创造了奇迹，因为它利用了一个复杂单词的子单词上下文，而不是像对待一个新单词一样对待它。</li><li id="d36b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu"><em class="lu">attention _ mask</em></strong><em class="lu">:</em>片段嵌入用于帮助 BERT 区分单个输入中的不同句子。对于来自同一个句子的单词，这个嵌入向量的元素都是相同的，并且如果句子不同，该值也会改变。<br/>让我们考虑一个例子:假设我们要将两个句子<em class="lu">“我有一支笔”</em>和<em class="lu">“笔是红色的”</em>传递给 BERT。分词器首先将这些句子分词为:<br/> <strong class="la iu"> ['[CLS]'，'我'，' have '，' a '，' pen '，'[SEP]'，' The '，' pen '，' is '，' red '，'[SEP]'] <br/> </strong>并且这些句子的段嵌入将看起来像:<br/><strong class="la iu">【0，0，0，0，0，0，0，1，1，1，1，1，1] <br/> </strong>注意对应于第一个句子中的单词的所有元素如何具有</li><li id="11a9" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">由于 BERT 采用 512 维输入，并且假设我们只有 10 个单词的输入。为了使标记化的单词与输入大小兼容，我们将在末尾添加大小为 512–10 = 502 的填充。连同填充符一起，我们将生成大小为 512 的屏蔽令牌，其中对应于相关单词的索引将具有<strong class="la iu"> 1 </strong> s，对应于填充符的索引将具有<strong class="la iu"> 0 </strong> s。</strong></li></ul><p id="675c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。XLNet_base_cased: </strong></p><p id="976f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet </a>由谷歌人工智能大脑团队和 CMU 的研究人员在 2019 年年中提出。它的架构比 BERT 更大，并使用改进的方法进行训练。它在更大的数据上进行训练，在许多语言任务中表现出比 BERT 更好的性能。<strong class="la iu"> BERT </strong>和 XLNet 之间的概念差异在于，在训练 BERT 时，按照前一个预测单词有助于下一个单词预测的顺序预测单词，而<strong class="la iu"> XLNet </strong>学习按照任意顺序但以自回归方式(不一定是从左到右)预测单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/53e2f1048db7bd8754a2d4524e8f30be.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/0*V2CD2JYdmFiaPTIa"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="pu">传统语言模型的预测方案。阴影单词作为输入提供给模型，而非阴影单词被屏蔽掉。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/157576343f99b8d8b6fe5b26b83e2122.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/0*-Kpi1jr7I9ZeiyK9"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="pu">一个置换语言模型如何预测某个置换的记号的例子。阴影单词作为输入提供给模型，而非阴影单词被屏蔽掉。</em></p></figure><p id="c2e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这有助于模型学习双向关系，从而更好地处理单词之间的依赖性和关系。<br/>除了训练方法之外，XLNet 使用基于 Transformer XL 的架构和两个主要的关键思想:<em class="lu">相对位置嵌入</em>和<em class="lu">递归机制</em>，即使在没有基于排列的训练的情况下也表现出良好的性能。<br/> XLNet 用超过 130 GB 的文本数据和运行 2.5 天的 512 个 TPU 芯片进行训练，这两个数据都比 BERT 大得多。</p><p id="0c38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 XLNet，我将只使用<strong class="la iu"> input_ids </strong>和<strong class="la iu"> attention_mask </strong>作为输入。</p><p id="b080" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3。罗伯塔 _ 基地:</strong></p><p id="be4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">罗伯塔是脸书在 2019 年年中提出的。这是一种鲁棒优化的方法，用于预处理自然语言处理(NLP)系统，改进了 BERT 的自监督方法。<br/> RoBERTa 基于 BERT 的语言掩蔽策略，其中系统学习预测未标注语言示例中有意隐藏的文本部分。RoBERTa 修改了 BERT 中的关键超参数，包括删除 BERT 的下一句预测(NSP)目标，以及使用更大的小批量和学习率进行训练。与 BERT 相比，这允许 RoBERTa 改进屏蔽语言建模目标，并导致更好的下游任务性能。罗伯塔也比伯特接受了更多的数据和更长的时间的训练。使用的数据集来自现有的未标注的 NLP 数据集以及 CC-News，这是一个从公共新闻文章中提取的新数据集。</p><p id="e8ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 RoBERTa_base，我将只使用<strong class="la iu"> input_ids </strong>和<strong class="la iu"> attention_mask </strong>作为输入。</p><p id="9d37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">最后这里是伯特、XLNet、罗伯塔的对比:</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/af9dc336d6230f6e72ccfb90038fb5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNiXASsDWI86aMKZihMC1Q.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/66e33bcb63a183333639f09e6bb2c2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EfEZgjlXlGl0sXjG.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fbert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8&amp;psig=AOvVaw2cf6K31PfXF2YtrANVGKZe&amp;ust=1596463051519000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCJDg_rrW_OoCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">来源链接</a></p></figure><blockquote class="pk pl pm"><p id="7897" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">准备数据</strong></p></blockquote><p id="4e9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们已经对架构有了一些了解，让我们看看如何为基础学习者准备数据。</p><ul class=""><li id="9c1b" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">作为预处理步骤，我刚刚处理了特性中的 HTML 语法。我使用 html.unescape()从 HTML DOM 元素中提取文本。<br/>在下面的代码片段中，函数<strong class="la iu"> get_data() </strong>读取训练和测试数据，并对特征<strong class="la iu"> <em class="lu"> question_title、</em> </strong>和<strong class="la iu"> <em class="lu">答案进行预处理。</em> </strong></li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><ul class=""><li id="a06a" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">下一步是从输入句子中创建<strong class="la iu"> <em class="lu">输入 _ 标识、注意 _ 掩码、</em> </strong>和<strong class="la iu"> <em class="lu">令牌 _ 类型 _ 标识</em> </strong>。<br/>在下面的代码片段中，函数<strong class="la iu"> get_tokenizer() </strong>为不同的 base_learners 收集预训练的 tokenizer。<br/>第二个函数<strong class="la iu"> fix_length() </strong>检查生成的问题令牌和答案令牌，并确保令牌的最大数量为 512。确定记号数量的步骤如下:<br/> -如果输入句子的记号数量为&gt; 512，则将句子削减至 512。<br/> -为了削减令牌的数量，从开始的 256 个令牌和从结束的 256 个令牌被保留，其余的令牌被丢弃。<br/> -例如，假设一个答案有 700 个记号，为了将其减少到 512 个，从开头取 256 个记号，从结尾取 256 个记号，并连接起来得到 512 个记号。位于答案中间的其余[700-(256+256) = 288]个标记将被丢弃。<br/> -这种逻辑是有道理的，因为在一篇大文章中，开头部分通常描述文章的全部内容，结尾部分描述文章的结论。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="960d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来是用于生成<strong class="la iu">输入 _ 标识、注意 _ 屏蔽、</strong>和<strong class="la iu">令牌 _ 类型 _ 标识的代码块。</strong>我使用了一个条件来检查函数是否需要返回为依赖数据集<strong class="la iu">【问题标题+问题正文】</strong>或数据集<strong class="la iu">【问题标题+问题正文+答案】的基础学习者生成的数据。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="c382" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，这个函数利用上面初始化的函数，为所提供数据中的每个实例生成<strong class="la iu"> input_ids、attention_masks、</strong>和<strong class="la iu"> token_type_ids </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="d811" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使模型训练变得容易，我还创建了一个类，它在使用 KFlod CV 时，在上面指定的函数的帮助下，基于折叠生成训练和交叉验证数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><blockquote class="pk pl pm"><p id="3860" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">组装</strong></p></blockquote><p id="38ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在数据预处理之后，让我们从基础学习者开始创建模型架构。</p><p id="9d67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的代码将模型名称作为输入，根据输入名称收集预训练模型及其配置信息，并创建基本学习者模型。注意<strong class="la iu">output _ hidden _ States = True</strong>在添加配置数据后通过。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0822" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一个代码块是创建整体架构。该函数接受两个参数 name 和 model_type，name 表示我们要训练的模型的名称，model _ type 表示我们要训练的模型的类型。模型类型可以是<strong class="la iu"> bert-base-uncased、roberta-base </strong>或<strong class="la iu"> xlnet-base-cased </strong>，而模型类型可以是<strong class="la iu">问题、答案、</strong>或<strong class="la iu">问题 _ 答案。<br/> </strong>函数<strong class="la iu"> create_model() </strong>采用 model_name 和 model_type，生成一个可以根据指定数据进行训练的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="ef0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们创建一个用于计算评估指标<a class="ae lv" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Spearman 相关系数</a>的函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="ac9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们需要一个功能，可以收集基础学习者模型，根据基础学习者模型的数据，并训练模型。<br/>我用了 5 折的 K 折交叉验证进行训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="4fe6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，一旦我们训练了模型并生成了预测值，我们就需要一个函数来计算加权平均值。这是代码。<br/>*加权平均值中的权重均为 1。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0705" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在把所有东西放在一起之前，我还使用了一个函数来处理最终的预测值。记得在 EDA 部分有一个目标值分析，我们注意到目标值只有 25 个介于 0 和 1 之间的唯一浮点数。为了利用这些信息，我计算了 61 个(超参数)均匀分布的百分位值，并将它们映射到 25 个唯一值。这在目标值的上限和下限之间创建了 61 个均匀间隔的箱。现在，为了处理预测的数据，我使用这些容器来收集预测值，并将它们放在正确的位置/顺序。这一招在一定程度上帮助提高了最终提交排行榜的分数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="5c64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，为了将数据预处理、模型训练和后处理结合在一起，我创建了收集数据的<strong class="la iu"> get_predictions() </strong>函数。<br/> -创建 8 个基础学习者。<br/> -为基础学员准备数据。<br/> -训练基础学习者并从他们那里收集预测值。<br/> -计算预测值的加权平均值。<br/> -处理加权平均预测。<br/> -将最终预测值转换成 Kaggle 要求的数据帧格式提交并返回。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><blockquote class="pk pl pm"><p id="1282" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">从 Kaggle 获取分数</strong></p></blockquote><p id="d4f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦代码编译运行成功，就会生成一个输出文件，可以提交给 Kaggle 进行<strong class="la iu">分数</strong>计算。排行榜上代码的排名是使用<strong class="la iu">分数生成的。<br/> </strong>合奏模特获得<strong class="la iu"> 0.43658 </strong>的公众评分，在排行榜上位列前 4.4%。</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><h1 id="102e" class="lw lx it bd ly lz pf mb mc md pg mf mg jz ph ka mi kc pi kd mk kf pj kg mm mn bi translated">建模后分析</h1><p id="1a46" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu"> <em class="lu">检出完成后建模分析的笔记本(</em> </strong> <a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="lu"> Kaggle 链接</em> </strong> </a> <strong class="la iu"> <em class="lu">)。</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/913054bb02160705966b7c35f8698378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gTtCiVIl_oCN6Wpq4R-9g.png"/></div></div></figure><p id="f169" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是时候进行一些后建模分析了！</p><p id="9245" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这一节中，我们将分析训练数据，以找出模型在数据的哪些部分表现良好，在数据的哪些部分表现不佳。<br/>这一步背后的主要思想是了解训练模型的能力，如果应用得当，它可以很好地微调模型和数据。<br/>但在本节中，我们不会进入微调部分，我们将使用列车数据的预测目标值对列车数据执行一些基本的 EDA。我将逐一介绍这些数据。以下是我们将要进行分析的主要功能-</p><ul class=""><li id="7d84" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">问题标题、问题正文和答案。</li><li id="cb4a" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">问题标题、问题正文和答案的单词长度。</li><li id="9b13" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">主持</li><li id="cdce" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">种类</li></ul><p id="ff22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们必须将数据分为好数据和坏数据。好数据将是模型获得好分数的数据点，而坏数据将是模型获得坏分数的数据点。<br/>现在为了评分，我们将比较训练数据的实际目标值和训练数据的模型预测目标值。我使用<strong class="la iu">均方差(MSE) </strong>作为评分标准，因为它关注的是实际值和目标值的接近程度。记住 MSE 分数越大，数据点就越差。计算 MSE 分数相当简单。代码如下:</p><pre class="kj kk kl km gt pz nl qa qb aw qc bi"><span id="9fe3" class="nn lx it nl b gy qd qe l qf qg"># Generating the MSE-score for each data point in train data.<br/>from sklearn.metrics import mean_squared_error</span><span id="08c1" class="nn lx it nl b gy qh qe l qf qg">train_score = [mean_squared_error(i,j) for i,j in zip(y_pred, y_true)]</span><span id="777f" class="nn lx it nl b gy qh qe l qf qg"># sorting the losses from minimum to maximum index wise.<br/>train_score_args = np.argsort(train_score)</span></pre><blockquote class="pk pl pm"><p id="4e09" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">问题 _ 标题、问题 _ 正文和答案</strong></p></blockquote><p id="65de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从第一组特征开始，它们都是文本类型的特征，我将用它们来绘制单词云。计划是从得分最低的 5 个数据点和得分最高的另外 5 个数据点中分割出这些特征。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制单词云的功能</p></figure><p id="8586" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们运行代码，看看结果是什么样的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qi"><img src="../Images/1e00c75d1b96aace543732b1e3b7fb72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lOfSHKyTTV3YhsU4PxaDQ.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/b59c41e2a9168bffb40b2578bc1f6be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUtWG5KWy_c34dLGH2qJgA.png"/></div></div></figure><blockquote class="pk pl pm"><p id="f2c0" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">问题标题、问题正文和答案的单词长度</strong></p></blockquote><p id="f9c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来的分析是关于问题标题、问题正文和答案的单词长度。为此，我将为问题标题、问题正文和答案这三个特征中的每一个选择 30 个 MSE 分数最低的数据点和 30 个 MSE 分数最高的数据点。接下来，我将计算所有 3 个特征的这 30 个数据点的单词长度，并绘制它们以观察趋势。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qk"><img src="../Images/188ca65c8806788455e2a15f1d2f0fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfhNtTvxqVYPJYX1Emq7rA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ql"><img src="../Images/b8d07afd030cd43f7bf0c75130ac136a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uF-kg7A_2geR4UBVJty0Nw.png"/></div></div></figure><p id="ee56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们查看 question_title、question_body 和 answer 中的字数，我们可以观察到，产生高损失的数据具有高字数，这意味着问题和答案有点彻底。所以，当问题和答案简洁时，模型做得很好。</p><blockquote class="pk pl pm"><p id="7bd8" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">主持人</strong></p></blockquote><p id="e2f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一个分析是关于特性主机的。对于这个特性，我将挑选 100 个具有最低 MSE 分数的数据点和 100 个具有最高 MSE 分数的数据点，并选择特性主机中的值。然后，我将绘制这一分类特征的直方图，以查看分布情况。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qm"><img src="../Images/13370a49b752cee5f5481bc6ac35d8cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JdLsXYflDINgWEOMhWryUw.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qn"><img src="../Images/4218de39c2f57c988f80d25038c34e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-X1hQl1V8nchYotkUORpg.png"/></div></div></figure><p id="434d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">我们可以看到，来自英语、生物、科幻、物理领域的大量数据点造成了较小的损失值，而来自 drupal、程序员、tex 的大量数据点造成了较大的损失。</em></p><p id="eda2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们也来看看造成低分和高分的独特主机值的词云。使用顶部和底部的 100 个数据点再次进行该分析。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/33fa8165137a35dc70d1598e5f788d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eC0RkpXcA9jhzioVKIXTfQ.png"/></div></div></figure><blockquote class="pk pl pm"><p id="f621" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">类别</strong></p></blockquote><p id="af61" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后分析的是特征类别。对于这个特性，我将挑选 100 个具有最低 MSE 分数的数据点和 100 个具有最高 MSE 分数的数据点，并选择特性类别中的值。然后我会绘制一个这种分类特征的饼状图来查看比例。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/6075496394a079bdb7e944f22c1a90e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cR2JuXG7-r4_Rq5-yjE8kA.png"/></div></div></figure><p id="f364" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以注意到，类别为技术的数据点占了模型不能很好预测的数据的 50%,而像生活艺术、科学和文化这样的类别对坏预测的贡献要小得多。<br/>对于良好的预测，所有 5 个类别的贡献几乎相同，因为在比例上没有重大差异，但我们仍然可以说，以 StackOverflow 作为类别的数据点贡献最小。</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="5cbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至此，我们已经结束了这个博客和 3 部分系列。希望阅读愉快。<br/>你可以使用 <a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="lu">这个链接</em> </strong> </a> <em class="lu">在 Kaggle 上查看完整的笔记本，如果发现我的工作有用，可以留下一个向上的投票。我要感谢所有的创作者，感谢他们创作了我写这篇博客时提到的精彩内容。</em></p><p id="ce82" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">参考链接:</em></p><ul class=""><li id="115a" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><em class="lu">应用人工智能课程:</em><a class="ae lv" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank"><em class="lu">【https://www.appliedaicourse.com/】</em></a></li><li id="1653" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><a class="ae lv" href="https://www.kaggle.com/c/google-quest-challenge/notebooks" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/google-quest-challenge/notebooks</a></li><li id="0f55" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><a class="ae lv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"><em class="lu">http://jalammar.github.io/illustrated-transformer/</em></a></li><li id="13be" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><a class="ae lv" href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04" rel="noopener"><em class="lu">https://medium . com/inside-machine-learning/what-a-transformer-d 07 DD 1 fbec 04</em></a></li><li id="9ee6" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8">https://towards data science . com/Bert-Roberta-distil Bert-xlnet-one-to-use-3d 5 ab 82 ba 5 f 8</a></li></ul><blockquote class="pk pl pm"><p id="f1ea" class="ky kz lu la b lb lc ju ld le lf jx lg pn li lj lk po lm ln lo pp lq lr ls lt im bi translated"><strong class="la iu">最终注释</strong></p></blockquote><p id="17dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您阅读博客。我希望它对那些渴望做项目或学习 NLP 新概念的人有用。</p><p id="51ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第 1/3 部分中，我们报道了变形金刚如何在各种现代自然语言处理任务及其工作中成为最先进的。</p><p id="0af2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第 2/3 部分中，我们讨论了 BERT(变压器的双向编码器表示)。</p><p id="1435" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kaggle 深入 EDA 笔记本链接:<a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sarthakvajpayee/top-4-4-深入 eda-feature-scraping？scriptVersionId=40263047 </a></p><p id="6103" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kaggle 建模笔记本链接:<a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sarthakvajpayee/top-4-4-Bert-Roberta-xlnet</a></p><p id="762b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kaggle 后建模笔记本链接:<a class="ae lv" href="https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sarthakvajpayee/top-4-4-后建模-分析？scriptVersionId=40262842 </a></p><p id="e29c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 LinkedIn 上找到我:<a class="ae lv" href="http://www.linkedin.com/in/sarthak-vajpayee" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/sarthak-vajpayee</a></p><p id="95c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 Github 上找到这个项目:<a class="ae lv" href="https://github.com/SarthakV7/Kaggle_google_quest_challenge" rel="noopener ugc nofollow" target="_blank">https://github.com/SarthakV7/Kaggle_google_quest_challenge</a></p><p id="0e3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和平！☮</p></div></div>    
</body>
</html>