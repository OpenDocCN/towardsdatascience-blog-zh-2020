<html>
<head>
<title>Kafka, for your data pipeline? Why not?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卡夫卡，为了你的数据管道？为什么不呢？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kafka-for-your-data-pipeline-why-not-5a14b50efe7f?source=collection_archive---------13-----------------------#2020-06-29">https://towardsdatascience.com/kafka-for-your-data-pipeline-why-not-5a14b50efe7f?source=collection_archive---------13-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2235" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Docker、Kafka 和 Kafka Connect 创建流管道</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae06133ce6291df41ce063d1cf002e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSCJnRhJnCD8lldG8PH8zQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们在这个项目中所建造的</p></figure><p id="0c2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kafka 于 2011 年由 LinkedIn 开发并开源，此后它迅速从消息队列发展成为一个成熟的流媒体平台，拥有丰富的生态系统。除了 LinkedIn 之外，许多科技公司，如 Airbnb、Spotify 或 Twitter，都将 Kafka 用于其关键任务应用程序。</p><p id="6689" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kafka 可以用于很多事情，从消息传递、web 活动跟踪到日志聚合或流处理。从我作为数据专业人士的角度来看，Kafka 可以用作数据流管道的核心组件，以支持实时用例，如欺诈检测、预测性维护或实时分析。</p><p id="b0d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有整本书都是关于卡夫卡的，开始阅读可能会让人望而生畏。然而，在这个项目中，我将向您展示使用 Docker、Kafka 和 Kafka Connect 创建流数据管道是多么容易。</p><h1 id="aee6" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">商业问题</h1><p id="199a" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">让我们定义一些要努力解决的业务问题。</p><p id="4200" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设你是一名数据工程师，在一家不断有用户注册的电子商务网站工作。营销团队希望向每一位注册的客户发送一封个性化的电子邮件。新提出的功能有一些问题:</p><ul class=""><li id="451e" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">注册服务与电子邮件服务相结合。换句话说，每次市场需求发生变化，你都必须对注册服务做出改变，如果你不小心的话，这可能会导致整个事情的发生。</li><li id="bf04" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">市场营销要求电子邮件应该在用户注册时立即发送。用工程术语来说，他们指的是从客户注册开始的 5 秒钟内。</li><li id="39b4" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">个性化模型与您的注册服务托管在不同的网络上。</li></ul><p id="02b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与客户数据相关的另一个业务问题是，您的 CEO 希望每个员工都知道当前的客户数量以及他们从哪里注册。她让管理团队在办公室中央安装几个大显示器，而您的团队必须创建一个将在这些显示器上显示的仪表板。</p><ul class=""><li id="0dd9" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">您当前的数据仓库每天只获取用户数据，因此您不能使用现有的批处理管道。</li><li id="8e68" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">仪表板服务也在不同的网络上，因此您不能直接查询生产数据库。</li></ul><p id="96b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在研究了几个选项后，您意识到使用 Kafka 和运行在 Docker 上的 Kafka Connect 似乎是解决您的问题的最佳选项。以下是你研究后的发现。</p><h2 id="31df" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">卡夫卡是什么？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/6842f5d5b6b762d34b025234fcce8636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vUjKXckBRPXoj1TO.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ns" href="https://www.confluent.jp/blog/using-apache-kafka-drive-cutting-edge-machine-learning/" rel="noopener ugc nofollow" target="_blank">图像信用</a></p></figure><p id="7dad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kafka 是一个用 Scala 和 Java 编写的开源流处理平台。根据 Kafka <a class="ae ns" href="https://kafka.apache.org/intro" rel="noopener ugc nofollow" target="_blank">网站</a>的说法，流媒体平台有三个关键能力:</p><ul class=""><li id="11dc" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">发布和订阅记录流，类似于消息队列或企业消息传递系统。</li><li id="e661" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">以容错的持久方式存储记录流。</li><li id="6704" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">在记录流出现时处理它们。</li></ul><p id="cc77" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Kafka 通常用于构建对数据流做出反应的实时应用程序，或者可靠地在系统或应用程序之间获取数据的实时数据管道。在我们的用例中，我们需要将生产系统(Postgres DB)中的数据传输到 se 团队正在开发的一个单独的电子邮件服务(MySQL DB ),以及数据团队的数据湖 S3。</p><p id="b8ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于卡夫卡，这里有一些你应该熟悉的概念:</p><ul class=""><li id="a159" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">Broker: Kafa broker 接收来自生产者的消息，并通过惟一偏移量存储它们。代理还允许消费者通过主题、分区和偏移量获取消息。</li><li id="f494" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">消息:是卡夫卡中的一个数据单位。您可以将每条消息视为数据库中的一条记录。</li><li id="8bda" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">主题和分区:每个主题都是一个命名的消息流。一个主题由一个或多个分区组成。分区允许 Kafka 通过跨代理分发数据来进行水平扩展。</li></ul><h2 id="d45b" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">卡夫卡连线？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/0729ef368221537bc28660f76f24f14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dORd7k_z4d7R2J9t.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ns" href="https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/" rel="noopener ugc nofollow" target="_blank">形象信用</a></p></figure><p id="3ab1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae ns" href="https://docs.confluent.io/current/connect/index.html#:~:text=Kafka%20Connect%2C%20an%20open%20source,Kafka%20Connect%20for%20Confluent%20Platform." rel="noopener ugc nofollow" target="_blank"> Kafka Connect </a>，Kafka 的一个开源组件，是一个将 Kafa 与数据库、键值存储、搜索索引、文件系统等外部系统连接起来的框架。以下是一些与 Kafka Connect 相关的概念:</p><ul class=""><li id="af56" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">连接器:连接器是一个逻辑作业，负责管理 Kafka 和其他系统之间的数据复制</li><li id="6b99" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">源连接器:将数据从系统复制到 Kafka 的连接器</li><li id="22b1" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">接收器连接器:将数据从一个或多个 Kafka 主题复制到系统的连接器</li><li id="96d7" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">任务:每个连接器实例协调一组任务，将数据从一个系统复制到 Kafka，反之亦然</li></ul><h2 id="da92" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">Docker 和 docker-compose？</h2><p id="bd35" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">Docker 是一种容器技术，它允许我们将一个应用程序打包成它需要的所有部分，比如库和依赖项。您可以使用 Docker 在本地计算机上快速有效地部署这些服务，而不必在本地计算机上安装 Kafka、Kafka Connect 和所有数据库。</p><p id="f408" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Docker-compose 是一个高级命令，它允许您使用 YAML 配置文件，通过一个命令来部署 Docker 容器。</p><h2 id="8a03" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">建筑</h2><p id="7175" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">抛开所有术语，让我们来看看这个解决方案的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae06133ce6291df41ce063d1cf002e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSCJnRhJnCD8lldG8PH8zQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们解决方案的架构</p></figure><p id="751d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于我们的示例，我们将使用 Kafka connect 从本地生产数据库中捕获 Users 表中的更改，并写入 Kafka 主题。两个连接器将订阅上面的主题，并将任何更改写入我们的电子邮件服务的 MySQL 数据库以及我们的数据湖 S3。</p><h1 id="6ec7" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">有趣的部分</h1><p id="5968" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">现在，这是有趣的部分！让我们开始吧。</p><h2 id="406c" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">克隆我的回购</h2><p id="49d3" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">首先，通过在您的终端上键入以下命令来克隆我的回购:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0408" class="nf lv it nv b gy nz oa l ob oc"><a class="ae ns" href="https://github.com/tuanchris/kafka-pipeline" rel="noopener ugc nofollow" target="_blank">g</a>it clone <a class="ae ns" href="https://github.com/tuanchris/kafka-pipeline" rel="noopener ugc nofollow" target="_blank">https://github.com/tuanchris/kafka-pipeline</a><br/>cd kafka-pipeline</span></pre><div class="od oe gp gr of og"><a href="https://github.com/tuanchris/kafka-pipeline" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">图安克里斯/卡夫卡管道</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">Kafka 最近越来越受欢迎，因为企业依赖它来驱动任务关键型应用程序和数据…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">github.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><h2 id="db02" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">安装 Docker 和 docker-compose</h2><p id="764e" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们将在这个项目中使用 Docker 和<a class="ae ns" href="https://docs.docker.com/compose/install/" rel="noopener ugc nofollow" target="_blank"> docker-compose </a>，你可以快速查找如何为你的操作系统安装它们。</p><h2 id="447c" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">创造环境</h2><p id="001b" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">假设您已经安装了<code class="fe ov ow ox nv b">conda</code>,那么您可以创建一个新的 env 并通过运行以下命令来安装所需的包:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="5e2c" class="nf lv it nv b gy nz oa l ob oc">conda create -n kafka-pipeline python=3.7 -y<br/>conda activate kafka-pipeline<br/>pip install -r requirements.txt</span></pre><p id="f6f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将需要 PostgreSQL 来连接到我们的源数据库(Postgres)并生成流数据。在 Mac OS 上，您可以通过运行以下命令使用 Homebrew 安装 PostgreSQL:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="210a" class="nf lv it nv b gy nz oa l ob oc">brew install postgresql<br/>pip install psycopg2</span></pre><p id="361a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以谷歌一下如何为其他平台安装 PostgreSQL</p><h2 id="213c" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">启动生产数据库(Postgres)</h2><p id="7bc2" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们使用 docker-compose 以最小的努力启动服务。您可以使用以下命令启动 Postgres 生产数据库:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0f95" class="nf lv it nv b gy nz oa l ob oc">docker-compose -f docker-compose-pg.yml up -d</span></pre><p id="1498" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您的 Postgres 数据库应该运行在端口 5432 上，您可以通过在终端上键入<code class="fe ov ow ox nv b">docker ps</code>来检查容器的状态。</p><h2 id="95fe" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">生成流数据</h2><p id="33ae" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我写了一个简短的脚本来使用 Faker 库生成用户数据。该脚本将在我们的 Postgres 数据库中每秒生成一条记录，模拟一个生产数据库。您可以使用以下命令在单独的终端标签中运行脚本:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d39f" class="nf lv it nv b gy nz oa l ob oc">python generate_data.py</span></pre><p id="97d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果一切设置正确，您将看到如下输出:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d8e8" class="nf lv it nv b gy nz oa l ob oc">Inserting data {'job': 'Physiotherapist', 'company': 'Miller LLC', 'ssn': '097-38-8791', 'residence': '421 Dustin Ramp Apt. 793\nPort Luis, AR 69680', 'username': 'terri24', 'name': 'Sarah Moran', 'sex': 'F', 'address': '906 Andrea Springs\nWest Tylerberg, ID 29968', 'mail': 'nsmith@hotmail.com', 'birthdate': datetime.date(1917, 6, 3), 'timestamp': datetime.datetime(2020, 6, 29, 11, 20, 20, 355755)}</span></pre><p id="0500" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">据此，我们模拟了每秒钟都有新客户数据的生产数据库。很整洁，是吧？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/3197a13f6dde965478d046b23d067d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*o0IeDaFva-m_muGwUcYajQ.gif"/></div></div></figure><p id="8cda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以使用 SQL 客户机(如 DataGrip 或 DBeaver)连接到 Postgres 数据库，以双重检查数据是否正在写入 Users 表。连接字符串应该是<code class="fe ov ow ox nv b">jdbc:postgresql://TEST:password@postgres:5432/TEST</code></p><h2 id="512a" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">开始我们的卡夫卡经纪人</h2><p id="458c" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">很好，现在我们已经有了一个运行着数据流的生产数据库，让我们开始模拟的主要组件。我们将运行以下服务:</p><ul class=""><li id="30ac" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated">Kafka broker: Kafa broker 接收来自生产者的消息，并按唯一偏移量存储它们。代理还允许消费者通过主题、分区和偏移量获取消息。</li><li id="412c" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">Zookeeper: Zookeeper 跟踪 Kafka 集群节点以及 Kafka 主题和分区的状态</li><li id="a2b1" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">Schema registry: Schema registry 是一个获取和服务元数据(关于数据的数据)的层，比如数据类型、精度、小数位数…并提供不同服务之间的兼容性设置。</li><li id="a5f0" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">Kafka Connect: Kafka Connect 是一个框架，用于连接 Kafka 与外部系统，如数据库、键值存储、搜索索引和文件系统。</li><li id="f84e" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated">Kafdrop: Kafdrop 是一个开源的 web UI，用于查看 Kafka 主题和浏览消费群体。这将使检查和调试我们的消息更加容易。</li></ul><p id="3548" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过运行以下命令来启动所有这些服务:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f9e6" class="nf lv it nv b gy nz oa l ob oc">docker-compose -f docker-compose-kafka.yml up -d</span></pre><p id="ad0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">等待几分钟，让 docker 下载图像并启动服务，然后您可以继续下一步。您可以使用以下命令查看上一个命令完成后的日志输出:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="755b" class="nf lv it nv b gy nz oa l ob oc">docker-compose -f docker-compose-kafka.yml logs -f</span></pre><h2 id="35eb" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">配置源连接器</h2><p id="d804" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">接下来，我们将使用 Kafka connect rest API 配置我们的生产数据库(Postgres)的源连接器。将以下内容粘贴到您的终端:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f01f" class="nf lv it nv b gy nz oa l ob oc">curl -i -X PUT http://localhost:8083/connectors/SOURCE_POSTGRES/config \<br/>     -H "Content-Type: application/json" \<br/>     -d '{<br/>            "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",<br/>            "connection.url":"jdbc:postgresql://postgres:5432/TEST",<br/>            "connection.user":"TEST",<br/>            "connection.password":"password",<br/>            "poll.interval.ms":"1000",<br/>            "mode":"incrementing",<br/>            "incrementing.column.name":"index",<br/>            "topic.prefix":"P_",<br/>            "table.whitelist":"USERS",<br/>            "validate.non.null":"false"<br/>        }'</span></pre><p id="e41d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您看到<code class="fe ov ow ox nv b">HTTP/1.1 201 Created</code>时，连接器已成功创建。这个命令的作用是向 Kafka Connect 实例发送一个包含我们的配置的 JSON 消息。我将在这里解释一些配置，但是您可以在这里参考配置的完整列表<a class="ae ns" href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/source_config_options.html" rel="noopener ugc nofollow" target="_blank">。</a></p><ul class=""><li id="0a5f" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">connector.class</code>:我们使用 JDBC 源连接器连接到我们的生产数据库并提取数据。</li><li id="802a" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">connection.url</code>:我们源数据库的连接字符串。由于我们用的是 Docker 的内网，所以数据库地址是 Postgres。如果要连接到外部数据库，请用数据库的 IP 替换 Postgres。</li><li id="0156" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">connection.user</code> &amp; <code class="fe ov ow ox nv b">connection.password</code>:我们数据库的凭证。</li><li id="a619" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">poll.interval.ms</code>:轮询新数据的频率。我们每秒都在轮询。</li><li id="de50" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">mode</code>:轮询时更新每个表的模式。我们使用增量键(索引)，但是我们也可以使用时间戳或批量更新来更新。</li><li id="cb89" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">topic.prefix</code>:向卡夫卡写数据的题目前缀。</li><li id="5ef5" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">table.whitelist</code>:要在我们的数据库中查找的表名列表。您还可以设置一个<code class="fe ov ow ox nv b">query</code>参数来使用自定义查询。</li></ul><p id="d057" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着 Kafdrop 实例的运行，您可以打开浏览器并转到<code class="fe ov ow ox nv b">localhost:9000</code>来查看我们的<code class="fe ov ow ox nv b">P_USERS</code>主题。你可以进入主题，看看我们主题的一些示例消息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/6390ac80a2e81ffb41afc762810fabca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*I2WhoraQwf8RkDtqlADCtA.gif"/></div></div></figure><p id="ace9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就这样，你就有了流向卡夫卡的用户数据流。</p><h2 id="506f" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">创建 MySQL 接收器连接器</h2><p id="79eb" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">先从 Mysql 说起吧。通过运行以下命令启动 Mysql 数据库:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9323" class="nf lv it nv b gy nz oa l ob oc">docker-compose -f docker-compose-mysql.yml up -d</span></pre><p id="0d42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们的配置:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="2596" class="nf lv it nv b gy nz oa l ob oc">curl -i -X PUT http://localhost:8083/connectors/SINK_MYSQL/config \<br/>     -H "Content-Type: application/json" \<br/>     -d '{<br/>       		"connector.class":"io.confluent.connect.jdbc.JdbcSinkConnector",<br/>       		"tasks.max":1,<br/>       		"topics":"P_USERS",<br/>           "insert.mode":"insert",<br/>       		"connection.url":"jdbc:mysql://mysql:3306/TEST",<br/>       		"connection.user":"TEST",<br/>       		"connection.password":"password",<br/>       		"auto.create":true<br/>     	}'</span></pre><p id="723d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就是这样。您生成的数据现在应该从 Postgres 流到 Mysql。让我们回顾一下 Mysql sink 连接器的属性:</p><ul class=""><li id="052b" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">insert.mode</code>:如何将数据插入数据库。你可以在<code class="fe ov ow ox nv b">insert</code>和<code class="fe ov ow ox nv b">upsert</code>之间选择。</li><li id="549e" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">topics</code>:从中读取数据的主题</li><li id="c850" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">connection.url</code>:接收器连接 URL</li><li id="d7e3" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">connection.user</code> &amp; <code class="fe ov ow ox nv b">connection.password</code>:汇凭证</li><li id="3f5e" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">auto.create</code>:不存在时自动创建表格</li></ul><p id="5594" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们查询 MySQL 数据库，看看我们的数据是否在那里。我们可以看到记录计数和最大时间戳都在更新！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/dafd55b2c1eae3e4b74068de56232ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QSyN9QTVZUfS-C02rReKhw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过查询 SQL 数据库，我们可以实时看到新的数据</p></figure><h2 id="2bdf" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">创建 S3 水槽连接器</h2><p id="3c49" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">要将数据写入 S3，同样简单明了。您需要在<code class="fe ov ow ox nv b">docker-compose-kafka.yml</code>文件中设置环境变量:<code class="fe ov ow ox nv b">AWS_ACCESS_KEY_ID</code>和<code class="fe ov ow ox nv b">AWS_SECRET_ACCESS_KEY</code>。之后，您可以使用以下配置创建 S3 连接器:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="e95a" class="nf lv it nv b gy nz oa l ob oc">curl -i -X PUT -H "Accept:application/json" \<br/>    -H  "Content-Type:application/json" http://localhost:8083/connectors/SINK_S3/config \<br/>    -d '<br/>{<br/>    "connector.class": "io.confluent.connect.s3.S3SinkConnector",<br/>    "s3.region": "ap-southeast-1",<br/>    "s3.bucket.name": "bucket-name",<br/>    "topics": "P_USERS",<br/>    "flush.size": "5",<br/>    "timezone": "UTC",<br/>    "tasks.max": "1",<br/>    "value.converter.value.subject.name.strategy": "io.confluent.kafka.serializers.subject.RecordNameStrategy",<br/>    "locale": "US",<br/>    "format.class": "io.confluent.connect.s3.format.json.JsonFormat",<br/>    "partitioner.class": "io.confluent.connect.storage.partitioner.DefaultPartitioner",<br/>    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter",<br/>    "storage.class": "io.confluent.connect.s3.storage.S3Storage",<br/>    "rotate.schedule.interval.ms": "6000"<br/>}'</span></pre><p id="e9cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一些值得注意的配置:</p><ul class=""><li id="2f1c" class="mr ms it la b lb lc le lf lh mt ll mu lp mv lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">s3.region</code>:你的 S3 桶的区域</li><li id="8644" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">s3.bucket.name</code>:写入数据的桶名</li><li id="621d" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">topics</code>:读取数据的主题</li><li id="b682" class="mr ms it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><code class="fe ov ow ox nv b">format.class</code>:数据格式。您可以从<code class="fe ov ow ox nv b">JSON</code>、<code class="fe ov ow ox nv b">Avro</code>和<code class="fe ov ow ox nv b">Parquet</code>中选择</li></ul><h2 id="a69e" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">管道</h2><p id="b22c" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">瞧，你的管道现在完成了。通过几个 docker-compose 配置文件和连接器配置，您已经创建了一个支持近实时数据分析功能的流管道。相当强大的东西！</p><h2 id="9608" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">后续步骤</h2><p id="40f9" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">现在用户数据在 Kafka 和我们的水槽中。电子邮件服务可以实时获取客户数据，轮询推荐 API，并在 2 秒钟内向客户发送欢迎电子邮件。同样，利用数据湖中的数据，您可以为您的 CEO 创建一个实时仪表板。</p><p id="fb52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是您不必就此止步，因为 Kafka 及其组件是可横向扩展的。您可以使用 Kafka 为大部分(如果不是全部)管道供电。一些公司运行 Kafka 集群，拥有数千个生产者和订户。当然，在这样的规模下，除了简单的概念验证设置之外，还有更多工作要做。但是有一些托管的 Kafka 服务，你可以开箱即用，比如 AWS 上的 MSK，或者 Confluent(多平台)。您还可以添加更多组件来处理实时数据，如 Spark Streaming、KSQL、Beam 或 Flink。</p><h2 id="72c1" class="nf lv it bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">打扫</h2><p id="95de" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">如果您没有运行任何其他 docker 容器，您可以使用以下命令关闭这个项目的 docker 容器:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="1069" class="nf lv it nv b gy nz oa l ob oc">docker stop $(docker ps -aq)</span></pre><p id="c859" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者，您可以通过运行以下命令来清理本地下载的 docker 映像:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="cc45" class="nf lv it nv b gy nz oa l ob oc">docker system prune</span></pre><h1 id="a03a" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">结论</h1><p id="3bd3" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在这个项目中，我们使用 Docker、Kafka 和 Kafka Connect 构建了一个流数据管道，这让我们的营销团队和首席执行官非常高兴。有了我们构建的东西，其他团队可以很容易地从那里得到它，交付我们的涉众所要求的东西。</p><p id="ffa3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你以前从未使用过 Kafka，我鼓励你自己尝试这个项目。如果你有兴趣阅读类似的东西，neptune.ai 的人写了一篇关于如何用 Kedro 构建数据科学管道的优秀文章。</p><p id="8953" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">快乐学习:)</p></div></div>    
</body>
</html>