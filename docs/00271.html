<html>
<head>
<title>Understanding Singular Value Decomposition and its Application in Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解奇异值分解及其在数据科学中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d?source=collection_archive---------0-----------------------#2020-01-09">https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d?source=collection_archive---------0-----------------------#2020-01-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="470f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解奇异值分解背后的直觉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fdb68693ae3e57de69b97170f348842d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnvjYsiEhj-NzFf6ZeCETg.jpeg"/></div></div></figure><p id="a867" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在线性代数中，矩阵的奇异值分解(SVD)是将该矩阵分解成三个矩阵。它有一些有趣的代数性质，传达了关于线性变换的重要几何和理论见解。它在数据科学中也有一些重要的应用。在本文中，我将尝试解释支持向量机背后的数学直觉及其几何意义。我将使用Python库来进行计算，而不是手动计算，稍后我将为您提供一些在数据科学应用程序中使用SVD的示例。本文中，粗体小写字母(如<strong class="kw iu"> a </strong>)指代向量。粗体大写字母(如<strong class="kw iu"> A </strong>)指矩阵，斜体小写字母(如<em class="lq"> a </em>)指标量。</p><p id="82c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要理解SVD，我们需要首先理解矩阵的<em class="lq">特征值分解</em>。我们可以把矩阵<strong class="kw iu"> A </strong>想象成一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Transformation_matrix" rel="noopener ugc nofollow" target="_blank">变换</a>通过乘法作用于向量<strong class="kw iu"> x </strong>产生一个新的向量<strong class="kw iu"> Ax </strong>。我们用[ <strong class="kw iu"> A </strong> ]ij或<em class="lq"> aij </em>来表示矩阵<strong class="kw iu"> A </strong>在第<em class="lq"> i </em>行和第<em class="lq"> j </em>列的元素。若<strong class="kw iu"> A </strong>为<em class="lq"> m×p </em>基体，<strong class="kw iu"> B </strong>为<em class="lq"> p×n </em>基体，则基体积<strong class="kw iu"> C </strong> = <strong class="kw iu"> AB </strong>(为<em class="lq"> m×n基体</em>)定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a438a7a6601cce58017c5c5c37bb7a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*dIGf3mUcGLtxHezSN-tJYA@2x.png"/></div></figure><p id="f6c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，二维空间中的旋转矩阵可以定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/295a03fc05ffd8dd9e6af2e8c0d9dcb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*F-PAQjORuPLU6iRSDsNbvQ@2x.png"/></div></figure><p id="4276" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该矩阵将矢量绕原点旋转角度θ(正θ为逆时针旋转)。另一个例子是二维空间中的拉伸矩阵<strong class="kw iu"> B </strong>，定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/add59b8442d98a044cc1d89f12576cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*CdAvPkNyzRTzf4elJq1Jqw@2x.png"/></div></figure><p id="d12f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该矩阵以恒定的因子<em class="lq"> k </em>沿<em class="lq"> x </em>轴拉伸矢量，但在<em class="lq"> y </em>方向上不影响矢量。同样，我们可以在<em class="lq"> y </em>方向上有一个拉伸矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/0890fdc8aa4ef292931320bf0a0067e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*BHLLMpvoYHjFt2a0a3E7Tg@2x.png"/></div></figure><p id="b458" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">举个例子，如果我们有一个向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c726972d35e1dd893eff63cd46509066.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*KJurF41ZySx1kq_i4iegyQ@2x.png"/></div></figure><p id="a99e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">则<strong class="kw iu"> y </strong> = <strong class="kw iu"> Ax </strong>是<strong class="kw iu"> x </strong>旋转θ后的矢量，而<strong class="kw iu"> Bx </strong>是<strong class="kw iu"> x </strong>在<em class="lq"> x </em>方向上以恒定因子<em class="lq"> k拉伸的矢量。</em></p><p id="4557" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">清单1展示了如何将这些矩阵应用于vector <strong class="kw iu"> x </strong>并在Python中可视化。我们可以用NumPy数组作为向量和矩阵。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="85f8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，旋转矩阵是针对<em class="lq"> θ </em> =30⁰和拉伸矩阵<em class="lq"> k </em> =3计算的。<strong class="kw iu"> y </strong>是<strong class="kw iu"> x </strong>的变换向量。为了绘制矢量，使用了<code class="fe ly lz ma mb b">matplotlib</code>中的<code class="fe ly lz ma mb b">quiver()</code>函数。图1显示了代码的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/d08e20a83980cbcb9e2146eae3271d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8AH2rIBK5CQh5Nffk5fUDQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图1</p></figure><p id="b7a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">矩阵由NumPy中的二维数组表示。我们可以使用<code class="fe ly lz ma mb b">np.matmul(a,b)</code>函数将矩阵<code class="fe ly lz ma mb b">a</code>乘以<code class="fe ly lz ma mb b">b</code>，然而，使用<code class="fe ly lz ma mb b">@</code>操作符更容易做到这一点。向量可以用一个一维数组或一个二维数组来表示，其形状为行向量<code class="fe ly lz ma mb b">(1,n) </code>或列向量<code class="fe ly lz ma mb b">(n,1)</code>。</p><p id="4ae4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们要尝试一个不同的变换矩阵。假如</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9c0b565cd6872e45a4290c6d1d1ab6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*aTjz24-8TBisxhu3bqsfTg@2x.png"/></div></figure><p id="dcb8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，我们不仅仅将它应用于一个向量。最初，我们有一个圆，包含所有距离原点一个单位的向量。这些向量的一般形式为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/435a5c3366e560ac3229171ab861f950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*XYHA4ctulJbZip9c9FA1gw@2x.png"/></div></figure><p id="73df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们计算<strong class="kw iu"> t </strong> = <strong class="kw iu"> Ax </strong>。所以<strong class="kw iu"> t </strong>是<strong class="kw iu"> x </strong>中所有经过<strong class="kw iu"> A </strong>变换的向量的<strong class="kw iu">T25】集合。清单2展示了如何在Python中实现这一点。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="d4ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图2显示了<strong class="kw iu"> x </strong>和<strong class="kw iu"> t </strong>的曲线图，以及在<strong class="kw iu"> x </strong>中对两个样本向量<strong class="kw iu"> x1 </strong>和<strong class="kw iu"> x2 </strong>的变换效果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/9e6e8c4be341c51bd60369c34bbb60ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5en2om8fBULLc4eie3s9Zg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图2</p></figure><p id="9d75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">左边的初始向量(<strong class="kw iu"> x </strong>)如前所述形成一个圆，但是变换矩阵不知何故改变了这个圆，把它变成了一个椭圆。</p><p id="f8b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">圆内的样本向量<strong class="kw iu"> x1 </strong>和<strong class="kw iu"> x2 </strong>分别转换为<strong class="kw iu"> t1 </strong>和<strong class="kw iu"> t2 </strong>。所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8455116b0e7549f8b3f9e3d4db2f3cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*FhaUZJl0HczQ2N1oGem8wg@2x.png"/></div></figure><p id="1dd2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征值和特征向量</strong></p><p id="a6ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">矢量是既有大小又有方向的量。矩阵<strong class="kw iu"> A </strong>对<strong class="kw iu"> x </strong>中的向量的一般效果是旋转和拉伸的组合。例如，它改变向量<strong class="kw iu"> x1 </strong>的方向和大小，以给出变换后的向量<strong class="kw iu"> t1 </strong>。然而，对于矢量<strong class="kw iu"> x2 </strong>来说，变换后只有幅度发生变化。其实<strong class="kw iu"> x2 </strong>和<strong class="kw iu"> t2 </strong>方向一致。矩阵<strong class="kw iu"> A </strong>只在同一个方向拉伸<strong class="kw iu"> x2 </strong>并给出幅度更大的向量<strong class="kw iu"> t2 </strong>。改变向量的大小而不改变其方向的唯一方法是将它乘以一个标量。因此，如果我们有一个矢量<strong class="kw iu"> u </strong>，并且<strong class="kw iu"> </strong> <em class="lq"> λ </em>是一个标量，那么<em class="lq"> λ </em> <strong class="kw iu"> u </strong>具有相同的方向和不同的大小。所以对于图2中的<strong class="kw iu"> x2 </strong>这样的向量，乘以<strong class="kw iu"> A </strong>的效果就像是乘以<em class="lq"> λ这样的标量。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/33ab41fa8a4f8444a6943c28ba06f701.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*mnTVymIlSENzxq9Xl6to_A@2x.png"/></div></figure><p id="2339" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于<strong class="kw iu"> x </strong>中的所有矢量来说，情况并非如此。实际上，对于每个矩阵<strong class="kw iu"> A，</strong>只有部分向量具有这种性质。这些特殊向量称为<strong class="kw iu"> A </strong>的特征向量，它们对应的标量λ称为该特征向量的<strong class="kw iu"> A </strong>的特征值。因此一个<em class="lq"> n×n </em>矩阵<strong class="kw iu"> A </strong>的特征向量被定义为一个非零向量<strong class="kw iu"> u </strong>使得:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/aa24103ed030f313562768c8957816be.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*g3gcKRz25plU6ZA97r2ugQ@2x.png"/></div></figure><p id="0f7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="lq"> λ </em>为标量，称为<strong class="kw iu"> A </strong>的特征值，<strong class="kw iu"> u </strong>为<em class="lq"> λ </em>对应的特征向量。此外，如果你有任何其他形式的向量<em class="lq">a</em>t【60】u其中<em class="lq"> a </em>是一个标量，那么通过把它放入前面的等式我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/c04d754c5f8c73eb38161781943dd52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*-i9ypP07EdjLHfue6BGd4w@2x.png"/></div></figure><p id="cb30" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着与特征向量<strong class="kw iu"> u </strong>方向相同的任何向量(或者如果<em class="lq"> a </em>为负，则方向相反)也是具有相同对应特征值的特征向量。</p><p id="015f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，的特征值</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d4e2e89ce73d1e2bcbbbceab971ea74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*9NOAP_oktSjEfmE-qfc43w@2x.png"/></div></figure><p id="494f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是<em class="lq"> λ1=-1 </em>和<em class="lq"> λ2=-2 </em>，它们对应的特征向量是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e2f5b3b62e31ad84347b8183de441820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*PWEFUCmOn2dcYnpwYcCIew@2x.png"/></div></figure><p id="2d85" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/9a6744e54cf5480524f33a8da549e98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*LqkMIUtV3M8uA7A8pJPRNg@2x.png"/></div></figure><p id="94a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着当我们将矩阵<strong class="kw iu"> B </strong>应用于所有可能的向量时，它不会改变这两个向量(或任何具有相同或相反方向的向量)的方向，而只会拉伸它们。所以对于特征向量，矩阵乘法变成了简单的标量乘法。这里我不打算解释特征值和特征向量是如何用数学方法计算出来的。相反，我将向您展示如何在Python中获得它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="a308" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以使用NumPy中的<code class="fe ly lz ma mb b">LA.eig()</code>函数来计算特征值和特征向量。它返回一个元组。这个元组的第一个元素是存储特征值的数组，第二个元素是存储相应的特征向量的2-d数组。事实上，在清单3中，<code class="fe ly lz ma mb b">u[:,i]</code>列是特征值<code class="fe ly lz ma mb b">lam[i]</code>对应的特征向量。现在，如果我们检查清单3的输出，我们会得到:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="7559" class="mu mv it mb b gy mw mx l my mz">lam= [-1. -2.]</span><span id="ad0f" class="mu mv it mb b gy na mx l my mz">u= [[ 1.     -0.7071]<br/>    [ 0.      0.7071]]</span></pre><p id="1299" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能已经注意到<em class="lq"> λ=-1 </em>的特征向量与<strong class="kw iu"> u1 </strong>的特征向量相同，但另一个不同。这是因为<code class="fe ly lz ma mb b">LA.eig()</code>返回归一化的特征向量。归一化向量是长度为1的单位向量。但在解释如何计算长度之前，我们需要熟悉矩阵的转置和点积。</p><p id="8f95" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">转置</strong></p><p id="00c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">列向量<strong class="kw iu"> u ( </strong>用<strong class="kw iu"> u </strong>上标t表示)的转置是<strong class="kw iu"> u </strong>的行向量(在本文中有时我表示为<strong class="kw iu"> u </strong> ^T).一个<em class="lq"> m×n </em>矩阵<strong class="kw iu"> A </strong>的转置是一个<em class="lq"> n×m </em>矩阵，它的列由<strong class="kw iu"> A </strong>的相应行组成。例如，如果我们有</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2baadaa8daa138d4567a8257fcc47f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*dszdmPvl-zS60Nicx3vi7w@2x.png"/></div></figure><p id="27f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么<strong class="kw iu"> C </strong>的转置为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f9c1ecfc2a1bd948739fb600096e0fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*J_-Gli1ja0IvzCOl7A7cag@2x.png"/></div></figure><p id="b9c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以行向量的转置变成了具有相同元素的列向量，反之亦然。实际上，转置矩阵第<em class="lq"> i </em>行第<em class="lq"> j </em>列的元素等于原矩阵第<strong class="kw iu">j行第<em class="lq">I</em>列的元素。<strong class="kw iu"> </strong>如此</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f25db8e920aa5f8c4fa23958960b6725.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*jckFvit5snbv4nspWMwW_g@2x.png"/></div></figure><p id="1062" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在NumPy中，你可以使用<code class="fe ly lz ma mb b">transpose()</code>方法来计算转置。例如，为了计算矩阵<code class="fe ly lz ma mb b">C</code>的转置，我们写<code class="fe ly lz ma mb b">C.transpose().</code>我们也可以使用转置属性<code class="fe ly lz ma mb b">T</code>，并写<code class="fe ly lz ma mb b">C.T </code>来得到它的转置。转置有一些重要的性质。一、<strong class="kw iu"> A </strong>的转置的转置是<strong class="kw iu"> A </strong>。所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/214f153a30180780f2c4dd45f2fb806f.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*y-sl0j4K6vTKV5otIhsKog@2x.png"/></div></figure><p id="d0b2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，产品的转座是逆序转座的产品。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a50cce0ed606c825f57797a22ad9ee65.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*zNGcxWlRNyFF4ZGuI2-KJg@2x.png"/></div></figure><p id="993a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了证明这一点，请记住矩阵乘法的定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1c1093146d43d3c06adcfc191e3b5dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*_M1AtFYa50KhvHcgKMolFw@2x.png"/></div></figure><p id="3014" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">而根据矩阵转置的定义，左边是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/bd61adb31a3d8bd5032fb310ff36dec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*xm-xS3xoaZ9Y24_kpq6FBQ@2x.png"/></div></figure><p id="d024" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">右边是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/73b5acfb56f724b412498aeb896827b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lLAGaaZLIK6JQwgisI-Hw@2x.png"/></div></div></figure><p id="ce8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以等式两边是相等的。</p><p id="2fb1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">点积</strong></p><p id="271d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们有两个向量<strong class="kw iu"> u </strong>和<strong class="kw iu"> v </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/fde871ea96eed57e6a096f218405e2b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*Ra3Jm-_Eejv6NK2DIwAPOw@2x.png"/></div></figure><p id="60df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些向量的点积(或内积)定义为<strong class="kw iu"> u </strong>乘以<strong class="kw iu"> v </strong>的转置:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/f5e0b9431f8b3137d8674b61452a675a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkitgHwALrkRDdhIRmqbmg@2x.png"/></div></div></figure><p id="62c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于这个定义，点积是可交换的，所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0b7622dcc7ebadfddbd2be8e20efbbbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*Av4kctM7qtKpMqpdFiqMOg@2x.png"/></div></figure><p id="4012" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">分块矩阵</strong></p><p id="2999" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当计算矩阵的转置时，将其显示为分块矩阵通常是有用的。例如，矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2baadaa8daa138d4567a8257fcc47f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*dszdmPvl-zS60Nicx3vi7w@2x.png"/></div></figure><p id="7bbe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/25d7a0e718393628b1294f5c464c8da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*clIscWsItopRGiPoYgRtuQ@2x.png"/></div></figure><p id="9b59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/a741d5e11057902882aa08a7588071fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-KQpCTr6Gm3Iy9lZPkZ5Q@2x.png"/></div></div></figure><p id="07b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们可以把<strong class="kw iu"> C </strong>的每一列看成一个列向量，<strong class="kw iu"> C </strong>可以看成一个只有一行的矩阵。现在要写<strong class="kw iu"> C </strong>的转置，我们可以简单地把这一行变成一列，类似于我们对一个行向量所做的。唯一的区别是<strong class="kw iu"> C </strong>中的每个元素现在都是一个向量，也应该被转置。</p><p id="fd35" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们知道了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/0a1f90753c1aa45f2c918d2c5f79268f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEDwR3Qan_pjAaoeiK_7yA@2x.png"/></div></div></figure><p id="a329" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1f684eb7d84854792ab4cc08c3c01512.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*ka8s6IX-8nlo882SflJ7ww@2x.png"/></div></figure><p id="9b09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在<strong class="kw iu"> C </strong> ^T的每一行都是原矩阵<strong class="kw iu"> C </strong>对应列的转置。</p><p id="30d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在假设矩阵<strong class="kw iu"> A </strong>是<strong class="kw iu"> </strong>分块的列矩阵，矩阵<strong class="kw iu"> B </strong>是分块的行矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/4f170f4b4a6c7aa9cfbd20f535113633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScEeC1Ggi79_14nqUvT8XQ@2x.png"/></div></div></figure><p id="bfbf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中每列向量<strong class="kw iu"> ai </strong>定义为<strong class="kw iu"> A </strong>的第<em class="lq"> i </em>列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ce23e86f5950fb37bc3f15c989e6ed3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*Ek1lU1EMhhgE5tOliek4DA@2x.png"/></div></figure><p id="8a16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里对于每个元素，第一个下标指的是行号，第二个下标指的是列号。所以<strong class="kw iu"> A </strong>是一个<em class="lq"> m×p </em>矩阵。另外，<strong class="kw iu"> B </strong>是一个<em class="lq"> p×n </em>矩阵，其中<strong class="kw iu"> bi </strong> ^T中的每一行向量是<strong class="kw iu"> B </strong>的第<em class="lq"> i </em>行:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/83b65e5fc52cffbd403ab3069303ede1.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*kJ8QhgWtPpDTKiEij0fhFw@2x.png"/></div></figure><p id="003b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，第一个下标表示行号，第二个下标表示列号。请注意，通过对流，向量被写成列向量。所以要写一个行向量，我们把它写成列向量的转置。所以<strong class="kw iu"> bi </strong>是一个列向量，它的转置是一个行向量，它捕获了<strong class="kw iu"> B </strong>的第<em class="lq"> i </em>行。现在我们可以计算<strong class="kw iu"> AB </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/64451fb6e574b2fab218ca5deae73cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYdxg2PJHiQZmDMf6yx1vg@2x.png"/></div></div></figure><p id="e05f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> A </strong>的第<em class="lq"> i- </em>第<em class="lq">列</em>和<strong class="kw iu"> B </strong>的第<em class="lq"> i- </em>行的乘积给出了一个<em class="lq"> m×n </em>矩阵，所有这些矩阵相加得到<strong class="kw iu"> AB </strong>也是<strong class="kw iu"> </strong>一个<em class="lq"> m×n </em>矩阵。事实上，我们可以简单地假设我们正在用一个行向量<strong class="kw iu"> A </strong>乘以一个列向量<strong class="kw iu"> B </strong>。作为特例，假设<strong class="kw iu"> x </strong>是列向量。现在我们可以类似地计算<strong class="kw iu"> Ax </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c8fa93025e4278befa719c93c2de1530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*-eAvLfyi9OhnRCEYW_0GNA@2x.png"/></div></figure><p id="e3db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> Ax </strong>只是<strong class="kw iu"> A </strong>的列的线性组合。</p><p id="bdf1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要计算NumPy中两个向量<code class="fe ly lz ma mb b">a</code>和<code class="fe ly lz ma mb b">b</code>的点积，如果两者都是一维数组，我们可以写<code class="fe ly lz ma mb b">np.dot(a,b)</code>，或者简单使用点积的定义，写<code class="fe ly lz ma mb b">a.T @ b</code>。</p><p id="32cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经熟悉了转置和点积，我们可以将向量<strong class="kw iu"> u </strong>的长度(也称为2范数)定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c55b97ac3b9ab24bb9d43e60420613f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZeDO5k5UXBMFxLlZTqWBA@2x.png"/></div></div></figure><p id="15d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了归一化一个向量<strong class="kw iu"> u </strong>，我们简单地将它除以它的长度，得到归一化的向量<strong class="kw iu"> n </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/06bf810a7758dd96d4061744e0724afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*Ehav92Eyj7wni6tTBVjOpg@2x.png"/></div></figure><p id="7b77" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">归一化向量<strong class="kw iu"> n </strong>仍与<strong class="kw iu"> u，</strong>方向相同，但其长度为1。现在我们可以归一化之前看到的<em class="lq"> λ= </em> -2的特征向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/14505c20ef41f8cc40b08fdd467a05f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3l0QgbiEMEBO0FcVDbtsA@2x.png"/></div></div></figure><p id="facd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这与清单3的输出相同。如前所示，如果将一个特征向量乘以(或除以)一个常数，新向量仍然是同一特征值的特征向量，因此通过归一化对应于某个特征值的特征向量，您仍然有该特征值的特征向量。</p><p id="ec8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是为什么特征向量对我们很重要呢？如前所述，特征向量将矩阵乘法简化为标量乘法。此外，它们还有一些更有趣的性质。让我回到清单2中使用的矩阵<strong class="kw iu"> A </strong>并计算它的特征向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9c0b565cd6872e45a4290c6d1d1ab6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*aTjz24-8TBisxhu3bqsfTg@2x.png"/></div></figure><p id="440c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如您所记得的，这个矩阵将一组形成圆的向量转换成一组形成椭圆的新向量(图2)。我们将使用<code class="fe ly lz ma mb b">LA.eig()</code>来计算清单4中的特征向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="5ce4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出是:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="b498" class="mu mv it mb b gy mw mx l my mz">lam= [3. 2.]<br/>u= [[ 1.     -0.8944]<br/>    [ 0.      0.4472]]</span></pre><p id="4cfa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们有两个特征向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ed85941cc4fcf83d2c81f3815cfb5142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*nGg74uaTbEmgKhgGLM9oxg@2x.png"/></div></figure><p id="7c19" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相应的特征值为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/26e141842629b07e1f743af5327a1edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*MM3o1bOijfGLGko2S0xvzg@2x.png"/></div></figure><p id="ded2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们在变换后的向量上绘制特征向量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/6d0ee2102ba04214a99ac80ba629544c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhhHBmVmEvcJAqZ_GLu47g.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图3</p></figure><p id="a39c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图3中的这些特征向量没有什么特别的。现在让我试试另一个矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fd65ec50228f71d37789b43719a6cb28.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*7LjqrB-BPc2hQhEhqU4_cA@2x.png"/></div></figure><p id="f404" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我们有两个特征向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f11e243d545466aef1bb2b1dd6596aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7bctIshMi2Yu6eYmzTWNw@2x.png"/></div></figure><p id="56b0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相应的特征值为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/716e3ec03b5d3376e0bdef27af5c205f.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*-Cn7vJmX1QmXfBXCFBKB9g@2x.png"/></div></figure><p id="c635" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们可以通过替换清单5中的这个新矩阵，在转换后的向量上绘制特征向量。结果如图4所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/336098b206d5e6f821ae485089248f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPPom_D9fYgRvg9bL7Lk7w.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图4</p></figure><p id="5908" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这次特征向量有一个有趣的性质。我们看到特征向量沿着椭圆的长轴和短轴(主轴)。椭圆可以被认为是一个沿其主轴拉伸或收缩的圆，如图5所示，矩阵<strong class="kw iu"> B </strong>通过沿<strong class="kw iu"> u1 </strong>和<strong class="kw iu">U2</strong><strong class="kw iu">B</strong>的特征向量拉伸它来变换初始圆。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/0762139ce7436f32554e7235f7b135d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-E17ZXotM0mjAwj99JbJg.jpeg"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图5</p></figure><p id="d572" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是为什么<strong class="kw iu"> A </strong>的特征向量没有这个性质呢？那是因为<strong class="kw iu"> B </strong>是一个对称矩阵。<a class="ae lr" href="https://en.wikipedia.org/wiki/Symmetric_matrix" rel="noopener ugc nofollow" target="_blank">对称矩阵</a>是与其转置矩阵相等的矩阵。因此主对角线上的元素是任意的，但是对于其他元素，行<em class="lq"> i </em>和列<em class="lq"> j </em>上的每个元素等于行<em class="lq"> j </em>和列<em class="lq"> i </em>上的元素(<em class="lq"> aij </em> = <em class="lq"> aji </em>)。这是一个对称矩阵的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/815342aa674db2906711e94b1a5b8933.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*JdueTQX0ym7-mFfF_5brsw@2x.png"/></div></figure><p id="1713" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对称矩阵总是一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Square_matrix" rel="noopener ugc nofollow" target="_blank">方阵</a> ( <em class="lq"> n×n </em>)。你现在可以很容易地看到<strong class="kw iu">和</strong>是不对称的。对称矩阵通过沿向量的特征向量拉伸或收缩来变换向量。此外，我们知道所有的矩阵都通过将特征向量的长度(或幅度)乘以相应的特征值来转换特征向量。我们知道圆中的初始向量长度为1，并且<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>都被归一化，所以它们是初始向量<strong class="kw iu"> x </strong>的一部分。现在它们的变换向量是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/9a6744e54cf5480524f33a8da549e98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*LqkMIUtV3M8uA7A8pJPRNg@2x.png"/></div></figure><p id="65a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，沿着每个特征向量的拉伸或收缩量与相应的特征值成比例，如图6所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/c8a3e315f7b008c57d6aa3636636c8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9etp5c6n8fQeH6TgWZ23yg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图6</p></figure><p id="8f4d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以当你在一个特征向量的方向上有更多的拉伸，对应于那个特征向量的特征值会更大。实际上，如果一个特征值的绝对值大于1，圆<strong class="kw iu"> x </strong>就沿着它拉伸，如果绝对值小于1，就沿着它收缩。让我试试这个矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6cf497341140ebf00b7e5d5c4e5f4c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*FsC3QxMX5TPczd46iJUekA@2x.png"/></div></figure><p id="c49c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征向量和相应的特征值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/1f6b0adbd4c21010e90d3151d6485298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yS9a3yp-LZcJSpURvKrYew@2x.png"/></div></div></figure><p id="3ba1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们画出变换后的向量，我们会得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/eae693b9e57f96025dcd19fec42c5abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-Cs9X7-NVoLMU7FN5jeHw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图7</p></figure><p id="9752" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如你现在看到的，我们沿着<strong class="kw iu"> u1 </strong>拉伸，沿着<strong class="kw iu"> u2 </strong>收缩。关于这些特征向量的另一件重要的事情是，它们可以形成向量空间的<em class="lq">基</em>。</p><p id="d821" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">基础</strong></p><p id="d89a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一组向量{ <strong class="kw iu"> v1 </strong>，<strong class="kw iu"> v2 </strong>，<strong class="kw iu"> v3 </strong> …，<strong class="kw iu"> vn} </strong>形成了一个<a class="ae lr" href="http://mathworld.wolfram.com/VectorSpace.html" rel="noopener ugc nofollow" target="_blank">向量空间</a> <strong class="kw iu"> V </strong>的基，如果它们是<a class="ae lr" href="https://en.wikipedia.org/wiki/Linear_independence" rel="noopener ugc nofollow" target="_blank">线性无关</a>和<a class="ae lr" href="http://mathworld.wolfram.com/VectorSpaceSpan.html" rel="noopener ugc nofollow" target="_blank">跨度</a> <strong class="kw iu"> V </strong>。向量空间是一组可以用标量相加或相乘的向量。这是一个封闭的集合，所以当向量与一个标量相加或相乘时，结果仍然属于这个集合。向量加法和标量乘法的运算必须满足某些要求，这里不讨论这些要求。欧几里得空间是向量空间的一个例子。</p><p id="6ce2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当一组向量线性无关时，这意味着该组向量中没有一个向量可以写成其他向量的线性组合。所以不可能写</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/f01e8a8b89fb5ed281d4078ebb9023ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VE3ChAW7PYvlaKc_ImZQ5A@2x.png"/></div></div></figure><p id="94ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当某些<em class="lq"> a1 </em>、<em class="lq"> a2 </em>、..、<em class="lq">和</em>都不为零。换句话说，这个集合中的<strong class="kw iu"> vi </strong>矢量都不能用其他矢量来表示。如果空间中每隔一个向量可以写成一个生成集的线性组合，则一个向量集生成一个空间。所以<strong class="kw iu"> V </strong>中的每个矢量<strong class="kw iu"> s </strong>都可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a679909ce6356831886e4e0a1b174043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*IaBeRQL6uUtylKIi6xHJMA@2x.png"/></div></figure><p id="bd17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个向量空间<strong class="kw iu"> V </strong>可以有许多不同的向量基，但是每个基总是有相同数量的基向量。向量空间<strong class="kw iu"> V </strong>的基向量个数称为<strong class="kw iu"> V </strong>的<em class="lq">维</em>。在欧几里得空间<strong class="kw iu"> R </strong>中，向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/2724b9643dd2c1840920d43f87c7d25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*JViCg-HnhTS-1jnnWbD9wg@2x.png"/></div></figure><p id="971a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是基的最简单的例子，因为它们是线性独立的，并且<strong class="kw iu"> R </strong>中的每个向量都可以表示为它们的线性组合。它们被称为<strong class="kw iu"> R </strong>的<em class="lq">标准依据</em>。因此<strong class="kw iu"> R </strong>的尺寸为2。它可以有其他的基，但是它们都有两个线性无关的向量，并且跨越它。例如，向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/44085820e5690ecbc2206ab6467d180e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*L2Xxcb0lQMud0Z08n3abpQ@2x.png"/></div></figure><p id="d5b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也可以构成<strong class="kw iu"> R </strong>的基础。寻找向量空间的基的一个重要原因是要有一个坐标系统。如果向量组<em class="lq"> B </em> ={ <strong class="kw iu"> v1 </strong>，<strong class="kw iu"> v2 </strong>，<strong class="kw iu"> v3 </strong> …，<strong class="kw iu"> vn} </strong>形成了向量空间的基，那么该空间中的每个向量<strong class="kw iu"> x </strong>可以使用这些基向量来唯一地指定:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3e79bc5bc9b684c938904e7824ef1b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*GqB2P-1c5AtSzE4dnchKYw@2x.png"/></div></figure><p id="c3be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在<strong class="kw iu"> x </strong>相对于此基准<em class="lq"> B </em>的坐标为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/fc124b022937db99182eeb6dd243077a.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*NuqeN0GrfekjIlgYHoRn7w@2x.png"/></div></figure><p id="279b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">事实上，当我们在<strong class="kw iu"> R </strong>中写向量的时候，我们已经在表达它相对于标准基的坐标了。<strong class="kw iu"> </strong>那是因为任何矢量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f28a47266a3cebb6446224985ac14124.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*Tx3YX2xa_Mz02itTXxUHJg@2x.png"/></div></figure><p id="0c91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cbe93fe890164dfbdf4ffe3b9b9e38fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*hW2mUgsX4jhtd8WtnMF74g@2x.png"/></div></figure><p id="4f88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在出现了一个问题。如果我们知道一个向量相对于标准基的坐标，我们如何找到它相对于一个新基的坐标？</p><p id="14e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3e79bc5bc9b684c938904e7824ef1b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*GqB2P-1c5AtSzE4dnchKYw@2x.png"/></div></figure><p id="78fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/ef874f47620c7f12b4b6f6c7600465d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRHdO1gDGx0j9fLWm0HAXA@2x.png"/></div></div></figure><p id="ea8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/fdfc222a4f7b30b9c20f31e308814b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Z2rD2jC5iJFhmLRstTGGjw@2x.png"/></div></figure><p id="02b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">叫做坐标变换矩阵。这个矩阵的列是基<em class="lq"> B </em>中的向量。方程式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/90f40796dcf4a2ad15ab51392e668636.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*YVtECPcuB7Si7ZbpPAiTdw@2x.png"/></div></figure><p id="343e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果知道<strong class="kw iu"> x </strong>在<strong class="kw iu"> R </strong> ^n中的坐标，则给出其在基准<em class="lq"> B </em>中的坐标。如果我们需要相反的结果，我们可以将这个方程的两边乘以坐标变化矩阵的逆矩阵，得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f63ea671d62167b07a56e9646d4a572f.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*MN-I6zNsJXDd3-8JsxpZLQ@2x.png"/></div></figure><p id="382d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们知道<strong class="kw iu"> x </strong>在<strong class="kw iu"> R </strong> ^n中的坐标(简单地说就是<strong class="kw iu"> x </strong>本身)，我们可以将它乘以坐标变化矩阵的逆矩阵，从而得到它相对于基底<em class="lq"> B </em>的坐标。例如，假设我们的基集<em class="lq"> B </em>由向量构成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/00e15abd9f9366b31b3e331ad585d95b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*qfAWkVtIoyo83XRq5sHq9Q@2x.png"/></div></figure><p id="5b67" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有一个向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/296f30b848581d0e416ac4ccf8763dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*Jvn77JRQHM1vogDsJEMB4g@2x.png"/></div></figure><p id="e90b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了计算<em class="lq"> B </em>中<strong class="kw iu"> x </strong>的坐标，首先我们形成<em class="lq"> </em>坐标变化矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2ec63c1ed076ed503eecf7e58021730c.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*bgyPMmoFQFIiUinSmxfZ_A@2x.png"/></div></figure><p id="b464" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在<strong class="kw iu"> x </strong>相对于<em class="lq"> B </em>的坐标为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/518c0f21bb8d5a10d78ab7693701d996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfupo2IYA5ykY531q-k9XQ@2x.png"/></div></div></figure><p id="e39b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">清单6展示了如何在NumPy中进行计算。要计算矩阵的逆矩阵，可以使用函数<code class="fe ly lz ma mb b">np.linalg.inv()</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="0bf4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出显示了x在B中的坐标:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="bfd6" class="mu mv it mb b gy mw mx l my mz">x_B= [[4.    ]<br/>      [2.83]]</span></pre><p id="de7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图8显示了改变基础的效果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/0160ec60aaa1e02c628a2a673e81b1f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjPeaQqQqlCfmVIH76JlyQ.jpeg"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图8</p></figure><p id="1d63" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了找到基准<em class="lq"> B </em>中<strong class="kw iu"> x </strong>的<strong class="kw iu">u1</strong>-坐标，我们可以从<strong class="kw iu"> x </strong>画一条线，平行于<strong class="kw iu"> u2 </strong>，看它与<strong class="kw iu"> u1 </strong>轴相交的位置。<strong class="kw iu">U2</strong>-坐标的查找方法如图8所示。在一个<em class="lq"> n </em>维空间中，为了找到<strong class="kw iu"> ui </strong>的坐标，我们需要画一个从<strong class="kw iu"> x </strong>穿过，平行于除<strong class="kw iu"> ui </strong>之外的所有其他特征向量的超平面，并查看它与<strong class="kw iu"> ui </strong>轴的交点。如图8(左)所示，当特征向量正交时(像<strong class="kw iu"> R </strong>中的<strong class="kw iu"> i </strong>和<strong class="kw iu"> j </strong>，我们只需要画一条穿过点<strong class="kw iu"> x </strong>并垂直于我们想要求其坐标的轴的线。</p><p id="7ce5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">对称矩阵的性质</strong></p><p id="1282" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如图5-7所示，对称矩阵的特征向量<strong class="kw iu"> B </strong>和<strong class="kw iu"> C </strong>相互垂直，形成<a class="ae lr" href="https://en.wikipedia.org/wiki/Orthogonality" rel="noopener ugc nofollow" target="_blank">正交向量</a>。这不是巧合，是对称矩阵的一个性质。</p><p id="38c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对称矩阵的一个重要性质是一个<em class="lq"> n×n </em>对称矩阵有<em class="lq"> n个</em>线性无关且正交的特征向量，并且它有<em class="lq"> n个</em>实特征值对应于那些特征向量。重要的是要注意，这些特征值不一定彼此不同，其中一些可以相等。对称矩阵的另一个重要性质是它们可以正交对角化。</p><p id="bf78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征分解</strong></p><p id="8beb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对称矩阵是正交对角化的。这意味着如果我们有一个<em class="lq"> n×n </em>对称矩阵<strong class="kw iu"> A </strong>，<strong class="kw iu"> </strong>我们可以把它分解为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4fc70e274bc1592d77e34f97fcaa21c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*vwSEaqojcKyueynznYP5tQ@2x.png"/></div></figure><p id="2aec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> D </strong>是由<strong class="kw iu"> A </strong>的<em class="lq"> n </em>个特征值组成的<em class="lq"> n×n </em>对角矩阵。<strong class="kw iu"> P </strong>也是一个<em class="lq"> n×n </em>矩阵，<strong class="kw iu"> P </strong>的列是<strong class="kw iu"> A </strong>的<em class="lq"> n </em>线性无关特征向量<strong class="kw iu"> </strong>分别对应<strong class="kw iu">D</strong>中的那些特征值。换句话说，如果<strong class="kw iu"> u1 </strong>、<strong class="kw iu"> u2 </strong>、<strong class="kw iu"> u3 </strong> …、<strong class="kw iu"> un </strong>是<strong class="kw iu"> A </strong>的特征向量<strong class="kw iu">、<strong class="kw iu"> </strong> <em class="lq"> λ1、λ2、…、λn </em>分别是它们对应的特征值，那么<strong class="kw iu"> A </strong>可以写成</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/5774cbfb3073ce1192039bba4d89e697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAF1hPtnJeu2rT1WPUTkXA@2x.png"/></div></div></figure><p id="57a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这也可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/4a8592651b38506d65f1fd7043cbcd02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aSiE9ab0VzTqqj4hE-ouQ@2x.png"/></div></div></figure><p id="cecb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你应该注意到每个<strong class="kw iu"> ui </strong>被认为是一个列向量，它的转置是一个行向量。<strong class="kw iu"> </strong>所以<strong class="kw iu"> P </strong>的转置是根据<strong class="kw iu"> P </strong>的列的转置写成的。<strong class="kw iu">T51<strong class="kw iu">A</strong>的这种因式分解叫做<strong class="kw iu"> A </strong>的本征分解。</strong></p><p id="5bc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我举个例子来说明一下。假如</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/a9ec851b8624fd56380b35ae7a9afca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*oWeogvD9LNdKBS1168v0iw@2x.png"/></div></figure><p id="12e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它有两个特征向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f11e243d545466aef1bb2b1dd6596aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7bctIshMi2Yu6eYmzTWNw@2x.png"/></div></figure><p id="e283" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相应的特征值为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/716e3ec03b5d3376e0bdef27af5c205f.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*-Cn7vJmX1QmXfBXCFBKB9g@2x.png"/></div></figure><p id="423d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> D </strong>可以定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0b464aaa972572eadf2ff44018a8f508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*Ered4ywupVUNHjgXLdU2OA@2x.png"/></div></figure><p id="645a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在<strong class="kw iu"> P </strong>的列是<strong class="kw iu"> A </strong>的特征向量<strong class="kw iu">分别对应<strong class="kw iu">D</strong>中的那些特征值。因此</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/87f8d1a3e7840018b9d2bbf8c57d18db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JyVkjwHZ6buES_4c-nJZzQ@2x.png"/></div></div></figure><p id="2689" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> P </strong>的转置为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/8801565d68550bb36eb32dd080a60089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5wh1GOlRO8NIRIFMnqViA@2x.png"/></div></div></figure><p id="cc3e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu">一个</strong>可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/fb3434574079dc0606fc40dfd5792b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7bwtjlrjT9fRBNFZpcckA@2x.png"/></div></div></figure><p id="4f46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重要的是要注意，如果你在上面等式的右边做乘法，你不会精确地得到<strong class="kw iu"> A </strong>。这是因为我们在NumPy中有舍入误差来计算通常出现在特征值和特征向量中的无理数，我们也在这里舍入了特征值和特征向量的值，但是，理论上，两边应该是相等的。但这意味着什么呢？为了更好地理解特征分解，我们可以看看它的几何解释。</p><p id="30c1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征分解的几何解释</strong></p><p id="bc88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了更好地理解特征分解方程，我们需要首先简化它。如果我们假设每个特征向量<strong class="kw iu"> ui </strong>是一个<strong class="kw iu"> </strong> <em class="lq"> n × 1 </em>列向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/655cece8acd4744f1cc1493768b65658.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*cawMHpQmY9w2P_FcYWQ8hQ@2x.png"/></div></figure><p id="7b23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么<strong class="kw iu"> ui </strong>的转置就是一个<em class="lq"> 1 × n </em>行向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e7f5517343e6d29b4eebc1c2fc07f2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zBBorr3bPpbfUzgx-WuRSg@2x.png"/></div></figure><p id="c58a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和它们的繁殖</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/876ade1dce88187acaf387def6ebef33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7Ex2sX8xpwUJc0gJiYQ-g@2x.png"/></div></div></figure><p id="129f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">变成了一个<strong class="kw iu"> </strong> <em class="lq"> n×n </em>的矩阵。首先，我们计算<strong class="kw iu"> DP </strong> ^T以简化特征分解方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/072f5610699a0fee364f84b38284aae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPCJSXEqjap1UhZ6gIwNeA@2x.png"/></div></div></figure><p id="8ea4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，本征分解方程变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/7224964d0d23d8fa745390030cc671bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*otI0a76fmqhHxHbm7cT9ZQ@2x.png"/></div></figure><p id="19ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<em class="lq"> n×n </em>矩阵<strong class="kw iu"> A </strong>可以分解成形状相同的<em class="lq"> n </em>矩阵(<em class="lq"> n×n </em>)，这些矩阵中的每一个都有一个乘数等于对应的特征值<em class="lq"> λi </em>。每个矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/6424b268b16871e7fc9dc343be712f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*mXm8ZJrPCypsq831S2W20A@2x.png"/></div></figure><p id="319b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">被称为<em class="lq">投影矩阵</em>。想象我们有一个矢量<strong class="kw iu"> x </strong>和一个单位矢量<strong class="kw iu"> v </strong>。<strong class="kw iu"> v </strong>和<strong class="kw iu"> x </strong>的内积等于<strong class="kw iu"> v </strong>。<strong class="kw iu"> x=v^T x </strong>给出了<strong class="kw iu"> x </strong>到<strong class="kw iu"> v </strong>的标量投影(这是<strong class="kw iu"> x </strong>到<strong class="kw iu"> v </strong> ) <strong class="kw iu">、</strong>的矢量投影的长度)，如果我们再乘以<strong class="kw iu"> v </strong>，它给出了一个矢量，称为<strong class="kw iu"> x </strong>到<strong class="kw iu"> v </strong>的正交投影。这如图9所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/e3e565d772dc411ebd9224de2d5c91bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beEZWnMrNd4goDIHx3J3rQ.jpeg"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图9</p></figure><p id="ace8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以当<strong class="kw iu"> v </strong>是单位矢量时，乘</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/6e17c3730baba509be99df90b0c95b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*_vKWS5_S6qrRkthSamNyqA@2x.png"/></div></figure><p id="81dd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过<strong class="kw iu"> x </strong> , <strong class="kw iu"> </strong>将<strong class="kw iu"> x </strong>的<strong class="kw iu"> </strong>正交投影到<strong class="kw iu"> v </strong>上，这就是为什么它被称为投影矩阵。所以用<strong class="kw iu"> ui ui </strong> ^T乘以<strong class="kw iu"> x </strong>，我们得到<strong class="kw iu"> x </strong>到<strong class="kw iu"> ui </strong>的正交投影。</p><p id="fae0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我来计算前面提到的矩阵<strong class="kw iu"> A </strong>的投影矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/a9ec851b8624fd56380b35ae7a9afca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*oWeogvD9LNdKBS1168v0iw@2x.png"/></div></figure><p id="adcf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们已经计算了<strong class="kw iu"> A </strong>的特征值和特征向量。</p><p id="7b17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用清单7的输出，我们得到了特征分解方程中的第一项(这里我们称之为<strong class="kw iu"> A1 </strong>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/aee7d7304b81b3c089bddc682715474d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bSL_9oK5w8Wg593faC-eg@2x.png"/></div></div></figure><p id="2c3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，它也是一个对称矩阵。实际上，特征分解方程中的所有投影矩阵都是对称的。这是因为每个矩阵的行<em class="lq"> m </em>和列<em class="lq"> n </em>中的元素</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/92fd8e1a66a555b44257ef47ee34ff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*8RSaRzPNdeKYnRzFdkbVKw@2x.png"/></div></figure><p id="59cd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等于</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c2952d147d8ad42639310c9fda6e0441.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*atRbDzLJ2LhQnYgq_59R-Q@2x.png"/></div></figure><p id="e852" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且行<em class="lq"> n </em>和列<em class="lq"> m </em>处的元素具有相同的值，这使其成为对称矩阵。这个投影矩阵有一些有趣的性质。首先，我们可以计算它的特征值和特征向量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="e76c" class="mu mv it mb b gy mw mx l my mz">lam= [ 3.618  0.   ]<br/>u= [[ 0.8507 -0.5257]<br/>    [ 0.5257  0.8507]]</span></pre><p id="9873" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，它有两个特征值(因为它是一个2 <em class="lq"> × </em> 2 <em class="lq"> </em>对称矩阵)。其中一个为零，另一个等于原矩阵<strong class="kw iu"> A </strong>的<em class="lq"> λ1 </em>。另外，特征向量与<strong class="kw iu"> A </strong>的特征向量完全相同。这不是巧合。假设我们得到本征分解方程中的第<em class="lq"> i- </em>项，乘以<strong class="kw iu"> ui </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/d3708cb18a8e0c8186291312aab5b8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*EVZ_V4LRAlFEJ8w6bjUCGw@2x.png"/></div></figure><p id="7eb8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道<strong class="kw iu"> ui </strong>是一个特征向量，它是归一化的，所以它的长度和它与自身的内积都等于1。所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/f3ad7fef0d12d3dd37ccbc7345c431a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*lLeHieKd1RhJh7_dKEu_lA@2x.png"/></div></figure><p id="6f05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你看特征向量的定义，这个方程意味着矩阵的一个特征值</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/92fd8e1a66a555b44257ef47ee34ff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*8RSaRzPNdeKYnRzFdkbVKw@2x.png"/></div></figure><p id="fd55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是<em class="lq"> λi </em>对应的特征向量是<strong class="kw iu"> ui </strong>。但是这个矩阵是一个<strong class="kw iu"> </strong> <em class="lq"> n×n对称</em>矩阵，应该有<em class="lq"> n个</em>特征值和特征向量。现在我们可以将它乘以<strong class="kw iu"> A </strong>的剩余<em class="lq"> (n-1) </em>个特征值中的任何一个，得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/081d5532c6ae42b726b2afaea599670a.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*_ieadJ8Rn_gvrfGlGgCGoA@2x.png"/></div></figure><p id="7783" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中i ≠ j .我们知道<strong class="kw iu"> A </strong>的特征值是正交的，这意味着它们中的每一对都是垂直的。两个垂直向量的内积为零(因为一个向量到另一个向量的标量投影应该为零)。所以<strong class="kw iu"> ui </strong>和<strong class="kw iu"> uj </strong>的内积为零，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/d1a73a856452becdec1598ae73718bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfYKohBj88qDm0tFgdiczQ@2x.png"/></div></div></figure><p id="802a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着<strong class="kw iu"> uj </strong>也是一个特征向量，其对应的特征值为零。所以我们得出结论，每个矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/92fd8e1a66a555b44257ef47ee34ff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*8RSaRzPNdeKYnRzFdkbVKw@2x.png"/></div></figure><p id="3eca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征分解方程中是一个对称的<em class="lq"> n×n </em>矩阵，具有<em class="lq"> n </em>个特征向量<em class="lq">。</em>特征向量与原矩阵<strong class="kw iu"> A </strong>相同，分别为<strong class="kw iu"> u1，u2，… un </strong>。<strong class="kw iu"> ui </strong>对应的特征值为<strong class="kw iu"> </strong> <em class="lq"> λi </em>(与<strong class="kw iu"> A </strong>相同)，但其他特征值均为零。现在，记住对称矩阵是如何变换向量的。它会沿着向量的特征向量拉伸或收缩向量，拉伸或收缩的量与对应的特征值成正比。所以这个矩阵将沿着<strong class="kw iu"> ui </strong>拉伸一个向量。但是因为其他特征值都是零，它会在那些方向上收缩到零。让我回到matrix <strong class="kw iu"> A </strong>并使用清单9绘制出<strong class="kw iu"> A1 </strong>的转换效果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/fa4a22bb1c1333abc11bf7aa2fc5344f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aI131dj3soU1TbAcNkEaA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图10</p></figure><p id="4eb4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，最初的圆沿着<strong class="kw iu"> u1 </strong>被拉伸，沿着<strong class="kw iu"> u2收缩到零。</strong>所以这个变换的结果是一条直线，而不是一个椭圆。这与<strong class="kw iu"> A1 </strong>是一个投影矩阵，应该把一切都投影到<strong class="kw iu"> u1 </strong>上的事实是一致的，所以结果应该是沿着<strong class="kw iu"> u1的一条直线。</strong></p><p id="25fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">排名</strong></p><p id="36cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图10显示了一个有趣的例子，其中2 <em class="lq"> × </em> 2 <em class="lq"> </em>矩阵<em class="lq"> </em> <strong class="kw iu"> A1 </strong>乘以一个二维矢量<strong class="kw iu"> x </strong>，但是变换后的矢量<strong class="kw iu"> Ax </strong>是一条直线。这是另一个例子。假设我们有一个矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0be38d17c1d9cc4855a85210ebf7db8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*Fghk1ea1PIVVdwOAbLVLCw@2x.png"/></div></figure><p id="ba12" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图11显示了它如何转换单位向量<strong class="kw iu"> x </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/c58c438c2633f884cd09a4dfff582854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YcR3KRiVuRIGzENqD9qlXg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图11</p></figure><p id="87cd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以它作为一个投影矩阵，将<strong class="kw iu"> x </strong>中的所有向量投影到直线<em class="lq"> y= </em> 2 <em class="lq"> x </em>上。这是因为F的列不是线性独立的。其实如果把<strong class="kw iu"> F </strong>的列分别叫做<strong class="kw iu"> f1 </strong>和<strong class="kw iu"> f2 </strong>，那么我们就有了<strong class="kw iu"> f1 </strong> = <strong class="kw iu"> 2f2 </strong>。记得我们把矩阵和向量的乘法写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/a281b4c6800cc11acf9a9c980dbbd186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oc4BNxRFLAyOS0IxNSi-2Q@2x.png"/></div></div></figure><p id="5585" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以不像<strong class="kw iu"> x </strong>中的矢量需要两个坐标，<strong class="kw iu"> Fx </strong>只需要一个坐标，存在于一维空间中。一般来说，<em class="lq"> m×n </em>矩阵不一定将一个<em class="lq"> n </em>维向量转换成另一个<em class="lq"> m </em>维向量。如果矩阵的列不是线性独立的，则变换向量的维数可以更低。</p><p id="615e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">矩阵<strong class="kw iu"> A </strong>的<em class="lq">列空间</em>记为<em class="lq">Col</em>T58】A定义为<strong class="kw iu"> A </strong>列的所有线性组合的集合，由于<strong class="kw iu"> Ax </strong>也是<strong class="kw iu"> A </strong>列的线性组合，<em class="lq"> Col </em> <strong class="kw iu"> A </strong>是所有的集合<em class="lq">Col</em>T74】A的基矢个数或者<em class="lq">Col</em>T78】A的维数称为<strong class="kw iu"> A </strong>的秩。所以<strong class="kw iu"> A </strong>的等级就是<strong class="kw iu"> Ax </strong>的尺寸。</p><p id="229c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> A </strong>的秩也是<strong class="kw iu"> A </strong>的线性无关列的最大个数。这是因为我们可以把所有的相关列写成这些线性无关列的线性组合，而<strong class="kw iu"> Ax </strong>也就是<strong class="kw iu"> </strong>所有列的线性组合可以写成这些线性无关列的线性组合。于是它们跨越了<strong class="kw iu"> Ax </strong>并形成了<em class="lq">col</em>T12】A的一个基，这些向量的个数就成为了<strong class="kw iu"> A </strong>的<em class="lq"> col </em>的维数或者<strong class="kw iu"> A </strong>的秩。</p><p id="0107" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在前面的例子中，<strong class="kw iu"> F </strong>的排名是1。另外，在特征分解方程中，每个矩阵的秩</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/92fd8e1a66a555b44257ef47ee34ff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*8RSaRzPNdeKYnRzFdkbVKw@2x.png"/></div></figure><p id="b077" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是1。记住，它们只有一个非零特征值，这不是巧合。可以证明对称矩阵的秩等于其非零特征值的个数。</p><p id="47e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们再次回到特征分解方程。假设我们将对称矩阵<strong class="kw iu"> A </strong>应用于任意向量<strong class="kw iu"> x </strong>。现在，本征分解方程变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/27629966d74cccc512b48017db5263f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bHtlUeMLVlkXEyRnj3wIqQ@2x.png"/></div></div></figure><p id="cc59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个特征向量<strong class="kw iu"> ui </strong>都是归一化的，所以它们是单位向量。现在在特征分解方程的每一项中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/f479085fd43e32632b4dcdaebdfe18ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*NLMreifQ3LA886xzkShzcg@2x.png"/></div></figure><p id="5942" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给出一个新的矢量，它是<strong class="kw iu"> x </strong>在<strong class="kw iu"> ui </strong>上的正交投影。<strong class="kw iu"> </strong>然后这个向量乘以<em class="lq"> λi </em>。由于<em class="lq"> λi </em>是一个标量，将其乘以一个矢量，只会改变该矢量的大小，而不会改变其方向。所以<em class="lq"> λi </em>只改变的幅度</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/f479085fd43e32632b4dcdaebdfe18ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*NLMreifQ3LA886xzkShzcg@2x.png"/></div></figure><p id="dea0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后所有的<em class="lq"> n个</em>向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c324ab918457c7984ad04c42afbfa9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*JrjL-WA9tbdODexF9GiDww@2x.png"/></div></figure><p id="858f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">加在一起就是给<strong class="kw iu">斧头</strong>。这个过程如图12所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/c44cb8eaece72ba53c0c9ec9affafc99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMBmVvBUMUhKsr6Q8oMpzA.jpeg"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图12</p></figure><p id="e490" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，本征分解从数学上解释了对称矩阵的一个重要性质，我们在之前的图中看到了。对称矩阵通过沿着向量的特征向量拉伸或收缩向量来变换向量，并且沿着每个特征向量的拉伸或收缩量与对应的特征值成比例。</p><p id="15f8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，特征分解可以将一个<em class="lq"> n×n </em>对称矩阵分解成形状相同的<em class="lq"> n </em>个矩阵<em class="lq"/>(<em class="lq">n×n</em>)乘以其中一个特征值。特征值在这里起着重要的作用，因为它们可以被认为是一个乘数。投影矩阵只将<strong class="kw iu"> x </strong>投影到每个<strong class="kw iu"> ui </strong>上，但是特征值缩放了向量投影的长度(<strong class="kw iu"> ui ui^Tx </strong>)。特征值越大，得到的向量(<em class="lq"> λi </em> <strong class="kw iu"> ui ui^Tx </strong>)的长度越大，赋予其对应矩阵的权重也越大(<strong class="kw iu"> ui ui^T </strong>)。因此，我们可以通过对具有最高特征值的项求和来近似原始对称矩阵<strong class="kw iu"> A </strong>。例如，如果我们假设特征值<em class="lq"> λi </em>已经按降序排序，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/f09097860f41cdcf65f5cb78c7a153d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q3obxvAgZKuvV2Cz6p1bjw@2x.png"/></div></div></figure><p id="9120" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么我们只能取特征分解方程中的前<em class="lq"> k </em>项来很好地近似原始矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/8ad3730515135fa9178f235490322ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKxlMniky18a9WdsG3fE-w@2x.png"/></div></div></figure><p id="8b09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> Ak </strong>是<strong class="kw iu"> A </strong>与<strong class="kw iu"/><strong class="kw iu"/><em class="lq">k</em><strong class="kw iu"/>的近似术语。如果我们只将前<em class="lq"> k </em>个特征值和特征向量包含在原特征分解方程中，我们得到相同的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/1913068ccf10fa401a73592329a78d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXbElSjrgnH-Lt8tOIKvow@2x.png"/></div></div></figure><p id="e62b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在<strong class="kw iu"> Dk </strong>是由<em class="lq">A的</em>第一<em class="lq"> k </em>特征值组成的<em class="lq"> k×k </em>对角矩阵，Pk 是由<em class="lq">A的</em>第一<em class="lq"> k </em>特征向量组成的<em class="lq"> n×k </em>矩阵，其转置成为<em class="lq"> k×n </em>矩阵。<em class="lq"> </em>所以他们的乘法仍然给出一个<em class="lq"> n×n </em>矩阵，它与<strong class="kw iu"> A </strong>的近似相同。</p><p id="e874" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果在原矩阵<strong class="kw iu"> A </strong>中，我们略去的其他(<em class="lq"> n-k) </em>特征值都很小，接近于零，那么这个近似矩阵和原矩阵很相似，我们就有了很好的近似。[数]矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/c2ce9259a96b826885d0890865f4d81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*pT6y33oLXxpmiVwgrpXv8g@2x.png"/></div></figure><p id="f111" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">随着</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/41a4c84f58b3ee3d5c78d8bab01cddb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*Jx9crQ5BZeRKsue5jqWDOQ@2x.png"/></div></figure><p id="b3b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就是一个例子。这里<em class="lq"> λ2 </em>比较小。<em class="lq"> W </em> e调用单位圆内的向量<strong class="kw iu"> x </strong>，用原矩阵(<strong class="kw iu"> Cx </strong>)绘制它们的变换。然后，我们用矩阵<strong class="kw iu"> C </strong>的特征分解方程中的第一项来近似矩阵<strong class="kw iu">C</strong>,该方程为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d1fd1225c43d487488fe7b67171aa6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*Lyi82UX15rXJlbF4wQcESQ@2x.png"/></div></figure><p id="7ca9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并由此绘制出<strong class="kw iu"> s </strong>的变换。如图13所示，近似矩阵的结果是一条直线，非常接近原始矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/e8f0c53711d973f5097db0e58a9cdcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2pYTn5tEWbScloiJ6oGxsQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图13</p></figure><p id="ac90" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为什么特征分解方程成立，为什么需要对称矩阵？记住对称矩阵的重要性质。假设<strong class="kw iu"> x </strong>是一个<em class="lq"> n×1 </em>列向量。<em class="lq">T5如果<strong class="kw iu"> A </strong>是一个<em class="lq"> n×n </em>对称矩阵，那么它有<em class="lq"> n个</em>线性无关且正交的特征向量可以作为新的基。所以我们现在可以写出<strong class="kw iu"> x </strong>相对于这个新基的坐标:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4e380bbd47f500f34d84071c3b8f96cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*aCCTktYzk6kNQgoqrlZVNg@2x.png"/></div></figure><p id="76ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">而基于基的定义，任何向量<strong class="kw iu"> x </strong>都可以唯一地写成<strong class="kw iu"> A </strong>的特征向量的线性组合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/250729ab06b0465eaf28b6636391fc2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*S9Z46Ths7uWZcwhNuI5FLg@2x.png"/></div></figure><p id="cac6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是对称矩阵的特征向量也是正交的。所以要找到每个坐标<em class="lq"> ai </em>，我们只需要通过点<strong class="kw iu"> x </strong>画一条垂直于<strong class="kw iu"> ui </strong>的轴的线，看它在哪里相交(参见图8)。如前所述，这也可以使用投影矩阵来完成。所以每项<em class="lq"> ai </em>等于<strong class="kw iu"> x </strong>和<strong class="kw iu"> ui </strong>的点积(参考图9)，可以写成<strong class="kw iu"> x </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/66229f081e74f3e77b45936a8fe3f216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9YIMpidq6cBBQlmeML9gw@2x.png"/></div></div></figure><p id="c6db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们需要一个对称矩阵将<strong class="kw iu"> x </strong>表示为上式中特征向量的线性组合。现在，如果我们将<strong class="kw iu"> A </strong>乘以<strong class="kw iu"> x </strong>，<strong class="kw iu"> </strong>，我们就可以分解出<em class="lq"> ai </em>项，因为它们是标量。所以<strong class="kw iu"> </strong>我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/ee654bfcdfcbd3cfa7ffaaf7bf8fca31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSt3L79RJTdl8BfjxG3OJQ@2x.png"/></div></div></figure><p id="0c4a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于<strong class="kw iu"> ui </strong>向量是<strong class="kw iu"> A </strong>的特征向量，我们最终得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/582ad38f3a055991aac693b6e44108cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CI6GrWiYKTjoPKBceMVn6Q@2x.png"/></div></div></figure><p id="181a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是特征分解方程。在乘以<strong class="kw iu"> A </strong>后发生的任何事情对所有矩阵都成立，并且不需要对称矩阵。我们需要一个<em class="lq"> n×n </em>对称矩阵，因为它有<em class="lq"> n个</em>实特征值加上<em class="lq"> n个</em>线性独立和正交的特征向量，可以作为<strong class="kw iu"> x </strong>的新基。当你有一个非对称矩阵时，你就没有这样的组合。例如，假设您有一个非对称矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/c63cfc756ce746236a4dfd6ea9fbf29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*EMDKwbnxiF08f4ZFlFeb7Q@2x.png"/></div></div></figure><p id="ca8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你计算这个矩阵的特征值和特征向量，你得到:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="dfd2" class="mu mv it mb b gy mw mx l my mz">lam= [2.5+0.866j 2.5-0.866j]<br/>u= [[0.7071+0.j     0.7071-0.j    ]<br/>    [0.3536-0.6124j 0.3536+0.6124j]]</span></pre><p id="f69b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着你没有真正的特征值来做分解。另一个例子是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/6a92f503f1df91ab4865e1da73fee427.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*GhnzMx13oI5PKUTFIV4ZHg@2x.png"/></div></figure><p id="5fac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你会得到:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="bc5b" class="mu mv it mb b gy mw mx l my mz">lam= [2. 2.]<br/>u= [[ 1. -1.]<br/>    [ 0.  0.]]</span></pre><p id="7234" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，特征向量不是线性独立的。其实<strong class="kw iu"> u1 </strong> = - <strong class="kw iu"> u2 </strong>。<strong class="kw iu"> </strong>所以你不能像图11一样只使用一个特征向量来重建<strong class="kw iu"> A </strong>。此外，它没有显示如图14所示的该矩阵的拉伸方向。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/d97365055e2129895c5cac5148ac484e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXpAMCioaXM27GSQ1JEXsg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图14</p></figure><p id="5c06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，请记住</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9c0b565cd6872e45a4290c6d1d1ab6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*aTjz24-8TBisxhu3bqsfTg@2x.png"/></div></figure><p id="9b56" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="05dd" class="mu mv it mb b gy mw mx l my mz">lam= [ 7.8151 -2.8151]<br/>u= [[ 0.639  -0.5667]<br/>    [ 0.7692  0.8239]]</span></pre><p id="331b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，特征向量是线性独立的，但它们不是正交的(参见图3)，并且它们没有显示变换后该矩阵的正确拉伸方向。</p><p id="a864" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征分解法非常有用，但只对对称矩阵有效。对称矩阵总是正方形矩阵，所以如果你有一个矩阵不是正方形的，或者是正方形但非对称的矩阵，那么你就不能用特征分解法用其他矩阵来近似它。SVD可以克服这个问题。</p><p id="f0c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">奇异值</strong></p><p id="b40e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在讨论SVD之前，我们应该找到一种方法来计算非对称矩阵的拉伸方向。假设<strong class="kw iu"> A </strong>是一个不一定对称的<em class="lq"> m×n </em>矩阵。那么可以证明</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/5b73a7896aa75603529b98e069655fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*6u1pexiMOACM4kz-Vfv6zQ@2x.png"/></div></figure><p id="38d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是一个<em class="lq"> n×n </em>对称<em class="lq">T9】矩阵。记住乘积的转置是逆序转置的乘积。因此</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/aeae962a4a89286afa4d6847367528ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*bSqsqGzaZP3wuegSmPYzjg@2x.png"/></div></figure><p id="9515" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu">a^t</strong>T12】a等于它的转置，而且是对称矩阵。我们想要计算非对称矩阵的拉伸方向。，但是我们如何从数学上定义拉伸方向呢？</p><p id="380d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">到目前为止，我们只关注了二维空间中的向量，但是我们可以在二维空间中使用相同的概念。在这里，我集中在一个三维空间，以便能够形象化的概念。现在列向量有3个元素。最初，我们有一个球体，它包含所有距离原点一个单位的向量，如图15所示。如果我们称这些向量为<strong class="kw iu"> x </strong>，那么|| <strong class="kw iu"> x </strong> ||=1。<em class="lq"> </em>现在如果我们把它们乘以一个3 <em class="lq"> × </em> 3 <em class="lq"> </em>对称<em class="lq"> </em>矩阵，<strong class="kw iu"> Ax </strong>就变成了一个三维椭圆。拉伸的第一方向可以被定义为在该椭圆中具有最大长度的向量的方向(图15中的<strong class="kw iu"> Av1 </strong>)。实际上，<strong class="kw iu"> Av1 </strong>是|| <strong class="kw iu"> Ax </strong> ||在所有单位向量<strong class="kw iu"> x </strong>上的最大值。这个矢量是<strong class="kw iu"> A </strong>对矢量<strong class="kw iu"> v1 </strong>的变换。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/2d47bb095fd210129a59d4141978c7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40LHp7Z8Pa6DbSY-6MlqKQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图15</p></figure><p id="b4ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">拉伸的第二方向是沿着向量<strong class="kw iu"> Av2 </strong>。<strong class="kw iu"> Av2 </strong>是|| <strong class="kw iu"> Ax </strong> ||在<strong class="kw iu"> x </strong>中垂直于<strong class="kw iu"> v1 </strong>的所有向量上的最大值。所以在<strong class="kw iu"> x中的所有向量中，</strong>我们<strong class="kw iu"> </strong>最大化<strong class="kw iu"> </strong> || <strong class="kw iu"> Ax </strong> ||以此约束<strong class="kw iu"> x </strong>垂直于<strong class="kw iu"> v1 </strong>。最后，<strong class="kw iu"> v3 </strong>是垂直于<strong class="kw iu"> v1 </strong>和<strong class="kw iu"> v2 </strong>的向量，并且在这些约束条件下给出了<strong class="kw iu"> Ax </strong>的最大长度。<strong class="kw iu"> Av3 </strong>的方向决定拉伸的第三个方向。所以一般在一个<em class="lq"> n </em>维空间中，第<em class="lq"> i </em>个拉伸方向是矢量<strong class="kw iu"> Avi </strong>的方向，它具有最大的长度，并且垂直于前面的(<em class="lq"> i </em> -1)个拉伸方向。</p><p id="a488" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让<strong class="kw iu"> A </strong>成为一个<em class="lq"> m×n </em>矩阵。我们证明了<strong class="kw iu"> A^T A </strong>是一个对称矩阵，所以它有<em class="lq"> n个</em>实特征值和<em class="lq"> n个</em>线性独立和正交的特征向量，这些特征向量可以构成它可以变换的<em class="lq"> n- </em>元素向量的基础(在<strong class="kw iu"> R^n </strong>空间中)。我们称这些特征向量为<strong class="kw iu"> v1 </strong>，<strong class="kw iu"> v2 </strong>，… <strong class="kw iu"> vn </strong>，我们假设它们是归一化的。对于这些特征向量中的每一个，我们可以使用长度的定义和转置矩阵乘积的规则来得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/1a84592ec9a7dee2592501123a530799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*ps8X2pX2KsSKiKUj0XGwgg@2x.png"/></div></figure><p id="6fbf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们假设<strong class="kw iu"> vi </strong>对应的特征值为<em class="lq"> λi </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/76488dd98f0b5a46d329c1328b7eb8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*v26RBaNz02UJih_5dtckkw@2x.png"/></div></figure><p id="9990" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是vi是归一化的，所以</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/f13a50beef8c89b74751e7bd80f95cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*RRESlJnXlyj0vUy54NrJnA@2x.png"/></div></figure><p id="de24" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/691685a1335fe56a789a9bd97a51734a.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*OpQU2srrPeA4VbdRasvEzA@2x.png"/></div></figure><p id="bd33" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个结果表明所有的特征值都是正的。现在假设我们按降序给它们贴标签，那么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/a8ec4cb3277e904c065938def5abffb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*PwL_Ecjp_cgt-5asB1_0Sw@2x.png"/></div></figure><p id="9ee6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们将<strong class="kw iu"> A </strong>的奇异值定义为<em class="lq">λI</em>(<strong class="kw iu">a^t a</strong>的特征值)的平方根，我们用<em class="lq"> σi </em>表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/fcc35ba4165830c56c701866e9c674b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*gXS1pbGjpdcH6HkWIi3Mgw@2x.png"/></div></figure><p id="b001" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> A </strong>的奇异值就是向量<strong class="kw iu"> Avi </strong>的长度。现在我们可以总结一个重要的结果，它构成了奇异值分解方法的基础。可以看出，|| <strong class="kw iu"> Ax </strong> ||的最大值受到约束</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/896c3b5dcd54112bbc03c311df44f134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzfkPQ6meu49NxgBULxULQ@2x.png"/></div></div></figure><p id="cf40" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是<em class="lq"> σk </em>，并且该最大值在<strong class="kw iu"> vk </strong>处达到。对于约束条件，我们使用了这样的事实:当<strong class="kw iu"> x </strong>垂直于<strong class="kw iu"> vi </strong>时，它们的点积为零。</p><p id="929c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以如果<strong class="kw iu"> vi </strong>是<strong class="kw iu">a^t</strong>T4】a的特征向量(按其对应的奇异值排序)，并假设|| <strong class="kw iu"> x </strong> ||=1，那么<strong class="kw iu"> Avi </strong>为<strong class="kw iu"> Ax </strong>呈现一个拉伸方向，对应的奇异值<strong class="kw iu"> </strong> <em class="lq"> σi </em>给出了<strong class="kw iu"> Avi </strong>的长度。</p><p id="c1b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">奇异值也可以决定<strong class="kw iu"> A </strong>的秩。假设非零奇异值的个数为<em class="lq"> r </em>。因为它们是正的，并按递减顺序标注，所以我们可以把它们写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/8f34f67abb143e2b6aaafc6afb57a163.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*OTmqPSkeCa_EOJWnUc-Brw@2x.png"/></div></figure><p id="1e22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这对应于</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/3ce6ccf75f03cdf9ef792ac0651f3563.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*XK4pDHyPxObX4F_xHjQ_-g@2x.png"/></div></figure><p id="779a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且每个<em class="lq"> λi </em>都是<strong class="kw iu"> vi </strong>对应的特征值。那么可以表明，秩<strong class="kw iu"> A </strong>是形成<strong class="kw iu"> Ax </strong>的基础的向量的数量，是<em class="lq"> r </em>。还可以看出，集合{ <strong class="kw iu"> Av1 </strong>，<strong class="kw iu"> Av2 </strong>，…，<strong class="kw iu"> Avr </strong> }是<strong class="kw iu">Ax</strong>(<em class="lq">Col</em><strong class="kw iu">A</strong>)的正交基。因此矢量<strong class="kw iu"> Avi </strong>相互垂直，如图15所示。</p><p id="0b93" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们回到非对称矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9c0b565cd6872e45a4290c6d1d1ab6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*aTjz24-8TBisxhu3bqsfTg@2x.png"/></div></figure><p id="ae5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们在图3中绘制了<strong class="kw iu"> A </strong>的特征向量，并提到它们没有显示<strong class="kw iu"> Ax </strong>的拉伸方向。在图16中，左侧绘制了<strong class="kw iu"> A^T A </strong>的特征向量(<strong class="kw iu"> v1 </strong>和<strong class="kw iu"> v2 </strong>)。由于<strong class="kw iu"> A^T A </strong>是对称矩阵，这些向量显示了它的拉伸方向。<strong class="kw iu"> </strong>在右侧，画出了矢量<strong class="kw iu"> Av1 </strong>和<strong class="kw iu"> Av2 </strong>，很明显这些矢量显示了<strong class="kw iu"> Ax </strong>的拉伸方向。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qg"><img src="../Images/2239ee2fd20e90907810b8a7c9528dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHMRtxwR7h4IzW6bXsLapQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图16</p></figure><p id="326b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此<strong class="kw iu"> Avi </strong>显示<strong class="kw iu"> A </strong>的拉伸方向，无论<strong class="kw iu"> A </strong>是否对称。</p><p id="0bfc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在想象矩阵<strong class="kw iu"> A </strong>是对称的，并且等于它的转置矩阵。另外，假设其第<em class="lq"> i </em>个特征向量为<strong class="kw iu"> ui </strong>，对应的特征值为<em class="lq"> λi </em>。如果我们将<strong class="kw iu"> A^T A </strong>乘以<strong class="kw iu"> ui </strong>，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/e79e44830832b16e0d9348746a45936a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5ocPHZ-rviaizGIflKdgQ@2x.png"/></div></div></figure><p id="0a4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也就是说<strong class="kw iu"> ui </strong>也是<strong class="kw iu"> A^T A </strong>，<strong class="kw iu"> </strong>的一个特征向量但是它对应的特征值是<em class="lq"> λi </em>。所以当<strong class="kw iu"> A </strong>对称时，不用计算<strong class="kw iu"> Avi </strong>(其中<strong class="kw iu"> vi </strong>是<strong class="kw iu"> A^T A的特征向量)</strong>我们可以简单地使用<strong class="kw iu">ui</strong>(<strong class="kw iu">a</strong>的特征向量)来得到拉伸的方向，这正是我们对特征分解过程所做的。既然我们知道了如何计算非对称矩阵的拉伸方向，我们就可以看SVD方程了。</p><p id="9c34" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">奇异值分解</strong></p><p id="a0e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">设<strong class="kw iu"> A </strong>为<em class="lq"> m×n </em>矩阵，秩<strong class="kw iu"> A </strong> = <em class="lq"> r </em>。所以<strong class="kw iu"> A </strong>的非零奇异值个数为<em class="lq"> r </em>。因为它们是正的，并按递减顺序标注，所以我们可以把它们写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/9072761a1fbb47d3850ccfd545ba7ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*D5CjdPz2_ZwO0FCVpa-txQ@2x.png"/></div></figure><p id="e64d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/50fea502311f970c29c8444743db05fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6I5HHYIOGh7OKCD3Sqjleg@2x.png"/></div></figure><p id="33c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道每个奇异值<em class="lq"> σi </em>是<em class="lq"> λi的平方根(<strong class="kw iu"> A^TA </strong>的</em>特征值)，对应一个同阶的特征向量<strong class="kw iu"> vi </strong>。现在我们可以把<strong class="kw iu">的<em class="lq">奇异值分解</em>写成</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/9810e9eb653fffd96372ccb1a7673528.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*N0vu8qXewtKCLG_cztrxhA@2x.png"/></div></figure><p id="c14f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> V </strong>是一个<em class="lq"> n×n </em>矩阵，其列为<strong class="kw iu"> vi </strong>。所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/af8a00eda75da239ddcd30292971df1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*r47Bm8neVvTpAAukeBBbxQ@2x.png"/></div></figure><p id="8778" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们称一组正交且归一化的向量为<a class="ae lr" href="https://en.wikipedia.org/wiki/Orthonormality" rel="noopener ugc nofollow" target="_blank"> <em class="lq">正交</em> </a>集合。所以集合{ <strong class="kw iu"> vi </strong> }是一个正交集合。列是正交集合的矩阵称为<a class="ae lr" href="https://en.wikipedia.org/wiki/Orthogonal_matrix" rel="noopener ugc nofollow" target="_blank"> <em class="lq">正交矩阵</em> </a>，并且<strong class="kw iu"> V </strong>是正交矩阵。</p><p id="7d6a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">σ</strong>是一个形式为<em class="lq"> m×n </em>的对角矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/b98a76cf3f1a90174a290a511efeae0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKbir6j6rDlcmHqgebaRyw@2x.png"/></div></div></figure><p id="548b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们先做一个<em class="lq"> r × r </em>对角矩阵，对角元素为<em class="lq"> σ1，σ2，…，σr </em>。然后我们用零填充它，使它成为一个<em class="lq"> m × n </em>矩阵。</p><p id="7f04" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还知道集合{ <strong class="kw iu"> Av1 </strong>，<strong class="kw iu"> Av2 </strong>，…，<strong class="kw iu"> Avr </strong> }是<em class="lq"> Col </em> <strong class="kw iu"> A，</strong>和<em class="lq">σI =</em>|<strong class="kw iu">Avi</strong>| |的正交基。因此，我们可以通过将向量除以它们的长度来标准化这些向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dfafa0ac3c803759997efc8f23c6c74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XPO8kthsc1HqLwN8qrCKZQ@2x.png"/></div></figure><p id="79a2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们有一个集合{ <strong class="kw iu"> u1 </strong>，<strong class="kw iu"> u2 </strong>，…，<strong class="kw iu"> ur </strong> }它是<strong class="kw iu"> Ax </strong>的正交基，Ax 是<em class="lq"> r </em>维的。我们知道<strong class="kw iu"> A </strong>是一个<em class="lq"> m × n </em>矩阵，<strong class="kw iu"> A </strong>的秩最多可以是<em class="lq"> m </em>(当<strong class="kw iu"> A </strong>的所有列线性无关时)。由于我们需要一个<strong class="kw iu"> U </strong>的<em class="lq"> m×m </em>矩阵，我们将<strong class="kw iu"/><em class="lq">(m-r)</em><strong class="kw iu"/>向量添加到<strong class="kw iu"> ui </strong>集合中，使其成为一个m维空间<strong class="kw iu">r</strong>^m<strong class="kw iu"/>的归一化基(有几种方法可以用于此目的。例如，我们可以使用<em class="lq">格拉姆-施密特过程。</em>然而，解释它超出了本文的范围)。所以现在我们有了一个标准正交基{ <strong class="kw iu"> u1 </strong>，<strong class="kw iu"> u2 </strong>，…，<strong class="kw iu"> um </strong> }。这些向量将是正交矩阵<em class="lq"> m×m </em>的<strong class="kw iu"> U </strong>的列</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3497bd869e1dada0248fc92b195d9a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*wtszjOSHBINcoCd1l9_FWQ@2x.png"/></div></figure><p id="2990" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以最后，我们可以将<strong class="kw iu"> A </strong>分解为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qk"><img src="../Images/d0c1796412b31f47cedaf5bd6ffed957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whsfU88o1UEvXmVnmp6IuQ@2x.png"/></div></div></figure><p id="4733" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了更好地理解这个等式，我们需要简化它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ql"><img src="../Images/070206804b08d0619b3cf91cc9d86879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPRdjjlN8FQyIF2o2NjDog@2x.png"/></div></div></figure><p id="44fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道<em class="lq"> σi </em>是标量；<em class="lq"> </em> <strong class="kw iu"> ui </strong>是m维列向量，<strong class="kw iu"> vi </strong>是n维列向量。于是每个<em class="lq">σI</em><strong class="kw iu">ui</strong><strong class="kw iu">VI</strong>^t是一个<em class="lq"> m×n </em>矩阵，SVD方程将矩阵<strong class="kw iu"> A </strong>分解成形状相同的<em class="lq"> r </em>矩阵(<em class="lq"> m×n </em>)。</p><p id="44d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，让我来说明为什么这个等式是有效的。如果我们将SVD方程的两边乘以<strong class="kw iu"> x </strong>，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qm"><img src="../Images/c552e1e4fb5584db8154e35f1f718444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdGC2LQKJqrZQtLshGbD9Q@2x.png"/></div></div></figure><p id="0e68" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道集合{ <strong class="kw iu"> u1 </strong>，<strong class="kw iu"> u2 </strong>，…，<strong class="kw iu"> ur </strong> }是<strong class="kw iu"> Ax </strong>的一个正交基。所以向量<strong class="kw iu"> Ax </strong>可以写成它们的线性组合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/b578fea327adfb4e76effc347e110341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*1xfG3we15GusdPTDJmiQFQ@2x.png"/></div></figure><p id="9b18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且由于<strong class="kw iu"> ui </strong>向量是正交的，每一项<em class="lq"> ai </em>等于<strong class="kw iu"> Ax </strong>和<strong class="kw iu"> ui </strong>(标量投影<strong class="kw iu"> Ax </strong>到<strong class="kw iu"> ui </strong>)的点积:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0af6b513260e1384dca4def03fd90f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*85erE7ph2y03haXMZqgf4w@2x.png"/></div></figure><p id="a5f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是我们也知道</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qn"><img src="../Images/24c73ef50258b87e36e1761bcc2e6dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*Jp-TJG88sLTQiIoe3VQCwA@2x.png"/></div></div></figure><p id="ed5e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，将它代入前面的等式，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/1c554fe4bb794b09f5fe80e96e603ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*7Mwfsd4kORPbylaf7VkOGA@2x.png"/></div></figure><p id="8a3d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还知道<strong class="kw iu"> vi </strong>是<strong class="kw iu"> A </strong> ^T <strong class="kw iu"> A </strong>的特征向量，其对应的特征值<em class="lq"> λi </em>是奇异值<em class="lq"> σi </em>的平方</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qp"><img src="../Images/823862cff22039fea774d58a6354a3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFWPW3yiV3pSsz5zVO5HjA@2x.png"/></div></div></figure><p id="1c92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是点积是可交换的，所以</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/59d329dd9d64b1102489965c887267ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*nD260OaUkpkhVQTbiQf2MA@2x.png"/></div></figure><p id="220b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，<strong class="kw iu"> vi^Tx </strong>给出了<strong class="kw iu"> x </strong>到<strong class="kw iu"> vi </strong>的标量投影，长度由奇异值缩放。现在，如果我们将<em class="lq"> ai </em>值代入<strong class="kw iu"> Ax </strong>、<strong class="kw iu"> </strong>的方程，我们得到SVD方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qm"><img src="../Images/c552e1e4fb5584db8154e35f1f718444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdGC2LQKJqrZQtLshGbD9Q@2x.png"/></div></div></figure><p id="951d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以每个<em class="lq">ai</em>=<em class="lq">σI</em><strong class="kw iu">VI</strong><strong class="kw iu">^tx</strong>都是<strong class="kw iu"> Ax </strong>到<strong class="kw iu"> ui </strong>上的标量投影，如果乘以<strong class="kw iu"> ui </strong>，结果就是<strong class="kw iu"> </strong>一个向量，是<strong class="kw iu"> Ax </strong>到<strong class="kw iu"> ui </strong>上的正交投影。奇异值<em class="lq"> σi </em>沿<strong class="kw iu"> ui </strong>缩放该向量的长度。记住，在特征分解方程中，每个<strong class="kw iu"> ui ui^T </strong>是一个投影矩阵，它给出了<strong class="kw iu"> x </strong>到<strong class="kw iu"> ui </strong>的正交投影。这里<em class="lq">σI</em><strong class="kw iu">VI</strong><strong class="kw iu">^t</strong>可以认为是一个取<strong class="kw iu"> x </strong>的投影矩阵，但是将<strong class="kw iu"> Ax </strong>投影到<strong class="kw iu"> ui </strong>上。由于它将所有向量投影到<strong class="kw iu"> ui </strong>上，所以它的秩是1。图17总结了SVD所需的所有步骤。我们首先从所有长度为1的矢量中挑选一个随机的二维矢量<strong class="kw iu"> x1 </strong>(图17–1)。然后我们尝试使用SVD方法计算<strong class="kw iu"> Ax1 </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qr"><img src="../Images/47cd3dd83cf85452b86114e6746fb7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XRjl83lHD7HBrMlrSyN8zQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图17</p></figure><p id="65a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先我们计算<strong class="kw iu"> A^TA </strong>的特征值(<em class="lq"> λ1 </em>、<em class="lq"> λ2 </em>)和特征向量(<strong class="kw iu"> v1 </strong>、<strong class="kw iu"> v2 </strong>)。我们知道奇异值是特征值的平方根(<em class="lq"> σi </em> = <em class="lq"> λi </em>)，如(Figure 17–2)所示。<strong class="kw iu"> Av1 </strong>和<strong class="kw iu"> Av2 </strong>表示<strong class="kw iu"> Ax </strong>的拉伸方向，<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>是<strong class="kw iu"> Av1 </strong>和<strong class="kw iu"> Av2 </strong>的单位矢量(图17-4)。<strong class="kw iu"> Ax1 </strong>在<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>上的正交投影为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/3ae8fe4aa53bdbb30979eba31a27df56.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*jQnt7h8ukFEex4XklzbiGQ@2x.png"/></div></figure><p id="3f54" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分别(图17-5)，简单地把它们加在一起，我们得到<strong class="kw iu"> Ax1 </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/49a789911e0b138c1cf9140e886d05ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*omr4G_E_lCCue5ZsNeTWzw@2x.png"/></div></figure><p id="45f2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如所示(Figure 17–6)。</p><p id="6ab8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面这个例子展示了如何用Python计算矩阵的SVD。我们想找到的奇异值分解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/17667644aef45bee5a8777ae23ae9aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*A2wLKTFQW_c6gJoA6Bn0IA@2x.png"/></div></figure><p id="4fea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是一个2 <em class="lq"> × </em> 3 <em class="lq"> </em>的矩阵。<em class="lq"> </em>所以<em class="lq"> </em> <strong class="kw iu"> x </strong>是三维列向量，但是<strong class="kw iu"> Ax </strong>不是三维向量，<strong class="kw iu"> x </strong>和<strong class="kw iu"> Ax </strong>存在于不同的向量空间。首先，我们计算<strong class="kw iu"> A^T A </strong>的特征值和特征向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="3a8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出是:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="41d1" class="mu mv it mb b gy mw mx l my mz">lam= [90.1167  0.     12.8833]<br/>v= [[ 0.9415  0.3228  0.0969]<br/>    [ 0.3314 -0.9391 -0.0906]<br/>    [-0.0617 -0.1174  0.9912]]</span></pre><p id="5c93" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，第二特征值为零。由于<strong class="kw iu"> A^T A </strong>是对称矩阵，有两个非零特征值，所以它的秩是2。图18从不同角度显示了<strong class="kw iu"> A^T斧</strong>的两幅图。由于<strong class="kw iu"> A^TA </strong>的秩是2，所以所有向量<strong class="kw iu"> A^TAx </strong>都在一个平面上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qu"><img src="../Images/174c72c7e848f24b68322571f619e125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDe_-Ac1cA3-8byohplE5Q.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图18</p></figure><p id="5145" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">清单11展示了如何构建矩阵<strong class="kw iu">σ</strong>和<strong class="kw iu"> V </strong>。我们首先按降序排列特征值。<strong class="kw iu"> V </strong>的列是对应的相同顺序的特征向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="40e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们过滤非零特征值，取它们的平方根，得到非零奇异值。我们知道<strong class="kw iu">σ</strong>应该是一个3 <em class="lq"> × </em> 3 <em class="lq"> </em>矩阵。因此，我们将两个非零奇异值放在一个2 <em class="lq"> × </em> 2 <em class="lq"> </em>对角矩阵中，并用零填充它，得到一个<em class="lq"> 3 × 3 </em>矩阵。输出是:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="e161" class="mu mv it mb b gy mw mx l my mz">Sigma= [[9.493  0.     0.    ]<br/>        [0.     3.5893 0.    ]]<br/>V= [[ 0.9415  0.0969  0.3228]<br/>    [ 0.3314 -0.0906 -0.9391]<br/>    [-0.0617  0.9912 -0.1174]]</span></pre><p id="95bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了构造<strong class="kw iu"> V </strong>，我们取与<strong class="kw iu"> A </strong>的<em class="lq"> r </em>非零奇异值对应的<strong class="kw iu"> vi </strong>向量，并除以它们对应的奇异值。既然<strong class="kw iu"> A </strong>是一个2 <em class="lq"> × </em> 3 <em class="lq"> </em>矩阵，<strong class="kw iu"> U </strong>应该是一个2 <em class="lq"> × </em> 2 <em class="lq"> </em>矩阵。我们有2个非零奇异值，所以<strong class="kw iu"> A </strong>的秩是2，<em class="lq"> r </em> =2。因此，我们已经有足够多的<strong class="kw iu"> vi </strong>向量来组成<strong class="kw iu"> U </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="af92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出是:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="ceb7" class="mu mv it mb b gy mw mx l my mz">U= [[ 0.4121  0.9111]<br/>    [ 0.9111 -0.4121]]</span></pre><p id="3a59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们得到一个对<strong class="kw iu">的分解:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qv"><img src="../Images/c7c7c0936c64f4d955021a9f5ab6c040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_yd1UtvkhiiPx9kFeAiRw@2x.png"/></div></div></figure><p id="9400" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们真的不需要遵循所有这些步骤。NumPy有一个名为<code class="fe ly lz ma mb b">svd()</code>的函数可以为我们做同样的事情。清单13展示了我们如何使用这个函数轻松计算矩阵<strong class="kw iu"> A </strong>的SVD。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="63cb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出是:</p><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="81e0" class="mu mv it mb b gy mw mx l my mz">U= [[-0.4121 -0.9111]<br/>    [-0.9111  0.4121]]<br/>s= [9.493  3.5893]<br/>V [[-0.9415 -0.0969 -0.3228]<br/>   [-0.3314  0.0906  0.9391]<br/>   [ 0.0617 -0.9912  0.1174]]</span></pre><p id="d549" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您应该注意到输出中的一些事情。首先，这个函数返回一个奇异值数组，它位于<strong class="kw iu">σ</strong>的主对角线上，而不是矩阵<strong class="kw iu">σ</strong>上。另外，它返回<strong class="kw iu"> V^T，</strong>不是<strong class="kw iu"> V </strong>，所以我打印了它返回的数组<strong class="kw iu"> VT </strong>的转置。最后，<code class="fe ly lz ma mb b">svd()</code>报告的<strong class="kw iu"> ui </strong>和<strong class="kw iu"> vi </strong>向量与清单10-12中计算的<strong class="kw iu"> ui </strong>和<strong class="kw iu"> vi </strong>向量符号相反。记住如果<strong class="kw iu"> vi </strong>是某个特征值的特征向量，那么(-1) <strong class="kw iu"> vi </strong>也是同一特征值的特征向量，其长度也是相同的。所以如果<strong class="kw iu"> vi </strong>归一化，(-1) <strong class="kw iu"> vi </strong>也归一化。事实上，在清单10中，我们用不同的方法计算了<strong class="kw iu"> vi </strong>，而<code class="fe ly lz ma mb b">svd()</code>只是报告(-1) <strong class="kw iu"> vi </strong>，这仍然是正确的。由于<strong class="kw iu"> ui </strong> = <strong class="kw iu"> Avi/ </strong> σi，<code class="fe ly lz ma mb b">svd()</code>上报的<strong class="kw iu"> ui </strong>的集合也会有相反的符号。</p><p id="a9d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以很容易地构造矩阵<strong class="kw iu">σ</strong>并检查这些矩阵相乘是否得到<strong class="kw iu"> A. </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><pre class="kj kk kl km gt mq mb mr ms aw mt bi"><span id="70b0" class="mu mv it mb b gy mw mx l my mz">Reconstructed A= [[ 4.  1.  3.]<br/>                  [ 8.  3. -2.]]</span></pre><p id="e010" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在图19中，你可以看到一个由<strong class="kw iu"> x </strong>和<strong class="kw iu"> Ax </strong>组成的图，前者是单位球体中的矢量，后者是由<strong class="kw iu"> A </strong>产生的一组二维矢量。矢量<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>表示拉伸的方向。由<strong class="kw iu"> Ax </strong>生成的椭圆不像我们之前看到的那样是空心的(例如在图6中)，变换后的矢量完全填充了它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qw"><img src="../Images/f637f2a3af4c612d8160054d5ae8ddb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUeBWOlU-Bnx1YQj_Xf3zw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图19</p></figure><p id="f796" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">类似于特征分解方法，我们可以通过对具有最高奇异值的项求和来近似我们的原始矩阵<strong class="kw iu"> A </strong>。因此，我们可以使用SVD方程中的前<em class="lq"> k </em>项，使用<em class="lq"> k </em>最高奇异值，这意味着我们只包括分解方程中的<strong class="kw iu"> U </strong>和<strong class="kw iu"> V </strong>矩阵中的前<em class="lq"> k </em>向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qx"><img src="../Images/5561765f3aad1bac0c828195e8f32062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKZkc4fGEt5Fr03D_5SQdQ@2x.png"/></div></div></figure><p id="956f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道集合{ <strong class="kw iu"> u1 </strong>、<strong class="kw iu"> u2 </strong>、…、<strong class="kw iu"> ur} </strong>构成了<strong class="kw iu"> Ax </strong>的基础。所以当我们从这个集合中选取<em class="lq"> k </em>向量时，<strong class="kw iu"> Ak x </strong>就写成了<strong class="kw iu"> u1，u2，… uk </strong>的线性组合。<strong class="kw iu"> </strong>因此它们跨越<strong class="kw iu"> Ak x </strong>，并且由于它们是线性独立的，所以它们形成了<strong class="kw iu"> Ak x </strong>(或<em class="lq"> col </em> <strong class="kw iu"> A </strong>)的基础。所以<strong class="kw iu"> Ak </strong>的秩是<em class="lq"> k </em>，通过挑选第一个<em class="lq"> k </em>奇异值，我们用秩- <em class="lq"> k </em>矩阵来近似<strong class="kw iu"> A </strong>。</p><p id="99da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">举个例子，假设我们要计算矩阵的奇异值分解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qy"><img src="../Images/8852a00766584fed4ff9f66dd6d255c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*q4oIN0ibAYDFmQkk1Nha2A@2x.png"/></div></figure><p id="c68a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样,<strong class="kw iu"> x </strong>是单位球中的矢量(图19左图)。奇异值为<em class="lq"> σ1= </em> 11.97，<em class="lq"> σ2= </em> 5.57，<em class="lq"> σ3= </em> 3.25，<strong class="kw iu"> A </strong>的秩为3。因此<strong class="kw iu"> Ax </strong>在三维空间中是一个椭球体，如图20(左)所示。如果我们使用第一个奇异值对其进行近似，那么<strong class="kw iu"> Ak </strong>的秩将是1，并且<strong class="kw iu"> Ak </strong>乘以<strong class="kw iu"> x </strong>将是一条线(图20右侧)。如果只使用前两个奇异值，<strong class="kw iu"> Ak </strong>的秩将为2，<strong class="kw iu"> Ak </strong>乘以<strong class="kw iu"> x </strong>将为一个平面(图20中)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qz"><img src="../Images/1aa294f4146eaab80a757e36d9518c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6p1d3TXkcvYZXOBMKSeugA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图20</p></figure><p id="c773" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">值得注意的是，如果我们有一个对称矩阵，SVD方程简化为特征分解方程。假设对称矩阵<strong class="kw iu"> A </strong>具有特征向量<strong class="kw iu"> vi </strong>以及相应的特征值<em class="lq"> λi </em>。所以我们</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/8a5ce905b66ce4853c402e09bb2f8322.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*qtw_mGnmoZZTBQemGtj73Q@2x.png"/></div></figure><p id="fcd7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们已经表明，对于对称矩阵，<strong class="kw iu"> vi </strong>也是<strong class="kw iu"> A^TA </strong>的特征向量，对应的特征值为<em class="lq"> λi . </em>所以<strong class="kw iu"> A </strong>的奇异值为<em class="lq"> λi </em>和<em class="lq"> σi </em> = <em class="lq"> λi </em>的平方根。现在我们可以计算<strong class="kw iu"> ui </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/18c9cadeb4bffc5c128386b0347b4f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*yiCreEYpC0gXx9Xvu5RuCQ@2x.png"/></div></figure><p id="3788" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> ui </strong>是<strong class="kw iu"> A </strong>对应<em class="lq"> λi </em>(以及<em class="lq"> σi </em>)的特征向量。现在我们可以简化SVD方程，得到特征分解方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/78eff9759c36482c8c7c50a6ee4ce421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*8ZrIUbVbdw3gCI2llk4AsQ@2x.png"/></div></figure><p id="3f6f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，可以看出SVD是用秩- <em class="lq"> k </em>矩阵逼近<strong class="kw iu"> A </strong>的最佳方式。一个<em class="lq"> m </em> × <em class="lq"> n </em>矩阵<strong class="kw iu"> A </strong>的<a class="ae lr" href="http://mathworld.wolfram.com/FrobeniusNorm.html" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> Frobenius范数</em> </a>定义为其元素的绝对平方和的平方根:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/7bb4fb93183ac27bcca78788465860a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*UBWqe8hWWNVwi8J4uDes_A@2x.png"/></div></figure><p id="d50a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就像矩阵向量长度的推广。现在如果矩阵<em class="lq">m</em>×<em class="lq">n</em>Ak是由奇异值分解逼近的秩- <em class="lq"> k </em>矩阵，我们可以认为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/6311771ac1eab272f713f275e5ba0acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*KDaBDcbdQu6i0PukF5UlGw@2x.png"/></div></figure><p id="a7d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为<strong class="kw iu"> A </strong>与<strong class="kw iu"> Ak </strong>之间的距离。这个距离越小，<strong class="kw iu"> Ak </strong>越接近<strong class="kw iu"> A </strong>越好。现在如果<strong class="kw iu"> B </strong>是任意一个<em class="lq"> m </em> × <em class="lq"> n </em>秩- <em class="lq"> k </em>矩阵，可以看出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi rb"><img src="../Images/f0fd16f061edced99250d61680c5f14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*6RtVCH_dgkUtyjmnvJX0Yg@2x.png"/></div></figure><p id="a400" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">换句话说，SVD生成的<strong class="kw iu"> A </strong>与其秩- <em class="lq"> k </em>逼近之差具有最小的Frobenius范数，没有其他秩- <em class="lq"> k </em>矩阵能够给出对<strong class="kw iu"> A </strong>更好的逼近(就Frobenius范数而言距离更近)<strong class="kw iu">。</strong></p><p id="3245" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经熟悉了SVD，我们可以看到它在数据科学中的一些应用。</p><h1 id="bf4d" class="rc mv it bd rd re rf rg rh ri rj rk rl jz rm ka rn kc ro kd rp kf rq kg rr rs bi translated">应用程序</h1><p id="0ef4" class="pw-post-body-paragraph ku kv it kw b kx rt ju kz la ru jx lc ld rv lf lg lh rw lj lk ll rx ln lo lp im bi translated"><strong class="kw iu">降维</strong></p><p id="fd96" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以在矩阵中存储图像。每个图像由一组像素组成，这些像素是该图像的构建块。每个像素代表图像中特定位置的光的颜色或强度。在<a class="ae lr" href="https://fileinfo.com/extension/png" rel="noopener ugc nofollow" target="_blank"> PNG格式</a>的灰度图像中，每个像素都有一个介于0和1之间的值，其中0对应黑色，1对应白色。所以一个有<em class="lq"> m </em> × <em class="lq"> n </em>个像素的灰度图像可以存储在一个<em class="lq"> m </em> × <em class="lq"> n </em>矩阵或者NumPy数组中。<em class="lq"> </em>这里我们使用<code class="fe ly lz ma mb b">imread()</code>函数将480 × 423像素的爱因斯坦的灰度图像加载到一个二维数组中。然后我们用SVD分解矩阵，用前30个奇异值重构。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ry"><img src="../Images/c72cb0455cee245a5edcb08ef09afa55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LIDW2EVLFx4PA0mozo641A.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图21。<a class="ae lr" href="https://pixabay.com/photos/albert-einstein-portrait-1933340/" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="0d32" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">原始矩阵为480×423。所以我们需要存储480×423=203040个值。在SVD之后，每个<strong class="kw iu"> ui </strong>有480个元素，每个<strong class="kw iu"> vi </strong>有423个元素。为了能够使用前30个奇异值重建图像，我们只需要保留前30个σi、<strong class="kw iu"> ui </strong>和<strong class="kw iu"> vi </strong>，这意味着存储30×(1+480+423)=27120个值。这大约是原始图像所需数值的13%。因此，使用奇异值分解，我们可以很好地逼近原始图像，并节省大量内存。清单16，并计算对应于前6个奇异值的矩阵。每个矩阵<em class="lq">σI</em><strong class="kw iu">ui</strong><em class="lq"/><strong class="kw iu">VI</strong><strong class="kw iu">^t</strong>的秩为1，行数和列数与原始矩阵相同。图22显示了结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rz"><img src="../Images/c0db98645bd5b90d89325ecd84a2d91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_u3Myo1RUG9ffOb_-p4lOQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图22</p></figure><p id="7853" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，与原始灰度图像不同，这些秩1矩阵的元素的值可以大于1或小于0，并且它们不应被解释为灰度图像。所以我没有使用<code class="fe ly lz ma mb b">cmap='gray'</code>，也没有将它们显示为灰度图像。当绘制它们时，我们不关心像素的绝对值。相反，我们关心他们相对于彼此的价值。</p><p id="b5e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了理解图像信息是如何存储在每个矩阵中的，我们可以研究一个简单得多的图像。在清单17中，我们读取了一个包含五个简单形状的二进制图像:一个矩形和四个圆形。结果如图23所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sa"><img src="../Images/29aa91833560d57b2bc46d0ffff46292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a63vqqKoRJBzljadSSsumQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图23</p></figure><p id="fc54" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用前2、4和6个奇异值重建图像。现在我们绘制对应于前6个奇异值的矩阵:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sa"><img src="../Images/0af0dc82b41f3a7cb4494af01f1b4444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Av_OghzlRo6rSrwuBpqggg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图24</p></figure><p id="3c78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个矩阵(<em class="lq">σI</em><strong class="kw iu">ui</strong><em class="lq"/><strong class="kw iu">VI</strong><strong class="kw iu">^t</strong>)的秩为1，这意味着它只有一个独立列，所有其他列都是该列的标量乘法。所以如果调用独立列<strong class="kw iu"> c1 </strong>(或者它可以是其他任何一列)，这些列的一般形式为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sb"><img src="../Images/63999518fd4b28c9aae0561ef9db1230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ElD6vkWgq67aRJM-cKhaKw@2x.png"/></div></div></figure><p id="b8ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="lq"> ai </em>是标量乘法器。另外，这个矩阵把所有的向量都投影到<strong class="kw iu"> ui </strong>上，所以每一列也是<strong class="kw iu"> ui </strong>的标量乘法。这可以在图25中看到。显示了矩阵的两列<em class="lq">σ2</em>t39】U2<em class="lq"/><strong class="kw iu">v2^t</strong>与<strong class="kw iu"> u2 </strong>。两列具有相同的<strong class="kw iu"> u2 </strong>模式，但值不同(列#300的<em class="lq"> ai </em>为负值)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sc"><img src="../Images/000b0fa4b40e180eba25e21eed60271f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBIU4UsCk7f86X_qwUy4HA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图25</p></figure><p id="ee4c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以使用<strong class="kw iu"> c1 </strong>和<em class="lq"> ai </em>(或<strong class="kw iu"> u2 </strong>及其乘数)的值，每个矩阵捕捉原始图像的一些细节。在图24中，前两个矩阵可以捕获原始图像中左矩形的几乎所有信息。在图24的前两个矩阵中，这四个圆被粗略地捕捉为四个矩形，关于它们的更多细节被添加到后四个矩阵中。这也可以在图23中看到，当我们添加更多的奇异值时，重建图像中的圆变得更圆。这些秩为1的矩阵可能看起来很简单，但它们能够捕捉图像中重复模式的一些信息。例如，在图26中，我们有苏格兰国家纪念碑的图像，它有6根柱子(在图像中)，对应于第一个奇异值的矩阵可以捕捉原始图像中柱子的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sd"><img src="../Images/fa255a55978e8a5215d7513a0b78d7e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fe7Tzli7Sv55gldatM9khw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图26。<a class="ae lr" href="https://pixabay.com/photos/national-monument-of-scotland-1252932/" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="d234" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征脸</strong></p><p id="ee00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本例中，我们将使用Scikit-learn库中的Olivetti faces数据集。该数据集包含400幅图像。这些照片拍摄于1992年4月至1994年4月间，地点是剑桥美国电话电报公司实验室。这些图像显示了40个不同主体的面部。对于一些受试者来说，这些照片是在不同的时间拍摄的，改变了光线、面部表情和面部细节。这些图像是灰度图像，每幅图像的像素为64×64。每个像素的强度是区间[0，1]上的一个数。首先，我们加载数据集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="f6c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">清单1中已经导入了<code class="fe ly lz ma mb b">fetch_olivetti_faces()</code>函数。我们称之为读取数据并将图像存储在<code class="fe ly lz ma mb b">imgs</code>数组中。这是一个(400，64，64)数组，包含400个灰度64×64的图像。我们可以在这里展示其中的一些例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi se"><img src="../Images/d09173e5df36f1200f7e3bbbbaba8064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9y3yIPAj47EUi0ZTBVQP8Q.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图27</p></figure><p id="35f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在前面的例子中，我们将原始图像存储在一个矩阵中，然后使用SVD对其进行分解。这里我们采用另一种方法。我们知道我们有400张图片，所以我们给每张图片一个从1到400的标签。现在我们使用<a class="ae lr" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">一键编码</a>通过一个向量来表示这些标签。我们使用一个有400个元素的列向量。对于每个标签<em class="lq"> k，</em>除了第<em class="lq"> k </em>个元素外，所有元素都为零。因此标签<em class="lq"> k </em>将由向量表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sf"><img src="../Images/9b9696d343848611ed56bf17436c12a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*X73SSOjGpfbYuIs_73DOZg@2x.png"/></div></figure><p id="38eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将每个图像存储在一个列向量中。每个图像有64 × 64 = 4096个像素。因此，我们可以展平每个图像，并将像素值放入具有4096个元素的列向量<strong class="kw iu"> f </strong>中，如图28所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sg"><img src="../Images/73722be5198a78d21f3a46ded68d1861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zz0OcBoh-PTxup0WDoZ3dw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图28</p></figure><p id="6c55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以每张标签为<em class="lq"> k </em>的图片都会被存储在矢量<strong class="kw iu"> fk </strong>中，我们需要400个<strong class="kw iu"> fk </strong>矢量来保存所有的图片。现在我们定义一个变换矩阵<strong class="kw iu"> M </strong>，它将标签向量<strong class="kw iu"> ik </strong>变换为其对应的图像向量<strong class="kw iu"> fk </strong>。向量<strong class="kw iu"> fk </strong>将是矩阵<strong class="kw iu"> M </strong>的列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sh"><img src="../Images/f7d431ecb8ae447980b28e3eb916eb6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-k5n1TsyzcztBBV7g9_oA.png"/></div></div></figure><p id="2778" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个矩阵有4096行和400列。我们可以简单的用<strong class="kw iu"> y=Mx </strong>找到每个标签对应的图像(<strong class="kw iu"> x </strong>可以是任意向量<strong class="kw iu"> ik </strong>，<strong class="kw iu"> y </strong>会是对应的<strong class="kw iu"> fk </strong>)。例如，对于该数据集的第三幅图像，标签是3，并且除了第三个元素是1之外，<strong class="kw iu"> i3 </strong>的所有元素都是0。现在，记住分块矩阵的乘法。当我们将<strong class="kw iu"> M </strong>乘以<strong class="kw iu"> i3 </strong>时，除了第三列<strong class="kw iu"> f3 </strong>、<strong class="kw iu"> </strong>之外，<strong class="kw iu"> M </strong>的所有列都乘以零，所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi si"><img src="../Images/264222e1b24e4a394f31e64236d9462b.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*TSW1Xj8UWUP0jf-yX2IBAw@2x.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sj"><img src="../Images/7074b10f06a8adb7c5c5c15128813eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLxzJPc_XymvMr3fMGUwdQ.png"/></div></div></figure><p id="302e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">清单21展示了我们如何构建<strong class="kw iu"> M </strong>并使用它来显示数据集中的某个图像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="11df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个标签向量<strong class="kw iu"> ik </strong>的长度是1，并且这些标签向量形成400维空间的标准基础。在这个空间中，每个轴对应于一个标签，其值可以是零或一。向量<strong class="kw iu"> fk </strong>存在于4096维空间中，其中每个轴对应图像的一个像素，矩阵<strong class="kw iu"> M </strong>将<strong class="kw iu"> ik </strong>映射到<strong class="kw iu"> fk </strong>。现在我们可以用SVD分解<strong class="kw iu"> M </strong>。请记住，当我们将M<strong class="kw iu">分解为r 时</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sk"><img src="../Images/81fc9830e42830fe1130b0a71797636f.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*_8J22PLHwMFag318D197Sg@2x.png"/></div></figure><p id="15e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">集合{ <strong class="kw iu"> u1 </strong>、<strong class="kw iu"> u2 </strong>、…、<strong class="kw iu"> ur </strong> }是<strong class="kw iu"> U </strong>的第一个<em class="lq"> r </em>列，将作为<strong class="kw iu"> Mx </strong>的基础。每个向量<strong class="kw iu"> ui </strong>将有4096个元素。由于<strong class="kw iu"> y </strong> = <strong class="kw iu"> Mx </strong>是我们的图像向量所在的空间，向量<strong class="kw iu"> ui </strong>形成了图像向量的基础，如图29所示。在这幅图中，我试图想象一个n维向量空间。这在<em class="lq"> n </em> ≥3时当然是不可能的，但这只是一个虚构的图解，帮助你理解这个方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sl"><img src="../Images/9090800399b24f11f0c336c24d0cf9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgS7S16nGNOaPVM9bXZemg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图29</p></figure><p id="3162" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们可以将<strong class="kw iu"> ui </strong>重塑成一个64 ×64的像素阵列，并尝试像绘制图像一样绘制它。这些向量的元素值可以大于1或小于0，并且当对它们进行整形时，它们不应该被解释为灰度图像。所以我在显示它们的时候没有使用<code class="fe ly lz ma mb b">cmap='gray'</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="6f18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sm"><img src="../Images/507397c47dfe81a926e374fbe88ead2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRnYlP6t38h16Zbz8udfiQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图30</p></figure><p id="f6e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以检查清单22中的数组<code class="fe ly lz ma mb b">s</code>有400个元素，所以我们有400个非零奇异值，矩阵的秩是400。因此，我们需要<strong class="kw iu"> U </strong>的前400个向量来完全重构矩阵。我们可以使用基本向量轻松地重建其中一幅图像:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="953b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，我们采用图像#160，并使用不同数量的奇异值对其进行重建:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi se"><img src="../Images/87f33d051672318af90e8f4865941367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hgi2edVTFDkuF6D-z6PS8g.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图31</p></figure><p id="6a75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">向量<strong class="kw iu"> ui </strong>被称为<em class="lq">特征脸</em>，可以用于人脸识别。正如您在图30中看到的，每个特征脸捕获了图像向量的一些信息。比如<strong class="kw iu"> u1 </strong>大部分是关于眼睛的，或者<strong class="kw iu"> u6 </strong>抓住了鼻子的一部分。当重建图31中的图像时，第一个奇异值添加了眼睛，但是脸部的其余部分是模糊的。通过增加<em class="lq"> k，</em>鼻子、眉毛、胡须和眼镜被添加到脸上。有些人认为眼睛是你面部最重要的特征。似乎SVD同意他们的观点，因为具有最高奇异值的第一个特征脸捕获了眼睛。</p><p id="dd8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">降低噪音</strong></p><p id="2db0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">奇异值分解可以用来降低图像中的噪声。清单24显示了一个例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sn"><img src="../Images/408a12de4202dfd7b65446c3aad0bbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpcSxFY7NChq67woWZbVnA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图32</p></figure><p id="9e6d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，我们首先加载图像，并添加一些噪声。然后，我们使用前20、55和200个奇异值来重建图像。如图32所示，随着重构矩阵的秩增加，噪声量也会增加。因此，如果我们使用像20这样的较低等级，我们可以显著降低图像中的噪声。重要的是要理解为什么它在低级别工作得更好。</p><p id="8d0f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有一个简单的例子来说明SVD如何降低噪声。假设我们有清单25中定义的3×15矩阵:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="327c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该矩阵的颜色图如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi so"><img src="../Images/aab6cfc228b5e1891e524c5f73b977b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKsDkTT8y3GiqncNONu2oA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图33</p></figure><p id="faa8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">矩阵列可以分为两类。在前5列中，只有第一个元素不为零，在后10列中，只有第一个元素为零。我们还有一个嘈杂的列(列#12 ),它应该属于第二类，但是它的第一个和最后一个元素没有正确的值。我们可以假设这两个元素包含一些噪声。现在我们用奇异值分解这个矩阵。矩阵的秩为3，并且只有3个非零奇异值。现在我们用前两个和三个奇异值来重建它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sp"><img src="../Images/aec8ebbac9d99cc008bbcfbaa191f5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7OXjaHv8es7ySJL-3Pn1yA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图34</p></figure><p id="9939" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如图34所示，通过使用前两个奇异值，列#12发生变化，并遵循第二类中列的相同模式。但是，现在它的元素的实际值要低一点。如果我们使用所有3个奇异值，我们得到原始的噪声列。图35显示了这些列在三维空间中的曲线图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sq"><img src="../Images/1cc34731749a75e00d391e7d45824acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8SKl0NUzq9UtsvLma5vTg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图35</p></figure><p id="69d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">先看SVD生成的<strong class="kw iu"> ui </strong>向量。<strong class="kw iu"> u1 </strong>显示第一类列向量的平均方向。类似地，<strong class="kw iu"> u2 </strong>显示了第二类的平均方向。当然方向相反，但这不重要(记住如果<strong class="kw iu"> vi </strong>是某个特征值的特征向量，那么(-1) <strong class="kw iu"> vi </strong>也是同一特征值的特征向量，并且由于<strong class="kw iu">ui</strong>=<strong class="kw iu">Avi/</strong><em class="lq">σI</em>，那么它的符号取决于<strong class="kw iu"> vi </strong> ) <em class="lq">。</em>重要的是拉伸方向，而不是矢量的符号。</p><p id="a7e2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">噪声列由矢量<strong class="kw iu"> n </strong>表示。不沿着<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>走。现在如果我们以<strong class="kw iu"> ui </strong>为基础，就可以分解<strong class="kw iu"> n </strong>，找到它在<strong class="kw iu"> ui </strong>上的正交投影。如你所见，它有一个沿<strong class="kw iu"> u3 </strong>(反方向)<strong class="kw iu"> </strong>的分量，这是噪声方向。<strong class="kw iu"> </strong>这个方向代表出现在<strong class="kw iu"> n </strong>的第三个元素中的噪声。它具有最低的奇异值，这意味着SVD不认为它是一个重要的特征。当我们使用前两个奇异值重构<strong class="kw iu"> n </strong>时，我们忽略这个方向，第三个元素中存在的噪声被消除。现在我们只有沿着<strong class="kw iu"> u1 </strong>和<strong class="kw iu"> u2 </strong>的矢量投影。但是沿着<strong class="kw iu"> u1 </strong>的标量投影具有高得多的值。这是因为矢量<strong class="kw iu"> n </strong>更类似于第一类。</p><p id="df89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以<strong class="kw iu"> n </strong>在<strong class="kw iu"> u1-u2 </strong>平面的投影几乎是沿着<strong class="kw iu"> u1 </strong>的，利用前两个奇异值重构<strong class="kw iu"> n </strong>给出了一个更类似于第一类的向量。重要的是要注意，由<strong class="kw iu"> u2 </strong>表示的第一元件中的噪声没有被消除。此外，尽管重构的<strong class="kw iu"> n </strong>的方向几乎是正确的，但是与第一类中的向量相比，其幅度较小。事实上，在重构的矢量中，第二个元素(不包含噪声)现在具有比原始矢量更低的值(图36)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sr"><img src="../Images/b8c01cd64e9e5c668b5fecb32a81bcc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1q9WgoytpsInqRkehk9sg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图36</p></figure><p id="eece" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，SVD将大部分噪声(但不是全部)分配给由较低奇异值表示的向量。如果我们重构一个低秩矩阵(忽略较低的奇异值)，噪声将会减少，然而，矩阵的正确部分也会改变。结果是一个矩阵，它只是我们正在寻找的无噪声矩阵的近似。这可以在图32中看到。图像背景是白色的，噪声像素是黑色的。当我们重建低秩图像时，背景更加均匀，但现在是灰色的。事实上，如果图像中没有噪声，我们得到的是白色背景的一个噪声较少的近似。</p><p id="d003" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望你喜欢阅读这篇文章。如果您有任何问题或建议，请告诉我。本文中的所有代码清单都可以从GitHub下载，网址是:<a class="ae lr" href="https://github.com/reza-bagheri/SVD_article" rel="noopener ugc nofollow" target="_blank">https://github.com/reza-bagheri/SVD_article</a></p><p id="0283" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">延伸阅读:</strong></p><p id="81bc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征分解和SVD也可以用于主成分分析(PCA)。PCA对于降维非常有用。要了解更多关于PCA中特征分解和SVD的应用，您可以阅读以下文章:</p><p id="8727" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lr" href="https://reza-bagheri79.medium.com/understanding-principal-component-analysis-and-its-application-in-data-science-part-1-54481cd0ad01" rel="noopener">https://Reza-bag heri 79 . medium . com/understanding-principal-component-analysis-and-its-application-in-data-science-part-1-54481 CD 0ad 01</a></p><p id="e61b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lr" href="https://reza-bagheri79.medium.com/understanding-principal-component-analysis-and-its-application-in-data-science-part-2-e16b1b225620" rel="noopener">https://Reza-bag heri 79 . medium . com/understanding-principal-component-analysis-and-its-application-in-data-science-part-2-e16b 1b 225620</a></p></div></div>    
</body>
</html>