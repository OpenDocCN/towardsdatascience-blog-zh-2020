<html>
<head>
<title>Fake Job Classification with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特的假工作分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fake-job-classification-with-bert-6575c908a9aa?source=collection_archive---------29-----------------------#2020-03-25">https://towardsdatascience.com/fake-job-classification-with-bert-6575c908a9aa?source=collection_archive---------29-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a87" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中的文本分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0509521f9abf8eb4b3fee77f569eb932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QnzPgLhQN0ImVdggTwR1Tg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.pexels.com/photo/adult-blur-boss-business-288477/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="02d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">近日，爱琴海大学公布了<a class="ae ky" href="https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction" rel="noopener ugc nofollow" target="_blank"> <em class="lv">就业骗局爱琴海数据集</em> </a>。该数据包含大约18K个现实生活中的招聘广告。其目的是为研究界提供一个清晰的雇佣诈骗问题的图景。在本帖中，我们将使用BERT对<a class="ae ky" href="https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction" rel="noopener ugc nofollow" target="_blank"> <em class="lv">就业骗局爱琴海数据集</em> </a>中的虚假职位描述进行分类。</p><p id="8a72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们开始之前，让我们简单回顾一下BERT方法。</p><p id="3108" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT代表来自变压器的双向编码器表示。描述BERT算法的论文由Google发布，可以在这里找到<a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"/>。BERT的工作原理是随机屏蔽单词标记，并用基于上下文的向量来表示每个被屏蔽的单词。BERT的两个应用是“预训练”和“微调”。</p><p id="0af6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">预训练BERT </strong></p><p id="0b54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于预训练BERT算法，研究人员训练了两个无监督学习任务。第一个任务被描述为屏蔽LM。其工作原理是随机屏蔽15%的文档，并预测这些被屏蔽的标记。第二个任务是下一句预测(NSP)。这是由问题回答和自然语言推理等任务激发的。这些任务需要模型来准确捕捉句子之间的关系。为了解决这个问题，他们对二进制预测任务进行了预训练，该任务可以从单一语言的任何语料库中轻松生成。他们在论文中给出的例子如下:如果你有句子A和B，A有50%的时间被标注为“isNext”，另外50%的时间是从语料库中随机选取的句子，被标注为“notNext”。针对这一任务的预训练被证明对于问题回答和自然语言推理任务是有益的。</p><p id="f28f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">微调伯特</strong></p><p id="bd15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微调BERT的工作原理是用自我关注对连接在一起的文本对进行编码。自我注意是学习当前单词和先前单词之间相关性的过程。这一点的一个早期应用是在长短期记忆(<a class="ae ky" href="https://arxiv.org/pdf/1601.06733.pdf" rel="noopener ugc nofollow" target="_blank"/>)论文(Dong2016)中，研究人员利用自我注意进行机器阅读。BERT的好处在于，通过对具有自我关注的串联文本进行编码，可以捕捉句子对之间的双向交叉关注。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/4f2955670d1bc323c1420711b5b57eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JuHrhr2kyoYwnZK6V1luQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="673a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将应用BERT来预测一个职位发布是否是欺诈性的。这篇文章的灵感来自于<a class="ae ky" rel="noopener" target="_blank" href="/bert-to-the-rescue-17671379687f"> <em class="lv"> BERT to the Rescue </em> </a>，它使用BERT对IMDB数据集进行情感分类。从<a class="ae ky" rel="noopener" target="_blank" href="/bert-to-the-rescue-17671379687f"> <em class="lv">伯特到营救</em> </a>的代码可以在<a class="ae ky" href="https://github.com/shudima/notebooks/blob/master/BERT_to_the_rescue.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="fd9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们对单句分类感兴趣，相关的架构是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/99abc849ade8034da02c32892dad0c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*yoaLZYOSxZmI3o9KcNc32A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，BERT算法的输入是单词序列，输出是编码的单词表示(向量)。对于单句分类，我们使用每个单词的向量表示作为分类模型的输入。</p><p id="914f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们开始吧！</p><ol class=""><li id="1965" class="ly lz it lb b lc ld lf lg li ma lm mb lq mc lu md me mf mg bi translated"><strong class="lb iu">导入包</strong></li></ol><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="8448" class="mm mn it mi b gy mo mp l mq mr">import pandas as pd <br/>import numpy as np <br/>import torch.nn as nn<br/>from pytorch_pretrained_bert import BertTokenizer, BertModel<br/>import torch<br/>from keras.preprocessing.sequence import pad_sequences<br/>from sklearn.metrics import classification_report</span></pre><p id="03cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu">数据探索</strong></p><p id="5f84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们将数据读入数据框并打印前五行。我们还可以将最大显示列数设置为“无”:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="98f8" class="mm mn it mi b gy mo mp l mq mr">pd.set_option('display.max_columns', None)<br/>df = pd.read_csv("fake_job_postings.csv")<br/>print(df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fdbe6895f955e2793fd1462d44a068be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rv7CgdED3xGgl62j6VNT3A.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/b72f6116c25a73314951a30f00ea9296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J57oqMoKML8JsPzgNlmzBg.png"/></div></div></figure><p id="5204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为简单起见，让我们看看“描述”和“欺诈”栏:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="031c" class="mm mn it mi b gy mo mp l mq mr">df = df[['description', 'fraudulent']]<br/>print(df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/5b7ab84a5f75c9765db32bd4aabc8ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIMmSzzZ9ITODzOsC99Q6A.png"/></div></div></figure><p id="eb8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的分类模型的目标在“欺诈”列中。为了了解“欺诈”值的分布和种类，我们可以使用集合模块中的“计数器”:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="c75a" class="mm mn it mi b gy mo mp l mq mr">from collections import Counter<br/>print(Counter(df['fraudulent'].values))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/0e0d84852c3a44abc22504840d838126.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*ZdAWyH3n-_IsD8CIVaUiug.png"/></div></figure><p id="cc3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“0”值对应于正常的工作发布,“1”值对应于欺诈性发布。我们看到数据略有不平衡，这意味着正常的职位发布(17K)比欺诈性发布(866)多。</p><p id="28b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在继续之前，让我们删除“NaN”值:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="fd09" class="mm mn it mi b gy mo mp l mq mr">df.dropna(inplace = True)</span></pre><p id="b02c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们希望平衡我们的数据集，使“欺诈”和“非欺诈”类型的数量相等。我们还应该随机改变目标:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="5385" class="mm mn it mi b gy mo mp l mq mr">df_fraudulent= df[df['fraudulent'] == 1] <br/>df_normal = df[df['fraudulent'] == 0] <br/>df_normal = df_normal.sample(n=len(df_fraudulent))<br/>df = df_normal.append(df_fraudulent)<br/>df = df.sample(frac=1, random_state = 24).reset_index(drop=True)</span></pre><p id="afd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再次验证我们得到了想要的结果:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="0d1a" class="mm mn it mi b gy mo mp l mq mr">print(Counter(df['fraudulent'].values))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/eed18d78350f51e58a6ecf7ba34c7c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*YknLQFhPW90Zny5_vZfwtA.png"/></div></figure><p id="3569" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们想要格式化数据，以便它可以用作我们的BERT模型的输入。我们将数据分为训练集和测试集:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="447c" class="mm mn it mi b gy mo mp l mq mr">train_data = df.head(866)<br/>test_data = df.tail(866)</span></pre><p id="baef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们生成一个包含“描述”和“欺诈”关键字的字典列表:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="51f8" class="mm mn it mi b gy mo mp l mq mr">train_data = [{'description': description, 'fraudulent': fraudulent } for description in list(train_data['description']) for fraudulent in list(train_data['fraudulent'])]</span><span id="62b1" class="mm mn it mi b gy mx mp l mq mr">test_data = [{'description': description, 'fraudulent': fraudulent } for description in list(test_data['description']) for fraudulent in list(test_data['fraudulent'])]</span></pre><p id="99f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从字典列表中生成元组列表:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="670f" class="mm mn it mi b gy mo mp l mq mr">train_texts, train_labels = list(zip(*map(lambda d: (d['description'], d['fraudulent']), train_data)))<br/>test_texts, test_labels = list(zip(*map(lambda d: (d['description'], d['fraudulent']), test_data)))</span></pre><p id="1014" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成令牌和令牌id:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="5b27" class="mm mn it mi b gy mo mp l mq mr">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)<br/>train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))<br/>test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))</span><span id="6ade" class="mm mn it mi b gy mx mp l mq mr">train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))<br/>test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))</span><span id="f0b4" class="mm mn it mi b gy mx mp l mq mr">train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating="post", padding="post", dtype="int")<br/>test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating="post", padding="post", dtype="int")</span></pre><p id="053a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们将输入字符串截断为512个字符，因为这是BERT可以处理的最大令牌数。</p><p id="28c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，为我们的测试和训练集生成一个基于“欺诈”值的布尔数组:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="6ec3" class="mm mn it mi b gy mo mp l mq mr">train_y = np.array(train_labels) == 1<br/>test_y = np.array(test_labels) == 1</span></pre><p id="b258" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.<strong class="lb iu">模型构建</strong></p><p id="4e1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们创建了我们的BERT分类器，它包含一个“初始化”方法和一个返回令牌概率的“转发”方法:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="87d3" class="mm mn it mi b gy mo mp l mq mr">class BertBinaryClassifier(nn.Module):<br/>    def __init__(self, dropout=0.1):<br/>        super(BertBinaryClassifier, self).__init__()<br/>        self.bert = BertModel.from_pretrained('bert-base-uncased')<br/>        self.dropout = nn.Dropout(dropout)<br/>        self.linear = nn.Linear(768, 1)<br/>        self.sigmoid = nn.Sigmoid()<br/>    <br/>    def forward(self, tokens, masks=None):<br/>        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)<br/>        dropout_output = self.dropout(pooled_output)<br/>        linear_output = self.linear(dropout_output)<br/>        proba = self.sigmoid(linear_output)<br/>        return proba</span></pre><p id="720c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们生成训练和测试掩码:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="e4a5" class="mm mn it mi b gy mo mp l mq mr">train_masks = [[float(i &gt; 0) for i in ii] for ii in train_tokens_ids]<br/>test_masks = [[float(i &gt; 0) for i in ii] for ii in test_tokens_ids]<br/>train_masks_tensor = torch.tensor(train_masks)<br/>test_masks_tensor = torch.tensor(test_masks)</span></pre><p id="9dd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成用于训练和测试的令牌张量:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="5110" class="mm mn it mi b gy mo mp l mq mr">train_tokens_tensor = torch.tensor(train_tokens_ids)<br/>train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()<br/>test_tokens_tensor = torch.tensor(test_tokens_ids)<br/>test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()</span></pre><p id="d129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，准备我们的数据加载器:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="4282" class="mm mn it mi b gy mo mp l mq mr">BATCH_SIZE = 1<br/>train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)<br/>train_sampler =  torch.utils.data.RandomSampler(train_dataset)<br/>train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)</span><span id="5b90" class="mm mn it mi b gy mx mp l mq mr">test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)<br/>test_sampler =  torch.utils.data.SequentialSampler(test_dataset)<br/>test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)</span></pre><p id="4728" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.<strong class="lb iu">微调</strong></p><p id="4ca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用Adam优化器来最小化二进制交叉熵损失，并且我们使用1个时期的批量大小1来训练:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="c2d3" class="mm mn it mi b gy mo mp l mq mr">BATCH_SIZE = 1<br/>EPOCHS = 1</span><span id="7e33" class="mm mn it mi b gy mx mp l mq mr">bert_clf = BertBinaryClassifier()<br/>optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)</span><span id="ec69" class="mm mn it mi b gy mx mp l mq mr">for epoch_num in range(EPOCHS):<br/>    bert_clf.train()<br/>    train_loss = 0<br/>    for step_num, batch_data in enumerate(train_dataloader):<br/>        token_ids, masks, labels = tuple(t for t in batch_data)<br/>        probas = bert_clf(token_ids, masks)<br/>        loss_func = nn.BCELoss()<br/>        batch_loss = loss_func(probas, labels)<br/>        train_loss += batch_loss.item()<br/>        bert_clf.zero_grad()<br/>        batch_loss.backward()<br/>        optimizer.step()<br/>        print('Epoch: ', epoch_num + 1)<br/>        print("\r" + "{0}/{1} loss: {2} ".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))</span></pre><p id="c859" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们评估我们的模型:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="93ae" class="mm mn it mi b gy mo mp l mq mr">bert_clf.eval()<br/>bert_predicted = []<br/>all_logits = []<br/>with torch.no_grad():<br/>    for step_num, batch_data in enumerate(test_dataloader):</span><span id="3c12" class="mm mn it mi b gy mx mp l mq mr">token_ids, masks, labels = tuple(t for t in batch_data)</span><span id="ecb7" class="mm mn it mi b gy mx mp l mq mr">logits = bert_clf(token_ids, masks)<br/>        loss_func = nn.BCELoss()<br/>        loss = loss_func(logits, labels)<br/>        numpy_logits = logits.cpu().detach().numpy()<br/>        <br/>        bert_predicted += list(numpy_logits[:, 0] &gt; 0.5)<br/>        all_logits += list(numpy_logits[:, 0])<br/>        <br/>print(classification_report(test_y, bert_predicted))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6686f27b4f05451f23de49aaf4206ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*1Ilq3RLIqCIjdYnE4-JUiQ.png"/></div></figure><p id="e12c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型在预测真实帖子方面做得不错。预测欺诈性帖子的性能没有那么好，但可以通过增加历元数和进一步的特征工程来提高。我鼓励您尝试超参数调整和训练数据，看看是否可以提高分类性能。</p><p id="042e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，我们建立了一个BERT分类器来预测招聘信息是真实的还是虚假的。如果对BERT的其他应用感兴趣，可以阅读<a class="ae ky" rel="noopener" target="_blank" href="/fake-news-classification-with-bert-afbeee601f41"> <em class="lv">假新闻分类搭配BERT </em> </a>和<a class="ae ky" rel="noopener" target="_blank" href="/russian-troll-tweets-classification-using-bert-abec09e43558"> <em class="lv">俄罗斯巨魔推文:分类搭配BERT </em> </a>。如果你对伯特方法有兴趣，我鼓励你阅读<a class="ae ky" rel="noopener" target="_blank" href="/bert-to-the-rescue-17671379687f"> <em class="lv">伯特拯救</em> </a> <em class="lv">。</em>这篇文章的代码可以在<a class="ae ky" href="https://github.com/spierre91/medium_code/tree/master/BERT_fake_job_classification" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。感谢您的阅读！</p></div></div>    
</body>
</html>