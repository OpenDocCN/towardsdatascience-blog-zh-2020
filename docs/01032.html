<html>
<head>
<title>Deep Learning with Spiking Networks: Optimising Energy Consumption</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用尖峰网络的深度学习:优化能耗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-with-spiking-networks-optimising-energy-consumption-50588b4435fd?source=collection_archive---------28-----------------------#2020-01-29">https://towardsdatascience.com/deep-learning-with-spiking-networks-optimising-energy-consumption-50588b4435fd?source=collection_archive---------28-----------------------#2020-01-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ff02" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经形态硬件提供了通过尖峰进行通信的超低功耗神经网络，有点像真正的神经元。但是我们如何训练他们呢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/646715a3c3665a1ff602de3759a0d4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfFVkqlme7mqJvfySyfXcg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经形态芯片的特写。照片 Ole Richter/aiCTX AG</p></figure><p id="958a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在如今被称为“神经网络”(在深度学习的意义上)和大脑中的神经元网络之间有几个重要的区别。一个特别明显的例子是，人工网络具有模拟激活——人工神经元的输出是一个连续的数字。相反，生物神经元通过称为动作电位或<em class="lu">尖峰的离散、全有或全无的电事件进行交流。</em>当然，我们也可以模拟和使用尖峰网络(SNNs)。</p><p id="c9c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可能想训练一个尖峰神经网络有两个原因:也许，你正试图理解大脑计算或学习的某些方面；或者，您可能希望使用脉冲网络来执行机器学习任务，就像您使用卷积网络(CNN)等“常规”神经网络一样。</p><p id="8c52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你为什么要做这种事？如何训练一个尖峰网络肯定不如深度学习中使用的反向传播和梯度下降的过程那样为人所知。尖峰网络不会优于 CNN，并且训练和模拟会困难得多。然而，人们有兴趣这样做。这是因为与传统计算机相比，脉冲网络可以在极低功耗的神经形态硬件上运行<em class="lu">。这是因为神经形态芯片是专门为低功率推理而构建的，并在其电路中复制一个尖峰神经元的动态，而不是在计算机上模拟它。其次，尖峰网络以基于事件的方式进行计算，这使得它们成为处理基于事件的数据的理想工具，例如<a class="ae lv" href="https://inivation.com/dvs/" rel="noopener ugc nofollow" target="_blank">动态视觉传感器</a>的输出。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/f48dd972c79e225d0b9cb2ddeb492883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*bNb5o8D9PAD5dn496YGqTQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DVS 录音示例。图像由记录了位置和时间的事件(类似于尖峰)组成。没有帧率。当场景中没有变化时，就没有事件(因此背景大部分是不可见的)。(自己的工作)</p></figure><p id="027b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经形态工程是一个不断发展的领域，开发有用的脉冲网络模型是一个相应的有趣的挑战。可以使用“替代梯度”技巧通过梯度下降来训练脉冲网络，避免信号离散性的问题，但这仍然复杂而缓慢[1，4]。当我们对生物合理性不感兴趣，也对时间相关方面不太感兴趣时，事实证明训练传统(模拟)CNN 更容易，然后在尖峰网络上使用相同的权重[2]。你可以想象，如果放电频率足够高，在给定的时间间隔内接收到的棘波数量可以很好地近似一个连续的数字:CNN 中相应神经元的激活值。这就是计算神经科学家所说的<em class="lu">速率编码</em>。</p><h1 id="b7a8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">优化能源消耗的费率代码</h1><p id="c0ae" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">我们试图以稍微聪明一点的方式使用这种蛮力方法。如我所说，在速率编码中，每个神经元都试图用特定的尖峰频率来代表激活。这个频率应该和激活成正比，但是应该有多大呢？1.42 这个数字应该用每秒多少个尖峰来表示？有两种风险:</p><ul class=""><li id="db2b" class="mu mv it la b lb lc le lf lh mw ll mx lp my lt mz na nb nc bi translated">如果尖峰信号的数量<em class="lu">太低</em>，我们最终会离散化激活，我们将无法安全地分辨出，比如说，1.42 和 1.35，因为它们都由相同数量的尖峰信号表示。</li><li id="0b94" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">如果尖峰信号太多，我们将会消耗更多的能量。当网络静默时，神经形态硬件几乎不使用任何功率，但每当每个下游神经元接收到一个尖峰信号时，都会消耗能量。这些<em class="lu">突触操作</em>的数量与神经形态芯片的有效功耗成正比。顺便说一句，大脑也面临类似的限制。</li></ul><p id="d067" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们介绍了两种对策，帮助我们在这两个问题之间找到正确的平衡。我们的方法类似于[5]中为 TrueNorth 神经形态系统开发的方法。</p><h2 id="40f7" class="ni ly it bd lz nj nk dn md nl nm dp mh lh nn no mj ll np nq ml lp nr ns mn nt bi translated">量化感知训练</h2><p id="c37c" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">首先，虽然训练是在常规的模拟 CNN 上进行的，但我们希望我们的网络至少能够<em class="lu">意识到</em>它的激活将在某个点上被转移到一个脉冲 convnet 上，信号将被强制离散化。为此，我们在每个 ReLU 层将所有 CNN 激活向下舍入到下一个整数值。这是为了保护我们免受上面列出的两个问题中的第一个。训练过程将考虑离散化误差。</p><p id="f7e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如一些读者已经猜到的，这导致梯度几乎在所有地方都变为零，对反向传播造成灾难性的后果。但不要担心:我们总是可以用一个替代(即假)梯度来装备我们的“量化”ReLU (QReLU)，这忽略了离散化。换句话说，我们在向前传递时进行向下舍入，但在向后传递时不进行向下舍入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/03274b93dab55183b59a91d7b3265866.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/0*K2ZGRaxefjV5oDZs.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">QReLU 响应函数。(自己的工作)</p></figure><p id="7e3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您想知道如何欺骗 PyTorch 将手动选择的渐变替换为实际渐变，那么使用自定义的 backward()定义 PyTorch 函数非常简单:</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="5306" class="ni ly it nw b gy oa ob l oc od"># Defining a PyTorch function with a custom backward()<br/>class _Quantize(torch.autograd.Function):<br/>    @staticmethod<br/>    # ctx is a "context object" that we don't need<br/>    def forward(ctx, input):<br/>        # round down<br/>        return input.floor()<br/><br/>    @staticmethod<br/>    def backward(ctx, grad_output):<br/>        # don't do anything (identity function)<br/>        return grad_output.clone()</span></pre><p id="3ef6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练期间的量化并不是一个新的想法，实际上也越来越受欢迎，但在一个相当不同的背景下。当我们想为激活、网络参数或两者使用低精度数据类型(通常是 int8，而不是 PyTorch 中的标准，即 float32)来节省<em class="lu">内存</em>时，这很有用。PyTorch 最近<a class="ae lv" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank">引入了这个特性</a>。</p><h2 id="2b7c" class="ni ly it bd lz nj nk dn md nl nm dp mh lh nn no mj ll np nq ml lp nr ns mn nt bi translated">最小化突触操作的数量</h2><p id="c9ab" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">我们在做机器学习，我们想最小化一些东西，我们该怎么做？当然是加在损失上。</p><p id="24f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">即使在训练期间，我们也可以估计 SNN 中将要发生的突触操作的数量，这是在它完全变成 SNN 之前。我们通过观察我们的量化激活来估计尖峰的数量，并将每个神经元的尖峰率乘以该神经元的“扇出”——将接收其尖峰的神经元数量。我们为这个量设定一个目标(我称之为“SynOps”)，并给损失加上一个二次惩罚。</p><p id="6b05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了这两个工具，即“SynOp loss”和量化感知训练，我们可以训练更好的脉冲卷积神经网络来解决类似 CNN 的计算机视觉任务。值得重复的是，我们离大脑中的尖峰网络学习的目标还很远。但这不是我们的目标(我希望现在已经很清楚了)。</p><h2 id="7413" class="ni ly it bd lz nj nk dn md nl nm dp mh lh nn no mj ll np nq ml lp nr ns mn nt bi translated">测试</h2><p id="9455" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">我们在两个物体识别任务上测试了我们的 snn。一个是常见的 CIFAR10，另一个是<a class="ae lv" href="http://www2.imse-cnm.csic.es/caviar/MNISTDVS.html" rel="noopener ugc nofollow" target="_blank"> MNIST-DVS </a>，这是一组用动态视觉传感器(基于事件的相机)记录的移动 MNIST 手指的短视频。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7867317d3141d58afcc438a7a7540f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*VEkG7QQ66ahpycZ5_eucOw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自 MNIST-DVS 数据集的样本，显示数字 2。(自己的工作)</p></figure><p id="0880" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在适当的尖峰网络模拟上测试结果，并在测试集上测量概要(记住，它与网络的能量消耗成比例)和分类准确度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/569ebcc95b689490f6bef3a2173754e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1u-iRui_In3TLiLs.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST-DVS 的调查结果。(来源:[3])</p></figure><p id="ac52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 MNIST-DVS 数据集上，与上下缩放权重(或输入)以减少活动的基线方法相比，我们获得了明显更好的能量准确性平衡。例如，用我们的方法训练的特别好的网络具有比基线稍低的准确度(从 96.3%到 95.0%)，几乎低一个数量级的天气计数。在 CIFAR10 上，我们实现了非常高的 SNN 精度，但功耗却低得多。</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="39b9" class="lx ly it bd lz ma on mc md me oo mg mh jz op ka mj kc oq kd ml kf or kg mn mo bi translated">文献学</h1><p id="6db6" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">[1] E. Neftci，H. Mostafa 和 F. Zenke，<a class="ae lv" href="https://ieeexplore.ieee.org/abstract/document/8891809/" rel="noopener ugc nofollow" target="_blank">脉冲神经网络中的代理梯度学习:将基于梯度的优化的力量引入脉冲神经网络</a> (2019)，IEEE 信号处理杂志 36，6。</p><p id="5bd1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] B. Rueckauer 等<a class="ae lv" href="https://www.frontiersin.org/articles/10.3389/fnins.2017.00682/full?report=reader" rel="noopener ugc nofollow" target="_blank">连续值深度网络向高效事件驱动网络的转换用于图像分类，</a> (2017)神经科学前沿。</p><p id="1d7b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] M. Sorbaro，Q. Liu，M. Bortone 和 S. Sheik，<a class="ae lv" href="https://arxiv.org/abs/1912.01268" rel="noopener ugc nofollow" target="_blank">优化用于神经形态应用的脉冲神经网络的能量消耗</a> (2019)，arXiv 预印本。</p><p id="f5ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] Shrestha，S. B .，&amp; Orchard，G. (2018 年)。杀戮者:钉层错误重新分配时间。在<em class="lu">神经信息处理系统的进展</em>(第 1412-1421 页)。</p><p id="5180" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5]埃塞、S. K .、梅罗拉、P. A .、阿瑟、J. V .、卡西迪、阿普斯瓦米、r .、安德烈奥普洛斯、a .、… &amp;巴奇、D. R. (2016)。用于快速、节能神经形态计算的卷积网络。2016.<em class="lu">ArXiv 上的预印本。</em><a class="ae lv" href="http://arxiv.org/abs/1603.08270" rel="noopener ugc nofollow" target="_blank"><em class="lu"/></a><em class="lu">。</em></p><h1 id="1976" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">更多信息</h1><p id="9bc4" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">请阅读(并引用)我们在 arXiv 上提供的<a class="ae lv" href="https://arxiv.org/abs/1912.01268" rel="noopener ugc nofollow" target="_blank">预印本</a>。这项工作已经作为原始研究文章提交给神经科学前沿专题。</p><h1 id="62ef" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">复制我们的作品</h1><p id="cdd0" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">GitLab 上有一个<a class="ae lv" href="https://gitlab.com/aiCTX/synoploss" rel="noopener ugc nofollow" target="_blank">公共知识库</a>供那些想要复制这项工作的人使用。您还需要下载(免费提供的)数据集。这在自述文件中有解释，但是如果您需要帮助，请联系我们。</p></div></div>    
</body>
</html>