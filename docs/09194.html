<html>
<head>
<title>Predicting HR Attrition using Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于支持向量机的人力资源流失预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=collection_archive---------38-----------------------#2020-07-01">https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=collection_archive---------38-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b090" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习按照最佳实践训练 SVM 模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bb10120ffdf0862367dbf620ea7e2b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T_80SvL18DLPIeH4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯文·Ku 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="eb4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继我之前的帖子之后，我将介绍我们如何在真实世界数据集上使用 python 和 Scikit-Learn 来应用支持向量机。这些数据取自最近由 IIT、古瓦哈蒂在 Kaggle 上举办的 InClass hackathon，作为他们夏季分析 2020 顶点项目的一部分，你可以从<a class="ae ky" href="https://drive.google.com/drive/folders/1wKpNOY20UhyG0JQ_Y1L2Rp5bEH0Gft5V?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="dea3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，你会看到训练和调音 SVM 并让他们为你的问题陈述工作是多么容易。用这个我在黑客马拉松中排名第 29 位(前 4%)，只做了很少的预处理和特征工程。</p><p id="5c27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们继续之前，您必须对 SVM 的工作方式有一个直观的了解。我建议你仔细阅读我之前的文章，深入了解这个算法。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/decoding-support-vector-machines-5b81d2f7b76f"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">解码支持向量机</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">直观理解支持向量机的工作原理</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="9160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在假设你理解支持向量机背后的理论…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/4e47b2e96b6a3210336edf5e8ae02462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*UsLct33-A7nSWtlU.jpg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://imgflip.com/i/346y74" rel="noopener ugc nofollow" target="_blank"> imgflip </a></p></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h2 id="cd8d" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">问题陈述</h2><p id="fa53" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">随着新冠肺炎不断释放它的浩劫，世界继续被推入大经济衰退的危机，越来越多的公司开始削减他们表现不佳的员工。公司解雇成百上千的员工是当今典型的头条新闻。裁员或降薪是一个艰难的决定。这一点需要非常小心，因为对表现不佳的员工的识别不准确可能会破坏员工的职业生涯和公司在市场上的声誉。</p><p id="93b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目的:</strong>根据给定的员工历史数据，预测员工流失情况。</p><h2 id="e9b9" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">导入所有必需的包</h2><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="f39f" class="mv mw it nu b gy ny nz l oa ob"><strong class="nu iu">import</strong> <strong class="nu iu">pandas</strong> <strong class="nu iu">as</strong> <strong class="nu iu">pd</strong><br/><strong class="nu iu">import</strong> <strong class="nu iu">numpy</strong> <strong class="nu iu">as</strong> <strong class="nu iu">np</strong><br/><strong class="nu iu">import</strong> <strong class="nu iu">matplotlib.pyplot</strong> <strong class="nu iu">as</strong> <strong class="nu iu">plt</strong><br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.model_selection</strong> <strong class="nu iu">import</strong> StratifiedKFold<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.compose</strong> importmake_column_transformer<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.ensemble</strong> <strong class="nu iu">import</strong> RandomForestClassifier<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.model_selection</strong> <strong class="nu iu">import</strong> GridSearchCV,RandomizedSearchCV<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.svm</strong> <strong class="nu iu">import</strong>  SVC<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.decomposition</strong> <strong class="nu iu">import</strong> PCA<br/><strong class="nu iu">from</strong> <strong class="nu iu">xgboost</strong> <strong class="nu iu">import</strong> XGBClassifier<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.model_selection</strong> <strong class="nu iu">import</strong> cross_val_score<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.preprocessing</strong> <strong class="nu iu">import</strong> StandardScaler,RobustScaler<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.preprocessing</strong> <strong class="nu iu">import</strong> OneHotEncoder,LabelEncoder<br/><strong class="nu iu">from</strong> <strong class="nu iu">sklearn.pipeline</strong> <strong class="nu iu">import</strong> make_pipeline<br/>pd.set_option('display.max_columns', 500)<br/>pd.set_option('display.max_rows', 1000)</span></pre><h2 id="d5a2" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">加载数据集</h2><p id="ad50" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">为了保存原始的培训数据，我制作了一份副本。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="79cc" class="mv mw it nu b gy ny nz l oa ob">training = pd.read_csv('train.csv')<br/>train_data = training.copy()<br/>test_data = pd.read_csv('test.csv')</span></pre><h2 id="4bb1" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">基础 EDA</h2><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="7c58" class="mv mw it nu b gy ny nz l oa ob">train_data.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ba1a58fee64f7c1aa0bb431ba657fd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*Y89YaHQjRtfly-6OaBT8lA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1</p></figure><p id="dd29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有 1628 个观察值和 29 个特征，其中 22 个是整数，7 个是对象。一些整数数据类型特征也可能是分类的。我们必须预测<em class="od">自然减员</em>，它可以是 0 或 1(如果员工离开公司，则为 1)。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="b230" class="mv mw it nu b gy ny nz l oa ob">train_data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/43694b73a48fa4e03c4d987ea5dccea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qAQsqNJKY4cmMd8pdFZAHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2</p></figure><p id="0452" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们打印出一些变量的统计数据</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/a99233b9b2fb9a28b5cabcd468872a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDhbldmueVLUvlHGTZqrhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3</p></figure><p id="2d1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里要注意的奇怪的事情是<em class="od">行为</em>的标准差为 0，均值=min=max = 1。这意味着对于所有的观察，该列的值都是 1，所以我们将删除它。我们还将删除<em class="od"> Id </em>，因为它对所有员工都有唯一的值。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="1e8e" class="mv mw it nu b gy ny nz l oa ob">train_id = train_data.Id<br/>train_data = train_data.drop(['Behaviour','Id'],axis = 1)<br/><br/>test_id = test_data.Id<br/>test_data = test_data.drop(['Behaviour','Id'],axis = 1)</span></pre><p id="0c2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进一步研究后，我发现特性 P<em class="od">performance rating</em>只有两个值，3 或 4，所以我将它们分别映射到 0 和 1。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="eb68" class="mv mw it nu b gy ny nz l oa ob">train_data['PerformanceRating'] = train_data['PerformanceRating'].apply(<strong class="nu iu">lambda</strong> x: 0 <strong class="nu iu">if</strong> x == 3 <strong class="nu iu">else</strong> 1)<br/>test_data['PerformanceRating'] = test_data['PerformanceRating'].apply(<strong class="nu iu">lambda</strong> x: 0 <strong class="nu iu">if</strong> x == 3 <strong class="nu iu">else</strong> 1)</span></pre><p id="bd07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查我们的目标变量<em class="od">消耗</em>的分布。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="0239" class="mv mw it nu b gy ny nz l oa ob">train_data[‘Attrition’].value_counts().plot(kind = ‘bar’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3696fdf5d8627fa47559dfb235baec07.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*6RtabSpdORYspY9EiMgTAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4</p></figure><p id="f9f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的分布大致平衡。</p><p id="fa9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查重复项</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="f38b" class="mv mw it nu b gy ny nz l oa ob">print('Number of duplicates: ',train_data.duplicated().sum())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f8c9597e446adb4dd874a66e7bceb277.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*aE6XWwNTSQz6cZFtGY9Hqw.png"/></div></figure><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="b5f5" class="mv mw it nu b gy ny nz l oa ob">train_data[train_data.duplicated()]['Attrition'].value_counts().plot(kind = 'bar')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3067fa069fdc95da0014b9b20131901c.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*_o6YwbrNdtHE8U-MTEWxOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5</p></figure><p id="31f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，数据有 628 个副本，所有副本都对应于损耗 1。这意味着数据被过采样以保持平衡。我们现在将删除重复的内容，并再次检查分布情况。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="0bd7" class="mv mw it nu b gy ny nz l oa ob">train_unq = train_data.drop_duplicates()<br/>print('New train set: ',train_unq.shape)<br/>X = train_unq.drop('Attrition',axis = 1)<br/>y = train_unq['Attrition']<br/>y.value_counts().plot(kind = 'bar')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c575ad499a55f22fb3da57a3ace46a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*OnNebTCs-kPBLn9U7o1B5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6</p></figure><p id="49be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们的训练数据有 1000 个数据点，而<strong class="lb iu">目标变量是不平衡的</strong>。有许多方法可以处理不平衡的数据集，如使用 SMOTE 进行上采样或下采样。</p><p id="80f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用交叉验证策略，例如分层 k 折叠，它可以保持目标变量(此处为<em class="od">损耗</em>)在折叠中的分布相似。使用<strong class="lb iu">分层抽样</strong>而不是随机抽样来分割训练和验证数据。这里的 stratas 是我们的目标变量的两个值。如果你不明白这意味着什么，那么不要担心，只要记住这是一个有效的方法来处理不平衡的数据集，而我们训练我们的模型。你可以在 scikit-learn 交叉验证用户指南中了解更多信息<a class="ae ky" href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" rel="noopener ugc nofollow" target="_blank">点击</a>。</p><h2 id="6f2d" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">预处理、培训和验证</h2><p id="bfda" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">我们现在将遵循以下步骤:</p><ul class=""><li id="7eed" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu oo op oq or bi translated">降低性能等级(在探索中，我发现 85%的值属于一个类，这可能会导致过度拟合)</li><li id="cee4" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">一个热编码所有的“对象”数据类型特征</li><li id="d9fe" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">对所有整数数据类型要素使用标准缩放。</li><li id="2875" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">使用预处理的数据，并使用分层 K-Fold 对其进行分割。</li><li id="a7c3" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">用 3 个候选模型进行拟合和验证:随机森林、XGBoost 和<strong class="lb iu">支持向量分类器</strong></li></ul><p id="fd07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是很多步骤。scikit-learn 允许我们使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html" rel="noopener ugc nofollow" target="_blank">管道</a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html" rel="noopener ugc nofollow" target="_blank">列变压器</a>用几行简单的代码完成所有这些工作。</p><p id="de3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，首先，我们为预处理做一个管道</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="9540" class="mv mw it nu b gy ny nz l oa ob">categorical = [f <strong class="nu iu">for</strong> f <strong class="nu iu">in</strong> training.columns <strong class="nu iu">if</strong> training[f].dtype == object]<br/>numeric = [f <strong class="nu iu">for</strong> f <strong class="nu iu">in</strong> X.columns <strong class="nu iu">if</strong> f <strong class="nu iu">not</strong> <strong class="nu iu">in</strong> categorical+['Id',','Behaviour','PerformanceRating']]<br/><br/>pre_pipe = make_column_transformer((OneHotEncoder(),categorical),(StandardScaler(),numeric))</span></pre><p id="996e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变量<em class="od">分类</em>存储对象数据类型特征的所有列名，而<em class="od">数字</em>存储所有整数数据类型列。</p><p id="d508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们为模型定义管道，并打印出不同模型的交叉验证分数。使用 cross_val_score，我们可以将“skf”传递给“cv”参数，它将负责为我们进行拆分迭代和计算分数。由于数据不平衡，我使用“roc_auc”作为评分标准。为了更好地理解 cross_val_score 的参数，请查看其<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="2c7d" class="mv mw it nu b gy ny nz l oa ob"># random forest pipeline<br/>pipe_rf = make_pipeline(pre_pipe,RandomForestClassifier())<br/>#xgboost pipeline<br/>pipe_xgb = make_pipeline(pre_pipe,XGBClassifier()) <br/># SVM pipeline<br/>pipe_svc = make_pipeline(pre_pipe,SVC(probability=<strong class="nu iu">True</strong>))</span><span id="d21b" class="mv mw it nu b gy ox nz l oa ob">print(‘RF:‘,np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_rf,scoring=’roc_auc’))) </span><span id="aec7" class="mv mw it nu b gy ox nz l oa ob">print(‘XGB:‘,np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_xgb,scoring=’roc_auc’))) </span><span id="71b1" class="mv mw it nu b gy ox nz l oa ob">print(‘SVC:’,np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_svc,scoring=’roc_auc’)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/76cf0b73784939f6ab6b97c266f9da2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*TNhw6Oj6OstsM5jM_wQChA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7</p></figure><p id="851f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，支持向量分类器比基于树的模型表现更好。移除更多功能和调整后，随机森林和 XGBoost 的性能可能会更好，但在本文中，我们将使用 SVM。</p><p id="d32d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一次热编码后，我们在数据集中总共有 46 个特征，随着维度的增加，有更多的机会过度拟合，也可能有不相关的特征。所以我决定用主成分分析来降低维数。</p><p id="e1bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，首先我拿了 46 个组件，并在我们的管道中加入了 PCA。我已经看到了累积的解释差异。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="8884" class="mv mw it nu b gy ny nz l oa ob">n = 46<br/>pipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=<strong class="nu iu">True</strong>,C = 1,kernel='rbf'))<br/><br/>plt.figure(figsize=(10,8))<br/>pipe_svc.fit(X,y)<br/>plt.plot(range(1,n+1),pipe_svc.named_steps['pca'].explained_variance_ratio_.cumsum())<br/>plt.xticks(range(1,n+1,2))<br/>plt.title('Explained Variance')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/7024e7ab03c9a5f9d86bab90cbb3bbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*t3Fh4m4Lg-ULzm74wZBZYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8</p></figure><p id="85e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到大约 34 个成分解释了 100%的方差。所以我们用了 34 个组件来适应我们的模型。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="32c4" class="mv mw it nu b gy ny nz l oa ob">n = 34<br/>pre_pipe = make_column_transformer((OneHotEncoder(),categorical),(StandardScaler(),numeric),remainder = 'passthrough')<br/>pipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=True,C = 1,kernel='rbf'))<br/>print('SVC: ',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_svc,scoring='roc_auc')))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0be35c17e116d9e23c85279505ec43f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*4woP9-BlVex63did3gZ-Mg.png"/></div></figure><p id="3ec8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的分数降低了 0.01，但我们已经大大降低了维度，现在这个模型在看不见的数据上表现更好的机会更大了。</p><h2 id="99b3" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">超参数调谐</h2><p id="9bd9" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">为了从模型中挤出所有的性能，现在是时候调整我们的 SVM 模型了。我们将为此使用 GridSearchCV，如果你不熟悉它，请看一下<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="8c80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以调整的参数有:</p><ul class=""><li id="b60d" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu oo op oq or bi translated">c:这是正则化参数</li><li id="5c55" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">核:线性、多项式或 RBF(高斯)</li><li id="7a0d" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">Gamma:相当于高斯核中的方差项</li><li id="37db" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">类别权重:平衡目标变量中类别的权重(权重与类别频率成反比)</li></ul><p id="ee47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有更多参数，但这些是影响性能的重要参数，在进一步讨论之前，请查看 scikit-learn <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="deae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于管道，调整也是一个非常简单的过程，我们只需定义参数网格，然后将整个管道传递到网格搜索中。我没有在这里调优内核。因为“rbf”是最常用的内核，并且适用于大多数任务，所以我只使用它来节省一些计算时间。您也可以继续尝试调优内核。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="27ce" class="mv mw it nu b gy ny nz l oa ob">param_grid = {<br/>    <br/>    'svc__C':[0.001,0.01,0.1,1,10,100,1000],<br/>    'svc__gamma': ['auto','scale'],<br/>    'svc__class_weight': ['balanced',<strong class="nu iu">None</strong>]<br/>}<br/><br/>grid_search = GridSearchCV(pipe_svc,param_grid=param_grid,cv = skf, verbose=2, n_jobs = -1,scoring='roc_auc')<br/>grid_search.fit(X,y)<br/>print('Best score ',grid_search.best_score_)<br/>print('Best parameters ',grid_search.best_params_)<br/>best_svc = grid_search.best_estimator_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/73f492a5bc21f1f83947165c5d28dbdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qr9NTwxgXVo6CiD_mczTEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9</p></figure><p id="fe28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能已经注意到，我在参数网格中的所有参数前面添加了“svc__ ”,这样做只是为了指定我们在网格搜索中调整管道的哪个<strong class="lb iu">步骤</strong>,因为我们不只是将一个估计器传递给 GridSearchCV，而是传递给整个管道。我相信可能有更好的方法来做到这一点，如果你知道更好的技术，请在评论中告诉我。</p><p id="acfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以尝试进一步调整我们的模型，通过调整 C 的范围和固定其他超参数的值来找到最佳的 C，就像这样</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="cd1b" class="mv mw it nu b gy ny nz l oa ob">pipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=<strong class="nu iu">True</strong>,C = 1,kernel='rbf',class_weight=<strong class="nu iu">None</strong>,gamma='auto'))<br/>param_grid={<br/>    'svc__C':[0.01,0.03,0.05,0.07,0.1,0.3,0.5,0.7,1]  <br/>}<br/>grid_search = GridSearchCV(pipe_svc,param_grid=param_grid,cv = skf, verbose=2, n_jobs = -1,scoring = 'roc_auc')<br/>grid_search.fit(X,y)<br/>print('Best score ',grid_search.best_score_)<br/>print('Best parameters ',grid_search.best_params_)<br/>best_svc = grid_search.best_estimator_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/3f33f2900aec17dd028387904989ed4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeiTwlBRFaOPuZghp1YkNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 10</p></figure><p id="c9ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终的平均验证分数现在是 0.8319。是时候向 Kaggle 提交了。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="65a4" class="mv mw it nu b gy ny nz l oa ob">best_svc.predict_proba(test_data)[:,1]<br/>submission = pd.DataFrame(prediction,columns=['Attrition'])</span><span id="93f3" class="mv mw it nu b gy ox nz l oa ob">submission['Id'] = test['Id']<br/>submission = submission[['Id','Attrition']]<br/>submission.to_csv('submissionfile_SVC.csv',index = None)</span></pre><p id="0a39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/c20e2c893258ed17c8ba1adbe05e360b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWxbAR-QryHHQw1d2jGDpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 11</p></figure><p id="499d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，我的排名下降了 4 位。发生这种情况是因为我的模型过度适应训练数据，我可能不应该太疯狂地调整“C”。更好的特征选择也可能有助于提高分数。你怎么想呢?请随意查看我的<a class="ae ky" href="https://www.kaggle.com/mishraboi/hr-attrition-prediction" rel="noopener ugc nofollow" target="_blank"> Kaggle 笔记本</a>并执行代码。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="25c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你坚持到最后，谢谢你。我希望你觉得这篇文章有用，并从中学习到新的东西。请在评论中提供宝贵的反馈，我很想知道你对提高性能的想法。</p></div></div>    
</body>
</html>