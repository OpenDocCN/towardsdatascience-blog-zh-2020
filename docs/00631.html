<html>
<head>
<title>Linear Algebra for Deep Learning Models on TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流上深度学习模型的线性代数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-linear-algebra-in-tensorflow-part1-input-layer-e0480a8d69bf?source=collection_archive---------17-----------------------#2020-01-18">https://towardsdatascience.com/mastering-linear-algebra-in-tensorflow-part1-input-layer-e0480a8d69bf?source=collection_archive---------17-----------------------#2020-01-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6e6b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">以表格数据为例进行处理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6a57c4119873459707f5812fc9b8917e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcQMydNMpzqdMU5XtMqR4A.png"/></div></div></figure></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="29d4" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">初学者的一个耗时问题是不同深度学习架构下的矩阵运算。换句话说，处理线性代数的东西代表了你对矩阵计算机制的理解程度。如果你对线性代数中的矩阵或张量没有什么概念，可以看看维哈尔·鞍马在Medium上的文章。为了减少混乱，<strong class="la ir"> TensorFlow 1。x和张量流2。x都将包含在本教程</strong>中。所有代码在Google Colab上都是可执行的，你可以在最短的时间内进入主题。</p><h1 id="ff50" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">主要数据格式</h1><p id="895d" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">在详细介绍之前，输入数据格式有四种主要类型:</p><p id="3265" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 1。矢量数据</strong></p><p id="e155" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(1)数据形状:(数据样本*特征)</p><p id="7fca" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 2。时间序列数据或序列数据</strong></p><p id="e7f6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(1)数据形状:(数据样本*时间戳*特征)</p><p id="dfbb" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于序列数据，时间戳代表输入数据的序列长度，该长度在输入模型之前应该是固定的。通常的方法包括零填充和切片。至于时间序列数据，它代表每个时间段。</p><p id="333e" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 3。图像数据</strong></p><p id="9604" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(1)数据形状:(数据样本*高度*宽度*通道)</p><p id="692a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">高度和宽度指的是图像的形状。通道表示RGB(通道是3)、灰度(通道是1)等。</p><p id="b711" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(2)数据形状:(数据样本*特征)如果展平高度*宽度*通道</p><p id="c1e6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 4。视频数据</strong></p><p id="d054" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(1)数据形状:(数据样本*帧*高度*宽度*通道)</p><p id="2d21" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(2)数据形状:(数据样本*特征)如果展平框架*高度*宽度*通道</p></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><h1 id="6674" class="lv lw iq bd lx ly ms ma mb mc mt me mf jw mu jx mh jz mv ka mj kc mw kd ml mm bi translated">矢量数据</h1><p id="5334" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">今天，在构建这种类型的模型时，我们将重点关注矢量数据和线性代数处理。请注意，数据预处理可能会被忽略，因为我们现在关注的内容不在这里。向量数据，也称为2D张量。如果我们说单个数据是一个向量，那么2D向量数据就是我们一次处理多个单个数据的时候。2D向量数据是机器学习中最常见的任务，我们有时也会将图像数据等高维矩阵转换为2D向量。</p><h1 id="2cd5" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">矢量数据—分类问题</h1><h2 id="1431" class="mx lw iq bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">1.数据准备</h2><p id="b558" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">假设我们正在TensorFlow 1.X上为虹膜数据集实现一个3层多层感知器，用于分类。首先，我们导入一些必要的包:</p><p id="c612" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">关于TensorFlow 1。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6ba3" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">在TensorFlow 2上。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="c902" class="mx lw iq bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">2.模型架构准备</h2><p id="907d" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">然后，我们分配每层数据的维度:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="4b00" class="mx lw iq bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">3.模型构建(输入和隐藏层)</h2><p id="b191" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">为了方便起见，我们将隐藏层的输出形状设置为相同的维度，即4。<code class="fe nm nn no np b">n_class</code>指目标特征的类别数。那么，我们先来处理棘手的部分:<code class="fe nm nn no np b">inputs, and labels</code>。</p><p id="6f79" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">关于TensorFlow 1。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="dc48" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">根据TensorFlow的设计，我们倾向于将<code class="fe nm nn no np b">feature_size</code>作为第二维度，将<code class="fe nm nn no np b">None</code>作为第一维度的任意数量的样本(批量)。对于权重(<code class="fe nm nn no np b">w1</code>)，我们使用<code class="fe nm nn no np b">[feature_size, hidden_1]</code>。至于偏，形状是<code class="fe nm nn no np b">[hidden_1, ]</code>。因此，我们做第1层<code class="fe nm nn no np b">tf.add(tf.matmul(x, w1), b1)</code>。<strong class="la ir">但是，为什么不是W*x + b呢？为什么x*W更有意义？我们来看下图:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/0f8437d94b8526e9ca3c4545b45dc5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1m8FtnSdSJ90UtEpC7B4A.png"/></div></div></figure><p id="d57d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">TensorFlow预期形状为<code class="fe nm nn no np b">(number of examples, num_classes)</code>。因此，做<code class="fe nm nn no np b">tf.matmul(x, W)</code>对于适应TensorFlow的API格式更直观。不过，你可能会看到一些输入数据或教程，如来自吴恩达的<a class="ae lu" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank"><em class="nj">Deep learning . ai</em></a>use<code class="fe nm nn no np b">tf.matmul(W, x)</code>。<strong class="la ir">但是，这种格式需要点积后的转置操作(</strong> <code class="fe nm nn no np b">tf.transpose(tf.matmul(W, x))</code> <strong class="la ir"> ) </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/aa5a799b93e7bf4a1eff497b60816be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UMrwMqBhUBGBVbPfaKe3lg.png"/></div></div></figure><p id="0988" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">根据这个原因，就更容易知道为什么下面几层是这个样子了:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="e296" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">在TensorFlow 2上。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7815" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">张量流2。x就方便多了，你只需要小心输入层，应该是<code class="fe nm nn no np b">(feature_size, )</code>。至于其他的，你只需要指定输出形状，TensorFlow 2。x会帮你完成剩下的部分。注意，代码的格式是函数式API格式，保持了更大的灵活性。但是您仍然可以使用通用的，因为我两者都提供了。</p><h2 id="9954" class="mx lw iq bd lx my mz dn mb na nb dp mf lh nc nd mh ll ne nf mj lp ng nh ml ni bi translated">4.模型构建(优化器和预测)</h2><p id="bc6f" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">对于初学者来说，预测也是一个棘手的部分，尤其是当他们看到到处都是像<code class="fe nm nn no np b">tf.argmax</code>和<code class="fe nm nn no np b">tf.reduce_mean</code>这样的语法时。</p><p id="f6b0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">在TensorFlow 1上。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4d95" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><code class="fe nm nn no np b">tf.argmax</code>是一个返回数组中最大数字的索引的函数。参数<code class="fe nm nn no np b">axis</code>指定您正在比较哪个维度。例如，<code class="fe nm nn no np b">axis = 1</code>比较第二维，即Iris数据集中的特征列。为了说明清楚，我们来看下图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/99d0921a988eec10a267c2c631699723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwPG4cIZhkljNfpyyajePg.png"/></div></div></figure><p id="e3ae" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在<code class="fe nm nn no np b">tf.equal(tf.argmax(targets, 1), tf.argmax(output_logits, 1))</code>之后，我们将收到一个布尔值列表，指示两个索引是否匹配相同的值，也就是说，正确的预测。然后，<code class="fe nm nn no np b">tf.reduce_mean</code>是对结果进行平均的函数，总精度！</p><p id="6c3a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="nj">在TensorFlow 2上。X: </em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="612d" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">向量数据—回归问题</h1><p id="c044" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">至于回归问题，它们比分类要简单得多，因为标签只是一个1D向量。你需要的是改变输出维度和激活函数，比如把激活函数从‘soft max’改成‘relu’，就万事大吉了。</p></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="07d5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir">表格数据教程到此结束。接下来，我们将开始处理序列数据和时序数据上的线性代数！</strong></p></div></div>    
</body>
</html>