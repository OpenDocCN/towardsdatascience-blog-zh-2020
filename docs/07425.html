<html>
<head>
<title>Types of Ensemble methods in Machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中集成方法的类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/types-of-ensemble-methods-in-machine-learning-4ddaf73879db?source=collection_archive---------10-----------------------#2020-06-05">https://towardsdatascience.com/types-of-ensemble-methods-in-machine-learning-4ddaf73879db?source=collection_archive---------10-----------------------#2020-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0519ca60e901cb763bf246945e11d03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4I9-d9NSb89ZU9GY"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">弗拉德·希利塔努在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2956" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我之前的文章中，我们讨论了机器学习中不同类型的<a class="ae kf" href="https://medium.com/datadriveninvestor/types-of-regression-in-machine-learning-bd0f5c4772fc" rel="noopener">回归</a>。因此，今天我将重点讨论数据科学中的各种集成方法。</p><p id="3050" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在职业生涯中经历的一件事是，在学习任何技能时，当我们试图将概念与我们日常生活中面临的事件联系起来时，事情会变得更容易。当我们有一个真实的生活类比时，记住事情是毫不费力的。因此，记住这一点，我们将通过与日常场景的比较来研究各种集合技术。</p><h1 id="4c86" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">目录</h1><p id="4e04" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">1.介绍</p><p id="4d14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.集合方法的类型</p><p id="cc51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.装袋和增压方法之间的相似性</p><p id="8fc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.两种方法之间的差异</p><p id="b15d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.投票的类型</p><p id="6b02" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.对概念的详细理解</p><ul class=""><li id="e1af" class="mh mi it ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">制袋材料</li><li id="8764" class="mh mi it ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">助推</li></ul><p id="751d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.结论</p><h1 id="be84" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> 1。简介</strong></h1><p id="58c3" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">什么是合奏？</em>T11】</strong></p><p id="d729" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在简单的英语中，ensemble 指的是一组项目。例如:一群牧师、一群舞蹈演员等。</p><p id="eea0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">什么是机器学习中的集成方法？</em>T15】</strong></p><p id="cce2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">集成方法是一种使用多个独立的相似或不同的模型/弱学习器来获得输出或做出一些预测的技术。</p><p id="9f6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如</p><p id="0416" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">随机森林是多个决策树的集合。</em></p><p id="0c28" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">集合也可以通过不同模型的组合来构建，如随机森林、SVM、逻辑回归等。</em></p><p id="fdf7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们用一个真实的例子来比较一下。</p><p id="ed73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设议会通过了一项法案，并召开了部长会议。</p><p id="029d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而不是总统或总理独自做出决定可能不是一个很好的主意，因为这说明了独裁统治，结果可能不是那么有利于公众。因此，这种类比可以指机器学习中的单一模型，如决策树、逻辑回归等，它们的性能总是略低于集成方法。</p><p id="7c16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，如果在不同的部长之间举行会议，比如说四个部长，他们每个人都会提供一个有正反两面的意见。最后，最佳意见将在多数投票的基础上选出。这正是系综方法。</p><p id="e7c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">部长 1 </strong>:基于他的赞成和反对意见，认为该法案应该通过。</p><p id="ea00" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">部长 2 :基于他的支持和反对，他说这个法案没有用，可能会引发很多挑战。因此不应通过。</p><p id="4ac7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">部长 3: </strong>表示议案可以通过。</p><p id="51ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">部长 4: </strong>也表示议案可以通过。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/1734d45409ba214f96c71f1ef5c6bbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6Lkqu_ZADie_VJ0kSlDbA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">大多数人说“通过议案”。因此，该法案在议会获得通过。</p></figure><p id="9e69" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，我们可以在下面看到使用了不同的模型，并且基于每个模型的输出，最终的决定将通过多数投票做出。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/64b8b0d2d7debbfde4ff9d8a97803b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txkswfOUrdtUUInduuB2Zg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:谷歌</p></figure><p id="e647" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">为什么我们需要合奏技巧？</em>T9】</strong></p><p id="1bc4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">众所周知，任何机器学习模型中的错误和预测都会受到偏差、方差和噪声的不利影响。为了克服这些缺点，使用了集合方法。</p><h1 id="0d60" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nc"> 2。集成方法的类型</em> </strong></h1><p id="bd9c" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">有许多可用的集成技术，但我们将讨论以下两种最广泛使用的方法:</p><p id="fc51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 1。装袋</strong></p><p id="dcce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 2。增压</strong></p><p id="df84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先了解这两个术语之间的异同。</p><h1 id="3448" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nc"> 3。装袋和增压方法的相似性</em> </strong></h1><p id="4c42" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这两种方法都可以用于分类(离散输出)和回归(连续输出)问题。</p><p id="74c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与单一模型相比，这两种方法在性能上都更好，因为它们通过投票聚集了所有弱学习者的输出，从而导致预测更准确。</p><h1 id="189a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nc"> 4。</em>两种方法的区别</strong></h1><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/5cb754d6c0c43a67db5fc691bd993a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XtgP_b-TJ24gKqxDkRbfA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">装袋和增压的区别</p></figure><h1 id="4ce5" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nc"> 5。</em>投票类型</strong></h1><p id="2a64" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">有两种类型的投票用于汇总所有弱学习者的输出。</p><p id="8eec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">硬投票</strong> —如果我们收到每个弱学习者在类中的输出(例如:0/1)，最后我们选择多数返回的输出类，那么这种方法称为硬投票。</p><p id="fa17" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如:在我们的部长小组的例子中，由于 3 名部长赞成该法案，最终决定通过该法案。这是艰难的投票。多数获胜。</p><p id="dc4e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">软投票</strong> —有许多算法也为我们提供预测概率，如果我们收到概率输出，我们会取每个类别概率的平均值，平均值最高的类别将成为最终预测。这种预测方式被称为软投票，它比硬投票更好，因为我们在概率方面获得了更多的信息。</p><p id="96db" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一件要记住的重要事情是，这种投票方法被用作随机森林等群体的默认技术。</p><p id="e360" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用下图来理解这两个概念，其中 1 是类 1，0 是类 0。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/5c88a53e6f3dee7268b20490c3435bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IG6Pe5FmrkEJlCmEQt1e5g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">因为类 1 的平均概率在软投票中更高(63%)，所以我们选择类 1 作为最终输出</p></figure><h1 id="183d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> 6。概念的详细理解</strong></h1><h2 id="410e" class="nf lf it bd lg ng nh dn lk ni nj dp lo kr nk nl ls kv nm nn lw kz no np ma nq bi translated"><strong class="ak">装袋:</strong></h2><p id="913d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">也称为 Bootstrap aggregating，因为训练集被分成多个子样本，并被提供给每个弱学习器(例如:每个决策树)。并且每个子样本包含几组特征以及随机选择的几个观察值。</p><p id="61d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">既然这篇文章的主旨是用通俗的语言来理解概念，那么让我们举一个简单的例子来理解装袋。</em></p><p id="01d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们都参加面试，面试过程是我能遇到的最简单的场景，类似于装袋。</p><p id="e778" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设你去参加一个面试，一进房间，你就看到五个表情严肃的人坐在那里，向你提出所有的问题。你是做什么的？感到紧张，冲出房间！！没有对吗？你去那里是因为你需要这份工作，你知道你足够勇敢，足够聪明，可以面对任何事情，无论如何你都会和他们打招呼，坐在他们面前。所以，让我们面对现实吧。</p><p id="13b5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在考虑一下，五位面试官中的每一位都将从五个不同的方面对应聘者进行面试，比如能力、基本技能、高级技能、沟通和管理技能。</p><p id="14f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，公司可能已经准备了一套基于不同方面的问题来测试候选人的技能。我们称之为“问题集”。</p><p id="0cc1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，采访现在开始。我们的目标变量是候选人是否被选中。</p><p id="2a12" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试官 1 </strong> — <strong class="ki iu">面试官 1 开始测试你的能力倾向技能</strong>从“问题集”中抽取一个基于能力倾向的问题样本，并设定你是否应该被选中的概率。</p><p id="70d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试官 2 </strong> — <strong class="ki iu">面试官 2 也开始测试你的基本技能，</strong>再次从“问题集”中抽取初步技术问题的样本，并设定你是否应该被选中的概率。</p><p id="d452" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试官 3</strong>—<strong class="ki iu">同样，面试官 3 开始测试你的高级技能</strong>从“问题集”中抽取另一个稍微困难的技术问题样本，并再次设定你是否应该被选中的概率。</p><p id="3975" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">现在，他也有可能问你一些从“问题集”中随机挑选的问题，但这些问题是相似的，并且已经在基本技术测试或任何先前的测试中被问过。这种情况在 bagging 方法中被称为 bootstrap，因为由于使用替换随机抽样，可能会再次选取相同的样本。</em></p><p id="bac0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试者 4 &amp;面试者 5: </strong>面试者 4 和面试者 5 重复上述相同的过程，从“问题集”中抽取相关问题的样本，测试候选人的沟通和管理技能，并相应地设定一些选择或不选择的概率。</p><p id="1306" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">结果:</strong>最后，候选人面试结束，他走出房间，让面试官根据“问题集”中对他进行评分的问题的子样本来分析他们各自对他技能的评分。最终，他们汇总所有个人意见，做出是否选择该候选人的最终决定。</p><p id="7950" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">这与合奏的方法完全相似。</em> </strong></p><p id="9513" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">让我们使用 python 尝试几种集成方法，并比较它们对应于几种单一模型的准确性。</em></p><p id="17ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">注</strong>:jupyter 笔记本完整代码在<a class="ae kf" href="https://github.com/anjuraj-ops/Projects-in-data-science/blob/master/advertisment_success_Anju.ipynb" rel="noopener ugc nofollow" target="_blank"> github 中。</a></p><p id="3e63" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">为此，我使用 Kaggle 的广告成功数据集，并希望预测广告的成功。</em></p><h2 id="1b58" class="nf lf it bd lg ng nh dn lk ni nj dp lo kr nk nl ls kv nm nn lw kz no np ma nq bi translated">1.<strong class="ak"> <em class="nc">单款</em> </strong></h2><p id="cfd6" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu"> a .决策树</strong></p><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="d3dd" class="nf lf it ns b gy nw nx l ny nz"><em class="mv"># Decision tree classifier with grid seacrh CV and model evaluation using accuracy score, precision score and AUC/ROC curve.</em><br/><br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.tree</strong> <strong class="ns iu">import</strong> DecisionTreeClassifier<br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.model_selection</strong> <strong class="ns iu">import</strong> GridSearchCV<br/><br/>parameters = {'max_features': [0.5,0.6,0.7,0.8,0.9,1.0], 'max_depth': [2,3,4,5,6,7],'min_samples_leaf':[1,10,100],'random_state':[14]} <br/><br/>clf = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, scoring='roc_auc')<br/>clf.fit(X_train, y_train)</span></pre><p id="e1ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">应用于示例数据的决策树评估指标:</strong></p><p id="0695" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> AUC 评分</strong>为 85.84%。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/c969967b05ebe187a4ec25aa2b1731d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hH5GEbh2WWZ6y4dSoU7-nw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">决策树的精确度和准确性</p></figure><p id="dc12" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mv">决策树总是容易出现</em> <a class="ae kf" href="https://medium.com/analytics-vidhya/overfitting-and-underfitting-in-machine-learning-d829d8501335" rel="noopener"> <em class="mv">过拟合</em> </a> <em class="mv">如果我们没有选择正确的参数，如树叶的最小样本、节点的最小样本、树的最大深度，那么随着深度的增加，模型将更精确地捕捉训练集的数据点，从而在训练数据集本身中产生出色的预测，但在新数据上却失败了。因此，集成方法是有助于减少方差从而解决单个决策树遇到的</em> <a class="ae kf" href="https://medium.com/analytics-vidhya/overfitting-and-underfitting-in-machine-learning-d829d8501335" rel="noopener"> <em class="mv">过拟合</em> </a> <em class="mv">的技术之一。</em></p><p id="0d29" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> b .单一逻辑回归分类器</strong></p><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="3ba8" class="nf lf it ns b gy nw nx l ny nz"><em class="mv">#Single Logistic Regression</em><br/><br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.linear_model</strong> <strong class="ns iu">import</strong> LogisticRegression<br/>log = LogisticRegression(random_state=0, solver='lbfgs') <br/>log.fit(X_train, y_train)<br/><br/>y_pred = log.predict(X_test)</span></pre><p id="8a6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">单个 Logistic 回归分类器应用于实例数据的评价指标:</strong></p><p id="2f95" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AUC 评分为 83.84 %。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/96cb7f3daa53b2bafd15369da502ee9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M8GCYM12ROejp4dCFFG5Xw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">单一逻辑回归分类器的精度和准确度</p></figure><h2 id="41e0" class="nf lf it bd lg ng nh dn lk ni nj dp lo kr nk nl ls kv nm nn lw kz no np ma nq bi translated">2.<strong class="ak">相同分类器的集成</strong></h2><p id="3243" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">几个例子是随机森林、额外树分类器/回归器、线性回归器集成、逻辑回归分类器集成、SVM 集成等。</p><p id="e511" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> a. </strong> <strong class="ki iu">随机森林——多个决策树的集合</strong></p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/82b26e230034c8e7fa19e032406d3c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rAWLrwSpNzzht7CHBr65w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:谷歌</p></figure><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="38cf" class="nf lf it ns b gy nw nx l ny nz"><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.ensemble</strong> <strong class="ns iu">import</strong> RandomForestClassifier<br/>parameters = {'n_estimators':[700],'n_jobs':[-1], 'max_features': [0.5,0.7,0.9], 'max_depth': [3,5,7],'min_samples_leaf':[1,10],'random_state':[14]} <br/><br/>clf1 = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring='roc_auc')<br/>clf1.fit(X_train, y_train)</span></pre><p id="5a03" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">随机森林评价指标应用于实例数据:</strong></p><p id="cb90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> AUC </strong>得分为 86.53 %。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/8a6fb63f0c287f023ae5edb252fa21c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0X_I6acFsCrrnSMLsxD_cA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">随机森林的精度和准确度</p></figure><p id="fb18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">b.<strong class="ki iu">逻辑回归分类器集成。</strong></p><p id="e92d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们使用相同分类器的集合时，Sklearn 的<strong class="ki iu"> BaggingClassifier </strong>用于获得 OOB(袋外)分数，这是在 bagging 中检查分类器性能的一个非常重要的分数，因为它给我们提供了在测试集上要达到的准确度的概念。众所周知，在训练期间，子样本被馈送到每个独立的分类器，但是在 bagging 技术中，同样的样本也可能被传递两到三次到任何其他单独的分类器。因此，在这种情况下，对于特定分类器来说是新的样本，即还没有看到的样本被称为袋外样本。它与测试集的目的相同，因为 OOB 实例是以前没有见过的东西<strong class="ki iu">。在使用 Bagging 分类器构建集成之后，我们可以使用‘Classifier . OOB _ score _’函数</strong>来获得 OOB 分数。</p><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="93a1" class="nf lf it ns b gy nw nx l ny nz"><em class="mv"># Multiple logistic regression classifiers using bagging Classifier.<br/># Number of logistic regression classifiers we are using here are 400.</em></span><span id="18b1" class="nf lf it ns b gy oe nx l ny nz">logbagClf = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs'), n_estimators = 400, oob_score = <strong class="ns iu">True</strong>, random_state = 90)<br/>logbagClf.fit(X_train, y_train)</span></pre><p id="7605" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">bagging 分类器(多重逻辑回归分类器)的评价指标:</strong></p><p id="dd59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AUC 评分为 84.44 %。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/e88dfea6d0b57d393cc530222bf85199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3bC-_2DhOJNldOk7hNFqA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">bagging 分类器(多重逻辑回归分类器)的精度和准确度</p></figure><h2 id="eabf" class="nf lf it bd lg ng nh dn lk ni nj dp lo kr nk nl ls kv nm nn lw kz no np ma nq bi translated"><strong class="ak"> 3。几种不同类型模型的集合</strong></h2><p id="b77c" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">对于这个技术，我们可以使用 Sklearn 的 VotingClassifier <strong class="ki iu">。</strong></p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/53fa18097761f28a943846a6e3f0e490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jH03mrdbCFGdiE1HG1hYmA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">不同机器学习模型的集成</p></figure><p id="97e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了我的测试目的，我使用了 RandomForestClassifier、支持向量机和逻辑回归的集成。</p><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="8e5f" class="nf lf it ns b gy nw nx l ny nz"><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.ensemble</strong> <strong class="ns iu">import</strong> RandomForestClassifier, VotingClassifier<br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.svm</strong> <strong class="ns iu">import</strong> SVC<br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.linear_model</strong> <strong class="ns iu">import</strong> LogisticRegression<br/><strong class="ns iu">from</strong> <strong class="ns iu">sklearn.naive_bayes</strong> <strong class="ns iu">import</strong> GaussianNB<br/><br/>rfClf = RandomForestClassifier(n_estimators=500, random_state=0) <em class="mv"># 500 trees. </em><br/>svmClf = SVC(probability=<strong class="ns iu">True</strong>, random_state=0) <em class="mv"># probability calculation</em><br/>logClf = LogisticRegression(random_state=0)<br/><em class="mv">#nbclf = GaussianNB(random_state=0)</em><br/><br/><em class="mv"># constructing the ensemble classifier by mentioning the individual classifiers.</em><br/>clf2 = VotingClassifier(estimators = [('rf',rfClf), ('svm',svmClf), ('log', logClf)], voting='soft') <br/><br/><em class="mv"># train the ensemble classifier</em><br/>clf2.fit(X_train, y_train)</span></pre><p id="0092" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">投票分类器(多模型集成)的评价指标:</em> </strong></p><p id="818c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> AUC </strong>得分为 84.92 %</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/13fabdbd363c38f3361b8a84eae28e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjrRrXuFQpAOSgPnrz1LHA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><em class="nc">投票分类器的精度和准确度(多个模型的集成)</em></p></figure><h1 id="15c2" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">增压:</strong></h1><p id="de5e" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">boosting 背后的主要思想是在连续迭代中将弱学习者转化为强学习者。</p><p id="cb06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再次以面试为例来理解助推的概念。</p><p id="a329" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设一个应聘者在五家不同的公司参加了五次面试。根据他在这五次面试中的表现，他想对自己的技能进行评估。</p><p id="7f32" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">面试 1 —公司 1  —他只能回答几个问题，但他知道可能会再次被问到什么类型的问题。他回家学习。他的学习速度提高了。</p><p id="897b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试 2 —公司 2 </strong> —在这里，他比第一家公司做得更好，因为他学会了他在第一次面试中错过的所有概念，但仍然没有被选中。他回到家，进行了更多的学习，从而纠正了他在前两次面试中犯的所有错误。</p><p id="786c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">面试 3 和面试 4 同样，</strong>他面临着同样的问题，当他参加第 5 次面试时，他成了专家，几乎能回答所有的问题。</p><p id="5feb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">最终，我们在这里看到的是候选人接触到不同的问题样本，他逐渐学习和提高自己，直到成为专家。结果是，在从错误中吸取教训并改正自己之后，他可以给自己一个最好的评价。每一步都有一个学习率。同样是助推的概念。</em> </strong></p><p id="43f0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将在数据集上实现 boosting 算法 XGBoost。</p><pre class="mx my mz na gt nr ns nt nu aw nv bi"><span id="b45d" class="nf lf it ns b gy nw nx l ny nz"><strong class="ns iu">import</strong> <strong class="ns iu">xgboost</strong> <strong class="ns iu">as</strong> <strong class="ns iu">xgb</strong><br/><br/>xgb_clf = xgb.XGBClassifier(max_depth=3,n_estimators=300,learning_rate=0.05)<br/>    <br/>xgb_clf.fit(X_train,y_train)</span></pre><p id="e3e1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">XGBoost 的评估指标<em class="mv">应用于示例数据:</em> </strong></p><p id="eb32" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> AUC </strong>得分为 86.46 %。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/5b98e5d0a5b92138aae284f43d590d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*120eqGqx1udTHvbrHIvuVg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><em class="nc">XGBoost 算法的精度和准确度</em></p></figure><h1 id="c9f1" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> 7。结论</strong></h1><p id="40fa" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">让我们比较一下单一模型和集合模型的准确度、精度和 AUC 得分。</em> </strong></p><ol class=""><li id="de0f" class="mh mi it ki b kj kk kn ko kr mj kv mk kz ml ld oj mn mo mp bi translated"><strong class="ki iu">准确率:</strong> Boosting 算法‘XGBoost’在准确率上领先。</li></ol><ul class=""><li id="c874" class="mh mi it ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">我们还可以注意到，集成分类器'随机森林'和'多重逻辑回归分类器'分别比单一分类器'决策树'和'逻辑回归'具有更好准确性。</li></ul><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/2320d3f404f6299c5e02962160c745ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pESakEXccYshoa3AgOYNvQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">各种算法的准确率评分。</p></figure><p id="450d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 2。精度分数:</strong> Boosting 算法' XGboost '在精度分数上领先。</p><ul class=""><li id="31d0" class="mh mi it ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">我们可以注意到，集成分类器'随机森林'和'多重逻辑回归分类器'分别比单一分类器'决策树'和'逻辑回归'具有更好精度。</li></ul><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/bcad807dae08acc5903280472c05d8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ODNWG0lynzrFdxG0uhJOw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">各种算法的精度评分。</p></figure><p id="7df6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 3。</strong> <strong class="ki iu"> AUC 得分:</strong> Bagging 算法<strong class="ki iu"/>Random forest 在 AUC 得分上领先，尽管与 XGboost 的 AUC 得分有非常微小的差异。</p><ul class=""><li id="0c9d" class="mh mi it ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">我们可以注意到，集成分类器'随机森林'和'多重逻辑回归分类器'分别比单一分类器'决策树'和'逻辑回归'具有更好 AUC 分数。</li></ul><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/b0824d91c398d083b0edbb9652adac22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXvbwNvlY_qxKuHQveQc7A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">各种分类器的 AUC 分数。</p></figure></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="f942" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们可以得出结论，boosting 方法在我们的数据集上表现最佳，我们最终可以部署 XGBoost 模型，因为它将最精确地预测广告活动的成功，这是我们问题陈述的主要议程。</p><p id="1cea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，这取决于我们正在处理的数据的类型。</p><p id="7a48" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们可以推断，与单一模型相比，装袋和增压方法总是表现得更好，精确度更高。</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="4585" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请添加任何额外的想法，评论，因为这将有助于我更好地理解事情。</p><p id="9037" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！！</p></div></div>    
</body>
</html>