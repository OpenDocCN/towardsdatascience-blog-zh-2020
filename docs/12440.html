<html>
<head>
<title>Frequentist vs. Bayesian Approaches in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的频繁主义者与贝叶斯方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/frequentist-vs-bayesian-approaches-in-machine-learning-86ece21e820e?source=collection_archive---------4-----------------------#2020-08-27">https://towardsdatascience.com/frequentist-vs-bayesian-approaches-in-machine-learning-86ece21e820e?source=collection_archive---------4-----------------------#2020-08-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6210" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性回归和贝叶斯线性回归的比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9689645a002e83a28e5534bf16abbc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBmsN3IkeBKhtmeAd91TTQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="a83b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝叶斯统计推断和频率主义统计推断之间一直存在争论。20 世纪，频繁主义者主导了统计实践。许多常见的机器学习算法，如线性回归和逻辑回归，都使用频率主义方法来执行统计推断。虽然贝叶斯主义者在 20 世纪之前主导了统计实践，但近年来，贝叶斯学派的许多算法，如期望最大化、贝叶斯神经网络和马尔可夫链蒙特卡罗，在机器学习中得到了普及。</p><p id="5934" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将在机器学习的背景下讨论它们的区别和联系。我们还将使用两种算法进行说明:<strong class="la iu">线性回归</strong>和<strong class="la iu">贝叶斯线性回归。</strong></p><h1 id="2cc3" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">假设</h1><p id="53db" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">为简单起见，本文中我们将使用θ来表示模型参数。</p><p id="bfb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> Frequentist 方法</strong>假设观测数据是从某个分布中抽样得到的。我们将这种数据分布称为似然性:P(Data|θ)，其中θ被视为常数，目标是找到使似然性最大化的θ。例如，在逻辑回归中，假设数据是从伯努利分布中抽样的，而在线性回归中，假设数据是从高斯分布中抽样的。</p><p id="fd55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">贝叶斯方法</strong>假设数据和假设的概率(指定数据分布的参数)。在贝叶斯理论中，θ是一个变量，假设包括假设 P(θ)的先验分布和数据 P(Data|θ)的可能性。对贝叶斯推理的主要批评是先验的主观性，因为不同的先验可能得出不同的后验和结论。</p><h1 id="3bea" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">参数学习</h1><p id="4ea0" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated"><strong class="la iu">频率学家</strong>使用<strong class="la iu">最大似然估计</strong>来获得参数θ的点估计。对数似然表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/883ea76953d32af08c5a2ba1bc0b2085.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*6RqLztrZRXbMB7noeVBSFQ.png"/></div></figure><p id="2cd8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过最大化对数似然或最小化负对数似然(损失函数)来估计参数θ:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b1dd27164422c1173a894c7a372c4195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*tXra9AX3j9dFQDdMiHgvmg.png"/></div></figure><p id="b5df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> Bayesians </strong>不是点估计，而是使用 Bayes 公式估计参数的完全后验分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/c20856a61a1bf43fe2dcf954c596a7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*ggYZH275VHcmA1P5Wh-sEw.png"/></div></figure><p id="abbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能已经注意到，分母的计算可能是 NP 难的，因为它在θ的所有可能值上都有一个积分(或分类情况下的求和)。你可能也想知道我们是否能得到θ的点估计，就像 MLE 所做的那样。这就是<strong class="la iu">最大后验概率</strong> ( <strong class="la iu">图</strong>)估算发挥作用的地方。MAP 绕过了后验分布的繁琐计算，而是试图找到使后验分布最大化的θ的点估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6d0f577a68bb7b56f79b6cc38a36471c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*fo0QvcgN-pAG_DsQgT5REw.png"/></div></figure><p id="cf64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于对数函数是单调的，我们可以在对数空间重写上述等式，并将其分解为两部分:最大化似然性和最大化先验分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/615f220e12764b5766428103d65c2e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*89GpriYwu-G5zZHkeXG4Qw.png"/></div></figure><p id="6a6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个看起来不像 MLE 吗？</p><p id="9114" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，这两者之间的联系是 MAP 可以被视为对正则化损失函数执行 MLE，其中先验对应于正则化项。例如，如果我们假设先验分布为高斯分布，则 MAP 等于 L2 正则化的 MLE 如果我们假设先验分布是拉普拉斯分布，那么在 L1 正则化的情况下，MAP 等于 MLE。</p><p id="5e2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有一种方法可以得到后验分布的点估计:<strong class="la iu">期望后验(EAP) </strong>估计。MAP 和 EAP 的区别在于 MAP 得到的是后验分布的众数(最大值)，而 EAP 得到的是后验分布的期望值。</p><h1 id="d90d" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">不确定</h1><p id="dc2f" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">频率主义者和贝叶斯方法之间的主要区别是他们在参数估计中测量不确定性的方式。</p><p id="1a53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们前面提到的，<strong class="la iu">常客</strong>使用 MLE 来获得未知参数的点估计，他们不会给可能的参数值分配概率。因此，为了衡量不确定性，频率主义者依赖于零假设和置信区间。然而，需要指出的是，置信区间并不直接转化为假设的概率。例如，95%的置信区间，只意味着你生成的 95%的置信区间将覆盖真实的估计，但是说它以 95%的概率覆盖真实的估计是不正确的。</p><p id="0362" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">另一方面，贝叶斯</strong>在可能的参数值上有一个完整的后验分布，这允许他们通过整合完整的后验分布来获得估计的不确定性。</p><h1 id="73d6" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">计算</h1><p id="0ae6" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">由于在许多参数上的整合，Bayesians 通常比 frequent ists<strong class="la iu">计算更密集。有一些方法通过使用共轭先验或使用采样方法或变分推理来近似后验分布来降低计算强度。</strong></p><h1 id="c19e" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">例子</h1><p id="a001" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在本节中，我们将了解如何使用两种算法进行训练和预测:线性回归和贝叶斯线性回归。</p><h2 id="b7a5" class="mw lv it bd lw mx my dn ma mz na dp me lh nb nc mg ll nd ne mi lp nf ng mk nh bi translated">线性回归(频率主义者)</h2><p id="6dcc" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们假设线性回归模型的以下形式，其中截距包含在参数θ中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/49c8c67a0b374e114252bfd1d4284b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*y1f4-_FXKv-lpGqDIJhO0A.png"/></div></figure><p id="ba35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设数据按照高斯分布分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/95bb9c17aa26bcd8d7f6108eaed257f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*7asCP5OWi2faB0Z_Par_hA.png"/></div></figure><p id="a403" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 MLE 最大化对数似然，我们可以得到θ的点估计，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/7109b069e3476ff8b9099675727a2f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*U1Lw3oGSObgquxC5TeNgjQ.png"/></div></figure><p id="2f46" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们从训练数据中学习了参数θ，我们就可以直接使用它来对新数据进行预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4760a59cfde0061a064bee5bb34b1056.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*3v07Tcqvmu0MeOIh8YhOxw.png"/></div></figure><h2 id="3b59" class="mw lv it bd lw mx my dn ma mz na dp me lh nb nc mg ll nd ne mi lp nf ng mk nh bi translated">贝叶斯线性回归(贝叶斯)</h2><p id="df4e" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">如前所述，贝叶斯方法是对先验和可能性都进行假设:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7c550a8f895cf9818b18a9891d60d852.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*svJyLzkjjvmYakTviVoy0A.png"/></div></figure><p id="46e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用这些假设和贝叶斯公式，我们可以得到后验分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/c57b212061480db650fda4c9076efcda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ps8kU4-Om4zW2LnyVov40A.png"/></div></div></figure><p id="b6e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在预测时，我们使用后验分布和可能性来计算后验预测分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/dd7e1be24b4a652baeff1a8dd8d97abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7VJRMotulIJEsvZ1mLqHA.png"/></div></div></figure><p id="baa9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，参数和预测的估计值都是全分布。当然，如果我们只需要一个点估计，我们总是可以使用 MAP 或 EAP。</p><h1 id="0a96" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">结论</h1><p id="5c42" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">机器学习的主要目标是使用从训练数据中学习到的参数进行预测。我们应该使用频率主义者还是贝叶斯方法来实现目标取决于:</p><ol class=""><li id="4d07" class="np nq it la b lb lc le lf lh nr ll ns lp nt lt nu nv nw nx bi translated">我们想要的预测类型:点估计或潜在值的概率。</li><li id="4d47" class="np nq it la b lb ny le nz lh oa ll ob lp oc lt nu nv nw nx bi translated">我们是否有可以整合到建模过程中的先验知识。</li></ol><p id="fc28" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">顺便说一下，我们之前讨论过<a class="ae od" rel="noopener" target="_blank" href="/generative-vs-2528de43a836">判别模型和生成模型</a>。一个常见的误解是将判别模型称为频率主义模型，将生成模型称为贝叶斯模型。事实上，频率主义和贝叶斯方法都可以用于判别或生成模型。你可以参考<a class="ae od" href="https://lingpipe-blog.com/2013/04/12/generative-vs-discriminative-bayesian-vs-frequentist/" rel="noopener ugc nofollow" target="_blank">这个帖子</a>获得更多的澄清。</p><p id="cf86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢阅读这篇文章。:)</p></div></div>    
</body>
</html>