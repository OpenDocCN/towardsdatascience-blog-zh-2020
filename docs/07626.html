<html>
<head>
<title>Be in charge of Query Execution in Spark SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">负责Spark SQL中的查询执行</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/be-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8?source=collection_archive---------23-----------------------#2020-06-08">https://towardsdatascience.com/be-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8?source=collection_archive---------23-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4e2f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解您的查询计划</h2></div><p id="e148" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自从Spark 2.x以来，由于SQL和声明性DataFrame API，在Spark中查询数据已经成为一种奢侈。仅使用几行高级代码就可以表达非常复杂的逻辑并执行复杂的转换。API的最大好处是用户不需要考虑执行，可以让优化器找出执行查询的最有效方式。高效的查询执行经常是一个需求，不仅因为资源可能变得昂贵，而且因为它通过减少最终用户等待计算结果的时间而使最终用户的工作更加舒适。</p><p id="5900" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Spark SQL优化器确实非常成熟，尤其是现在即将推出的3.0版本，它将引入一些新的内部优化，如动态分区修剪和自适应查询执行。优化器在内部处理一个查询计划，并且通常能够通过各种规则来简化和优化它。例如，它可以改变一些转换的顺序，或者如果最终输出不需要这些转换，就完全丢弃它们。尽管所有的优化都很聪明，但是仍然存在人脑可以做得更好的情况。在本文中，我们将研究其中一个案例，看看如何使用一个简单的技巧，让Spark实现更高效的执行。</p><p id="3032" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该代码在Spark的当前版本2.4.5(编写于2020年6月)中进行了测试，并对照Spark 3.0.0-preview2进行了检查，以查看即将到来的Spark 3.0中可能的变化。</p><h1 id="7489" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">模型示例</h1><p id="13c6" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">现在让我首先介绍一个简单的例子，我们将努力实现有效的执行。假设我们有json格式的数据，结构如下:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="0761" class="mk lf it mg b gy ml mm l mn mo">{"id": 1, "user_id": 100, "price": 50}<br/>{"id": 2, "user_id": 100, "price": 200}<br/>{"id": 3, "user_id": 101, "price": 120}<br/>{"id": 4, "price": 120}</span></pre><p id="0ef4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个记录就像一个交易，所以<em class="mp"> user_id </em>列可能包含许多重复的值(可能包括空值),除了这三列之外，还有许多其他字段描述交易。现在，我们的查询将基于两个相似聚合的并集，其中每个聚合都有一些不同的条件。在第一个聚合中，我们希望获得<em class="mp">价格</em>之和小于50的用户，在第二个聚合中，我们获得<em class="mp">价格</em>之和大于100的用户。此外，在第二个聚合中，我们希望只考虑<em class="mp"> user_id </em>不为空的记录。这个模型示例只是实践中可能出现的更复杂情况的简化版本，为了简单起见，我们将在整篇文章中使用它。下面是如何使用PySpark的DataFrame API来表达这种查询的基本方法(非常类似地，我们也可以使用Scala API来编写它):</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="01ca" class="mk lf it mg b gy ml mm l mn mo">df = spark.read.json(data_path)</span><span id="d27b" class="mk lf it mg b gy mq mm l mn mo">df_small = (<br/>  df<br/>  .groupBy("user_id")<br/>  .agg(sum("price").alias("price"))<br/>  .filter(col("price") &lt; 50)<br/>)</span><span id="2851" class="mk lf it mg b gy mq mm l mn mo">df_big = (<br/>  df<br/>  .filter(col("user_id").isNotNull())<br/>  .groupBy("user_id")<br/>  .agg(sum("price").alias("price"))<br/>  .filter(col("price") &gt; 100)  <br/>)</span><span id="e10b" class="mk lf it mg b gy mq mm l mn mo">result = df_small.union(df_big)</span></pre><h1 id="59df" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">解释计划</h1><p id="d25c" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">实现良好查询性能的关键是理解和解释查询计划的能力。计划本身可以通过调用Spark数据帧上的<em class="mp"> explain </em>函数来显示，或者如果查询已经运行(或已经完成),我们也可以转到Spark UI并在SQL选项卡中找到计划。</p><figure class="mb mc md me gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mr"><img src="../Images/e85477b55a3f0010e56c85537705db21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0iDCUauhtPDBSkr9JvQODw.png"/></div></div></figure><p id="3830" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SQL选项卡列出了集群上已完成和正在运行的查询，因此通过选择我们的查询，我们将看到物理计划的图形表示(此处我删除了指标信息以缩小图形):</p><figure class="mb mc md me gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mz"><img src="../Images/fe9b95858f1a5e2e396310a04c1057e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDoWOu_1_Z835g5SKVrMug.png"/></div></div></figure><p id="acc5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该计划有一个树形结构，其中每个节点代表一些操作者，这些操作者携带一些关于执行的信息。我们可以看到，在我们的例子中有两个分支，根在底部，叶在顶部，执行从那里开始。叶子<em class="mp">扫描json </em>代表从源读取数据，然后有一对<em class="mp">哈希聚合</em>操作符负责聚合，在它们之间有<em class="mp">交换</em>代表洗牌。<em class="mp">过滤器</em>操作符携带关于过滤条件的信息。</p><p id="e709" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该计划具有union操作的典型形状，union中的每个数据帧都有一个新的分支，由于在我们的示例中两个数据帧都基于同一个数据源，这意味着数据源将被扫描两次。现在我们可以看到有改进的空间。只扫描一次数据源可以得到很好的优化，尤其是在I/O开销很大的情况下。</p><p id="9fc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，我们在这里想要实现的是重用一些计算—扫描数据并计算聚合，因为这些操作在两个数据帧中都是相同的，原则上只需要计算一次就足够了。</p><h1 id="fcf0" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">贮藏</h1><p id="8490" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在Spark中重用计算的一个典型方法是使用缓存。有一个函数<em class="mp">缓存</em>可以在数据帧上调用:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="d788" class="mk lf it mg b gy ml mm l mn mo">df.cache()</span></pre><p id="fb66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一种惰性转换，这意味着在我们调用一些操作后，数据将被放入缓存层。缓存是Spark中非常常用的技术，但是它有其局限性，尤其是当缓存的数据很大而集群上的资源有限时。还需要注意的是，将数据存储在缓存层(内存或磁盘)会带来一些额外的开销，而且操作本身也不是免费的。在整个数据帧<em class="mp"> df </em>上调用<em class="mp"> cache </em>也不是最佳选择，因为它会尝试将所有列放入可能不必要的内存中。更谨慎的方法是选择将在后续查询中使用的所有列的超集，然后在选择之后调用<em class="mp">缓存</em>函数。</p><h1 id="8ae7" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">交换再利用</h1><p id="d455" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">除了缓存，还有一种技术在文献中没有很好地描述，这种技术基于重用<em class="mp">交换</em>。<em class="mp">交换</em>操作符代表shuffle，这是集群上的一种物理数据移动。当数据必须重新组织(重新分区)时会发生这种情况，这通常是聚合、连接和其他一些转换所需要的。关于shuffle重要的一点是，当数据被重新分区时，Spark将始终以<em class="mp"> shuffle write </em>的形式将其保存在磁盘上(这是一种内部行为，不受最终用户的控制)。因为它保存在磁盘上，所以以后需要时可以重复使用。如果Spark找到机会，它确实会重用这些数据。每当Spark检测到从叶节点到一个<em class="mp">交换</em>的同一个分支在计划中的某个地方重复时，就会发生这种情况。如果存在这种情况，这意味着这些重复的分支代表相同的计算，因此只计算一次然后重用它就足够了。我们可以从计划中看出Spark是否找到了这样的案例，因为那些分支会像这样合并在一起:</p><figure class="mb mc md me gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi na"><img src="../Images/85842d0de1efd81a35f478711972a04c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7dMDbyr0J6anHbWwVg7w.png"/></div></div></figure><p id="dd82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，Spark没有重用交换，但是通过一个简单的技巧，我们可以促使他这样做。在我们的查询中没有重用<em class="mp">交换</em>的原因是对应于过滤条件<em class="mp"> user_id </em>的右侧分支中的<em class="mp">过滤器</em>不为空。过滤器实际上是我们在联合中的两个数据帧的唯一区别，所以如果我们可以消除这种区别并使两个分支相同，Spark将处理剩下的部分，并将重用<em class="mp">交换</em>。</p><h1 id="6c51" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">调整计划</h1><p id="2401" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">怎么才能让树枝一样呢？好吧，如果唯一的区别是过滤器，我们当然可以交换转换的顺序，并在聚合之后调用过滤器，因为这不会对将要产生的结果的正确性产生任何影响。然而有一个陷阱！如果我们像这样移动过滤器:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ab29" class="mk lf it mg b gy ml mm l mn mo">df_big = (<br/>  df.groupBy("user_id")<br/>  .agg(sum("price").alias("price"))<br/>  .filter(col("price") &gt; 100)<br/>  <strong class="mg iu">.filter(col("price").isNotNull())</strong><br/>)</span></pre><p id="71ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且检查最终的查询计划，我们会看到计划根本没有改变！原因很简单——优化器将过滤器移回了原处。</p><p id="fad8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，理解有两种主要类型的查询计划是有好处的——逻辑计划和物理计划。并且逻辑计划在变成物理计划之前经历优化阶段，物理计划是将被执行的最终计划。当我们更改一些转换时，它会反映在逻辑计划中，但是我们会失去对后续步骤的控制。优化器将应用一组优化规则，这些规则主要基于一些启发式规则。与我们的示例相关的规则被称为<em class="mp"> PushDownPredicate </em>，该规则确保过滤器被尽快应用，并被推到更靠近源的位置。它基于这样一种思想，即首先过滤数据，然后在缩减的数据集上进行计算会更有效。这条规则在大多数情况下确实非常有用，但在这种情况下，它却对我们不利。</p><p id="e17f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了实现<em class="mp">过滤器</em>在计划中的自定义位置，我们必须限制优化器。从Spark 2.4开始，这是可能的，因为有一个配置设置允许我们列出所有我们想从优化器中排除的优化规则:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c4c9" class="mk lf it mg b gy ml mm l mn mo">spark.conf.set(<br/>"spark.sql.optimizer.excludedRules",     "org.apache.spark.sql.catalyst.optimizer.PushDownPredicate")</span></pre><p id="d8ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在设置了这个配置并再次运行查询之后，我们将会看到现在过滤器保持在我们需要的位置。这两个分支变得完全相同，Spark现在将重用交换！数据集现在将只扫描一次，计算聚合也是如此。</p><p id="5907" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Spark 3.0中，情况有了一点改变，优化规则现在有了一个不同的名称—<em class="mp">PushDownPredicate</em><strong class="kk iu"><em class="mp">s</em></strong>，并且有一个额外的规则也负责推送一个过滤器<em class="mp">pushpredictethrunjoin</em>，所以我们实际上需要排除这两个规则来实现预期的目标。</p><h1 id="9624" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">最后的想法</h1><p id="d986" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我们可以看到，通过这种技术，Spark开发人员给了我们控制优化器的能力。但是权力也伴随着责任。让我们列出使用这种技术时需要牢记的几点:</p><ul class=""><li id="588e" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">当我们停止<em class="mp">pushdownpreditate</em>时，我们将负责查询中的所有过滤器，而不仅仅是我们想要重新定位的过滤器。可能还有其他过滤器，例如分区过滤器，需要尽快安装，因此我们需要确保它们的位置正确。</li><li id="5343" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">限制优化器和处理过滤器是用户方面的一些额外工作，所以最好是值得的。在我们的模型示例中，加速查询的潜力将出现在I/O很昂贵的情况下，因为我们将实现只扫描一次数据。例如，如果数据集有许多列，那么json或csv之类的非分栏文件格式可能就是这种情况。</li><li id="a869" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">此外，如果数据集很小，可能不值得额外控制优化器，因为简单的缓存就可以完成这项工作。然而，当数据集很大时，在缓存层存储数据的开销将变得明显。另一方面，重用的<em class="mp">交换</em>不会带来额外的开销，因为计算出的洗牌将被存储在磁盘上。</li><li id="7ef4" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">这种技术是基于Spark的内部行为，它没有官方文档，如果这个功能有什么变化，可能更难发现。在我们的示例中，我们可以看到Spark 3.0中实际上有一个变化，一个规则被重命名，另一个规则被添加。</li></ul><h1 id="5b9a" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="4d5a" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我们已经看到，要获得最佳性能，可能需要理解查询计划。Spark optimizer通过使用一组启发式规则来优化我们的查询，做得非常好。然而，在有些情况下，这些规则会错过最佳配置。有时重写查询已经足够好了，但有时却不行，因为通过重写查询，我们将实现不同的逻辑计划，但我们无法直接控制将要执行的物理计划。从Spark 2.4开始，我们可以使用一个配置设置<em class="mp"> excludedRules </em>，它允许我们限制优化器，从而将Spark导航到一个更加定制的物理计划。</p><p id="5597" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在许多情况下，依赖优化器会产生一个执行效率相当高的可靠计划，但是，在大多数情况下，在性能关键型工作负载中，可能值得检查最终计划，看看我们是否可以通过控制优化器来改进它。</p></div></div>    
</body>
</html>