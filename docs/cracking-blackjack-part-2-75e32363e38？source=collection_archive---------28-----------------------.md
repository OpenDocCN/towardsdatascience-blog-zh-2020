# 破解 21 点——第二部分

> 原文：<https://towardsdatascience.com/cracking-blackjack-part-2-75e32363e38?source=collection_archive---------28----------------------->

## [破解二十一点](https://towardsdatascience.com/tagged/Cracking-Blackjack)

## 强化学习和开放式健身房简介

嗨！

如果您还没有这样做，请在继续之前阅读[第 1 部分](https://medium.com/@ppalanic/cracking-blackjack-part-1-31da28aeb4bb?source=friends_link&sk=ce1251c59477bf8a6977ef232fd1d234)。本文的其余部分将假设您已经阅读并理解了第 1 部分。

![](img/a7978b05f97abdb221b52a213f2da86f.png)

图片来自 [Unsplash](https://unsplash.com/photos/P787-xixGio)

## 强化学习周期

![](img/c93862ffd54fc51e34dcc620c9b79430.png)

作者制作的图像

为了推进我们使用强化学习“破解”21 点的任务，我们必须理解上图。下面，我将图像分解成它的组成部分，以及它们在 21 点中的对应部分:

*   代理:执行学习过程的人工智能抽象。在 21 点中，经纪人就是玩家。
*   **环境**:代理可以与之交互的所有可能情况的集合，每种情况下可用的动作，以及与每种情况相关的结果(奖励+惩罚)。在 21 点中，这是所有可能的玩家手牌、庄家向上的牌、玩家动作(打或站)和结果(赢/输/平)的集合。
*   **陈述:**有助于构成环境的“情境”。在我们的 21 点版本中，状态将由玩家的手牌值和庄家的上牌值组成。球员/经纪人在做决定的时候只能看到这两样东西。
*   **动作**:代理在与某个给定状态交互时可用的选项。在我们的 21 点版本中，可用的操作是击打和站立。
*   **奖励**:在某种给定的状态下，代理人收到的对其行为的反馈。这是由程序员定义的，以帮助向代理显示哪个结果是更好的，代理可以相应地开始调整它的动作。在我们的 21 点游戏中，赢=+100 美元，输=-100 美元，平=+0 美元。决定胜负/平手的逻辑在环境中；奖励是分配给每个结果的数值。

## **这些组件如何在 21 点中一起工作**

*   一轮 21 点开始:2 张牌发给玩家和庄家，代理只能看到自己的牌和庄家的一张牌。环境通过向代理发送一个**初始状态**(玩家手牌值+庄家升牌值)来对此建模。

![](img/ab03619e8fc71f9295553d6f1c37512f.png)

作者制作的图像

*   玩家有两个动作:打或站。代理使用其**策略**(在[第一部分](https://medium.com/@ppalanic/cracking-blackjack-part-1-31da28aeb4bb?source=friends_link&sk=ce1251c59477bf8a6977ef232fd1d234)中讨论)基于当前状态选择一个动作。

![](img/cdc0c8e988bbcaabea277062dbf3a1f4.png)

作者制作的图像

*   代理选择的状态+动作被发送回环境。

![](img/0092058bcc19a2c57c2d6bdee307f339.png)

作者制作的图像

*   在给定状态的情况下，环境在内部处理代理的动作。它为“击中”发任何新牌。如果合适的话，它会计算谁赢了这一轮(玩家站着，21 点！，或者半身像)。

![](img/f4a6cb4fe0dd5e0df2ce3e962d66239c.png)

作者制作的图像

*   如果这轮现在结束了，环境发送给代理一个代表下一轮 21 点的**新状态**，以及前一轮的奖励。代理使用该结果来更新其策略。

![](img/37de8745e5c455b227102621d876fe33.png)

作者制作的图像

*   如果代理在当前回合中有更多的动作要做，则环境向代理发送一个状态，该状态具有更新的玩家手牌值和+$0 的奖励，因为该回合尚未结束。

## 我们的代理如何从这个循环中学习

![](img/c93862ffd54fc51e34dcc620c9b79430.png)

作者制作的图像

上面的循环意味着这个循环将无限期地继续下去，那么实际的学习在哪里/什么时候发生呢？

单个周期可以表示为一个序列:

状态→行动→奖励

当我们围绕这个循环进行循环时，我们可以记录这些状态/动作/回报元组。在循环中经历大约“n”个循环，并在进行过程中记录状态/动作/奖励元组，这被称为**情节。**

在我们完成了我们期望的循环次数(假设 50 次)之后，我们的代理将遍历状态/动作/回报元组，并相应地更新它的策略。在下一篇文章中，我们将深入研究我们的强化学习算法将如何指导我们的代理使用这些状态/动作/奖励元组来优化它的策略。

## 偶发任务与连续任务

一集应该由多少个循环组成？在我的 21 点环境中，我认为一轮 21 点是一集。这意味着每集通常会有 1-3 个状态/行动/奖励元组，因为代理人在每轮 21 点中可能只会做出 1-3 个立/打决定(在极少数情况下会做出更多决定)。

幸运的是，我们在 21 点中有一个“回合”的概念来帮助定义一集。然而，其他上下文，如应用强化学习来预测股票市场，没有“回合”来帮助定义情节。股市是没有起点和终点的，在定义剧集的时候你必须要有创意！

由于这些原因，使用强化学习预测股票市场将被认为是一个持续的任务，而破解 21 点将被认为是一个阶段性的任务。

## 为什么要使用 OpenAI Gym 重建我们的 21 点环境？

我们在[第 1 部分](https://medium.com/@ppalanic/cracking-blackjack-part-1-31da28aeb4bb?source=friends_link&sk=ce1251c59477bf8a6977ef232fd1d234)中设置的 21 点游戏没有准确模拟强化学习周期。我们遗漏了关键的部分，比如记录状态/动作/回报元组和定义一集。

我们将在 [OpenAI Gym](https://gym.openai.com/) 的框架内构建新的 21 点环境。这个框架提供了一种定义我们环境的标准化方法，这样我们就可以很容易地用其他自己编写的和开源的强化学习算法进行实验。这个框架还提供了有用的 API 来更全面地定义我们的状态、动作和奖励。

## 我们新的 OpenAI 健身房 21 点环境概述

为了构建我们的环境，我们将在 OpenAI Gym 环境模板中定义函数。这些函数是对强化学习的循环过程建模的必要部分，如下所示:

*   `__init__()`:初始化 21 点牌组，奖励空间(+$100，+$0，或-$100)，动作空间(击打或站立)，观察空间(或所有可能状态的集合。每个状态由玩家手牌值和庄家升牌组成)。
*   `reset()`:重置剧集，给代理初始状态。
*   `step(action)`:代理使用我们的算法选择一个动作，并使用这个函数将这个动作发送到环境中。这个函数模拟环境的过程(击中时发新卡，计算奖励等)。然后，它返回一个新的状态，奖励，以及该集是否完成。
*   `render()`(推荐):显示一集不同阶段的状态/动作/奖励元组等相关信息的功能。无助于循环往复。

## 使用 OpenAI 健身房 21 点环境

在深入研究这些函数的代码之前，让我们看看这些函数如何一起工作来模拟强化学习周期。在下面的模拟中，我们使用我们的 OpenAI 健身房环境和随机选择击中/站立的策略来寻找每轮的平均回报。

![](img/46e6ee968345818e64acb5569e09d965.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

*   首先，我们初始化环境:`env = BlackjackEnv()`。该行将运行前面描述的`__init__()`功能。
*   然后，我们运行 1000 集(或 1000 轮 21 点)的模拟。
*   在每集开始时，我们调用`env.reset()`函数给代理一个新的初始状态，以确定 hit/stand。
*   在`step()`函数中将环境的`env.done`值更改为`True`之前，代理会随机选择击中/站立作为其对`step()`函数的动作。在下一篇文章中，我们的算法将改进拣选动作的过程。
*   一旦`env.done`到了`True`，这一集就结束了，这一集的总奖励加到总奖励里，下一集开始。在下一篇文章中，我们的算法将向代理展示如何在每一集之后改进其策略。

## 构建开放式健身房 21 点环境

下面的代码片段包含了我在 OpenAI 健身房环境中实现的 21 点。现在先浏览一下，在完成这篇文章后再详细浏览。我在下面详细介绍了每个功能的关键线路。这些函数重用了第 1 部分中内置的一些函数和类。

`**__init__()**` **:**

首先，我们利用 OpenAI Gym 的`spaces`功能来定义我们的动作空间:击打或站立。我们使用两个二进制选择的离散集合:0 表示击中，1 表示站立。

![](img/083c942437107dd139468c088b9b5390.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

接下来，我们定义观察空间。观察空间是代理可以观察和响应的所有可能状态的集合。在我们的 21 点版本中，状态是玩家的手牌值和庄家的上牌。

观察空间是强化学习循环中**环境**的一个组成部分。其他组件，如计算奖励和玩家手牌值，在`step()`函数中实现或从第 1 部分重用。

我们考虑的可能牌值范围是 3 到 20。2 是不可能的，因为它需要两个 a，而我们计算手牌值的逻辑不会将两个 a 评估为 2。对于手牌值 21(21 点！)和 beyond(半身像)，代理人不需要做任何决定。

所有可能的庄家大牌从 2 到 a。Ace 仅由 11 表示，而不是 1。在我们的算法中，代理在学习和优化时并不关心庄家的 Ace 评估为 1 还是 11。

玩家的牌值范围是 3 到 20，也就是 18 个离散值的范围。庄家升牌的范围是 2 到 Ace，这是一个 10 个离散值的范围。我们再次使用 OpenAI Gym 的`spaces`函数来表示这一点。我们还使用`spaces.Tuple`来表示这两个范围必须相乘才能得到所有可能的状态。

![](img/6a592d17866bcd8b077f667a3a540291.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

最后，将环境的`done`变量初始化为`False`，以确保在一集结束前玩满一轮 21 点。

![](img/9a1611566da6e98cfe58d48b02240488.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

`**reset()**`T14:

如前所述，该函数重新使用第 1 部分中的函数来重置和重新洗牌，向玩家和庄家发 2 张牌，并评估玩家的手牌价值。

该函数还将初始状态作为一个包含两个值的`numpy`数组返回:玩家的手牌值和庄家的上牌。

![](img/2f0a2b4181719a219ecaec6f5ffce673.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

`**step(action)**` **:**

该函数接收代理的动作。调用`_take_action(action)`函数来发一张新卡，如果动作被击中，则重新计算玩家的手牌值。

![](img/9ba97a4dc812578520669f893d56fcf1.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

然后，该功能会在服用`action`后查看二十一点的插曲现在是否完成。

![](img/8bd12bb2e688fc00b844804526034576.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

如果这一集已经结束，那么这个函数将重新使用第 1 部分中的逻辑来决定赢家和奖励。

最后，该函数返回新状态、奖励以及该集是否完成。`{}`是一个返回值，用来描述函数中发生了什么，但是我们现在让它为空。

![](img/afb71ea2a149ec066a624c934264ea2c.png)

图片作者。查看代码[此处](https://github.com/adithyasolai/Monte-Carlo-Blackjack/blob/master/MC_Blackjack_Part_2/Monte%20Carlo%20Blackjack%20Part%202.ipynb)。

`**render()**` **:**

这个函数只是使用简单的`print()`函数来显示玩家的手牌值、玩家手中的牌、庄家的上牌以及`env.done`的值。

该函数主要用于调试。

## 下一步是什么

在接下来的文章中，我们将利用我们定制的 OpenAI Gym 环境和强化学习的新知识来设计、实现和测试我们自己的强化学习算法！

我们将使用首次访问蒙特卡罗方法对我们的算法进行建模，并调整关键杠杆，如γ(贴现率)、α(学习率)和ε(探索与利用)以最大化回报。

我们一直在讨论的一切最终将在下一篇文章中汇集在一起。我们将最终指导一个人工智能代理有机地学习一个最优策略来**破解 21 点**！

感谢您的阅读！

我真的很感激任何形式的反馈！我想成为一个更一致、更有效的内容创作者。

你学到什么了吗？我很难理解吗？

欢迎在下面留言或发电子邮件给我，地址是 adithyasolai7@gmail.com！

[本项目 GitHub 回购。](https://github.com/adithyasolai/Monte-Carlo-Blackjack)

## 请继续阅读！

[*此处第 3 部分。*](/cracking-blackjack-part-3-8fd3a5870efd?source=friends_link&sk=98055a00e2e685239d7148524a2d0b17)