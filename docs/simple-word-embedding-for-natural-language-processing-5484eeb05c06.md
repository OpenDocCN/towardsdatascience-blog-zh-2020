# 自然语言处理中的简单单词嵌入

> 原文：<https://towardsdatascience.com/simple-word-embedding-for-natural-language-processing-5484eeb05c06?source=collection_archive---------29----------------------->

单词嵌入是一种将单词或短语表示为实数向量的语言建模技术。这些单词被组合在一起以获得具有相似含义的单词的相似表示。单词嵌入学习单词之间的关系来构建表示。这是通过各种方法实现的，如共生矩阵、概率建模、神经网络。它已经成为自然语言处理的基础知识之一。

![](img/8707a67b86976e8131b625c4132592f2.png)

乔恩·泰森在 Unsplash 上拍摄的照片

一键向量是单词的简单表示之一。每个单词都可以用一个热向量来表示，但是随着单词数量的增加，维数也会增加。单词嵌入通过使用各种方式来减少单词向量的维数，例如单词如何集体出现，如 King、Queen 或可以交替使用的单词，如 car、vehicle 等。所以相似的词用相似的向量表示。这降低了向量的维数。虽然一个热点向量很容易构建，但通常没有一个好的选择来表示大量的单词。这是因为它确实允许表达语料库中单词之间的相似性。

Word2Vec，GloVe 是流行的单词嵌入。伯特是一个最新的单词嵌入。

单词嵌入分为两种类型

基于频率的嵌入—计数向量、共现向量、哈希向量器、TF-IDF

预训练的单词嵌入— Word2Vec、GloVe、BERT、fastText

本文讨论了基于频率的嵌入及其在自然语言处理问题中的应用

**频率基嵌入**

基于频率的嵌入根据单词在文本/文档中出现的频率对文本进行矢量化。使用频率基嵌入的一个例子可以在文本分类中使用。考虑到我们将文本分为不同的技术、政治、经济等类别。使用基于频率的矢量器，例如计数矢量器，对文本中的单词进行矢量化。然后根据频率矢量化得到的向量对文档进行训练。编写的模型将能够预测文档类型。这是因为有些词在每种类型的文档中出现的频率会更高。与政治或经济文档相比，技术文档会有一套不同的词汇。

**计数矢量器**

CountVectorizer 是一个非常简单的矢量器，它获取文本中单词的频率。CountVectorizer 用于将文本文档集合转换为单词/单词计数。

考虑文本“杯子在桌子上”

Data =['The '，' cup '，' is '，' present '，' on '，' The '，' table']

这可以矢量化为

![](img/7233a8887bcd0283129165a5553a54dc.png)

计数矢量化可以通过 Sklearn 的 CountVectorizer 完成。下面是使用它的例子。计数矢量器根据单词在文档中出现的频率对文档进行编码。CountVectorizer 给出了文档中单词/ n 元语法的频率的稀疏矩阵。这对于创建文档分类、文档聚类、情感分析

![](img/c5fc69cce022855a17dfc57ff18c7aa7.png)

**同现向量机或 N 元语法**

ngram_range 的默认参数是(1，1)，它从文档中一次取一个单词的频率。ngram 范围给出了单词的下限和上限。这是必需的，因为有些单词如果单独使用，并不能给出完整的意思。例如，以文本“好书”、“好书”、“一美分”、“相当便宜”为例。在这些例子中，单词“nice read”的集合给出了比单个单词更完整的意思。

![](img/5dd539290595bc096fa0b7d5a39673e4.png)

计数矢量器从文本中提取特征的简单方法之一，这些特征可以在以后的模型开发中使用。

**哈希矢量器**

HasingVectorizer 类似于 CountVectorizer，但它不存储文档的词汇。它使用散列来查找令牌字符串名称到特征索引的映射。

![](img/7a8173a4454b0c654cc4befeb0952aba.png)

哈希矢量器将文档转换为包含标记出现频率的稀疏矩阵。根据 norm 参数，这被归一化为令牌频率。可以是 l1 范数，也可以是 l2 范数。

哈希矢量器有几个优点。

因为不需要存储词汇词典，所以减少了存储器消耗。

因为它不保持任何状态，所以浸酸和去浸酸很快。

由于没有使用状态，它可用于流式或并行管道数据。

这有一些缺点

不可能检索字符串名称，因为它们没有被存储。

如果特征的数量较少，则可能存在特征共谋。

没有 IDF 加权，因为它会呈现此状态。

当您拥有大型数据集时，最好使用哈希矢量器，因为它可以减少内存消耗并加快算法速度。如果数据集很小，那么最好使用计数矢量器，因为它很简单。

**TF-IDF 矢量器**

简单的基于频率的矢量器有时可能达不到目的，因为它只给出单词的频率。TF-IDF 矢量器试图解决这个问题。TF-IDF 是“词频—逆文档频率”的首字母缩写。

TF —术语频率:这是单词在文档中出现的次数

TF =(单词在文档中出现的次数)/(文档中的单词数)

IDF-逆文档频率:这将缩减文档中频繁出现的单词。

IDF = log_e(文档数/出现该术语的文档数)

基本上，TF-IDF 增加了对文档重要的单词或标记的值。

搜索引擎使用 TF-IDF 对与查询相关的文档进行排序。

![](img/42acc800bc7d596d42497fb2b22b0def.png)![](img/9d59c38bdcc7ae52d828c7dde75229f9.png)![](img/03f74167ac670c1a6ad12fd95907298d.png)

CountVectorizer 给出了单词使用的绝对数量，但是 TF-IDF 给出了单词在文档中的重要性。

考虑一个文本分类问题。有些词在某种文件中很重要。考虑到关于药物的文档中会有一些医学术语，这些术语在文档中会很重要，TF-IDF 会突出显示这些术语。类似地，考虑与体育相关的文档，TF-IDF 将突出显示与体育和游戏相关的术语。

**使用单词嵌入解决简单文本分类问题**

让我们看看简单的测试分类问题，并使用单词嵌入来解决它。

这是一个垃圾短信分类的问题。这是从拥有 CC0 许可证的 Kaggle 获取的数据集。

![](img/0eae824687b52f39655f2990d8e6092e.png)

将信息读入熊猫数据框。

![](img/24978687a9902eddb9a98601337d65e3.png)

**使用计数矢量器作为单词嵌入**

用计数矢量器拟合单词嵌入

![](img/20c6eccaf171729a76f92dffe6a61b60.png)

使用 Sci-kit 中的标签编码器学习编码标签

![](img/064d46bc79f222295d50bfefe99cab19.png)

将数据集分为训练数据集和测试数据集。

![](img/6708975c5bd2fdef0c30a82d01f8a690.png)

现在我们使用 Keras 创建一个简单的神经网络模型。

![](img/7d307415cfb9c9e643bda58fa26771df.png)

我们创建了一个简单的模型，有两个隐藏层，分别是 64 个节点和 32 个节点。我们已经使用 Softmax 层获得了火腿和垃圾邮件的输出。

我们使用 adam optimizer 进行优化，使用分类交叉熵进行损失。

![](img/5780a43b5a58ede8b79368e2da568104.png)

我们可以使用 evaluate 函数来评估我们的模型

![](img/fe640234e73dec7f0fe44810d0e00195.png)![](img/8d927c314b0e590a29a8c3cb74de8e2b.png)

**用 TF-TDF 做词嵌入** 用 TF-TDF 拟合向量

![](img/4d29c2fefeb907df5e7417ffa49730c1.png)

使用 Sci-kit 中的标签编码器学习编码标签

![](img/064d46bc79f222295d50bfefe99cab19.png)

将数据集分为训练数据集和测试数据集。

![](img/6708975c5bd2fdef0c30a82d01f8a690.png)

现在我们使用 Keras 创建一个简单的神经网络模型。

![](img/7d307415cfb9c9e643bda58fa26771df.png)

我们创建了一个简单的模型，有两个隐藏层，分别是 64 个节点和 32 个节点。我们已经使用 Softmax 层获得了火腿和垃圾邮件的输出。

我们使用 adam optimizer 进行优化，使用分类交叉熵进行损失。

评估模型

![](img/d8d147c60f839e8f50c62cc53c141000.png)![](img/598c9d43e9195eb7aac01bfc1507ed32.png)![](img/18df57bc3e7f90e3432fe4ce0343cf59.png)

预测函数可用于进行预测。

得到模型的概要。

![](img/3fdc89c2f35f2a7f82030109997d73fb.png)

**使用哈希矢量器作为单词嵌入**

用哈希矢量器拟合矢量

![](img/91979c6b11e1959bece50e5ef7229ce0.png)

标签编码器的使用和模型的创建保持不变。

![](img/3d6687a0a40390ca5ac86f64327d9eac.png)![](img/dffc55ecade3938d59efbbf6e60eaa7b.png)![](img/cb6cfd5d440b9a1dbb006ce684d6c6c4.png)

我们看到参数的数量减少了，这取决于所用特征的大小。

**结论**

计数矢量器、散列矢量器和 TF-IDF 矢量器可用于为自然语言处理任务的单词创建单词嵌入。

在哈希矢量器中，参数所需的内存较少。参数的数量取决于特征的大小。此外，在哈希矢量器中，由于词汇不存储在哈希矢量器中，因此节省了词汇的存储量。

对于稍微复杂的分类工作，TF-IDF 获得了更好的准确性，因为单词在特定类型的文档中的出现被赋予了重要性。

**作者**

斯里尼瓦斯·查克拉瓦蒂——srinivas.yeeda@gmail.com

钱德拉舍卡——chandru4ni@gmail.com