<html>
<head>
<title>Video Prediction using Deep Learning and PyTorch (-lightning)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习和 PyTorch (-lightning)的视频预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/video-prediction-using-convlstm-with-pytorch-lightning-27b195fd21a2?source=collection_archive---------6-----------------------#2020-07-17">https://towardsdatascience.com/video-prediction-using-convlstm-with-pytorch-lightning-27b195fd21a2?source=collection_archive---------6-----------------------#2020-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3f77" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">卷积 LSTM 模型的简单实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1b102db379553a8309c1110d632ce847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UC8sdRKmFDxaOadunYzZ6Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">托马斯·威廉在 Unsplash<a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拍摄的照片</a></p></figure><p id="2220" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本指南中，我将向您展示如何使用一个<strong class="ky ir">自动编码器</strong> (seq2seq)架构对卷积长短期记忆(ConvLSTM)进行编码，该架构使用 MovingMNIST 数据集进行帧预测(但也可以轻松集成自定义数据集)。</p><p id="13d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法最初用于 2015 年<a class="ae kv" href="https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting.pdf" rel="noopener ugc nofollow" target="_blank">NIPS</a>的降水预报，此后通过 PredRNN、PredRNN++和 Eidetic 3D LSTM 等方法得到了广泛的扩展…</p><p id="3b78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还使用了<strong class="ky ir"> pytorch-lightning </strong>框架，这对于删除大量样板代码非常有用，并且可以轻松集成 16 位训练和多 GPU 训练。</p><p id="8535" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开始之前，我们将简要概述一下我们正在使用的库:</p><p id="1627" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">python=3.6.8<br/>torch=1.1.0<br/>torchvision=0.3.0<br/>pytorch-lightning=0.7.1<br/>matplotlib=3.1.3<br/>tensorboard=1.15.0a20190708</code></p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="a0a9" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">1.数据加载器</h1><p id="a5a5" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">从以下 repo<a class="ae kv" href="https://github.com/tychovdo/MovingMNIST" rel="noopener ugc nofollow" target="_blank">tychovdo/moving NIST</a>下载数据加载器脚本。<br/>该数据集最初是由<a class="ae kv" href="http://www.cs.toronto.edu/~nitish/unsup_video.pdf" rel="noopener ugc nofollow" target="_blank">开发并在此处</a>描述的，它包含 10000 个序列，每个序列长度为 20，帧大小为 64 x 64，显示 2 个数字在不同轨迹上移动(并重叠)。</p><p id="62d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要预先注意的是手指轨迹固有的随机性。我们确实期望这将成为我们将要描述的模型的一个主要障碍，并且我们也注意到更新的方法，例如变化的自动编码器，对于这种类型的任务可能是一个更有效的模型。</p><h1 id="0266" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">2.模型架构</h1><p id="921e" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们将使用的特定模型类型称为 seq2seq 模型，通常用于 NLP 或时序任务(它实际上是在 2016 年的谷歌翻译引擎中实现的)。</p><p id="f4b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于 seq2seq 的原论文有<a class="ae kv" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> Sutskever 等人，2014 </a>和<a class="ae kv" href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" rel="noopener ugc nofollow" target="_blank"> Cho 等人，2014 </a>。</p><p id="4a79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在其最简单的配置中，seq2seq 模型将一系列项目作为输入(例如单词、单词嵌入、字母等。)并输出另一个项目序列。对于机器翻译，输入可以是西班牙语单词序列，输出是英语翻译。</p><p id="dccd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将 seq2seq 模型分成三个部分，分别是</p><p id="051b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a)编码器(编码输入列表)<br/> b)编码器嵌入向量(整个输入序列的最终嵌入)<br/> c)解码器(将嵌入向量解码成输出序列)</p><p id="2e8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的机器翻译示例，这意味着:</p><ul class=""><li id="043d" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated">编码器通过顺序处理每个单词，将西班牙语序列作为输入</li><li id="26a9" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">编码器输出一个嵌入向量作为我们输入的最终表示</li><li id="926b" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">解码器将嵌入向量作为输入，然后输出英语翻译序列</li></ul><p id="4928" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望 a)部分和 c)部分对你来说比较清楚。就 seq2seq 模型的直觉而言，可以说最棘手的部分是编码器嵌入向量。你如何准确定义这个向量？</p><h1 id="547b" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">2.1 认识 RNN</h1><p id="4d31" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">在你继续前进之前，我强烈推荐以下<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">关于 RNN/LSTM 的精彩博文</a>。密切了解 LSTM 的是大多数 seq2seq 模型的必要先决条件！</p><p id="7e22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是常规 LSTM 电池的方程式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/417d7c53f943a9ec685ff7b2252ad6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJ-zlk484okQ4Uqs9mduVQ.png"/></div></div></figure><p id="7e21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中∘表示哈达玛乘积。</p><p id="8a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设你完全理解什么是 LSTM 细胞，以及细胞状态和隐藏状态是如何工作的。seq2seq 型号中的编码器和解码器通常由 LSTM 单元组成，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c74e7fe15ccaccc06b460de854103543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*5nvwJsH4EfONv_fdKNvobA.png"/></div></figure><h1 id="85bc" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">分解</h1><ul class=""><li id="af5b" class="nf ng iq ky b kz mv lc mw lf nv lj nw ln nx lr nk nl nm nn bi translated">LSTM 编码器由 4 个 LSTM 单元组成，LSTM 解码器由 4 个 LSTM 单元组成。</li><li id="931d" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">每个输入(单词或单词嵌入)与来自先前 LSTM 单元的隐藏状态(输出)一起被馈送到新的编码器 LSTM 单元</li><li id="8076" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">来自最终 LSTM 编码器单元的隐藏状态(通常)是编码器嵌入。它也可以是来自所有编码器 LSTM 单元的隐藏状态的整个序列(注意——这与注意不同)</li><li id="551a" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">LSTM 解码器使用编码器状态作为输入，并通过各种 LSTM 单元迭代处理这些状态以产生输出。这可以是单向的或双向的</li></ul><p id="b9e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">存在对标准 seq2seq 模型的几个扩展；最引人注目的是<a class="ae kv" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">注意力模块</a>。</p><p id="ceb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">讨论了 seq2seq 模型之后，让我们将注意力转向帧预测任务！</p><h1 id="3ab7" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">2.2 帧预测</h1><p id="d02e" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">帧预测与 seq2seq 原有的任务如机器翻译有着本质的区别。这是因为编码器和解码器中的 RNN 模块(LSTM)使用全连接层来编码和解码单词嵌入(表示为向量)。</p><p id="fa20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们处理了帧，我们就有了 2D 张量，为了按顺序编码和解码，我们需要扩展原始的 LSTM seq2seq 模型。</p><h1 id="dc34" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">ConvLSTM</h1><p id="55d7" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">这就是卷积 LSTM(convltm)的用武之地。在 2015 年的<a class="ae kv" href="https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting.pdf" rel="noopener ugc nofollow" target="_blank"> NIPS 上展示，ConvLSTM 修改了 LSTM 机制的内部工作方式，使用卷积运算而不是简单的矩阵乘法。让我们为 ConvLSTM 单元写出新的方程:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/95b9dfa0e81b02410d12adf1590fc5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dg0xsa0WMoAqsFZ4ttia_g.png"/></div></div></figure><p id="2454" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">∫表示卷积运算，∘表示阿达玛乘积，如前所述。</p><p id="3471" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你能发现这些方程和常规 LSTM 之间的细微差别吗？我们简单地替换四个门之间的乘法运算</p><p id="8d8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a)权重矩阵和输入<em class="nz"> (Wₓ xₜ </em>与<em class="nz">、Wₓ </em>、<em class="nz">和<br/>)b)权重矩阵和先前隐藏状态(<em class="nz"> Wₕ hₜ₋₁ </em>与<em class="nz"> Wₕ </em>、<em class="nz"> Hₜ₋₁ </em>)。<br/>否则，一切照旧。</em></p><p id="91b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你不想深究上面的方程，首先要注意的是，我们使用卷积(核)来处理我们的输入图像，以获得特征图，而不是从完全连接的层中获得的矢量。</p><h1 id="e0a5" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">2.2.2 n 步预测</h1><p id="f0ba" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">设计帧预测模型(使用 ConvLSTM)时，最困难的事情之一是定义如何产生帧预测。我们在这里列出了两种方法(但也存在其他方法):</p><ol class=""><li id="5267" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr oa nl nm nn bi translated">预测下一帧，并将其反馈到网络中，经过多个<em class="nz"> n </em>步骤，产生<em class="nz"> n </em>帧预测(自回归)</li><li id="bbd7" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">通过使 ConvLSTM 层的数量<em class="nz"> l </em>等于<em class="nz"> n </em>步的数量，一次性预测所有未来的时间步。因此，我们可以简单地使用每个解码器 LSTM 单元的输出作为我们的预测。</li></ol><p id="1965" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将重点关注数字 1，特别是因为它可以在未来产生任意数量的预测，而不必完全改变架构。此外，如果我们要预测未来的许多步骤，选项 2 在计算上变得越来越昂贵。</p><h1 id="177b" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">2.2.3 ConvLSTM 实施</h1><p id="f5a6" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">对于我们的 ConvLSTM 实现，我们使用来自<a class="ae kv" href="https://raw.githubusercontent.com/ndrplz/ConvLSTM_pytorch/master/convlstm.py" rel="noopener ugc nofollow" target="_blank"> ndrplz </a>的 PyTorch 实现</p><p id="492f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它看起来如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f814" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望您可以看到前面定义的方程是如何在上面的代码中编写的。</p><h1 id="d498" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">Seq2Seq 实施</h1><p id="a312" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们使用的具体架构如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/53188b9a2084a1c1c12c7873e2483e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1OcNQlsimCOOW6BriZENA.png"/></div></div></figure><h2 id="1475" class="oe me iq bd mf of og dn mj oh oi dp mn lf oj ok mp lj ol om mr ln on oo mt op bi translated">编码器和解码器</h2><p id="dbae" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">编码器和解码器使用两个 ConvLSTM 单元(encoder_1_convlstm、encoder_2_convlstm、decoder_1_convlstm、decoder_2_convlstm)。</p><h2 id="9633" class="oe me iq bd mf of og dn mj oh oi dp mn lf oj ok mp lj ol om mr ln on oo mt op bi translated">3D CNN</h2><p id="23dd" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们的最终 ConvLSTM 单元(decoder_2 <em class="nz"> convlstm)为每个预测帧(12、10、64、64、64)输出 _nf </em>特征图。</p><p id="ec91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们本质上是在做回归(预测像素值)，所以我们需要将这些特征图转换成实际的预测，类似于您在经典图像分类中所做的。</p><p id="e807" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现这一点，我们实现了一个 3D-CNN 层。3D CNN 层执行以下操作:</p><ol class=""><li id="5300" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr oa nl nm nn bi translated">将作为每个批次和时间步长的输入(nf、宽度、高度)</li><li id="7772" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">使用 3D 内核迭代所有<em class="nz"> n </em>个预测帧</li><li id="bc86" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">每个图像输出一个通道(1，宽度，高度)，即预测的像素值</li></ol><h2 id="a3e7" class="oe me iq bd mf of og dn mj oh oi dp mn lf oj ok mp lj ol om mr ln on oo mt op bi translated">乙状结肠层</h2><p id="08e2" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">最后，由于我们已经将像素值转换为[0，1]，我们使用 sigmoid 函数将我们的 3D CNN 激活转换为[0，1]。</p><p id="9e8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上就是这样！</p><p id="1f1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们为 seq2seq 模型定义 python 实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="f20d" class="md me iq bd mf mg na mi mj mk nb mm mn jw nc jx mp jz nd ka mr kc ne kd mt mu bi translated">3.培养</h1><p id="474a" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">也许你已经知道 pytorch-lightning 这个优秀的库，它在使用 pytorch 时基本上把所有的锅炉板工程从机器学习中去掉了，比如下面的命令:optimizer.zero_grad()、optimizer.step()。<br/>它还标准化了培训模块，并支持针对 Volta 架构 GPU 卡的轻松多 GPU 功能和混合精度培训。</p><p id="fbfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">pytorch-lightning 中有如此多的可用功能，我将尝试演示我创建的工作流，我认为它工作得相当好。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="2e60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">class MovingMNISTLightning</code>的大多数功能都是不言自明的。以下是整体工作流程:</p><ol class=""><li id="8f09" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr oa nl nm nn bi translated">我们实例化我们的类并定义所有相关的参数</li><li id="cbf9" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">我们采取训练步骤(针对每一批)</li><li id="cbdc" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">创建预测 y_hat</li><li id="b0c8" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">计算 MSE 损失—</li><li id="380b" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">在 tensorboard 中每 250 个全局步长保存一个带有输入和地面实况的预测可视化</li><li id="6019" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr oa nl nm nn bi translated">将每批的学习率和损耗保存到 tensorboard 中</li></ol></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="df7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们实际运行我们的<code class="fe ls lt lu lv b">main.py</code>脚本时，我们可以定义几个相关的参数。例如，如果我们想要使用 2 个 GPU 运行，混合精度且 batch_size = 16，我们只需键入:</p><pre class="kg kh ki kj gt oq lv or os aw ot bi"><span id="df4c" class="oe me iq lv b gy ou ov l ow ox">python main.py --n_gpus<strong class="lv ir">=</strong>2 --use_amp<strong class="lv ir">=</strong>True --batch_size<strong class="lv ir">=</strong>16</span></pre><p id="8eb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请随意尝试各种配置！</p><p id="4eb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们运行<code class="fe ls lt lu lv b">main.py</code>脚本时，我们会使用多重处理自动启动 tensorboard 会话，在这里您可以迭代跟踪我们模型的性能，还可以看到我们每 250 个全局步长的预测可视化。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="fddb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读这篇文章！我希望你喜欢它！</p><p id="5831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您对上述论文有任何问题或评论，请联系这里或在<a class="ae kv" href="https://twitter.com/HolmML" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上。你也可以在我的网页上找到更多的教程<a class="ae kv" href="https://holmdk.github.io/" rel="noopener ugc nofollow" target="_blank">https://holmdk.github.io/</a>。</p></div></div>    
</body>
</html>