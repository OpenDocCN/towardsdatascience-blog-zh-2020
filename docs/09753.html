<html>
<head>
<title>Creating DenseNet 121 with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 创建 DenseNet 121</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-densenet-121-with-tensorflow-edbc08a956d8?source=collection_archive---------4-----------------------#2020-07-11">https://towardsdatascience.com/creating-densenet-121-with-tensorflow-edbc08a956d8?source=collection_archive---------4-----------------------#2020-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="19a6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将了解如何使用 Tensorflow 从头开始创建 DenseNet 121 架构。</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/3b04618134ff04af70a8a60bda95cbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*u4hyohOF9SIRRLBAzqYXfQ.jpeg"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 1:DenseNet 中的各种块和层(来源:dense net 原始文件)</p></figure><p id="0289" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">DenseNet 论文链接:<a class="ae lx" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1608.06993.pdf</a></p><p id="b52e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">DenseNet(密集卷积网络)是一种架构，专注于通过使用层间更短的连接，使深度学习网络更深入，但同时使它们更有效地训练。DenseNet 是一个卷积神经网络，其中每一层都连接到网络中更深层的所有其他层，即第一层连接到第二层、第三层、第四层等等，第二层连接到第三层、第四层、第五层等等。这样做是为了实现网络层之间的最大信息流。为了保持前馈性质，每一层从所有前面的层获得输入，并将它自己的特征映射传递给它后面的所有层。与 Resnets 不同，它不是通过求和来组合特征，而是通过连接它们来组合特征。因此“第 I”层具有“I”个输入，并且由所有其前面的卷积块的特征图组成。它自己的特征地图被传递给所有下一个“I-i”层。这在网络中引入了'(I(I+1))/2 '连接，而不是像传统深度学习架构中那样仅仅是' I '连接。因此，它比传统的卷积神经网络需要更少的参数，因为不需要学习不重要的特征映射。</p><p id="06c0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">DenseNet 由两个重要的模块组成，而不是基本的卷积层和池层。它们是致密块体和过渡层。</p><p id="8fe6" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">接下来，我们看看所有这些块和层是什么样子，以及如何用 python 实现它们。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/a34b2a25f026a290a9507705b7594f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDS5oNj_reozEcv8bIaNgA.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 DenseNet121 框架(来源:原始 DenseNet 论文，由作者编辑)</p></figure><p id="2acf" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">DenseNet 从一个基本的卷积和池层开始。然后是一个密集块后面跟着一个过渡层，另一个密集块后面跟着一个过渡层，另一个密集块后面跟着一个过渡层，最后是一个密集块后面跟着一个分类层。</p><p id="c243" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">第一卷积块有 64 个大小为 7×7 的滤波器，跨距为 2。接下来是最大池层，最大池为 3×3，跨距为 2。这两行可以用 python 中的以下代码来表示。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="7ea9" class="mi mj it me b gy mk ml l mm mn">input = Input (input_shape)<br/>x = Conv2D(64, 7, strides = 2, padding = 'same')(input)<br/>x = MaxPool2D(3, strides = 2, padding = 'same')(x)</span></pre><p id="6f12" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">定义卷积块</strong> —输入后的每个卷积块有如下顺序:BatchNormalization，后面是 ReLU activation，然后是实际的 Conv2D 层。为了实现这一点，我们可以编写下面的函数。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="8cf1" class="mi mj it me b gy mk ml l mm mn"><em class="mo">#batch norm + relu + conv</em><br/><strong class="me iu">def</strong> bn_rl_conv(x,filters,kernel=1,strides=1):<br/>        <br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    x = Conv2D(filters, kernel, strides=strides,padding = 'same')(x)<br/>    <strong class="me iu">return</strong> x</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/a290249db37b4d8f217642f5ff32718d.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*0xqcgL7WiJXeoveIiyxGIg.jpeg"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 3。密集块(来源:DenseNet 纸-由作者编辑)</p></figure><p id="c88e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">定义密集块</strong> —如图 3 所示，每个密集块都有两个卷积，内核大小分别为 1x1 和 3x3。在密集块 1 中，重复 6 次，在密集块 2 中重复 12 次，在密集块 3 中重复 24 次，最后在密集块 4 中重复 16 次。</p><p id="0bd0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在密集块中，每个 1x1 卷积具有 4 倍数量的过滤器。所以我们使用 4 *滤镜，但是 3 * 3 滤镜只出现一次。此外，我们必须连接输入和输出张量。</p><p id="b4c9" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">使用“for 循环”，每个模块分别重复 6、12、24、16 次。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="7348" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">def</strong> dense_block(x, repetition):<br/>        <br/>   <strong class="me iu">for</strong> _ <strong class="me iu">in</strong> range(repetition):<br/>        y = bn_rl_conv(x, 4*filters)<br/>        y = bn_rl_conv(y, filters, 3)<br/>        x = concatenate([y,x])<br/>   <strong class="me iu">return</strong> x</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4c8114d03a3c8366299c0d1d6299ae9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*aKn1IYfvdnytDj3UxekLQg.jpeg"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 4:过渡层(来源:DenseNet 论文-作者编辑)</p></figure><p id="1c0d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">定义过渡层</strong> —在过渡层，我们要将通道数量减少到现有通道的一半。有一个 1x1 卷积层和一个 2x2 平均池层，步长为 2。在函数 bn_rl_conv 中已经设置了 1x1 的内核大小，所以我们不需要明确地再次定义它。</p><p id="11f0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在过渡层中，我们必须将通道减少到现有通道的一半。我们有输入张量 x，我们想知道有多少个通道，我们需要得到其中的一半。因此，我们可以使用 Keras backend (K)获取张量 x，并返回一个维度为 x 的元组。我们只需要该形状的最后一个数字，即过滤器的数量。所以我们加[-1]。最后，我们可以将这个数量的滤波器除以 2，得到想要的结果。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="3c01" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">def</strong> transition_layer(x):<br/>        <br/>    x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )<br/>    x = AvgPool2D(2, strides = 2, padding = 'same')(x)<br/>    <strong class="me iu">return</strong> x</span></pre><p id="bc00" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们已经完成了密集数据块和过渡层的定义。现在我们需要将密集的块和过渡层堆叠在一起。所以我们写了一个 for 循环来运行 6，12，24，16 次重复。因此循环运行 4 次，每次使用 6、12、24 或 16 中的一个值。这完成了 4 个密集块和过渡层。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="35b2" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">for</strong> repetition <strong class="me iu">in</strong> [6,12,24,16]:<br/>        <br/>    d = dense_block(x, repetition)<br/>    x = transition_layer(d)</span></pre><p id="b382" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">最后是 GlobalAveragePooling，接下来是最终的输出层。我们在上面的代码块中看到，密集块是用‘d’定义的，在最后一层，密集块 4 之后，没有过渡层 4，而是直接进入分类层。因此，“d”是应用 GlobalAveragePooling 的连接，而不是“x”上的连接。另一种方法是从上面的代码中删除“for”循环，然后一层接一层地堆叠这些层，而没有最后的过渡层。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="042a" class="mi mj it me b gy mk ml l mm mn">x = GlobalAveragePooling2D()(d)<br/>output = Dense(n_classes, activation = 'softmax')(x)</span></pre><p id="7942" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在我们已经把所有的模块放在一起了，让我们把它们合并起来，看看整个 DenseNet 架构。</p><p id="6baa" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">完成 DenseNet 121 架构:</strong></p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="2011" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">import</strong> <strong class="me iu">tensorflow</strong> <strong class="me iu">as</strong> <strong class="me iu">tf</strong><br/><strong class="me iu">from</strong> <strong class="me iu">tensorflow.keras.layers</strong> <strong class="me iu">import</strong> Input, Conv2D, BatchNormalization, Dense<br/><strong class="me iu">from</strong> <strong class="me iu">tensorflow.keras.layers</strong> <strong class="me iu">import</strong> AvgPool2D, GlobalAveragePooling2D, MaxPool2D<br/><strong class="me iu">from</strong> <strong class="me iu">tensorflow.keras.models</strong> <strong class="me iu">import</strong> Model<br/><strong class="me iu">from</strong> <strong class="me iu">tensorflow.keras.layers</strong> <strong class="me iu">import</strong> ReLU, concatenate<br/><strong class="me iu">import</strong> <strong class="me iu">tensorflow.keras.backend</strong> <strong class="me iu">as</strong> <strong class="me iu">K</strong></span><span id="f856" class="mi mj it me b gy mr ml l mm mn"><em class="mo"># Creating Densenet121</em></span><span id="d2e3" class="mi mj it me b gy mr ml l mm mn"><strong class="me iu">def</strong> densenet(input_shape, n_classes, filters = 32):<br/>    <br/>    <em class="mo">#batch norm + relu + conv</em><br/>    <strong class="me iu">def</strong> bn_rl_conv(x,filters,kernel=1,strides=1):<br/>        <br/>        x = BatchNormalization()(x)<br/>        x = ReLU()(x)<br/>        x = Conv2D(filters, kernel, strides=strides,padding = 'same')(x)<br/>        <strong class="me iu">return</strong> x<br/>    <br/>    <strong class="me iu">def</strong> dense_block(x, repetition):<br/>        <br/>        <strong class="me iu">for</strong> _ <strong class="me iu">in</strong> range(repetition):<br/>            y = bn_rl_conv(x, 4*filters)<br/>            y = bn_rl_conv(y, filters, 3)<br/>            x = concatenate([y,x])<br/>        <strong class="me iu">return</strong> x<br/>        <br/>    <strong class="me iu">def</strong> transition_layer(x):<br/>        <br/>        x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )<br/>        x = AvgPool2D(2, strides = 2, padding = 'same')(x)<br/>        <strong class="me iu">return</strong> x<br/>    <br/>    input = Input (input_shape)<br/>    x = Conv2D(64, 7, strides = 2, padding = 'same')(input)<br/>    x = MaxPool2D(3, strides = 2, padding = 'same')(x)<br/>    <br/>    <strong class="me iu">for</strong> repetition <strong class="me iu">in</strong> [6,12,24,16]:<br/>        <br/>        d = dense_block(x, repetition)<br/>        x = transition_layer(d)</span><span id="691f" class="mi mj it me b gy mr ml l mm mn">    x = GlobalAveragePooling2D()(d)<br/>    output = Dense(n_classes, activation = 'softmax')(x)<br/>    <br/>    model = Model(input, output)<br/>    <strong class="me iu">return</strong> model</span><span id="bbc0" class="mi mj it me b gy mr ml l mm mn">input_shape = 224, 224, 3<br/>n_classes = 3</span><span id="e29d" class="mi mj it me b gy mr ml l mm mn">model = densenet(input_shape,n_classes)<br/>model.summary()</span></pre><p id="a964" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">输出:(假设最后 3 节课—模型总结的最后几行)</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ms"><img src="../Images/81c812cc5e0b88fd78f46f3d63166ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHMFwbg0G9rO90RjYA14Lg.jpeg"/></div></div></figure><p id="9714" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">要查看<strong class="ld iu">架构图</strong>，可以使用下面的代码。</p><pre class="kq kr ks kt gt md me mf mg aw mh bi"><span id="ec45" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">from</strong> <strong class="me iu">tensorflow.python.keras.utils.vis_utils</strong> <strong class="me iu">import</strong> model_to_dot<br/><strong class="me iu">from</strong> <strong class="me iu">IPython.display</strong> <strong class="me iu">import</strong> SVG<br/><strong class="me iu">import</strong> <strong class="me iu">pydot</strong><br/><strong class="me iu">import</strong> <strong class="me iu">graphviz</strong><br/><br/>SVG(model_to_dot(<br/>    model, show_shapes=<strong class="me iu">True</strong>, show_layer_names=<strong class="me iu">True</strong>, rankdir='TB',<br/>    expand_nested=<strong class="me iu">False</strong>, dpi=60, subgraph=<strong class="me iu">False</strong><br/>).create(prog='dot',format='svg'))</span></pre><p id="09f7" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">输出—图表的前几块</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/16d00e41e547cd70c758b128b41bd795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*QdMHwMRRPBUxdxp9gA2pTA.jpeg"/></div></figure><p id="deaa" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这就是我们实现 DenseNet 121 架构的方式。</p><p id="8949" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">参考文献:</strong></p><ol class=""><li id="2632" class="mu mv it ld b le lf lh li lk mw lo mx ls my lw mz na nb nc bi translated">1.黄高和刘庄以及劳伦斯·范德马腾和基利安·q·温伯格，密集连接的卷积网络，arXiv 1608.06993 (2016 年)</li></ol></div></div>    
</body>
</html>