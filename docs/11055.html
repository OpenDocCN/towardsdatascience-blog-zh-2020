<html>
<head>
<title>Feature Scaling — Effectively Choose Input Variables Based on Distributions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">要素缩放-根据分布有效选择输入变量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-scaling-effectively-choose-input-variables-based-on-distributions-3032207c921f?source=collection_archive---------9-----------------------#2020-08-01">https://towardsdatascience.com/feature-scaling-effectively-choose-input-variables-based-on-distributions-3032207c921f?source=collection_archive---------9-----------------------#2020-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ad3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">演示如何明智地选择数值变量进行缩放，从而提高模型的准确性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/466713ccb0a475b028b3d7d52fd85c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8ioOmFVJPwH1o3_7Ojyjg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由 Siora Photography 在 Unsplash 上拍摄</p></figure><p id="feec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在构建 ML 模型时，我们经常会遇到处理由不同范围、单位和数量组成的各种数值变量的情况。作为惯例，我们将在构建模型之前对所有特征应用标准化或规范化技术。然而，在决定应用哪种技术进行特征缩放之前，研究数据的分布是至关重要的。</p><p id="6f50" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将讨论标准化和规范化之间的区别，并理解数据的分布。最后，我们将看到如何根据特征的高斯和非高斯分布来选择策略，以提高逻辑回归模型的性能。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="5fef" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">标准化与规范化</h2><p id="249c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">这两种技术有时可以互换使用，但它们指的是不同的方法。</p><p id="8ab2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">标准化</em> </strong>:该技术将数据转换为平均值为零，标准差为 1。</p><p id="81eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">规范化</em> </strong>:该技术将变量中的值在 0 和 1 之间转换。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9c77" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们正在使用皮马印度糖尿病数据集，你可以在这里找到相同的[<a class="ae na" href="https://github.com/SushmithaPulagam/FeatureScaling-with-Distributions" rel="noopener ugc nofollow" target="_blank"/></p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="afa4" class="mc md it nc b gy ng nh l ni nj">import pandas as pd<br/>import numpy as np<br/>data = pd.read_csv(“Pima Indian Diabetes.csv”)<br/>data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/bbce4e824d6ee5c4f74375c82b7ef129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6yOVm_TCOksy8upqY1Nxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的前几条记录</p></figure><p id="5bef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面我们可以看到，数值变量在不同的范围内变化，结果就是目标变量。我们将执行缩放技术和应用逻辑回归。</p><p id="6096" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">将标准化应用于所有特征和建模。</strong></p><p id="d699" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从 sklearn 库中，我们需要使用 StandardScaler 来实现标准化。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="3ee6" class="mc md it nc b gy ng nh l ni nj">from sklearn.preprocessing import StandardScaler<br/>Y = data.Outcome<br/>X = data.drop("Outcome", axis = 1)<br/>columns = X.columns<br/>scaler = StandardScaler()<br/>X_std = scaler.fit_transform(X)<br/>X_std = pd.DataFrame(X_std, columns = columns)<br/>X_std.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f698a8b2a72074d2ac5fdee812db8ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVp5VZllku1L_K91p1EF6Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用标准化后输入要素的转换</p></figure><p id="8a1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们进行训练并测试标准化特征的分割。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1f79" class="mc md it nc b gy ng nh l ni nj">from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(X_std, Y, test_size = 0.15, random_state = 45)</span></pre><p id="13f7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将对标准化数据集应用逻辑回归。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="58ec" class="mc md it nc b gy ng nh l ni nj">#Building Logistic Regression model on the Standardized variables<br/>from sklearn.linear_model import LogisticRegression<br/>lr_std = LogisticRegression()<br/>lr_std.fit(x_train, y_train)<br/>y_pred = lr_std.predict(x_test)<br/>print('Accuracy of logistic regression on test set with standardized features: {:.2f}'.format(lr_std.score(x_test, y_test)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/ab9710e5a1e0aff193a2e10968bd9b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HVgQlbPe1vBDzEnp-lUczQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有标准化特征的模型的准确性</p></figure><p id="e930" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面我们可以看出，应用标准化技术的所有特征的模型的准确度是 72%。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2a5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">对所有特征和建模应用标准化。</strong></p><p id="9fd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从 sklearn 库中，我们需要使用 MinMaxScaler 来实现规范化。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ea82" class="mc md it nc b gy ng nh l ni nj">from sklearn.preprocessing import MinMaxScaler<br/>norm = MinMaxScaler()<br/>X_norm = norm.fit_transform(X)<br/>X_norm = pd.DataFrame(X_norm, columns = columns)<br/>X_norm.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/01e2b544253f252315ca86b91e6d9616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyyo1NywvjlqtCsYMfsn7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用归一化后输入要素的变换</p></figure><p id="2251" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们进行训练并测试归一化特征的分割。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6786" class="mc md it nc b gy ng nh l ni nj"># Train and Test split of Normalized features<br/>from sklearn.model_selection import train_test_split<br/>x1_train, x1_test, y1_train, y1_test = train_test_split(X_norm, Y, test_size = 0.15, random_state = 45)</span></pre><p id="7a18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对规范化数据集应用逻辑回归。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="35ca" class="mc md it nc b gy ng nh l ni nj">#Building Logistic Regression model on the Normalized variables<br/>from sklearn.linear_model import LogisticRegression<br/>lr_norm = LogisticRegression()<br/>lr_norm.fit(x1_train, y1_train)<br/>y_pred = lr_norm.predict(x1_test)<br/>print(‘Accuracy of logistic regression on test set with Normalized features: {:.2f}’.format(lr_norm.score(x1_test, y1_test)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/8308caf9613459ea19e9a65b2338820c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJTfAnn8GZYZf2VatSO4vQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有归一化特征的模型的精度</p></figure><p id="6a09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当所有特征都归一化时，模型的准确度为 74%。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="cc45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">了解特征分布</strong></p><p id="d0d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们画出变量的直方图来研究分布。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="baa6" class="mc md it nc b gy ng nh l ni nj"># Plotting the histograms of each variable<br/>from matplotlib import pyplot<br/>data.hist(alpha=0.5, figsize=(20, 10))<br/>pyplot.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8dfc6aaf8741faf4929b0b0a4cec57ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*iz3KdpLBpBxYGVFiFqEb2Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个特征的直方图，以了解分布情况</p></figure><p id="2d4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">高斯分布</em> </strong> —身体质量指数，血压，葡萄糖。</p><p id="c9c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">非高斯分布</em> </strong> —年龄、糖尿病、胰岛素、怀孕、皮肤厚度</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="e245" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">👉归一化非高斯特征和标准化类高斯特征</h2><p id="5057" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">最后，我们来到一个实验，等待选择变量，并根据同一数据集上的分布应用两种策略。</p><p id="b202" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了应用这个策略，我们将使用 sklearn 中的列转换器和管道概念，因为我们需要通过对列进行子集化来实现混合类型的技术。</p><p id="5a50" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所述，我们正在为高斯和非高斯特征启动不同的流水线</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="485a" class="mc md it nc b gy ng nh l ni nj">from sklearn.compose import ColumnTransformer<br/>from sklearn.pipeline import Pipeline<br/><strong class="nc iu">Standardize_Var = ['BMI','BloodPressure', 'Glucose']</strong><br/>Standardize_transformer = Pipeline(steps=[('standard', StandardScaler())])<br/><strong class="nc iu">Normalize_Var = ['Age','DiabetesPedigreeFunction','Insulin','Pregnancies','SkinThickness']</strong><br/>Normalize_transformer = Pipeline(steps=[('norm', MinMaxScaler())])</span></pre><p id="5fa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们用标准化和规范化的选择性特征对数据建立逻辑回归模型。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="4511" class="mc md it nc b gy ng nh l ni nj">x2_train, x2_test, y2_train, y2_test = train_test_split(X, Y, test_size=0.2)<br/><strong class="nc iu">preprocessor = ColumnTransformer(transformers=<br/> [(‘standard’, Standardize_transformer, Standardize_Var),<br/> (‘norm’, Normalize_transformer, Normalize_Var)])</strong></span><span id="5da9" class="mc md it nc b gy nq nh l ni nj">clf = Pipeline(steps=[(‘preprocessor’, preprocessor),<br/> (‘classifier’, LogisticRegression(solver=’lbfgs’))])<br/>clf.fit(x2_train, y2_train)<br/>print(‘Accuracy after standardizing Gaussian distributed features and normalizing Non-Gaussian features: {:.2f}’.format(clf.score(x2_test, y2_test)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/291ec445513690c05aadc122e8bc5468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AmP4sDfItcub4xY0VpshgQ.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e458" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">最终关键细节</strong></p><p id="c768" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是迄今为止我们已经建立的不同模型的精度细节。</p><p id="2cd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有特征标准化后的精度:<strong class="la iu"> 0.72 </strong></p><p id="8e13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有特征归一化后的精度:<strong class="la iu"> 0.74 </strong></p><p id="501c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">对高斯分布特征进行标准化，对非高斯分布特征进行归一化后的精度:0.79 </em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="87f2" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">摘要</h2><p id="739f" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">当我们处理基于梯度下降的算法(线性和逻辑回归、神经网络)和基于距离的算法(KNN、K 均值、SVM)时，我们需要执行特征缩放，因为这些算法对数据点的范围非常敏感。在处理基于树的算法时，这一步不是强制性的。</p><p id="ea67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文的主要重点是解释数据的分布如何在特征缩放中发挥重要作用，以及如何选择基于高斯和非高斯分布的策略来提高模型的整体精度。</p><p id="f5ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从我的 GitHub [ <a class="ae na" href="https://github.com/SushmithaPulagam/FeatureScaling-with-Distributions" rel="noopener ugc nofollow" target="_blank"> profile </a> ]中获得完整的代码</p><p id="3dbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读，快乐学习！🙂</p></div></div>    
</body>
</html>