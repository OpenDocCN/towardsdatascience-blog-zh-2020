<html>
<head>
<title>Overview of tokenization algorithms in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的标记化算法综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9?source=collection_archive---------5-----------------------#2020-08-12">https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9?source=collection_archive---------5-----------------------#2020-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="65c8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">符号化方法介绍，包括子词、BPE、词块和句子块</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4b7d56d9e4d31f588248f8e763dc4482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ULGO6dZTbm0PM7dn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">汉娜·赖特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="e4af" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">⚠️读了我在⚠️博客中的<a class="ae ky" href="https://anebz.eu/nlp-tokenization-algorithms" rel="noopener ugc nofollow" target="_blank">原帖</a></h2><p id="7c5f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">本文是对标记化算法的概述，从词级、字符级和子词级标记化，重点是 BPE、单字 LM、词块和句子块。这意味着专家和初学者都可以阅读。如果有任何概念或解释不清楚，请与我联系，我很乐意澄清任何需要。</p><h1 id="9a95" class="mo la it bd lb mp mq mr le ms mt mu lh jz mv ka ll kc mw kd lp kf mx kg lt my bi translated">什么是标记化？</h1><p id="d0f3" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">标记化是自然语言处理的第一步，它的任务是将一个文本序列分割成具有语义意义的单元。这些单元被称为记号，记号化的难点在于如何得到理想的拆分，使得文本中的所有记号都具有正确的含义，并且没有遗漏记号。</p><p id="cbdc" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">在大多数语言中，文本由空格分隔的单词组成，其中每个单词都有一个语义。我们稍后会看到使用符号的语言会发生什么，其中符号的含义比单词复杂得多。现在我们可以用英语工作。举个例子:</p><ul class=""><li id="1dd9" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nj nk nl nm bi translated">原文:我吃了一个汉堡，味道不错。</li><li id="4d36" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nj nk nl nm bi translated">标记化文本:['I '，' ate '，' a '，' burger '，'，' and '，' it '，' was '，' good '，'.']</li></ul><p id="71ef" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">“Burger”是一种食物，“and”是连词，“good”是积极的形容词，等等。通过这种方式标记，每个元素都有一个意义，通过连接每个标记的所有意义，我们可以理解整个句子的意义。标点符号也有自己的记号，逗号分隔从句，句号表示句子的结束。下面是另一种标记化方法:</p><ul class=""><li id="bde5" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nj nk nl nm bi translated">标记化的文本:['I ate '，' a '，' bur '，' ger '，' an '，' d it '，' wa '，' s good '，'.']</li></ul><p id="e5ff" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">对于多词单位“我吃了”，我们可以只添加“我”和“吃了”的含义，对于子词单位“bur”和“ger”，它们没有单独的含义，但通过将它们连接起来，我们可以得到熟悉的单词，我们可以理解它的意思。</p><p id="6a96" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">但是我们怎么处理它呢？这是什么意思？作为人类和说英语的人，我们可以推断出‘it’是代词，字母‘d’属于前一个单词。但在这种标记化之后，前面的单词“an”在英语中已经有了意义，冠词“an”与“and”非常不同。这个怎么处理？你可能会想:<strong class="lx iu">坚持使用单词，给标点符号赋予它们自己的符号。</strong>这是最常见的记号化方式，叫做词级记号化。</p><h1 id="0699" class="mo la it bd lb mp mq mr le ms mt mu lh jz mv ka ll kc mw kd lp kf mx kg lt my bi translated">单词级标记化</h1><p id="5601" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">它只包括用空格和标点符号分割句子。Python 中有很多这样的库，包括 NLTK、SpaCy、Keras、Gensim，或者你可以自定义正则表达式。</p><p id="1e32" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">对空白进行拆分也可以拆分应该被视为单个标记的元素，例如 New York。这是有问题的，尤其是名字、借用的外国短语以及有时由多个单词组成的复合词。</p><p id="4def" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">像“不要”这样的词，或者像“约翰的”这样的缩写词呢？获得标记“不要”或“做”和“不要”更好吗？如果文中出现错别字，burger 变成了‘birger’怎么办？我们人类可以看到这是一个打字错误，用'汉堡'代替这个词并继续，但机器不能。错别字会影响整个 NLP 管道吗？</p><p id="6643" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">单词级标记化的另一个缺点是它产生了巨大的词汇量。每个标记都保存在一个标记词汇表中，如果这个词汇表是用所有输入文本中的所有唯一单词构建的，那么它将创建一个巨大的词汇表，这会在以后产生内存和性能问题。一个当前最先进的深度学习架构，<a class="ae ky" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer XL </a>，拥有 267735 的词汇量。为了解决词汇量大的问题，我们可以考虑用<strong class="lx iu">字符代替单词</strong>来创建标记，这就是所谓的字符级标记化。</p><h1 id="dbad" class="mo la it bd lb mp mq mr le ms mt mu lh jz mv ka ll kc mw kd lp kf mx kg lt my bi translated">字符级标记化</h1><p id="0ff6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">由<a class="ae ky" href="https://github.com/karpathy/char-rnn" rel="noopener ugc nofollow" target="_blank"> Karpathy 在 2015 年</a>首次推出，它不是将文本拆分成单词，而是拆分成字符，例如，smarter 变成了 s-m-a-r-t-e-r。词汇量大大减少到语言中的字符数，英语加上特殊字符为 26 个。拼写错误或生僻字处理得更好，因为它们被分解成字符，而这些字符在词汇表中是已知的。</p><p id="c990" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">在字符级标记序列已经显示出一些令人印象深刻的结果。<a class="ae ky" href="https://arxiv.org/abs/1704.01444" rel="noopener ugc nofollow" target="_blank">来自 OpenAI 的拉德福德等人(2017) </a>表明字符级模型可以捕捉文本的语义属性。<a class="ae ky" href="https://arxiv.org/abs/1610.10099" rel="noopener ugc nofollow" target="_blank">来自 Deepmind 的 Kalchbrenner 等人(2016) </a>和<a class="ae ky" href="https://www.aclweb.org/anthology/Q17-1026" rel="noopener ugc nofollow" target="_blank"> Leet 等人(2017) </a>都演示了角色层面的翻译。这些都是特别令人印象深刻的结果，因为翻译的任务是捕捉潜在文本的语义理解。</p><p id="01f2" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">减小词汇表的大小需要权衡序列长度。现在，每个单词被拆分成所有的字符，标记化的序列比初始文本长得多。单词“smarter”被转化为 7 种不同的符号。此外，标记化的主要目标没有实现，因为字符，至少在英语中，没有语义。只有当字符连在一起时，它们才有意义。作为单词和字符标记化之间的中介，子单词标记化产生了<strong class="lx iu">子单词单元</strong>，比单词小，但比字符大。</p><h1 id="1c5a" class="mo la it bd lb mp mq mr le ms mt mu lh jz mv ka ll kc mw kd lp kf mx kg lt my bi translated">子词级标记化</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/87722acf0ca0cbc782008d62a66f161f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_s0e2RuWz5_0GYwqQzFlQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">子词标记化示例</p></figure><p id="6303" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">子词级标记化不会转换大多数常用词，而是将罕见词分解成有意义的子词单元。如果“不友好”被标记为一个罕见的词，它将被分解为“un-friend-ly”，这些都是有意义的单位，“un”表示相反的意思，“friend”是一个名词，“ly”将其转化为副词。这里的挑战是如何进行细分，我们如何做到“不友好”而不是“不友好”。</p><p id="39ab" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">截至 2020 年，最先进的深度学习架构，基于<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Transformers、</a>使用子词级标记化。<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>对这个例子做了如下的记号化:</p><ul class=""><li id="86bd" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nj nk nl nm bi translated">原文:我有一个新的 GPU。</li><li id="d3c2" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nj nk nl nm bi translated">标记化文本:['i '，' have '，' a '，' new '，' gp '，' ##u '，'.']</li></ul><p id="a3aa" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">词汇表中的单词被标记为单词本身，但是在词汇表中找不到“GPU ”,它被视为一个罕见的单词。根据一种算法，决定将其分割成‘gp-u’。“u”前的##表示这个子词与前一个子词属于同一个词。BPE、单字标记法、单词块和句子块是最常见的子词标记化算法。因为这是一个介绍性的帖子，所以会对它们进行简单的解释，如果你对更深入的描述感兴趣，请告诉我，我会为它们每个做更详细的帖子。</p><h2 id="645c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">BPE</h2><p id="d05a" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">由<a class="ae ky" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank"> Sennrich 等人在 2015 年</a>推出，它迭代合并最频繁出现的字符或字符序列。该算法大致是这样工作的:</p><ol class=""><li id="7e1b" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nt nk nl nm bi translated">获得足够大的语料库。</li><li id="42b4" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">定义所需的子词词汇大小。</li><li id="3cf4" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">将单词拆分为字符序列，并附加一个特殊的标记，分别显示单词的开头或单词的结尾的词缀/后缀。</li><li id="b3df" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">计算文本中的序列对及其频率。例如，(' t '，' h ')有频率 X，(' h '，' e ')有频率 y。</li><li id="b424" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">根据出现频率最高的序列对生成一个新子词。例如，如果(‘t’，‘h’)在一组对中具有最高的频率，则新的子词单元将变成‘th’。</li><li id="e2c1" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">从步骤 3 开始重复，直到达到子词词汇大小(在步骤 2 中定义)或者下一个最高频率对是 1。按照这个例子，语料库中的(' t '，' h ')将被替换为' th '，再次计算对，再次获得最频繁的对，并再次合并。</li></ol><p id="a570" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">BPE 是一种贪婪和确定性的算法，不能提供多重分割。也就是说，对于给定的文本，标记化的文本总是相同的。关于 BPE 如何工作的更详细的解释将在后面的文章中详述，或者你也可以在<a class="ae ky" href="https://leimao.github.io/blog/Byte-Pair-Encoding/" rel="noopener ugc nofollow" target="_blank">的许多其他文章</a>中找到。</p><h2 id="2b59" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak"> Unigram LM </strong></h2><p id="58f2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">Unigram 语言建模(<a class="ae ky" href="https://arxiv.org/abs/1804.10959" rel="noopener ugc nofollow" target="_blank"> Kudo，2018 </a>)基于所有子词出现都是独立的假设，因此子词序列是由子词出现概率的乘积产生的。算法的步骤如下:</p><ol class=""><li id="e9f6" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nt nk nl nm bi translated">获得足够大的语料库。</li><li id="8f5f" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">定义所需的子词词汇大小。</li><li id="2fee" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">通过给出一个单词序列来优化单词出现的概率。</li><li id="ac36" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">计算每个子字的损失。</li><li id="58c8" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">按损失对符号进行排序，保留单词的前 X %(例如 X=80%)。为了避免超出词汇表实例，建议将字符级作为子词的子集包括在内。</li><li id="12d7" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">重复步骤 3-5，直到达到子词词汇大小(在步骤 2 中定义)或者没有变化(步骤 5)</li></ol><p id="7319" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">Kudo 认为，unigram LM 模型比 BPE 模型更灵活，因为它基于概率 LM，可以输出多个分段及其概率。它不是从一组基本符号开始学习，也不是像 BPE 或单词块那样用某种规则进行合并，而是从大量词汇(例如，所有预标记的单词和最常见的子字符串)开始，然后逐渐减少。</p><h2 id="0100" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">文字片</h2><p id="47b9" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">word piece(<a class="ae ky" href="https://research.google.com/pubs/archive/37842.pdf" rel="noopener ugc nofollow" target="_blank">Schuster and Nakajima，2012 </a>)最初用于解决日语和韩语语音问题，目前已知用于 BERT，但精确的标记化算法和/或代码尚未公开。它在许多方面与 BPE 相似，只是它基于似然性而不是下一个最高频率对来形成新的子词。算法的步骤如下:</p><ol class=""><li id="02ff" class="ne nf it lx b ly mz mb na li ng lm nh lq ni mn nt nk nl nm bi translated">获得足够大的语料库。</li><li id="94ef" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">定义所需的子词词汇大小。</li><li id="b52b" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">将单词拆分成字符序列。</li><li id="79ff" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">用文本中的所有字符初始化词汇表。</li><li id="529e" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">根据词汇建立一个语言模型。</li><li id="ad05" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">通过组合当前词汇中的两个单元来生成新的子词单元，以将词汇递增 1。当添加到模型中时，从所有可能性中选择最能增加训练数据的可能性的新子词单元。</li><li id="8b9e" class="ne nf it lx b ly nn mb no li np lm nq lq nr mn nt nk nl nm bi translated">重复步骤 5，直到达到子词词汇大小(在步骤 2 中定义)或者可能性增加低于某个阈值。</li></ol><h2 id="a0d5" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">句子片断</h2><p id="65b0" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">到目前为止，所有的标记化方法都需要某种形式的预标记化，这构成了一个问题，因为不是所有的语言都使用空格来分隔单词，或者有些语言是由符号组成的。SentencePiece 可以接受特定语言的预发音。你可以在<a class="ae ky" href="https://github.com/google/sentencepiece" rel="noopener ugc nofollow" target="_blank"> Github </a>找到开源软件。例如，<a class="ae ky" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM </a>使用了句子片断，并为中文、日文和泰文添加了特定的前缀词。</p><p id="428d" class="pw-post-body-paragraph lv lw it lx b ly mz ju ma mb na jx md li nb mf mg lm nc mi mj lq nd ml mm mn im bi translated">SentencePiece 在概念上类似于 BPE，但它不使用贪婪的编码策略，实现了更高质量的标记化。SentencePiece 认为字符分组中的模糊性是训练期间模型正则化的一个来源，这使得训练变得更慢，因为有更多的参数需要优化，并且不鼓励 Google 在 BERT 中使用它，而是选择 WordPiece。</p><h1 id="670d" class="mo la it bd lb mp mq mr le ms mt mu lh jz mv ka ll kc mw kd lp kf mx kg lt my bi translated">结论</h1><p id="f4b6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">历史上，标记化方法已经从单词发展到字符，最近发展到子单词级别。这是对标记化方法的一个快速概述，我希望文本是可读和可理解的。关注我的推特了解更多 NLP 信息，或者在那里问我任何问题:)</p></div></div>    
</body>
</html>