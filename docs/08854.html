<html>
<head>
<title>OOPS! Predicting Unintentional Action in Video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">哎呀！预测视频中的无意动作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3?source=collection_archive---------41-----------------------#2020-06-25">https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3?source=collection_archive---------41-----------------------#2020-06-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9161" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解运动的意向性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f8bee99c395a199620dc160f2308dd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOe1f5XZwE5QbGRsS-TcqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">有意与无意的行动<strong class="bd kv">【1】</strong></p></figure><p id="da68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，人类是不完美的代理人，他们的行为可能是不稳定的和不可预测的。虽然以前的研究主要集中在人类活动识别和预测上，但哥伦比亚大学的研究人员采用了一种新的方法——分析目标导向的人类行动。戴夫·爱泼斯坦、袁波·陈和卡尔·冯德里克在《哎呀！预测视频中的无意动作<strong class="ky ir">【1】</strong>:</p><ol class=""><li id="3a8f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">提出3项新任务:分类、定位和意外行为预测</li><li id="9d3c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">引入新的基准数据集:大型、公共和(部分)注释的(光流、无意运动的时间戳等)</li><li id="06d6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对比无意动作中级感知线索:视频速度(新)、视频上下文<strong class="ky ir">【2】</strong>、事件顺序<strong class="ky ir">【3】</strong></li></ol><p id="032f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">运动的意向性</strong></p><p id="9f8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多研究试图模拟人类行为的物理和原子后果，但很少有人试图理解运动背后的意图。本文区分有意运动和无意运动，旨在识别、定位和预测无意运动。</p><p id="6cf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">哎呀！数据集</strong></p><p id="c63a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哎呀！数据集<strong class="ky ir">【4】</strong>由20，338个视频剪辑(3-30秒长，总计50多个小时)组成，这些视频剪辑来自YouTube fail汇编，都被证实包含一些无意的“野外”人类行为。由于作者提出了一种自我监督的方法来完成这项任务，该数据集被分成3个子集:7，368个视频作为标记的训练集，6，739个标记的视频作为测试集，其余的是用于预训练的未标记集。为了分类，视频中的动作被标注为“有意的”、“无意的”或“过渡的”；对于定位，工作人员在故障的时间位置(故障开始的时刻)标注时间戳标记。附加数据集注释包括光流和自然语言描述。在数据集中，270个视频被指定为诊断集，可以看到更细粒度的手动注释。这些视频被分为9种类型的无意行动:“有限的技能”、“有限的知识”、“环境”、“意外”、“有限的可见性”、“计划错误”、“执行错误”、“单代理人”、“多代理人”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/9ab0f0759e9c42f6a6a3508f80b499a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JgvytZ7enxZEGVz0zScr4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:数据集统计<strong class="bd kv">【1】</strong></p></figure><p id="9f7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者还报告了各种数据集统计数据，包括视频剪辑长度和故障时间标签的分布，以说明数据的多样性；来自不同人类注释者的标签的标准偏差，以展示高度的人类(注释者)一致性；动作和场景类别的分布(由它们的完全监督基线预测)。</p><p id="63a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">来自中级感知线索的自我监督特征</strong></p><p id="b741" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者调查了视频中自然存在的自我监督线索(或需要最小限度的注释)，以了解人类行动中意向性的深层、可转移的表征。具体来说，检查视频速度、视频上下文和事件顺序。所有的通讯网络都是通过ResNet3D-18模型<strong class="ky ir">【5】</strong>实现的。</p><p id="41c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">视频速度</strong></p><p id="0e40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于之前的研究<strong class="ky ir">【6】</strong>，作者指出，人类对意图的判断在很大程度上受到视频速度的影响。由于视频速度是每个视频固有的，因此通过速度进行推断需要最少的预处理。对于训练，作者综合改变视频的速度，并训练一个自我监督的神经网络来预测真实的帧速率。正如作者所指出的，这种ConvNet生成的特征与事件的预期持续时间相关联(鲁棒性来自对具有综合改变的速度的视频的训练)，并对逐帧运动信息进行编码，从而构建了视频速度信息的有用表示。</p><p id="1c4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">视频背景</strong></p><p id="410f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者声明“无意的行动往往是对预期的偏离”，并探讨了作为意向性视觉度量框架的可预测性。在先前研究<strong class="ky ir">【2】</strong>的指导下，他们将帧<em class="mh"> x_{t-1} </em>和<em class="mh"> x_{t+1} </em>视为周围视频上下文，并激励模型对中间目标帧<em class="mh"> x_t </em>的特征图进行插值。值得注意的是，他们利用噪声对比估计<strong class="ky ir">【7】</strong>和对比预测编码<strong class="ky ir">【2，3，8】</strong>的概念来构造要最大化的目标函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/88a1d8f1be271ecc86a7a90195911430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUgthfqxnLuaS6SjEfMvLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标函数<strong class="bd kv">【1】</strong></p></figure><p id="f838" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以便最大化目标帧特征和上下文嵌入之间的距离，同时最小化目标帧和非上下文剪辑特征之间的距离。</p><p id="c09e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">事件顺序</strong></p><p id="e16f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者提出的基本原理是“无意运动往往表现为混乱或不可逆的运动”，导致独特的时间事件顺序。为了生成事件顺序的表示<strong class="ky ir">【3】</strong>，他们对视频中的二次抽样剪辑进行置换和洗牌，并训练一个ConvNet来预测所应用的置换序列。这是通过一个3部分模型实现的，该模型由片段特征编码器、成对片段关系网络(其中特征表示片段的相似性)和事件顺序预测器组成。</p><p id="c9cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上述自监督模型中提取的特征然后被用作线性分类器的输入，该线性分类器利用类来执行3类分类:“有意的”、“无意的”和“过渡的”运动。</p><p id="3d67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实验</strong></p><p id="4549" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绩效以3个任务为基准——分类、定位(定位从有意到无意运动的过渡的时间边界)、预测(预测失败的开始)，并进行3个层次的比较:首先，在不同的自我监督附带线索(视频速度(新提出的)、视频上下文、事件顺序)之间进行比较；其次，比较自监督模型和全监督基线(在动力学动作识别数据集上的预训练加上微调、细粒度注释:运动幅度、划痕、机会)；最后，比较机器和人的表现(人的同意)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/840060a7dad59293c2ad79f757cdd49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVg01KHLMYBVHN0wEK2RPw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1(分类精度)、表2(定位精度)、表3(预测精度)<strong class="bd kv">【1】</strong></p></figure><p id="99cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">分类、定位、预测</strong></p><p id="83d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在所有3个任务中，动力学监督产生了最好的机器性能，而视频速度监督始终优于所有其他自我监督和完全监督的方法。全监督和自监督方法之间的性能差距在分类中最小(表1)，在时间定位中最大(表2)。为了量化定位精度，与任何地面实况时间位置(一些视频有多个地面实况)重叠的预测(在1秒和0.25秒内报告的结果)被认为是正确的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/0eae2350cd38681fb82ac2c5630f7756.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*48Mhf4FUKgdGZO3Wwl1eOQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:分类混淆矩阵<strong class="bd kv">【1】</strong></p></figure><p id="67bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别是，自我监督的模型比完全监督的模型遭受更多的假阳性边界预测，在完全监督的模型中，它们将有意运动与故障的开始混为一谈(图2)。此外，作者对9种无意运动类别中的每一种进行了详细的错误率分析(图3 )(在上述诊断集中)。他们报告说，由意外因素(“如突然俯冲的鸟”)或环境因素(“如在冰上滑倒”)引起的无意运动最难检测，并假设多智能体场景由于其更明显的视觉线索而记录了最低的错误率。其他挑战包括有限的视频可见性(被遮挡的物体)和有限的知识(“例如理解火是热的”)。从结果可以看出，自我监督和完全监督的方法都明显落后于人的表现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/f25f2f3bab32661efba005a7d26ddc36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*FQkBZGvsy6KzTgLEXuZSkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:性能分解【T6【1】</p></figure><p id="4118" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="3c57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“哎呀！预测视频中的无意行为”介绍了理解人类行为中的意向性的3个新任务，并为未来的工作提供了一个大型基准数据集。作者提出了一种自我监督的方法，并报告了使用视频速度作为视频表示的附带线索的有希望的结果。</p><p id="5d75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="3961" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]戴夫·爱泼斯坦、袁波·陈和卡尔·冯德里克。糟糕！预测视频中的无意动作。2020年，CVPR。<br/> [2]亚伦·范·登·奥尔德、亚哲·李和奥里奥尔·维尼亚尔斯。对比预测编码的表征学习。<a class="ae mm" href="https://arxiv.org/pdf/1807.03748.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv预印本arXiv:1807.03748 </a>，2018。<br/> [3]徐德静，，，邵剑，，庄悦婷。<a class="ae mm" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Self-Supervised_Spatiotemporal_Learning_via_Video_Clip_Order_Prediction_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">通过视频片段顺序预测的自我监督时空学习。</a>2019年，CVPR。[4]戴夫·爱泼斯坦、袁波·陈和卡尔·冯德里克。<a class="ae mm" href="http://oops.cs.columbia.edu/" rel="noopener ugc nofollow" target="_blank">哎呀！预测视频中的无意动作。</a>检索自<a class="ae mm" href="https://oops.cs.columbia.edu/" rel="noopener ugc nofollow" target="_blank">https://oops.cs.columbia.edu/</a><br/>【5】原研哉、片冈博胜、佐藤丰。<a class="ae mm" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">时空3d cnns能否追溯2d cnns和imagenet的历史？</a>2018年CVPR。尤金·M·卡鲁索，扎卡里·C·伯恩斯，本杰明·匡威。<a class="ae mm" href="https://www.pnas.org/content/pnas/113/33/9250.full.pdf" rel="noopener ugc nofollow" target="_blank">慢动作增加感知意图。</a>2016年PNAS。<br/> [7] Rafal Jozefowicz、Oriol Vinyals、Mike Schuster、Noam Shazeer和吴永辉。探索语言建模的极限。 arXiv预印本arXiv:1602.02410，2016。<br/>[8]韩腾达、、谢和安德鲁·齐塞曼。<a class="ae mm" href="https://arxiv.org/pdf/1909.04656.pdf" rel="noopener ugc nofollow" target="_blank">通过密集预测编码的视频表示学习。</a>2019年ICCV研讨会。</p></div></div>    
</body>
</html>