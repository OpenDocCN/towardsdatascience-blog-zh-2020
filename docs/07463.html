<html>
<head>
<title>Let the machine write next..!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让机器接着写..！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-the-machine-write-next-16ba1a0cbdda?source=collection_archive---------48-----------------------#2020-06-05">https://towardsdatascience.com/let-the-machine-write-next-16ba1a0cbdda?source=collection_archive---------48-----------------------#2020-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f618" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用香草LSTM、注意力和GPT-2的文本生成</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0c9578c74b5c09416d9c30d19e0e5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FdR7uKRDB2m8PgMh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">劳拉·乔伊特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="c8e8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">目的:</strong>在给定输入句子的基础上自动生成新句子。</p><p id="52f6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">路线图:</strong></p><ol class=""><li id="182b" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">商业问题</li><li id="fc69" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">数据讨论</li><li id="b3a9" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">电子设计自动化(Electronic Design Automation)</li><li id="f53f" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">数据管道</li><li id="35e5" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">模型架构</li><li id="fb2f" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">推理</li><li id="6610" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">比较</li><li id="e08c" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">结论</li><li id="3c72" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">未来的工作</li><li id="a9cf" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">参考</li></ol></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="3b9b" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">介绍</h2><p id="edc5" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">时光飞逝，从17世纪哲学家如<a class="ae ky" href="https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz" rel="noopener ugc nofollow" target="_blank">莱布尼茨</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Descartes" rel="noopener ugc nofollow" target="_blank">笛卡尔</a>提出了将语言之间的单词联系起来的代码建议，到1950年<a class="ae ky" href="https://en.wikipedia.org/wiki/Alan_Turing" rel="noopener ugc nofollow" target="_blank">艾伦图灵</a>发表了一篇题为“<a class="ae ky" href="https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence" rel="noopener ugc nofollow" target="_blank">计算机器和智能</a>”的文章，NLP(自然语言处理)开始被探索。NLP经历了从复杂的手写规则集(1980年)到统计模型、机器学习模型到深度学习模型的演变。今天，自然语言处理的任务，如文本分类、文本摘要、问答系统、语言建模、语音识别、机器翻译、字幕生成、文本生成，都利用了最新的模型，如Transformer、BERT、Attention。虽然螺母和螺栓是“神经元”，但它们的连接、训练、学习和行为取决于模型与模型，如RNNs、seq2seq、编码器-解码器、注意力、变压器等。让我们来研究其中一项任务—文本生成。</p><h2 id="81fb" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">1.商业问题</h2><p id="48fb" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">如果不是手动记下句子，而是假设我们精心设计和学习的系统生成或建议下一段或下一句话，会怎么样？这会让写作变得更容易。它会暗示一个句子的延续。它会产生新的标题或故事或文章或一本书的章节。Gmail在自动回复和智能撰写中也利用了这一概念。</p><ul class=""><li id="903c" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated"><strong class="li iu">问题描述</strong></li></ul><p id="0aaa" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">开发一个模型，它可以学习在延续中生成下一个句子或单词，并可以完成段落或内容。</p><ul class=""><li id="971e" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated"><strong class="li iu">深度学习的作用</strong></li></ul><p id="473e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">众所周知，人工神经网络从生物系统的信息处理和分布式通信节点中获得灵感，而数字逻辑也有从数据中学习的能力。</p><p id="0ca9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，我们可以将这个问题作为DL任务来提出，在这个任务中，我们将为模型提供数据，并以这样一种方式训练它，使它能够预测接下来的序列。我们将使用DL的所有工具和技术来使我们的模型自我学习。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="818f" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">2.数据讨论</h2><p id="fb92" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我使用了从<a class="ae ky" href="https://www.universetoday.com/" rel="noopener ugc nofollow" target="_blank">到</a>的太空和天文新闻数据。我已经从网站上删除了近3700篇文章。你也可以在这里找到<a class="ae ky" href="https://github.com/khushi810/Text-Generation/blob/master/universal-today-scrapper.py" rel="noopener ugc nofollow" target="_blank">的剧本。</a></p><p id="d08d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因为所有的文章都是从网上搜集来的，所以它们包含了一些额外的信息，比如添加、弹出窗口、视频链接、在各种社交平台上分享的信息等等。因此，最好适当地清理数据，因为我们的模型将从收集的数据中学习。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="9be9" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">3.电子设计自动化(Electronic Design Automation)</h2><p id="2dbd" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">现在轮到探索和理解数据了。在数据科学领域经常有人说‘你越了解数据，你的模型就越能从中学习’！所以我们来试着探索一下。</p><p id="12af" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> 3.1清洁</strong></p><p id="860d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们的主要目标是使我们的文本数据清晰明了，以便我们的模型可以学习和理解英语句子的模式和行为。</p><ul class=""><li id="6ea2" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">正如我前面提到的，我们的刮擦文章有一些额外的不必要的信息，我们必须删除。</li><li id="9880" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将把我们的文本转换成小写字母，这样我们的单词大小就会变小。</li><li id="a2a1" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将用“数字标签”替换所有数字，用“字母数字”标签替换字母数字单词，以减少我们的vocab大小。</li><li id="2372" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将删除所有标点符号，除了(。,?！)来生成有意义的句子。</li></ul><p id="757b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">数据清理</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c378" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> 3.2数据分析</strong></p><p id="8b11" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将从文本数据中分析单词和字符。让我们从我们的文章中分析总单词和总字符的各种情节。</p><div class="kj kk kl km gt ab cb"><figure class="nr kn ns nt nu nv nw paragraph-image"><img src="../Images/12f3e447d6c2c8caf663786165bace61.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*14iQGAaV5RIjP9U7bdvQ3w.png"/></figure><figure class="nr kn nx nt nu nv nw paragraph-image"><img src="../Images/455d1384e557ac4c45ffb60d16d8917f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*kKmg-dCvDm99BWDIYc1OuQ.png"/><p class="ku kv gj gh gi kw kx bd b be z dk ny di nz oa translated">单词的PDF和CDF</p></figure></div><p id="da55" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">观察:</strong></p><ul class=""><li id="33a8" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">大多数文章的长度在1000到2000字之间。很少有文章内容很长。几乎95%到98%的文章少于2000字。</li></ul><p id="0896" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">分割训练集和验证集后，我们现在将分别分析每个集中的文本数据。</p><p id="8bb3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们从训练部分来分析一下单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/b0078ba6903925af913dfb4dfee554f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZF0zZkILHptpRgqI4C0EVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练数据中单词的直方图</p></figure><ul class=""><li id="ce7f" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">从上面的柱状图中，我们可以看到，‘the’，‘the’，‘the .’、' of '、' and '、' numtag '是在所有培训文章中最常见的单词。</li></ul><p id="0408" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">来自训练和验证集的字符分析:</strong></p><div class="kj kk kl km gt ab cb"><figure class="nr kn oc nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b4e09907cfe355d96e03228e8aee4f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*gwL7Sk1Gc0Bw_gQXIbN6vw.png"/></div></figure><figure class="nr kn od nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/863c861773e20832fe02032cd8d8de20.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*_CR1dhcQY77ClzmsuqFpew.png"/></div></figure></div><div class="ab cb"><figure class="nr kn oe nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/cbae782de816d5cc8430b52e29463462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*FUdEdqvivQ_7NppT9NqSog.png"/></div></figure><figure class="nr kn of nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/130cc3032e6f5a090138d1ed0bf4a5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*Z1iK2OatWp2t0QKA9g2guA.png"/></div></figure></div><p id="7ee2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">观察:</strong></p><ul class=""><li id="4ae6" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">上面的图包括关于训练和验证数据集中字符数的信息。</li><li id="95cd" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">训练数据中的大多数文章的字符数在10到50之间。内容的字符数在10k到15k之间。</li><li id="bca0" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">Val数据文章标题中的字符数在10到70之间，而内容中的字符数在10.5k到11.5k之间。</li></ul><p id="fda4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">来自训练和验证集的词的分析:</strong></p><div class="kj kk kl km gt ab cb"><figure class="nr kn og nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/468a86623f8aed66583106818121fd6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*cBfVhE2rid3ySbNXZop5fg.png"/></div></figure><figure class="nr kn oh nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/fdb8f16d9dccc6e7fd402db7b80296d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*lcF3TIgUYgxcCRFHs6ftRQ.png"/></div></figure></div><div class="ab cb"><figure class="nr kn oi nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/19d5baa2f46b291872ba83aec3940e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*AzaDmxC2OTggzDtjfRG_Cg.png"/></div></figure><figure class="nr kn oj nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/106c8927aee3260bd5e6941ea4364be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*c0Ah9okntmST5DP3eml4yw.png"/></div></figure></div><p id="ee34" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">观察:</strong></p><ul class=""><li id="b6da" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">上面的图包括关于训练和验证数据集中字数的信息。</li><li id="e2cd" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">训练文章在标题中具有从5到15字数，且在内容中具有大约2k的字数。</li><li id="1914" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">来自验证数据的文章在标题中的字数在5到15之间，在内容中的字数在1.9k到2k之间。</li></ul><p id="0f47" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">结论:</strong></p><ul class=""><li id="69e4" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">一些文章每周都有关于太空视频的更新和讨论，字数很少。所以从这类文章中得到的信息几乎是相似的，比如主机名、剧集名等等。</li><li id="7e12" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">视频内容不包括在文章中，所以除此之外手头没有多样性的内容。所以我们不会考虑那种文章。</li><li id="1e91" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">有些文章字数很少，我们也不会考虑。</li></ul><p id="db7e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> 3.3数据预处理</strong></p><p id="983b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以使用基于字符或基于单词的方法来解决这个问题。基于字符的方法可能会导致句子拼写错误，所以让我们专注于基于单词的方法，在这种方法中，我们将用某个固定的数字替换每个单词，因为模型只理解数字，而不是单词！(我们将从预训练的300维手套向量中创建嵌入矩阵(<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">此处为</a>)，用于我们训练数据集中出现的单词)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6f57" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下面的功能是将文本数据转换成数字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5f65" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们只为训练数据集中出现的单词创建一个嵌入矩阵。这是为了避免数据泄露问题。其向量不存在于手套模型中的单词，我们将为它们生成随机向量，而不是给它们分配零值。如果我们再次执行代码片段，我们将使用random.seed()再次生成相同的随机向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="13ac" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">正如我们所分析的，我们的文章有不同的字数，所以让我们决定固定数据长度并填充它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="b518" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">4.数据管道</h2><p id="1cff" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们的任务是处理序列数据，我们将使用递归神经网络(更准确地说是LSTM！).主要方面是抓取数据，通过模型。</p><ul class=""><li id="8438" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">我们有1700字的文章，一起传递会导致内存崩溃问题。因此，我们将把每篇文章分成固定大小的块以及一些序列长度。为了实现这一点，我们将使用<a class="ae ky" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"> tf.data </a> API，它使我们能够从简单的、可重用的部分构建复杂的输入管道。</li><li id="67db" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将构建batch_generator，它根据给定的文章以固定的批量生成输入数据和相应的标签。</li><li id="7e41" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">它将首先生成一个具有固定数量序列的水平方向的批次，然后它将垂直生成一个固定数量的批量大小的文章。这种方法将保持每篇文章中单词序列的流动，因此我们的模型可以学习有意义的句子。</li><li id="1e2a" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将建立两个模型:简单的LSTM模型和基于注意力的编解码模型。对于每个模型，我们将创建一个单独的生成器。这里，我们将使用多对多模型，这样我们的输入数据及其标签的大小相同。</li></ul><p id="ffbc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">简单LSTM的批处理生成器</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="dda9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">基于注意力的编解码器的批处理生成器</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="f231" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">5.模型架构</h2><p id="ca8f" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们将用三种模型进行实验:1)。LSTM 2)。注意3)。GPT-2(生成式预训练变压器)。其中，我们将构建前两个模型。所以让我们开始一个一个的建造。</p><p id="837f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">一、LSTM模式</strong></p><p id="cb11" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">LSTMs在记忆序列信息和状态方面更加强大。Christopher Olah写了一篇很棒的博客来了解LSTM，你可以在这里找到。我们将使用有状态LSTM和return_state来维护训练和推理阶段的序列信息。因此，不再拖延，让我们开始使用子类和tf.keras.Model API构建模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="232e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上述类将实现LSTM功能，并返回模型生成的输出以及隐藏状态，我们将在预测中使用。</p><p id="ff51" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">损失函数和优化器</strong></p><p id="4cfd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy?version=nightly" rel="noopener ugc nofollow" target="_blank">稀疏分类交叉熵</a>作为损失函数。使用它的原因是为了减少一些计算时间，因为不需要在一次热编码中编码标签。我们可以只对我们的标签进行二维编码，这样可以加快这个过程。我们也将设置<code class="fe ok ol om on b">from_logits=True</code>,因此没有必要将我们的逻辑转换成概率，这将导致输出层没有激活函数。</p><p id="6d38" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在各种优化器中，我们将使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop?version=nightly" rel="noopener ugc nofollow" target="_blank"> RMSprop </a>，因为它为我们的数据集提供了更好的性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="f26f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">培训</strong></p><p id="99b5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将使用TensorFlow的<a class="ae ky" href="https://www.tensorflow.org/guide/eager#eager_training" rel="noopener ugc nofollow" target="_blank">急切执行</a>进行训练，因为它可以立即评估操作，而无需构建图形，并帮助我们调试我们的模型。我们将为tensorboard写下我们的损失和梯度，并为推断阶段保存我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="e3fb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们训练模型</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3d3e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将在下一节做预测和一些实验。</p><p id="ca65" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">二。注意力模式</strong></p><p id="0db6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Bahdanau在<a class="ae ky" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">的这篇</a>论文中讨论了基于注意力的方法。它基本上是在序列中给单词更多的权重。这些权重是通过另一个神经网络学习的。在这里，在<a class="ae ky" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">这个</a>博客里，你可以找到关于各种注意力算法的细节。在我们的模型中，我们将实现Bahadanau风格的注意。代码的灵感来自<a class="ae ky" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">这里</a>的任务是语言翻译，但在我们的任务中，我们可以试验它。</p><p id="b517" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所以不要再拖延了，让我们深入研究一下。</p><p id="c743" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">编码器</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4ead" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">编码器将获取输入的单词序列，并通过嵌入穿过LSTM层，然后返回其输出和隐藏状态。</p><p id="7dc5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">关注层</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><ul class=""><li id="1ea3" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">注意层用于根据编码器输出和解码器状态生成分数。Softmax将应用于这些分数，以获得0到1之间的权重值。权重通过网络学习，并与编码器输出相乘。</li><li id="860f" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">最后，重要的编码器输出被赋予更大的权重。</li></ul><p id="1f3c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">解码器</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c25b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">解码器用于将上下文向量与其输入连接起来，并生成输出。</p><p id="6481" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">培训</strong></p><p id="3a5f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将使用相同的损失函数和优化器。训练将如上所述进行，但是这里我们将向编码器提供第一个字序列，向解码器提供下一个字序列。所以我们的批处理生成器和第一个有点不同。在解码机制中，我们将使用“教师强制”机制。</p><p id="deff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">训练步骤</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="abb4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">验证步骤</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d5aa" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4c1d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">使用注意力模型的推理将在下一节讨论。</p><p id="e8bc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">三。GPT-2 </strong></p><ul class=""><li id="50d8" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">当Vaswani和来自Google Brain的团队提出标题为“注意力是你所需要的”的论文时，NLP感觉就像是一个瞬间。我们可以使用变压器解决尖端解决方案。你可以阅读<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">这篇</a>很酷的博客了解更多细节。</li><li id="d3a2" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">GPT-2是一个基于大型<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> transformer </a>的语言模型，拥有15亿个参数，在800万个网页的数据集上进行训练。GPT-2训练有一个简单的目标:预测下一个单词，给定一些文本中的所有以前的单词。你可以从<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多内容。了解GPT-2再爽<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">博客</a>等着你！</li><li id="7124" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">我们将使用预训练的GPT-2(小型-124米)和微调我们的文章数据。这就像NLP中的迁移学习。该模型非常强大，我们需要直接传递文本数据来训练它，它将完成所有的标记化、预处理并从中学习。</li><li id="fb89" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">太好了！！所以不要再耽搁了，让我们继续吧。代码摘自<a class="ae ky" href="https://github.com/minimaxir/gpt-2-simple" rel="noopener ugc nofollow" target="_blank">这里</a>和上一篇<a class="ae ky" href="https://minimaxir.com/2019/09/howto-gpt2/" rel="noopener ugc nofollow" target="_blank">博客</a>！</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="1070" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上述函数将根据我们的数据进行微调，并从中学习。对于实验，我们必须跳到推论部分，它就在下面:)</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="3fb5" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">6.推理</h2><p id="0a0e" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">对于推理部分，我们将使用来自检查点的最佳权重。所以我们来预测一下，看看吧。</p><p id="df4f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">一、LSTM模式</strong></p><p id="ec78" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将从logits的随机分类分布中抽取样本。并把它们转化成文字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="0eb9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将使用batch_size=1初始化我们的模型，并从检查点恢复训练好的<br/>权重。</p><p id="3729" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，我们还从LSTM传递隐藏状态，以根据单词的顺序维护状态信息。来看看成果吧！</p><pre class="kj kk kl km gt oo on op oq aw or bi"><span id="0ebd" class="mq mr it on b gy os ot l ou ov"># Prediction<br/>seed = 'In the history of spaceflight , only one nation'<br/>temperature = 1.0<br/>temp = predicttion_fun_using_disb(seed, stacked_model, temperature, 20)<br/>print(seed + ' ' + temp)</span><span id="6520" class="mq mr it on b gy ow ot l ou ov"># input : 'In the history of spaceflight , only one nation'<br/># Output:<br/>In the history of spaceflight , only one nation accord happy rubins<br/>attainable accelerates writing violating contract topography finder<br/>child walking concerns whitish barren equuleus generates undertakin<br/>gs parasites females</span><span id="a19e" class="mq mr it on b gy ow ot l ou ov">seed = 'Next to actually sending another spacecraft there'<br/>temperature = 0.5<br/>temp = predicttion_fun_using_disb(seed, stacked_model, temperature, 50)<br/>print(seed + ' ' + temp)</span><span id="f9eb" class="mq mr it on b gy ow ot l ou ov"># input: 'Next to actually sending another spacecraft there'<br/># output: Next to actually sending another spacecraft there occasional noam famines scenic magnification mandated elliptic glow satellites scribbled permit tre warmer phases amateurs quotes meantime hooks loss lambda negatively appendages teams entailing terminate refrangibilities pella tarantula andrian selection ark dataset utterly mx enabled robot surya bands hangar sarah distinguishing etnos thunder jhuapl analogous interfering mathematicians tuesday writings uniting</span></pre><ul class=""><li id="4f49" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated">从上面的结果中，我们可以看到，当给定一些输入种子词时，我们的模型正在生成句子，尽管所有生成的词都与句子的意思无关。但是这个模型是理解动词和形容词的位置。</li></ul><p id="bf8f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">二。注意</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="72f9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们看看输出。</p><pre class="kj kk kl km gt oo on op oq aw or bi"><span id="0d93" class="mq mr it on b gy os ot l ou ov"># Prediction</span><span id="23a6" class="mq mr it on b gy ow ot l ou ov">seed = 'In the history of spaceflight , only one nation'<br/>temperature = 1.0<br/>final_string = predict_function_attention(seed, 20, temperature)<br/>print(final_string)</span><span id="d234" class="mq mr it on b gy ow ot l ou ov"># input : 'In the history of spaceflight , only one nation'<br/># output: In the history of spaceflight , only one nation infall deadly subsy<br/>stems atkinson moderate himalayan lance abrahamic improve construct<br/>ing scistarter streak insufficient viking east deistic earthguide s<br/>hellfish especially repurposed megalodon pressures vac definition i<br/>nstantaneously komsomol zingers astrobee effect propane cancelled p<br/>rematurely icesheetmike sandstones application volcanoes cephalopod<br/>s misty send combes</span></pre><p id="b346" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从结果来看，可以说它能理解动词、名词、形容词的一点点排列。</p><p id="8bf6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是有意义的句子呢？我们的GPT 2号会处理好的。</p><p id="31b8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">三。GPT-2 </strong></p><pre class="kj kk kl km gt oo on op oq aw or bi"><span id="1e6f" class="mq mr it on b gy os ot l ou ov"># restore from drive</span><span id="b4d9" class="mq mr it on b gy ow ot l ou ov">gpt2.copy_checkpoint_from_gdrive(run_name='run1')<br/>sess = gpt2.start_tf_sess()<br/>gpt2.load_gpt2(sess, run_name='run1')</span><span id="9c2a" class="mq mr it on b gy ow ot l ou ov"># Generate</span><span id="5648" class="mq mr it on b gy ow ot l ou ov">prefix = 'In the history of spaceflight , only one nation'<br/>gpt2.generate(sess, length=50, temperature=0.5, prefix=prefix,<br/>nsamples=5, batch_size=5 )</span><span id="2a05" class="mq mr it on b gy ow ot l ou ov"># input : 'In the history of spaceflight , only one nation'<br/># output :</span><span id="e55d" class="mq mr it on b gy ow ot l ou ov">In the history of spaceflight , only one nation has achieved the fe<br/>at: the Soviet Union. But the Soviet Union did it. It was the first<br/>nation to launch a rocket from a launch pad. It was the first natio<br/>n to launch a rocket from a launch pad. It was the first to launch<br/>====================<br/>In the history of spaceflight , only one nation has managed to colo<br/>nize another planet: the Soviet Union. The Soviets launched their f<br/>irst lunar lander in November of 1972, and by the time it landed, t<br/>he Moon was still in lunar orbit. They also made several attempts t<br/>o colonize the Moon<br/>====================<br/>In the history of spaceflight , only one nation has held the title<br/>of sole superpower – the United States. The Soviet Union, who were<br/>the first nation to achieve space flight, was the only nation to ac<br/>hieve space flight. And although they were technically still in the<br/>process of achieving space flight, they<br/>====================<br/>In the history of spaceflight , only one nation has been able to la<br/>unch satellites into space. The Soviet Union, for example, was able<br/>to launch satellites into space on two separate occasions. However,<br/>the Soviet Union was not able to carry out their first lunar base u<br/>ntil the mid-1970s.<br/>====================<br/>In the history of spaceflight , only one nation has been able to ac<br/>hieve the feat: the Soviet Union. Their first attempts were in Febr<br/>uary of 1961, when the Soviet Union launched their first interconti<br/>nental ballistic missile (ICBM) at New York City. They successfully<br/>launched their ICBM on March 8<br/>====================</span></pre><p id="265f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">呜！正如我们所见，它生成的句子就像它真的了解空间一样！！再来看看其他一些结果。</p><pre class="kj kk kl km gt oo on op oq aw or bi"><span id="4cbc" class="mq mr it on b gy os ot l ou ov">gpt2.generate(sess, length=50, temperature=0.7,<br/>prefix="ISRO has launched Chandrayaan II", nsamples=5,<br/>batch_size=5)</span><span id="a33c" class="mq mr it on b gy ow ot l ou ov"># input  :'ISRO has launched Chandryaan II'<br/># Output : <br/>ISRO has launched Chandrayaan II at an altitude of only about 20 km<br/>(15 miles) at a time from the launch pad in Chandrayaan, India. The<br/>lander and its two-stage lander will be launched in the evening of<br/>March 18th at 13:03<br/>====================<br/>ISRO has launched Chandrayaan II on September 18, 2019. This missio<br/>n will launch from the Vastal Pad, the Space Launch Complex 41 (SLC<br/>-41) in Chandrayaan, India. The satellite was launched by the India<br/>n Space Research Organization (ISRO) under<br/>====================<br/>ISRO has launched Chandrayaan II as it reaches its second mission l<br/>aunch from Pad 39A at the end of this month.This comes just a week<br/>after the probe launched from its command module on April 18, and w<br/>ill launch on Pad 39A at the end of this month. And<br/>====================<br/>ISRO has launched Chandrayaan II to its orbit earlier this month. C<br/>handrayaan II is the third mission to orbit the Earth from home and<br/>the first space mission to orbit the Moon. The probe’s main goal is<br/>to study the Moon’s interior structure, analyze<br/>====================<br/>ISRO has launched Chandrayaan II mission on the orbit of DSO on Jul<br/>y 5, 2019.The maiden blastoff of this mission on the Salyut-2 (D-2)<br/>mission is slated for July 5, 2019. The launch window for DSO runs<br/>from July<br/>====================</span></pre><p id="0dc0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以看到，它也能够连接ISRO-印度空间研究组织的完整形式。它还可以关联2019年7月，因为这是发布月。我们可以看到它真的很强大。</p><p id="3eba" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在让我们给出一个与空间无关的句子，看看它会做什么。</p><pre class="kj kk kl km gt oo on op oq aw or bi"><span id="3625" class="mq mr it on b gy os ot l ou ov">gpt2.generate(sess, length=50, temperature=0.7,<br/>prefix="Shah rukh khan has been offered a film",<br/>nsamples=5, batch_size=5)</span><span id="9dec" class="mq mr it on b gy ow ot l ou ov"># input : 'Shah rukh khan has been offered a film'<br/># output :</span><span id="f1aa" class="mq mr it on b gy ow ot l ou ov">Shah rukh khan has been offered a film called “Liftoff of Light“. I<br/>n the film, a group of mission controllers (the crew members and th<br/>e scientists) are shown how the probe will be launched en route to<br/>the Moon. The mission team is also shown how<br/>====================<br/>Shah rukh khan has been offered a film about the events of the tim<br/>e, and the circumstances leading up to it. In it, the filmmaker ask<br/>s the reader to “walk on the Moon.” It’s a happy-couple film, for s<br/>ure, but it�<br/>====================<br/>Shah rukh khan has been offered a film in the form of an interestin<br/>g documentary film. “Khan’s ‘film’ is about the issues facing the<br/>Asian people and the cause of human rights,” said Shah rukh, in rep<br/>ly to a<br/>====================<br/>Shah rukh khan has been offered a film about the first lunar landin<br/>g, called The Moon Under the Moon. This year sees the first opport<br/>unity for Moon tourism in 2019, when the launch of the China Lunar<br/>Exploration Program (CLEP) is scheduled to happen. The Chinese Luna<br/>r Exploration Program (<br/>====================<br/>Shah rukh khan has been offered a film titled “Mysteries of the Uni<br/>verse” that includes a holographic image of a star that is also the<br/>subject of the film. The film will be released on May 29th, 2019.<br/>A cow’s head has<br/>====================</span></pre><p id="ba6c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">它还可以生成与空间相关的电影标题，并且可以包括关于电影的细节，而无需教导。是不是很神奇？</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="c20e" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">7.比较</h2><p id="8dea" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们用不同的批量大小、序列长度、lstm单位、时期和学习率对前两个模型进行了实验。我们在GPT-2模型中损失最小，关于生成的句子，我们已经看到GPT-2是最好的。下面是对比表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/df21bff9f1078b8cf2c8289d8852fd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*Ka9zEomqUnRb0pC0vMj-6Q.png"/></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="2a11" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">8.结论</h2><ul class=""><li id="8469" class="mc md it li b lj nj lm nk lp oy lt oz lx pa mb no mi mj mk bi translated">当我们为更多的时期训练LSTM和注意力模型时，由于我们的数据较少，它开始过度拟合。</li><li id="b20b" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">当我们提高学习率时，损失就会增加，这表明我们可能会超出最优解空间。当我们降低学习速率时，损失会非常缓慢地减少，这意味着学习进行得非常缓慢。</li><li id="0c86" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">如果我们保持较高的序列长度，我们的模型影响消失梯度问题。</li><li id="b0f1" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">在GPT-2的情况下，它已经用大量的数据进行了训练。GPT-2有更多的参数(124米)，这是非常大的比我们以上两个模型。它还使用了自我关注的概念，这有助于关注相邻的单词序列。</li><li id="5cd6" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb no mi mj mk bi translated">由于强大的架构，GPT-2提供了比其他型号更好的性能。也收敛的更快。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="4d54" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">9.未来的工作</h2><ul class=""><li id="cf68" class="mc md it li b lj nj lm nk lp oy lt oz lx pa mb no mi mj mk bi translated">我们可以训练大量的数据来得到更好的结果。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="1209" class="mq mr it bd ms mt mu dn mv mw mx dp my lp mz na nb lt nc nd ne lx nf ng nh ni bi translated">10.参考</h2><ul class=""><li id="fa97" class="mc md it li b lj nj lm nk lp oy lt oz lx pa mb no mi mj mk bi translated"><strong class="li iu">论文:</strong></li></ul><p id="a366" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[1]陆，朱Y，张W，王J，于Y .【神经文本生成:过去、现在与超越】。2018年3月15日。</p><p id="5cdd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[2]陈MX，李BN，班萨尔G，曹Y，张S，陆J，蔡J，王Y，戴AM，陈Z，孙T. <a class="ae ky" href="https://arxiv.org/abs/1906.00080" rel="noopener ugc nofollow" target="_blank"> Gmail智能撰写:实时辅助写作</a>。第25届ACM SIGKDD知识发现国际会议论文集&amp;数据挖掘2019年7月25日(第2287–2295页)。</p><ul class=""><li id="404c" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated"><strong class="li iu">博客:</strong></li></ul><ol class=""><li id="8d30" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated"><a class="ae ky" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li id="999d" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/text/text_generation</a></li><li id="8a94" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></li><li id="b181" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/better-language-models/</a></li><li id="f9a5" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></li><li id="dc31" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-gpt2/</a></li><li id="9402" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">https://minimaxir.com/2019/09/howto-gpt2/<a class="ae ky" href="https://minimaxir.com/2019/09/howto-gpt2/" rel="noopener ugc nofollow" target="_blank"/></li></ol><ul class=""><li id="cc68" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated"><strong class="li iu">平台:</strong></li></ul><ol class=""><li id="188d" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated"><a class="ae ky" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/</a></li></ol><ul class=""><li id="8214" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb no mi mj mk bi translated"><strong class="li iu">课程:</strong></li></ul><ol class=""><li id="115d" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated"><a class="ae ky" href="https://www.appliedaicourse.com/course/11/Applied-Machine-" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/course/11/Applied-Machine-</a><br/>学习-课程</li></ol><p id="1ec3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">感谢您的关注。如果你有任何想法，可以留下评论、反馈或任何<br/>建议。</p><p id="0c5e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你可以在我的Github repo上找到完整的代码(<a class="ae ky" href="https://github.com/khushi810/Text-Generation" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="20c3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">很高兴在<a class="ae ky" href="https://www.linkedin.com/in/khushali-vithani/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>:)上与您联系</p></div></div>    
</body>
</html>