# 医疗人工智能的上市后责任

> 原文：<https://towardsdatascience.com/postmarket-responsibilities-for-medical-ai-56ba67fd8cc?source=collection_archive---------35----------------------->

![](img/2dbfa61ff7fa6cf6f55f9b321817d9b4.png)

metamorworks/Shutterstock.com

## 第二部分——在医疗人工智能商业化方面，行业应该做些什么

在本系列文章的第一部分中，我描述了 FDA 和行业之间关于 FDA 对医疗人工智能的适当监督程度的紧张关系。我解释说，目前看来 FDA 可能会要求国会给予更多的上市后权力，以便能够以更深入的方式监督医疗人工智能的商业化，尽管事实上 FDA 已经拥有相当大的权力。

在第二部分，我提出了另一种选择。我建议行业采用最佳实践来确保病人得到最好的护理，可以说是自我调节。在[第三部分](/postmarket-responsibilities-for-medical-ai-5cd43521f546)，我将把 FDA 的监督限制在它现有的法定权限内。

在商业化阶段，行业应在四个方面积极采取措施，以确保其基于人工智能的医疗设备的安全性和有效性。

# A.追求增强的上市后警惕性

将自适应特别是自主的人工智能算法部署到市场上需要部署它们的公司大大提高上市后的警惕性。根据定义，自适应算法在使用时进行调整。作为质量保证和风险缓解的问题，也作为持续改进产品的方法的研究问题，公司应该密切监控其算法在市场上的性能。

应该怎么做？

在回答这个问题之前，重要的是要理解为什么监控很重要。识别可能出错的事情告诉我们什么样的监控是必要的。

这里我大量借用了奥姆·德什穆克的一篇优秀的[文章](https://www.analyticsvidhya.com/blog/2019/10/deployed-machine-learning-model-post-production-monitoring/)“部署你的机器学习模型？以下是你需要知道的后期制作监控”2019 年 10 月 7 日。

下面是几个不同的场景，它们会显著改变已训练算法的性能。

**在域中但看不见数据**。我们开始意识到每个人是多么的不同。个性化医疗基于我们对人类基因组的深入了解，以及种族、民族、性别等因素如何对诊断和治疗决策产生重大影响。这使得训练集具有来自算法最终将使用的所有群体的数据变得尤其重要。例如，很有可能一个训练有素的算法可能不包括美洲土著人口，当该算法用于这些人口时，其性能可能会下降。从某种意义上说，在这种情况下的训练会对美国土著人有偏见。

**输入数据不断变化的性质**。自适应算法必然需要新数据。并且很容易预测新数据可能不总是以与算法最初被训练的数据相同的方式被创建。如果自适应 AI 分析放射图像，如果产生图像的技术发生变化，这些变化可能会导致自适应算法以不同方式解释数据。很容易想象技术引发的变化，以及数据生产中的社会变化。医疗专业人员甚至患者自己或其护理人员收集数据的方式可能会改变，从而影响算法解释数据的方式。那些花大量时间清理数据的人明白，从字符串转换为整数这样的小事会对算法的运行产生深远的影响。

**数据解释的变化。**现有数据的含义会随着时间而变化。即使数据点本身不会改变，但它们的意义会因社会或环境因素而改变。在医疗环境中，可以想象电子健康记录中的单词的含义会随着医生和其他健康护理人员的实践的改变而逐渐改变。随着时间的推移，医生可能会使用相同的词语，但有了新的含义。这不同于上述数据发生变化的问题。

**部署在意想不到的环境中的机器学习系统。**这在医疗技术领域非常常见，因为医生和其他医疗保健专业人员会尝试现有技术的不同用途。你可以想象，原本打算用于追踪流感症状的软件，在某些医生的手中，可能会被用于追踪 COVID 19 症状。随着医疗专业人员决定探索其他领域的实用性，这意味着算法功能所基于的数据可能会发生变化，从而对产品的整体性能产生影响。

**缺失数据。**对于使用各种数据的算法，某些数据流可能会被切断或中断。

**模型漂移。**如果不关注原因，模型可能会因为各种原因而表现不佳。

**恶意的行为。**一旦人工智能设备处于“野生状态”，恶意行为者可能会故意引入数据集，故意扭曲设备的行为，这可能比传统的“黑客攻击”更难检测到(例如，“闯入”系统并改变参数)。

关键是我们需要明白，当一个算法在现实世界中使用时，会发生影响性能的事情。不难想象。

那么公司在上市后监管方面到底应该怎么做呢？也许我们都同意的一件事是，一种尺寸不适合所有人。然而，这些软件开发人员可以遵循一些通用原则，以确保他们能够充分地监控他们的产品。

概括地说，我们可以把监控分为两种不同的类型:主动的和被动的。先说主动。

# 1.主动模型监控

Deshmukh 先生建议我们根据一组关键指标持续跟踪机器学习模型的性能，并生成特定的基于事件的警报。虽然德什穆克先生笼统地谈到了这个问题，但我想重点谈谈医学背景。

这项技术的目标是“识别哪些输入样本明显偏离了训练数据中的模式，然后让人类专家仔细检查这些样本。”问题是，感兴趣的模式在很大程度上取决于数据的领域、问题的性质以及所使用的机器学习模型。

假设我们有一个算法，它正在观察医院里一个病人的 10 种不同的生命体征，着眼于识别这些生命体征何时达到表明病人正在变得败血症的程度。实现这种监控的一种方法是统计地查看这些生命体征如何正常流动，并设置统计分布或其他参数，如果超过这些参数，将向算法开发者发出警报。请注意，这些警报不同于用于确定特定患者何时需要医疗护理的警报。这些是基于统计数据的警报，表明算法正在分析的数据类型发生了一些变化。这样的警告将会给为开发人员工作的人一个机会去调查是否有需要更新或纠正的地方。

除了检查统计异常之外，我们可能会进行其他形式的实时数据检查，例如，确保模型仍然接收到它需要的所有不同的数据。这对于分类变量和数值变量都是正确的。该公司可以检查数据，以确定在活动模型中是否存在任何特征依赖性。

该公司还可以在客户的配合下进行模型预测和性能测试。在这种情况下，公司将需要从最终用户那里获得有关预测事件是否会发生的数据。如果软件开发人员可以协商访问这些数据，公司就可以不断地评估其模型的性能。这在医学上通常是困难的，因为后果可能是几个月甚至几年之后。然而，在某些情况下，预期事件很可能在短时间内发生(或不发生),因此可以持续评估性能。如果公司无法获得访问权限，那么简单地跟踪预测的总体分布以查看该分布是否会随时间而变化是很重要的。

# 2.反应式模型监控

反应式模型监控非常类似于医疗设备制造商已经在做的事情。根据法律要求，这些制造商拥有收集和分析用户向公司报告的问题的系统。然而，在人工智能环境中，反应式模型监控可以在更系统的基础上进行。

客户可能会报告不准确的结果。这些中的一定数量是可以预料的。很可能，在模型的训练和验证之后，该公司了解到该模型在一定的统计精度水平上表现得不够完美。因此，公司知道有时模型会产生不正确的预测。

虽然这些客户投诉在意料之中，但它们也创造了持续改进的机会。通过收集和分析这些报告，公司可以寻找系统的根本原因，以及在未来提高准确性的方法。

话虽如此，这也是很有可能的，公司需要注意这样一个事实，即他们不应该通过过度适应来追求业绩。很容易想象，一家公司可能会通过善意的行动，认为它正在根据对某一组不准确预测的反馈进行改进，但无意中它损害了算法的未来整体性能，降低了它在以前表现良好的领域的准确性。

毫无疑问，公司可以从失败中学到很多东西，持续分析失败的趋势以及失败的总体水平是一项富有成效的工作。挖掘根源非常重要，而且说起来容易做起来难。

如果需要对算法进行实质性的修改，该公司需要让 FDA 重新参与讨论，因为没有 FDA 的事先审查和批准，对通过某个阈值的产品的修改是不允许的。这可以包括，例如，算法的重要再训练。

# B.与用户共享信息

许多术语都涉及到人工智能可理解性的一般主题。人们谈论“透明度”、“黑盒”、“可解释性”、“可解释性”等等，而我自己的感觉是，这些词并没有被一致地使用。事实上，在我职业生涯的过去几年里，我在如何使用这些词上并不一致。所以我现在想用我自己的语言说得更清楚些。

关于 FDA 法律，我认为有两个密切相关的概念也是可以清楚区分的。我选择下面的词来描述这两种不同的想法有点武断，因为坦率地说，没有准确的英语标签可用。我将把这些区别建立在 FDA 法律中的重要原则上。

# 1.结构

# a.透明度

FDA 的法律关注的是在用户决定产品是否适合特定用途时提供给他们的信息。FDA 高度重视这一决定，因为正确使用 FDA 监管的产品是其安全性和有效性的关键决定因素。事实上，FDA 最常提起的执法行动之一是对标签外促销的索赔，这是一项法律指控，即销售者提供的信息鼓励产品的使用范围超过 FDA 授权的范围。

需要明确的是，医疗保健专业人员可以随心所欲地使用产品。FDA 的主要关注点是确保医疗保健专业人员从制造商处获得真实的、无误导性的信息。然而，在 FDA 的世界观中，这也意味着将制造商提供的信息限制在 FDA 合法授权的范围内。

我将把人工智能开发者分享公司与这一使用决策相关的信息的义务称为“透明度”。我在这里使用“透明”这个词，因为在这个上下文中，透明意味着分享你所知道的。我稍后会解释，这与开发新信息以解决知识缺口的义务形成对比。透明度在很大程度上是被动的，不需要难以收集的信息。字面意思是不要把信息留给自己。

与任何医疗设备一样，对于基于人工智能的医疗产品，产品的安全性和有效性信息是潜在用户在决定产品是否适合给定的计划用途时需要了解的重要信息。但与其他医疗设备不同的是，随着算法根据新数据进行调整以及性能的发展，这些信息将不断变化。因此，与传统医疗设备相比，人工智能开发人员需要保持这些公共信息最新。

# b.可解释性

除了使用前的适宜性决定，FDA 法律中还有一个相对较新的重点，即医疗保健专业人员是否能够理解软件生成的关于特定患者使用后的建议的基础。这一新的关注点来自 2016 年的一项法律，即《21 世纪治愈法案》。该法案的第 3060 条明确指出，任何授权医疗专业人员独立审查诊断或治疗建议的基础的软件，只要该专业人员不必主要依赖软件建议来做出关于单个患者的临床诊断或治疗决定，则不受监管。在这个世界上，FDA 的规定可能意味着几年的延迟和几十万美元的额外费用，这一条款创造了相当大的激励，以确保用户可以审查建议的基础，而不是依赖它。

我将通过确保用户能够充分理解特定建议的基础来称之为避免 FDA 监管的机会:“可解释性”正如我所使用的术语，可解释性不同于透明性，因为透明性是被动的，专注于使用前的适用性决策(实际上是共享已经存在的信息)，可解释性在人工智能的情况下通常要求在算法产生结果后主动创建新的信息。我小心翼翼地措辞以关注结果，认识到可解释性的基础显然必须在软件使用之前就建立起来。

法令并不局限于利用人工智能的软件，对于专家系统来说，可解释性更容易实现，例如，仅仅应用临床指南形式的标准知识来提出建议。非人工智能中的可解释性通常意味着简单地构建软件，以便用户可以轻松地回到知识的原始来源，理解推荐的基础。不幸的是，在人工智能的世界里，解释是极其困难的。

换句话说，可解释性是事实之后的信息，我应该考虑评估我是否相信算法得到了正确的答案。如果算法说我 Brad 很可能有 COVID 19，我该相信吗？

患者和临床医生可能会有关于人工智能生成的建议的基础的具体问题，包括以下内容:

*   算法是如何得出这个结论的？
*   在我的特殊情况下，算法考虑了哪些信息？
*   该算法最依赖什么症状和生理测量作为其结论的基础？
*   为什么算法在得出关于我的结论时如此重视这些症状？
*   我有没有不符合诊断的症状，因为某些原因被忽略了？
*   算法以最终推荐的形式考虑并拒绝了哪些可能性？

重要的是要明白透明性和可解释性是表亲。良好的透明度为更有效的可解释性奠定了基础。但是，虽然它们是相关的，但它们显然是不同的。它们服务于不同的目的，具有不同的监管后果，并且以截然不同的方式产生。

当然，特别是在过去的五年里，在医学人工智能的可解释性方面有大量的研究。该研究面临的核心挑战是，人工智能算法不会像人类一样思考。人类寻找因果关系，而算法寻找关联。在这种意义上，仅仅叙述算法发现 X 和 y 之间的关联是不够的。人类需要更深入的解释，理想地解决因果关系或至少上述问题。

重要的是，法令不要求算法是可解释的。我注意到文献中围绕这一点有些混乱。一些评论家抱怨说，作为其监管过程的一部分，FDA 坚持可解释性。他们不是。相反，如上所述，实现可解释性的一个动机是人工智能开发者希望避免 FDA 的监管以及所有随之而来的成本和延迟。然而，如果一个软件开发者想制造一个不可解释的黑盒，它可能会。该公司只需通过 FDA 的流程，提供足够的证据来证明该算法的安全性和有效性，在这种算法中，用户将无法理解推荐的基础。因此，即使是受管制的产品，也有动机在标签或其他地方向用户提供至少一些解释，作为降低风险的工具，减少所需的安全性和有效性的证据。应该注意的是，尽管 FDA 可能不会坚持向最终用户解释，但作为一个科学机构，FDA 在没有对因果关系有所了解的情况下，将很难授权人工智能的上市。

# 2.选择最佳政策方案时需要平衡的因素

正如在本系列的第一部分中所解释的，找出什么对病人是最好的需要平衡许多因素，这些因素在某些情况下是相互冲突的。我们希望确保用户——通常是医疗保健专业人员(越来越多的患者)——能够在使用人工智能算法之前了解它的优势、劣势和适当使用，以便他们能够在正确的情况下将它用于正确的患者。为了确保知情同意，我们还需要确保患者了解算法的性质，如果有必要依赖黑盒算法，他们也了解风险。与此同时，在一个相互冲突的优先事项中，在我们共享的信息的透明度和可解释性方面，我们需要尊重那些其数据被算法训练的人的隐私利益。

还有其他领域会出现冲突。例如，我们需要平衡这种上市后警惕所带来的负担与创新和获得潜在救命人工智能技术的需求。这意味着不要求更多的监控和更多的纠正措施，但也意味着不要求开发商与公众分享更多不必要的信息，商业秘密信息可能会损害他们将产品商业化的竞争能力。医疗保健是一个行业，它遵循的经济规则与其他行业相同。这意味着如果我们强迫人工智能开发者分享他们的商业秘密，他们可能无法在竞争中生存，因此不愿意投资未来的发展。最后，所有的发展都需要以生产一种具有成本效益的产品的方式进行，这样我们的集体经济资源就能提供这种产品。

# 3.提议:活标签

基于这些竞争因素，我建议我们追求一个“活标签”的概念我所使用的“活标签”这个术语，意味着标签会不断地更新信息，包括透明度和可解释性。通常，对于医疗器械，标签是 FDA 在上市前审查中同意的一组固定的词语。随着时间的推移，有一定的灵活性来改变它们，但不是很多。有了这个活标签的概念，将会有一个与 FDA 的预许可协议，即开发者将更新标签以反映 ABC 区间上最新的 X、Y 和 Z，而不需要新的监管提交。

基于人工智能的医疗设备需要投入更多精力来持续更新标签信息。总的来说，对于一个不断进化的自适应算法，在透明性下跟踪的指标需要不断更新。虽然可解释性可能是静态的，因为软件必须在每次使用时被编程来解释它自己，但是如果可解释性包括动态元素，那么这些动态元素需要保持最新。

在我在第一部分引用的 FDA 报告中，你看到 FDA 提出了本质上相同的东西，即开发者不断共享更新的信息。所以这个概念对我来说并不新鲜，也不独特。然而，我确实有一个与 FDA 不同的实施方案。我们的观点似乎有两点不同。首先，FDA 似乎打算与该机构持续分享至少部分信息。第二，FDA 考虑的可能是更深入、更广泛的信息共享，而不是关键统计数据的汇总。

我建议软件开发人员以概要形式共享信息，而不(1)违反患者保密原则，(2)共享会损害其商业秘密地位的信息，或(3)需要过度昂贵的信息收集和处理。根据人工智能的类型和软件产品的预期用途，确切的方法会有很大的不同。这里有一些公司可以考虑的一般原则。

# a.透明度

市场上基于人工智能的医疗解决方案的开发者应该让现有和潜在的客户了解当前的性能。这意味着共享作为上述监控结果而跟踪的关键指标的适当摘要。这不仅仅是当公司认为发生了有趣的事情时的偶尔更新。这意味着，在适当的时间间隔内，通过某种机制(如公司网站)或更直接地通过产品本身更新关键信息是理所当然的。

这是除了与理解人工智能模型的适当使用相关的信息之外，这些信息可能不会改变，但仍然需要共享。不变的信息包括历史上算法是如何被训练的，这包括关于数据是如何被获取和使用的细节。这是关键，因为基于人工智能的产品的每个用户都需要判断原始训练集中是否存在潜在的任何偏差，这会影响给定的使用。例如，如果我清楚地认识到算法不是针对青少年训练的，我知道如果我想对青少年使用该产品，就会有一些无法量化的风险，即结果将是不准确的。

用户可能还想知道数据是如何清理的？模型训练的特征或维度是什么？数据增加了吗？虽然这些数据显然更具技术性，但一个有见识的用户很可能会对该算法对其特定用途的适用性得出结论。

同样重要的是，公司在质量上要透明，以监控和管理产品上市后的质量。用户会对公司如何允许算法适应、适应有什么限制以及公司正在进行什么质量检查和测试非常感兴趣。随着产品的发展，这些都提供了与使用产品相关的风险的洞察力。

例如，如果我知道该产品被标记为供成人使用，但我看到该公司在程序上没有将新数据仅限于成人，我知道随着时间的推移会有一些风险，即其他用户会添加青少年数据，这将改变产品的性能。再比如，允许算法多长时间适应一次？在像 COVID 19 这样的新传染病发作时，了解数据集受 COVID 患者影响的速度有多快，有助于我理解算法的性能在疫情期间可能会如何变化，或者至少变化有多快。

正如为《福布斯》杂志撰稿的评论员 Ron Schmelzer 所建议的那样，包含模型版本化可能也很重要。他建议“那些生产模型的人也应该清楚和一致地说明模型将如何版本化，模型版本化的频率，以及如果新模型开始表现不佳时使用旧模型版本的能力。”

为了支持正确使用算法的决策，FDA 已经规定了标签中要包含的大量信息，我在此不再赘述。但它包括开发者的预期用途以及在 FDA 批准前用于验证该技术的临床试验结果。

# b.可解释性

用于实现可解释性或 XAI 的确切机制因数据科学领域而异。例如，使用图像分析可能相对容易实现可解释性，软件可以被编程来突出显示感兴趣的区域，即使它不能精确地阐明为什么该区域是感兴趣的。

相比之下，对于利用电子健康记录数据的数据科学模型，XAI 方法可能有点困难，包括以下策略:

*   特征交互和重要性，包括对特征重要性和成对特征交互强度的分析。
*   注意机制，包括在序列中找到与预测任务最相关信息的一组位置的能力。
*   数据降维，正如你可能猜到的，它集中在最重要的特性上。
*   知识提炼和规则提取，将知识从复杂而精确的模型转移到更小、更简单、更快但仍然精确的模型。
*   本质上可解释的模型，它依赖于保留不太复杂的机器学习方法的可解释性，同时通过增强和优化技术来提高它们的性能。[【2】](#_ftn2)

指出关键的有统计学意义的特征类似于一个放射科医生指着图像上的一个地方说，“我以前见过那个，它是恶性的。”我们在医学上“知道”的很多东西只是联想，而没有对因果关系的更深入理解。创新的数据科学家也在开发全新的方法，如视觉模型，以更准确地解释数据科学的结果。

总而言之，为了帮助用户解释人工智能的输出，开发人员可以:

解释能解释的。不要把问题扩大化。如果该软件实际上是专家系统和机器学习的混合，如果某个特定的建议是基于专家系统的，例如简单地查找患者 EHR 中的药物过敏，遵循简单的计算模型或推荐一种治疗方法，因为它更便宜，那么该建议应该会揭示原因。

对于真正依赖于人工智能的软件输出，尽可能精确地陈述支持特定建议的特定关联。如前所述，根据新的研究，在某些情况下，可以确定哪些特性对推荐的影响最大。

传达特定建议的可信度。不确定性对于沟通以做出最佳决策和法律上的知情同意总是很重要的。

# C.向 FDA 报告符合法律可报告性测试的问题

销售 FDA 监管医疗器械的公司必须建立质量体系。该质量体系包括要求这些公司有投诉处理程序。用户投诉可能导致问题的识别。如果这些投诉导致确定了根据所谓的医疗器械医疗器械报告(MDR)法规(21 CFR Part 803)触发报告义务的情况，公司需要向 FDA 报告这些情况。

这种报告义务之所以存在，是因为在某种严重程度上，如果 FDA 认为需要比公司本身计划的更重大的补救措施，并监控同一法规或产品代码中规定的其他产品中类似问题的普遍性，则需要向 FDA 报告。美国食品和药物管理局在人工智能应用于医疗保健之前制定了 MDR 法规，因此可能需要更新指南，以明确在人工智能情况下何时触发报告义务。然而，正如我将在第三部分详细讨论的，FDA 不应该扩大 MDR 义务。

MDR 报告阈值反映了信号和噪声之间的谨慎平衡。软件开发人员有责任从噪音中提取信号，并向机构报告足够严重的信号。这有助于确保该机构接收足够的信息，但不要接收太多的信息，否则会使该机构的系统负担过重，坦率地说，会导致 FDA 错过信号。

如上所述，也有向 FDA 报告的商品化产品的重大变更，但没有上升到需要 FDA 上市前审查的水平。公司显然也应该做这些报告。

# D.允许 FDA 检查公司的数据和记录

除了报告之外，质量体系还要求制造商记录设备的每一项变更，并记录投诉调查和其他经验数据。事实上，几乎所有 FDA 感兴趣的信息都可以在质量体系的记录中找到。

《联邦食品、药品和化妆品法案》第 704 条授权 FDA 在合理的时间、合理的限度内，以合理的方式对生产医疗器械的组织进行检查。这些现场访问的范围允许机构检查质量系统信息以及物理制造和质量系统，以确保符合性。因此，FDA 可以对包含相关信息的质量体系记录进行物理检查。根据法律，医疗人工智能开发者需要允许这些现场 FDA 检查。然而，正如我将在第三部分中解释的那样，FDA 不应该试图扩大法令允许的范围。

在[第三部分](/postmarket-responsibilities-for-medical-ai-5cd43521f546)，我将为允许 FDA 更多地参与人工智能商业化的监督所造成的损害陈述理由。FDA 当然是一个重要的监管机构，但作为一个天生保守的监管机构，FDA 并不打算参与人工智能商业化所需的日常决策。

[[1]](#_ftnref1) 我要指出的是，虽然我首先是作为一名律师来处理这个问题，其次才是作为一名数据科学家，但数据科学家能够独立地看到并理解这种差异。参见 Hlozinger 等人的《医学中人工智能的可因性和可解释性》第 4 页，教科书题为《有线数据挖掘知识发现 2019》

[【2】](#_ftnref2)Payrovnaziri，at L，Review，使用真实世界电子健康记录数据的可解释人工智能模型:系统范围审查，美国医学信息学协会杂志，2020 年

[【3】](#_ftnref3)Lamy 等，可解释的乳腺癌人工智能:一种可视化的基于案例的推理方法，人工智能在医学中，2019。