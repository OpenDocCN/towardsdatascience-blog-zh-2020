<html>
<head>
<title>Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/q-learning-a4f1bcec58be?source=collection_archive---------18-----------------------#2020-03-07">https://towardsdatascience.com/q-learning-a4f1bcec58be?source=collection_archive---------18-----------------------#2020-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7589" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">强化学习之旅</a></h2><div class=""/><div class=""><h2 id="f01a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">强化学习的早期突破——非策略时差控制方法</h2></div><p id="57a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">欢迎来到我的强化学习专栏，在这里我花了一些时间来回顾一些非常有趣的概念，这些概念围绕着用计算方法学习的本质。正如大多数学习一样，它与环境有互动，正如萨顿和巴尔托在<em class="ln">强化学习:介绍中所说的那样，“从互动中学习是几乎所有学习和智力理论的基础思想。”</em></p><p id="ab3c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我的上一篇文章中，我们回顾了在<em class="ln">时差</em> (TD)学习中的政策控制方法，特别是——<em class="ln">Sarsa</em>，在这种情况下，我们在一个步骤之后朝着估计的回报更新我们的价值函数，从一个状态-动作对移动到下一个。今天，我们将学习<em class="ln"> Q-Learning </em>，一种非策略 TD 控制方法。一些概念在之前的帖子里解释过，你可以在这里找到<a class="ae lo" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank"/>。我用来学习这个很酷的话题的资源链接会在文章的底部。</p></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="95d1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ln">问</em>-学习的概念很容易理解:我们根据我们的行为政策选择下一步行动，<em class="ln">但是</em>我们也会考虑如果我们遵循我们的目标政策，我们可能会采取的替代行动。这允许行为和目标策略改进，利用动作值<em class="ln"> Q(s，a) </em>。该流程的工作方式类似于不符合政策的<a class="ae lo" rel="noopener" target="_blank" href="/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032">蒙特卡罗方法</a>。更新如下所示:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/af8c5104dec44b83315e033c3b3201de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*3diZ6Niwnq5kykgN2NsIow.png"/></div></figure><p id="8048" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如您所见，我们的 Sarsa 更新被替换为<em class="ln">q</em>-值<em class="ln"> Q(Sₜ，Aₜ】，</em>在目标策略下朝着奖励加上<strong class="kt jd">备选</strong>行动的下一个状态的贴现值的方向更新了一点。这是之前的<a class="ae lo" href="https://medium.com/@reubena.kavalov/my-journey-into-reinforcement-learning-part-3-dynamic-programming-3cb8a8d0815c" rel="noopener">贝尔曼方程，但是现在对于<em class="ln"> Q </em>来说，不需要重要性抽样。</a></p><p id="9cef" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们对 MC 方法的偏离策略控制所做的那样，我们将使目标策略<em class="ln">π</em>T32】贪婪关于我们的价值函数<em class="ln"> Q(s，a) </em>在每一步:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/ae47629fa309a22e46fb54daac016cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*aZ9_q_TLXwBZDop3mwahNw.png"/></div></figure><p id="c8d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的行为策略<em class="ln"> b </em>是<strong class="kt jd">ε-贪婪</strong>关于我们的价值函数<em class="ln"> Q(s，a) </em>，允许一点探索，同时仍然大致遵循一条明智的道路。</p><p id="e376" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过将此代入我们的贝尔曼方程，我们可以看到<em class="ln">Q</em>-学习目标简化为以下内容:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/be79c27f06c3757c7a438ba5fab42043.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*JjExhPaTkB4LGWSPbmnRvA.png"/></div></figure><p id="3940" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种算法也可以称为<em class="ln"> SARSAMAX </em>，因为它选择在采取步骤后具有最大可用值的动作。这个简单的图表清楚地显示了这一点。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/44ac5336c4c388b36064626833e1f96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00GnaYEjVHQbea6Ul_27vQ.png"/></div></div></figure><p id="6114" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面是算法伪代码:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/7e493c0ab96771233fe9a96a9ad6490f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgEKefbQww0-ADFrcJ7mOg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><a class="ae lo" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/RLbook2018.pdf</a></p></figure></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="408e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">像往常一样，让我们把这个例子翻译成另一个<a class="ae lo" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>环境<em class="ln"> Cliffwalking </em>。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/20ffbfc5dd52e66445c63a1625d5c2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*CD2E94bMwI68SPYE3tkH7Q.png"/></div></figure><p id="b59e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与 gridworld 场景一样，目标是在给定特定起点(<strong class="kt jd"> S </strong>)的情况下达到特定目标(<strong class="kt jd"> G </strong>)。代理人可以在四个基本方向上移动，代理人每走一步，就会获得<strong class="kt jd"> -1 </strong>的奖励。因此，就像我们的 gridworld 示例一样，存在着更快达到目标<em class="ln">的动机</em>。这个环境的变化包括增加了悬崖，如果代理人踩到它所占据的任何空间，它会将代理人重置回左下角，并给予一个重大惩罚，奖励<strong class="kt jd"> -100 </strong>。从开始到结束显然有多条安全路线，但只有一条最优路线(巧合的是，这条路线是最“危险”的)。使用非策略时间差控制来控制我们的代理为我们提供了一些很好的结果——我们可以观察到良好而快速地收敛到最优策略，因为代理学习通过优化动作值函数来最大化回报。</p><div class="lx ly lz ma gt ab cb"><figure class="mq mb mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/9c062ced7cb876c9b064a9d462cbd232.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*lGX_9byWoTJWHlYB2xZ3oQ.png"/></div></figure><figure class="mq mb mw ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/2fb6bd5624cae1a6b358f1507092cc06.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*swCZ6h66Cj3nxUQxw6e_qg.png"/></div></figure><figure class="mq mb mx ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/4edb04eb8f77ee6d01ada9b462b65e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*4XTx5V_4c0urOPjt0oguOA.png"/></div></figure></div><p id="20e3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对我来说，这些可视化的美妙之处在于，即使在找到最优目标策略之后，你也能看到ε-贪婪行为策略在起作用。当然，代理会继续采取行动，探索它认为是最佳的路径，偶尔会跌入悬崖，不得不重新设置。但这个过程让我想象了一个未来，这个智能体所在的世界经历了某种破坏性的重大变化，也许目标空间被移动到了环境的其他某个部分，或者悬崖以不同的方式分散。我们的代理可能会困惑一会儿，但肯定会再次找到最佳路径。</p><p id="4667" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">非常感谢你加入我对强化学习知识的讨伐。下一次，我们将看一看<em class="ln">价值函数逼近</em>，给我们一个更新价值函数的更复杂的方法！</p></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><h1 id="263a" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">资源</h1><p id="e28e" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la oa lc ld le ob lg lh li oc lk ll lm im bi translated"><a class="ae lo" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">强化学习:简介</strong>由<em class="ln">萨顿和</em>巴尔托</a></p><p id="5e8a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">YouTube 上大卫·西尔弗的 RL 课程</p><p id="6263" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae lo" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">强化学习 Github </a>由<em class="ln"> dennybritz </em></p><p id="b453" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae lo" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ln"> OpenAI </em>旗下<em class="ln">健身房</em> </a></p></div></div>    
</body>
</html>