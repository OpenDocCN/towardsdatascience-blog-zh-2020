<html>
<head>
<title>Custom loss function in Tensorflow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 2.0中的自定义损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-loss-function-in-tensorflow-2-0-d8fa35405e4e?source=collection_archive---------2-----------------------#2020-01-06">https://towardsdatascience.com/custom-loss-function-in-tensorflow-2-0-d8fa35405e4e?source=collection_archive---------2-----------------------#2020-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="398e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">高低位执行亏损。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0a89e2904d9efa86a6fe971eca3f20a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cozITwlOo6tzXSWrSxv_Cg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">2020年不逃2.0版本</p></figure><p id="a043" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我学习在Tensorflow (TF)中编写自己的图层时，我遇到的第一个困难是如何编写一个损失函数。TF包含了几乎所有你需要的损失函数，但有时这还不够。当实现深度强化学习或构建您自己的模型时，您可能需要编写您的损失函数。这正是这篇博文想要阐明的。我们将以两种不同的方式编写损失函数:</p><ol class=""><li id="aa5a" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">对于tf.keras模型(高级)</li><li id="a1e4" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">对于定制TF型号(低级别)</li></ol><p id="eeb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这两种情况，我们将构建一个简单的神经网络来学习数字的平方。网络将接收一个输入，并有一个输出。这个网络绝不是成功或完整的。这是非常初级的，只是为了演示不同的损失函数实现。</p><h1 id="2ffa" class="mi mj it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">1.tf.keras自定义损失(高级别)</h1><p id="1d08" class="pw-post-body-paragraph ky kz it la b lb na ju ld le nb jx lg lh nc lj lk ll nd ln lo lp ne lr ls lt im bi translated">我们来看一个高阶损失函数。我们假设已经使用tf.keras构建了一个模型。可以通过以下方式实现模型的自定义损失函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tf.keras中的高级损耗实现</p></figure><p id="4886" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，自定义损失函数<strong class="la iu">总是</strong>需要两个参数。第一个是实际值(y_actual)，第二个是通过模型预测的值(y_model)。需要注意的是，这两个都是<strong class="la iu"> TF张量</strong>和<strong class="la iu">而不是</strong> Numpy数组。在函数内部，你可以随意计算损失，除了返回值必须是一个TF标量。</p><p id="09da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的例子中，我使用tensor flow . keras . back end . square()计算了平方误差损失。然而，没有必要使用Keras后端，任何有效的张量运算都可以。</p><p id="f4c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦损失函数被计算出来，我们需要在模型编译调用中传递它。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="bbf5" class="nm mj it ni b gy nn no l np nq">model.compile(loss=custom_loss,optimizer=optimizer)</span></pre><p id="1a05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完整的代码可以在这里找到:<a class="ae nr" href="https://github.com/sol0invictus/Blog_stuff/blob/master/custom%20loss/high_level_keras.py" rel="noopener ugc nofollow" target="_blank">链接</a></p><h1 id="3fb8" class="mi mj it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">2.自定义TF损失(低水平)</h1><p id="cbff" class="pw-post-body-paragraph ky kz it la b lb na ju ld le nb jx lg lh nc lj lk ll nd ln lo lp ne lr ls lt im bi translated">在前一部分中，我们看了一个tf.keras模型。如果我们想在TF中从头开始写一个网络，在这种情况下我们会如何实现loss函数呢？这将是模型的低级实现。让我们再写一个同样的模型。下面给出了完整的实现，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF 2.0中模型的底层实现</p></figure><p id="a267" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">乌夫！代码太多了。让我们解开信息。</p><p id="8eaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> __init__() </strong>:构造器构造模型的层(不返回tf.keras.model。</p><p id="3d2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> run() </strong>:通过将输入手动传递到各层来运行给定输入的模型，并返回最后一层的输出。</p><p id="7043" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> get_loss() </strong>:计算损失并将其作为TF张量值返回</p><p id="dbef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，实现loss似乎很容易，因为我们直接处理模型，但现在我们需要通过自动微分来执行学习。这是在TF 2.0中使用TF实现的。GradientTape()。函数<strong class="la iu"> get_grad() </strong>计算层变量的梯度wrt。需要注意的是，所有的变量和自变量都是TF张量。</p><p id="7c8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> network_learn() </strong>:我们使用从get_grad()函数获得的梯度在这个函数中应用梯度下降步骤。</p><p id="cf53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">写完所有内容后，我们可以如下实现训练循环:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8383" class="nm mj it ni b gy nn no l np nq">x=[1,2,3,4,5,6,7,8,9,10]<br/>x=np.asarray(x,dtype=np.float32).reshape((10,1))<br/>y=[1,4,9,16,25,36,49,64,81,100]<br/>y=np.asarray(y,dtype=np.float32).reshape((10,1))<br/>model=model()<br/>for i in range(100):<br/>    model.network_learn(x,y)</span></pre><p id="99aa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完整代码可以在这里找到:<a class="ae nr" href="https://github.com/sol0invictus/Blog_stuff/blob/master/custom%20loss/low_level_tf.py" rel="noopener ugc nofollow" target="_blank">链接</a></p><h1 id="99a3" class="mi mj it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">结论</h1><p id="eb2d" class="pw-post-body-paragraph ky kz it la b lb na ju ld le nb jx lg lh nc lj lk ll nd ln lo lp ne lr ls lt im bi translated">在这篇文章中，我们看到了TensorFlow 2.0中自定义损失函数的高级和低级植入。知道如何实现自定义损失函数在强化学习或高级深度学习中是不可或缺的，我希望这篇小帖子能让你更容易实现自己的损失函数。关于自定义损失函数的更多细节，我们请读者参考<a class="ae nr" href="https://www.tensorflow.org/guide/keras/train_and_evaluate" rel="noopener ugc nofollow" target="_blank">张量流文档</a>。</p></div></div>    
</body>
</html>