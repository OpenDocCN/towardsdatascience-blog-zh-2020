<html>
<head>
<title>Exploring Financial Consumer Complaints with Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Spark探索金融消费者投诉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-financial-consumer-complaints-with-spark-48a253f9c830?source=collection_archive---------29-----------------------#2020-01-19">https://towardsdatascience.com/exploring-financial-consumer-complaints-with-spark-48a253f9c830?source=collection_archive---------29-----------------------#2020-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c48e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PySpark数据框架入门</h2></div><p id="a43d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在仔细阅读“美国政府公开数据的所在地”Data.gov时，我偶然发现了美国联邦金融服务消费者投诉数据库。我想到了一些初步的问题，包括:这要追溯到多久以前？这是最新的吗？人们会提出多少投诉？哪些公司产生的投诉最多？</p><p id="d3a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了回答这些问题，我决定求助于Spark。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/0900efa760dd82785c9f441886967df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YqSbV3N7KgRj48hEwYzUQw.jpeg"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">当你有资格向消费者金融保护局投诉时，你可能需要的宁静形象。<a class="ae lq" href="https://unsplash.com/@milkovi?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">米尔科维</a>在<a class="ae lq" href="https://unsplash.com/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="b099" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">如何在你的电脑上设置Spark</h1><p id="237c" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">互联网上有很多博客/帖子/地方可以找到在你的电脑上安装Spark的方法(见<a class="ae lq" href="https://mortada.net/3-easy-steps-to-set-up-pyspark.html" rel="noopener ugc nofollow" target="_blank">莫塔达·梅希亚尔的帖子</a>)。Spark曾经以在你的电脑上启动和运行是一个挑战而闻名，但我在1.6 GHz英特尔酷睿i5和8 GB内存上使用运行macOS Mojave的MacBook Air时并没有太多的挣扎。对于不同的设置或以前版本的Spark，这可能更具挑战性。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/f6dc64c29de34f1a120d9feaf3b27805.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*oE43K-30DjutvpPwx9gFOA.jpeg"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">在这个阶段，你可能也需要一点额外的平静。照片由<a class="ae lq" href="https://unsplash.com/@obofili?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> A. Shuau (Obofili) </a>在<a class="ae lq" href="https://unsplash.com/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="3de8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">开始使用Spark之前需要了解的一些事情</h1><ul class=""><li id="a410" class="mp mq it kk b kl mj ko mk kr mr kv ms kz mt ld mu mv mw mx bi translated">关于为什么你可能想使用它的一点点(无耻地塞给我的<a class="ae lq" rel="noopener" target="_blank" href="/the-what-why-and-when-of-apache-spark-6c27abc19527#b4a0">文章</a>，或者<a class="ae lq" href="https://hackernoon.com/high-level-overview-of-apache-spark-c225a0a162e9" rel="noopener ugc nofollow" target="_blank"> HackerNoon </a>的，<a class="ae lq" href="https://www.toptal.com/spark/introduction-to-apache-spark" rel="noopener ugc nofollow" target="_blank"> Toptal </a>的)</li><li id="1971" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">使用Spark时,“你可以编写代码来描述你希望如何处理数据，而不是你希望如何执行，然后Spark代表你‘做正确的事情’来尽可能高效地运行它”(3)</li><li id="4d74" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">Spark使用“懒惰评估”，在你询问答案之前什么都不做。然后，它仍然只进行获得答案所需的计算，从而最小化工作量。你应该避免强迫它在不必要的中间步骤进行评估。</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="dc9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们已经准备好使用SQL/pandas风格的工作来研究一些真实的数据。今天，我们将调查金融服务消费者投诉数据库。我从Data.gov<a class="ae lq" href="https://catalog.data.gov/dataset/consumer-complaint-database" rel="noopener ugc nofollow" target="_blank">这里</a>下载的。坦白地说，我很难使用Spark的CSV加载器正确加载CSV，所以我使用pandas将CSV转换为parquet文件。我很喜欢你有任何提示来解决我的问题，其中一些文本列被切成两半(可能在逗号？)，导致表的行数和列数大约是原来的两倍。</p><p id="ef7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意:您需要安装pyarrow或fastparquet才能运行。下面镶木地板。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="04c3" class="np ls it nl b gy nq nr l ns nt">import pandas as pd</span><span id="c3e2" class="np ls it nl b gy nu nr l ns nt">pd_df = pd.read_csv('../data/Consumer_Complaints.csv')<br/>pd_df.columns = [col.lower().replace(' ', '_') for col in <br/>                 pd_df.columns]<br/>pd_df.to_parquet('../data/consumer_complaints.parquet')</span></pre><p id="5684" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在决定进行切换后，Spark中parquet文件的加载速度比CSV文件快得多，这给我留下了深刻的印象。唯一的不便是，似乎Spark和/或Parquet文件不能很好地处理带空格的列名。在转换到拼花文件之前，我选择在pandas中删除它们。总的来说，还有另一个理由来为你的数据库/仓库/湖泊起一个合理的、没有空格的名字。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/52ca9ae0adfeb3e551d6ac9bd9fca05f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*SSd3R7impjZh9HbNaaveMQ.jpeg"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">现在你知道该怎么做了:根据你所处的环境，你可能有很多机会利用照片来恢复宁静。<a class="ae lq" href="https://unsplash.com/@lleung1?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">梁朝伟</a>在<a class="ae lq" href="https://unsplash.com/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="fbf5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">让火花在您的环境中运行</h1><p id="838b" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">如果您使用的是基本的Spark特性和方法，那么您应该从初始化SparkContext开始，通过连接到特定的执行环境来设置Spark应用程序。如果我想在本地机器的所有内核上运行，我会使用:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="4f31" class="np ls it nl b gy nq nr l ns nt">import pyspark</span><span id="8053" class="np ls it nl b gy nu nr l ns nt">sc = pyspark.SparkContext('local[*]')</span></pre><p id="1807" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以随时使用<code class="fe nv nw nx nl b">sc.stop()</code>停止SparkContext。</p><p id="ef94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的DataFrame工作，我们将让SparkSession为我们初始化SparkContext:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="ae49" class="np ls it nl b gy nq nr l ns nt">import pyspark</span><span id="8d6d" class="np ls it nl b gy nu nr l ns nt">spark = pyspark.sql.SparkSession \<br/>     .builder \<br/>     .master('local[*]') \<br/>     .appName('Python Spark Consumer Complaints example') \<br/>     .getOrCreate()</span></pre><p id="7108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们使用SparkSession构建器来“获取或创建”(如果尚不存在，则创建)一个SparkSession，它具有指定的执行环境(我的本地计算机上的所有内核),并命名该应用程序以便在<a class="ae lq" href="https://spark.apache.org/docs/latest/monitoring.html" rel="noopener ugc nofollow" target="_blank">在线用户界面</a>上进行识别。</p><p id="d2a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为检查，我运行了<code class="fe nv nw nx nl b">spark.sparkContext.defaultParallelism</code>来确保Spark使用了预期数量的内核。</p><h1 id="c0ad" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">将数据导入数据框架</h1><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="df6b" class="np ls it nl b gy nq nr l ns nt">df = spark.read.load('../data/consumer_complaints.parquet',<br/>     inferSchema='true', header='true')</span></pre><p id="a972" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅此而已。我做了一个快速的<code class="fe nv nw nx nl b">df.count()</code>来确认这次Spark读取了正确的行数。</p><h1 id="b5be" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">使用PySpark数据框架</h1><p id="f677" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">现在我们有了数据框，让我们继续使用它来回答一些问题:</p><h2 id="5907" class="np ls it bd lt ny nz dn lx oa ob dp mb kr oc od md kv oe of mf kz og oh mh oi bi translated">我们在看什么时间框架？</h2><p id="235c" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">这里我们需要转换成日期时间类型。我“选择”(像SQL一样)接收的日期转换成日期格式的日期类型，并给它一个别名。然后我按日期排序，取1。Take产生的结果与limit相同，但我不确定Spark各版本的性能差异。Take(n)是一个基本的spark经典，返回前n个结果，而limit()对于面向SQL的人来说更直观。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="be8e" class="np ls it nl b gy nq nr l ns nt">from pyspark.sql.functions import to_date</span><span id="a9f3" class="np ls it nl b gy nu nr l ns nt">first_date = df.select(to_date(df.date_received,<br/>                              'MM/dd/yyyy').alias('date')) \<br/>     .orderBy('date').take(1)<br/>first_date</span><span id="30b5" class="np ls it nl b gy nu nr l ns nt"><br/>Out: [Row(date=datetime.date(2011, 12, 1))]</span><span id="07a5" class="np ls it nl b gy nu nr l ns nt">last_date = df.select(to_date(df.date_received, <br/>                             'MM/dd/yyyy').alias('date')) \<br/>     .orderBy('date', ascending=False).take(1)<br/>last_date</span><span id="f25c" class="np ls it nl b gy nu nr l ns nt"><br/>Out: [Row(date=datetime.date(2020, 1, 12))]</span></pre><p id="d6c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过几行代码，我们看到数据集的时间跨度从2011年12月1日到2020年1月12日。</p><h2 id="ea6c" class="np ls it bd lt ny nz dn lx oa ob dp mb kr oc od md kv oe of mf kz og oh mh oi bi translated">我们总共有多少投诉？每天？</h2><p id="baa7" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">对于投诉总数，快速<code class="fe nv nw nx nl b">df.count()</code>返回约150万。</p><p id="556f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了获得每天的投诉，我们将使用上面的datetime对象:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="9047" class="np ls it nl b gy nq nr l ns nt">days = last_date[0]['date']-first_date[0]['date']<br/>days</span><span id="1a11" class="np ls it nl b gy nu nr l ns nt"><br/>Out: datetime.timedelta(days=2964)</span></pre><p id="ccf6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每天平均投诉数量:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="a7d0" class="np ls it nl b gy nq nr l ns nt">df.count()/days.days</span><span id="9989" class="np ls it nl b gy nu nr l ns nt"><br/>Out: 497.49257759784075</span></pre><p id="cf62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我通过将PySpark数据帧转换成熊猫数据帧并使用<code class="fe nv nw nx nl b">matplotlib</code>来可视化每天的抱怨:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="df3b" class="np ls it nl b gy nq nr l ns nt">dates = df.select(to_date(df.date_received, <br/>                  'MM/dd/yyyy').alias('date'))<br/>complaints_per_day_pd =dates.groupBy('date').count() \<br/>      .sort('date').toPandas()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ok ol di om bf on"><div class="gh gi oj"><img src="../Images/0653ff5b2488f17dd59d9cd403a00000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NZQb8bs8VTbV5e_k"/></div></div></figure><p id="cb3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我注意到了这些峰值，并决定进行调查。你可以像在<code class="fe nv nw nx nl b">pandas</code>中使用<code class="fe nv nw nx nl b">.loc</code>一样使用filter(有时与where和when结合使用)。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="9d52" class="np ls it nl b gy nq nr l ns nt">daily_complaint_counts = dates.groupBy('date').count()<br/>daily_complaint_counts.filter(<br/>     daily_complaint_counts['count'] &gt; 1500) \<br/>     .orderBy('date').show()</span></pre><p id="915c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">返回的格式为SQL样式:</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="e1e3" class="np ls it nl b gy nq nr l ns nt">+— — — — — + — — -+<br/>| date|count|<br/>+ — — — — — + — — -+<br/>|2017–01–19| 2070|<br/>|2017–01–20| 1633|<br/>|2017–09–08| 3553|<br/>|2017–09–09| 2709|<br/>|2017–09–13| 1600|<br/>+ — — — — — + — — -+</span></pre><p id="e693" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2017年9月是Equifax安全漏洞。我不确定2017年1月发生了什么，除了特朗普总统20日的就职典礼。</p><h2 id="4fa8" class="np ls it bd lt ny nz dn lx oa ob dp mb kr oc od md kv oe of mf kz og oh mh oi bi translated">哪些公司被投诉最多？</h2><p id="0dc1" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">这里我想再次使用<code class="fe nv nw nx nl b">matplotlib</code>可视化结果，所以我将结果转换成熊猫。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="d3d3" class="np ls it nl b gy nq nr l ns nt">company_counts = df.groupBy('company').count()<br/>company_counts_for_graph = company_counts.filter(<br/>     company_counts['count'] &gt;1000) \<br/>     .sort('count', ascending=False).toPandas()</span></pre><p id="88c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我使用<code class="fe nv nw nx nl b">matplotlib</code>来可视化结果:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ok ol di om bf on"><div class="gh gi oj"><img src="../Images/9bdb43b8c06f6f9f1305a2455f804086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UdUFJU5ohYZi6GP2"/></div></div></figure><p id="2262" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于信用报告机构来说并不可爱…但是我们看到Spark的这个模块用相当SQL-y的代码很快就得到答案。如果您知道如何在SQL或pandas中实现，PySpark的方式可能是相似的，或者至少在PySpark中可能有相似的方式。像往常一样，有许多方法可以得到相同的结果。</p><h2 id="0db5" class="np ls it bd lt ny nz dn lx oa ob dp mb kr oc od md kv oe of mf kz og oh mh oi bi translated">一些最后的问题</h2><p id="6cf3" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">投诉是怎么接到的？主要是通过网络。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="9b9b" class="np ls it nl b gy nq nr l ns nt">df.groupBy('submitted_via').count().sort('count', <br/>     ascending=False).show()</span><span id="5cab" class="np ls it nl b gy nu nr l ns nt">Out:<br/>+ — — — — — — -+ — — — -+<br/>| submitted_via| count|<br/>+ — — — — — — -+ — — — -+<br/>|          Web|1106079|<br/>|     Referral| 187007|<br/>|        Phone|  88292|<br/>|  Postal mail|  72485|<br/>|          Fax|  20297|<br/>|        Email|    408|<br/>+ — — — — — — -+ — — — -+</span></pre><p id="cf14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回应是否及时？通常情况下。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="9dff" class="np ls it nl b gy nq nr l ns nt">df.groupBy('timely_response?').count().sort('count', <br/>     ascending=False).show()</span><span id="e59f" class="np ls it nl b gy nu nr l ns nt">Out:<br/>+ — — — — — — — — + — — — -+<br/>| timely_response?|  count|<br/>+ — — — — — — — — + — — — -+<br/>|              Yes|1439485|<br/>|               No|  35083|<br/>+ — — — — — — — — + — — — -+</span></pre><p id="af00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们有争议吗？有时候。这取决于null在这一列中的含义。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="f76c" class="np ls it nl b gy nq nr l ns nt">df.groupBy('timely_response?').count().sort('count', <br/>     ascending=False).show()</span><span id="667e" class="np ls it nl b gy nu nr l ns nt">Out:<br/>+ — — — — — — — — — + — — — +<br/>|consumer_disputed?| count|<br/>+ — — — — — — — — — + — — — +<br/>|              null|706088|<br/>|                No|620102|<br/>|               Yes|148378|<br/>+ — — — — — — — — — + — — — +</span></pre></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="bae9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的最后一个提示:你可以用<code class="fe nv nw nx nl b"><em class="oo">SparkSession</em>.stop()</code>关闭你的SparkSession。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="f543" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这有助于您开始使用Spark查看数据！你可以在这里找到回购<a class="ae lq" href="https://github.com/allisonhonold/spark-blog" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="604f" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">了解Spark工作原理的其他资源</h1><ol class=""><li id="f596" class="mp mq it kk b kl mj ko mk kr mr kv ms kz mt ld op mv mw mx bi translated"><a class="ae lq" href="https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/" rel="noopener ugc nofollow" target="_blank">https://mapr . com/blog/datasets-data frames-and-spark-SQL-for-processing-of-tabular-data/</a></li><li id="d117" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld op mv mw mx bi translated">https://mapr.com/blog/how-spark-runs-your-applications/<a class="ae lq" href="https://mapr.com/blog/how-spark-runs-your-applications/" rel="noopener ugc nofollow" target="_blank"/></li><li id="1825" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld op mv mw mx bi translated"><a class="ae lq" href="https://mapr.com/blog/5-minute-guide-understanding-significance-apache-spark/" rel="noopener ugc nofollow" target="_blank"> https://mapr.com/blog/5分钟-指南-理解-意义-apache-spark/ </a></li><li id="6422" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld op mv mw mx bi translated"><a class="ae lq" href="https://realpython.com/pyspark-intro/" rel="noopener ugc nofollow" target="_blank">https://realpython.com/pyspark-intro/</a></li></ol></div></div>    
</body>
</html>