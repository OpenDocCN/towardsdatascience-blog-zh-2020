<html>
<head>
<title>Optimizing Pose Estimation On The Coral Edge TPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化珊瑚边缘 TPU 的姿态估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-pose-estimation-on-the-coral-edge-tpu-d331c63cfed?source=collection_archive---------35-----------------------#2020-07-24">https://towardsdatascience.com/optimizing-pose-estimation-on-the-coral-edge-tpu-d331c63cfed?source=collection_archive---------35-----------------------#2020-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="996a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">辅导的</h2><div class=""/><div class=""><h2 id="69e3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何访问 PoseNet 模型中的低级特征并在边缘 TPU 设备上释放姿态估计的全部潜力</h2></div><p id="90e0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2018 年，谷歌宣布发布 PoseNet 的 TensorFlow，这是一种机器学习模型，能够检测图像中的人，并估计他们身体部位的位置，这种技术被称为<strong class="kt jd">姿势估计</strong>。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/51100eaef8dabe18de5d6bc9ca55bd5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IOm2GxkbUNaaPGUZP0NHVw.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">由 Coral Edge TPU 加速器上运行的 PoseNet 算法估计的姿态。右侧是在边缘 TPU 模型中通常不可访问的关键点热图的叠加图。原<a class="ae md" href="https://en.wikipedia.org/wiki/Hindu_wedding#/media/File:Hindu_marriage_ceremony_offering.jpg" rel="noopener ugc nofollow" target="_blank">照片</a> by <a class="ae md" href="https://en.wikipedia.org/wiki/User:Jaisingh_rathore" rel="noopener ugc nofollow" target="_blank">贾辛格拉索尔</a> / <a class="ae md" href="https://creativecommons.org/licenses/by/2.5" rel="noopener ugc nofollow" target="_blank"> CC BY 2.5 </a>。</p></figure><p id="a3a7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">PoseNet 检测框架可用于 JavaScript(<a class="ae md" href="https://github.com/tensorflow/tfjs-models/tree/master/posenet" rel="noopener ugc nofollow" target="_blank">tensor flow . js</a>)、Android/iOS 移动设备(<a class="ae md" href="https://www.tensorflow.org/lite/models/pose_estimation/overview" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a>)和 Edge TPU 加速器(<a class="ae md" href="https://github.com/google-coral/project-posenet" rel="noopener ugc nofollow" target="_blank"> Google Coral </a>)。</p><p id="2a70" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最近，我对该框架的<strong class="kt jd"> Edge TPU 版本</strong>最感兴趣，因为我一直在从事几个涉及“野外”人员检测和跟踪的项目，TPU 的 USB 版本使得在<strong class="kt jd">小型嵌入式设备</strong>上执行<strong class="kt jd">实时姿态估计</strong>成为可能，如<a class="ae md" href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/" rel="noopener ugc nofollow" target="_blank">树莓 Pi </a>或<a class="ae md" href="https://www.st.com/en/evaluation-tools/stm32mp157c-dk2.html" rel="noopener ugc nofollow" target="_blank"> STM32P1 </a>平台。</p><p id="a29d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">针对边缘 TPU 的 PoseNet 的限制</strong></p><p id="3cb8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Coral 工程师在打包代码和模型方面做得很好，使得开发人员可以轻松地使用 PoseNet 框架。然而，为了简化使用，一些模型参数已经被硬编码。例如，可以在图像中检测到的<strong class="kt jd">个人物/姿势被限制为 10 个</strong>。解码算法的其他参数也是硬编码的，虽然我发现默认值在许多情况下都很好，但我认为有机会通过微调一些隐藏的参数来改善姿态估计。如果你对细节感兴趣，请继续读下去。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi me"><img src="../Images/3b96aa40726381edc3a253df0e860380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p2HNWeh25TETNin373ydNA.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">谷歌珊瑚缘 TPU 产品，来源<a class="ae md" href="https://coral.ai/" rel="noopener ugc nofollow" target="_blank"> https://coral.ai </a></p></figure><h1 id="363d" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated"><strong class="ak">背景:波森特建筑</strong></h1><p id="41c0" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">PoseNet 实现基于两级架构，包括<strong class="kt jd"/><strong class="kt jd">卷积神经网络(CNN) </strong>和<strong class="kt jd">解码算法</strong>。</p><p id="4616" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">卷积网络</strong>被训练生成<strong class="kt jd">热图</strong>，预测图像中所有关键点(即身体部位)的位置。它还生成<strong class="kt jd">短程和中程偏移向量</strong>，有助于在同一幅图像中出现多人时“连接这些点”。</p><p id="8568" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">解码算法</strong>获取 CNN 生成的热图和偏移向量，并创建身体部位和人物实例之间的关联，试图确保来自同一个人的所有关键点都关联到同一实例。你可以从开发这项技术的谷歌研究团队的两份出版物[ <a class="ae md" href="https://arxiv.org/abs/1701.01779" rel="noopener ugc nofollow" target="_blank"> 1 </a>、<a class="ae md" href="https://arxiv.org/abs/1803.08225" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]中读到所有细节。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nc"><img src="../Images/f1c32a46e1f199ea322c354c299dba25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q9FAfaY_MSmxKArKrTN41w.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">PoseNet-source<a class="ae md" href="https://arxiv.org/pdf/1803.08225.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>中热图、偏移和位移矢量之间的关系</p></figure><p id="dd62" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">解码算法的参数— </strong>在拥挤的场景中，确保身体部位和人物实例之间的正确关联是一项相当具有挑战性的任务，在拥挤的场景中，许多人可能看起来彼此紧密接触。为了在各种条件下确保最大的准确性，PoseNet 的作者提供了一些参数来控制解码算法的工作方式。</p><p id="003c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在计算<strong class="kt jd"> </strong>最终输出<strong class="kt jd">、</strong>时，影响<strong class="kt jd">速度</strong>和<strong class="kt jd">精度</strong>的一些参数如下:</p><ul class=""><li id="6f3c" class="nd ne it kt b ku kv kx ky la nf le ng li nh lm ni nj nk nl bi translated"><strong class="kt jd">最大姿态检测</strong> —要检测的最大姿态数。</li><li id="fa4c" class="nd ne it kt b ku nm kx nn la no le np li nq lm ni nj nk nl bi translated"><strong class="kt jd">姿势置信度阈值</strong> — 0.0 到 1.0。在高层次上，这控制了返回的姿势的最小置信度得分。</li><li id="e405" class="nd ne it kt b ku nm kx nn la no le np li nq lm ni nj nk nl bi translated"><strong class="kt jd">非最大抑制(NMS)半径</strong> —以像素为单位的数字。在高层次上，这控制返回的姿势之间的最小距离。</li></ul><p id="9d50" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请参见 TensorFlow 组的<a class="ae md" href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" rel="noopener">中的帖子</a>以获得参数的通俗易懂的描述。通过试验实时<a class="ae md" href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html" rel="noopener ugc nofollow" target="_blank">多姿态在线演示</a>中的参数值，可以获得额外的见解。</p><h1 id="e59a" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">边缘 TPU 上的 PoseNet 实现</h1><p id="97cf" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">在 JavaScript 和移动实现(Android 和 iOS)中，解码算法都包含在源代码库中，参数可以在运行时更改。然而，对于 EdgeTPU 版本，代码维护人员选择了不同的方法，并决定将<strong class="kt jd">解码算法作为自定义操作符</strong>直接嵌入 TensorFlow Lite 模型中。</p><blockquote class="nr ns nt"><p id="d0de" class="kr ks nu kt b ku kv kd kw kx ky kg kz nv lb lc ld nw lf lg lh nx lj lk ll lm im bi translated">[……]请注意，与<a class="ae md" href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" rel="noopener"> TensorflowJS 版本</a>不同，我们在 Tensorflow Lite 中创建了一个自定义 OP，并将其附加到网络图本身。[……]优势在于我们不必直接处理热图，当我们通过 Coral Python API 调用该网络时，我们只需从网络中获取一系列关键点。(来源:<a class="ae md" href="https://github.com/google-coral/project-posenet" rel="noopener ugc nofollow" target="_blank">https://github.com/google-coral/project-posenet</a>)</p></blockquote><p id="567c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然一方面，这种方法使得在边缘 TPU 上启动和运行 PoseNet 更容易，但另一方面，它阻止了我们调整解码参数并在精度和速度之间实现最佳平衡的可能性。例如，在 EdgeTPU 实现中，<strong class="kt jd">姿势的最大数量固定为 10 个</strong>，因此如果我们试图处理超过 10 个人的图像，我们将无法获得预期的结果。此外，如果我们要处理单姿态场景，我们可能会浪费一些 CPU 周期来解码不在图像中的姿态。</p><h1 id="4896" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">访问边缘 TPU 模型中的热图和偏移向量</h1><p id="12b1" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">在接下来的部分中，我们将探讨一些可以用来(重新)访问卷积网络输出的步骤，看看如何“手动”解码，同时保留调整算法参数的可能性。我们将首先了解如何检查边缘 TPU 实现中可用的 PoseNet 模型，以及如何修改它们以访问卷积层产生的低级功能。</p><h2 id="648f" class="ny mg it bd mh nz oa dn ml ob oc dp mp la od oe mr le of og mt li oh oi mv iz bi translated">步骤 1:检查原始 TFLite 模型(可选)</h2><p id="ca43" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">PoseNet 的珊瑚 TPU 代码库包含三个基于 MobileNet 架构的模型，并针对三种不同的图像分辨率进行了优化:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="604d" class="ny mg it ok b gy oo op l oq or">1) posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite<br/>2) posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite<br/>3) posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite</span></pre><p id="acf0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">模型以 TFLite 格式保存，可以使用<a class="ae md" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow 代码库</a>中包含的<code class="fe os ot ou ok b">visualize.py</code>工具检查其内容。</p><p id="ac8d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将把最小的模型(<strong class="kt jd"> 353 x 481 </strong>)转换成 HTML 文件进行检查。注意，由于模型的名称很长，创建一个到它的符号链接更容易。</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="4c93" class="ny mg it ok b gy oo op l oq or"># Create symlink to original model (353x481)<br/>MODEL=/path/to/posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite<br/>ln -s ${MODEL} /tmp/input_model.tflite</span><span id="7c70" class="ny mg it ok b gy ov op l oq or"># Convert file from TFLITE format to HTML<br/>cd ~/tensorflow/tensorflow/lite/tools<br/>python visualize.py /tmp/input_model.tflite /tmp/input_model.html</span></pre><p id="f82a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">HTML 文件包含关于模型中存在的输入/输出张量和运算符的所有信息:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ow"><img src="../Images/a03e9c43369720b0ad1234240a04454f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PzVNbTWrKtbzWfGo4avS6w.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">用 theTensorflow 实用程序<strong class="bd ox"> visualize.py </strong>生成的 TFLITE 模型 posenet _ mobilenet _ v1 _ 075 _ 353 _ 481 _ quant _ decoder _ edge TPU . TFLITE 的 Html 表示</p></figure><p id="9766" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从 HTML 文件中我们可以看到有两个操作符<strong class="kt jd"> Ops </strong> (0，1)。第一个，<strong class="kt jd"> Ops #0，</strong>将在具有维度[1，353，481，3]的张量 3 内以 RGB 格式存储的原始图像作为输入。第二个操作符<strong class="kt jd"> Ops #2 </strong>，用姿态估计过程的结果产生输出张量:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="6ee6" class="ny mg it ok b gy oo op l oq or">- Tensor 4, FLOAT32 [1, <strong class="ok jd">10</strong>, <strong class="ok jd">17</strong>, 2]...: <strong class="ok jd">Keypoint coordinates</strong> (y, x)<br/>- Tensor 5, FLOAT32 [1, <strong class="ok jd">10</strong>, <strong class="ok jd">17</strong>]......: <strong class="ok jd">Keypoint scores<br/></strong>- Tensor 6, FLOAT32 [1, <strong class="ok jd">10</strong>]..........: <strong class="ok jd">Poses scores<br/></strong>- Tensor 7, FLOAT32 []...............: <strong class="ok jd">Number of poses</strong></span></pre><p id="efbc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">张量 4、5、6 中的第二维度等于 10 是因为，如前一节所述，姿势的最大数量参数被硬编码为 10。张量 4 和 5 中的第三维值与 PoseNet 当前检测到的<strong class="kt jd"> 17 个关键点</strong>相匹配:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oy"><img src="../Images/6183b6bf758cf633b90d32b74245db71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iU6GJmGYvoPn-mzc.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">PoseNet - Image source 检测到 17 个姿态关键点<a class="ae md" href="https://github.com/google-coral/project-posenet" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a></p></figure><p id="046e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们想要解码 10 个以上的姿势或改变一些其他输入参数，我们将需要使用存储在以下张量中的第一个操作符<strong class="kt jd"> Ops#0 </strong>的输出:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="2c47" class="ny mg it ok b gy oo op l oq or">- Tensor 0, UINT8 [1, 23, 31, 17]....: <strong class="ok jd">Keypoint heatmap<br/></strong>- Tensor 1, UINT8 [1, 23, 31, 34]....: <strong class="ok jd">Keypoint offsets<br/></strong>- Tensor 2, UINT8 [1, 23, 31, 64]....: <strong class="ok jd">Forward and backward displacement vectors (mid-range offsets)</strong></span></pre><p id="5f85" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae md" href="https://arxiv.org/pdf/1803.08225.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>中提供了热图、关键点偏移和位移矢量的详细描述。</p><p id="3009" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意，热图的大小为<code class="fe os ot ou ok b">[23, 31]</code>，因为该特定模型使用了<code class="fe os ot ou ok b">OUTPUT_STRIDE = 16</code>，图像大小和热图大小之间的关系由以下等式给出:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="911d" class="ny mg it ok b gy oo op l oq or">heatmap_height = 1 + (img_heigth - 1) / OUTPUT_STRIDE<br/>heatmap_width = 1 + (img_width - 1) / OUTPUT_STRIDE</span></pre><h2 id="233f" class="ny mg it bd mh nz oa dn ml ob oc dp mp la od oe mr le of og mt li oh oi mv iz bi translated">步骤 2:获取热图、偏移量和位移向量</h2><p id="4877" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">TFLite 模型使用<a class="ae md" href="https://github.com/google/flatbuffers" rel="noopener ugc nofollow" target="_blank"> Google Flatbuffer </a>协议文件格式序列化到磁盘。在大多数情况下，处理这种格式所需的工具链必须从源代码安装(参见本<a class="ae md" href="https://stackoverflow.com/questions/55394537/how-to-install-flatc-and-flatbuffers-on-linux-ubuntu" rel="noopener ugc nofollow" target="_blank"> StackOverflow 帖子</a>中的说明作为示例)。安装完成后，您将可以访问<code class="fe os ot ou ok b">flatc</code>模式编译器，该编译器支持从 TFLite 格式到 JSON 的转换，反之亦然。转换所需的模式文件可在 tensor flow repo(<a class="ae md" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs" rel="noopener ugc nofollow" target="_blank">schema . FBS</a>)中获得，应在具有以下语法的调用中使用:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="54a2" class="ny mg it ok b gy oo op l oq or"># Converts from TFLITE to JSON<br/>SCHEMA=~/tensorflow/tensorflow/lite/schema/schema.fbs</span><span id="19b7" class="ny mg it ok b gy ov op l oq or">flatc -t --strict-json --defaults-json -o /tmp ${SCHEMA} -- /tmp/input_model.tflite</span></pre><p id="8bbf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下图显示了 JSON 输出中包含的数据的概要，它与我们在 HTML 表示中已经看到的内容相匹配:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/87ad7022331f7823ccef983b8c7609e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*BpSt8ctHAJBY6Fy2hzpc-w.png"/></div></figure><p id="8156" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">JSON 文件相当大(~25MB)，不容易“手动”编辑。但是我们可以用一些简单的 Python 代码来操作它，如下所示。其思想是移除自定义算子解码算法(Ops #1)、任何未使用的张量，并确保模型输出由张量[0，1，2](热图、偏移和位移向量)给出。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pa pb l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">用于修改 TFLITE 模型的 Python 脚本</p></figure><p id="b2da" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，我们使用与第一步转换相同的<code class="fe os ot ou ok b">flatc</code>实用程序将输出从 JSON 格式转换回 TFLite 格式:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="2b77" class="ny mg it ok b gy oo op l oq or"># Converts from TFLITE to JSON<br/>SCHEMA=~/tensorflow/tensorflow/lite/schema/schema.fbs</span><span id="09b4" class="ny mg it ok b gy ov op l oq or">flatc -c -b -o /tmp ${SCHEMA} /tmp/output_model.json</span></pre><p id="8061" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">转换的结果现在应该可以在<code class="fe os ot ou ok b">output_model.tflite</code>(你可能想给它一个更具体的名字)flatbuffer 文件中找到。我们可以验证当我们从 TensorFlow 加载这个模型时，它会产生三个具有预期大小的输出张量:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="2a8d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">输出张量的大小与预期尺寸相匹配:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="7eba" class="ny mg it ok b gy oo op l oq or">Output #0, UINT8 [1, 23, 31, 17]....: <strong class="ok jd">12121</strong> elements (heatmap)<strong class="ok jd"><br/></strong>Output #1, UINT8 [1, 23, 31, 34]....: <strong class="ok jd">24242</strong> elements (offsets)<strong class="ok jd"><br/></strong>Output #2, UINT8 [1, 23, 31, 64]....: <strong class="ok jd">45632</strong> elements (disp. vectors)</span></pre><p id="b5da" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">到目前为止，一切顺利……:-)</p><h1 id="793b" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">解码热图和偏移向量</h1><p id="b637" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">我们现在准备好了过程的最后一步，包括从模型中获取新的输出(热图、偏移和位移向量)并提取姿态。</p><p id="2320" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如介绍中提到的，PoseNet 的 JavaScript 和 Android/iOS 版本都包含解码算法的实现。谷歌没有发布相同算法的官方 Python 实现，但幸运的是，我们可以依赖 GitHub 上辉煌项目<a class="ae md" href="https://github.com/rwightman/posenet-python" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> PoseNet-Python </strong> </a>中可用的非官方端口。</p><p id="7fea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从 PoseNet 的 JavaScript 版本移植而来的代码位于文件<code class="fe os ot ou ok b"><a class="ae md" href="https://github.com/rwightman/posenet-python/blob/master/posenet/decode_multi.py" rel="noopener ugc nofollow" target="_blank">decode_multi.py</a></code>中，该文件包含一个带有以下签名的函数:</p><pre class="lo lp lq lr gt oj ok ol om aw on bi"><span id="fedd" class="ny mg it ok b gy oo op l oq or">def decode_multiple_poses(<br/>        scores, offsets, displacements_fwd, displacements_bwd,<br/>        output_stride, max_pose_detections=10, score_threshold=0.5,<br/>        nms_radius=20, min_pose_score=0.5):</span></pre><p id="cd7f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上面的函数中，第一个参数<code class="fe os ot ou ok b">scores</code>代表关键点热图，而偏移和位移向量(向前和向后)与我们之前讨论的张量相匹配。</p><p id="b8fe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让这个函数起作用的唯一棘手的部分是正确地预处理热图，并重塑位移向量，以匹配<code class="fe os ot ou ok b">decode_multiple_pose</code>函数所期望的格式。下面的代码片段包含函数<code class="fe os ot ou ok b">extract_outputs</code>，它负责将修改后的 TFLite 模型的输出转换成可以直接传递给解码函数的格式。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pa pb l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">演示如何解码修改后的 TFLite 模型的输出的最小工作示例。注意，我们现在可以访问解码算法的所有参数。</p></figure><h1 id="63b7" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">结论</h1><p id="1929" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">这篇文章展示了如何检查一个边缘 TPU TFLite 模型，并改变它，以获得在一个 PoseNet 模型卷积层的输出。它还显示了如何利用解码算法的参数来获取该输出并提取姿态。我打算写一篇后续文章，探索在基于姿态估计的真实世界计算机视觉应用中改变这些参数的影响。如果您有想法或建议，请留下评论或通过<a class="ae md" href="https://stura.io" rel="noopener ugc nofollow" target="_blank"> Stura.io </a>联系我。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/27e0a1a29ed44d5b93adc928cc893fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*gWCqFjplC0jq4gRp6oE1vQ.gif"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">具有重叠关键点热图的姿态检测输出(右半部分)</p></figure><h1 id="3693" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">参考</h1><p id="d5e8" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">[1] G .帕潘德里欧<em class="nu">等人</em>。，<a class="ae md" href="https://arxiv.org/abs/1701.01779" rel="noopener ugc nofollow" target="_blank">野外多人姿态精确估计</a> (2017)，CVPR 论文集。</p><p id="893a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[2] G .帕潘德里欧<em class="nu">等人</em>。，<a class="ae md" href="https://arxiv.org/abs/1803.08225" rel="noopener ugc nofollow" target="_blank"> PersonLab:利用自下而上、基于部分的几何嵌入模型进行人物姿态估计和实例分割</a> (2018)。</p></div></div>    
</body>
</html>