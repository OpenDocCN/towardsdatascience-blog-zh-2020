<html>
<head>
<title>Predictive Maintenance of Turbofan Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">涡扇发动机的预测维修</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predictive-maintenance-of-turbofan-engine-64911e39c367?source=collection_archive---------42-----------------------#2020-06-08">https://towardsdatascience.com/predictive-maintenance-of-turbofan-engine-64911e39c367?source=collection_archive---------42-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b5e4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用时间序列数据并询问 RNN“下一次故障何时发生？”</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9ea00d5961766b2b4faf6c3ed563bf58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikw5vI2nQhMw2ozaoa_aSQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比尔·杰伦在<a class="ae ky" href="https://unsplash.com/photos/lt6gE86VyaA" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="809c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预测性维护对于制造商和维护人员都非常重要，它通过在问题导致设备故障之前解决问题来降低维护成本、延长设备寿命、减少停机时间和提高生产质量。</p><blockquote class="lv"><p id="918a" class="lw lx it bd ly lz ma mb mc md me lu dk translated">“预测性维护技术旨在帮助确定在用设备的状况，以便估计何时应该进行维护”——来源维基百科</p></blockquote><blockquote class="mf mg mh"><p id="9b4c" class="kz la mi lb b lc mj ju le lf mk jx lh ml mm lk ll mn mo lo lp mp mq ls lt lu im bi translated">在这篇文章中，我想展示一下使用<strong class="lb iu"> RNN(递归神经网络)/LSTM(长短期记忆)</strong>架构不仅<strong class="lb iu">更准确，而且与之前由 Marco Cerliani 编写的<strong class="lb iu"> CNN(卷积神经网络)</strong>方法相比，它在准确分类结果方面表现得更好。</strong></p></blockquote><h1 id="7e75" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">资料组</h1><p id="b614" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">本帖使用<a class="ae ky" href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> C-MAPSS 数据集</strong> </a> <strong class="lb iu"> </strong>进行涡扇发动机的预测性维护。这里的挑战是确定<strong class="lb iu">剩余使用寿命(RUL) </strong>直到下一次发动机发生故障。</p><p id="e88b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集可以在<a class="ae ky" href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这里</strong> </a>找到，下面简单介绍一下数据集，</p><blockquote class="mf mg mh"><p id="4c0b" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">“在每个时间序列开始时，发动机正常运行<strong class="lb iu">，但在序列中的某个时间点出现故障。</strong></p><p id="5d0c" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">在<strong class="lb iu">训练集中，故障在数量上增长，直到系统故障</strong>。</p><p id="0062" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">在测试组<strong class="lb iu">中，时间序列在系统故障</strong>之前的某个时间结束。</p></blockquote><p id="5d32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是模型训练中使用的发动机状况</p><blockquote class="mf mg mh"><p id="baac" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">列车轨迹:100 个<br/>测试轨迹:100 个<br/>条件:一个(海平面)<br/>故障模式:一个(HPC 退化)</p></blockquote><p id="85fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">了解数据集</strong></p><p id="6c62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加载数据集后，我们将获得 100 台发动机的时间序列数据，其中包含每 100 台发动机的运行设置和传感器读数，以及故障发生的不同场景和总共 20631 个训练示例。举例来说，下面是我们的训练数据集的前 5 个训练示例。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="6d89" class="nt ms it np b gy nu nv l nw nx">train_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/f22733b862e9d72bb08342dff1d13ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0XVgFa9LQs4yh3kdi2LQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:训练数据</p></figure><p id="e99a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了进一步理解给定的数据，(见图 2)描述了对于给定的发动机，在下一个故障发生之前还有多少循环。</p><p id="1d01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">例 1:69 号发动机(最左边)在故障前大约还有 360 个循环。</em></p><p id="2393" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">例 2:发动机识别号 39(最右边)在故障前大约还有 110 个循环。</em></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="04dc" class="nt ms it np b gy nu nv l nw nx">train_df.id.value_counts().plot.bar()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/9efcefc93d23a51a5d6d4c8eab913d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qka1jxJyNGk_lhC-Mnvdcw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:发动机及其各自的剩余有用循环直到故障</p></figure><p id="0a6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下(图 3 和图 4)是 id 为 69 的发动机的时间序列数据。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="3be1" class="nt ms it np b gy nu nv l nw nx">engine_id = train_df[train_df['id'] == 69]</span><span id="7604" class="nt ms it np b gy oa nv l nw nx">ax1 = engine_id[train_df.columns[2:]].plot(subplots=True, sharex=True, figsize=(20,30))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/39b5e1a35876f56c0369fdf891bfa490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wYQwH0CMYmVAr5zLgvFNUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:运行设置 1、2 和 3 的时间序列读数以及 s2 到 s7 的传感器读数</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/1a07e76b00a03cda7cfcfb8091a11e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHb14ho5GHuTs2elGAwzjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:传感器 s8 直到 s20 的时间序列读数</p></figure><p id="2f03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">*图片(图 2、图 3、图 4)使用 Marco Cerliani 的 GitHub Notebook ( </em> <a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/Remaining_Life_Estimation/Remaining_Life_Estimation.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="mi">此处</em> </strong> </a> <em class="mi">)的源代码获得。</em></p><h1 id="7602" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">数据预处理</h1><p id="cc65" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">数据预处理是训练任何神经网络的最重要的步骤。对于像 RNN(递归神经网络)这样的神经网络，网络对输入数据非常敏感，数据需要在-1 比 1 或 0 比 1 的范围内。这个范围即-1 比 1 或 0 比 1，通常是因为<strong class="lb iu"> <em class="mi"> tanh ( </em> </strong> <em class="mi">见图 5) </em>是伴随在网络隐层中的激活函数。因此，在训练模型之前，必须对数据进行标准化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c267df6203e8156ba3d7587ddd4c0125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*viG5sbutu-6ZnjLI-s9YXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:<strong class="bd oe">MathWorks 文档中的 tanh </strong>函数</p></figure><p id="f0cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 sklearn 的预处理库提供的<strong class="lb iu">最小最大缩放器</strong>函数<strong class="lb iu"> </strong>，我们在<strong class="lb iu"> 0 到 1、</strong>的范围内归一化我们的训练数据，尽管理论上我们可以将我们的数据归一化并实验到-1 到 1。然而，这篇文章仅仅展示了数据在<strong class="lb iu"> 0 到 1 范围内的缩放。</strong></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="b08d" class="nt ms it np b gy nu nv l nw nx">from sklearn.preprocessing import MinMaxScaler</span><span id="4766" class="nt ms it np b gy oa nv l nw nx">sc = MinMaxScaler(feature_range=(0,1))<br/>train_df[train_df.columns[2:26]] = sc.fit_transform(train_df[ train_df.columns[2:26]])<br/>train_df = train_df.dropna(axis=1)</span></pre><p id="6ad7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用列号 2 到 26(见图 1)的原因是，我们采用操作设置 1(列号 2)、设置 2、设置 3、传感器 1 直到传感器 21(列 25)，python 范围不考虑上限，因此上限为 26。为便于说明，这里是训练数据标准化后的前 5 个训练示例(图 6)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/fa015d59032e755db8fba77617c5b0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_ySc_eqsCb_0bOJLlx6sw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:训练集的标准化数据</p></figure><p id="8389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们将数据标准化，我们就采用一种分类方法来预测 RUL。我们通过以下方式为我们的分类方法在数据集上添加新的标签来做到这一点。</p><p id="ecf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">"使用以下源代码:from</em><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/Remaining_Life_Estimation/Remaining_Life_Estimation.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mi">Github</em></a><em class="mi">"</em></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="9bd6" class="nt ms it np b gy nu nv l nw nx">w1 = 45<br/>w0 = 15</span><span id="5926" class="nt ms it np b gy oa nv l nw nx">train_df['class1'] = np.where(train_df['RUL'] &lt;= w1, 1, 0 )<br/>train_df['class2'] = train_df['class1']<br/>train_df.loc[train_df['RUL'] &lt;= w0, 'class2'] = 2</span></pre><p id="2caf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这段代码现在为我们的分类问题创建标签(见图 7 ),分类方法如下:</p><blockquote class="mf mg mh"><p id="e37c" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">标签 0:当剩余 45+个循环后出现故障。</p><p id="0737" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">标签 1:当在 16 和 45 之间的周期直到故障时。</p><p id="9492" class="kz la mi lb b lc ld ju le lf lg jx lh ml lj lk ll mn ln lo lp mp lr ls lt lu im bi translated">标签 2:当在 0 和 15 之间的周期直到故障时。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/cc2f5d4e2f8fbd3b8206e1b704bfbd9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rjnD8NZWEdebITVGRowECg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:添加标签</p></figure><p id="7416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了！现在，我们需要进一步准备数据，以便神经网络有效地处理时间序列数据，我们通过指定时间步长(或窗口大小)来做到这一点。诸如 RNN 或 CNN 之类的神经网络要求输入数据为三维形式。因此，我们现在需要将二维数据转换成三维数据。</p><p id="3518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了演示这一转换过程(见图 8)，我们简单地通过指定时间步长(窗口大小)来运行时间序列数据。这个过程也被称为<em class="mi"> </em> <strong class="lb iu"> <em class="mi">滑动窗口技术</em> </strong> <em class="mi">。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/a94c0967398c4d65a18718ef95edf15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6opnpcspWVTOu-Pz5Yj7rA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:来自<a class="ae ky" href="https://link.springer.com/article/10.1007/s40095-014-0105-5" rel="noopener ugc nofollow" target="_blank">弹簧连杆</a>的滑动窗口技术</p></figure><p id="6ee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的时间序列数据，我们将<em class="mi">滑动窗口技术</em>用于所有传感器和操作设置，通过<em class="mi"> </em>指定时间步长(或窗口大小)为 50，尽管时间步长大小可以任意设置。下面的代码片段将我们的二维数据转换为大小为 15631x50x17 的三维数据(numpy pandas 数组),这对于神经网络的输入是最佳的。</p><p id="fe59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">“以下源代码修改:来自</em><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/Remaining_Life_Estimation/Remaining_Life_Estimation.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mi">Github</em></a><em class="mi"/></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="4419" class="nt ms it np b gy nu nv l nw nx">time_steps = 50</span><span id="cae4" class="nt ms it np b gy oa nv l nw nx">def gen_sequence(id_df):</span><span id="4ace" class="nt ms it np b gy oa nv l nw nx">data_matrix = id_df.iloc[2:26]<br/>     num_elements = data_matrix.shape[0]</span><span id="9f98" class="nt ms it np b gy oa nv l nw nx">for start, stop in zip(range(0, num_elements-time_steps),    range(time_steps, num_elements)):<br/>         yield data_matrix[start:stop, :]</span><span id="54ae" class="nt ms it np b gy oa nv l nw nx">def gen_labels(id_df, label):</span><span id="7c06" class="nt ms it np b gy oa nv l nw nx">data_matrix = id_df[label].values<br/>    num_elements = data_matrix.shape[0]</span><span id="e79b" class="nt ms it np b gy oa nv l nw nx">return data_matrix[time_steps:num_elements, :]</span><span id="1250" class="nt ms it np b gy oa nv l nw nx">x_train, y_train = [], []<br/>for engine_id in train_df.id.unique():<br/>     for sequence in gen_sequence(train_df[train_df.id==engine_id]):<br/>           x_train.append(sequence) <br/>     <br/>    for label in gen_labels(train_df[train_df.id==engine_id['label2']):<br/>           y_train.append(label)</span><span id="5955" class="nt ms it np b gy oa nv l nw nx">x_train = np.asarray(x_train)<br/>y_train = np.asarray(y_train).reshape(-1,1)</span></pre><p id="9244" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">*更多时间序列数据的进一步阅读，请阅读文章(</em> <a class="ae ky" rel="noopener" target="_blank" href="/playing-with-time-series-data-in-python-959e2485bff8"> <em class="mi">此处</em> </a> <em class="mi">)。</em></p><h1 id="613c" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">深度学习模型</h1><p id="c977" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">RNN/LSTM 在处理时间序列数据方面得到了最好的证明，网上有大量的文章证明了其在广泛应用中的有效性。因此，我们采用 RNN/LSTM 架构。</p><p id="b2cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，由于我们的数据已经准备好，并且是三维形式，我们现在可以定义 RNN/LSTM 神经网络架构，该架构包括 2 个隐藏层，每个隐藏层具有激活函数<strong class="lb iu"> tanh </strong>(见图 5)，后面是一层<strong class="lb iu"> softmax 分类器</strong>。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="987f" class="nt ms it np b gy nu nv l nw nx">model = Sequential()</span><span id="b3a9" class="nt ms it np b gy oa nv l nw nx">#input</span><span id="7bbe" class="nt ms it np b gy oa nv l nw nx">model.add(LSTM(units=50, return_sequences='true', activation='tanh',<br/>input_shape = (x_train.shape[1], x_train.shape[2])) )<br/>model.add(Dropout(0.2))</span><span id="e5df" class="nt ms it np b gy oa nv l nw nx">#hidden layer 1<br/>model.add(LSTM(units=60, return_sequences='true',activation='tanh'))<br/>model.add(Dropout(0.2))</span><span id="e3ba" class="nt ms it np b gy oa nv l nw nx">#hidden layer 2<br/>model.add(LSTM(units=60, activation='tanh'))<br/>model.add(Dropout(0.2))</span><span id="13e0" class="nt ms it np b gy oa nv l nw nx">#output<br/>model.add(Dense(units=3,activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="4bfb" class="nt ms it np b gy oa nv l nw nx">print(model.summary())</span></pre><p id="b283" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是模型摘要的输出(见图 9)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/97580cb01ab86f68f76b2cc88c8e7d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P8V10e6R1c6JJUhOG55y-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9:模型架构</p></figure><p id="ea4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">培训 RNN/LSTM 模型</strong></p><p id="8557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RNN/LSTM 模型总共被训练了 30 个时期，尽管我试图训练该模型 40 个时期，该模型被视为过度拟合。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="a200" class="nt ms it np b gy nu nv l nw nx">history = model.fit(x_train, y_train,batch_size=32, epochs=30)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/f0ca64f433a471d44e532e4d765e8f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXvYYrdlQsabwsq_PFRqtA.png"/></div></div></figure><p id="e0a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mi">注:‘</em><strong class="lb iu"><em class="mi">历史</em></strong><em class="mi">’变量用于记录模型在训练过程中的损耗、准确度等必要参数。</em></p><p id="2a6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用 keras.evaluate()函数获得准确度，并且获得接近 94%的总准确度(参见图 10)。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="60be" class="nt ms it np b gy nu nv l nw nx">model.evaluate(x_test, y_test, verbose=2)</span></pre><p id="b76a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码展示了如何绘制精度和损耗。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="2757" class="nt ms it np b gy nu nv l nw nx">plt.subplot(211)<br/>plt.plot(history.history['accuracy'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['accuracy'], loc='upper left')</span></pre><p id="4e0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是模型精度的输出，与 79%的精度相比，获得了大约 94%的精度。我们已经看到了对以前方法的改进。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/6416ce65dcea528495bc216b891240ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9daBCZMrG_DZlbBt-xrsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 10:模型的准确性</p></figure><p id="daf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们获得的模型损耗为 0.12，比 CNN 方法好<strong class="lb iu"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/48807405efcbbf56c826d27499fb81bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFXhBJps49UpRe8tL4NUkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 11:模型的总损失</p></figure><p id="0ba6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是混淆矩阵(见图 12 ),它更深入地展示了模型在分类中的实际表现。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="2e5e" class="nt ms it np b gy nu nv l nw nx">cnf_matrix = confusion_matrix(np.where(y_test != 0)[1], model.predict_classes(x_test))</span><span id="f2b3" class="nt ms it np b gy oa nv l nw nx">plt.figure(figsize=(7,7))</span><span id="20ce" class="nt ms it np b gy oa nv l nw nx">plot_confusion_matrix(cnf_matrix, classes=np.unique(np.where(y_test != 0)[1]), title="Confusion matrix")</span><span id="d75a" class="nt ms it np b gy oa nv l nw nx">plt.show()</span></pre><p id="82f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">* <em class="mi">注:函数-混淆 _ 矩阵()位于</em> <a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank"> <em class="mi"> sklearn 文档</em> </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/6d2b80b7a4f9abecf0785357ab105e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGCz0zNOmqJmA3ntKkT1nw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 12 : <a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">来自 sklearn 文档</a></p></figure><h1 id="f686" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="99fb" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在这篇文章中，我们看到了一种替代方法，如何使用<strong class="lb iu"> RNN/LSTM </strong>神经网络架构通过<strong class="lb iu"> </strong>解决<strong class="lb iu">预测性维护问题</strong>，并证明比之前的<strong class="lb iu"> CNN </strong>方法更好。</p><h1 id="3808" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">参考</h1><ul class=""><li id="58a0" class="om on it lb b lc nj lf nk li oo lm op lq oq lu or os ot ou bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/playing-with-time-series-data-in-python-959e2485bff8">https://towards data science . com/playing-with-time-series-data-in-python-959 e 2485 BF F8</a></li><li id="e047" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">https://towards data science . com/extreme-rare-event-class ification-using-auto encoders-in-keras-a565b 386 f 098</a></li><li id="0022" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/remaining-life-estimation-with-keras-2334514f9c61">https://towards data science . com/remaining-life-estimation-with-keras-2334514 f9c 61</a></li></ul></div></div>    
</body>
</html>