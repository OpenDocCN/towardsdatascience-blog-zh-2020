<html>
<head>
<title>Build Your Own Neural Network From Scratch In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始构建自己的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-your-own-neural-network-from-scratch-with-python-dbe0282bd9e3?source=collection_archive---------30-----------------------#2020-05-20">https://towardsdatascience.com/build-your-own-neural-network-from-scratch-with-python-dbe0282bd9e3?source=collection_archive---------30-----------------------#2020-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d9eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解神经网络的基础知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ff491677b90e7ac49346bb353d4b8481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2xZwUla4CB7R4Jcw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">娜塔莎·康奈尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4ff9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多python库可以用来构建和训练神经网络，如Tensorflow和Keras。但要真正理解神经网络，我们需要了解它的基本结构，并能够建立和训练我们自己的网络。</p><p id="3fb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与常规的机器学习算法相比，神经网络可以更好地学习数据。事实上，<em class="lv">神经网络算法可以解释为一堆线性回归</em>，其中每个节点都是一个线性回归的输出。</p><p id="8fd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络包括:</p><ul class=""><li id="ffd8" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">一个输入层，<strong class="lb iu"> <em class="lv"> x </em> </strong></li><li id="ac57" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">任意数量的隐藏层</li><li id="3919" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">一个输出层，<strong class="lb iu"> <em class="lv"> y` </em> </strong></li></ul><p id="b54d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，输入层有3个节点，下一层(隐藏)有4个节点，输出层有2个节点。输入层不包括在层数中，因此它是一个2层网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/720d19fc1add3e858400a5b28ad0ce1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*0QeueiN9Se6IHLD-5sYchA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两层神经网络结构</p></figure><p id="d457" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两层中的每个节点都是一个线性回归的输出。在每一层，在执行线性回归之后，这些值被馈送到激活函数。</p><p id="95bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络的基本算法应该是这样的。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="f333" class="mq mr it mm b gy ms mt l mu mv">for n epochs:<br/>    1. forward_propagation() #predicting output<br/>    2. backward_propagation() #updating parameters according to loss</span></pre><p id="ead4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数名暗示了算法的基本结构。在本文中，我们将建立一个2层神经网络。</p><p id="45ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看我们将使用的常见命名约定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/7dfd493a277c133aaa060266136e5da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt-hfAQBQj8TtjzLqJCc8Q.png"/></div></div></figure><p id="c8a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mx my mz mm b">Z</code>是线性向前值，即<code class="fe mx my mz mm b">Z = W.X</code>，<code class="fe mx my mz mm b">A</code>是节点激活，即<code class="fe mx my mz mm b">A = sigmoid(Z)</code>。我们将在网络中使用sigmoid激活函数。</p><h1 id="deb2" class="na mr it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">1.初始化参数</h1><p id="d128" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">首先，我们初始化神经网络的权重和层大小。</p><p id="3c33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将变量定义如下:</p><ul class=""><li id="9a39" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><code class="fe mx my mz mm b">[num_samples, n_x]</code>:输入的形状</li><li id="1235" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe mx my mz mm b">n_x</code>:输入X的特征数</li><li id="dcfd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe mx my mz mm b">n_h</code>:隐藏层节点</li><li id="7acb" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe mx my mz mm b">n_y</code>:每个样本的目标值数量</li></ul><p id="b69b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们根据输入和输出值以及我们选择的隐藏层大小来初始化层大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="855a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们基于层大小初始化权重。</p><ul class=""><li id="9a94" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">在上图中，对于第一层，我们需要定义权重，以便能够计算4个<code class="fe mx my mz mm b">(n_h)</code>线性回归，下一层中的每个节点一个。</li><li id="29ee" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">所以第一层中的权重，<code class="fe mx my mz mm b">W1</code>应该是<code class="fe mx my mz mm b">(3,4)</code>或<code class="fe mx my mz mm b">(n_x,n_h)</code>(对三个(n_x)输入特征的四个(n_h)线性回归)。</li></ul><p id="cf4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用这个类比，你可以很容易地猜出上面网络中权重<code class="fe mx my mz mm b">W2</code>的形状。</p><p id="2596" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义初始化权重的函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="c4e5" class="na mr it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">2.正向传播</h1><p id="2de4" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">正向传播由一组线性回归和每层的非线性激活函数组成。</p><p id="8420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用sigmoid函数作为神经网络的激活函数。</p><p id="3ce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你得到正确的权重，前向传播是非常简单的。</p><blockquote class="ny"><p id="7126" class="nz oa it bd ob oc od oe of og oh lu dk translated">输出= sigmoid((W2 * sigmoid(W1 * X+B1))+B2)</p></blockquote><p id="dbc3" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">让我们定义我们的前向函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正向传播</p></figure><h1 id="9a9f" class="na mr it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">3.反向传播</h1><p id="2f06" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">这是棘手的部分。反向传播相应地在损失最小化的方向上更新权重参数。<strong class="lb iu">我们将使用<em class="lv">均方差(MSE) </em>损失来评估我们的网络。这个计算看起来很复杂，但实际上很简单。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/04c3aaf1de5ef707f0911ecd76107d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLpQnky7WXP_WGgPyfcdUQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6d9c418366b89b225d9c74bd271bac21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcc6Ssht4C9EG4MtGaJqGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失函数的加权导数和sigmoid导数</p></figure><p id="1039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很简单，对吧？现在，让我们定义反向传播的函数。我们将计算梯度，并在向后的步骤中更新权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">反向传播</p></figure><h1 id="4fed" class="na mr it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">4.培养</h1><p id="148a" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">训练中的一个时期包括一个向前和向后传播步骤。</p><p id="a77d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用来自<code class="fe mx my mz mm b">sklearn.datasets</code>模块的乳腺癌数据集。该数据集有30个特征和569个样本，并用隐藏层中的4个节点训练我们的网络1000个时期。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="7268" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较10个随机样本的预测值和输出值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/3df4e8cc5a9c85013fc6c643d23447ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XyPMSJwOgTPA5cJm_AfUUg.png"/></div></div></figure><p id="a531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来我们的网络工作得非常好！</p><p id="89b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，预测值与实际值略有不同。这是可取的，因为这意味着模型不会过度拟合，并且会更好地概括。</p><h1 id="772e" class="na mr it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">结论</h1><p id="127f" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在我们的神经网络中，我们在激活层使用了sigmoid函数。但是，根据问题的类型，还有其他的激活功能可能会有用。事实上，激活函数可以是你选择的任何函数，如果你认为这个函数非常适合我们试图解决的特定问题。但是神经网络中常用的激活函数可以在这里找到<a class="ae ky" href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>