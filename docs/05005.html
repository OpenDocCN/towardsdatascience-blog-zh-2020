<html>
<head>
<title>Predicting App Subscription using Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用逻辑回归预测应用订阅</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-how-to-on-app-behavior-data-8a95802a988f?source=collection_archive---------37-----------------------#2020-04-30">https://towardsdatascience.com/logistic-regression-how-to-on-app-behavior-data-8a95802a988f?source=collection_archive---------37-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1102" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Sklearn对应用程序行为数据进行功能工程、数据处理和逻辑回归建模演练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/7a7c7285ec75c30254be57db436f8000.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*B3w2eh1Aj-Jz_t1RMnfkTw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://unsplash.com/photos/F1I4IN86NiE" rel="noopener ugc nofollow" target="_blank">链接</a>改编自Unsplash的Img</p></figure><p id="e516" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">之前的<a class="ae ku" rel="noopener" target="_blank" href="/eda-how-to-on-app-behavior-data-77fde7384a70">文章</a>是关于原始数据上的EDA。在这里，我将解释如何执行<strong class="kx iu">特征工程、数据处理</strong>，并最终<strong class="kx iu">使用移动app行为数据创建逻辑回归模型</strong>。它分为7个部分。</p><ol class=""><li id="6c54" class="lr ls it kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">问题陈述</li><li id="0e5d" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">特征工程</li><li id="cd9b" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">数据处理</li><li id="06a1" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">模型结构</li><li id="ee90" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">模型试验</li><li id="50dd" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">模型验证</li><li id="df6c" class="lr ls it kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">摘要</li></ol><p id="e341" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在让我们开始吧🏃‍♀️🏃‍♂️.</p><p id="6c6b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">1.<strong class="kx iu">问题陈述</strong></p><p id="ea9c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一家金融科技公司委托我们分析移动应用行为数据，以帮助引导客户获得付费订阅服务。具体来说，任务是确定哪些用户最有可能不注册。图1是具有<strong class="kx iu"> <em class="mf"> 12列和50，000行</em> </strong>的原始数据片段。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/3519a1fc492229b0856be1110d31d6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WI9Igp2HAE9Xy_DtCgQeVA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1原始数据片段</p></figure><p id="1f21" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.<strong class="kx iu">特征工程</strong></p><p id="3f04" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="mf">特征工程是将原始数据转化为最能代表问题的特征的艺术。</em> </strong>只有用正确的特征工程，模型才能做出最好的预测。我们将从两个方面进行特征工程。</p><p id="2683" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.1因变量工程</p><p id="3bbf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因变量是列'<em class="mf">已注册'</em>。与'<em class="mf"> enrolled' </em>密切相关的一列是'<em class="mf"> enrolled_date' </em>。基本上，用户可以在任何日期报名，以'<em class="mf">报名'</em>为1。由于大多数应用程序功能在第一个24小时后不可试用，我们需要设置注册的时间限制。</p><p id="9ff0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了设置有效的登记时间限制，我们计算'<em class="mf"> first_open '和' enrolled_date' </em>之间的时间差，并调查登记分布。</p><p id="7eb8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第一步是计算'<em class="mf"> first_open '和' enrolled_date' </em>之间的时差。具体来说，将'<em class="mf"> first_open '和' enrolled_date' </em>解析为<em class="mf"> datetime </em>类型，并以小时为单位计算时差。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="f2a7" class="mq mr it mm b gy ms mt l mu mv">dataset[“first_open”] = [parser.parse(row_date) for row_date in dataset[“first_open”]]</span><span id="52e8" class="mq mr it mm b gy mw mt l mu mv">dataset[“enrolled_date”] = [parser.parse(row_date) if isinstance(row_date, str) else row_date for row_date in dataset[“enrolled_date”]]</span><span id="de37" class="mq mr it mm b gy mw mt l mu mv">dataset[“difference”] = (dataset.enrolled_date-dataset.first_open).astype(‘timedelta64[h]’)</span></pre><p id="c252" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第二步是通过绘制“<em class="mf">差异”</em>列的直方图来查看注册分布。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="41e3" class="mq mr it mm b gy ms mt l mu mv">response_hist = plt.hist(dataset[“difference”].dropna(), color=’#3F5D7D’)<br/>plt.title(‘Distribution of Time-Since-Screen-Reached’)<br/>plt.show()</span></pre><p id="edb8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">图2显示用户注册高度集中在前50个小时。因此，我们将响应限制设置为48小时。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mx"><img src="../Images/e384fe3d52b2bec0975725bb2997a92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SvnAv-OR0vXWWAuwTG9tWw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2注册分布</p></figure><p id="70ba" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，有了48小时的登记限制，我们将'T38已登记'T39列重置为0，即未登记。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="9876" class="mq mr it mm b gy ms mt l mu mv">dataset.loc[dataset.difference &gt; 48, ‘enrolled’] = 0<br/>dataset = dataset.drop(columns=[‘enrolled_date’, ‘difference’, ‘first_open’])</span></pre><p id="fb51" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上面我们删除了<em class="mf">【注册日期】</em><em class="mf">【差异】【首次公开】</em>栏，因为培训不需要它们。</p><p id="cdcf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.2自变量工程</p><p id="97bf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">数据集中唯一的非数字列是<em class="mf">‘screen _ list’</em>。这是一个包含用户查看的所有屏幕功能的列表。所以我们需要把它转换成数值变量。一种方法是将<em class="mf">‘screen _ list’</em>中的每个唯一屏幕转换为分类变量。<em class="mf">但是，独特的屏幕太多了。因此，我们将只关注那些最受欢迎的屏幕。</em>具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="8343" class="mq mr it mm b gy ms mt l mu mv">top_screens = pd.read_csv(‘top_screens.csv’).top_screens.values<br/>dataset[“screen_list”] = dataset.screen_list.astype(str) + ‘,’ <br/>for sc in top_screens:<br/>    dataset[sc] = dataset.screen_list.str.contains(sc).astype(int)<br/>    dataset['screen_list'] = dataset.screen_list.str.replace(sc+",", "")</span></pre><p id="a527" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上面，我们为代表最受欢迎屏幕的<em class="mf">‘top _ screens’</em>中的每个屏幕创建了一个列。</p><p id="b3cc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们创建一个<em class="mf">‘Other’</em>列，作为所有非流行屏幕的总和。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="74d0" class="mq mr it mm b gy ms mt l mu mv">dataset[‘Other’] = dataset.screen_list.str.count(“,”)<br/>dataset = dataset.drop(columns=[‘screen_list’])</span></pre><p id="c0dd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">最后，如果我们仔细检查列的名称，我们会发现许多列代表相同的特性。</strong>例如，<em class="mf">保存1 </em>、<em class="mf">保存5 </em>是关于保存的屏幕，而<em class="mf">信用1 </em>、<em class="mf">信用3 </em>是关于信用的屏幕。我们需要通过合计相同功能屏幕的数量来聚合相同的功能。具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="c626" class="mq mr it mm b gy ms mt l mu mv">cm_screens = [“Credit1”, “Credit2”, “Credit3”, “Credit3Container”, “Credit3Dashboard”]<br/>dataset[“CMCount”] = dataset[cm_screens].sum(axis=1)<br/>dataset = dataset.drop(columns=cm_screens)</span></pre><p id="586d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对其他特征重复相同的方法(例如，<em class="mf">保存</em>、<em class="mf">贷款</em>等)，我们得到了包含所有数字变量的最终数据集。图3显示了所有的列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a236e3541634f3054f1e318b29507873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*eHLeBSJXP4qzZ7XJhh4kkw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3所有数据列名称</p></figure><p id="acb5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">总之，我们使用数据挖掘技术来提炼和提取最能代表移动应用用户行为的属性。</p><p id="31fd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.<strong class="kx iu">数据处理</strong></p><p id="443b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">数据处理包括数据分割和特征缩放。</p><p id="af6d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.1数据分割</p><p id="14fb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第一步是分离自变量和因变量。具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="53b6" class="mq mr it mm b gy ms mt l mu mv">response = dataset[“enrolled”]<br/>dataset = dataset.drop(columns=”enrolled”)</span></pre><p id="14ef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">第二步是将数据分成训练集和测试集。具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="5a85" class="mq mr it mm b gy ms mt l mu mv">X_train, X_test, y_train, y_test = train_test_split(dataset, response, test_size = 0.2, random_state = 0)</span></pre><p id="3d22" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.2特征缩放</p><p id="d0bb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">特征缩放是为了避免任何变量支配其他变量，即采用更高的权重和对模型学习的强烈影响。这里，我们通过去除平均值并缩放到单位方差来标准化特征。具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="8eff" class="mq mr it mm b gy ms mt l mu mv">from sklearn.preprocessing import StandardScaler<br/>sc_X = StandardScaler()<br/>X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))<br/>X_test2 = pd.DataFrame(sc_X.transform(X_test)) <br/>X_train2.columns = X_train.columns.values<br/>X_test2.columns = X_test.columns.values<br/>X_train2.index = X_train.index.values<br/>X_test2.index = X_test.index.values<br/>X_train = X_train2<br/>X_test = X_test2</span></pre><p id="9ee1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意，<em class="mf"> StandardScaler() </em>返回一个<em class="mf"> numpy </em>数组，该数组会丢失列名和索引。因此，我们再次将缩放后的数据转换为<em class="mf">数据帧</em>，以保留行和列标识符。</p><p id="5a47" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">太好了。模型的数据终于准备好了。图4是具有<strong class="kx iu"> <em class="mf"> 50列和50，000行</em> </strong>的最终数据的简要视图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图4最终数据的简要视图</p></figure><p id="74cd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4.<strong class="kx iu">模型建筑</strong></p><p id="3eb0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里我们将创建一个<strong class="kx iu">逻辑回归</strong>模型来预测一个二元因变量，即是否入学。根据<a class="ae ku" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a>，标签为1的概率的对数是一个或多个独立变量的线性组合。本质上，我们试图估计一个逻辑模型的系数，如图5所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0d1ea26fc22f444e35858ebb0616b8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mtnFy_uS2AjONsAbXS1z3Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图5逻辑回归模型(作者创建的Img)</p></figure><p id="2550" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="0bf0" class="mq mr it mm b gy ms mt l mu mv">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0, penalty = ‘l1’)<br/>classifier.fit(X_train, y_train)</span></pre><p id="2893" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意我们使用拉索(<em class="mf">【L1】</em>)正则化模型，而不是正态回归模型。<em class="mf"> L1 </em>正则化给损失函数增加一个等于<strong class="kx iu"> <em class="mf">系数</em> </strong>大小的绝对值之和的惩罚，如图6所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d663b1592a8a67aba752a4b04972165f.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*ifPqQKx7urnyUA3HTMlrHw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图L1正则化的损失函数(作者创建的Img)</p></figure><p id="4690" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意<em class="mf"> L1 </em>和<em class="mf"> L2 </em>正则化的区别在于<em class="mf"> L2 </em>相加的惩罚是<strong class="kx iu"> <em class="mf">系数</em></strong><em class="mf"/>大小的平方值之和，如图7所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/090617b59ea4440d3390f8c3dc42a4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*cx8p6d-iaLacqn5E6KJIEQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图7 L2正则化损失函数(作者创建的Img)</p></figure><p id="d630" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">5.<strong class="kx iu">模型测试</strong></p><p id="c5a4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">训练好模型后，让我们在<em class="mf"> X_test </em>上测试模型。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="a209" class="mq mr it mm b gy ms mt l mu mv">y_pred = classifier.predict(X_test)<br/>cm = confusion_matrix(y_test, y_pred)</span></pre><p id="5268" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了更好地回顾预测结果，让我们将其与实际结果进行比较。所以，具体来说，</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="4557" class="mq mr it mm b gy ms mt l mu mv">final_results = pd.concat([y_test, test_identity],axis =1).dropna()<br/>final_results[‘predicted_results’] = y_pred<br/>final_results[[‘user’, ‘enrolled’,‘predicted_results’]].reset_index(drop=True)</span></pre><p id="800f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">图8显示了实际登记的结果和预测的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/27938b4f36a2ee6a74e75842648c51d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*9_E9w5j43TgBj9FQHRAFAw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图8预测和实际结果对比</p></figure><p id="1db8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">图9展示了混淆矩阵。这告诉我们测试精度为0.768。不错的结果😃。如果你想知道如何计算精度，请阅读这篇<a class="ae ku" href="https://medium.com/@vistaxjtu/intuitively-explain-accuracy-precision-recall-and-f1-777563342aca" rel="noopener">文章</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3ede255f851f13e658a631b87d59987e.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*EMhjFcROFInuDTdc4fMDYg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图9混淆矩阵</p></figure><p id="1ffa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">6.<strong class="kx iu">模型验证</strong></p><p id="bdb4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有了上面的测试准确性，作为一个数据科学家，你应该问一个问题:这是模型性能的真实反映吗🤔？为了回答这个问题，我们将使用K倍交叉验证。</p><p id="8e6e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">具体来说，将训练数据分成10个子集，使用9个子集来训练模型，剩余的用于验证。重复这个训练和验证10次。最后平均准确率和损耗。</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="2c1f" class="mq mr it mm b gy ms mt l mu mv">accuracies = cross_val_score(estimator= classifier, X= X_train, y = y_train, cv = 10)</span></pre><p id="b051" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们得到的平均精度为<strong class="kx iu"> 0.767 </strong>，标准偏差为<strong class="kx iu"> 0.10 </strong>。很好，模型显示出很小的差异，即模型始终是准确的。</p><p id="85bb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">7.<strong class="kx iu">总结</strong></p><p id="62d7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">概括地说，我们经历了特征工程、数据处理、模型构建、测试和验证。特征工程和数据处理是耗时的，但是为模型准备数据是最重要的。如果你想了解模型优化，请阅读这篇<a class="ae ku" rel="noopener" target="_blank" href="/ann-classification-model-evaluation-and-parameter-tuning-9174fd5ad0c2">文章</a>。</p><p id="1318" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">太好了！这就是所有的旅程！如果您需要源代码，请随时访问我的</strong> <a class="ae ku" href="https://github.com/luke4u/Customer_Behaviour_Prediction/tree/main/enrollment_prediction" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu"> Github </strong> </a> <strong class="kx iu">页面🤞🤞。</strong></p></div></div>    
</body>
</html>