<html>
<head>
<title>Variational Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可变自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/variational-autoencoders-63191b75c576?source=collection_archive---------17-----------------------#2020-02-24">https://towardsdatascience.com/variational-autoencoders-63191b75c576?source=collection_archive---------17-----------------------#2020-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eac2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">VAE和在哪里找到他们</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/f5c5196c6ddbd8ca57fe9f53de0da2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*U4sgcl6DT6-abPh4cWKqSQ.png"/></div></figure><p id="9555" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，在过去两年的机器学习面试中，我被要求解释变分自动编码器(VAE)大约五次。对于某个公司来说，这个问题实际上是在同一天由两个不同的面试官重复提出的。</p><p id="01c0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为什么VAE是如此受欢迎的面试问题？嗯，它们确实涵盖了概率论和信息论中的一些有趣的主题，而且它们被广泛使用，被认为是相对普通的知识。这些模型有一个独特的规格和一个长期的衍生。这意味着他们可以花30分钟走一遍，但仍然很容易让面试官验证。此外，人们通常只了解VAE背后的直觉，所以这是一个很好的面试问题(在我看来)。</p><p id="c8ba" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">显然，我真的不喜欢把VAE作为一个面试问题，但我也认为他们的申请由于其他原因被夸大了。在以后的文章中，我将推导出我所看到的这类“优化游戏”的真正的潜在机制。现在，让我们彻底了解一下VAE，作为VAE召回的一站式商店。</p><h1 id="dfd9" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">自动编码器</strong></h1><p id="fe92" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">首先，自动编码器是神经网络的一种形式，其使用一些损失函数L(x，r(x))专门训练重构函数r = g(f (x))，以便产生更多信息和/或更低维度的数据重构。这个重构函数是编码函数z = f(x，θ)和解码函数x = g(z，θ)的组合，编码函数将数据映射到学习的潜在空间，解码函数x = g(z，θ)充当生成函数，在潜在空间中为z产生x。这种重建是通过无监督学习创建的，并且通常独立于任何标记或预测任务。</p><p id="5142" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">虽然神经网络最常用于生成高维参数化函数以解决判别任务，但自动编码器被训练来重新生成模型被训练的数据。如果我们对我们的损失函数进行正则化，并对我们的网络进行战略性结构调整，我们可以改变自动编码器的行为。</p><p id="a95a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通过引入正则化子ω，我们可以影响我们的自动编码器学习的空间类型。一种这样的增强是稀疏自动编码器(SAE ),它学习稀疏潜在表示。考虑正则化损失函数项L(x，r(x))+ω(z)，其中ω(z)=λ∑| z _ I |惩罚我们的潜在表示的分量的幅度。这种损失使自动编码器的编码函数z = f (x，θ)发生偏移，以根据λ的标度为潜在表示的尽可能多的分量产生零或小的值。</p><p id="f485" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">作为正则化损失函数的一种替代方法，我们也可以考虑对输入到自动编码器的数据进行正则化。去噪自动编码器(DAE)旨在通过最小化准则L_DAE = E[L(x，r(N(x))]]来生成简化的重建r(x)，其中对于某些高斯误差ε = N (0，σ)，N(x) = x + ε。输入到网络中的数据本质上是被破坏的，模型学习过滤掉噪声，以便再现源数据。[1]</p><p id="8c4c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这些模型使用什么类型的信息来产生这些潜在的表征？自动编码器学习训练数据的概率密度p(x)的流形结构<br/> ∂对数p(x)/∂x。这种流形结构有助于引导数据潜在空间的构建。形式上，对于去噪自动编码器，可以示出我们产生以下最优重构函数r*(x) = x + σ^2 ∂对数p(x)/∂x + o(σ),其行为渐近于高斯噪声的方差。渐近行为(r *(x)-x)/σ→∂对数p(x)/∂x为<br/> σ → 0允许我们通过p(x)上的诱导流形确定x的信息表示。[2]</p><h1 id="dc95" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">可变自动编码器</h1><p id="7e11" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">变分自动编码器(VAE)产生了我们的潜在代表符合给定分布的愿望，以及变分推理过程的简单近似使计算易处理的观察。我们将回顾定义潜在空间分布的步骤，以及使用变分推论来训练我们的模型的步骤。</p><p id="5326" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">VAE的嵌入机制是一种学习的编码分布z = q(x ),可以更好地对数据集的潜在结构进行排序。根据这种分布，可以构建概括的推理模型，以便重现数据。通过概括，我们意味着当通过给定的生成函数g(z)馈送时，从在潜在空间Z中的样本Z上学习的分布P(Z)中抽取的样本更有可能产生我们的原始数据集X的样本。母函数g(z)可以被视为参数化函数g(z，θ)或数据P(X | z；θ).形式上，当我们构建我们的潜在空间时，我们试图最大化观察每个X的概率，用于由下式定义的生成过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/ff921db997760bc70e99b338539e2916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*QezyKqQjLhJ0CZ9pi1-RqQ.png"/></div></figure><p id="ca4b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了评估这个函数，我们需要建立假设的先验分布P(Z ),以及训练的分布P(X | Z；θ).先验分布通常假设为正态分布，使得P(Z) = N(0，I)。</p><p id="4f68" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于VAE，解码网络用于生成P(X|z，θ)的样本。我们假设的输出服从(通常为多元正态)分布，均值为g(z，θ),方差为σ2∫I，如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/86246ef092fd174a19b896690324945a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*FSL16sHoKSBdD-lC-dih-w.png"/></div></figure><p id="321a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了评估这些样本有多精确，我们需要建立一些简化的方法来评估变分推断积分。显然，在g的每一个优化步骤中，在整个区域内对样本进行充分的积分计算是非常昂贵的。</p><p id="4a01" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了简化对P(X|z)的评估，我们只关注(近似地)对那些可能产生X的z进行采样，服从生成过程g。我们选择用来表示该采样的分布是Q(z|X)，由我们的编码函数f实现。然后我们用下式近似P(X|z)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2bd54d45290872335f3d2cbc0a7582c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*ZitYiLVgOtvcMjKmAPbprw.png"/></div></figure><p id="4428" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们需要把期望E[P(X|z)]和P(X)联系起来。我们可以通过对潜在空间上任意分布φ(z)的KL散度和交叉熵公式进行一些代数操作来做到这一点。</p><p id="09a1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">从KL(φ(z)| | P(z | X))，我们可以导出(在一些对数、互信息和交叉熵替换之后)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/88fdbc85d1233db48db16381f0d59ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mEQJZrJN05h27ElKViCV-w.png"/></div></div></figure><p id="0081" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们设置φ(z)= Q(z | X)，我们可以看到log P(X)由我们的编码器f∞Q(z | X)、我们的解码器g∞P(X | z)和我们假设的先验分布P(z)决定。变分自动编码器的主要工程发展是可以有效地优化上述等式的右边。如果Q(z|X)和P(z)都假定为高斯分布，KL(Q(z|X)||P(z))有一个封闭形式的解。我们可以引入函数μ(X，θ)和σ(X，θ)，作为编码神经网络层，这又允许我们指定Q(z|X) = N(μ(X)，σ(X))。VAE损耗方程的闭合形式的KL发散项KL(Q(z|X)||P(z))是[3]</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2c0db304cc6c99a4bec987de18bcbe09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*6XNjDKOPeRokaCHomU9RZg.png"/></div></figure><p id="75d1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们如何强制我们的编码分布Q(z|X)遵循正态分布N(μ(X)，σ(X))的约束？在训练期间从<br/> Q(z|X) = μ(X，θ)+εσ(X，θ)采样，噪声项为ε ~N(0，1)，允许我们强制我们的样本表现为Q上的期望正态分布。这被称为<em class="ms">重采样技巧。</em>在评估期间，我们只需要设置ε=0，我们的输出就是确定性的。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="adfa" class="lm ln it bd lo lp na lr ls lt nb lv lw jz nc ka ly kc nd kd ma kf ne kg mc md bi translated">结尾部分</h1><p id="d806" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">现在我们有了VAE损失函数的推导，以及VAE建筑核心力学的概率解释的基本内容。</p><p id="71d8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在实践中，特别是对于更复杂的层，实际上很难证明由变分体系结构产生的表示实际上遵循其公式中使用的先验分布。这部分是因为在进行任何类型的降维时都会丢失信息。然而，我也认为大多数类似VAE的模型在学术数据集之外不太适合他们的先验知识。</p><p id="8f51" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在任何情况下，我都会在以后的文章中，试着回顾一下我对变分步骤的更多理论上的抱怨。</p><h2 id="c6f2" class="nf ln it bd lo ng nh dn ls ni nj dp lw kz nk nl ly ld nm nn ma lh no np mc nq bi translated">参考</h2><p id="a869" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">[1]伊恩·古德费勒，约舒阿·本吉奥和亚伦·库维尔。<em class="ms">深度学习。</em>麻省理工学院出版社。2016.</p><p id="811f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[2]纪尧姆·阿兰和约舒阿·本吉奥。<em class="ms">正则化自动编码器从数据生成分布中学习到什么。2014年</em></p><p id="25dc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[3]卡尔·多尔什。变型自动编码器教程。 2016年。</p></div></div>    
</body>
</html>