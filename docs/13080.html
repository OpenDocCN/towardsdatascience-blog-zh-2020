<html>
<head>
<title>Introduction to Gradient Descent with linear regression example using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyTorch 的线性回归示例的梯度下降介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-gradient-descent-with-linear-regression-example-using-pytorch-dbf7a9540d78?source=collection_archive---------34-----------------------#2020-09-08">https://towardsdatascience.com/introduction-to-gradient-descent-with-linear-regression-example-using-pytorch-dbf7a9540d78?source=collection_archive---------34-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="24bc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器/深度学习中广泛使用的优化方法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bf951fe4f8d258f722c0dccc354b235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A6p20dONOeXsdALk"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰里米·毕晓普在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="24ce" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="7fd7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在机器学习中，通常情况下，我们需要找到一个损失函数(或成本函数)的最小值。梯度下降法是一种广泛应用的优化方法。</p><p id="1001" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我将通过一些例子讨论梯度下降法，包括使用 PyTorch 的线性回归。</p><h1 id="e5b8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结石</h1><p id="a6ce" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">求函数最大值或最小值的一种方法是求斜率为零的点。函数的最大值或最小值将是函数的导数等于零的解。</p><p id="1762" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以这个函数为例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d3238077bfed11550a443fb43598c74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*QpLCcA6oFiNRdM_xenLFgA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/3fcf859e8d2fcb2a5969392d9df8d88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vA70dCK8FMZ5hzy79Pa3HA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">f(x)= x * * 2–5x+5，图片由作者提供</p></figure><p id="446b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该函数的导数为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a90515c29805f08d7bcc51d182c18dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*sJWEKGIgrebcshuQHLLoRA.png"/></div></figure><p id="c339" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以设 f'(x) = 0，我们可以求出 x=2 为解。这意味着函数的斜率在 x=2 时等于零，函数的最小值是 f(2)=1。</p><p id="5f4b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种方法找到了解析解，然而，在实际应用中有时很难将这种方法应用于复杂函数。因此，我们需要数值方法来寻找近似解，梯度下降法就是其中一种方法。</p><h1 id="4d38" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降</h1><p id="90ad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">梯度的数学解释可以在这个<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank">环节</a>中找到。</p><p id="5347" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于一维函数 f(x)，梯度表示为 f'(x)。对于多维函数，表示如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/7d54a8ee1d44ff629033c5cd50793d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*AfgeM2EZ1W2E8LQd63DCvA.png"/></div></figure><blockquote class="mu mv mw"><p id="ef62" class="lr ls mx lt b lu mn ju lw lx mo jx lz my mp mc md mz mq mg mh na mr mk ml mm im bi translated">来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank"> wiki </a>:如果函数的梯度在<em class="it"> p </em>点不为零，那么梯度的方向就是函数从<em class="it"> p </em>开始增加最快的方向，梯度的大小就是那个方向的增加率。</p></blockquote><p id="e8e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">梯度下降试图通过下降到梯度的相反方向来接近函数的最小值。</p><p id="10a9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它迭代地更新参数(这里是 x)来寻找解。所以首先，我们需要一个解的初始猜测(x0)，然后基于初始猜测计算梯度，然后基于计算的梯度更新解(x)。可以用这个公式来解释:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/68522fdf79918bddceaa5f1beb3d1b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*0xNM_JpTU9b-UrNKQUV3gA.png"/></div></figure><p id="11b7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">t 是迭代次数，r 是学习率。</p><p id="688a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以，首先，我们有一个初始猜测 x0，通过上面的等式，我们可以找到 x1:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/32a10c794c7d2c1400c77ce7d7dec3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gkIt3OHWeRzS2sEd_EV9XQ.png"/></div></figure><p id="5e4f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后通过 x1，我们可以找到 x2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f292f72452df9979e47e047bdba0ba79.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gkX3FQeBKH3grQyepLtkBw.png"/></div></figure><p id="935e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过多次重复这一过程(历元)，我们应该能够找到参数(x ),其中函数在其最小值。</p><h2 id="b22d" class="nb la it bd lb nc nd dn lf ne nf dp lj ma ng nh ll me ni nj ln mi nk nl lp nm bi translated">示例 1:</h2><p id="3028" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们以前面的函数为例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d3238077bfed11550a443fb43598c74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*QpLCcA6oFiNRdM_xenLFgA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a90515c29805f08d7bcc51d182c18dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*sJWEKGIgrebcshuQHLLoRA.png"/></div></figure><p id="fdca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并将其应用于梯度下降方程</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/68522fdf79918bddceaa5f1beb3d1b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*0xNM_JpTU9b-UrNKQUV3gA.png"/></div></figure><p id="dbf6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">重新排列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b0bf4f1bffa95298653f4398cff3276b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*oTQfolSbbC4ljNxtla_dIg.png"/></div></figure><p id="6cf3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们设定学习率= 0.1，初始猜测值 x0=7。然后，我们可以轻松地更新和计算 x1、x2、x3…以及 f(x1)、f(x2)、f(x3)…</p><p id="ce38" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本例中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ec2586c3cedfa98943087c9ae70a6cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*DQ9Qj_u4uvOEKbQim1uDWw.png"/></div></figure><p id="f4de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以看到参数(x)接近 2，函数接近最小值 1。</p><p id="31fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们把它放在图表上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/489c6ac5b5868e9afdb41ad04c2dd238.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*AT4NevG5c2OEOnfa1me2oQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降(学习率= 0.1)(图片由作者提供)</p></figure><p id="69ca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们用一个更小的学习率(0.01)呢？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/296cf71429a174864015c2fa629a2002.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*8HJvJ1bmPukRvbWZaMt-bQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降(学习率= 0.01)(图片由作者提供)</p></figure><p id="2da2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如所料，达到最小值需要更多的迭代。</p><p id="c549" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们把学习提高到 0.3，它比 0.1 更快达到最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/277759b3fe44005a3e326a5ca4b14376.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*SSbzhkT22VSS5XEzPR20sg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降(学习率= 0.3)(图片由作者提供)</p></figure><p id="9ff9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们把它进一步提高到 0.7，它就开始超调了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/cb9b682e635ee3016ddbc5f26c6ff958.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*ZM-kmySNr4KsOV4gEhm6Rg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降(学习率= 0.7)(图片由作者提供)</p></figure><p id="eed9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们把它增加到 1，它根本达不到最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5964011cf25c9ec8c3f8b88f8b177b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*vw7Jd0FRSCxeJBBaxbaEmA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降(学习率= 1)(图片由作者提供)</p></figure><p id="00dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以学习率对梯度下降非常重要。如果太小，就需要大量的迭代，如果太大，就可能达不到最小值。</p><p id="9235" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们对梯度下降如何工作有了一个概念，让我们尝试将其应用到一个简单的机器学习模型——线性回归。</p><h2 id="c704" class="nb la it bd lb nc nd dn lf ne nf dp lj ma ng nh ll me ni nj ln mi nk nl lp nm bi translated">示例 2 —线性回归:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fe56f00bde50a8fd5f22a4cdec6c146b.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*mNgkLahFJAppKyjA_vr38Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归示例</p></figure><p id="25c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">线性回归是一种寻找两个变量之间线性关系的方法。它找到一条线，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/7412ca7a29bcbcb341282fdf4741098f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*_AqZuXl2wREAt1df8OdQbQ.png"/></div></figure><p id="90bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">描述图中所示的数据点。m 是直线的斜率，c 是截距。任务是找到最符合数据点的直线(m 和 c)。</p><p id="8bb4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">损失函数</strong></p><p id="cce5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用均方误差(MSE)来度量线和数据点的误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/fa876f24e1ba49cec76ad22d2af3d2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*P0MzMsR5Ppat0p5IlsffPA.png"/></div></figure><p id="2167" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它对点和线之间的差的平方和进行平均。</p><p id="0e47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以重新排列等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8d04b4b16d2ad97e431bfe148389e3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ywDEmJwBaxbnmLLqsKTWvw.png"/></div></figure><p id="fb0c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们想找到 m 和 c，它们给出了 MSE 的最小值。</p><p id="d4b5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">梯度下降</strong></p><p id="88a0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就像前面的例子，我们需要找到梯度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/699f91b33b8902e7c899bcc1e0b8d08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8H1XOFyJF_m99rPQ_KfG9g.png"/></div></div></figure><p id="d8c2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们可以用这个等式来更新 m 和 c:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c1809c124a8e8c056640c117d1135942.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gOxfk08dLDhuF4zM1qHykQ.png"/></div></figure><p id="8ee4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，我们需要对 m 和 c 的初始猜测，让我们从 m=0 和 c=0 开始，学习率= 0.0001</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="83ad" class="nb la it nq b gy nu nv l nw nx">#Gradient Descent</span><span id="140a" class="nb la it nq b gy ny nv l nw nx">for i in range(epochs):<br/>   Y_pred = m*X + c<br/>   d_m = (-2/n) * sum(X * (Y - Y_pred))<br/>   d_c = (-2/n) * sum(Y - Y_pred)<br/>   # Update m<br/>   m = m - r * d_m<br/>   # Update c<br/>   c = c - r * d_c<br/>   mse=(1/n) * sum((Y - m*X + c)**2)</span></pre><p id="8f2f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以基于梯度更新 m 和 c。让我们把它放在图表上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/dc72cca73fff5628cbcd0a9dfb65762b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*YeVPpGtdBhRTE2juBxSnjQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降-线性回归例子，学习率= 0.0001。(图片由作者提供)</p></figure><p id="40c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，仔细选择学习率是重要的，如果学习率增加到 0.01，计算将不会收敛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1420d94ec2fd8765504329af4de460c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*ywGyAbYTqbrMUTh46kMCXA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降-找不到最佳 m 和 c，学习率= 0.01。(图片由作者提供)</p></figure><p id="610d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个简单的例子中，我们自己通过对函数的<em class="mx">求导</em>来计算梯度，这对于更复杂的问题可能会变得困难。幸运的是，PyTorch 提供了一个工具来自动计算几乎任何函数的导数。</p><p id="99d6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> Pytorch 方法</strong></p><p id="3e77" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们定义这条线:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="3737" class="nb la it nq b gy nu nv l nw nx"><strong class="nq iu">def</strong> f(x, params):<br/>    m, c= params<br/>    <strong class="nq iu">return</strong> m*x + c</span></pre><p id="2ad5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">损失函数——均方误差:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="e351" class="nb la it nq b gy nu nv l nw nx"><strong class="nq iu">def</strong> mse(preds, targets): <strong class="nq iu">return</strong> ((preds-targets)**2).mean()</span></pre><p id="e1f9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">再次，我们由 m=0 开始，c=0，这里的<strong class="lt iu"> <em class="mx">需要 _grad_() </em> </strong>，这里是用来计算梯度的。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="6062" class="nb la it nq b gy nu nv l nw nx">params = torch.zeros(2).requires_grad_()</span></pre><p id="e957" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们可以根据第一个参数预测 y 值，并画出来。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="f6ab" class="nb la it nq b gy nu nv l nw nx">preds = f(X_t, params)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/48f39c0b66a681fd058c23f8077a497c.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*WnAJkDgjEPN9tzB9UKsmag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Pytorch 梯度下降-初步猜测。(图片由作者提供)</p></figure><p id="4e75" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们可以计算损失:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="be05" class="nb la it nq b gy nu nv l nw nx">loss = mse(preds, Y_t)</span></pre><p id="67b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以及 PyTorch 函数的梯度:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="0ea1" class="nb la it nq b gy nu nv l nw nx">loss.backward()</span></pre><p id="b8e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">之后，我们可以检查梯度:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="0771" class="nb la it nq b gy nu nv l nw nx">params.grad</span></pre><p id="63aa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它返回一个张量，即梯度:张量([433.6485，18.2594])</p><p id="d934" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们使用梯度和学习率更新参数:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="f533" class="nb la it nq b gy nu nv l nw nx">lr = 1e-4<br/>params.data -= lr * params.grad.data<br/>params.grad = <strong class="nq iu">None</strong></span></pre><p id="14cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并使用这些新参数预测 y:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2da7e5c2ff7c557c5b946fd36a6bbee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*0K8dvb5Tmvs-a7kXrohNCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Pytorch 梯度下降—第二纪元(图片由作者提供)</p></figure><p id="4340" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们需要多次重复这个过程，让我们制作一个函数:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="d1ff" class="nb la it nq b gy nu nv l nw nx"><strong class="nq iu">def</strong> apply_step(params):<br/>  preds = f(X_t, params)<br/>  loss = mse(preds, Y_t)<br/>  loss.backward()<br/>  params.data -= lr * params.grad.data<br/>  params.grad = None<br/>  return pred</span></pre><p id="3e68" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们可以跑几个纪元。看起来随着时间的推移，损失在减少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/f082d3814350bb9565a0d322f97acec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_tpoV7nMJy0ddB5xQ4MQBg.png"/></div></div></figure><p id="80be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">把它放在图表上</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/1b8637b5368c3532df54e8621e9f3c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2xN8bzN1GR3sD4PNcYdkxA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Pytorch 的梯度下降(图片由作者提供)</p></figure><p id="4d91" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就是这个！通过使用 PyTorch，我们可以轻松地计算梯度，并为机器和深度学习模型执行梯度下降。</p><p id="097c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">感谢阅读。</p></div></div>    
</body>
</html>