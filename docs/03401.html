<html>
<head>
<title>Understanding Reinforcement Learning through Multi-Armed Bandits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过多武装匪徒理解强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-reinforcement-learning-through-multi-armed-bandits-39095dee6846?source=collection_archive---------23-----------------------#2020-03-31">https://towardsdatascience.com/understanding-reinforcement-learning-through-multi-armed-bandits-39095dee6846?source=collection_archive---------23-----------------------#2020-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="4397" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">强化学习</h1><p id="70b6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">强化学习(RL)是机器学习的一个方向，源于观察人类如何与环境互动。</p><p id="174f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">为了更好的介绍RL，我们先把它放在人类学习的背景下来理解。当一个孩子进入一个新的环境(即充满玩具的房间)，他不知道环境中的目标是什么。相反，他执行基本上是随机的任务，在行动中学习周围环境的新事物。例如，他随机拿起一个尖锐的玩具，并意识到尖锐的玩具会引起疼痛。他选择了一个盒子，发现里面有一份快餐。通过这一集，孩子在学习。孩子知道尖锐的物体会引起疼痛，盒子里可能会有有用的物品，从而发展出一套规则来帮助指导他以后在这个环境中的决定。孩子通过环境的强化来学习。<br/> <br/>强化学习的特点是一种学习框架。学习目标是指导代理通过与环境交互来实现目标。<strong class="kn ir">强化学习与监督学习的不同之处在于从专家监督者那里获得的信息的指导性。</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/ee055b7233d464b499a0cd00e03fc184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5j7Qq26FqhBei7xU6CVrGA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图解。插图通过<a class="ae me" href="https://publicdomainvectors.org/en/free-clipart/Raised-hand-student/82848.html" rel="noopener ugc nofollow" target="_blank">公共领域矢量</a>和<a class="ae me" href="https://freesvg.org/cartoon-desktop-computer-vector-image" rel="noopener ugc nofollow" target="_blank">免费SVG </a>。</p></figure><p id="176d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在监督学习中，我们可以将使用(例如，标签)对的训练视为一个学习代理，向专家监督器提供给定训练示例的预测标签，专家监督器返回真实标签。专家的标签是<strong class="kn ir">完整的</strong>反馈，其中包含要采取的正确行动(真实标签)。</p><p id="19e0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">另一方面，强化学习中的环境oracle只提供<strong class="kn ir">不完整的</strong>反馈。RL代理不是接收正确的(真实的)动作，而是仅获得与他们选择的动作相对应的奖励。代理不知道是否存在更好的行动，并且必须依赖于过去奖励的一些历史来比较他们收到的奖励，以便获得所选行动是否有利的感觉。完整的信息只有通过与环境的反复互动才能获得。</p><p id="32ac" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，让我们定义一个强化学习系统的主要元素:政策、奖励函数、价值函数和环境模型。例如，我们将考虑的环境是一盘棋。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mf"><img src="../Images/28c47035a7f01e2ef87620b2195dca57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cmB6q5HINYG4kIQJKMFFuw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图解。照片由<a class="ae me" href="https://pixabay.com/users/stevepb-282134/" rel="noopener ugc nofollow" target="_blank"> stevepb </a>通过<a class="ae me" href="https://pixabay.com/photos/checkmate-chess-resignation-1511866/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄。经由<a class="ae me" href="https://publicdomainvectors.org/en/free-clipart/Chess-coloring-book/60712.html" rel="noopener ugc nofollow" target="_blank">公共领域载体</a>的说明。</p></figure><p id="ad21" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">代理和环境在序列中的离散时间步骤上相互作用。在每一个时间步，代理接收环境的状态的一些表示。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mg"><img src="../Images/0c2c1bfba86e0596362746e98a0fb5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMA0BTK_lw4jyDWbkXkVjA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">这是国际象棋环境的两种不同状态。这盘棋有<a class="ae me" href="https://en.wikipedia.org/wiki/Shannon_number" rel="noopener ugc nofollow" target="_blank"> 10个⁰ </a>不同的状态。截图来自<a class="ae me" href="https://www.chess.com/play/computer" rel="noopener ugc nofollow" target="_blank">棋牌在线</a>。</p></figure><p id="e2cb" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">基于环境的状态，代理从其可能的动作集中选择一个<strong class="kn ir">动作</strong>。一个时间步长后，环境会发生变化，以反映所选操作产生的下一个状态。随着环境的变化，代理人从环境中收到一个数字奖励<strong class="kn ir">。</strong></p><ol class=""><li id="e829" class="mh mi iq kn b ko lj ks lk kw mj la mk le ml li mm mn mo mp bi translated"><strong class="kn ir">代理</strong>是通过强化执行学习的系统的参与者。</li><li id="eb1d" class="mh mi iq kn b ko mq ks mr kw ms la mt le mu li mm mn mo mp bi translated"><strong class="kn ir">策略</strong>是学习代理的行为方式:它们用来在环境的不同状态下选择动作的一套规则。例如，国际象棋选手选择下哪一枚棋子的算法。</li><li id="30da" class="mh mi iq kn b ko mq ks mr kw ms la mt le mu li mm mn mo mp bi translated">奖励函数定义了环境中的目标。它将状态(或状态-动作对)映射到单个实数值。在国际象棋的例子中，当游戏进行时，所有状态的奖励可能是0，一旦游戏结束(将死)，如果代理赢了，奖励是1，如果他输了，奖励是-1。RL代理的目标是最大化长期回报。</li><li id="0aa7" class="mh mi iq kn b ko mq ks mr kw ms la mt le mu li mm mn mo mp bi translated"><strong class="kn ir">价值函数</strong>概括了每个行动对最大化长期回报目标的影响。价值函数将环境的状态映射到单个值——如果从该状态开始游戏，代理可以预期获得的奖励总额。价值函数有助于在博弈过程中指导代理人。在国际象棋中，一些走法(如危及你的国王)通常是不利的，即使它们不会导致输掉比赛，价值函数捕捉这些信息以向学习者提供比简单的奖励函数更多的信息，而奖励函数通常是不够的。</li><li id="0b06" class="mh mi iq kn b ko mq ks mr kw ms la mt le mu li mm mn mo mp bi translated">环境的<strong class="kn ir">模型是代理在与环境交互后对环境如何运行的预测。该模型将状态-动作对映射到状态。在国际象棋的例子中，环境模型可以简单地是选择一个动作移动棋盘上的选定棋子，并且当着陆位置在对手棋子的顶部时消除对手棋子。</strong></li></ol><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/e225b7ff55d1ef95350c5761e6cb235c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z4yMUzrqcWvXlKyYrUVbgQ.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">图片来自理查德·萨顿和安德鲁·巴尔托的强化学习[1]</p></figure><h1 id="ade0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">多股武装匪徒问题</h1><p id="6815" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这导致我们现在的多武装匪徒问题，最简单的强化学习问题的公式。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cce3e7c7dba35f0cc09e3afd3fe694ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*aFpjXFow8pY9ivdPYnYjPw.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">多臂土匪。(图片来源:<a class="ae me" href="http://research.microsoft.com/en-us/projects/bandits/" rel="noopener ugc nofollow" target="_blank">微软研究院</a>)</p></figure><p id="64ab" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">问题如下。考虑一个带<em class="mx"> n </em>操纵杆的老虎机。你会反复面对拉动<em class="mx"> n </em>控制杆的选择。每拉一次杠杆，你就会从基于你选择的杠杆的固定概率分布中得到一些奖励。你的目标是通过杠杆拉动的总次数来最大化你的长期累积回报。这就是<strong class="kn ir"> N臂土匪问题</strong>。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi my"><img src="../Images/e139203dd1057d221d13618cee986933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zt-Z4D2K4oXWeDL-eKBmnw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">强化学习框架中的多武装匪徒问题。形象的灵感来源于[1]。</p></figure><p id="156c" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">作为玩家，最好的解决方案是什么？如果你找到一个能提供积极回报的杠杆，那很好，但这是最好的杠杆吗？是否有其他更好的选择能带来更高的平均回报，寻找其他杠杆是否值得？或者你应该坚持你所知道的，拉你知道是体面的杠杆，但可能不会提供最高的回报？这些问题定义了期望-探索的权衡。</p><p id="3c92" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">期望-探索权衡</strong>是在重复决策中的两难问题，在这种情况下，对世界有不完全了解的代理人必须决定是重复以前行之有效的决策(利用)还是选择新的、未探索的决策，以期获得更大的回报(探索)。选择重复过去最有效的决策被称为<strong class="kn ir">贪婪</strong>行动。</p><p id="f766" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">解决期望-探索权衡问题的一种方法是在利用迄今为止的最佳行动和探索其他行动之间交替进行。这些近乎贪婪的方法被称为<strong class="kn ir"> ɛ-greedy方法</strong>。大多数时候，代理人的行为是贪婪的，但偶尔，用概率ɛ，我们在所有可能的行为中随机地、均匀地随机选择一个行为。一种变化是基于不确定性从分布中随机选择动作。因此，代理更有可能选择过去不常选择的动作。汤普森采样是另一种解决多武装匪徒问题的算法。这篇帖子的重点不是解决多股武装匪徒的问题，因此我推荐康纳·麦克唐纳的<a class="ae me" rel="noopener" target="_blank" href="/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50">帖子</a>来解决多股武装匪徒的问题。</p><p id="f004" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这篇文章通过强化学习的视角展示了多臂土匪框架。强化学习代理，如多臂强盗，在没有任务先验知识的情况下进行优化，使用来自环境的奖励来理解目标并更新其参数。</p><h2 id="4c31" class="mz jo iq bd jp na nb dn jt nc nd dp jx kw ne nf kb la ng nh kf le ni nj kj nk bi translated">参考</h2><p id="4282" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[1]理查德·萨顿和安德鲁·巴尔托。强化学习导论。麻省理工学院出版社，美国马萨诸塞州剑桥，第一版，1998年。</p><p id="477e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">[2]<a class="ae me" rel="noopener" target="_blank" href="/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50">https://towards data science . com/solving-multi armed-bottons-a-comparison-of-epsilon-greedy-and-Thompson-sampling-d 97167 ca9a 50</a></p></div></div>    
</body>
</html>