<html>
<head>
<title>The Vanishing/Exploding Gradient Problem in Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络中的消失/爆炸梯度问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11?source=collection_archive---------4-----------------------#2020-05-17">https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11?source=collection_archive---------4-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a2f8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解我们在构建深度神经网络时面临的障碍</h2></div><p id="bb39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在训练深度神经网络时面临的一个困难是梯度的消失或爆炸。在很长一段时间里，这个障碍是训练大型网络的主要障碍。然而，到本文结束时，你不仅会知道渐变消失和渐变爆炸的问题是什么，你还会知道当你面临这些问题时如何检测，以及一些可能的解决方案来调试你的模型。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/b28743c464a2bb83f6abc8b1c73cdfb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*--1is25v_kpFCvmX"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="https://unsplash.com/@jingdachen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">陈京达</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="8693" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">什么问题？</strong></p><p id="4221" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用基于梯度的学习和反向传播来训练深度神经网络时，我们通过从最终层(y_hat)到初始层遍历网络来找到偏导数。使用链式法则，网络中更深处的层要经历连续的矩阵乘法，以计算它们的导数。</p><p id="904e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在一个有<em class="mc"> n </em>个隐藏层的网络中，<em class="mc"> n </em>个导数会相乘在一起。如果导数很大，那么随着我们沿着模型向下传播，梯度将呈指数增加，直到它们最终爆炸，这就是我们所说的<strong class="kk iu"> <em class="mc">爆炸梯度</em> </strong>的问题。或者，如果导数很小，那么当我们通过模型传播时，梯度将呈指数下降，直到它最终消失，这就是<strong class="kk iu"> <em class="mc">消失梯度</em> </strong>问题。</p><p id="be44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在爆炸梯度的情况下，大导数的累积导致模型非常不稳定并且不能有效学习，模型权重的大变化产生非常不稳定的网络，在极端值时，权重变得如此之大以至于导致溢出，导致其权重值不能再被更新。<em class="mc"> </em>另一方面，小梯度的累积导致模型不能学习有意义的见解，因为倾向于从输入数据(X)学习核心特征的初始层的权重和偏差将不会被有效地更新。在最坏的情况下，梯度将为0，这又将停止网络，从而停止进一步的训练。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="b6ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">怎么知道？</strong></p><p id="9e95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mc">爆炸渐变</em></p><p id="1199" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有几个微妙的方法，你可以用来确定你的模型是否受到爆炸梯度的问题；</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi md"><img src="../Images/f22602e72176f50fba456062f2b36077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ry_DpGoVjuxE8Faq"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">Yosh Ginsu 在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><ul class=""><li id="7044" class="me mf it kk b kl km ko kp kr mg kv mh kz mi ld mj mk ml mm bi translated">该模型在训练数据上学习不多，因此导致了较差的损失。</li><li id="bf69" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">由于模型的不稳定性，该模型在每次更新时损失会有大的变化。</li><li id="4d50" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">在训练中，模特的损失将会很大。</li></ul><p id="c68d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当面对这些问题时，要确认问题是否是由于爆炸梯度引起的，有一些更明显的迹象，例如:</p><ul class=""><li id="7725" class="me mf it kk b kl km ko kp kr mg kv mh kz mi ld mj mk ml mm bi translated">模型权重呈指数增长，并且在训练模型时变得非常大。</li><li id="e090" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">模型权重在训练阶段变为NaN。</li><li id="f128" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">导数是不断的</li></ul><p id="9625" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mc">消失渐变</em></p><p id="95ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有一些方法可以检测出你的深层网络是否存在梯度消失的问题</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ms"><img src="../Images/a48be618c4c13c269f9ec4aa3060ce1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ReIkVrBUxQ4dHdkG"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@david_besh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">大卫·贝什</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><ul class=""><li id="b6c1" class="me mf it kk b kl km ko kp kr mg kv mh kz mi ld mj mk ml mm bi translated">在训练阶段，模型的改进非常缓慢，也有可能训练很早就停止了，这意味着任何进一步的训练都不会改进模型。</li><li id="899a" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">越靠近模型输出图层的权重变化越大，而越靠近输入图层的权重变化就越小(如果有变化的话)。</li><li id="c08c" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">训练模型时，模型权重呈指数级收缩，变得非常小。</li><li id="b4b6" class="me mf it kk b kl mn ko mo kr mp kv mq kz mr ld mj mk ml mm bi translated">模型权重在训练阶段变为0。</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="fc06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">解决方案</strong></p><p id="fe20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多解决爆炸和消失梯度的方法；本节列出了您可以使用的三种方法。</p><ol class=""><li id="2203" class="me mf it kk b kl km ko kp kr mg kv mh kz mi ld mt mk ml mm bi translated"><strong class="kk iu">减少层数</strong></li></ol><p id="e1f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是可以在两种情况下使用的解决方案(爆炸和消失渐变)。然而，通过减少网络中的层数，我们放弃了一些模型的复杂性，因为更多的层数使网络更有能力表示复杂的映射。</p><p id="5481" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。渐变剪辑(爆炸渐变)</strong></p><p id="cb58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">检查和限制梯度的大小，而我们的模型火车是另一个解决方案。深入研究这种技术的细节已经超出了本文的范围，但是你可以在一篇由Wanshun Wong撰写的标题为什么是<a class="ae lu" rel="noopener" target="_blank" href="/what-is-gradient-clipping-b8e815cdfb48">渐变裁剪</a>的文章中阅读更多关于渐变裁剪的内容。</p><p id="d723" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3。重量初始化</strong></p><p id="6697" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为您的网络选择更仔细的随机初始化往往是部分解决方案，因为它不能完全解决问题。查看詹姆斯·德林杰的这篇文章 - <a class="ae lu" rel="noopener" target="_blank" href="/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">神经网络中的权重初始化:从基础到明凯的旅程</a></p><p id="0fd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="8915" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们学习了什么是爆炸和消失渐变，如何检测它们和一些解决方案。我知道在这篇文章中，我没有详细介绍RNN结构，这种结构容易消失梯度，有用的资源，以了解更多将在下面链接。</p><p id="57cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">其他资源</strong></p><p id="b9f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="mu mv ep" href="https://medium.com/u/9ddaaec52a09?source=post_page-----191358470c11--------------------------------" rel="noopener" target="_blank">齐-汪锋</a> - <a class="ae lu" rel="noopener" target="_blank" href="/the-vanishing-gradient-problem-69bf08b15484">消失渐变问题</a></p><p id="5dbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="mu mv ep" href="https://medium.com/u/34fc912a20c6?source=post_page-----191358470c11--------------------------------" rel="noopener" target="_blank"> Eniola Alese </a> - <a class="ae lu" href="https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb" rel="noopener">消失的奇案&amp;爆炸渐变</a></p></div></div>    
</body>
</html>