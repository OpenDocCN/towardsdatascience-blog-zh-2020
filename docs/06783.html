<html>
<head>
<title>Eigenfaces — Face Classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征脸 Python 中的人脸分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eigenfaces-face-classification-in-python-7b8d2af3d3ea?source=collection_archive---------13-----------------------#2020-05-27">https://towardsdatascience.com/eigenfaces-face-classification-in-python-7b8d2af3d3ea?source=collection_archive---------13-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="77e6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习的数据不够？试试特征脸。</h2></div><p id="2a40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如今，我们可以使用神经网络来执行最先进的图像分类，或在这种情况下的人脸分类。但是采取一种更简单的方法怎么样呢？这就是本文的目的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/546ea4f00dee8fd273e0b1c8de4650d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4EO6SXl3HJoY2FRm"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@speckfechta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> x ) </a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="70a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">官方回购:</em> </strong> <em class="lv"> </em> <a class="ae lu" href="https://github.com/daradecic/Python-Eigenfaces" rel="noopener ugc nofollow" target="_blank"> <em class="lv">访问这里</em> </a> <em class="lv">获取数据和代码。</em></p><p id="6067" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初，将原始像素值作为输入特征的想法可能看起来很愚蠢——这很可能是真的，主要是因为我们会丢失所有的 2D 信息，而且还有卷积神经网络来提取重要的特征(因为不是所有的像素都相关)。</p><p id="c85c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天我们将介绍特征脸算法的思想——它只是一种应用于人脸识别问题的<strong class="kk iu">主成分分析</strong>。通过这样做，我们希望降低数据集的维度，只保留解释最大差异的成分，然后应用简单的分类算法(如 SVM)来完成分类任务。</p><p id="2927" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">听起来像是一个计划，但是在阅读这篇文章之前，你应该知道什么？  <em class="lv"> </em>这是个好问题。你要精通 Python 和它的数据分析库，也要知道什么是主成分分析，至少在高层是这样的。</p><p id="ade6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">还在读书？那我想你已经具备了先决条件。在开始编写代码之前，我们要讨论的最后一件事是文章结构，可以列举如下:</em></p><ul class=""><li id="5d8c" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mb mc md me bi translated">导入和数据集浏览</li><li id="6c43" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">图像可视化</li><li id="5aac" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">主成分分析</li><li id="6f74" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">模型培训和评估</li><li id="9cb8" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">结论</li></ul><p id="1dc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，事不宜迟，我们开始吧！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="ffef" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">导入和数据集浏览</h1><p id="5846" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">正如您可能已经预料到的那样，我们将需要常见的怀疑对象— <em class="lv"> Numpy </em>、<em class="lv"> Pandas </em>和<em class="lv"> Matplotlib </em>，但也将使用来自<em class="lv"> ScikitLearn </em>的一堆东西—像 SVM、PCA、train test split 和一些用于评估模型性能的指标。</p><p id="80b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是所有的进口:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="d601" class="nt ms it np b gy nu nv l nw nx">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt</span><span id="6744" class="nt ms it np b gy ny nv l nw nx">from sklearn.svm import SVC<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.decomposition import PCA<br/>from sklearn.metrics import confusion_matrix, classification_report</span><span id="0caa" class="nt ms it np b gy ny nv l nw nx">import warnings<br/>warnings.filterwarnings(‘ignore’)</span></pre><p id="bb7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">至于数据集，我们已经在 GitHub 上找到了，但现在似乎找不到了。可以从我的<a class="ae lu" href="https://github.com/daradecic/Python-Eigenfaces" rel="noopener ugc nofollow" target="_blank"> GitHub 页面</a>下载。</p><p id="b460" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是你如何把它装载到熊猫身上:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="96ed" class="nt ms it np b gy nu nv l nw nx">df = pd.read_csv(‘face_data.csv’)<br/>df.head()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nz"><img src="../Images/8a9f3996e08c98008f406f6b299439ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVeM8etOW1j9zyNyKsSfTQ.png"/></div></div></figure><p id="3aba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以快速检查数据集的形状:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="6027" class="nt ms it np b gy nu nv l nw nx">df.shape</span><span id="9973" class="nt ms it np b gy ny nv l nw nx"><strong class="np iu">&gt;&gt;&gt; (400, 4097)</strong></span></pre><p id="02e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，400 行和 4097 列，一个奇怪的组合。对于这些列，我们这里有<strong class="kk iu">归一化像素值</strong>(表示范围(0，1)内的值)，最后我们有一个<strong class="kk iu">目标</strong>列，指示照片上的人是谁。</p><p id="19d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们仔细看看目标列的唯一元素的数量，我们会得到数据集中的总人数:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="f72f" class="nt ms it np b gy nu nv l nw nx">df[‘target’].nunique()</span><span id="62ac" class="nt ms it np b gy ny nv l nw nx"><strong class="np iu">&gt;&gt;&gt; 40</strong></span></pre><p id="ce1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们有 4096 个功能，这是一个单一颜色通道中 64x64 图像的清晰指示器:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="7a3f" class="nt ms it np b gy nu nv l nw nx">64 * 64</span><span id="30b0" class="nt ms it np b gy ny nv l nw nx"><strong class="np iu">&gt;&gt;&gt; 4096</strong></span></pre><p id="e775" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了，我们现在有了关于数据集的一些基本信息，在下一节中，我们将进行一些可视化。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="80db" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">图像可视化</h1><p id="b198" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">为了可视化几张脸，我们将声明一个将 1D 向量转换为 2D 矩阵的函数，并使用<em class="lv"> Matplotlib 的</em> <strong class="kk iu"> imshow </strong>功能将其显示为灰度图像:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="d657" class="nt ms it np b gy nu nv l nw nx">def plot_faces(pixels):<br/>    fig, axes = plt.subplots(5, 5, figsize=(6, 6))<br/>    for i, ax in enumerate(axes.flat):<br/>        ax.imshow(np.array(pixels)[i].reshape(64, 64), cmap=’gray’)<br/>    plt.show()</span></pre><p id="856f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是在绘制之前，我们需要将特征从目标中分离出来，否则，我们的数据集将溢出 64x64 矩阵边界:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="bb63" class="nt ms it np b gy nu nv l nw nx">X = df.drop(‘target’, axis=1)<br/>y = df[‘target’]</span></pre><p id="1b1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就这样，现在我们可以使用声明的函数了:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oa"><img src="../Images/0662efb8febb43b7a76c3f59ad0273a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjsQKr3mUFcWMHs3eY59uw.png"/></div></div></figure><p id="804f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这部分就讲到这里。下一次，我们将执行训练测试分割和 PCA。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="ac5e" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">主成分分析</h1><p id="2e02" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">本节的目标是通过只保留那些解释最大差异的成分来减少问题的维度。简而言之，这是常设仲裁院的一个目标。但在此之前，我们必须将数据集分成训练和测试部分:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="9c84" class="nt ms it np b gy nu nv l nw nx">X_train, X_test, y_train, y_test = train_test_split(X, y)</span></pre><p id="cc8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以将主成分分析应用于训练特征。然后很容易画出解释方差的<strong class="kk iu">累积和</strong>，这样我们就可以近似得出多少个主成分就足够了:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="5a13" class="nt ms it np b gy nu nv l nw nx">pca = PCA().fit(X_train)</span><span id="f085" class="nt ms it np b gy ny nv l nw nx">plt.figure(figsize=(18, 7))<br/>plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ob"><img src="../Images/88a90e4af8cc1a90520d2ef349d01950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ev08531MIpUaPzMijF8t_Q.png"/></div></div></figure><p id="68c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅通过查看图表，看起来大约 100 个主成分将保持大约 95%的方差，但让我们验证一下这一说法:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="52f2" class="nt ms it np b gy nu nv l nw nx">np.where(pca.explained_variance_ratio_.cumsum() &gt; 0.95)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oc"><img src="../Images/2fde2ac11559ee8d5d733dc2b9364d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4UPZWBdYX6x4-T_pc3cv5A.png"/></div></div></figure><p id="207f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是的，看起来 105 个组件就够了。记住，95%不是一成不变的，你可以自由选择更低或更高的百分比。</p><p id="73fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再次执行 PCA，但这次使用了额外的<strong class="kk iu"> n_components </strong>参数:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="27fa" class="nt ms it np b gy nu nv l nw nx">pca = PCA(n_components=105).fit(X_train)</span></pre><p id="55f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们必须转变培训功能:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="797d" class="nt ms it np b gy nu nv l nw nx">X_train_pca = pca.transform(X_train)</span></pre><p id="7800" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！这一节就到这里，下一节我们将训练和评估 SVM 模型。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="a1f4" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">模型培训和评估</h1><p id="b3ee" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">到目前为止，培训功能已经发生了变化。训练模型的过程非常简单，只需制作一个实例并拟合训练数据即可:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="3057" class="nt ms it np b gy nu nv l nw nx">classifier = SVC().fit(X_train_pca, y_train)</span></pre><p id="5133" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">厉害！模型现在已经训练好了，为了在测试集上对它进行评估，我们首先需要将测试特征带到同一个<strong class="kk iu">特征空间</strong>。一旦完成，SVM 就被用来做预测:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="04e9" class="nt ms it np b gy nu nv l nw nx">X_test_pca = pca.transform(X_test)<br/>predictions = classifier.predict(X_test_pca)</span></pre><p id="5a93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们终于可以看到它的表现了。为此，我们将使用来自<em class="lv"> ScikitLearn </em>的<strong class="kk iu">分类 _ 报告</strong>，因为它比 40x40 混淆矩阵更容易查看:</p><pre class="lf lg lh li gt no np nq nr aw ns bi"><span id="f88e" class="nt ms it np b gy nu nv l nw nx">print(classification_report(y_test, predictions))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/22658b5d44f2ab6fed128ea9e827464e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*HBqNZ8N7ZspuAGjfJimGYQ.jpeg"/></div></figure><p id="087c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以大约 90%的准确率，对于 40 个不同的类和默认模型来说当然不可怕。</p><p id="bc47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于本文来说就是这样，让我们在下一节快速浏览一下可能的改进领域。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="bfe2" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="94bc" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">这是一个相当快速的指南——故意的。您可以自由地执行<strong class="kk iu">网格搜索</strong>来为分类器找到最佳超参数，或者甚至使用完全不同的算法。</p><p id="9e1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，尝试选择 90%和 99%的解释方差比率，以查看模型性能如何变化。</p><p id="0040" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读，欢迎在评论区留下您的想法。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="0e23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">喜欢这篇文章吗？成为</em> <a class="ae lu" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="lv">中等会员</em> </a> <em class="lv">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="oe of gp gr og oh"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">通过我的推荐链接加入 Medium-Dario rade ci</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">medium.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov lo oh"/></div></div></a></div></div></div>    
</body>
</html>