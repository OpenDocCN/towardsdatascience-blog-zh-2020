<html>
<head>
<title>Under the Hood — Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">幕后——逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4?source=collection_archive---------26-----------------------#2020-09-02">https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4?source=collection_archive---------26-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="76c4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解逻辑回归背后的数学原理，手动创建分类模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/168501d4dcbe6166804d7cc2e2da930c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKB_bBi3v8St8S2L3CHcTw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔尔·罗兰在<a class="ae ky" href="https://unsplash.com/s/photos/repair?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b07c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一系列文章中的第二篇，在这一系列文章中，我们将使用各种 ML 算法的基本数学方程来理解它们的“幕后”工作。</p><ol class=""><li id="3c94" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-under-the-hood-583003d0bf38">引擎盖下——线性回归</a></li><li id="d663" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/under-the-hood-logistic-regression-407c0276c0b4">幕后——逻辑回归</a></li><li id="42de" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/under-the-hood-decision-tree-454f8581684e">幕后——决策树</a></li></ol><p id="75de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有这么多优化的实现，我们有时太关注库和它提供的抽象，而太少关注进入模型的底层计算。理解这些计算往往是一个好模型和一个伟大模型的区别。</p><p id="da57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本系列中，我将重点放在手工实现算法上，以理解其背后的数学原理，这将有望帮助我们训练和部署更好的模型。</p><p id="c2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意——本系列假设您了解机器学习的基础知识以及我们为什么需要它。如果没有，请阅读本文来了解我们为什么以及如何利用 ML。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="f3b9" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">逻辑回归</h1><p id="3909" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">逻辑回归建立在<a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-under-the-hood-583003d0bf38">线性回归</a>的概念上，其中模型产生一个将输入特征(X)与目标变量(Y)相关联的线性方程。</p><p id="ccc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归算法的两个主要区别特征是—</p><ol class=""><li id="7cef" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">目标变量是离散值(0 或 1 ),与连续值不同，线性回归在计算线性方程的输出后增加了一个额外的步骤来获得离散值。</li><li id="d7b2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">该模型建立的方程侧重于分离 target 的各种离散值，试图确定一条线，使所有 1 都落在该线的一侧，所有 0 都落在另一侧。</li></ol><p id="27ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑以下数据，具有两个输入特征— <strong class="lb iu"> X1，X2 </strong>，和一个二进制(0/1)目标特征— <strong class="lb iu"> Y </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1e13181aa6e02558965db0936fa611f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AmccWdGlzidkprdJRfplYQ.png"/></div></div></figure><p id="d3a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归将试图找到参数<em class="no"> w1 </em>、<em class="no"> w2 </em>和<em class="no"> b </em>的最佳值，因此—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/9c4dac533f31ac702a05fccc5f3ac3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPXzn66AesHxR1QG97BXgQ.png"/></div></div></figure><p id="3685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，函数<em class="no"> H </em>，也称为激活函数，将<em class="no"> y </em>的连续输出值转换为离散值。这将确保等式能够输出类似于输入数据的 1 或 0。</p><p id="ea8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法通过以下步骤找到这些最佳值—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/b5601e5f42bb5979e5f133ae0d13d31f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HD8Va7zW9NM5I6zbnv18QA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">0.包含二进制(0，1)目标的样本数据</p></figure><ol class=""><li id="3ef8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将随机值分配给<em class="no"> w1 </em>、<em class="no"> w2 </em>和<em class="no"> b. </em></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/65ec9adc22f35465a8e6d66ab3f6c6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*81ZYjMi25DT_zO_lzOm_wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将随机值分配给 w1、w2 和 b</p></figure><p id="33ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.选取数据的一个实例，并计算连续输出(z)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/69a0fd954dc4852b555114ea7ffcad79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sPyrIgGpU5Oj4D6m-88cQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择一个实例并计算 z</p></figure><p id="f552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.使用激活功能<em class="no"> H()计算离散输出(ŷ)。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/d3b30b84767d1bd0825631e45e00d156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_gCYoQXLklXtV8w93vcLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用<em class="ns"> H() </em>计算ŷ</p></figure><p id="fcfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.计算损失——当实际目标是 1 时，我们的假设是否让我们接近 1？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/a71cb08235abd86eaca84c4505578ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_1G8Obj-jIGMC4TDCPQtA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算损失</p></figure><p id="5751" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.计算<em class="no"> w1 </em>、<em class="no"> w2 </em>和<em class="no"> b </em>的斜率——我们应该如何改变参数以更接近实际输出？</p><p id="2d41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.更新<em class="no"> w1 </em>、<em class="no"> w2 </em>和<em class="no"> b </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/ffac2ec5d035d6e8aeefa0f00b39c6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p54cqFRjHD8DOMFEW0Re3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算梯度并更新参数</p></figure><p id="967a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.重复步骤 2–6，直到收敛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/53e019fecc3c1d2b87ae38870a9cbc1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KADz7eoaNj7qp1OjQKko0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用另一个实例重复</p></figure><p id="720f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们详细看一下这些步骤。</p><h2 id="b3a6" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated"><strong class="ak"> 1。给<em class="ns"> w1 </em>、<em class="ns"> w2 </em>和 b </strong>分配随机值</h2><p id="6607" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们从给模型参数分配随机值开始—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7b5a608cff21f1dd7a1188e9ce137166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n_aNl3YRyBCTfSNaHlS1yA.png"/></div></div></figure><h2 id="6e03" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated">2.<strong class="ak">选取一个数据实例，计算连续输出(z) </strong></h2><p id="300b" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">让我们从数据的第一行开始——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/2c41f357368f2a22c6d5c037fa6dcc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7T5aGZ0N1hQeVm3_vbWyIQ.png"/></div></div></figure><p id="0259" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入我们假设的参数值，我们得到—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/1012e08ebc6d194aa30cdac3cd2ec516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21-CJ0nQClBoijDgwJI-zw.png"/></div></div></figure><h2 id="6456" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated">3.<strong class="ak">使用激活函数<em class="ns"> H() — </em> </strong>计算离散输出( ŷ <strong class="ak">)</strong></h2><p id="88fc" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">如果您一直在关注“幕后”系列，您会注意到步骤 1 和 2 与线性回归中的步骤完全相同。这一点会经常出现，因为线性回归形成了大多数算法的基础——从简单的 ML 算法到神经网络。</p><blockquote class="oh"><p id="e547" class="oi oj it bd ok ol om on oo op oq lu dk translated"><em class="ns">都是 Y = MX + c </em>供电</p></blockquote><p id="fe18" class="pw-post-body-paragraph kz la it lb b lc or ju le lf os jx lh li ot lk ll lm ou lo lp lq ov ls lt lu im bi translated">有了我们的基础，我们现在关注的是什么使逻辑回归不同于线性回归，这就是“激活函数”的作用。</p><p id="91e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">激活函数在具有连续目标值的线性世界和“逻辑”世界之间架起了一座桥梁(就像我们现在工作的这个世界！)具有离散的目标值。</p><p id="6687" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">激活函数的简单形式是阈值函数(也称为“阶跃函数”)—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/ab2ddad7ee8cf5617317049ce671034c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylGN2P5c80F2WIQxysiPDA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/f6c28e74c71d0ce13eec0eb5592bb536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4U3bD8iFXlqETxnnSJbqw.png"/></div></div></figure><p id="d969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，和所有 ML 的事情一样，<em class="no">“不可能那么简单”</em>。</p><p id="6480" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像这样的简单阈值函数的一个主要缺陷是，每次我们建立分类模型时，我们必须手动选择正确的阈值(基于输出的范围)。根据输入变量和权重，我们的值可以有任意范围。</p><p id="f889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个不利于这样一个简单函数的事实是，它在<em class="no"> z=threshold </em>处不可微。我们需要遵循损失的管道- &gt; <strong class="lb iu">梯度</strong> - &gt;权重更新，因此我们需要将这里的事情变得复杂一点，这样当我们计算损失的导数(梯度)时，我们的生活就简单了。</p><p id="f0a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是稍微修改过的阈值版本 Sigmoid 函数——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/6b9d3affa7fc594ffbb68fdfb61b302d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mpg-R_I2yILE7QgjeNzAHg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/9572a49fded65b04069afac1bb05813f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HfFW1VN_uw5CIH8XxRL7HA.png"/></div></div></figure><p id="ffb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像阈值处理一样，sigmoid 函数将实数转换为 0 和 1 之间的值，中点在 z=0 处(这样就解决了我们的第一个问题)。从图中可以明显看出，该函数在所有点上都是平滑的，这在计算梯度时将是有益的(从而解决了我们的第二个问题)。</p><p id="e336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sigmoid 函数的另一个优点是，它告诉我们估计值有多接近 0 或 1。这有助于我们更好地理解模型损失(错误)—对于实际值为 1 的行，预测值为 0.9 比预测值为 0.7 要好。当使用阶跃函数时，这种复杂性就消失了。</p><p id="949f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用 sigmoid 函数来计算我们的估计输出—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/c5326c06e9a8f93bd409f9d8ab65285e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQqC9VO58tDmfQU3dGnBdw.png"/></div></div></figure><h2 id="40f7" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated">4.<strong class="ak">计算损耗</strong></h2><p id="8970" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">基于线性回归，我们可以选择平方误差来表示我们的损失，但这总是会使误差变小，因为我们的值在 0-1 范围内。我们需要一个函数，当我们的假设提供一个接近于 0 的值，而实际是 1 时，输出一个大的损失，反之亦然。</p><p id="6006" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个这样的函数是 log-loss，它使用我们的预测输出(z)的对数变换来计算损失。</p><p id="06ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将损失函数定义为—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/fc200abd011b9451a609419a7f0241a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XEpKczJ4dwKbClbLwSKZiw.png"/></div></div></figure><p id="8f0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当输入接近 0 时，对数函数的输出接近负无穷大，当输入为 1 时，输出为 0。负号反转对数值，以确保我们的损失位于 0 到无穷大之间。</p><p id="01a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以把这个函数写成一个方程—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/1caae927d412c62a833a8fb0d2769c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rce-35oNavMxGlr-Xj3YLg.png"/></div></div></figure><p id="76a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以计算我们的假设给我们带来的误差—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/393d37b6245e970898cbed8cd156203d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5YFRTzUJG2KfyTXOoJLqw.png"/></div></div></figure><h2 id="fcbc" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated">5.<strong class="ak">计算<em class="ns"> w1 </em>、<em class="ns"> w2 </em>和<em class="ns">b</em>T13】的斜率</strong></h2><p id="a4ba" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在，我们通过计算每个参数对损耗的梯度来计算每个参数对预测输出(和损耗)的影响。</p><p id="7bf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是管道中的微分函数使我们的生活变得简单的地方，为我们提供了每个参数的导数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/ad1dcdd372491b3eae658ac6fc8f9b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UbMjgPPuoHWAi3SucExEA.png"/></div></div></figure><h2 id="c355" class="nt mr it bd ms nu nv dn mw nw nx dp na li ny nz nc lm oa ob ne lq oc od ng oe bi translated">6.<strong class="ak">更新<em class="ns"> w1 </em>、<em class="ns"> w2 </em>和<em class="ns">b</em>T21】</strong></h2><p id="4b3d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">梯度告诉我们，我们应该改变多少参数假设，以减少损失，并使我们的预测输出更接近实际输出。</p><p id="04e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们一次“训练”一个实例的模型，所以我们希望限制这个单独实例的损失对我们的参数的影响。因此，在更新参数之前，我们使用“学习率(<strong class="lb iu">η)</strong>”—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/c63795e2c8029f6b359fa88109f64bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0jYAxrFeCjQSh8jxTVtag.png"/></div></div></figure><p id="348e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这完成了训练的一个迭代。</p><p id="a8a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得最佳参数值，我们重复上述步骤固定次数，或者直到我们的损失停止减少<em class="no">，即</em>收敛。</p><p id="6249" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过另一个迭代，使用更新的参数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/0ec3dcb83e7b69f7b90913194323d6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqGHLZhDyK6AGDFDukWm8w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/564b713d84a0d22eedba583603fe13e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNYsaGF-ARYq-foE0gbAqg.png"/></div></div></figure><p id="0099" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看来这次我们的损失增加了。这显示了二进制实例如何将权重拉向相反的方向，以在两个类之间生成决策边界。</p><p id="9e44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复这些步骤，再进行几次迭代*，从前 4 行中随机采样数据，我们得到最佳权重如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/3a2fb94b8b867f03e44db6228a4bf1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUmiD6IvS-tPLtT4P5msSg.png"/></div></div></figure><p id="8895" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对我们最后一行数据(我们在训练时没有使用)使用这些值，我们得到—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/31c08a2adbc37f9b029ab6c3cb22daa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3McCqyUxXWsJI1UrlndNw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/9c78b070affab19123e234f59b18737b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5Su6MPKlD7fR2VhWqdMew.png"/></div></div></figure><p id="80de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这非常接近我们的实际类“1”。</p><p id="8c85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择截止值为 0.5(我们的 sigmoid 曲线的中点)，对于这种情况，我们的预测值为 1。</p><p id="ccb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。在其核心，这是所有的逻辑回归算法。</p><h1 id="b24f" class="mq mr it bd ms mt oz mv mw mx pa mz na jz pb ka nc kc pc kd ne kf pd kg ng nh bi translated"><strong class="ak">这真的是逻辑回归所做的“全部”吗？</strong></h1><p id="3b73" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">“引擎盖下”是本系列的焦点，我们看了看逻辑回归的基础，一次取一个样本，并更新我们的参数以适应数据。</p><p id="dc9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这确实是逻辑回归的核心，但是一个好的逻辑回归模型还有很多其他的东西，比如</p><p id="e12d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.正规化——L1 和 L2</p><p id="7b59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.学习率调度</p><p id="ec66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.为什么选择乙状结肠活化？</p><p id="b840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.缩放和标准化变量</p><p id="8c3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.多类分类</p><p id="13ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在一个平行系列中介绍这些概念，重点是我们在这个系列中介绍的不同 ML 算法的各种复杂性。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="d82f" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated"><strong class="ak">下一步是什么？</strong></h1><p id="6a76" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在本系列的下一篇文章中，我们将继续分类任务，并了解另一类算法— <a class="ae ky" rel="noopener" target="_blank" href="/under-the-hood-decision-tree-454f8581684e"> <strong class="lb iu">决策树</strong> </a> <strong class="lb iu">，</strong>，它们的工作方式非常类似于您和我如何使用推理来得出关于数据的结论。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="baa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">*经过 5 次迭代，这是我们的损失、梯度和参数值的变化方式— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/0540a58681c07a6e2239fca4890eaf9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkwtHiamrCV9t1CwdEYQfw.png"/></div></div></figure><p id="5f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">** <em class="no">在本文中，我还没有介绍我们如何获得损失函数相对于参数的导数，因为推导过程非常广泛，需要单独撰写一篇文章(将在另一篇文章中介绍:)</em></p></div></div>    
</body>
</html>