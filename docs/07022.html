<html>
<head>
<title>Latent Variables &amp; Expectation Maximization Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在变量&amp;期望最大化算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c?source=collection_archive---------7-----------------------#2020-05-30">https://towardsdatascience.com/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c?source=collection_archive---------7-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d70c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习的贝叶斯方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d7a83033806907aa82ba523e60557a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8M296Vhd1JGgE0fKKDQzA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜伏的‘烟雾’:(图片来源:作者)</p></figure><p id="2aaa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“潜伏”一词源于拉丁语，意思是隐藏起来。大概你们都知道潜热，是相变保持温度不变所需的热能。因此，我们观察到一种变化，但其背后的原因显然是隐藏的。潜在变量模型(LVM)的动机是用一些潜在的隐藏变量来解释数据的表层结构。在这篇文章中，我们将通过一个例子来理解LVMs和非常著名的处理这类模型的算法，即期望最大化(EM)算法。我个人认为这个主题是贝叶斯机器学习的核心之一，它将涉及大量的数学知识。但是，相信我，会很容易的。此外，了解EM算法的核心概念将有助于您真正理解变分自动编码器和最终GAN的基础。你可以从这篇文章中学到什么:</p><ol class=""><li id="aa25" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">为什么是潜变量模型(LVM)？</li><li id="63a9" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">高斯混合模型(GMM)。</li><li id="2841" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">期望值最大化算法。</li><li id="0aee" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">变分推理和EM算法。</li><li id="3c05" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">用EM算法训练GMM的例子。</li></ol></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="fcd5" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">潜在变量和LVM:</h2><p id="ccb9" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">为什么是隐藏变量？潜在变量可能是一些理论概念，或者是无法观察到的真实物理变量。我们用几个现实的例子来阐述吧。在文本文档中，提取的“单词”可以被视为特征。通过分解这些特征，我们可以找到文档的“主题”。因此，观察到的特征(“单词”——树枝、根、叶子、花、果实)会分解为潜在特征(“主题”——树、花园)。每当我们有大规模、高维度的噪声特征时，在潜在特征上建立分类器是有意义的。在对象识别任务的情况下，每个数据点(像素强度矩阵)对应于一个对象的图像。在这种情况下，潜在变量可以解释为对象的位置方向。正如克里斯·毕晓普在他的<a class="ae nn" href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738" rel="noopener ugc nofollow" target="_blank">模式识别书</a>中提到的—</p><blockquote class="no np nq"><p id="9bce" class="ky kz nr la b lb lc ju ld le lf jx lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">潜在变量的主要作用是允许观察到的变量上的复杂分布根据从较简单的(通常是指数族)条件分布构建的模型来表示。</p></blockquote><p id="c0fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通常一个LVM <em class="nr"> p </em>，是2组变量<em class="nr"> x，z </em>的概率分布；<em class="nr"> p(x，z)。x </em>是数据集D中学习时的观察变量，而<em class="nr"> z </em>从未观察到。模型的联合概率分布可以写成<em class="nr"> p(x，z) = p(x|z) p(z) </em>。</p><p id="94ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您将会看到，开发LVMs的拟合算法相当困难，但是研究这一点并没有什么好处</p><ol class=""><li id="4706" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">LVM通常比原始模型具有更少的参数，这些参数直接表示可见空间中的相关性。</li><li id="4003" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">LVMs中的LV主要充当数据的压缩表示，这也称为“瓶颈”。这是无监督学习的基础。如果你想到一个形象(例如。人脸)作为观察变量<em class="nr"> x </em>那么，潜在变量<em class="nr"> z </em>可以编码人脸的特征(在训练期间看不到)，就像它可以编码人脸是快乐还是悲伤，男性还是女性等等。</li><li id="00dc" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">LVMs还可以帮助我们处理缺失数据。前一点和这一点是相关的。如果我们可以对潜在变量进行后验推断，即给定一幅图像<em class="nr"> x </em>潜在因子<em class="nr"> z </em>是什么，那么一旦我们发现某些图像有缺失部分，潜在因子就可以用于重建。</li></ol><p id="8e96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了讨论和发展一个现实的LVM，我们将根据<em class="nr">离散的</em>潜变量来公式化高斯混合模型(GMM)。由于获取LVMs中最大似然估计量的最通用技术是EM算法，我们将在接下来的章节中详细讨论。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="e425" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">概率聚类和GMM；</h2><p id="c84d" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">在我之前的<a class="ae nn" rel="noopener" target="_blank" href="/dbscan-algorithm-complete-guide-and-application-with-python-scikit-learn-d690cbae4c5d"> DBSCAN算法文章</a>中，我已经讨论了K-Means算法的缺点，特别是在处理不同密度、大小和数据点的空间聚类时，包括噪声和异常值。K-Means的另一个问题是它执行硬聚类。让我们看一个使用简单的<code class="fe nv nw nx ny b"><a class="ae nn" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html" rel="noopener ugc nofollow" target="_blank">make_blobs</a></code>数据集的例子。在下面的图1中，我们看到了包含3个带标签和不带标签的聚类的数据集。一旦我们应用K-Means算法，它将识别3个聚类，但是不可能将任何概率分配给聚类边界处的数据点。如果我们想知道每个数据点属于这三个聚类的概率或可能性(对于聚类边界上的点尤其重要)，该怎么办？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/efafed841deef79848d12f1d554f7459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZyIWfjVGkNYNVHvG1ACRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:创建Blobs数据集(左)和包含标签的相同数据集(右)。来源:作者</p></figure><p id="2bc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在图2中，我们展示了一个将高斯混合模型(GMM)聚类应用于数据集的示例，每个数据点的大小与GMM预测的确定性成比例。这与K-Means结果进行比较，您可以清楚地看到差异，尤其是对于边界处的点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/0709baa78435a7f53f7428e98de162b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwSzYCIPfRyuMI869IMU2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:使用GMM的软聚类(右)的比较通过给每个数据点分配预测的确定性来描述。这与硬聚类-K-Means(左)相比较。来源:作者</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="7d05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将这种软聚类作为一种动机，让我们使用潜在变量建立GMM，但在此之前，让我们回忆一下多元正态分布。这里使用的符号很常见。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/abb959a956a435e541119c2d9befa851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeKziSCH4G7utR3EigKPEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验一:多元正态分布。</p></figure><p id="e444" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用混合正态分布来开发一个概率聚类(软聚类)模型。认为GMM是高斯分量的线性叠加的天真想法。每个分量将具有与其相关联的某种概率来表示特定的数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/d504ed35c2ed81aad84e477bf4643403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wU2Hixf_38jBOMYTMgODhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验2:高斯分布的混合。</p></figure><p id="4b34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">π在Exp中称为混合系数。2.对于单个数据点，总和超过聚类数<em class="nr"> K </em>。变量<em class="nr"> x </em>和<em class="nr"> z </em>(潜伏)的联合概率分布为:<em class="nr"> p(x，z) </em> = <em class="nr"> p(x|z) p(z) </em>。因此数据点<em class="nr"> x </em>上的边际分布将由下式给出:<em class="nr"> p(x) = ∑ p(x|z) p(z) dz </em>(通过对变量z求和/积分获得)。考虑这个<em class="nr"> p(x) </em>的一种方式是混合分布<em class="nr"> p(x|z) </em>，用<em class="nr"> p(z) </em>加权。对于GMM上下文，我们可以把<em class="nr"> z </em>看作是一个二进制随机变量，具有1的<em class="nr"> K </em>表示(你可以把这看作是分类表示)，其中一个特定元素等于1，其他所有元素都是0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/621a595b225077692368915d43a67689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJBxYK0FgXAAahpnueDvvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验3:潜在变量的边际分布。</p></figure><p id="8a99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，混合系数和潜在变量<em class="nr"> z </em>，当我们考虑z上的边际分布是按照上面表达式中定义的混合系数来指定时，都落入一个位置。混合系数所满足的条件是必要的，以便它们可以有效地解释为概率。我们认为第k个混合系数(π_k)是z_k等于1的先验概率。由于潜在变量使用1/K的表示法(把这看作是分类表示法)，我们可以用另一种方式写出p(z)和相应的条件分布如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ef71a169207dbe092d5e513793dbc99c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*iCtti9c4iGa1kD17aBqRFw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验4:边际和条件分布</p></figure><p id="e3b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，结合多元正态分布和潜在变量分布，我们可以重写GMM模型如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/78f6d912cb203380d2210692617123ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*rqHtZg5Dn-z16DHLEaSrwA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验5:混合系数和多元正态分布的GMM。</p></figure><p id="7a7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述表达式对单个数据点有效。对于几个数据点，我们将有<em class="nr">一个对应的潜在变量与每个观察到的数据点相关联。我们现在可以处理联合分布p(x，z)，而不是只处理边际分布。我们让事情变得更复杂了吗？为了回答这个问题，我们必须在可能性估计的数学中稍微深入一点。下一节将揭示潜在变量将如何帮助我们解决最大似然估计(MLE)容易。</em></p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="8f12" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">期望值最大化算法(动机):</h2><p id="6414" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">我们对GMM的主要刺激是进行数据点的软聚类。获得每个聚类的最佳参数(混合系数、聚类均值、协变矩阵)的一般直觉是执行通常的MLE任务。在上面的表达式4中，我们已经为单个数据点写了GMM模型。让我们考虑一个具有N个点的数据集，这样该观察集将是{x1，x2，x3，…，xN}。对于N个数据点，似然函数如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/6c5925b3559e89c52775770a9e778866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qY4Leku9p10m5AaV_D5O2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验6:对数似然函数包括所有N个数据点。</p></figure><p id="1256" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个表达式的最大问题是对数并不直接作用于高斯，而是作用于高斯混合的和。因此，相对于对数对指数的作用，我们没有优势。所以确实这个表情很难还原。意识到这不是单个高斯函数的问题。除了前面的问题，另一个可能出现的问题是当GMM的一个组成部分只解释了一个点。这意味着其中一个μ等于数据点x_n，在方差趋于0的极限内，似然项的行为类似于δ函数。所以最大化可能性在这个问题中不是一个好主意。与单个高斯分布问题相比，考虑高斯混合问题的另一种方式是，虽然单个高斯模型属于具有凹对数似然(对于足够简单的族来说是易处理的)的<a class="ae nn" href="http://www.stat.cmu.edu/~siva/705/lec13.pdf" rel="noopener ugc nofollow" target="_blank">指数族，但是分量的混合不会具有这种性质。因此，所有这些原因都需要一种新的方法来确定参数，并且是时候深入研究期望值最大化(EM)算法了，该算法确实是基于贝叶斯更新规则的。</a></p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="e0fb" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">EM算法:</h2><p id="20d0" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">在学习EM算法之前，我们需要先推导一些重要的表达式。首先，我们将利用贝叶斯定理从条件分布<em class="nr"> p(x|z) </em>中得到给定<em class="nr"> x </em>的<em class="nr"> z </em>的后验分布表达式，如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/dcd8704f2c28fd04630748c4a3fc4e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*Dx0Dl-8dfd4XbMnEdH25ZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验7:潜在变量的后验分布</p></figure><p id="1f78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nr"> γ </em>项被称为责任，它代表潜在变量<em class="nr"> z </em>的第k个分量为解释观察值<em class="nr"> x </em>所承担的责任。顾名思义，EM算法依赖于两个简单的步骤:期望(E步)和最大化(M步)</p><p id="6e15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">a)。E-step:期望步骤是我们计算后验分布的地方，即基于当前参数的责任(exp: 5)。这更多的是解决推断问题，即哪个高斯负责哪个数据点？</p><p id="067f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b)。M-Step:从E-step获得的概率然后用于计算更新的参数(μ、σ和π)。</p><p id="18cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在几次迭代之后，该过程将收敛，并且我们获得参数的最佳拟合值。让我们更一般地回顾一下EM算法的步骤——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/c8a522250531a4fa4aa20bfe3dfed884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NOTHty_FmKqQZwPAIkrRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验8:EM算法的一般步骤</p></figure><p id="d11e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，重要的是要提到，一旦我们知道了集群分配，就可以获得最佳参数(这是我们在E-step上尝试的一部分)。还要意识到，由于从E-step中获得了后验概率，我们可以使用期望的定义，然后我们需要最大化完整数据可能性的<strong class="la iu">期望</strong>而不是数据可能性。我们如何进入这种期望的表达，在接下来的部分将会更加清楚，但是现在记住在<em class="nr">责任</em>(在实验中定义)方面。7) <a class="ae nn" href="http://cs229.stanford.edu/notes/cs229-notes8.pdf" rel="noopener ugc nofollow" target="_blank">我们可以计算出</a> [1]参数(μ，σ，π)，表达式如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/0e70f38f91a1eaf5241535ac8302f43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BxMyKiKDlyVxAMZETndWA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验8:从MLE获得的参数。</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="c5d3" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">变分下界和EM算法；</h2><p id="6de4" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">这是刚才提到的，直到收敛重复EM算法的步骤。你可能会想怎么保证算法收敛？我们来深入挖掘一下！我们从GMM开始说，对于每个数据点<em class="nr"> x_n </em>都有一个潜在变量<em class="nr"> z_n </em>。完整数据集的对数似然采用如下形式—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/9e15114980060f848a9f42e02f227e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msqoRvQUEYC5yrkKGK-TFQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验9:包含潜在变量的完全可能性</p></figure><p id="fc37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如前所述，这个表达式的问题是对数是作用于高斯混合的和。我们将耍些小花招来解决这个问题。让我们在潜在变量上引入任意概率分布(没有任何特殊性质)— <em class="nr"> q(z) </em>现在我们可以修改上面的表达式——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2ad30c2b0497a17faec154d7b09b24ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*UrNMiA6BdNRpB1bw2WFTOA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验10:数据似然性的微小变化</p></figure><p id="0def" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用期望值的定义，这个表达式可以进一步简化为—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c03d5fba111160ea6eebad009a9f1e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwjV4JLPZS8HI-_UcZ37wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验11:使用期望的定义</p></figure><p id="924c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">期望项可以理解为从分布<em class="nr"> q_i </em>中提取的<em class="nr"> z_i </em>的期望值。我们可以将这个表达式与Exp进行比较。为了理解对潜在变量求和是如何给出期望值的。在Exp中。11、日志正在对期望值进行操作。鉴于<strong class="la iu"> Log是一个凹函数，</strong>让我们应用<a class="ae nn" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">詹森不等式</strong> </a>来进一步简化表达式——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/68d384e9d4d85d5c23e15fd9352d8f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QoMJABVklG1kyfOe8PeAJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验12:用詹森不等式修改实验11。</p></figure><p id="c3b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的表达式表示完整数据集的对数似然大于或等于上面的期望值。期望项被称为<strong class="la iu">证据下限(ELBO) </strong>，通过对数展开和应用贝叶斯定理，我们可以得出更有意义的结论</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/c6d7224114c7fcae0d1a00e418129c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QMD-2XrcDp9EnMZtmZpOA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验13:证据下限</p></figure><p id="1c83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用<a class="ae nn" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL散度</a>的定义，上述表达式变成—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c907afbd97899924ad2a2c18f6143e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*-SB6Ih57W-pOc1T9d0yuOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实验14: ELBO和KL背离</p></figure><p id="91bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当KL散度≥ 0时，这意味着ELBO ≤ <em class="nr"> log p(x) </em>。现在我们知道它为什么被称为下界了。我们的目标是最大化我们的数据可能性<em class="nr"> log p(X) </em>，相反，我们最大化ELBO。基于这个表达式(Exp。14)让我们再回顾一遍EM算法的步骤—</p><ol class=""><li id="fa75" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">通过随机初始化参数来启动算法。</li><li id="4eca" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">e步骤:最大化ELBO w.r.t <em class="nr"> q(z) </em>并保持参数<em class="nr"> (π，μ，σ)</em>不变。由于<em class="nr"> log p(X) </em>独立于<em class="nr">q(z)</em>这个步骤本质上只是计算KL散度，并且在GMM的情况下<em class="nr"> q(z) </em>被设置为等于后验概率。这是可能的，因为<em class="nr">在GMM </em>中可以计算每个数据点的后验概率。</li><li id="b239" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">m步:ELBO相对于参数最大化，而<em class="nr"> q(z) </em>保持固定，这与MLE估计非常相似。在这个步骤中，参数被更新。</li></ol><p id="f3da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">为什么趋同:</strong></p><p id="2e80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们以一个问题开始这一节——为什么EM算法能保证收敛？EM算法是一个迭代过程，因此E和M步骤循环进行。重要的是要记住，在EM算法的每一步中，首先将分布<em class="nr"> q(z) </em>设置为等于后验<em class="nr"> p(z|x) </em> (E步)，并且从最大化开始在M步中更新参数。随着更新的参数，下限增加(除非算法已经达到局部最大值)。这又增加了对数似然性，但是log <em class="nr"> p(x) </em>比下限增加得更多，因为虽然<em class="nr"> q(z) </em>是针对旧参数获得的，但是现在在E步骤中，当我们再次计算后验<em class="nr"> p(z|x) </em>时，并且这次使用来自M步骤的新参数，产生非零KL发散项。因此，下限以及数据似然性<em class="nr">单调增加</em>。为了确保算法不会陷入局部最大值，我们对不同的参数初始值运行算法。数据似然性的这种单调递增的性质正是我们想要展示的收敛的代理。如果我们看到可能性在每一步都没有增加，这有时对调试代码非常有帮助。为了运行该算法，我们将给出迭代次数作为参数，并且还为损失函数的分数增加设置容差。现在，是时候将所有的理论付诸实践了，让我们来编码吧！！请查看笔记本，了解详细的代码以及参考资料部分的其他参考资料。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="7745" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">用Python实现EM算法；</h2><p id="f7d1" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">让我们从定义E-step开始。这里，我们的主要目标是定义后验分布，在我们把它定义为Exp中的责任之前。7.下面是代码块—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/bc94c3c6a9246af4a8d4d6502c550ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oVRp1DfgKXSEjpxHp6wqsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">EM算法的e步(适用于GMM)</p></figure><p id="8e18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们实施M步，这是关于最大化的期望，对于GMM，我们已经知道最好的参数，一旦我们从E步得到后验。最大似然估计的参数在实验中给出。8，我们将使用它们来定义M-step，如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/34ac8572d9dfdaa360e61175402602c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2A2QaulYuNbRpQMbm27jA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM的M-step</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/fa92090692ae0cc5f6a5c8adf377449b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YavnGo5RotSg4qo-078yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Exp。15: ELBO的可能性和后验概率</p></figure><p id="7074" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们定义了E和M步骤，现在我们将实现证据下限作为损失函数。我们从Exp修改下界表达式。12至如实验所示。15.对于潜在变量的分布<em class="nr"> q(z)，</em>我们将使用从E-step计算出来的后验。考虑到这些，让我们将ELBO定义如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a3f488eb03e3b824cba5c4f8456f44c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzfvXtonrF7HIS2GK-XqRQ.png"/></div></div></figure><p id="71c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们已经准备好定义训练循环，我们设置迭代次数和容差，并确保算法以随机初始化的参数启动几次，以免陷入局部最大值。在训练期间，我们需要指定集群的数量，对于复杂的问题，这可能是一个问题。由于EM算法很耗时，通常建议运行K-Means算法来确定聚类数，然后对GMM运行EM。另外，确定这一点的另一种方式是使用<a class="ae nn" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"> Akaike信息标准</a>。在我的博士研究中，我确实用AIC来分离天体物理模型，但这与我们目前的目的不同。训练循环的代码如下—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="ab59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，在训练之后，我们可以画出轮廓—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/2771841c37aa2b1a871312d919eb194b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*neQPTgTJ24ZMBzo9GyDzvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图:左:2D数据集，聚类成3个高斯混合(右)。来源:作者</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="f2be" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">使用Scikit-Learn:</h2><p id="d953" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">使用sklearn的<a class="ae nn" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html" rel="noopener ugc nofollow" target="_blank"> GaussianMixture </a>类，用3行代码就可以实现GMM EM算法的所有这些理解和相应的长代码。但是当你知道基本知识的时候，生活总是更好！下面是代码块—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/fb24facd27f084d0f91d6049ae28735c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEyzAIZKroG8A3u0yJQdxw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用scikit-learn实现GMM很容易！</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="092f" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">结论:</h2><p id="2d3d" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">我们已经完成了这项工作，在这个过程中，我们学习了贝叶斯方法在无监督学习中的几个重要概念。在许多重要的概念中，我希望你们至少能回忆起最重要的一些——</p><ol class=""><li id="8fc2" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">通过EM算法的迭代步骤更新基于先验知识的概率分布。这是贝叶斯推断的基础——一旦我们更新了数据(证据)，我们的信念会如何改变。</li><li id="699a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">为什么引入潜在变量有助于我们更好地理解数据，并最终帮助计算最大似然？</li><li id="57dd" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">KL散度和ELBO通过数据的可能性的关系。这一点你会在生成模型的概念中反复遇到(GMM是生成模型的一个简单例子)。</li><li id="70f4" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">对于GMM，单个数据点的后验概率可以通过分析计算得出，但在复杂模型中可能不是这种情况(大多数情况下后验概率是难以处理的)，因此，它不能作为潜在变量分布的代理(<em class="nr"> q(z) = p(z|x) </em>)。在这种情况下，我们试图从尽可能接近后验分布的分布族<em class="nr"> Q </em>中选择<em class="nr"> q </em>。这是变分贝叶斯和最终变分自动编码器(VAE)和生成对抗网络(GAN)的基础。</li></ol><p id="9736" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读，希望这能对你有所帮助。干杯！！！</p><p id="5709" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">页（page的缩写）s:我在另一篇文章中讨论了贝叶斯机器学习中的另一个重要概念，称为<a class="ae nn" rel="noopener" target="_blank" href="/understanding-conjugate-priors-21b2824cddae">共轭先验</a>。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="560e" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated"><em class="ow">参考文献:</em></h2><p id="7f83" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">[1] <a class="ae nn" href="http://cs229.stanford.edu/notes/cs229-notes8.pdf" rel="noopener ugc nofollow" target="_blank"> EM算法:</a> Andrew NG笔记来自斯坦福知识库。</p><p id="ff67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] <a class="ae nn" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">变分推论:</a>大卫·布雷的评论。</p><p id="3902" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]克里斯·毕晓普的<a class="ae nn" href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738" rel="noopener ugc nofollow" target="_blank">模式识别</a>书:第9章。</p><p id="54ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]笔记本链接:<a class="ae nn" href="https://github.com/suvoooo/Machine_Learning/blob/master/ExMax_ALgo/LVM.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p></div></div>    
</body>
</html>