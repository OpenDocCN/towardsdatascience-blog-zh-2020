<html>
<head>
<title>K-Means Clustering — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类—已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-explained-4528df86a120?source=collection_archive---------3-----------------------#2020-03-03">https://towardsdatascience.com/k-means-clustering-explained-4528df86a120?source=collection_archive---------3-----------------------#2020-03-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a027" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细的理论解释和 scikit-learn 实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38946ce451cdce190f1fe08d3c102c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jg90VC_It42Cq0KZO6JQ3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://aeroleads.com/blog/10-best-practices-customer-segmentation/" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><p id="3b17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们开始做生意，向人们出售一些服务。业务进展顺利，几个月内我们的数据库中就有了大约一万名客户。我们希望留住我们的客户，同时增加每位客户的收入。所以我们计划向数据库中的客户提供一笔交易。我们有两个选择:</p><ul class=""><li id="6a41" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">向所有客户提供相同的交易</li><li id="7b59" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">准备客户特定的交易</li></ul><p id="e8ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一种选择很简单。“所有服务打九折”的交易就可以了。然而，与针对客户的交易相比，这种方式效率较低，利润也较低。此外，针对客户的交易可能对客户更有吸引力。一些顾客喜欢某一件商品打折，而另一些顾客喜欢买一送一的交易。一些客户在周末购买服务 A，而另一些客户在周一上午购买。根据企业规模和客户数量，我们可以列出更多不同的选项。</p><p id="61d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们决定准备客户特定的交易。下一步是决定提供什么样的交易。我们不能只为每个客户创建不同的交易。这根本无法管理。一个明智的选择可能是发现具有相似兴趣或购买行为的客户，并将他们分组。分组的标准可以是客户偏好、品味、兴趣、客户服务组合等等。假设客户数据库中有每个客户的以下信息:</p><ul class=""><li id="c349" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">客户年龄、地址</li><li id="2624" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">平均采购金额</li><li id="8c6d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">平均购买数量</li><li id="0afb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">购买频率</li><li id="1eee" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">购买的时间和类型</li></ul><p id="d653" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个列表很容易扩展。手动对客户进行分组极其困难。然后我们向<strong class="lb iu">机器学习</strong>寻求帮助。将相似客户分组的任务称为<strong class="lb iu">聚类</strong>。维基百科上更正式的定义:</p><blockquote class="mj"><p id="d3ca" class="mk ml it bd mm mn mo mp mq mr ms lu dk translated"><strong class="ak">聚类分析</strong>或<strong class="ak">聚类</strong>是对一组对象进行分组的任务，使同一组中的对象(称为<strong class="ak">聚类</strong>)比其他组(聚类)中的对象彼此更相似(在某种意义上)。</p></blockquote><p id="1114" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">在我们的例子中，对象是客户。</p><p id="5736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类是无监督的，这意味着没有样本(或数据点)的标签。聚类在很多行业都有应用。以下是集群的一些示例:</p><ul class=""><li id="3a95" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">客户细分</li><li id="db28" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">图象分割法</li><li id="eb9f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">图像/颜色压缩</li><li id="4a70" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">检测异常或异常值</li></ul><p id="c00f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多不同的聚类算法。在这篇文章中，我将介绍一种最常见的聚类算法:<strong class="lb iu"> K-Means 聚类。</strong></p><h1 id="5915" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">聚类 vs 分类</strong></h1><p id="5da2" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在开始讨论 k-means 聚类之前，我想指出聚类和分类之间的区别。</p><ul class=""><li id="4631" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">分类任务中的样本有标签。每个数据点根据一些测量结果进行分类。分类算法试图对样本的测量值(特征)和它们的指定类别之间的关系进行建模。然后模型预测新样本的类别。</li><li id="f413" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">聚类分析中的样本没有标签。我们希望该模型能够在数据集中找到结构，以便相似的样本能够被分组到聚类中。我们基本上是让模特给样品贴标签。</li></ul><h1 id="11c7" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">K-均值聚类</strong></h1><p id="ffa8" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">k-均值聚类旨在将数据划分为 k 个聚类，使得同一聚类中的数据点相似，而不同聚类中的数据点相距较远。</p><blockquote class="nv nw nx"><p id="fbd6" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">两点的相似性由它们之间的距离决定。</p></blockquote><p id="51ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测量距离的方法有很多。<a class="ae ky" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧氏距离</a>(p = 2 的闵可夫斯基距离)是最常用的距离度量之一。下图显示了如何计算二维空间中两点之间的欧几里德距离。它是使用点的 x 和 y 坐标之差的平方来计算的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b0f391d25bc42025dc1b4ed0a9abced3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*kF0Qj3NJDjxnz-Nx.png"/></div></figure><p id="3686" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，欧几里德距离是(16 + 9)的平方根，也就是 5。二维中的欧几里得距离让我们想起了著名的<a class="ae ky" href="https://en.wikipedia.org/wiki/Pythagorean_theorem" rel="noopener ugc nofollow" target="_blank">勾股定理</a>。</p><p id="67e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他方法来衡量距离如余弦相似度，平均距离等等。相似性度量是 k-means 聚类的核心。最佳方法取决于问题的类型。因此，为了选择最佳的测量类型，拥有良好的领域知识是很重要的。</p><blockquote class="nv nw nx"><p id="ddc1" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">K-means 聚类试图最小化一个类内的距离，最大化不同类之间的距离。</p></blockquote><p id="1b1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从一个简单的例子开始理解这个概念。像往常一样，我们首先导入依赖项:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="06ae" class="oi mz it oe b gy oj ok l ol om"># Importing necessary libraries<br/>import numpy as np<br/>import pandas as pd</span><span id="7701" class="oi mz it oe b gy on ok l ol om">import matplotlib.pyplot as plt</span><span id="883a" class="oi mz it oe b gy on ok l ol om">from sklearn.datasets import make_blobs<br/>from sklearn.cluster import KMeans</span></pre><blockquote class="nv nw nx"><p id="001e" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">Scikit-learn 提供了许多有用的功能来创建合成数据集，这对练习机器学习算法非常有帮助。我将使用<strong class="lb iu"> make_blobs </strong>函数。</p></blockquote><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="9598" class="oi mz it oe b gy oj ok l ol om">X, y = make_blobs(n_samples = 200, centers=4, cluster_std = 0.5, random_state = 0)</span><span id="094e" class="oi mz it oe b gy on ok l ol om">plt.scatter(X[:, 0], X[:, 1], s=50)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/595e2cb6f58330cfbc9637fdc17f7957.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*j4qpQ8YyBe81r1lfTSup_w.png"/></div></figure><p id="1143" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们创建一个 KMeans 对象并拟合数据:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="5dcd" class="oi mz it oe b gy oj ok l ol om">kmeans = KMeans(n_clusters = 4)<br/>kmeans.fit(X)</span></pre><p id="0740" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以将数据集划分为多个集群:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="a4b8" class="oi mz it oe b gy oj ok l ol om">y_pred = kmeans.predict(X)<br/>plt.scatter(X[:, 0], X[:, 1], c = y_pred, s=50)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3f4a0db991b5d9a2af433d3fb5ff3c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*XyvJ8ELaSqMJwC5ewgimYg.png"/></div></figure><p id="fc61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现实生活中的数据集要复杂得多，其中的聚类没有明显的区分。但是，算法的工作方式是一样的。</p><blockquote class="nv nw nx"><p id="0a13" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">K-means 算法不能确定聚类数。我们需要在创建 KMeans 对象时定义它，这可能是一项具有挑战性的任务。</p></blockquote><p id="c6eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means 是一个迭代过程。它基于<a class="ae ky" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>算法。确定集群数量后，它通过执行以下步骤来工作:</p><ol class=""><li id="ead9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu op mb mc md bi translated">为每个簇随机选择质心(簇的中心)。</li><li id="1e4a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu op mb mc md bi translated">计算所有数据点到质心的距离。</li><li id="038f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu op mb mc md bi translated">将数据点分配给最近的聚类。</li><li id="e0fd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu op mb mc md bi translated">通过取聚类中所有数据点的平均值，找到每个聚类的新质心。</li><li id="b345" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu op mb mc md bi translated">重复步骤 2、3 和 4，直到所有点收敛并且聚类中心停止移动。</li></ol><blockquote class="nv nw nx"><p id="6873" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>:初始质心是随机选择的，这可能会导致最终的聚类有些不同。为了克服这个问题，scikit learn 提供了<strong class="lb iu"> n_init </strong>参数。k-means 算法以不同的初始质心运行“n_init”次，并且最终结果将根据 n_init 次连续运行来确定。</p></blockquote><p id="0182" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以使用 Naftali Harris 在他的博客上准备的这个工具来可视化每个步骤。我从这个互动工具中截取了截图，这样上面的步骤就更容易理解了。我强烈建议花点时间使用这个工具。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/e3b3299b99d2e1c4036aa4906dbed427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVY5-Oe_kxMnROyNyCqsuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机选择质心</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/b14805832be36cccb0f76cb39ccefbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*W46mjLFIcmlxzp9xTSZrMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算所有数据点到质心的距离，并将数据点分配给最近的聚类</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9c011b200f7965c6f8a6ab83ef970368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*clqgjxgzRHo1FdRu6fGebA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过取平均值来计算每个聚类的新质心</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/26870ab18a0480d31c8ed47201c69f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*Z1ckWdlH37qXtmx5U8HOWw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将数据点重新分配到最近的聚类中心</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ad655f9174499ee652b3ff4874a44290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*-kdp-Oefr5pNLvBSAyMSmA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当聚类中心(质心)不再移动时，模型收敛</p></figure><h1 id="24af" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">利弊</strong></h1><p id="ce14" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="9fa3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">容易理解</li><li id="9dff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">相对较快</li><li id="83c7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">可扩展用于大型数据集</li><li id="2150" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">能够以一种聪明的方式选择初始质心的位置，从而加速收敛</li><li id="794a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">保证收敛</li></ul><p id="32cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="a327" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">集群的数量必须预先确定。K-means 算法无法猜测数据中存在多少个聚类。确定集群的数量可能是一项具有挑战性的任务。</li><li id="5572" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">只能画线性边界。如果数据中存在非线性结构来分隔组，k-means 将不是一个好的选择。</li><li id="c45d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随着样本数量的增加而变慢，因为在每一步，k-means 算法都会访问所有数据点并计算距离。另一种方法是使用数据点的子集来更新质心的位置(即 sk learn . cluster . minibatchkmeans)</li><li id="a177" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对异常值敏感</li></ul></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="f745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="00f7" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">我关于机器学习算法的其他帖子</h1><ul class=""><li id="20d8" class="lv lw it lb b lc nq lf nr li pc lm pd lq pe lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/naive-bayes-classifier-explained-50f9723571ed">朴素贝叶斯分类器—解释</a></li><li id="e912" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/logistic-regression-explained-593e9ddb7c6c">逻辑回归—已解释</a></li><li id="3406" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/k-nearest-neighbors-knn-explained-cbc31849a7e3">K-最近邻(kNN)-解释</a></li><li id="3578" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd">支持向量机—解释</a></li><li id="1885" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/decision-tree-and-random-forest-explained-8d20ddabc9dd">决策树和随机森林——解释</a></li><li id="2c8c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af">梯度增强决策树—解释</a></li><li id="0037" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/principal-component-analysis-explained-d404c34d76e7">主成分分析—已解释</a></li><li id="f264" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/predicting-used-car-prices-with-machine-learning-fea53811b1ab">用机器学习预测二手车价格</a></li></ul></div></div>    
</body>
</html>