<html>
<head>
<title>An Introduction to Graph Neural Network(GNN) For Analysing Structured Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于结构化数据分析的图形神经网络(GNN)简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-graph-neural-network-gnn-for-analysing-structured-data-afce79f4cfdc?source=collection_archive---------0-----------------------#2020-03-05">https://towardsdatascience.com/an-introduction-to-graph-neural-network-gnn-for-analysing-structured-data-afce79f4cfdc?source=collection_archive---------0-----------------------#2020-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h2 id="c189" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">了解GNN是什么，GNN能做什么</h2><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi kt"><img src="../Images/f376019d18b20a7854ab76b8fd8f7693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SkXaIgBCjF7F8g99.jpg"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">来源:<a class="ae lj" href="https://pixabay.com/users/manuchi-1728328/?tab=popular&amp;pagi=2" rel="noopener ugc nofollow" target="_blank">马努奇</a>，via <a class="ae lj" href="https://pixabay.com/en/labrador-breed-dogs-animal-animals-805838/" rel="noopener ugc nofollow" target="_blank"> pixabay </a> (CC0)</p></figure><p id="1bd6" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">图形神经网络(GNN)由于其分析图形结构数据的能力最近受到了很多关注。本文对图形神经网络作了简要介绍。为了便于理解图和分析图中的问题，它涵盖了一些图论。然后介绍了不同形式的图形神经网络及其原理。它还涵盖了GNN可以做什么和GNN的一些应用。</p><h1 id="fbef" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">图论</h1><p id="ee1e" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">首先，我们需要知道什么是图。</p><p id="e4d7" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">图是由两部分组成的数据结构:<strong class="lm iu"> <em class="nb">顶点、</em> </strong>和<strong class="lm iu"> <em class="nb">边</em> </strong>。它被用作一种数学结构来分析对象和实体之间的成对关系。通常，一个图被定义为<em class="nb"> G=(V，E)，</em>，其中<em class="nb"> V </em>是一组节点，<em class="nb"> E </em>是它们之间的边。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d31dbc25a09948d96216c36532314d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*2TzGbB-dqiNPZArYKu1y9g.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">一个简单的图表。按作者分列的数字</p></figure><p id="adf2" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">一个图往往用一个邻接矩阵来表示，<em class="nb"> A. </em>如果一个图有<em class="nb"> N个</em>节点，那么A的维数为(<em class="nb"> N </em> x <em class="nb"> N </em>)。人们有时会提供另一个特征矩阵来描述图中的节点。如果每个节点有<em class="nb"> F </em>个特征，那么特征矩阵<em class="nb"> X </em>的维数为(<em class="nb"> N </em> x <em class="nb"> F </em>)。</p><h2 id="f3c4" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">为什么一个图很难分析？</h2><p id="b313" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">首先，一个图不存在于欧几里得空间中，这意味着它不能用我们熟悉的任何坐标系来表示。与波形、图像或时间序列信号(“文本”也可以被视为时间序列)等其他类型的数据相比，这使得图形数据的解释更加困难，这些数据可以很容易地映射到二维或三维欧几里德空间。</p><p id="ef85" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">其次，图形没有固定的形式。为什么？看看下面的例子。图(A)和图(B)结构完全不同，视觉上也不同。但是当我们将其转换为邻接矩阵表示时，这两个图具有相同的邻接矩阵(如果不考虑边的权重)。那么我们应该认为这两幅图是相同的还是不同的呢？</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/11f3746fc504bf739d50bd9b3b19a12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*9TagaOGBuXWP2X1rD2uSvg.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图表(A)。按作者分列的数字</p></figure><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ba66e05b7e9ede5b0aac6b918e1ebab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*xiSWbn1V21YfSJ7m852WoA.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图表(B)。按作者分列的数字</p></figure><p id="ca81" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">最后，一个图表通常很难被人类理解。我说的不是上面例子中的小图。我说的是包含成百上千个节点的巨型图。维度非常高，节点密集分组，使得人类甚至难以理解图形。因此，训练一台机器来完成这项任务具有挑战性。下面的例子显示了集成电路中逻辑门的模型图。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nf"><img src="../Images/8619ac3d10d3f2e71267de4e3fec594b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdX_wV_GHyYFUdudzgIXLA.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">巨型图的例子:电路网表。图来自J. Baehr等人。艾尔。“机器学习和逆向工程的结构特征”</p></figure><h2 id="f3e7" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">为什么要用图表？</h2><p id="abb5" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">人们选择处理图表的原因可以总结为以下几点:</p><ol class=""><li id="6b2e" class="ng nh it lm b ln lo lr ls kg ni kk nj ko nk me nl nm nn no bi translated">图形提供了一种更好的处理抽象概念的方式，比如关系和交互。它们还提供了一种思考这些概念的直观方式。图表也是分析社会关系的自然基础。</li><li id="5f2f" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">图形可以通过将问题简化为更简单的表示来解决更复杂的问题，或者从不同的角度将问题转换为表示。</li><li id="2ce3" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">图论和概念用于研究和建模社交网络、欺诈模式、功耗模式、病毒式传播和社交媒体中的影响。社交网络分析(SNA)可能是图论在数据科学中最著名的应用。</li></ol><h1 id="23c6" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">传统图形分析方法</h1><p id="a4a7" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">传统方法大多基于算法，例如:</p><ol class=""><li id="73bd" class="ng nh it lm b ln lo lr ls kg ni kk nj ko nk me nl nm nn no bi translated">搜索算法，例如BFS、DFS</li><li id="205f" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">最短路径算法，例如Dijkstra算法、最近邻算法</li><li id="1aa5" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">生成树算法，例如Prim算法</li><li id="d320" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">聚类方法，例如高度连接的组件、k均值</li></ol><p id="a76f" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">这种算法的局限性在于，在应用该算法之前，我们需要以一定的置信度获得图的先验知识。换句话说，它没有给我们提供研究图形本身的方法。而且最重要的是，没有办法进行图级分类。</p><h1 id="1dac" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">图形神经网络</h1><p id="e85b" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">图形神经网络，顾名思义，是一种可以直接应用于图形的神经网络。它为节点级、边级和图级预测任务提供了一种方便的方式。</p><p id="9335" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">文献中主要有三种类型的图形神经网络:</p><ol class=""><li id="fdca" class="ng nh it lm b ln lo lr ls kg ni kk nj ko nk me nl nm nn no bi translated">递归图神经网络</li><li id="8c87" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">空间卷积网络</li><li id="9d6f" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">光谱卷积网络</li></ol><p id="2fb3" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">GNN的直觉是，节点是由它们的邻居和连接自然定义的。为了理解这一点，我们可以简单地想象，如果我们删除一个节点周围的邻居和连接，那么该节点将丢失其所有信息。因此，节点的邻居和到邻居的连接定义了节点的概念。</p><p id="5e91" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">记住这一点，然后我们给每个节点一个状态<em class="nb"> (x) </em>来表示它的概念。我们可以使用节点状态<em class="nb"> (x) </em>来产生一个输出<em class="nb"> (o) </em>，即关于概念的决策。节点的最终状态<em class="nb"> (x_n) </em>通常称为“节点嵌入”。所有GNN的任务是通过查看其相邻节点的信息来确定每个节点的“节点嵌入”。</p><p id="63d8" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">我们将从图形神经网络的最先锋版本开始，递归图形神经网络，或<em class="nb"> RecGNN </em>。</p><h2 id="9f7b" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">递归图神经网络</h2><p id="7258" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">正如在<a class="ae lj" href="https://ieeexplore.ieee.org/document/4700287" rel="noopener ugc nofollow" target="_blank">最初的GNN论文</a>中所介绍的，RecGNN是基于Banach不动点定理的假设而建立的。Banach不动点定理陈述:<em class="nb">设(X，d)为完备度量空间，设(T:X→X)为压缩映射。那么T有一个唯一的不动点(X∫),对于任何x∈X，n→∞的序列TN(X)收敛于(X∫)。</em>这意味着如果我将映射<em class="nb"> T </em>应用到<em class="nb"> x </em>上<em class="nb"> k </em>次，x^k应该几乎等于x^(k-1)，即:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/142a52e43948af496940a74a138f2d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*sdk_VhgQHQqZKz75kWYX_g.png"/></div></figure><p id="d5e9" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">RecGNN定义了一个参数化函数<em class="nb"> f_w: </em></p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/42dec2d841a0e6451d2c4c7543c76af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*mSHmW58lfL2onzATerldpw.png"/></div></figure><p id="76c7" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">其中<strong class="lm iu"> <em class="nb"> l_n，l_co，x_ne，l_ne </em> </strong> <em class="nb"> </em>表示当前节点<strong class="lm iu"><em class="nb">n</em></strong>的特征，节点<strong class="lm iu"><em class="nb">n</em></strong>的边，邻居节点的状态，邻居节点的特征。(在原始论文中，作者将节点特征称为节点标签。这可能会造成一些混乱。)</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nw"><img src="../Images/b5c1b7143f5e149ccc257618f7a606aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvPgUpwk17qHGRgUOkH1kQ.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">基于邻居信息的节点状态更新示意图。图来自“图形神经网络模型”</p></figure><p id="6c71" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">最后，在k次迭代之后，最终的节点状态被用于产生输出，以对每个节点做出决定。输出函数定义为:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8082d675fc7247d0ca383632c16a6755.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Vmo93eMtu9xLpUi8Ex3-Ow.png"/></div></figure><h2 id="38d8" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">空间卷积网络</h2><p id="057f" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">空间卷积网络的直觉类似于著名的CNN，它主导了图像分类和分割任务的文献。要了解CNN的图片，你可以看看这篇详细解释CNN的<a class="ae lj" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">帖子</a>。</p><p id="e43a" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">简而言之，图像卷积的思想是对中心像素周围的相邻像素求和，中心像素由具有参数化大小和可学习权重的过滤器指定。空间卷积网络采用同样的思想，将相邻节点的特征聚集到中心节点。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8bae5aa439c28a3ddf814eb1fd50295a.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*3cE5PnjfPC1VGckCS21k_A.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">左图:图像等常规图形上的卷积。右图:任意图形结构上的卷积。图来自<a class="ae lj" href="https://arxiv.org/abs/1901.00596" rel="noopener ugc nofollow" target="_blank">图神经网络综述</a></p></figure><h2 id="f62f" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">光谱卷积网络</h2><p id="6c3c" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">与其他类型的GNN相比，这种图形卷积网络有很强的数学基础。谱卷积网络是建立在图形信号处理理论基础上的。以及通过图形卷积的简化和近似。</p><p id="74e1" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">通过<em class="nb"/><a class="ae lj" href="https://arxiv.org/pdf/0912.3848.pdf" rel="noopener ugc nofollow" target="_blank">【切比雪夫多项式逼近】(Hammond et al. 2011) </a>，图形卷积可以简化为如下形式:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e623e6d4b8987fbad8be15185d7e2843.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*OMoUnN7C2UmFEXekBeTqLw.png"/></div></figure><p id="e15e" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">经过进一步简化，<a class="ae lj" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> GCN论文</a>提出了一种两层神经网络结构，可以用下面的一个等式来描述:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi oa"><img src="../Images/fdc61ab3be639e26b0ea262d120345a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WkKAzrFJi9AFxj1hTgQgLw.png"/></div></div></figure><p id="a881" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">其中A_head是原始图邻接矩阵A的预处理拉普拉斯算子。(数学的细节可以在GCN论文中找到。要完全解释清楚还需要很大的努力。)</p><p id="b6e5" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">如果你有一些机器学习的经验，这个公式看起来非常熟悉。这不过是两个常用的全连接层结构。但在这种情况下，它确实起到了图形卷积的作用。我将在下面展示为什么它可以执行图形卷积。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b653d5cad60ca11dc957d87c048979df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Zgf5j2l_UJmpl8KXmsiw9w.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">为每个节点分配了一个功能的图表示例。由作者描绘</p></figure><p id="c353" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">让我们考虑一个有4个节点的简单图形。如上图所示，每个节点都被分配了一个特征矩阵。很容易得出如下所示的图邻接矩阵和特征矩阵:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi oc"><img src="../Images/5768f93a19e4c589aa6518525bdc8c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jy7A04FrEoAU4y9p9_2ynA.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">邻接矩阵和特征矩阵的例子。按作者分列的数字</p></figure><p id="5fff" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated"><em class="nb">请注意，邻接矩阵的对角线特意更改为“1 ”,以便为每个节点添加自循环。这是为了在我们执行特征聚合时包括每个节点本身的特征。</em></p><p id="ee26" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">然后我们执行<em class="nb"> A </em> x <em class="nb"> X ( </em>为了解释简单起见，我们先忘掉A的拉普拉斯算子和权重矩阵<em class="nb"> W </em>。<em class="nb"> ) </em></p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi od"><img src="../Images/3c0ded9ddf7892c6b3dc351e7fd420b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMZrQGKua0WB_Tmwl2PycQ.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">用矩阵乘法进行图形卷积的例子。按作者分列的数字</p></figure><p id="5b13" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">矩阵乘法的结果显示在最右边的矩阵中。让我们以第一个节点的结果特性为例。不难注意到，结果是[节点1]的所有特征的总和，包括[节点1]本身的特征，并且不包括[节点4]中的特征，因为它不是[节点1]的邻居。从数学上来说，图的邻接矩阵只有在有边的情况下才具有值“1”，否则为“0”。这使得矩阵乘法成为连接到参考节点的节点的特征的总和。</p><p id="cba3" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">因此，虽然频谱卷积网络和空间卷积网络是在不同的基础上开始的，但是它们共享相同的传播规则。</p><p id="9bca" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">目前可用的所有卷积图神经网络共享相同的格式。它们都试图学习一个函数，通过这个消息传递过程来传递节点信息和更新节点状态。</p><p id="2a8c" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">任何图神经网络都可以表示为一个消息传递神经网络(J. Gilmer et al .，2017)，具有一个<strong class="lm iu"> <em class="nb">消息传递</em> </strong>函数，一个<strong class="lm iu"> <em class="nb">节点更新</em> </strong>函数和一个<strong class="lm iu"> <em class="nb">读出</em> </strong>函数。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi oe"><img src="../Images/b2dd94e616e5da256d2da0d5384b6dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ki0XMgBSZu-WS7XSM8zljw.png"/></div></div></figure><h1 id="fe00" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">GNN能做什么？</h1><p id="6811" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">GNN解决的问题可以大致分为三类:</p><ol class=""><li id="ba5f" class="ng nh it lm b ln lo lr ls kg ni kk nj ko nk me nl nm nn no bi translated">节点分类</li><li id="2e91" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">链接预测</li><li id="7042" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">图形分类</li></ol><p id="7fba" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">在<em class="nb">节点分类</em>中，任务是预测图中每个节点的节点嵌入。这种类型的问题通常以半监督的方式训练，其中只有部分图被标记。节点分类的典型应用包括引用网络、Reddit帖子、Youtube视频和脸书朋友关系。</p><p id="2527" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">在<em class="nb">链接预测</em>中，任务是理解图中实体之间的关系，并预测两个实体之间是否有连接。例如，推荐系统可以被视为链接预测问题，其中模型被给定一组用户对不同产品的评论，任务是预测用户的偏好并根据用户的兴趣调整推荐系统以推送更相关的产品。</p><p id="f8af" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">在<em class="nb">图形分类</em>中，任务是将整个图形分为不同的类别。它类似于图像分类，但是目标变成了图形域。图分类可以应用于广泛的工业问题，例如，在化学、生物医学、物理学中，模型被给定一个分子结构，并被要求将目标分类成有意义的类别。它加速了原子、分子或任何其他结构化数据类型的分析。</p><h1 id="d1da" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">一些真实的应用</h1><p id="ed72" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">了解了GNN可以执行的分析类型后，你一定想知道我可以用图形做什么真正的事情。嗯，这一节将让你更深入地了解GNN在现实世界中的应用。</p><h2 id="3f1a" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">自然语言处理中的GNN</h2><p id="ab04" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">GNN广泛应用于自然语言处理领域。实际上，这也是GNN最初开始的地方。如果你们中的一些人有NLP的经验，你一定会认为文本应该是一种时序或时态数据，这种数据可以用RNN或LTSM来最好地描述。GNN从一个完全不同的角度看待这个问题。GNN利用单词或文档的内在联系来预测类别。例如，引用网络试图预测网络中每篇论文的标签，这些标签由论文引用关系和其他论文中引用的词给出。它还可以通过查看句子的不同部分来建立句法模型，而不是像RNN或LTSM那样纯粹按顺序排列。</p><h2 id="a43a" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">计算机视觉中的GNN</h2><p id="87af" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">许多基于CNN的方法已经在图像中的对象检测中实现了最先进的性能，但是我们还不知道对象之间的关系。GNN在CV中的一个成功应用是使用图形来模拟由基于CNN的检测器检测到的对象之间的关系。在从图像中检测到对象后，它们被送入GNN推理中进行关系预测。GNN推理的结果是一个生成的图形，它模拟了不同对象之间的关系。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c8e97643d0863fa3081975c4269438c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*bV3JKhMl_imslqT41hvHug.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">场景图生成。图来自徐达明、朱永源、蔡志斌和，“通过迭代消息传递生成场景图”，在<em class="og"> Proc。2017年CVPR </em></p></figure><p id="bcbc" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">CV中另一个有趣的应用是从图形描述生成图像。这可以解释为几乎与上述应用相反。传统的图像生成方式是使用GAN或自动编码器进行文本到图像的生成。图形到图像的生成提供了关于图像语义结构的更多信息，而不是使用文本来描述图像。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/50603b042eca7a013741d93687f572f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*gj0mWilPILG4gX4_uVnvHg.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">从场景图生成的图像。图来自j .约翰逊、a .古普塔和l .飞飞，“从场景图生成图像”，<em class="og">会议录。2018年CVPR </em></p></figure><p id="6847" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">我想分享的最有趣的应用是零起点学习(ZSL)。你可以找到<a class="ae lj" rel="noopener" target="_blank" href="/applications-of-zero-shot-learning-f65bb232963f">这篇文章</a>全面介绍ZSL。简而言之，ZSL正在尝试学习给定<strong class="lm iu">根本没有</strong>训练样本(目标班级)的情况下对班级进行分类。这非常具有挑战性，因为如果没有给定训练样本，我们需要让模型进行逻辑“思考”来识别目标。举个例子，如果给我们三张图片(如下图所示)，并让我们在其中找到“okapi”。我们以前可能没见过“okapi”。但是，如果我们也被告知“霍加皮”是一种有四条腿的鹿脸动物，有斑马条纹的皮肤，那么我们就不难猜出哪一个是“霍加皮”。典型的方法是通过将检测到的特征转换成文本来模拟这种“思维过程”。然而，文本编码是相互独立的。很难对文本描述之间的关系进行建模。在其他方面，图形表示很好地模拟了这些关系，使机器以更“类似人类”的方式思考。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi oi"><img src="../Images/9e0c26235899e22f56bb42ca86cf4d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-XfrIURP8ZxkvDgaDTGjg.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图来自X. Wang，Y. Ye，A. Gupta，“基于语义嵌入和知识图的零镜头识别”，<em class="og">2018</em></p></figure><h2 id="6f28" class="jx jy it bd jz ka kb dn kc kd ke dp kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">其他领域的GNN</h2><p id="bbbc" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">GNN的更多实际应用包括人类行为检测、交通控制、分子结构研究、推荐系统、程序验证、逻辑推理、社会影响预测和对抗性攻击预防。下图展示了社交网络中人们关系的模型。GNN可以用来把人们分成不同的社区群体。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi kt"><img src="../Images/6d23869f37a584b38665bb9b5d4d26bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vOf5GavAOEeO3fM1.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">社交网络图。图片来自<a class="ae lj" href="https://pixabay.com/vectors/social-media-connections-networking-3846597/" rel="noopener ugc nofollow" target="_blank"> GDJ </a>，通过Pixabay</p></figure><h1 id="3c49" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">结论</h1><p id="3984" class="pw-post-body-paragraph lk ll it lm b ln mw lp lq lr mx lt lu kg my lw lx kk mz lz ma ko na mc md me im bi translated">在本文中，我们介绍了一些图论知识，并强调了分析图的重要性。人们总是把机器学习算法看成一个“<strong class="lm iu">黑箱</strong>”。大多数机器学习算法只从训练数据的特征中学习，但没有实际的逻辑来执行。有了图，我们或许可以把一些“逻辑”传递给机器，让它更自然地“思考”。</p><p id="90dd" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu kg lv lw lx kk ly lz ma ko mb mc md me im bi translated">GNN仍然是一个相对较新的领域，值得更多的研究关注。它是分析图形数据的有力工具。然而，它不仅限于图中的问题。它可以很容易地推广到任何可以用图表建模的研究。图形建模是分析问题的自然方法。</p><h1 id="c177" class="mf jy it bd jz mg mh mi kc mj mk ml kf mm mn mo kj mp mq mr kn ms mt mu kr mv bi translated">参考资料:</h1><ol class=""><li id="2034" class="ng nh it lm b ln mw lr mx kg oj kk ok ko ol me nl nm nn no bi translated">F.Scarselli，M.Gori，“图形神经网络模型”，<em class="nb"> IEEE神经网络汇刊，2009 </em></li><li id="9025" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">T.N. Kipf和M. Welling，“使用图卷积网络的半监督分类”，载于<em class="nb"> Proc。ICLR的</em>，2017。</li><li id="3cde" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">Z.吴，潘世安，陈芳芳，龙国光，张春春，俞，图神经网络综述，arXiv:1901.00596 </li><li id="8cdd" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">D.徐，朱，蔡志斌，和，“通过迭代信息传递生成场景图”，<em class="nb">会议录。CVPR</em>2017年第2期</li><li id="d323" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">J.约翰逊、a .古普塔和l .飞飞，“从场景图生成图像”，<em class="nb">会议录。2018年CVPR </em></li><li id="f029" class="ng nh it lm b ln np lr nq kg nr kk ns ko nt me nl nm nn no bi translated">X.王，叶，古普塔，“基于语义嵌入和知识图的零镜头识别”，<em class="nb">2018</em></li></ol></div></div>    
</body>
</html>