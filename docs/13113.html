<html>
<head>
<title>Maximum Likelihood (ML) vs. REML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然(ML)与 REML</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf?source=collection_archive---------3-----------------------#2020-09-09">https://towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf?source=collection_archive---------3-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0774" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/stats-ml-life-sciences" rel="noopener" target="_blank">生命科学的数理统计和机器学习</a></h2><div class=""/><div class=""><h2 id="7f83" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于限制最大似然的线性混合模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/871e276de735b42dcf5e0bfc5a9c48dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RkrBD8WJsc43q9ndDhiWXg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="b0df" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是来自专栏<a class="ae md" href="https://towardsdatascience.com/tagged/stats-ml-life-sciences?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="lj jd">生命科学的数理统计和机器学习</strong> </a>的第十九篇文章，我试图以简单的方式解释生物信息学和计算生物学中使用的一些神秘的分析技术。这是致力于线性混合模型(LMM)系列的最后一篇文章。之前我们讲过<a class="ae md" rel="noopener" target="_blank" href="/how-linear-mixed-model-works-350950a82911"> <strong class="lj jd">线性混合模型如何工作</strong> </a>，如何从 R 中的<strong class="lj jd">最大似然(ML) </strong>原理出发，从零开始推导并编程<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4"> <strong class="lj jd">线性混合模型</strong> </a> <strong class="lj jd"> </strong>。今天我们将讨论<strong class="lj jd">限制最大似然(REML) </strong>的概念，它为什么有用，以及如何将其应用于线性混合模型。</p><h1 id="3b66" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">最大似然有偏方差估计量</h1><p id="1a0d" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">受限最大似然(<strong class="lj jd"> REML </strong>)的思想来源于认识到最大似然(ML)给出的方差估计量是有偏的。什么是估计量，它以何种方式存在偏差？估计量只是模型参数的近似值/估计值。假设统计观测值服从正态分布，如果要对观测值进行汇总，需要估计两个参数:<em class="nb"> μ </em>(均值)和<em class="nb"> σ </em>(方差)。原来最大似然(ML)给出的<strong class="lj jd">方差估计量</strong>是<strong class="lj jd">有偏的</strong>，即我们从 ML 模型得到的值高估或者<strong class="lj jd">低估了</strong> <strong class="lj jd">真实方差</strong>，见下图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/fd0c41b2b7fb49a0afe3161ca0c5659c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NWtW2-KjEY4tTrKWolGdA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">有偏与无偏估计量的图解。作者图片</p></figure><p id="bc1f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在实践中，当我们例如使用 ML 求解一个<a class="ae md" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型时，我们很少考虑方差估计量中的偏差，因为我们通常对线性模型的<strong class="lj jd">系数感兴趣，即</strong> <strong class="lj jd">平均值</strong>，并且经常没有意识到我们并行地估计<strong class="lj jd">另一个</strong>拟合参数，即方差。在这种情况下，方差被认为是所谓的<a class="ae md" href="https://en.wikipedia.org/wiki/Nuisance_parameter" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">干扰参数</strong> </a> <strong class="lj jd"> </strong>，这不是我们主要关心的。</p><p id="735b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了证明 ML 确实给出了有偏方差估计量，考虑一个简单的一维情况，其中变量<em class="nb"> y </em> = ( <em class="nb"> y </em> 1，<em class="nb"> y </em> 2，…，<em class="nb"> yN </em>)遵循正态分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/bf2ff206e5715c781aad11996061c66b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6eWJyNAcP4Zg8GJ6RLZzA.png"/></div></div></figure><p id="b298" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最大似然，等式。(1)，导致均值和方差的估计量，方程。(2)、推导请到我的<a class="ae md" href="https://github.com/NikolayOskolkov/REML" rel="noopener ugc nofollow" target="_blank"> github </a>查看笔记本。为了证明等式中的方差估计量。(2)有偏，我们将推导方差估计量的期望值，并证明它不等于方差的真值<em class="nb">，σ </em>。为此，我们首先重新安排方差估计量，方程。(2)通过将未知的真实均值<em class="nb"> μ </em>显式地包括到等式中:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/bdc0e9ba0439fffe39a96be05d3452b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-mSe8RV_xhajggPcD8U8Q.png"/></div></div></figure><p id="98d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，让我们计算方差估计量的期望值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/86d94762b78c808f1344f13713b0100f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k6qQq4j0EmEjbNQ3-RxVQ.png"/></div></div></figure><p id="b9f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们可以看到，ML 方差估计值的<strong class="lj jd">期望值不等于真实方差<em class="nb"> σ </em>，尽管它在大样本量时接近真实方差。因此，ML 给出的方差估计量<strong class="lj jd">向下偏移</strong>，即<strong class="lj jd">低估了真实方差</strong>。当 N &gt; &gt; 1 时，偏差似乎可以忽略不计，直到我们认识到等式。(8)是针对一维数据获得的。使用高维数据，这是现实世界中的典型问题，我们可以得到方差的严重有偏估计，因为它可以被导出，例如查看精彩的教程<a class="ae md" href="https://people.csail.mit.edu/xiuming/docs/tutorials/reml.pdf" rel="noopener ugc nofollow" target="_blank">这里的</a>，对于 k 维数据，方差估计的期望值采用以下形式:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b8a0684fd135cb94e50e66df2f352d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*tdkT3gzUVr1-x2hhcB3Psw.png"/></div></figure><p id="fde2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，当维数<em class="nb"> k </em>接近样本数/统计观测值<em class="nb">，N </em>时，ML 低估真实方差的问题变得尤其尖锐。我们得出结论，在<strong class="lj jd">高维</strong>空间中，最大似然(ML)原理只在极限<em class="nb"> k </em> &lt; &lt; <em class="nb"> N </em>中起作用，而<strong class="lj jd">有偏</strong> <strong class="lj jd">结果</strong>可以在<strong class="lj jd"><em class="nb">k</em>≈<em class="nb">N</em></strong><em class="nb">中得到。</em>这种偏见需要被考虑进去，这正是 REML 发挥作用的地方。</p><h1 id="98bc" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">基于 REML 的线性混合模型</h1><p id="a894" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">ML 的<strong class="lj jd">有偏方差估计量的问题似乎是由于我们使用了一个<strong class="lj jd">未知的均值估计量</strong>来计算方差估计量。相反，如果我们确保对数似然函数<strong class="lj jd">不包含任何关于均值</strong>的信息，我们可以针对方差分量对其进行优化，并获得一个<strong class="lj jd">无偏方差估计量</strong>。这就是受限最大似然法(REML)的本质。在这种情况下，<strong class="lj jd">平均值(不像 ML 的方差)</strong>被认为是一个<strong class="lj jd">干扰参数</strong>，应该以某种方式从等式中移除。从对数似然函数中去除关于平均值的信息的一种方法是计算一个<strong class="lj jd">边际概率</strong>，即<strong class="lj jd">对平均值</strong>上的对数似然进行积分。在之前的帖子<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4"> <strong class="lj jd"> LMM 从零开始</strong> </a>中，我们看到了对于处理高维数据的多元分析，Eq 的扩展。(1)由<strong class="lj jd">给出的多元高斯分布</strong>:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/23cd62535c97f286fc927b0aa3def3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*oF-ibs_S_kbzzZvT4g6xdw.png"/></div></figure><p id="4983" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中<strong class="lj jd">σ</strong>y<strong class="lj jd"/>是方差-协方差矩阵，它是等式中简单残差方差的推广。(1).这里，我们将对多元高斯分布(对数似然)取对数，并对平均值<em class="nb">、β </em>的对数似然进行积分，得到方差分量的<strong class="lj jd">无偏估计值</strong>。因此，我们需要计算以下积分:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/8434b146dd75f3b916d020d7d2b61a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmmRWKnb8slZMkTRAAS2xg.png"/></div></div></figure><p id="7422" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为此我们将使用<strong class="lj jd">鞍点</strong>方法(<a class="ae md" href="https://en.wikipedia.org/wiki/Laplace%27s_method" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">拉普拉斯近似</strong> </a>)。在这种方法中，由于指数函数在方程的第三项积分下。(9)下降非常快，这足以计算指数<em class="nb">、指数</em> ( <em class="nb"> f </em> ( <em class="nb"> β </em>))中函数<em class="nb"> f </em> ( <em class="nb"> β </em>)的<strong class="lj jd">最大值</strong>中的积分，这将对指数做出最大贡献，并因此对等式(1)中的积分做出最大贡献。(9)，并因此的可能性。通过<em class="nb"> f </em> ( <em class="nb"> β </em>表示指数中的函数，我们可以在均值估计点附近通过泰勒级数展开<strong class="lj jd">来近似它:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1081ec7accc0c9da6bdc32f916989f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*bqnPs2KG96dL1juvhAMChQ.png"/></div></figure><p id="8fe4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里<strong class="lj jd">线性项为零</strong>是因为<a class="ae md" href="https://en.wikipedia.org/wiki/Maxima_and_minima" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">极值条件</strong> </a>。这里，我们假设在现实中，真实均值的似然性是最大的，但是估计量离真实均值不远，因此可以执行泰勒级数展开。回到等式的第三项。(9)，并且将指数中的函数表示为<em class="nb"> f </em> ( <em class="nb"> β </em>)，围绕均值估计量的泰勒级数展开给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/13ea68cef725f36d3ab95b152210c6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKdq1gVveLTNc1pkarI7Eg.png"/></div></div></figure><p id="febd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中<strong class="lj jd"> |…| </strong>是行列式的符号。等式中的前两项。(12)是我们在<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4"> <strong class="lj jd"> LMM 从无到有</strong> </a> (Eq。(10)在那篇文章中)。相比之下，<strong class="lj jd">第三项</strong>来自 REML 方法。人们可以把这个额外的项看作是一个<strong class="lj jd">惩罚模型</strong> ( <a class="ae md" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">脊/套索/弹性网</a>)中的一个惩罚(或偏差)，我们在线性回归(或 LMM)模型中的系数上加了一个<strong class="lj jd">约束</strong>。让我们来看看这个来自 REML 的<strong class="lj jd">附加项如何影响<strong class="lj jd">玩具数据集</strong>的线性混合模型(LMM)的解决方案，该模型是在<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4"> LMM 从零开始</a>帖子中介绍的。</strong></p><h1 id="b4c9" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">LMM 通过 REML 的玩具数据集</h1><p id="65a2" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">为了简单起见，我们只考虑了<strong class="lj jd"> 4 个数据点</strong>: 2 个来自个人#1，另外 2 个来自个人#2。此外，这 4 个点分布在两种情况之间:<strong class="lj jd">未治疗和治疗</strong>，请参见下图。在处理栏中，0 表示未处理，1 表示已处理。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/a49d7b3643149952090655896b8841de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IacvvDHmixyCxEHvyT0xOw.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/06ea25456e0da1b76cc0f1f0c9e9a7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFylyesftH8N4szv4999pg.png"/></div></div></figure><p id="a657" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们使用 LMM 拟合数据，使用<strong class="lj jd"> lme4 </strong> R 包中的<strong class="lj jd"> lmer </strong>函数对斜率和截距进行<strong class="lj jd">固定效应，对截距进行</strong>随机效应。包括考虑分组因子<strong class="lj jd"> Ind </strong>(个体 ID)的随机效应截距，我们还需要为<strong class="lj jd"> lmer </strong>函数使用特殊的语法<strong class="lj jd"> (1 | Ind) </strong>。现在，我们可以使用<strong class="lj jd">限制最大似然(REML) </strong>方法来拟合 LMM 模型，为此，我们指定<strong class="lj jd"> REML = TRUE </strong>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/c42906418d98d6427a6498d95375386b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMuOWFLBdJwUS5YDdznvTg.png"/></div></div></figure><p id="3fde" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意<strong class="lj jd">共有的</strong>和<strong class="lj jd">剩余标准差</strong>、<strong class="lj jd"> 8.155 </strong>和<strong class="lj jd"> 6.0 </strong>，我们在上一篇文章中分别表示为<strong class="lj jd"> <em class="nb"> σs </em> </strong>和<strong class="lj jd"> <em class="nb"> σ </em> </strong>。我们将在稍后为 LMM 实施 REML 解决方案时重现这些值。正如我们在之前的<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4">LMM _ 从头开始</a>教程中看到的，知道数据点 y11，y12，y21，y22 的坐标，方程中的前两项。(12)可以计算为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/b9a553bcab96b24a2f104d9afbda9980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EY5C3L9duHzdnlal1V_-Q.png"/></div></div></figure><p id="bf0e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">等式中的第三项。(12)也可以从<strong class="lj jd">解析导出</strong>，因为我们既知道<strong class="lj jd"> X </strong>、<strong class="lj jd">设计矩阵</strong>，也知道逆方差协方差矩阵<strong class="lj jd">σ</strong>y。下面我们呈现一个来自<a class="ae md" href="https://www.maplesoft.com/" rel="noopener ugc nofollow" target="_blank"> Maple 软件</a>的截图，其中显示等式(1)中的第三项。(12)采取的形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/be9c0a4ce3ecb2e1457f4ade15c5f51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDZ4vZdXgx6vA5S364IYwg.png"/></div></div></figure><p id="e01b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，等式中的第三项。(12)有以下简单的表达式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2b4122c9c9af35dae95705b377235993.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*F1m_g7AMRmjJJIJ1PCPmgQ.png"/></div></figure><p id="e69f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们现在可以在限制最大似然(REML)近似中最小化对数似然函数，即当对数似然等式。(12)函数不包含任何关于平均值<em class="nb"> β，</em>的信息，即这<strong class="lj jd">不再是优化</strong>的参数，而是具有固定/估计值<em class="nb"> β </em> 1=6，<em class="nb"> β </em> 2=15.5，这是<a class="ae md" rel="noopener" target="_blank" href="/linear-mixed-model-from-scratch-f29b2e45f0a4">之前找到的</a>。现在，一切都准备好执行对数似然函数 Eq 的<strong class="lj jd">数值最小化</strong>。(12)关于 REML 近似中的<em class="nb"> σs </em>和<em class="nb"> σ </em>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/becf86f90c27f8cc49a7a9ffbace7f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kRlH6jkmahCG13CNvhHYg.png"/></div></div></figure><p id="631c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过对数似然函数的最小化，我们获得了<strong class="lj jd"><em class="nb"/>= 6.00</strong>和<strong class="lj jd"><em class="nb">【σs】</em>= 8.155</strong>，这正是我们通过 lmer <strong class="lj jd"> </strong>函数以及<strong class="lj jd"> REML = TRUE </strong>获得的标准偏差。我们复制了<strong class="lj jd">随机效应残差方差<em class="nb"> σ </em>，并在本帖中分享了<strong class="lj jd">最大似然(REML =假)</strong>和<strong class="lj jd">受限最大似然(REML =真)</strong>的数据点方差<em class="nb"> σs </em> </strong>。而且，我们已经用 R 从头开始推导并编码了它，干得好！</p><h1 id="6988" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="c7a5" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">在本文中，我们了解到最大似然(ML) <strong class="lj jd">方差估计量是有偏的，特别是对于高维数据</strong>，由于使用了未知的均值估计量。限制最大似然(REML)通过在最小化对数似然函数之前首先移除关于均值估计器的所有信息来解决这个问题。我们成功地复制了 lmer 报告的 REML = TRUE 的方差分量，并使用 r 从头推导和编码了 REML。</p><p id="62c4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下面的评论中，让我知道哪些来自生命科学的分析技术对你来说似乎是 T42 特别神秘的，我会在以后的文章中尽量涉及它们。检查我的<a class="ae md" href="https://github.com/NikolayOskolkov/REML" rel="noopener ugc nofollow" target="_blank"> Github </a>上的帖子中的代码。请在媒体<a class="nt nu ep" href="https://medium.com/u/8570b484f56c?source=post_page-----78cf79bef2cf--------------------------------" rel="noopener" target="_blank"> Nikolay Oskolkov </a>关注我，在 Twitter @NikolayOskolkov 上关注我，并在<a class="ae md" href="http://linkedin.com/in/nikolay-oskolkov-abb321186?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上关注我。在下一篇文章中，我们将讨论<strong class="lj jd">如何在 UMAP 空间聚集</strong>，敬请关注。</p></div></div>    
</body>
</html>