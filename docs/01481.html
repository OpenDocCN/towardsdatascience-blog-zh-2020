<html>
<head>
<title>Machine Learning: Step-By-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:循序渐进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-step-by-step-6fbde95c455a?source=collection_archive---------1-----------------------#2020-02-10">https://towardsdatascience.com/machine-learning-step-by-step-6fbde95c455a?source=collection_archive---------1-----------------------#2020-02-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="21cd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用随机森林、PCA和超参数调整的Python机器学习分类分步指南——带代码！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0edb33adf55baf277d2551d5ffb76cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cNqkriZyJyNfWavE.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.quora.com/How-should-I-start-in-the-field-of-machine-learning-and-AI-with-knowledge-of-programming-and-algorithms" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="ab5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为数据科学家，我们有许多选择来创建分类模型。最流行和可靠的方法之一是使用随机森林。我们可以对<a class="ae kv" rel="noopener" target="_blank" href="/understanding-random-forest-58381e0602d2"> <strong class="ky ir">随机森林</strong> </a> <strong class="ky ir"> </strong>执行<a class="ae kv" rel="noopener" target="_blank" href="/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"> <strong class="ky ir">超参数调整</strong> </a>来尝试优化模型的性能。</p><p id="efcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在将我们的数据拟合到模型之前，尝试<a class="ae kv" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"><strong class="ky ir">【PCA】</strong></a>也是常见的做法。但是我们为什么还要增加这一步呢？随机森林的全部意义不就是帮助我们容易地解释特征重要性吗？</p><p id="80dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，当我们分析随机森林模型的“特征重要性”时，PCA会使解释每个“特征”变得更加困难。但是，PCA执行降维，这可以减少随机森林要处理的要素数量，因此PCA可能有助于加快随机森林模型的训练。请注意，计算成本是随机森林的最大缺点之一(运行模型可能需要很长时间)。主成分分析会变得非常重要，尤其是在处理成百上千个预测特征的时候。因此，如果最重要的事情是简单地拥有性能最好的模型，并且可以牺牲解释特征重要性，那么PCA可能是有用的尝试。</p><p id="e65b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们开始我们的例子。我们将使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Scikit-learn“乳腺癌”数据集。</strong> </a>我们将创建3个模型，并相互比较它们的性能:</p><ul class=""><li id="003c" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">1.随机森林</li><li id="5dca" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">2.PCA降维的随机森林</li><li id="96cd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">3.PCA降维的随机森林&amp;超参数调整</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c5d5" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">1.输入数据</h1><p id="e861" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">首先，我们加载数据并创建一个数据框架。由于这是来自Scikit-learn的预先清理过的“玩具”数据集，我们可以继续进行建模过程了。但是，作为最佳实践，我们应该始终做到以下几点:</p><ul class=""><li id="7d49" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">使用<strong class="ky ir"> df.head() </strong>浏览新的数据帧，确保它看起来像预期的那样。</li><li id="8296" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用<strong class="ky ir"> df.info() </strong>来了解每一列中的数据类型和计数。您可能需要根据需要转换数据类型。</li><li id="0129" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用<strong class="ky ir"> df.isna() </strong>来确保没有NaN值。您可能需要根据需要估算值或删除行。</li><li id="1390" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用<strong class="ky ir"> df.describe() </strong>了解每一列的最小值、最大值、平均值、中值、标准偏差和四分位间距。</li></ul><p id="d397" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">名为<strong class="ky ir">“cancer”</strong>的列是我们希望使用模型预测的目标变量。<strong class="ky ir">“0”表示“没有癌症”，“1”表示“癌症”。</strong></p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="8a54" class="np mo iq nl b gy nq nr l ns nt">import pandas as pd<br/>from sklearn.datasets import load_breast_cancer</span><span id="55b2" class="np mo iq nl b gy nu nr l ns nt">columns = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']</span><span id="f12b" class="np mo iq nl b gy nu nr l ns nt">dataset = load_breast_cancer()<br/>data = pd.DataFrame(dataset['data'], columns=columns)<br/>data['cancer'] = dataset['target']</span><span id="cfc5" class="np mo iq nl b gy nu nr l ns nt">display(data.head())<br/>display(data.info())<br/>display(data.isna().sum())<br/>display(data.describe())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/fb475e03edef1b03cb3e35f823cf043d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrHaG0CQ1O1lIYM-hG36DA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上面是乳腺癌数据框架的一部分。每一行都有关于患者的观察结果。名为“癌症”的最后一列是我们试图预测的目标变量。<strong class="bd nw"> 0表示“没有癌症”，1表示“癌症”。</strong></p></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="a058" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">2.训练/测试分割</h1><p id="2d4b" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，我们使用Scikit-learn的“train_test_split”函数分割数据。我们希望为模型提供尽可能多的数据来进行训练。然而，我们还想确保我们有足够的数据让模型进行自我测试。通常，随着数据集中行数的增加，我们可以提供给定型集的数据就越多。</p><p id="e970" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果我们有数百万行，我们可以有90%的训练/ 10%的测试分割。然而，我们的数据集只有569行，这对于训练或测试来说并不是一个非常大的数据集。因此，为了公平对待训练和测试，我们将数据分成50%训练和50%测试。我们设置<strong class="ky ir">分层=y </strong>以确保训练集和测试集与原始数据集具有相同比例的0和1。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="deb2" class="np mo iq nl b gy nq nr l ns nt">from sklearn.model_selection import train_test_split</span><span id="eea7" class="np mo iq nl b gy nu nr l ns nt">X = data.drop('cancer', axis=1)  <br/>y = data['cancer'] <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state = 2020, stratify=y)</span></pre></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="738c" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">3.缩放数据</h1><p id="c923" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在建模之前，我们需要通过缩放来“居中”和“标准化”我们的数据。我们按比例控制，因为不同的变量在不同的比例上测量。我们进行缩放，以便每个预测者可以在决定重要性时相互“公平竞争”。<a class="ae kv" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">见此文。</a>我们还将“y_train”从Pandas“Series”对象转换为NumPy数组，以便模型稍后接受目标训练数据。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="8d07" class="np mo iq nl b gy nq nr l ns nt">import numpy as np<br/>from sklearn.preprocessing import StandardScaler</span><span id="18c7" class="np mo iq nl b gy nu nr l ns nt">ss = StandardScaler()<br/>X_train_scaled = ss.fit_transform(X_train)<br/>X_test_scaled = ss.transform(X_test)<br/>y_train = np.array(y_train)</span></pre></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="e428" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">4.适合“基线”随机森林模型</h1><p id="7510" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在我们创建一个“基线”随机森林模型。该模型使用所有预测功能以及在<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn随机森林分类器文档中定义的默认设置。</a>首先，我们对模型进行实例化，并对缩放后的数据进行拟合。我们可以根据我们的训练数据来衡量模型的准确性。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="78f8" class="np mo iq nl b gy nq nr l ns nt">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import recall_score</span><span id="59d6" class="np mo iq nl b gy nu nr l ns nt">rfc = RandomForestClassifier()<br/>rfc.fit(X_train_scaled, y_train)<br/>display(rfc.score(X_train_scaled, y_train))</span><span id="2977" class="np mo iq nl b gy nu nr l ns nt"># 1.0</span></pre><p id="604f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想知道哪些特征对随机森林模型预测乳腺癌最重要，我们可以通过调用<strong class="ky ir"> "feature_importances_" </strong>方法来可视化和量化重要性:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="5bc8" class="np mo iq nl b gy nq nr l ns nt">feats = {}<br/>for feature, importance in zip(data.columns, rfc_1.feature_importances_):<br/>    feats[feature] = importance</span><span id="1e21" class="np mo iq nl b gy nu nr l ns nt">importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})<br/>importances = importances.sort_values(by='Gini-Importance', ascending=False)<br/>importances = importances.reset_index()<br/>importances = importances.rename(columns={'index': 'Features'})</span><span id="2008" class="np mo iq nl b gy nu nr l ns nt">sns.set(font_scale = 5)<br/>sns.set(style="whitegrid", color_codes=True, font_scale = 1.7)<br/>fig, ax = plt.subplots()<br/>fig.set_size_inches(30,15)<br/>sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='skyblue')<br/>plt.xlabel('Importance', fontsize=25, weight = 'bold')<br/>plt.ylabel('Features', fontsize=25, weight = 'bold')<br/>plt.title('Feature Importance', fontsize=25, weight = 'bold')</span><span id="0f73" class="np mo iq nl b gy nu nr l ns nt">display(plt.show())<br/>display(importances)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/b14028bc00b67667318a962c8ac09786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sMW0PGuA5pksfCNPHwjTLA.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8ff2249f356b90ce671956411d315752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEDws5dV-iaF3IPV-Gcbrg.jpeg"/></div></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="3b4c" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">5.主成分分析</h1><p id="4f39" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，我们如何改进我们的基线模型？使用降维，我们可以用更少的变量逼近原始数据集，同时降低运行模型的计算能力。使用主成分分析，我们可以研究这些特征的累积解释方差比率，以了解哪些特征解释了数据中的最大方差。</p><p id="65bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们实例化PCA函数，并设置我们想要考虑的组件(特征)的数量。我们将它设置为“30 ”,以查看所有生成组件的解释差异，然后决定在哪里进行切割。然后，我们将缩放后的X_train数据“拟合”到PCA函数。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="d606" class="np mo iq nl b gy nq nr l ns nt">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.decomposition import PCA</span><span id="1adb" class="np mo iq nl b gy nu nr l ns nt">pca_test = PCA(n_components=30)<br/>pca_test.fit(X_train_scaled)</span><span id="b673" class="np mo iq nl b gy nu nr l ns nt">sns.set(style='whitegrid')<br/>plt.plot(np.cumsum(pca_test.explained_variance_ratio_))<br/>plt.xlabel('number of components')<br/>plt.ylabel('cumulative explained variance')<br/>plt.axvline(linewidth=4, color='r', linestyle = '--', x=10, ymin=0, ymax=1)<br/>display(plt.show())</span><span id="758f" class="np mo iq nl b gy nu nr l ns nt">evr = pca_test.explained_variance_ratio_<br/>cvr = np.cumsum(pca_test.explained_variance_ratio_)</span><span id="f5c4" class="np mo iq nl b gy nu nr l ns nt">pca_df = pd.DataFrame()<br/>pca_df['Cumulative Variance Ratio'] = cvr<br/>pca_df['Explained Variance Ratio'] = evr<br/>display(pca_df.head(10))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/6bf3fcda833457ca1f357b2be7f1193f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3mSOuXVzu2GpE25Xs_B-g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该图显示，超过10个成分后，我们没有获得太多的解释方差。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/a85b5907f811c8f36d50e899559b0329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n74vNqQnSawLsDDqlNThsg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该数据帧显示了累积方差比(解释了数据的总方差)和解释方差比(每个PCA成分解释了数据的总方差)。</p></figure><p id="9e7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">看上面的数据框架，当我们使用主成分分析将30个预测变量减少到10个成分时，我们仍然可以解释超过95%的方差。</strong>其他20个成分解释了不到5%的方差，因此我们可以剔除它们。使用这个逻辑，我们将使用PCA将X_train和X_test的组件数量从30个减少到10个。我们将把这些重新创建的“降维”数据集分配给“X_train_scaled_pca”和“X_test_scaled_pca”。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="0a16" class="np mo iq nl b gy nq nr l ns nt">pca = PCA(n_components=10)<br/>pca.fit(X_train_scaled)</span><span id="d492" class="np mo iq nl b gy nu nr l ns nt">X_train_scaled_pca = pca.transform(X_train_scaled)<br/>X_test_scaled_pca = pca.transform(X_test_scaled)</span></pre><p id="97c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个分量都是原始变量与相应“权重”的线性组合。通过创建数据帧，我们可以看到每个PCA成分的“权重”。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="4c88" class="np mo iq nl b gy nq nr l ns nt">pca_dims = []<br/>for x in range(0, len(pca_df)):<br/>    pca_dims.append('PCA Component {}'.format(x))</span><span id="cf8c" class="np mo iq nl b gy nu nr l ns nt">pca_test_df = pd.DataFrame(pca_test.components_, columns=columns, index=pca_dims)<br/>pca_test_df.head(10).T</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/009612a2200af0bedbf11b3c452e95ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMeiF1FMouLv_d6qu7ZYbw.jpeg"/></div></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="9904" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">6.PCA后适合“基线”随机森林模型</h1><p id="dbbf" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，我们可以将我们的X_train_scaled_pca和y_train数据拟合到另一个“基线”随机森林模型，以查看我们是否对模型的预测有任何改进。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="84a0" class="np mo iq nl b gy nq nr l ns nt">rfc = RandomForestClassifier()<br/>rfc.fit(X_train_scaled_pca, y_train)</span><span id="23b9" class="np mo iq nl b gy nu nr l ns nt">display(rfc.score(X_train_scaled_pca, y_train))</span><span id="b1bd" class="np mo iq nl b gy nu nr l ns nt"># 1.0</span></pre></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="370d" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">7.第一轮超参数调整:RandomSearchCV</h1><p id="68f0" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在执行PCA之后，我们还可以尝试一些<a class="ae kv" rel="noopener" target="_blank" href="/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">超参数调整</a>来调整我们的随机森林，以获得更好的预测性能。超参数可以被认为是模型的“设置”。一个数据集的完美设置对于另一个数据集是不一样的，所以我们必须“调整”模型。</p><p id="ae68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们可以从RandomSearchCV开始考虑广泛的值。随机森林的所有超参数都可以在<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn随机森林分类器文档中找到。</a></p><p id="00ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们生成一个“param_dist ”,其中包含一系列值，用于尝试每个超参数。RandomSearchCV被实例化，我们的随机森林模型首先被传入，然后是我们的“param_dist”、要尝试的迭代次数以及要执行的<a class="ae kv" rel="noopener" target="_blank" href="/cross-validation-a-beginners-guide-5b8ca04962cd">交叉验证的次数</a>。</p><p id="a0ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">“详细”</strong>超参数在模型运行时为您提供或多或少的输出(比如状态更新)。<strong class="ky ir"> "n_jobs" </strong>超参数让您决定要使用多少个处理器内核来运行模型。设置“n_jobs = -1”将使模型运行得最快，因为它使用了您所有的计算机核心。</p><p id="eb3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将调整这些超参数:</p><ul class=""><li id="b4ad" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> n_estimators: </strong>我们的随机森林中“树”的数量。</li><li id="5d48" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> max_features: </strong>每次分割的特征数量。</li><li id="849f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> max_depth: </strong>每棵树可以拥有的最大“分裂”数。</li><li id="61d7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> min_samples_split: </strong>树的节点可以自我分裂之前所需的最小观察次数。</li><li id="ea9a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> min_samples_leaf: </strong>每棵树末端的每片叶子所需的最小观察次数。</li><li id="4506" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> bootstrap: </strong>是否使用bootstrap为随机森林中的每棵树提供数据。(Bootstrapping是从数据集中随机取样并替换。)</li></ul><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="efea" class="np mo iq nl b gy nq nr l ns nt">from sklearn.model_selection import RandomizedSearchCV</span><span id="3408" class="np mo iq nl b gy nu nr l ns nt">n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]</span><span id="bb26" class="np mo iq nl b gy nu nr l ns nt">max_features = ['log2', 'sqrt']</span><span id="fc01" class="np mo iq nl b gy nu nr l ns nt">max_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]</span><span id="5ac1" class="np mo iq nl b gy nu nr l ns nt">min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]</span><span id="79b9" class="np mo iq nl b gy nu nr l ns nt">min_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]</span><span id="d1c0" class="np mo iq nl b gy nu nr l ns nt">bootstrap = [True, False]</span><span id="1c5b" class="np mo iq nl b gy nu nr l ns nt">param_dist = {'n_estimators': n_estimators,<br/>               'max_features': max_features,<br/>               'max_depth': max_depth,<br/>               'min_samples_split': min_samples_split,<br/>               'min_samples_leaf': min_samples_leaf,<br/>               'bootstrap': bootstrap}</span><span id="1e6d" class="np mo iq nl b gy nu nr l ns nt">rs = RandomizedSearchCV(rfc_2, <br/>                        param_dist, <br/>                        n_iter = 100, <br/>                        cv = 3, <br/>                        verbose = 1, <br/>                        n_jobs=-1, <br/>                        random_state=0)</span><span id="6f9c" class="np mo iq nl b gy nu nr l ns nt">rs.fit(X_train_scaled_pca, y_train)<br/>rs.best_params_</span><span id="3445" class="np mo iq nl b gy nu nr l ns nt"># {'n_estimators': 700,<br/># 'min_samples_split': 2,<br/># 'min_samples_leaf': 2,<br/># 'max_features': 'log2',<br/># 'max_depth': 11,<br/># 'bootstrap': True}</span></pre><p id="094b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在n_iter = 100和cv = 3的情况下，我们创建了300个随机森林模型，</strong>随机采样上面输入的超参数的组合。我们可以调用<strong class="ky ir"> "best_params_" </strong>来获得性能最好的模型的参数(显示在上面代码框的底部)。然而，这个阶段的“最佳参数”可能不会给我们最好的洞察力来获得一系列参数，以尝试下一轮的超参数调整。为了获得接下来要尝试的一系列值，我们可以很容易地获得RandomSearchCV结果的数据框架。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="45a3" class="np mo iq nl b gy nq nr l ns nt">rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)<br/>rs_df = rs_df.drop([<br/>            'mean_fit_time', <br/>            'std_fit_time', <br/>            'mean_score_time',<br/>            'std_score_time', <br/>            'params', <br/>            'split0_test_score', <br/>            'split1_test_score', <br/>            'split2_test_score', <br/>            'std_test_score'],<br/>            axis=1)<br/>rs_df.head(10)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/19b73ab322060ad667e121f660310a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3Km0cD1xcQx_rN5UXZfWA.png"/></div></div></figure><p id="0e13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们在x轴上创建每个超参数的条形图，以及在每个值上建立的模型的平均得分，以查看平均而言哪些值是最成功的:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="9499" class="np mo iq nl b gy nq nr l ns nt">fig, axs = plt.subplots(ncols=3, nrows=2)<br/>sns.set(style="whitegrid", color_codes=True, font_scale = 2)<br/>fig.set_size_inches(30,25)</span><span id="8ae3" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_n_estimators', y='mean_test_score', data=rs_df, ax=axs[0,0], color='lightgrey')<br/>axs[0,0].set_ylim([.83,.93])axs[0,0].set_title(label = 'n_estimators', size=30, weight='bold')</span><span id="6d56" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')<br/>axs[0,1].set_ylim([.85,.93])axs[0,1].set_title(label = 'min_samples_split', size=30, weight='bold')</span><span id="e16f" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')<br/>axs[0,2].set_ylim([.80,.93])axs[0,2].set_title(label = 'min_samples_leaf', size=30, weight='bold')</span><span id="6563" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')<br/>axs[1,0].set_ylim([.88,.92])axs[1,0].set_title(label = 'max_features', size=30, weight='bold')</span><span id="b55b" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')<br/>axs[1,1].set_ylim([.80,.93])axs[1,1].set_title(label = 'max_depth', size=30, weight='bold')</span><span id="c7d2" class="np mo iq nl b gy nu nr l ns nt">sns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')<br/>axs[1,2].set_ylim([.88,.92])<br/>axs[1,2].set_title(label = 'bootstrap', size=30, weight='bold')</span><span id="e0a4" class="np mo iq nl b gy nu nr l ns nt">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/044a230d1c89d1336aae14775f091a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uV7LMeAFQ_zmNhg00FFUcw.jpeg"/></div></div></figure><p id="8395" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看上面的图表，我们可以了解每个超参数的每个值平均表现如何。</p><p id="081d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> n_estimators: </strong> 300，500，700似乎平均得分最高。</p><p id="35a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> min_samples_split: </strong>像2和7这样的较小值似乎得分较高。23岁也有高分。我们可以尝试2以上的几个值，23左右的几个值。</p><p id="55fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">min_samples_leaf: 较小的值似乎与较高的分数相关…我们可以尝试2-7之间的值。</p><p id="31c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">max _ features:</strong>“sqrt”平均分最高。</p><p id="62d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> max_depth: </strong>没有明确的模式，但是数值2，3，7，11，15似乎做的不错。</p><p id="abbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> bootstrap: </strong>“假”平均分最高。</p><p id="cdaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，现在我们可以将这些见解^，并进入第二轮超参数调整，以进一步缩小我们的选择。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="4679" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">8.第二轮超参数调优:GridSearchCV</h1><p id="203c" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在使用RandomSearchCV之后，我们可以使用GridSearchCV对我们的最佳超参数进行更精确的搜索。超参数是相同的，但是现在我们使用GridSearchCV执行更“详尽”的搜索。<strong class="ky ir">在GridSearchCV中，尝试超参数值的每一个组合，这比RandomSearchCV需要更多的计算能力，在RandomSearchCV中，我们可以直接控制我们想要尝试的迭代次数。</strong>例如，为我们的6个参数中的每一个仅搜索10个不同的参数值，使用三重交叉验证将需要10⁶ x 3或3，000，000次模型拟合！这就是为什么我们在使用RandomSearchCV之后执行GridSearchCV，以帮助我们首先缩小搜索范围。</p><p id="122e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，使用我们从随机搜索CV中了解到的信息，让我们插入每个超参数的平均最佳表现范围:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="1aa0" class="np mo iq nl b gy nq nr l ns nt">from sklearn.model_selection import GridSearchCV</span><span id="362b" class="np mo iq nl b gy nu nr l ns nt">n_estimators = [300,500,700]<br/>max_features = ['sqrt']<br/>max_depth = [2,3,7,11,15]<br/>min_samples_split = [2,3,4,22,23,24]<br/>min_samples_leaf = [2,3,4,5,6,7]<br/>bootstrap = [False]</span><span id="f804" class="np mo iq nl b gy nu nr l ns nt">param_grid = {'n_estimators': n_estimators,<br/>               'max_features': max_features,<br/>               'max_depth': max_depth,<br/>               'min_samples_split': min_samples_split,<br/>               'min_samples_leaf': min_samples_leaf,<br/>               'bootstrap': bootstrap}</span><span id="71b4" class="np mo iq nl b gy nu nr l ns nt">gs = GridSearchCV(rfc_2, param_grid, cv = 3, verbose = 1, n_jobs=-1)<br/>gs.fit(X_train_scaled_pca, y_train)<br/>rfc_3 = gs.best_estimator_<br/>gs.best_params_</span><span id="3625" class="np mo iq nl b gy nu nr l ns nt"># {'bootstrap': False,<br/># 'max_depth': 7,<br/># 'max_features': 'sqrt',<br/># 'min_samples_leaf': 3,<br/># 'min_samples_split': 2,<br/># 'n_estimators': 500}</span></pre><p id="209f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在^，我们正在对3x 1 x 5x 6 x 6 x 1 = 540个模型拟合执行三重交叉验证，总共有1，620个模型拟合！</strong>现在，在执行了RandomizedSearchCV和GridSearchCV之后，我们可以调用“best_params_”来获得一个最佳模型来尝试和预测我们的数据(显示在上面代码框的底部)。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="093c" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">9.根据测试数据评估模型的性能</h1><p id="b8b5" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，我们可以评估我们根据测试数据建立的每个模型。请记住，我们正在测试3种模型:</p><ul class=""><li id="302a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">1.基线随机森林</li><li id="eb07" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">2.PCA降维的基线随机森林</li><li id="4fc6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">3.使用PCA降维的基线随机森林和超参数调整</li></ul><p id="7dd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们生成每个模型的预测:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="b8bb" class="np mo iq nl b gy nq nr l ns nt">y_pred = rfc.predict(X_test_scaled)<br/>y_pred_pca = rfc.predict(X_test_scaled_pca)<br/>y_pred_gs = gs.best_estimator_.predict(X_test_scaled_pca)</span></pre><p id="7e43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们为每个模型创建混淆矩阵，看看每个模型预测乳腺癌的能力如何:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="2907" class="np mo iq nl b gy nq nr l ns nt">from sklearn.metrics import confusion_matrix</span><span id="a0a3" class="np mo iq nl b gy nu nr l ns nt">conf_matrix_baseline = pd.DataFrame(confusion_matrix(y_test, y_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])</span><span id="e287" class="np mo iq nl b gy nu nr l ns nt">conf_matrix_baseline_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_pca), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])</span><span id="dd01" class="np mo iq nl b gy nu nr l ns nt">conf_matrix_tuned_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])</span><span id="8ec5" class="np mo iq nl b gy nu nr l ns nt">display(conf_matrix_baseline)<br/>display('Baseline Random Forest recall score', recall_score(y_test, y_pred))<br/>display(conf_matrix_baseline_pca)<br/>display('Baseline Random Forest With PCA recall score', recall_score(y_test, y_pred_pca))<br/>display(conf_matrix_tuned_pca)<br/>display('Hyperparameter Tuned Random Forest With PCA Reduced Dimensionality recall score', recall_score(y_test, y_pred_gs))</span></pre><p id="0ddf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面，我们有我们劳动的最终成果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5edc60e5b2062abeb8396c0ec1a75eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEbaFiPe7nDbHNTGhmpxug.png"/></div></div></figure><p id="a7bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们使用召回作为我们的性能指标，因为我们正在处理癌症诊断，并且最关心的是最小化我们模型中的假阴性预测错误。</strong></p><p id="9641" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到这个^，看起来我们的基线随机森林模型做得最好，具有最高的召回分数<strong class="ky ir"> 94.97% </strong>。给定我们的测试数据集，基线模型正确预测了总共179名实际患有癌症的人中有170名患者患有癌症。</p><p id="1ed1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个案例研究提出了一个重要的注意事项:有时，在PCA之后，或者甚至在广泛的超参数调优之后，一个调优的模型的性能可能不如一个普通的老式“普通”模型。但是尝试是很重要的。你永远不知道哪种模式会做得最好，除非你都尝试过。就预测癌症而言，模型越好，就能拯救越多的生命。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="2379" class="np mo iq bd mp oe of dn mt og oh dp mx lf oi oj mz lj ok ol nb ln om on nd oo bi translated">希望这有所帮助！快乐造型！</h2></div></div>    
</body>
</html>