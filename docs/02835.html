<html>
<head>
<title>Predicting survivors of Titanic</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测泰坦尼克号的幸存者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-survivors-of-titanic-e7280822b00b?source=collection_archive---------35-----------------------#2020-03-18">https://towardsdatascience.com/predicting-survivors-of-titanic-e7280822b00b?source=collection_archive---------35-----------------------#2020-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f1fe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谁会在海难中幸存？我们可以用机器学习来回答这样的问题。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2824b89a71aa1778638200b143bb87ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-DEKilyY0qdhx2Yvv-qCw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯图亚特(1843-1923)/公共领域</p></figure><p id="8809" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">1912年4月15日凌晨，由白星航运公司运营的英国客轮泰坦尼克号在从南安普敦到纽约的处女航中撞上冰山后沉没在北大西洋。据估计，船上2224名乘客和船员中，超过1500人死亡，这使得沉船成为现代史上最致命的和平时期商业海上灾难之一。</p><p id="67c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在想要的是创建一个机器学习模型，能够预测谁将在泰坦尼克号的沉船中幸存。为此，我们将使用<a class="ae lv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">这个来自Kaggle的</a>数据集——这个任务也有一个Kaggle竞赛，这是一个开始Kaggle竞赛的好地方。该数据集包含关于乘客的以下信息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/22533c13e2355db062db2bcbdcaccbc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G17MThdg4KPG5JX7KOzV1Q.png"/></div></div></figure><h1 id="01e3" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">探索性分析</h1><p id="cf9b" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">现在，在对这个数据集进行任何机器学习之前，做一些探索性分析并看看我们的数据看起来如何是一个好的做法。我们也想一路准备/打扫。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="47bd" class="mz ly it mv b gy na nb l nc nd">import numpy as np<br/>import pandas as pd</span><span id="cad7" class="mz ly it mv b gy ne nb l nc nd">df = pd.read_csv('train.csv')<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/0cd869b7a34acc2f57cb1716905a21a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Qa4CUZsufHO87iY31cXIA.png"/></div></div></figure><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="13d4" class="mz ly it mv b gy na nb l nc nd">df.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/3efb9426a49c72aca550a302f56597a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHtkOw2-FZ1w3sMkK35fug.png"/></div></div></figure><p id="5e92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在数据中看到了什么？我们看到<em class="lu"> PassengerId </em>列有一个与每个乘客相关联的唯一编号。机器学习算法可以很容易地使用这个字段来记住每个乘客的结果，而没有概括的能力。此外，还有一个<em class="lu">名字</em>变量，我个人认为它不能以任何方式决定一个人是否幸存。所以，我们将删除这两个变量。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a8a7" class="mz ly it mv b gy na nb l nc nd">del df['PassengerId']<br/>del df['Name']</span></pre><p id="d429" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们看看我们是否有丢失的值，以及它们有多少。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a56a" class="mz ly it mv b gy na nb l nc nd">df.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/17ad95c00ec5f5d5c538f8e6ca94bd17.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*nTrgAO_VmMt9BBvtYSw3Qg.png"/></div></figure><p id="92c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在891个样本中，687个样本的<em class="lu">座舱</em>变量为空值。此变量缺少太多值，无法使用它。我们会删除它。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="aa1f" class="mz ly it mv b gy na nb l nc nd">del df['Cabin']</span></pre><p id="1a61" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不想删除其他两列，因为它们很少缺少值，我们将增加它们。<br/>对于<em class="lu">装载的</em>变量，由于它是一个分类变量，我们将查看每个类别的计数，并用拥有最多项目的类别替换2个空值。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="e279" class="mz ly it mv b gy na nb l nc nd">df['Embarked'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/eb6d312e5c8c8eee0ffd3d9f20b3c9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*qF9ioG2M-Ecxo2E8TnY1Zg.png"/></div></div></figure><p id="6ba2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">oaked最常见的值是“S ”,因此我们将使用它来替换空值。</em></p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="450b" class="mz ly it mv b gy na nb l nc nd">df['Embarked'].loc[pd.isnull(df['Embarked'])] = 'S'</span></pre><p id="7ac5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于年龄，我们将用平均年龄替换缺失值。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="6fa6" class="mz ly it mv b gy na nb l nc nd">mean_age_train = np.mean(df['Age'].loc[pd.isnull(df['Age']) == False].values)<br/>df['Age'].loc[pd.isnull(df['Age'])] = mean_age_train</span></pre><p id="1b12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们应该存储我们从训练数据中了解到的一切，例如<em class="lu">开始</em>上课的频率或<em class="lu">年龄</em>的平均值，因为我们将在进行预测时使用这些信息，以防测试数据中也有缺失值。我们还将计算并存储<em class="lu">费用</em>平均值，以备测试数据需要。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="852b" class="mz ly it mv b gy na nb l nc nd">mean_fare_train = np.mean(df['Fare'].loc[pd.isnull(df['Fare']) == False].values)</span></pre><p id="c4e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们去掉了空值，现在让我们看看接下来会发生什么。</p><h1 id="143f" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">顺序编码</h1><p id="ae14" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">机器学习算法对文本不起作用(至少不能直接起作用)，我们需要用数字转换所有字符串。我们将使用序数编码进行这种转换。序号编码是一种通过给每个类别分配一个数字来将分类变量转换为数字的方法。我们将使用Scikit-Learn的<code class="fe nj nk nl mv b">OrdinalEncoder</code>对变量<em class="lu">性别</em>、<em class="lu">票</em>、<em class="lu">上船</em>进行这种转换。<br/>在此之前，我们将备份当前的数据格式以备将来使用。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="eecc" class="mz ly it mv b gy na nb l nc nd">df_bkp = df.copy()<br/>from sklearn.preprocessing import OrdinalEncoder</span><span id="518e" class="mz ly it mv b gy ne nb l nc nd">df['Sex'] = OrdinalEncoder().fit_transform(df['Sex'].values.reshape((-1, 1)))<br/>df['Ticket'] = OrdinalEncoder().fit_transform(df['Ticket'].values.reshape((-1, 1)))<br/>df['Embarked'] = OrdinalEncoder().fit_transform(df['Embarked'].values.reshape((-1, 1)))</span></pre><h1 id="0d6b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">形象化</h1><p id="7a47" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">现在，我们将可视化我们的数据，看看我们的变量在存活和未存活类中的分布如何变化。<br/>下面是我们数据集中每个变量的直方图(以及生成它的代码)，左边是幸存人员的子集，右边是未幸存人员的子集。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="ce65" class="mz ly it mv b gy na nb l nc nd">import matplotlib.pyplot as plt<br/>from IPython.display import display, Markdown</span><span id="5996" class="mz ly it mv b gy ne nb l nc nd">def show(txt):<br/>    # this function is for printing markdown in jupyter notebook<br/>    display(Markdown(txt))</span><span id="46c5" class="mz ly it mv b gy ne nb l nc nd">for i in range(1, 9):<br/>    show(f'### {df.columns[i]}')<br/>    f, (survived, not_survived) = plt.subplots(1, 2, sharey=True, figsize=(18, 8))<br/>    survived.hist(df.iloc[np.where(df['Survived'] == 1)[0], i])<br/>    survived.set_title('Survived')</span><span id="27f3" class="mz ly it mv b gy ne nb l nc nd">not_survived.hist(df.iloc[np.where(df['Survived'] == 0)[0], i])<br/>    not_survived.set_title('Not Survived')<br/>    plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/dc206a67eb1641f2d7b520614daf4988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_lyaEFguYGgJJ36XTHvBw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/0f4af56830fc365e5afbe4b62008b839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRYqP15HKFjXosUYWm0HAg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/e7280e45edf25d54abf2d7582b202d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gr8FR42xu8f_udIFL5r11Q.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/2e8f2e7fb6988a70115b3c80b845e10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8QIk1JwUiNz9GcNbD7H_w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/8e574d1931f496cae60239f383035a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0eH9HEZndO75mod50Ly_w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/7f440138a5371e94f0157ca232922975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D07XWP-VzcWYpXm-qmoG8w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/49506ed0d3d487ed4a5e43fd26146d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXAsVL68UdraJc6Cqm_JYQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/67f00411d07740f2b3b7da54691bf20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sr3a4Ng2bu6m7CSZM61vzA.png"/></div></div></figure><h1 id="e5ef" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">对这种数据格式运行机器学习</h1><p id="613d" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">现在，我们将在数据集上尝试几种机器学习方法，看看我们会得到什么结果。我们将使用的机器学习方法有:</p><ul class=""><li id="beb5" class="no np it la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">逻辑回归</li><li id="8c31" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">支持向量机</li><li id="e6a1" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">决策图表</li><li id="2ac5" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">k个最近邻居</li><li id="518a" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">多层感知器</li></ul><p id="59c2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用Scikit-Learn的<code class="fe nj nk nl mv b">cross_val_score</code>进行(5重)交叉验证，而不是选择一个固定的验证集来估计测试集的准确性，sci kit-Learn的<code class="fe nj nk nl mv b">cross_val_score</code>返回一个数组，其中包含每次交叉验证迭代的分数。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="7b21" class="mz ly it mv b gy na nb l nc nd">from sklearn.model_selection import cross_val_score<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.neural_network import MLPClassifier</span><span id="8b4f" class="mz ly it mv b gy ne nb l nc nd">X = df.iloc[:, 1:].values<br/>y = df.iloc[:, 0].values</span><span id="9761" class="mz ly it mv b gy ne nb l nc nd"># Logistic Regression<br/>lr = LogisticRegression()<br/>lr_score = np.mean(cross_val_score(lr, X, y))<br/>print(f'Logistic Regression: {lr_score}')</span><span id="2b53" class="mz ly it mv b gy ne nb l nc nd"># Support Vector Machine<br/>svc = SVC()<br/>svc_score = np.mean(cross_val_score(svc, X, y))<br/>print(f'Support Vector Machine: {svc_score}')</span><span id="7747" class="mz ly it mv b gy ne nb l nc nd"># Decision Tree<br/>dtc = DecisionTreeClassifier()<br/>dtc_score = np.mean(cross_val_score(dtc, X, y))<br/>print(f'Decision Tree: {dtc_score}')</span><span id="96be" class="mz ly it mv b gy ne nb l nc nd"># K Nearest Neighbors<br/>knc = KNeighborsClassifier()<br/>knc_score = np.mean(cross_val_score(knc, X, y))<br/>print(f'K Nearest Neighbors: {knc_score}')</span><span id="fc77" class="mz ly it mv b gy ne nb l nc nd"># Multi-Layer Perceptron<br/>mlpc = MLPClassifier()<br/>mlpc_score = np.mean(cross_val_score(mlpc, X, y))<br/>print(f'Multi-Layer Perceptron: {mlpc_score}')</span></pre><p id="80cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">运行这段代码，我们得到了以下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/27254fb9072095902d0ea0a6f3431dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*W50e8jF2__e7o8nYdcaozg.png"/></div></figure><h1 id="451a" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">特征工程</h1><p id="f319" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">让我们看看是否可以通过处理数据集的要素来提高精确度。为此，我们将首先恢复到应用序号编码之前所做的备份。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="d6a5" class="mz ly it mv b gy na nb l nc nd">df = df_bkp.copy()</span></pre><h1 id="2d6b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">一键编码</h1><p id="67ae" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">现在，我们想对我们的分类变量应用一次性编码，而不是顺序编码。使用一键编码，我们不是给每个类分配一个数字，而是给一个向量分配所有的0，除了一个特定于该类的位置，在该位置我们放置一个1。也就是说，我们将每个类转换成一个独立的变量，其值为0或1。我们将使用Scikit-Learn的<code class="fe nj nk nl mv b">OneHotEncoder</code>对变量<em class="lu"> Pclass </em>、<em class="lu"> Sex </em>、<em class="lu"> Ticket </em>和<em class="lu">embedded</em>应用一键编码。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="d324" class="mz ly it mv b gy na nb l nc nd">from sklearn.preprocessing import OneHotEncoder</span><span id="e7c4" class="mz ly it mv b gy ne nb l nc nd"># Pclass<br/>pclass_transf = OneHotEncoder(sparse=False, dtype=np.uint8, handle_unknown='ignore')<br/>pclass_transf.fit(df['Pclass'].values.reshape((-1, 1)))<br/>pclass = pclass_transf.transform(df['Pclass'].values.reshape((-1, 1)))<br/>df['Pclass0'] = pclass[:, 0]<br/>df['Pclass1'] = pclass[:, 1]<br/>df['Pclass2'] = pclass[:, 2]<br/>del df['Pclass']</span><span id="1f63" class="mz ly it mv b gy ne nb l nc nd"># Sex<br/>gender_transf = OneHotEncoder(sparse=False, dtype=np.uint8, handle_unknown='ignore')<br/>gender_transf.fit(df['Sex'].values.reshape((-1, 1)))<br/>gender = gender_transf.transform(df['Sex'].values.reshape((-1, 1)))<br/>df['Male'] = gender[:, 0]<br/>df['Female'] = gender[:, 1]<br/>del df['Sex']</span><span id="c209" class="mz ly it mv b gy ne nb l nc nd"># Ticket<br/>ticket_transf = OneHotEncoder(sparse=False, dtype=np.uint8, handle_unknown='ignore')<br/>ticket_transf.fit(df['Ticket'].values.reshape((-1, 1)))<br/>ticket = ticket_transf.transform(df['Ticket'].values.reshape((-1, 1)))<br/>for i in range(ticket.shape[1]):<br/>    df[f'Ticket{i}'] = ticket[:, i]<br/>del df['Ticket']</span><span id="c1e3" class="mz ly it mv b gy ne nb l nc nd"># Embarked<br/>embarked_transf = OneHotEncoder(sparse=False, dtype=np.uint8, handle_unknown='ignore')<br/>embarked_transf.fit(df['Embarked'].values.reshape((-1, 1)))<br/>embarked = embarked_transf.transform(df['Embarked'].values.reshape((-1, 1)))<br/>for i in range(embarked.shape[1]):<br/>    df[f'Embarked{i}'] = embarked[:, i]<br/>del df['Embarked']</span></pre><h1 id="ad51" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">缩放至[0，1]范围</h1><p id="52a0" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">我们还想将数值变量缩放到[0，1]范围。为此，我们将使用<code class="fe nj nk nl mv b">MinMaxScaler</code>来缩放变量，以便最小值移动到0，最大值移动到1，其他中间值相应地在0，1之间缩放。我们将把这个转换应用到变量<em class="lu">年龄</em>、<em class="lu"> SibSp </em>、<em class="lu"> Parch </em>、<em class="lu">费用</em>。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="1770" class="mz ly it mv b gy na nb l nc nd">from sklearn.preprocessing import MinMaxScaler</span><span id="4367" class="mz ly it mv b gy ne nb l nc nd">age_transf = MinMaxScaler().fit(df['Age'].values.reshape(-1, 1))<br/>df['Age'] = age_transf.transform(df['Age'].values.reshape(-1, 1))</span><span id="83b2" class="mz ly it mv b gy ne nb l nc nd">sibsp_transf = MinMaxScaler().fit(df['SibSp'].values.reshape(-1, 1))<br/>df['SibSp'] = sibsp_transf.transform(df['SibSp'].values.reshape(-1, 1))</span><span id="e19c" class="mz ly it mv b gy ne nb l nc nd">parch_transf = MinMaxScaler().fit(df['Parch'].values.reshape(-1, 1))<br/>df['Parch'] = parch_transf.transform(df['Parch'].values.reshape(-1, 1))</span><span id="c805" class="mz ly it mv b gy ne nb l nc nd">fare_transf = MinMaxScaler().fit(df['Fare'].values.reshape(-1, 1))<br/>df['Fare'] = fare_transf.transform(df['Fare'].values.reshape(-1, 1))</span></pre><h1 id="3921" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">对这种新的数据格式进行机器学习</h1><p id="9679" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">现在，我们将对这种新的数据格式运行相同的机器学习算法，看看我们会得到什么结果。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="e3c3" class="mz ly it mv b gy na nb l nc nd">X = df.iloc[:, 1:].values<br/>y = df.iloc[:, 0].values</span><span id="0fe1" class="mz ly it mv b gy ne nb l nc nd"># Logistic Regression<br/>lr = LogisticRegression()<br/>lr_score = np.mean(cross_val_score(lr, X, y))<br/>print(f'Logistic Regression: {lr_score}')</span><span id="7381" class="mz ly it mv b gy ne nb l nc nd"># Support Vector Machine<br/>svc = SVC()<br/>svc_score = np.mean(cross_val_score(svc, X, y))<br/>print(f'Support Vector Machine: {svc_score}')</span><span id="acda" class="mz ly it mv b gy ne nb l nc nd"># Decision Tree<br/>dtc = DecisionTreeClassifier()<br/>dtc_score = np.mean(cross_val_score(dtc, X, y))<br/>print(f'Decision Tree: {dtc_score}')</span><span id="7b1d" class="mz ly it mv b gy ne nb l nc nd"># K Nearest Neighbors<br/>knc = KNeighborsClassifier()<br/>knc_score = np.mean(cross_val_score(knc, X, y))<br/>print(f'K Nearest Neighbors: {knc_score}')</span><span id="9927" class="mz ly it mv b gy ne nb l nc nd"># Multi-Layer Perceptron<br/>mlpc = MLPClassifier()<br/>mlpc_score = np.mean(cross_val_score(mlpc, X, y))<br/>print(f'Multi-Layer Perceptron: {mlpc_score}')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/3a60ede82a906d7d0e9ff584907160e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*dNx86XTZLKPSUN7zJcMlUQ.png"/></div></figure><p id="1370" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们以这种方式设计了我们的特征之后，我们在所有分类器的准确性上得到了显著的提高。其中最好的分类器是决策树分类器。现在我们试图通过使用<code class="fe nj nk nl mv b">GridSearchCV</code>进行超参数调整来改进它。</p><h1 id="dca4" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">超参数调谐</h1><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a6c5" class="mz ly it mv b gy na nb l nc nd">from sklearn.model_selection import GridSearchCV</span><span id="f215" class="mz ly it mv b gy ne nb l nc nd">dtc = DecisionTreeClassifier()<br/>params = {<br/>    'max_depth': list(range(2, 151)),<br/>    'min_samples_split': list(range(2, 15))<br/>}<br/>clf = GridSearchCV(dtc, params)<br/>clf.fit(X, y)</span><span id="3f86" class="mz ly it mv b gy ne nb l nc nd">print(f'Best params: {clf.best_params_}')<br/>print(f'Best score: {clf.best_score_}')</span></pre><p id="e334" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到的参数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/fda4398ea570a83f1a140427a21d9354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*YMNO-CHHRxKbpCZOXcTLvw.png"/></div></figure><p id="10d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到的分数是<strong class="la iu"> 84.40% </strong>，在超参数调优后提高了<strong class="la iu"> 0.9% </strong>。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="19d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">我希望这些信息对您有用，感谢您的阅读！</em></p><p id="217e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae lv" href="https://www.nablasquared.com/predicting-survivors-of-titanic-with-machine-learning/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>