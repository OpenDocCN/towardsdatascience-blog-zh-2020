<html>
<head>
<title>Intuition behind Residual Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">残差神经网络背后的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7?source=collection_archive---------8-----------------------#2020-07-22">https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7?source=collection_archive---------8-----------------------#2020-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4638" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解剩余网络是如何工作的，以及它们是如何自然产生的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6fd190dbe09eb6f76a8a7ca79d06def7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O4Yeogq7PytlX86K"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">越深越好？(里卡多·佩拉蒂在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure><p id="cdd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度神经网络——因为层数多而“深”，已经在许多机器学习任务中走了很长的路。但是有多深呢？再来看图像分类的流行案例:<a class="ae ky" href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>普及堆叠 CNN 图层。它由 5 个回旋层组成。很快，人们相信叠加更多的卷积层会带来更好的精度。<a class="ae ky" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank"> Inception </a>是首批通过使用非常深入的架构来展示更好性能的架构之一。它使用了 22 个卷积层。现在，为了获得更高的精度，我们能到达的最深处是什么？</p><p id="4eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们建立一个玩具数据集，并在其上用具有越来越多完全连接层的神经网络进行训练:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="89fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这创建了“sin 函数”数据集，创建了具有 1 到 30 个隐藏层的神经网络，在数据集上训练它们。人们可能会认为损耗值应该是递减的，然后在某一点饱和并保持不变。但是结果是不同的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/7213bb84dc6d0601abd3a637fbbb8d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*XPj_TXR4NtaPX6lOG4qA3w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络损耗随层数增加？</p></figure><p id="1bc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么？！最初，当具有 1 个隐藏层时，我们具有高损耗，其中增加层数实际上减少了损耗，但是当超过 9 层时，损耗增加。这里我们是为<strong class="lb iu"> <em class="ly">历元=20*t </em> </strong>进行训练，意思是更大的模型需要更多的训练历元。我们在 9 层获得最低损耗，但在此之上，损耗会增加。这里需要注意的重要一点是<strong class="lb iu">没有过度配合</strong>，因为这只是我们考虑的训练损失。这是否意味着层数越多，性能越差？</p><h1 id="ee51" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">为什么会这样？</h1><p id="680b" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">让我们试着直观地理解这个问题。如果一个“浅层”模型能够达到一定的精度，那么它们更深层的对应模型至少应该具有相同的精度。但是，当模型变得更深时，各层传播来自浅层的信息变得越来越困难，并且信息丢失。这被称为<strong class="lb iu">退化问题</strong>。</p><p id="630e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet 论文</a>关于这个问题的研究:</p><blockquote class="mw"><p id="e684" class="mx my it bd mz na nb nc nd ne nf lu dk translated">当更深的网络能够开始收敛时，退化问题就暴露出来了:随着网络深度的增加，精度达到饱和(这可能不足为奇)，然后迅速退化。</p></blockquote><p id="5758" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了解决这个问题，较深的层必须直接传播来自较浅的层的信息，即身份映射。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/fbc3279e03989ae278e8e67598ebfcea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RP94azymP220VzxPkxyoGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随着更多的层被添加到网络中，从浅层传播信息变得更加困难。</p></figure><p id="dd4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，9 层模型是性能最好的。在具有 30 层的模型中，也存在相同的 9 层，如果另外的 21 层传播与第 9 层相同的结果，则整个模型将具有相同的损耗。所以现在这个问题简化为让那些层学习“恒等函数”，<em class="ly"> f(x) = x </em>。</p><h1 id="c53b" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">用“捷径”解决问题</h1><p id="8e97" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我们能改变我们的网络来避免这种信息丢失吗？一个直观的解决方法是把浅层和深层直接连接起来，这样信息直接传递到深层，像 identity 函数一样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/3c6c596cd3ef9faaa281fe5053104299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGZ_wE2jItb-NDjNKxYhyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">救援的捷径！</p></figure><p id="911f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们绕过中间层，将浅层连接到深层。在我们的例子中，我们可以将第 9 层神经元直接连接到第 30 层，然后深度模型将与浅层模型一样运行。这样，信息直接作为标识函数传递。这是残留网络背后的直觉。</p><p id="ef14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过“捷径”或“跳过连接”，我们的意思是一个神经元的结果被直接添加到深层的相应神经元。当添加时，中间层将学习它们的权重为零，从而形成恒等函数。现在，让我们正式看看剩余学习。</p><h1 id="1d15" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">剩余学习</h1><p id="442f" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">设<em class="ly"> g(x) </em>为各层学习的函数。让我们考虑<em class="ly"> h(x) = g(x)+x </em>，具有跳跃连接的层。这里<em class="ly"> +x </em>术语表示跳过连接。</p><p id="aad3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="ly"> h(x)=g(x)+x </em>中，+x 项将带来原始值，层 g(x)只需学习<strong class="lb iu">值</strong>的变化，或<strong class="lb iu">余数</strong>或δx。无论在<em class="ly"> g(x) </em>中学习什么，都只是余数，正或负，以将 x 修改为所需值。因此得名“剩余学习”。</p><p id="eb3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使<em class="ly"> h(x) </em>成为恒等函数，余数<em class="ly"> g(x) </em>就必须成为零函数，这是<strong class="lb iu">非常容易学的</strong>，即把所有的权重都设置为零。然后<em class="ly"> h(x) = 0+x = x </em>，这就是所需的恒等函数。这将有助于克服退化问题。</p><p id="cddb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在没有跳跃连接的情况下，必须修改权重和偏差值，以使其对应于单位函数。从头开始学习恒等函数非常困难，层中的非线性加剧了这种困难，并导致退化问题。</p><blockquote class="nm nn no"><p id="d4e0" class="kz la ly lb b lc ld ju le lf lg jx lh np lj lk ll nq ln lo lp nr lr ls lt lu im bi translated">利用剩余学习重新公式化，如果恒等式映射是最优的，则求解器可以简单地将多个非线性层的权重驱向零，以逼近恒等式映射。</p></blockquote><h1 id="f248" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">残差神经网络</h1><p id="801f" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">让我们看看残差神经网络或“ResNets”的构建块，即残差块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9368edae2bc7482273167f619889f192.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*pQhmHnRaqs4uX_D80Qb8YA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">剩余块[1]</p></figure><p id="589f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，跳过连接有助于将标识功能带到更深的层。在残块中，有人可能会注意到两点:</p><ol class=""><li id="f048" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">为什么在添加跳过连接后应用 relu？</li><li id="363d" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">为什么一个残块有两个重量层？</li></ol><p id="c8ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 1，如果我们在加法之前执行了 relu，那么所有的余数都是正的或零。仅学习到该标识的正增量，这显著降低了学习能力。例如在 sin 函数中，sin(3 <em class="ly"> π </em> /2) = -1，这将需要负余数。类似地，使用 sigmoid 也将是不利的，因为它仅产生 0 到 1 内的残基。理想情况下，我们希望来自权重层(跨越任何数值范围)的<strong class="lb iu">无约束响应被添加到跳过层，然后应用激活以提供非线性。这有助于模型学习任何函数。</strong></p><p id="92a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 2，如果我们使用单个权重层，则在 relu 之前添加跳过连接，得到 F(x) = Wx+x，这是简单的线性函数。这相当于只有一个权重层，添加跳过连接没有意义。所以我们需要<strong class="lb iu">在增加跳跃连接</strong>之前至少有一个非线性，这是通过使用两层来实现的。</p><p id="0db6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在上一节中看到的，这些块中的权重层正在学习残差。这些模块可以堆叠得越来越多，但性能不会下降。</p><h1 id="3a4b" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">履行</h1><p id="34d7" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">足够的理论，让我们看看我们如何实现剩余块:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="87cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是残差块的简单实现。我们可以多次调用这个函数来堆叠越来越多的块。在这里，我们可以在图像的情况下用卷积层代替密集层。此残差块的一个约束条件是，图层输出必须与输入具有相同的形状，但有解决方法。</p><p id="cf61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用实验来验证这些结果是否如我们描述的那样工作。我们逐步构建更深层次的模型，并在 sin 函数数据集上对其进行训练，类似于上面的示例，这次使用的是残差块。结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/14b7c551360e317df347a1b7171990a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ku0ChYxemQyF1hz348ExVA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ResNets 没有降级问题！</p></figure><p id="71cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的图表清楚地验证了这个结果的全部意义！我们看到，当我们增加层数时，ResNet 的性能并没有下降。其实是在进步，更好！在 sin 函数数据集上训练平原网和结果网的代码在下面的 github 报告中:</p><div class="oh oi gp gr oj ok"><a href="https://github.com/ilango100/resnet" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">ilango100/resnet</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">此储存库包含残差神经网络的 tensorflow 实现。作为比较，三个网络…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ks ok"/></div></div></a></div><p id="7a8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ResNet 的这一特性有助于训练非常深入的模型，产生了几个流行的神经网络，即 ResNet-50、ResNet-101 等。我们能更深入吗？瞧着吧，<a class="ae ky" href="https://github.com/KaimingHe/resnet-1k-layers" rel="noopener ugc nofollow" target="_blank"> ResNet-1001 </a>！</p><h1 id="738a" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">CIFAR-10 的结果</h1><p id="131f" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">研究玩具数据集有助于理解 ResNet。现在，是一些真实世界数据集的时候了。上面的 github repo 有代码在 CIFAR-10 上构建和训练 ResNets 和 PlainNets 的多种配置。您可以在那里看到所有的实现细节。它是使用 Tensorflow (Keras API)构建的。</p><p id="ee55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 CIFAR-10 上的训练结果可以在这个<a class="ae ky" href="https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag" rel="noopener ugc nofollow" target="_blank">张量板实验</a>中得到。可以看到 PlainNet 和 ResNet 不同深度的对比:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/a13fd7f1104a0bfa10ee7d0dc20c7728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NccjoBLh4NC3N2VZb2hjXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag" rel="noopener ugc nofollow" target="_blank"> Tensorboard 让生活更轻松！</a></p></figure><p id="b27e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行名称为<strong class="lb iu"> <em class="ly">网络 x 尺寸</em> </strong>。在“图形”选项卡中，您可以可视化网络架构。我们可以在 ResNet 模型中看到跳跃连接，而在 PlainNets 中则看不到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c92f1764890fb28438c61b409627dee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*OEMy6FKLvQTaHaR1AjJKlw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag/#graphs" rel="noopener ugc nofollow" target="_blank"> ResNet 及其跳过连接</a></p></figure><p id="5f0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制精度值与网络大小的关系图，我们可以清楚地看到，对于 PlainNet，精度值随着网络大小的增加而降低，显示了我们之前看到的同样的性能下降问题。至于 ResNet，随着网络深度的增加，我们看到了准确性的提高。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/92f58bc7eb303c74d7b52506217fba7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*d7kyxMT4db9zIjfCOjmF3w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">规模不断扩大的网络的训练精度</p></figure><p id="1c11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看着这些图表，你可能想知道，ResNetV2 是什么。ResNetV2 是经过一些改进的 ResNet。这将是另一篇文章的主题！</p><h1 id="663a" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">结论</h1><p id="deeb" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">这里的关键要点是:</p><ul class=""><li id="db5f" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu pb nz oa ob bi translated">神经网络中的层数越多并不总是意味着性能越好。非常深的网络往往会降低性能。</li><li id="4264" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pb nz oa ob bi translated">残余网络通过捷径或跳过连接，通过将浅层短路到深层来解决退化问题。</li><li id="7eec" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pb nz oa ob bi translated">我们可以堆叠越来越多的剩余块，而不会降低性能。这使得能够建立非常深的网络。</li></ul></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><p id="53f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考资料:</p><p id="edbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]，何，，，任，，<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"/>【2015】</p></div></div>    
</body>
</html>