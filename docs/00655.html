<html>
<head>
<title>Speech-enhancement with Deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的语音增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speech-enhancement-with-deep-learning-36a1991d3d8d?source=collection_archive---------7-----------------------#2020-01-19">https://towardsdatascience.com/speech-enhancement-with-deep-learning-36a1991d3d8d?source=collection_archive---------7-----------------------#2020-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="15b3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这个项目旨在建立一个语音增强系统来衰减环境噪音。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6b9314ccf1407525bd5509bece7ba812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*DmWLTVGdFqllCGu6nQ7nZw.gif"/></div></div></figure><p id="5570" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">音频有许多不同的表示方式，从原始时间序列到时频分解。表示的选择对于系统的性能至关重要。在时频分解中，频谱图已被证明是音频处理的一种有用的表示方法。它们由代表短时傅立叶变换(STFT)序列的 2D 图像组成，以时间和频率为轴，亮度代表每个时间帧的频率分量的强度。因此，它们似乎是将图像的 CNN 架构直接应用于声音的自然领域。在幅度谱和相位谱之间，幅度谱包含了信号的大部分结构。相位谱图似乎只显示出很少的时间和光谱规律。</p><p id="ac16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这个项目中，我将使用幅度谱图作为声音的表示(参见下图),以预测噪声模型，并将其减去噪声声谱图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lq"><img src="../Images/a4b1095cb01910ea487ca3bd4cbc616a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gR6C0BbsB2PHajCmHU__xQ.png"/></div></div></figure><p id="ee27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">项目分解为三种模式:<code class="fe lr ls lt lu b">data creation</code>、<code class="fe lr ls lt lu b">training</code>和<code class="fe lr ls lt lu b">prediction</code>。该代码可从 Github 库获得:<a class="ae lv" href="https://github.com/vbelz/Speech-enhancement" rel="noopener ugc nofollow" target="_blank">https://github.com/vbelz/Speech-enhancement</a></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="a892" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">准备数据</h1><p id="c797" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">为了创建用于训练的数据集，我从不同的来源收集了带有干净声音和环境噪音的英语语音示例。</p><p id="924f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">干净的声音主要来自于<a class="ae lv" href="http://www.openslr.org/12/" rel="noopener ugc nofollow" target="_blank"> LibriSpeech </a>:一个基于公共领域有声读物的 ASR 语料库。我也使用了来自 SiSec 的一些数据。环境噪声从<a class="ae lv" href="https://github.com/karoldvl/ESC-50" rel="noopener ugc nofollow" target="_blank"> ESC-50 数据集</a>或<a class="ae lv" href="https://www.ee.columbia.edu/~dpwe/sounds/" rel="noopener ugc nofollow" target="_blank">https://www.ee.columbia.edu/~dpwe/sounds/</a>中采集。</p><p id="e86a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这个项目中，我关注了 10 类环境噪音:滴答声、脚步声、铃声、手锯声、闹钟声、烟火声、昆虫声、刷牙声、吸尘器声和鼾声。下图展示了这些类(我用来自<a class="ae lv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">https://unsplash.com</a>的图片创建了这张图片。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/3c51472fcc151ddc22af11590881f036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*arzJwKSvZvBs0GErKG9HCA.png"/></div></div></figure><p id="2af8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了创建用于训练/验证/测试的数据集，音频以 8kHz 进行采样，我提取了略高于 1 秒的窗口。我对环境噪声进行了一些数据扩充(在不同时间取窗口会产生不同的噪声窗口)。噪声已经混合到干净的声音中，并随机化噪声级别(在 20%和 80%之间)。最终，训练数据由 10h 的嘈杂语音和干净语音以及 1h 的声音的验证数据组成。</p><p id="4aab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了准备数据，我建议在与代码文件夹不同的位置创建 data/Train 和 data/Test 文件夹。然后创建下图所示的结构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9a9b6a61345b122a4f67824a6fae7567.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*IAMXYwz09MQSJdvdy12vDA.png"/></div></figure><p id="72e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以将<code class="fe lr ls lt lu b">noise_dir</code>、<code class="fe lr ls lt lu b">voice_dir</code>、<code class="fe lr ls lt lu b">path_save_spectrogram</code>、<code class="fe lr ls lt lu b">path_save_time_serie</code>和<code class="fe lr ls lt lu b">path_save_sound</code>路径名相应地修改到<code class="fe lr ls lt lu b">args.py</code>文件中，该文件采用程序的默认参数。</p><p id="557f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将您的噪音音频文件放入<code class="fe lr ls lt lu b">noise_dir</code>目录，将您的干净语音文件放入<code class="fe lr ls lt lu b">voice_dir</code>。</p><p id="7119" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<code class="fe lr ls lt lu b">args.py</code>中指定你想要创建多少帧作为<code class="fe lr ls lt lu b">nb_samples</code>(或者从终端传递它作为一个参数)我让 nb_samples 默认为 50，但对于生产，我建议有 40 000 或更多。</p><p id="f9e2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后运行<code class="fe lr ls lt lu b">python main.py --mode='data_creation'</code>。这将随机混合来自<code class="fe lr ls lt lu b">voice_dir</code>的一些干净声音和来自<code class="fe lr ls lt lu b">noise_dir</code>的一些噪音，并将嘈杂声音、噪音和干净声音的频谱图以及复杂相位、时间序列和声音保存到磁盘(用于 QC 或测试其他网络)。它采用在<code class="fe lr ls lt lu b">args.py</code>中定义的输入参数。STFT、帧长、hop_length 的参数可以在<code class="fe lr ls lt lu b">args.py</code>中修改(或从终端作为参数传递)，但在默认参数下，每个窗口将被转换为 128 x 128 大小的谱图矩阵。</p><p id="e9c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于训练的数据集将是嘈杂声音的幅度谱图和干净声音的幅度谱图。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="b1d1" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">培养</h1><p id="100b" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">用于训练的模型是 U-Net，一种具有对称跳跃连接的深度卷积自动编码器。<a class="ae lv" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net </a>最初是为生物医学图像分割而开发的。这里，U-Net 已被用来对光谱图进行降噪处理。</p><p id="3a27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">作为网络的输入，噪声声音的幅度谱图。将噪声输出到模型(含噪语音幅度谱图—干净语音幅度谱图)。输入和输出矩阵都用全局缩放进行缩放，以映射到-1 和 1 之间的分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/33d0a718c59300ac8a91a23cfd57a63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GtWBCnzt-AyhFizyGPXVvA.png"/></div></div></figure><p id="a795" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">培训期间测试了许多配置。对于优选的配置，编码器由 10 个卷积层组成(具有 LeakyReLU、maxpooling 和 dropout)。解码器是具有跳跃连接的对称扩展路径。最后一个激活层是双曲正切(tanh ),其输出分布在-1 和 1 之间。对于从零开始的训练，初始随机权重用正常初始值设定。</p><p id="668b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">模型用 Adam optimizer 编译，使用的损失函数是 Huber 损失，作为 L1 和 L2 损失之间的折衷。</p><p id="d78d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在现代 GPU 上进行培训需要几个小时。</p><p id="0c3e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你本地电脑有深度学习计算的 GPU，可以用:<code class="fe lr ls lt lu b">python main.py --mode="training"</code>进行训练。它将<code class="fe lr ls lt lu b">args.py</code>中定义的参数作为输入。默认情况下，它将从头开始训练(你可以通过将<code class="fe lr ls lt lu b">training_from_scratch</code>设为 false 来改变这一点)。您可以从<code class="fe lr ls lt lu b">weights_folder</code>和<code class="fe lr ls lt lu b">name_model</code>中指定的预训练重量开始训练。我让<code class="fe lr ls lt lu b">model_unet.h5</code>使用我在<code class="fe lr ls lt lu b">./weights</code>训练中获得的重量。通过<code class="fe lr ls lt lu b">epochs</code>和<code class="fe lr ls lt lu b">batch_size</code>指定训练的时期数和批量大小。最佳重量在训练期间自动保存为<code class="fe lr ls lt lu b">model_best.h5</code>。可以调用 fit_generator 在训练时只将部分数据加载到磁盘上。</p><p id="3889" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就我个人而言，我在训练中使用了 Google Colab 提供的免费 GPU。我在<code class="fe lr ls lt lu b">./colab/Train_denoise.ipynb</code>举了一个笔记本例子。如果您的驱动器上有大量可用空间，您可以将所有训练数据加载到您的驱动器上，并在训练时使用 tensorflow.keras 的 fit_generator 选项加载部分数据。就我个人而言，我的 Google drive 上的可用空间有限，所以我提前准备了 5Gb 的批量加载到驱动器上进行训练。重量会定期保存，并在下次训练时重新加载。</p><p id="90c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最终我获得了 0.002129 的训练损失和 0.002406 的验证损失。下面是在一次培训中制作的损失图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/8011adda6c17486281df4932a318a6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOfHrvYw_rnxs5ITEEA0dw.png"/></div></div></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="e71d" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">预言；预测；预告</h1><p id="0581" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">为了进行预测，嘈杂的语音音频被转换成略高于 1 秒的窗口的 NumPy 时间序列。每个时间序列通过 STFT 变换转换成幅度谱图和相位谱图。噪声声谱图被传入 U-Net 网络，该网络将预测每个窗口的噪声模型(参见下图)。使用传统的 CPU，一个窗口的预测时间一旦转换成幅度谱图大约是 80 ms。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/8bf9d6fc6d1d5dd91a8133bb05c68546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4srsz87QPEFq0kM6Ff_4Q.png"/></div></div></figure><p id="1d20" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，从有噪声的语音频谱图中减去该模型(这里，我应用直接减法，因为它对于我的任务来说是足够的，我们可以想象训练第二个网络来适应噪声模型，或者应用诸如在信号处理中执行的匹配滤波器)。“去噪”的幅度谱图与初始相位相结合，作为反短时傅立叶变换(ISTFT)的输入。我们去噪后的时间序列可以转换成音频(参见下图)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/fa75a0cf792fbc951d37d643b1cbdfe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UzWPKH_m_6E-2XIHLyiSA.png"/></div></div></figure><p id="b9d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看在验证数据上的表现！</p><p id="e0d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我展示了一些警报/昆虫/真空吸尘器/铃声的验证结果。对于它们中的每一个，我都显示了初始的有噪语音谱图，网络预测的去噪谱图，以及真正干净的语音谱图。我们可以看到，该网络能够很好地概括噪声模型，并产生略微平滑版本的语音频谱图，非常接近真正干净的语音频谱图。</p><p id="b0d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">更多验证数据的声谱图去噪示例显示在存储库顶部的初始 gif 中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/52f5eb83886244aa642c3cbeadd188ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbUISmOaFJPKanMAaYe6SA.png"/></div></div></figure><p id="c33b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们听听转换回声音的结果:</p><h2 id="8604" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><em class="nt">报警示例的音频:</em></h2><p id="5441" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_alarm39.wav" rel="noopener ugc nofollow" target="_blank">输入示例报警</a></p><p id="95ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_alarm39.wav" rel="noopener ugc nofollow" target="_blank">预测输出示例报警</a></p><p id="b896" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_alarm39.wav" rel="noopener ugc nofollow" target="_blank">真实输出示例报警</a></p><h2 id="4df5" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><strong class="ak"> <em class="nt">昆虫音频示例:</em> </strong></h2><p id="8338" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_insect41.wav" rel="noopener ugc nofollow" target="_blank">输入示例昆虫</a></p><p id="8ec9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_insect41.wav" rel="noopener ugc nofollow" target="_blank">预测产量示例昆虫</a></p><p id="a981" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_insect41.wav" rel="noopener ugc nofollow" target="_blank">真实输出示例昆虫</a></p><h2 id="9b83" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><strong class="ak"> <em class="nt">吸尘器音频示例:</em> </strong></h2><p id="9026" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_vaccum35.wav" rel="noopener ugc nofollow" target="_blank">输入示例吸尘器</a></p><p id="677c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_vaccum35.wav" rel="noopener ugc nofollow" target="_blank">预测产量示例吸尘器</a></p><p id="c738" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_vaccum35.wav" rel="noopener ugc nofollow" target="_blank">真实输出示例真空吸尘器</a></p><h2 id="4aa2" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><strong class="ak"> <em class="nt">铃声示例:</em> </strong></h2><p id="743c" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_bells28.wav" rel="noopener ugc nofollow" target="_blank">输入示例铃声</a></p><p id="1942" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_bells28.wav" rel="noopener ugc nofollow" target="_blank">预测输出示例铃声</a></p><p id="e0ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_bells28.wav" rel="noopener ugc nofollow" target="_blank">真实输出示例铃声</a></p><p id="9441" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我展示了转换回时间序列的相应显示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/6b771dbe91c84409aeb0e334c87c66f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AM4W_DrrjfFmdv1wO7uLZg.png"/></div></div></figure><p id="2233" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以看看我在<code class="fe lr ls lt lu b">./demo_data</code>文件夹中提供的 jupyter 笔记本<code class="fe lr ls lt lu b">demo_predictions.ipynb</code>中的这些显示/音频。</p><p id="c22b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面，我展示的是时间序列域的声谱图去噪 gif(库顶)对应的 gif。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e0b2a492f95fe5efdea5adf3918c8787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Re8KE2BWY53bvTfpal3oWA.gif"/></div></div></figure><p id="b9b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">作为极端测试，我应用于一些混合了许多高水平噪音的声音。该网络在去噪方面表现得出奇地好。对 5 秒钟的音频进行降噪的总时间约为 4 秒钟(使用传统 CPU)。</p><p id="4def" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是一些例子:</p><h2 id="4a6b" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><strong class="ak"> <em class="nt">例一:</em> </strong></h2><p id="b740" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/test/noisy_voice_long_t2.wav" rel="noopener ugc nofollow" target="_blank">输入示例测试 1 </a></p><p id="49ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/save_predictions/denoise_t2.wav" rel="noopener ugc nofollow" target="_blank">预测输出示例测试 1 </a></p><h2 id="66ab" class="nh me it bd mf ni nj dn mj nk nl dp mn ld nm nn mp lh no np mr ll nq nr mt ns bi translated"><em class="nt">例二:</em></h2><p id="7273" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/test/noisy_voice_long_t1.wav" rel="noopener ugc nofollow" target="_blank">输入示例测试 2 </a></p><p id="b8f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lv" href="https://vbelz.github.io/Speech-enhancement/demo_data/save_predictions/denoise_t1.wav" rel="noopener ugc nofollow" target="_blank">预测输出示例测试 2 </a></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c7c1" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="cbb0" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">提出了一种用于衰减环境噪声的深度学习语音增强系统。通过使用声音的幅度谱图表示，音频去噪问题被转化为图像处理问题，简化了其分辨率。要去除的噪声已经由 U-Net 建模，U-Net 是具有对称跳跃连接的深度卷积自动编码器。经过训练后，该网络能够模拟 10 类环境噪声。这种方法是不完美的，因为没有对噪声相位谱图进行建模，这在特定情况下可能会降低整体性能。此外，这种方法更适合离线去噪，因为去噪 5 秒的音频目前需要大约 4 秒(使用经典 CPU 进行预测)。尽管如此，这种方法是稳健的，能够推广到许多语音和噪声配置。此外，它可以转移学习到其他语言以外的英语和新类型的噪音。</p><h1 id="7211" class="md me it bd mf mg nv mi mj mk nw mm mn jz nx ka mp kc ny kd mr kf nz kg mt mu bi translated">参考</h1><blockquote class="oa ob oc"><p id="b2f7" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated">扬松、安德里亚斯、埃里克·汉弗莱、尼古拉·蒙特基奥、雷切尔·比特纳、阿帕娜·库马尔和蒂尔曼·韦德。基于深度 U 网卷积网络的歌唱声分离。伊斯米尔<em class="it"> (2017)。</em></p><p id="b7d6" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated">【https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf】<a class="ae lv" href="https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a><em class="it"/></p><p id="8a53" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated"><em class="it">格赖斯、艾玛德 m .和普拉姆布利、马克 d,《使用卷积去噪自动编码器的单通道音频源分离》( 2017 年)。</em></p><p id="7b22" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated"><em class="it"/><a class="ae lv" href="https://arxiv.org/abs/1703.08019" rel="noopener ugc nofollow" target="_blank"><em class="it">https://arxiv.org/abs/1703.08019</em></a><em class="it"/></p><p id="51e3" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated"><em class="it"> Ronneberger O .，Fischer P .，Brox T. (2015) U-Net:用于生物医学图像分割的卷积网络。载于:Navab N .，Hornegger J .，Wells W .，Frangi A. (eds) </em>医学图像计算和计算机辅助干预— MICCAI 2015 <em class="it">。MICCAI 2015。计算机科学讲义，第 9351 卷。施普林格·查姆</em></p><p id="5afb" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated"><em class="it"/><a class="ae lv" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"><em class="it">https://arxiv.org/abs/1505.04597</em></a><em class="it"/></p><p id="a4c2" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated">K. J .皮扎克。环境声音分类数据集。第 23 届 ACM 多媒体年会论文集<em class="it">，澳大利亚布里斯班，2015。</em></p><p id="efa0" class="ku kv od kw b kx ky ju kz la lb jx lc oe le lf lg of li lj lk og lm ln lo lp im bi translated"><em class="it">【土井:</em><a class="ae lv" href="http://dx.doi.org/10.1145/2733373.2806390" rel="noopener ugc nofollow" target="_blank"><em class="it">http://dx.doi.org/10.1145/2733373.2806390</em></a><em class="it"/></p></blockquote></div></div>    
</body>
</html>