<html>
<head>
<title>Day 113 of #NLP365: NLP Papers Summary — On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 113 天:NLP 论文摘要——关于使用 Transformer 语言模型的提取和抽象神经文档摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc?source=collection_archive---------51-----------------------#2020-04-22">https://towardsdatascience.com/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc?source=collection_archive---------51-----------------------#2020-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="2baf" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="4977" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="e565" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 257 天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="efda" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的 NLP 论文是<strong class="lf jp"> <em class="lz">关于使用 Transformer 语言模型</em> </strong>的提取和抽象神经文档摘要。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="24de" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="7456" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了一种长文档的抽象摘要方法。这是通过提取和抽象概括的两步过程实现的。提取步骤的输出用于训练抽象转换器语言模型。这一提取步骤对最终总结结果非常重要。此外，生成的抽象概要比使用复制机制的先前工作更抽象，并且还产生更高的 ROUGE 分数。这些贡献是:</p><ol class=""><li id="560c" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">展示了 transformer 语言模型在总结长篇科学文章方面的有效性，优于 Seq2Seq 模型</li><li id="d34b" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">与以前的工作相比，提出的模型能够产生更抽象的摘要，并且仍然获得更高的 ROUGE 分数</li></ol><h2 id="0bc0" class="ns mi jf bd mj nt nu dn mn nv nw dp mr lm nx ny mt lq nz oa mv lu ob oc mx jl bi translated">人类总结过程</h2><ol class=""><li id="1f44" class="ne nf jf lf b lg mz lj na lm od lq oe lu of ly nj nk nl nm bi translated">阅读并理解源文件</li><li id="8bcb" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">选择源文档中最重要的部分</li><li id="1075" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">解释这些重要部分中的关键概念</li><li id="9af8" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">生成连贯流畅的输出摘要</li></ol><h1 id="e00c" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">数据集</h1><p id="47d8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有四种不同的长文档汇总数据集:</p><ol class=""><li id="488e" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">arXiv</li><li id="c463" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">Pubmed</li><li id="0c3e" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">大专利</li><li id="9fb5" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">报刊阅览室</li></ol><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1c30e2abc41667647b05e6ff1acefb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*Bi_5FeZb-op8S7pw.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">汇总数据集的描述性统计[1]</p></figure><h1 id="f0b0" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">结构</h1><p id="9d66" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提议的框架分为两个独立的部分:</p><ol class=""><li id="2044" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated"><em class="lz">摘录总结</em>。一种分层文档模型，它复制或分类文档中的句子以构建摘录摘要</li><li id="a36a" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">抽象概括</em>。摘录摘要以及文档用于调节转换器语言模型</li></ol><h2 id="e145" class="ns mi jf bd mj nt nu dn mn nv nw dp mr lm nx ny mt lq nz oa mv lu ob oc mx jl bi translated">摘录摘要</h2><p id="cac8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提取步骤包括使用两种不同的分层文档模型进行句子提取:分层 seq2seq 句子指针和句子分类器。目标是过滤掉嘈杂的句子，提取重要的句子，更好地训练我们的 transformer 语言模型。分级 seq2seq 语句指针具有编码器-解码器架构:</p><ol class=""><li id="c8ef" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated">编码器是单词和句子级别(分级)的双向 LSTM</li><li id="489d" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated">解码器是自回归 LSTM</li></ol><p id="4a08" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">分级编码器结合了单词级和句子级的方向 LSTM。令牌级 biLSTM 对文档中的每个句子进行编码，以获得句子嵌入。句子级 biLSTM 对这些句子嵌入进行编码，以获得文档表示。解码器是一个自回归 LSTM，它将先前提取的句子的隐藏状态作为输入，并预测下一个要提取的句子。</p><p id="0fbb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">类似于指针网络，句子分类器使用分级 LSTM 来编码文档并产生句子嵌入序列。最终的文档表示是这些句子嵌入的平均值。最终的文档表示被连接到每个嵌入的句子，并被馈送到具有 sigmoid 函数的神经网络中，以获得每个句子被包括在摘要中的概率。</p><h2 id="fa07" class="ns mi jf bd mj nt nu dn mn nv nw dp mr lm nx ny mt lq nz oa mv lu ob oc mx jl bi translated">抽象概括</h2><p id="14a8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们使用“格式化的”数据从头开始训练一个单一的 transformer 语言模型。转换器语言模型是 GPT-2。通过自回归分解单词的联合分布来训练语言模型。这启发我们以特定的格式组织训练数据，我们将基础事实摘要放在模型通常用来生成摘要的信息之后。这样，我们在训练期间对文档和摘要的联合分布进行建模，并在推理时使用条件分布(给定文档)来生成摘要。因此，训练数据被格式化为 4 个不同的部分:</p><ol class=""><li id="3f74" class="ne nf jf lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated"><em class="lz">论文简介</em>。假设简介应包含足够的内容以生成摘要</li><li id="7a1f" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">摘录摘要(摘自摘录摘要)</em></li><li id="e0f6" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">摘要(地面实况总结)</em></li><li id="f28b" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><em class="lz">论文其余部分</em>。用于训练语言模型以理解领域语言</li></ol><p id="a611" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于一些数据集，引言部分将是整个文档，因为没有论文部分的其余部分。下图展示了整体框架。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/bf0c8e663ac5c444048f54a2157ee824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*dI7jHITIrAkQ_nNe.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">建议对研究论文进行抽象概括的训练过程[1]</p></figure><h1 id="0f27" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">结果和分析</h1><p id="f122" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">表 2 和表 4 显示，我们的提取模型在 arXiv 和 PubMed 数据集上都优于所有以前的提取基线。在新闻编辑室数据集(表 6)上，我们的 TLM 远远超过了另一个抽象模型 Seq2Seq，也超过了指针生成器网络。然而，消费后模型主导了提取和混合结果。</p><div class="om on oo op gt ab cb"><figure class="or iv os ot ou ov ow paragraph-image"><img src="../Images/008b907ca84e4d602f259bc60074e27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*cbvoa5aaMugmkHQ5.png"/></figure><figure class="or iv ox ot ou ov ow paragraph-image"><img src="../Images/777ba1589584b828f375bcb4c2cbcb36.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*GfpUrOEjNVzI8Bgy.png"/><p class="iy iz gj gh gi ja jb bd b be z dk oy di oz pa translated">arXiv 和 PubMed 上的总结结果[1]</p></figure></div><div class="ab cb"><figure class="or iv pb ot ou ov ow paragraph-image"><div role="button" tabindex="0" class="pc pd di pe bf pf"><img src="../Images/ab4115afa8c3033212fbb8513204364c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*pgcU2yp0P8s_ieAt.png"/></div></figure><figure class="or iv pg ot ou ov ow paragraph-image"><img src="../Images/7ec693614aaf8599fe3a1364a8e7a807.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*qDe_AmiQcAvWahn9.png"/><p class="iy iz gj gh gi ja jb bd b be z dk ph di pi pa translated">bigPatent 和新闻编辑室的总结结果[1]</p></figure></div><p id="4075" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">性能最好的 TLM (TLM-I+E (G，M))在除了 ROUGE-L 之外的大多数 ROUGE 得分指标上都超过了以前的抽象结果。我们认为这可能是因为我们没有适当的复制机制，这使得在大型 n 元文法上获得精确匹配非常具有挑战性。下图支持了这一假设，因为话语感知模型的复制机制可以从源文档中复制多达 25 个字母。此外，下图还展示了我们的 TLM 通过生成的摘要和源文档之间低百分比的 n 元语法重叠生成了比以前的工作更抽象的摘要。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/20fd41eb7b50248a822d27e477afecc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*bL4uoaugoTvNJG0m.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">对生成的摘要进行 n 元语法重叠分析[1]</p></figure><p id="b28b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们还通过在训练和测试中包括基本事实提取的句子来测量我们的 TLM (TLM-I+E (G，G))的上限性能。最后，下图展示了 TLM 生成的摘要的定性结果。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="pc pd di pe bf pf"><div class="gh gi pj"><img src="../Images/a18743212ea40d5a5ca6c8eb2783c082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KGRd5XjRGYr2PUZT.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">提议模型生成的摘要示例[1]</p></figure><h1 id="0d15" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">结论和未来工作</h1><p id="ff80" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">生成的摘要的流畅性和连贯性是很强的。然而，仍然存在抽象概要产生虚构/不准确内容的问题。在评估总结模型时，潜在的未来工作可以更加关注事实的正确性和一致性。</p><h2 id="1e7e" class="ns mi jf bd mj nt nu dn mn nv nw dp mr lm nx ny mt lq nz oa mv lu ob oc mx jl bi translated">来源:</h2><p id="8b80" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1] Subramanian，s .，Li，r .，Pilault，j .和 Pal，c .，2019。基于 transformer 语言模型的抽取和抽象神经文档摘要。<em class="lz"> arXiv 预印本 arXiv:1909.03186 </em>。</p><p id="02ba" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于 2020 年 4 月 22 日</em><a class="ae jc" href="https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarization-with-transformer-language-models/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="0eb8" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="6dde" class="ne nf jf lf b lg mz lj na lm od lq oe lu of ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="d616" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="60ca" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitivity-analysis-f 24 a2 EC 1 ca 32</a></li><li id="3173" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="80e4" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="a708" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="76e1" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li></ul><h1 id="9df0" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">总结</h1><ul class=""><li id="0a0c" class="ne nf jf lf b lg mz lj na lm od lq oe lu of ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="de59" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li></ul><h1 id="c165" class="mh mi jf bd mj mk og mm mn mo oh mq mr ku oi kv mt kx oj ky mv la ok lb mx my bi translated">其他人</h1><ul class=""><li id="bf02" class="ne nf jf lf b lg mz lj na lm od lq oe lu of ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="2d3d" class="ne nf jf lf b lg nn lj no lm np lq nq lu nr ly pk nk nl nm bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li></ul></div></div>    
</body>
</html>