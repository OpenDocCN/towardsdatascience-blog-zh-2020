<html>
<head>
<title>Topic Modeling the comment section from a New York Times article</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纽约时报文章评论部分的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-the-comment-section-from-a-new-york-times-article-e4775261530e?source=collection_archive---------40-----------------------#2020-06-04">https://towardsdatascience.com/topic-modeling-the-comment-section-from-a-new-york-times-article-e4775261530e?source=collection_archive---------40-----------------------#2020-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/238e61a3ed6f1d6c6705572cd148d9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T5QBR8rogcbAZwcI"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@andrewtneel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安德鲁·尼尔</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><div class=""><h2 id="0a97" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用潜在狄利克雷分配(LDA)和非负矩阵分解(NMF)主题建模方法</h2></div><p id="4f69" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每次我读《纽约时报》的文章，我通常喜欢通读一些评论，以便对一个故事有更好的看法。然而，有时有成千上万的评论，我没有时间通读它们，我想知道主要讨论点的要点。</p><p id="5431" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">建立一个快速的主题建模脚本可以帮助我抓住人们在文章评论部分讨论的关键主题的要点。</strong></p><p id="5825" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意:在本教程中，我们将使用潜在狄利克雷分配(LDA)和非负矩阵分解(NMF)方法。我不会深入解释它是如何工作的，因为有许多关于medium的优秀文章在讨论这个问题。严格来说，这是一个编程教程，可以帮助您快速设置主题建模代码。</p><h2 id="5a84" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤1:收集意见</h2><p id="e02d" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我将分析这篇<a class="ae jd" href="https://www.nytimes.com/2020/06/02/us/george-floyd-video-autopsy-protests.html" rel="noopener ugc nofollow" target="_blank">纽约时报文章</a>的评论部分。</p><p id="d590" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我上周发表了一篇关于如何从《纽约时报》的一篇文章中收集评论的中型文章。在继续本教程之前，请先遵循该教程。</p><div class="ip iq gp gr ir mq"><a rel="noopener follow" target="_blank" href="/how-to-collect-comments-from-any-new-york-times-article-to-a-pandas-dataframe-a595ec6a1ddf"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd jh gy z fp mv fr fs mw fu fw jf bi translated">如何从《纽约时报》的任何一篇文章中收集对熊猫数据框架的评论</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">《纽约时报》( NYT)最精彩的部分是他们对文章积极且高度节制的评论部分。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ix mq"/></div></div></a></div><h2 id="db85" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">第二步:设置事物</h2><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="afe1" class="ls lt jg nk b gy no np l nq nr">#Importing Required Plugins<br/>import pandas as pd<br/>import numpy as np</span><span id="a55d" class="ls lt jg nk b gy ns np l nq nr">#load csv file to Pandas Dataframe <br/>df = pd.read_csv('nyt_comments.csv', index_col = 0)</span></pre><p id="5c61" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你按照我的教程从这个<a class="ae jd" rel="noopener" target="_blank" href="/how-to-collect-comments-from-any-new-york-times-article-to-a-pandas-dataframe-a595ec6a1ddf">中型职位</a>，你的数据帧应该看起来像这样。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/82c80f7e879a995dd5b29ed6b224e1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3547GaIdWpGA1ufTe7Qv-g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">保存NYT评论的数据帧示例</p></figure><h2 id="246e" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤3:预处理文本数据。</h2><p id="c783" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">对于主题建模，我们使用的数据类型是文本数据，它需要某种形式的预处理，以使结果更加清晰。</p><p id="05ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在预处理阶段，通常:</p><ul class=""><li id="2204" class="nu nv jg kx b ky kz lb lc le nw li nx lm ny lq nz oa ob oc bi translated">将所有单词设为小写</li><li id="1f0f" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">删除标点符号</li><li id="1240" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">删除停用词(and、the等。)</li><li id="2cd1" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">把单词还原成它们的词根形式，这叫做词干化。</li></ul><p id="5df4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">注意:我在下面的脚本中使用了SnowballStemmer，但是你可以使用任何其他的词干分析器。</em></p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="2b1b" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤4:使用Sklearn的tfidf矢量器将我们的文本数据转换成词频逆文档矩阵</h2><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><blockquote class="ok ol om"><p id="eba6" class="kv kw lr kx b ky kz kh la lb lc kk ld on lf lg lh oo lj lk ll op ln lo lp lq ij bi translated">TF–IDF值与单词在文档中出现的次数成比例增加，并由语料库中包含该单词的文档数抵消，这有助于调整某些单词通常更频繁出现的事实。——维基百科</p></blockquote><p id="98c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">注意:您可以在我们的TfidfVectorizer上将min_df设置为您喜欢的任何值，但是我选择了0.05，因为它似乎对我的数据运行很有效。</em></p><h2 id="cdf8" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤5:使用LDA进行主题建模</h2><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/c1f9e2232525305c19a9bc4ff6dda9a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_mmbMNVZeId07B6SUvbQPQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">LDA模型生成的15个主题的输出。</p></figure><p id="20c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章的评论部分，我们可以清楚地发现一些有趣的话题。</p><p id="6d8f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，我想知道如果我们采用前25个单词而不是前15个，我们是否可以获得更多一点的上下文。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/bea57d063142ae6a7a4f9716bc4e7247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2m5g7XCHoCXl7DCfWFAdg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输出前25个单词的15个主题</p></figure><p id="bf2e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就我们对各种主题的理解而言，这要好得多。</p><p id="a944" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以采取的另一个步骤是增加或减少我们的模型搜索的主题数量，而不仅仅是增加热门词的显示。</p><p id="fc1b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">这就是我们需要创造性思维的地方，在分析我们模型结果的输入中。</strong></p><p id="03e0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，让我们快速了解一下当我们将生成的主题数量减少到10个时会发生什么。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/b16cb0d42597255e783d0f40c8db0d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sd-7KSiK-1P554-r8IRjQQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">LDA模型的10个主题的输出。</p></figure><p id="7e6e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看起来生成的主题越少，在文章评论部分区分不同主题的效果就越好。</p><p id="4037" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，有些主题并不真正有意义，或者它告诉我们同样的事情。这可能是因为这些评论已经在讨论一篇特定的文章，可以确定的主题可能有限，并且许多相同的词被重复使用。</p><h2 id="5e11" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤6:用pyLDAvis插件可视化LDA模型生成的主题</h2><p id="1f6f" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated"><a class="ae jd" href="https://pypi.org/project/pyLDAvis/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>插件生成了一个交互式的图，在我看来，这让我更容易分析生成的各种主题以及我的LDA模型的表现。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/447eb438350a089aa162d1ce53ccf36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_tBXTkh2J0iZBEC-AOTpQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">pyLDAvis的互动情节</p></figure><h2 id="d691" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">第七步:用NMF进行主题建模</h2><p id="6511" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">老实说，设置并没有完全不同，完全一样。但是，这个算法会吐出不同的结果。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ou"><img src="../Images/2b26f63706baaabbf069c7db9c6809de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O9Ei-hIHEGjk6XCtzGBxaA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">NMF模型的5个主题的输出。</p></figure><p id="aca5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">进一步减少主题的数量似乎可以更好地区分评论中讨论的各种主题。</p><p id="e577" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">注意:不幸的是，您不能使用pyLDAvis插件来可视化NMF模型的主题。</em></p><p id="d6b2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">好了，这就是你如何建立一个主题建模的快速脚本，以确定NYT文章评论部分的各种讨论主题。</strong></p></div></div>    
</body>
</html>