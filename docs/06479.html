<html>
<head>
<title>Contrasting contrastive loss functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对比对比损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e?source=collection_archive---------12-----------------------#2020-05-23">https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e?source=collection_archive---------12-----------------------#2020-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="67d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于对比学习的四种对比损失函数综合指南</h2></div><p id="9022" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" rel="noopener" target="_blank" href="/contrastive-loss-for-supervised-classification-224ae35692e7">在之前的一篇文章</a>中，我写了关于监督分类中的对比学习，并在MNIST数据集上做了一些实验，发现<a class="ae le" href="https://arxiv.org/abs/2004.11362" rel="noopener ugc nofollow" target="_blank">科斯拉<em class="lf">等人</em>提出的两阶段方法。2020年论文</a>通过学习具有对比损失的有意义嵌入，确实显示了监督分类任务的显著改进。后来我发现我的实验实际上使用了与科斯拉等人不同的对比损失函数。已提议。尽管共享相同的直觉，即关于它们的标签，显式地对比彼此的例子，但是不同的对比损失函数可以有它们自己的细微差别。在这篇文章中，我将回顾一系列对比损失函数，并比较它们在监督分类任务中的性能。</p><h1 id="092a" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">初步的</h1><p id="b49e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">对比损失函数是为<a class="ae le" href="https://en.wikipedia.org/wiki/Similarity_learning" rel="noopener ugc nofollow" target="_blank">度量学习</a>发明的，它旨在学习测量一对对象之间的相似性或距离的相似性函数。在分类的上下文中，期望的度量将使得具有相同标签的一对示例比具有不同标签的一对示例更加相似。深度度量学习涉及深度神经网络，将数据点嵌入到具有非线性的低维空间，然后使用对比损失函数来优化神经网络中的参数。最近的研究项目已经将深度度量学习应用于自我监督学习、监督学习甚至强化学习，例如<a class="ae le" href="https://arxiv.org/abs/1911.12247" rel="noopener ugc nofollow" target="_blank">对比训练的结构化世界模型(C-SWMs) </a>。</p><p id="ede2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了在深度度量学习的背景下回顾不同的对比损失函数，我使用以下形式化。让𝐱作为输入特征向量，𝑦作为它的标签。设𝑓(⋅)是将输入空间映射到嵌入空间的编码器网络，设𝐳=𝑓(𝐱)是嵌入向量。</p><h1 id="f1ed" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">对比损失函数的类型</h1><p id="ca24" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">这里我按时间顺序回顾四个对比损失函数。我稍微修改了几个函数的名称，以突出它们与众不同的特点。</p><h2 id="f283" class="md lh it bd li me mf dn lm mg mh dp lq kr mi mj ls kv mk ml lu kz mm mn lw mo bi translated">1.最大利润对比损失(Hadsell等人，2006年)</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/6274c7e8b3534378f5916c9bf03a0390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-N6WvtkpPPDNDKmxZZAjA.png"/></div></div></figure><p id="4072" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最大间隔对比损失函数以一对嵌入向量<strong class="kk iu"> z_i </strong>和<strong class="kk iu"> z_j </strong>作为输入。如果它们具有相同的标签(<em class="lf"> y_i=y_j </em>)，那么它们之间的欧几里德距离基本上相等，否则等于<a class="ae le" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">铰链损耗</a>。它有一个余量参数<em class="lf"> m &gt; 0 </em>来对具有不同标签的一对样本之间的距离施加一个下限。</p><h2 id="5976" class="md lh it bd li me mf dn lm mg mh dp lq kr mi mj ls kv mk ml lu kz mm mn lw mo bi translated">2.三重态损失(Weinberger等人，2006年)</h2><p id="331e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">三元组丢失对其标签遵循𝑦_𝑖=𝑦_𝑗和𝑦_𝑖≠𝑦_𝑘.的三元组向量进行操作也就是说，三个矢量中的两个(𝐳_𝐢和𝐳_𝐣)共享相同的标签，而第三个矢量𝐳_𝐤具有不同的标签。在三元组学习文献中，它们分别被称为锚(<strong class="kk iu"> z_i </strong>)、正(<strong class="kk iu"> z_j </strong>)和负(<strong class="kk iu"> z_k </strong>)。三重态损耗定义为:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nb"><img src="../Images/e35bb37e4f06556baf3439ff4a4f2955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGD1h8mokDaHVUN7zqzbaQ.png"/></div></div></figure><p id="99aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中，𝑚也是一个边缘参数，要求锚正和锚负之间的距离增量大于𝑚.这个损失函数的直觉是将负样本推到邻域之外一个余量，同时将正样本保持在邻域内。这是一个很好的图形演示，显示了原始论文中三重态损失的影响:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/b351338864233256272e367668d47139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DJpArhpURP7FN2UH240XWg.png"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">使用三重态损失的训练前后(来自Weinberger等人，2005)</p></figure><p id="76a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">三重开采</strong></p><p id="9a1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据三联体丢失的定义，在任何训练之前，三联体可能有以下三种情况:</p><ul class=""><li id="cad7" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated"><strong class="kk iu">简单</strong>:损失为0的三元组，因为负数已经比正数离锚点多了一个裕量</li><li id="a893" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><strong class="kk iu"/>:三连音，其中负数比正数更接近锚点</li><li id="bbff" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><strong class="kk iu">半硬</strong>:三连音，负片位于页边</li></ul><p id="050b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">三重损失已经被用于在<a class="ae le" href="https://arxiv.org/abs/1503.03832" rel="noopener ugc nofollow" target="_blank">面网中学习面的嵌入(Schroff <em class="lf">等人</em>)。2015)论文</a>。施罗夫<em class="lf">等人</em>。认为三元组挖掘对于模型性能和收敛性至关重要。他们还发现，最难的三元组在训练早期导致局部最小值，特别是导致模型崩溃，而半难的三元组产生更稳定的结果和更快的收敛。</p><h2 id="2dcb" class="md lh it bd li me mf dn lm mg mh dp lq kr mi mj ls kv mk ml lu kz mm mn lw mo bi translated">3.多类N对损耗(Sohn 2016)</h2><p id="bb90" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">多类N对损失是三重损失的推广，允许在多个负样本之间进行联合比较。当应用于一对阳性样品时，𝐳_𝐢和𝐳_𝐣与2𝑁样品具有相同的标签(𝑦_𝑖=𝑦_𝑗),计算公式如下:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nv"><img src="../Images/4703055c673d07af99369a8bc2b0823d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bAzSYNMHerMjPifl0fG2g.png"/></div></div></figure><p id="8453" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">，其中<strong class="kk iu"> z_i </strong> <strong class="kk iu"> z_j </strong>为内积，当两个向量都有单位范数时等价于余弦相似。</p><p id="dcc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如下图所示，N对丢失同时将<em class="lf"> 2N-1 </em>负样本<strong class="kk iu">推开</strong>，而不是一次一个:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nw"><img src="../Images/8004ac0baf57f78afeb552efc302b9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWfBU3mHIUoa7Y-IJKfXuQ.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">三重态损失(左)及其延伸(N+1)-三重态损失(右)(来自Sohn 2016)</p></figure><p id="f60e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过一些代数运算，多类N对损耗可以写成如下:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nx"><img src="../Images/60ce0460550c197a7741f9748d417141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTILK4EruULJyUraUm1JJw.png"/></div></div></figure><p id="a363" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种形式的多类N对损失帮助我们引入下一个损失函数。</p><h2 id="dc07" class="md lh it bd li me mf dn lm mg mh dp lq kr mi mj ls kv mk ml lu kz mm mn lw mo bi translated">4.监督NT-Xent损失(Khosla等人，2020年)</h2><p id="3dcd" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们先来看一下NT-Xent loss的自监督版本。NT-Xent是由<a class="ae le" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank">陈<em class="lf">等人</em>杜撰的。2020，是“归一化温度标度交叉熵损失”的简称。它是对多类n对损耗的修改，增加了温度参数(𝜏)来衡量余弦相似性:</a></p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ny"><img src="../Images/fd6fc9c95ddf83477a5af2d8c34bd38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7PZmRt95lcoR_iZRvqxIxw.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">自我监督NT-Xent损失</p></figure><p id="808e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">陈<em class="lf">等</em>发现一个合适的温度参数可以帮助模型从硬底片中学习。此外，他们还表明，最佳温度在不同的批次大小和训练时期数上有所不同。</p><p id="c117" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">科斯拉<em class="lf">等人</em>。用于监督学习的后来扩展的NT-Xent损失:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nz"><img src="../Images/aef2e114145910e01df8a0bf40531420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*py0ANbnM9LeFlYwrGoc99g.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">监督NT-Xent损失</p></figure><h1 id="9368" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">实验结果</h1><p id="d4e3" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">接下来，我评估这些对比损失函数是否可以帮助编码器网络学习数据的有意义的表示，以帮助分类任务。遵循与我前一篇文章中<a class="ae le" href="https://docs.google.com/presentation/d/1iB59aKvWtjeN2ZYlPih2ygH8R4J8YS6k3iGQJNfukeE/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">完全相同的实验设置，使用小批量(32)和低学习率(0.001)，我发现除了具有硬负挖掘的三重损失之外，所有这些对比损失函数都优于没有阶段1预训练的MLP基线:</a></p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oa"><img src="../Images/3e33dc06a5bd4254c155e8b49b08decc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IcjGSYz64_6CY8WJQd5eew.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">在MNIST和时尚MNIST数据集的保留测试集上的性能(准确性)(来自具有硬负挖掘的三元组的结果未显示)。</p></figure><p id="b905" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些结果证实了在网络的编码器部分的预训练中使用对比损失函数对于后续分类的益处。它还强调了三重态开采对三重态损失的重要性。具体来说，半硬开采在这些实验中效果最好，这与FaceNet的论文一致。</p><p id="01d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Chen <em class="lf">等人</em> (SimCLR)和Khosla <em class="lf">等人</em>都使用非常大的批量和更高的学习率来获得更好的NT-Xent损失性能。接下来，我用不同的批量32、256和2048进行了实验，学习率分别为0.001、0.01和0.2。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ob"><img src="../Images/7683404a5c52f0dca4e43e0546c1f071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T_VUUF5lphx0CpnspbmB_Q.png"/></div></div></figure><p id="32e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果表明，对于所有损失函数，性能随着批量的增加而降低。虽然半硬负挖掘的三重丢失在中小批量上表现非常好，但它非常占用内存，我的16G RAM不可能处理2048的批量。与同类产品相比，受监督的NT-Xent损失确实在较大批量上表现得相对更好。如果我要优化温度参数，受监督的NT-Xent可能还有改进的空间。我用的温度是0.5。</p><p id="df89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我检查了使用对比损失函数学习的嵌入的PCA投影，以查看它们是否在预训练阶段学习了任何信息表示。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oc"><img src="../Images/3001b5affabdb4dc515f227d1eba119a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGEQDMv65LayhfywaJWLMQ.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated"><strong class="bd od">MNIST数据集上不同对比损失函数和批量大小的编码网络学习嵌入的PCA投影。</strong>从左到右:通过1)最大边际损失学习的预测；2)半硬开采的三重损失；3)多类N对损失；4)监督NT-Xent丢失。从上到下:批量大小为32，256，2048。</p></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oe"><img src="../Images/9928ff2cda66dd7f36f1beaa494038d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zIV2dYmslG9JhhAaHfaUDQ.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated"><strong class="bd od">显示MNIST数据集模型学习的PCA投影密度的联合图。</strong>从左至右:通过1)最大边际损失学习的预测；2)半硬开采的三重损失；3)多类N对损失；4)监督NT-Xent丢失。从上到下:批量大小为32，256，2048。</p></figure><p id="ba9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从彩色PCA投影和它们的密度来判断，我们可以看到最大余量和监督NT-Xent为每个类学习更紧密的聚类，而来自半硬挖掘的三重丢失的聚类最大程度地扩大，但仍然是独特的。随着批量大小的增加，在多类N对损失和最大边际损失中，表示质量退化，但在监督NT-Xent损失中不那么严重，这表明这种损失对于更大的批量大小确实更稳健。</p><p id="8685" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是在更困难的时尚MNIST数据集上的学习表示的PCA投影。总的来说，它显示了与MNIST相似的观察结果。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi of"><img src="../Images/b053b1784352f4df821114415f41755f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Ep8SpZtf6_VepduHNEzdA.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated"><strong class="bd od">不同对比损失函数和批量大小的编码网络学习到的嵌入在时尚MNIST数据集上的PCA投影。</strong>从左至右:通过1)最大边际损失学习的预测；2)半硬开采的三重损失；3)多类N对损失；4)监督NT-Xent丢失。从上到下:批量大小为32，256，2048。</p></figure><h1 id="e82c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated"><strong class="ak">总结</strong></h1><p id="3d7b" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">对比损失函数对于通过学习有用的表示来改进监督分类任务非常有帮助。最大利润和监督NT-Xent损失在实验数据集(MNIST和时尚MNIST)中表现最佳。此外，NT-Xent损失对于大批量是稳健的。</p><p id="5b10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">值得注意的是，这里回顾的所有对比损失函数都具有超参数，例如输入向量的裕度、温度、相似性/距离度量。这些超参数可能会严重影响其他研究的结果，因此应该针对不同的数据集进行优化。</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="d867" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些实验所用的代码可以在这里找到:<a class="ae le" href="https://github.com/wangz10/contrastive_loss" rel="noopener ugc nofollow" target="_blank">https://github.com/wangz10/contrastive_loss</a></p><h1 id="e21f" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">参考</h1><ul class=""><li id="1e3f" class="nh ni it kk b kl ly ko lz kr on kv oo kz op ld nm nn no np bi translated"><a class="ae le" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank">哈德塞尔，r .，乔普拉，s .&amp;纽约勒村(2006年6月)。通过学习不变映射进行降维。</a>2006年IEEE计算机学会计算机视觉和模式识别会议(CVPR’06)(第2卷，第1735-1742页)。IEEE。</li><li id="181e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae le" href="https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf" rel="noopener ugc nofollow" target="_blank">温伯格，K. Q .，布利泽，j .，&amp;索尔，L. K. (2006)。用于大间隔最近邻分类的距离度量学习。</a>神经信息处理系统的进展(第1473-1480页)。</li><li id="3d91" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae le" href="https://arxiv.org/abs/1503.03832" rel="noopener ugc nofollow" target="_blank">f .施罗夫，d .卡列尼琴科，&amp;j .菲尔宾(2015)。Facenet:人脸识别和聚类的统一嵌入。</a>IEEE计算机视觉和模式识别会议论文集(第815-823页)。</li><li id="7729" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae le" href="https://papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective" rel="noopener ugc nofollow" target="_blank"> Sohn，K. (2016)。具有多类n对损失目标的改进深度度量学习。</a>神经信息处理系统的进展(第1857-1865页)。</li><li id="6d87" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">t .陈、s .科恩布利思、m .&amp;辛顿G. (2020)。视觉表征对比学习的简单框架。</a> arXiv预印本arXiv:2002.05709</li><li id="8f0d" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae le" href="https://arxiv.org/pdf/2004.11362.pdf" rel="noopener ugc nofollow" target="_blank"> Khosla，p .，Teterwak，p .，Wang，c .，Sarna，a .，Tian，y .，Isola，p .，… &amp; Krishnan，D. (2020)。监督对比学习。arXiv预印本arXiv:2004.11362。</a></li></ul></div></div>    
</body>
</html>