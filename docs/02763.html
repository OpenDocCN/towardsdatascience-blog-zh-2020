<html>
<head>
<title>Feature Importance with Time Series and Recurrent Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列和递归神经网络的特征重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-importance-with-time-series-and-recurrent-neural-network-27346d500b9c?source=collection_archive---------7-----------------------#2020-03-17">https://towardsdatascience.com/feature-importance-with-time-series-and-recurrent-neural-network-27346d500b9c?source=collection_archive---------7-----------------------#2020-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f975" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">想象并解释你的深度学习模型想要对你说什么</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ed032381f77d2968a0fa3a1452760521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gVtBNoMZUpgl8NmX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@metatzon297?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Shin Roran </a>拍摄的照片</p></figure><p id="d353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络通常被认为是黑盒算法。这是真的，但是通过一些技巧和一些有用的外部推理技术，我们可以从中提取大量信息。惊人的例子来自深度学习对图像的应用。从网络中提取内部特征有助于理解网络是如何工作的:著名的热图表示是指图像中模型做出决策的感兴趣区域。</p><p id="ad64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个知识提取过程背后的概念可以推广到深度学习的每个应用领域，如NLP和时间序列预测。实际上，我们可以用几行代码查询我们训练好的神经网络，检查输出滤波器对最终预测的重要性。</p><p id="d80b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，<strong class="lb iu">我调查了一个被训练用来预测未来的神经网络所做的决定</strong>。我使用了一个递归结构来自动学习时间维度的信息。通过一些简单的步骤，我们提取了理解模型输出所需的所有内容。我们这样做不需要使用任何额外的外部库，只是简单地压缩模型所学的东西。</p><h1 id="6d21" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><p id="c1f4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们实验的数据是从UCI知识库中收集的。顾名思义,<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data" rel="noopener ugc nofollow" target="_blank">北京PM2.5数据集</a>存储pm2.5每小时观测值(从2010年到2015年)以及其他天气回归量，如温度、压力、风力和降雨量。我们的范围是提前一步预测pm2.5的准确值(下一小时的浓度)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/41a42fe4e5776efd886f2d39d0ee9d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4ZtdK_IWyV3Mw3eayWDyA.png"/></div></div></figure><p id="f208" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用前三年作为训练集，后两年分别用于验证和测试。</p><p id="0475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练我们的序列神经网络，我们将数据适当地重新排列为维度的3D序列:<em class="mt">样本x时间维度x特征</em>。下面是一个生成的长度为30小时的序列的示例，其中每个序列的值都用标准比例进行了标准化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/f73ea85115a11c43e43f25a009d52ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fl9AM0lgrNPdx1aSYkKF6Q.png"/></div></div></figure><p id="dda3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们模型的输入样本示例。我们还用平均值和标准差来标准化目标。</p><h1 id="32d5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">模型</h1><p id="39c0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们用于时间序列预测的神经网络结构如下:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="16fe" class="na lw it mw b gy nb nc l nd ne">def get_model(params):<br/>    <br/>    inp = Input(shape=(sequence_length, len(columns)))</span><span id="377a" class="na lw it mw b gy nf nc l nd ne">    x = GRU(params['units_gru'], return_sequences=True)(inp)<br/>    x = AveragePooling1D(2)(x)<br/>    x = Conv1D(params['units_cnn'], 3, <br/>               activation='relu', padding='same', <br/>               name='extractor')(x)<br/>    x = Flatten()(x)<br/>    out = Dense(1)(x)<br/>    <br/>    model = Model(inp, out)<br/>    model.compile(optimizer=Adam(lr=params['lr']), loss='mse')<br/>    <br/>    return model</span></pre><p id="abc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">递归层处理初始序列，以序列的形式返回完整的输出。平均池压缩输出，然后由一维卷积层处理。最后，在生成最终预测之前，所有的都被展平，变成2D维度。下面是测试集的子样本中预测值和实际值的比较(这是一个演示，我们对性能不感兴趣)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/67b87f59912db5ac0afb27e36354aab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bceFKZL5chshd9_AO3X8VA.png"/></div></div></figure><p id="c9d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练好我们的神经网络后，我们就可以检验这些预测了。我提供了对模型输出的两种解释，对于每一个测试序列都可以很容易地检索到。</p><h1 id="d871" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">梯度重要性</h1><p id="48be" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的想法是检查每个单一输入特征对最终预测输出的贡献。在我们的例子中，贡献是由从预测的输入序列的微分操作中获得的梯度值给出的。</p><p id="d217" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Tensorflow，该方法的实施只需3个步骤:</p><ul class=""><li id="748f" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated">使用<em class="mt"> GradientTape </em>对象捕捉输入的梯度；</li><li id="e37b" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">用<em class="mt"> tape.gradient </em>获取梯度:该操作产生与单个输入序列相同形状的梯度(<em class="mt">时间维度x特征)；</em></li><li id="3ebe" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">获取每个序列特征在时间维度上的平均影响。</li></ul><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="0ae9" class="na lw it mw b gy nb nc l nd ne">def gradient_importance(seq, model):</span><span id="0a1f" class="na lw it mw b gy nf nc l nd ne">    seq = tf.Variable(seq[np.newaxis,:,:], dtype=tf.float32)</span><span id="1eb1" class="na lw it mw b gy nf nc l nd ne">    with tf.GradientTape() as tape:<br/>        predictions = model(seq)</span><span id="98bb" class="na lw it mw b gy nf nc l nd ne">    grads = tape.gradient(predictions, seq)<br/>    grads = tf.reduce_mean(grads, axis=1).numpy()[0]<br/>    <br/>    return grads</span></pre><p id="8b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果的一些图形示例如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/7ef640d1275b875d35fe6e8e2f1db816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3-nFPKbujcajpcOBJ4DCfg.png"/></div></div></figure><p id="64cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均梯度越高，该特征对最终预测的影响就越大。因此，对于上面显示的每种情况，我们可以确定在输入序列中，哪个变量对于预测下一个小时的pm2.5浓度值更重要。</p><h1 id="7528" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预测的激活图</h1><p id="f47d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">可视化哪个输入要素对预测影响最大有助于检测奇怪的行为。然而，它对神经网络为什么做出决定给出了更少的见解。这种方法倾向于强调输入的哪个特定部分影响输出值。</p><p id="4ec5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">得出这些结论的过程是基于之前所做的相同假设。换句话说，我们需要操作梯度重要性。现在的区别是，这个操作是用一些内部滤波器计算的，而不是用原始输入序列。给定我们的内部卷积/递归滤波器，我们对输出进行梯度运算。为了确定每个过滤器在决策中的重要性，我们取其权重的平均值(梯度重要性),并将每个图乘以其相应的权重。如果激活图在向前传递过程中变亮，并且其梯度很大，这意味着激活的区域对决策有很大的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/ee6ba3d56dde545191a04d662d85dd67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3_bEsLgyyM5jzcSZ-U-zQ.png"/></div></div></figure><p id="9903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用代码语言来说，几乎不需要做什么改变；特别是在我们操作归一化的最后，我们需要调整维度，以匹配输入序列。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="7dd0" class="na lw it mw b gy nb nc l nd ne">def activation_grad(seq, model):<br/>    <br/>    seq = seq[np.newaxis,:,:]<br/>    grad_model = Model([model.inputs], <br/>                       [model.get_layer('extractor').output, <br/>                        model.output])</span><span id="7815" class="na lw it mw b gy nf nc l nd ne">    # Obtain the predicted value and the intermediate filters<br/>    with tf.GradientTape() as tape:<br/>        seq_outputs, predictions = grad_model(seq)</span><span id="6a62" class="na lw it mw b gy nf nc l nd ne">    # Extract filters and gradients<br/>    output = seq_outputs[0]<br/>    grads = tape.gradient(predictions, seq_outputs)[0]</span><span id="841f" class="na lw it mw b gy nf nc l nd ne">    # Average gradients spatially<br/>    weights = tf.reduce_mean(grads, axis=0)</span><span id="61f6" class="na lw it mw b gy nf nc l nd ne">    # Get a ponderated map of filters according to grad importance<br/>    cam = np.ones(output.shape[0], dtype=np.float32)<br/>    for index, w in enumerate(weights):<br/>        cam += w * output[:, index]</span><span id="02fd" class="na lw it mw b gy nf nc l nd ne">    time = int(seq.shape[1]/output.shape[0])<br/>    cam = zoom(cam.numpy(), time, order=1)<br/>    heatmap = (cam - cam.min())/(cam.max() - cam.min())<br/>    <br/>    return heatmap</span></pre><h1 id="0fa7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="09f2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇文章中，我为时间序列预测建立了一个序列神经网络。我们的目的是从中提取有用的信息。为此，我们提供了两种不同的方法，基于梯度计算，在训练后进行推理。我已经表明，对于时序神经网络(递归/卷积)，在时间序列应用程序中，可以用简单的技巧和几行代码来研究深度模型的行为。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="5639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="d28d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>