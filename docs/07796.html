<html>
<head>
<title>Logistic Regression with Python Using Optimization Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用优化函数的Python逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-with-python-using-optimization-function-91bd2aee79b?source=collection_archive---------19-----------------------#2020-06-10">https://towardsdatascience.com/logistic-regression-with-python-using-optimization-function-91bd2aee79b?source=collection_archive---------19-----------------------#2020-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="56d9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习用python编写逻辑回归算法来执行二元分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/50cd8f704106a1ca9fe3c5d942895342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*ojZtVV6BrjqDbVvhwz25xQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">资料来源:Franck V. 的Unsplash</p></figure><p id="ec88" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">逻辑回归是一个强大的分类工具。只有当因变量是分类变量时，它才适用。有几种不同的方法来实现它。今天我将解释一个简单的方法来执行二进制分类。我将使用python中可用的优化函数。</p><h2 id="c79a" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">概念和公式</h2><p id="7f81" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">逻辑回归使用sigmoid函数来估计返回值从0到1的输出。因为这是二进制分类，所以输出应该是0或1。这是sigmoid函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/b7e3370e616d64c20c73519f5f5383dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*rFpYApi3IoT-S-HN7cvjgQ.png"/></div></figure><p id="c544" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里z是输入变量X和随机初始化的系数θ的乘积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/86b7b09188d897513a089a319baf8ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*2uT0_yE_sJQTjSaVgG4XuA.png"/></div></figure><p id="640d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">需要为每个输入要素初始化一个θ值。成本函数中一个非常重要的参数。成本函数给出了预测与实际产出的差距。下面是成本函数的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/14cfec09f906e988d3f72a48ca678251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*utYepafL5fXZ4ETpmejVZA.png"/></div></figure><p id="1fc7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里，y是原始输出变量，h是预测输出变量。我们的目标是尽可能降低成本。现在，我们需要更新θ值，以便我们的预测尽可能接近原始输出变量。如果我们用θ对成本函数进行偏导数，我们将找到θ值的梯度。我不想在这里讨论微积分。我们用来更新θ的梯度下降将会是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/4d2b7b5afb00012ae44bba774517d5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*V0fWZ20oyixl0E76bx-Ilw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/64909b4443ba9af37e28eee46bacd710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*qbcCH8qke7Guwx-0WsJmHA.png"/></div></figure><p id="5549" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你不理解所有的方程式，先不要担心。请看实现部分。希望你能理解如何使用所有的方程。</p><h2 id="b24d" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">逻辑回归的Python实现</h2><ol class=""><li id="0a0d" class="mu mv it kx b ky mk lb ml le mw li mx lm my lq mz na nb nc bi translated">导入必要的包和数据集。我从吴恩达在Coursera上的机器学习课程中找到了这个数据集。</li></ol><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="b7d9" class="lr ls it ne b gy ni nj l nk nl">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>df = pd.read_csv('ex2data1.txt', header=None)<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/16c3e2f8e7666dedb3a63f5e1504527c.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*eoc7l8eIvn7wISr0pCPmlw.png"/></div></figure><p id="519e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.将输入变量和输出变量分开。在该数据集中，列0和1是输入变量，列2是输出变量。所以我们必须预测第二列。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="7d81" class="lr ls it ne b gy ni nj l nk nl">X = df.iloc[:, :-1]<br/>y = df.iloc[:, -1]</span></pre><p id="6b80" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.向x添加一个偏差列。偏差列的值通常为1。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="a323" class="lr ls it ne b gy ni nj l nk nl">X = np.c_[np.ones((X.shape[0], 1)), X]<br/>X[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f2a25a6f8ef5e64047efb27e9d85392f.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*AliVSwyZTKm1tdyK5jstWA.png"/></div></figure><p id="b24f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4.这里，我们的X是二维数组，y是一维数组。让我们把“y”做成二维的，以匹配维度。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="8b92" class="lr ls it ne b gy ni nj l nk nl">y = y[:, np.newaxis]<br/>y[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/93f65d610f3f54d111d35a74be4853f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*_rkH7D5SPYNQpMXGsduOLA.png"/></div></figure><p id="5b76" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">5.定义sigmoid函数</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="b273" class="lr ls it ne b gy ni nj l nk nl">def sigmoid(x, theta):<br/>    z= np.dot(x, theta)<br/>    return 1/(1+np.exp(-z))</span></pre><p id="f267" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">6.使用此sigmoid函数写出预测输出的假设函数:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f6f6" class="lr ls it ne b gy ni nj l nk nl">def hypothesis(theta, x):<br/>    return sigmoid(x, theta)</span></pre><p id="5122" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">7.使用上面解释的公式写出成本函数的定义。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="44cd" class="lr ls it ne b gy ni nj l nk nl">def cost_function(theta, x, y):<br/>    m = X.shape[0]<br/>    h = hypothesis(theta, x)<br/>    return -(1/m)*np.sum(y*np.log(h) + (1-y)*np.log(1-h))</span></pre><p id="d070" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">8.根据上式写出梯度下降函数:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="662a" class="lr ls it ne b gy ni nj l nk nl">def gradient(theta, x, y):<br/>    m = X.shape[0]<br/>    h = hypothesis(theta, x)<br/>    return (1/m) * np.dot(X.T, (h-y))</span></pre><p id="bf28" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">9.导入一个优化函数，为我们优化theta。这个优化将把要优化的函数、梯度函数和要传递给函数的参数作为输入。在这个问题中，要优化的函数是成本函数。因为我们希望最小化成本，梯度函数将是gradient_descent，参数是X和y。该函数还将采用“x0 ”,这是要优化的参数。在我们的例子中，我们需要优化θ。所以，我们必须初始化θ。我把θ值初始化为零。正如我前面提到的，我们需要为每个输入特征初始化一个θ值。我们有三个输入特性。如果你看X，我们有0和1列，然后我们增加了一个偏差列。所以，我们需要初始化三个θ值。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="cbca" class="lr ls it ne b gy ni nj l nk nl">theta = np.zeros((X.shape[1], 1))<br/>from scipy.optimize import minimize,fmin_tnc<br/>def fit(x, y, theta):<br/>    opt_weights = fmin_tnc(func=cost_function, x0=theta, fprime=gradient, args=(x, y.flatten()))<br/>    return opt_weights[0]<br/>parameters = fit(X, y, theta)</span></pre><p id="5366" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">参数出来是[-25.16131854，0.20623159，0.20147149]。</p><p id="9d8f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">10.使用这些参数作为θ值和假设函数来计算最终假设。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f34b" class="lr ls it ne b gy ni nj l nk nl">h = hypothesis(parameters, X)</span></pre><p id="a045" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">11.使用假设来预测输出变量:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="cb77" class="lr ls it ne b gy ni nj l nk nl">def predict(h):<br/>    h1 = []<br/>    for i in h:<br/>        if i&gt;=0.5:<br/>            h1.append(1)<br/>        else:<br/>            h1.append(0)<br/>    return h1<br/>y_pred = predict(h)</span></pre><p id="8ba3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">12.计算准确度。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="e71e" class="lr ls it ne b gy ni nj l nk nl">accuracy = 0<br/>for i in range(0, len(y_pred)):<br/>    if y_pred[i] == y[i]:<br/>        accuracy += 1<br/>accuracy/len(y)</span></pre><p id="dbd4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终准确率为89%。</p><p id="b10d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">您也可以使用梯度下降作为优化函数来执行此逻辑回归。下面是一篇实现梯度下降优化方法的文章:</p><ol class=""><li id="bfd4" class="mu mv it kx b ky kz lb lc le np li nq lm nr lq mz na nb nc bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-in-python-from-scratch-to-end-with-real-dataset-12998f7b5739">梯度下降优化逻辑回归。</a></li><li id="ee52" class="mu mv it kx b ky ns lb nt le nu li nv lm nw lq mz na nb nc bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/multiclass-classification-with-logistic-regression-one-vs-all-method-from-scratch-using-python-853037783616">用梯度下降法进行多类分类。</a></li><li id="1116" class="mu mv it kx b ky ns lb nt le nu li nv lm nw lq mz na nb nc bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/polynomial-regression-from-scratch-in-python-a8d64845495f">Python中的多项式回归从无到有。</a></li></ol><p id="849f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">转到此页面获取数据集:</p><div class="nx ny gp gr nz oa"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/ex2data1.txt" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">rashida 048/用Python进行机器学习</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">Permalink GitHub是5000多万开发人员的家园，他们一起工作来托管和审查代码、管理项目以及…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">github.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ko oa"/></div></div></a></div><p id="e69f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">以下是完整的工作代码:</p><div class="nx ny gp gr nz oa"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/logisticRegressionWithOptimizationFunc.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">rashida 048/用Python进行机器学习</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">github.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo ko oa"/></div></div></a></div></div></div>    
</body>
</html>