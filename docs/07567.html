<html>
<head>
<title>Interpretable Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-clustering-39b120f95a45?source=collection_archive---------36-----------------------#2020-06-07">https://towardsdatascience.com/interpretable-clustering-39b120f95a45?source=collection_archive---------36-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/205f8058d38fb6082f4aa05cc37702a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oAHvxyBDEHLWVc_4"/></div></div></figure><div class=""/><div class=""><h2 id="719f" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">如何使用 CART 来消除描述集群时的猜测</h2></div></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><p id="889b" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">聚类算法如 K-Means、凝聚聚类和 DBSCAN 是强大的无监督机器学习技术。然而，总结每个集群的关键特征需要相当定性的方法，成为需要领域专业知识的冗长且不严格的过程。</p><p id="a581" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一项经常被忽视的技术可能是数据科学家武器库中的王牌:使用决策树来定量评估每个集群的特征。具体来说，在对未标记的数据进行聚类之后，我们可以将相应的聚类作为标签分配给每个样本。然后，我们可以使用标签作为目标变量来训练一个购物车模型，然后检查生成的决策树来突出显示集群的特征。</p><h1 id="6425" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">数据聚类</h1><p id="d873" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">在对数据进行相应的处理后，我们可以选择我们喜欢的聚类算法。通常，选项有:</p><ul class=""><li id="2673" class="mt mu je lc b ld le lg lh lj mv ln mw lr mx lv my mz na nb bi translated">k 均值</li><li id="c7ab" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">凝聚聚类</li><li id="6aad" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">基于密度的噪声应用空间聚类</li></ul><h2 id="6869" class="nh lx je bd ly ni nj dn mc nk nl dp mg lj nm nn mi ln no np mk lr nq nr mm ns bi translated">k 均值</h2><p id="c58b" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">K-Means 是最常见的无监督学习技术，主要是由于它的简单性和有效性。这是一种快速算法，可以在数百万次观察中运行，但它也有缺点。首先，它不能很好地处理非球形的数据结构。当点的密度不均匀时，也就是说在分布的某些区域中密度比其他区域中的密度高，它的表现也不好。最后，还必须选择聚类数 K，这需要做出某种程度上的定性决定。</p><p id="0353" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然对于前两个问题，除了选择另一种算法之外没有补救方法，但是可以以相对严格的方式选择聚类的数量 K。我们可以迭代地运行 K-Means 算法，增加 K 的数量，直到一个稍大的数字(例如 50)。然后，我们可以在图表上为 K 的每个值绘制一个性能度量。通过检查图表，我们可以找到“肘部”，即性能达到峰值的地方，然后在较高的 K 值下产生边际回报。我们可以选择的 K 值正好是性能增益相对于 K 的增加处于边际的时候。在许多性能测量中，最常用的通常是轮廓系数、失真和差距统计。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/3cbae33848ec2accfeef9e80799c2e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43Q2DXu_cFdWOCls-TrMFA.png"/></div></div></figure><h2 id="d16f" class="nh lx je bd ly ni nj dn mc nk nl dp mg lj nm nn mi ln no np mk lr nq nr mm ns bi translated">凝聚聚类</h2><p id="1cf1" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">凝聚聚类是一种分层聚类，它迭代地将样本分组在一起，从每个样本的一个聚类开始，到整体的单个聚类。这是执行聚类时的首选方法之一，因为它允许快速选择 k 的最佳值。事实上，我们可以绘制树状图，这是一种在 x 轴上显示样本分组的树结构，而在 y 轴上显示来自其他分组的信息增益。</p><p id="b431" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">类似于肘分析，我们可以简单地选择从进一步分裂中获得的信息略微增加的聚类数，因此不值得额外的复杂性。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/6918bd84899921ab72c31526f44a155a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DRI2NkET3UPUqVA46G7_ow.png"/></div></div></figure><h2 id="4501" class="nh lx je bd ly ni nj dn mc nk nl dp mg lj nm nn mi ln no np mk lr nq nr mm ns bi translated">基于密度的噪声应用空间聚类</h2><p id="13e7" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">最后，作为参考，DBSCAN 是另一种广泛用于聚类分析的技术。它的工作原理是检查附近样本的密度，如果密度足够高，就将样本分配给聚类。然而，由于凝聚聚类已经是一种成功的技术，DBSCAN 在实践中不太常见。</p><h1 id="08cd" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">解释集群</h1><p id="9002" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">现在我们已经对未标记的数据进行了聚类，我们可以提取聚类信息并将其作为标签分配给每个样本。</p><p id="3424" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">例如，使用凝聚聚类，我们可以首先选择我们选择的聚类数 K...</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="b9b5" class="nh lx je oa b gy oe of l og oh">cut = scipy.cluster.hierarchy.cut_tree(Z, n_clusters=K)</span></pre><p id="5dfa" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">...然后提取标签，将它们分配给数据:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="b42f" class="nh lx je oa b gy oe of l og oh">labels = list([i[0] for i in cut])<br/>labeled_data = pd.DataFrame(data, columns=data_columns)<br/>labeled_data['label'] = labels</span></pre><p id="631a" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，我们可以继续使用标记数据集作为训练来训练决策树。为了避免一棵巨大的树，我建议把它构造成一个二元分类问题，如果一个点在所选的聚类中，y 等于 1，否则 y 等于 0。这将为每个集群重复，给你不同的更简单的决策树，更容易理解，并在以后提出。具体来说，我们可以绘制决策树，使用:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="ac56" class="nh lx je oa b gy oe of l og oh">fig, axes = mp.subplots(nrows = 1, <br/>                        ncols = 1, <br/>                        figsize = (4,4), <br/>                        dpi=300)</span><span id="a04a" class="nh lx je oa b gy oi of l og oh">sklearn.tree.plot_tree(model,<br/>                       feature_names = X.columns,<br/>                       filled = True,<br/>                       class_names=True);</span></pre><p id="9f3f" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通过检查决策树，我们可以突出相应聚类的特征。例如，我们可能会看到，只有在某些条件下，观测值才会被分配到这样的集群中。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/2ac3933bc82e5ba05c808d4d943c0ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGKkC-G7P0LAK6cD5t8qMw.png"/></div></div></figure><h1 id="1fd3" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">摘要</h1><p id="57e8" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">我们可以通过结合无监督和有监督的学习技术来阐明聚类。具体来说，我们可以:</p><ul class=""><li id="db2f" class="mt mu je lc b ld le lg lh lj mv ln mw lr mx lv my mz na nb bi translated">首先，使用 K-Means、凝聚聚类或 DBSCAN 对未标记的数据进行聚类</li><li id="9380" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">然后，我们可以选择要使用的聚类数 K</li><li id="ef10" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">我们给每个样本分配标签，使其成为一个监督学习任务</li><li id="4fea" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">我们训练一个决策树模型</li><li id="5cae" class="mt mu je lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">最后，我们检查决策树的输出，以定量地突出集群的特征</li></ul><h1 id="9506" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">参考</h1><p id="6ff7" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">麻省理工学院的一些同事最近完成了一项有趣的工作，利用优化使聚类变得可解释。你可以在这里阅读更多相关信息<a class="ae ok" href="https://arxiv.org/pdf/1812.00539.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><p id="24ec" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="ol">要阅读更多类似的文章，请关注我的</em><a class="ae ok" href="https://twitter.com/jayzuccarelli" rel="noopener ugc nofollow" target="_blank"><em class="ol">Twitter</em></a><em class="ol"/><a class="ae ok" href="https://www.linkedin.com/in/ezuccarelli" rel="noopener ugc nofollow" target="_blank"><em class="ol">LinkedIn</em></a><em class="ol">或我的</em> <a class="ae ok" href="https://eugeniozuccarelli.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ol">网站</em> </a> <em class="ol">。</em></p></div></div>    
</body>
</html>