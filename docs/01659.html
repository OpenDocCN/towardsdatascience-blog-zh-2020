<html>
<head>
<title>How to Tokenize Tweets with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python 对 Tweets 进行标记</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7?source=collection_archive---------4-----------------------#2020-02-15">https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7?source=collection_archive---------4-----------------------#2020-02-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59ea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们应该选择 TweetTokenizers 还是其他 4 种常见的 Tokenizers？</h2></div><h1 id="8d55" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">什么是标记化？</h1><p id="9ad5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">记号是整体的一个片段，所以单词是句子中的记号，句子是段落中的记号。记号化是将一个字符串分割成一系列记号的过程<strong class="lc iu">。</strong></p><p id="1836" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你对标记化有点熟悉，但不知道文本使用哪种标记化，本文将使用 Twitter 上的原始 Tweets 来展示不同的标记化及其工作原理。</p><p id="50e2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本文将介绍如何用以下语句将句子标记成单词:</p><ul class=""><li id="28c6" class="mb mc it lc b ld lw lg lx lj md ln me lr mf lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">word_tokenize</code></li><li id="5ea6" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">WordPunctTokenizer</code></li><li id="78b7" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">RegrexTokenizer</code></li><li id="b7f2" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">TweetTokenizer</code></li></ul><p id="b314" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">标记化是预处理原始文本的第一步，所以我希望您对掌握这个重要的概念感到兴奋！</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi mt"><img src="../Images/3013929ccb887d8b3038f3e2fbba348f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mADiyrSVymvJURRa"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">照片由<a class="ae nj" href="https://unsplash.com/@eprouzet?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Eric Prouzet </a>拍摄于<a class="ae nj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="9cbb" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">要处理的数据</h1><p id="a6b1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Twitter 是一个社交平台，每天都会发布许多有趣的推文。因为与正式文本相比，tweets 更难标记，所以我们将使用 tweets 中的文本数据作为示例。</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="8c9a" class="no kj it mn b gy np nq l nr ns">"https://t.co/9z2J3P33Uc FB needs to hurry up and add a laugh/cry button 😬😭😓🤢🙄😱 Since eating my feelings has not fixed the world's problems, I guess I'll try to sleep... HOLY CRAP: DeVos questionnaire appears to include passages from uncited sources <a class="ae nj" href="https://t.co/FNRoOlfw9s" rel="noopener ugc nofollow" target="_blank">https://t.co/FNRoOlfw9s</a> well played, Senator Murray Keep the pressure on: <a class="ae nj" href="https://t.co/4hfOsmdk0l" rel="noopener ugc nofollow" target="_blank">https://t.co/4hfOsmdk0l</a> @datageneral thx Mr Taussig It's interesting how many people contact me about applying for a PhD and don't spell my name right."</span></pre><p id="d110" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">上面的句子里有很多信息。在对整个句子进行记号化之前，我们先挑选一些我们有兴趣比较的句子。这个列表将用于比较不同令牌化器之间的性能。</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="0305" class="no kj it mn b gy np nq l nr ns">compare_list = ['<a class="ae nj" href="https://t.co/9z2J3P33Uc'" rel="noopener ugc nofollow" target="_blank">https://t.co/9z2J3P33Uc'</a>,<br/>               'laugh/cry',<br/>               '😬😭😓🤢🙄😱',<br/>               "world's problems",<br/>               "<a class="ae nj" href="http://twitter.com/datageneral" rel="noopener ugc nofollow" target="_blank">@datageneral</a>",<br/>                "It's interesting",<br/>               "don't spell my name right",<br/>               'all-nighter']</span></pre><h1 id="b22c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">word_tokenize</h1><p id="5048" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">将句子标记成单词时最流行的方法是使用<code class="fe mk ml mm mn b">word_tokenize.</code>和<strong class="lc iu">空格和标点符号将单词分开。</strong></p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="dfee" class="no kj it mn b gy np nq l nr ns">from nltk.tokenize import word_tokenize</span><span id="a386" class="no kj it mn b gy nt nq l nr ns">word_tokens = []<br/>for sent in compare_list:<br/>    print(word_tokenize(sent))<br/>    word_tokens.append(word_tokenize(sent))</span></pre><p id="fbd9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">结果:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="30b6" class="no kj it mn b gy np nq l nr ns">['https', ':', '//t.co/9z2J3P33Uc']<br/>['laugh/cry']<br/>['😬😭😓🤢🙄😱']<br/>['world', "'s", 'problems']<br/>['@', 'datageneral']<br/>['It', "'s", 'interesting']<br/>['do', "n't", 'spell', 'my', 'name', 'right']<br/>['all-nighter']</span></pre><p id="5ada" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们希望<code class="fe mk ml mm mn b">laugh/cry</code>被拆分成 2 个单词。所以我们应该考虑另一个记号赋予器选项。</p><h1 id="29cf" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">单词标点符号化器</h1><p id="db94" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe mk ml mm mn b">WordPunctTokenizer</code> <strong class="lc iu">将所有标点符号</strong>拆分成单独的记号。所以这可能就是我们想要的？</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="c639" class="no kj it mn b gy np nq l nr ns">from nltk.tokenize import WordPunctTokenizer</span><span id="564a" class="no kj it mn b gy nt nq l nr ns">punct_tokenizer = WordPunctTokenizer()</span><span id="94b1" class="no kj it mn b gy nt nq l nr ns">punct_tokens = []<br/>for sent in compare_list:<br/>    print(punct_tokenizer.tokenize(sent))<br/>    punct_tokens.append(punct_tokenizer.tokenize(sent))</span></pre><p id="96fb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">结果:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="1168" class="no kj it mn b gy np nq l nr ns">['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc']<br/>['laugh', '/', 'cry']<br/>['😬😭😓🤢🙄😱']<br/>['world', "'", 's', 'problems']<br/>['@', 'datageneral']<br/>['It', "'", 's', 'interesting']<br/>['don', "'", 't', 'spell', 'my', 'name', 'right']<br/>['all', '-', 'nighter']</span></pre><p id="adeb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">嗯，这个分词器成功地把<code class="fe mk ml mm mn b">laugh/cry</code>拆分成两个单词。但是缺点是:</p><ul class=""><li id="4d4d" class="mb mc it lc b ld lw lg lx lj md ln me lr mf lv mg mh mi mj bi translated">链接<code class="fe mk ml mm mn b">‘<a class="ae nj" href="https://t.co/9z2J3P33Uc'" rel="noopener ugc nofollow" target="_blank">https://t.co/9z2J3P33Uc'</a></code>被分成 7 个单词</li><li id="c86d" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">world's</code>被<code class="fe mk ml mm mn b">"'"</code>字符拆分成两个单词</li><li id="9288" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">@datageneral</code>分为<code class="fe mk ml mm mn b">@</code>和<code class="fe mk ml mm mn b">datageneral</code></li><li id="40cc" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated"><code class="fe mk ml mm mn b">don't</code>被拆分为<code class="fe mk ml mm mn b">do</code>和<code class="fe mk ml mm mn b">n't</code></li></ul><p id="4098" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">既然这些单词应该被认为是一个单词，那么这个分词器也不是我们想要的。有没有一种方法可以根据空格来拆分单词？</p><h1 id="925e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">再氧化器</h1><p id="c6fd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由于没有分词器专门根据空格拆分单词，我们可以使用<code class="fe mk ml mm mn b">RegrexTokenizer</code>来控制如何对文本进行分词。有两种方法可以避免根据标点符号或缩写来拆分单词:</p><ul class=""><li id="4a16" class="mb mc it lc b ld lw lg lx lj md ln me lr mf lv mg mh mi mj bi translated">在代币上匹配</li><li id="24c5" class="mb mc it lc b ld mo lg mp lj mq ln mr lr ms lv mg mh mi mj bi translated">匹配分隔符或间隙</li></ul><h2 id="338b" class="no kj it bd kk nu nv dn ko nw nx dp ks lj ny nz ku ln oa ob kw lr oc od ky oe bi translated">在代币上匹配</h2><p id="a374" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe mk ml mm mn b">RegexpTokenizer</code>类通过<strong class="lc iu">编译我们的模式</strong>，然后在我们的文本上调用<code class="fe mk ml mm mn b">re.findall()</code>来工作。我们可以利用这个函数来匹配字母数字标记和单引号</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="8a0d" class="no kj it mn b gy np nq l nr ns">from nltk.tokenize import RegexpTokenizer<br/>match_tokenizer = RegexpTokenizer("[\w']+")</span><span id="3285" class="no kj it mn b gy nt nq l nr ns">match_tokens = []<br/>for sent in compare_list:   <br/>    print(match_tokenizer.tokenize(sent))<br/>    match_tokens.append(match_tokenizer.tokenize(sent))</span></pre><p id="f6b6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果您不熟悉正则表达式语法，<code class="fe mk ml mm mn b">\w+</code>匹配一个或多个单词字符(字母数字&amp;下划线)</p><p id="5319" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">结果:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="8ff9" class="no kj it mn b gy np nq l nr ns">['https', 't', 'co', '9z2J3P33Uc']<br/>['laugh', 'cry']<br/>[]<br/>["world's", 'problems']<br/>['datageneral']<br/>["It's", 'interesting']<br/>["don't", 'spell', 'my', 'name', 'right']<br/>['all', 'nighter']</span></pre><p id="b833" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">虽然像<code class="fe mk ml mm mn b">'world’s', 'It’s', 'don’t’</code>这样的单词如我们所愿被保留为一个实体，但是<code class="fe mk ml mm mn b">‘<a class="ae nj" href="https://t.co/9z2J3P33Uc'" rel="noopener ugc nofollow" target="_blank">https://t.co/9z2J3P33Uc'</a></code>仍然被拆分成不同的单词，并且我们失去了<code class="fe mk ml mm mn b">“datageneral”</code>之前的<code class="fe mk ml mm mn b">“@”</code>字符。也许我们可以根据空白分割？</p><h2 id="c979" class="no kj it bd kk nu nv dn ko nw nx dp ks lj ny nz ku ln oa ob kw lr oc od ky oe bi translated">空白匹配</h2><p id="036c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe mk ml mm mn b">RegexpTokenizer</code>也可以通过<strong class="lc iu">匹配缺口</strong>来工作。当添加参数<code class="fe mk ml mm mn b">gaps=True</code>时，匹配模式将被用作分隔符。<code class="fe mk ml mm mn b">\s+</code>匹配一个或多个空格。</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="08e1" class="no kj it mn b gy np nq l nr ns">space_tokenizer = RegexpTokenizer("\s+", gaps=True)</span><span id="c636" class="no kj it mn b gy nt nq l nr ns">space_tokens = []<br/>for sent in compare_list:<br/>    <br/>    print(space_tokenizer.tokenize(sent))<br/>    space_tokens.append(space_tokenizer.tokenize(sent))</span></pre><p id="c014" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">结果:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="3cc6" class="no kj it mn b gy np nq l nr ns">['https://t.co/9z2J3P33Uc']<br/>['laugh/cry']<br/>['😬😭😓🤢🙄😱']<br/>["world's", 'problems']<br/>['@datageneral']<br/>["It's", 'interesting']<br/>["don't", 'spell', 'my', 'name', 'right']<br/>['all-nighter']</span></pre><p id="c761" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">不错！现在我们将链接<code class="fe mk ml mm mn b">‘https://t.co/9z2J3P33Uc'</code>解释为一个单词！但是看起来表情符号组合成了一个单词。由于不同的表情符号在情感分析中可能是有意义的，我们可能希望将它们分成不同的单词。所以我们需要考虑另一种正则表达式模式来实现这一点。</p><p id="5210" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">好消息！有一个标记器可以在不使用正则表达式的情况下有效地拆分 tweets。</p><h1 id="c4a7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">TweetTokenizer</h1><p id="7d06" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">是的，对 tweet 进行标记的最好方法是使用标记器来标记 tweet</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="3d00" class="no kj it mn b gy np nq l nr ns">from nltk.tokenize import TweetTokenizer<br/>tweet_tokenizer = TweetTokenizer()</span><span id="a0e8" class="no kj it mn b gy nt nq l nr ns">tweet_tokens = []<br/>for sent in compare_list:<br/>    print(tweet_tokenizer.tokenize(sent))<br/>    tweet_tokens.append(tweet_tokenizer.tokenize(sent))</span></pre><p id="11ec" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">结果:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="8d40" class="no kj it mn b gy np nq l nr ns">['https://t.co/9z2J3P33Uc']<br/>['laugh', '/', 'cry']<br/>['😬', '😭', '😓', '🤢', '🙄', '😱']<br/>["world's", 'problems']<br/>['@datageneral']<br/>["It's", 'interesting']<br/>["don't", 'spell', 'my', 'name', 'right']<br/>['all-nighter']</span></pre><p id="6456" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">厉害！推文被标记成我们想要的样子！</p><h1 id="e7a4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">把所有东西放在一起</h1><p id="b781" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以把所有东西放在一个<code class="fe mk ml mm mn b">pd.dataframe</code>中进行快速准确的解释，而不是花时间去分析每个分词器的结果。</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="ca37" class="no kj it mn b gy np nq l nr ns">import pandas as pd</span><span id="c627" class="no kj it mn b gy nt nq l nr ns">tokenizers = {'word_tokenize': word_tokens,<br/>             'WordPunctTokenize':punct_tokens,<br/>             'RegrexTokenizer for matching':match_tokens,<br/>             'RegrexTokenizer for white space': space_tokens,<br/>             'TweetTokenizer': tweet_tokens }</span><span id="7bdb" class="no kj it mn b gy nt nq l nr ns">df = pd.DataFrame.from_dict(tokenizers)</span></pre><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi of"><img src="../Images/3d850d2774795ad1bc2c8ff7a3efeee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLVWAVL1pkAOpN9CoVBehA.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">不同标记器之间的比较</p></figure><p id="95e0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">根据对上表的观察，<code class="fe mk ml mm mn b">TweetTokenizer</code>似乎是最佳选择。所以我们可以继续用这个来标记我们的句子:</p><pre class="mu mv mw mx gt nk mn nl nm aw nn bi"><span id="9a7b" class="no kj it mn b gy np nq l nr ns">tweet_tokenizer.tokenize(sent)</span></pre><h1 id="dfab" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="278f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">恭喜你！您已经从 nltk 库中学习了不同的分词器来将句子分词。似乎标记 Twitter 原始文本的赢家是<code class="fe mk ml mm mn b">TweetTokenizer</code>。但情况并非总是如此，你的选择可能会根据你分析的文本而改变。重要的一点是，您知道这些标记器的功能差异，这样您就可以做出正确的选择来标记您的文本。在这个<a class="ae nj" href="https://github.com/khuyentran1401/Data-science/blob/master/nlp/tweets_tokenize.ipynb" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中，您可以随意使用本文的代码。</p><p id="8377" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我喜欢写一些基本的数据科学概念，并尝试不同的算法和数据科学工具。你可以在<a class="ae nj" href="https://www.linkedin.com/in/khuyen-tran-1401/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae nj" href="https://twitter.com/KhuyenTran16" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上和我联系。</p><p id="a120" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你想查看我写的所有文章的代码，请点击这里。在 Medium 上关注我，了解我的最新数据科学文章，例如:</p><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">用美丽的声音抓取维基百科</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">关于如何使用 Beautiful Soup 的分步教程，这是一个用于 web 抓取的简单易用的 Python 库</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox nd oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/find-common-words-in-article-with-python-module-newspaper-and-nltk-8c7d6c75733"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">用 Python 模块 Newspaper 和 NLTK 查找文章中的常用词</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">使用 newspaper3k 和 NLTK 从报纸中提取信息和发现见解的分步指南</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox nd oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/python-tricks-for-keeping-track-of-your-data-aef3dc817a4e"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">跟踪数据的 Python 技巧</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">如何用列表、字典计数器和命名元组来跟踪信息</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="oz l ou ov ow os ox nd oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/maximize-your-productivity-with-python-6110004b45f7"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">使用 Python 最大化您的生产力</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">你创建了一个待办事项清单来提高效率，但最终却把时间浪费在了不重要的任务上。如果你能创造…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="pa l ou ov ow os ox nd oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/timing-the-performance-to-choose-the-right-python-object-for-your-data-science-project-670db6f11b8e"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">高效 Python 代码的计时</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">如何比较列表、集合和其他方法的性能</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="pb l ou ov ow os ox nd oj"/></div></div></a></div></div></div>    
</body>
</html>