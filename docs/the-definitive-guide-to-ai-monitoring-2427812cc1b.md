# 人工智能监控的权威指南

> 原文：<https://towardsdatascience.com/the-definitive-guide-to-ai-monitoring-2427812cc1b?source=collection_archive---------28----------------------->

## 从我们的工作中学习跨深度学习和机器学习用例为团队创建生产可见性

![](img/75400550df6dccdb709315b8b8812ad0.png)

图片来自 [Shutterstock](https://www.shutterstock.com/image-vector/business-people-analytics-monitoring-investment-finance-1499251079) (标准许可下)

*本文与* [*Itai 吧 Sinai*](https://medium.com/u/dae6c2441260?source=post_page-----2427812cc1b--------------------------------) *合著。*

各垂直行业的人工智能团队强烈同意，他们的数据和模型必须在生产中受到监控。然而，许多团队很难准确定义要监控的内容。具体来说，在“推断时间”收集什么数据，跟踪什么指标以及如何分析这些指标。

人工智能系统的多样性和复杂性决定了[“一刀切”的监控方法是行不通的](/the-platform-approach-to-ai-monitoring-dcc43dee6c6e)。然而，我们在这里提供一些清晰度，并讨论普遍适用的方法。

在与多个垂直行业的团队(以及深度学习和机器学习)合作后，我们听到了一些一致的动机，包括:

*   希望更快地解决问题
*   强烈需要从“被动”转变为“主动”，即在业务 KPI 受到负面影响或客户抱怨之前检测数据和模型问题

那么，你应该如何跟踪和分析你的 AI 呢？

# 1.定义模型性能指标

为生产人工智能获得成功的客观测量需要你的推断数据的标签或“基础事实”。可能出现这种情况的几个例子包括:

*   人在回路的机制，注释者、客户或第三方至少标记一个推理数据样本。例如，欺诈检测系统接收实际欺诈交易的列表(事后)。
*   商业 KPI 可以提供一种“标签”。例如，对于一个搜索或推荐模型，您可以跟踪点击或转换(与每个推断联系在一起)。

顺便说一句，后者可能导致监控的圣杯——能够精确地评估模型对业务结果的影响(积极的或消极的)。

标签的可用性使得能够计算和分析常见的模型验证指标，例如假阳性/假阴性率、误差/损失函数、AUC/ROC、精确度/召回率等。

需要注意的是，上面提到的标签通常在推断时不可用。可能在模型运行(例如，用户点击推荐的广告)几秒后，但也可能在模型运行(例如，商家向欺诈系统通知真实的欺诈交易)几周后，“基本事实”反馈才可用。因此，人工智能监控系统应该能够异步更新标签(和其他类型的数据)。

## 关于监控注释器的说明

不用说，被标记的数据的好坏取决于标记过程和标记它的注释者。具有前瞻性思维的人工智能团队利用监控能力来评估他们的注释过程和注释者。你会怎么做？一个例子是跟踪模型和注释器之间的平均差值。如果这个度量超过了某个阈值——我们可以假设要么是模型表现不佳，要么是注释器出错了。

> **“…监控的圣杯——能够精确评估模型对业务成果的影响(积极或消极)。”**

# 2.建立模型输出的粒度行为指标

跟踪模型输出是必须的。

从一个角度来看，输出行为可以指示通过查看其他地方几乎检测不到的问题(即，模型的高灵敏度可能意味着输入中几乎检测不到的变化可能真的“脱离模型”)。从另一个角度来看，输入要素可能会发生重大变化，但不会对输出行为产生太大影响。因此，基于产出的指标是监控范围内的首要任务。

以下是从模型输出中创建的一些指标示例:

*   原始分数的基本统计分析，例如欺诈概率分数的周平均值和标准差
*   置信度得分/区间，例如，

1.  距决策边界的距离(例如，距 SVM 模型中的超平面，或使用简单阈值时)
2.  多类别分类模型中所选类别和第二位置之间的差值

*   在分类模型中，所选类别的分布
*   未分类的比率(即，当您的班级没有一个分数超过您的阈值时)

总的来说，基于输出创建的度量中的异常告诉团队有事情正在发生。为了理解原因，以及是否和如何解决正在发生的事情，团队应该将功能和元数据包括在监控范围内。以下是更多相关信息。

# 3.单独或作为一个集合跟踪要素行为

跟踪特征行为有两个目的:

1.  解释在输出行为中检测到的变化
2.  检测上游阶段的问题(例如，数据接收和准备)

当在输出行为中检测到问题时，可能会调用特性来解释原因。在这种情况下，解释问题的过程可能需要进行特征重要性分析，利用一系列流行的方法，如 [SHAP](https://github.com/slundberg/shap) 和[莱姆](https://github.com/marcotcr/lime)。

另外，跟踪功能行为的变化是另一种不看输出就能发现问题的独立方法。那么，哪些上游事件可能表现为异常特征行为？太多了，数不过来。一些例子包括:

*   新客户涌入等业务变化
*   外部数据源的变化(例如，新浏览器、新设备)
*   先前管道中引入的更改，例如，数据摄取代码新版本中的错误

由于上述原因，收集和分析生产中的特征行为是监控范围的关键部分。

# 4.收集元数据以正确划分指标行为

到目前为止，我们已经讨论了为了创建行为度量而收集的数据类别。这些指标可以在全球范围内进行跟踪和分析。然而，为了真正实现监控的价值，必须考虑模型运行的子部分的行为度量。

例如(有点琐碎)，广告服务模型可能整体表现一致，但为退休人员提供的推荐逐渐变差(通过点击率下降来衡量)，这通过为年轻专业人员提供的逐渐更好的推荐来平衡(通过点击率增加来代表)。人工智能团队希望了解每个子群体的行为，并在必要时采取纠正措施。

基于片段的行为分析的关键使能因素是全面收集关于模型运行的上下文元数据。这些上下文元数据通常存在于人工智能系统中，但对模型的特性没有贡献。

以下是元数据驱动细分价值的几个额外示例:

*   **合规性评估:**一家银行希望确保其承销模式不偏向(或反对)特定性别或种族。性别和种族不是模型特征，但却是评估模型指标并确保其符合贷款法规的重要维度。
*   **根本原因分析:**一个营销团队发现有一个消费者子群体，推荐模型对他们不太有效。通过元数据驱动的细分，他们能够将这些消费者与特定的设备和浏览器相关联。经过进一步分析，意识到该特定设备和浏览器的数据接收过程存在缺陷。

## 关于模型版本的注释

有助于跟踪的元数据的另一个突出例子是模型版本(以及 AI 系统中其他组件的版本)。这使得恶化的行为与对系统的实际改变相关联。

> “基于细分的行为分析的关键使能因素是全面收集关于模型运行的上下文元数据。”

# 5.在训练、测试和推理期间跟踪数据

在推理时进行全面的监控可以产生巨大的好处。然而，为了更深入地了解人工智能系统，前瞻性思维团队将监控范围扩大到包括训练和测试数据。当模型在推理时表现不佳时，能够将该数据段的特征分布与模型被训练时的对应分布进行比较，可以提供对行为变化的根本原因的最佳洞察。

如果可能，我们强烈建议跟踪上面讨论的相同元数据字段，在记录训练运行时也是如此。通过这样做，团队可以真正地比较相应的数据段，并更快、更准确地找到问题的根源。

# 摘要

评估复杂人工智能系统在生产中的性能和行为具有挑战性。一项全面的监测战略可以真正发挥作用。

根据我们的经验，此类监控策略包括使用推断阶段甚至更晚阶段可用的数据定义模型性能指标(例如，精度、AUC/ROC 等)，建立模型输出的粒度行为指标，单独或作为一个集合跟踪特征行为，以及收集有助于分割指标行为的元数据。

建议将监控范围扩展到培训和测试阶段，以全面了解系统状态，并更快地隔离问题的根本原因。

表现最好的人工智能团队已经在实施类似的监控策略，作为他们人工智能生命周期不可或缺的一部分。这些团队对潜在生产问题的焦虑更少，而且更好的是，能够将他们的研究扩展到生产中，并随着时间的推移显著改进他们的模型。