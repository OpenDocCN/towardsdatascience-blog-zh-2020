<html>
<head>
<title>Review: Newell ECCV’16 &amp; Newell POCV’16 — Stacked Hourglass Networks (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:纽维尔·ECCV 16年和纽维尔·POCV 16年——堆叠沙漏网络(人体姿势估计)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5?source=collection_archive---------22-----------------------#2020-03-12">https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5?source=collection_archive---------22-----------------------#2020-03-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="946b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用堆叠沙漏网络的自下而上和自上而下的重复处理，胜过了<a class="ae kf" href="https://medium.com/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3" rel="noopener"> DeepCut </a>、<a class="ae kf" href="https://medium.com/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5" rel="noopener"> IEF </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------">汤普森CVPR 15</a>和<a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a>。</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/8d497e5fbeda4972e87506d11545d2a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvmniDipdMixYUfFDy0ftQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">多重堆叠沙漏网络</strong></p></figure><p id="c90c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事里，<strong class="kz ir">16级的纽维尔·ECCV和16级的纽维尔·POCV</strong>，由<strong class="kz ir">密执安大学</strong>创作，回顾。在纽维尔·POCV的16中，仅使用了2个堆叠沙漏网络，而在纽维尔·ECCV的16中，使用了8个堆叠沙漏网络。ECCV的版本更详细，引用了1500多次。因此，这里介绍ECCV的情况。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----a9eeb76d40a5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="80b8" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="acfc" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir">网络架构</strong></li><li id="f6fb" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">中间监督</strong></li><li id="36f3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">一些训练细节</strong></li><li id="e48e" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">消融研究</strong></li><li id="dfa1" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">与SOTA方法的比较</strong></li><li id="14bc" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">进一步分析</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="29a1" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。网络架构</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/c950ac64cd4f44609028e7dc8f58cafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tErO9nHD9VD2lwUn60g2OA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">单个“沙漏”模块(不包括最后的1×1层)。每个盒子对应于如下剩余模块。</strong></p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nu"><img src="../Images/b9aedc36b303a84c4ea9a39814ce233a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izUwa8kxDvxy7U_APYJC0w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">一个剩余模块。</strong></p></figure><ul class=""><li id="f031" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">卷积层和最大池层</strong>用于处理低至极低分辨率的特征。</li><li id="3dde" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">达到最低分辨率后，网络开始自上而下的上采样序列和跨尺度的特征组合。</li><li id="e299" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">对于上采样路径，<strong class="kz ir">完成较低分辨率的最近邻上采样</strong><strong class="kz ir">，随后是两组特征的逐元素相加</strong>。</li><li id="3b92" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在达到网络的输出分辨率后，<strong class="kz ir">应用两轮连续的1×1卷积</strong>来产生最终的网络预测。</li><li id="af16" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">最终设计大量使用了<strong class="kz ir">剩余模块</strong>。</li><li id="854b" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">从不使用大于3×3的过滤器。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="d09f" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">2.中间监督</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b922c64fbfdc1102bebc37c5d579a2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*ID4XVZDIx2MtvOBBWK7uDA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">中间监督</strong></p></figure><ul class=""><li id="3d71" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">网络分裂并产生一组热图(蓝色轮廓),其中可以应用损失</strong>。</li><li id="aacf" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">1×1卷积重新映射热图</strong>以匹配中间特征的通道数量。这些要素与前面沙漏中的要素一起添加。</li><li id="3024" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在最终的网络设计中，使用了<strong class="kz ir">八个沙漏</strong>。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="e9ab" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak">一些训练细节</strong></h1><ul class=""><li id="43ba" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls ny nl nm nn bi translated"><strong class="kz ir"> FLIC </strong> : 5003张图片(3987次训练，1016次测试)取自电影。</li><li id="8741" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> MPII人体姿态</strong> : 25k带注释的图像，供多人使用，提供40k带注释的样本(28k训练，11k测试)。</li><li id="2c10" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">约3000个样本的验证集。</li><li id="8f92" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">然后，所有输入图像的大小都被调整为256×256像素。我们进行数据扩充，包括旋转(30度)和缩放(0.75-1.25度)。</li><li id="3562" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">Torch7，在12GB的NVIDIA TitanX GPU上训练需要3天左右。</li><li id="8e51" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><a class="ae kf" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">使用批量标准化</a>。</li><li id="3d58" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">为了生成最终的测试预测，我们通过网络运行原始输入和图像的翻转版本，并将热图平均在一起。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oa"><img src="../Images/e0044ddf6229cf6657c645556af38dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SyNeqqbWH6N4OfoZFePAeA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">示例输出(从左到右:脖子、左肘、左手腕、右膝盖、右脚踝)</strong></p></figure><ul class=""><li id="d435" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">如上所示，网络的最终预测是给定关节的热图的最大激活位置。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="196a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 4。消融研究</strong></h1><h2 id="0fcb" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.1.变体</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi on"><img src="../Images/261b2caf360a7f0151f6255b22c89850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZY0wqtB7cn6XKLq4I4qKA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">随着培训的进展验证准确性</strong></p></figure><ul class=""><li id="c16a" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir"> HG-Half </strong>:表现最差的单个沙漏。</li><li id="3242" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> HG: </strong>单个长沙漏，性能更好。</li><li id="10c9" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> HG-Int </strong>:单个长沙漏，中间监督，性能更好。</li><li id="a831" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> HG-Stack </strong>:两个堆叠沙漏但没有中间监管，性能类似。</li><li id="dc17" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> HG-Stack-Int </strong>:两个堆叠沙漏中间监督，性能最佳。</li></ul><h2 id="6a42" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.2.沙漏数</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oo"><img src="../Images/4e1d1efa68968b71e7bb879c7a863005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zcEsmd4kB6wRsIkGCrspJA.png"/></div></div></figure><ul class=""><li id="3682" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">左</strong>:示例验证图像，说明从中间阶段(第二个沙漏)(左)到最终预测(第八个沙漏)(右)的预测变化。</li><li id="fcb3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">右</strong>:比较不同堆叠方式下网络中间阶段的验证精度。</li><li id="b5ab" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">从87.4%到87.8%再到88.1 %的每一次叠加，最终性能都会有适度的提高。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="b1a0" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">5.<strong class="ak">与SOTA方法的比较</strong></h1><h2 id="1333" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.1.警察</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi op"><img src="../Images/ed4f288f2d7cff95dd0c1caca5165a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSey-R8xW7mAwKuHsFIHTA.png"/></div></div></figure><ul class=""><li id="5ea5" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">99%的PCK@0.2精度在肘部，97%在腕部。</li><li id="eab3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">所提出的方法优于SOTA的方法，例如<a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------"> DeepPose </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------">汤普森CVPR 15</a>和<a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a>。</li></ul><h2 id="4331" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.2.MPII</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oq"><img src="../Images/3cb6d7a76d86b3324ef9b92216bc3eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brKhkQ6f2IX_FABjrDIWyA.png"/></div></div></figure><ul class=""><li id="0396" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">所提出的方法优于SOTA的方法，包括<a class="ae kf" href="https://medium.com/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3" rel="noopener"> DeepCut </a>、<a class="ae kf" href="https://medium.com/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5" rel="noopener"> IEF </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------">汤普森CVPR</a>和<a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a>。</li><li id="fd7c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在像手腕、肘部、膝盖和脚踝这样的困难关节上，所提出的方法在SOTA结果的基础上平均提高了3.5% (PCKh@0.5)，平均错误率从16.3 %下降到12.8%。</li><li id="d3b3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">最终肘关节准确率为91.2%，腕关节准确率为87.1 %。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oa"><img src="../Images/6d273ce99f51af36f9d3ecc35f08e1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMoskO0kwYBsdzeWVC8ZfA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">MPII的一些例子</strong></p></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="db16" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">6.进一步分析</h1><h2 id="1175" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">6.1多人</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oa"><img src="../Images/6323ff7c2addb20252bfeb9ccc602b2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNwJEPZ3Dqhgbzo7hhVNpw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">多个人靠在一起甚至重叠</strong></p></figure><ul class=""><li id="3d68" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">检测多人超出了本文的范围。但是作者仍然分析它。</li><li id="2247" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">通过轻微平移和改变输入图像的比例，检测到不同的人或没有人。</li></ul><h2 id="5c93" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">6.2.闭塞</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi or"><img src="../Images/9d942b33b304690e964a8bbacf0a21af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rrd2eWNguc_IkdaVs2Dqow.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">左图</strong> : PCKh曲线，验证比较仅考虑可见(或不可见)接头时的性能。<strong class="bd kw">右</strong>:显示预测一个关节是否存在注释的精确度的精确召回曲线。</p></figure><ul class=""><li id="8a22" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">在仅考虑可见关节的情况下，腕部准确度从85.5%上升到93.6%(验证性能略差于测试集性能87.1 %)。另一方面，在完全闭塞的关节上的性能是61.1 %。</li><li id="c4b0" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">对于肘部，对于可见关节，准确率从90.5%到95.1%，对于遮挡关节，准确率下降到74.0%。</li><li id="b778" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在存在注释的情况下，获得了AUC为92.1%的膝盖和AUC为96.0%的脚踝。这是在2958个样本的验证集上完成的，其中16.1%的可能膝盖和28.4%的可能脚踝没有地面真实注释。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="aaba" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">参考</h2><p id="a529" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg os li lj lk ot lm ln lo ou lq lr ls ij bi translated">【2016 ECCV】【纽维尔·ECCV’16<strong class="kz ir"><br/></strong><a class="ae kf" href="https://arxiv.org/abs/1603.06937" rel="noopener ugc nofollow" target="_blank">用于人体姿态估计的堆叠沙漏网络</a></p><p id="6290" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">【2016 POCV】【纽维尔·POCV’16<strong class="kz ir"><br/></strong>用于人体姿态估计的堆叠沙漏网络<br/>(我已经下载了但是抱歉现在找不到链接。这是一篇只有2页的论文，还不如看ECCV版，哈哈。)</p><h1 id="ed6c" class="ml mm iq bd mn mo ov mq mr ms ow mu mv jw ox jx mx jz oy ka mz kc oz kd nb nc bi translated">我以前的评论</h1><p id="5521" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg os li lj lk ot lm ln lo ou lq lr ls ij bi translated"><strong class="kz ir">图像分类</strong>[<a class="ae kf" href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17?source=post_page---------------------------" rel="noopener">lenet</a>][<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------" rel="noopener">alexnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-maxout-network-image-classification-40ecd77f7ce4?source=post_page---------------------------">max out</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-nin-network-in-network-image-classification-69e271e499ee?source=post_page---------------------------">in</a>][<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103?source=post_page---------------------------" rel="noopener">znet</a>][] [ <a class="ae kf" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c?source=post_page---------------------------" rel="noopener">感受性-v3 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc?source=post_page---------------------------">感受性-v4 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568?source=post_page---------------------------">异常</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69?source=post_page---------------------------">mobile netv 1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------">resnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e?source=post_page---------------------------">预活化 [ </a><a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------"> ResNeXt </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------">致密</a> ] [ <a class="ae kf" href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78?source=post_page---------------------------" rel="noopener">金字塔网</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=post_page---------------------------">drn</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dpn-dual-path-networks-image-classification-d0135dce8817?source=post_page---------------------------">dpn</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8?source=post_page---------------------------">残馀关注网络【t】</a></p><p id="24e0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测</strong> [ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快R-CNN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------">快R-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------">MR-CNN&amp;S-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------">DeepID-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------">CRAFT</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------">R-FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------">离子</a><a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------">DCN</a></p><p id="bd55" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义分割</strong> [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------">解码网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">深层波1&amp;</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------">CRF-rnn</a>][] [<a class="ae kf" rel="noopener" target="_blank" href="/review-fc-densenet-one-hundred-layer-tiramisu-semantic-segmentation-22ee3be434d5?source=post_page---------------------------">fc-denne</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-idw-cnn-learning-from-image-descriptions-in-the-wild-dataset-boosts-the-accuracy-807eb5ffe371?source=post_page---------------------------">idw-CNN</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-dis-dual-image-segmentation-semantic-segmentation-103477de6cbf" rel="noopener">说</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-sdn-stacked-deconvolutional-network-using-densenet-semantic-segmentation-f929c94906b" rel="noopener">sdn</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="noopener">deep labv 3+</a></p><p id="834c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割</strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6?source=post_page---------------------------" rel="noopener"/>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560?source=post_page---------------------------" rel="noopener">【T2/DCA】</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------">【u-net】</a>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6?source=post_page---------------------------" rel="noopener">【CFS-fcn】</a><a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43?source=post_page---------------------------" rel="noopener"> [ </a><a class="ae kf" href="https://medium.com/@sh.tsang/review-attention-u-net-learning-where-to-look-for-the-pancreas-biomedical-image-segmentation-e5f4699daf9f" rel="noopener">注意u-net</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-ru-net-r2u-net-recurrent-residual-convolutional-neural-network-medical-image-38945a957df1" rel="noopener">ru-net&amp;r2u-net</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-voxresnet-deep-voxelwise-residual-networks-for-volumetric-brain-segmentation-biomedical-4986df62f590" rel="noopener">voxrsnet</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-densevoxnet-volumetric-brain-segmentation-biomedical-image-segmentation-9136bb6128dd" rel="noopener">致密电子数码</a> ][ <a class="ae kf" href="https://medium.com/@sh.tsang/review-unet-a-nested-u-net-architecture-biomedical-image-segmentation-57be56859b20" rel="noopener"> UNet++ </a></p><p id="210c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割</strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------">清晰度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------"> FCIS </a></p><p id="c811" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">超分辨率</strong>[<a class="ae kf" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c?source=post_page---------------------------" rel="noopener">SR CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4?source=post_page---------------------------">fsr CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f?source=post_page---------------------------">VDSR</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350?source=post_page---------------------------" rel="noopener">ESPCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e?source=post_page---------------------------" rel="noopener">红网</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20?source=post_page---------------------------" rel="noopener">DRCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994?source=post_page---------------------------">DRRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8?source=post_page---------------------------">LapSRN&amp;MS-LapSRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8?source=post_page---------------------------">SRDenseNet</a>][【T20</p><p id="faf5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿势估计</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------">深层姿势</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------">tompson nips’14</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------">tompson cvpr’15</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener">CPM</a>[<a class="ae kf" href="https://medium.com/@sh.tsang/review-fcgn-fully-convolutional-google-net-human-pose-estimation-52022a359cb3" rel="noopener">fcgn</a></p><p id="b106" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">后处理编码解码器</strong>【arcnn】][<a class="ae kf" href="https://medium.com/@sh.tsang/review-cnn-for-h-264-hevc-compressed-image-deblocking-codec-post-processing-361a84e65b94" rel="noopener">【Lin DCC ' 16】</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-ifcnn-in-loop-filtering-using-convolutional-neural-network-codec-post-processing-1b89c8ddf417" rel="noopener">【ifcnn】</a>][<a class="ae kf" href="https://medium.com/@sh.tsang/review-cnn-for-compressed-image-deblocking-deblocking-44508bf99bdc" rel="noopener">【Li icme ' 17】</a>[<a class="ae kf" href="https://medium.com/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c" rel="noopener">vrcnn【t】</a></p><p id="997c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生成对抗网络</strong> [ <a class="ae kf" href="https://medium.com/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75" rel="noopener">甘</a></p></div></div>    
</body>
</html>