<html>
<head>
<title>All About Stochastic Gradient Descent Extension: Nesterov momentum, the simple way!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有关于随机梯度下降扩展:内斯特罗夫动量，简单的方法！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-about-stochastic-gradient-descent-extension-nesterov-momentum-the-simple-way-84685ee050dd?source=collection_archive---------45-----------------------#2020-05-21">https://towardsdatascience.com/all-about-stochastic-gradient-descent-extension-nesterov-momentum-the-simple-way-84685ee050dd?source=collection_archive---------45-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi jx"><img src="../Images/de66dd6d7e82cb008a0a4996216406b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkOXhZDX6Vym2x3ZChu2wA.jpeg"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">罗科·卡鲁索在<a class="ae kn" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2344" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们在生活中做的每件事都与<strong class="kq iu">成本相关，就像从一个地方到另一个地方会产生成本，我们希望通过选择更便宜的旅行方式或找到更短的方式来最小化成本。随着我们在生活中日复一日地做家务，我们估计做这些工作的成本，并无意识地将它们最小化，以节省我们的时间和金钱。这种最小化成本并获得最佳结果的过程就是我们可以与梯度下降联系起来的。</strong></p><h1 id="9f7a" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">概述原始梯度下降算法</strong></h1><p id="bbcb" class="pw-post-body-paragraph ko kp it kq b kr mk kt ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll im bi translated">让我们回忆一些基础知识，梯度是一个成本相对于另一个变量的微小变化。下降是移动的(向下)，进一步改变成本到它可能的最小值。<strong class="kq iu">梯度下降</strong>正在逐步达到最小成本，但它通过一次输入所有训练数据来计算每一步的成本，这对于大型数据集需要很高的计算能力，有时甚至是不可能的。为此，使用<strong class="kq iu">随机梯度下降</strong>，其中单个随机训练数据点用于计算每一步的成本。但是这个任务也变得令人讨厌，因为它在达到最小成本之前会产生大量噪声(振荡)。<strong class="kq iu">小批量梯度下降</strong>通过用小批量训练数据点计算每一步的成本来处理这个问题。这有助于减少噪音，并提供更快和更好方法来达到最低成本。</p><p id="f869" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图 1 解释了最小化成本背后的直觉。通过改变等式-1 中<strong class="kq iu"> m </strong>的值，我们可以计算梯度/随机/小批量算法的成本。突出显示的等式定义了假设的成本。如果我们的假设预测值等于真实值，那么成本为零。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mp"><img src="../Images/e143c7419876fc6ec8a6f0cf8c86eb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgLNFS-Ls0OxPUEcFcgLmQ.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图一。最小化成本背后的直觉</p></figure><p id="5f0e" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在图 2 中，我们定义了梯度下降方程。我们还在学习参数θ和特定θ值下的假设相关成本之间绘制了一条曲线。θmin 左侧所有点的斜率为负，θmin 右侧所有点的斜率为正。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mq"><img src="../Images/30d6a98af15ebe3ca11af64bc305bfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_0M75DE1REh1cWfjJqLcQ.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图二。定义梯度下降</p></figure><p id="4200" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，让我们看看梯度下降如何帮助我们的假设达到最小值(最小化成本)。正如我们从图 3 中看到的，在θmin 时成本最小，所以我们的梯度下降方程应该帮助我们得到那些成本最小的θ值，即θmin。当我们试图找到更好的θ时，我们正在向全局最小值前进。这就是梯度下降算法的妙处。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mr"><img src="../Images/0262889d2e684615d136ee5708fc8b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IS-30PfIbkDaYHNeWjFbYg.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图三。梯度下降如何帮助达到最小值</p></figure><h1 id="670d" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">动量(内斯特罗夫加速动量)</strong></h1><p id="f812" class="pw-post-body-paragraph ko kp it kq b kr mk kt ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll im bi translated">对于数据科学中的每一个算法来说，达到最佳的最优解非常重要，因为它决定了我们的模型的准确性和表现。在当前情况下，我们能多好多快地达成一个最佳解决方案(最小化成本)很重要。这就是物理学可以帮助我们实现的地方，使用一种叫做<strong class="kq iu">动量的现象。</strong>动量可以认为是物体运动时获得的一种动力。当我们把球滚下山时，它的速度会增加，因为它倾向于向山下的相同方向移动，因此动量会很快到达山底(最小成本)。类似地，如果我们在梯度下降算法中倾向于以相同的方向移动以最小化成本，那么为什么不以更高的速度移动以快速到达最小值。</p><p id="102b" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们举个例子简单解释一下这个动量概念。在图 4.1 中，我们有一个场景，一个人需要从 A 点移动到 b 点。我们假设在每次迭代中，这个人只能移动两步。比较图 4.2 中没有动量和有动量的两种情况，我们可以通过使用动量的概念大大减少到达目的地 B 的时间。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi ms"><img src="../Images/c30f086d9518ed98eb05226bb42dd76c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoRwN6Ox4w5hI0KFhEpgpQ.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图 4.1 动量方程如何帮助快速达到目标</p></figure><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mt"><img src="../Images/11865880cc9a7dc28e0cefcd0a6591d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oKxnArncpHW78evuxU2_DQ.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图 4.2 动量方程如何帮助快速达到目标</p></figure><p id="d200" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">类似地，我们可以重新考虑动量梯度下降方程，如图 5 所示。动量有助于参数θ达到成本最小的值，通过取梯度变化的大值可以快得多。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mu"><img src="../Images/2cad3e928c02dc98213fac2b364f8b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5zpCHnpY2q_B5y0MQD1zg.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图五。用随机梯度下降定义内斯特罗夫加速动量</p></figure><p id="b38c" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当我们滚一个球下山时，当球到达底部时，它有一个很大的加速度，它不会在底部停下来，而是越过底部。类似的情况也发生在我们的内斯特罗夫加速动量算法中，但是当我们移动曲线的右侧时，斜率从负值变为正值，因此它试图减小动量。最后，在底部振荡之后，对于参数θ的特定值，实现成本函数的最小值。</p><h1 id="cbb2" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">随机梯度下降中添加动量的影响</strong></h1><p id="70ea" class="pw-post-body-paragraph ko kp it kq b kr mk kt ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll im bi translated">在图 6 中，我们比较了两种梯度下降算法达到最小成本值所需的迭代次数。随机梯度下降需要 35 次迭代，而内斯特罗夫加速动量需要 11 次迭代。因此，可以清楚地看到，内斯特罗夫加速动量很快达到最小值，因为它在同方向移动时获得了动量，并在达到最小值之前进行了大的θ跳跃。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi mv"><img src="../Images/20dc885655371ef0b3dae31d93c5ae48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-zwJzbIj2T7rdyjgJbscA.png"/></div></div><p class="kj kk gj gh gi kl km bd b be z dk translated">图六。比较 SGD 和 NAM-SGD 达到全局最小值所需的步骤数</p></figure><p id="96f6" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这里有很多内容可以展示高级梯度下降算法背后的数学原理。我已经尽力使它尽可能简单。在下面的帖子中，我将涵盖其他 AdaGrad、RMSprop 和 Adam 算法，这些算法在深度学习领域的当今优化技术中被广泛使用。我会向你解释简单的方法，以及在什么时候使用哪种方法。到时候见！</p><p id="6496" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">特别感谢尤里·内斯特罗夫，这一启蒙运动背后的数学家。</p></div></div>    
</body>
</html>