<html>
<head>
<title>Reinforcement Learning, Brain, and Psychology: Classical and Instrumental Conditioning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习、大脑和心理学:经典和工具性条件作用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-brain-and-psychology-part-2-classical-and-instrumental-conditioning-217a4f0a989?source=collection_archive---------18-----------------------#2020-01-09">https://towardsdatascience.com/reinforcement-learning-brain-and-psychology-part-2-classical-and-instrumental-conditioning-217a4f0a989?source=collection_archive---------18-----------------------#2020-01-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="33ec" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/rl-and-humans" rel="noopener" target="_blank">强化学习、人工智能和人类</a></h2><div class=""/><div class=""><h2 id="8974" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">心理学研究如何启发强化学习中的主体训练？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/da9c59a6b1c456cf2480aae4fac87eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Bbw4qHjNAzUMubZpch94w.png"/></div></div></figure><blockquote class="ld"><p id="7602" class="le lf it bd lg lh li lj lk ll lm ln dk translated">"一个人相信事物是因为他已经习惯于相信它们。"—阿尔多斯·赫胥黎，美丽新世界。</p></blockquote><p id="7658" class="pw-post-body-paragraph lo lp it lq b lr ls kd lt lu lv kg lw lx ly lz ma mb mc md me mf mg mh mi ln im bi translated">在强化学习领域，有两大类算法:用于<strong class="lq jd">预测的算法</strong>和用于<strong class="lq jd">控制的算法</strong>。<br/>对于稍微熟悉题目的人来说，<em class="mj">预测算法</em>通常是<strong class="lq jd">基于值的</strong>算法，也就是说它们的目的是预测一种情况的结果(<em class="mj">奖励</em>)。而<em class="mj">控制算法</em>大多是<strong class="lq jd">基于策略的</strong>算法，应该引导你度过难关。</p><p id="d1d1" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">同样，这些类别对应着心理学的两项研究:<strong class="lq jd">经典(巴甫洛夫)</strong> <strong class="lq jd">条件反射</strong>和<strong class="lq jd">工具(操作)条件反射</strong>。<em class="mj">预测算法</em>和<em class="mj">经典条件作用</em>之间的对应关系取决于它们预测即将到来的刺激的共同特性，无论这些刺激是奖励还是惩罚。有了<em class="mj">工具性条件作用</em>，一个<em class="mj">主体</em>根据它所做的得到奖励或惩罚，因此它学会增加产生奖励行为的倾向，减少产生惩罚行为的倾向。当然，这种对应不是偶然的，因为心理学对RL的影响很大。</p><p id="6c58" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">这是灵感来源系列的第二部分，所以让我们跳进兔子洞，看看如何让人们相信。</p><h1 id="ab47" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated"><strong class="ak">经典条件作用</strong></h1><p id="82d8" class="pw-post-body-paragraph lo lp it lq b lr nh kd lt lu ni kg lw lx nj lz ma mb nk md me mf nl mh mi ln im bi translated">我敢肯定，你们所有人都听说过<strong class="lq jd">伊凡·巴甫洛夫</strong>和他用狗做的实验，这是<em class="mj">经典条件反射</em>的最好例子，尽管在这个领域还有许多其他作品，让我们坚持到底。</p><p id="0a8b" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">巴甫洛夫博士正在研究一只狗对<strong class="lq jd">无条件刺激</strong> (US)的<strong class="lq jd">无条件反应</strong> (UR)，分别是流涎和食物演示。当给狗一些食物时，它几乎立刻开始分泌唾液。这个过程对我们来说非常简单和直观，但这引发了一个想法，即是否可以训练狗对其他刺激做出类似的反应。</p><p id="a462" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated"><strong class="lq jd">条件刺激</strong> (CS)是狗以前不关心的东西，它可以是声音、光或基本上任何其他最初是中性刺激的东西。在巴甫洛夫的实验中，这是节拍器的声音。对CS的反应被称为<strong class="lq jd">条件反应</strong> (CR)，它有时类似于UR，但通常开始得更早，通常更强烈。</p><p id="58cc" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">回到巴甫洛夫的实验，在重复播放了几次节拍器的声音(CS)之后，就在展示食物(US)之前不久<strong class="lq jd">，狗学会了对节拍器的声音(CR)分泌唾液，就像它对食物(UR)分泌唾液一样。美国被称为“强化者”,因为它强化了铬的产生以回应铯。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/c72c3b0882ca9ffc760e36d880c7ae9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-cnnyFQgimW77KDk1jZCA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">巴甫洛夫实验管道</p></figure><p id="188c" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">最初的中性刺激变成了条件刺激，因为狗知道它预测到了美国，所以开始产生条件刺激。</p><p id="cd90" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">关于这个项目还有许多其他有趣的细节，但这对我们来说已经足够了。</p><h2 id="db3d" class="nr mq it bd mr ns nt dn mv nu nv dp mz lx nw nx nb mb ny nz nd mf oa ob nf iz bi translated">强化学习和经典条件作用</h2><p id="8d76" class="pw-post-body-paragraph lo lp it lq b lr nh kd lt lu ni kg lw lx nj lz ma mb nk md me mf nl mh mi ln im bi translated">狗的学习过程有点类似于我们如何训练一个RL <em class="mj">代理</em>。如我所说，<em class="mj">经典条件</em>对应于<em class="mj">预测</em>算法，该算法在给定的<em class="mj">状态下预测<em class="mj">奖励</em>。</em>这与狗身上发生的事情非常相似(<em class="mj">一个代理</em>)。</p><p id="e346" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">在<em class="mj">状态</em>中，当食物出现时，狗开始分泌唾液，这意味着它期待<em class="mj">奖励</em>。所以我们以这样一种方式构建<em class="mj"> </em>我们的训练循环，当我们在食物演示之前引入CS时，有另一个<em class="mj">状态</em>。经过几个训练时期后，狗学会了CS的<em class="mj">状态</em>和食物之间的依赖性，因此它开始分泌唾液(表明它期待CS的<em class="mj">奖励</em>)。</p><p id="d32c" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">就强化学习而言，我们可以把这个问题写成如下。</p><p id="89e5" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">吃的过程本身可以设定为RL <em class="mj">奖励</em>(食物美味营养)<em class="mj">，动作</em>会流口水，或者不流口水，̶t̶h̶a̶t̶̶i̶s̶̶t̶h̶e̶̶q̶u̶e̶s̶t̶i̶o̶n̶，即使狗不是故意的，<em class="mj">状态</em>是食物演示过程和节拍器的声音。</p><p id="80c6" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">现在你知道了，当<em class="mj">状态</em>包括节拍器的声音时，我们的<em class="mj">代理</em>将会预测(垂涎)奖励<em class="mj"/>。</p><p id="829e" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">让我们引用这一领域之父的话来结束这一部分:</p><blockquote class="ld"><p id="5ee3" class="le lf it bd lg lh oc od oe of og ln dk translated">“很明显，在自然条件下，正常的动物不仅必须对自己带来直接利益或伤害的刺激做出反应，而且还必须对其他物理或化学机构——声波、光波等——做出反应，这些反应本身只标志着这些刺激的接近；虽然对小动物有害的不是猛兽的视觉和声音，而是它的牙齿和爪子。”——伊凡·巴甫洛夫博士。</p></blockquote><h1 id="0f4c" class="mp mq it bd mr ms mt mu mv mw mx my mz ki oh kj nb kl oi km nd ko oj kp nf ng bi translated">工具性学习</h1><p id="bfa5" class="pw-post-body-paragraph lo lp it lq b lr nh kd lt lu ni kg lw lx nj lz ma mb nk md me mf nl mh mi ln im bi translated">与经典的条件反射实验不同，在T2，仪器条件反射实验依赖于动物的具体行为。在<em class="mj">经典条件反射</em>中，无论动物的行为如何，它都会被呈现一个强化物(我们)。而这也是为什么<em class="mj">乐器调理</em>远比<em class="mj"> </em>大家都知道的更接近RL。</p><p id="8b0d" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated"><em class="mj">工具性条件反射</em>通常被认为与<strong class="lq jd">操作性条件反射</strong>相同，后者是由<strong class="lq jd"> B. F .斯金纳</strong>引入的术语，但我们不打算深究差异，而是将它视为一个单一的事物。</p><p id="39dd" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated"><strong class="lq jd">爱德华·桑戴克</strong>是这个领域的先驱，他用被放进<strong class="lq jd">“拼图盒子”</strong>的猫做实验。这些盒子有多种变化，但理念是相同的。猫要完成一系列动作才能从盒子里出来取食物。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/6b8ad87c687db77da19ba8e0a0703aed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uizz1zlZAU1NKkCs-nbYhA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">桑代克对猫的实验</p></figure><p id="bb2f" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">看，又回到食物上了。这是我们的代理人T21的增强剂，对所有这些实验都至关重要。正所谓“没钱没蜜”。你必须为你的<em class="mj">代理</em>提供某种<em class="mj">奖励</em>(积极的或消极的)，因为没有它就无法工作。</p><p id="1a00" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">所有这一切真正有趣的是，在几次连续的经历后，猫学会了如何更快地逃离，它们不再凭直觉(随机)行动。桑代克是这样描述的:</p><blockquote class="ld"><p id="d94c" class="le lf it bd lg lh oc od oe of og ln dk translated">“在她冲动的挣扎中在盒子里到处抓的猫可能会抓绳子、环或按钮来开门。逐渐地，所有其他不成功的冲动将被消除，导致成功行为的特定冲动将被由此产生的快乐所取代，直到经过多次尝试后，猫在被放入盒子后，会立即以确定的方式抓住按钮或环。”爱德华·桑戴克博士。</p></blockquote><p id="c175" class="pw-post-body-paragraph lo lp it lq b lr ls kd lt lu lv kg lw lx ly lz ma mb mc md me mf mg mh mi ln im bi translated">所以，简单地说，猫会想出一系列的动作，引导它们走出盒子，最终它们会得到食物。但对我们来说重要的是，他们是通过桑代克发现并称之为<strong class="lq jd">效果法则</strong>也就是<strong class="lq jd">试错法</strong>来学习的。</p><p id="aa6c" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">现在让我们更关注强化学习部分，因为它有一些令人兴奋的方面。</p><h2 id="a27c" class="nr mq it bd mr ns nt dn mv nu nv dp mz lx nw nx nb mb ny nz nd mf oa ob nf iz bi translated">强化学习和仪器条件作用</h2><p id="0f3a" class="pw-post-body-paragraph lo lp it lq b lr nh kd lt lu ni kg lw lx nj lz ma mb nk md me mf nl mh mi ln im bi translated"><em class="mj">效果法则</em> <strong class="lq jd"> </strong>描述了<strong class="lq jd"> </strong>反映在RL算法中的动物学习的两个基本特征。首先，一个算法必须是<em class="mj">选择的</em>，<em class="mj"> </em>，这意味着它尝试不同的<em class="mj">动作</em>，并通过比较它们的结果在其中进行选择。第二，算法必须是<em class="mj">关联的</em>，这意味着它将特定情况(<em class="mj">状态</em>)与在选择阶段发现的<em class="mj">动作</em>相关联。</p><p id="58be" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated"><em class="mj">效果定律</em>说，重要的是不仅要找到给予大量<em class="mj">奖励</em>的<em class="mj">动作</em>，还要将<em class="mj"> </em>那些<em class="mj">动作</em>与<em class="mj">状态</em>联系起来。</p><p id="a583" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">选择过程最臭名昭著的例子是进化中的自然选择——最强的生命——但它不是相关的。<em class="mj">关联</em>方法的一个例子可以是监督学习，但它不是<em class="mj">选择</em>，因为它需要直接指令来改变它的行为。</p><p id="8510" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">对于RL算法<em class="mj">来说，选择</em>过程就是探索，有很多方法可以实现。例如，<strong class="lq jd">ε-贪婪策略</strong>，它声明一个<em class="mj">代理</em>以<em class="mj"> </em> ε <strong class="lq jd"> </strong>的概率挑选一个随机的<em class="mj">动作</em>，它以1-ε的概率挑选贪婪(选择给予最大<em class="mj">即时奖励</em>的<em class="mj">动作</em>)。通过在训练期间逐渐减少ε，我们试图解决<em class="mj">探索-开发困境，</em>简单地说，就是什么时候应该停止探索，开始开发。</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="c57b" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">另一个好奇的事情是<em class="mj">动机</em>。对于<em class="mj">工具性条件反射</em>来说，它影响行为的强度和方向。在桑代克的实验中，是放在盒子外面的食物。每当我们的猫逃出盒子，它就会得到食物，这加强了它逃跑的行动。</p><p id="e4c9" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">当然，它并不直接对应于RL，但<em class="mj">奖励</em>信号基本上就是我们所说的动机，整个技巧就是让<em class="mj">代理人的</em>体验<em class="mj"> </em>获得奖励。尽管我们自己的动机是非常复杂和高度等级化的东西，我们可以得到一些想法。例如，当我们饿的时候，我们更有动力去获取食物，而当我们刚吃东西的时候，我们就没有动力了，也许我们可以把这种行为转移到强化学习中。然而，关于这一主题的作品却很少。至少据我所知，最近的一本是Nathaniel D. Daw和Daphna Shohamy的《动机和学习的认知神经科学》。</p><p id="59d1" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">如你所见，<em class="mj">工具性条件作用</em>的抽象与强化学习非常相似，但它们在任何方面都不是等同的陈述。将动物行为这样复杂的东西映射到计算问题上，就像用棍子和石头解释弦理论一样。</p><h1 id="b671" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">结论</h1><p id="8ea2" class="pw-post-body-paragraph lo lp it lq b lr nh kd lt lu ni kg lw lx nj lz ma mb nk md me mf nl mh mi ln im bi translated">我希望这种相似性现在对你来说已经很清楚了，但是请记住RL方法只是受到心理学的启发，它们从来不应该严格地反映动物和人类的行为。</p><p id="c10e" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">可能不清楚为什么所有的东西都是关于动物而不是人类的，首先，这是因为我们人类和我们的狗和猫一样都是动物。第二，我们的行为模式非常相似，但难度成倍增加，所以在我们的宠物身上做实验更容易。但是仍然有可能使用<em class="mj">操作性条件反射</em>来训练人，例如，谢尔顿在《生活大爆炸》中对佩妮所做的。</p><p id="0705" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">我们仍然有太多关于自己的探索，加入这项研究永远都不晚。也许我们，人类，真的在我们的生物学和心理学上有一些特别之处，这让我们成为了优势物种，或者也许我们只是运气好。重要的是我们学习的方式中有一个谜，我们试图用棍子和石头解释弦理论，这可能是解释我们为什么在这里的重要一步吗？</p><p id="e586" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated">P.S. Pavlov实际上是个好人，他养了所有的狗，并在实验后照顾它们。</p><p id="10fd" class="pw-post-body-paragraph lo lp it lq b lr mk kd lt lu ml kg lw lx mm lz ma mb mn md me mf mo mh mi ln im bi translated"><em class="mj">在</em><a class="ae or" href="https://twitter.com/poddiachyi" rel="noopener ugc nofollow" target="_blank"><em class="mj">Twitter</em></a><em class="mj">，</em><a class="ae or" href="https://www.linkedin.com/in/poddiachyi/" rel="noopener ugc nofollow" target="_blank"><em class="mj">LinkedIn</em></a><em class="mj">，</em> <a class="ae or" href="https://www.facebook.com/poddiachyi" rel="noopener ugc nofollow" target="_blank"> <em class="mj">脸书</em> </a> <em class="mj">上与我连线并关注</em><a class="ae or" href="https://github.com/Poddiachyi" rel="noopener ugc nofollow" target="_blank"><em class="mj">GitHub</em></a><em class="mj">！</em></p></div></div>    
</body>
</html>