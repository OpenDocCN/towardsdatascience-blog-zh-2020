<html>
<head>
<title>Distillation of Knowledge in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中知识的提炼</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6?source=collection_archive---------6-----------------------#2020-01-26">https://towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6?source=collection_archive---------6-----------------------#2020-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7b0b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在更小更快的型号上实现最先进的性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5034814ee057020c35e9298c8ee00d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDQRMa_l1WqeKT-q_ysvTQ.png"/></div></div></figure><blockquote class="ku"><p id="5013" class="kv kw it bd kx ky kz la lb lc ld le dk translated">知识的提炼(在机器学习中)是一种架构不可知的方法，用于在一个神经网络内概括知识(巩固知识)以训练另一个神经网络。</p></blockquote><h1 id="6c1b" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">重要</h1><p id="6e79" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">目前，特别是在NLP中，正在训练非常大规模的模型。其中很大一部分甚至无法安装在普通人的硬件上。此外，由于收益递减规律，模型规模的巨大增加几乎不能映射到精确度的微小增加。</p><p id="04a6" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">这些模型几乎不能在商业服务器上运行，更不用说在智能手机上了。</p><p id="ecbf" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">使用蒸馏，人们可以将像BERT这样的模型的大小减少87%,而仍然保持其96%的性能。</p><p id="3791" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">基本上，蒸馏使人能够:</p><ul class=""><li id="b0c7" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">获得一流的精度</li><li id="34c4" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">只有很小一部分</li><li id="e021" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">在很短的响应时间内</li><li id="bd3e" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">在微调时间的一小部分内</li><li id="7a5b" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">在单个GPU上安装合奏</li><li id="8051" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">可以在CPU上运行模型</li><li id="485b" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">支持联盟学习</li><li id="6041" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">等等</li></ul></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="3858" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">普通神经网络的问题</h1><blockquote class="ku"><p id="603d" class="kv kw it bd kx ky nx ny nz oa ob le dk translated">每个学习者的目标是优化其在训练数据上的性能。这并不能完全解释为数据集中知识的概括。</p></blockquote><p id="2b32" class="pw-post-body-paragraph lx ly it lz b ma oc ju mc md od jx mf mg oe mi mj mk of mm mn mo og mq mr le im bi translated">以MNIST数据集为例。我们来选一张3号的样图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/5d86cc8a356f2b49b2e787ba83d24b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*sk3dzRqi-rH_EWa_py6gLg.jpeg"/></div></figure><p id="55ef" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">在训练数据中，数字3转化为相应的一个热点向量:</p><p id="2b4c" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated"><strong class="lz iu">0 0 1 0 0 0 0 0 0 0 0</strong></p><p id="44c0" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">这个向量简单地告诉我们这个图像中的数字是3，但是</p><blockquote class="oi oj ok"><p id="adbe" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">没有明确提到任何关于数字3的形状。像3的形状类似于8。</p></blockquote><p id="2bb0" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">因此:</p><blockquote class="oi oj ok"><p id="e2fb" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">从来没有明确要求神经网络学习对训练数据的一般理解。泛化程度就是神经网络的隐含能力。</p></blockquote><p id="cd1b" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">因此，在正常训练的神经网络中，每个神经元内的信息(检测到的特征)相对于期望的输出并不同等重要。</p><p id="60a6" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">简而言之，正常训练的神经网络以神经元的形式承载了大量的负载，这些神经元从未学会概括数据，因此导致测试数据的准确性降低。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="2f0a" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">蒸馏</h1><p id="8818" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">蒸馏使我们能够使用预训练的网络来训练另一个神经网络，而没有原始神经网络的自重。</p><blockquote class="oi oj ok"><p id="d8ac" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">使我们能够压缩网络的规模而不损失太多的准确性。</p></blockquote><blockquote class="ku"><p id="666b" class="kv kw it bd kx ky kz la lb lc ld le dk translated">因此，经过提炼的模型比正常训练的模型具有更高的精确度。</p></blockquote><blockquote class="oi oj ok"><p id="daa5" class="lx ly ol lz b ma oc ju mc md od jx mf om oe mi mj on of mm mn oo og mq mr le im bi translated">注意:知识的提炼可以从任何形式的学习者(逻辑回归，SVM，神经网络等)到任何其他形式的学习者。</p></blockquote><p id="d10a" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">虽然为了博客的简单，我将只引用神经网络。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="d945" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">信息的概括</h1><p id="46b8" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">让我们后退一步，修改神经网络的目标:</p><blockquote class="ku"><p id="e393" class="kv kw it bd kx ky nx ny nz oa ob le dk translated">通过归纳训练数据中的知识，预测网络在训练期间从未见过的样本的输出。</p></blockquote><p id="d2c4" class="pw-post-body-paragraph lx ly it lz b ma oc ju mc md od jx mf mg oe mi mj mk of mm mn mo og mq mr le im bi translated">以鉴别神经网络为例，其目标是识别给定输入的相关类别。</p><blockquote class="oi oj ok"><p id="b6ef" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">现在，神经网络返回所有类别的概率分布，甚至是错误的类别。</p></blockquote><p id="802e" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated"><em class="ol">这告诉我们很多关于网络在训练数据中概括概念的能力。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="589a" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">一般化的度量</h1><p id="541b" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">对于MNIST上训练有素的神经网络来说，以下观察将是正确的:</p><ul class=""><li id="4feb" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">即使数字3的概率明显大于数字8和数字0的概率</li><li id="a434" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">8和0的概率是相当的</li><li id="1f9c" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">8和0的概率比其他数字更高</li></ul><p id="8265" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">因此，神经网络能够识别图像中数字3的形状，但是<strong class="lz iu">神经网络还表明3的形状与数字8和0的形状非常相似(都非常弯曲)</strong>。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="79fa" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">信息概括过程</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/c608cab24d3904464eea38659f3823aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzVBiFQv4_OiSBpSry8EMg.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">不，你实际上不需要所有这些设备。这只是有趣和相关的。</p></figure><p id="7f5d" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">让我们从显而易见的开始，以正常的方式训练一个巨大的神经网络(你的硬件可以支持)。我们将把这个网络称为<strong class="lz iu">繁琐网络</strong>。</p><blockquote class="oi oj ok"><p id="b510" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">注意:这个笨重的模型很可能是多个正常训练的模型的集合。</p></blockquote><p id="7641" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">参考资料:</p><ul class=""><li id="84c2" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated"><strong class="lz iu">软目标</strong>:网络在所有类别中的概率/权重分布</li><li id="a6dc" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated"><strong class="lz iu">硬目标</strong>:原始训练数据内的一键向量表示</li></ul><p id="c2fa" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">当繁琐模型不是单个模型而是多个模型的集合时，其输出的算术/几何平均值作为软目标。</p><p id="1e30" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">新模型可以在与原始模型相同的数据集上训练，也可以在称为“转移集”的不同数据集上训练。</p><blockquote class="oi oj ok"><p id="1b51" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">Transfer-Set:通过繁琐的模型传递数据，并使用其输出(概率分布)作为各自的真值。它可以由用于训练原始模型的数据集、新数据集或两者组成。</p></blockquote></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="004a" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">温度和熵</h1><p id="081f" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">通过调整软目标的温度，可以调整该转移集的大小。</p><p id="8c9d" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">当软目标具有高熵时，它们在每个训练样本上比硬目标给出更多的信息(让我们马上回到这个话题)。</p><p id="773f" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">这导致:</p><ul class=""><li id="eea7" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">损失更小，因此校正梯度更小(反向传播)</li><li id="b0f7" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">不同训练示例的梯度之间的差异较小</li></ul><p id="8d4b" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">因此:</p><ul class=""><li id="1d6e" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">可以使用更大的学习速率来训练模型</li><li id="52b0" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">可以使用较小的数据集来训练模型</li></ul></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="eb6c" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">更多关于温度</h1><p id="081d" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">我们在物理学中学到的东西在这里适用'<strong class="lz iu">熵</strong>随着<strong class="lz iu">温度'</strong>增加。</p><p id="4b32" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">让我们打个比方:</p><p id="f171" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">想象一个盒子，里面的球一个接一个堆叠在一起。如果我们通过摇动盒子来增加它的熵，球不会从盒子里掉出来，而是会散开一点。</p><blockquote class="oi oj ok"><p id="c4e4" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">得到的分布具有与原始分布相似的形状，但是其峰值的大小发生了变化。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/07e184790460060954f17ebcbc2805b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZ7W5390tlqCMpcHyqY1VA.png"/></div></div></figure><p id="b8bd" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">然而，在更高的<strong class="lz iu">温度</strong>下，一定量的热量添加到系统中会导致<strong class="lz iu">熵</strong>比相同量的热量在更低的<strong class="lz iu">温度</strong>下产生更小的变化。</p><p id="56be" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">下面是一个概率分布的例子，比较一个系统的低温和高温。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/47963dcd9042fda6519b708fe9538529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhCn-Q4fdEFQd2pmD31hyQ.png"/></div></div></figure><p id="b8c5" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">现在，我们用温度公式将知识从一个神经网络转移到另一个神经网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d856a30bd0cd1e7fd60c8227b7360e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_czduk1sf0CN1IaRCsThA.png"/></div></div></figure><ul class=""><li id="c59c" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">ᵢ:结果概率</li><li id="097c" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">ᵢ:一个阶级的逻辑</li><li id="8ff3" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">ⱼ:其他逻辑</li><li id="4585" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">t:温度</li></ul><blockquote class="oi oj ok"><p id="11fb" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated"><a class="ae ox" href="https://developers.google.com/machine-learning/glossary/#logits" rel="noopener ugc nofollow" target="_blank"> Logits </a>:分类模型生成的原始(非标准化)预测向量，通常会传递给标准化函数。如果模型正在解决多类分类问题，logits通常会成为softmax函数的输入。</p></blockquote><p id="8dcb" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">好吧，想象一下下面的场景:</p><p id="2bd7" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">想象一个MNIST数据点的数字3(是的，我喜欢数字3)，但是让我们只关注3个(是的是的)类:</p><ul class=""><li id="b80d" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated"><strong class="lz iu"> 0 </strong></li><li id="25c3" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated"><strong class="lz iu"> 3 </strong></li><li id="a6ba" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated"><strong class="lz iu"> 8 </strong></li></ul><blockquote class="oi oj ok"><p id="c71f" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated"><strong class="lz iu">数据集值</strong> : 0，1，0</p><p id="ac1a" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated"><strong class="lz iu">逻辑值(神经网络输出)</strong> : 0.1，0.7，0.2</p><p id="3279" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">温度0.5:  0.1831659252，0.5939990325</p><p id="e443" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">温度1:  0.254628528，0.4639634285</p><p id="ed9c" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">温度2:  0.294019937，0.3968854016</p><p id="e056" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">温度5:  0.3176924658，0.358197259，0.35825357</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/e1139c521fadba01e40dd15c6403aced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pp1c-qYHDps6MowOOZ6Kbg.png"/></div></div></figure><p id="a62a" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">更高的温度导致更柔和的类概率分布。</p><p id="cb16" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">现在，让我们想象一下随着温度的升高，概率分布的平滑度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/f2f6e15f2779baadc2a1f3eda8db259b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yWUpJW2LFkytjR4Y7rRKSQ.png"/></div></div></figure></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="a4ef" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">训练蒸馏模型</h1><p id="4930" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">最简单的蒸馏形式是使用由具有高温的笨重模型产生的软目标来训练模型，并将其蒸馏到具有相同温度的另一个模型中。</p><blockquote class="oi oj ok"><p id="7423" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">在训练之后，提取的模型的温度被设置为1。</p></blockquote><p id="42cf" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">到目前为止，我们已经确定，提取的网络可以在由软目标组成的转移集上训练。</p><p id="cefa" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">酪</p><p id="ca20" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">我们还可以利用所有或部分数据已知的真实值或硬目标。</p><p id="fbed" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">最有效的方法之一是使用两个目标函数:</p><ul class=""><li id="f1bf" class="mx my it lz b ma ms md mt mg mz mk na mo nb le nc nd ne nf bi translated">使用高温笨重模型的软目标的交叉熵</li><li id="99cf" class="mx my it lz b ma ng md nh mg ni mk nj mo nk le nc nd ne nf bi translated">交叉熵与硬目标使用相同的繁琐模型，但温度设置为1</li></ul><blockquote class="oi oj ok"><p id="115b" class="lx ly ol lz b ma ms ju mc md mt jx mf om mu mi mj on mv mm mn oo mw mq mr le im bi translated">注意:软目标的幅度被缩放到i/T倍，而硬目标没有这样的缩放。因此，我们将软目标乘以T，以标准化软目标和硬目标的影响。</p></blockquote></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="5591" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">密码</h1><p id="653c" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">HuggingFace实际上提供了脚本来训练你自己的<strong class="lz iu">distil Bert</strong>&amp;<strong class="lz iu">distil Roberta</strong>，它的体积小了40%，速度快了60%，同时保留了原始模型99%的准确性。</p><p id="9926" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">要获取存储库，请使用:</p><pre class="kj kk kl km gt oz pa pb pc aw pd bi"><span id="3e25" class="pe lg it pa b gy pf pg l ph pi">git clone <a class="ae ox" href="https://github.com/huggingface/transformers.git" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers.gi</a>t</span></pre><p id="5e1e" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">然后</p><pre class="kj kk kl km gt oz pa pb pc aw pd bi"><span id="b74b" class="pe lg it pa b gy pf pg l ph pi">cd transformers/examples/distillation</span></pre><p id="700a" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">首先，我们将对数据进行二进制化，也就是说，对数据进行标记化，并在模型词汇表的索引中转换每个标记。</p><pre class="kj kk kl km gt oz pa pb pc aw pd bi"><span id="e626" class="pe lg it pa b gy pf pg l ph pi">python scripts/binarized_data.py \<br/>    --file_path data/dump.txt \<br/>    --tokenizer_type bert \<br/>    --tokenizer_name bert-base-uncased \<br/>    --dump_file data/binarized_text</span></pre><p id="f96c" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">《拥抱脸》沿袭了《T4》和《XLM》的风格，通过一个更加强调生僻字的因素，平滑了屏蔽的可能性。因此，计算数据中每个标记的出现次数:</p><pre class="kj kk kl km gt oz pa pb pc aw pd bi"><span id="75d6" class="pe lg it pa b gy pf pg l ph pi">python scripts/token_counts.py \<br/>    --data_file data/binarized_text.bert-base-uncased.pickle \<br/>    --token_counts_dump data/token_counts.bert-base-uncased.pickle \<br/>    --vocab_size 30522</span></pre><p id="0f5e" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">一旦对数据进行预处理，蒸馏训练就变得非常简单:</p><pre class="kj kk kl km gt oz pa pb pc aw pd bi"><span id="4d52" class="pe lg it pa b gy pf pg l ph pi">python train.py \<br/>    --student_type distilbert \<br/>    --student_config training_configs/distilbert-base-uncased.json \<br/>    --teacher_type bert \<br/>    --teacher_name bert-base-uncased \<br/>    --alpha_ce 5.0 --alpha_mlm 2.0 --alpha_cos 1.0 --alpha_clm 0.0 --mlm \<br/>    --freeze_pos_embs \<br/>    --dump_path serialization_dir/my_first_training \<br/>    --data_file data/binarized_text.bert-base-uncased.pickle \<br/>    --token_counts data/token_counts.bert-base-uncased.pickle \<br/>    --force # overwrites the `dump_path` if it already exists.</span></pre></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="6b81" class="lf lg it bd lh li ns lk ll lm nt lo lp jz nu ka lr kc nv kd lt kf nw kg lv lw bi translated">结论</h1><p id="fdca" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr le im bi translated">通过提取神经网络，我们获得了一个更小的模型，它与原始模型有很多相似之处，同时更轻、更小、运行速度更快。</p><p id="3838" class="pw-post-body-paragraph lx ly it lz b ma ms ju mc md mt jx mf mg mu mi mj mk mv mm mn mo mw mq mr le im bi translated">因此，提取模型是将大规模神经网络投入生产的一个有趣的选择。</p></div></div>    
</body>
</html>