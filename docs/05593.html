<html>
<head>
<title>Logistic regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-and-build-logistic-regression-from-scratch-4ca4a7a40a4?source=collection_archive---------35-----------------------#2020-05-10">https://towardsdatascience.com/understand-and-build-logistic-regression-from-scratch-4ca4a7a40a4?source=collection_archive---------35-----------------------#2020-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d5c6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在 NumPy 中从零开始建立逻辑回归。这比你想象的要容易得多！</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/71aa0555f0903a724ccbaae5fac298b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EN3PMhKNPsb02IiUEC0siw.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://pixabay.com/users/Pexels-2286921/" rel="noopener ugc nofollow" target="_blank">的照片</a>在<a class="ae kz" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">的照片</a></p></figure><p id="d770" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在许多方面，逻辑回归模型是数据科学家工具箱中最简单的机器学习分类工具之一。理解这些如何直接工作有助于我们理解用于分类任务的深度神经网络。</p><p id="8de4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这里，我们将讨论模型的数学，如何训练它，以及如何使用 numpy 在 Python 中构建它。</p><h2 id="a958" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">模型背后的数学</h2><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/722d4d86ca417b4a17777f6c30911de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*6Nnsfy3b1v2O6RGH32gnyw.png"/></div></figure><p id="9860" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们首先想象一下，我们有一个像上面这样的数据集，其中黄色点对应于类 0，紫色点对应于类 1。为了预测类标签，我们需要一个模型，它将两个数据维度作为输入，并预测一个介于 0 和 1 之间的数字。考虑这是给定数据和模型参数的标签的概率:ŷ = p( <strong class="lc iu"> y = </strong> 1| <strong class="lc iu"> X </strong>，<strong class="lc iu"> w </strong>)。</p><p id="bd04" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">逻辑回归模型旨在拟合两个类别之间的直线(或平面/超平面),以便线上的点的概率为 0.5，而远离线的点的概率为 0 或 1，具体取决于它们位于线的哪一侧。</p><p id="7d67" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在我们理解了这个概念，让我们试着稍微形式化一下。</p><p id="979e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们的直线/平面/超平面可以表示为:x̂ = <strong class="lc iu"> w </strong> ᵀ <strong class="lc iu"> x </strong> + b 其中<strong class="lc iu"> w </strong>是权重的向量，b 是偏差。</p><p id="7ec3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">“这很好，但我如何获得分配给标签的概率的封闭形式？”，我听到你问。</p><p id="b7c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">很棒的问题！</p><p id="710b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">好了，我们现在需要的是一种方法来映射我们的线性模型到一个有效的概率。这意味着该函数必须具有以下属性:</p><ol class=""><li id="bbdc" class="mq mr it lc b ld le lg lh lj ms ln mt lr mu lv mv mw mx my bi translated">在给定连续输入的情况下，它只能输出[0，1]范围内的值</li><li id="a741" class="mq mr it lc b ld mz lg na lj nb ln nc lr nd lv mv mw mx my bi translated">它必须是光滑的和可微的(其原因将在后面变得更加清楚)</li></ol><p id="eda2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这就是乙状结肠或逻辑功能发挥作用的地方。sigmoid 函数的图形及其函数形式如下所示。这意味着我们有了一个表示某个点属于某个特定类的概率的表达式。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3a8f41fae8f80ee7111d0f0ee60194a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/1*XwiKqPHY3LTEwE_Y2rX0qw.gif"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nf"><img src="../Images/ee90fc6c1587c3145380361cbb9ad218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FiO1wbgE420ec9HmBSZdjA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Sigmoid 激活函数</p></figure><p id="8718" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以我们整个模型可以表述为:ŷ = σ( <strong class="lc iu"> w </strong> ᵀ <strong class="lc iu"> x </strong> + b)。</p><h2 id="54b7" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">训练我们的模型</h2><p id="bade" class="pw-post-body-paragraph la lb it lc b ld ng ju lf lg nh jx li lj ni ll lm ln nj lp lq lr nk lt lu lv im bi translated">我们知道模型有能力为两类中的每一类输出一个概率，但是我们如何选择模型的权重(<strong class="lc iu"> w </strong>，b)？</p><p id="6e8d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为此，我们需要为模型定义一个成本函数，并根据模型参数最小化该函数。</p><p id="53f6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为我们的模型选择成本函数是为了最大化指定类别的可能性:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/36486f9a771fe34565d30e28d5c68de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/1*NyTGICe0uHl40ys6vLQ3pg.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">从对数似然到二元交叉熵。x 是数据，w 是可训练的权重，ŷ是模型输出，y 是标签。</p></figure><p id="b7c3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这种损失也被称为二进制交叉熵。</p><p id="a397" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">好消息是，我们就快成功了！我们现在需要使用梯度下降来优化基于成本函数的参数。这种优化过程是训练网络式机器学习模型的最常见方式。它包括计算成本函数相对于每个模型参数的导数——因此，sigmoid 函数的平滑和可微性非常重要。</p><p id="c528" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于那些懂一点微积分的人来说，你可能知道这是链式法则的一个应用，但多亏了 Geoff Hinton [1]，在机器学习领域，我们称之为反向传播。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9c50416611fef1bd0fd829997645b573.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/1*lyOZc-2PZA1DFF2EFMv6dg.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">成本函数相对于模型参数的梯度。如果你想自己证明一个 sigmoid 函数的导数是σ(1-σ)，我推荐<a class="ae kz" rel="noopener" target="_blank" href="/derivative-of-the-sigmoid-function-536880cf918e">这篇</a>文章。</p></figure><p id="2961" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然我们有了关于模型权重的梯度，我们可以采取小的步骤来降低成本函数。μ是指算法的学习速率。如果学习率太小，模型的训练时间就会太长，并且可能会陷入局部极小值。如果它太大，那么训练过程可能根本不收敛或者它可能收敛到一个差的最小值。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e4306426c32a7894e91e71291d4519c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/1*l6yFR-7JxEbSFRV6XShiww.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">更新模型的权重。I 表示反向传播算法的第 I 次迭代。</p></figure><p id="c558" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果我们有很多数据，我们可以创建小批量，并为训练数据中的每个批量采取一个步骤。或者，我们可以选择更简单的方法，计算整个数据集的成本函数，并在每个时期采取一个梯度步骤。我们重复计算成本、梯度，然后更新权重的过程，直到成本函数收敛。</p><h2 id="388c" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">从逻辑回归到深度神经网络</h2><p id="729b" class="pw-post-body-paragraph la lb it lc b ld ng ju lf lg nh jx li lj ni ll lm ln nj lp lq lr nk lt lu lv im bi translated">正如我们刚刚看到的，逻辑回归将线性模型拟合到数据，并将输出传递给 sigmoid 激活，以预测每个类别的概率。</p><p id="2f16" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果我们想要从输入维度到输出维度进行更复杂的映射，那么我们可以在逻辑回归层之前添加额外的转换。为了训练这些层的权重，我们可以使用相同的反向传播原理来找到这些权重的梯度，并以与我们之前完全相同的方式更新它们。</p><h2 id="2908" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">代码</h2><p id="a456" class="pw-post-body-paragraph la lb it lc b ld ng ju lf lg nh jx li lj ni ll lm ln nj lp lq lr nk lt lu lv im bi translated">这是理论，但我们如何把它付诸实践呢？</p><p id="e581" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们实现向前传递，用模型进行预测。</p><pre class="kk kl km kn gt no np nq nr aw ns bi"><span id="4274" class="lw lx it np b gy nt nu l nv nw">import numpy as np</span><span id="1b00" class="lw lx it np b gy nx nu l nv nw">def _sigmoid(x):<br/>    return 1/(1 + np.exp(-x))</span><span id="a7a3" class="lw lx it np b gy nx nu l nv nw">def predict(X,W):<br/>        X = np.asarray(X)<br/>        if len(X.shape) != 2:<br/>            raise ValueError("Shape must be (dims,n_data_points)")<br/>        X = np.concatenate([X,np.ones([1,X.shape[-1]])],axis=0)</span><span id="5203" class="lw lx it np b gy nx nu l nv nw">        X_hat = np.matmul(W,X)<br/>        y_hat = _sigmoid(X_hat)<br/>        <br/>        return y_hat</span></pre><p id="299d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在实施过程中，有两件事需要特别注意:</p><p id="2b02" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，我们没有包括单独的偏差，而是做了一个小小的简化，增加了一个额外的权重，并将 1 附加到特征维度的数据上。这使得我们有一个单一的权重向量，而不是两个。对于读者来说，这是一个练习(因为你已经读得太久了)，以检查简化后的梯度是否仍然正确。</p><p id="04f7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其次，请注意，在数据点上没有循环，相反，计算是矢量化的，以提高性能。这通常在机器学习中很重要，以便在训练期间可以利用 GPU 的能力，但对于这个更简单的模型来说不是必要的。</p><p id="dfea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在让我们定义成本函数…</p><pre class="kk kl km kn gt no np nq nr aw ns bi"><span id="30a1" class="lw lx it np b gy nt nu l nv nw">cost = -np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))</span></pre><p id="8859" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">…梯度下降步骤:</p><pre class="kk kl km kn gt no np nq nr aw ns bi"><span id="847e" class="lw lx it np b gy nt nu l nv nw">dc_dw = -np.sum((y-y_hat)*X,axis=-1)[np.newaxis,:]<br/>self.W = self.W - dc_dw * lr</span></pre><p id="d4f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们现在可以用一个类中的<code class="fe ny nz oa np b">fit</code>和<code class="fe ny nz oa np b">predict</code>方法来模仿 sk-learn 类型的 API。</p><pre class="kk kl km kn gt no np nq nr aw ns bi"><span id="4605" class="lw lx it np b gy nt nu l nv nw">class LogisticRegressor():<br/>    def __init__(self):<br/>        self.loss = []<br/>        self.W = None<br/>    <br/>    def _sigmoid(self,x):<br/>        return 1/(1 + np.exp(-x))</span><span id="ee31" class="lw lx it np b gy nx nu l nv nw">def fit(self,X:np.ndarray,y:np.ndarray,epochs: int=100,lr: float=0.01):<br/>        self.epochs = epochs<br/>        self.lr = lr<br/>        X = np.asarray(X)<br/>        y = np.asarray(y)<br/>        if len(X.shape) != 2:<br/>            raise ValueError("Shape must be (dims,n_data_points)")<br/>        X = np.concatenate([X,np.ones([1,X.shape[-1]])],axis=0)<br/>        dims, n_data_points = X.shape<br/>        <br/>        if self.W is None:<br/>            self.W = np.random.randn(1,dims)<br/>        <br/>        for i in range(epochs):<br/>            X_hat = np.matmul(self.W,X)<br/>            y_hat = self._sigmoid(X_hat)<br/>            <br/>            cost = -np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))<br/>            self.loss.append(cost)<br/>            <br/>            dc_dw = -np.sum((y-y_hat)*X,axis=-1)[np.newaxis,:]<br/>            self.W = self.W - dc_dw * self.lr<br/>        <br/>    def plot_loss(self):<br/>        plt.scatter(list(range(len(self.loss))),self.loss)<br/>        <br/>    def predict(self,X:np.ndarray)-&gt; np.ndarray:<br/>        X = np.asarray(X)<br/>        if len(X.shape) != 2:<br/>            raise ValueError("Shape must be (dims,n_data_points)")<br/>        X = np.concatenate([X,np.ones([1,X.shape[-1]])],axis=0)</span><span id="cd81" class="lw lx it np b gy nx nu l nv nw">        X_hat = np.matmul(self.W,X)<br/>        y_hat = self._sigmoid(X_hat)<br/>        <br/>        return y_hat</span></pre><p id="4380" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们来测试一下！以本文开头所示的模拟数据集为例，我们已经准备好测试我们的模型…</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/5ccec362c17b53180f0e0dc60cfb8ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Xx7y2TbCosMC13TvNc9O6w.png"/></div></figure><p id="47b6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这种情况下，我们已经可视化了两个类之间的决策边界。这可以在分类器预测 0.5 概率的地方找到。换句话说，就是<strong class="lc iu"> w </strong> ᵀ <strong class="lc iu"> x </strong> + b = 0 的那条线。所以我们的努力看起来是有回报的，我们的模型可以正确识别 0.89%的数据点，并在对接近决策边界的点进行分类时具有表达不确定性的能力！</p><h2 id="edcc" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">那么，我们学到了什么？</h2><p id="dfe9" class="pw-post-body-paragraph la lb it lc b ld ng ju lf lg nh jx li lj ni ll lm ln nj lp lq lr nk lt lu lv im bi translated">如果你已经做到了这一步，那么希望你能更好地理解逻辑回归是如何工作的，以及为什么二元交叉熵只是表达模型预测的对数似然性的一种不同方式。</p><p id="1ec6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">训练逻辑回归模型允许我们尝试反向传播，这为理解如何训练更复杂的深度神经网络提供了基础。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="b0c5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1]:鲁梅尔哈特博士、辛顿博士和威廉斯博士，1986 年出版。通过反向传播误差学习表征。<em class="oi">自然</em>，323(6088)，第 533–536 页。</p></div></div>    
</body>
</html>