# å¦‚ä½•ç”¨ 10 ä¸ªæ­¥éª¤æ„å»ºåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-build-your-first-spam-classifier-in-10-steps-fdbf5b1b3870?source=collection_archive---------5----------------------->

å¦‚æœä½ åˆšåˆšå¼€å§‹æœºå™¨å­¦ä¹ ï¼Œå¾ˆæœ‰å¯èƒ½ä½ ä¼šè¿›è¡Œä¸€ä¸ªåˆ†ç±»é¡¹ç›®ã€‚ä½œä¸ºä¸€ä¸ªåˆå­¦è€…ï¼Œæˆ‘å»ºç«‹äº†ä¸€ä¸ªåƒåœ¾çŸ­ä¿¡åˆ†ç±»å™¨ï¼Œä½†åšäº†å¤§é‡çš„ç ”ç©¶ï¼ŒçŸ¥é“ä»å“ªé‡Œå¼€å§‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ç”¨ 10 ä¸ªæ­¥éª¤å‘æ‚¨ä»‹ç»æˆ‘çš„é¡¹ç›®ï¼Œè®©æ‚¨æ›´å®¹æ˜“ä½¿ç”¨ Tf-IDF çŸ¢é‡å™¨å’Œæœ´ç´ è´å¶æ–¯æ¨¡å‹æ„å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ï¼

# 1.åŠ è½½å¹¶ç®€åŒ–æ•°æ®é›†

å¦‚æœæ‚¨åœ¨ pandas ä¸­é˜…è¯»ï¼Œæˆ‘ä»¬çš„ SMS æ–‡æœ¬æ¶ˆæ¯æ•°æ®é›†æœ‰ 5 åˆ—:v1(åŒ…å«æ¯æ¡æ–‡æœ¬æ¶ˆæ¯çš„åˆ†ç±»æ ‡ç­¾ ham/spam)ã€v2(åŒ…å«æ–‡æœ¬æ¶ˆæ¯æœ¬èº«)å’Œä¸‰ä¸ªæ²¡æœ‰ä½¿ç”¨çš„æœªå‘½ååˆ—ã€‚æˆ‘ä»¬å°† v1 å’Œ v2 åˆ—åˆ†åˆ«é‡å‘½åä¸º class_label å’Œ messageï¼ŒåŒæ—¶å»æ‰å…¶ä½™çš„åˆ—ã€‚

```
import pandas as pd
df = pd.read_csv(r'spam.csv',encoding='ISO-8859-1')
df.rename(columns = {'v1':'class_label', 'v2':'message'}, inplace = True)
df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace = True)df
```

![](img/d2900da2d48e333e6817f6957991066b.png)

çœ‹çœ‹â€œ5572 è¡Œ x 2 åˆ—â€æ„å‘³ç€æˆ‘ä»¬çš„æ•°æ®é›†æœ‰ 5572 æ¡çŸ­ä¿¡ï¼

# 2.æµè§ˆæ•°æ®é›†:æ¡å½¢å›¾

åœ¨å¼€å§‹å¤„ç†æ•°æ®ä¹‹å‰ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸­æ‰§è¡Œä¸€äº›æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œä»¥ä¾¿å¯è§†åŒ–ã€ä»ä¸­è·å–ä¸€äº›ä¿¡æ¯æˆ–æ‰¾åˆ°æ•°æ®çš„ä»»ä½•é—®é¢˜ã€‚æˆ‘ä»¬å°†æŸ¥çœ‹æˆ‘ä»¬æœ‰å¤šå°‘åƒåœ¾é‚®ä»¶ï¼Œå¹¶ä¸ºå…¶åˆ›å»ºä¸€ä¸ªæ¡å½¢å›¾ã€‚

```
#exploring the datasetdf['class_label'].value_counts()
```

![](img/7bebee07a6ac8327b5fa18bd4c2dd5c2.png)

æˆ‘ä»¬çš„æ•°æ®é›†æœ‰ 4825 å°åƒåœ¾é‚®ä»¶å’Œ 747 å°åƒåœ¾é‚®ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†ï¼›ç«è…¿ä¿¡æ¯çš„æ•°é‡è¿œè¿œé«˜äºåƒåœ¾ä¿¡æ¯çš„æ•°é‡ï¼è¿™å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹æœ‰åå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ï¼Œä»¥è·å¾—ç›¸åŒæ•°é‡çš„åƒåœ¾é‚®ä»¶ã€‚

ä¸ºäº†ç”Ÿæˆæ¡å½¢å›¾ï¼Œæˆ‘ä»¬ä½¿ç”¨ Matplotlib ä¸­çš„ NumPy å’Œ pyplotã€‚

![](img/5d3a79aed41b2cbc07f1ebc55367f56d.png)

# 3.æ¢ç´¢æ•°æ®é›†:å•è¯äº‘

åœ¨æˆ‘çš„é¡¹ç›®ä¸­ï¼Œæˆ‘ç”Ÿæˆäº†åƒåœ¾é‚®ä»¶ä¸­æœ€å¸¸å‡ºç°çš„å•è¯çš„å•è¯äº‘ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»æ•°æ®é›†ä¸­è¿‡æ»¤æ‰æ‰€æœ‰åƒåœ¾é‚®ä»¶ã€‚df_spam æ˜¯ä¸€ä¸ªåªåŒ…å«åƒåœ¾æ¶ˆæ¯çš„æ•°æ®å¸§ã€‚

```
df_spam = df[df.class_label=='spam']df_spam
```

![](img/df1cafe516c916709c9a9ab12313dfb1.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠŠæ•°æ®å¸§è½¬æ¢æˆä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯åƒåœ¾æ¶ˆæ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ è¿æ¥æˆä¸€ä¸ªå¤§çš„åƒåœ¾é‚®ä»¶å­—ç¬¦ä¸²ã€‚è¯¥å­—ç¬¦ä¸²çš„å°å†™å½¢å¼æ˜¯æˆ‘ä»¬åˆ›å»ºå•è¯äº‘æ‰€éœ€çš„æ ¼å¼ã€‚

```
spam_list= df_spam['message'].tolist()filtered_spam = filtered_spam.lower()
```

æœ€åï¼Œæˆ‘ä»¬å°†å¯¼å…¥ç›¸å…³çš„åº“ï¼Œå¹¶å°†æˆ‘ä»¬çš„å­—ç¬¦ä¸²ä½œä¸ºå‚æ•°ä¼ å…¥:

```
import os
from wordcloud import WordCloud
from PIL import Imagecomment_mask = np.array(Image.open("comment.png"))
#create and generate a word cloud image
wordcloud = WordCloud(max_font_size = 160, margin=0, mask = comment_mask, background_color = "white", colormap="Reds").generate(filtered_spam)
```

æ˜¾ç¤ºå:

![](img/efa6b1d18195eadb0a7f05a67ad6d5dd.png)

å¾ˆé…·å§ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ï¼Œåƒåœ¾çŸ­ä¿¡ä¸­æœ€å¸¸è§çš„è¯æ˜¯â€œå…è´¹â€ã€â€œç«‹å³æ‰“ç”µè¯â€ã€â€œè®¤é¢†â€ã€â€œä¸­å¥–â€ç­‰ã€‚

å¯¹äºè¿™ä¸ªå•è¯äº‘ï¼Œæˆ‘ä»¬éœ€è¦æ•å¤´åº“ï¼Œåªæ˜¯å› ä¸ºæˆ‘ä½¿ç”¨äº†é®ç½©æ¥åˆ›å»ºæ¼‚äº®çš„è¯­éŸ³æ°”æ³¡å½¢çŠ¶ã€‚å¦‚æœæ‚¨å¸Œæœ›å®ƒæ˜¯æ–¹å½¢çš„ï¼Œè¯·çœç•¥ mask å‚æ•°ã€‚

ç±»ä¼¼åœ°ï¼Œå¯¹äºä¸šä½™æ¶ˆæ¯:

![](img/ac2f58c355b1b2db1b4d5e7244c2716f.png)

# 4.å¤„ç†ä¸å¹³è¡¡çš„æ•°æ®é›†

è¦å¤„ç†ä¸å¹³è¡¡çš„æ•°æ®ï¼Œæ‚¨æœ‰å¤šç§é€‰æ‹©ã€‚æˆ‘åœ¨æˆ‘çš„é¡¹ç›®ä¸­å¾—åˆ°äº†ä¸€ä¸ªç›¸å½“å¥½çš„ f å€¼ï¼Œå³ä½¿æ˜¯æœªé‡‡æ ·çš„æ•°æ®ï¼Œä½†æ˜¯å¦‚æœä½ æƒ³é‡æ–°é‡‡æ ·ï¼Œè¯·çœ‹[è¿™ä¸ª](https://elitedatascience.com/imbalanced-classes)ã€‚

# 5.åˆ†å‰²æ•°æ®é›†

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°†ç±»æ ‡ç­¾ä»å­—ç¬¦ä¸²è½¬æ¢æˆæ•°å­—å½¢å¼:

```
df['class_label'] = df['class_label'].apply(lambda x: 1 if x == 'spam' else 0)
```

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å°†æ•°æ®åˆ†æˆä¸¤ä¸ªå­é›†â€”â€”è®­ç»ƒå’Œæµ‹è¯•ã€‚æˆ‘ä»¬å°†è®­ç»ƒé›†ä»¥åŠå®ƒçš„å·²çŸ¥è¾“å‡ºå€¼(åœ¨æœ¬ä¾‹ä¸­ï¼Œ0 æˆ– 1 å¯¹åº”äºåƒåœ¾é‚®ä»¶æˆ–ç«è…¿)æä¾›ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ä¾¿å®ƒå­¦ä¹ æˆ‘ä»¬æ•°æ®ä¸­çš„æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•é›†æ¥è·å¾—æ¨¡å‹åœ¨è¿™ä¸ªå­é›†ä¸Šçš„é¢„æµ‹æ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ†å‰²æˆ‘ä»¬çš„æ•°æ®ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä» sklearn åº“ä¸­å¯¼å…¥ç›¸å…³çš„æ¨¡å—:

```
from sklearn.model_selection import train_test_split
```

ç„¶åæˆ‘ä»¬åˆ†å¼€:

```
x_train, x_test, y_train, y_test = train_test_split(df['message'], df['class_label'], test_size = 0.3, random_state = 0)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æµ‹è¯•å’Œè®­ç»ƒå­é›†æœ‰å¤šå°‘æ¡æ¶ˆæ¯:

```
print('rows in test set: ' + str(x_test.shape))
print('rows in train set: ' + str(x_train.shape))
```

![](img/f5a487a746102f54ff7bda5df100c33d.png)

æ‰€ä»¥æˆ‘ä»¬æœ‰ 1672 æ¡æ¶ˆæ¯ç”¨äºæµ‹è¯•ï¼Œ3900 æ¡æ¶ˆæ¯ç”¨äºè®­ç»ƒï¼

# 6.åº”ç”¨ Tf-IDF çŸ¢é‡å™¨è¿›è¡Œç‰¹å¾æå–

æˆ‘ä»¬çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹è¦æ±‚æ•°æ®è¦ä¹ˆåœ¨ Tf-IDF å‘é‡ä¸­ï¼Œè¦ä¹ˆåœ¨å•è¯å‘é‡è®¡æ•°ä¸­ã€‚åè€…æ˜¯ä½¿ç”¨è®¡æ•°çŸ¢é‡å™¨å®ç°çš„ï¼Œä½†æˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨ Tf-IDF çŸ¢é‡å™¨è·å¾—å‰è€…ã€‚

Tf-IDF çŸ¢é‡å™¨ä¸ºçŸ­ä¿¡ä¸­çš„æ¯ä¸ªå•è¯åˆ›å»º TF-IDF å€¼ã€‚Tf-IDF å€¼çš„è®¡ç®—æ–¹å¼æ˜¯ä¸ºå‡ºç°é¢‘ç‡è¾ƒä½çš„è¯èµ‹äºˆè¾ƒé«˜çš„å€¼ï¼Œä»¥ä¾¿ç”±äºè‹±è¯­è¯­æ³•è€Œå‡ºç°å¤šæ¬¡çš„è¯ä¸ä¼šæ©ç›–å‡ºç°é¢‘ç‡è¾ƒä½ä½†æ›´æœ‰æ„ä¹‰å’Œæœ‰è¶£çš„è¯ã€‚

```
lst = x_train.tolist()
vectorizer = TfidfVectorizer(
input= lst ,  # input is the actual text
lowercase=True,      # convert to lowercase before tokenizing
stop_words='english' # remove stop words
)features_train_transformed = vectorizer.fit_transform(list) #gives tf idf vector for x_train
features_test_transformed  = vectorizer.transform(x_test) #gives tf idf vector for x_test
```

# 7.è®­ç»ƒæˆ‘ä»¬çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹

æˆ‘ä»¬å°†æˆ‘ä»¬çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹(ä¹Ÿç§°ä¸ºå¤šé¡¹å¼)æ‹Ÿåˆåˆ° x_train çš„ Tf-IDF çŸ¢é‡ç‰ˆæœ¬ï¼Œå¹¶å°†çœŸå®è¾“å‡ºæ ‡ç­¾å­˜å‚¨åœ¨ y_train ä¸­ã€‚

```
from sklearn.naive_bayes import MultinomialNB
# train the model
classifier = MultinomialNB()
classifier.fit(features_train_transformed, y_train)
```

![](img/0f3c5e00449cb1413753b285b0ec1358.png)

# 8.æ£€æŸ¥ç²¾ç¡®åº¦å’Œ f å€¼

æ˜¯æ—¶å€™ä¼ å…¥æˆ‘ä»¬å¯¹åº”äº x_test çš„ Tf-IDF çŸ©é˜µï¼Œä»¥åŠçœŸå®çš„è¾“å‡ºæ ‡ç­¾(y_test)äº†ï¼Œæ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹åšå¾—æœ‰å¤šå¥½ï¼

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹çš„å‡†ç¡®æ€§:

```
print("classifier accuracy {:.2f}%".format(classifier.score(features_test_transformed, y_test) * 100))
```

![](img/0099b3e4efae7b8227b0304236e07701.png)

æˆ‘ä»¬çš„å‡†ç¡®åº¦å¾ˆé«˜ï¼ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çš„æ¨¡å‹å˜å¾—æœ‰åå·®ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ‰§è¡Œä¸‹ä¸€æ­¥ã€‚

# 9.æŸ¥çœ‹æ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æ··æ·†çŸ©é˜µå’Œ f-measure åˆ†æ•°ï¼Œä»¥*ç¡®è®¤*æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æ­£å¸¸:

```
labels = classifier.predict(features_test_transformed)
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_reportactual = y_test.tolist()
predicted = labels
results = confusion_matrix(actual, predicted)
print('Confusion Matrix :')
print(results)
print ('Accuracy Score :',accuracy_score(actual, predicted))
print ('Report : ')
print (classification_report(actual, predicted) )
score_2 = f1_score(actual, predicted, average = 'binary')
print('F-Measure: %.3f' % score_2)
```

![](img/f13fcad76de0b6f5b2b2c5c3cb521297.png)

æˆ‘ä»¬çš„ f å€¼æ˜¯ 0.853ï¼Œæˆ‘ä»¬çš„æ··æ·†çŸ©é˜µæ˜¾ç¤ºæˆ‘ä»¬çš„æ¨¡å‹åªåšäº† 61 ä¸ªé”™è¯¯çš„åˆ†ç±»ã€‚åœ¨æˆ‘çœ‹æ¥ç›¸å½“ä¸é”™ğŸ˜Š

# 10.æˆ‘ä»¬æ··æ·†çŸ©é˜µçš„çƒ­å›¾(å¯é€‰)

æ‚¨å¯ä»¥ä½¿ç”¨ seaborn åº“åˆ›å»ºä¸€ä¸ªçƒ­å›¾æ¥å¯è§†åŒ–æ‚¨çš„æ··æ·†çŸ©é˜µã€‚ä¸‹é¢çš„ä»£ç å°±æ˜¯è¿™ä¹ˆåšçš„ã€‚

![](img/0b53e87c0b9d4126eb4df63bd9a07008.png)

è¿™å°±æ˜¯åˆ¶ä½œä½ è‡ªå·±çš„åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ï¼æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯¼å…¥äº†æ•°æ®é›†å¹¶å¯¹å…¶è¿›è¡Œäº†å¯è§†åŒ–ã€‚ç„¶åæˆ‘ä»¬æŠŠå®ƒåˆ†æˆ train/testï¼Œè½¬æ¢æˆ Tf-IDF å‘é‡ã€‚æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒäº†æˆ‘ä»¬çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹ï¼Œå¹¶çœ‹åˆ°äº†ç»“æœï¼å¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼ŒæŠŠå®ƒéƒ¨ç½²æˆä¸€ä¸ª web åº”ç”¨ç¨‹åºã€‚

## å‚è€ƒèµ„æ–™/èµ„æº:

[1] D. Tï¼Œæ··æ·†çŸ©é˜µå¯è§†åŒ–(2019)ï¼Œ[https://medium . com/@ dtuk 81/æ··æ·†-çŸ©é˜µ-å¯è§†åŒ–-fc31e3f30fea](https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea)

1.  C.æ–‡æ–¯ï¼Œæœ´ç´ è´å¶æ–¯åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨(2018)ï¼Œ[https://www . code project . com/Articles/1231994/Naive-Bayes-Spam-Classifier](https://www.codeproject.com/Articles/1231994/Naive-Bayes-Spam-Classifier)
2.  H.Attriï¼Œä½¿ç”¨ TF-IDF ç®—æ³•çš„ç‰¹å¾æå–(2019)ï¼Œ[https://medium . com/@ hritikattri 10/Feature-Extraction-using-TF-IDF-algorithm-44 eedb 37305 e](https://medium.com/@hritikattri10/feature-extraction-using-tf-idf-algorithm-44eedb37305e)
3.  A.Bronshteinï¼ŒPython ä¸­çš„è®­ç»ƒ/æµ‹è¯•æ‹†åˆ†å’Œäº¤å‰éªŒè¯(2017)ï¼Œ[https://towardsdatascience . com/Train-Test-Split-and-Cross-Validation-in-Python-80 b 61 beca 4 b 6](/train-test-split-and-cross-validation-in-python-80b61beca4b6)
4.  **æ•°æ®é›†**:[https://www.kaggle.com/uciml/sms-spam-collection-dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)
5.  **å®Œæ•´ä»£ç **:[https://github . com/samimakhan/Spam-Classification-Project/tree/master/Naive-Bayes](https://github.com/samimakhan/Spam-Classification-Project/tree/master/Naive-Bayes)