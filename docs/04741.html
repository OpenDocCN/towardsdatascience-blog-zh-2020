<html>
<head>
<title>Training Neural Network from Scratch using PyTorch in just 7 cells</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅在7个细胞中使用PyTorch从头开始训练神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-neural-network-from-scratch-using-pytorch-in-just-7-cells-e6e904070a1d?source=collection_archive---------34-----------------------#2020-04-26">https://towardsdatascience.com/training-neural-network-from-scratch-using-pytorch-in-just-7-cells-e6e904070a1d?source=collection_archive---------34-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6e05" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">神经网络</h2><div class=""/><div class=""><h2 id="78ad" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">MNIST手数字识别使用PyTorch在短短7个细胞使用神经网络(多层感知器)从零开始</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/048c087ef8409b4a9d7ebc880eaf95d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/1*Fo9C8bc3WPxbScOhkpXVgg.gif"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">用PyTorch进行MNIST手形数字识别</p></figure><p id="fc82" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">博客内容:</strong></p><ol class=""><li id="e31e" class="lz ma it lf b lg lh lj lk lm mb lq mc lu md ly me mf mg mh bi translated">安装和导入模块</li><li id="887b" class="lz ma it lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">重新处理和加载数据集</li><li id="a1b5" class="lz ma it lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">设计模型</li><li id="cd0c" class="lz ma it lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">训练模型</li><li id="866f" class="lz ma it lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">可视化输出</li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="e35e" class="mu mv it bd mw mx my dn mz na nb dp nc lm nd ne nf lq ng nh ni lu nj nk nl iz bi translated">安装:</h2><p id="cb38" class="pw-post-body-paragraph ld le it lf b lg nm kd li lj nn kg ll lm no lo lp lq np ls lt lu nq lw lx ly im bi translated"><strong class="lf jd">先做第一件事。</strong>无论操作系统如何，只需运行下面的命令，该命令将安装运行下面代码片段所需的所有模块。如果你使用anaconda，那么你也可以用conda命令安装它。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="38f5" class="mu mv it ns b gy nw nx l ny nz">pip install torch torchvision numpy matplotlib</span><span id="3a87" class="mu mv it ns b gy oa nx l ny nz">conda install torch torchvision numpy matplotlib</span></pre><h2 id="61f4" class="mu mv it bd mw mx my dn mz na nb dp nc lm nd ne nf lq ng nh ni lu nj nk nl iz bi translated">导入语句:</h2><p id="4cce" class="pw-post-body-paragraph ld le it lf b lg nm kd li lj nn kg ll lm no lo lp lq np ls lt lu nq lw lx ly im bi translated"><code class="fe ob oc od ns b">import torch</code>用于添加构建神经网络的所有必要模块，而<code class="fe ob oc od ns b">torchvision</code>用于添加其他功能，如数据的预处理和转换。<code class="fe ob oc od ns b">numpy</code>用于处理图像数组，<code class="fe ob oc od ns b">matplotlib</code>用于显示图像。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="eeb5" class="mu mv it ns b gy nw nx l ny nz">import torch<br/>import torchvision<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from torchvision import datasets, transforms<br/>import matplotlib.pyplot as plt<br/>from torch import optim<br/>import numpy as np<br/>%matplotlib inline</span></pre><p id="8668" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">PyTorch拥有<code class="fe ob oc od ns b">transform</code>模块，可以将图像转换为张量，并预处理每幅图像，使其标准化，标准偏差为1。<code class="fe ob oc od ns b">torchvison</code>有内置的数据集<code class="fe ob oc od ns b">MNIST</code>手形数字，我将用它来进一步解释下面所有的代码片段。<code class="fe ob oc od ns b">DataLoader</code>是PyTorch模块，将图像和它对应的标签组合在一个包中。所以我们可以很容易地同时访问这两个东西。请注意，我们将<code class="fe ob oc od ns b">batch_size</code>添加为64，以便在一次迭代中创建一批64个图像。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="9968" class="mu mv it ns b gy nw nx l ny nz">transform = transforms.Compose([<br/>    transforms.ToTensor(),<br/>    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])<br/>trainset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', train=True, transform=transform, download=True)<br/>trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/165c37c1fabfdf36181cc60a3f7a4094.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*RfW9XmqAYkhvxG5xLqtZcg.gif"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">来源:男高音</p></figure><p id="1811" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">要训练任何神经网络，首先我们必须了解图像的输入大小、输出类别的数量以及神经网络的隐藏层。因此检查<code class="fe ob oc od ns b">trainloader.dataset.train_data.shape</code>我们将得到64，1，28，28，这表示64幅图像，高度和宽度为28，通道1为灰度图像。</p><p id="232c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">输入尺寸:</strong>所以输入尺寸是784，是图像的高(28)和宽(28)的乘积。图像只有一个通道，所以不需要在输入尺寸中添加它。</p><p id="6443" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">输出大小:</strong>我们有从0到9的数字，因此共有10个可能的类别选项。因此，输出大小为10</p><p id="b419" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">隐藏层:</strong>输入层和输出层之间的层基本上称为隐藏层。在我们的例子中，我们有一个784个节点的输入图像，输出大小为10，因此在两者之间，我们添加了128和64层。因此，我们的网络将从784扩展到128，再扩展到64到10。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="6140" class="mu mv it ns b gy nw nx l ny nz">input_size = trainloader.dataset.train_data.shape[1] * trainloader.dataset.train_data.shape[2]<br/>hidden_layers = [128,64]<br/>output_size = 10</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi of"><img src="../Images/04cd3c8c6424a8cb8cf065a0c5a5812c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWhBextdDSkxYvz0kEMTVg.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">用于手数字MNIST分类的简单神经网络(多层感知器)</p></figure><p id="1d44" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><code class="fe ob oc od ns b">torchvision</code>具有<code class="fe ob oc od ns b">nn</code>模块，该模块具有构建神经网络的所有功能。最初，将输入大小添加到第一个隐藏层，即784到128，然后是ReLU(激活函数)。从128到64具有相同的ReLU激活功能，而64到10在最后一层。为了得到概率分布，我们添加了最后一层<code class="fe ob oc od ns b">LogSoftmax</code>，维度= 1，因为我们有64个图像批次，所以它将在输出中给出64x10的结果。</p><p id="6bb6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算神经网络的误差和错误，我们添加了<code class="fe ob oc od ns b">NLLLoss</code>交叉熵损失(负对数似然损失)作为标准(误差函数)，优化器作为学习率为0.003的<code class="fe ob oc od ns b">SGD</code>(随机梯度下降)。</p><blockquote class="ok ol om"><p id="d144" class="ld le on lf b lg lh kd li lj lk kg ll oo ln lo lp op lr ls lt oq lv lw lx ly im bi translated">阅读更多关于交叉熵损失<a class="ae or" href="https://pytorch.org/docs/stable/nn.html#crossentropyloss" rel="noopener ugc nofollow" target="_blank">这里</a>和随机梯度下降<a class="ae or" href="https://pytorch.org/docs/stable/optim.html#torch.optim.SGD" rel="noopener ugc nofollow" target="_blank">这里</a></p></blockquote><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="fbd4" class="mu mv it ns b gy nw nx l ny nz">model = nn.Sequential(<br/>    nn.Linear(input_size, hidden_layers[0]),<br/>    nn.ReLU(),<br/>    nn.Linear(hidden_layers[0], hidden_layers[1]),<br/>    nn.ReLU(),<br/>    nn.Linear(hidden_layers[1], output_size),<br/>    nn.LogSoftmax(dim=1)<br/>)<br/>print(model)<br/>criterion = nn.NLLLoss()<br/>optimizer = optim.SGD(model.parameters(), lr=0.003)</span></pre><p id="c9dc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">培训:</strong></p><p id="0e63" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们通过传递图像和相应的标签，成功地定义了模型及其训练模型的时间。但在此之前，我们将图像从28x28展平到784x1，并将所有梯度设置为零，以训练模型的权重和偏差。</p><p id="f14d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后<code class="fe ob oc od ns b">model(images)</code>将训练模型，<code class="fe ob oc od ns b">criterion</code>将计算损失。<code class="fe ob oc od ns b">loss.backward()</code>用于反向传播，<code class="fe ob oc od ns b">optimizer.step()</code>将根据反向传播的权重和偏差更新权重。</p><p id="ebb3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们把损失印在每个训练时期。只要确保你的训练损失会随着纪元的增加而减少。如果你没有在每个时期减少损失，那么你在代码中犯了一些错误。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="b950" class="mu mv it ns b gy nw nx l ny nz">epochs = 5<br/>for e in range(epochs):<br/>    running_loss = 0<br/>    for images, labels in trainloader:<br/>        # Flatten the Image from 28*28 to 784 column vector<br/>        images = images.view(images.shape[0], -1)<br/>        <br/>        # setting gradient to zeros<br/>        optimizer.zero_grad()        <br/>        output = model(images)<br/>        loss = criterion(output, labels)<br/>        <br/>        # backward propagation<br/>        loss.backward()<br/>        <br/>        # update the gradient to new gradients<br/>        optimizer.step()<br/>        running_loss += loss.item()<br/>    else:<br/>        print("Training loss: ",(running_loss/len(trainloader)))</span></pre><p id="fba5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">可视化:</strong></p><p id="8b4a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在神经网络的预测阶段，我们将图像及其概率分布传递给可视化图像。<code class="fe ob oc od ns b">ax1</code>是任意数字的原始图像，<code class="fe ob oc od ns b">ax2</code>是概率分布。只需将xlabel和ylabel设置在0–9和地块标题之间。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="de49" class="mu mv it ns b gy nw nx l ny nz">def view_classify(img, ps):<br/>    ps = ps.data.numpy().squeeze()<br/>    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)<br/>    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())<br/>    ax1.axis('off')<br/>    ax2.barh(np.arange(10), ps)<br/>    ax2.set_aspect(0.1)<br/>    ax2.set_yticks(np.arange(10))<br/>    ax2.set_yticklabels(np.arange(10))<br/>    ax2.set_title('Class Probability')<br/>    ax2.set_xlim(0, 1.1)<br/>    plt.tight_layout()</span></pre><p id="8a10" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">预测:</strong></p><p id="b250" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关闭梯度，因为我们使用相同的模型，这将开始训练，所以我们关闭所有的梯度，并获得测试图像的概率分布。所有的概率都是对数，所以我们将其转换为0–1，并使用该函数可视化图像。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="15d5" class="mu mv it ns b gy nw nx l ny nz"># Getting the image to test<br/>images, labels = next(iter(trainloader))</span><span id="6828" class="mu mv it ns b gy oa nx l ny nz"># Flatten the image to pass in the model<br/>img = images[0].view(1, 784)</span><span id="7aa6" class="mu mv it ns b gy oa nx l ny nz"># Turn off gradients to speed up this part<br/>with torch.no_grad():<br/>    logps = model(img)</span><span id="1529" class="mu mv it ns b gy oa nx l ny nz"># Output of the network are log-probabilities, need to take exponential for probabilities<br/>ps = torch.exp(logps)<br/>view_classify(img, ps)</span></pre><p id="02e6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用PyTorch，您可以在MNIST手数字识别数据集上从头开始训练一个神经网络。</p><blockquote class="os"><p id="3aa0" class="ot ou it bd ov ow ox oy oz pa pb ly dk translated">现在，是庆祝的时候了，因为你实现了！！！</p></blockquote><figure class="pd pe pf pg ph kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/163ef80addea4a8d79396491591573a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*NsjAC6kqR-beGC9YD_hcrQ.gif"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">来源:男高音</p></figure><p id="2240" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">谢谢你阅读我的博客，感谢我的努力。请随时评论和提问，并提出您的建议。你可以在<a class="ae or" href="https://www.linkedin.com/in/ikhushpatel" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae or" href="https://twitter.com/ikhushpatel" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和我的<a class="ae or" href="http://khushpatel.com" rel="noopener ugc nofollow" target="_blank">网站</a>上与我联系，了解更多深度学习项目。快乐学习！！！</p></div></div>    
</body>
</html>