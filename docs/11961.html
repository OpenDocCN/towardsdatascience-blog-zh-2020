<html>
<head>
<title>Reformer: The Efficient Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改革家:高效的变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703?source=collection_archive---------37-----------------------#2020-08-18">https://towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703?source=collection_archive---------37-----------------------#2020-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e57" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/39c1ef30d390c60042448b4df51231c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NHY1lq6MNIjDWw9o"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Denys Nevozhai 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7100" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/transformers-explained-65454c0f3fa7">变压器</a> ( <a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人。艾尔。</a>)很棒，它关注更长的上下文，它提供了 rnn 没有的并行计算，最重要的是，它们拥有最先进的结果。</p><p id="7253" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论改革者模型，它是由 Google Research 在论文<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">Reformer:The Efficient Transformer</a>中提出的。该模型本质上解决了转换器模型的一些效率约束，并提出了转换器的改进版本，该版本实现了<a class="ae ky" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" rel="noopener ugc nofollow" target="_blank">位置敏感散列(LSH) </a>和<a class="ae ky" href="https://arxiv.org/abs/1707.04585" rel="noopener ugc nofollow" target="_blank">可逆层</a>以使模型更加高效。我们将在本帖接下来的部分更详细地讨论这些。</p><h1 id="6945" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">变压器的低效率</h1><p id="7797" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">尽管是最先进的，变压器非常昂贵(w.r.t .内存)。其中最著名的变形金刚是<a class="ae ky" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener">伯特</a> ( <a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">德夫林等人。艾尔。</a>)，也是针对最大允许序列长度 512 进行训练的。为了进一步说明这一点，我们举一个与本文中相同的例子:</p><p id="4a4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">报道的最大变压器配置有 64 层，每层有 0.5B 个参数。假设我们想为一个长度为 64K 的序列训练一个转换器。这里，0.5B 参数占 2GB 内存。此外，批量大小为 8 的 1024 维嵌入权重占 64K x 1K x 8 = 0.5B 个浮点，这也是 2GB 的内存。现在，如果我们要为单个层训练这个模型，我们可以很容易地在单个 GPU 上训练它，但是，还有 63 层要附加。此外，训练 BERT 的语料库需要 17GB 来存储。</p><p id="0a32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于上述情况，以下是原始变压器中解决的问题:</p><ol class=""><li id="ce77" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu"> N 层需要比单层多 N 倍的内存</strong>，这是因为每层的<strong class="lb iu">输入都需要存储用于反向传播</strong>。</li><li id="4750" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><strong class="lb iu"><em class="ng">d _ ff</em></strong><em class="ng"/><strong class="lb iu">(中间前馈层的深度)与<em class="ng">d _ model</em><strong class="lb iu"><em class="ng"/></strong>相比相当大</strong>，因此占用大量内存。</li><li id="db60" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">长度为 L 的序列的注意力计算在<strong class="lb iu">计算和空间复杂度</strong>中都考虑了<strong class="lb iu"> O(L ) </strong>。</li></ol><p id="d84e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在接下来的章节中，我们将看到 Reformer 如何克服这些问题。</p><h1 id="fafc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">区分位置哈希(LSH)注意</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b1bca8c4570a14d72fd812bcbe8a716c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*rBi3HyBH3JRUQ6n5.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Vaswani 等人。艾尔。”</p></figure><p id="70bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是原始 Transformer 模型中按比例缩放的点积注意力。这里，实际的嵌入首先被激活到 3 个不同的向量中，即查询(Q)、密钥(K)和值(V)(对于每个令牌)。然后获得查询和键的矢量点积，告诉<strong class="lb iu">每个矢量对获得给定矢量</strong>有多大贡献(基本上是注意)。关于自我关注的更多内容，你可以阅读我关于变形金刚的博客。</p><h2 id="ff0d" class="ni lw it bd lx nj nk dn mb nl nm dp mf li nn no mh lm np nq mj lq nr ns ml nt bi translated">记忆有效注意</h2><p id="b3b6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">主要问题在于 QKᵀ的任期。假设查询和键的形状分别为<strong class="lb iu"> ( <em class="ng"> batch_size，seq_length，d_model </em> ) </strong>。现在，QKᵀ会产生一个<strong class="lb iu"> ( <em class="ng"> batch_size，seq_length，seq_length </em> ) </strong>的形状。因此，即使批量大小为 1，64K 长度的序列将具有 64k×64k 大小的矩阵(16GB 内存)的 QKᵀ项。</p><p id="60a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，这个问题的解决方法是，我们可以分别计算每个查询<em class="ng"> q_i </em>的关注度，而不是整个 Q 项，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8554bd81d9f4cde97223b6f14f424549.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*ZLfO-ehYSLXXbdoItNx0IQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器文件</a>进行内存高效关注</p></figure><p id="84ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这听起来效率很低，因为这在某种程度上剥夺了变压器的并行处理能力。然而，你会惊讶地知道，LSH 的注意力(我们即将看到)补偿了这一部分。</p><h2 id="647b" class="ni lw it bd lx nj nk dn mb nl nm dp mf li nn no mh lm np nq mj lq nr ns ml nt bi translated">Q = K</h2><p id="24c7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在变压器的原始实现中，使用 3 组不同的权重(线性层)来激活 Q、K 和 V。相反，作者建议对查询和键使用相同的权重。这被称为共享 QK 模式。</p><blockquote class="nv nw nx"><p id="317f" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">事实证明，共享 QK 并不影响变形金刚的性能</p><p id="5d4b" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">— <a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器纸</a></p></blockquote><h2 id="9747" class="ni lw it bd lx nj nk dn mb nl nm dp mf li nn no mh lm np nq mj lq nr ns ml nt bi translated">LSH 注意了</h2><p id="1fc1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">因此，我们讨论了分别计算每个查询的关注度，这似乎很低效，因为它不是并行的。但是如果我们从长度为 64K 的完整序列中取出 32 或 64 个最接近给定查询的键，然后由 T2 计算它们的关注度，会怎么样呢？这正是 LSH 注意力所做的。让我们看看 LSH 是如何工作的:</p><p id="5589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了清楚地了解这一点，我们先来快速讨论一下向量空间的作用。因此，在转换器的第一层，我们本质上将给定序列中的每个记号映射到一个“向量”。这表明我们正在将所有的标记映射到一个公共的向量空间，其中词汇表中所有标记的向量表示共存。现在，在一种语言上训练这些映射，使得<strong class="lb iu">相似或相关标记的向量表示彼此更接近</strong>，而<strong class="lb iu">不相关的标记彼此远离</strong>(参考消息，我们可以使用距离度量来测量这些向量的相关性；例如余弦距离)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/88d9544315fcf917c28dedfc268731c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ig8Ps566wtB1Bpeo-LyNmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器纸</a>进行角度位置敏感散列</p></figure><p id="4649" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将我们之前讨论的向量空间中的任意两个向量视为两点 x 和 y。我们认为这个假想的圆(实际上是球体)包含这些点(比如说，这个圆就是向量空间)。然后，我们将圆分成 4 部分(4 个散列桶),并随机旋转该分区，即随机旋转圆。我们在这里有两个观察结果(参考图):</p><ol class=""><li id="7826" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">在第一种情况下(向上)，向量(点)彼此相对<strong class="lb iu">远</strong>。因此，在随机旋转时，向量很可能<strong class="lb iu">以高概率</strong>在不同的桶中结束。</li><li id="49a8" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">而在第二种情况下(向下),向量彼此明显地<strong class="lb iu">更接近</strong>。所以你可以看到，他们更有可能<strong class="lb iu">以高概率</strong>结束在同一个桶里。</li></ol><blockquote class="nv nw nx"><p id="c27a" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">如果附近的向量很有可能获得相同的散列，而远处的向量则不会，则将每个向量 x 分配给散列 h(x)的散列方案被称为位置敏感的。</p><p id="0719" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">— <a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器纸</a></p></blockquote><p id="3627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在重整器中，这是通过取大小为(<em class="ng"> d_k，b/2 </em>)的随机矩阵<em class="ng"> R </em>来实现的，其中<em class="ng"> d_k </em>是密钥向量的大小，<em class="ng"> b </em>是桶的数量。并且向量 x 的散列函数 h 被给定为:</p><blockquote class="oc"><p id="ed47" class="od oe it bd of og oh oi oj ok ol lu dk translated">h(x)= arg max([xR；-xR ])</p></blockquote><p id="fc7b" class="pw-post-body-paragraph kz la it lb b lc om ju le lf on jx lh li oo lk ll lm op lo lp lq oq ls lt lu im bi translated">其中<em class="ng">【a；b ] </em>是串联。所以通过拥有<em class="ng"> xR </em>和<em class="ng"> -xR </em>，我们实质上是在<em class="ng"> x </em>上进行随机投影。直观来说，我们是取<em class="ng"> b </em>大小为<em class="ng"> d_k </em>和<strong class="lb iu">的向量(桶)评估 x 所属的桶</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/2b5a4d2ddb2b5120461e94b5ed189642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*5E-HOkbBqAJ4nSStr_b_5Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSH 通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">关注改革家论文</a></p></figure><p id="b427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图描述了在转化炉中实现的 LSH 注意的流程。</p><ol class=""><li id="2d7e" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">使用我们刚刚讨论的<strong class="lb iu"> LSH </strong>方案，将查询/键(查询=键)向量分配给它们各自的桶。</li><li id="6407" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">我们<strong class="lb iu">根据它们的桶对</strong>查询/关键向量进行排序。</li><li id="7d22" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">由于哈希桶的大小可能不均匀，因此很难进行批处理。因此，采用的方法是，取固定大小的<strong class="lb iu">块</strong>加偏移量 1；即，我们取大小为<em class="ng"> m 的块</em>和<strong class="lb iu">来计算来自相同桶和相同块以及一个块回</strong>的向量的关注度。通常，块的大小是<em class="ng"> 2l / b </em>，其中<em class="ng"> l </em>是序列长度。</li></ol><ul class=""><li id="6e6a" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu os my mz na bi translated">对于因果屏蔽(或前瞻屏蔽)，首先，获取查询/关键向量的位置 id，根据桶式排序顺序进行排序，然后通过比较这些位置 id，计算屏蔽。</li><li id="d8cc" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu os my mz na bi translated">此外，相似的向量有可能落在不同的桶中。为了解决这个问题，我们可以用不同的散列函数执行<strong class="lb iu"> <em class="ng"> n 轮</em> </strong>轮散列<strong class="lb iu">。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ddc9d79e5bcd0073cc51a5d46d8a4dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*JbiMeTZdX2DC5HxP6eooWw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">改革者论文</a>LSH 注意的每一步上的注意向量的行为和稀疏性</p></figure><h1 id="56bd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">可逆变压器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/52452820c6c8e77964893a84be642883.png" data-original-src="https://miro.medium.com/v2/format:webp/0*LKLPNIsCbR2lZ0AI.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Gomez 等人的常规剩余连接(a) v/s RevNets (b)、(c)的比较。艾尔。通过<a class="ae ky" href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a></p></figure><p id="8a57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们利用前面讨论过的 LSH 注意解决了注意计算中的记忆问题。然而，当涉及到变形金刚消耗的内存时，还有另一个主要的关注点。在前馈层中，通常有一个变压器，其<em class="ng"> d_ff </em>值为～4K，层数为～16。此外，每层的<strong class="lb iu">输入需要被存储用于反向传播</strong>。在这样的设置中，序列长度为 64K，我们最终仍然会得到不切实际的 16GB 内存范围。</p><p id="8b31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了解决这个问题，Reformer 借用了 RevNets(可逆残差神经网络)的思想，这是 Gomez 等人在<a class="ae ky" href="https://arxiv.org/abs/1707.04585" rel="noopener ugc nofollow" target="_blank">中提出的。艾尔。</a>为了理解这种架构，请看上图:</p><p id="7dd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(A)<strong class="lb iu">ResNet 中的常规跳过连接:</strong>我们获取输入，计算层值(实际上是一个函数)，并将其添加到原始层输入中。在图中，x 和 y 的值需要存储在内存中用于反向传播。所以:</p><blockquote class="nv nw nx"><p id="bdd8" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> y = x + F(x) </strong></p><p id="b8ce" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> z = y + G(y) </strong></p></blockquote><p id="17be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(b)<strong class="lb iu">RevNet 中的前向传递:</strong>其思想是允许从下一层的输入中恢复任一层的输入；即</p><blockquote class="nv nw nx"><p id="2b67" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu">y1 = x1；</strong></p><p id="979c" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> y2 = z2 = x2 + F(x1) </strong></p><p id="e396" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> z1 = y1 + G(y2) </strong></p></blockquote><p id="ca86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(c)<strong class="lb iu">RevNet 中的反向传递:</strong>现在，我们可以使用以下公式轻松地重建各层的输入:</p><blockquote class="nv nw nx"><p id="8040" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu">x1 = y1 = z1g(y2)</strong></p><p id="ff8e" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu">x2 = y2 F(x1)= z2f(x1)</strong></p></blockquote><p id="0ace" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，很明显，我们可能会重建输入值，而不是存储它们用于反向传播和节省内存。</p><p id="7b3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，可逆变压器实现如下:</p><blockquote class="nv nw nx"><p id="a538" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> Y1 = X1 +注意(X2) </strong></p><p id="5f38" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu"> Y2 = X2 +前馈(Y1) </strong></p></blockquote><h1 id="28df" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">拆分 d_ff</h1><p id="68c8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最后，我们处理具有<em class="ng"> d_ff </em>输出单元的中间前馈层，其中<em class="ng"> d_ff </em>与<em class="ng"> d_model </em>相比相当大(一般为~4K)。</p><p id="b8fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑具有 64K 令牌的序列，在标准转换器中，所有输出都是并行计算的，因此权重占用更多内存。虽然前馈<strong class="lb iu">输出是针对整个序列</strong>、<strong class="lb iu">并行计算的，但它不一定是</strong>，因为任何给定令牌向量的输出都独立于其他向量。</p><p id="d4d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，重整者建议分块处理该层:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/3f178370a51645740f194d70d27e221b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEPHspPnx4tNWOVIEMW9bg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器纸</a>在中间前馈层分块处理序列</p></figure><blockquote class="nv nw nx"><p id="a966" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">利用分块+可逆层，重整器模型的层输入记忆与层数无关。</p></blockquote><h1 id="bb4f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">总结一下</h1><ol class=""><li id="7525" class="ms mt it lb b lc mn lf mo li ow lm ox lq oy lu mx my mz na bi translated">注意力计算-记忆问题通过使用 LSH 注意力来克服。</li><li id="fc7f" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">通过使用可逆层解决了层输入存储问题。</li><li id="cb5c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">具有<em class="ng"> d_ff </em>输出单元的高维前馈层可以通过分块处理输入序列来中和。</li></ol><h1 id="4b80" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/d39c4cbc32780ef1b9724a981d5a4ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrKPPJmdhi1FFt6NnGYizg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">重整器论文</a>比较各种变压器架构的复杂性</p></figure><p id="1446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里，</p><p id="6aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ng"> b </em> </strong> = &gt;批量，<strong class="lb iu"> <em class="ng"> l </em> </strong> = &gt;序列长度，<strong class="lb iu"> <em class="ng"> d_ff </em> </strong> = &gt;中间前馈的尺寸，<strong class="lb iu"> <em class="ng"> n_h </em> </strong> = &gt;头数，<strong class="lb iu"> <em class="ng"> n_l </em> </strong> = &gt;层数，【T20</p><p id="139b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重整器可以用于从部分图像生成完整的图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/d1b7a6dd809290dc25ff66bfe073d8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zdJ08G1pqt9QEf_S.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd pb">顶部:</strong>用作重整器输入的图像片段。<strong class="bd pb">下图:</strong>“已完成”的全帧图像。原始图像来自<a class="ae ky" href="https://arxiv.org/abs/1707.08819" rel="noopener ugc nofollow" target="_blank"> Imagenet64 数据集</a>通过<a class="ae ky" href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>。</p></figure><blockquote class="nv nw nx"><p id="e371" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">有趣的事实:这个重整器非常高效，它可以在一个只有 16GB 内存的 GPU 上处理长达 100 万字的文本序列。</p><p id="be9a" class="kz la ng lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><strong class="lb iu">有趣的事实:Reformer 可以在一台设备上一次性处理整部小说。</strong></p></blockquote><h1 id="f615" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="63a5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇(很长的)文章中，我们深入讨论了重整器模型。我们看到了它在变压器中解决了哪些效率缺陷，以及如何克服这些缺陷。</p><p id="4f6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/google/trax/tree/master/trax/models/reformer" rel="noopener ugc nofollow" target="_blank">这里是<a class="ae ky" href="https://github.com/google/trax" rel="noopener ugc nofollow" target="_blank"> Google trax </a>提供的重整器代码的 Github 库</a>的链接。</p><p id="25ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个 Colab 笔记本，用于谷歌的<a class="ae ky" href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" rel="noopener ugc nofollow" target="_blank">图像生成演示</a>，另一个用于<a class="ae ky" href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" rel="noopener ugc nofollow" target="_blank">文本生成演示</a>。</p><p id="6bf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/transformers/model_doc/reformer.html" rel="noopener ugc nofollow" target="_blank">这里是 huggingface 的 API 文档</a>的链接，用于重整器实施和预训练重量。</p><h1 id="1472" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="334f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">重整器纸:<a class="ae ky" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2001.04451</a></p><p id="18b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RevNets 论文:<a class="ae ky" href="https://arxiv.org/abs/1707.04585" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.04585</a></p><p id="a112" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变压器纸:<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></p><div class="pc pd gp gr pe pf"><a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">改革家:高效的变压器</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">理解连续数据——如语言、音乐或视频——是一项具有挑战性的任务，尤其是当有…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">ai.googleblog.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/transformers-explained-65454c0f3fa7"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">变形金刚解释</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">对谷歌 Transformer 模型的详尽解释；从理论到实施</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pu l pq pr ps po pt ks pf"/></div></div></a></div></div></div>    
</body>
</html>