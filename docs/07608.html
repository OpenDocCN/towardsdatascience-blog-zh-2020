<html>
<head>
<title>K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-94395f445221?source=collection_archive---------5-----------------------#2020-06-08">https://towardsdatascience.com/k-nearest-neighbors-94395f445221?source=collection_archive---------5-----------------------#2020-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d4a9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于KNN你需要知道的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b83665cf69eaa078c238c9b5fb230ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iCbC9V1ylRZT8ztl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">罗伯特·卡茨基在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw"><p id="2b18" class="kx ky iq bd kz la lb lc ld le lf lg dk translated">"一个人因他交的朋友而出名。"</p></blockquote><p id="266f" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb lg ij bi translated">我必须说这是一个完美的开场白，用来展示K个最近的邻居。是的，这就是KNN背后的简单概念。它只是根据一个数据点最近的几个邻居对其进行分类。有多少邻居？这是我们的决定。</p><p id="3f8d" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">看起来你已经知道了很多关于这个简单模型的知识。让我们深入了解一下。</p><p id="52ab" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">在继续之前，知道KNN可用于分类和回归问题是很重要的。我们将首先理解它是如何解决分类问题的，从而使可视化回归变得更容易。</p><h2 id="06e0" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lq mq mr ms lu mt mu mv ly mw mx my mz bi translated">KNN分类器</h2><p id="fc30" class="pw-post-body-paragraph lh li iq lj b lk na jr lm ln nb ju lp lq nc ls lt lu nd lw lx ly ne ma mb lg ij bi translated">我们将要使用的数据是<a class="ae kv" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv" rel="noopener ugc nofollow" target="_blank"> <em class="nf">乳腺癌威斯康星(诊断)数据集</em> </a> <em class="nf">。</em>有30个属性对应于为考虑中的细胞核计算的实值特征。该数据中共有569个样本，其中357个被归类为<em class="nf">‘良性’</em>(无害)，其余212个被归类为<em class="nf">‘恶性’</em>(有害)。</p><p id="03ea" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated"><em class="nf">诊断</em>栏分别包含恶性和良性癌症的“M”或“B”值。为了更好的分析，我将这些值分别改为1和0。</p><p id="9c9d" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">此外，为了这篇文章，我将只使用数据中的两个属性→“平均半径”和“平均纹理”。这将有助于我们理解KNN划定的决策界限。下面是最终数据的样子(洗牌后):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7affb5c44ad1cb4f52e2ee046ed2865b.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*J-XVpcW4-sWrrwS3b5W8kg.png"/></div></figure><p id="b6d9" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">让我们给KNN编码:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="7bbb" class="mh mi iq ni b gy nm nn l no np"># Defining X and y<br/>X = data.drop('diagnosis',axis=1)<br/>y = data.diagnosis</span><span id="f73f" class="mh mi iq ni b gy nq nn l no np"># Splitting data into train and test<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)</span><span id="dc7e" class="mh mi iq ni b gy nq nn l no np"># Importing and fitting KNN classifier for k=3<br/>from sklearn.neighbors import KNeighborsClassifier<br/>knn = KNeighborsClassifier(n_neighbors=3)<br/>knn.fit(X_train,y_train)</span><span id="f9fc" class="mh mi iq ni b gy nq nn l no np"># Predicting results using Test data set<br/>pred = knn.predict(X_test)<br/>from sklearn.metrics import accuracy_score<br/>accuracy_score(pred,y_test)</span></pre><p id="50dc" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">上面的代码应该会给你一个稍微不同的输出。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="2d3a" class="mh mi iq ni b gy nm nn l no np">0.8601398601398601</span></pre><p id="8b7e" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">刚刚发生了什么？当我们在训练数据上训练KNN时，它对每个数据样本采取以下步骤:</p><ol class=""><li id="8515" class="nr ns iq lj b lk mc ln md lq nt lu nu ly nv lg nw nx ny nz bi translated">借助欧几里得方法计算数据样本和其他样本之间的距离。</li><li id="a6ac" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg nw nx ny nz bi translated">按升序排列这些距离值。</li><li id="1e48" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg nw nx ny nz bi translated">从排序的距离中选择前K个值。</li><li id="be02" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg nw nx ny nz bi translated">根据上述K值中最常见的类别将类别分配给样本。</li></ol><p id="f898" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">让我们想象一下KNN是如何在训练数据集上画出一个决策边界的，以及这个边界是如何被用来对测试数据集进行分类的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/53f0723706c6370e02e2ca365032d3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qm1aQ8WeB9pCx95JeYc3Gw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K=3时的KNN分类。图片由<a class="og oh ep" href="https://medium.com/u/db3258338f2f?source=post_page-----94395f445221--------------------------------" rel="noopener" target="_blank"> Sangeet Aggarwal </a></p></figure><p id="1703" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">训练准确率为93%,测试准确率为86%,我们的模型在这里可能显示出过度拟合。为什么这样</p><p id="907c" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">当K值或邻域数太低时，模型仅选取最接近数据样本的值，从而形成如上所示的非常复杂的决策边界。这种模型无法在测试数据集上很好地概括，从而显示出较差的结果。</p><p id="e00f" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">这个问题可以通过调整参数<em class="nf"> n_neighbors </em>的值来解决。<em class="nf">随着我们增加邻居的数量，模型开始很好地泛化，但是增加太多的值会再次降低性能。</em></p><p id="b229" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">因此，找到K的最佳值很重要，这样模型就能够在测试数据集上很好地分类。让我们观察训练，并随着邻居数量的增加测试精确度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a34a6a9c09b49f761e107ae649fc1da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*WEBsOX1tzHMNS0MUFFzYog.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在k=11时观察到最佳结果</p></figure><p id="b5f2" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">上面的结果可以通过下面的图得到最好的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f30b1bee8bb3eca8ebd40ae66f7a63e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*-HjYYn4bT-jFlzSboNa39g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="og oh ep" href="https://medium.com/u/db3258338f2f?source=post_page-----94395f445221--------------------------------" rel="noopener" target="_blank"> Sangeet Aggarwal </a></p></figure><p id="869f" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">该图显示了测试精度在某一点上的总体上升趋势，之后精度再次开始下降。这是最近邻的最佳数量，在本例中为11，测试准确率为90%。</p><p id="ed24" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">让我们再次画出k=11的决策边界，看看它是什么样子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/6a4cce0032ed548f8acceea80a9bdb6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aHgyjJM_TM3aV003LIPJ8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K=11时的KNN分类。图片由<a class="og oh ep" href="https://medium.com/u/db3258338f2f?source=post_page-----94395f445221--------------------------------" rel="noopener" target="_blank"> Sangeet Aggarwal </a>提供</p></figure><p id="89fb" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">我们通过微调邻居的数量来改善结果。此外，KNN的决策边界现在平滑得多，能够很好地概括测试数据。</p><p id="715a" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">现在让我们来理解KNN是如何用于回归的。</p><h2 id="8bdf" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lq mq mr ms lu mt mu mv ly mw mx my mz bi translated">KNN回归量</h2><p id="5fcb" class="pw-post-body-paragraph lh li iq lj b lk na jr lm ln nb ju lp lq nc ls lt lu nd lw lx ly ne ma mb lg ij bi translated"><em class="nf">KNN分类器返回最近K个邻居的模式，而KNN回归器返回最近K个邻居的平均值。</em></p><p id="001c" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">我们将用<a class="ae kv" href="http://faculty.marshall.usc.edu/gareth-james/ISL/data.html" rel="noopener ugc nofollow" target="_blank">广告数据</a>来理解KNN的回归。下面是电视预算和销量的前几排。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e3678ef90907e7340b2c6ee7b824709c.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/1*BNiR_TDXXTQH9UCd5FslKw.png"/></div></figure><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="3560" class="mh mi iq ni b gy nm nn l no np"># Defining X and Y<br/>X_ad = ad.TV.values.reshape(-1,1)<br/>y_ad = ad.sales</span><span id="5ff1" class="mh mi iq ni b gy nq nn l no np"># Splitting data into train and test<br/>train_x, test_x, train_y, test_y = train_test_split(X_ad, y_ad, test_size=0.25, random_state=42)</span><span id="e025" class="mh mi iq ni b gy nq nn l no np"># Running KNN for various values of n_neighbors and storing results<br/>knn_r_acc = []</span><span id="742a" class="mh mi iq ni b gy nq nn l no np">for i in range(1,17,1):<br/>    knn = KNeighborsRegressor(n_neighbors=i)<br/>    knn.fit(train_x,train_y)</span><span id="2f13" class="mh mi iq ni b gy nq nn l no np">    test_score = knn.score(test_x,test_y)<br/>    train_score = knn.score(train_x,train_y)</span><span id="797e" class="mh mi iq ni b gy nq nn l no np">    knn_r_acc.append((i, test_score ,train_score))</span><span id="c2a3" class="mh mi iq ni b gy nq nn l no np">df = pd.DataFrame(knn_r_acc, columns=['K','Test Score','Train Score'])<br/>print(df)</span></pre><p id="62ba" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">上面的代码将针对不同的K值(从1到16)运行KNN，并将训练和测试分数存储在一个数据帧中。让我们看看当我们增加<em class="nf"> n_neighbors(或K) </em>的值时，这些分数是如何变化的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/181afe80e57209daa20df81a858f1528.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*IOdstP_uo6s9LVmT1gq0mA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K=4时的最佳结果</p></figure><p id="1571" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">在K=1时，KNN倾向于紧密跟随训练数据，因此显示出高的训练分数。但相比较而言，测试分数相当低，从而说明过度拟合。</p><p id="9bcd" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">让我们想象一下KNN如何绘制不同k值的回归路径</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/8c6851330c0a8211d0e272a2d7beb4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDl3IVqFv0X0ha9zaTC2-w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd on">左:</strong>用KNN回归器训练数据集<strong class="bd on">右:</strong>用相同的KNN回归器测试数据集。图片由<a class="og oh ep" href="https://medium.com/u/db3258338f2f?source=post_page-----94395f445221--------------------------------" rel="noopener" target="_blank"> Sangeet Aggarwal </a>提供</p></figure><p id="550f" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">随着K的增加，KNN将数据拟合为更平滑的曲线。这是因为K值越高，考虑的数据就越多，从而降低了模型的整体复杂性和灵活性。</p><p id="9ac0" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">正如我们之前看到的，增加K值会将分数提高到一定程度，之后分数会再次下降。这一点可以通过下面的情节更好的理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/241e2b30abce85e2fbfb9ed76b15a5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*GD3_9ICuIDdBqK0XYRrxeQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="og oh ep" href="https://medium.com/u/db3258338f2f?source=post_page-----94395f445221--------------------------------" rel="noopener" target="_blank"> Sangeet Aggarwal </a>提供</p></figure><p id="fa2a" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">正如我们在该图中看到的，该模型在K=4时产生最佳结果。我用R来评估这个模型，这是我们能得到的最好的结果。这是因为我们的数据集太小而且太分散。</p><p id="65d8" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">关于KNN，需要了解的其他几点很重要:</p><ul class=""><li id="daab" class="nr ns iq lj b lk mc ln md lq nt lu nu ly nv lg oo nx ny nz bi translated">KNN分类器没有任何专门的训练阶段，因为它使用所有的训练样本进行分类，并简单地将结果存储在内存中。</li><li id="745a" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg oo nx ny nz bi translated">KNN是一种非参数算法，因为它没有对训练数据做任何假设。这对于具有非线性数据的问题非常有用。</li><li id="7d03" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg oo nx ny nz bi translated">如果数据非常大，KNN在时间和存储方面的计算都很昂贵，因为KNN必须存储训练数据才能工作。其他监督学习模型一般不会出现这种情况。</li><li id="eaed" class="nr ns iq lj b lk oa ln ob lq oc lu od ly oe lg oo nx ny nz bi translated">KNN对数据的规模非常敏感，因为它依赖于计算距离。对于比例较高的要素，计算出的距离可能会很大，结果可能会很差。因此，建议在运行KNN之前缩放数据。</li></ul><p id="c5e6" class="pw-post-body-paragraph lh li iq lj b lk mc jr lm ln md ju lp lq me ls lt lu mf lw lx ly mg ma mb lg ij bi translated">这个帖子到此为止。我希望你在学习KNN的时候过得愉快。欲知详情，敬请关注。</p></div></div>    
</body>
</html>