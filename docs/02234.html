<html>
<head>
<title>An Introduction to Support Vector Regression (SVR)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量回归机简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2?source=collection_archive---------0-----------------------#2020-03-03">https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2?source=collection_archive---------0-----------------------#2020-03-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0ce9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用支持向量机进行回归</h2></div><p id="1ec6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">支持向量机在分类问题中是众所周知的。然而，在回归中使用支持向量机并没有很好的记录。这些类型的模型被称为支持向量回归(SVR)。</p><p id="0cd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我将介绍SVR与其他回归模型相比的用处，深入研究算法背后的数学，并提供一个使用波士顿房价数据集的示例。</p><h2 id="e6b1" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">简单线性回归</h2><p id="0140" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在大多数线性回归模型中，目标是最小化误差平方和。以普通最小二乘法(OLS)为例。具有一个预测值(特征)的OLS的目标函数如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/b3fb41faf3c0881d10584411410e9100.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*C0eH2vDkD_PvWWMdxr4X8Q.png"/></div></figure><p id="7ae8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中，yᵢ是目标，wᵢ是系数，xᵢ是预测值(特征)。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/23562e25d5a01665b137d01b14653cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*mR2v3pLqTPyyp9BOCQuvEQ.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">OLS对波士顿房价的预测</p></figure><p id="d61a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Lasso、Ridge和ElasticNet都是这个简单方程的扩展，带有一个额外的惩罚参数，旨在最小化复杂性和/或减少最终模型中使用的特征数量。不管怎样，和许多模型一样，目标是减少测试集的误差。</p><p id="6afd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，如果我们只关心在一定程度上减少误差呢？如果我们不在乎我们的误差有多大，只要它们在可接受的范围内，会怎么样？</p><p id="d094" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以房价为例。如果我们认为预测在一定的金额范围内——比如说5000美元——是可以接受的，那会怎么样呢？只要误差在这个范围之内，我们就可以在寻找预测值时给我们的模型一些灵活性。</p><h2 id="7920" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">SVR FTW</h2><p id="61c0" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">输入支持向量回归。SVR让我们可以灵活地定义模型中可以接受的误差，并找到合适的线(或更高维的超平面)来拟合数据。</p><p id="7758" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与OLS相反，SVR的目标函数是最小化系数——更具体地说，是系数向量的<em class="mp"> l </em> 2范数——而不是平方误差。误差项改为在约束中处理，其中我们将绝对误差设置为小于或等于指定的裕量，称为最大误差ϵ(ε)。我们可以调整epsilon来获得我们的模型所需的精度。我们新的目标函数和约束如下:</p><p id="e9a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最小化:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2f5bbb6782bcb7cdc19c46af8678c5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*6M8yyY7yC7xJX6nFN2SdCQ.png"/></div></figure><p id="7759" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">约束:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/c3ead56d05e988da37591bcc2d0ef430.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*gpN_ZxDuLgusn-O0fck13A.png"/></div></figure><p id="57ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">说明性示例:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/bd31252e773764e11683b4f75a68cbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrXHNqC_hqpyux7GUbtqAQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">简单SVR的说明性示例</p></figure><p id="61b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在数据集上尝试简单的SVR。下图显示了训练好的SVR模型对波士顿房价数据的结果。红线代表最佳拟合线，黑线代表误差幅度，ϵ，我们将其设置为5(＄5000)。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/1e03f6644c9f84d826f00c0b2a02357a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*bSZn9bK43MaA5vVDamRQ2A.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">基于ϵ=5的波士顿房价SVR预测</p></figure><p id="6214" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会很快意识到这个算法并不适用于所有的数据点。该算法尽可能好地解决了目标函数，但是一些点仍然落在边界之外。因此，我们需要考虑比ϵ.更大的误差的可能性我们可以用松弛变量做到这一点。</p><h2 id="6229" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">给我们自己一些空间(和另一个超参数)</h2><p id="2ba3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">松弛变量的概念很简单:对于任何落在ϵ之外的值，我们可以用ξ来表示它与边距的偏差。</p><p id="4cdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们知道这些偏差有可能存在，但是我们仍然希望尽可能地减少它们。因此，我们可以将这些偏差添加到目标函数中。</p><p id="9ee9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最小化:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/7cc284e0cc74000ec6d56672892d5a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*6Pk3E2Mkg99jXbQTFla99Q.png"/></div></figure><p id="234f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">约束:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi my"><img src="../Images/0c79916f981fc1780872b13c0adeab6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*2BcaD3X72lAj7IezAOLJxg.png"/></div></figure><p id="1de6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">例证:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/f86806b06db83f7b16a0f59651518089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BunsGiCZCPAHzp33L2lCBA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">带有松弛变量的支持向量回归的示例</p></figure><p id="fef8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有了一个额外的超参数，<em class="mp"> C </em>，<em class="mp"> </em>，我们可以对其进行调整<em class="mp">。</em>随着<em class="mp"> C </em>的增加，我们对ϵ以外积分的容忍度也在增加。当<em class="mp"> C </em>接近0时，容差接近0，方程就变成了简化的(尽管有时不可行)方程。</p><p id="7673" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们设置<em class="mp"> C </em> =1.0，重新训练上面的模型。结果绘制如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e5b554c52ab12700aee752ef4f7b8ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*bi8rm_6KlJfoZdX5CNPM7w.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">用ϵ=5，C=1.0的SVR预测波士顿房价</p></figure><h2 id="6c70" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">寻找C的最佳值</h2><p id="2ba9" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">上述模型似乎更符合数据。我们可以更进一步，在<em class="mp"> C </em>上进行网格搜索，以获得更好的解决方案。让我们定义一个评分标准，ε内的<em class="mp"> %。该指标衡量测试集中的总点数有多少落在我们的误差范围内。我们还可以监控平均绝对误差(<em class="mp"> MAE </em>)如何随<em class="mp"> C </em>变化。</em></p><p id="9309" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是网格搜索结果图，x轴上的值为<em class="mp"> C </em>，左右y轴上的值分别为ε和<em class="mp"> MAE </em>内的值为<em class="mp"> %。</em></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mz"><img src="../Images/a8017d9a86b371c257d90e99cda34b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL3ErBjXHfFk1eF0oTLvDw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">C语言网格搜索</p></figure><p id="f84d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，<em class="mp"> MAE </em>一般随着<em class="mp"> C </em>的增加而减少。然而，我们看到ε指标中的<em class="mp"> %出现最大值。因为我们这个模型的最初目标是在我们的误差范围(＄5000)内最大化预测，我们想要找到在ε</em>内最大化<em class="mp"> %的<em class="mp"> C </em>的值。因此，<em class="mp"> C </em> =6.13。</em></p><p id="534c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用我们最终的超参数，ϵ=5，<em class="mp"> C </em> =6.13，建立最后一个模型。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/2ab026d3a45d2cba0f2091a337cf78d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*CgDvkD7OtXVhOScMpyMKsw.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">用ϵ=5的SVR预测波士顿房价，C=6.13</p></figure><p id="3a3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上图所示，正如预期的那样，这个模型又比以前的模型有所改进。</p><h2 id="1e4f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">结论</h2><p id="6d23" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">SVR是一种强大的算法，允许我们通过可接受的误差margin(ϵ和调整我们对超出可接受误差率的容忍度来选择我们对误差的容忍度。希望本教程已经向您展示了SVR的来龙去脉，并让您有足够的信心将它添加到您的建模武库中。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="a925" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！如果你喜欢这个帖子或者有任何问题/评论，请在下面留下评论！</p><p id="8357" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想更新我正在写的东西、我正在构建的辅助项目或我觉得有趣的文章，请随时加入我的时事通讯— <a class="ae nh" href="http://aspiringdatascientist.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">【有抱负的数据科学家</strong> </a>。</p><p id="0abd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mp">这篇帖子的代码可以在我的</em> <a class="ae nh" href="https://github.com/tomsharp/SVR/blob/master/SVR.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> GitHub </em> </a> <em class="mp">页面找到。</em></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="865b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mp">关于这个话题的附加信息:</em><a class="ae nh" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . SVM . SVR . html</a><a class="ae nh" href="https://en.wikipedia.org/wiki/Support-vector_machine#Regression" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Support-vector _ machine # Regression</a><a class="ae nh" href="https://www.saedsayad.com/support_vector_machine_reg.htm" rel="noopener ugc nofollow" target="_blank">https://www.saedsayad.com/support_vector_machine_reg.htm</a></p></div></div>    
</body>
</html>