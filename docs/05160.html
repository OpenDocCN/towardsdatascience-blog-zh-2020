<html>
<head>
<title>Random Forest Algorithm From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的随机森林算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-random-forest-classifier-c73a4cae6781?source=collection_archive---------17-----------------------#2020-05-03">https://towardsdatascience.com/building-a-random-forest-classifier-c73a4cae6781?source=collection_archive---------17-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9ace" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直觉、伪代码和代码</h2></div><p id="0a2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从头开始实现随机森林 ML 算法听起来像是一项令人生畏的任务。仅仅是想到我们要做对多少细节，就很容易感到不知所措。或者至少我是这么觉得的。我从哪里开始？第一步是什么？我如何知道它是否有效？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/da2de02cbb1dd37aa5fea17e25e90c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IAKsdhVbgijdvHgZ_UlTGw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae lv" href="https://pixabay.com/users/Tumisu-148124/" rel="noopener ugc nofollow" target="_blank">图米苏</a>在<a class="ae lv" href="https://pixabay.com/photos/mistake-error-question-mark-fail-1966448/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上的照片</p></figure><p id="579a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">转折点是当我意识到我需要从<strong class="kk iu">而不是</strong>思考代码开始。想象一下，我站在算法里面，在一棵树的根部，我会怎么做？然后呢？问这些问题有助于分解和简化问题。回答这些问题将建立对算法内部工作的直觉。在本帖中，我们将和<a class="ae lv" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>一起探讨这些问题和答案。</p><p id="04f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的议程:</p><ol class=""><li id="c98f" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mb mc md me bi translated">涵盖随机森林功能的高级概述</li><li id="020c" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">为一个<strong class="kk iu">二进制随机森林分类器</strong>编写伪代码</li><li id="d7c4" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">解决一些最小的数据预处理需求</li><li id="084d" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">编写代码(可在此处完整访问<a class="ae lv" href="https://gist.github.com/viennabai/8d69174b1a02756326f2150d3ac25513" rel="noopener ugc nofollow" target="_blank"/></li><li id="0746" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">对照 scikit-learn 算法检查结果</li></ol><p id="708f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，这篇帖子的灵感来自于<a class="ae lv" href="http://course18.fast.ai/ml" rel="noopener ugc nofollow" target="_blank"> Fast.ai 机器学习课程</a>，我强烈推荐给那些通过做来学习最好的人。</p><p id="c1d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们开始吧！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="2a19" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated"><strong class="ak">树</strong></h1><p id="0209" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">如果你在泰坦尼克号上，你会幸存吗？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi no"><img src="../Images/506c7933bfe05f6013386235a31512ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7c8j-7sPpWtggG9S71sGw.png"/></div></div></figure><p id="2ad0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“均值”是指那个群体中每个人的平均存活率。</p><ul class=""><li id="4b3e" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld np mc md me bi translated">开始时，船上每个人的预期存活率是 40%</li><li id="a972" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld np mc md me bi translated">那么对于所有女性，75%</li><li id="b2b6" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld np mc md me bi translated">而对于所有男性，20%……等等。</li></ul><p id="3d81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对我来说，我是一个女性，说我有一张三等票。按照这个决策树，我在泰坦尼克号上幸存的概率是 50/50。</p><p id="0a28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个人，男性，也在第三类，将有 10%的生存机会；除非他未满 15 岁，否则他生还的可能性是 90%。</p><p id="508a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是随机森林中的一棵决策树。我们已经被提供了根据哪个特征(即年龄)以及在哪个级别(即 15 岁以下)进行划分。为了预测一个新轮廓的存活几率，我们简单地跟踪这些分支。</p><p id="52e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们拿走树枝。想象我们在树根处。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/14a08d6c9b2289a526e7b7f88116b08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*UG-eV0EHjAo6-0XO5IZq-w.png"/></div></figure><p id="76ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">据我们所知，所有样本中，有 40%存活了下来。我们必须决定拆分哪个功能，以及在什么级别上拆分。怎么会？</p><p id="5d46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将尝试每一个功能和每一个级别，对每一个组合进行评分，并选择最好的。</p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="cae5" class="nw ms it ns b gy nx ny l nz oa">for feature in list of features: <br/>     for (unique) row in data: <br/>          what is the score of we split here? <br/>          if the score is better what we have: update it </span></pre><p id="a15a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是分数是多少呢？我们如何知道哪种分割是最好的？</p><h2 id="1998" class="nw ms it bd mt ob oc dn mx od oe dp nb kr of og nd kv oh oi nf kz oj ok nh ol bi translated">分数</h2><p id="10ce" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">对于分类问题，一个常用的指标是基尼不纯度(这也是 scikit-learn 的缺省值)。每个分割都有一个基尼系数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6b60366fcce88a003c4c8c9f93a9910d.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*liOUjLTF_KN34-T66YGEfg.png"/></div></figure><p id="8f56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个简单的例子:</p><p id="fe06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">如果我把女性和男性分开，基尼系数会是多少？</em></p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="e5a0" class="nw ms it ns b gy nx ny l nz oa">x = [1, 1, 1, 1, 1, 2, 2, 2] #5 female &amp; 4 male<br/>y = [1, 1, 1, 1, 0, 0, 0, 0] #4 survived &amp; 4 died</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1d7013ac781ec0caf70040e9c0088684.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*D9bWFUy4qzzpFyfRBgwQ5w.png"/></div></figure><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="fd2d" class="nw ms it ns b gy nx ny l nz oa"><strong class="ns iu">Decision Tree:</strong></span><span id="641a" class="nw ms it ns b gy oo ny l nz oa">def check_features: <br/>     for feature in list of features: <br/>          find_best_split of this feature</span><span id="9817" class="nw ms it ns b gy oo ny l nz oa">def find_best_split(feature):<br/>     for (unique) row in data:<br/>          left = list of all rows &lt;= current row          <br/>          right = list of all rows &gt; current row <br/>          calculate the score           <br/>          if the score is better than what we have: update it</span><span id="50de" class="nw ms it ns b gy oo ny l nz oa">def find_gini(left_split, right_split, y): <br/>     p(left) = n_left/n_y<br/>     p(right) = n_right/n_y<br/>     for each outcome: #in this case, Survived or Died<br/>          p = probability of this outcome for the left <br/>          sum_left += p^2 <br/>          p = probability of this outcome for the right<br/>          sum_right += p^2<br/>     gini = weighted average of (1-sum_left) and (1-sum_right)</span><span id="545d" class="nw ms it ns b gy oo ny l nz oa">def prediction():  <br/>     return array of predictions for this tree </span></pre><h1 id="c10a" class="mr ms it bd mt mu op mw mx my oq na nb jz or ka nd kc os kd nf kf ot kg nh ni bi translated">随机森林</h1><p id="fe29" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">一个森林由无数的决策树组成。<em class="le">随机部分呢？</em>假设我们有一个包含 1000 个样本和 5 个特征的数据集。我们想建 10 棵树。所有的树都会在同一个地方裂开，最后形成 10 棵完全相同的树。这对我们帮助不大。</p><p id="41d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们每次取数据集的不同部分会怎么样？假设我们想要 100 棵树，这意味着每棵树 10 个样本，这将导致相当高的方差。</p><p id="8bd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有没有办法让我们每次都用 1000 个样品？而且每棵树的数据也略有不同？是:<a class="ae lv" href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">自举</strong> </a>。我们从数据集中抽取 1000 个样本，进行替换。平均来说，大约 63%的原始观测值将被包括在内。这对于减少过度拟合很重要。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/53fa2d07729e8ce5dae773071e58f790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*g8e2_XwoPjtZOkcozolrQw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">使用 3 个样本自举</p></figure><p id="ef6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用这种方法，我们将为每棵树提供不同的数据。随机森林的目标是拥有彼此尽可能不同的树，但非常擅长预测给定的数据。然后，如果你有许多不相关的树，每个树都有很高的预测能力，当你把它们加起来时，误差总和将为零。剩下的将是 X 和 y 变量之间的真实关系。</p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="63fc" class="nw ms it ns b gy nx ny l nz oa"><strong class="ns iu">Random Forest</strong>:</span><span id="fa73" class="nw ms it ns b gy oo ny l nz oa">def __init__ (x, y, n_trees, sample_size, min_leaf):<br/>     for numbers up till 1-n_trees: <br/>          create a tree </span><span id="37b7" class="nw ms it ns b gy oo ny l nz oa">def create_tree():<br/>     get sample_size samples, use np.random.choice w/ replacement<br/>     create an instance of the Decision Tree class </span><span id="0096" class="nw ms it ns b gy oo ny l nz oa">def predict():<br/>     average of predictions from n_trees <br/>     return binary outcome 0 or 1 </span></pre><p id="c84c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有了这两个类的伪代码。让我们将它转换成代码，并与 scikit-learn 的 RandomForestClassifier 的结果进行比较。</p><h2 id="237e" class="nw ms it bd mt ob oc dn mx od oe dp nb kr of og nd kv oh oi nf kz oj ok nh ol bi translated">数据</h2><ul class=""><li id="ee77" class="lw lx it kk b kl nj ko nk kr ov kv ow kz ox ld np mc md me bi translated">输入和目标必须是数字</li><li id="b135" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld np mc md me bi translated">分类特征是标签编码的(如果是有序的)或一个热编码的</li></ul><p id="5337" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">离群值和特征缩放怎么办？</em>原来随机森林一般对这些免疫。它的决定是有序的。请记住，在每次考虑分割时，我们将样本分为两组:大于分割值的样本，以及小于或等于分割值的样本。如果我们考虑在 15 岁分开:不管你的年龄是 16 岁还是 60 岁，两者都将被放在一组。随机森林很少假设数据的分布，这是一个很好的起点。</p><p id="41c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是预处理后的 Titanic 数据集的输入内容:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oy"><img src="../Images/7feca2a436f4169b81d8699aaf123d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L52nDYeM_4rGK4FGPYzF4Q.png"/></div></div></figure><p id="df0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们关注 2 个特性的子集:Pclass 和 Sex。</p><h1 id="0920" class="mr ms it bd mt mu op mw mx my oq na nb jz or ka nd kc os kd nf kf ot kg nh ni bi translated">代码</h1><h2 id="8455" class="nw ms it bd mt ob oc dn mx od oe dp nb kr of og nd kv oh oi nf kz oj ok nh ol bi translated">第一次拆分</h2><p id="ea2e" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">我们回到了第一棵树的根部。让我们写吧</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="b83c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">scikit-learn 树告诉我们这是第一次分裂。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/2d4bf6a9200b696a908670a929bb666e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*WAxzIIKLk3TMpNdxl7aTFw.png"/></div></figure><p id="7f15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的树也做了同样的决定！</p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="6a1b" class="nw ms it ns b gy nx ny l nz oa">tree = RandomForest(x_sub, y_train, 1).tree[0]<br/>[output] gini: 0.318; split:1, var: Sex</span></pre><h2 id="bf46" class="nw ms it bd mt ob oc dn mx od oe dp nb kr of og nd kv oh oi nf kz oj ok nh ol bi translated">进一步分裂</h2><p id="6952" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">如果我们允许 scikit-learn 树再分裂一次(深度=2):</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pc"><img src="../Images/9e8a50a84dc8a1a6df5361a5ca187e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYMlBKDgTCU0y4tfKRZWBQ.png"/></div></div></figure><p id="b472" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一次分开后，我们把所有的女人分在一组，所有的男人分在另一组。对于下一次拆分，这两个组将有效地成为他们自己的决策树的根。对于女性来说，下一步的划分是将第三等级从其他等级中分离出来。对男人来说，下一个分裂是把第一阶层从其他阶层中分裂出来。</p><p id="3830" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们修改一下我们的伪代码:</p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="4e11" class="nw ms it ns b gy nx ny l nz oa">def check_features: <br/>     for feature in list of features: <br/>          find_best_split of this feature<br/>     left_hand_side = rows &lt;= our split point <br/>     right_hand_side = rows &gt; our split point <br/>     DecisionTree(left_hand_side)<br/>     DecisionTree(right_hand_side)<br/></span></pre><p id="4b27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果与 scikit-learn 的结果相匹配:</p><pre class="lg lh li lj gt nr ns nt nu aw nv bi"><span id="b7ec" class="nw ms it ns b gy nx ny l nz oa">tree = RandomForest(x_sub, y_train, 1).tree[0]<br/>[output] gini: 0.318; split:1, var: Sex</span><span id="edc8" class="nw ms it ns b gy oo ny l nz oa">tree.lhs <br/>[output] gini: 0.264; split:2, var: Pclass</span><span id="9d3a" class="nw ms it ns b gy oo ny l nz oa">tree.rhs<br/>[output] gini: 0.272; split:1, var: Pclass</span></pre><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h1 id="6bbb" class="mr ms it bd mt mu op mw mx my oq na nb jz or ka nd kc os kd nf kf ot kg nh ni bi translated">预测</h1><p id="e102" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">让我们加入更多特性，看看我们的模型是如何工作的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi gj"><img src="../Images/38cae66eb4049de13a4af607f5163dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HH-839_dzM6gQY1iumbLoA.png"/></div></div></figure><p id="6aae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 10 个决策树，我们的模型在测试集上有 81%的准确率。分数低于 scikit-learn 的分数。但是对于一个我们从零开始构建的算法来说还不错！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="33a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以你看，从零开始建立一个随机的森林并不一定是令人生畏的。学习过程的这一部分让我能够区分出算法的哪些部分我真正理解了，哪些部分我还需要继续努力。有时候，除非你尝试自己去实现，否则你不会明白。</p><p id="ab4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你让我分享我所学到的东西。我希望这篇文章对你有所帮助。欢迎提出问题和建议！</p><p id="3fcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><ul class=""><li id="1beb" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld np mc md me bi translated">J.霍华德，<a class="ae lv" href="http://course18.fast.ai/ml" rel="noopener ugc nofollow" target="_blank"> Fast.ai 机器学习</a>，第 7 课</li><li id="bc36" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld np mc md me bi translated">G.詹姆斯、d .威滕、w .特雷弗和 r .蒂布拉尼，<a class="ae lv" href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf" rel="noopener ugc nofollow" target="_blank">统计学习简介</a>，第 5 章</li></ul><p id="e8e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直到下一次，</p><p id="331d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">V</p></div></div>    
</body>
</html>