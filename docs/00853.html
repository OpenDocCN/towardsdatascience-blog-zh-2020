<html>
<head>
<title>Deep Learning: Solving Problems With TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:用TensorFlow解决问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-solving-problems-with-tensorflow-3722b8eeccb1?source=collection_archive---------11-----------------------#2020-01-24">https://towardsdatascience.com/deep-learning-solving-problems-with-tensorflow-3722b8eeccb1?source=collection_archive---------11-----------------------#2020-01-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3b2c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何解决优化问题，并使用MNIST数据集训练您的第一个神经网络！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/323b2e6c75b33200c55a1f4ea3766b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5TxrZBKoyQwSMywM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/_af0_qAh4K4" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="0ae1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="9ba0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文的目标是用TensorFlow定义和解决实际用例。为此，我们将解决:</p><ul class=""><li id="4f94" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">最优化问题</li><li id="eefd" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">线性回归问题，我们将调整回归直线到数据集</li><li id="f001" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">并且我们将结束用MINST数据集解决深度学习分类项目的“Hello World”。</li></ul><h1 id="ed0a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">优化问题</h1><p id="a47a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">网飞决定在一栋大楼里放置他们的一张著名海报。营销团队已经决定，广告海报的面积必须达到600平方米，上下各2米，左右各4米。</p><p id="dbe3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">然而，他们没有被告知该建筑的立面尺寸。我们可以发一封电子邮件给业主，问他，但由于我们知道数学，我们可以很容易地解决它。我们怎样才能知道这座建筑物的尺寸？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1cddb0a13a9c87b68cbe2a4fb229742e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*mQtaCbmtkXURTdLUIG3wXw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="133d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">该建筑的总面积为:</p><p id="c786" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">宽度= 4 + x + 4 = x +8 </strong></p><p id="d191" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">身高= 2 + y + 2 = y +4 </strong></p><p id="6cfb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">面积=宽度x高度= (x + 8)*(y + 4) </strong></p><p id="4608" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">并且存在约束:<strong class="lt iu"> x*y = 600 </strong></p><p id="cd8d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这允许我们写出一个方程系统:</p><p id="f957" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">xy = 600 → <strong class="lt iu"> x = 600/y </strong></p><p id="9f3f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">S(y)</strong>=(600/y+8)(y+4)= 600+8y+4 * 600/y+32 =<strong class="lt iu">632+8y+2400/y</strong></p><p id="3053" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在优化问题中，函数(导数)的斜率信息用于计算其最小值。我们必须使一阶导数等于0，然后检查二阶导数是否为正。所以，在这种情况下:</p><p id="bce9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">S '(y)= 8–2400/y</strong></p><p id="61fc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> S''(y) = 4800/y </strong></p><p id="548a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">s '(y)= 0→0 = 8–2400/y→8 = 2400/y→y = 2400/8 = 300→<strong class="lt iu">y =</strong>sqrt(300)= sqrt(100–3)= sqrt(100)-sqrt(3)= 10-sqrt(3)=<strong class="lt iu">17.32</strong>(我们舍弃负号，因为它没有物理意义)</p><p id="7df0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在x中替换:</p><p id="38ec" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">x =</strong>600/10-sqrt(3)= 60/sqrt(3)= 60-sqrt(3)/sqrt(3)-sqrt(3)= 60-sqrt(3)/3 = 20-sqrt(3)=<strong class="lt iu">34.64</strong></p><p id="ca88" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">至于y = 17.32 -&gt; S''(y) = 0.9238 &gt; 0，我们找到了最小解。</p><p id="3c18" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">因此，建筑的尺寸为:</p><p id="dc29" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">宽度:x + 8 = 42.64米</strong></p><p id="15a6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">高度:y + 4 = 21.32米</strong></p><p id="7463" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">你看到衍生品有多有用了吗？我们只是分析性地解决了这个问题。我们已经能够解决它，因为这是一个简单的问题，但有许多问题，它是非常昂贵的计算来解决他们的分析，所以我们使用数值方法。这些方法之一是梯度下降。</p><p id="f575" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果我们这次用张量流数值解决这个问题，你觉得怎么样？我们走吧！</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8a48" class="nm la it ni b gy nn no l np nq">import numpy as np<br/>import tensorflow as tf</span><span id="54fe" class="nm la it ni b gy nr no l np nq">x = tf.Variable(initial_value=tf.random_uniform([1], 34, 35),name=’x’)<br/>y = tf.Variable(initial_value=tf.random_uniform([1], 0., 50.), name=’y’)</span><span id="6707" class="nm la it ni b gy nr no l np nq"># Loss function<br/>s = tf.add(tf.add(632.0, tf.multiply(8.0, y)), tf.divide(2400.0, y), ‘s’)</span><span id="3a77" class="nm la it ni b gy nr no l np nq">opt = tf.train.GradientDescentOptimizer(0.05)<br/>train = opt.minimize(s)</span><span id="ee86" class="nm la it ni b gy nr no l np nq">sess = tf.Session()</span><span id="c1ca" class="nm la it ni b gy nr no l np nq">init = tf.initialize_all_variables()<br/>sess.run(init)</span><span id="13f7" class="nm la it ni b gy nr no l np nq">old_solution = 0<br/>tolerance = 1e-4<br/>for step in range(500):<br/> sess.run(train)<br/> solution = sess.run(y)<br/> if np.abs(solution — old_solution) &lt; tolerance:<br/> print(“The solution is y = {}”.format(old_solution))<br/> break<br/> <br/> old_solution = solution<br/> if step % 10 == 0:<br/> print(step, “y = “ + str(old_solution), “s = “ + str(sess.run(s)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/da4a5f01585a7814011db3374a8bd21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*bbQCIRYMutI1wP013bSw4A.png"/></div></figure><p id="564e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们已经使用梯度下降算法计算出了y 。当然，我们现在需要计算<strong class="lt iu"> x </strong>代入x = 600/y</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7251" class="nm la it ni b gy nn no l np nq">x = 600/old_solution[0]<br/>print(x)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/e20b1f050183a120f731c5969e39e06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*pu6rVbxy-pHFR1zO59lsBA.png"/></div></figure><p id="fa72" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这与我们的结果相符，所以它似乎是有效的！让我们画出结果:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e754" class="nm la it ni b gy nn no l np nq">import matplotlib.pyplot as plt</span><span id="9193" class="nm la it ni b gy nr no l np nq">y = np.linspace(0, 400., 500)<br/>s = 632.0 + 8*y + 2400/y<br/>plt.plot(y, s)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/244cce78671d3d6f28be0437e16a5046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*YRqAvKGFuRtq_x_5_p7ZnA.png"/></div></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="67f3" class="nm la it ni b gy nn no l np nq">print("The function minimum is in {}".format(np.min(s)))<br/>min_s = np.min(s)<br/>s_min_idx = np.nonzero(s==min_s)<br/>y_min = y[s_min_idx]<br/>print("The y value that reaches the minimum is {}".format(y_min[0]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6b9887ac72557df08d1f358cc240bdf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*zx0JH-UHM0JbxEnBdTJIWA.png"/></div></figure><h1 id="bed5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">让我们看看其他例子</h1><p id="46ef" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这种情况下，我们希望找到y = log2(x)函数的最小值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="43ba" class="nm la it ni b gy nn no l np nq">x = tf.Variable(15, name='x', dtype=tf.float32)<br/>log_x = tf.log(x)<br/>log_x_squared = tf.square(log_x)</span><span id="c3a4" class="nm la it ni b gy nr no l np nq">optimizer = tf.train.GradientDescentOptimizer(0.5)<br/>train = optimizer.minimize(log_x_squared)</span><span id="ac81" class="nm la it ni b gy nr no l np nq">init = tf.initialize_all_variables()</span><span id="da75" class="nm la it ni b gy nr no l np nq">def optimize():<br/>  with tf.Session() as session:<br/>    session.run(init)<br/>    print("starting at", "x:", session.run(x), "log(x)^2:", session.run(log_x_squared))<br/>    for step in range(100):  <br/>      session.run(train)<br/>      print("step", step, "x:", session.run(x), "log(x)^2:", session.run(log_x_squared))<br/>      <br/>optimize()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/81c7f6bd10b094bc24dc657f00ca65af.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*9sF9svb_1eYoI6kozqF1uA.png"/></div></figure><p id="b10e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们来策划一下吧！</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="73aa" class="nm la it ni b gy nn no l np nq">x_values = np.linspace(0,10,100)<br/>fx = np.log(x_values)**2<br/>plt.plot(x_values, fx)</span><span id="db53" class="nm la it ni b gy nr no l np nq">print("The function minimum is in {}".format(np.min(fx)))<br/>min_fx = np.min(fx)<br/>fx_min_idx = np.nonzero(fx==min_fx)<br/>x_min_value = x_values[fx_min_idx]<br/>print("The y value that reaches the minimum is {}".format(x_min_value[0]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8bce9c8b28a10ab3245cd589bff4238e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*BtUhKIyzej4h3TBwqPTFjA.png"/></div></figure><h1 id="bdc5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">让我们解决一个线性回归问题</h1><p id="bcf4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们看看如何将一条直线调整到一个数据集，这个数据集代表了辛普森一家中每个角色的智力，从拉尔夫·威根到弗林克医生。</p><p id="2ed2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们绘制智力随年龄的分布图，从0到1归一化，其中玛吉最小，蒙哥马利·伯恩斯最大:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6cee" class="nm la it ni b gy nn no l np nq">n_observations = 50<br/>_, ax = plt.subplots(1, 1)<br/>xs = np.linspace(0., 1., n_observations)<br/>ys = 100 * np.sin(xs) + np.random.uniform(0., 50., n_observations)<br/>ax.scatter(xs, ys)<br/>plt.draw()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/48e33bd4cbd98e680560ba544f1bed42.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*84jJBuZZh-o2zt5IA2IBBg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="cc11" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在，我们需要两个tf .占位符，一个在回归算法的入口，另一个在出口。占位符是在执行网络之前不需要赋值的变量。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="0f64" class="nm la it ni b gy nn no l np nq">X = tf.placeholder(tf.float32)<br/>Y = tf.placeholder(tf.float32)</span></pre><p id="fa69" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们试着优化线性回归的直线。我们需要两个变量，权重(W)和偏差(b)。tf类型的元素。变量需要初始化，并且其类型在声明后不能更改。我们可以通过“赋值”方法改变它的值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4299" class="nm la it ni b gy nn no l np nq">W = tf.Variable(tf.random_normal([1]), name='weight')<br/>b = tf.Variable(tf.random_normal([1]), name='bias')<br/>Y_pred = tf.add(tf.multiply(X, W), b)</span></pre><p id="da3f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在让我们把成本函数定义为我们的预测值和真实值之间的差值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7156" class="nm la it ni b gy nn no l np nq">loss = tf.reduce_mean(tf.pow(Y_pred - y, 2))</span></pre><p id="fdb8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们现在定义优化方法，我们将使用梯度下降。基本上，它计算每个权重相对于总误差的变化，并更新每个权重，使得总误差在随后的迭代中减小。学习率表示权重更新的突然程度。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="3725" class="nm la it ni b gy nn no l np nq">learning_rate = 0.01<br/>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><span id="1d39" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># Definition of the number of iterations and start the initialization using the GPU</strong><br/>n_epochs = 1000</span><span id="841b" class="nm la it ni b gy nr no l np nq">with tf.Session() as sess:<br/>  with tf.device("/GPU:0"):</span><span id="20a8" class="nm la it ni b gy nr no l np nq"><strong class="ni iu">    # We initialize now all the defined variables</strong><br/>    sess.run(tf.global_variables_initializer())</span><span id="e045" class="nm la it ni b gy nr no l np nq">    <strong class="ni iu"># Start the adjust</strong><br/>    prev_training_loss = 0.0<br/>    for epoch_i in range(n_epochs):<br/>      for (x, y) in zip(xs, ys):<br/>        sess.run(optimizer, feed_dict={X: x, Y: y})</span><span id="de60" class="nm la it ni b gy nr no l np nq">      W_, b_, training_loss = sess.run([W, b, loss], feed_dict={X: xs, Y: ys})</span><span id="9e65" class="nm la it ni b gy nr no l np nq">      <strong class="ni iu"># We print the losses every 20 epochs</strong><br/>      if epoch_i % 20 == 0:<br/>        print(training_loss)</span><span id="ec45" class="nm la it ni b gy nr no l np nq">      <strong class="ni iu"># Ending conditions</strong><br/>      if np.abs(prev_training_loss - training_loss) &lt; 0.000001:<br/>        print(W_, b_)<br/>        break<br/>      prev_training_loss = training_loss</span><span id="b9a5" class="nm la it ni b gy nr no l np nq">    <strong class="ni iu"># Plot of the result</strong><br/>    plt.scatter(xs, ys)<br/>    plt.plot(xs, Y_pred.eval(feed_dict={X: xs}, session=sess))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c69ebc2f3e4297ad7a4cb2c5d61713e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*hRLAjb-1EcObiQyVgnHaYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="b023" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们有了！有了这条回归线，我们将能够预测每个辛普森的性格的智力知道年龄。</p><h1 id="13f1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">MNIST数据集</h1><p id="5037" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们看看如何用逻辑回归对数字图像进行分类。我们将使用深度学习数据集的“Hello world”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1dffd61a68d1e332c52fb45c7be3910e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*Uo6nTcVYdhiXR5hO.jpg"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/45505dc1bc059566d5d2452749b5adf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o7rRab5wYOfixC8Q.png"/></div></div></figure><p id="b8d0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们导入相关的库和数据集MNIST:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="cfff" class="nm la it ni b gy nn no l np nq">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>from tensorflow.examples.tutorials.mnist import input_data</span></pre><p id="4d52" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们通过用一键编码对标签进行编码来加载数据集(它将每个标签转换为长度= N_CLASSES的向量，除了指示图像所属的类的索引(包含1)之外，所有的0都是0)。比如我们有10个类(数字从0到9)，标签属于数字5:label =[0 0 0 1 0 0 0 0 0]。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="1e3f" class="nm la it ni b gy nn no l np nq">mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)</span><span id="bb47" class="nm la it ni b gy nr no l np nq">print("Train examples: {}".format(mnist.train.num_examples))<br/>print("Test examples: {}".format(mnist.test.num_examples))<br/>print("Validation examples: {}".format(mnist.validation.num_examples))</span><span id="ac57" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># Images are stored in a 2D tensor: images_number x image_pixels_vector<br/># Labels are stored in a 2D tensor: images_number x classes_number (one-hot)</strong><br/>print("Images Size train: {}".format(mnist.train.images.shape))<br/>print("Images Size train: {}".format(mnist.train.labels.shape))</span><span id="8b1d" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># To see the range of the images values</strong><br/>print("Min value: {}".format(np.min(mnist.train.images)))<br/>print("Max value: {}".format(np.max(mnist.train.images)))</span><span id="e9b5" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># To see some images we will acess a vector of the dataset and resize it to 28x28</strong><br/>plt.subplot(131)<br/>plt.imshow(np.reshape(mnist.train.images[0, :], (28, 28)), cmap='gray')<br/>plt.subplot(132)<br/>plt.imshow(np.reshape(mnist.train.images[27500, :], (28, 28)), cmap='gray')<br/>plt.subplot(133)<br/>plt.imshow(np.reshape(mnist.train.images[54999, :], (28, 28)), cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0b32cb91ce70abea900ab1e9f937d4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*WfEpQ6RYjf-Dfi9joaqbiA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0aa233c9e8ecff65065b27d26b355634.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*TIhpG148A9WbyVfnbYuxHA.png"/></div></figure><p id="c6cf" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们已经了解了MNIST数据集的一些内容。现在，让我们创建我们的回归变量:</p><p id="9f08" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">首先，我们为输入数据创建占位符。在这种情况下，输入将是一组大小为768的向量(我们将一次传递几个图像到我们的回归器，这样，当它计算梯度时，它将在几个图像中扫描，因此估计将比它只使用一个图像更精确)</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="65c2" class="nm la it ni b gy nn no l np nq">n_input = 784  <strong class="ni iu"># Number of data features: number of pixels of the image</strong><br/>n_output = 10  <strong class="ni iu"># Number of classes: from 0 to 9</strong><br/>net_input = tf.placeholder(tf.float32, [None, n_input])  <strong class="ni iu"># We create the placeholder</strong></span></pre><p id="f59c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在让我们定义回归方程:y = W*x + b</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c1c2" class="nm la it ni b gy nn no l np nq">W = tf.Variable(tf.zeros([n_input, n_output]))<br/>b = tf.Variable(tf.zeros([n_output]))</span></pre><p id="00e9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">由于输出是多类的，我们需要一个函数返回图像属于每个可能类的概率。例如，如果我们放一个5的图像，可能的输出是:[0.05 0.05 0.05 0.05 0.55 0.05 0.05 0.05 0.05 0.05]其概率之和是1，概率最高的类是5。</p><p id="7f5f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们应用softmax函数来归一化输出概率:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="782c" class="nm la it ni b gy nn no l np nq">net_output = tf.nn.softmax(tf.matmul(net_input, W) + b)</span></pre><h2 id="0100" class="nm la it bd lb oe of dn lf og oh dp lj ma oi oj ll me ok ol ln mi om on lp oo bi translated">SoftMax函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/bc5a25872391209f2d231f4c07061474.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/0*OIkhBe_xf_G0QZbp.png"/></div></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6b4a" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># We also need a placeholder for the image label, with which we will compare our prediction And finally, we define our loss function: cross entropy</strong><br/>y_true = tf.placeholder(tf.float32, [None, n_output])</span><span id="924f" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># We check if our prediction matches the label</strong><br/>cross_entropy = -tf.reduce_sum(y_true * tf.log(net_output))<br/>idx_prediction = tf.argmax(net_output, 1)<br/>idx_label = tf.argmax(y_true, 1)<br/>correct_prediction = tf.equal(idx_prediction, idx_label)</span><span id="ee32" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># We define our measure of accuracy as the number of hits in relation to the number of predicted samples</strong><br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))</span><span id="02e0" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># We now indicate that we want to minimize our loss function (the cross entropy) by using the gradient descent algorithm and with a rate of learning = 0.01.</strong><br/>optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</span></pre><p id="3ee5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在一切都设置好了！让我们执行图表:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9b30" class="nm la it ni b gy nn no l np nq">from IPython.display import clear_output</span><span id="890d" class="nm la it ni b gy nr no l np nq">with tf.Session() as sess:</span><span id="f19c" class="nm la it ni b gy nr no l np nq">  sess.run(tf.global_variables_initializer())</span><span id="6ff9" class="nm la it ni b gy nr no l np nq">  <strong class="ni iu"># Let's train the regressor</strong><br/>  batch_size = 10 <br/>  for sample_i in range(mnist.train.num_examples):<br/>    sample_x, sample_y = mnist.train.next_batch(batch_size)<br/>    sess.run(optimizer, feed_dict={net_input: sample_x, <br/>                                   y_true: sample_y})</span><span id="ff37" class="nm la it ni b gy nr no l np nq"><strong class="ni iu">    # Let's check how is performing the regressor</strong><br/>    if sample_i &lt; 50 or sample_i % 200 == 0:<br/>      val_acc = sess.run(accuracy, feed_dict={net_input: mnist.validation.images, y_true: mnist.validation.labels})<br/>      print("({}/{}) Acc: {}".format(sample_i, mnist.train.num_examples, val_acc))</span><span id="ce6a" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># Let's show the final accuracy</strong><br/>  print('Teste accuracy: ', sess.run(accuracy, feed_dict={net_input: mnist.test.images, y_true: mnist.test.labels}))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/da3fde8b91aa42fd6058519f3572cf34.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*QBnsX02mn9Xb_TqY9nfjXQ.png"/></div></figure><p id="5068" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们刚刚用TensorFlow训练了我们的第一个神经元网络！</p><p id="abde" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">想一想我们刚刚做了什么。</p><p id="7606" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们实现了一个逻辑回归，公式为:y = G(Wx + b)，其中G = softmax()，而不是典型的G = sigmoid()。</p><p id="d786" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果你看下图，它定义了感知器(单层神经网络)，你可以看到输出=激活_功能(Wx)。你看到了吗？只差偏了！注意输入是1？所以权重w0没有乘以任何东西。没错。权重w0是偏差，用这种符号表示只是为了能够将其实现为矩阵乘法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/ff44ccc5cf7607266079df8bb5dca329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99rdKJKfC_7AzQIC647K6A.png"/></div></div></figure><p id="2c6f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">所以，我们刚刚实现的是一个感知器</p><ul class=""><li id="5c10" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">批处理大小= 10</li><li id="addc" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">1个纪元</li><li id="aca7" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">下降梯度作为优化器</li><li id="8cc0" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">和softmax作为激活函数。</li></ul><h1 id="2f53" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的话</h1><p id="9a6b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一如既往，我希望你<strong class="lt iu"> </strong>喜欢这篇文章，你已经学会了如何使用TensorFlow来解决线性问题，并且你已经成功地训练了你的第一个神经网络！</p><p id="0588" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><em class="os">如果你喜欢这篇帖子，那么你可以看看我在数据科学和机器学习方面的其他帖子</em><a class="ae ky" href="https://medium.com/@rromanss23" rel="noopener"><em class="os"/></a><em class="os">。</em></p><p id="c845" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><em class="os">如果你想了解更多关于机器学习和人工智能的知识</em> <strong class="lt iu"> <em class="os">请在Medium </em> </strong> <em class="os">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>