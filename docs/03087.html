<html>
<head>
<title>K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-fa4df5990fff?source=collection_archive---------22-----------------------#2020-03-24">https://towardsdatascience.com/k-means-clustering-fa4df5990fff?source=collection_archive---------22-----------------------#2020-03-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ab68" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">概述</strong></h2></div><p id="25df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天机器学习的大多数应用都是基于监督学习的。然而，大部分数据是没有标签的:我们有输入<strong class="kh ir"> X </strong>，但没有标签<strong class="kh ir">y</strong>计算机科学家扬·勒库恩的名言</p><blockquote class="lb lc ld"><p id="502b" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">如果智能是一块蛋糕，无监督学习就是蛋糕，有监督学习就是糖衣，强化学习就是上面的樱桃。</p></blockquote><p id="0569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">换句话说，无监督学习有巨大的潜力。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="c536" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无监督学习的一种形式是将相似的实例分组到<em class="le">簇中。</em>聚类是数据分析、客户细分、推荐系统、搜索引擎、半监督学习、降维等等的一个很好的工具。</p><p id="f071" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你每天在一个公园里散步，你可能会无意中发现一棵你从未见过的树。如果你环顾四周，注意到更多的一些，它们并不完全相同，但看起来非常相似，你知道它们属于同一个属。你可能需要一个树木学家来告诉你这是哪一个物种，但是你不需要专家来给你的树分类。这就是<em class="le">集群</em>的本质:识别相似实例，并将它们分配到<em class="le">集群</em>或相似实例组中的任务。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="b131" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">k均值</h1><p id="bed5" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">最常用的聚类技术之一是K-means。K-means算法是由Stuart Lloyd和Bell Labs在1957年作为一种脉冲编码调制技术提出的，但直到1982年才在公司外部发表。Edward Forgy在1965年发表了几乎相同的算法，因此它被称为Lloyd-Forgy。以下内容生成了我们将进行聚类的数据:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="9a3b" class="mv lq iq mr b gy mw mx l my mz">from sklearn.datasets import make_blobs</span><span id="19d7" class="mv lq iq mr b gy na mx l my mz">blob_centers = np.array([[ 0.2,  2.3],[-1.5 ,  2.3],[-2.8,  1.8],[-2.8,  2.8],[-2.8,  1.3]])</span><span id="03ac" class="mv lq iq mr b gy na mx l my mz">blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])</span><span id="c5e1" class="mv lq iq mr b gy na mx l my mz">X, y = make_blobs(n_samples=2000, centers=blob_centers,<br/>cluster_std=blob_std, random_state=7)</span><span id="7608" class="mv lq iq mr b gy na mx l my mz">plt.figure(figsize=(8, 4))<br/>plot_clusters(X)<br/>save_fig("blobs_plot")<br/>plt.show()</span></pre><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e8b26ed79709d96b3be48ffff2acabdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*WNHiTayuhdF8WvnO4euykg.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">未标记的数据集</p></figure><p id="83db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">K-Means在聚类上面的数据时非常有效，通常只需要很少的迭代。它将试图找到每个集群的中心，并将每个实例分配给最近的集群。让我们训练一个K均值聚类器:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="df18" class="mv lq iq mr b gy mw mx l my mz">from sklearn.cluster import KMeans<br/>k = 5<br/>kmeans = KMeans(n_clusters = k)<br/>y_pred = kmeans.fit_predict(X)</span></pre><p id="8e3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个实例被分配到五个集群中的一个。它接收一个标签<em class="le">作为它被分配到的集群的索引。</em></p><p id="73d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到这些标签:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="5586" class="mv lq iq mr b gy mw mx l my mz">y_pred<br/>array([4, 0, 1, ..., 2, 1, 0], dtype=int32)</span><span id="a513" class="mv lq iq mr b gy na mx l my mz">y_pred is kmeans.labels_<br/>True</span></pre><p id="e999" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以看到算法找到的五个质心(聚类中心):</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="3290" class="mv lq iq mr b gy mw mx l my mz">kmeans.cluster_centers_<br/>array([[ 0.20876306,  2.25551336],<br/>       [-2.80580621,  1.80056812],<br/>       [-1.46046922,  2.30278886],<br/>       [-2.79290307,  2.79641063],<br/>       [-1.99213367,  1.3131094 ]])</span></pre><p id="5da7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于新数据，我们也可以很容易地做到这一点，通过查看任何新数据点最接近哪个质心。我们也可以可视化决策边界，每个质心用x表示。</p><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e4282601343ca4612b94009c1c8112b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*Ba-E4kC9ebksTuSGKFf24Q.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">k意味着判定边界和质心</p></figure><p id="96bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最初，随机放置质心，标记实例，并更新质心。这个过程反复进行，直到质心停止移动，并且已经定义了聚类。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="e214" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">加速K-均值算法</h1><p id="c208" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">加速K-Means是Sklearn的默认设置。它通过跟踪实例和质心之间的距离的下限和上限，大大加速了该算法。你可以强迫Sklearn使用原来的算法，尽管不太可能需要。</p><h1 id="16bf" class="lp lq iq bd lr ls nk lu lv lw nl ly lz jw nm jx mb jz nn ka md kc no kd mf mg bi translated">小批量K均值</h1><p id="e55b" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">代替在每次迭代中使用完整的数据集，该算法能够使用小批量，在每次迭代中稍微移动质心。这通常会将算法速度提高3到4倍。尤其重要的是，它使得对不适合内存的大型数据集进行聚类成为可能。一个限制是它的惯性通常稍差，尤其是当集群增加时，但是对于许多集群，使用小批量的速度要快得多。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="7efe" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">寻找最佳的聚类数</h1><p id="7e9a" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这个数据集中，我们可以清楚地看到有5个我们想要彼此分割的聚类。然而，情况并不总是如此，我们的数据通常没有像这样明显地分段。如果我们不采取预防措施来计算最佳集群数量，我们的结果可能会很差:</p><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/885da5cae8ba44fd73aa2e8bb8bed146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*DyurGjH_AAm3GWlbqehUyQ.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">K值过低</p></figure><p id="f9c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个想法是选择最小化惯性的k，但是我们不能这样做，因为惯性总是随着k的增大而减小。事实上，集群越多，每个实例越接近其最近的质心，因此惯性越低。正如我们在k=8的情况下所看到的，我们正在毫无理由地分割集群。</p><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f40f73df01747e624eb10626a52df46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*0L49AoQiNGX_E2kgz_-lZQ.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">惯性作为k的函数</p></figure><p id="a48e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">惯性很快下降到5，但之后下降很慢。任何低于5的k值，增益都是巨大的，任何高于5的k值，我们都不会得到更多的信息。这是一种相当粗糙的，主观的分配k的方法，但是它通常工作得很好。这样做时，我们也可以考虑业务问题的特定需求。</p><p id="1c20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在这种情况下，我们可以看到有5个集群我们想要细分。4个集群可能足够了，但是我们应该研究k=4和k=5之间的差异</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="0b38" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">剪影分数</h1><p id="2480" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">另一种更精确的方法是对每个实例使用轮廓分数，并对不同数量的聚类绘制它们，然而这种方法计算量更大，它将给出更清晰的最佳k:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="1443" class="mv lq iq mr b gy mw mx l my mz">from sklearn.metrics import silhouette_score</span><span id="6af1" class="mv lq iq mr b gy na mx l my mz">silhouette_score(X, kmeans.labels_)</span><span id="87cf" class="mv lq iq mr b gy na mx l my mz">silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]</span></pre><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/621fac35a5005266f2cd4f803df9504c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*SzQNBO1pIqumDDzbOEpTWw.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">聚类数和轮廓分数</p></figure><p id="fc13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，使用k的每个级别的轮廓分数，哪一个k是最优的就更加明显了。</p><p id="76e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更有用的是绘制每个实例的轮廓系数，按它们所在的聚类和系数的值排序。这是一张剪影图。形状的高度表示聚类包含的实例数量，其宽度表示聚类中实例的排序轮廓系数(越宽越好)。虚线表示平均轮廓系数。</p><figure class="mm mn mo mp gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi ns"><img src="../Images/edb45221302efeeb2fd9ce43264fdea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yVZ48l-E1L_A-tnXV5I8_g.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">分析各种K的轮廓图</p></figure><p id="4242" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">垂直虚线表示每个聚类数的轮廓分数。如果许多实例在虚线的短边(左边)停止，那么这个集群就相当糟糕，因为这意味着这些实例彼此靠得太近。在k=3和k=6时，我们得到的是坏簇，但在k=4和k=5时，它们相当好。大多数情况下会超出虚线。然而，当k=4时，橙色集群相当大，而在k=5时，它们都是相似的大小，所以看起来应该使用5来获得相似大小的集群。</p><p id="563b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">执行这一过程显示了肘方法的明显局限性，定期执行它可以增强您选择k的稳健性。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="3cf9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">K-意味着限制</strong></h1><p id="8e4c" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">作为一名优秀的数据科学家，了解你所使用的方法背后的假设是很重要的，这样你才能对它们的优缺点有所了解。这将帮助您决定何时以及在什么情况下使用每种方法:</p><ul class=""><li id="5cf3" class="nx ny iq kh b ki kj kl km ko nz ks oa kw ob la oc od oe of bi translated">我们经常会得到次优解和局部极小值(由于质心的随机初始化)，因此需要多次运行算法来避免这种情况。</li><li id="888c" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">除此之外，我们还需要指定k，这可能是主观的，有时相当麻烦，取决于数据集。</li><li id="a4a5" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">最大的问题是，当聚类具有不同的大小、密度或非球形形状时，K-Means的性能不是特别好。根据数据的不同，另一种聚类算法可能会更好。(通常，在执行K-Means之前缩放输入要素会有所帮助，但这并不能保证所有聚类都是完美的球形)</li><li id="bd89" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">K-Means也给予较大的聚类比较小的聚类更多的权重。换句话说，较小聚类中的数据点可以远离质心，以便更多地集中在较大的聚类上。这可能仅仅因为不平衡的数据而导致较小的集群分配不当。</li><li id="54e9" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">最后，因为K-Means将每个实例分配到一个非重叠的聚类中，所以对于位于边界线附近的点没有不确定性的度量</li></ul><p id="a5e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我讨论了最著名的聚类算法之一——K-Means。我们研究了在使用K-Means时可能面临的挑战。</p><p id="43a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们实现了k-means并查看了肘形曲线，这有助于在K-Means算法中找到最佳的聚类数，同时也展示了它的局限性。</p><p id="2d4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有任何疑问或反馈，欢迎在下面的评论区分享</p><p id="c2ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><p id="da22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Aurelien Geron (2019) <em class="le">用Scikit-Learn、Keras和TensorFlow进行动手机器学习:构建智能系统的概念、工具和技术图书</em>，第2版。，:奥赖利。</p><p id="525c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有可视化的代码都可以在我的GitHub <a class="ae ol" href="https://github.com/Carterbouley/kmeans" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></div></div>    
</body>
</html>