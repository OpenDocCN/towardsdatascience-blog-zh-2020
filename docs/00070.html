<html>
<head>
<title>An Introduction to Random Forest with Python and scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 和 scikit 介绍随机森林-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-random-forest-with-python-and-scikit-learn-acf44e514034?source=collection_archive---------41-----------------------#2020-01-02">https://towardsdatascience.com/an-introduction-to-random-forest-with-python-and-scikit-learn-acf44e514034?source=collection_archive---------41-----------------------#2020-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9f1a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">获得对随机森林的直观理解和数学理解的完整指南，使用 scikit 实现您的第一个模型——学习 Python</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1aab68707d058033c0c993d8465ca6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSPhw_Im0z3ur1Oi7IaEHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">随机森林可视化与 50 个不同的决策树</p></figure><blockquote class="kv kw kx"><p id="384e" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">注意:这篇文章假设你对决策树有基本的了解。如果您需要更新决策树的工作方式，我建议您首先阅读<a class="ae lv" rel="noopener" target="_blank" href="/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f">Python 和 scikit 决策树介绍-学习</a>。</p></blockquote><p id="7676" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">关于随机森林的好处是，如果我们很好地理解了决策树，那么理解随机森林应该也很容易。随机森林这个名字实际上很好地描述了添加的额外功能。首先，我们现在有了<em class="la">随机</em>，我将更深入地解释它。其次，提醒你自己一个森林是由什么组成的，也就是一堆树，所以我们基本上有一堆决策树，它们被称为一个森林。非常直观地将这两个术语联系起来，实际上只有森林是随机的，因为它由一堆基于随机数据样本的决策树组成。</p><h1 id="91b0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">了解随机森林</h1><p id="0477" class="pw-post-body-paragraph ky kz iq lb b lc mr jr le lf ms ju lh lw mt lk ll lx mu lo lp ly mv ls lt lu ij bi translated">为了理解随机森林，我们实际上只需要理解什么是自举，或者换句话说，置换随机抽样。对于每个单独的决策树，我们随机选择给定次数的随机观察(通常对应于观察的总数)。唯一微小的细节是，相同的观察可能会出现多次(否则，我们基本上只会以随机顺序获得每个决策树的相同数据，这将导致每个树的结果完全相同)。</p><p id="ea66" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">如果你还记得我在<a class="ae lv" rel="noopener" target="_blank" href="/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f">决策树</a>上发布的代码(如果不记得，更新后的代码会在下面发布)，下面这一行实际上是我们唯一需要添加的内容:</p><pre class="kg kh ki kj gt mw mx my mz aw na bi"><span id="482a" class="nb ma iq mx b gy nc nd l ne nf">bootstrap = data[np.random.randint(0, rows-1, rows)].reshape(rows, columns)</span></pre><p id="9def" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">这行代码所做的，基本上是从完整的数据集中随机地获取一些观察值，也就是带有替换的自举/采样。此外，我们还必须为算法添加一个新的外部循环，它对应于我们想要生成的决策树的数量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><blockquote class="kv kw kx"><p id="cd3d" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">注意:我在外部树循环(第 14 行)之后的第一行中添加了一行选择随机特征，该行随机选择与特征数量的平方根相对应的多个随机特征。为了简单起见，在之前的文章中 GitHub Gist 没有添加这一点。我在这里添加了它，因为它是 sklearn 中的默认设置，它使代码运行得更快，因为我的算法根本没有优化，已经运行得很慢了。</p></blockquote><pre class="kg kh ki kj gt mw mx my mz aw na bi"><span id="7ffe" class="nb ma iq mx b gy nc nd l ne nf"><strong class="mx ir">Out [1]:</strong></span><span id="ace7" class="nb ma iq mx b gy ni nd l ne nf">TREE 1: Best split-feature     : Fare<br/>TREE 1: Value to split on      : 52.5542<br/>TREE 1: Weighted gini impurity : 0.42814<br/><br/>TREE 2: Best split-feature     : Sex<br/>TREE 2: Value to split on      : 1.0<br/>TREE 2: Weighted gini impurity : 0.32462<br/><br/>TREE 3: Best split-feature     : Parents/Children Aboard<br/>TREE 3: Value to split on      : 0.0<br/>TREE 3: Weighted gini impurity : 0.45518</span></pre><p id="9c8b" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">因此，这将是林中每个决策树的第一次拆分。实际上，我们不应该只给每棵树做一个裂口，而是让它们完全生长出来。类似地，如果你用 3 棵完全生长的不同的树建立一个随机的森林，你会得到这样的结果:</p><div class="kg kh ki kj gt ab cb"><figure class="nj kk nk nl nm nn no paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/68bdd916f4b3a1ed4d82677e01f567c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*glTVHsmVSIZkrleZ7uOk-w.png"/></div></figure><figure class="nj kk np nl nm nn no paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/dfcf7410405cf33da387655667de942d.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*3Ggh6hHp5G5je4HaUqXVKQ.png"/></div></figure><figure class="nj kk nq nl nm nn no paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a7c3261c9ea3153ef4e7fa0644eb2007.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*oSi0ck2tfghm06vzb0XNIw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nr di ns nt translated">基于自举数据的由 3 棵决策树组成的随机森林</p></figure></div><blockquote class="kv kw kx"><p id="375d" class="ky kz la lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">注意:随机森林中的三个决策树不会在相同的初始注释上分裂，因为您必须控制几个随机因素才能获得完全相同的结果，这将导致更多的代码。</p></blockquote><h1 id="09c3" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用随机森林进行预测</h1><p id="22ea" class="pw-post-body-paragraph ky kz iq lb b lc mr jr le lf ms ju lh lw mt lk ll lx mu lo lp ly mv ls lt lu ij bi translated">现在到了随机森林的简单部分，来做预测。我们要做的一切，实际上只是在每棵树上输入一个观察值，看看它能预测什么。然后，我们采用大多数树选择的预测，例如，在上图中，如果三棵树中的两棵树预测有乘客幸存，那么这也将是我们的最终预测。</p><p id="2a49" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">为什么这是个好主意呢？决策树往往会过度拟合数据，但是通过利用大量的单独决策树来预测最终结果，我们至少可以在某种程度上防止这种过度拟合。</p><p id="7045" class="pw-post-body-paragraph ky kz iq lb b lc ld jr le lf lg ju lh lw lj lk ll lx ln lo lp ly lr ls lt lu ij bi translated">希望这篇文章对你更深入的了解随机森林，理解算法的工作原理有所帮助。请随意留下您的评论或问题。</p></div></div>    
</body>
</html>