<html>
<head>
<title>Unsupervised NER using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BERT 的无监督 NER</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a?source=collection_archive---------2-----------------------#2020-02-28">https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a?source=collection_archive---------2-----------------------#2020-02-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6e869cd7996f2651d097cd9c416a9347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r24Ho5-ziFCDsmU_H3AeKA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图一。</strong>图示了使用 BERT (bert-large-cased)执行的无监督 NER 的标记句子样本，没有微调。这些例子突出了用这种方法标记的几个实体类型。标记 500 个句子产生了大约 1000 种独特的实体类型——其中一些被映射到上面显示的合成标签上。Bert-large-case 模型无法区分基因和蛋白质，因为这些实体的描述符落在屏蔽术语的预测分布的同一尾部(它们在基本词汇中也是不可区分的)。区分这些密切相关的实体可能需要 MLM 对特定领域的语料库进行微调，或者使用定制的词汇从头开始预先训练模型(下面将进行讨论)</p></figure><h1 id="7802" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">TL；博士</strong></h1><blockquote class="le lf lg"><p id="d524" class="lh li lj lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">【2022 年 1 月  <strong class="lk iu">发布的这种方法的改进版本</strong> <a class="ae mg" rel="noopener" target="_blank" href="/ssl-could-avoid-supervised-learning-fd049a27cd1b"> <strong class="lk iu">描述了如何将其扩展到大量的实体类型(例如，跨越生物学和 PHI 实体领域的 68 种实体类型，如人、位置、组织)。</strong></a></p></blockquote><p id="5101" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在自然语言处理中，识别句子中感兴趣的实体，如人、地点、组织等。需要带标签的数据。我们需要用感兴趣的实体来标记句子，其中每个句子的标记都是手动或通过某种自动方法来完成的<em class="lj">(通常使用试探法来创建有噪声/弱标记的数据集)</em>。然后，这些标记的句子被用来训练一个模型，以识别这些实体作为监督学习任务。</p><p id="b92c" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">这篇文章描述了一种实现无监督 NER 的方法。NER 是在没有标记句子的情况下使用 BERT 模型在无监督的情况下完成的，该模型仅在具有掩蔽语言模型目标的语料库上被无监督地训练。</p><p id="0ab9" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">该模型在 25 个实体类型<em class="lj">(维基文本语料库)</em>的小数据集上具有 97%的 F1 分数，在 CoNLL-2003 语料库上对于人和地点具有 86%的 F1 分数。在 CoNLL-2003 语料库上，它对于人、地点和组织具有较低的 F1 分数 76%，这主要是因为句子中固有的实体歧义<em class="lj">(在下面的评估部分中检查)</em>。这两项测试都是在没有对模型在<em class="lj">上进行测试的数据进行任何预训练/微调的情况下进行的(与对特定领域数据进行预训练/微调模型或使用标记数据进行监督训练形成鲜明对比)。</em></p><h1 id="1357" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak"> <em class="mk">这是怎么回事？</em>T25】</strong></h1><p id="9b66" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">如果我们被问到一个术语的实体类型<em class="lj"> ( </em> <strong class="lk iu"> <em class="lj">术语</em> </strong> <em class="lj">既指</em> <strong class="lk iu"> <em class="lj">单词</em> </strong> <em class="lj">又指</em> <strong class="lk iu"> <em class="lj">短语</em></strong><em class="lj"/>我们从未见过，我们可以通过术语的发音和/或术语出现的句子结构来猜测。也就是说，</p><ul class=""><li id="6793" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">一个术语的子词结构提供了其实体类型的线索。</li></ul><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="af15" class="ni kh it ne b gy nj nk l nl nm"><em class="lj">Nonenbury is a _____</em></span></pre><ul class=""><li id="9fb5" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">这是一个虚构的城市名称，但我们可以猜测它可能是一个后缀为“bury”的地点。在这里，术语后缀给了我们一个线索，即使我们从句子上下文中没有任何其他关于实体类型的线索。</li><li id="42c2" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">句子结构提供了术语实体类型的线索。</li></ul><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="83cf" class="ni kh it ne b gy nj nk l nl nm"><em class="lj">He flew from _____ to Chester</em></span></pre><p id="3211" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">这里，句子的上下文给了我们一个线索，即未知项是一个位置。即使在<em class="lj">(例如 Nonenbury) </em>之前没有见过，我们也可以猜测句子中空白位置的任何术语都可能是位置。</p><p id="0c93" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">伯特的掩蔽语言模型 head(<em class="lj">MLM——图 7 </em>)可以预测上述掩蔽位置的候选单词，给定之前描述的训练目标——它通过预测句子中被删除的单词来学习。然后，在推断过程中使用这种学习，以输出对句子中被屏蔽的术语的预测，其中该预测是对 BERT 的固定词汇的概率分布。这个输出分布有一个明显但很小的尾部<em class="lj"> ( &lt; ~总质量的 0.1)</em>，其中驻留了捕获术语的上下文敏感实体类型的候选单词。这个尾部充当使用 BERT 词汇表表示的术语的<em class="lj">上下文敏感签名。例如，句子中屏蔽位置的上下文敏感签名如下所示</em></p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="cdf9" class="ni kh it ne b gy nj nk l nl nm"><em class="lj">Nonenbury is a _____</em></span><span id="f688" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">Context Sensitive (CS) Predictions: </strong>village hamlet town settlement parish Village Hamlet farm place Town</span></pre><p id="4717" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">BERT 的固定词汇<em class="lj">(BERT-large-cased 为 28，996)</em>用作一组通用描述符<em class="lj">(例如专有名词、普通名词、代词等。).</em>这些描述符的子集<em class="lj">(可能是重叠的)</em>由下面描述的聚类过程获得，独立于其句子上下文来表征术语的实体类型。这些子集是<em class="lj">术语的上下文无关签名。</em>BERT 词汇表中捕获接近上述上下文敏感签名的实体类型的上下文无关子集是</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b495" class="ni kh it ne b gy nj nk l nl nm">['villages', 'towns', 'village', 'settlements', 'villagers', 'communities', 'cities']<br/>['Village', 'village']<br/>['city', 'town', 'City', 'cities', 'village']<br/>['community', 'communities', 'Community']<br/>['settlement', 'settlements', 'Settlement']<br/>['Township', 'townships', 'township']<br/>['parish', 'Parish', 'parishes']<br/>['neighborhood', 'neighbourhood', 'neighborhoods']<br/>['castle', 'castles', 'Castle', 'fortress', 'palace']<br/>['Town', 'town']</span></pre><p id="ad4e" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在上下文敏感签名/聚类和上下文无关签名/聚类之间的 BERT 词汇表的嵌入空间中实现的最接近匹配函数<em class="lj">(基于单词嵌入的余弦相似度)</em>产生表示术语的 NER 标签的上下文无关签名的子集。具体来说，<strong class="lk iu"> m </strong>条款<em class="lj"> {B1，B2，C3，…。Bm} </em>构成上下文敏感签名，n 组术语<em class="lj"> {{C11，C12，C13，…Ck1} </em>，<em class="lj"> {C21，C22，C23，…Ck2}，… {Cn1,Cn₂,Cn₃,…Ckn}} </em>构成上下文无关签名，产生表示术语<em class="lj">(下面图 2)</em>的 NER 标签的上下文无关签名的子集。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/edc175592dd815afdf1a2c81ca376805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tgBGjI4zo6cvf3A8oVKB4w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图二。</strong> NER 标注了一句话。(1)在最小预处理之后，将带有屏蔽词的输入句子输入到模型中。(2)对 BERT 的 28，996 个词汇的前 10 个预测。(3)这 10 项在伯特的词嵌入空间中被一个函数重新排序。(4)来自重新排序的术语的前 k 个术语以及 6000 个聚类(离线计算)作为输入被馈送到输出匹配聚类的聚类匹配函数。这些聚类的标注(可以是用户定义的标注-一次性手动步骤，或者在某些用例中按原样使用)随后被聚合以输出 NER 标注。图中执行 3、4 和 5 的函数都是在 BERT 的嵌入空间中使用字向量之间的余弦相似性来完成的。约 6000 个聚类的一次性离线生成也是使用 BERT 的单词嵌入空间中单词向量之间的余弦相似度来完成的。图中显示了隐藏大小为 768 的 BERT 基本模型。帖子中的 NER 例子是用隐藏尺寸为 1024 的大号伯特箱完成的</p></figure><p id="5fb7" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">给定我们可以从 BERT 的词汇表中自动收获的上下文无关签名的数量为数以千计(<em class="lj"> ~6000，对于 bert-large-cased </em> ) —这种方法允许我们在细粒度级别上对大量实体类型执行无监督的实体识别，而不需要标记数据。</p><p id="d110" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">上面描述的无监督 NER 方法很大程度上是因为，正如本文 中的<a class="ae mg" rel="noopener" target="_blank" href="/examining-berts-raw-embeddings-fd905cb22df7"> <em class="lj">所考察的</em></a></p><ul class=""><li id="4668" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">BERT 的原始单词嵌入捕捉有用的和可分离的信息<em class="lj">(明显的直方图尾部，小于 0.1 %的词汇)</em>关于使用 BERT 词汇中其他单词的术语。这用于生成 6000+个集群</li><li id="3b57" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">由具有 MLM 头的 BERT 模型输出的这些嵌入的变换版本被用于预测屏蔽字。预测<a class="ae mg" rel="noopener" target="_blank" href="/examining-berts-raw-embeddings-fd905cb22df7">也有一个明显的尾巴</a>。这用于选择术语的上下文敏感签名。</li></ul><h1 id="b870" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">执行无监督 NER 的步骤</h1><h2 id="a3c2" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">一次性离线处理</h2><p id="7635" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">一次离线处理从从 BERT 词汇表中获取的上下文无关签名集合创建到单个描述符/标签的映射。</p><h2 id="da6b" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第一步。过滤 BERT 的词汇以挑选上下文敏感的签名术语</h2><p id="2952" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">伯特的词汇是普通名词、专有名词、子词和符号的混合体。这个集合的最小过滤是去除标点符号、单个字符、子词和 BERT 的特殊符号。得到的 21，418 个术语的集合—普通名词和专有名词的混合用作描述实体类型的描述符。</p><h2 id="ac31" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第二步。从 BERT 的词汇表中生成上下文无关的签名</h2><p id="9e38" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">如果我们简单地从尾部为 bert 词汇表中的每个术语挑选上下文无关的签名，我们将获得相当大量的聚类<em class="lj"> (~20，000) </em>，即使具有 0.5 的高余弦相似性阈值<em class="lj">(对于 BERT 大型模型，对于高于 0.5 的余弦阈值的术语，平均约有 0.1%的术语位于尾部)</em>。这不仅是大量的聚类，它也不能捕获这些签名之间的相似性。所以，相反我们</p><ul class=""><li id="fbd6" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">遍历 BERT 词汇表中的所有术语<em class="lj">(忽略子词和大多数单个字符；补充说明有更多细节)</em>，并为 0.5 以上的每一项挑选上下文无关的签名。将单词尾部的术语视为一个完整的图，其中边的强度是余弦相似值。</li><li id="08e6" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">选择与图中所有其他节点具有最大连接强度的节点。</li><li id="ae90" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">该节点充当由这些节点组成的上下文无关签名的中枢。此节点是此图中所有其他节点的最近邻居。</li></ul><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/9d8c7e9dfd5ec902d1261ca5b8d7657c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kp-SBVjtCSzVtS6QBY8zlA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图三。在一个完整的图中寻找枢轴节点</strong>。在上面的完整图形中，节点“平滑地”具有到其邻居的最大平均连接强度。所以“平滑地”是这个图<em class="mk">的中枢节点——这个图中所有其他节点的最近邻居。</em></p></figure><ul class=""><li id="0c8a" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">一旦一个术语被选作签名的一部分，它将不会被考虑用于 pivot 候选评估<em class="lj">(但是，如果另一个术语的 pivot 节点计算使其成为 pivot，则它可以间接成为 pivot) </em>。本质上，一个术语可以是多个集合的一个元素，一个中枢或间接中枢。</li></ul><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="ca64" class="ni kh it ne b gy nj nk l nl nm">airport 0.6 0.1 Airport airport airports Airfield airfield<br/>stroking 0.59 0.07 stroking stroked caressed rubbing brushing caress<br/>Journalism 0.58 0.09 Journalism journalism Journalists Photography   <br/>                     journalists<br/>smoothly 0.52 0.01 smoothly neatly efficiently swiftly calmly<br/></span></pre><p id="1ac4" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在上面的示例签名中，两个数值是子图边强度的平均值和标准偏差。第一列术语用作代表该签名的枢纽术语。这些术语充当实体标签代理。这些可以手动映射<em class="lj">(一次性操作)</em>到用户定义的标签。在下面的图 4a 和 4b 中示出了映射这些实体集群的例子。我们只需要映射那些代表与我们的特定应用程序相关的实体类型的集合。其余的可以通过编程映射到一个合成标签“other/misc”。参考部分描述了一种使用模型本身来引导/加速描述符到用户定义标签的手动映射的方法。</p><p id="9a12" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">因为伯特的词汇中大约 30%是专有名词<em class="lj">(人名、地名等)。)</em>尽管我们只是标注了一小部分术语<em class="lj">(约 2000 个人工标注集群，耗时约 5 个工时，如图 4 和 4b 所示)</em>而不是大量的句子，但看起来我们似乎在作弊。将句子的标注问题转化为标注上下文不敏感描述符的一个关键优点是，这是一个一次性的过程。这与监督训练方法形成对比，在监督训练方法中，我们必须不可避免地创建更多标记数据，不仅要训练我们的模型，还要根据我们在训练<em class="lj">(通常在部署中)</em>之后发现的不分布的句子对其进行再训练。然而，在我们的情况下，更糟糕的情况是，我们必须在没有监督的情况下对这些新句子重新训练/微调我们的 BERT 模型——我们不必再做任何标记。</p><p id="a37a" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">上面描述的上下文不敏感签名生成产生大约 6000 个集合，平均基数大约为 4 +/- 7 个节点。这 6000 组数据的聚类强度平均值为 0.59，偏差为 0.007，这些聚类非常紧密，因为平均值远远高于我们从分布<em class="lj">(图 4c) </em>中选择的阈值。大约 5000 个词汇术语<em class="lj"> (17 %的词汇)</em>是单体集合，被忽略。如果选择集合的阈值改变，这些数字也会改变。例如，阈值 0.4 会将总尾部质量增加到 0.2%，并且还会增加聚类平均值<em class="lj">(但是聚类会因实体类型混合而变得嘈杂)。</em></p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/f5b1a56c63fd1cdb190bf682593021e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxqN1ZFij5B4gAPAwcE-oQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 4 </strong>。BERT 的上下文无关签名集统计信息(bert-large-cased)。平均基数约为 4，标准偏差为 7。这 6110 组的聚类强度的平均值为 0.59，偏差为 0.007，这些聚类是非常紧密的聚类，因为平均值远远高于我们从分布中选择的阈值。我们将看到，相比之下，上下文敏感术语通常是弱聚类。大约 17%的 BERT 词汇是单态的。聚类不考虑子词、特殊标记和大多数单字符标记。</p></figure><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/60035b379da6521ed9d5f2d037d1dd1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RcBIctJHgUlp9MBS9cXeGg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 4a。</strong>BERT(BERT-large-cased)词汇中簇的实体分布。大部分实体集中在人员、位置和组织中。AMB 指的是在给定簇中的术语的情况下不明确的簇。例如，如下图 4b 所示，7 个聚类在人和位置之间具有模糊性。其他人在人、物、运动/生物等方面有歧义。当我们想要发现特定领域的实体类型时，使用自定义词汇表将是有价值的——这可能会消除对个人、位置和组织的偏见。</p></figure><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/e62f7dd348dbc20a8635dd43e89101fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VrBlgJdNaRpwwrbKDbXo0g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 4b。</strong>BERT 词汇中的实体子类型分布。这些是图 4a 中主要类型的细粒度实体子类型。</p></figure><p id="d038" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">从 BERT 词汇表中获得的上下文无关聚类的例子</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/38f34d3d6083bbadd4f06f11accb267c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPoPjIa3ydNBROk2wGR6vg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 4c。</strong>来自 BERT 词汇表的聚类示例(bert-large-cased)。不明确的上下文不敏感聚类被标记为 AMB。当子词被考虑用于聚类时，附加注释显示子词聚类(尽管对于本文中的实体识别结果，它们被过滤掉了)</p></figure><h2 id="81f2" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">每个输入句子的实体预测</h2><p id="0ce7" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">执行这些步骤来标记输入句子中的术语。</p><h2 id="1349" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第三步。最小预处理输入句子</h2><p id="4689" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">给定一个输入句子来标记实体，对输入进行非常简单的预处理。其中之一是大小写规范化——全大写的句子<em class="lj">(通常出现在文档标题中)</em>被转换为小写，同时保留每个单词中第一个字母的大小写。这有助于提高下一步检测短语跨度的准确性。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="27ba" class="ni kh it ne b gy nj nk l nl nm">He flew from New York to SFO<br/>          <em class="lj">becomes</em><br/>He flew from New York to Sfo</span></pre><h2 id="17ff" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第四步。识别句子中的短语跨度</h2><p id="8ed8" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">一个 POS tagger <em class="lj">(经过理想训练也能处理所有小写单词句子)</em>用于标记输入句子。这些标签用于识别短语以及大写每个名词短语的第一个字母。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="a887" class="ni kh it ne b gy nj nk l nl nm">He flew from <strong class="ne iu">New York</strong> to <strong class="ne iu">Sfo</strong></span></pre><p id="841f" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">标记为名词形式的术语在上面用粗体表示。BERT 的屏蔽词预测对大小写非常敏感，因此使用一个好的词类标记器来可靠地标记名词形式，即使只是小写也是标记性能的关键。例如，下面句子的屏蔽预测通过改变句子中一个字母的大小写来改变实体意义</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="087d" class="ni kh it ne b gy nj nk l nl nm">Elon <strong class="ne iu">M</strong>usk is a ____<br/><strong class="ne iu">CS Predictions: </strong>politician musician writer son student businessman biologist lawyer painter member</span><span id="bbe3" class="ni kh it ne b gy ns nk l nl nm">Elon <strong class="ne iu">m</strong>usk is a ____<br/><strong class="ne iu">CS Predictions: </strong>brand Japanese beer German common Turkish popular French Russian Brazilian</span></pre><p id="3396" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated"><em class="lj">顺便说一句，BERT 的屏蔽预测仅对于检测实体类型(上面第一个例子中的</em> <strong class="lk iu"> <em class="lj">人</em> </strong> <em class="lj">)是可靠的，对于事实上准确的预测是不可靠的，尽管事实上 BERT 可能偶尔做出事实上准确的预测。</em></p><h2 id="ebce" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第五步。使用伯特的 MLM 头来预测每个屏蔽位置</h2><p id="ec94" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">对于句子中的每个名词术语，生成一个包含该术语的句子。使用伯特的 MLM 头来预测屏蔽位置的上下文敏感签名。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="9127" class="ni kh it ne b gy nj nk l nl nm">He flew from __ to Sfo<br/><strong class="ne iu">CS Predictions:</strong> Rome there Athens Paris London Italy Cairo here Naples Egypt</span><span id="92dd" class="ni kh it ne b gy ns nk l nl nm">He flew from New York to ___<br/><strong class="ne iu">CS Predictions:</strong> London Paris Singapore Moscow Japan Tokyo Chicago Boston France Houston</span></pre><p id="82dc" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">找到这些节点相对于集合中其他节点的强度，就像我们在图 2 中找到枢轴节点一样。按实力排序。这给出了单词嵌入的嵌入空间中的 CS 预测的重新排序列表。重新排序将共享实体意义的术语集合在一起，尽管重新排序是在独立于上下文的单词嵌入的嵌入空间中进行的。例如，下面第一个例子中的重新排序将术语“there”和“here”<em class="lj">(空白位置的有效上下文敏感预测)</em>推到最末尾。我们将在下一步中挑选这些重新排序的节点的顶部 k (k ≥ 1)。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="2c36" class="ni kh it ne b gy nj nk l nl nm">He flew from __ to Sfo<br/><strong class="ne iu">CS Predictions:</strong> Rome there Athens Paris London Italy Cairo here Naples Egypt<br/><strong class="ne iu">CI space ordering of CS predictions</strong>: Rome Paris Athens Naples Italy Cairo Egypt London there here</span><span id="f00d" class="ni kh it ne b gy ns nk l nl nm">He flew from New York to __<br/><strong class="ne iu">CS Predictions:</strong> London Paris Singapore Moscow Japan Tokyo Chicago Boston France Houston<br/><strong class="ne iu">CI space ordering of CS predictions</strong>: Paris London Tokyo Chicago Moscow Japan Boston France Houston Singapore</span></pre><h2 id="3255" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">第六步。查找上下文相关签名和上下文无关签名之间的紧密匹配</h2><p id="b124" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">产生合理结果的一个简单的紧密匹配函数是，从前一项中仅选取上下文敏感签名的一个中枢节点，并与上下文无关签名集中的所有 6000+中枢做该项的点积，然后对它们进行排序以获得实体标签候选。在这种情况下，紧密匹配功能实质上是寻找与上下文敏感聚类中枢最近的上下文不敏感聚类中枢。除了顶部支点，我们还可以采用顶部 k 个支点来提高标记/预测的可信度<em class="lj">(图 5) </em>。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/361f17322f7fa5e99c7bab2539d010ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bG9P_sc07kUXLU38htnCpQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 5 </strong>。单词嵌入空间中上下文相关签名和上下文无关签名的紧密匹配。(A)紧密匹配的简单实现是上下文敏感签名的中枢节点和上下文无关签名中集合的中枢之间的点积。<strong class="bd kf">在这种情况下，紧密匹配功能实质上是寻找与上下文敏感聚类中枢</strong>最近的上下文不敏感聚类中枢。更好的实现是基于上下文敏感签名中的节点的平均值和标准偏差来决定被认为是中枢的节点的数量，然后选择要在二分图中考虑的中枢的数量，以找到与每个上下文敏感聚类中枢最近的上下文不敏感聚类中枢。(b)示出了对于上下文敏感的和仅一个节点上下文无关的项，计数为 3 的情况(奇数可能是打破平局的更好选择；此外，我们不需要上下文不敏感集合中的 3 个节点，因为它们是紧密的集群，平均偏差为 0.007，如我们之前看到的)。在计算中使用来自上下文敏感签名的所有节点不太可能产生好的结果，因为上下文敏感节点中的平均标准偏差要高得多。这可能是因为当在嵌入空间中评估时，上下文敏感签名扩展到更大的区域，甚至当捕获单个实体类型时。</p></figure><p id="3d70" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">仅具有上下文敏感签名的顶部中枢的标签预测如下所示。标签和用户标签如下所示。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="1364" class="ni kh it ne b gy nj nk l nl nm">He flew from __ to Sfo<br/><strong class="ne iu">CI space ordering of CS predictions</strong>: Rome Paris Athens Naples Italy Cairo Egypt London there here </span><span id="6f9a" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">Tags</strong>: Italy Venice Athens Bologna Madrid Carthage Roma Sicily Turin Vatican</span><span id="5c71" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">User Label</strong> - location location location location location location location location location location</span><span id="2fcb" class="ni kh it ne b gy ns nk l nl nm">He flew from New York to __<br/><strong class="ne iu">CI space ordering of CS predictions</strong>: Paris London Tokyo Chicago Moscow Japan Boston France Houston Singapore</span><span id="4d3f" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">Tags</strong>: London Madrid Geneva Vienna Bordeaux Chicago Metz Athens Cologne Istanbul</span><span id="a31b" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">User Label</strong> - location location location location location location location location location location</span></pre><h1 id="9fba" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">评估结果</h1><p id="5dbe" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">该模型在两个数据集上进行评估:( 1)具有要标记的三种实体类型(人、位置、组织)的标准数据集 CoNLL-2003，以及(2)具有大约 25 种实体类型的维基文本数据集。</p><p id="8f3d" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在 CoNLL-2003 数据集中，所有三种数据类型的平均 F1 分数<em class="lj">(PER-81.5%；LOC-73%；ORG—66%；MISC-83.87%) </em>仅为 76%。这是由于两个原因。</p><ul class=""><li id="21c0" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">在测试数据的很大一部分中，CoNLL 中的文本结构不是完整的句子——它是板球比分的简洁报告，没有规则的句子结构。由于该模型没有对句子进行预训练或微调，因此它很难预测这些非分布句子中的屏蔽词。在实践中，这可以通过预训练或至少在新的句子结构上微调模型来减轻</li><li id="715c" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">测试数据将许多来自特定位置的团队标记为一个位置。该模型几乎总是将它们标记为位置，而不是团队名称(org)。使用这种无监督的 NER 方法，这个问题没有简单的解决方案。它将只使用最自然地匹配屏蔽位置的实体描述符来标记术语。它不能用不能使用其描述符映射的人工标签来标记术语。虽然从一个角度来看，这可能被认为是一个缺点，但这也是该模型的关键优势——它用来标记屏蔽位置的描述符自然地出现在它被训练的语料库中——它不是从无关的人类标记源学习的。将这些描述符映射到用户定义的标签可能是一种方法，但它可能不是一种干净的解决方案<em class="lj">(例如，在上述将位置描述符视为组织和位置的模糊标签的情况下)</em></li></ul><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/5f10471b27c4d64a05b849906f39253d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6SvUdvGzGvAxLCr3CPmRQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 5a </strong>。CoNLL-2003 结果</p></figure><p id="ac95" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在小测试数据上的模型性能是大约 97%的平均 F1 分数，但是具有全部自然发生的句子集合和大约 25 种标签类型</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/257cee79b3d16f4bbe790c279a02cda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r2Xd6fFSEdwkBDy1Ch6KqQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 5b </strong>。约 25 种实体类型的维基数据结果</p></figure><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/3706a73496095f4c201ca1171dbd2086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UTSJdCxwdZC3MlN8SPyMjg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 5c。</strong>维基数据集的实体分布。在这个数据集上，F1 得分的平均表现为 97%。这种方法的一个关键区别是，模型不仅没有使用标记数据进行训练，甚至没有在测试数据集上进行预训练和微调。</p></figure><h1 id="e34e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">这种方法的局限性和挑战</h1><h2 id="5a6f" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">语料库偏倚</h2><p id="9c8a" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">虽然单个实体预测说明了模型从子词信息中解释实体类型的能力，但实际上，它们只能与具有多种实体类型的句子结合使用。没有太多上下文的单个实体句子对语料偏差很敏感，如下图所示的谷歌和脸书预测。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="3d36" class="ni kh it ne b gy nj nk l nl nm">Facebook is a __<br/><strong class="ne iu">CS Predictions</strong>: <strong class="ne iu">joke monster</strong> killer friend story person company failure website fault</span><span id="72cb" class="ni kh it ne b gy ns nk l nl nm">Microsoft is a __<br/><strong class="ne iu">CS Predictions</strong>: company website competitor people friend player Winner winner person brand</span><span id="4c21" class="ni kh it ne b gy ns nk l nl nm">Google is a __<br/><strong class="ne iu">CS Predictions</strong>: friend website <strong class="ne iu">monster</strong> company killer person man story dog winner</span></pre><h2 id="00a2" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated"><strong class="ak">实体预测中的模糊性</strong></h2><p id="6496" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">这种方法产生了两种模糊性</p><ul class=""><li id="7e59" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">由上下文不敏感描述符表征的实体类型中的模糊性<em class="lj">(图 4c 中的示例)</em>。包含<em class="lj">“银行、银行、银行、银行”</em>的集群可以代表一个组织或位置。然而，在大多数情况下，当通过实体类型的多数投票将上下文敏感签名与上下文不敏感签名紧密匹配时，即使一些匹配的上下文不敏感签名是不明确的，也可以解决这种不明确性。</li><li id="7ddf" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">下面描述的第二个歧义更难解决</li></ul><p id="e3ef" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">有些句子允许不同的实体类型来填充一个隐藏的术语。例如，在预测下面句子中的实体类型“纽约”时</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="d126" class="ni kh it ne b gy nj nk l nl nm">He felt <strong class="ne iu">New York</strong> has a chance to win this year's competition</span></pre><p id="2ab1" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">屏蔽词的实体预测可以是暗示<em class="lj">人</em>的词，这是如下所示的有效预测</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="5901" class="ni kh it ne b gy nj nk l nl nm">He felt __he____ has a chance to win this year's competition</span></pre><p id="3e14" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">模糊性是由于屏蔽而产生的，在大多数情况下，可以通过确定被屏蔽的术语本身的实体类型(纽约)来解决。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="8260" class="ni kh it ne b gy nj nk l nl nm">New York is a _____</span><span id="4696" class="ni kh it ne b gy ns nk l nl nm"><strong class="ne iu">CS Predictions: </strong>city town place City capital reality square country dream star</span></pre><p id="1a1f" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">然而，在某些情况下，甚至被屏蔽的术语也是含糊不清的，使得确定实体具有挑战性。例如，如果原句是</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="0bbe" class="ni kh it ne b gy nj nk l nl nm">He felt <strong class="ne iu">Dolphins</strong> has a chance to win this year's competition</span></pre><p id="9e2a" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">海豚可以是一个音乐团体或运动队。</p><p id="c972" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">这些挑战在很大程度上可以通过多种方法来解决</p><ul class=""><li id="0c1c" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">在特定领域语料库上微调模型可以帮助减少特定领域实体类型中的歧义。例如， BERT 预训练<em class="lj"> </em>中的 BRAF <em class="lj">(这是一个基因)在其签名中没有基因意义，而基因意义存在于生物医学语料库上微调的模型中。</em></li></ul><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="83e1" class="ni kh it ne b gy nj nk l nl nm">BRAF is a _____<br/><strong class="ne iu">CS Prediction</strong>: British German new standard the variant name version World world</span><span id="6ec0" class="ni kh it ne b gy ns nk l nl nm">In a model fine tuned on a biomedical corpus, <br/>BRAF is a _____<br/><strong class="ne iu">CS Prediction</strong>: protein gene kinase structural non family reaction functional receptor molecule</span></pre><ul class=""><li id="4f86" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">用自定义词汇表从<a class="ae mg" rel="noopener" target="_blank" href="/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379">开始预训练模型可以帮助解决实体歧义，更重要的是还可以提高实体标记性能。BERT 的默认词汇丰富，包含全词和子词，用于检测实体类型，如人员、位置、组织等<em class="lj">(图 4a 和 b) </em>。然而，它在获取生物医学领域的全部和部分术语方面存在缺陷。比如像<em class="lj">IMA</em><strong class="lk iu"><em class="lj">tinib</em></strong><em class="lj">、nilo</em><strong class="lk iu"><em class="lj">tinib</em></strong><em class="lj">、dasa</em><strong class="lk iu"><em class="lj">tinib</em></strong>这样的药物的标记化，不考虑常见的子词<em class="lj">【tinib】</em>。伊马替尼标记为<em class="lj"> i ##mat ##ini ##b </em>，而达沙替尼标记为<em class="lj"> das ##ati ##ni ##b </em>。如果我们在生物医学语料库上使用</a><a class="ae mg" href="https://github.com/google/sentencepiece" rel="noopener ugc nofollow" target="_blank">句子片段</a>创建自己的词汇，我们会得到<em class="lj"> im ##a ##tinib </em>和<em class="lj">d # # as # # a # # tinib</em>——捕捉常见后缀。此外，自定义词汇包含来自生物医学领域的完整单词，更好地捕捉生物医学领域的特征。例如像<em class="lj">先天性、癌症、致癌物、心脏病专家等词语</em>。在默认的 BERT 预训练模型中不存在。默认 BERT 词汇中的人称、位置的优势被专有名词和子词所取代，这些专有名词和子词在生物医学语料库中捕捉药物和疾病状况。此外，从生物医学语料库中提取的定制词汇具有大约 45%的新完整词，其中只有 25%与公开可用的 BERT 预训练模型的完整词重叠。当微调 BERT 模型时，有一个选项可以添加 100 个自定义词汇，但不仅数量很少，默认的 BERT 词汇(如前所述)严重偏向某些实体类型，如人员、位置、组织等，如图 4a 所示。</li></ul><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="03a7" class="ni kh it ne b gy nj nk l nl nm"><strong class="ne iu">Token:</strong>            imatinib             dasatinib<br/><strong class="ne iu">BERT</strong> (default):   i ##mat ##ni ##b     das ##ati  ##nib<br/><strong class="ne iu">Custom</strong>:           im ##a  ##tinib      d ##as ##a ##tinib</span></pre><h1 id="897b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">最后的想法</h1><p id="cee8" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">NER 是一个从输入句子到对应于句子中术语的一组标签的映射任务。传统上，模型被训练/微调以使用标记数据作为监督任务来执行该映射。这不考虑利用像 BERT 这样的预训练模型，该模型在语料库上无监督地学习。</p><p id="fef6" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">这篇文章描述了一种在无监督的情况下执行 NER 的方法，而不需要对用屏蔽语言模型目标无监督训练的预训练/微调 BERT 模型进行任何改变。除了矢量处理的最后阶段使用传统算法<em class="lj">(聚类和最近邻居)</em>来确定 NER 标签之外，这是通过仅端到端地对学习到的分布式表示(矢量)进行操作来实现的。此外，与来自顶层的向量用于下游任务的大多数用例相反，BERT 对屏蔽句子的输出仅用作种子符号信息，以在算法上对其自己的最低层(其单词嵌入)进行操作，从而收获句子的 NER 标签。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/b2376b6512483c8f4bc2ed489d1de9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52INPL2ZIhdboyM_TucfGw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图六</strong>。传统的有监督的 NER(左图)与本文描述的无监督的 NER(右图)形成对比。传统上，NER 一直是一项受监督的标签映射任务，其中模型被训练/微调以执行该任务(左侧路径)。相比之下，对于无监督的 NER，按原样使用用屏蔽语言模型目标无监督训练的预训练/微调模型，并且模型的输出被用作播种信息，以在算法上对 BERT 模型的最低层(其单词嵌入)进行操作，从而收获句子的 NER 标签。</p></figure><p id="ddf6" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">总之，执行 NER(传统上是监督学习任务)所需的所有信息已经存在于非监督的 BERT 模型中，其关键部分作为单词嵌入驻留在其最低层中。</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="3ff9" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">上述无监督 NER 的原型实现可在<a class="ae mg" href="https://github.com/ajitrajasekharan/unsupervised_NER.git" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得</p><h1 id="ec87" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">相关作品/参考文献</h1><p id="82cc" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated"><a class="ae mg" href="https://homes.cs.washington.edu/~eunsol/open_entity.html" rel="noopener ugc nofollow" target="_blank">https://homes.cs.washington.edu/~eunsol/open_entity.html</a>这篇 2018 年的论文使用远程监督来执行实体识别。<a class="ae mg" href="http://nlp.cs.washington.edu/entity_type/slides.pdf" rel="noopener ugc nofollow" target="_blank">细粒度标签</a>是为训练模型众包的。</p><p id="1d4b" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated"><a class="ae mg" href="https://www.aclweb.org/anthology/N19-1084.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/N19-1084.pdf</a>。本文使用监督多标签分类模型对超过 10，000 个自由类型执行细粒度的实体分类</p><p id="755f" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">命名实体识别一直是一个被广泛研究的问题，迄今为止，arXiv 中约有<a class="ae mg" href="https://arxiv.org/search/?query=named+entity+recognition&amp;searchtype=all&amp;source=header" rel="noopener ugc nofollow" target="_blank"> 400 篇论文，Google scholar <em class="lj">(自 2016) </em> </a>中有~<a class="ae mg" href="https://scholar.google.com/scholar?as_ylo=2016&amp;q=named+entity+recognition&amp;hl=en&amp;as_sdt=0,33" rel="noopener ugc nofollow" target="_blank">50000 个结果。</a></p><p id="39c1" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated"><a class="ae mg" rel="noopener" target="_blank" href="/examining-berts-raw-embeddings-fd905cb22df7">检查 BERT 的原始嵌入</a></p><h1 id="1a94" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">附加注释</h1><h2 id="f8dd" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">贝茨·MLM·海德—快速回顾</h2><p id="125b" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">伯特的 MLM 头本质上是伯特模型，上面堆叠了一个单一的变换层。下图显示了带有 9 个标记的句子的 BERT 输出<em class="lj">(标记化后)</em>，是一个 9 x 768 矩阵<em class="lj">(768—BERT 基本模型的维度)。这穿过了 MLM 头部的致密层。对具有所有 28，996 个字向量的 9x768 输出执行点积，以找出句子中某个位置的向量输出与 28，996 个字向量有多相似。对于屏蔽的位置，这产生了在该位置的标记的预测。在训练/微调模式中，掩蔽位置的预测误差被反向传播到模型中，一直向下传播到嵌入<em class="lj">(解码器权重和嵌入层权重是相同的)。</em>在推理模式中，嵌入用于表示标记化的文本，以及在头的顶层输出逻辑。</em></p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/b41d5d5bb7be7eb9ccc74a2923424639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrDprtt9Uds1t53E3tSBWw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd kf">图 7。</strong>伯特的 MLM 头像— <em class="mk">显示一个 9 个单词的符号化输入流过模型及其 MLM 头像。解码器使用来自嵌入层的相同向量(在代码中绑定权重，但单独驻留在 pytorch.bin 文件中)。</em></p></figure><h2 id="f675" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">这种方法的性能</h2><p id="6ae7" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">对于像这样的句子，</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/f0eb12ad17c128d80867284017b3a11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlezfEf4rOatUN_f2z8P7Q.png"/></div></div></figure><p id="37e4" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在使用 BERT 的典型监督方法中，通过将整个句子发送到微调的 BERT 模型一次，将获得如下所示的 NER 输出标签<em class="lj"> (B_PER，I_PER，O…) </em></p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/42a0e9cd0c42871f24ad8ac56ddd89f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QknBOsh21flEk7W6eJjBcg.png"/></div></div></figure><p id="ca8b" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">本帖中描述的无监督 NER 方法要求我们将上述句子传递给一个 MLM 负责人四次，以识别四个实体——John Doe、New York、Rio De Janiro 和 Miami <em class="lj">(前面描述的这四个实体位置由 POS tagger 与 chunker 协同识别)</em>。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/659411be929500ecc42cbd130d927a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgj2JqxR7iOhLIQEYcPhdQ.png"/></div></div></figure><p id="0547" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">具体来说，下面句子的 4 个标记化版本将被传递到 MLM 模型中</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/f5fef1cb26113f65040a02fa9d2a6bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6V6IGAp3j671PSuRdgfZw.png"/></div></div></figure><p id="a7ea" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">以检索每个屏蔽位置的上下文敏感签名，然后将其与上下文不敏感签名进行匹配，以产生每个位置的实体预测，如下所示。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pb"><img src="../Images/d6062a08ece010ce83a93232f0314755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SU1N3AehNW9h_fQjrYo2uQ.png"/></div></div></figure><p id="90d2" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">尽管原则上我们可以一次检索输入句子中每个单词的 MLM 上下文敏感签名，但实际上我们必须将句子的屏蔽版本分别发送到 MLM 模型以确定实体类型，因为不清楚是否有一种方法来组合对跨越多个单词的短语或子词的上下文敏感预测， 确定实体类型<em class="lj">(如果原始句子只有单个单词实体，并且这些单词的标记化版本也存在于 BERT 的词汇表中——我们可以一次推断上下文敏感签名)</em>。 例如，部分出现在词汇中的短语，如<em class="lj"> New York </em>，以及分解为子词的单词，如<em class="lj">Imatinib-I # # mat # # ini # # b</em>。使这个问题复杂化的是具有不同默认含义的子词，例如伊马替尼中的<em class="lj">I-I # # mat # # ini # # b，</em>产生具有高方差的上下文敏感签名。像在子词上的波束搜索这样的尝试产生新的看似合理的单个记号，但是很可能不是基本词汇表的一部分，从而导致上下文敏感签名中的较大差异。鉴于 SpanBERT 具有预测跨度的能力，它似乎是一个可以考虑的选项，但它仍然可以预测屏蔽短语中的每个标记——它不会对屏蔽短语给出一个预测。</p><p id="56f1" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">在实践中，通过并行预测一个句子的所有屏蔽版本，可以解决单个句子的多个屏蔽句子的性能问题。在上面的例子中，被屏蔽的术语占句子中术语总数的 50%——但实际上，平均来看，这个数字要少于这个数字。<em class="lj">如果我们也用一个单独的句子来确认句子中每一项的实体预测，该单独的句子仅包括 from</em><strong class="lk iu"><em class="lj">Term</em></strong><em class="lj">is a _ _(如句子“Nonenbury is a __”)，那么发送到 MLM 模型进行预测的句子的数量将是该句子中被屏蔽的项的数量的两倍。</em></p><h2 id="42b3" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">引导将标签描述符映射到用户定义的标签</h2><p id="3020" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">如果我们对应用程序中的一组特定实体感兴趣，那么我们可以利用任何未标记的语料库，这些实体主要出现在</p><ul class=""><li id="8a65" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">通过模型发送这些句子，并获取模型输出的标签描述符</li><li id="78e2" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">通过出现次数对这些描述符进行排序，应该会在顶部产生描述我们感兴趣的实体的描述符。</li><li id="7d02" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">我们只需要手动扫描这些描述符，并将它们映射到我们选择的实体标签。</li><li id="e5f4" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">如果我们用来获取这些标签的未标记语料库真正代表了我们的实体类型，那么这应该覆盖了我们实体的很大一部分</li></ul><p id="2886" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">这种无人监管的方法</p><ul class=""><li id="2a2b" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">将特定用例中带有感兴趣实体的句子的标注问题转化为标注代表这些感兴趣标签的上下文不敏感描述符。通过这样做，如前所述，它消除了使用更多标记数据来重新训练模型以处理非分布句子的需要。</li><li id="e6bf" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">它还利用了一个经过训练的词性标注器来标注任何句子。然而，识别签名和候选描述符的关键部分是由被无监督地训练/微调的 BERT 执行的。</li></ul><h2 id="eea7" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">无子词过滤的上下文不敏感聚类统计</h2><p id="9e24" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">由于难以为子词找到标签，所以不考虑用子词来创建上下文不敏感的聚类。然而，将它们考虑在内的聚类运行揭示了捕获对某些应用可能有价值的有趣信息的聚类。Bert 大箱模型词汇表有 6477 个子词，其中 1399 个形成了簇中枢。其余的被吸收到 59 个非子词簇中枢<em class="lj"> (2872 个是单态)</em>。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pc"><img src="../Images/0fe0a308c98725429c51fa5074960dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZOCjHwRm881HDGHphL6XQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">将子词作为中枢以及包含子词的其他非子词中枢的上下文不敏感聚类。子词不包括在创建上下文不敏感的聚类中，它们在这里显示只是为了强调一些聚类捕获有趣的可解释信息(其他的不形成实体标记的观点)。</p></figure><h2 id="17c1" class="ni kh it bd ki nu nv dn km nw nx dp kq mh ny nz ku mi oa ob ky mj oc od lc oe bi translated">此方法的其他使用案例</h2><p id="f797" class="pw-post-body-paragraph lh li it lk b ll ml ln lo lp mm lr ls mh mn lv lw mi mo lz ma mj mp md me mf im bi translated">假设实体类型的确定纯粹是基于一组术语来完成的，那么可以将它用于各种应用程序</p><ul class=""><li id="1138" class="mq mr it lk b ll lm lp lq mh ms mi mt mj mu mf mv mw mx my bi translated">查找两个或多个术语是否属于同一实体类型。分别输入包含这些术语的句子，找到上下文敏感签名，并检查模型输出的标签是否相同/相似</li><li id="9639" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">获取特定实体类型的更多术语。</li><li id="30bf" class="mq mr it lk b ll nn lp no mh np mi nq mj nr mf mv mw mx my bi translated">当不仅仅局限于标记名词短语时，这种方法的输出可以<em class="lj">(可选地结合词性标记器和依存解析器)</em>用于为下游监督任务生成标记数据，如分类、关系提取等。在某些情况下，如果不替换被监督的任务本身，至少可以创建一个基线。</li></ul><p id="8014" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated">【2021 年 2 月更新。对术语<em class="lj">(或一般短语)</em>(如“Nonenbury”)的实体类型预测可以使用术语<em class="lj">的[CLS]表示来完成(假设模型已经在下一个句子预测任务中训练好)</em>。这种方法可以与上面文章中描述的上下文不敏感预测一起使用—使用合成句子“Nonenbury is a ____”预测实体类型。</p><p id="ceae" class="pw-post-body-paragraph lh li it lk b ll lm ln lo lp lq lr ls mh lu lv lw mi ly lz ma mj mc md me mf im bi translated"><em class="lj">这篇文章是从</em> <a class="ae mg" href="https://qr.ae/Tf8hlX" rel="noopener ugc nofollow" target="_blank"> <em class="lj"> Quora </em> </a>手工导入的</p></div></div>    
</body>
</html>