<html>
<head>
<title>Manage High Content Screening CellProfiler Pipelines with Apache Airflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Airflow管理高含量筛选CellProfiler管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/manage-high-content-screening-cellprofiler-pipelines-with-apache-airflow-66c6ec494686?source=collection_archive---------63-----------------------#2020-05-31">https://towardsdatascience.com/manage-high-content-screening-cellprofiler-pipelines-with-apache-airflow-66c6ec494686?source=collection_archive---------63-----------------------#2020-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/3b55f093fb2a2ac3d0858d68d69a85ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mkF6x29adSLcfZ59.jpg"/></div></div></figure><p id="41b9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你正在运行一个高内容筛选管道，你可能有很多移动的部分。作为一个非详尽的列表，您需要:</p><ul class=""><li id="9552" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">触发<a class="ae lf" href="https://cellprofiler.org/" rel="noopener ugc nofollow" target="_blank"> CellProfiler </a>分析，要么来自LIMS系统，通过观察文件系统，要么来自其他进程。</li><li id="caae" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">跟踪CellProfiler分析的相关性-首先运行光照校正，然后进行分析。</li><li id="1a44" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">如果您有一个大型数据集，并且希望在本世纪的某个时候对其进行分析，您需要拆分分析、运行，然后收集结果。</li><li id="4c82" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">一旦你有了结果，你需要决定一个组织方法。你需要把你的数据放入数据库，并建立深度分析管道。</li></ul><p id="5f21" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当您拥有一个为科学工作流而构建的系统或框架时，这些任务会更容易完成。</p><p id="29ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你更喜欢看，我有一个视频，我通过这个教程中的所有步骤。</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div></figure><h1 id="5004" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">进入阿帕奇气流</h1><p id="0551" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated"><a class="ae lf" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇气流</a>是:</p><blockquote class="mu mv mw"><p id="11dc" class="jy jz mx ka b kb kc kd ke kf kg kh ki my kk kl km mz ko kp kq na ks kt ku kv ij bi translated"><em class="iq"> Airflow是一个由社区创建的平台，用于以编程方式创作、安排和监控工作流。</em></p></blockquote><p id="6e45" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于Apache Airflow有很多很好的介绍资源，但是我在这里将非常简要地介绍一下。</p><p id="8ff8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Apache Airflow为您提供了一个框架，将您的分析组织到Dag或有向无环图中。如果你不熟悉这个术语，它实际上只是一种说法，第三步依赖于第二步，第二步依赖于第一步，或<code class="fe nb nc nd ne b">Step1 -&gt; Step2 -&gt; Step3</code>。</p><p id="c65c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Apache Airflow使用<strong class="ka ir">Dag</strong>，这些Dag是你用来进行分析的桶。您的DAG由操作员和传感器组成。操作符是对你正在完成的任务的一种抽象。这些通常是Bash、Python、SSH，但也可以是更酷的东西，如Docker、Kubernetes、AWS Batch、AWS ECS、数据库操作、文件推送器等等。还有<strong class="ka ir">传感器</strong>，它们是等待各种操作的漂亮而闪亮的方式，无论是等待文件出现、数据库中的记录出现，还是等待另一个任务完成。</p><p id="59cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">开箱即用，您可以获得许多好处，包括一个漂亮的web界面，带有任务的可视化浏览器、调度器、可配置的并行性、日志记录、观察器和任意数量的执行器。因为所有的配置都是用代码编写的，所以非常灵活。它可以与现有系统集成，也可以独立运行。</p><p id="f158" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有许多科学的工作流管理器，在我完成这篇文章的时候，将会有更多的出现。阿帕奇气流是我最喜欢的，但你应该货比三家，看看你点击什么！</p><h1 id="c11d" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">计算后端</h1><p id="5662" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">我之前简单地提到了这一点，但是最初吸引我到Apache Airflow的好处之一是它对您的计算环境是完全不可知的。您可以拥有一台笔记本电脑、一台服务器、一个HPC集群，或者在AWS或GCP上执行。气流本身并不在意。您需要做的就是规划出您的逻辑，确保数据可用，并使用任何合适的操作符。</p><h1 id="c6ae" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">CellProfiler分析工作流程示例</h1><p id="625c" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">在这篇文章中，我将讨论<a class="ae lf" href="https://data.broadinstitute.org/bbbc/BBBC021/" rel="noopener ugc nofollow" target="_blank"> BBBC021 </a>数据集以及我将如何组织和批处理分析。</p><p id="7797" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我决定进行一个简单的设置，即将Apache Airflow与docker-compose结合使用，并使用docker操作符来执行CellProfiler分析。规划好逻辑和工作流后，您可以针对任何计算基础架构使用任何操作员，无论是AWS ECS还是HPC。我最近最喜欢的是Kubernetes，因为它不依赖于任何平台，可以在AWS、GCP或室内使用。您可以使用Kubernetes部署您的数据可视化应用程序，如<a class="ae lf" href="https://medium.com/swlh/deploy-rshiny-with-kubernetes-using-aws-eks-and-terraform-655921d9e13c" rel="noopener"> RShiny </a>、<a class="ae lf" href="https://plotly.com/dash/" rel="noopener ugc nofollow" target="_blank"> Dash </a>或<a class="ae lf" href="https://redash.io/" rel="noopener ugc nofollow" target="_blank"> Redash </a>，如果您使用网络存储或S3，您的所有应用程序都可以访问相同的数据！</p><h1 id="356a" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">项目设置</h1><p id="7783" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">让我们设置我们的项目目录结构！</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="daeb" class="nj ls iq ne b gy nk nl l nm nn">mkdir CellProfiler-Apache-Airflow<br/>cd CellProfiler-Apache-Airflow<br/>mkdir -p data/dags<br/>mkdir -p data/BBBC021/Week1<br/>cd data/BBBC021/Week1<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/BBBC021_v1_images_Week1_22123.zip<br/>find $(pwd) -name "*zip" | xargs -I {} unzip {}<br/># Clean up the zips, we don't need them anymore<br/>find $(pwd) -name "*zip" | xargs -I {} rm -rf {} <br/>cd ../<br/># Run $(pwd) to check where you are. You should be in /project/BBBC021<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/BBBC021_v1_image.csv<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/BBBC021_v1_compound.csv<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/BBBC021_v1_moa.csv<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/analysis.cppipe<br/>wget https://data.broadinstitute.org/bbbc/BBBC021/illum.cppipe<br/><br/># Let's create a data file ONLY for the week1 images, the first dataset<br/>head -n 1 BBBC021_v1_image.csv &gt; images_week1.csv<br/>cat BBBC021_v1_image.csv | grep Week1_22123 &gt;&gt; images_week1.csv</span></pre><p id="17b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这主要是来自<a class="ae lf" href="https://github.com/bitnami/bitnami-docker-airflow" rel="noopener ugc nofollow" target="_blank"> Bitnami </a>的阿帕奇气流配置。Bitnami很棒，我一直在用他们的配置和图片。我对此做了一些修改，以绑定我们的分析Dag，还做了一个快速更改，以便我们可以使用docker操作符。</p><h1 id="5ce6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">Dockerfile文件</h1><p id="f94b" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated"><code class="fe nb nc nd ne b">We're going to use a custom Cellprofiler image to run our pipelines. Create a Dockerfile</code>有了这个:</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="76ac" class="nj ls iq ne b gy nk nl l nm nn">FROM cellprofiler/cellprofiler:3.1.9<br/><br/>RUN apt-get update -y; apt-get install -y unzip imagemagick<br/><br/>ENV TINI_VERSION v0.16.1<br/>ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /usr/bin/tini<br/>RUN chmod +x /usr/bin/tini<br/><br/>ENTRYPOINT [ "/usr/bin/tini", "--" ]<br/>CMD [ "/bin/bash" ]</span></pre><p id="a832" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们将构建新的CellProfiler图像！</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="5b17" class="nj ls iq ne b gy nk nl l nm nn">docker build -t cellprofiler .</span></pre><h1 id="0f1d" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">获取Docker撰写配置</h1><p id="dbb4" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">抓取这个文件并以<code class="fe nb nc nd ne b">docker-compose.yml</code>的名称保存在您的项目根目录中。</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="7aed" class="nj ls iq ne b gy nk nl l nm nn">version: '2'<br/><br/>services:<br/>  postgresql:<br/>    image: 'bitnami/postgresql:10'<br/>      - 'postgresql_data:/bitnami/postgresql'<br/>    environment:<br/>      - POSTGRESQL_DATABASE=bitnami_airflow<br/>      - POSTGRESQL_USERNAME=bn_airflow<br/>      - POSTGRESQL_PASSWORD=bitnami1<br/>      - ALLOW_EMPTY_PASSWORD=yes<br/>  redis:<br/>    image: bitnami/redis:5.0<br/>    volumes:<br/>      - 'redis_data:/bitnami'<br/>    environment:<br/>      - ALLOW_EMPTY_PASSWORD=yes<br/>  airflow-scheduler:<br/>    image: bitnami/airflow-scheduler:1<br/>    environment:<br/>      - AIRFLOW_LOAD_EXAMPLES=no<br/>      - AIRFLOW_DATABASE_NAME=bitnami_airflow<br/>      - AIRFLOW_DATABASE_USERNAME=bn_airflow<br/>      - AIRFLOW_DATABASE_PASSWORD=bitnami1<br/>      - AIRFLOW_EXECUTOR=CeleryExecutor<br/>    volumes:<br/>      - /var/run/docker.sock:/var/run/docker.sock<br/>      - ./data:/data<br/>      - ./dags:/opt/bitnami/airflow/dags<br/>      - airflow_scheduler_data:/bitnami<br/>  airflow-worker:<br/>    image: bitnami/airflow-worker:1<br/>    environment:<br/>      - AIRFLOW_LOAD_EXAMPLES=no<br/>      - AIRFLOW_DATABASE_NAME=bitnami_airflow<br/>      - AIRFLOW_DATABASE_USERNAME=bn_airflow<br/>      - AIRFLOW_DATABASE_PASSWORD=bitnami1<br/>      - AIRFLOW_EXECUTOR=CeleryExecutor<br/>    volumes:<br/>      - /var/run/docker.sock:/var/run/docker.sock<br/>      - ./data:/data<br/>      - ./dags:/opt/bitnami/airflow/dags<br/>      - airflow_worker_data:/bitnami<br/>  airflow:<br/>    image: bitnami/airflow:1<br/>    environment:<br/>      - AIRFLOW_LOAD_EXAMPLES=no<br/>      - AIRFLOW_DATABASE_NAME=bitnami_airflow<br/>      - AIRFLOW_DATABASE_USERNAME=bn_airflow<br/>      - AIRFLOW_DATABASE_PASSWORD=bitnami1<br/>      - AIRFLOW_EXECUTOR=CeleryExecutor<br/>    ports:<br/>      - '8080:8080'<br/>    volumes:<br/>      - /var/run/docker.sock:/var/run/docker.sock<br/>      - ./data:/data<br/>      - ./dags:/opt/bitnami/airflow/dags<br/>      - airflow_data:/bitnami<br/>volumes:<br/>  airflow_scheduler_data:<br/>    driver: local<br/>  airflow_worker_data:<br/>    driver: local<br/>  airflow_data:<br/>    driver: local<br/>  postgresql_data:<br/>    driver: local<br/>  redis_data:<br/>    driver: local</span></pre><p id="a184" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您不习惯容器，这可能会有点棘手，但有一点要注意，主机上的路径不一定与docker容器中的路径相同，例如，我们的数据目录可能在主机系统上的任何地方，但在我们的容器上被绑定为<code class="fe nb nc nd ne b">/data</code>。</p><p id="0301" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">把<code class="fe nb nc nd ne b">docker-compose.yml</code>文件放到你的项目目录中，用<code class="fe nb nc nd ne b">docker-compose up</code>调出。初始化可能需要一些时间。这是我去泡茶的时候。；-)</p><p id="0bab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦启动，您将能够使用默认配置在<code class="fe nb nc nd ne b">localhost:8080</code>访问您的Airflow实例。</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="b5af" class="nj ls iq ne b gy nk nl l nm nn">AIRFLOW_USERNAME: Airflow application username. Default: user AIRFLOW_PASSWORD: Airflow application password. Default: bitnami</span></pre><p id="c5bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里还不会有什么有趣的东西，因为我们还没有分析到位。</p><h1 id="9b62" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">抓住细胞分析仪的分析工具</h1><p id="168e" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">首先拿起照明匕首。将它放在您的<code class="fe nb nc nd ne b">dags</code>文件夹中。它可以被命名为任何东西，气流引用的是<code class="fe nb nc nd ne b">dag_id</code>，但我会引用它为<code class="fe nb nc nd ne b">cellprofiler-illum-dag.py</code>。</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="46c8" class="nj ls iq ne b gy nk nl l nm nn"># dags/cellprofiler-illum-dag.py<br/>from airflow import DAG<br/>from datetime import datetime, timedelta<br/>import string<br/>import random<br/>from airflow.utils import timezone<br/>from airflow.operators.dagrun_operator import TriggerDagRunOperator<br/>from airflow.operators.bash_operator import BashOperator<br/>from airflow.operators.python_operator import PythonOperator<br/># Depending on which version of airflow you are on you will use either operators.docker_operator or providers.docker<br/>from airflow.operators.docker_operator import DockerOperator<br/># from airflow.providers.docker.operators.docker import DockerOperator<br/>from airflow.api.common.experimental.trigger_dag import trigger_dag<br/>from airflow.sensors.external_task_sensor import ExternalTaskSensor<br/>from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun<br/>import time<br/>import os<br/>from pprint import pprint<br/>from airflow.utils.state import State<br/><br/>this_env = os.environ.copy()<br/><br/>this_dir = os.path.dirname(os.path.realpath(__file__))<br/><br/>default_args = {<br/>    'owner': 'airflow',<br/>    'depends_on_past': False,<br/>    'start_date': datetime(2019, 1, 1),<br/>    'email': ['airflow@example.com'],<br/>    'email_on_failure': False,<br/>    'email_on_retry': False,<br/>    'retries': 1,<br/>    'retry_delay': timedelta(minutes=5),<br/>}<br/><br/>dag = DAG('cellprofiler_illumination', default_args=default_args, schedule_interval=None)<br/><br/>EXAMPLE_TRIGGER = """<br/>{<br/>    "illum_pipeline" : "/data/BBBC021/illum.cppipe",<br/>    "analysis_pipeline" : "/data/BBBC021/analysis.cppipe",<br/>    "pipeline" : "/data/BBBC021/illum.cppipe",<br/>    "output": "/data/BBBC021/Week1/Week1_22123",<br/>    "input": "/data/BBBC021/Week1/Week1_22123",<br/>    "data_file": "/data/BBBC021/images_week1.csv"<br/>}<br/>"""<br/><br/># Volumes are from the HOST MACHINE<br/>illum = DockerOperator(<br/>    dag=dag,<br/>    task_id='illum',<br/>    retries=1,<br/>    volumes=[<br/>        # UPDATE THIS to your path! <br/>        '/path-to-project-on-HOST/data:/data'<br/>    ],<br/>    working_dir='/data/BBBC021',<br/>    tty=True,<br/>    image='cellprofiler',<br/>    command=[<br/>        "bash", "-c",<br/>        """cellprofiler --run --run-headless \<br/>            -p {{ dag_run.conf['illum_pipeline'] }}  \<br/>            -o {{ dag_run.conf['output'] }}  \<br/>            -i {{ dag_run.conf['input'] }}  \<br/>            -data-file {{ dag_run.conf['data_file'] }} \<br/>            -c -r"""<br/>    ]<br/>)<br/><br/><br/>def get_number_of_tasks(data_file):<br/>    """<br/>    Parse the file to get the number of lines<br/>    The number of lines, minus 1 for the header<br/>    is the number of groups<br/>    :param data_file:<br/>    :return:<br/>    """<br/>    file = open(data_file, "r")<br/>    number_of_lines = 0<br/>    for line in file:<br/>        number_of_lines += 1<br/>    file.close()<br/>    return number_of_lines - 1<br/><br/><br/>def watch_task(triggers):<br/>    """<br/>    This is only here for demonstration purposes<br/>    to show how you could dynamically watch the cellprofiler analysis DAG<br/>    :param triggers:<br/>    :return:<br/>    """<br/>    print('-------------------------------------------')<br/>    print('Checking up on our dag...')<br/>    check_dag = check_and_get_dag(dag_id='cellprofiler_analysis')<br/>    dag_run = check_and_get_dagrun(check_dag, triggers[0].execution_date)<br/>    state = dag_run.get_state()<br/>    finished = State.finished()<br/>    unfinished = State.unfinished()<br/><br/>    while state in unfinished:<br/>        time.sleep(10)<br/>        state = dag_run.get_state()<br/><br/>    print('-------------------------------------------')<br/>    print('Dag run finished or dead')<br/>    pprint(dag_run.get_state())<br/><br/><br/>def trigger_analysis(ds, **kwargs):<br/>    """<br/>    Trigger the cellprofiler analysis DAG<br/>    We want one DAG run per row in the datafile, or -f / -l combo<br/>    :param ds:<br/>    :param kwargs:<br/>    :return:<br/>    """<br/>    print('-------------------------------------------')<br/>    print("Here's the conf!")<br/>    pprint(kwargs['dag_run'].conf)<br/>    output = kwargs['dag_run'].conf['output']<br/>    data_file = kwargs['dag_run'].conf['data_file']<br/>    no_tasks = get_number_of_tasks(str(data_file))<br/>    triggers = []<br/>    print('-------------------------------------------')<br/>    print('Triggering our dag...')<br/>    for index, value in enumerate(range(1, no_tasks + 1)):<br/>        trigger = trigger_dag(<br/>            dag_id="cellprofiler_analysis",<br/>            replace_microseconds=False,<br/>            run_id="trig__{}__f_{}__l_{}".format(<br/>                timezone.utcnow().isoformat(),<br/>                value,<br/>                value<br/>            ),<br/>            conf={<br/>                "pipeline": kwargs['dag_run'].conf['analysis_pipeline'],<br/>                "output": "{}/f-{}__l-{}".format(output, value, value),<br/>                "input": kwargs['dag_run'].conf['input'],<br/>                "data_file": data_file,<br/>                "first": value,<br/>                "last": value,<br/>            }<br/>        )<br/>        triggers.append(trigger)<br/><br/><br/>trigger_analysis_task = PythonOperator(<br/>    dag=dag,<br/>    task_id='trigger_analysis',<br/>    provide_context=True,<br/>    python_callable=trigger_analysis<br/>)<br/><br/>trigger_analysis_task.set_upstream(illum)</span></pre><p id="93a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">而现在我们的<code class="fe nb nc nd ne b">cellprofiler-analysis-dag.py</code>。</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="b4bf" class="nj ls iq ne b gy nk nl l nm nn"># dags/cellprofiler-analysis-dag.py<br/>from airflow import DAG<br/>from datetime import datetime, timedelta<br/>from airflow.operators.python_operator import PythonOperator<br/>from airflow.operators.docker_operator import DockerOperator<br/>import os<br/>from pprint import pprint<br/><br/>this_env = os.environ.copy()<br/><br/>this_dir = os.path.dirname(os.path.realpath(__file__))<br/><br/>default_args = {<br/>    'owner': 'airflow',<br/>    'depends_on_past': False,<br/>    'start_date': datetime(2019, 1, 1),<br/>    'email': ['airflow@example.com'],<br/>    'email_on_failure': False,<br/>    'email_on_retry': False,<br/>    'retries': 1,<br/>    'retry_delay': timedelta(minutes=5),<br/>}<br/><br/>dag = DAG('cellprofiler_analysis', default_args=default_args, schedule_interval=None)<br/><br/>analysis = DockerOperator(<br/>    dag=dag,<br/>    task_id='analysis',<br/>    retries=1,<br/>    volumes=[<br/>        # Volumes are from the HOST MACHINE<br/>        # UPDATE THIS to your path! <br/>        '/path-on-HOST/data:/data'<br/>    ],<br/>    tty=True,<br/>    image='cellprofiler',<br/>    command=[<br/>        "bash", "-c",<br/>        """cellprofiler --run --run-headless \<br/>            -p {{ dag_run.conf['pipeline'] }}  \<br/>            -o {{ dag_run.conf['output'] }}  \<br/>            -i {{ dag_run.conf['input'] }} \<br/>            --data-file {{ dag_run.conf['data_file'] }} \<br/>            -c -r -f {{ dag_run.conf['first'] }} -l {{ dag_run.conf['last'] }}"""<br/>    ]<br/>)<br/><br/><br/>def gather_results(ds, **kwargs):<br/>    """<br/>    Once we have the Cellprofiler results let's do something with them!<br/>    :param ds:<br/>    :param kwargs:<br/>    :return:<br/>    """<br/>    pass<br/><br/><br/>gather_results_task = PythonOperator(<br/>    dag=dag,<br/>    task_id='gather_results_task',<br/>    provide_context=True,<br/>    python_callable=gather_results<br/>)<br/><br/>gather_results_task.set_upstream(analysis)</span></pre><p id="da46" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">确保您更新了<code class="fe nb nc nd ne b">DockerOperator</code> <code class="fe nb nc nd ne b">volumes</code>以匹配您的本地文件系统！否则你的分析是行不通的！</p><h1 id="ab34" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">分析组织概述</h1><p id="2015" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">我们这里有2个独立的Dag，每个CellProfiler分析一个。</p><p id="510d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些步骤是:</p><ul class=""><li id="53f0" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">所有图像的过程照明管道</li><li id="cf11" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">获取数据文件，看看我们有多少图像</li><li id="ed01" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">为每个图像动态生成一个CellProfiler提交。</li><li id="041b" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated">(Placeholder)对我们的结果做点什么吧！</li></ul><p id="e879" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您会注意到，我们正在动态拆分CellProfiler分析。这将使我们的分析更快完成，当你购买产生大量数据的花哨的机器人显微镜时，这变得越来越重要。Airflow负责处理引擎下的任务队列，所以我们需要做的就是找出我们如何分割分析的逻辑。如果你想知道更多关于并行性在气流中是如何处理的<a class="ae lf" href="https://www.astronomer.io/guides/airflow-scaling-workers/" rel="noopener ugc nofollow" target="_blank">，这是一篇很棒的文章</a>。</p><p id="8d2d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好的，最后一个只是一个占位符。我可以想象，一旦你有了结果，你会想对它们做些什么，比如把它们放入数据库，根据某些标准进行一个或多个分析，或者做一些后处理，但现在这是空白的，所以你可以想知道可能性。</p><h1 id="0e58" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">将参数传递给我们的分析</h1><p id="d58c" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">当我开始使用气流时，这一点对我来说并不明显，现在你可以听到关于它的一切了！；-)</p><p id="008b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以使用conf对象或参数向气流传递参数。根据运算符类型的不同，这可能会略有不同。如果你使用的是Python操作符，你可以把它作为一个字典<code class="fe nb nc nd ne b">kwargs['dag_run'].conf</code>来访问，如果你使用的是<code class="fe nb nc nd ne b">Bash</code>，或者在这个例子中是<code class="fe nb nc nd ne b">Docker</code>，你可以把它作为一个<a class="ae lf" href="https://airflow.apache.org/docs/stable/tutorial.html#templating-with-jinja" rel="noopener ugc nofollow" target="_blank">模板变量</a> <code class="fe nb nc nd ne b">{{ dag_run.conf['variable'] }}</code>来访问。</p><p id="237c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将带您了解如何使用Airflow web界面来触发DAG并传入变量，但是您也可以使用REST API、通过Airflow CLI或使用Python代码以编程方式来触发DAG。无论如何触发DAG，都要将配置变量作为JSON字符串传入。</p><h1 id="9cbb" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">关于Docker卷的一个注记</h1><p id="868d" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">这有点奇怪，因为我们使用docker-compose来运行Airflow，然后使用docker操作符。请记住，当您使用Docker操作符时，您使用主机上的路径来映射卷，而不是在docker-compose容器中。你的主机就是你运行<code class="fe nb nc nd ne b">docker-compose up</code>的机器。</p><p id="8225" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如:</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="0e2f" class="nj ls iq ne b gy nk nl l nm nn">analysis = DockerOperator(<br/>    ...<br/>    volumes=[<br/>        # Volumes are from the HOST MACHINE<br/>        '/path-on-HOST/data:/data'<br/>        # NOT<br/>        # '/data:/data'<br/>        # Even though in our docker-compose instance the volume is bound as /data<br/>    ],<br/>    ...<br/>)</span></pre><h1 id="f74b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">分析一下！</h1><p id="4969" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">我们准备好了！现在您已经有了Dag(您需要有Dag)所有设置，我们可以开始分析数据了！</p><p id="c8a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用用户<code class="fe nb nc nd ne b">user</code>和密码<code class="fe nb nc nd ne b">bitnami</code>在<code class="fe nb nc nd ne b">localhost:8080</code>登录。您应该会看到如下所示的屏幕:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/61eb5d0e31d0f749c0588d342d99a6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oaTL9FFlFcbhMU8e.jpg"/></div></div></figure><p id="7cee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">默认情况下，您的dags将关闭。确保在进行下一步之前打开它们！</p><h1 id="041e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">触发你的分析</h1><p id="cf99" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">前往你在<code class="fe nb nc nd ne b">localhost:8080</code>的主页并触发<code class="fe nb nc nd ne b">cellprofiler-illum</code> DAG。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/2858e82a339940170fa6fd1ed8d8b6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*__Knfug8Pnlc75Ea.jpg"/></div></div></figure><p id="bb83" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">系统会提示您添加一些JSON配置变量。</p><pre class="ll lm ln lo gt nf ne ng nh aw ni bi"><span id="8922" class="nj ls iq ne b gy nk nl l nm nn">{ "illum_pipeline" : "/data/BBBC021/illum.cppipe", "analysis_pipeline" : "/data/BBBC021/analysis.cppipe", "pipeline" : "/data/BBBC021/illum.cppipe", "output": "/data/BBBC021/Week1/Week1_22123", "input": "/data/BBBC021/Week1/Week1_22123", "data_file": "/data/BBBC021/images_week1.csv" }</span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/c82e0aabb7a87d01063ae49598fe2297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jqoD8WtcY6k5fIuC.jpg"/></div></div></figure><p id="0ff3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您将返回到主页面，并且应该看到您的CellProfiler光照分析正在运行！</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/d3a1e720566a2faa891e147449abd0f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uh-imcxNTpTTSZ3A.png"/></div></div></figure><p id="53bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有一个简短的(~1分钟)视频，展示了如何浏览气流界面来研究您的分析。</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div></figure><h1 id="0918" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">包裹</h1><p id="e552" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">就是这样！让气流运行，你将有你的照明和分析管道运行，所有分裂很好，你不是保姆跟踪作业队列，日志，和成功/失败率。您还可以使用REST API、CLI或代码将Airflow与任何其他系统集成，例如LIMS、报告数据库或辅助分析工作流。</p><p id="1487" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你想了解更多，请查看我的网站，或者直接联系jillian@dabbleofdevops.com。</p><h1 id="b81e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">感谢</h1><p id="1192" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">特别感谢Broad BioImage Repository提供的数据集，以及Anne Carpenter博士、Beth Cimini和Becki Ledford提供的极有价值的反馈和编辑！</p><h1 id="f418" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">引文</h1><p id="387f" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated"><a class="ae lf" href="https://data.broadinstitute.org/bbbc/BBBC021/" rel="noopener ugc nofollow" target="_blank">https://data.broadinstitute.org/bbbc/BBBC021/</a></p><p id="1a15" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">“我们使用图像集bbbc 021v 1[蔡锷等人，分子癌症治疗学，2010]，可从广泛的生物图像基准收集[Ljosa等人，自然方法，2012]。”</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="f68c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mx">最初发表于</em><a class="ae lf" href="https://www.dabbleofdevops.com/blog/manage-high-content-screening-cellprofiler-pipelines-with-apache-airflow" rel="noopener ugc nofollow" target="_blank"><em class="mx">【https://www.dabbleofdevops.com】</em></a><em class="mx">。</em></p></div></div>    
</body>
</html>