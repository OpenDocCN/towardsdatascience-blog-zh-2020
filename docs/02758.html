<html>
<head>
<title>Deep Learning based Super Resolution with OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的OpenCV超分辨率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-based-super-resolution-with-opencv-4fd736678066?source=collection_archive---------2-----------------------#2020-03-17">https://towardsdatascience.com/deep-learning-based-super-resolution-with-opencv-4fd736678066?source=collection_archive---------2-----------------------#2020-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="65a5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用最新的算法，通过OpenCV库中一个简单易用的函数来提高图像的分辨率。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2886bc7b2991cdf2bf55817bbcd5fa30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-NTAFhdX3PLvZIRv2mlyw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@kathysg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">凯西·塞尔文</a>在<a class="ae kv" href="https://unsplash.com/s/photos/butterfly?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">号航天飞机</a>上的照片</p></figure><p id="0a00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">目录</strong></p><ul class=""><li id="fecf" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">超分辨率</li><li id="c6ae" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在OpenCV中放大图像的步骤</li><li id="59cf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">不同的预训练模型</li><li id="7a45" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">结果样本</li><li id="ff72" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">注释和参考资料</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c4c7" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">OpenCV中的超分辨率</strong></h1><p id="683f" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">OpenCV是一个开源的计算机视觉库，有大量优秀的算法。自从最近的一次合并以来，OpenCV包含了一个基于深度学习方法的易于使用的界面来实现超级分辨率(SR)。该界面包含经过预先训练的模型，可以非常容易和有效地用于推理。在这篇文章中，我将解释它能做什么，并逐步展示如何使用它。<strong class="ky ir">到目前为止，它与C++和Python一起工作。</strong></p><p id="b483" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:使用此代码不需要任何关于SR的知识。我在一台Ubuntu 16.04机器上运行这个，但是它适用于任何其他发行版(它也适用于视窗)。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="01cf" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">步骤</strong></h1><ul class=""><li id="1276" class="ls lt iq ky b kz nf lc ng lf nk lj nl ln nm lr lx ly lz ma bi translated">1.使用contrib模块(包含dnn_superres)安装OpenCV。</li><li id="b7e1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">2.下载经过预先培训的模型。</li><li id="3fd0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">3.放大图像。</li></ul><ol class=""><li id="9886" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nn ly lz ma bi translated"><strong class="ky ir">与contrib模块一起安装OpenCV。</strong></li></ol><p id="4719" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的第一步是安装OpenCV。<strong class="ky ir">有些功能会在以后发布，所以要小心版本:4.2.0为C++，4.3.0添加Python wrap，4.4.0添加GPU推理。</strong>您可以按照<a class="ae kv" href="https://docs.opencv.org/master/d7/d9f/tutorial_linux_install.html" rel="noopener ugc nofollow" target="_blank"> opencv文档</a>中的说明进行操作。<em class="no">非常重要的是</em>你也要安装contrib模块，因为这是SR接口代码所在的地方。您可以选择安装所有contrib模块或只安装必要的SR模块。我们将使用的接口或模块称为dnn_superres (dnn代表深度神经网络；超级分辨率的超级图像)。</p><p id="9e40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ir">下载预先训练好的模型。</strong></p><p id="1267" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要单独下载预训练的模型，因为OpenCV代码库不包含它们。原因是有些模型很大。因此，如果它们在标准代码库中，可能不需要它们的人仍然必须下载它们。有几个模型可供选择(参见本文中的模型一节)。都是流行SR论文的实现。就目前来说，还是选个小型号吧。你可以在这里下载<a class="ae kv" href="https://github.com/Saafke/FSRCNN_Tensorflow/blob/master/models/FSRCNN_x2.pb" rel="noopener ugc nofollow" target="_blank">。这个预先训练的模型可以将图像的分辨率提高2倍。更多模型及其解释，请参见本文中的模型部分。</a></p><p id="12a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ir">放大图像。</strong></p><p id="3127" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们准备升级我们自己的图像甚至视频。我将提供C++和Python的示例代码。(如果要用GPU进行推理，也请参考本页底部的注释。)我们先来看<strong class="ky ir"> C++ </strong>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c215" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将解释代码的重要部分。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="14db" class="nw mo iq ns b gy nx ny l nz oa">//Read the desired model<br/>string path = "FSRCNN_x2.pb";<br/>sr.readModel(path);</span></pre><p id="3884" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将加载所选模型的所有变量，并为神经网络的推断做准备。参数是下载的预训练模型的路径。当您想要使用不同的模型时，只需下载它(参见本文的模型部分)并更新路径。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1bd3" class="nw mo iq ns b gy nx ny l nz oa">//Set the desired model and scale to get correct pre- and post-processing<br/>sr.setModel("fsrcnn", 2);</span></pre><p id="3569" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这让模块知道您选择了哪个模型，因此它可以选择正确的预处理和后处理。您必须指定正确的模型，因为每个模型使用不同的处理。</p><p id="ce77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个参数是模型的名称。可以在“edsr”、“fsrcnn”、“lapsrn”、“espcn”中选择。<em class="no">非常重要的一点是</em>该型号是您在“sr.readModel()”中指定的型号的正确型号。请参见页面底部的型号部分，了解各型号的规格。</p><p id="c855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个参数是放大系数，即你将增加多少倍的分辨率。同样，这需要与您选择的型号相匹配。每个模型都有一个单独的放大系数文件。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1720" class="nw mo iq ns b gy nx ny l nz oa">//Upscale<br/>Mat img_new;<br/>sr.upsample(img, img_new);<br/>cv::imwrite( "upscaled.png", img_new);</span></pre><p id="4bea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是推理部分，它通过神经网络运行你的图像，并产生你的放大图像。</p><p id="881c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在只需编译和运行您的文件来升级您的图像！</p><p id="dfea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是用<strong class="ky ir"> Python </strong>编写的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5d49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它和C++中的几乎完全一样，除了两件事。</p><ol class=""><li id="09d7" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nn ly lz ma bi translated">您必须使用不同的构造函数:</li></ol><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="edd0" class="nw mo iq ns b gy nx ny l nz oa"># Create an SR object<br/>sr = dnn_superres.DnnSuperResImpl_create()</span></pre><p id="55e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.放大时，您可以直接分配放大的图像，而不是创建占位符图像:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="51c8" class="nw mo iq ns b gy nx ny l nz oa"># Upscale the image<br/>result = sr.upsample(image)</span></pre></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="1268" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">型号</strong></h1><p id="b99c" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">该模块目前支持4种不同的SR型号。它们都可以按2、3和4的比例放大图像。LapSRN甚至可以扩大到8倍。它们在准确性、大小和速度上有所不同。</p><ol class=""><li id="b99a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nn ly lz ma bi translated"><strong class="ky ir">EDSR</strong>【1】<strong class="ky ir">。</strong>这是性能最好的型号。然而，它也是最大的模型，因此具有最大的文件大小和最慢的推理。你可以在这里下载<a class="ae kv" href="https://github.com/Saafke/EDSR_Tensorflow/tree/master/models" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="a5b2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nn ly lz ma bi translated"><strong class="ky ir">ESPCN</strong>【2】<strong class="ky ir">。</strong>这是一个小模型，推理又快又好。它可以进行实时视频放大(取决于图像大小)。你可以在这里下载<a class="ae kv" href="https://github.com/fannymonori/TF-ESPCN/tree/master/export" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="b55a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nn ly lz ma bi translated"><strong class="ky ir">fsr CNN</strong>【3】<strong class="ky ir">。</strong>这也是推理快速准确的小模型。也可以做实时视频向上扩展。你可以在这里下载<a class="ae kv" href="https://github.com/Saafke/FSRCNN_Tensorflow/tree/master/models" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="175a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nn ly lz ma bi translated"><strong class="ky ir">LapSRN</strong>【4】<strong class="ky ir">。</strong>这是一个中等大小的模型，可以放大8倍。你可以在这里下载<a class="ae kv" href="https://github.com/fannymonori/TF-LapSRN/tree/master/export" rel="noopener ugc nofollow" target="_blank">。</a></li></ol><p id="8158" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关这些模型的更多信息和实现，请参见<a class="ae kv" href="https://github.com/opencv/opencv_contrib/tree/master/modules/dnn_superres" rel="noopener ugc nofollow" target="_blank">模块的GitHub自述文件</a>。关于广泛的基准测试和比较，请点击。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f083" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">样品</h1><p id="5683" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">我们来看一些结果！(如果你在手机上，放大看区别更清楚。)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/52ca06138f50443bd4609abb9ecb22ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*A8yToxEh-f0_1Up8u51aHQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">投入</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/df94dcc956e0352167a49d027257c7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*U7fbkyr4gvceALewEOi3Dw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过双线性插值放大(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b39fcab762614a23d1cb14fe1f5007c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*5288YahA2Zp-XXtUfb3iDA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由FSRCNN放大(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5c031011cb18196cb2f6647d7a7071d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*swfxLCsjlBq7Y6etJWdleQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由ESDR升级(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/acfba115bd4ac4e51f3fec1315faa464.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*7sqZ6SIRlyR6ex1IbFjtFg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输入。Randy Fath 在<a class="ae kv" href="https://unsplash.com/s/photos/vegetable?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/32ab6056e2cc09baa7ae80de0ef83c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vSikPDJavzxRXMI-wo6vQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过双线性插值放大(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/d54356c8b94bc42c1b3d830e4e67b39b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LAXDHA3ygACf8HIOuQTEmg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由FSRCNN放大(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/1e5b7dd3fc039057b00945e91dc6f516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYeg9TBGItkfmyO2NKEFGA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由ESDR升级(系数为3)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/ec7458cc062421022b09b2f106cc4b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWLSzj0A3-neu2NGZIfcXQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原图。</p></figure><p id="d904" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，这些模型产生了非常令人满意的结果，完全破坏了简单双线性插值的性能。特别是EDSR给出了惊人的结果，尽管它有点慢(几秒钟的推断)。自己试试吧！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="4366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实施注意事项:</strong></p><ul class=""><li id="4924" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如果它在使用。jpg图片，试试。png格式。</li><li id="de12" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">确保setModel()参数与readModel()中使用的模型相匹配。</li><li id="edbe" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">尝试不同的模型，在速度和性能方面得到不同的结果。</li><li id="54e9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果你想使用你的GPU进行推理(标准是CPU)，你可以在读取模型后将后端和目标设置为CUDA。根据你的GPU，它会给你一个速度上的提升。这是新特性，所以需要4.4.0版本。<a class="ae kv" href="https://github.com/opencv/opencv_contrib/pull/2599" rel="noopener ugc nofollow" target="_blank">参见相关的拉取请求。</a>你还需要构建支持CUDA的opencv，比如像这样:<a class="ae kv" href="https://gist.github.com/Saafke/3e53662286ac3254d34b1d6b9e5b1604" rel="noopener ugc nofollow" target="_blank">链接到opencv构建命令</a>。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在C++中设置后端/目标</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在Python中设置后端/目标</p></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="caac" class="nw mo iq bd mp og oh dn mt oi oj dp mx lf ok ol mz lj om on nb ln oo op nd oq bi translated">参考</h2><p id="5526" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">[1] Bee Lim，Sanghyun Son，Heewon Kim，Seungjun Nah，和Kyoung Mu Lee，“用于单个图像超分辨率的增强深度残差网络”，<em class="no">第二届NTIRE:图像恢复和增强研讨会的新趋势以及与2017年CVPR相关的图像超分辨率挑战。</em></p><p id="187c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Shi，w .，Caballero，j .，Huszár，f .，Totz，j .，Aitken，a .，Bishop，r .，Rueckert，d .和Wang，z .，“使用高效亚像素卷积神经网络的实时单幅图像和视频超分辨率”，<em class="no">IEEE计算机视觉和模式识别会议论文集</em>2016。</p><p id="68e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]赵东，陈变来，唐晓鸥。“加速超分辨率卷积神经网络”，<em class="no">2016年ECCV</em>欧洲计算机视觉会议论文集。</p><p id="b66e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]赖伟生，黄，J. B .，Ahuja，n .和杨，M. H .，<em class="no">“用于快速和精确超分辨率的深度拉普拉斯金字塔网络”，</em>IEEE计算机视觉和模式识别会议论文集，2017。</p></div></div>    
</body>
</html>