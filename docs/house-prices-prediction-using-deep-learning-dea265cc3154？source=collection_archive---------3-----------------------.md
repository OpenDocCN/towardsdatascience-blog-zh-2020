# åŸºäºæ·±åº¦å­¦ä¹ çš„æˆ¿ä»·é¢„æµ‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/house-prices-prediction-using-deep-learning-dea265cc3154?source=collection_archive---------3----------------------->

## Keras å›å½’ä¸å¤šå…ƒçº¿æ€§å›å½’

![](img/e67b869cb04007e784a408b0f7945edc.png)

ç…§ç‰‡ç”±ä½›ç½—é‡Œè¾¾å· KW insta gram ä¸Šçš„@ [Kusseyl](https://www.instagram.com/p/B-2gBVqhuME/?utm_source=ig_web_copy_link) æ‹æ‘„

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æˆ¿ä»·ğŸ¡åŸºäºä¸åŒå¸‚åœºçš„å„ç§å› ç´ ã€‚

# é—®é¢˜é™ˆè¿°

è¿™ä¸€ç»Ÿè®¡åˆ†æçš„ç›®çš„æ˜¯å¸®åŠ©æˆ‘ä»¬ç†è§£æˆ¿å±‹ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨è¿™äº›å˜é‡æ¥é¢„æµ‹æˆ¿ä»·ã€‚

# ç›®æ ‡

*   é¢„æµ‹æˆ¿ä»·
*   åœ¨æœ€å°åŒ–é¢„æµ‹å’Œå®é™…è¯„çº§ä¹‹é—´çš„å·®å¼‚æ–¹é¢ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ¨¡å‹

**æ‰€ç”¨æ•°æ®:**[ka ggle-KC _ house Dataset](https://www.kaggle.com/harlfoxem/housesalesprediction)
**GitHub:**ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æˆ‘çš„æºä»£ç 

# æ­¥éª¤ 1:æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯¼å…¥æ•°æ®ï¼Œçœ‹çœ‹æˆ‘ä»¬æ­£åœ¨å¤„ç†å“ªç§æ•°æ®:

```
**#import required libraries** import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt**#import Data** Data = pd.read_csv('kc_house_data.csv')
Data.head(5).T**#get some information about our Data-Set** Data.info()
Data.describe().transpose()
```

![](img/cf845c18ee334af78096e9f245e65460.png)

æˆ‘ä»¬æ•°æ®é›†çš„å‰ 5 æ¡è®°å½•

![](img/1f8fda225be74ceb5cb263981ab2fed4.png)

å…³äºæ•°æ®é›†çš„ä¿¡æ¯ï¼Œä½ çš„å˜é‡æ˜¯å“ªç§æ•°æ®ç±»å‹

![](img/b582524c12630222fe830c074a509573.png)

æ•°æ®é›†çš„ç»Ÿè®¡æ‘˜è¦

å·²æä¾›ä»¥ä¸‹åŠŸèƒ½:***æ—¥æœŸ:*** *æˆ¿å±‹å‡ºå”®æ—¥æœŸ* âœ”ï¸ ***ä»·æ ¼:*** *ä»·æ ¼ä¸ºé¢„æµ‹ç›®æ ‡* âœ”ï¸ ***å§å®¤:*** *å§å®¤/æˆ¿å±‹æ•°é‡* âœ”ï¸ ***å«ç”Ÿé—´:*** *å«ç”Ÿé—´æ•°é‡ æ‹å“é•œå¤´* âœ”ï¸ ***æ¥¼å±‚:*** *æˆ¿å±‹æ€»å±‚æ•°* âœ”ï¸ ***æ»¨æ°´:*** *å¯ä»¥çœ‹åˆ°æ»¨æ°´æ™¯è§‚çš„æˆ¿å±‹* ***æŸ¥çœ‹:*** *å·²æŸ¥çœ‹* âœ”ï¸ ***æ ¹æ®æ™¯å¿åˆ†çº§ç³»ç»Ÿ* âœ”ï¸***sqft _ above:****æˆ¿å±‹é¢ç§¯(ä¸å«åœ°ä¸‹å®¤)* âœ”ï¸***sqft _ base:****åœ°ä¸‹å®¤é¢ç§¯* âœ”ï¸***yr _ build:****å»ºé€ å¹´ä»½
ã€T10 åæ ‡* âœ”ï¸ ***é•¿:*** *ç»åº¦åæ ‡* âœ”ï¸***sqft _ living 15:****å®¢å…é¢ç§¯ 2015 å¹´(æš—ç¤ºâ€”éƒ¨åˆ†è£…ä¿®)* âœ”ï¸***sqft _ lot 15:*****

è®©æˆ‘ä»¬ç»˜åˆ¶å‡ ä¸ªç‰¹å¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ•°æ®

```
**#visualizing house prices**
fig = plt.figure(figsize=(10,7))
fig.add_subplot(2,1,1)
sns.distplot(Data['price'])
fig.add_subplot(2,1,2)
sns.boxplot(Data['price'])
plt.tight_layout()**#visualizing square footage of (home,lot,above and basement)**fig = plt.figure(figsize=(16,5))
fig.add_subplot(2,2,1)
sns.scatterplot(Data['sqft_above'], Data['price'])
fig.add_subplot(2,2,2)
sns.scatterplot(Data['sqft_lot'],Data['price'])
fig.add_subplot(2,2,3)
sns.scatterplot(Data['sqft_living'],Data['price'])
fig.add_subplot(2,2,4)
sns.scatterplot(Data['sqft_basement'],Data['price'])**#visualizing bedrooms,bathrooms,floors,grade** fig = plt.figure(figsize=(15,7))
fig.add_subplot(2,2,1)
sns.countplot(Data['bedrooms'])
fig.add_subplot(2,2,2)
sns.countplot(Data['floors'])
fig.add_subplot(2,2,3)
sns.countplot(Data['bathrooms'])
fig.add_subplot(2,2,4)
sns.countplot(Data['grade'])
plt.tight_layout()
```

é€šè¿‡ä»·æ ¼åˆ†å¸ƒå›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§å¤šæ•°ä»·æ ¼åœ¨ 0 åˆ° 100 ä¸‡ä¹‹é—´ï¼Œåªæœ‰å°‘æ•°å¼‚å¸¸å€¼æ¥è¿‘ 800 ä¸‡(è±ªå®…ğŸ˜‰).åœ¨æˆ‘ä»¬çš„åˆ†æä¸­å»æ‰é‚£äº›å¼‚å¸¸å€¼æ˜¯æœ‰æ„ä¹‰çš„ã€‚

![](img/0479909b936f1a93beecad4aac5f14c6.png)

æˆ¿ä»·é¢„æµ‹

å¿«é€Ÿæµè§ˆä¸åŒç‰¹å¾åˆ†å¸ƒä¸æˆ¿ä»·çš„å…³ç³»æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚

![](img/4c44b419f8602533d92f88fd44087e58.png)

æ•£ç‚¹å›¾â€”â€”å¹³æ–¹è‹±å°º(ä½å®…ã€åœ°æ®µã€æ¥¼ä¸Šå’Œåœ°ä¸‹å®¤)

![](img/9b30be011ac49062da86273784818406.png)

è®¡æ•°å›¾â€”å§å®¤ã€æµ´å®¤ã€åœ°æ¿ã€ç­‰çº§

åœ¨è¿™é‡Œï¼Œæˆ‘å°†æ—¥æœŸåˆ—åˆ†è§£ä¸ºå¹´å’Œæœˆï¼Œä»¥æŸ¥çœ‹æˆ¿ä»·æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚

```
**#let's break date to years, months** Data['date'] = pd.to_datetime(Data['date'])
Data['month'] = Data['date'].apply(lambda date:date.month)
Data['year'] = Data['date'].apply(lambda date:date.year)**#data visualization house price vs months and years**
fig = plt.figure(figsize=(16,5))
fig.add_subplot(1,2,1)
Data.groupby('month').mean()['price'].plot()
fig.add_subplot(1,2,2)
Data.groupby('year').mean()['price'].plot()
```

![](img/1249e52c797f1449a6c21ab4730e0b13.png)

æˆ¿ä»·ä¸æœˆä»½å’Œå¹´ä»½

è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰ç©ºæ•°æ®ï¼Œå¹¶åˆ é™¤ä¸€äº›æˆ‘ä»¬ä¸éœ€è¦çš„åˆ—(è¿™ä¸ªæ•°æ®é›†æ²¡æœ‰ä¸€äº›ä¸¢å¤±çš„å€¼)

```
**# check if there are any Null values**
Data.isnull().sum()**# drop some unnecessary columns** Data = Data.drop('date',axis=1)
Data = Data.drop('id',axis=1)
Data = Data.drop('zipcode',axis=1)
```

# æ­¥éª¤ 2:æ•°æ®é›†å‡†å¤‡(æ‹†åˆ†å’Œç¼©æ”¾)

æ•°æ®åˆ†ä¸º`Train`ç»„å’Œ`Test`ç»„ã€‚æˆ‘ä»¬ä½¿ç”¨`Train`é›†åˆè®©ç®—æ³•å­¦ä¹ æ•°æ®çš„è¡Œä¸ºï¼Œç„¶ååœ¨`Test`é›†åˆä¸Šæ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

*   ç‰¹æ€§(`X`):æ’å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­çš„åˆ—å°†ç”¨äºè¿›è¡Œé¢„æµ‹ã€‚
*   é¢„æµ‹(`y`):ç‰¹å¾é¢„æµ‹çš„ç›®æ ‡å˜é‡

```
X = Data.drop('price',axis =1).values
y = Data['price'].values**#splitting Train and Test** from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)
```

ç‰¹å¾ç¼©æ”¾å°†å¸®åŠ©æˆ‘ä»¬ä»ç›¸åŒçš„é•œå¤´(ç›¸åŒçš„æ¯”ä¾‹)çœ‹åˆ°æ‰€æœ‰çš„å˜é‡ï¼Œå®ƒè¿˜å°†å¸®åŠ©æˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å¾—æ›´å¿«ã€‚

```
**#standardization scaler - fit&transform on train, fit only on test** from sklearn.preprocessing import StandardScaler
s_scaler = StandardScaler()
X_train = s_scaler.fit_transform(X_train.astype(np.float))
X_test = s_scaler.transform(X_test.astype(np.float))
```

# æ­¥éª¤ 3:æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°

# ğŸ’¡**æ¨¡å‹ 1:** å¤šå…ƒçº¿æ€§å›å½’

å¤šå…ƒçº¿æ€§å›å½’æ˜¯ç®€å•çº¿æ€§å›å½’çš„å»¶ä¼¸(æ­¤å¤„é˜…è¯»æ›´å¤š)å¹¶å‡è®¾å› å˜é‡`Y`å’Œè‡ªå˜é‡`X`ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»

è®©æˆ‘ä»¬åœ¨å›å½’æ¨¡å‹ä¸­æ€»ç»“è®­ç»ƒè¿‡ç¨‹:

```
**# Multiple Liner Regression** from sklearn.linear_model import LinearRegression
regressor = LinearRegression()  
regressor.fit(X_train, y_train)**#evaluate the model (intercept and slope)** print(regressor.intercept_)
print(regressor.coef_)**#predicting the test set result**
y_pred = regressor.predict(X_test)**#put results as a DataFrame**
coeff_df = pd.DataFrame(regressor.coef_, Data.drop('price',axis =1).columns, columns=['Coefficient']) 
coeff_df
```

é€šè¿‡å¯è§†åŒ–æ®‹å·®ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ­£æ€åˆ†å¸ƒ(è¯æ˜ä¸å› å˜é‡å…·æœ‰çº¿æ€§å…³ç³»)

```
**# visualizing residuals** fig = plt.figure(figsize=(10,5))
residuals = (y_test- y_pred)
sns.distplot(residuals)
```

![](img/c9a2eefa99bde1f279da1d26bbd053d6.png)

å‰©ä½™å¯è§†åŒ–

è®©æˆ‘ä»¬æ¯”è¾ƒå®é™…äº§é‡å’Œé¢„æµ‹å€¼ï¼Œä»¥è¡¡é‡æˆ‘ä»¬çš„é¢„æµ‹ä¸å®é™…æˆ¿ä»·æœ‰å¤šè¿œã€‚

```
**#compare actual output values with predicted values**
y_pred = regressor.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df1 = df.head(10)
df1**# evaluate the performance of the algorithm (MAE - MSE - RMSE)** from sklearn import metricsprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))print('VarScore:',metrics.explained_variance_score(y_test,y_pred))
```

![](img/34b63a57a89f919b75627caf161f9595.png)

å¤šå…ƒçº¿æ€§å›å½’ç»“æœ

# ğŸ’¡æ¨¡å‹ 2: Keras å›å½’

è®©æˆ‘ä»¬ä¸ºå›å½’é—®é¢˜åˆ›å»ºä¸€ä¸ªåŸºçº¿ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä»æ‰€æœ‰éœ€è¦çš„å‡½æ•°å’Œå¯¹è±¡å¼€å§‹ã€‚

```
**# Creating a Neural Network Model** from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
```

ç”±äºæˆ‘ä»¬æœ‰ 19 ä¸ªç‰¹å¾ï¼Œè®©æˆ‘ä»¬æ’å…¥ 19 ä¸ªç¥ç»å…ƒä½œä¸ºå¼€å§‹ï¼Œ4 ä¸ªéšè—å±‚å’Œ 1 ä¸ªè¾“å‡ºå±‚ç”±äºé¢„æµ‹æˆ¿ä»·ã€‚

æ­¤å¤–ï¼ŒADAM ä¼˜åŒ–ç®—æ³•ç”¨äºä¼˜åŒ–æŸå¤±å‡½æ•°(å‡æ–¹è¯¯å·®)

```
**# having 19 neuron is based on the number of available features** model = Sequential()
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))model.compile(optimizer='Adam',loss='mes')
```

ç„¶åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œ 400 ä¸ªå†å…ƒçš„è®­ç»ƒï¼Œæ¯æ¬¡éƒ½åœ¨å†å²å¯¹è±¡ä¸­è®°å½•è®­ç»ƒå’ŒéªŒè¯ç²¾åº¦ã€‚ä¸ºäº†è·Ÿè¸ªæ¨¡å‹åœ¨æ¯ä¸ªæ—¶æœŸçš„è¡¨ç°ï¼Œæ¨¡å‹å°†åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­è¿è¡Œï¼ŒåŒæ—¶è®¡ç®—æŸå¤±å‡½æ•°ã€‚

```
model.fit(x=X_train,y=y_train,
          validation_data=(X_test,y_test),
          batch_size=128,epochs=400)model.summary()
```

![](img/3a94d7eb618de62b78c8423ba4c89f7e.png)

```
loss_df = pd.DataFrame(model.history.history)
loss_df.plot(figsize=(12,8))
```

![](img/138daf4959437c5fb73f2af20afb4092.png)

# æµ‹è¯•æ•°æ®è¯„ä¼°

```
y_pred = model.predict(X_test)from sklearn import metricsprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))print('VarScore:',metrics.explained_variance_score(y_test,y_pred))**# Visualizing Our predictions** fig = plt.figure(figsize=(10,5))
plt.scatter(y_test,y_pred)
**# Perfect predictions** plt.plot(y_test,y_test,'r')
```

![](img/c88f05a882dedcd8bde405ff576e108e.png)

```
**# visualizing residuals** fig = plt.figure(figsize=(10,5))
residuals = (y_test- y_pred)
sns.distplot(residuals)
```

![](img/ccfa70b8406c25b1084fda0f56bf51c7.png)

# Keras å›å½’ vs å¤šå…ƒçº¿æ€§å›å½’ï¼

æˆ‘ä»¬æˆåŠŸäº†ï¼ğŸ’ª
æˆ‘ä»¬ç”¨ä¸¤ç§ä¸åŒçš„ ML æ¨¡å‹ç®—æ³•é¢„æµ‹äº†æˆ¿ä»·ã€‚

æˆ‘ä»¬çš„**å¤šå…ƒçº¿æ€§å›å½’**çš„å¾—åˆ†åœ¨ 69%å·¦å³ï¼Œæ‰€ä»¥è¿™ä¸ªæ¨¡å‹è¿˜æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚ç„¶åæˆ‘ä»¬ç”¨ **Keras å›å½’æ¨¡å‹**å¾—åˆ°äº†çº¦ 81%çš„å‡†ç¡®ç‡ã€‚
å¦å¤–ï¼Œè¯·æ³¨æ„ï¼ŒKeras å›å½’æ¨¡å‹çš„ RMSE(æŸå¤±å‡½æ•°)è¾ƒä½ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„é¢„æµ‹æ›´æ¥è¿‘å®é™…è¯„çº§ä»·æ ¼ã€‚

![](img/6dec11542b36c22f35e06deb868f32ef.png)

ç»“æœ:å–€æ‹‰æ–¯åŒºã€‚vs å¤šä¸ªçº¿æ€§ç¨³å‹å™¨ã€‚

æ¯«æ— ç–‘é—®ï¼Œè¿™ä¸ªåˆ†æ•°å¯ä»¥é€šè¿‡ç‰¹å¾é€‰æ‹©æˆ–ä½¿ç”¨å…¶ä»–å›å½’æ¨¡å‹æ¥æé«˜ã€‚

æ„Ÿè°¢æ‚¨çš„é˜…è¯»ğŸ¤“ã€‚å†æ¬¡åé¦ˆæ€»æ˜¯å—æ¬¢è¿çš„ï¼

![](img/5752c5da744c2897e8c79719cf133d57.png)