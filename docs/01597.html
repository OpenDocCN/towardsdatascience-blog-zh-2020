<html>
<head>
<title>Speech Recognition with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python进行语音识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-speech-recognition-in-python-programs-a64851ad29b3?source=collection_archive---------18-----------------------#2020-02-13">https://towardsdatascience.com/automatic-speech-recognition-in-python-programs-a64851ad29b3?source=collection_archive---------18-----------------------#2020-02-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e34574c161e95e0df547c92d2db34cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X90jc9fD9AbgcTfG3IBT2g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://slanglabs.in" rel="noopener ugc nofollow" target="_blank">俚语实验室</a></p></figure><h2 id="5364" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">语音识别</h2><div class=""/><div class=""><h2 id="ca6d" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">比较9个最突出的选择。</h2></div><p id="a8d0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">语音识别技术在过去的几年里发展迅速，并且正在从科学领域过渡到工程领域。随着Alexa、Siri和Google Assistant等语音助手的日益流行，一些应用程序(例如，<a class="ae jg" href="https://medium.com/slanglabs/voice-in-apps-youtube-25bcc288ac4c" rel="noopener"> YouTube </a>、<a class="ae jg" href="https://medium.com/slanglabs/voice-in-apps-gaana-1f6e2d8b026b" rel="noopener">加纳王国</a>、<a class="ae jg" href="https://medium.com/slanglabs/voice-in-apps-paytm-travel-5bee6aea76dc" rel="noopener"> Paytm Travel </a>、<a class="ae jg" href="https://medium.com/slanglabs/voice-in-apps-my-jio-5dc8f2e298d" rel="noopener"> My Jio </a>)开始拥有语音控制的功能。在Slang Labs，我们正在为程序员构建一个平台，让他们能够轻松地用语音体验来增强现有的应用程序。</p><p id="63cf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">自动语音识别(ASR)是处理语音的必要的第一步。在ASR中，对麦克风说出的音频文件或语音经过处理并转换为文本，因此也称为语音到文本(STT)。然后，该文本被馈送到自然语言处理/理解(NLP/NLU)以理解和提取关键信息(例如意图、情感)，然后采取适当的行动。ASR也有独立的应用，例如记录口述，或为视频制作实时字幕。</p><p id="036a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们对ASR和NLU感兴趣，尤其是它们在应用程序中语音到动作循环的功效。我们的<a class="ae jg" href="https://docs.slanglabs.in/" rel="noopener ugc nofollow" target="_blank"> Android和Web SDK</a>提供了从应用程序员的角度来看合适的简单API，而俚语平台则处理将ASR、NLU和文本到语音(TTS)缝合在一起的复杂性负担。但是，很自然地，我们对ASR、NLU和TTS的技术状态感到好奇，尽管我们没有将我们技术堆栈的这些部分作为单独的SaaS产品来展示。对现有ASR解决方案的探索就是这种好奇心的结果。</p><h1 id="2b68" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">服务与软件</h1><p id="6443" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">有两种可能性:在云上调用语音转文本SaaS，或者在应用程序中托管一个ASR软件包。</p><p id="0a16" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">服务</strong>是最容易开始的方式。你必须报名参加SaaS并获得钥匙/证书。然后，您就可以在代码中使用它了，或者通过HTTP端点，或者通过您选择的编程语言中的库。然而，对于合理的大量使用，它通常花费更多的钱。</p><p id="4533" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">软件</strong>软件包在您托管它时为您提供完全的控制，还可以为您的应用创建更小的模型，并将其部署在设备/边缘上，而无需网络连接。但训练和部署模型需要专业知识和前期努力。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/e501bd64c67fdc4bfadd9dbb446657c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hbKFxnJpAofYLE8Hi1ahRw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">使用语音云服务与自托管ASR软件包的权衡</p></figure><p id="bd3a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一个可逆的选择。例如，您可以从云服务开始，如果需要，迁移到您自己的软件包部署；反之亦然。您可以设计您的代码来限制这种反转的爆炸半径，以及在您迁移到另一个SaaS或软件包的情况下。</p><h1 id="3daa" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">批处理与流式</h1><p id="a046" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">您需要确定您的应用程序需要批处理ASR还是流式ASR。</p><p id="04ef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">批处理:</strong>如果你有需要离线转录的录音，那么批处理就足够了，而且更经济。在batch API中，音频文件作为参数传递，语音到文本的转换一次性完成。</p><p id="0503" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">流媒体:</strong>如果你需要实时处理语音(比如声控应用，视频字幕)，你就需要一个流媒体API。在流式API的情况下，使用音频缓冲区的可用块重复调用它。它可能会发送临时结果，但最终结果在最后才可用。</p><p id="5835" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所有的服务和软件包都有批处理API，但是目前有些缺少流式API。因此，如果你有一个流媒体应用程序，这消除了一些选择。</p><h1 id="c2a3" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">Python的选择</h1><p id="80c1" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">大多数语音服务提供流行编程语言的库。在最坏的情况下，您总是可以使用HTTP端点。对于语音包来说也是如此，这些包带有各种编程语言的绑定。在最坏的情况下，您可以自己创建绑定。所以使用Python没有任何约束。</p><blockquote class="nf"><p id="f495" class="ng nh jj bd ni nj nk nl nm nn no mc dk translated">我为本文选择Python，因为大多数语音云服务和ASR软件包都有Python库。此外，你可以在浏览器中使用它的同伴<a class="ae jg" href="https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> Colab notebook </strong> </a>运行文章的代码片段，而不需要在你的计算机上安装任何东西。</p></blockquote><p id="d954" class="pw-post-body-paragraph lh li jj lj b lk np kt lm ln nq kw lp lq nr ls lt lu ns lw lx ly nt ma mb mc im bi translated">一个常见的用例是从麦克风收集音频，并将缓冲区(批处理或流)传递给语音识别API。不变的是，在这样的转录器中，麦克风是通过<a class="ae jg" href="https://people.csail.mit.edu/hubert/pyaudio/" rel="noopener ugc nofollow" target="_blank"> PyAudio </a>访问的，这是通过<a class="ae jg" href="http://www.portaudio.com/" rel="noopener ugc nofollow" target="_blank"> PortAudio </a>实现的。但是由于麦克风在Colab上不可用，我们将其简化。我们将使用一个完整的音频文件来检查批处理API。对于流式API，我们将把一个音频文件分成块并模拟流。</p><h1 id="adbd" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">如何最好地利用文章的其余部分</h1><p id="a7b7" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">涵盖以下服务和软件包。</p><p id="a9a8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">服务:</strong></p><ol class=""><li id="1ee3" class="nu nv jj lj b lk ll ln lo lq nw lu nx ly ny mc nz oa ob oc bi translated">谷歌语音转文本</li><li id="8c85" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">微软Azure语音</li><li id="2a53" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">IBM Watson语音测试</li><li id="2dcd" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">亚马逊转录</li><li id="f479" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">细微差别</li></ol><p id="3241" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">软件:</strong></p><ol class=""><li id="9586" class="nu nv jj lj b lk ll ln lo lq nw lu nx ly ny mc nz oa ob oc bi translated">CMU狮身人面像</li><li id="4167" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">Mozilla DeepSpeech</li><li id="8a61" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">卡尔迪</li><li id="fb75" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">脸书wav2字母</li></ol><p id="af8e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于某些特性或限制(在各自的章节中列出)，没有为Amazon Transcribe、Nuance、Kaldi和脸书wav2letter提供代码示例。相反，给出了代码示例和资源的链接。</p><p id="f78b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下一节有常见的实用函数和测试用例。最后一节介绍了Python <code class="fe oi oj ok ol b">SpeechRecognition</code>包，它提供了对几个服务和软件包的批处理API的抽象。</p><p id="2f22" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想对所有的服务和软件包有一个总体的了解，那么请打开<a class="ae jg" href="https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab </a>，在你阅读这篇文章的时候执行代码。如果您只对特定的服务或套餐感兴趣，请直接跳到该部分。但是不管是哪种情况，都要使用Colab中的代码来更好地探索它。</p><p id="9140" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们深入研究代码。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="f582" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">通用设置</h1><p id="1e9b" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">下载我们将用于测试语音识别服务和软件包的音频文件:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="e2ad" class="pc me jj ol b gy pd pe l pf pg">$ curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/audio-0.6.0.tar.gz<br/><br/>$ tar -xvzf audio-0.6.0.tar.gz<br/><br/>$ ls -l ./audio/</span></pre><p id="e466" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它有三个音频文件。用需要的元数据定义测试用例:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="a84a" class="pc me jj ol b gy pd pe l pf pg">TESTCASES <strong class="ol jt">=</strong> [<br/>  {<br/>    'filename': 'audio/2830-3980-0043.wav',<br/>    'text': 'experience proves this',<br/>    'encoding': 'LINEAR16',<br/>    'lang': 'en-US'<br/>  },<br/>  {<br/>    'filename': 'audio/4507-16021-0012.wav',<br/>    'text': 'why should one halt on the way',<br/>    'encoding': 'LINEAR16',<br/>    'lang': 'en-US'<br/>  },<br/>  {<br/>    'filename': 'audio/8455-210777-0068.wav',<br/>    'text': 'your power is sufficient i said',<br/>    'encoding': 'LINEAR16',<br/>    'lang': 'en-US'<br/>  }<br/>]</span></pre><p id="117a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另外，写一些实用函数。<code class="fe oi oj ok ol b">read_wav_file()</code>获取音频文件的路径，并返回缓冲字节和采样率:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="118f" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">def</strong> <strong class="ol jt">read_wav_file</strong>(filename) <strong class="ol jt">-&gt;</strong> Tuple[bytes, int]:<br/>    <strong class="ol jt">with</strong> wave<strong class="ol jt">.</strong>open(filename, 'rb') <strong class="ol jt">as</strong> w:<br/>        rate <strong class="ol jt">=</strong> w<strong class="ol jt">.</strong>getframerate()<br/>        frames <strong class="ol jt">=</strong> w<strong class="ol jt">.</strong>getnframes()<br/>        buffer <strong class="ol jt">=</strong> w<strong class="ol jt">.</strong>readframes(frames)<br/><br/>    <strong class="ol jt">return</strong> buffer, rate</span></pre><p id="d5d0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oi oj ok ol b">simulate_stream()</code>对于模拟steam来尝试流式API很有用。通常，会有一个类似麦克风的音频源。每隔一段时间，麦克风将生成一个语音块，该语音块必须传递给流API。<code class="fe oi oj ok ol b">simulate_stream()</code>函数有助于避免所有的复杂性，并专注于API。它需要一个音频缓冲区和批处理大小，<em class="ph">生成该大小的</em>块。请注意下面的<code class="fe oi oj ok ol b">yield buf</code>语句:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="28f4" class="pc me jj ol b gy pd pe l pf pg">def simulate_stream(buffer: bytes, batch_size: int = 4096):<br/>    buffer_len = len(buffer)<br/>    offset = 0<br/>    while offset &lt; buffer_len:<br/>        end_offset = offset + batch_size<br/>        buf = buffer[offset:end_offset]<br/>        yield buf<br/>        offset = end_offset</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="4877" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">谷歌语音转文本</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/92290ce55f3bf5322fdd5abe9e14b8de.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*jXx0WUanqCDsRs-HcaAkfA.png"/></div></figure><p id="1466" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谷歌将<a class="ae jg" href="https://cloud.google.com/speech-to-text/docs" rel="noopener ugc nofollow" target="_blank">语音转文本</a>作为谷歌云服务之一。它有C#、Go、Java、JavaScript、PHP、Python和Ruby的<a class="ae jg" href="https://cloud.google.com/speech-to-text/docs/reference/libraries" rel="noopener ugc nofollow" target="_blank">库</a>。它支持批处理和流模式。</p><h2 id="47ea" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">设置</h2><p id="6962" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">你将需要你的<a class="ae jg" href="https://developers.google.com/accounts/docs/application-default-credentials" rel="noopener ugc nofollow" target="_blank">谷歌云证书</a>。您需要设置指向cred文件的环境变量<code class="fe oi oj ok ol b">GOOGLE_APPLICATION_CREDENTIALS</code>:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="8671" class="pc me jj ol b gy pd pe l pf pg">$ export GOOGLE_APPLICATION_CREDENTIALS<strong class="ol jt">=</strong>'/path/to/google/cloud/cred/file/gc-creds.json'<br/>$ ls -l $GOOGLE_APPLICATION_CREDENTIALS</span></pre><h2 id="b105" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="74ff" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">使用批量语音转文本API很简单。您需要创建一个<code class="fe oi oj ok ol b">SpeechClient</code>，创建一个带有音频元数据的<code class="fe oi oj ok ol b">config</code>，并调用语音客户端的<code class="fe oi oj ok ol b">recognize()</code>方法。</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="c7fc" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">from</strong> google.cloud <strong class="ol jt">import</strong> speech_v1<br/><strong class="ol jt">from</strong> google.cloud.speech_v1 <strong class="ol jt">import</strong> enums<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">google_batch_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    client <strong class="ol jt">=</strong> speech_v1<strong class="ol jt">.</strong>SpeechClient()<br/><br/>    config <strong class="ol jt">=</strong> {<br/>        'language_code': lang,<br/>        'sample_rate_hertz': rate,<br/>        'encoding': enums<strong class="ol jt">.</strong>RecognitionConfig<strong class="ol jt">.</strong>AudioEncoding[encoding]<br/>    }<br/><br/>    audio <strong class="ol jt">=</strong> {<br/>        'content': buffer<br/>    }<br/><br/>    response <strong class="ol jt">=</strong> client<strong class="ol jt">.</strong>recognize(config, audio)<br/>    <em class="ph"># For bigger audio file, replace previous line with following:<br/></em>    <em class="ph"># operation = client.long_running_recognize(config, audio)<br/></em>    <em class="ph"># response = operation.result()<br/></em><br/>    <strong class="ol jt">for</strong> result <strong class="ol jt">in</strong> response<strong class="ol jt">.</strong>results:<br/>        <em class="ph"># First alternative is the most probable result<br/></em>        alternative <strong class="ol jt">=</strong> result<strong class="ol jt">.</strong>alternatives[0]<br/>        <strong class="ol jt">return</strong> alternative<strong class="ol jt">.</strong>transcript<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('google-cloud-batch-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        google_batch_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="911a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当您运行它时，您将在输出中看到每个音频测试文件的文本:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="8236" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>google-cloud-batch-stt: "experience proves this"<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>google-cloud-batch-stt: "why should one halt on the way"<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>google-cloud-batch-stt: "your power is sufficient I said"</span></pre><h2 id="3db2" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">流式API</h2><p id="1827" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">谷歌的流媒体API也相当简单。为了处理音频流，您可以使用可用的音频块重复调用流API，它将返回临时结果:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="f028" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">from</strong> google.cloud <strong class="ol jt">import</strong> speech<br/><strong class="ol jt">from</strong> google.cloud.speech <strong class="ol jt">import</strong> enums<br/><strong class="ol jt">from</strong> google.cloud.speech <strong class="ol jt">import</strong> types<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">response_stream_processor</strong>(responses):<br/>    <strong class="ol jt">print</strong>('interim results: ')<br/><br/>    transcript <strong class="ol jt">=</strong> ''<br/>    num_chars_printed <strong class="ol jt">=</strong> 0<br/>    <strong class="ol jt">for</strong> response <strong class="ol jt">in</strong> responses:<br/>        <strong class="ol jt">if</strong> <strong class="ol jt">not</strong> response<strong class="ol jt">.</strong>results:<br/>            <strong class="ol jt">continue</strong><br/><br/>        result <strong class="ol jt">=</strong> response<strong class="ol jt">.</strong>results[0]<br/>        <strong class="ol jt">if</strong> <strong class="ol jt">not</strong> result<strong class="ol jt">.</strong>alternatives:<br/>            <strong class="ol jt">continue</strong><br/><br/>        transcript <strong class="ol jt">=</strong> result<strong class="ol jt">.</strong>alternatives[0]<strong class="ol jt">.</strong>transcript<br/>        <strong class="ol jt">print</strong>('{0}final: {1}'<strong class="ol jt">.</strong>format(<br/>            '' <strong class="ol jt">if</strong> result<strong class="ol jt">.</strong>is_final <strong class="ol jt">else</strong> 'not ',<br/>            transcript<br/>        ))<br/><br/>    <strong class="ol jt">return</strong> transcript<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">google_streaming_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/><br/>    client <strong class="ol jt">=</strong> speech<strong class="ol jt">.</strong>SpeechClient()<br/><br/>    config <strong class="ol jt">=</strong> types<strong class="ol jt">.</strong>RecognitionConfig(<br/>        encoding<strong class="ol jt">=</strong>enums<strong class="ol jt">.</strong>RecognitionConfig<strong class="ol jt">.</strong>AudioEncoding[encoding],<br/>        sample_rate_hertz<strong class="ol jt">=</strong>rate,<br/>        language_code<strong class="ol jt">=</strong>lang<br/>    )<br/><br/>    streaming_config <strong class="ol jt">=</strong> types<strong class="ol jt">.</strong>StreamingRecognitionConfig(<br/>        config<strong class="ol jt">=</strong>config,<br/>        interim_results<strong class="ol jt">=</strong>True<br/>    )<br/><br/>    audio_generator <strong class="ol jt">=</strong> simulate_stream(buffer)  <em class="ph"># chunk generator<br/></em>    requests <strong class="ol jt">=</strong> (<br/>        types<strong class="ol jt">.</strong>StreamingRecognizeRequest(audio_content<strong class="ol jt">=</strong>chunk)<br/>        <strong class="ol jt">for</strong> chunk <strong class="ol jt">in</strong> audio_generator<br/>    )<br/>    responses <strong class="ol jt">=</strong> client<strong class="ol jt">.</strong>streaming_recognize(<br/>        streaming_config, requests<br/>    )<br/>    <em class="ph"># Now, put the transcription responses to use.<br/></em>    <strong class="ol jt">return</strong> response_stream_processor(responses)<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('google-cloud-streaming-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        google_streaming_stt(<br/>            t['filename'], t['lang'], t['encoding']<br/>        )<br/>    ))</span></pre><p id="c672" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在输出中，您可以看到随着输入更多音频，效果会有所改善:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="113a" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>interim results: <br/>not final: next<br/>not final: iSpy<br/>not final: Aspira<br/>not final: Xperia<br/>not final: Experian<br/>not final: experience<br/>not final: experience proved<br/>not final: experience proves<br/>not final: experience proves the<br/>not final: experience proves that<br/>not final: experience<br/>final: experience proves this<br/>google-cloud-streaming-stt: "experience proves this"</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="0174" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">微软Azure语音</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/48ffdce314fbf6314b4244e5e17ba9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*RQ15ybhKM3ib_E__b16Upg.png"/></div></figure><p id="4404" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">微软Azure认知服务是一个AI服务和认知API的家族。<a class="ae jg" href="https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/" rel="noopener ugc nofollow" target="_blank">语音服务</a>包括<a class="ae jg" href="https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/" rel="noopener ugc nofollow" target="_blank">语音转文本</a>，文本转语音，语音翻译服务。</p><h2 id="7f6b" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">设置</h2><p id="f082" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">安装Azure语音包:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="86ab" class="pc me jj ol b gy pd pe l pf pg">$ pip3 install azure-cognitiveservices-speech</span></pre><p id="a7a4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您可以在<a class="ae jg" href="https://portal.azure.com/" rel="noopener ugc nofollow" target="_blank"> Microsoft Azure portal </a>上启用语音服务并查找您帐户的凭据。你可以在这里开一个免费账户<a class="ae jg" href="https://azure.microsoft.com/en-in/free/ai/" rel="noopener ugc nofollow" target="_blank">。服务凭据:</a></p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="dc7f" class="pc me jj ol b gy pd pe l pf pg">AZURE_SPEECH_KEY <strong class="ol jt">=</strong> 'YOUR AZURE SPEECH KEY'<br/>AZURE_SERVICE_REGION <strong class="ol jt">=</strong> 'YOUR AZURE SERVICE REGION'</span></pre><h2 id="1a76" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="e4a1" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">Azure的批处理API也很简单。它接受一个配置和音频输入，并返回文本:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="911c" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> azure.cognitiveservices.speech <strong class="ol jt">as</strong> speechsdk<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">azure_batch_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    speech_config <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>SpeechConfig(<br/>        subscription<strong class="ol jt">=</strong>AZURE_SPEECH_KEY,<br/>        region<strong class="ol jt">=</strong>AZURE_SERVICE_REGION<br/>    )<br/>    audio_input <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>AudioConfig(filename<strong class="ol jt">=</strong>filename)<br/>    speech_recognizer <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>SpeechRecognizer(<br/>        speech_config<strong class="ol jt">=</strong>speech_config,<br/>        audio_config<strong class="ol jt">=</strong>audio_input<br/>    )<br/>    result <strong class="ol jt">=</strong> speech_recognizer<strong class="ol jt">.</strong>recognize_once()<br/><br/>    <strong class="ol jt">return</strong> result<strong class="ol jt">.</strong>text <strong class="ol jt">if</strong> result<strong class="ol jt">.</strong>reason <strong class="ol jt">==</strong> speechsdk<strong class="ol jt">.</strong>ResultReason<strong class="ol jt">.</strong>RecognizedSpeech <strong class="ol jt">else</strong> None<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('azure-batch-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        azure_batch_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="bdeb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出将如下所示:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="8f4f" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>azure-batch-stt: "Experience proves this."<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>azure-batch-stt: "Whi should one halt on the way."<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>azure-batch-stt: "Your power is sufficient I said."</span></pre><h2 id="dc5c" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">流式API</h2><p id="679f" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">Azure有几种流API。通过创建不同类型的音频源，人们可以推送音频块，或者向Azure传递回调来拉取音频块。它触发几种类型的语音识别事件来连接回调。以下是如何将推送音频流与音频流发生器连接起来:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="df62" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> time<br/><strong class="ol jt">import</strong> azure.cognitiveservices.speech <strong class="ol jt">as</strong> speechsdk<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">azure_streaming_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    speech_config <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>SpeechConfig(<br/>        subscription<strong class="ol jt">=</strong>AZURE_SPEECH_KEY,<br/>        region<strong class="ol jt">=</strong>AZURE_SERVICE_REGION<br/>    )<br/>    stream <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>audio<strong class="ol jt">.</strong>PushAudioInputStream()<br/>    audio_config <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>audio<strong class="ol jt">.</strong>AudioConfig(stream<strong class="ol jt">=</strong>stream)<br/>    speech_recognizer <strong class="ol jt">=</strong> speechsdk<strong class="ol jt">.</strong>SpeechRecognizer(<br/>        speech_config<strong class="ol jt">=</strong>speech_config,<br/>        audio_config<strong class="ol jt">=</strong>audio_config<br/>    )<br/><br/>    <em class="ph"># Connect callbacks to the events fired by the speech recognizer<br/></em>    speech_recognizer<strong class="ol jt">.</strong>recognizing<strong class="ol jt">.</strong>connect(<br/>        <strong class="ol jt">lambda</strong> evt: <strong class="ol jt">print</strong>('interim text: "{}"'<strong class="ol jt">.</strong>format(<br/>            evt<strong class="ol jt">.</strong>result<strong class="ol jt">.</strong>text<br/>        ))<br/>    )<br/>    speech_recognizer<strong class="ol jt">.</strong>recognized<strong class="ol jt">.</strong>connect(<br/>        <strong class="ol jt">lambda</strong> evt:  <strong class="ol jt">print</strong>('azure-streaming-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>            evt<strong class="ol jt">.</strong>result<strong class="ol jt">.</strong>text<br/>        ))<br/>    )<br/><br/>    <em class="ph"># start continuous speech recognition<br/></em>    speech_recognizer<strong class="ol jt">.</strong>start_continuous_recognition()<br/><br/>    <em class="ph"># push buffer chunks to stream<br/></em>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    audio_generator <strong class="ol jt">=</strong> simulate_stream(buffer)<br/>    <strong class="ol jt">for</strong> chunk <strong class="ol jt">in</strong> audio_generator:<br/>      stream<strong class="ol jt">.</strong>write(chunk)<br/>      time<strong class="ol jt">.</strong>sleep(0.1)  <em class="ph"># to give callback a chance against fast loop<br/></em><br/>    <em class="ph"># stop continuous speech recognition<br/></em>    stream<strong class="ol jt">.</strong>close()<br/>    time<strong class="ol jt">.</strong>sleep(0.5)  <em class="ph"># give chance to VAD to kick in<br/></em>    speech_recognizer<strong class="ol jt">.</strong>stop_continuous_recognition()<br/>    time<strong class="ol jt">.</strong>sleep(0.5)  <em class="ph"># Let all callback run<br/></em><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    azure_streaming_stt(t['filename'], t['lang'], t['encoding'])</span></pre><p id="4d51" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一个测试用例的输出如下所示:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="8615" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>interim text: "experience"<br/>interim text: "experienced"<br/>interim text: "experience"<br/>interim text: "experience proves"<br/>interim text: "experience proves this"<br/>azure-streaming-stt: "Experience proves this."</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="f6d7" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">IBM Watson语音转文本</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/3530daad4d5bf9381465a8deacb1bc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*x66VME3xLLuHkJtgowAQjw.png"/></div></figure><p id="fd53" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">IBM <a class="ae jg" href="https://www.ibm.com/in-en/cloud/watson-speech-to-text" rel="noopener ugc nofollow" target="_blank"> Watson语音转文本</a>是一项ASR服务。NET，Go，JavaScript，<a class="ae jg" href="https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text?code=python" rel="noopener ugc nofollow" target="_blank"> Python </a>，Ruby，Swift，Unity API库，还有HTTP端点。它有丰富的文档。</p><h2 id="d977" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">设置</h2><p id="c5e5" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">您需要<a class="ae jg" href="https://cloud.ibm.com/docs/services/text-to-speech?topic=text-to-speech-gettingStarted" rel="noopener ugc nofollow" target="_blank">注册/登录</a>，获取API密钥凭证和服务URL，并填写在下面。</p><h2 id="ba9d" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="3aef" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">可以预见，批处理API非常简单:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="e432" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> os<br/><br/><strong class="ol jt">from</strong> ibm_watson <strong class="ol jt">import</strong> SpeechToTextV1<br/><strong class="ol jt">from</strong> ibm_cloud_sdk_core.authenticators <strong class="ol jt">import</strong> IAMAuthenticator<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">watson_batch_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    authenticator <strong class="ol jt">=</strong> IAMAuthenticator(WATSON_API_KEY)<br/>    speech_to_text <strong class="ol jt">=</strong> SpeechToTextV1(authenticator<strong class="ol jt">=</strong>authenticator)<br/>    speech_to_text<strong class="ol jt">.</strong>set_service_url(WATSON_STT_URL)<br/><br/>    <strong class="ol jt">with</strong> open(filename, 'rb') <strong class="ol jt">as</strong> audio_file:<br/>        response <strong class="ol jt">=</strong> speech_to_text<strong class="ol jt">.</strong>recognize(<br/>            audio<strong class="ol jt">=</strong>audio_file,<br/>            content_type<strong class="ol jt">=</strong>'audio/{}'<strong class="ol jt">.</strong>format(<br/>                os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>splitext(filename)[1][1:]<br/>            ),<br/>            model<strong class="ol jt">=</strong>lang <strong class="ol jt">+</strong> '_BroadbandModel',<br/>            max_alternatives<strong class="ol jt">=</strong>3,<br/>        )<strong class="ol jt">.</strong>get_result()<br/><br/>    <strong class="ol jt">return</strong> response['results'][0]['alternatives'][0]['transcript']<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('watson-batch-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        watson_batch_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="602f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下是输出:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="4c71" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>watson-batch-stt: "experience proves this "<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>watson-batch-stt: "why should one hold on the way "<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>watson-batch-stt: "your power is sufficient I set "</span></pre><h2 id="dfcb" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">流式API</h2><p id="dc6c" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">Watson的流媒体API在WebSocket上工作，只需要一点点工作就可以设置好。它具有以下步骤:</p><ul class=""><li id="de5a" class="nu nv jj lj b lk ll ln lo lq nw lu nx ly ny mc pt oa ob oc bi translated">创建一个用于接收语音识别通知和结果的<code class="fe oi oj ok ol b">RecognizeCallback</code>对象。</li><li id="3b6f" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc pt oa ob oc bi translated">创建缓冲队列。麦克风(或流模拟器)产生的音频块应该写入该队列，Watson读取并使用这些块。</li><li id="d03d" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc pt oa ob oc bi translated">启动一个执行语音识别(以及WebSocket通信)的线程。</li><li id="3896" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc pt oa ob oc bi translated">启动麦克风或语音模拟器，开始产生音频块</li><li id="4fb3" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc pt oa ob oc bi translated">完成后，加入语音识别线程(即，等待它完成)。</li></ul><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="e803" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> json<br/><strong class="ol jt">import</strong> logging<br/><strong class="ol jt">import</strong> os<br/><strong class="ol jt">from</strong> queue <strong class="ol jt">import</strong> Queue<br/><strong class="ol jt">from</strong> threading <strong class="ol jt">import</strong> Thread<br/><strong class="ol jt">import</strong> time<br/><br/><strong class="ol jt">from</strong> ibm_watson <strong class="ol jt">import</strong> SpeechToTextV1<br/><strong class="ol jt">from</strong> ibm_watson.websocket <strong class="ol jt">import</strong> RecognizeCallback, AudioSource<br/><strong class="ol jt">from</strong> ibm_cloud_sdk_core.authenticators <strong class="ol jt">import</strong> IAMAuthenticator<br/><br/><em class="ph"># Watson websocket prints justs too many debug logs, so disable it<br/></em>logging<strong class="ol jt">.</strong>disable(logging<strong class="ol jt">.</strong>CRITICAL)<br/><br/><em class="ph"># Chunk and buffer size<br/></em>CHUNK_SIZE <strong class="ol jt">=</strong> 4096<br/>BUFFER_MAX_ELEMENT <strong class="ol jt">=</strong> 10<br/><br/><em class="ph"># A callback class to process various streaming STT events<br/></em><strong class="ol jt">class</strong> <strong class="ol jt">MyRecognizeCallback</strong>(RecognizeCallback):<br/>    <strong class="ol jt">def</strong> <strong class="ol jt">__init__</strong>(self):<br/>        RecognizeCallback<strong class="ol jt">.</strong>__init__(self)<br/>        self<strong class="ol jt">.</strong>transcript <strong class="ol jt">=</strong> None<br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_transcription</strong>(self, transcript):<br/>        <em class="ph"># print('transcript: {}'.format(transcript))<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_connected</strong>(self):<br/>        <em class="ph"># print('Connection was successful')<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_error</strong>(self, error):<br/>        <em class="ph"># print('Error received: {}'.format(error))<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_inactivity_timeout</strong>(self, error):<br/>        <em class="ph"># print('Inactivity timeout: {}'.format(error))<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_listening</strong>(self):<br/>        <em class="ph"># print('Service is listening')<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_hypothesis</strong>(self, hypothesis):<br/>        <em class="ph"># print('hypothesis: {}'.format(hypothesis))<br/></em>        <strong class="ol jt">pass</strong><br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_data</strong>(self, data):<br/>        self<strong class="ol jt">.</strong>transcript <strong class="ol jt">=</strong> data['results'][0]['alternatives'][0]['transcript']<br/>        <strong class="ol jt">print</strong>('{0}final: {1}'<strong class="ol jt">.</strong>format(<br/>            '' <strong class="ol jt">if</strong> data['results'][0]['final'] <strong class="ol jt">else</strong> 'not ',<br/>            self<strong class="ol jt">.</strong>transcript<br/>        ))<br/><br/>    <strong class="ol jt">def</strong> <strong class="ol jt">on_close</strong>(self):<br/>        <em class="ph"># print("Connection closed")<br/></em>        <strong class="ol jt">pass</strong><br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">watson_streaming_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    authenticator <strong class="ol jt">=</strong> IAMAuthenticator(WATSON_API_KEY)<br/>    speech_to_text <strong class="ol jt">=</strong> SpeechToTextV1(authenticator<strong class="ol jt">=</strong>authenticator)<br/>    speech_to_text<strong class="ol jt">.</strong>set_service_url(WATSON_STT_URL)<br/><br/>    <em class="ph"># Make watson audio source fed by a buffer queue<br/></em>    buffer_queue <strong class="ol jt">=</strong> Queue(maxsize<strong class="ol jt">=</strong>BUFFER_MAX_ELEMENT)<br/>    audio_source <strong class="ol jt">=</strong> AudioSource(buffer_queue, True, True)<br/><br/>    <em class="ph"># Callback object<br/></em>    mycallback <strong class="ol jt">=</strong> MyRecognizeCallback()<br/><br/>    <em class="ph"># Read the file<br/></em>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/><br/>    <em class="ph"># Start Speech-to-Text recognition thread<br/></em>    stt_stream_thread <strong class="ol jt">=</strong> Thread(<br/>        target<strong class="ol jt">=</strong>speech_to_text<strong class="ol jt">.</strong>recognize_using_websocket,<br/>        kwargs<strong class="ol jt">=</strong>{<br/>            'audio': audio_source,<br/>            'content_type': 'audio/l16; rate={}'<strong class="ol jt">.</strong>format(rate),<br/>            'recognize_callback': mycallback,<br/>            'interim_results': True<br/>        }<br/>    )<br/>    stt_stream_thread<strong class="ol jt">.</strong>start()<br/><br/>    <em class="ph"># Simulation audio stream by breaking file into chunks and filling buffer queue<br/></em>    audio_generator <strong class="ol jt">=</strong> simulate_stream(buffer, CHUNK_SIZE)<br/>    <strong class="ol jt">for</strong> chunk <strong class="ol jt">in</strong> audio_generator:<br/>        buffer_queue<strong class="ol jt">.</strong>put(chunk)<br/>        time<strong class="ol jt">.</strong>sleep(0.5)  <em class="ph"># give a chance to callback<br/></em><br/>    <em class="ph"># Close the audio feed and wait for STTT thread to complete<br/></em>    audio_source<strong class="ol jt">.</strong>completed_recording()<br/>    stt_stream_thread<strong class="ol jt">.</strong>join()<br/><br/>    <em class="ph"># send final result<br/></em>    <strong class="ol jt">return</strong> mycallback<strong class="ol jt">.</strong>transcript<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('watson-cloud-streaming-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        watson_streaming_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="4937" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">产出:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="cf01" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>not final: X. <br/>not final: experts <br/>not final: experience <br/>not final: experienced <br/>not final: experience prove <br/>not final: experience proves <br/>not final: experience proves that <br/>not final: experience proves this <br/>final: experience proves this <br/>watson-cloud-streaming-stt: "experience proves this "</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="6b0f" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">亚马逊转录</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pu"><img src="../Images/21e21035b015d09cdee56a191d288e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKGHgxkHGMfoOf1GwBEm7g.jpeg"/></div></div></figure><p id="b7ce" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://aws.amazon.com/transcribe/" rel="noopener ugc nofollow" target="_blank">Amazon transcript</a>是一个<a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/getting-started.html" rel="noopener ugc nofollow" target="_blank">语音转文本</a> AWS云服务，拥有C#、Go、Java、JavaScript、PHP、Python和Ruby的<a class="ae jg" href="https://aws.amazon.com/transcribe/resources/" rel="noopener ugc nofollow" target="_blank">库</a>。它有一个<a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-python.html" rel="noopener ugc nofollow" target="_blank">批处理</a>语音转文本API(也可以作为<a class="ae jg" href="https://docs.aws.amazon.com/cli/latest/reference/transcribe/start-transcription-job.html" rel="noopener ugc nofollow" target="_blank">命令行</a>使用)，但是它要求音频文件要么在S3桶中，要么可以通过HTTP获得。它在<a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/websocket.html" rel="noopener ugc nofollow" target="_blank"> WebSocket </a>和<a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/how-streaming.html" rel="noopener ugc nofollow" target="_blank"> HTTP/2 </a>上也有一个流媒体API。这里有一个使用AWS Java SDK 的<a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-streaming.html" rel="noopener ugc nofollow" target="_blank">示例，但是没有Python绑定(当然可以使用Python套接字库，但是需要进入</a><a class="ae jg" href="https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html" rel="noopener ugc nofollow" target="_blank">低级事件流编码</a>)。</p><p id="2d2b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Amazon Transcribe Python APIs目前不支持本文中涉及的用例，因此这里不包括代码示例。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="2875" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">细微差别</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/e5f189b725cae156e0a99e4f263d63e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*46A1FV2xt27anSs6la-YXQ.png"/></div></figure><p id="2370" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Nuance很可能是最古老的<a class="ae jg" href="https://www.nuance.com/dragon.html" rel="noopener ugc nofollow" target="_blank">商业语音识别产品</a>，甚至是为各种领域和行业定制的。他们确实有语音识别服务的Python绑定。这里是他们GitHub repo中的一个<a class="ae jg" href="https://github.com/NuanceDev/ndev-python-http-cli/blob/master/ndev/asr.py" rel="noopener ugc nofollow" target="_blank">代码样本</a>。</p><p id="64a1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我想不出创建开发者账户的方法。我希望有一种方法可以获得类似于其他产品的有限期限的免费试用信用，并获得访问服务所需的凭据。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="6337" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">CMU狮身人面像</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pv"><img src="../Images/9cb0587b08fd8dae439adf4f159b1dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvWUNMda9jWr-oXuDwyWVw.png"/></div></div></figure><p id="6a9e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://cmusphinx.github.io/" rel="noopener ugc nofollow" target="_blank"> CMUSphinx </a>已经存在了相当一段时间，并且一直在适应ASR技术的进步。<a class="ae jg" href="https://github.com/cmusphinx/pocketsphinx-python" rel="noopener ugc nofollow" target="_blank"> PocketSphinx </a>是一个语音转文本解码器Python包。</p><h2 id="3814" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">设置</h2><p id="00eb" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">首先安装<a class="ae jg" href="http://www.swig.org/" rel="noopener ugc nofollow" target="_blank"> swig </a>。在macOS上，您可以使用<code class="fe oi oj ok ol b">brew</code>进行安装:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="f9ab" class="pc me jj ol b gy pd pe l pf pg">$ brew install swig<br/>$ swig -version</span></pre><p id="3e12" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在Linux上，可以使用<code class="fe oi oj ok ol b">apt-get</code>:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="f2e0" class="pc me jj ol b gy pd pe l pf pg">$ apt-get install -y swig libpulse-dev<br/>$ swig -version</span></pre><p id="95a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后使用pip安装<code class="fe oi oj ok ol b">pocketsphinx</code>:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="8e87" class="pc me jj ol b gy pd pe l pf pg">$ pip3 install pocketsphinx<br/>$ pip3 list | grep pocketsphinx</span></pre><h2 id="a6c7" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">创建解码器对象</h2><p id="f4bd" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">无论您使用批处理还是流式API，您都需要一个解码器对象:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="9f14" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> pocketsphinx<br/><strong class="ol jt">import</strong> os<br/><br/>MODELDIR <strong class="ol jt">=</strong> os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>join(os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>dirname(pocketsphinx<strong class="ol jt">.</strong>__file__), 'model')<br/><br/>config <strong class="ol jt">=</strong> pocketsphinx<strong class="ol jt">.</strong>Decoder<strong class="ol jt">.</strong>default_config()<br/>config<strong class="ol jt">.</strong>set_string('-hmm', os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>join(MODELDIR, 'en-us'))<br/>config<strong class="ol jt">.</strong>set_string('-lm', os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>join(MODELDIR, 'en-us.lm.bin'))<br/>config<strong class="ol jt">.</strong>set_string('-dict', os<strong class="ol jt">.</strong>path<strong class="ol jt">.</strong>join(MODELDIR, 'cmudict-en-us.dict'))<br/><br/>decoder <strong class="ol jt">=</strong> pocketsphinx<strong class="ol jt">.</strong>Decoder(config)</span></pre><h2 id="1bf7" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="5bf1" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">批处理API非常简单，只有几行代码:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="2441" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">def</strong> <strong class="ol jt">sphinx_batch_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    decoder<strong class="ol jt">.</strong>start_utt()<br/>    decoder<strong class="ol jt">.</strong>process_raw(buffer, False, False)<br/>    decoder<strong class="ol jt">.</strong>end_utt()<br/>    hypothesis <strong class="ol jt">=</strong> decoder<strong class="ol jt">.</strong>hyp()<br/>    <strong class="ol jt">return</strong> hypothesis<strong class="ol jt">.</strong>hypstr<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text'])<br/>    )<br/>    <strong class="ol jt">print</strong>('sphinx-batch-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        sphinx_batch_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="fd3a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您将看到现在熟悉的输出:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="ca43" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>sphinx-batch-stt: "experience proves this"<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>sphinx-batch-stt: "why should one hold on the way"<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>sphinx-batch-stt: "your paris sufficient i said"</span></pre><p id="4802" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意抄写中的错误。随着更多的训练数据，它通常会改善。</p><h2 id="ea39" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">流式API</h2><p id="c1bb" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">流式API也非常简单，但是没有挂钩来获得中间结果:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="b6c7" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">def</strong> <strong class="ol jt">sphinx_streaming_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    audio_generator <strong class="ol jt">=</strong> simulate_stream(buffer)<br/><br/>    decoder<strong class="ol jt">.</strong>start_utt()<br/>    <strong class="ol jt">for</strong> chunk <strong class="ol jt">in</strong> audio_generator:<br/>        decoder<strong class="ol jt">.</strong>process_raw(chunk, False, False)<br/>    decoder<strong class="ol jt">.</strong>end_utt()<br/><br/>    hypothesis <strong class="ol jt">=</strong> decoder<strong class="ol jt">.</strong>hyp()<br/>    <strong class="ol jt">return</strong> hypothesis<strong class="ol jt">.</strong>hypstr<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('sphinx-streaming-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        sphinx_streaming_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="26ee" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">Mozilla DeepSpeech</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pw"><img src="../Images/eaf9f588adb24dd77972c2f734a3b267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RyGOJ6-1P_3Hz_9sVpNN2Q.png"/></div></div></figure><p id="e943" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Mozilla在2019年12月发布了<a class="ae jg" href="https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/" rel="noopener ugc nofollow" target="_blank"> DeepSpeech 0.6 </a>软件包，用C、Java、.NET、<a class="ae jg" href="https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html" rel="noopener ugc nofollow" target="_blank"> Python </a>和JavaScript，包括支持在edge设备上使用TensorFlow Lite模型。</p><h2 id="c6d9" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">设置</h2><p id="9766" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">您可以使用pip安装DeepSpeech(如果您想在Colab运行时或您的机器上使用GPU，请使用它<code class="fe oi oj ok ol b">deepspeech-gpu==0.6.0</code>):</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="04e5" class="pc me jj ol b gy pd pe l pf pg">$ pip install deepspeech<strong class="ol jt">==</strong>0.6.0</span></pre><p id="c145" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下载并解压缩模型(这需要一段时间):</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="ae8e" class="pc me jj ol b gy pd pe l pf pg">$ curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz<br/>$ tar -xvzf deepspeech-0.6.0-models.tar.gz<br/>$ ls -l ./deepspeech-0.6.0-models/</span></pre><p id="c2c0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">测试它是否一切正常。检查最后三个命令的输出，您将分别看到结果<em class="ph">【经验证明少】</em><em class="ph">【为什么要中途停顿】</em><em class="ph">【我说你的动力够用了】</em>。你都准备好了。</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="830f" class="pc me jj ol b gy pd pe l pf pg">$ deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/2830-3980-0043.wav<br/><br/>$ deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/4507-16021-0012.wav<br/><br/>$ deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/8455-210777-0068.wav</span></pre><h2 id="a419" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">创建模型对象</h2><p id="7026" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">第一步是读取模型文件并创建一个DeepSpeech模型对象。</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="fd08" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> deepspeech<br/><br/>model_file_path <strong class="ol jt">=</strong> 'deepspeech-0.6.0-models/output_graph.pbmm'<br/>beam_width <strong class="ol jt">=</strong> 500<br/>model <strong class="ol jt">=</strong> deepspeech<strong class="ol jt">.</strong>Model(model_file_path, beam_width)<br/><br/><em class="ph"># Add language model for better accuracy<br/></em>lm_file_path <strong class="ol jt">=</strong> 'deepspeech-0.6.0-models/lm.binary'<br/>trie_file_path <strong class="ol jt">=</strong> 'deepspeech-0.6.0-models/trie'<br/>lm_alpha <strong class="ol jt">=</strong> 0.75<br/>lm_beta <strong class="ol jt">=</strong> 1.85<br/>model<strong class="ol jt">.</strong>enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)</span></pre><h2 id="c293" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="ee01" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">批量语音转文本只需要几行代码:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="5e43" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> numpy <strong class="ol jt">as</strong> np<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">deepspeech_batch_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    data16 <strong class="ol jt">=</strong> np<strong class="ol jt">.</strong>frombuffer(buffer, dtype<strong class="ol jt">=</strong>np<strong class="ol jt">.</strong>int16)<br/>    <strong class="ol jt">return</strong> model<strong class="ol jt">.</strong>stt(data16)<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('deepspeech-batch-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        deepspeech_batch_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="c350" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="5ba7" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>deepspeech-batch-stt: "experience proof less"<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>deepspeech-batch-stt: "why should one halt on the way"<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>deepspeech-batch-stt: "your power is sufficient i said"</span></pre><h2 id="65c3" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">流式API</h2><p id="640e" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">DeepSpeech streaming API需要创建一个流上下文，并重复使用它来提供音频块:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="7a3a" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">def</strong> <strong class="ol jt">deepspeech_streaming_stt</strong>(filename: str, lang: str, encoding: str) <strong class="ol jt">-&gt;</strong> str:<br/>    buffer, rate <strong class="ol jt">=</strong> read_wav_file(filename)<br/>    audio_generator <strong class="ol jt">=</strong> simulate_stream(buffer)<br/><br/>    <em class="ph"># Create stream<br/></em>    context <strong class="ol jt">=</strong> model<strong class="ol jt">.</strong>createStream()<br/><br/>    text <strong class="ol jt">=</strong> ''<br/>    <strong class="ol jt">for</strong> chunk <strong class="ol jt">in</strong> audio_generator:<br/>        data16 <strong class="ol jt">=</strong> np<strong class="ol jt">.</strong>frombuffer(chunk, dtype<strong class="ol jt">=</strong>np<strong class="ol jt">.</strong>int16)<br/>        <em class="ph"># feed stream of chunks<br/></em>        model<strong class="ol jt">.</strong>feedAudioContent(context, data16)<br/>        interim_text <strong class="ol jt">=</strong> model<strong class="ol jt">.</strong>intermediateDecode(context)<br/>        <strong class="ol jt">if</strong> interim_text <strong class="ol jt">!=</strong> text:<br/>            text <strong class="ol jt">=</strong> interim_text<br/>            <strong class="ol jt">print</strong>('inetrim text: {}'<strong class="ol jt">.</strong>format(text))<br/><br/>    <em class="ph"># get final resut and close stream<br/></em>    text <strong class="ol jt">=</strong> model<strong class="ol jt">.</strong>finishStream(context)<br/>    <strong class="ol jt">return</strong> text<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        t['filename'], t['text']<br/>    ))<br/>    <strong class="ol jt">print</strong>('deepspeech-streaming-stt: "{}"'<strong class="ol jt">.</strong>format(<br/>        deepspeech_streaming_stt(t['filename'], t['lang'], t['encoding'])<br/>    ))</span></pre><p id="001e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">DeepSpeech返回中期结果:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="274c" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>inetrim text: i<br/>inetrim text: e<br/>inetrim text: experi en<br/>inetrim text: experience pro<br/>inetrim text: experience proof les<br/>deepspeech-streaming-stt: "experience proof less"</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="cd88" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">卡尔迪</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/7250087be1d98b7e418654d96f266b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*6EnhsZW-MFi6RL_A9R7HLA.png"/></div></figure><p id="188d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">卡尔迪是一个在研究社区中非常流行的语音识别工具包。它旨在试验不同的研究理念和可能性。它有各种可能的技术和备选方案的丰富集合。与代码实验室中讨论的其他替代方案相比，学习曲线更加陡峭。</p><p id="c599" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://pykaldi.github.io/" rel="noopener ugc nofollow" target="_blank"> PyKaldi </a>提供<a class="ae jg" href="https://pykaldi.github.io/api/kaldi.asr.html" rel="noopener ugc nofollow" target="_blank"> Python绑定</a>。请务必看一下他们的GitHub repo的自述文件。</p><p id="08d8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">没有预构建的PyPI即用包，您必须从源代码或Conda构建它。这两种选择都不适合Colab环境。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="db5f" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">脸书wav2字母</h1><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/204accca6aebfad8a7d1015d881f4c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*nDyqkyAPOMuofQJCbwsUnA.png"/></div></figure><p id="ff68" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">脸书于2020年1月发布了<a class="ae jg" href="https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/" rel="noopener ugc nofollow" target="_blank"> wav2letter@anywhere </a>。它拥有一个完全卷积的(CNN)声学模型，而不是其他解决方案使用的递归神经网络(RNN)。它非常有前途，包括用于边缘设备。它为其<a class="ae jg" href="https://github.com/facebookresearch/wav2letter/wiki/Inference-Framework" rel="noopener ugc nofollow" target="_blank">推理框架</a>提供了<a class="ae jg" href="https://github.com/facebookresearch/wav2letter/wiki/Python-bindings" rel="noopener ugc nofollow" target="_blank"> Python绑定</a>。</p><p id="0f74" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">和Kaldi一样，这也不提供PyPI包，需要从<a class="ae jg" href="https://github.com/facebookresearch/wav2letter/" rel="noopener ugc nofollow" target="_blank">源</a>进行构建和安装。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="b07e" class="md me jj bd mf mg ot mi mj mk ou mm mn ky ov kz mp lb ow lc mr le ox lf mt mu bi translated">语音识别Python包</h1><p id="f08f" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated"><a class="ae jg" href="https://pypi.org/project/SpeechRecognition/" rel="noopener ugc nofollow" target="_blank"> SpeechRecognition </a>包提供了几种解决方案的良好抽象。我们已经探索使用谷歌服务和CMU Sphinxpackage。现在我们将通过SpeechRecognition包API来使用这些API。可以使用pip进行安装:</p><h2 id="eca3" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">批处理API</h2><p id="80e1" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">SpeechRecognition只有批处理API。第一步是从文件或麦克风创建音频记录，第二步是调用<code class="fe oi oj ok ol b">recognize_&lt;speech engine name&gt;</code>函数。它目前有CMU狮身人面像、谷歌、微软、IBM、Houndify和Wit的API。让我们通过SpeechRecognition抽象使用一个云服务(Google)和一个软件包(Sphinx)来结帐。</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="c19b" class="pc me jj ol b gy pd pe l pf pg"><strong class="ol jt">import</strong> speech_recognition <strong class="ol jt">as</strong> sr<br/><strong class="ol jt">from</strong> enum <strong class="ol jt">import</strong> Enum, unique<br/><br/><strong class="ol jt">@</strong>unique<br/><strong class="ol jt">class</strong> <strong class="ol jt">ASREngine</strong>(Enum):<br/>    sphinx <strong class="ol jt">=</strong> 0<br/>    google <strong class="ol jt">=</strong> 1<br/><br/><strong class="ol jt">def</strong> <strong class="ol jt">speech_to_text</strong>(filename: str, engine: ASREngine, language: str, show_all: bool <strong class="ol jt">=</strong> False) <strong class="ol jt">-&gt;</strong> str:<br/>    r <strong class="ol jt">=</strong> sr<strong class="ol jt">.</strong>Recognizer()<br/><br/>    <strong class="ol jt">with</strong> sr<strong class="ol jt">.</strong>AudioFile(filename) <strong class="ol jt">as</strong> source:<br/>        audio <strong class="ol jt">=</strong> r<strong class="ol jt">.</strong>record(source)<br/><br/>    asr_functions <strong class="ol jt">=</strong> {<br/>        ASREngine<strong class="ol jt">.</strong>sphinx: r<strong class="ol jt">.</strong>recognize_sphinx,<br/>        ASREngine<strong class="ol jt">.</strong>google: r<strong class="ol jt">.</strong>recognize_google,<br/>    }<br/><br/>    response <strong class="ol jt">=</strong> asr_functions[engine](audio, language<strong class="ol jt">=</strong>language, show_all<strong class="ol jt">=</strong>show_all)<br/>    <strong class="ol jt">return</strong> response<br/><br/><em class="ph"># Run tests<br/></em><strong class="ol jt">for</strong> t <strong class="ol jt">in</strong> TESTCASES:<br/>    filename <strong class="ol jt">=</strong> t['filename']<br/>    text <strong class="ol jt">=</strong> t['text']<br/>    lang <strong class="ol jt">=</strong> t['lang']<br/><br/>    <strong class="ol jt">print</strong>('\naudio file="{0}"    expected text="{1}"'<strong class="ol jt">.</strong>format(<br/>        filename, text<br/>    ))<br/>    <strong class="ol jt">for</strong> asr_engine <strong class="ol jt">in</strong> ASREngine:<br/>        <strong class="ol jt">try</strong>:<br/>            response <strong class="ol jt">=</strong> speech_to_text(filename, asr_engine, language<strong class="ol jt">=</strong>lang)<br/>            <strong class="ol jt">print</strong>('{0}: "{1}"'<strong class="ol jt">.</strong>format(asr_engine<strong class="ol jt">.</strong>name, response))<br/>        <strong class="ol jt">except</strong> sr<strong class="ol jt">.</strong>UnknownValueError:<br/>            <strong class="ol jt">print</strong>('{0} could not understand audio'<strong class="ol jt">.</strong>format(<br/>                asr_engine<strong class="ol jt">.</strong>name<br/>            ))<br/>        <strong class="ol jt">except</strong> sr<strong class="ol jt">.</strong>RequestError <strong class="ol jt">as</strong> e:<br/>            <strong class="ol jt">print</strong>('{0} error: {0}'<strong class="ol jt">.</strong>format(asr_engine<strong class="ol jt">.</strong>name, e))</span></pre><p id="0071" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出:</p><pre class="nb nc nd ne gt oy ol oz pa aw pb bi"><span id="4ac9" class="pc me jj ol b gy pd pe l pf pg">audio file="audio/2830-3980-0043.wav"    expected text="experience proves this"<br/>sphinx: "experience proves that"<br/>google: "experience proves this"<br/><br/>audio file="audio/4507-16021-0012.wav"    expected text="why should one halt on the way"<br/>sphinx: "why should one hold on the way"<br/>google: "why should one halt on the way"<br/><br/>audio file="audio/8455-210777-0068.wav"    expected text="your power is sufficient i said"<br/>sphinx: "your paris official said"<br/>google: "your power is sufficient I said"</span></pre><h2 id="e8ab" class="pc me jj bd mf pj pk dn mj pl pm dp mn lq pn po mp lu pp pq mr ly pr ps mt jp bi translated">其他提供商的API</h2><p id="2399" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">对于其他语音识别提供商，您将需要创建API凭证，您必须将这些凭证传递给<code class="fe oi oj ok ol b">recognize_&lt;speech engine name&gt;</code>函数，您可以查看<a class="ae jg" href="https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py" rel="noopener ugc nofollow" target="_blank">这个示例</a>。</p><p id="4fdb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它还有一个很好的麦克风抽象，通过PyAudio/PortAudio实现。在<a class="ae jg" href="https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py" rel="noopener ugc nofollow" target="_blank">批次</a>中检查示例以捕捉来自麦克风的输入，并在<a class="ae jg" href="https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py" rel="noopener ugc nofollow" target="_blank">背景</a>中持续进行。</p><p id="0da5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="ph">想写一个把麦克风输入转换成文本的Python转录器？看看这个:</em> <a class="ae jg" href="https://www.ml4devs.com/articles/how-to-build-python-transcriber-using-mozilla-deepspeech/" rel="noopener ugc nofollow" target="_blank"> <em class="ph">如何使用Mozilla deep speech</em></a><em class="ph">构建Python转录器。</em></p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><p id="0793" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">如果您喜欢，请:</strong></p><div class="nb nc nd ne gt ab cb"><figure class="px iv py pz qa qb qc paragraph-image"><a href="https://www.ml4devs.com/newsletter/"><img src="../Images/c0b3ff1cd100b0d083b64ab62d80415e.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*5WJhoEqI8dgVeCQdXcISzA.png"/></a></figure><figure class="px iv py pz qa qb qc paragraph-image"><a href="https://twitter.com/intent/follow?user_id=29633907"><img src="../Images/bac2c0b02e577402bc596ec11cb947ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*23xdhWOYiyv7oyEyiHBFyA.png"/></a></figure><figure class="px iv py pz qa qb qc paragraph-image"><a href="https://www.linkedin.com/in/scgupta/"><img src="../Images/e264d4e0ccfd62b7b649f4ddbf47cc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*_37VjowTkhzWFS1v6qpTSA.png"/></a></figure></div></div></div>    
</body>
</html>