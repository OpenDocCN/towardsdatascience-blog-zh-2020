<html>
<head>
<title>Regression-based neural networks: Predicting Average Daily Rates for Hotels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于回归的神经网络:预测酒店的平均日房价</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-based-neural-networks-with-tensorflow-v2-0-predicting-average-daily-rates-e20fffa7ac9a?source=collection_archive---------1-----------------------#2020-05-15">https://towardsdatascience.com/regression-based-neural-networks-with-tensorflow-v2-0-predicting-average-daily-rates-e20fffa7ac9a?source=collection_archive---------1-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ed07" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谈到酒店预订，<strong class="ak">平均每日房价(ADR) </strong>是一个特别重要的指标。这反映了特定客户在整个逗留期间每天支付的平均费用。</h2></div><p id="3b17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Keras是一种用于运行高级神经网络的API——该API现在作为默认API包含在TensorFlow 2.0下，由Google开发。</p><p id="0c7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，Keras的主要竞争对手是脸书开发的PyTorch。虽然PyTorch有更高级别的社区支持，但它是一种特别冗长的语言，我个人更喜欢Keras，因为它在构建和部署模型时更加简单易用。</p><p id="b86e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个特殊的例子中，Keras建立了一个神经网络来解决一个回归问题，即我们的因变量(y)为区间格式，我们试图尽可能准确地预测y的数量。</p><h1 id="21a5" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">什么是神经网络？</h1><p id="7aa1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">神经网络是一种基于现有数据创建预测的计算系统。让我们使用r。</p><p id="7e23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络包括:</p><ul class=""><li id="fb8d" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated">输入图层:基于现有数据获取输入的图层</li><li id="31aa" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">隐藏层:使用反向传播来优化输入变量的权重以提高模型预测能力的层</li><li id="7346" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">输出层:基于输入层和隐藏层数据的预测输出</li></ul><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/fffa2129cd3af0a3d68d69561ee1f57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*7-KjMeBfGT_XdHqa.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">来源:作者创作</p></figure><h1 id="e50b" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">背景</h1><p id="0a9a" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">本研究侧重于酒店预订分析。说到酒店预订，<strong class="kk iu">【日均房价(ADR)】</strong>是一个特别重要的指标。这反映了特定客户在整个逗留期间每天支付的平均费用。</p><p id="9bc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">衡量ADR可以让酒店更准确地识别最有利可图的客户，并相应地调整营销策略。</p><p id="9b47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原始数据集可从<a class="ae nb" href="https://www.sciencedirect.com/science/article/pii/S2352340918315191" rel="noopener ugc nofollow" target="_blank"> Antonio，Almedia和Nunes (2019)，酒店预订需求数据集</a>获得。</p><p id="2873" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个例子，我们使用keras库中的线性激活函数来创建基于回归的神经网络。这个神经网络的目的是为每个客户预测一个<strong class="kk iu"> ADR </strong>值。形成该神经网络输入的所选特征如下:</p><ul class=""><li id="d2e6" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated">取消</li><li id="1173" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">原产国</li><li id="c93c" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">细分市场</li><li id="f76c" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">存款类型</li><li id="6520" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">客户类型</li><li id="6caf" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">所需的停车位</li><li id="7d10" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">到达日期:周数</li></ul><p id="f53f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，导入相关的库。请注意，您需要在系统上安装TensorFlow才能执行以下代码。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="1db1" class="nh lf it nd b gy ni nj l nk nl">import math<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from numpy.random import seed<br/>seed(1)<br/>import pandas as pd<br/>import statsmodels.api as sm<br/>import statsmodels.formula.api as smf</span><span id="8ad2" class="nh lf it nd b gy nm nj l nk nl">import tensorflow<br/>tensorflow.random.set_seed(1)<br/>from tensorflow.python.keras.layers import Dense<br/>from tensorflow.keras.layers import Dropout<br/>from tensorflow.python.keras.models import Sequential<br/>from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor</span><span id="3e6b" class="nh lf it nd b gy nm nj l nk nl">from sklearn.metrics import mean_absolute_error<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import MinMaxScaler</span></pre><h1 id="b110" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">数据准备</h1><p id="470b" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在这个实例中，ADR被设置为y变量，因为这是我们试图预测的特征。变量存储为numpy数组。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="6da2" class="nh lf it nd b gy ni nj l nk nl">y1 = np.array(adr)</span></pre><p id="7ae2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数字变量和分类变量是有区别的。例如，诸如原产国的分类变量被定义为一个类别(为了防止神经网络给代码分配顺序，例如，如果1 =葡萄牙，2 =德国，我们不希望出现德国的排名比葡萄牙“高”的情况)。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="11e1" class="nh lf it nd b gy ni nj l nk nl">countrycat=train_df.Country.astype("category").cat.codes<br/>countrycat=pd.Series(countrycat)</span></pre><p id="0876" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建所选特征的numpy堆栈:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="3585" class="nh lf it nd b gy ni nj l nk nl">x1 = np.column_stack((IsCanceled,countrycat,marketsegmentcat,deposittypecat,customertypecat,rcps,arrivaldateweekno))<br/>x1 = sm.add_constant(x1, prepend=True)</span></pre><p id="4e36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与任何神经网络一样，数据需要进行缩放，以便网络进行正确的解释，这一过程称为标准化。最小最大缩放器用于此目的。</p><p id="65e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，这伴随着一个警告。必须在将数据分成训练集、验证集和测试集之后<strong class="kk iu">进行缩放，每个测试集都单独进行缩放。</strong></p><p id="b2b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">配置神经网络时的一个常见错误是，在拆分数据之前先对数据进行规范化。</p><p id="73db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是错误的，因为归一化技术将使用来自验证和测试集的数据作为整体缩放数据时的参考点。这将无意中影响训练数据的值，本质上导致验证和测试集的数据泄漏。</p><p id="f06f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，数据首先被分成训练和验证数据:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="1aaf" class="nh lf it nd b gy ni nj l nk nl">X_train, X_val, y_train, y_val = train_test_split(x1, y1)</span></pre><p id="18ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后使用<strong class="kk iu">最小最大缩放器:</strong>缩放训练和验证数据</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="b39e" class="nh lf it nd b gy ni nj l nk nl">y_train=np.reshape(y_train, (-1,1))<br/>y_val=np.reshape(y_val, (-1,1))</span><span id="0cc2" class="nh lf it nd b gy nm nj l nk nl">scaler_x = MinMaxScaler()<br/>scaler_y = MinMaxScaler()</span><span id="5a17" class="nh lf it nd b gy nm nj l nk nl">print(scaler_x.fit(X_train))<br/>xtrain_scale=scaler_x.transform(X_train)<br/>print(scaler_x.fit(X_val))<br/>xval_scale=scaler_x.transform(X_val)</span><span id="f5ab" class="nh lf it nd b gy nm nj l nk nl">print(scaler_y.fit(y_train))<br/>ytrain_scale=scaler_y.transform(y_train)<br/>print(scaler_y.fit(y_val))<br/>yval_scale=scaler_y.transform(y_val)</span></pre><h1 id="99ec" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">神经网络配置</h1><p id="2468" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">训练神经网络时，最重要的考虑因素之一是选择输入层和隐藏层中包含的神经元数量。假设输出层是结果层，默认情况下，该层有1个神经元。</p><p id="63ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如<a class="ae nb" href="https://medium.com/fintechexplained/what-are-hidden-layers-4f54f7328263" rel="noopener">法尔哈德·马利克</a>在本文中所解释的，每层中神经元的数量配置如下:</p><ul class=""><li id="5035" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated"><strong class="kk iu">输入层:</strong>输入层的神经元数量计算如下:</li></ul><p id="883a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nn no np nd b">Number of features in the training set + 1</code></p><p id="70ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，由于开始时训练集中有7个特征，因此相应地定义了<strong class="kk iu"> 8个</strong>输入神经元。</p><ul class=""><li id="c833" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated"><strong class="kk iu">隐藏层:</strong>定义一个隐藏层，因为单个层适用于大多数数据集。隐藏层中神经元的数量确定如下:</li></ul><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="7f8f" class="nh lf it nd b gy ni nj l nk nl">Training Data Samples/Factor * (Input Neurons + Output Neurons)</span></pre><p id="e627" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，将因子设置为1，该因子的目的是防止过度拟合。因子可以取1到10之间的值。输入层中有8个神经元，输出层中有1个神经元，训练集中有24036个观察值，隐藏层被分配了2，670个神经元。</p><ul class=""><li id="6a4b" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated"><strong class="kk iu">输出层:</strong>由于这是结果层，输出层默认取值1。</li></ul><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="af04" class="nh lf it nd b gy ni nj l nk nl">model = Sequential()<br/>model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation='relu'))<br/>model.add(Dense(2670, activation='relu'))<br/>model.add(Dense(1, activation='linear'))<br/>model.summary()</span></pre><p id="b4f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">均方误差(mse)和平均绝对误差(mae)是我们的损失函数，即神经网络预测测试数据的准确度的估计值。我们可以看到，当validation_split设置为0.2时，80%的训练数据用于训练模型，而剩余的20%用于测试目的。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="c4ac" class="nh lf it nd b gy ni nj l nk nl">model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])<br/>history=model.fit(xtrain_scale, ytrain_scale, epochs=30, batch_size=150, verbose=1, validation_split=0.2)<br/>predictions = model.predict(xval_scale)</span></pre><p id="ed3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型配置如下:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="1b0a" class="nh lf it nd b gy ni nj l nk nl">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense (Dense)                (None, 8)                 72        <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2670)              24030     <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 1)                 2671      <br/>=================================================================<br/>Total params: 26,773<br/>Trainable params: 26,773<br/>Non-trainable params: 0</span></pre><p id="68ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们来拟合我们的模型。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="b88d" class="nh lf it nd b gy ni nj l nk nl">Epoch 1/30<br/>161/161 [==============================] - 1s 4ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0878 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0714<br/>Epoch 2/30<br/>161/161 [==============================] - 0s 3ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0672 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0609<br/>...<br/>Epoch 28/30<br/>161/161 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0481 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0474<br/>Epoch 29/30<br/>161/161 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0482 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0470<br/>Epoch 30/30<br/>161/161 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0484 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0467</span></pre><p id="16a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以看到，训练损失和验证损失都在计算中，即预测y和实际y之间的偏差，由均方误差测量。</p><p id="37fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">已经为我们的模型指定了30个时期。这意味着我们实际上是在30次向前和向后传递中训练我们的模型，期望我们的损失将随着每个时期而减少，这意味着随着我们继续训练模型，我们的模型正在更准确地预测y的值。</p><p id="857c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看当绘制各自的损失时会是什么样子:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="98e9" class="nh lf it nd b gy ni nj l nk nl">print(history.history.keys())<br/># "Loss"<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'validation'], loc='upper left')<br/>plt.show()</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/501e37db989e46c98bc1e639a4b42025.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*pOwQsn7kdYpvr1yxnrDENA.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="bd0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着时代数量的增加，训练和验证损失都以指数方式减少，这表明随着时代(或向前和向后传递的数量)的增加，模型获得了高度的准确性。</p><p id="45e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在可以将使用来自验证集的特征生成的预测与来自该验证集的实际ADR值进行比较。</p><p id="4f0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预测将按比例缩小到原始比例:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="f3ca" class="nh lf it nd b gy ni nj l nk nl">predictions = scaler_y.inverse_transform(predictions)<br/>predictions</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/08f849231a1762feaeeffb0c98902b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nV10wmXclRUmxB2POuNwQw.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="f5f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与验证集中平均ADR值94相比，MAE为26，RMSE为38，该模型显示出显著的预测能力。然而，真正的测试是对以前未见过的数据进行预测，并将结果与新数据集中的实际ADR值进行比较。H2数据集用于此目的。</p><h1 id="1dac" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">预测—测试集(H2)</h1><p id="4b14" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">使用在H1数据集上生成的神经网络模型，来自H2数据集的特征现在被输入到网络中，以便预测H2的ADR值并将这些预测与实际ADR值进行比较。</p><p id="e961" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算出的<strong class="kk iu">平均绝对误差</strong>和<strong class="kk iu">均方根误差</strong>如下:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="f987" class="nh lf it nd b gy ni nj l nk nl">&gt;&gt;&gt; mean_absolute_error(btest, bpred)<br/>44.41596076208908</span><span id="9b72" class="nh lf it nd b gy nm nj l nk nl">&gt;&gt;&gt; mean_squared_error(btest, bpred)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(btest, bpred))<br/>57.899202401480025</span></pre><p id="33ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">H2数据集的平均ADR为105.30。</p><h1 id="d1ce" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">时期与批量大小</h1><p id="25bf" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在构建神经网络时，一个关键的权衡是关于用于训练模型的<strong class="kk iu">次数</strong>和<strong class="kk iu">批量</strong>。</p><ul class=""><li id="6c59" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated"><strong class="kk iu">批量:</strong>指每一个正向/反向传递的训练样本数。</li><li id="2dbf" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated"><strong class="kk iu">历元:</strong>所有训练实例一次正反向传递。</li></ul><p id="5e19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这份关于<a class="ae nb" href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks" rel="noopener ugc nofollow" target="_blank">堆栈溢出</a>的精彩总结对上述定义进行了更详细的阐述。</p><p id="fafc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">构建神经网络时面临的关键权衡是批量大小和迭代次数。</p><p id="7286" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，训练数据包含24，036个样本，批量大小为150。这意味着需要160次迭代来完成1个时期。</p><p id="62d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，可以增加批量大小以使每个时期具有更少的迭代，或者减小批量大小，这意味着每个时期需要更多的迭代。</p><p id="3a9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着，在其他条件相同的情况下，神经网络要么需要较高的批量来训练固定数量的时期，要么需要较低的批量来训练更多数量的时期。</p><h2 id="f0b7" class="nh lf it bd lg nw nx dn lk ny nz dp lo kr oa ob lq kv oc od ls kz oe of lu og bi translated">150个历元，batch_size = 50</h2><p id="c32c" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">这是测试集上的模型性能，此时历元数增加到150，批量减少到50。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="1f21" class="nh lf it nd b gy ni nj l nk nl">&gt;&gt;&gt; mean_absolute_error(btest, bpred)<br/>45.20461343967321</span><span id="4d70" class="nh lf it nd b gy nm nj l nk nl">&gt;&gt;&gt; mean_squared_error(btest, bpred)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(btest, bpred))<br/>58.47641486637935</span></pre><p id="4a16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用30个时期和150个批次时，平均事件发生率和RMSE稍低，这表明在预测ADR时，具有较大批次的较少时期更有优势。</p><h2 id="8bf9" class="nh lf it bd lg nw nx dn lk ny nz dp lo kr oa ob lq kv oc od ls kz oe of lu og bi translated">150个历元，batch_size = 150</h2><p id="00a7" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">但是，如果时期数和批处理大小都设置为150，会怎么样呢？成绩有进一步提高吗？</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="bcd2" class="nh lf it nd b gy ni nj l nk nl">&gt;&gt;&gt; mean_absolute_error(btest, bpred)<br/>45.030375561153335</span><span id="d9a1" class="nh lf it nd b gy nm nj l nk nl">&gt;&gt;&gt; mean_squared_error(btest, bpred)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(btest, bpred))<br/>58.43900275965334</span></pre><p id="ceca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当与30个时期的150个批量相比时，结果实际上是相同的，当使用30个时期时，RMSE略低。</p><p id="d6ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这点上，增加批次大小和历元数并没有导致测试集上模型性能的改善。</p><h1 id="929e" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">激活功能</h1><p id="cf22" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">当形成神经网络时，还必须考虑所选择的激活函数。</p><p id="ed34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，激活函数的目的是将非线性引入输入层和隐藏层，以便产生输出层所产生的更精确的结果。当我们处理回归问题时，即输出变量是数字而不是分类的，ReLU激活函数(修正的线性激活函数)非常流行。</p><p id="c77b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具体而言，该激活函数解决了所谓的消失梯度问题，由此神经网络将无法将重要的梯度信息从输出层反馈回输入层。关于消失梯度问题的更多信息可以在<a class="ae nb" href="https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/" rel="noopener ugc nofollow" target="_blank">机器学习大师</a>的教程中找到。</p><p id="0b8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ReLU在上面的例子中被使用，并且在30个时期和150个批量中表现出最好的准确性。</p><p id="2b2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，对于这个特定的问题，是否存在更合适的激活函数呢？</p><p id="c1bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为一个例子，<a class="ae nb" href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" rel="noopener ugc nofollow" target="_blank"> ELU </a>激活函数(代表指数线性单位)的功能与ReLU非常相似，但主要区别是ELU允许负输入，也可以产生负输出。</p><p id="26f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从技术上讲，我们的数据集没有负输入。但是，数据集中的许多ADR值都是0。毕竟，如果客户取消了酒店预订，那么酒店就不能向他们收费(在绝大多数情况下)。</p><p id="550c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，ADR有大量的<strong class="kk iu"> 0 </strong>条目，事实上，也有一个实例记录了该变量的负面观察结果。</p><p id="66f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这方面，神经网络再次运行30个时期，这一次使用ELU激活函数代替ReLU。</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="82d7" class="nh lf it nd b gy ni nj l nk nl">model = Sequential()<br/>model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation='elu'))<br/>model.add(Dense(2670, activation='elu'))<br/>model.add(Dense(1, activation='linear'))<br/>model.summary()</span></pre><p id="3a0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是模型配置:</p><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="ec68" class="nh lf it nd b gy ni nj l nk nl">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense (Dense)                (None, 8)                 72        <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2670)              24030     <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 1)                 2671      <br/>=================================================================<br/>Total params: 26,773<br/>Trainable params: 26,773<br/>Non-trainable params: 0</span></pre><p id="1f13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是训练和验证的损失。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/925197697d1b95f629679ef0409c918b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*gFN7T0QU4pfLRAHsud5eeg.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><pre class="mq mr ms mt gt nc nd ne nf aw ng bi"><span id="b754" class="nh lf it nd b gy ni nj l nk nl">&gt;&gt;&gt; mean_absolute_error(btest, bpred)<br/>28.908454264679218</span><span id="8f41" class="nh lf it nd b gy nm nj l nk nl">&gt;&gt;&gt; mean_squared_error(btest, bpred)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(btest, bpred))<br/>43.66170887622355</span></pre><p id="cc71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与ReLU相比，使用ELU时，平均绝对误差和均方根误差较低。这表明改变激活函数已经导致精度的提高。</p><h1 id="24fb" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="1911" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">此示例说明了如何:</p><ul class=""><li id="35f5" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated">用Keras构造神经网络</li><li id="b134" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">使用MinMaxScaler适当缩放数据</li><li id="e0c5" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">计算培训和测试损失</li><li id="68bc" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">使用神经网络模型进行预测</li><li id="91be" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">选择正确激活功能的重要性</li><li id="a962" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">考虑迭代次数和批量之间的权衡</li></ul><p id="9ee7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到原始文章<a class="ae nb" href="https://www.michael-grogan.com/articles/regression-based-neural-network" rel="noopener ugc nofollow" target="_blank"/>，它包含了一个到GitHub知识库的链接，这个知识库包含了与上述例子相关的笔记本和数据集。</p><p id="6def" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oh">免责声明:本文是在“原样”的基础上编写的，没有担保。本文旨在提供数据科学概念的概述，不应以任何方式解释为专业建议。</em></p></div></div>    
</body>
</html>