<html>
<head>
<title>Supervised Learning Algorithms: Explanaition and Simple code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督学习算法:解释和简单代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-learning-algorithms-explanaition-and-simple-code-4fbd1276f8aa?source=collection_archive---------16-----------------------#2020-02-14">https://towardsdatascience.com/supervised-learning-algorithms-explanaition-and-simple-code-4fbd1276f8aa?source=collection_archive---------16-----------------------#2020-02-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="cfd7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">监督学习算法采用一组已知的输入数据(学习集)和对数据的已知响应(输出)，并形成一个模型来生成对新输入数据的响应的合理预测。如果您有正在尝试预测的输出的现有数据，请使用监督学习。</p><p id="df6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它们可以用于分类或回归。在这篇文章中，我将展示每一个的一些例子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/d2cf3b098384370d43d74f5611e9589b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*rhycRTOgSKjsCzzR0Pn2Qw.png"/></div></figure></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="aba8" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak">分类算法</strong></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/dd7129397e546bf1107e3b89694b35d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*Cx0VWwdDJPWzP_VRsQf_mA.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">k最近邻分类器</p></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="5020" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import the Classifier </em></strong><em class="mq"> <br/></em><strong class="mh iu">from </strong>sklearn.neighbors <strong class="mh iu">import   </strong>KNeighborsClassifier</span><span id="8b5c" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Create arrays for the features and the response variable  </em></strong>    <br/>y = df['target'].values      <br/>X = df.drop('target', axis=1).values              </span><span id="bc65" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) Create a k-NN classifier with 6 neighbors     </em></strong>  <br/>knn = KNeighborsClassifier(n_neighbors =   6)              </span><span id="374c" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">4) Fit the classifier to the data   </em></strong>    <br/>knn.fit(X,y)                    </span><span id="9557" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">5) make predictions</em></strong><br/>new_prediction = knn.predict(X_new)</span></pre><h2 id="391c" class="ml le it bd lf ms mt dn lj mu mv dp ln kb mw mx lr kf my mz lv kj na nb lz nc bi translated"><strong class="ak">选择正确的集群数量</strong></h2><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="c03f" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Setup arrays to store train and test accuracies<br/></em></strong>neighbors = np.arange(1, 9)<br/>train_accuracy = np.empty(len(neighbors))<br/>test_accuracy = np.empty(len(neighbors))</span><span id="a9fc" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Loop over different values of k</em></strong><br/>for i, k in enumerate(neighbors):</span><span id="a7c1" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">   3) Setup a k-NN Classifier with k neighbors</em></strong><br/>   knn = KNeighborsClassifier(n_neighbors=k)</span><span id="2359" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">   4) Fit the classifier to the training data</em></strong><br/>   knn.fit(X_train, y_train)</span><span id="8e51" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">   5) Compute accuracy on the training set</em></strong><br/>   train_accuracy[i] = knn.score(X_train, y_train)</span><span id="8ab3" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">   6) Compute accuracy on the testing set</em></strong><br/>   test_accuracy[i] = knn.score(X_test, y_test)</span><span id="b93e" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">7) Generate plot</em></strong><br/>plt.title('k-NN: Varying Number of Neighbors')<br/>plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')<br/>plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')<br/>plt.legend()<br/>plt.xlabel('Number of Neighbors')<br/>plt.ylabel('Accuracy')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b77d460ddb52051ad17599259ba8b6f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*xBuxbQqMgg9QtVL744Kqgg.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/73508ff716a20c9826919b8ef261456a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*uUd7jGqFJAGPpmyWHViO8w.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">随机森林分类器</p></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="2412" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import the Classifier<br/></em>from </strong>sklearn.ensemble <strong class="mh iu">import </strong>RandomForestClassifier</span><span id="b2ed" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) input data as np array<br/></em></strong>y = df[‘target'].values<br/>X = df.drop(‘target', axis=1).values</span><span id="f1be" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) import model and fit(train) data<br/></em></strong>rfc = RandomForestClassifier(n_estimators=100)<br/>rfc.fit(X, y)</span><span id="beea" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu">4) make predictions<br/></strong>pred_rfc = rfc.predict(X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/840058fb6d5c2ddf2a3628f0a4f383c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAe9EVKhUaWojSssyrxDCA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">支持向量机(SVM)和判别分析</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c78394c7068287545f6942b098afad76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*glqX7qT5DdW1DskH-XVWww.png"/></div></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="6c47" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import the necessary modules  </em></strong>     <br/><strong class="mh iu">from </strong>sklearn.linear_model <strong class="mh iu">import </strong>LogisticRegression       <br/><strong class="mh iu">from </strong>sklearn.metrics <strong class="mh iu">import </strong>confusion_matrix, classification_report              </span><span id="ab9a" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Create training and test sets </em></strong><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)              </span><span id="2dcf" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) Create the classifier: </em></strong>   <br/>logreg = LogisticRegression()              </span><span id="268f" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">4) Fit the classifier to the training data  </em></strong>     logreg.fit(X_train,y_train)              </span><span id="c2e5" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">5) Predict the labels of the test set  </em></strong>     <br/>y_pred = logreg.predict(X_test)</span></pre><p id="6625" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">logistic回归的ROC曲线</strong></p><p id="8e05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC曲线是一种图示，其说明了二元分类器系统在其辨别阈值变化时的诊断能力。ROC曲线是通过在各种阈值设置下绘制真阳性率对假阳性率而创建的。真阳性率也称为灵敏度、召回率或检测概率。假阳性率也称为假警报概率</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4077e56a0aa9e6059b546d2230fed289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*cOe0Kd0iO9FjDK_TNonMFQ.jpeg"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">来自:<a class="ae nm" href="https://flowingdata.com/2014/05/09/type-i-and-ii-errors-simplified/" rel="noopener ugc nofollow" target="_blank">https://flowing data . com/2014/05/09/type-I-and-ii-errors-simplified/</a></p></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="5658" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import the necessary modules       <br/></em></strong>from sklearn.metrics import roc_curve              </span><span id="5001" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Compute predicted probabilities</em></strong>    <br/>y_pred_prob = logreg.predict_proba(X_test)[:,1]              </span><span id="68e6" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) Generate ROC curve values     </em></strong><br/>fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)  </span><span id="c2a1" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">4) Plot ROC curve   </em></strong>    <br/>plt.plot([0, 1], [0, 1], 'k--')       <br/>plt.plot(fpr, tpr)       <br/>plt.xlabel('False Positive Rate')       <br/>plt.ylabel('True Positive Rate')       <br/>plt.title('ROC Curve')       <br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f755ffb1ef75d92caf55a499e59d3ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*dMe9n8xfO8k-8rZcT7YjCg.png"/></div></figure><p id="d1c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC曲线下的面积越大，模型越好。</p></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="5b7c" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">回归算法</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi no"><img src="../Images/409bf95cdfefb608915c429f899759c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qrwbMdd3ZfCUP1M34hcMA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">线性和非线性回归</p></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="e48b" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import LinearRegression  </em></strong>     <br/><strong class="mh iu">from </strong>sklearn <strong class="mh iu">import </strong>linear_model       <br/><strong class="mh iu">from </strong>sklearn.linear_model <strong class="mh iu">import   </strong>LinearRegression</span><span id="ac0e" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Create the regressor </em></strong>    <br/>reg = linear_model.LinearRegression()</span><span id="c981" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) Create the prediction space  </em></strong>    <br/>prediction_space =  np.linspace(min(X_fertility),max(X_fertility)).reshape(-1,1)</span><span id="060f" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">4) Fit the model to the data   </em></strong>    <br/>reg.fit(X_fertility,y)</span><span id="961d" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">5) Compute predictions  </em></strong><br/>y_pred = reg.predict(prediction_space)</span><span id="1b96" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">6) Plot regression line</em></strong>       <br/>plt.plot(prediction_space, y_pred,  color='black', linewidth=3)       plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b47992383f59b5934517072ef38c2f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*h9ttPPjCoTKs9jGJX_K5Gg.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f599d8f18cd30e8d8577897931797fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*gdZXofEXDshELyxND84MyA.png"/></div></figure><pre class="kp kq kr ks gt mg mh mi mj aw mk bi"><span id="410c" class="ml le it mh b gy mm mn l mo mp"><strong class="mh iu"><em class="mq">1) Import Decision Tree Regressor</em></strong><br/>from sklearn.tree   import DecisionTreeRegressor</span><span id="2564" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">2) Create a decision tree regression model   </em></strong>  <br/>decision_tree = DecisionTreeRegressor()            </span><span id="f714" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">3) Fit the model to the training features  and targets  </em></strong>     decision_tree.fit(train_features,train_targets)              </span><span id="f754" class="ml le it mh b gy mr mn l mo mp"><strong class="mh iu"><em class="mq">4) Check the score on train and test   </em></strong>    print(decision_tree.score(train_features,   train_targets))       print(decision_tree.score(test_features,test_targets))</span></pre><p id="6bc2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，还有两个回归算法:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nr"><img src="../Images/a19bee3762e902a6ece7ddc82411efdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWvzItdoRsRM9CV8nXgGdg.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated"><em class="ns">高斯过程回归模型(GPR)和支持向量机(SVM)回归</em></p></figure></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="9221" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">感谢阅读！！</h1><blockquote class="nt"><p id="0250" class="nu nv it bd nw nx ny nz oa ob oc kn dk translated">如果你想继续阅读这样的故事，你可以<a class="ae nm" href="https://fesan818181.medium.com/membership" rel="noopener">在这里订阅！</a></p></blockquote><p id="7d71" class="pw-post-body-paragraph jq jr it js b jt od jv jw jx oe jz ka kb of kd ke kf og kh ki kj oh kl km kn im bi translated">如果你想看看这些算法在真实世界的数据集中表现如何，我建议你阅读下一篇文章:</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/feature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">泰坦尼克号数据集的特征工程和算法精度</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">机器学习最流行的数据集之一对应于泰坦尼克号事故</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ku ol"/></div></div></a></div><p id="3049" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者阅读这个项目，我正在处理来自我的研究的数据:</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/datascience-in-oil-and-gas-engineering-projects-daace6e6c7f"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">石油和天然气工程项目中的数据科学。</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">探索性数据分析</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ku ol"/></div></div></a></div></div></div>    
</body>
</html>