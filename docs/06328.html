<html>
<head>
<title>How to train_test_split : KFold vs StratifiedKFold</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练 _ 测试 _ 拆分:KFold vs StratifiedKFold</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-test-split-kfold-vs-stratifiedkfold-281767b93869?source=collection_archive---------3-----------------------#2020-05-21">https://towardsdatascience.com/how-to-train-test-split-kfold-vs-stratifiedkfold-281767b93869?source=collection_archive---------3-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f3c9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">举例说明</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/23d0740228897bbcf75514aff107f139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itAiiY0aVMrHZfW6N-0wtw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伊丽莎白躺在<a class="ae ky" href="https://unsplash.com/s/photos/divide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上</p></figure><p id="04a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">监督学习任务中使用的数据包含一组观察值的特征和标签。这些算法试图对特征(自变量)和标签(因变量)之间的关系进行建模。我们首先通过为一些观察结果提供特征和标签来训练模型。然后通过仅提供特征并期望它预测标签来测试模型。因此，我们需要将数据分成训练和测试子集。我们让模型在训练集上学习，然后在测试集上测量它的性能。</p><p id="a851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn 库提供了许多工具来将数据分成训练集和测试集。最基本的一个是<strong class="lb iu"> train_test_split </strong>，它只是按照指定的划分比例将数据分成两部分。例如，<strong class="lb iu">train _ test _ split(test _ size = 0.2)</strong>将留出 20%的数据用于测试，80%用于训练。让我们看一个例子。我们将创建一个带有一个要素和一个标签的示例数据帧:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e530" class="ma mb it lw b gy mc md l me mf">import pandas as pd<br/>import numpy as np</span><span id="35c8" class="ma mb it lw b gy mg md l me mf">target = np.ones(25)<br/>target[-5:] = 0</span><span id="22d1" class="ma mb it lw b gy mg md l me mf">df = pd.DataFrame({'col_a':np.random.random(25),<br/>                  'target':target})</span><span id="7916" class="ma mb it lw b gy mg md l me mf">df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/b7eb68ab60c776729efad2f139c4f72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*0AT0MgQuY7Vcb-XW_X3WMw.png"/></div></figure><p id="c6ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们应用<strong class="lb iu"> train_test_split </strong>函数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f6ed" class="ma mb it lw b gy mc md l me mf">from sklearn.model_selection import train_test_split</span><span id="9297" class="ma mb it lw b gy mg md l me mf">X = df.col_a<br/>y = df.target</span><span id="f7b5" class="ma mb it lw b gy mg md l me mf">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)</span><span id="39ab" class="ma mb it lw b gy mg md l me mf">print("TRAIN:", X_train.index, "TEST:", X_test.index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/7319558a2bd52f643da124fdbeee6d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCa3VSJaqYQBKAx0NtDQSA.png"/></div></div></figure><p id="a3b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前 80%是训练，后 20%是测试集。如果我们将<strong class="lb iu"> shuffle </strong>参数设置为真，数据将被随机分割:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6bff" class="ma mb it lw b gy mc md l me mf">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)</span><span id="f5d7" class="ma mb it lw b gy mg md l me mf">print("TRAIN:", X_train.index, "TEST:", X_test.index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/d9601757e6ab1e76c6f53c4b9c2a0dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XLVeDsxMBO-_UvupbU0CMg.png"/></div></div></figure><p id="9828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">shuffle 的默认值为 True，因此如果我们不指定 shuffle 参数，数据将被随机拆分。如果我们希望分割是可再现的，我们还需要将一个整数传递给<strong class="lb iu"> random_state </strong>参数。否则，每次我们运行 train_test_split 时，不同的索引将被拆分成训练集和测试集。请注意，输出中看到的数字是数据点的索引，而不是实际值。</p><p id="b3e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据是宝贵的资产，我们希望充分利用它。如果我们使用 train_test_split 来拆分数据，我们只能使用为培训而留出的部分来培训模型。随着训练数据量的增加，模型变得更好。克服这个问题的一个解决方案是<strong class="lb iu">交叉验证。</strong>通过交叉验证，数据集被分成 n 份。N-1 分割用于训练，剩余的分割用于测试。该模型遍历整个数据集 n 次，每次都使用不同的分割进行测试。因此，我们使用所有的数据点进行训练和测试。交叉验证也有助于更准确地衡量模型的性能，尤其是在新的、以前看不到的数据点上。</p><p id="a479" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉验证中有不同的数据分割方法。常用的有<strong class="lb iu">折叠</strong>和<strong class="lb iu">分层折叠</strong>。</p><h1 id="ff82" class="mk mb it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> KFold </strong></h1><p id="d918" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">顾名思义，KFold 将数据集分成 k 个折叠。如果 shuffle 设置为 False，连续折叠将是前一个折叠的移位版本:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2b3b" class="ma mb it lw b gy mc md l me mf">X = df.col_a<br/>y = df.target</span><span id="f5f8" class="ma mb it lw b gy mg md l me mf">kf = KFold(n_splits=4)</span><span id="e81e" class="ma mb it lw b gy mg md l me mf">for train_index, test_index in kf.split(X):<br/>    print("TRAIN:", train_index, "TEST:", test_index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/b28c38faa6a15d3f6fdaed88e921ea85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F1wnMC8pX_WpmtH8N-yiFA.png"/></div></div></figure><p id="548e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一次迭代中，测试集是前四个索引。然后 KFold 不断移动测试集 k 次。如果 shuffle 设置为 True，那么拆分将是随机的。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a7e1" class="ma mb it lw b gy mc md l me mf">kf = KFold(n_splits=4, shuffle=True, random_state=1)</span><span id="cd3c" class="ma mb it lw b gy mg md l me mf">for train_index, test_index in kf.split(X):<br/>    print("TRAIN:", train_index, "TEST:", test_index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/110cab3be55c2d8a6bce1640664bcceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TFlZHHqcMVu8EUJ5lKmNXQ.png"/></div></div></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="d983" class="mk mb it bd ml mm np mo mp mq nq ms mt jz nr ka mv kc ns kd mx kf nt kg mz na bi translated"><strong class="ak">分层折叠</strong></h1><p id="6428" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">StratifiedKFold 将交叉验证向前推进了一步。数据集中的类分布保留在训练和测试拆分中。让我们来看看我们的示例数据框架:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/b7eb68ab60c776729efad2f139c4f72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*0AT0MgQuY7Vcb-XW_X3WMw.png"/></div></figure><p id="5944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有 16 个数据点。其中 12 人属于 1 级，其余 4 人属于 0 级，因此这是一个不平衡的等级分布。KFold 没有考虑到这一点。因此，在类别分布不平衡的分类任务中，我们应该优先选择 StratifiedKFold 而不是 KFold。</p><p id="83b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0 类和 1 类的比例是 1/3。如果我们设置 k=4，那么测试集包括来自类 1 的三个数据点和来自类 0 的一个数据点。因此，训练集包括来自类 0 的三个数据点和来自类 1 的九个数据点。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d761" class="ma mb it lw b gy mc md l me mf">skf = StratifiedKFold(n_splits=4)</span><span id="5dfa" class="ma mb it lw b gy mg md l me mf">for train_index, test_index in skf.split(X, y):<br/>    print("TRAIN:", train_index, "TEST:", test_index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d1c61ab39da7441b0b8e8eec2c2be57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmbdmO7lxe_T5vxM-hL06Q.png"/></div></div></figure><p id="ab77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等级 0 的指数是 12、13、14 和 15。正如我们所看到的，数据集的类分布保留在分割中。我们也可以对 StratifiedKFold 使用洗牌:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c651" class="ma mb it lw b gy mc md l me mf">skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)</span><span id="151e" class="ma mb it lw b gy mg md l me mf">for train_index, test_index in skf.split(X, y):<br/>    print("TRAIN:", train_index, "TEST:", test_index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/463c27d2267a97bdd0001912aa43e428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3-b6KyYuxKZvokCDWfT8TQ.png"/></div></div></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="2c79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种用于分割的方法被称为“留一个出来”，该方法仅使用一个数据点进行测试，剩余的数据点用于训练。Scikit learn 有<strong class="lb iu"> LeaveOneOut </strong>类来执行这种类型的分区。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="ce00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我想提一下 scikit 提供的另一个重要工具——learn，即<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> cross_val_score。</strong>T13】</a></p><p id="26d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Cross_val_score 获取数据集并应用交叉验证来拆分数据。然后，使用指定的估计器(如逻辑回归、决策树等)训练模型，并测量模型的性能(评分参数)。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="6b05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>