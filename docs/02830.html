<html>
<head>
<title>A pair of interrelated neural networks in Deep Q-Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度Q网络中的一对相关神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4?source=collection_archive---------30-----------------------#2020-03-18">https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4?source=collection_archive---------30-----------------------#2020-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a825" class="pw-subtitle-paragraph jy is it bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">在DQN和双DQN模型中，比较两个相互关联的神经网络是至关重要的。</h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/3d636d48c89dbb28397b799feff04dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WPhQyl4nY6JoS6bHOsMjnQ.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">资料来源:123rf.com</p></figure><p id="3c1a" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将遵循在开发<em class="mc"> DQN </em>和<em class="mc">双</em> <em class="mc"> DQN </em>算法中为对抗相关性和高估而采取的一些步骤。作为<em class="mc"> DQN </em>和<em class="mc">双DQN </em>应用的例子，我们展示了<em class="mc"> CartPole-v0 </em>和<em class="mc"> CartPole-v1 </em>环境的训练结果。最后一节包含一些关于<em class="mc"> PyTorch </em>张量的提示。</p><p id="11f6" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">从查找表到神经网络</strong></p><p id="5273" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">直到2013年左右，深度Q-Learning还是一种强化学习(<em class="mc"> RL </em>)方法，只对查找表有效。神经网络在计算机视觉中的成功激起了人们在《T21》RL中尝试它们的兴趣。论文“<a class="ae md" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank"> <em class="mc">用深度RL </em> </a>玩雅达利”(V.Mnih等人，2013，DeepMind)提出了第一个使用神经网络函数逼近的成功的<em class="mc">深度学习</em>模型。在2015年，<a class="ae md" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank"> DeepMind展示了</a>Deep Q-network代理，仅接收行像素数据和游戏分数作为输入，就能够超过之前所有算法的性能。</p><blockquote class="me"><p id="40f3" class="mf mg it bd mh mi mj mk ml mm mn mb dk translated">在这两个DeepMind工作之后，查找表被神经网络取代，一个<em class="mo">Q-学习</em>变成了一个<em class="mo">深度Q-学习、</em>或者等价地，<em class="mo">深度Q-网络(DQN)。</em>其实是RL特工训练的一个突破。</p></blockquote><p id="db3f" class="pw-post-body-paragraph lg lh it li b lj mp kc ll lm mq kf lo lp mr lr ls lt ms lv lw lx mt lz ma mb im bi translated"><em class="mc"> DQN </em>是将Q学习与神经网络相结合的算法。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0ff4b2d7c99c98d52a985f964da83a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*nL9mJpL4WPAZ2fzs-lr88w.png"/></div></figure><h1 id="28ae" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated"><strong class="ak"><em class="mo">DQN</em></strong>组件</h1><ol class=""><li id="8659" class="nn no it li b lj np lm nq lp nr lt ns lx nt mb nu nv nw nx bi translated">一对<em class="mc">Q</em>-网络(本地和目标)。</li><li id="ec43" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">体验回放<strong class="li iu"> </strong> —一种生物启发机制。</li><li id="80f4" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">ε-贪婪机制。</li><li id="2713" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated"><em class="mc">Q</em>-学习，即将最大值用于所有可能的动作。</li><li id="5191" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">利用梯度下降机制最小化损失函数。</li></ol><p id="d44d" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">相关性有害</strong></p><p id="4add" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当神经网络被用作函数逼近时，强化学习被认为是不稳定的。这种不稳定性的原因如下:</p><ul class=""><li id="13dc" class="nn no it li b lj lk lm ln lp od lt oe lx of mb og nv nw nx bi translated">观察序列中的相关性，</li><li id="ca5f" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb og nv nw nx bi translated">对<em class="mc"> Q </em>值的微小更新可能会显著改变策略，</li><li id="cada" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb og nv nw nx bi translated"><em class="mc">Q</em>-值<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q(s_t, a_t)</em></strong></code>和<em class="mc">TD</em>-目标值之间的相关性</li></ul><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ol"><img src="../Images/8d37f7992e66b3b2f787c41f4e98ece2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xjcBX8vYXmYS-PqihsrDqA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om">TD-目标</strong></p></figure><p id="eed5" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">用于<em class="mc"> DQN </em>的关键方程，eq。(2).</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi on"><img src="../Images/883b1e1f37267d4a2a387d976cb028b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGZlYs8Ow97yI0pmuxr0qA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om"> DQN关键方程</strong></p></figure><p id="b0cc" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">对<em class="mc">Q</em>-网络:本地和目标</strong></p><p id="152b" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> DQN </em>算法在对抗相关性中的一个重要组成部分就是使用目标网络<em class="mc"> ( </em> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_target</em></strong></code> <em class="mc"> ) </em>。目标网络(具有参数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">θ*</em></strong></code>)与本地网络相同，除了它的参数从本地网络(<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_local</em></strong></code>)的每个<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">τ</em></strong></code>步骤被复制，以便然后<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">θ</em>*_<em class="mc">t</em> = <em class="mc">θ</em>_<em class="mc">t</em></strong></code>，并在所有其他步骤保持固定。在看起来如下:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi oo"><img src="../Images/303ef17ce5d6c6818c31c1fd1a785165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxHImOYwdJsw2uYFjFmtPw.png"/></div></div></figure><p id="6983" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在<a class="ae md" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/Cartpole-Deep-Q-Learning/CartPole-v0_DQN_Pytorch.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mc">横翻</em> </a> <em class="mc"> </em>中设置<em class="mc"> τ = </em> TARGET_UPDATE = 10。</p><p id="9436" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">损失函数为<em class="mc"> DQN </em>代理</strong></p><blockquote class="me"><p id="0b35" class="mf mg it bd mh mi mj mk ml mm mn mb dk translated">比较代表相同Q-表的两个神经网络并找到这些网络非常接近的点是DQN算法的基本部分。</p></blockquote><p id="1d0b" class="pw-post-body-paragraph lg lh it li b lj mp kc ll lm mq kf lo lp mr lr ls lt ms lv lw lx mt lz ma mb im bi translated">由网络<em class="mc"> q_local </em>和<em class="mc"> q_target </em>张量<em class="mc">当前估计</em> <strong class="li iu"> <em class="mc"> </em> </strong> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q</em>(s_<em class="mc">t, a_t</em>)</strong></code>和<em class="mc">备选估计</em> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">G_t</em></strong></code> <em class="mc">，</em>参见<em class="mc"> </em> (1)，进行计算。此外，在<code class="fe oh oi oj ok b"><strong class="li iu">class Agent</strong></code>的功能<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">learn()</em></strong></code>中</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi op"><img src="../Images/b3fa8903486169664c7ba68e286f99aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pztthpr3EZPFQll3o24p5w.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om">表1。</strong> <strong class="bd om">三张量(PyTorch中)，DQN </strong></p></figure><ul class=""><li id="f6f1" class="nn no it li b lj lk lm ln lp od lt oe lx of mb og nv nw nx bi translated"><code class="fe oh oi oj ok b"><strong class="li iu">Q_targets </strong></code>与<code class="fe oh oi oj ok b"><strong class="li iu">Q_expected</strong></code> <strong class="li iu"> </strong>之间的距离由<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">MSELoss </em></strong></code>函数计算<strong class="li iu"/>；<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">loss</em></strong></code>值通过梯度下降机制最小化；</li><li id="02eb" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb og nv nw nx bi translated">执行<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">loss</em></strong></code>张量的<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">backpropagation </em></strong></code>机制；</li><li id="406b" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb og nv nw nx bi translated">梯度下降算法由优化器执行，例如<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">torch.optim.Adam</em>, <em class="mc">torch.optim.RMSprop</em></strong></code>或任何其他。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="oq or l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="ak">DQN的学习功能</strong></p></figure><p id="57f8" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">体验回放——一种受生物启发的机制</strong></p><p id="fb5e" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> DQN </em>用来减少相关性的另一个东西是<strong class="li iu">体验回放</strong>机制，它将数据放入特定的内存存储中，并从内存存储中随机接收数据。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="d0ea" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">ε-贪婪机制</strong></p><p id="ef6e" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">DQN</em><code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>-贪婪机制促进了<em class="mc">勘探</em>和<em class="mc">开采之间的合理比例。</em>该机构提供参数<em class="mc"> </em> <code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>，其中<code class="fe oh oi oj ok b"><strong class="li iu">0 &lt; ε &lt; 1</strong></code>，允许控制该比例。对于任何<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>，以概率<code class="fe oh oi oj ok b"><strong class="li iu">1-ε</strong></code>，选择剥削。<em class="mc">利用</em>意味着代理通过在给定状态的所有可能动作中最大化<em class="mc">Q</em>-值找到以下动作，参见函数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">get_action()</em></strong></code>中的相关行:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="8496" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">退火ε-贪婪机制</strong></p><p id="60ba" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是如何选择epsilon呢？一个流行的选择是退火<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>——贪婪算法。对于任何一集<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">i</em></strong></code>，动作都是贪婪概率<code class="fe oh oi oj ok b"><strong class="li iu">1-ε</strong></code>。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi os"><img src="../Images/286005c4e01bcae8e99c8fda9d14cea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ufZOECksWvdVdShmV1DI7A.png"/></div></div></figure><p id="d444" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在eq中。(3)<code class="fe oh oi oj ok b"><strong class="li iu">ε_m</strong></code><strong class="li iu"/>是<strong class="li iu"> </strong>中<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>的最小值，在<code class="fe oh oi oj ok b"><strong class="li iu">Mε</strong></code> <strong class="li iu">中必须达到。</strong>由(3)如果<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">i</em> = 0</strong></code> <strong class="li iu"> </strong>我们有<code class="fe oh oi oj ok b"><strong class="li iu">ε = 1</strong></code>；如果<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">i = </em>1</strong> </code>那么<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code> <strong class="li iu"> </strong>接近<code class="fe oh oi oj ok b"><strong class="li iu">1</strong></code> <strong class="li iu">。</strong>为例，对于<code class="fe oh oi oj ok b"><strong class="li iu">Mε = 50</strong></code><strong class="li iu"/><code class="fe oh oi oj ok b"><strong class="li iu">ε_m =0.01</strong></code><strong class="li iu"/>我们有<code class="fe oh oi oj ok b"><strong class="li iu">ε = 0.98</strong></code> <strong class="li iu">。</strong>然后以概率<code class="fe oh oi oj ok b"><strong class="li iu">0.02</strong></code>选择<em class="mc">开采</em>，以概率<code class="fe oh oi oj ok b"><strong class="li iu">0.98</strong></code> <strong class="li iu">选择<em class="mc"> </em>，</strong>即<em class="mc">动作号</em>的一个相当随机的值。因此，对于第一集来说，动作会被随机选择，这就是探索。</p><p id="2b5b" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">i = </em>Mε</strong></code> <strong class="li iu">，</strong>我们得到<code class="fe oh oi oj ok b"><strong class="li iu">ε = ε_m</strong></code> <strong class="li iu">。</strong>以<code class="fe oh oi oj ok b"><strong class="li iu">ε = 0.01</strong></code>为例，<strong class="li iu">。</strong>然后以概率<code class="fe oh oi oj ok b"><strong class="li iu">0.99</strong></code>选择利用，仅在1%的情况下获得行动数的随机值。对于从<code class="fe oh oi oj ok b"><strong class="li iu">0</strong></code> <strong class="li iu"> </strong>到<code class="fe oh oi oj ok b"><strong class="li iu">Mε</strong></code>的剧集，<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code> <strong class="li iu"> </strong>的值从<code class="fe oh oi oj ok b"><strong class="li iu">1</strong></code>到<code class="fe oh oi oj ok b"><strong class="li iu">ε_m</strong></code><strong class="li iu"/>递减，之后固定在<code class="fe oh oi oj ok b"><strong class="li iu">ε_m</strong></code>。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="oq or l"/></div></figure><h1 id="7aa1" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated">双DQN对DQN</h1><p id="9564" class="pw-post-body-paragraph lg lh it li b lj np kc ll lm nq kf lo lp ot lr ls lt ou lv lw lx ov lz ma mb im bi translated"><strong class="li iu">高估<em class="mc">DQN</em>T28】</strong></p><p id="00e0" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">众所周知，<em class="mc"> DQN </em>算法高估了动作值。<a class="ae md" href="https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"> Thrun和Schwartz (1993) </a>首次调查了<em class="mc"> DQN </em>的<em class="mc">高估。他们给出了一个例子，其中这些高估渐进地导致次优政策。假设，对于某个状态<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">s</em></strong>’</code>，所有动作<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">a</em></strong></code>的真值为<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q(s’, a) = 0</em></strong></code>，而<em class="mc"> Q </em>的估计值分布在零上下。那么这些估计值的最大值就是对真实值的高估。2015年，(<a class="ae md" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"> Hasselt et。艾尔。，</a> DeepMind)表明，估计误差会使估计值上升并远离真正的最优值。他们假设减少高估的解决方案:<em class="mc">双DQN。</em></em></p><p id="7465" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">高估的原因是什么？问题出在eqs中的<code class="fe oh oi oj ok b"><strong class="li iu">max</strong></code>运算符。(1)和(2)。假设<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q(S_t, a)</em></strong></code> <em class="mc"> </em>的评估值已经被高估。然后在方程中得到的动作值。(1)或(2)变得更加高估。等式中的<em class="mc">TD</em>-目标<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">G_t</em></strong></code>。(1)可以重写为<em class="mc">TD</em>-目标<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">G^Q_t</em></strong></code>如下:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ow"><img src="../Images/0b8ac53387a055ba6d17d28aaea870f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*33ygUBQmec36qkwyDgkazQ.png"/></div></div></figure><p id="c926" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">θ</em>_<em class="mc">t</em></strong></code>是网络的权值集合<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_target</em></strong></code>，<em class="mc">，</em>见上表1。</p><p id="8f03" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">解耦动作和评估</strong></p><p id="d562" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">双DQN </em>的解决方案是在<em class="mc">解耦</em>中选择<em class="mc">动作</em> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">argmax_a</em></strong></code> <em class="mc"> </em>从<em class="mc">求值</em> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q</em>(<em class="mc">S_{t+1}, argmax_a</em>)</strong></code>，参见(4)。<em class="mc"> </em>这是使用另一个具有权重集<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">θ’</em>_<em class="mc">t</em></strong></code> <em class="mc"> : </em>的网络来完成的</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ox"><img src="../Images/ac44d2d18be82488aebdfd29f21ad734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTedy5B1ib9ud_MO_a6mXw.png"/></div></div></figure><blockquote class="me"><p id="5c1f" class="mf mg it bd mh mi oy oz pa pb pc mb dk translated">通过(5)贪婪策略(argmax)由神经网络<code class="fe oh oi oj ok b"><em class="mo">q_local</em><strong class="ak"><em class="mo"> (</em></strong></code>权重<code class="fe oh oi oj ok b"><strong class="ak"><em class="mo">θ</em>_<em class="mo">t). </em></strong></code>估计，网络<code class="fe oh oi oj ok b"> q_target (</code>权重<code class="fe oh oi oj ok b"><strong class="ak"><em class="mo">θ'</em>_<em class="mo">t) </em></strong></code>用于公平地评估该策略。这个方案是<em class="mo">双DQN </em>的主要思想。</p></blockquote><p id="0381" class="pw-post-body-paragraph lg lh it li b lj mp kc ll lm mq kf lo lp mr lr ls lt ms lv lw lx mt lz ma mb im bi translated"><strong class="li iu">损失函数为<em class="mc">双</em> <em class="mc"> DQN </em>代理</strong></p><p id="7cf6" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于<em class="mc">双</em> <em class="mc"> DQN </em>代理<strong class="li iu"> </strong>的情况，张量<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q</em>(s_<em class="mc">t, a_t</em>)</strong></code>和<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">G_t</em></strong></code> <em class="mc"> </em>的计算同<em class="mc"> DQN、</em>见<em class="mc"> </em>表1和表2。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pd"><img src="../Images/df1228de7436a20b2afdb0589c271653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2rYCbNmo-c8VvfyS31fUnA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om">表二。四张量(PyTorch中)，双DQN </strong></p></figure><p id="547e" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">表1和表2的主要区别在于张量<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_target_next </em></strong></code>的计算(评估<em class="mc"> Q </em>值):</p><ul class=""><li id="bd43" class="nn no it li b lj lk lm ln lp od lt oe lx of mb og nv nw nx bi translated">在表1中，<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_target_next</em></strong></code>仅由一个网络计算<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_target, </em></strong></code>见(4)。</li><li id="f28b" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb og nv nw nx bi translated">在表2中，<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_target_next </em></strong></code>使用两个网络计算:<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_target</em></strong></code>和<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">q_local</em></strong></code>(见(5)和<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_max_action</em></strong></code>)。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="oq or l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="ak">双DQN的学习功能</strong></p></figure><h1 id="ce89" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated">杠-v0和杠-v1训练</h1><p id="91b4" class="pw-post-body-paragraph lg lh it li b lj np kc ll lm nq kf lo lp ot lr ls lt ou lv lw lx ov lz ma mb im bi translated">一根杆子通过一个接头连接到一辆沿轨道移动的车上。通过对推车施加+1或-1的力来控制该系统。</p><p id="b336" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">第<em class="mc">题</em>是一个二元分类问题</strong></p><p id="a65a" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">小车</em>观察空间的维数为4，因为有4个特征构成输入:<em class="mc">小车坐标</em>、<em class="mc">速度</em>、<em class="mc">立柱与垂线的角度</em>及其<em class="mc">导数</em>(立柱“下落”速度)。<em class="mc">横竿</em>是一个二元分类问题，因为在每个时间步，代理人在移动<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">left</em></strong></code>或<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">right</em></strong></code>之间进行选择。钟摆开始直立，目标是防止它翻倒。柱子保持直立的每个时间步长提供+1的奖励。当柱子偏离垂直方向超过15度，或者手推车偏离中心超过2.4个单位时，该集结束。</p><blockquote class="me"><p id="a424" class="mf mg it bd mh mi mj mk ml mm mn mb dk translated"><em class="mo">如果连续100次试验获得平均奖励&gt;</em><strong class="ak"><em class="mo">195</em></strong><em class="mo">，则认为环境</em>问题解决；横竿-v1 <em class="mo">如果连续100次试验得到平均奖励&gt;</em><strong class="ak"><em class="mo">475</em></strong><em class="mo">就认为解决了。</em></p></blockquote><p id="92a8" class="pw-post-body-paragraph lg lh it li b lj mp kc ll lm mq kf lo lp mr lr ls lt ms lv lw lx mt lz ma mb im bi translated"><strong class="li iu">对<em class="mc">小车-v0 </em>和<em class="mc">小车-v1 </em>和</strong>的训练实验</p><p id="312a" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下面是我用<em class="mc"> DQN </em>和<em class="mc">双DQN </em>进行实验的结果，是在训练<em class="mc"> CartPole-v0 </em>和<em class="mc"> CartPole-v1 </em>时获得的。对于所有情况，LEARNING_RATE = 0.001。贪婪参数<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>从<code class="fe oh oi oj ok b"><strong class="li iu">1</strong></code>变为<code class="fe oh oi oj ok b"><strong class="li iu">ε_m = 0.01</strong></code></p><p id="91cd" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae md" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Cartpole-Deep-Q-Learning" rel="noopener ugc nofollow" target="_blank"> <strong class="li iu">与DQN一起翻筋斗</strong> </a></p><p id="38a3" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于<em class="mc"> CartPole-v0 </em>和<em class="mc"> CartPole-v1、</em>我们把<em class="mc"> </em> <code class="fe oh oi oj ok b"><strong class="li iu">Mε= 50.</strong></code></p><ol class=""><li id="9e01" class="nn no it li b lj lk lm ln lp od lt oe lx of mb nu nv nw nx bi translated"><em class="mc"> DQN，钢管舞-v0，</em>奖励<strong class="li iu"> 195 </strong>在第<strong class="li iu"> 962集达成。</strong></li><li id="d3ef" class="nn no it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated"><em class="mc"> DQN，扁担-v1，</em>奖励<strong class="li iu"> 475 </strong>在第<strong class="li iu"> 1345 </strong>集达成。</li></ol><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pe"><img src="../Images/80410f01922851f62122d30870eaaa76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nULXgTjtrPJQMju-8AH2Sg.png"/></div></div></figure><p id="c683" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae md" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Cartpole-Double-Deep-Q-Learning" rel="noopener ugc nofollow" target="_blank"> <strong class="li iu">翻筋斗带双DQN </strong> </a></p><p id="dfba" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于<em class="mc"> CartPole-v0 </em>我们把<em class="mc"> </em> <code class="fe oh oi oj ok b"><strong class="li iu">Mε= 200;</strong></code>对于<em class="mc"> CartPole-v1，</em>我们把<em class="mc"> </em> <code class="fe oh oi oj ok b"><strong class="li iu">Mε= 150.</strong></code>回忆一下<code class="fe oh oi oj ok b"><strong class="li iu">Mε</strong></code> <strong class="li iu"> </strong>是<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code> <strong class="li iu"> </strong>达到最小值<code class="fe oh oi oj ok b"><strong class="li iu">ε_m.</strong></code>的那一集的数量</p><p id="7b08" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">3.<em class="mc">双DQN，弹弓-v0，</em>奖励<strong class="li iu"> 195 </strong>在第<strong class="li iu"> 612集达成。</strong></p><p id="98b0" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">4.<em class="mc">双DQN，横竿-v1，</em>奖励<strong class="li iu"> 475 </strong>在第<strong class="li iu"> 1030 </strong>集达成。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pe"><img src="../Images/7fbe24f2e390f9b7b3266b0fffe44407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWMEXARHjFsdvgQbum-RmQ.png"/></div></div></figure><p id="2589" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">超参数的选择</strong>T9】</p><p id="ffa2" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果<code class="fe oh oi oj ok b"><strong class="li iu">Mε</strong></code>设置得太大，那么<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>的选择将在<em class="mc">探索</em>的高概率(<code class="fe oh oi oj ok b"><strong class="li iu">&gt; ε_m</strong></code>)条件下进行很长时间。换句话说，很长一段时间<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>将在神经网络中没有信息积累的情况下进行。这意味着在移动<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">left</em></strong></code>或<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">right</em></strong></code>之间做出选择，我们可能会在很长一段时间内有一半的情况是错误的。</p><p id="6cc0" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果<code class="fe oh oi oj ok b"><strong class="li iu">Mε</strong></code>设置过小，那么<code class="fe oh oi oj ok b"><strong class="li iu">ε</strong></code>的选择将会在<em class="mc">利用</em>的高概率(<code class="fe oh oi oj ok b"><strong class="li iu">= ε_m</strong></code>)的情况下进行很长时间。这在神经网络训练的早期阶段可能非常糟糕，因为使用<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">argmax</em></strong></code>的<em class="mc">动作</em>的选择将从神经网络中做出，这仍然非常粗糙。那么在很多情况下，选择的<em class="mc">动作</em>就会被弄错。</p><h1 id="d5f0" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated">结论</h1><p id="97f4" class="pw-post-body-paragraph lg lh it li b lj np kc ll lm nq kf lo lp ot lr ls lt ou lv lw lx ov lz ma mb im bi translated">在开发<em class="mc"> DQN </em>和<em class="mc">双DQN </em>算法时，采取了三个步骤来对抗相关性和高估:(1)目标和局部网络，(2)经验重放机制，(3)将选择与评估分离。这些机制是通过大量使用两个相关的神经网络开发出来的。</p><h1 id="6ca7" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated">附录。关于PyTorch张量</h1><p id="b0d3" class="pw-post-body-paragraph lg lh it li b lj np kc ll lm nq kf lo lp ot lr ls lt ou lv lw lx ov lz ma mb im bi translated"><strong class="li iu">用torch.no_grad() </strong></p><p id="0cbc" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> PyTorch </em>函数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">no_grad()</em></strong></code> <strong class="li iu"> </strong>从渐变计算中排除一些元素。当确信没有执行<em class="mc">反向传播</em>过程时使用。该功能减少了内存消耗，参见<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">get_action().</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>使用<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">detach()</em></strong></code> <em class="mc"> </em>功能时也会出现类似的效果。<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">with</em></strong></code>声明阐明了对应于<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">try...finally </em></strong></code>块的代码。</p><p id="9e56" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> optim.zero_grad() </strong></p><p id="64ed" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">清除上一步的旧梯度(否则梯度将从所有<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">loss.backward()</em></strong></code>调用中累加)</p><p id="9c04" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">视图(1，1) </strong></p><p id="f1f2" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">该函数返回一个新的张量，与原始张量相同，但形状不同。试图去除<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">view(1,1)</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>中的<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">get_action()</em></strong></code><strong class="li iu"><em class="mc"/></strong>我们得到了<em class="mc">中不同形状的动作张量</em>的两个分支<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">get_action().</em></strong></code>然后在<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">learn()</em></strong></code>中我们得到了由各种形状的张量组成的<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">batch.action</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>。这是失败。函数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">view(1,1)</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>将形状从<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">tensor([a])</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>改变为<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">tensor([[a]]).</em></strong></code> <strong class="li iu"> <em class="mc"> </em> </strong>参数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">1,1</em></strong></code>表示每个维度的元素个数。例如，view(1，1，1，1，1)表示<br/> <code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">tensor([[[[[a]]]]]).</em></strong></code></p><p id="584a" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> torch.cat </strong></p><p id="3e3e" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">将给定的张量元组连接成单个张量。例如在<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">learn()</em></strong></code>函数中，<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">batch.state </em></strong></code>是形状[1，4]的64个张量的<em class="mc">元组</em>。函数<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">torch.cat</em></strong></code>将该元组转换为形状【64，4】的单个张量<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">states</em></strong></code>,如下所示:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pf"><img src="../Images/58ce54b1f272a1192870b84e561fa878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2B-05c0O2mrynQ89oj4EBQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om">States = torch . cat(batch . state)</strong></p></figure><p id="c9fd" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">整形(-1) </strong></p><p id="2ad1" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为什么我们要用<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">reshape(-1)</em></strong></code>来求<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_targets_next</em></strong></code>张量，见表2？在<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">learn()</em></strong></code>函数中我们比较两个张量:<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_targets.unsqueeze(1)</em></strong></code>和<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">Q_expected.</em></strong></code>如果我们不使用<code class="fe oh oi oj ok b"><strong class="li iu"><em class="mc">reshape</em></strong></code>函数，那么由表3可知这些张量具有不同的形状，那么比较就是失败的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pg"><img src="../Images/83b0a7293df3c98da5b922d04e610dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*neRurSj0lMVnQeKru4kOXQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated"><strong class="bd om">表3 learn()函数中比较的张量形状</strong></p></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ph"><img src="../Images/cd27443951ca72d8bd98b0adc9d33a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiOwLz-BXv2XUSyJzTyxVQ.png"/></div></div></figure><p id="d15f" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其他深度强化学习项目，见我的<a class="ae md" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity" rel="noopener ugc nofollow" target="_blank"> github目录</a>。关于贝尔曼方程和神经网络之间的相互关系，见<a class="ae md" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a">我之前的论文</a>。同一篇文章提供了关于<em class="mc"> PyTorch </em>的更多提示。</p><h1 id="a1ef" class="mv mw it bd mx my mz na nb nc nd ne nf kh ng ki nh kk ni kl nj kn nk ko nl nm bi translated">参考</h1><p id="c63d" class="pw-post-body-paragraph lg lh it li b lj np kc ll lm nq kf lo lp ot lr ls lt ou lv lw lx ov lz ma mb im bi translated">[1] V.Minh等人艾尔。，用深度强化学习玩雅达利(2013)，arXiv:1312.5602</p><p id="ab38" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[2] H .范·哈塞尔特等人。艾尔。，采用双Q学习的深度强化学习(2015)，arXiv:1509.06461</p><p id="b1db" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[3] A.Karpathy，深度强化学习:Pong from Pixels (2016)，karpathy.github.io</p><p id="55a2" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[4]魔方密码，双Q学习导论，(2020)，rubikscode.net</p><p id="3a4c" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[5] S.Karagiannakos，将深度Q网络推进一步，(2018)，TheAISummer</p><p id="0d45" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[6] V.Minh等人艾尔。，通过深度强化学习的人类级控制，(2015)，《自然》</p><p id="8560" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[7] R.Stekolshchik，贝尔曼方程在Deep RL中是如何工作的？，(2020)，走向数据科学</p><p id="5d68" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[8] C.Yoon，双深度Q-Networks 2019，走向数据科学</p><p id="8e17" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[9] S.Thrun和A.Schwartz，使用函数逼近进行强化学习的问题，(1993年)，<br/>卡内基梅隆大学机器人研究所<br/><br/>【10】F . Mutsch，cart pole with Q-Learning——open ai Gym的首次体验(2017年)，muetsch.io</p><p id="de77" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[11] T.Seno，欢迎来到深度强化学习第一部分:DQN，(2017)，走向数据科学</p><p id="27a6" class="pw-post-body-paragraph lg lh it li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[12]<a class="ae md" rel="noopener" target="_blank" href="/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb">https://towards data science . com/dqn-part-1-vanilla-deep-q-networks-6 EB 4a 00 feb FB</a></p></div></div>    
</body>
</html>