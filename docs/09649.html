<html>
<head>
<title>A Mathematical Explanation of AdaBoost in 5 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5 分钟 AdaBoost 的数学解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382?source=collection_archive---------9-----------------------#2020-07-09">https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382?source=collection_archive---------9-----------------------#2020-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bda0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一个例子彻底解释 AdaBoost</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bae105070c7c6e519b780a8036843b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLUhrb27BKMtXAXRfy15Vw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的图像</p></figure><h1 id="ff0c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">目录</h1><ol class=""><li id="ad28" class="lq lr it ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">介绍</li><li id="f61b" class="lq lr it ls b lt mi lv mj lx mk lz ml mb mm md me mf mg mh bi translated">AdaBoost 有何不同</li><li id="c038" class="lq lr it ls b lt mi lv mj lx mk lz ml mb mm md me mf mg mh bi translated">AdaBoost 工作原理的示例</li><li id="131a" class="lq lr it ls b lt mi lv mj lx mk lz ml mb mm md me mf mg mh bi translated">如何评估一个新的点</li></ol><h1 id="e1d2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="113d" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">AdaBoost，或称自适应 Boost，是一种相对较新的机器学习分类算法。这是一个<strong class="ls iu">集成</strong>算法，它将许多弱学习器(决策树)组合在一起，并将其转化为一个强学习器。因此，其算法利用<strong class="ls iu">装袋</strong>和<strong class="ls iu">助推</strong>方法来开发增强的预测器。</p><p id="cad9" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">如果这些词让你感到困惑，不要担心。在本文中，我们将通过一个简单的例子来展示 AdaBoost 的工作原理及其背后的数学原理。</p><h1 id="5135" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">AdaBoost 有何不同</h1><p id="af40" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">AdaBoost 类似于随机森林，因为预测来自许多决策树。然而，AdaBoost 的独特之处主要有三点:</p><ol class=""><li id="047b" class="lq lr it ls b lt na lv nb lx nf lz ng mb nh md me mf mg mh bi translated">首先，AdaBoost 创造了一个树桩森林，而不是树木。树桩是只由一个节点和两片叶子组成的树(如上图)。</li><li id="90ca" class="lq lr it ls b lt mi lv mj lx mk lz ml mb mm md me mf mg mh bi translated">第二，生成的树桩在最终决策(最终预测)中的权重不同。造成更多错误的树桩在最终决策中的发言权会更小。</li><li id="59b8" class="lq lr it ls b lt mi lv mj lx mk lz ml mb mm md me mf mg mh bi translated">最后，生成树桩的顺序很重要，因为每个树桩都旨在减少前一个树桩所犯的错误。</li></ol><h1 id="0f13" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">AdaBoost 工作原理的示例</h1><p id="7521" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">现在我们来看一个例子。假设我们有下面的样本数据，有三个特征(x1，x2，x3)和一个输出(Y)。<em class="ni">注意 T =真，F =假。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1ddf7ec650557326fdeffd6d6a36ba94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Rg1Hq7vri-RwkEAAIDsp2w.png"/></div></figure><h2 id="62fb" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">步骤 1:为每个样品分配样品重量</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/a48dea356c5d4e4ae60924d971ca82e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SylpC7fzbjXv32tgeS8JPw.png"/></div></div></figure><p id="feaf" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">使用上面的等式，计算每个样品的样品重量。第一轮，样品重量相等。在本例中，每个样品的样品重量将等于 1/6。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/96fbfa888b5e6f71cff45f580a98ead2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*W6mMdotMVmsmmc5Dqqs3WA.png"/></div></figure><h2 id="13b1" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">第二步:计算每个变量的基尼系数</h2><p id="4b09" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">下一步是计算每个变量的基尼系数。这样做是为了确定使用哪个变量来创建第一个树桩。计算每个节点的基尼系数的公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b602174a5412bd77d88453cc35d8c99d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*5gKCPY8lklHDhoZb3M_geA.png"/></div></figure><p id="ab7a" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">一旦你计算出每个节点的基尼系数，每个变量的基尼系数就是每个节点的加权平均值。</p><p id="8cb0" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">举个例子，我们来计算 x2 的基尼杂质。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/4e3fd96996ecb4f605942c28f040110e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMzOpfL29OiL8XO5-ypMFQ.png"/></div></div></figure><p id="b0a2" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">上面是样本的合并表，显示了适合每个类别的样本数(无论 x2 和 Y 是 T 还是 F)。</p><p id="16b8" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">接下来，我们可以为 x2 计算每个叶节点的 Gini 杂质。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/1fa71f6f12e199f543ce109864d1543b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvvWkP6oHgrWbiViH-_wWQ.png"/></div></div></figure><p id="f137" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">一旦计算出每个叶节点的基尼系数，就可以通过对两个单独的系数进行加权平均来计算出总的基尼系数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/abfa22009628f5b4c5b0c151ab4a0a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/0*7AQUyX6CeU5MHXPz"/></div></figure><p id="17ff" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">因此，x2 的基尼系数= 0.25。</p><p id="7580" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">如果你对每个变量都这样做，你会得到 x2 具有最低的基尼系数，所以 x2 将被用来创建第一个树桩。</p><h2 id="1a49" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">第三步:计算已创建的树桩的发言权</h2><p id="97e1" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">接下来，我们将使用总误差来计算这个树桩获得的“发言权”。</p><p id="3682" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated"><strong class="ls iu">总误差</strong> <strong class="ls iu">等于错误分类样本的权重之和。</strong>由于其中一个样本被错误地分类为 x2，总误差等于 1/6。</p><p id="b0a7" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">一旦你知道了总误差，你就可以计算出<strong class="ls iu">量，比如:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/49ad2b9c70e85ac19c5f3944e66aede9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*-XwQ0g0qIfZblSyYPgGuMQ.png"/></div></figure><p id="edfb" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">因此对于这个树桩…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/5dd0785ef457b7695f2ed8319a9ee8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*kzWCe0iZSiO88LU2FTYZvg.png"/></div></figure><h2 id="9289" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">步骤 4:计算下一个树桩的新样本权重</h2><p id="6f57" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">接下来，我们将使用以下等式增加错误分类样本的样本权重，并减少正确分类样本的样本权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/801f03626928e9ac5c0260bc47a4c78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9q--XyIDnsLyDtS4LAxvhw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/970b5cf2dfd305d53fffbca3276d2088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThIcJ-ycECTY8W3N1vvJ4Q.png"/></div></div></figure><p id="90d4" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">因此，利用上面的等式，我们能够计算新的样本权重。由于样本权重的总和等于 0.84，我们通过将每个权重除以 0.84 来标准化样本权重，使得新样本权重的总和等于 1。</p><h2 id="1c71" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">步骤 5:创建一个引导数据集，根据新的样本权重选择每个样本的概率。</h2><p id="a836" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">在这一步中，我们将从数据集中随机选择 6 个带有替换物的样本<strong class="ls iu">，选择每个样本的几率基于它们新的样本重量。</strong></p><p id="f30c" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">请注意，被错误分类的那一个的权重是其他的两倍多。这意味着它更有可能被选择多次，因此，下一个 stump 将更多地关注正确分类错误的样本。这就是 AdaBoost 的力量！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/118fbdc3ae561c74cec86c56ce3d988c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68LQdoKPDU0Xnz4xhwcFJQ.png"/></div></div></figure><p id="f19d" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">一旦创建了新的引导数据集，样本将再次被赋予相等的权重，并重复该过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/1f6fc9b355ba7a808d72128de547f928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VE4JA7NBftaVTZhUmE2uzA.png"/></div></div></figure><h2 id="b8b6" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">步骤 6:重复这个过程 n 次</h2><p id="49df" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">最后，重复这个过程，直到产生 n 个树桩，每个树桩都有自己的发言权。一旦完成，模型就完成了，可以对新点进行分类。</p><p id="f996" class="pw-post-body-paragraph mn mo it ls b lt na ju mp lv nb jx mq lx nc ms mt lz nd mv mw mb ne my mz md im bi translated">通过在所有树桩中运行新点并查看它们是如何分类的，可以对新点进行分类。然后，对每一个类的 say 量进行求和，say 量较高的类就是新点的分类。</p><h1 id="6102" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">感谢阅读！</h1><p id="1a82" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated">学完本课程后，您应该理解 AdaBoost 模型是如何创建的，并且应该知道它背后的数学原理。</p><h2 id="f744" class="nk kz it bd la nl nm dn le nn no dp li lx np nq lk lz nr ns lm mb nt nu lo nv bi translated">特伦斯·申</h2><p id="761f" class="pw-post-body-paragraph mn mo it ls b lt lu ju mp lv lw jx mq lx mr ms mt lz mu mv mw mb mx my mz md im bi translated"><em class="ni">创始人</em><a class="ae oi" href="https://shintwin.com/" rel="noopener ugc nofollow" target="_blank"><em class="ni">ShinTwin</em></a><em class="ni">|我们连线上</em><a class="ae oi" href="https://www.linkedin.com/in/terenceshin/" rel="noopener ugc nofollow" target="_blank"><em class="ni">LinkedIn</em></a><em class="ni">|项目组合是</em> <a class="ae oi" href="http://terenceshin.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ni">这里</em> </a> <em class="ni">。</em></p></div></div>    
</body>
</html>