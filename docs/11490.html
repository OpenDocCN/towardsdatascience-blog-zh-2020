<html>
<head>
<title>mlmachine - Hyperparameter Tuning with Bayesian Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于贝叶斯优化的多机超参数整定</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d?source=collection_archive---------32-----------------------#2020-08-09">https://towardsdatascience.com/mlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d?source=collection_archive---------32-----------------------#2020-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b97b8885fcdadde567a21f38c5c2c6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phWwscV6aRCycLfvoQfdnQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4197733" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jd" href="https://pixabay.com/users/ThorstenF-7677369/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4197733" rel="noopener ugc nofollow" target="_blank">托尔斯滕·弗伦泽尔</a></p></figure><h2 id="abec" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">多层机器</h2><div class=""/><div class=""><h2 id="d6c6" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">这个新的 Python 包加速了基于笔记本的机器学习实验</h2></div><h2 id="5d8c" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">TL；速度三角形定位法(dead reckoning)</h2><p id="d2af" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">mlmachine 是一个 Python 库，用于组织和加速基于笔记本的机器学习实验。</p><p id="3934" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在本文中，我们使用 mlmachine 来完成原本需要大量编码和工作的操作，包括:</p><ul class=""><li id="5d95" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated">一次多估值器的贝叶斯优化</li><li id="88fe" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">结果分析</li><li id="285b" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">模型再现</li></ul><p id="216b" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">查看这篇文章的<a class="ae jd" href="https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_4.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>。</p><p id="9062" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在 GitHub 上查看<a class="ae jd" href="https://github.com/petersontylerd/mlmachine" rel="noopener ugc nofollow" target="_blank">项目</a>。</p><p id="ecea" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">并查看过去的 mlmachine 文章:</p><div class="ip iq gp gr ir nl"><a rel="noopener follow" target="_blank" href="/mlmachine-clean-ml-experiments-elegant-eda-pandas-pipelines-daba951dde0a"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd jq gy z fp nq fr fs nr fu fw jp bi translated">mlmachine -干净的 ML 实验，优雅的 EDA 和 Pandas 管道</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">这个新的 Python 包加速了基于笔记本的机器学习实验</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ix nl"/></div></div></a></div><div class="ip iq gp gr ir nl"><a rel="noopener follow" target="_blank" href="/mlmachine-groupbyimputer-kfoldencoder-and-skew-correction-357f202d2212"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd jq gy z fp nq fr fs nr fu fw jp bi translated">ml machine-group by inputr、KFoldEncoder 和倾斜校正</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">这个新的 Python 包加速了基于笔记本的机器学习实验</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="oa l nw nx ny nu nz ix nl"/></div></div></a></div><div class="ip iq gp gr ir nl"><a rel="noopener follow" target="_blank" href="/mlmachine-crowd-sourced-feature-selection-50cd2bbda1b7"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd jq gy z fp nq fr fs nr fu fw jp bi translated">mlmachine -众包特征选择</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">这个新的 Python 包加速了基于笔记本的机器学习实验</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="ob l nw nx ny nu nz ix nl"/></div></div></a></div></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h1 id="74c6" class="oj lf jg bd lg ok ol om lj on oo op lm kv oq kw lq ky or kz lu lb os lc ly ot bi translated"><strong class="ak">一次多估值器的贝叶斯优化</strong></h1><p id="99f0" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">贝叶斯优化通常被描述为超越穷举网格搜索的进步，这是正确的。这种超参数调整策略通过使用先验信息来通知给定估计器的未来参数选择而成功。查看<a class="ae jd" rel="noopener" target="_blank" href="/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0"> Will Koehrsen 关于 Medium </a>的文章，获得关于该包的精彩概述。</p><p id="c935" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">mlmachine 使用 hyperopt 作为执行贝叶斯优化的基础，并通过简化的工作流将 hyperopt 的功能性向前推进了一步，该工作流允许在单个流程执行中优化多个模型。在本文中，我们将优化四个分类器:</p><ul class=""><li id="27b6" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">LogisticRegression()</strong></code></li><li id="898b" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">XGBClassifier()</strong></code></li><li id="6122" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">RandomForestClassifier()</strong></code></li><li id="9a25" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">KNeighborsClassifier()</strong></code></li></ul><h2 id="7826" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">准备数据</h2><p id="9f3f" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">首先，我们应用数据预处理技术来清理数据。我们将首先创建两个<code class="fe ou ov ow ox b"><strong class="mb jq">Machine()</strong></code>对象——一个用于训练数据，另一个用于验证数据:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="9cf2" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">现在，我们通过输入空值和应用各种宁滨、特征工程和编码技术来处理数据:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="121d" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">这里是输出，还在一个<code class="fe ou ov ow ox b"><strong class="mb jq">DataFrame</strong></code>:</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/1da011f7327d42af925794d94fa50861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZzX00c8ZOkalkjR0Y4zRQ.jpeg"/></div></div></figure><h2 id="14c6" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">功能重要性摘要</h2><p id="79de" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">作为第二个准备步骤，我们希望为每个分类器执行特征选择:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h2 id="5f67" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated"><strong class="ak">穷举迭代特征选择</strong></h2><p id="5bd3" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">对于我们最后的准备步骤，我们使用这个特征选择概要对每个估计量的越来越小的特征子集进行迭代交叉验证:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="b38f" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">从这个结果中，我们提取每个估计量的最佳特征集的字典:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="b9c5" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">关键字是估计器名称，相关值是包含每个估计器的最佳性能特征子集的列名的列表。以下是<code class="fe ou ov ow ox b"><strong class="mb jq">XGBClassifier()</strong></code>的键/值对，它仅使用了 43 个可用特征中的 10 个来实现验证数据集上的最佳平均交叉验证准确性:</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/79210fe2302ac70e0dbbee1e9f54b9b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*unGuuJk8uI-o0dfSWJJ77w.jpeg"/></div></figure><p id="3c0b" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">有了经过处理的数据集和最佳特征子集，是时候使用贝叶斯优化来调整 4 个估计器的超参数了。</p><h2 id="72c0" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">概述我们的特征空间</h2><p id="9040" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">首先，我们需要为每个估计量的每个参数建立我们的特征空间:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="5460" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">字典最外层的键是分类器的名称，用字符串表示。关联的值也是字典，其中键是参数名，表示为字符串，值是从中选择参数值的超点采样分布。</p><h2 id="eee7" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">运行贝叶斯优化作业</h2><p id="aca0" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">现在，我们准备运行我们的贝叶斯优化超参数调优工作。我们将使用一个属于我们的<code class="fe ou ov ow ox b"><strong class="mb jq">Machine()</strong></code>对象<code class="fe ou ov ow ox b"><strong class="mb jq">exec_bayes_optim_search()</strong></code>的内置方法。让我们看看 mlmachine 的运行情况:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="a83b" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">让我们回顾一下参数:</p><ul class=""><li id="8534" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">estimator_parameter_space</strong></code>:我们上面设置的基于字典的特征空间。</li><li id="0b78" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">data</strong></code>:我们的观察。</li><li id="2517" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">target</strong></code>:我们的目标数据。</li><li id="ed2c" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">columns</strong></code>:可选参数，允许我们对输入数据集特征进行子集划分。接受功能名称列表，该列表将同等地应用于所有估算器。还接受一个字典，其中键表示估计器类名，值是与相关估计器一起使用的功能名称列表。在这个例子中，我们通过传入由上面的<code class="fe ou ov ow ox b"><strong class="mb jq">FeatureSelector()</strong></code>工作流中的<code class="fe ou ov ow ox b"><strong class="mb jq">cross_val_feature_dict()</strong></code>返回的字典来使用后者。</li><li id="a9a1" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">scoring</strong></code>:待评估的评分标准。</li><li id="f74e" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">n_folds</strong></code>:交叉验证程序中使用的折叠数。</li><li id="ad19" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">iters</strong></code>:运行超参数调整过程的总迭代次数。在这个例子中，我们运行了 200 次实验。</li><li id="0953" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated"><code class="fe ou ov ow ox b"><strong class="mb jq">show_progressbar</strong></code>:控制进程中进度条是否显示和主动更新。</li></ul><p id="67a1" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">任何熟悉远视的人都会想知道目标函数在哪里。mlmachine 抽象掉了这种复杂性。</p><p id="424e" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">过程运行时间取决于几个属性，包括硬件、使用的估计器的数量和类型、折叠次数、特征选择和采样迭代次数。运行时可能会很长。因此，<code class="fe ou ov ow ox b"><strong class="mb jq">exec_bayes_optim_search()</strong></code>自动将每次迭代的结果保存到一个 CSV 文件中。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h1 id="513c" class="oj lf jg bd lg ok ol om lj on oo op lm kv oq kw lq ky or kz lu lb os lc ly ot bi translated">结果分析</h1><h2 id="3438" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated"><strong class="ak">结果汇总</strong></h2><p id="bc64" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">让我们从加载和检查结果开始:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/6a1dbb05a74a35cd62e668100367bc97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qt0ezaLU4mwLPth33f37ng.jpeg"/></div></div></figure><p id="e48f" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">我们的贝叶斯优化日志保存了每次迭代的关键信息:</p><ul class=""><li id="4b31" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated">迭代次数、估计量和评分标准</li><li id="ba15" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">交叉验证汇总统计数据</li><li id="871b" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">迭代训练时间</li><li id="c108" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">使用的参数字典</li></ul><p id="0dad" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">该日志为我们分析和评估贝叶斯优化过程的有效性提供了大量数据。</p><h2 id="006b" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated"><strong class="ak">模型优化评估</strong></h2><p id="c7ae" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">首先也是最重要的，我们想看看性能在迭代过程中是如何提高的。</p><p id="f834" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">让我们通过迭代来可视化<code class="fe ou ov ow ox b"><strong class="mb jq">XGBClassifier()</strong></code>损失:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/769fd72f66321c8b18c22753aef0d8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh4VxHvBznv7ScaGVInV3A.jpeg"/></div></div></figure><p id="297d" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">每个点代表我们 200 个实验之一的表现。需要注意的关键细节是，最佳拟合线有一个明显的向下斜率——这正是我们想要的。这意味着，与之前的迭代相比，每次迭代的模型性能都会有所提高。</p><h2 id="3b83" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated"><strong class="ak">参数选择评估</strong></h2><p id="fbe2" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">贝叶斯优化最酷的部分之一是看到参数选择是如何优化的。</p><p id="9013" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">对于每个模型和每个模型的参数，我们可以生成两个面板的视觉效果。</p><p id="83e2" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">对于数字参数，如<code class="fe ou ov ow ox b"><strong class="mb jq">n_estimators</strong></code>或<code class="fe ou ov ow ox b"><strong class="mb jq">learning_rate</strong></code>，双视觉面板包括:</p><ul class=""><li id="3d6e" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated">参数选择 KDE，过度强调理论分布 KDE</li><li id="6c18" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">通过迭代散点图和最佳拟合线进行参数选择</li></ul><p id="7d57" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">对于分类参数，如损失函数，双视觉面板包括:</p><ul class=""><li id="7970" class="mx my jg mb b mc ms mf mt ln mz lr na lv nb mr nc nd ne nf bi translated">参数选择和理论分布条形图</li><li id="c596" class="mx my jg mb b mc ng mf nh ln ni lr nj lv nk mr nc nd ne nf bi translated">通过迭代散点图选择参数，按参数类别分面</li></ul><p id="6a7f" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">让我们回顾一下<code class="fe ou ov ow ox b"><strong class="mb jq">KNeighborsClassifier()</strong></code>的参数选择面板:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pi"><img src="../Images/37bd713cf6157ffe6398e485c1b3ea74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kwKgd0g3Sj3e27FKJLdgqQ.gif"/></div></div></figure><p id="8b1b" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">内置方法<code class="fe ou ov ow ox b"><strong class="mb jq">model_param_plot()</strong></code>循环遍历估计器的参数，并根据每个参数的类型显示适当的面板。让我们分别看一下数值参数和分类参数。</p><p id="cf19" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">首先，我们将查看数字参数<code class="fe ou ov ow ox b"><strong class="mb jq">n_neighbors</strong></code>的面板:</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pj"><img src="../Images/c5b55cb543bb5bc5d936d6c7be2382e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxX_yGh9OI1nfnro6yVL_Q.jpeg"/></div></div></figure><p id="7872" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在左侧，我们可以看到两个重叠的核密度图，总结了实际参数选择和理论参数分布。紫色线对应于理论分布，正如所料，这条曲线是平滑和均匀分布的。蓝绿色线对应于实际的参数选择，很明显，hyperopt 更喜欢 5 到 10 之间的值。</p><p id="57dc" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在右侧，散点图显示了迭代过程中的<code class="fe ou ov ow ox b"><strong class="mb jq">n_neighbors</strong></code>值选择。随着贝叶斯优化过程在 7 左右的值上磨砺，最佳拟合线稍微向下倾斜。</p><p id="3753" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">接下来，我们将查看分类参数<code class="fe ou ov ow ox b"><strong class="mb jq">algorithm</strong></code>的面板:</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/3b3904cfc4ccc862d284603611c1d4a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpC673IKxmSplDhzJJjFnw.jpeg"/></div></div></figure><p id="fcc5" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在左侧，我们看到一个条形图，显示了参数选择的计数，按实际参数选择和理论分布选择分面。代表理论分布选择的紫色条比代表实际选择的蓝绿色条更均匀。</p><p id="c4f5" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">在右侧，散点图再次显示了迭代过程中的<code class="fe ou ov ow ox b"><strong class="mb jq">algorithm</strong></code>值选择。在迭代过程中，“ball_tree”和“auto”的选择明显减少，而“kd_tree”和“brute”更受青睐。</p><h1 id="8e33" class="oj lf jg bd lg ok pl om lj on pm op lm kv pn kw lq ky po kz lu lb pp lc ly ot bi translated">模型再现</h1><h2 id="f899" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">顶级模型识别</h2><p id="3f33" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">我们的<code class="fe ou ov ow ox b"><strong class="mb jq">Machine()</strong></code>对象有一个名为<code class="fe ou ov ow ox b"><strong class="mb jq">top_bayes_optim_models()</strong></code>的内置方法，它根据我们的贝叶斯优化日志中的结果为每种估计器类型确定最佳模型。</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="3173" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">使用这种方法，我们可以根据平均交叉验证分数来确定每个估计量的前 N 个模型。在这个实验中，<code class="fe ou ov ow ox b"><strong class="mb jq">top_bayes_optim_models()</strong></code>返回下面的字典，它告诉我们<code class="fe ou ov ow ox b"><strong class="mb jq">LogisticRegression()</strong></code>在第 30 次迭代中确定了它的顶级模型，<code class="fe ou ov ow ox b"><strong class="mb jq">XGBClassifier()</strong></code>在第 61 次迭代中，<code class="fe ou ov ow ox b"><strong class="mb jq">RandomForestClassifier()</strong></code>在第 46 次迭代中，<code class="fe ou ov ow ox b"><strong class="mb jq">KNeighborsClassifier()</strong></code>在第 109 次迭代中。</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/3293b0b62dd144803e03a81da54cf7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*1rftK-jLai1UNx9s6STjiw.jpeg"/></div></figure><h2 id="e86c" class="le lf jg bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly jm bi translated">将模型投入使用</h2><p id="2620" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">为了重新实例化一个模型，我们利用了我们的<code class="fe ou ov ow ox b"><strong class="mb jq">Machine()</strong></code>对象的内置方法<code class="fe ou ov ow ox b"><strong class="mb jq">BayesOptimClassifierBuilder()</strong></code>。为了使用这个方法，我们传递我们的结果日志，指定一个估计类和迭代次数。这将使用存储在日志记录中的参数实例化一个模型对象:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="e54f" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">这里我们看到了模型参数:</p><figure class="oy oz pa pb gt is gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/76265c43a6b7ed8b5b55434ddeac290f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*UNDd7b1em1TWNtewzpxH3g.png"/></div></figure><p id="e5a6" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">用<code class="fe ou ov ow ox b"><strong class="mb jq">BayesOptimClassifierBuilder()</strong></code>实例化的模型使用<code class="fe ou ov ow ox b"><strong class="mb jq">.fit()</strong></code>和<code class="fe ou ov ow ox b"><strong class="mb jq">.predict()</strong></code>的方式应该感觉很熟悉。</p><p id="9fff" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">让我们以一个非常基本的模型性能评估来结束本文。我们将在训练数据和标签上拟合此<code class="fe ou ov ow ox b"><strong class="mb jq">RandomForestClassifier()</strong></code>，使用训练数据生成预测，并通过将这些预测与实际情况进行比较来评估模型的性能:</p><figure class="oy oz pa pb gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="oy oz pa pb gt is gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/5f1722a1ec7b1557227b338334e205c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*gzLpPgnEdEScsDlIMHDLkQ.jpeg"/></div></figure><p id="c84d" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">任何熟悉 Scikit-learn 的人都应该有宾至如归的感觉。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h1 id="7f0e" class="oj lf jg bd lg ok ol om lj on oo op lm kv oq kw lq ky or kz lu lb os lc ly ot bi translated">最后</h1><p id="c651" class="pw-post-body-paragraph lz ma jg mb b mc md kq me mf mg kt mh ln mi mj mk lr ml mm mn lv mo mp mq mr ij bi translated">mlmachine 使得一次有效地优化多个估计器的超参数变得容易，并且便于模型改进和参数选择的可视化检查。</p><p id="5bc0" class="pw-post-body-paragraph lz ma jg mb b mc ms kq me mf mt kt mh ln mu mj mk lr mv mm mn lv mw mp mq mr ij bi translated">查看<a class="ae jd" href="https://github.com/petersontylerd/mlmachine" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>，并关注其他专栏条目。</p></div></div>    
</body>
</html>