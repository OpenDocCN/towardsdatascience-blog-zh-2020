<html>
<head>
<title>Make Your KNN Smooth with Gaussian Kernel</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用高斯核使你的KNN平滑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/make-your-knn-smooth-with-gaussian-kernel-7673fceb26b9?source=collection_archive---------26-----------------------#2020-02-14">https://towardsdatascience.com/make-your-knn-smooth-with-gaussian-kernel-7673fceb26b9?source=collection_archive---------26-----------------------#2020-02-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="08c0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">找出让你的KNN更强大的方法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ea2efcbf15bef30d8ab8aa02ed504679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jc7FB_BbV9vbYuXTx5wyeA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自https://unsplash.com/photos/8Zv3Ygm2CLM<a class="ae ky" href="https://unsplash.com/photos/8Zv3Ygm2CLM" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="d3d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章假设你知道KNN的基本知识。所有源代码都在https://github.com/seho0808/knn_gaussian_medium的<a class="ae ky" href="https://github.com/seho0808/knn_gaussian_medium" rel="noopener ugc nofollow" target="_blank"/>。请随意使用。</p><p id="519a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有的要点都在第一部分和第二部分。之后只是为了帮助你更好地理解和彻底浏览源代码。</p><h1 id="03d2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">第1部分:默认KNN是离散的</h1><p id="833c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一般来说，已知使用KNN和随机森林的回归具有离散的预测值。换句话说，预测值不是平滑的，因为我们取原始数据点的平均值。如果你看下面的图，你可以看到默认KNN的预测是非常离散的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/826b94c39a8b92145c0b89deb99d7314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LL_xDCTP38tpePmru2N_-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1均匀权重的KNN插值</p></figure><p id="787a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-Learn库中的默认KNN回归器只是在K-neighbors中使用统一的权重。换句话说，我们简单地取K个最近邻居的平均值。Scikit-Learn库为此提供了另一种选择:反距离加权。这使得通过使用距离的倒数作为权重，更近的点对预测有更大的影响。下面的图2显示了反距离加权。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/7f839aa640e9805e98aef9ae7729560b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VB2fhbpu8QfuDRLhV29F-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2使用反距离权重的KNN插值</p></figure><p id="4a14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在看起来比以前光滑了。您注意到的第一件事是X=(2，4)中的小跳跃。这些缝隙在那里是因为我们没有太多的输入数据。然而，真正的问题是预测曲线似乎过度适应真实数据的噪声。如果给定数据中没有随机噪声，这可能不是过度拟合，但在大多数情况下，像这样完全拟合到单个点是有害的。假设在X=6处给定一个新点。如果我们的预测曲线过度拟合随机噪声，新值的误差将会很高。要检查模型是否过度拟合，可以进行交叉验证。</p><h1 id="e48c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">第2部分:过度拟合的解决方案</h1><p id="9fd9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了解决这个过度拟合的问题，我们需要提出一个新的权重函数。可以说最著名的核函数高斯核解决了过拟合问题。图3显示了结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/97c8869d2488fe4adcacff4275465271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RuBK3Jbz-vscHTd4YgMAFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3高斯内核的KNN插值(宽度=2)</p></figure><p id="9fbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">高斯内核使用下面图4中的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/15d9900fe16133532fe2200fe23a1295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUanHX0Hj8LXGUvd9oqJLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4高斯核方程</p></figure><p id="e5be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">w是权重，d(a，b)是a和b之间的距离，σ是我们设定的一个参数。查询点是我们试图估计的点，所以我们取K个最近点之一的距离，并赋予其权重，如图4所示。那么我们如何在Python中做到这一点呢？图5显示了我的源代码的截图。我在文章顶部附上了Github链接，供大家免费使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/83eaa143b1c1723e3af06c0bdd293503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rdgwcuu6Vox8gSqJeg9uHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5在Scikit-Learn上使用高斯内核的源代码</p></figure><p id="7406" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图5所示，我们将<em class="mv"> weights </em>参数设置为自定义函数。需要注意的一件重要事情是，您可以将<em class="mv">内核宽度</em>设置为您想要的任何值。对于默认的KNN，您只需要调整一个参数:K-最近邻。使用高斯核时，还需要调整核宽度参数。我建议对每个参数组合进行交叉验证的网格搜索。(即，对于K=4，宽度=2为10 K倍，对于K=4，宽度=2.1为10 K倍，等等。)</p><h1 id="a958" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">第3部分:调优内核宽度和n邻居</h1><p id="7b31" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这部分更像是附录。这个的源代码也包含在顶部的同一个Github链接中。我生成了图6中的数据。简单来说就是y=x+ε，其中ε~N(0，5)。我们将学习如何调整高斯核KNN。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/f3f7d658d6d7f1c00d7e7d846380672f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEpD6QmYwAfdyprpGYZlTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6生成的数据</p></figure><p id="72be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要调整两个参数:n_neighbors和kernel_width。“n_neighbors”控制我们在KNN使用多少邻居。“kernel_width”控制高斯核中的分割参数。有许多方法可以优化超参数。优化算法完全是另一个话题，所以我不会深入细节。通常，对于简单的问题，我们使用网格搜索和随机搜索。网格搜索简单地说就是沿着笛卡尔平面线性探测超参数的组合。随机搜索是设置边界并通过超参数的随机组合进行搜索。图7显示了搜索结果。我结合使用了网格搜索和随机搜索。n_neighbors采用网格搜索方式，因为我们用尽了范围[1，11]内的所有值。kernel_width采用随机搜索方式，我们在0到100之间随机选择100个值。(确切的说，我用的是“<em class="mv"> np.random.randint(5，100000，100)/1000”</em>)。</p><p id="908c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个组合(图7中的单个数据点)，我运行了100次引导运行。换句话说，图7中的每个点都是100次KNNruns的平均误差度量。自举运行可以替代K-fold。在这种情况下，我采用9:1的测试比率来训练数据集大小。为了更容易理解，引导交叉验证过程应该是:</p><blockquote class="mx my mz"><p id="0ccd" class="kz la mv lb b lc ld ju le lf lg jx lh na lj lk ll nb ln lo lp nc lr ls lt lu im bi translated">随机分割90%的训练集和10%的测试集= &gt;用选择的90%的训练集训练KNN模型= &gt;预测10%的测试集并获得准确性指标= &gt;同样，随机分割90%的训练集和10%的测试集= &gt;…做100次</p></blockquote><p id="0273" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我通常会做10~20次引导运行，但是越多越好。在这种情况下，数据集很小，所以我运行了100次。如果您想使用K-fold，请随意。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/c73143dcfbd9eb1669cfa42c37e6bff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBG7drsJ3Vx0Xulonuuq-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7优化两个超参数的随机网格搜索。</p></figure><p id="dc58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么我们如何解读图7呢？正如你在右边看到的颜色条，我们想要浅蓝色，这意味着低误差值。似乎总的趋势是，从6个邻居和更高的邻居开始，误差值的递减趋势似乎趋于平稳。此外，对于kernel_width，任何高于20值似乎都会产生较低的误差。从这里开始，一切都是启发式的:您可以根据自己的项目使用自己的逻辑来选择超参数。让我们检查最小误差点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/aed2170f884f3b019a7fb495c99bddae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*E5D7NRtzbyjH-lBlaWO7tg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8获取图7中最小点的代码截图。</p></figure><p id="4f64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最小误差似乎是在7个邻居处，对于核宽度为61.739，平均绝对误差为3.7698。在许多其他应用中，您可能也希望围绕最小值参数进行网格搜索，但我们在这里将跳过这一步。<strong class="lb iu">更需要注意的是，内核宽度的高值会使预测不平滑！</strong>实际上，我们的收益率值61.739绘制成图9。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/fd035ce22f6fb6cac7b4715a5bd0d32d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyp4wbsExWAVI5Yji0lzIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9可以看到贴合不平整！</p></figure><p id="9e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那我们该怎么办？如果降低宽度值，可以看到从kernel_width=10或更低的值开始，曲线变得更加平滑。因此，如果您想要获得平滑的曲线和高精度，您将不得不从小于10的kernel_width中选取较低的误差值。图10显示了结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/7f67654bea20d4e2303197677bd8670f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9apsykrVXkU260XNpjd9BA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10你可以看到kernel_width=8产生了更平滑的拟合。</p></figure><p id="4f31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对此我们该如何解读？好吧，至少对于这个生成的数据，统一权重似乎比高斯核稍微好一点。然而，对于更平滑的曲线，误差度量的差异也不是很大，因此您可以使用高斯核或统一权重。对于您自己的应用，您还必须仔细查看误差指标，以选择要使用的权重。生成您自己的图，如图7所示，以查看各种选项的趋势。</p><h1 id="5180" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="fe30" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">根据你的项目或应用，使用高斯核或任何其他核<strong class="lb iu">可以</strong>帮助你的模型更好地拟合数据。这篇文章展示了如何使用高斯核，这是最流行的核之一。如果您发现本文中有任何错误或有任何疑问，请通过koolee33@gmail.com或seho0808@vt.edu<strong class="lb iu">发邮件给我。你也可以用这篇文章的评论部分，但是我会反应比较慢。</strong></p><h1 id="63be" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ol class=""><li id="0cf0" class="nf ng it lb b lc mn lf mo li nh lm ni lq nj lu nk nl nm nn bi translated">Scikit-Learn库KNN回归器链接(你可以在这里找到权重选项):<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . neighbors . kneighborsregressor . html</a></li><li id="d8b0" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">对于KNN使用的其他内核:<a class="ae ky" href="https://www.ismll.uni-hildesheim.de/lehre/ml-07w/skript/ml-2up-03-nearest-neighbor.pdf" rel="noopener ugc nofollow" target="_blank">https://www . ismll . uni-hildesheim . de/lehre/ml-07w/skript/ml-2up-03-nearest-neighbor . pdf</a></li></ol></div></div>    
</body>
</html>