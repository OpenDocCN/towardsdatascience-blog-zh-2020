# 为什么批量归一化在深度神经网络中有用？

> 原文：<https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf?source=collection_archive---------8----------------------->

## 数据科学，深度学习

## 批量规范化有助于深度神经网络的规范化

![](img/df5617b41434c91db0d5b9c59dabb6d4.png)

纳斯蒂亚·杜尔希尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在本文中，您可以探索规范化与批量规范化，为什么深度学习需要批量规范化，批量规范化在深度神经网络中是如何执行的，以及批量规范化的优势是什么？

# **什么是规范化 vs 批量规范化？**

**标准化**是将数据集中数值变量的值更改为典型标度的过程，不会在值的范围内形成错误的对比。

**批量标准化**是一种用于训练深度神经网络的技术，可对每个小批量的层贡献进行标准化。这有助于解决学习过程，并大幅减少训练深度神经网络所需的训练次数。

# **深度学习为什么需要批量归一化？批处理规范化为什么有帮助？**

在**深度学习**中，准备具有许多层的**深度神经网络**,因为它们可能对底层初始随机权重和学习算法的设计很敏感。

这个问题背后的一个潜在目的是，当权重被刷新时，在每个小批量之后，网络中某处的层的输入分布可能改变。这可以使学习算法始终追求一个移动的目标。对网络中各层的输入分布的这种调整暗指了专门的名称**内部协变量移位**。

挑战在于，利用接受当前层之前的层中的权重是固定的误差估计，从输出到输入逐层反向刷新模型。

> ***批量规格化*** *给出了一个丰富的参数化实际上任何深度神经网络的方法。重新参数化从根本上减少了跨多个层规划更新的问题。*

# **深度神经网络是如何进行批量归一化的？**

它通过对每个小批量的每个输入变量的激活进行归一化，明确地缩放该层的输出，例如，来自最后一层的节点的制定。回顾一下，标准化意味着重新调整数据以使**平均值**为 0，而**标准偏差**为 1。

通过使每一层的输入变亮，它将朝着实现输入的**固定分布迈出一大步，这将消除**内部协变量移动**的不利影响。**

归一化早期层的**激活意味着后续层在权重更新期间做出的关于输入的传播和分布的假设不会改变，无论如何不会显著改变。这具有稳定和加速深度神经网络的准备训练过程的影响。**

对于不包含来自训练数据集的模型的代理分布的较小的**小批量**,训练和推理(利用训练后的模型)之间的标准化输入的差异可以在执行性能中带来可察觉的对比。这可以通过改变称为**批量重正化**的技术来实现，该技术使得小批量的变量平均值和标准偏差的评估越来越稳定。

这种输入的标准化可以应用于第一个隐藏层的输入变量，或者应用于更深层的隐藏层的激活。

*倾向于与大多数深度网络类型一起使用，例如* ***卷积神经网络(CNN)*** *和* ***递归神经网络(RNN)*** *。*

它可以用在先前层的输入上，或者用在过去层的激活功能之后。

*如果对于像* ***双曲正切*** *和* ***逻辑函数*** *这样的 s 形容量，激活函数后可能会更加合适。*

它可能在激活函数之前适合于可能在非高斯散射中上升**的激活，如**整流线性激活函数(ReLU)** ，这是大多数深度神经网络类型的前线默认值。**

它提供了一些正则化影响，减少了**泛化误差**，可能需要利用**丢失**进行正则化。

在**批量标准化网络**中，平均值和方差在整个网络中保持适度稳定。对于一个**非规范化的网络**，他们似乎以指数**发展**有深度。

# **批量标准化的优势是什么？**

*   该模型对**超参数调整**不太敏感。也就是说，尽管更大的**学习率**已经促进了无价值的模型，更大的 lr 在这一点上是令人满意的
*   收缩**内部协变**移位
*   减少**渐变**对参数比例或其潜在值的依赖
*   **权重初始化**在这一点上不太重要
*   **辍学者**可被疏散进行调整

# **结论**

这使我们达到了本文的最大限度，在这里我们发现了批处理规范化和利用它的优点。批处理规范化解决了一个叫做内部协变量移位的主要问题。

它有助于使神经网络中间层之间的数据流动看起来，这意味着你可以使用更高的学习速率。它有一个正规化的效果，这意味着你可以经常消除辍学。

*现在，把你对****Twitter****，****Linkedin****，以及****Github****！！*

***同意*** *还是* ***不同意*** *与 Saurav Singla 的观点和例子？想告诉我们你的故事吗？*

*他乐于接受建设性的反馈——如果您对此分析有后续想法，请在下面的* ***评论*** *或联系我们！！*

*推文*[***@ SauravSingla _ 08***](https://twitter.com/SAURAVSINGLA_08)*、评论*[***Saurav _ Singla***](http://www.linkedin.com/in/saurav-singla-5b412320)*以及明星*[***SauravSingla***](https://github.com/sauravsingla)*对*