<html>
<head>
<title>Dimensionality Reduction using t-Distributed Stochastic Neighbor Embedding (t-SNE) on the MNIST Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 MNIST 数据集上使用 t 分布随机邻居嵌入(t-SNE)进行降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-using-t-distributed-stochastic-neighbor-embedding-t-sne-on-the-mnist-9d36a3dd4521?source=collection_archive---------13-----------------------#2020-08-16">https://towardsdatascience.com/dimensionality-reduction-using-t-distributed-stochastic-neighbor-embedding-t-sne-on-the-mnist-9d36a3dd4521?source=collection_archive---------13-----------------------#2020-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/dcf8670580d092ac3be64c31f76b3b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XFSxzNm2LWbVxKsHxkFwiQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">应用 PCA (n_components = 50)和 t-SNE 后 MNIST 数据的 2D 散点图</p></figure><p id="807c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们很容易将二维或三维数据可视化，但一旦超出三维，就很难看到高维数据是什么样子了。</p><p id="9c0b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">今天，我们经常处于这样一种情况，我们需要在具有数千甚至数百万维的数据集上分析和发现模式，这使得可视化有点挑战性。然而，一个绝对可以帮助我们更好地理解数据的工具是<strong class="kh iu">降维</strong>。</p><p id="cb21" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这篇文章中，我将讨论 t-SNE，一种流行的非线性降维技术，以及如何使用<em class="ld"> sklearn </em>在 Python 中实现它。我在这里选择的数据集是流行的 MNIST 数据集。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="531c" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">古玩目录</h1><ol class=""><li id="99dd" class="mj mk it kh b ki ml km mm kq mn ku mo ky mp lc mq mr ms mt bi translated"><a class="ae mu" href="#f297" rel="noopener ugc nofollow">什么是 t-SNE，它是如何工作的？</a></li><li id="b352" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated"><a class="ae mu" href="#b00c" rel="noopener ugc nofollow">t-SNE 与 PCA 有何不同？</a></li><li id="dd3a" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">我们如何改进 t-SNE？</li><li id="011d" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated"><a class="ae mu" href="#6a66" rel="noopener ugc nofollow">有哪些局限性？</a></li><li id="951d" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated"><a class="ae mu" href="#fdf3" rel="noopener ugc nofollow">接下来我们能做什么？</a></li></ol></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="f297" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">概观</h1><p id="6700" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">t-分布式随机邻居嵌入，或 t-SNE，是一种机器学习算法，通常用于在低维空间嵌入高维数据[1]。</p><p id="eaac" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">简单来说，SNE 霸王龙的方法可以分为两步。第一步是通过构建概率分布<strong class="kh iu"> P </strong>来表示高维数据，其中相似点被选取的概率高，而不同点被选取的概率低。第二步是用另一个概率分布<strong class="kh iu"> Q </strong>创建一个低维空间，它尽可能地保持 P 的性质。</p><p id="40e1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在步骤 1 中，我们使用条件概率 p 来计算两个数据点之间的相似性。例如，给定 I 的 j 的条件概率表示<em class="ld"> x_j </em>将被<em class="ld"> x_i </em>挑选为其邻居，假设在以<em class="ld">x _ I</em>【1】为中心的<strong class="kh iu">高斯</strong>分布下，邻居与其概率密度成比例地被挑选。在步骤 2 中，我们让<em class="ld"> y_i </em>和<em class="ld"> y_j </em>分别是<em class="ld"> x_i </em>和<em class="ld"> x_j，</em>的低维对应物。然后，我们认为 q 是被<em class="ld"> y_i </em>挑选的<em class="ld"> y_j </em>的类似条件概率，并且我们在低维图中采用了<strong class="kh iu">学生 t 分布</strong>。通过最小化来自 q 的概率分布 P 的<a class="ae mu" href="http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kh iu">kull back–lei bler 散度</strong> </a>来确定低维数据点的位置</p><p id="7e5e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于 t-SNE 的更多技术细节，请查看本文。</p><p id="b7c3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我选择了 Kaggle ( <a class="ae mu" href="https://www.kaggle.com/c/digit-recognizer" rel="noopener ugc nofollow" target="_blank"> link </a>)的 MNIST 数据集作为这里的例子，因为它是一个简单的计算机视觉数据集，包含手写数字(0–9)的 28x28 像素图像。我们可以把每个实例想象成一个嵌入 784 维空间的数据点。</p><p id="28f4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">要查看完整的 Python 代码，请查看我的<a class="ae mu" href="https://www.kaggle.com/dehaozhang/t-sne-visualization" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>。</p><p id="da7c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">事不宜迟，让我们进入细节！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="3e16" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">探测</h1><p id="d307" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">请注意，在最初的 Kaggle 竞赛中，目标是使用具有真实标签的训练图像来建立 ML 模型，该模型可以准确预测测试集上的标签。出于我们的目的，我们将只使用训练集。</p><p id="fc9c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">像往常一样，我们首先检查它的形状:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="6009" class="nm lm it ni b gy nn no l np nq">train.shape<br/>--------------------------------------------------------------------<br/>(42000, 785)</span></pre><p id="07e5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">有 42K 个训练实例。785 列是 784 像素值，以及“标签”列。</p><p id="814e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们还可以检查标签分布:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="af53" class="nm lm it ni b gy nn no l np nq">label = train["label"]<br/>label.value_counts()<br/>--------------------------------------------------------------------<br/>1    4684<br/>7    4401<br/>3    4351<br/>9    4188<br/>2    4177<br/>6    4137<br/>0    4132<br/>4    4072<br/>8    4063<br/>5    3795<br/>Name: label, dtype: int64</span></pre><p id="b00c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">主成分分析</strong></p><p id="975f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在实现 t-SNE 之前，我们先来试试 PCA，这是一种流行的线性降维方法。</p><p id="1105" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">标准化数据后，我们可以使用 PCA 转换数据(将“n_components”指定为 2):</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="a04c" class="nm lm it ni b gy nn no l np nq">from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA</span><span id="7639" class="nm lm it ni b gy nr no l np nq">train = StandardScaler().fit_transform(train)<br/>pca = PCA(n_components=2)<br/>pca_res = pca.fit_transform(train)</span></pre><p id="8bdc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们制作一个散点图来直观显示结果:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="a275" class="nm lm it ni b gy nn no l np nq">sns.scatterplot(x = pca_res[:,0], y = pca_res[:,1], hue = label, palette = sns.hls_palette(10), legend = 'full');</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1e7ee47a0db437c9fee94402467160e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeiUVrfhsY_LdR9kb0ucEQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">应用主成分分析后 MNIST 数据的 2D 散点图</p></figure><p id="5fb9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如散点图所示，含有两种成分的五氯苯甲醚不足以提供关于不同标签的有意义的见解和模式。我们知道 PCA 的一个缺点是线性投影不能捕捉非线性相关性。现在让我们试试 SNE 霸王龙。</p><p id="5562" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> T-SNE 与<em class="ld">sk learn</em>T3】</strong></p><p id="f1a8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们将使用<em class="ld"> sklearn.manifold </em> ( <a class="ae mu" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank">文档</a>)实现 t-SNE:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="8d62" class="nm lm it ni b gy nn no l np nq">from sklearn.manifold import TSNE</span><span id="1f55" class="nm lm it ni b gy nr no l np nq">tsne = TSNE(n_components = 2, random_state=0)<br/>tsne_res = tsne.fit_transform(train)<br/><br/>sns.scatterplot(x = tsne_res[:,0], y = tsne_res[:,1], hue = label, palette = sns.hls_palette(10), legend = 'full');</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/196f5e18f3369fa5441ccb4c8faed898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BP3uGk4QHdl-RJYu2DWxAA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">应用 t-SNE 后 MNIST 数据的 2D 散点图</p></figure><p id="065c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在我们可以看到，与主成分分析的结果相比，不同的聚类更加可分。以下是对该图的一些观察:</p><ol class=""><li id="b840" class="mj mk it kh b ki kj km kn kq ns ku nt ky nu lc mq mr ms mt bi translated">与“2”和“4”等其他聚类相比，“5”数据点似乎更加分散。</li><li id="f7de" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">有几个“5”和“8”数据点类似于“3”</li><li id="1ea0" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">有两个“7”和“9”的集群，它们彼此相邻。</li></ol><p id="8444" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">结合两者的方法</strong></p><p id="ca7f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通常建议在应用 t-SNE 之前，使用 PCA 或 TruncatedSVD 将维数减少到合理的数量(例如 50)[2]。</p><p id="3e05" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这样做可以降低噪音水平并加快计算速度。</p><p id="1f82" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们先尝试主成分分析(50 个成分)，然后应用 t-SNE。这是散点图:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/dcf8670580d092ac3be64c31f76b3b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XFSxzNm2LWbVxKsHxkFwiQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">应用 PCA(50 个成分)和 t-SNE 后 MNIST 数据的 2D 散点图</p></figure><p id="74a8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">与之前的散点图相比，我们现在可以更好地分离出 10 个星团。以下是一些观察结果:</p><ol class=""><li id="84aa" class="mj mk it kh b ki kj km kn kq ns ku nt ky nu lc mq mr ms mt bi translated">大多数“5”数据点不像以前那样分散，尽管有几个看起来仍然像“3”。</li><li id="eefc" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">现在有一簇“7”和一簇“9”。</li></ol><p id="bf7a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，这种方法的运行时间减少了 60%以上。</p><p id="cb32" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">更多互动 3D 散点图，请查看<a class="ae mu" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="6a66" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">限制</h1><p id="10f7" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">以下是 SNE 霸王龙的一些局限性:</p><ol class=""><li id="905d" class="mj mk it kh b ki kj km kn kq ns ku nt ky nu lc mq mr ms mt bi translated">与主成分分析不同，t-SNE 的成本函数是非凸的，这意味着我们有可能陷入局部最小值。</li><li id="c7b5" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">类似于其他维度缩减技术，压缩维度的含义以及转换后的特征变得更难解释。</li></ol></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="fdf3" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">后续步骤</h1><p id="669b" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">以下是我们接下来可以尝试的几件事:</p><ol class=""><li id="ad36" class="mj mk it kh b ki kj km kn kq ns ku nt ky nu lc mq mr ms mt bi translated">超参数调整—尝试调整“困惑”并查看其对可视化输出的影响。</li><li id="5e62" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">尝试其他一些非线性技术，如<strong class="kh iu">均匀流形近似和投影</strong> (UMAP)，这是 t-SNE 的推广，它基于黎曼几何。</li><li id="87d7" class="mj mk it kh b ki mv km mw kq mx ku my ky mz lc mq mr ms mt bi translated">根据转换后的数据训练 ML 模型，并将其性能与未经降维的模型进行比较。</li></ol></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="2415" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">摘要</h1><p id="19c6" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">让我们快速回顾一下。</p><p id="a682" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们在 MNIST 数据集上使用<em class="ld"> sklearn </em>实现了 t-SNE。我们将可视化输出与使用主成分分析的输出进行了比较，最后，我们尝试了一种混合方法，该方法首先应用主成分分析，然后应用 t-SNE。</p><p id="5a7e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我希望你喜欢这篇博文，并请分享你的想法:)</p><p id="8075" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">看看我关于独立性卡方测试的另一篇文章:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/chi-square-test-for-independence-in-python-with-examples-from-the-ibm-hr-analytics-dataset-97b9ec9bb80a"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">Python 中独立性的卡方检验以及 IBM HR 分析数据集的示例</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">员工流失是否取决于因素“X”？</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om jz ny"/></div></div></a></div></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="220b" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考</h1><p id="2713" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq na ks kt ku nb kw kx ky nc la lb lc im bi translated">[1]<a class="ae mu" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/T-distributed _ random _ neighbor _ embedding</a><br/>【2】<a class="ae mu" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . manifold . tsne . html</a></p></div></div>    
</body>
</html>