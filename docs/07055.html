<html>
<head>
<title>State-of-the-Art Language Models in 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年最先进的语言模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/state-of-the-art-language-models-in-2020-2c25c081b558?source=collection_archive---------40-----------------------#2020-05-30">https://towardsdatascience.com/state-of-the-art-language-models-in-2020-2c25c081b558?source=collection_archive---------40-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="deb3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">突出显示最常见的NLP任务的模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/388e861ddbb0e351b66ea573546bcb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_Eyxl5ouvpvHA-zh3zLtQ.png"/></div></div></figure><p id="75d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">自然语言处理(NLP)中有许多任务，<a class="ae lq" href="https://nlpprogress.com/english/language_modeling.html" rel="noopener ugc nofollow" target="_blank">语言建模</a>，<a class="ae lq" href="https://nlpprogress.com/english/machine_translation.html" rel="noopener ugc nofollow" target="_blank">机器翻译</a>，<a class="ae lq" href="https://nlpprogress.com/english/natural_language_inference.html" rel="noopener ugc nofollow" target="_blank">自然语言推理</a>，<a class="ae lq" href="https://nlpprogress.com/english/question_answering.html" rel="noopener ugc nofollow" target="_blank">问题回答</a>，<a class="ae lq" href="https://nlpprogress.com/english/sentiment_analysis.html" rel="noopener ugc nofollow" target="_blank">情感分析</a>，<a class="ae lq" href="https://nlpprogress.com/english/text_classification.html" rel="noopener ugc nofollow" target="_blank">文本分类</a>，<a class="ae lq" href="https://nlpprogress.com/" rel="noopener ugc nofollow" target="_blank">更多的</a> …由于不同的模型在不同的领域有所侧重和擅长，本文将重点介绍最常见的NLP任务的最新模型。</p><h1 id="3877" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">变压器</h1><p id="f841" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">自从<a class="ae lq" rel="noopener" target="_blank" href="/illustrated-guide-to-transformer-cf6969ffa067?source=friends_link&amp;sk=82b8c207682ac8ffeb01a3af44822192"> Transformer </a>推出以来，它的变体已经应用于许多(如果不是全部)NLP任务，实现了最先进的性能。在引入变压器之前，RNN模型已经是用于序列建模和转导问题的最先进的结构。然而，鉴于RNN计算的顺序性质，训练期间的并行化主要受到限制，导致训练效率较低。作者提出了一种编码器-解码器结构，完全依赖于注意机制，而不使用递归结构。</p><p id="a0a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">变换器的编码器由6层组成，每层由一个多头自注意机制组成，后面是一个前馈网络。在每个子层之前和之后，添加剩余连接和add &amp; norm层。解码器具有相同数量的6层。然而，在自关注层和前馈层之间插入了一个编码器-解码器关注层。注意遮罩用于隐藏后续位置的标记。</p><p id="c6b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所使用的注意机制被称为缩放的点积注意，它通过来标准化逻辑，以防止由于softmax的小梯度而导致的缓慢收敛。此外，本文还提出了多头注意的概念，使得该模型能够关注来自不同表征子空间的信息。在最终分类层之前，来自所有子空间的隐藏状态然后被连接和投影。</p><p id="2120" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在编码器自我关注层中，所有的键、值和查询都来自前一层，允许编码器关注前一层的所有位置。然而，由于注意屏蔽，解码器自我注意只注意到领先位置。另一方面，为了引起编码器-解码器的注意，从先前的解码器层提取查询，而键和值来自编码器隐藏状态。这允许解码器关注编码序列中的所有位置，这在序列到序列任务中是至关重要的。</p><p id="3e29" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了注入相对或绝对的位置信息，位置编码被添加到输入嵌入中。理论上可以是学来的，也可以是固定的；然而，采用正弦位置编码是为了更容易学习和外推至更长的序列。</p><div class="mo mp gp gr mq mr"><a rel="noopener follow" target="_blank" href="/illustrated-guide-to-transformer-cf6969ffa067"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd iu gy z fp mw fr fs mx fu fw is bi translated">变压器图解指南</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">逐个组件的细分分析</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">towardsdatascience.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf ks mr"/></div></div></a></div><h1 id="4ba4" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">双向编码器表示转换器</h1><p id="71c7" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">语言模型传统上是单向的，在训练过程中只有先前时间步的单词是可见的。这是为了确保预测的单词不会间接“看到自己”然而，在许多句子级或标记级任务中，前向和后向上下文对于最佳性能都是必不可少的。</p><p id="7b64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2018年推出了双向编码器表示变压器(<a class="ae lq" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。它在转换器结构中利用屏蔽语言建模来促进双向表示训练。每个输入序列都以一个特殊的<em class="ng">【CLS】</em>标记开始，其隐藏状态用于表示分类任务。对于句子对输入，添加了<em class="ng">【SEP】</em>标记来指示单个句子之间的边界。每个标记的初始嵌入与片段嵌入和位置嵌入相加，作为变换器的输入。</p><p id="f971" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">BERT模型包括两项预训练任务:</p><ol class=""><li id="da9f" class="nh ni it kw b kx ky la lb ld nj lh nk ll nl lp nm nn no np bi translated"><strong class="kw iu">屏蔽语言模型</strong>。在预训练期间，随机选择所有标记中的15%作为标记预测的屏蔽标记。然而，由于在微调期间不存在<em class="ng">【MASK】</em>标记，这导致预训练和微调之间的不匹配。因此，在所有选择要屏蔽的令牌中，只有80%被替换为<em class="ng">【屏蔽】</em>令牌。10%的时间令牌将保持不变，10%的时间由随机令牌替换。作者进行了烧蚀研究，以表明这样的置换率会产生最佳的下游性能。值得注意的是，通过<em class="ng">【掩码】</em>的完全替换或通过随机令牌的完全替换导致次优性能。</li><li id="e43b" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">下一句预测</strong>。对两个句子之间的关系进行建模至关重要，尤其是对于问答或自然语言推理等下游任务。因此，作者提出了下一个句子预测任务来分类一个句子是否是另一个句子的尾随句。当选择句子1和2进行句子对输入时，50%的情况下，句子2是跟在1后面的实际句子。另外50%的时间，随机选择一个句子，作为反面样本。这个简单的任务导致了问答和自然语言推理任务的显著改善。</li></ol><p id="8dd2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在预训练之后，通过简单地替换适当的输入-输出对，该模型在各种下游任务上被微调。对于标记级任务，例如序列标记或问题回答，每个标记的隐藏状态被提供给输出层。另一方面，<em class="ng">【CLS】</em>隐藏状态用于句子级别的任务，例如蕴涵和情感分析。</p><p id="89f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">研究人员进一步<a class="ae lq" href="https://arxiv.org/abs/1905.05583" rel="noopener ugc nofollow" target="_blank">探索专门针对文本分类的BERT微调</a>，由三部分组成:</p><ol class=""><li id="4589" class="nh ni it kw b kx ky la lb ld nj lh nk ll nl lp nm nn no np bi translated">使用任务内或领域内的训练数据来进一步预训练BERT</li><li id="2de4" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated">使用多任务学习进行微调(可选，如果相关任务可用)</li><li id="2b8f" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated">针对目标任务微调BERT</li></ol><p id="df80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实现的方法包括长文本预处理、层的选择、层特定的学习速率调整、遗忘和少量学习。</p><h1 id="9c14" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">XLNet</h1><p id="e860" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">前述基于变换器的模型利用自动编码(AE)公式，而不是传统的自回归(AR)语言建模。由于AE模型不像AR模型那样分解正向产品中的概率，因此它们可以利用两个方向的上下文。因此，缩小了语言建模和实际下游任务之间的差距，后者通常需要双向信息。然而，由于真实数据中缺少<em class="ng">【MASK】</em>记号，这导致了预训练和微调之间的差异。此外，BERT假设预测的记号是彼此独立的，这是过于简化的。</p><p id="eb92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">鉴于AR和AE语言模型的优缺点，XLNet<a class="ae lq" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">被提出来利用它们的优点，同时最小化它们的局限性。与AR模型中使用固定的向前或向后因子分解不同，作者建议最大化因子分解顺序中每个可能排列的可能性。</a></p><p id="ce8d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为相同的参数集用于所有排列，所以模型将总是能够访问全局环境。因此，它通过提取双向信息超越了传统的AR模型。此外，由于没有从损坏的数据重建，它避免了训练前微调差异的问题，以及独立性假设。值得注意的是，应用位置编码是为了保持原始序列顺序。置换只能通过在因式分解顺序中的后续标记上使用注意掩码来实现。</p><p id="f642" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，所提出的公式导致变压器模型中两个矛盾的要求:</p><ol class=""><li id="7567" class="nh ni it kw b kx ky la lb ld nj lh nk ll nl lp nm nn no np bi translated">当在时间步长<em class="ng"> t </em>进行预测时，隐藏表示<em class="ng"> h(t) </em>应该只包含位置信息，而不包含内容信息</li><li id="5e1c" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated">当在大于<em class="ng"> t </em>的时间步长进行预测时，<em class="ng"> h(t) </em>包括令牌<em class="ng"> t </em>的位置和内容</li></ol><p id="f127" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了解决这个问题，使用了两个独立的流，即内容和查询表示。在计算上，查询流是可训练的，并且内容流用相应的单词嵌入来初始化。</p><p id="20ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">大量的排列导致模型的训练收敛缓慢。因此，作者选择只预测因子分解顺序的最后一段。这样，由于不需要为前导记号计算查询表示，所以节省了速度和存储器。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="1382" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，<a class="ae lq" href="https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language" rel="noopener ugc nofollow" target="_blank">像BERT这样的深度学习模型到底能不能理解语言</a>？Neptune博客上的这篇文章描述了我们应该从三个方面来理解NLP</p><ul class=""><li id="c166" class="nh ni it kw b kx ky la lb ld nj lh nk ll nl lp oc nn no np bi translated">概念界限</li><li id="5331" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp oc nn no np bi translated">技术限制</li><li id="f74c" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp oc nn no np bi translated">评估限制</li></ul><p id="ae2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对NLP演示感兴趣吗？使用TensorFlow.js查看关于文本相似性的现场演示:</p><div class="mo mp gp gr mq mr"><a rel="noopener follow" target="_blank" href="/how-to-build-a-textual-similarity-analysis-web-app-aa3139d4fb71"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd iu gy z fp mw fr fs mx fu fw is bi translated">使用TensorFlow.js通用句子编码器的文本相似性</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">从学者到构建相似句子分组网络应用的旅程</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">towardsdatascience.com</p></div></div><div class="na l"><div class="od l nc nd ne na nf ks mr"/></div></div></a></div></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ab cb"><figure class="oe kn of og oh oi oj paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/e6191b77eb1b195de751fecf706289ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="oe kn of og oh oi oj paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/7c898af9285ccd6872db2ff2f21ce5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*airGp_q6AXwaoL1LYXwYeQ.png"/></a></figure><figure class="oe kn of og oh oi oj paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/d370b96eace4b03cb3c36039b70735d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ESxUX6V6tAqj_2ZFSr-pUw.png"/></a></figure></div></div></div>    
</body>
</html>