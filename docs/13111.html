<html>
<head>
<title>Image Feature Extraction: Traditional and Deep Learning Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像特征提取:传统和深度学习技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-feature-extraction-traditional-and-deep-learning-techniques-ccc059195d04?source=collection_archive---------1-----------------------#2020-09-09">https://towardsdatascience.com/image-feature-extraction-traditional-and-deep-learning-techniques-ccc059195d04?source=collection_archive---------1-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="06f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简要的综述着重于给出用于特征提取的传统和深度学习技术的概述</h2></div><p id="d580" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特征提取是计算机视觉中的一项重要技术，广泛用于以下任务:</p><ul class=""><li id="79ca" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">物体识别</li><li id="b73d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">图像对齐和拼接(创建全景)</li><li id="50c3" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">三维立体重建</li><li id="ba44" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">机器人/自动驾驶汽车导航</li><li id="af90" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">还有更多…</li></ul><h2 id="0d44" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">什么是特性？</h2><p id="ff36" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">特征是图像中帮助识别对象的部分或模式。例如，一个正方形有四个角和四条边，它们可以被称为正方形的特征，它们帮助我们人类识别它是一个正方形。特征包括像角、边、感兴趣点的区域、脊等属性。</p><p id="b307" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如下图所示，黄色点表示使用哈里斯检测技术检测到的特征。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/ba6d39cfa7f3340bf1648fe794714a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sKILsROzmPXIQeu7KbYHQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated"><a class="ae ng" href="https://commons.wikimedia.org/wiki/File:Writing_Desk_with_Harris_Detector.png" rel="noopener ugc nofollow" target="_blank">(src:https://commons . wikimedia . org/wiki/File:Writing _ Desk _ with _ Harris _ detector . png</a>)</p></figure><h2 id="f6a3" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">传统特征检测技术一瞥</h2><p id="93f2" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">用于特征检测的传统计算机视觉技术包括:</p><ul class=""><li id="75f1" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu"> Harris 角点检测— </strong>使用高斯窗口函数来检测角点。(<a class="ae ng" href="https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li><li id="9287" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu"> Shi-Tomasi 角点检测器— </strong>作者修改了 Harris 角点检测中使用的评分函数，以实现更好的角点检测技术(<a class="ae ng" href="https://docs.opencv.org/3.4/d4/d8c/tutorial_py_shi_tomasi.html" rel="noopener ugc nofollow" target="_blank">阅读更多信息</a>)</li><li id="7c9d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">尺度不变特征变换(SIFT)——</strong>与前两种不同，这种技术是尺度不变的。(<a class="ae ng" href="https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li><li id="0ac7" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">加速鲁棒特征(SURF)——</strong>顾名思义，这是 SIFT 的更快版本。(<a class="ae ng" href="https://docs.opencv.org/3.4/df/dd2/tutorial_py_surf_intro.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li><li id="685d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">加速分段测试(FAST)的特征— </strong>这是一种比 SURF 更快的角点检测技术。(<a class="ae ng" href="https://docs.opencv.org/3.4/df/d0c/tutorial_py_fast.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li><li id="8089" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">二元鲁棒独立基本特征(BRIEF)——</strong>这只是一个特征描述符，可用于任何其他特征检测器。这种技术通过将浮点数形式的描述符转换为二进制字符串来减少内存使用。(<a class="ae ng" href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li><li id="c520" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">定向快速旋转简报(ORB) </strong> — SIFT 和 SURF 已获专利，OpenCV 实验室的这种算法是它们的免费替代品，它使用快速关键点检测器和简报描述符。(<a class="ae ng" href="https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)</li></ul><h2 id="fad9" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">深度学习特征提取技术一瞥</h2><p id="f049" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">传统的特征提取器可以由卷积神经网络(CNN)代替，因为 CNN 具有提取复杂特征的强大能力，这些复杂特征更详细地表达了图像，学习特定于任务的特征，并且更有效。在这方面已经做了许多工作。下面列出了其中的一些:</p><ul class=""><li id="b3de" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu"> SuperPoint:自我监督的兴趣点检测和描述</strong> ( <a class="ae ng" href="https://arxiv.org/pdf/1712.07629.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ) —作者提出了一个完全卷积的神经网络，它在单次正向传递中计算 SIFT 样的兴趣点位置和描述符。它使用 VGG 式编码来提取特征，然后使用两个解码器，一个用于点检测，另一个用于点描述。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/87d00f8a46722c84db29bc44fe3494d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NH5vTFkFsA5qKZZRQy8qfQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">超点架构(src:<a class="ae ng" href="https://arxiv.org/pdf/1712.07629.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1712.07629.pdf</a></p></figure><ul class=""><li id="4f70" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu"> D2 网:一个可训练的 CNN，用于局部特征的联合描述和检测</strong> <a class="ae ng" href="https://arxiv.org/pdf/1905.03561.pdf" rel="noopener ugc nofollow" target="_blank">【论文】</a>——作者提出了一个单一的卷积神经网络，它既是密集特征描述符，又是特征检测器。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ni"><img src="../Images/aed4740db7c42f17978868a5e4328559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXaECce4zcf24TRkuyZbqQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">检测并描述 https://arxiv.org/pdf/1905.03561.pdf 的 D2 网络</p></figure><ul class=""><li id="34c1" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu"> LF-Net:从图像中学习局部特征</strong> ( <a class="ae ng" href="https://papers.nips.cc/paper/7861-lf-net-learning-local-features-from-images.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ) <strong class="kk iu"> — </strong>作者建议使用稀疏匹配深度架构，并在具有相对姿态和深度图的图像对上使用端到端训练方法。他们在第一幅图像上运行检测器，找到最大值，然后优化权重，以便在第二幅图像上运行时，产生清晰的响应图，在正确的位置具有尖锐的最大值。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nj"><img src="../Images/6766fba78b61fc89dc39d2fc50d5f56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j0mkShK3yj2RN5-MZgB4fw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">LF-Net(src:<a class="ae ng" href="https://papers.nips.cc/paper/7861-lf-net-learning-local-features-from-images.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/7861-LF-Net-learning-local-features-from-images . pdf</a>)</p></figure><ul class=""><li id="41c9" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">基于深度学习的图像特征匹配</strong> ( <a class="ae ng" href="https://ieeexplore.ieee.org/abstract/document/8780936" rel="noopener ugc nofollow" target="_blank">论文</a>)——他们采用一种深度卷积神经网络(CNN)模型，在图像特征点匹配中关注图像块。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nk"><img src="../Images/62d1dc4f397bbb0b4835b207145f0123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duZ9ByfYs9SY3LaOqJRnfQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">与 SIFT 和 SURF 相比的结果(src:<a class="ae ng" href="https://ieeexplore.ieee.org/abstract/document/8780936" rel="noopener ugc nofollow" target="_blank">https://ieeexplore.ieee.org/abstract/document/8780936</a>)</p></figure><ul class=""><li id="74f8" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">特征匹配问题的深度图形特征学习</strong> ( <a class="ae ng" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ) —他们建议使用图形神经网络将特征点的坐标转换为局部特征，这样就可以很容易地使用简单的推理算法进行特征匹配</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nl"><img src="../Images/eb1fcaf6a79dc07557cde8bd65af2c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwJ4_7-Cqsl7Z_gr0Jo87A.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图神经网络(src:<a class="ae ng" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2019/papers/张 _ Deep _ Graphical _ Feature _ Learning _ for _ the _ Feature _ Matching _ Problem _ ICCV _ 2019 _ paper . pdf</a></p></figure><h2 id="27c5" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated"><strong class="ak">结论</strong></h2><p id="4a53" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">虽然看起来用于特征提取的深度学习技术对缩放、遮挡、变形、旋转等更加鲁棒，并且已经突破了使用传统计算机视觉技术的极限，但这并不意味着计算机视觉技术已经过时。</p><h2 id="ac44" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">放弃</h2><p id="99d0" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">这是一篇简短的文章，重点是概述传统的和深度学习的特征提取技术。如果你认为我可能错过了一个应该提到的算法，请在评论中留下它(将在这里加上适当的学分)。</p><h2 id="2d8a" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated"><strong class="ak">参考文献</strong></h2><p id="629e" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated"><a class="ae ng" href="https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_manual-1.8.4.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . UBC . ca/research/FLANN/uploads/FLANN/FLANN _ manual-1 . 8 . 4 . pdf</a></p><p id="a45c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://arxiv.org/pdf/1712.07629.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1712.07629.pdf</a></p><p id="a5b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://arxiv.org/pdf/1905.03561.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1905.03561.pdf</a></p><p id="28b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://arxiv.org/ftp/arxiv/papers/1910/1910.13796.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/1910/1910.13796.pdf</a></p><p id="9d86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://papers.nips.cc/paper/7861-lf-net-learning-local-features-from-images.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/7861-lf-net-learning-local-features-from-images . pdf</a></p><p id="5ced" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">【https://ieeexplore.ieee.org/abstract/document/8780936 T4】</p><p id="3b96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2019/papers/张 _ Deep _ Graphical _ Feature _ Learning _ for _ the _ Feature _ Matching _ Problem _ ICCV _ 2019 _ paper . pdf</a></p><p id="8737" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3.4/DC/d0d/tutorial _ py _ features _ Harris . html</a></p><p id="ce9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/d4/d8c/tutorial_py_shi_tomasi.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3.4/D4/d8c/tutorial _ py _ Shi _ tomasi . html</a></p><p id="caff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3.4/da/df5/tutorial _ py _ sift _ intro . html</a></p><p id="e696" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3.4/da/df5/tutorial _ py _ sift _ intro . html</a></p><p id="1539" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/df/dd2/tutorial_py_surf_intro.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3.4/df/dd2/tutorial _ py _ surf _ intro . html</a></p><p id="edf7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/df/d0c/tutorial_py_fast.html" rel="noopener ugc nofollow" target="_blank">https://docs.opencv.org/3.4/df/d0c/tutorial_py_fast.html</a></p><p id="faa4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html" rel="noopener ugc nofollow" target="_blank">https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html</a></p><p id="b263" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ng" href="https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html" rel="noopener ugc nofollow" target="_blank">https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html</a></p></div></div>    
</body>
</html>