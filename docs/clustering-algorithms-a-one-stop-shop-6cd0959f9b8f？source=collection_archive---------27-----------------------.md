# 聚类算法:一站式商店

> 原文：<https://towardsdatascience.com/clustering-algorithms-a-one-stop-shop-6cd0959f9b8f?source=collection_archive---------27----------------------->

## 比较层次聚类、K 均值、K 中值、K 模式、K 原型、DB 扫描和混合高斯模型的快速指南

![](img/e53b6232749cc37098676cbbd8ac3deb.png)

照片由来自 Pexels 的 Artem Beliakin 拍摄

说到集群，有这么多可供选择的方案。根据任务的不同，理解所有可用技术的结果如何以及为什么会有如此大的差异很容易让人感到困惑。考虑到聚类算法的非监督性质，可能很难区分哪种算法是最合适的。在这篇文章中，我试图揭穿他们之间的主要差异，并强调什么时候一个人可能更适合每项任务。

# 分层聚类

分层聚类可能是所有算法中最直观的，并且提供了很大的灵活性。

## 该算法

分层聚类是一种凝聚算法。本质上，在该过程的开始，每个数据点都在自己的群集中。使用相异度函数，该算法在数据集中找到最相似的两个点，并将它们聚集在一起。该算法像这样迭代运行，直到整个数据被聚类。此时，可以使用树状图来解释不同的聚类，并根据需要选择聚类的数量。

![](img/bc208ef213604818e246c1005b0be80f.png)

根据专门的[维基百科页面](https://en.wikipedia.org/wiki/Dendrogram)的树状图示例

## 最适合…

*   分类数据
*   发现异常值和异常组
*   使用易读的树状图显示结果
*   灵活性，因为有不同的相异函数可用(即完全连锁、单连锁、平均连锁、最小方差等。)各自给出非常不同的结果

## 棘手的时候…

*   随着数据量的增加，它会变得非常耗费时间/资源，因为它会反复处理每个数据点，每次都会遍历整个数据集。分层聚类不能很好地扩展。

# k 均值

K-Means 可能是最流行的聚类算法。由于这一点，以及它的简单性和扩展能力，它已经成为大多数数据科学家的首选。

## 该算法

用户决定结果聚类的数量(表示为 K)。k 个点被随机分配为聚类中心。从那里，该算法通过选择与该点的欧几里德距离最小的聚类，将数据集中的所有其他点分配给其中一个聚类。接下来，通过取每个点坐标的平均值来重新计算聚类中心。该算法将每个点重新分配给最近的聚类，并重复该过程，直到聚类收敛并且不再变化。

注意，由于随机初始化，结果可能取决于随机选择哪些点来初始化聚类。因此，该算法的大多数实现提供了以不同的“随机开始”多次运行该算法的能力，以便选择最小化这些点及其聚类中心的误差平方和(惯性)的聚类。

使用肘图，也很容易选择正确的集群数量(如果这不是由所涉问题预先确定的话)

![](img/9b96b031856ac693c62882a339d7502a.png)

我为说明目的而生成的肘图示例

## 最适合…

*   可能不需要聚类的可解释性的一般情况(即当用作监督问题的特征时)
*   大多数情况下，快速解决方案足以产生深刻见解的问题。K-Means 算法是相对高效的。
*   大数据问题，因为算法可以轻松扩展(scikit-learn 甚至提供了一个特别适合大量数据的小批量 K 均值版本

## 棘手的时候…

*   数据集包含许多分类变量。K-Means 倾向于聚集在分类变量周围(因为它们在标准化数据集中的方差相对较高)
*   离群值可能会严重扭曲聚类

# k-中间值

如上所述，大量异常值的存在可能会显著阻碍算法的效率。

## 该算法

K-Means 使用聚类中每个点的平均值来计算每个聚类的中心。然而，平均值并不是一个稳健的指标。因此，异常值的存在会使中心偏向异常值。

K-Medians 基于与 K-Means 相同的算法，不同之处在于它不是计算给定聚类中所有点的坐标的平均值，而是使用中值。因此，聚类将变得更加密集，对异常值也更加稳健。

## 最适合…

*   创建紧密/密集的集群，对异常值具有鲁棒性

## 棘手的时候…

*   需要一个快速的解决方案，因为它不在 scikit-learn 支持的算法中(需要使用 [PyClustering](https://pypi.org/project/pyclustering/) 或自定义代码来实现)

# k 模式

当存在分类变量时，K-Means 的表现也不好。至于 K-中位数，存在一个实现来利用分类数据上的 K-均值的效率。

## 该算法

K-Means 计算两点之间的欧几里德距离，而 K-Modes 试图最小化不相似性度量:它计算不相同的“特征”的数量。使用模式代替手段，K-模式变得能够有效地处理分类数据

## 最适合…

*   当数据集只包含分类数据时

## 棘手的时候…

*   数据类型是混合的
*   不一致的重要特征。因为 K-Modes 简单地计算不相似性的数量，所以它与“特征”点不同的算法无关。如果给定的类别特别普遍，这可能会成为一个问题，因为算法在聚类时不会考虑它。

# k 原型

K-Prototypes 扩展了 K-Means 和 K-Modes，特别适用于处理包含连续变量和分类变量的混合数据集。

## 该算法

为了处理分类变量和连续变量，K-Prototypes 使用了一个定制的相异度度量。要最小化的点到其聚类中心(其原型)的距离如下:

![](img/b38a919ea430bb2703a28371bbaaf3ce.png)

用于计算 K 原型中点/簇之间距离的等式

其中 E 是连续变量之间的欧几里德距离，C 是不同分类变量的计数(λ是控制分类变量在聚类过程中的影响的参数)。

## 最适合…

*   大型混合数据集(即超出了分层聚类的限制)

## 棘手的时候…

*   需要一个快速的解决方案，因为它不在 scikit-learn 支持的算法中(需要使用 [PyClustering](https://pypi.org/project/pyclustering/) 或自定义代码来实现)
*   分类变量的权重可能不清楚

# DB 扫描

创建 DB-Scan 是为了解决一个不同的集群问题。它将高密度的簇与低密度的簇隔离开来。

## 该算法

为了根据密度来划分集群，DB-Scan 首先将数据划分为 n 个维度。对于每个点，该算法在该点周围形成一个形状，计算落入该形状的其他观察值的数量。DB-Scan 迭代地扩展形状，直到某个距离内不再有点，记为 epsilon(为模型指定的参数)

## 最适合…

*   分离高密度和低密度的集群
*   隔离异常值

## 棘手的时候…

*   聚类具有相似的密度(因为算法分离密度)
*   数据具有高维性

# 高斯混合模型(GMM)

高斯混合模型是第一个“基于模型”的聚类算法。该领域仍然相对较新，但高斯混合模型在某些情况下显示出巨大的前景。

## 该算法

以前的算法将点“硬”分配给特定的聚类，而 GMM 将每个点“软”分配给多个聚类，其中每个分配由属于给定聚类的概率来定义。

该模型假设数据点是由混合高斯分布生成的，并试图使用期望最大化(EM)来找到后一种分布的参数。

使用贝叶斯信息标准(BIC)，GMM 还可以找到最佳数量的聚类来最好地解释数据。

## 最适合…

*   当集群是“隐藏的”/不可直接观察的，并且诸如由其他模型提供的“球形表示”是不够的时，复杂的集群。GMM 提供了额外的灵活性，并且可以提供更加复杂/非线性的聚类。

## 棘手的时候…

*   处理大量数据(扩展性不好)
*   数据量非常有限(算法需要能够估计协方差矩阵)

总之，存在大量的可用聚类算法，甚至超出了本文快速介绍的范围。其中绝大多数都可以通过开源 python 库(即 Scikit-learn 的多样化选项)轻松访问。它们之间的选择并不总是显而易见的，但是每种技术都有特定的优缺点，应该相应地加以利用。

Scikit-learn 提供了对大多数聚类算法及其一些优点/缺点的很好的总结，以及对其中大多数算法的易于使用的实现。我强烈建议您查看 it ,进一步了解您可能需要的算法以及如何针对您的具体情况进行调整。

*PS:我现在是柏克莱大学的工程硕士，我还在学习这方面的知识。如果有什么需要改正或不清楚的地方，请告诉我。你也可以在这里发邮件给我*[](mailto:ilias.miraoui@gmail.com)**。**