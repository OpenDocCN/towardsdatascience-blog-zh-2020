<html>
<head>
<title>Sentiment Analysis: An Introduction to Naive Bayes Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情感分析:朴素贝叶斯算法介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-introduction-to-naive-bayes-algorithm-96831d77ac91?source=collection_archive---------3-----------------------#2020-05-10">https://towardsdatascience.com/sentiment-analysis-introduction-to-naive-bayes-algorithm-96831d77ac91?source=collection_archive---------3-----------------------#2020-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e4b9a00604f2356206ea0190d15270fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*doy-O2kRpEJw6WJzALB6Uw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:stocksnap.io</p></figure><div class=""/><p id="4698" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据集:<a class="ae la" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/情操分析电影评论/数据</a></p><p id="dd89" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本教程假设读者对贝叶斯定理和文本分析完全无知。你只需要按照教程和一切都解释了一个新鲜的人工智能/毫升。如果你是专业用户，希望快速修改概念<strong class="ke jg">，你可以访问</strong> <a class="ae la" href="https://github.com/DrManishSharma/NLP.git" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">我的github库</strong></a><strong class="ke jg">(Senti _ analysis . ipynb)</strong>上的代码。</p><p id="d88a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于任何类型的数据科学项目，您需要做的第一件事就是加载数据并使用它。我更喜欢(也是我认识的大多数数据科学家)用二手熊猫来下载数据(图1)。给定数据以*为单位。tsv格式(制表符分隔变量)。我已经把它转换成*。csv使用excel，同时可以使用*。tsv格式也是。当我加载数据时，我遇到了一个小问题，因为在一些行中使用了特殊字符(大约4行包含\/、分隔符和反斜杠)。在加载数据之前，我必须手动删除这些字符。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/8c436aa82a6465082325ab163142f7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*7Nj9Fc_L6Z4drTK2ttatOg.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:加载数据</p></figure><p id="fdcb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我强烈建议新用户使用基本的熊猫指令来玩游戏。您可以使用:dataset.info()来获得不同列的汇总，如果任何列包含任何带有NAN数据的行(如果是，那么有多少？).请使用dataset.head()、dataset.tail()、dataset.describe()、dataset['$ColumnName'] e.t.c来深入了解数据集。例如(图2)使用数据集。$ColumnName.value_counts()给出每种情绪的计数(设0为一星评价，4为五星评价)。从这里我们可以猜测，平均来说这部电影似乎是一部三星级电影。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/fd4caa8fd9993bcda2c2170f81f778cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*EKBC_SwCz7jXchu6R2aBrw.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:基本pandas命令的使用</p></figure><p id="ef78" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了进一步进行情感分析，我们需要进行文本分类。我们可以用“词袋(BOW)”模型进行分析。用外行人的话来说，BOW模型将文本转换成数字形式，然后可以在算法中用于分析。</p><p id="591e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">具体来说，BOW模型用于文本数据中的特征提取。它返回一个包含所有单词和每个单词重复次数的向量。它被称为BOW，因为它只关心一个单词重复的次数，而不是单词的顺序。让我们举一个例子来更好地理解它(假设每个文档只包含一个句子):</p><p id="b2cf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">文件1:瑞士是一个美丽的国家。文档2:印度是一个拥有聪明的IT专业人士的国家。美国是一个充满机遇的国家。</p><p id="cfd9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下表称为文档术语矩阵(DTM)。<br/>———————<br/>| Words-&gt;Swzld是一个聪明的IT教授美丽的国家印度美国opport<br/>| Doc 1 Vector-&gt;1 1 1 1 1 0 0 0 0 0 0<br/> Doc 2 Vector-&gt;0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0<br/>| Doc 3 Vector-&gt;0 1 1 0 1 0 1 0 1 0 1 1 1<br/>|累计-&gt;1 3 1 3 1 2 1 1 1 1 1 1 1 1 1 1 1<br/>———————<br/>上图所示型号为monogram型号。 如果一次取两个单词(例如:瑞士是一个美丽的国家……)，那么它被称为双字母模型，同样地，一次取N个单词，它将是N字母模型。较高克数的模型往往比monogram模型效果更好。</p><p id="5f5f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">每个文档中的内容越长，每个向量的长度就越长(将包含许多零)。如果文档太大，文档向量将是稀疏向量。稀疏向量需要大量内存来存储，并且由于长度的原因，即使是计算也会变得很慢。为了减少稀疏向量的长度，可以使用诸如词干化、词汇化、转换成小写或忽略停用词等技术</p><p id="f9e8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，我们将使用sci-kit-learn的CountVectorizer模块生成DTM(图3)。要阅读更多关于CountVectorizer的参数，您可以访问<a class="ae la" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">这里</strong> </a>。如上所述，我们将使用:</p><ul class=""><li id="c72e" class="lh li jf ke b kf kg kj kk kn lj kr lk kv ll kz lm ln lo lp bi translated">tokenizer =覆盖字符串记号化步骤，我们从NLTK的Regex记号化器生成记号化器(缺省情况下:None)</li><li id="ed0b" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lm ln lo lp bi translated">lowercase = True(不需要使用，因为默认情况下设置为True)</li><li id="5ebb" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lm ln lo lp bi translated">stop_words = 'english '(默认情况下不使用，为了改善结果，我们可以提供一个自定义的停用词列表)</li><li id="2de2" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lm ln lo lp bi translated">ngram_range = (1，1)(默认情况下，将使用its (1，1)，即严格的单字母组合，(2，2)仅使用双字母组合，而(1，2)两者都使用)</li></ul><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lv"><img src="../Images/230944abc662187e63bfec43e8cd6cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxC2AvW8JLQVTmOkEafQfQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:使用计数矢量器准备“单词包”</p></figure><p id="b547" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们现在将分割数据用于训练和测试，以检查我们的模型表现如何(图3)。此外，我们将随机化数据，以防我们的数据首先包括所有积极的，然后是所有消极的或其他类型的偏见。我们将使用:scikit_learn的<a class="ae la" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg">train _ test _ split()</strong></a>来拆分text_count(其中包含我们的X)和dataset[‘情绪’](其中包含Y)。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lw"><img src="../Images/12e18b761e69fbae703cf6102be76fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBvl2GEEyndwB4E0Uex9pg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4:使用train_test_split将数据分为训练和测试数据集。</p></figure><p id="8189" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们有了训练和测试数据。我们应该开始分析了。我们的分析(和大多数ML分析一样)将分为5个步骤(记住它们的一个助记法是<strong class="ke jg"> DC-FEM </strong>记住哥伦比亚特区消防和紧急医疗服务):</p><ol class=""><li id="1b32" class="lh li jf ke b kf kg kj kk kn lj kr lk kv ll kz lx ln lo lp bi translated">定义模型</li><li id="8c02" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lx ln lo lp bi translated">编译模型</li><li id="4a33" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lx ln lo lp bi translated">拟合模型</li><li id="6ad1" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lx ln lo lp bi translated">评估模型</li><li id="103d" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lx ln lo lp bi translated">用模型做预测</li></ol><h1 id="195c" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">1.定义模型</h1><p id="be7c" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">我们将使用<a class="ae la" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">朴素贝叶斯【NB】</strong></a>分类器之一来定义模型。具体来说，我们将使用<a class="ae la" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">多项式分类器</strong> </a>。作为一个ML的新生，你可以使用sklearn <a class="ae la" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">这里</strong> </a>给出的备忘单来决定用于特定问题的最佳模型。它告诉我们使用NB分类器。让我们绕道了解更多关于NB模式的信息。</p><h2 id="b500" class="nb lz jf bd ma nc nd dn me ne nf dp mi kn ng nh mm kr ni nj mq kv nk nl mu nm bi translated">朴素贝叶斯模型</h2><p id="defa" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">该模型应用贝叶斯定理，并天真地假设不同特征之间没有关系。根据贝叶斯定理:</p><p id="b234" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">后验=可能性*命题/证据</p><p id="da71" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">或者</p><p id="f93c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">P(A|B) = P(B|A) * P(A)/P(B)</p><p id="4355" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">举个例子:在一副扑克牌中，选择一张牌。假定一张牌是一张脸牌，那么这张牌成为皇后的概率是多少？</strong> <br/>这个可以用贝叶斯定理解决。<br/> P(皇后给脸牌)= P(皇后|脸)<br/> P(给脸皇后)= P(脸|皇后)= 1 <br/> P(皇后)= 4/52 = 1/13 P(脸)= 3/13从贝叶斯定理:<br/> P(皇后|脸)= P(脸|皇后)P(皇后)/P(脸)= 1/3</p><p id="141f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于具有多个变量的输入:<br/> P(y|x1，x2，… xn) = P(x1，x2，… xn|y)* P(y)/P(x1，x2，…xn) <br/>使用朴素贝叶斯我们假设x1，x2 … xn相互独立，即:<br/> P(x1，x2，…xn | y)= P(x1 | y)* P(x2 | y)…* P(xn | y)<br/>P(Xi)分布中的假设例如，假设高斯分布将产生高斯朴素贝叶斯(GNB)，或者多项式分布将产生多项式朴素贝叶斯(MNB)。</p><p id="786f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">朴素贝叶斯模型特别适用于文本分类和垃圾邮件过滤。<strong class="ke jg">使用NB算法的优点</strong>是:</p><ul class=""><li id="e0c6" class="lh li jf ke b kf kg kj kk kn lj kr lk kv ll kz lm ln lo lp bi translated">需要少量的训练数据来学习参数</li><li id="a58a" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lm ln lo lp bi translated">与复杂的模型相比，可以相对快速地进行训练</li></ul><p id="b809" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">NB算法的主要缺点</strong>是:</p><ul class=""><li id="a91e" class="lh li jf ke b kf kg kj kk kn lj kr lk kv ll kz lm ln lo lp bi translated">这是一个不错的分类器，但却是一个糟糕的估计器</li><li id="3554" class="lh li jf ke b kf lq kj lr kn ls kr lt kv lu kz lm ln lo lp bi translated">它适用于离散值，但不适用于连续值(不能用于回归)</li></ul><h2 id="fa35" class="nb lz jf bd ma nc nd dn me ne nf dp mi kn ng nh mm kr ni nj mq kv nk nl mu nm bi translated">NB算法的困境</h2><p id="813c" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">关于NB算法的一个具有挑战性的问题是:尽管NB算法中的条件独立性假设在现实生活中几乎不成立，但是NB算法作为分类器是如何工作得如此之好的呢？我不会在这里讨论解决方案，而是将您引向包含解决方案的资源(<a class="ae la" href="https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">这里</strong> </a>)。简而言之，答案在于依赖性的分布，而不是依赖性，不知何故，由于分布，依赖性的影响抵消了。</p><h2 id="2798" class="nb lz jf bd ma nc nd dn me ne nf dp mi kn ng nh mm kr ni nj mq kv nk nl mu nm bi translated">NB分类的损失函数</h2><p id="f438" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">NB分类使用零一损失函数。在此函数中，错误=错误分类的数量。这里，误差函数不考虑概率估计的准确性，假设具有最高概率的类被正确预测。比如说有A和B两个类，给了不同的属性(x1，x2，… xn)。P(A |所有属性)= 0.95，P(B |所有属性)=0.05，但是NB可能估计P(A |所有属性)= 0.7，P(B |所有属性)= 0.3。这里虽然估计得远不准确，但分类是正确的。</p><p id="1802" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们回到我们的分析。定义和编译模型的前两步简化为从sklearn中识别和导入模型(正如sklearn给出的预编译模型)。</p><h1 id="2767" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">2.编译模型</h1><p id="89db" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">因为我们使用sklearn的模块和类，我们只需要导入预编译的类。Sklearn在这里 给出了所有<a class="ae la" href="https://scikit-learn.org/stable/modules/classes.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">职业的信息。</strong></a></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b8f7a9751156423d3b4fad212f216e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*BbBMyoAGEbBHB8YTWUxy_A.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5:从sklearn库中导入多项式朴素贝叶斯模型</p></figure><h1 id="abc2" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">3.拟合模型</h1><p id="2696" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">在这一步中，我们在多项式b中生成模型拟合数据集。为了寻找拟合模型时可以传递的参数，建议查看正在使用的模块的sklearn网页。对于MNB可以在这里勾选<a class="ae la" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg"/></a><strong class="ke jg">。</strong></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/c5a8c4fb93f8a98f16b4b1ed38648483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLsItIqTydxKws3faACkew.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6:在多项式朴素贝叶斯中拟合数据。</p></figure><h1 id="2209" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">4.评估模型</h1><p id="eca1" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">这里我们量化我们模型的质量。我们使用sklearn库中的<a class="ae la" href="https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">度量</strong> </a>模块来评估预测(图7)。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/61a10400b24dd8b1281c9b149b448a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSRVLhtIT6STDMJxU8qIig.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图7:使用sklearn的度量来评估模型</p></figure><h1 id="1cce" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">调整模型</h1><p id="a743" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">我们观察到我们的模型的准确率超过60%。我们现在可以利用我们的模型来增加它的准确性。</p><h2 id="59a2" class="nb lz jf bd ma nc nd dn me ne nf dp mi kn ng nh mm kr ni nj mq kv nk nl mu nm bi translated">尝试不同的n-grams</h2><p id="18e0" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">详情见图8(二元模型)和图9(三元模型)。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/b66599c96ba9402e0989f9142cd8c7d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SKvL3R4ZgkSNIxro0ph99Q.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图8:文本矢量化的二元模型。</p></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/78e66cc76bd23ac58518c5cf949cb11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOK3yfRdZHHgUE6Wtf2BYw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图9:文本矢量化的三元模型。</p></figure><h2 id="10e3" class="nb lz jf bd ma nc nd dn me ne nf dp mi kn ng nh mm kr ni nj mq kv nk nl mu nm bi translated">尝试不同的朴素贝叶斯算法</h2><p id="42ae" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">参考图10(补码NB)、图11(高斯NB)、图12(伯努利NB)。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/074210531271e3d7a7e05b32e497b9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4X2xDj9b3HyMIAPnbAiIQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图10:使用互补朴素贝叶斯模型进行情感分析</p></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/0b4182615f77968c36fda7905114b4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofSguFBllZV31_EkOHwQKw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图11:使用高斯朴素贝叶斯模型进行情感分析</p></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/d5a3de360424048495b7db5983140232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opgWH1BPL3r9WjVODlifFg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图12:使用伯努利朴素贝叶斯模型进行情感分析</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="ed8a" class="ly lz jf bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">提高准确性</h1><p id="d172" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">我们已经尝试使用不同的n-grams和不同的朴素贝叶斯模型，但最高准确率徘徊在60%左右。为了改进我们的模型，让我们尝试改变方式，创建弓。目前，我们用CountVectorizer创建了BOW，它计算文本中单词的出现次数。一个词出现的次数越多，它对分类就变得越重要。</p><h1 id="d844" class="ly lz jf bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">术语频率-逆文档频率</h1><p id="75b5" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">让我们使用TF-IDF，它考虑了术语频率和逆文档频率的乘积。术语频率是一个术语在文档中出现的频率。假设一个术语在一个文档中出现了“f”次，带有“d”个单词。<br/>词频= f/d <br/> IDF为‘逆文档频率’。如果一个语料库包含N个文档，而我们感兴趣的术语只出现在D个文档中，那么IDF是:<br/> IDF = log(N/D) TF-IDF是术语频率和逆文档频率的乘积。<strong class="ke jg"> TF-IDF显示了一个词在语料库中的稀有度。</strong>如果一个词很少见，那么它可能是某个特定情感/信息的标志性词汇。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/1b967435060002dca60027ae7f72b712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1Ofa4vdvQsR_4Wu24kiiA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图13:使用中的TfidfVectorizer</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="82d1" class="ly lz jf bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">尝试非贝叶斯算法</h1><p id="e2d5" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">即使是Tfidf矢量器(即创建不同的弓)也无助于提高模型的准确性。除了朴素贝叶斯算法，我们还可以选择随机梯度下降分类器或线性支持向量分类器。众所周知，这两种方法都能很好地处理文本数据分类。让我们试着用这些:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/3e6c18bd4e6ab4e5c24d8d496e02a78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQVEpBimtRrgnIWdNjrLRA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图14:尝试不同的文本分类算法</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="0fc4" class="ly lz jf bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">推理</h1><p id="8a4a" class="pw-post-body-paragraph kc kd jf ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">我们观察到，使用TF-IDF创建的BOW的线性支持向量分类器给出了最好的结果，准确率达到63.88%。虽然准确性仍然很低，但仍需要对模型进行改进，以给出更好的结果。</p><p id="9bd0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">祝贺大家完成教程。您应该已经了解了如何使用不同的分类器，以及朴素贝叶斯定理和与之相关的不同算法。我已经分享了一个关于建立和评估模型(DC-FEM)的广泛策略。还讨论了与朴素贝叶斯算法相关的挑战。我们讨论了“单词包”(BOW)模型以及使用CountVectorizer和TfidfVetorizer创建BOW的两种不同方式。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="baab" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">建议读数</strong>:</p><p id="1292" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在<a class="ae la" href="https://github.com/DrManishSharma/NLP.git" rel="noopener ugc nofollow" target="_blank">我的github库</a>访问完整代码。</p><p id="d15a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从零开始展开朴素贝叶斯</p><p id="c553" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk" rel="noopener ugc nofollow" target="_blank">使用NLTK的初学者文本分析</a></p></div></div>    
</body>
</html>