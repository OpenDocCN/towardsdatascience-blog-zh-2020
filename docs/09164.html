<html>
<head>
<title>NLP: Preparing text for deep learning model using TensorFlow2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:使用 TensorFlow2 为深度学习模型准备文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657?source=collection_archive---------8-----------------------#2020-07-01">https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657?source=collection_archive---------8-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0a14" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TensorFlow2 中的文本预处理(标记化、排序、填充)是如何工作的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/af2bc3f81a991ebaf892e211a438cbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADozCeNrsvSsxykhq9gbSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="3b9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自然语言处理(NLP)通常用于文本分类任务，例如垃圾邮件检测和情感分析、文本生成、语言翻译和文档分类。文本数据可以被认为是字符序列、单词序列或句子序列。最常见的是，对于大多数问题，文本数据被认为是单词序列。在本文中，我们将深入研究使用简单示例文本数据的预处理。然而，这里讨论的步骤适用于任何 NLP 任务。特别是，我们将使用 TensorFlow2 Keras 进行文本预处理，包括:</p><ul class=""><li id="6b6c" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">标记化</li><li id="8020" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">定序</li><li id="95d3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">填料</li></ul><p id="0ca2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图描述了文本预处理过程以及示例输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/8ddb861088475a9a65dfe4fd955fda8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*zsIXWoN0_CE9PXzmY3tIjQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从原始句子开始到填充序列的逐步文本预处理示例</p></figure><p id="79b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们导入所需的库。(我的<a class="ae mj" href="https://github.com/ShresthaSudip/Text-Preprocessing-Tensorflow2" rel="noopener ugc nofollow" target="_blank"> GitHub 页面</a>里有完整的 Jupyter 笔记本)。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="c12d" class="mp mq it ml b gy mr ms l mt mu">import tensorflow as tf<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><p id="ba1a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mv"> Tokenizer </em>是 TensorFlow Keras 中提供的一个 API，用来对句子进行分词。我们将文本数据定义为句子(每个句子用逗号分隔)和一组字符串。共有 4 个句子，包括 1 个最大长度为 5 的句子。我们的文本数据还包括标点符号，如下所示。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="0a9a" class="mp mq it ml b gy mr ms l mt mu">sentences = ["I want to go out.",<br/>             " I like to play.",<br/>             " No eating - ",<br/>             "No play!",<br/>            ]<br/>sentences</span><span id="e1b7" class="mp mq it ml b gy mw ms l mt mu">['I want to go out.', ' I like to play.', ' No eating - ', 'No play!']</span></pre><h1 id="9a4e" class="mx mq it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak">标记化</strong></h1><p id="6ebe" class="pw-post-body-paragraph ky kz it la b lb no ju ld le np jx lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">由于深度学习模型不理解文本，我们需要将文本转换成数字表示。为此，第一步是标记化。TensorFlow Keras 的<em class="mv"> Tokenizer </em> API 将句子拆分成单词，并将这些单词编码成整数。以下是在<em class="mv">记号赋予器</em> API 中使用的超参数:</p><ul class=""><li id="0051" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">num_words:限制训练时保留的最常用单词的最大数量。</li><li id="4ec1" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">过滤器:如果没有提供，默认情况下过滤掉所有标点符号(！idspnonenote)。"#$%&amp;()*+,-./:;&lt;=&gt;？@[\]^_'{|}~\t\n).</li><li id="7147" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">下限=1。这是默认设置，将所有单词转换为小写</li><li id="01aa" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">oov_tok:当它被使用时，词汇外标记将被添加到用于建立模型的语料库中的单词索引中。这用于在<em class="mv"> text_to_sequence </em>调用期间替换词汇表外的单词(不在我们的语料库中的单词)(见下文)。</li><li id="ebfd" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">word_index:将所有单词转换为整数索引。单词完整列表可用作键值属性:key =单词，value =单词的令牌</li></ul><p id="2274" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们使用<em class="mv">分词器</em>并打印出单词索引。我们已经使用了 num_words= 100，这对于这个数据来说是很多的，因为只有 9 个不同的单词和用于词汇外令牌的&lt; OOV &gt;字符串。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="1924" class="mp mq it ml b gy mr ms l mt mu">tokenizer = Tokenizer(num_words=100, lower= 1, oov_token="&lt;OOV&gt;")<br/>tokenizer.fit_on_texts(sentences)<br/>word_index = tokenizer.word_index</span><span id="741e" class="mp mq it ml b gy mw ms l mt mu">print(word_index)<br/>{'&lt;OOV&gt;': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}</span></pre><p id="1564" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所述，我们句子中的每个单词都被转换成了数字符号。例如，“I”的值为 2。分词器也忽略了单词后面的感叹号。例如，单词“play”或“play！”只有一个标记即 4。</p><h1 id="ce4c" class="mx mq it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak">排序</strong></h1><p id="c5ed" class="pw-post-body-paragraph ky kz it la b lb no ju ld le np jx lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">接下来，让我们使用来自 tokenizer 对象的<em class="mv"> texts_to_sequences </em>用数字序列表示每个句子。下面，我们打印出原始句子、单词索引和序列。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="6e9e" class="mp mq it ml b gy mr ms l mt mu">sequences = tokenizer.texts_to_sequences(sentences)<br/>print(sentences)<br/>print(word_index)<br/>print(sequences)</span><span id="0176" class="mp mq it ml b gy mw ms l mt mu">['I want to go out', ' I like to play', ' No eating - ', 'No play!']</span><span id="3f47" class="mp mq it ml b gy mw ms l mt mu">{'&lt;OOV&gt;': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}</span><span id="fefd" class="mp mq it ml b gy mw ms l mt mu">[[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]</span></pre><p id="c3d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所示，文本由序列表示。举个例子，</p><ul class=""><li id="f901" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">“我要出去”——&gt;[2，6，3，7，8]</li><li id="66de" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">“我喜欢玩”——&gt;[2，9，3，4]</li><li id="ab48" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">“不准吃东西”——&gt;[5，10]</li><li id="052a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">“不玩了！”— -&gt; [5, 4]</li></ul><h1 id="ebd9" class="mx mq it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak">填充</strong></h1><p id="e158" class="pw-post-body-paragraph ky kz it la b lb no ju ld le np jx lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">在任何原始文本数据中，自然会有不同长度的句子。然而，所有的神经网络都要求具有相同大小的输入。为此，需要进行填充。下面，我们用<em class="mv"> pad_sequences </em>进行填充。<em class="mv"> pad_sequences </em>使用 sequences、padding、maxlen、truncating、value 和 dtype 等参数。</p><ul class=""><li id="d27d" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">序列:我们之前创建的序列列表</li><li id="da89" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">padding = 'pre '或' post(默认 pre)。通过使用 pre，我们将在每个序列之前填充(添加 0 ), post 将在每个序列之后填充。</li><li id="2fdf" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">maxlen =所有序列的最大长度。如果没有提供，默认情况下它将使用最长句子的最大长度。</li><li id="766d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">truncating = 'pre '或' post '(默认为' pre ')。如果序列长度大于提供的 maxlen 值，则这些值将被截断为 maxlen 值。“前”选项将在序列的开头截断，而“后”将在序列的结尾截断。</li><li id="1161" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">值:填充值(默认值为 0)</li><li id="af34" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">dtype:输出序列类型(默认为 int32)</li></ul><p id="dd0b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们关注一下 pad_sequences 中使用的重要参数:padding、maxlen 和 truncating。</p><p id="09e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">前后填充</strong></p><p id="d114" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用“前”还是“后”填充取决于分析。在某些情况下，开头填充是合适的，而在其他情况下则不合适。例如，如果我们使用递归神经网络(RNN)进行垃圾邮件检测，那么在开始填充将是合适的，因为 RNN 不能学习长距离模式。开始时的填充允许我们保留最后的序列，因此 RNN 可以利用这些序列来预测下一个。然而，在任何情况下，填充都应该在仔细考虑和业务知识之后进行。</p><p id="05a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面显示了“前置”和“后置”填充的输出，带有序列最大长度的默认 maxlen 值。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="b2bc" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu"># pre padding</strong><br/>pre_pad = pad_sequences(sequences, padding='pre')</span><span id="c78f" class="mp mq it ml b gy mw ms l mt mu">print("\nword_index = ", word_index)<br/>print("\nsequences = ", sequences)<br/>print("\npadded_seq = " )<br/>print(pre_pad)</span><span id="69a5" class="mp mq it ml b gy mw ms l mt mu">word_index =  {'&lt;OOV&gt;': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}<br/><br/>sequences =  [[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]<br/><br/>padded_seq = <br/>[[ 2  6  3  7  8]<br/> [ <strong class="ml iu">0</strong>  2  9  3  4] &lt;---------- <strong class="ml iu">0 </strong>Padded at the beginning<br/> [ <strong class="ml iu">0  0  0</strong>  5 10] <br/> [ <strong class="ml iu">0  0  0</strong>  5  4]]</span></pre><p id="666b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们上面的例子中，最大长度的序列是[ 2，6，3，7，8]，对应的是“我想出去”。当使用 padding ='pre '时，在所有其他序列的开头添加填充值 0。因为其他序列比[ 2，6，3，7，8]更短，填充实际上使所有其他序列与此序列大小相同。</p><p id="9d80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，当使用 padding =“post”时，在序列的末尾添加填充值，即 0。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="184d" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu"># post padding</strong><br/>post_pad = pad_sequences(sequences, padding='post')<br/>print("\nword_index = ", word_index)<br/>print("\nsequences = ", sequences)<br/>print("\npadded_seq = " )<br/>print(post_pad)</span><span id="31f8" class="mp mq it ml b gy mw ms l mt mu">word_index =  {'&lt;OOV&gt;': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}<br/><br/>sequences =  [[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]<br/><br/>padded_seq = <br/>[[ 2  6  3  7  8]<br/> [ 2  9  3  4  <strong class="ml iu">0</strong>]&lt;---------- <strong class="ml iu">0 </strong>Padded at the end<br/> [ 5 10  <strong class="ml iu">0  0  0</strong>]<br/> [ 5  4  <strong class="ml iu">0  0  0</strong>]]</span></pre><p id="cb5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">使用 maxlen 和截断选项进行前后填充</strong></p><p id="6b8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果需要，我们可以同时使用填充和截断参数。下面我们展示了两种情况，1)预截断的预填充和 2)后截断的预填充</p><p id="8d78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用“pre”选项截断允许我们在开始时截断序列。然而，用“post”截断将在最后截断序列。</p><p id="612e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看一下使用前置截断进行前置填充的例子。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="c599" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu"># pre padding, maxlen and pre truncation</strong><br/>prepad_maxlen_pretrunc = pad_sequences(sequences, padding = ‘pre’, maxlen =4, truncating = ‘pre’)<br/>print(prepad_maxlen_pretrunc)</span><span id="c6cf" class="mp mq it ml b gy mw ms l mt mu">[[ 6  3  7  8]&lt;-----Truncated from length 5 to 4, at the beginning<br/> [ 2  9  3  4]<br/> [ <strong class="ml iu">0  0</strong>  5 10]&lt;---------- Padded at the beginning<br/> [ <strong class="ml iu">0  0 </strong> 5  4]]</span></pre><p id="c3e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过使用 maxlen =4，我们将填充序列的长度截断为 4。如上所示，maxlen=4 的使用影响了第一个序列[2，6，3，7，8]。该序列的长度为 5，被截断为 4。截断发生在开始时，因为我们使用了 truncating = 'pre '选项。</p><p id="1371" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看 truncation = 'post '选项。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="800b" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu"># pre padding, maxlen and post truncation</strong><br/>prepad_maxlen_posttrunc = pad_sequences(sequences, padding = 'pre', maxlen =4, truncating = 'post')<br/>print(prepad_maxlen_posttrunc)</span><span id="72ac" class="mp mq it ml b gy mw ms l mt mu">[[ 2  6  3  7]&lt;-----Truncated from length 5 to 4, at the end<br/> [ 2  9  3  4]<br/> [ <strong class="ml iu">0  0</strong>  5 10]&lt;---------- Padded at the beginning<br/> [ <strong class="ml iu">0  0</strong>  5  4]]</span></pre><p id="9167" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们使用 truncating = 'post '选项时，截断发生在最后。当应用后截断时，它影响第一个序列[ 2，6，3，7，8]并被截断为长度 4，从而产生序列[ 2，6，3，7]。</p><h1 id="6752" class="mx mq it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak">总结</strong></h1><p id="91d3" class="pw-post-body-paragraph ky kz it la b lb no ju ld le np jx lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">在本文中，我们专注于预处理原始文本数据，并为深度学习模型做准备。具体来说，我们讨论了对句子进行标记，将其表示为序列，并对其进行填充以使所有序列长度相同。该填充序列现在准备好用于神经网络的训练/测试分割。请参考 Laurence Moroney 的<a class="ae mj" href="https://www.youtube.com/watch?v=fNxaJsNG3-s" rel="noopener ugc nofollow" target="_blank"> NLP zero to hero </a>的视频进行进一步阅读。</p><p id="174e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在未来的文章中，我将解释我们如何在真实世界的数据中使用预处理，并在深度学习模型中使用这种嵌入的填充序列数据。</p><p id="5345" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读，如果您有任何意见或建议，请留下。</p><h2 id="4b78" class="mp mq it bd my nt nu dn nc nv nw dp ng lh nx ny ni ll nz oa nk lp ob oc nm od bi translated">快乐学习！！！</h2></div></div>    
</body>
</html>