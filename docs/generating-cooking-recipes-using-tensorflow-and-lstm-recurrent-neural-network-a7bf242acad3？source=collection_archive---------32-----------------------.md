# ä½¿ç”¨å¼ é‡æµå’Œ LSTM é€’å½’ç¥ç»ç½‘ç»œç”Ÿæˆçƒ¹é¥ªé£Ÿè°±:ä¸€æ­¥ä¸€æ­¥æŒ‡å—

> åŸæ–‡ï¼š<https://towardsdatascience.com/generating-cooking-recipes-using-tensorflow-and-lstm-recurrent-neural-network-a7bf242acad3?source=collection_archive---------32----------------------->

![](img/d3d0b291dbb98266cd7d509243995eb6.png)

ç…§ç‰‡ç”± [home_full_of_recipes](https://www.instagram.com/home_full_of_recipes/) æ‹æ‘„(Instagram é¢‘é“)

# TLï¼›é€Ÿåº¦ä¸‰è§’å½¢å®šä½æ³•(dead reckoning)

æˆ‘ç”¨ TensorFlow åœ¨ *~100k* é£Ÿè°±æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªäººç‰©çº§åˆ«çš„ LSTM *(é•¿çŸ­æœŸè®°å¿†)**ï¼Œå®ƒå»ºè®®æˆ‘åš*â€œå¥¶æ²¹è‹æ‰“åŠ æ´‹è‘±â€*ã€*ã€ã€æ¾é¥¼è‰è“æ±¤ã€‘ã€*ã€*ã€*ã€*ã€ã€ä¸‰æ–‡é±¼ç‰›è‚‰æ…•æ–¯å’Œå¢¨è¥¿å“¥èƒ¡æ¤’æ–¯è’‚å°”é¡¿æ²™æ‹‰ã€‘**

*åœ¨è¿™é‡Œï¼Œä½ å¯èƒ½ä¼šæ‰¾åˆ°æ›´å¤šæˆ‘æœ€åå¾—åˆ°çš„ä¾‹å­:*

*   *ğŸ¨ [**çƒ¹é¥ªé£Ÿè°±ç”Ÿæˆå™¨æ¼”ç¤º**](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN)â€”â€”åœ¨ä½ çš„æµè§ˆå™¨ä¸­äº¤äº’å¼åœ°å°è¯•è¯¥æ¨¡å‹ã€‚*
*   *ğŸ‹ğŸ»â€ [**LSTM æ¨¡å‹è®­ç»ƒæµç¨‹**](https://github.com/trekhleb/machine-learning-experiments/blob/master/experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb)â€”â€”çœ‹çœ‹æ¨¡å‹æ˜¯æ€ä¹ˆè®­ç»ƒå‡ºæ¥çš„ã€‚*
*   *[ğŸ¤–äº¤äº’å¼æœºå™¨å­¦ä¹ å®éªŒ](https://github.com/trekhleb/machine-learning-experiments) çŸ¥è¯†åº“â€”â€”æŸ¥çœ‹æ›´å¤šâ€œç‰©ä½“æ£€æµ‹â€ã€â€œè‰å›¾è¯†åˆ«â€ã€â€œå›¾åƒåˆ†ç±»â€ç­‰å®éªŒã€‚*

*æœ¬æ–‡åŒ…å«äº† LSTM æ¨¡å‹å¦‚ä½•åœ¨ Python ä¸Šä½¿ç”¨ [TensorFlow 2](https://www.tensorflow.org/) å’Œ [Keras API](https://www.tensorflow.org/guide/keras) è¿›è¡Œå®é™…è®­ç»ƒçš„ç»†èŠ‚ã€‚*

*![](img/159e198694de4956a567ee3b518a6b02.png)*

*æ¥è‡ª[æœºå™¨å­¦ä¹ å®éªŒ](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN)çš„å±å¹•è®°å½•*

# *æˆ‘ä»¬çš„æ¨¡å‹æœ€ç»ˆä¼šå­¦åˆ°ä»€ä¹ˆ*

*ç»è¿‡å‡ ä¸ªå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„è§’è‰²çº§ RNN æ¨¡å‹å°†å­¦ä¹ è‹±è¯­è¯­æ³•å’Œæ ‡ç‚¹ç¬¦å·çš„åŸºæœ¬æ¦‚å¿µ(æˆ‘å¸Œæœ›æˆ‘èƒ½è¿™ä¹ˆå¿«å­¦ä¼šè‹±è¯­ï¼).å®ƒè¿˜å°†å­¦ä¹ å¦‚ä½•ç”Ÿæˆé…æ–¹çš„ä¸åŒéƒ¨åˆ†ï¼Œå¦‚*ğŸ“—ã€é…æ–¹åç§°ã€‘*ï¼Œ*ğŸ¥•ã€é…æ–¹æˆåˆ†ã€‘*å’Œ*ğŸ“ã€é£Ÿè°±è¯´æ˜ã€‘*ã€‚æœ‰æ—¶å€™é£Ÿè°±åç§°ã€é…æ–™å’Œè¯´æ˜ä¼šå¾ˆæœ‰è¶£ï¼Œæœ‰æ—¶å€™å¾ˆæ„šè ¢ï¼Œæœ‰æ—¶å€™å¾ˆæœ‰è¶£ã€‚*

*ä¸‹é¢æ˜¯å‡ ä¸ªç”Ÿæˆçš„é…æ–¹ç¤ºä¾‹:*

```
*ğŸ“— [NAME]Orange Club Tea Sandwich CookiesğŸ¥• [INGREDIENTS]â€¢ 1 cup (2 sticks) unsalted butter, softened
â€¢ 1 cup confectioners' sugar
â€¢ 1/2 cup flaxseed meal
â€¢ 1/2 cup shelled pumpkin seeds (pecans, blanched and sliced)
â€¢ 2 teaspoons vanilla extractğŸ“ [INSTRUCTIONS]â–ªï¸ Preheat oven to 350 degrees F.
â–ªï¸ Combine cake mix, milk, egg and sugar in a large bowl. Stir until combined and smooth but not sticky. Using a spatula, sprinkle the dough biscuits over the bottom of the pan. Sprinkle with sugar, and spread evenly. Bake for 20 minutes. Remove from the oven and cool on a rack. To serve, add the chocolate.*
```

*æˆ–è€…å¦ä¸€ä¸ª:*

```
*ğŸ“— [NAME]Mushrooms with Lentil Stewed Shallots and TomatoesğŸ¥• [INGREDIENTS]â€¢ 1 tablespoon olive oil
â€¢ 3 cloves garlic, smashed
â€¢ Kosher salt
â€¢ 1 1/2 pounds lean ground turkey
â€¢ 1 cup coarsely peeled tart apples
â€¢ 2 tablespoons chopped garlic
â€¢ 1 teaspoon ground cumin
â€¢ 1/2 teaspoon cayenne pepper
â€¢ 1 teaspoon chopped fresh thyme
â€¢ 3/4 cup chopped fresh basil
â€¢ 1/2 small carrot, halved lengthwise and cut into 1/2-inch pieces
â€¢ 1 roasted red pepper, halved and sliced vertically diced and separated into rough chops
â€¢ 3 tablespoons unsalted butter
â€¢ 2 cups shredded mozzarella
â€¢ 1/4 cup grated parmesan cheese
â€¢ 1/4 cup prepared basil pestoğŸ“ [INSTRUCTIONS]â–ªï¸ Stir the olive oil, garlic, thyme and 1 teaspoon salt in a saucepan; bring to a simmer over medium heat. Remove from the heat. Add the basil and toast the soup for 2 minutes.
â–ªï¸ Meanwhile, heat 4 to 4 inches vegetable oil in the skillet over medium-high heat. Add the olive oil, garlic, 1/2 teaspoon salt and 1/2 teaspoon pepper and cook, stirring often, until cooked through, a*
```

*âš ï¸ *æœ¬æ–‡ä¸­çš„é£Ÿè°±åªæ˜¯ä¸ºäº†å¨±ä¹å’Œå­¦ä¹ ç›®çš„è€Œåˆ¶ä½œçš„ã€‚é£Ÿè°±æ˜¯* ***è€Œä¸æ˜¯*** *ç”¨äºå®é™…çƒ¹é¥ªï¼**

# *å…ˆéªŒçŸ¥è¯†*

*å‡è®¾ä½ å·²ç»ç†Ÿæ‚‰[é€’å½’ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Recurrent_neural_network)çš„æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯[é•¿çŸ­æœŸè®°å¿†(LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) æ¶æ„ã€‚*

*â„¹ï¸ï¼Œå¦‚æœè¿™äº›æ¦‚å¿µå¯¹ä½ æ¥è¯´æ˜¯æ–°çš„ï¼Œæˆ‘å¼ºçƒˆæ¨èä½ å‚åŠ  Coursera çš„æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹ã€‚æµè§ˆä¸€ä¸‹ Andrej Karpathy çš„æ–‡ç« *ä¸­çš„[å¾ªç¯ç¥ç»ç½‘ç»œ](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)çš„ä¸åˆç†çš„æœ‰æ•ˆæ€§å¯èƒ½ä¹Ÿæ˜¯æœ‰ç›Šçš„ã€‚**

*åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œ**é€’å½’ç¥ç»ç½‘ç»œ(RNN)** æ˜¯ä¸€ç±»æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæœ€å¸¸ç”¨äºåŸºäºåºåˆ—çš„æ•°æ®ï¼Œå¦‚è¯­éŸ³ã€å£°éŸ³ã€æ–‡æœ¬æˆ–éŸ³ä¹ã€‚å®ƒä»¬ç”¨äºæœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰ã€‚rnn çš„å…³é”®ç‰¹å¾æ˜¯å®ƒä»¬æ˜¯æœ‰çŠ¶æ€çš„ï¼Œå¹¶ä¸”å®ƒä»¬å…·æœ‰å†…éƒ¨å­˜å‚¨å™¨ï¼Œå…¶ä¸­å¯ä»¥å­˜å‚¨åºåˆ—çš„ä¸€äº›ä¸Šä¸‹æ–‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœåºåˆ—çš„ç¬¬ä¸€ä¸ªå•è¯æ˜¯`He`ï¼ŒRNN å¯èƒ½å‘`speaks`å»ºè®®ä¸‹ä¸€ä¸ªå•è¯ï¼Œè€Œä¸ä»…ä»…æ˜¯`speak`(ä»¥å½¢æˆ`He speaks`çŸ­è¯­)ï¼Œå› ä¸ºå…³äºç¬¬ä¸€ä¸ªå•è¯`He`çš„å…ˆéªŒçŸ¥è¯†å·²ç»åœ¨å†…å­˜ä¸­ã€‚*

*![](img/7be41e544849fdb250828ae676bcb1e5.png)*

*å›¾ç‰‡æ¥æº:[é€’å½’ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Recurrent_neural_network)ç»´åŸºç™¾ç§‘ä¸Šçš„æ–‡ç« *

*![](img/bb1ddaafa74637ddf04ca24c0b8c53cb.png)*

*å›¾ç‰‡æ¥æº:[LSTM å’Œ GRU](/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)å…³äºæ•°æ®ç§‘å­¦çš„å›¾æ–‡å¹¶èŒ‚çš„æ–‡ç« *

*ä»¤äººå…´å¥‹çš„æ˜¯ï¼ŒRNN(å°¤å…¶æ˜¯ LSTM)ä¸ä»…èƒ½è®°ä½*å•è¯å¯¹å•è¯*çš„ä¾å­˜å…³ç³»ï¼Œè¿˜èƒ½è®°ä½*å­—ç¬¦å¯¹å­—ç¬¦*çš„ä¾å­˜å…³ç³»ï¼åºåˆ—ç”±ä»€ä¹ˆç»„æˆå¹¶ä¸é‡è¦:å¯èƒ½æ˜¯å•è¯ï¼Œä¹Ÿå¯èƒ½æ˜¯å­—ç¬¦ã€‚é‡è¦çš„æ˜¯å®ƒä»¬å½¢æˆäº†ä¸€ä¸ªæ—¶é—´åˆ†å¸ƒçš„åºåˆ—ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå­—ç¬¦åºåˆ—`['H', 'e']`ã€‚å¦‚æœæˆ‘ä»¬é—® LSTM ä¸‹ä¸€æ­¥å¯èƒ½ä¼šåšä»€ä¹ˆï¼Œå®ƒå¯èƒ½ä¼šå»ºè®®ä¸€ä¸ª`<stop_word>`(æ„æ€æ˜¯ï¼Œç»„æˆå•è¯`He`çš„åºåˆ—å·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥åœæ­¢)ï¼Œæˆ–è€…å®ƒä¹Ÿå¯èƒ½ä¼šå»ºè®®ä¸€ä¸ªå­—ç¬¦`l`(æ„æ€æ˜¯ï¼Œå®ƒè¯•å›¾ä¸ºæˆ‘ä»¬å»ºç«‹ä¸€ä¸ª`Hello`åºåˆ—)ã€‚è¿™ç§ç±»å‹çš„ rnn è¢«ç§°ä¸º**å­—ç¬¦çº§ rnn**(ä¸**å•è¯çº§ rnn**ç›¸å¯¹)ã€‚*

*åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¾é  RNN ç½‘ç»œçš„è®°å¿†åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªè§’è‰²çº§åˆ«çš„ LSTM ç‰ˆæœ¬æ¥ç”Ÿæˆçƒ¹é¥ªé£Ÿè°±ã€‚*

# *æ¢ç´¢æ•°æ®é›†*

*è®©æˆ‘ä»¬æµè§ˆå‡ ä¸ªå¯ç”¨çš„æ•°æ®é›†ï¼Œå¹¶æ¢è®¨å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚æˆ‘å¸Œæœ›æ•°æ®é›†æ»¡è¶³çš„ä¸€ä¸ªè¦æ±‚æ˜¯ï¼Œå®ƒä¸ä»…åº”è¯¥æœ‰ä¸€ä¸ªé…æ–™åˆ—è¡¨ï¼Œè¿˜åº”è¯¥æœ‰çƒ¹é¥ªè¯´æ˜ã€‚æˆ‘è¿˜å¸Œæœ›å®ƒæœ‰ä¸€ä¸ªæªæ–½å’Œæ¯ç§æˆåˆ†çš„æ•°é‡ã€‚*

*ä»¥ä¸‹æ˜¯æˆ‘å‘ç°çš„å‡ ä¸ªçƒ¹é¥ªé£Ÿè°±æ•°æ®é›†:*

*   *ğŸ¤·[é£Ÿè°±é…æ–™æ•°æ®é›†](https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home) *(æ²¡æœ‰é…æ–™æ¯”ä¾‹)**
*   *ğŸ¤· [Recipe1M+](http://pic2recipe.csail.mit.edu/) *(é£Ÿè°±å¾ˆå¤šä½†éœ€è¦æ³¨å†Œæ‰èƒ½ä¸‹è½½)**
*   *ğŸ¤·[ç¾é£Ÿå®¶â€”â€”å¸¦è¯„çº§å’Œè¥å…»çš„é£Ÿè°±](https://www.kaggle.com/hugodarwood/epirecipes?select=full_format_recipes.json) *(ä»…çº¦ 20k ä»½é£Ÿè°±ï¼Œæœ€å¥½èƒ½æ‰¾åˆ°æ›´å¤š)**
*   *ğŸ‘ğŸ»[é£Ÿè°±æ¡†](https://eightportions.com/datasets/Recipes/)*(~ 12.5 ä¸‡ä»½é£Ÿè°±æ­é…é£Ÿææ¯”ä¾‹ï¼Œä¸é”™)**

*è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨â€œé…æ–¹ç®±â€æ•°æ®é›†ã€‚èœè°±çš„æ•°é‡çœ‹èµ·æ¥è¶³å¤Ÿå¤šï¼Œè€Œä¸”å®ƒæ—¢åŒ…å«é…æ–™åˆåŒ…å«çƒ¹é¥ªè¯´æ˜ã€‚çœ‹çœ‹ RNN æ˜¯å¦èƒ½å¤Ÿäº†è§£é…æ–™å’Œè¯´æ˜ä¹‹é—´çš„è”ç³»æ˜¯å¾ˆæœ‰è¶£çš„ã€‚*

# *ä¸ºè®­ç»ƒè®¾ç½® TensorFlow/Python æ²™ç›’*

*åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ä¸ªé€‰é¡¹æ¥è¯•éªŒä»£ç :*

1.  *ä½ å¯ä»¥åœ¨ä½ çš„æµè§ˆå™¨ *(ä¸éœ€è¦æœ¬åœ°è®¾ç½®)*ä¸­ä½¿ç”¨ [GoogleColab è¿›è¡Œå®éªŒã€‚](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb)*
2.  *ä½ å¯ä»¥åœ¨ä½ çš„æµè§ˆå™¨ä¸­ä½¿ç”¨ [Jupyter ç¬”è®°æœ¬](https://mybinder.org/v2/gh/trekhleb/machine-learning-experiments/master?filepath=experiments/recipe_generation_rnn/recipe_generation_rnn.ipynb) *(ä¸éœ€è¦æœ¬åœ°è®¾ç½®)*ã€‚*
3.  *ä½ å¯ä»¥åœ¨æœ¬åœ°å»ºç«‹ä¸€ä¸ª Jupyter ç¬”è®°æœ¬ã€‚*

*æˆ‘ä¼šå»ºè®®ä½¿ç”¨ GoogleColab é€‰é¡¹ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¯¹æ‚¨è¿›è¡Œä»»ä½•æœ¬åœ°è®¾ç½®(æ‚¨å¯ä»¥åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­è¿›è¡Œå®éªŒ)ï¼Œå¹¶ä¸”å®ƒè¿˜æä¾›äº†å¼ºå¤§çš„ GPU æ”¯æŒï¼Œå¯ä»¥ä½¿æ¨¡å‹è®­ç»ƒæ›´å¿«ã€‚æ‚¨ä¹Ÿå¯ä»¥å°è¯•è®­ç»ƒå‚æ•°ã€‚*

# *å¯¼å…¥ä¾èµ–å…³ç³»*

*è®©æˆ‘ä»¬ä»å¯¼å…¥ä¸€äº›æˆ‘ä»¬ä»¥åä¼šç”¨åˆ°çš„åŒ…å¼€å§‹ã€‚*

```
**# Packages for training the model and working with the dataset.*
**import** tensorflow **as** tf
**import** matplotlib.pyplot **as** plt
**import** numpy **as** np
**import** json*# Utility/helper packages.*
**import** platform
**import** time
**import** pathlib
**import** os*
```

*é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬çš„ç¯å¢ƒè®¾ç½®æ­£ç¡®ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ Tensorflow çš„ *2nd* ç‰ˆæœ¬ã€‚*

```
*print('Python version:', platform.python_version())
print('Tensorflow version:', tf.__version__)
print('Keras version:', tf.keras.__version__)*
```

**â”è¾“å‡º:**

```
*Python version: 3.7.6
Tensorflow version: 2.1.0
Keras version: 2.2.4-tf*
```

# *åŠ è½½æ•°æ®é›†*

*è®©æˆ‘ä»¬ä½¿ç”¨[TF . keras . utils . get _ file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)åŠ è½½æ•°æ®é›†ã€‚ä½¿ç”¨`get_file()`å®ç”¨ç¨‹åºå¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºå®ƒä¸ºæ‚¨å¤„ç†ç°æˆçš„ç¼“å­˜ã€‚è¿™æ„å‘³ç€æ‚¨å°†åªä¸‹è½½ä¸€æ¬¡æ•°æ®é›†æ–‡ä»¶ï¼Œå³ä½¿æ‚¨å†æ¬¡åœ¨ç¬”è®°æœ¬ä¸­å¯åŠ¨ç›¸åŒçš„ä»£ç å—ï¼Œå®ƒä¹Ÿä¼šä½¿ç”¨ç¼“å­˜ï¼Œå¹¶ä¸”ä»£ç å—çš„æ‰§è¡Œé€Ÿåº¦ä¼šæ›´å¿«ã€‚*

*å¦‚æœç¼“å­˜æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ:*

```
*CACHE_DIR = './tmp'
pathlib.Path(CACHE_DIR).mkdir(exist_ok=**True**)*
```

*ä¸‹è½½å¹¶è§£åŒ…æ•°æ®é›†:*

```
*dataset_file_name = 'recipes_raw.zip'
dataset_file_origin = 'https://storage.googleapis.com/recipe-box/recipes_raw.zip'dataset_file_path = tf.keras.utils.get_file(
    fname=dataset_file_name,
    origin=dataset_file_origin,
    cache_dir=CACHE_DIR,
    extract=**True**,
    archive_format='zip'
)print(dataset_file_path)*
```

*ä»¥ä¸‹æ˜¯ä¸‹è½½åæ•°æ®é›†æ–‡ä»¶çš„è·¯å¾„:*

**â”è¾“å‡º:**

```
*./tmp/datasets/recipes_raw.zip*
```

*è®©æˆ‘ä»¬æ‰“å°ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œçœ‹çœ‹åˆ°åº•ä¸‹è½½äº†ä»€ä¹ˆ:*

```
*!ls -la ./tmp/datasets/*
```

**â”è¾“å‡º:**

```
*total 521128
drwxr-xr-x  7        224 May 13 18:10 .
drwxr-xr-x  4        128 May 18 18:00 ..
-rw-r--r--  1      20437 May 20 06:46 LICENSE
-rw-r--r--  1   53355492 May 13 18:10 recipes_raw.zip
-rw-r--r--  1   49784325 May 20 06:46 recipes_raw_nosource_ar.json
-rw-r--r--  1   61133971 May 20 06:46 recipes_raw_nosource_epi.json
-rw-r--r--  1   93702755 May 20 06:46 recipes_raw_nosource_fn.json*
```

*å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†ç”± 3 ä¸ªæ–‡ä»¶ç»„æˆã€‚ç¨åï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›æ–‡ä»¶ä¸­çš„ä¿¡æ¯åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚*

*è®©æˆ‘ä»¬ä»`json`æ–‡ä»¶ä¸­åŠ è½½æ•°æ®é›†æ•°æ®ï¼Œå¹¶ä»ä¸­é¢„è§ˆç¤ºä¾‹ã€‚*

```
***def** **load_dataset**(silent=False):
    *# List of dataset files we want to merge.*
    dataset_file_names = [
        'recipes_raw_nosource_ar.json',
        'recipes_raw_nosource_epi.json',
        'recipes_raw_nosource_fn.json',
    ]

    dataset = [] **for** dataset_file_name **in** dataset_file_names:
        dataset_file_path = f'{CACHE_DIR}/datasets/{dataset_file_name}' **with** open(dataset_file_path) **as** dataset_file:
            json_data_dict = json.load(dataset_file)
            json_data_list = list(json_data_dict.values())
            dict_keys = [key **for** key **in** json_data_list[0]]
            dict_keys.sort()
            dataset += json_data_list *# This code block outputs the summary for each dataset.*
            **if** silent == **False**:
                print(dataset_file_path)
                print('===========================================')
                print('Number of examples: ', len(json_data_list), '\n')
                print('Example object keys:\n', dict_keys, '\n')
                print('Example object:\n', json_data_list[0], '\n')
                print('Required keys:\n')
                print('  title: ', json_data_list[0]['title'], '\n')
                print('  ingredients: ', json_data_list[0]['ingredients'], '\n')
                print('  instructions: ', json_data_list[0]['instructions'])
                print('\n\n') **return** dataset dataset_raw = load_dataset()*
```

**â”è¾“å‡º:**

```
*./tmp/datasets/recipes_raw_nosource_ar.json
===========================================
Number of examples:  39802 Example object keys:
 ['ingredients', 'instructions', 'picture_link', 'title'] Example object:
 {'title': 'Slow Cooker Chicken and Dumplings', 'ingredients': ['4 skinless, boneless chicken breast halves ADVERTISEMENT', '2 tablespoons butter ADVERTISEMENT', '2 (10.75 ounce) cans condensed cream of chicken soup ADVERTISEMENT', '1 onion, finely diced ADVERTISEMENT', '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ADVERTISEMENT', 'ADVERTISEMENT'], 'instructions': 'Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.\nCover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.\n', 'picture_link': '55lznCYBbs2mT8BTx6BTkLhynGHzM.S'} Required keys: title:  Slow Cooker Chicken and Dumplings  ingredients:  ['4 skinless, boneless chicken breast halves ADVERTISEMENT', '2 tablespoons butter ADVERTISEMENT', '2 (10.75 ounce) cans condensed cream of chicken soup ADVERTISEMENT', '1 onion, finely diced ADVERTISEMENT', '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ADVERTISEMENT', 'ADVERTISEMENT'] instructions:  Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.
Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center. ./tmp/datasets/recipes_raw_nosource_epi.json
===========================================
Number of examples:  25323 Example object keys:
 ['ingredients', 'instructions', 'picture_link', 'title'] Example object:
 {'ingredients': ['12 egg whites', '12 egg yolks', '1 1/2 cups sugar', '3/4 cup rye whiskey', '12 egg whites', '3/4 cup brandy', '1/2 cup rum', '1 to 2 cups heavy cream, lightly whipped', 'Garnish: ground nutmeg'], 'picture_link': None, 'instructions': 'Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.\nBeat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.', 'title': 'Christmas Eggnog '} Required keys: title:  Christmas Eggnog  ingredients:  ['12 egg whites', '12 egg yolks', '1 1/2 cups sugar', '3/4 cup rye whiskey', '12 egg whites', '3/4 cup brandy', '1/2 cup rum', '1 to 2 cups heavy cream, lightly whipped', 'Garnish: ground nutmeg']   instructions:  Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste.
Beat the egg whites until stiff, gradually adding in 3/4 cup sugar. Set aside. Beat the egg yolks until they are thick and pale and add the other 3/4 cup sugar and stir in rye whiskey. Blend well. Fold the egg white mixture into the yolk mixture and add the brandy and the rum. Beat the mixture well. To serve, fold the lightly whipped heavy cream into the eggnog. (If a thinner mixture is desired, add the heavy cream unwhipped.) Sprinkle the top of the eggnog with the nutmeg to taste../tmp/datasets/recipes_raw_nosource_fn.json
===========================================
Number of examples:  60039 Example object keys:
 ['ingredients', 'instructions', 'picture_link', 'title'] Example object:
 {'instructions': 'Toss ingredients lightly and spoon into a buttered baking dish. Top with additional crushed cracker crumbs, and brush with melted butter. Bake in a preheated at 350 degrees oven for 25 to 30 minutes or until delicately browned.', 'ingredients': ['1/2 cup celery, finely chopped', '1 small green pepper finely chopped', '1/2 cup finely sliced green onions', '1/4 cup chopped parsley', '1 pound crabmeat', '1 1/4 cups coarsely crushed cracker crumbs', '1/2 teaspoon salt', '3/4 teaspoons dry mustard', 'Dash hot sauce', '1/4 cup heavy cream', '1/2 cup melted butter'], 'title': "Grammie Hamblet's Deviled Crab", 'picture_link': None} Required keys: title:  Grammie Hamblet's Deviled Crab  ingredients:  ['1/2 cup celery, finely chopped', '1 small green pepper finely chopped', '1/2 cup finely sliced green onions', '1/4 cup chopped parsley', '1 pound crabmeat', '1 1/4 cups coarsely crushed cracker crumbs', '1/2 teaspoon salt', '3/4 teaspoons dry mustard', 'Dash hot sauce', '1/4 cup heavy cream', '1/2 cup melted butter']   instructions:  Toss ingredients lightly and spoon into a buttered baking dish. Top with additional crushed cracker crumbs, and brush with melted butter. Bake in a preheated at 350 degrees oven for 25 to 30 minutes or until delicately browned.*
```

*è®©æˆ‘ä»¬ç»Ÿè®¡ä¸€ä¸‹åˆå¹¶æ–‡ä»¶åçš„ç¤ºä¾‹æ€»æ•°:*

```
*print('Total number of raw examples: ', len(dataset_raw))*
```

**â”è¾“å‡º:**

```
*Total number of raw examples:  125164*
```

# *é¢„å¤„ç†æ•°æ®é›†*

# *è¿‡æ»¤æ‰ä¸å®Œæ•´çš„ä¾‹å­*

*æœ‰å¯èƒ½æœ‰äº›èœè°±æ²¡æœ‰ä¸€äº›å¿…å¡«å­—æ®µ(*åç§°*ã€*é…æ–™*æˆ–*è¯´æ˜*)ã€‚æˆ‘ä»¬éœ€è¦ä»è¿™äº›ä¸å®Œæ•´çš„ä¾‹å­ä¸­æ¸…ç†å‡ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚*

*ä»¥ä¸‹å‡½æ•°å°†å¸®åŠ©æˆ‘ä»¬ç­›é€‰å‡ºæ—¢æ²¡æœ‰æ ‡é¢˜ä¹Ÿæ²¡æœ‰é…æ–™æˆ–è¯´æ˜çš„é£Ÿè°±:*

```
***def** **recipe_validate_required_fields**(recipe):
    required_keys = ['title', 'ingredients', 'instructions']

    **if** **not** recipe:
        **return** **False**

    **for** required_key **in** required_keys:
        **if** **not** recipe[required_key]:
            **return** **False**

        **if** type(recipe[required_key]) == list **and** len(recipe[required_key]) == 0:
            **return** **False**

    **return** **True***
```

*ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨`recipe_validate_required_fields()`å‡½æ•°è¿›è¡Œè¿‡æ»¤:*

```
*dataset_validated = [recipe **for** recipe **in** dataset_raw **if** recipe_validate_required_fields(recipe)]print('Dataset size BEFORE validation', len(dataset_raw))
print('Dataset size AFTER validation', len(dataset_validated))
print('Number of incomplete recipes', len(dataset_raw) - len(dataset_validated))*
```

**â”è¾“å‡º:**

```
*Dataset size BEFORE validation 125164
Dataset size AFTER validation 122938
Number of incomplete recipes 2226*
```

*æ­£å¦‚ä½ å¯èƒ½çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„`2226`é£Ÿè°±æœ‰äº›ä¸å®Œæ•´ã€‚*

# *å°†é…æ–¹å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²*

*RNN ä¸ç†è§£ç‰©ä½“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°† recipes å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè½¬æ¢ä¸ºæ•°å­—(ç´¢å¼•)ã€‚è®©æˆ‘ä»¬ä»å°† recipes å¯¹è±¡è½¬æ¢æˆå­—ç¬¦ä¸²å¼€å§‹ã€‚*

*ä¸ºäº†å¸®åŠ©æˆ‘ä»¬çš„ RNN æ›´å¿«åœ°å­¦ä¹ æ–‡ç« çš„ç»“æ„ï¼Œè®©æˆ‘ä»¬ç»™å®ƒæ·»åŠ  3 ä¸ªâ€œåœ°æ ‡â€ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›ç‹¬ç‰¹çš„â€œæ ‡é¢˜â€ã€â€œé…æ–™â€å’Œâ€œè¯´æ˜â€æ ‡å¿—æ¥åˆ†éš”æ¯ä¸ªé£Ÿè°±çš„é€»è¾‘éƒ¨åˆ†ã€‚*

```
*STOP_WORD_TITLE = 'ğŸ“— '
STOP_WORD_INGREDIENTS = '\nğŸ¥•\n\n'
STOP_WORD_INSTRUCTIONS = '\nğŸ“\n\n'*
```

*ä¸‹é¢çš„å‡½æ•°å°† recipe å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²(å­—ç¬¦åºåˆ—),ä»¥ä¾¿ä»¥ååœ¨ RNN è¾“å…¥ä¸­ä½¿ç”¨ã€‚*

```
***def** **recipe_to_string**(recipe):
    *# This string is presented as a part of recipes so we need to clean it up.*
    noize_string = 'ADVERTISEMENT'

    title = recipe['title']
    ingredients = recipe['ingredients']
    instructions = recipe['instructions'].split('\n')

    ingredients_string = ''
    **for** ingredient **in** ingredients:
        ingredient = ingredient.replace(noize_string, '')
        **if** ingredient:
            ingredients_string += f'â€¢ {ingredient}\n'

    instructions_string = ''
    **for** instruction **in** instructions:
        instruction = instruction.replace(noize_string, '')
        **if** instruction:
            instructions_string += f'â–ªï¸ {instruction}\n'

    **return** f'{STOP_WORD_TITLE}{title}\n{STOP_WORD_INGREDIENTS}{ingredients_string}{STOP_WORD_INSTRUCTIONS}{instructions_string}'*
```

*è®©æˆ‘ä»¬å°†`recipe_to_string()`å‡½æ•°åº”ç”¨äº`dataset_validated`:*

```
*dataset_stringified = [recipe_to_string(recipe) **for** recipe **in** dataset_validated]print('Stringified dataset size: ', len(dataset_stringified))*
```

**â”è¾“å‡º:**

```
*Stringified dataset size:  122938*
```

*è®©æˆ‘ä»¬å…ˆæ¥é¢„è§ˆå‡ ä¸ªé£Ÿè°±:*

```
***for** recipe_index, recipe_string **in** enumerate(dataset_stringified[:3]):
    print('Recipe #{}\n---------'.format(recipe_index + 1))
    print(recipe_string)
    print('\n')*
```

**â”è¾“å‡º:**

```
*Recipe #1
---------
ğŸ“— Slow Cooker Chicken and DumplingsğŸ¥•â€¢ 4 skinless, boneless chicken breast halves 
â€¢ 2 tablespoons butter 
â€¢ 2 (10.75 ounce) cans condensed cream of chicken soup 
â€¢ 1 onion, finely diced 
â€¢ 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ğŸ“â–ªï¸ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.
â–ªï¸ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.Recipe #2
---------
ğŸ“— Awesome Slow Cooker Pot RoastğŸ¥•â€¢ 2 (10.75 ounce) cans condensed cream of mushroom soup 
â€¢ 1 (1 ounce) package dry onion soup mix 
â€¢ 1 1/4 cups water 
â€¢ 5 1/2 pounds pot roast ğŸ“â–ªï¸ In a slow cooker, mix cream of mushroom soup, dry onion soup mix and water. Place pot roast in slow cooker and coat with soup mixture.
â–ªï¸ Cook on High setting for 3 to 4 hours, or on Low setting for 8 to 9 hours.Recipe #3
---------
ğŸ“— Brown Sugar MeatloafğŸ¥•â€¢ 1/2 cup packed brown sugar 
â€¢ 1/2 cup ketchup 
â€¢ 1 1/2 pounds lean ground beef 
â€¢ 3/4 cup milk 
â€¢ 2 eggs 
â€¢ 1 1/2 teaspoons salt 
â€¢ 1/4 teaspoon ground black pepper 
â€¢ 1 small onion, chopped 
â€¢ 1/4 teaspoon ground ginger 
â€¢ 3/4 cup finely crushed saltine cracker crumbs ğŸ“â–ªï¸ Preheat oven to 350 degrees F (175 degrees C). Lightly grease a 5x9 inch loaf pan.
â–ªï¸ Press the brown sugar in the bottom of the prepared loaf pan and spread the ketchup over the sugar.
â–ªï¸ In a mixing bowl, mix thoroughly all remaining ingredients and shape into a loaf. Place on top of the ketchup.
â–ªï¸ Bake in preheated oven for 1 hour or until juices are clear.*
```

*å‡ºäºå¥½å¥‡ï¼Œè®©æˆ‘ä»¬ä»æ•°æ®é›†çš„ä¸­é—´é¢„è§ˆä¸€ä¸‹èœè°±ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦å…·æœ‰é¢„æœŸçš„æ•°æ®ç»“æ„:*

```
*print(dataset_stringified[50000])*
```

**â”è¾“å‡º:**

```
*ğŸ“— Herbed Bean RagoÃ»t ğŸ¥•â€¢ 6 ounces haricots verts (French thin green beans), trimmed and halved crosswise
â€¢ 1 (1-pound) bag frozen edamame (soybeans in the pod) or 1 1/4 cups frozen shelled edamame, not thawed
â€¢ 2/3 cup finely chopped onion
â€¢ 2 garlic cloves, minced
â€¢ 1 Turkish bay leaf or 1/2 California bay leaf
â€¢ 2 (3-inch) fresh rosemary sprigs
â€¢ 1/2 teaspoon salt
â€¢ 1/4 teaspoon black pepper
â€¢ 1 tablespoon olive oil
â€¢ 1 medium carrot, cut into 1/8-inch dice
â€¢ 1 medium celery rib, cut into 1/8-inch dice
â€¢ 1 (15- to 16-ounces) can small white beans, rinsed and drained
â€¢ 1 1/2 cups chicken stock or low-sodium broth
â€¢ 2 tablespoons unsalted butter
â€¢ 2 tablespoons finely chopped fresh flat-leaf parsley
â€¢ 1 tablespoon finely chopped fresh chervil (optional)
â€¢ Garnish: fresh chervil sprigsğŸ“â–ªï¸ Cook haricots verts in a large pot of boiling salted water until just tender, 3 to 4 minutes. Transfer with a slotted spoon to a bowl of ice and cold water, then drain. Add edamame to boiling water and cook 4 minutes. Drain in a colander, then rinse under cold water. If using edamame in pods, shell them and discard pods. Cook onion, garlic, bay leaf, rosemary, salt, and pepper in oil in a 2- to 4-quart heavy saucepan over moderately low heat, stirring, until softened, about 3 minutes. Add carrot and celery and cook, stirring, until softened, about 3 minutes. Add white beans and stock and simmer, covered, stirring occasionally, 10 minutes. Add haricots verts and edamame and simmer, uncovered, until heated through, 2 to 3 minutes. Add butter, parsley, and chervil (if using) and stir gently until butter is melted. Discard bay leaf and rosemary sprigs.
â–ªï¸ Cook haricots verts in a large pot of boiling salted water until just tender, 3 to 4 minutes. Transfer with a slotted spoon to a bowl of ice and cold water, then drain.
â–ªï¸ Add edamame to boiling water and cook 4 minutes. Drain in a colander, then rinse under cold water. If using edamame in pods, shell them and discard pods.
â–ªï¸ Cook onion, garlic, bay leaf, rosemary, salt, and pepper in oil in a 2- to 4-quart heavy saucepan over moderately low heat, stirring, until softened, about 3 minutes. Add carrot and celery and cook, stirring, until softened, about 3 minutes.
â–ªï¸ Add white beans and stock and simmer, covered, stirring occasionally, 10 minutes. Add haricots verts and edamame and simmer, uncovered, until heated through, 2 to 3 minutes. Add butter, parsley, and chervil (if using) and stir gently until butter is melted. Discard bay leaf and rosemary sprigs.*
```

# *è¿‡æ»¤æ‰å¤§é‡é£Ÿè°±*

*é£Ÿè°±é•¿çŸ­ä¸ä¸€ã€‚åœ¨å°†é…æ–¹åºåˆ—è¾“å…¥ RNN ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ª*ç¡¬ç¼–ç åºåˆ—é•¿åº¦*é™åˆ¶ã€‚æˆ‘ä»¬éœ€è¦æ‰¾å‡ºä»€ä¹ˆæ ·çš„é…æ–¹é•¿åº¦å¯ä»¥è¦†ç›–å¤§å¤šæ•°çš„é…æ–¹ç”¨ä¾‹ï¼ŒåŒæ—¶æˆ‘ä»¬å¸Œæœ›å®ƒå°½å¯èƒ½çš„çŸ­ï¼Œä»¥åŠ å¿«è®­ç»ƒè¿‡ç¨‹ã€‚*

```
*recipes_lengths = []
**for** recipe_text **in** dataset_stringified:
    recipes_lengths.append(len(recipe_text))plt.hist(recipes_lengths, bins=50)
plt.show()*
```

**â”è¾“å‡º:**

*![](img/2a79c60926a6dd3c33e0e7d4b437da55.png)*

*é…æ–¹é•¿åº¦(ä»£ç ç”Ÿæˆçš„å›¾åƒ)*

*å¤§å¤šæ•°é£Ÿè°±çš„é•¿åº¦å°äº`5000`ä¸ªå­—ç¬¦ã€‚è®©æˆ‘ä»¬æ”¾å¤§æ¥çœ‹æ›´è¯¦ç»†çš„å›¾ç‰‡:*

```
*plt.hist(recipes_lengths, range=(0, 8000), bins=50)
plt.show()*
```

**â”è¾“å‡º:**

*![](img/d08d98be936458b9d9e2c08b7923df1f.png)*

*é…æ–¹é•¿åº¦(ä»£ç ç”Ÿæˆçš„å›¾åƒ)*

*çœ‹èµ·æ¥é£Ÿè°±çš„å­—ç¬¦é™åˆ¶å°†è¦†ç›–å¤§å¤šæ•°æƒ…å†µã€‚æˆ‘ä»¬å¯ä»¥è¯•ç€ç”¨è¿™ä¸ªæœ€å¤§é£Ÿè°±é•¿åº¦é™åˆ¶æ¥è®­ç»ƒ RNNã€‚*

```
*MAX_RECIPE_LENGTH = 2000*
```

*å› æ­¤ï¼Œæˆ‘ä»¬æ¥è¿‡æ»¤æ‰æ‰€æœ‰é•¿äº`MAX_RECIPE_LENGTH`çš„èœè°±:*

```
***def** **filter_recipes_by_length**(recipe_test):
    **return** len(recipe_test) <= MAX_RECIPE_LENGTH dataset_filtered = [recipe_text **for** recipe_text **in** dataset_stringified **if** filter_recipes_by_length(recipe_text)]print('Dataset size BEFORE filtering: ', len(dataset_stringified))
print('Dataset size AFTER filtering: ', len(dataset_filtered))
print('Number of eliminated recipes: ', len(dataset_stringified) - len(dataset_filtered))*
```

**â”è¾“å‡º:**

```
*Dataset size BEFORE filtering:  122938
Dataset size AFTER filtering:  100212
Number of eliminated recipes:  22726*
```

*åœ¨è¿‡æ»¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†`22726`é…æ–¹ï¼Œä½†ç°åœ¨é…æ–¹çš„æ•°æ®æ›´åŠ å¯†é›†ã€‚*

# *æ±‡æ€»æ•°æ®é›†å‚æ•°*

```
*TOTAL_RECIPES_NUM = len(dataset_filtered)print('MAX_RECIPE_LENGTH: ', MAX_RECIPE_LENGTH)
print('TOTAL_RECIPES_NUM: ', TOTAL_RECIPES_NUM)*
```

**â”è¾“å‡º:**

```
*MAX_RECIPE_LENGTH:  2000
TOTAL_RECIPES_NUM:  100212*
```

*æœ€åï¼Œæˆ‘ä»¬ä»¥`~100k`é£Ÿè°±å‘Šç»ˆã€‚æ¯ä¸ªé…æ–¹éƒ½æœ‰`2000`å­—ç¬¦é•¿åº¦ã€‚*

# *åˆ›é€ è¯æ±‡*

*é€’å½’ç¥ç»ç½‘ç»œä¸ç†è§£å­—ç¬¦æˆ–å•è¯ã€‚ç›¸åï¼Œå®ƒç†è§£æ•°å­—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†é£Ÿè°±æ–‡æœ¬è½¬æ¢æˆæ•°å­—ã€‚*

*åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºäºå¤šå±‚ LSTM(é•¿çŸ­æœŸè®°å¿†)ç½‘ç»œçš„**å­—ç¬¦çº§**è¯­è¨€æ¨¡å‹(ä¸**å•è¯çº§**è¯­è¨€æ¨¡å‹ç›¸å¯¹)ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†ä¸ºå­—ç¬¦åˆ›å»ºå”¯ä¸€çš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯ä¸ºå•è¯åˆ›å»ºå”¯ä¸€çš„ç´¢å¼•ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬è®©ç½‘ç»œé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª*å­—ç¬¦*ï¼Œè€Œä¸æ˜¯ä¸‹ä¸€ä¸ª*å•è¯*ã€‚*

*â„¹ï¸ä½ å¯ä»¥åœ¨ Andrej Karpathy çš„æ–‡ç« [ä¸­æ‰¾åˆ°æ›´å¤šå…³äºå­—ç¬¦çº§ RNNs çš„è§£é‡Š:](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*

*ä¸ºäº†ä»é£Ÿè°±æ–‡æœ¬ä¸­åˆ›å»ºè¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[TF . keras . preprocessing . text . tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)ã€‚*

*æˆ‘ä»¬è¿˜éœ€è¦æ¥ä¸€äº›ç‹¬ç‰¹çš„å­—ç¬¦ï¼Œå°†è¢«è§†ä¸ºä¸€ä¸ª*åœæ­¢å­—ç¬¦*ï¼Œå°†è¡¨æ˜ä¸€ä¸ªé£Ÿè°±çš„ç»“æŸã€‚æˆ‘ä»¬éœ€è¦å®ƒæ¥ç”Ÿæˆèœè°±ï¼Œå› ä¸ºå¦‚æœæ²¡æœ‰è¿™ä¸ªåœæ­¢å­—ç¬¦ï¼Œæˆ‘ä»¬å°±ä¸çŸ¥é“æ­£åœ¨ç”Ÿæˆçš„èœè°±çš„ç»“å°¾åœ¨å“ªé‡Œã€‚*

```
*STOP_SIGN = 'â£'tokenizer = tf.keras.preprocessing.text.Tokenizer(
    char_level=**True**,
    filters='',
    lower=**False**,
    split=''
)*# Stop word is not a part of recipes, but tokenizer must know about it as well.*
tokenizer.fit_on_texts([STOP_SIGN])tokenizer.fit_on_texts(dataset_filtered)tokenizer.get_config()*
```

**â”è¾“å‡º:**

```
*{'num_words': None,
 'filters': '',
 'lower': False,
 'split': '',
 'char_level': True,
 'oov_token': None,
 'document_count': 100213, 'word_counts': '{"\\u2423": 1, "\\ud83d\\udcd7": 100212, " ": 17527888, "S": 270259, "l": 3815150, "o": 5987496, "w": 964459, "C": 222831, "k": 890982, "e": 9296022, "r": 4760887, "h": 2922100, "i": 4911812, "c": 2883507, "n": 5304396, "a": 6067157, "d": 3099679, "D": 63999, "u": 2717050, "m": 1794411, "p": 2679164, "g": 1698670, "s": 4704222, "\\n": 1955281, "\\ud83e\\udd55": 100212, "\\u2022": 922813, "4": 232607, ",": 1130487, "b": 1394803, "t": 5997722, "v": 746785, "2": 493933, "(": 144985, "1": 853931, "0": 145119, ".": 1052548, "7": 31098, "5": 154071, ")": 144977, "f": 1042981, "y": 666553, "\\ud83d\\udcdd": 100212, "\\u25aa": 331058, "\\ufe0e": 331058, "P": 200597, "6": 51398, "H": 43936, "A": 134274, "3": 213519, "R": 101253, "x": 201286, "/": 345257, "I": 81591, "L": 46138, "8": 55352, "9": 17697, "B": 123813, "M": 78684, "F": 104359, "j": 110008, "-": 219160, "W": 61616, "\\u00ae": 10159, "N": 12808, "q": 69654, "T": 101371, ";": 72045, "\'": 26831, "Z": 2428, "z": 115883, "G": 52043, ":": 31318, "E": 18582, "K": 18421, "X": 385, "\\"": 6445, "O": 28971, "Y": 6064, "\\u2122": 538, "Q": 3904, "J": 10269, "!": 3014, "U": 14132, "V": 12172, "&": 1039, "+": 87, "=": 113, "%": 993, "*": 3243, "\\u00a9": 99, "[": 30, "]": 31, "\\u00e9": 6727, "<": 76, ">": 86, "\\u00bd": 166, "#": 168, "\\u00f1": 891, "?": 327, "\\u2019": 111, "\\u00b0": 6808, "\\u201d": 6, "$": 84, "@": 5, "{": 8, "}": 9, "\\u2013": 1228, "\\u0096": 7, "\\u00e0": 26, "\\u00e2": 106, "\\u00e8": 846, "\\u00e1": 74, "\\u2014": 215, "\\u2044": 16, "\\u00ee": 415, "\\u00e7": 171, "_": 26, "\\u00fa": 48, "\\u00ef": 43, "\\u201a": 20, "\\u00fb": 36, "\\u00f3": 74, "\\u00ed": 130, "\\u25ca": 4, "\\u00f9": 12, "\\u00d7": 6, "\\u00ec": 8, "\\u00fc": 29, "\\u2031": 4, "\\u00ba": 19, "\\u201c": 4, "\\u00ad": 25, "\\u00ea": 27, "\\u00f6": 9, "\\u0301": 11, "\\u00f4": 8, "\\u00c1": 2, "\\u00be": 23, "\\u00bc": 95, "\\u00eb": 2, "\\u0097": 2, "\\u215b": 3, "\\u2027": 4, "\\u00e4": 15, "\\u001a": 2, "\\u00f8": 2, "\\ufffd": 20, "\\u02da": 6, "\\u00bf": 264, "\\u2153": 2, "|": 2, "\\u00e5": 3, "\\u00a4": 1, "\\u201f": 1, "\\u00a7": 5, "\\ufb02": 3, "\\u00a0": 1, "\\u01b0": 2, "\\u01a1": 1, "\\u0103": 1, "\\u0300": 1, "\\u00bb": 6, "`": 3, "\\u0092": 2, "\\u215e": 1, "\\u202d": 4, "\\u00b4": 2, "\\u2012": 2, "\\u00c9": 40, "\\u00da": 14, "\\u20ac": 1, "\\\\": 5, "~": 1, "\\u0095": 1, "\\u00c2": 2}', 'word_docs': '{"\\u2423": 1, "k": 97316, "0": 61954, "o": 100205, "r": 100207, "d": 100194, "u": 100161, "S": 89250, "\\u25aa": 100212, "D": 40870, "1": 99320, "g": 99975, "n": 100198, "b": 99702, "t": 100202, ".": 100163, " ": 100212, "7": 24377, "3": 79135, "\\ud83d\\udcd7": 100212, "i": 100207, "5": 65486, "f": 98331, "c": 100190, "4": 82453, "a": 100205, "2": 96743, "v": 97848, "C": 83328, "s": 100204, "\\n": 100212, "6": 35206, "\\ud83d\\udcdd": 100212, ",": 98524, "\\ufe0e": 100212, "l": 100206, "e": 100212, "y": 96387, ")": 67614, "p": 100046, "H": 31908, "\\ud83e\\udd55": 100212, "m": 99988, "w": 99227, "(": 67627, "A": 60900, "h": 100161, "\\u2022": 100212, "P": 79364, "R": 54040, "9": 14114, "8": 37000, "L": 32101, "x": 72133, "I": 46675, "/": 89051, "j": 47438, "F": 57940, "B": 64278, "M": 48332, "-": 74711, "T": 53758, "\\u00ae": 5819, "N": 9981, "W": 38981, "q": 36538, ";": 33863, "G": 35355, "\'": 18120, "z": 42430, "Z": 2184, ":": 18214, "E": 12161, "K": 14834, "X": 321, "\\"": 2617, "O": 20103, "Y": 5148, "\\u2122": 448, "Q": 3142, "J": 8225, "!": 2428, "U": 10621, "V": 9710, "&": 749, "+": 32, "=": 48, "%": 717, "*": 1780, "\\u00a9": 91, "]": 26, "[": 25, "\\u00e9": 2462, ">": 33, "<": 27, "\\u00bd": 81, "#": 139, "\\u00f1": 423, "?": 207, "\\u2019": 64, "\\u00b0": 3062, "\\u201d": 3, "@": 4, "$": 49, "{": 7, "}": 8, "\\u2013": 491, "\\u0096": 7, "\\u00e0": 22, "\\u00e2": 45, "\\u00e8": 335, "\\u00e1": 38, "\\u2014": 95, "\\u2044": 9, "\\u00ee": 122, "\\u00e7": 120, "_": 8, "\\u00fa": 25, "\\u00ef": 24, "\\u201a": 10, "\\u00fb": 29, "\\u00f3": 40, "\\u00ed": 52, "\\u25ca": 2, "\\u00f9": 6, "\\u00d7": 4, "\\u00ec": 4, "\\u00fc": 19, "\\u2031": 2, "\\u00ba": 9, "\\u201c": 2, "\\u00ad": 11, "\\u00ea": 4, "\\u00f6": 4, "\\u0301": 6, "\\u00f4": 5, "\\u00c1": 2, "\\u00be": 18, "\\u00bc": 55, "\\u00eb": 2, "\\u0097": 1, "\\u215b": 2, "\\u2027": 3, "\\u00e4": 8, "\\u001a": 1, "\\u00f8": 1, "\\ufffd": 4, "\\u02da": 3, "\\u00bf": 191, "\\u2153": 1, "|": 2, "\\u00e5": 1, "\\u00a4": 1, "\\u201f": 1, "\\u00a7": 3, "\\ufb02": 1, "\\u0300": 1, "\\u01a1": 1, "\\u00a0": 1, "\\u01b0": 1, "\\u0103": 1, "\\u00bb": 2, "`": 3, "\\u0092": 2, "\\u215e": 1, "\\u202d": 1, "\\u00b4": 1, "\\u2012": 1, "\\u00c9": 15, "\\u00da": 5, "\\u20ac": 1, "\\\\": 5, "~": 1, "\\u0095": 1, "\\u00c2": 1}', 'index_docs': '{"1": 100212, "165": 1, "25": 97316, "41": 61954, "5": 100205, "8": 100207, "11": 100194, "14": 100161, "33": 89250, "31": 100212, "58": 40870, "26": 99320, "18": 99975, "6": 100198, "19": 99702, "4": 100202, "21": 100163, "66": 24377, "37": 79135, "51": 100212, "7": 100207, "40": 65486, "22": 98331, "13": 100190, "34": 82453, "3": 100205, "29": 96743, "27": 97848, "35": 83328, "9": 100204, "16": 100212, "62": 35206, "53": 100212, "20": 98524, "32": 100212, "10": 100206, "2": 100212, "28": 96387, "43": 67614, "15": 100046, "64": 31908, "52": 100212, "17": 99988, "23": 99227, "42": 67627, "44": 60900, "12": 100161, "24": 100212, "39": 79364, "50": 54040, "71": 14114, "60": 37000, "63": 32101, "38": 72133, "54": 46675, "30": 89051, "47": 47438, "48": 57940, "45": 64278, "55": 48332, "36": 74711, "49": 53758, "76": 5819, "73": 9981, "59": 38981, "57": 36538, "56": 33863, "61": 35355, "68": 18120, "46": 42430, "84": 2184, "65": 18214, "69": 12161, "70": 14834, "92": 321, "79": 2617, "67": 20103, "80": 5148, "90": 448, "81": 3142, "75": 8225, "83": 2428, "72": 10621, "74": 9710, "86": 749, "105": 32, "100": 48, "87": 717, "82": 1780, "103": 91, "115": 26, "116": 25, "78": 2462, "106": 33, "108": 27, "98": 81, "97": 139, "88": 423, "93": 207, "101": 64, "77": 3062, "137": 3, "141": 4, "107": 49, "133": 7, "131": 8, "85": 491, "136": 7, "119": 22, "102": 45, "89": 335, "109": 38, "95": 95, "126": 9, "91": 122, "96": 120, "120": 8, "111": 25, "112": 24, "123": 10, "114": 29, "110": 40, "99": 52, "144": 2, "129": 6, "138": 4, "134": 4, "117": 19, "145": 2, "125": 9, "146": 2, "121": 11, "118": 4, "132": 4, "130": 6, "135": 5, "153": 2, "122": 18, "104": 55, "154": 2, "155": 1, "149": 2, "147": 3, "127": 8, "156": 1, "157": 1, "124": 4, "139": 3, "94": 191, "158": 1, "159": 2, "150": 1, "166": 1, "167": 1, "142": 3, "151": 1, "171": 1, "169": 1, "168": 1, "160": 1, "170": 1, "140": 2, "152": 3, "161": 2, "172": 1, "148": 1, "162": 1, "163": 1, "113": 15, "128": 5, "173": 1, "143": 5, "174": 1, "175": 1, "164": 1}', 'index_word': '{"1": " ", "2": "e", "3": "a", "4": "t", "5": "o", "6": "n", "7": "i", "8": "r", "9": "s", "10": "l", "11": "d", "12": "h", "13": "c", "14": "u", "15": "p", "16": "\\n", "17": "m", "18": "g", "19": "b", "20": ",", "21": ".", "22": "f", "23": "w", "24": "\\u2022", "25": "k", "26": "1", "27": "v", "28": "y", "29": "2", "30": "/", "31": "\\u25aa", "32": "\\ufe0e", "33": "S", "34": "4", "35": "C", "36": "-", "37": "3", "38": "x", "39": "P", "40": "5", "41": "0", "42": "(", "43": ")", "44": "A", "45": "B", "46": "z", "47": "j", "48": "F", "49": "T", "50": "R", "51": "\\ud83d\\udcd7", "52": "\\ud83e\\udd55", "53": "\\ud83d\\udcdd", "54": "I", "55": "M", "56": ";", "57": "q", "58": "D", "59": "W", "60": "8", "61": "G", "62": "6", "63": "L", "64": "H", "65": ":", "66": "7", "67": "O", "68": "\'", "69": "E", "70": "K", "71": "9", "72": "U", "73": "N", "74": "V", "75": "J", "76": "\\u00ae", "77": "\\u00b0", "78": "\\u00e9", "79": "\\"", "80": "Y", "81": "Q", "82": "*", "83": "!", "84": "Z", "85": "\\u2013", "86": "&", "87": "%", "88": "\\u00f1", "89": "\\u00e8", "90": "\\u2122", "91": "\\u00ee", "92": "X", "93": "?", "94": "\\u00bf", "95": "\\u2014", "96": "\\u00e7", "97": "#", "98": "\\u00bd", "99": "\\u00ed", "100": "=", "101": "\\u2019", "102": "\\u00e2", "103": "\\u00a9", "104": "\\u00bc", "105": "+", "106": ">", "107": "$", "108": "<", "109": "\\u00e1", "110": "\\u00f3", "111": "\\u00fa", "112": "\\u00ef", "113": "\\u00c9", "114": "\\u00fb", "115": "]", "116": "[", "117": "\\u00fc", "118": "\\u00ea", "119": "\\u00e0", "120": "_", "121": "\\u00ad", "122": "\\u00be", "123": "\\u201a", "124": "\\ufffd", "125": "\\u00ba", "126": "\\u2044", "127": "\\u00e4", "128": "\\u00da", "129": "\\u00f9", "130": "\\u0301", "131": "}", "132": "\\u00f6", "133": "{", "134": "\\u00ec", "135": "\\u00f4", "136": "\\u0096", "137": "\\u201d", "138": "\\u00d7", "139": "\\u02da", "140": "\\u00bb", "141": "@", "142": "\\u00a7", "143": "\\\\", "144": "\\u25ca", "145": "\\u2031", "146": "\\u201c", "147": "\\u2027", "148": "\\u202d", "149": "\\u215b", "150": "\\u00e5", "151": "\\ufb02", "152": "`", "153": "\\u00c1", "154": "\\u00eb", "155": "\\u0097", "156": "\\u001a", "157": "\\u00f8", "158": "\\u2153", "159": "|", "160": "\\u01b0", "161": "\\u0092", "162": "\\u00b4", "163": "\\u2012", "164": "\\u00c2", "165": "\\u2423", "166": "\\u00a4", "167": "\\u201f", "168": "\\u00a0", "169": "\\u01a1", "170": "\\u0103", "171": "\\u0300", "172": "\\u215e", "173": "\\u20ac", "174": "~", "175": "\\u0095"}', 'word_index': '{" ": 1, "e": 2, "a": 3, "t": 4, "o": 5, "n": 6, "i": 7, "r": 8, "s": 9, "l": 10, "d": 11, "h": 12, "c": 13, "u": 14, "p": 15, "\\n": 16, "m": 17, "g": 18, "b": 19, ",": 20, ".": 21, "f": 22, "w": 23, "\\u2022": 24, "k": 25, "1": 26, "v": 27, "y": 28, "2": 29, "/": 30, "\\u25aa": 31, "\\ufe0e": 32, "S": 33, "4": 34, "C": 35, "-": 36, "3": 37, "x": 38, "P": 39, "5": 40, "0": 41, "(": 42, ")": 43, "A": 44, "B": 45, "z": 46, "j": 47, "F": 48, "T": 49, "R": 50, "\\ud83d\\udcd7": 51, "\\ud83e\\udd55": 52, "\\ud83d\\udcdd": 53, "I": 54, "M": 55, ";": 56, "q": 57, "D": 58, "W": 59, "8": 60, "G": 61, "6": 62, "L": 63, "H": 64, ":": 65, "7": 66, "O": 67, "\'": 68, "E": 69, "K": 70, "9": 71, "U": 72, "N": 73, "V": 74, "J": 75, "\\u00ae": 76, "\\u00b0": 77, "\\u00e9": 78, "\\"": 79, "Y": 80, "Q": 81, "*": 82, "!": 83, "Z": 84, "\\u2013": 85, "&": 86, "%": 87, "\\u00f1": 88, "\\u00e8": 89, "\\u2122": 90, "\\u00ee": 91, "X": 92, "?": 93, "\\u00bf": 94, "\\u2014": 95, "\\u00e7": 96, "#": 97, "\\u00bd": 98, "\\u00ed": 99, "=": 100, "\\u2019": 101, "\\u00e2": 102, "\\u00a9": 103, "\\u00bc": 104, "+": 105, ">": 106, "$": 107, "<": 108, "\\u00e1": 109, "\\u00f3": 110, "\\u00fa": 111, "\\u00ef": 112, "\\u00c9": 113, "\\u00fb": 114, "]": 115, "[": 116, "\\u00fc": 117, "\\u00ea": 118, "\\u00e0": 119, "_": 120, "\\u00ad": 121, "\\u00be": 122, "\\u201a": 123, "\\ufffd": 124, "\\u00ba": 125, "\\u2044": 126, "\\u00e4": 127, "\\u00da": 128, "\\u00f9": 129, "\\u0301": 130, "}": 131, "\\u00f6": 132, "{": 133, "\\u00ec": 134, "\\u00f4": 135, "\\u0096": 136, "\\u201d": 137, "\\u00d7": 138, "\\u02da": 139, "\\u00bb": 140, "@": 141, "\\u00a7": 142, "\\\\": 143, "\\u25ca": 144, "\\u2031": 145, "\\u201c": 146, "\\u2027": 147, "\\u202d": 148, "\\u215b": 149, "\\u00e5": 150, "\\ufb02": 151, "`": 152, "\\u00c1": 153, "\\u00eb": 154, "\\u0097": 155, "\\u001a": 156, "\\u00f8": 157, "\\u2153": 158, "|": 159, "\\u01b0": 160, "\\u0092": 161, "\\u00b4": 162, "\\u2012": 163, "\\u00c2": 164, "\\u2423": 165, "\\u00a4": 166, "\\u201f": 167, "\\u00a0": 168, "\\u01a1": 169, "\\u0103": 170, "\\u0300": 171, "\\u215e": 172, "\\u20ac": 173, "~": 174, "\\u0095": 175}'}*
```

*ä¸ºäº†å¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬éœ€è¦å°†`+1`åŠ åˆ°å·²ç»æ³¨å†Œçš„å­—ç¬¦æ•°ä¸Šï¼Œå› ä¸º[ç´¢å¼•](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) `[0](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)` [æ˜¯ä¸€ä¸ªä¿ç•™ç´¢å¼•ï¼Œä¸ä¼šåˆ†é…ç»™ä»»ä½•å•è¯](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)ã€‚*

```
*VOCABULARY_SIZE = len(tokenizer.word_counts) + 1print('VOCABULARY_SIZE: ', VOCABULARY_SIZE)*
```

**â”è¾“å‡º:**

```
*VOCABULARY_SIZE:  176*
```

*è®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹è®°å·åŒ–å™¨å­—å…¸ï¼Œçœ‹çœ‹å¦‚ä½•å°†å­—ç¬¦è½¬æ¢æˆç´¢å¼•ï¼Œåä¹‹äº¦ç„¶:*

```
*print(tokenizer.index_word[5])
print(tokenizer.index_word[20])*
```

**â”è¾“å‡º:**

```
*o
,*
```

*è®©æˆ‘ä»¬å°è¯•å°†å­—ç¬¦è½¬æ¢ä¸ºç´¢å¼•:*

```
*tokenizer.word_index['r']*
```

**â”è¾“å‡º:**

```
*8*
```

*ä¸ºäº†è¯´æ˜å“ªç§å­—ç¬¦æ„æˆäº†æˆ‘ä»¬æ•°æ®é›†ä¸­çš„æ‰€æœ‰é£Ÿè°±ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬æ‰“å°æˆä¸€ä¸ªæ•°ç»„:*

```
*array_vocabulary = tokenizer.sequences_to_texts([[word_index] **for** word_index **in** range(VOCABULARY_SIZE)])
print([char **for** char **in** array_vocabulary])*
```

**â”è¾“å‡º:**

```
*['', ' ', 'e', 'a', 't', 'o', 'n', 'i', 'r', 's', 'l', 'd', 'h', 'c', 'u', 'p', '\n', 'm', 'g', 'b', ',', '.', 'f', 'w', 'â€¢', 'k', '1', 'v', 'y', '2', '/', 'â–ª', 'ï¸', 'S', '4', 'C', '-', '3', 'x', 'P', '5', '0', '(', ')', 'A', 'B', 'z', 'j', 'F', 'T', 'R', 'ğŸ“—', 'ğŸ¥•', 'ğŸ“', 'I', 'M', ';', 'q', 'D', 'W', '8', 'G', '6', 'L', 'H', ':', '7', 'O', "'", 'E', 'K', '9', 'U', 'N', 'V', 'J', 'Â®', 'Â°', 'Ã©', '"', 'Y', 'Q', '*', '!', 'Z', 'â€“', '&', '%', 'Ã±', 'Ã¨', 'â„¢', 'Ã®', 'X', '?', 'Â¿', 'â€”', 'Ã§', '#', 'Â½', 'Ã­', '=', 'â€™', 'Ã¢', 'Â©', 'Â¼', '+', '>', '$', '<', 'Ã¡', 'Ã³', 'Ãº', 'Ã¯', 'Ã‰', 'Ã»', ']', '[', 'Ã¼', 'Ãª', 'Ã ', '_', '\xad', 'Â¾', 'â€š', 'ï¿½', 'Âº', 'â„', 'Ã¤', 'Ãš', 'Ã¹', 'Ì', '}', 'Ã¶', '{', 'Ã¬', 'Ã´', '\x96', 'â€', 'Ã—', 'Ëš', 'Â»', '@', 'Â§', '\\', 'â—Š', 'â€±', 'â€œ', 'â€§', '\u202d', 'â…›', 'Ã¥', 'ï¬‚', '`', 'Ã', 'Ã«', '\x97', '\x1a', 'Ã¸', 'â…“', '|', 'Æ°', '\x92', 'Â´', 'â€’', 'Ã‚', 'â£', 'Â¤', 'â€Ÿ', '\xa0', 'Æ¡', 'Äƒ', 'Ì€', 'â…', 'â‚¬', '~', '\x95']*
```

*è¿™äº›éƒ½æ˜¯æˆ‘ä»¬çš„ RNN æ¨¡å‹å°†è¦å¤„ç†çš„è§’è‰²ã€‚å®ƒå°†å°è¯•å­¦ä¹ å¦‚ä½•å°†è¿™äº›å­—ç¬¦ç»„åˆæˆçœ‹èµ·æ¥åƒé£Ÿè°±çš„åºåˆ—ã€‚*

*è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨`tokenizer`å‡½æ•°å°†æ–‡æœ¬è½¬æ¢æˆç´¢å¼•:*

```
*tokenizer.texts_to_sequences(['ğŸ“— yes'])*
```

**â”è¾“å‡º:**

```
*[[51, 1, 28, 2, 9]]*
```

# *å¯¹æ•°æ®é›†è¿›è¡ŒçŸ¢é‡åŒ–*

*ç°åœ¨ï¼Œä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¯æ±‡è¡¨(`character --code`å’Œ`code --character`å…³ç³»)ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†é£Ÿè°±ä»æ–‡æœ¬è½¬æ¢æˆæ•°å­—(RNN å°†æ•°å­—ä½œä¸ºè¾“å…¥ï¼Œè€Œä¸æ˜¯æ–‡æœ¬)ã€‚*

```
*dataset_vectorized = tokenizer.texts_to_sequences(dataset_filtered)print('Vectorized dataset size', len(dataset_vectorized))*
```

**â”è¾“å‡º:**

```
*Vectorized dataset size 100212*
```

*è¿™æ˜¯ç¬¬ä¸€ä¸ªçŸ¢é‡åŒ–é£Ÿè°±çš„å¼€å¤´:*

```
*print(dataset_vectorized[0][:10], '...')*
```

**â”è¾“å‡º:**

```
*[51, 1, 33, 10, 5, 23, 1, 35, 5, 5] ...*
```

*è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†çŸ¢é‡åŒ–çš„é£Ÿè°±è½¬æ¢å›æ–‡æœ¬è¡¨ç¤º:*

```
***def** **recipe_sequence_to_string**(recipe_sequence):
    recipe_stringified = tokenizer.sequences_to_texts([recipe_sequence])[0]
    print(recipe_stringified)recipe_sequence_to_string(dataset_vectorized[0])*
```

**â”è¾“å‡º:**

```
*ğŸ“— Slow Cooker Chicken and DumplingsğŸ¥•â€¢ 4 skinless, boneless chicken breast halves 
â€¢ 2 tablespoons butter 
â€¢ 2 (10.75 ounce) cans condensed cream of chicken soup 
â€¢ 1 onion, finely diced 
â€¢ 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ğŸ“â–ªï¸ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.
â–ªï¸ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.*
```

# *ç»™åºåˆ—æ·»åŠ å¡«å……*

*æˆ‘ä»¬éœ€è¦æ‰€æœ‰çš„é£Ÿè°±éƒ½æœ‰ç›¸åŒçš„è®­ç»ƒé•¿åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[TF . keras . preprocessing . sequence . pad _ sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)å®ç”¨ç¨‹åºåœ¨æ¯ä¸ªé…æ–¹çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªåœæ­¢å­—ï¼Œå¹¶ä½¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚*

*è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹é£Ÿè°±çš„é•¿åº¦:*

```
***for** recipe_index, recipe **in** enumerate(dataset_vectorized[:10]):
    print('Recipe #{} length: {}'.format(recipe_index + 1, len(recipe)))*
```

**â”è¾“å‡º:**

```
*Recipe #1 length: 546
Recipe #2 length: 401
Recipe #3 length: 671
Recipe #4 length: 736
Recipe #5 length: 1518
Recipe #6 length: 740
Recipe #7 length: 839
Recipe #8 length: 667
Recipe #9 length: 1264
Recipe #10 length: 854*
```

*è®©æˆ‘ä»¬æŠŠæ‰€æœ‰çš„é£Ÿè°±éƒ½åŠ ä¸Šä¸€ä¸ª`STOP_SIGN`:*

```
*dataset_vectorized_padded_without_stops = tf.keras.preprocessing.sequence.pad_sequences(
    dataset_vectorized,
    padding='post',
    truncating='post',
    *# We use -1 here and +1 in the next step to make sure*
    *# that all recipes will have at least 1 stops sign at the end,*
    *# since each sequence will be shifted and truncated afterwards*
    *# (to generate X and Y sequences).*
    maxlen=MAX_RECIPE_LENGTH-1,
    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]
)dataset_vectorized_padded = tf.keras.preprocessing.sequence.pad_sequences(
    dataset_vectorized_padded_without_stops,
    padding='post',
    truncating='post',
    maxlen=MAX_RECIPE_LENGTH+1,
    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]
)**for** recipe_index, recipe **in** enumerate(dataset_vectorized_padded[:10]):
    print('Recipe #{} length: {}'.format(recipe_index, len(recipe)))*
```

**â”è¾“å‡º:**

```
*Recipe #0 length: 2001
Recipe #1 length: 2001
Recipe #2 length: 2001
Recipe #3 length: 2001
Recipe #4 length: 2001
Recipe #5 length: 2001
Recipe #6 length: 2001
Recipe #7 length: 2001
Recipe #8 length: 2001
Recipe #9 length: 2001*
```

*å¡«å……åï¼Œæ•°æ®é›†ä¸­çš„æ‰€æœ‰é£Ÿè°±ç°åœ¨éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼ŒRNN ä¹Ÿå°†èƒ½å¤ŸçŸ¥é“æ¯ä¸ªé£Ÿè°±åœ¨å“ªé‡Œåœæ­¢(é€šè¿‡è§‚å¯Ÿ`STOP_SIGN`çš„å‡ºç°)ã€‚*

*ä»¥ä¸‹æ˜¯å¡«å……åç¬¬ä¸€ä¸ªé…æ–¹çš„å¤–è§‚ç¤ºä¾‹ã€‚*

```
*recipe_sequence_to_string(dataset_vectorized_padded[0])*
```

**â”è¾“å‡º:**

```
*ğŸ“— Slow Cooker Chicken and DumplingsğŸ¥•â€¢ 4 skinless, boneless chicken breast halves 
â€¢ 2 tablespoons butter 
â€¢ 2 (10.75 ounce) cans condensed cream of chicken soup 
â€¢ 1 onion, finely diced 
â€¢ 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ğŸ“â–ªï¸ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.
â–ªï¸ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£*
```

*ç°åœ¨æ‰€æœ‰çš„é£Ÿè°±éƒ½ä»¥ä¸€ä¸ªæˆ–å¤šä¸ª`â£`ç¬¦å·ç»“å°¾ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ LSTM æ¨¡å‹çŸ¥é“ï¼Œæ¯å½“å®ƒçœ‹åˆ°`â£`åœæ­¢å­—ç¬¦æ—¶ï¼Œå®ƒå°±æ„å‘³ç€é£Ÿè°±ç»“æŸäº†ã€‚ä¸€æ—¦ç½‘ç»œå­¦ä¼šäº†è¿™ä¸ªæ¦‚å¿µï¼Œå®ƒå°±ä¼šåœ¨æ¯ä¸ªæ–°ç”Ÿæˆçš„èœè°±çš„æœ«å°¾åŠ ä¸Šåœæ­¢å­—ç¬¦ã€‚*

# *åˆ›å»ºå¼ é‡æµæ•°æ®é›†*

*åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ˜¯åƒå¤„ç† NumPy æ•°ç»„ä¸€æ ·å¤„ç†æ•°æ®é›†çš„ã€‚å¦‚æœæˆ‘ä»¬å°†æ•°æ®é›† NumPy æ•°ç»„è½¬æ¢æˆ [TensorFlow æ•°æ®é›†](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ›´æ–¹ä¾¿ã€‚å®ƒå°†èµ‹äºˆæˆ‘ä»¬ä½¿ç”¨`batch()`ã€`shuffle()`ã€`repeat()`ã€`prefecth()`ç­‰åŠ©æ‰‹åŠŸèƒ½çš„èƒ½åŠ›ã€‚ï¼š*

```
*dataset = tf.data.Dataset.from_tensor_slices(dataset_vectorized_padded)print(dataset)*
```

**â”è¾“å‡º:**

```
*<TensorSliceDataset shapes: (2001,), types: tf.int32>*
```

*è®©æˆ‘ä»¬è¿™æ¬¡é€šè¿‡ä½¿ç”¨ TensorFlow æ•°æ®é›† API æ¥çœ‹çœ‹æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªé…æ–¹æ˜¯ä»€ä¹ˆæ ·çš„:*

```
***for** recipe **in** dataset.take(1):
    print('Raw recipe:\n', recipe.numpy(), '\n\n\n')
    print('Stringified recipe:\n')
    recipe_sequence_to_string(recipe.numpy())*
```

**â”è¾“å‡º:**

```
*Raw recipe:
 [ 51   1  33 ... 165 165 165] Stringified recipe:ğŸ“— Slow Cooker Chicken and DumplingsğŸ¥•â€¢ 4 skinless, boneless chicken breast halves 
â€¢ 2 tablespoons butter 
â€¢ 2 (10.75 ounce) cans condensed cream of chicken soup 
â€¢ 1 onion, finely diced 
â€¢ 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ğŸ“â–ªï¸ Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.
â–ªï¸ Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£*
```

# *`input`å’Œ`target`æ–‡æœ¬ä¸Šçš„æ‹†åˆ†ç¤ºä¾‹*

*å¯¹äºæ¯ä¸ªåºåˆ—ï¼Œæˆ‘ä»¬éœ€è¦å¤åˆ¶å¹¶ç§»åŠ¨å®ƒï¼Œä»¥å½¢æˆ`input`å’Œ`target`æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œå‡è®¾`sequence_length`æ˜¯`4`ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬æ˜¯`Hello`ã€‚è¾“å…¥åºåˆ—æ˜¯`Hell`ï¼Œç›®æ ‡åºåˆ—æ˜¯`ello`ã€‚*

```
***def** **split_input_target**(recipe):
    input_text = recipe[:-1]
    target_text = recipe[1:]

    **return** input_text, target_textdataset_targeted = dataset.map(split_input_target)print(dataset_targeted)*
```

**â”è¾“å‡º:**

```
*<MapDataset shapes: ((2000,), (2000,)), types: (tf.int32, tf.int32)>*
```

*æ‚¨å¯èƒ½ä¼šä»ä¸Šé¢çš„è¡Œä¸­æ³¨æ„åˆ°ï¼Œç°åœ¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½ç”±ä¸¤ä¸ªå…ƒç»„ç»„æˆ:è¾“å…¥å’Œç›®æ ‡ã€‚è®©æˆ‘ä»¬æ‰“å°ä¸€ä¸ªä¾‹å­:*

```
***for** input_example, target_example **in** dataset_targeted.take(1):
    print('Input sequence size:', repr(len(input_example.numpy())))
    print('Target sequence size:', repr(len(target_example.numpy())))
    print()

    input_stringified = tokenizer.sequences_to_texts([input_example.numpy()[:50]])[0]
    target_stringified = tokenizer.sequences_to_texts([target_example.numpy()[:50]])[0]

    print('Input:  ', repr(''.join(input_stringified)))
    print('Target: ', repr(''.join(target_stringified)))*
```

**â”è¾“å‡º:**

```
*Input sequence size: 2000
Target sequence size: 2000Input:   'ğŸ“—   S l o w   C o o k e r   C h i c k e n   a n d   D u m p l i n g s \n \n ğŸ¥• \n \n â€¢   4   s k i n l e'
Target:  '  S l o w   C o o k e r   C h i c k e n   a n d   D u m p l i n g s \n \n ğŸ¥• \n \n â€¢   4   s k i n l e s'*
```

*RNN å°†è¿™äº›å‘é‡çš„æ¯ä¸ªç´¢å¼•ä½œä¸ºä¸€ä¸ªæ—¶é—´æ­¥é•¿è¿›è¡Œå¤„ç†ã€‚å¯¹äºæ—¶é—´æ­¥é•¿`0`çš„è¾“å…¥ï¼Œæ¨¡å‹æ¥æ”¶`ğŸ“—`çš„ç´¢å¼•ï¼Œå¹¶å°è¯•é¢„æµ‹`` `(ä¸€ä¸ªç©ºæ ¼å­—ç¬¦)çš„ç´¢å¼•ä½œä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒåšåŒæ ·çš„äº‹æƒ…ï¼Œä½†æ˜¯ RNN é™¤äº†è€ƒè™‘å½“å‰è¾“å…¥å­—ç¬¦å¤–ï¼Œè¿˜è€ƒè™‘å‰ä¸€æ­¥çš„ä¸Šä¸‹æ–‡ã€‚*

```
***for** i, (input_idx, target_idx) **in** enumerate(zip(input_example[:10], target_example[:10])):
    print('Step {:2d}'.format(i + 1))
    print('  input: {} ({:s})'.format(input_idx, repr(tokenizer.sequences_to_texts([[input_idx.numpy()]])[0])))
    print('  expected output: {} ({:s})'.format(target_idx, repr(tokenizer.sequences_to_texts([[target_idx.numpy()]])[0])))*
```

**â”è¾“å‡º:**

```
*Step  1
  input: 51 ('ğŸ“—')
  expected output: 1 (' ')
Step  2
  input: 1 (' ')
  expected output: 33 ('S')
Step  3
  input: 33 ('S')
  expected output: 10 ('l')
Step  4
  input: 10 ('l')
  expected output: 5 ('o')
Step  5
  input: 5 ('o')
  expected output: 23 ('w')
Step  6
  input: 23 ('w')
  expected output: 1 (' ')
Step  7
  input: 1 (' ')
  expected output: 35 ('C')
Step  8
  input: 35 ('C')
  expected output: 5 ('o')
Step  9
  input: 5 ('o')
  expected output: 5 ('o')
Step 10
  input: 5 ('o')
  expected output: 25 ('k')*
```

# *å°†æ•°æ®é›†åˆ†æˆå‡ æ‰¹*

*æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­æœ‰`~100k`ä¸ªé£Ÿè°±ï¼Œæ¯ä¸ªé£Ÿè°±æœ‰ä¸¤ä¸ªç”±`2000`å­—ç¬¦ç»„æˆçš„å…ƒç»„ã€‚*

```
*print(dataset_targeted)*
```

**â”è¾“å‡º:**

```
*<MapDataset shapes: ((2000,), (2000,)), types: (tf.int32, tf.int32)>*
```

*è®©æˆ‘ä»¬æ‰“å°å¸¸é‡å€¼:*

```
*print('TOTAL_RECIPES_NUM: ', TOTAL_RECIPES_NUM)
print('MAX_RECIPE_LENGTH: ', MAX_RECIPE_LENGTH)
print('VOCABULARY_SIZE: ', VOCABULARY_SIZE)*
```

**â”è¾“å‡º:**

```
*TOTAL_RECIPES_NUM:  100212
MAX_RECIPE_LENGTH:  2000
VOCABULARY_SIZE:  176*
```

*å¦‚æœæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å®Œæ•´çš„æ•°æ®é›†æä¾›ç»™æ¨¡å‹ï¼Œç„¶åå°è¯•ä¸€æ¬¡å¯¹æ‰€æœ‰ç¤ºä¾‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè€—å°½å†…å­˜ï¼Œå¹¶ä¸”æ¯ä¸ªè®­ç»ƒæ—¶æœŸå¯èƒ½éœ€è¦å¤ªé•¿æ—¶é—´æ¥æ‰§è¡Œã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®é›†åˆ†æˆå‡ æ‰¹ã€‚*

```
**# Batch size.*
BATCH_SIZE = 64*# Buffer size to shuffle the dataset (TF data is designed to work*
*# with possibly infinite sequences, so it doesn't attempt to shuffle*
*# the entire sequence in memory. Instead, it maintains a buffer in*
*# which it shuffles elements).*
SHUFFLE_BUFFER_SIZE = 1000dataset_train = dataset_targeted \
  *# Shuffling examples first.*
  .shuffle(SHUFFLE_BUFFER_SIZE) \
  *# Splitting examples on batches.*
  .batch(BATCH_SIZE, drop_remainder=**True**) \
  *# Making a dataset to be repeatable (it will never ends).* 
  .repeat()print(dataset_train)*
```

**â”è¾“å‡º:**

```
*<RepeatDataset shapes: ((64, 2000), (64, 2000)), types: (tf.int32, tf.int32)>*
```

*ä»ä¸Šé¢çš„è¡Œä¸­ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æˆ‘ä»¬çš„æ•°æ®é›†ç°åœ¨ç”±ç›¸åŒçš„ä¸¤ä¸ª`2000`å­—ç¬¦å…ƒç»„ç»„æˆï¼Œä½†æ˜¯ç°åœ¨å®ƒä»¬è¢«`64`åˆ†ç»„åˆ°æ‰¹å¤„ç†ä¸­ã€‚*

```
***for** input_text, target_text **in** dataset_train.take(1):
    print('1st batch: input_text:', input_text)
    print()
    print('1st batch: target_text:', target_text)*
```

**â”è¾“å‡º:**

```
*1st batch: input_text: tf.Tensor(
[[ 51   1  54 ... 165 165 165]
 [ 51   1  64 ... 165 165 165]
 [ 51   1  44 ... 165 165 165]
 ...
 [ 51   1  69 ... 165 165 165]
 [ 51   1  55 ... 165 165 165]
 [ 51   1  70 ... 165 165 165]], shape=(64, 2000), dtype=int32)1st batch: target_text: tf.Tensor(
[[  1  54   4 ... 165 165 165]
 [  1  64   5 ... 165 165 165]
 [  1  44   6 ... 165 165 165]
 ...
 [  1  69   3 ... 165 165 165]
 [  1  55   3 ... 165 165 165]
 [  1  70   2 ... 165 165 165]], shape=(64, 2000), dtype=int32)*
```

# *å»ºç«‹æ¨¡å‹*

*æˆ‘ä»¬å°†ä½¿ç”¨ [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) æ¥å®šä¹‰æ¨¡å‹ã€‚åœ¨æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å›¾å±‚ç±»å‹:*

*   *[TF . keras . layers . embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)â€”è¾“å…¥å±‚(ä¸€ä¸ªå¯è®­ç»ƒçš„æŸ¥æ‰¾è¡¨ï¼Œå°†æ¯ä¸ªå­—ç¬¦çš„æ•°å­—æ˜ å°„åˆ°ä¸€ä¸ªå…·æœ‰`embedding_dim`ç»´åº¦çš„å‘é‡)ï¼Œ*
*   *[tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) â€”ä¸€ç§å¤§å°ä¸º`units=rnn_units`çš„ RNN(è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) å›¾å±‚)ï¼Œ*
*   *[TF . keras . layers . dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)â€”è¾“å‡ºå±‚ï¼Œå¸¦æœ‰`VOCABULARY_SIZE`è¾“å‡ºã€‚*

# *å¼„æ¸…æ¥šåµŒå…¥å±‚æ˜¯å¦‚ä½•å·¥ä½œçš„*

*è®©æˆ‘ä»¬å¿«é€Ÿè¿‚å›ä¸€ä¸‹ï¼Œçœ‹çœ‹åµŒå…¥å±‚æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚å®ƒæ¥å—å‡ ä¸ªå­—ç¬¦ç´¢å¼•åºåˆ—(æ‰¹å¤„ç†)ä½œä¸ºè¾“å…¥ã€‚å®ƒå°†æ¯ä¸ªåºåˆ—çš„æ¯ä¸ªå­—ç¬¦ç¼–ç æˆä¸€ä¸ªé•¿åº¦ä¸º`tmp_embedding_size`çš„å‘é‡ã€‚*

```
*tmp_vocab_size = 10
tmp_embedding_size = 5
tmp_input_length = 8
tmp_batch_size = 2tmp_model = tf.keras.models.Sequential()
tmp_model.add(tf.keras.layers.Embedding(
  input_dim=tmp_vocab_size,
  output_dim=tmp_embedding_size,
  input_length=tmp_input_length
))
*# The model will take as input an integer matrix of size (batch, input_length).*
*# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).*
*# Now model.output_shape == (None, 10, 64), where None is the batch dimension.*
tmp_input_array = np.random.randint(
  low=0,
  high=tmp_vocab_size,
  size=(tmp_batch_size, tmp_input_length)
)
tmp_model.compile('rmsprop', 'mse')
tmp_output_array = tmp_model.predict(tmp_input_array)print('tmp_input_array shape:', tmp_input_array.shape)
print('tmp_input_array:')
print(tmp_input_array)
print()
print('tmp_output_array shape:', tmp_output_array.shape)
print('tmp_output_array:')
print(tmp_output_array)*
```

**â”è¾“å‡º:**

```
*tmp_input_array shape: (2, 8)
tmp_input_array:
[[2 4 7 5 1 6 9 7]
 [3 6 8 1 4 0 1 2]]tmp_output_array shape: (2, 8, 5)
tmp_output_array:
[[[-0.02229502 -0.02800617 -0.0120693  -0.01681594 -0.00650246]
  [-0.03046973 -0.03920818  0.04956308  0.04417323 -0.00446874]
  [-0.0215276   0.01532575 -0.02229529  0.02834387  0.02725342]
  [ 0.04567988  0.0141306   0.00877035 -0.02601192  0.00380837]
  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]
  [ 0.04598364 -0.00704358 -0.01386416  0.01195388 -0.00309662]
  [-0.00137572  0.01275543 -0.02348721 -0.04825885  0.00527108]
  [-0.0215276   0.01532575 -0.02229529  0.02834387  0.02725342]] [[ 0.01082945  0.03824175 -0.00450991 -0.02865709  0.02502238]
  [ 0.04598364 -0.00704358 -0.01386416  0.01195388 -0.00309662]
  [ 0.02275398  0.03806095 -0.03491788  0.04705564  0.00167596]
  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]
  [-0.03046973 -0.03920818  0.04956308  0.04417323 -0.00446874]
  [-0.02909902  0.04426369  0.00150937  0.04579213  0.02559013]
  [ 0.02969306  0.02994296 -0.00233263  0.00716375 -0.00847433]
  [-0.02229502 -0.02800617 -0.0120693  -0.01681594 -0.00650246]]]*
```

# *LSTM æ¨¡å‹*

*è®©æˆ‘ä»¬ç»„è£…æ¨¡å‹ã€‚*

*â„¹ï¸æ‚¨å¯ä»¥ä½¿ç”¨ TensorFlow æ–‡æ¡£ä¸­çš„ RNN ç¬”è®°æœ¬æ¥æ£€æŸ¥[æ–‡æœ¬ç”Ÿæˆï¼Œä»¥äº†è§£æœ‰å…³æ¨¡å‹ç»„ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚](https://www.tensorflow.org/tutorials/text/text_generation)*

```
***def** **build_model**(vocab_size, embedding_dim, rnn_units, batch_size):
    model = tf.keras.models.Sequential() model.add(tf.keras.layers.Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        batch_input_shape=[batch_size, **None**]
    )) model.add(tf.keras.layers.LSTM(
        units=rnn_units,
        return_sequences=**True**,
        stateful=**True**,
        recurrent_initializer=tf.keras.initializers.GlorotNormal()
    )) model.add(tf.keras.layers.Dense(vocab_size))

    **return** modelmodel = build_model(
  vocab_size=VOCABULARY_SIZE,
  embedding_dim=256,
  rnn_units=1024,
  batch_size=BATCH_SIZE
)model.summary()*
```

**â”è¾“å‡º:**

```
*Model: "sequential_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_13 (Embedding)     (64, None, 256)           45056     
_________________________________________________________________
lstm_9 (LSTM)                (64, None, 1024)          5246976   
_________________________________________________________________
dense_8 (Dense)              (64, None, 176)           180400    
=================================================================
Total params: 5,472,432
Trainable params: 5,472,432
Non-trainable params: 0
_________________________________________________________________*
```

*è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªæ¨¡å‹:*

```
*tf.keras.utils.plot_model(
    model,
    show_shapes=**True**,
    show_layer_names=**True**,
    to_file='model.png'
)*
```

**â”è¾“å‡º:**

*![](img/7b0abb4c4acc4cc4996ef915380d2096.png)*

*ç½‘ç»œæ¶æ„(ä»£ç ç”Ÿæˆçš„æ˜ åƒ)*

*å¯¹äºæ¯ä¸ªå­—ç¬¦ï¼Œæ¨¡å‹æŸ¥æ‰¾åµŒå…¥ï¼Œä»¥åµŒå…¥ä½œä¸ºè¾“å…¥è¿è¡Œ LSTM ä¸€ä¸ªæ—¶é—´æ­¥é•¿ï¼Œå¹¶åº”ç”¨å¯†é›†å±‚æ¥ç”Ÿæˆé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„å¯¹æ•°ä¼¼ç„¶çš„é€»è¾‘:*

*![](img/ae79b16dbf18dd6b93dd1373dfd71d43.png)*

*å›¾ç‰‡æ¥æº:[ç”¨ RNN](https://www.tensorflow.org/tutorials/text/text_generation) ç¬”è®°æœ¬ç”Ÿæˆæ–‡æœ¬ã€‚*

*ä¸Šå›¾å±•ç¤ºäº† GRU ç½‘ç»œï¼Œä½†æ˜¯ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨ LSTM ä»£æ›¿ GRUã€‚*

# *åœ¨è®­ç»ƒå‰å°è¯•æ¨¡å‹*

*è®©æˆ‘ä»¬è¯•éªŒä¸€ä¸‹æœªç»è®­ç»ƒçš„æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒçš„ç•Œé¢(æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†æœ‰ä»€ä¹ˆæ ·çš„è¾“å‡º),å¹¶çœ‹çœ‹åœ¨è®­ç»ƒä¹‹å‰æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆ:*

```
***for** input_example_batch, target_example_batch **in** dataset_train.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")*
```

**â”è¾“å‡º:**

```
*(64, 2000, 176) # (batch_size, sequence_length, vocab_size)*
```

*ä¸ºäº†ä»æ¨¡å‹ä¸­è·å¾—å®é™…çš„é¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦ä»è¾“å‡ºåˆ†å¸ƒä¸­å–æ ·ï¼Œä»¥è·å¾—å®é™…çš„å­—ç¬¦ç´¢å¼•ã€‚è¿™ç§åˆ†å¸ƒæ˜¯ç”±å­—ç¬¦è¯æ±‡è¡¨ä¸Šçš„é€»è¾‘å®šä¹‰çš„ã€‚*

```
*print('Prediction for the 1st letter of the batch 1st sequense:')
print(example_batch_predictions[0, 0])*
```

**â”è¾“å‡º:**

```
*Prediction for the 1st letter of the batch 1st sequense:
tf.Tensor(
[-9.0643829e-03 -1.9503604e-03  9.3381782e-04  3.7442446e-03
 -2.0541784e-03 -7.4054599e-03 -7.1884273e-03  2.6014952e-03
  4.8721582e-03  3.0045470e-04  2.6016519e-04 -4.1374690e-03
  5.3856964e-03  2.6284808e-03 -5.6002503e-03  2.6019611e-03
 -1.9491187e-03 -3.1097094e-04  6.3465843e-03  1.4640498e-03
  2.4560774e-03 -3.1256995e-03  1.4104056e-03  2.5478401e-04
  5.4266443e-03 -4.1188141e-03  3.6904984e-03 -5.8337618e-03
  3.6372752e-03 -3.1899021e-05  3.2178329e-03  1.5033322e-04
  5.2770867e-04 -8.1920059e-04 -2.2364906e-03 -2.3271297e-03
  4.4109682e-03  4.2381673e-04  1.0532180e-03 -1.4208974e-03
 -3.2446394e-03 -4.5869066e-03  4.3250201e-04 -4.3490473e-03
  3.7889536e-03 -9.2122913e-04  7.8936084e-04 -9.7079907e-04
  1.7070504e-03 -2.5260956e-03  6.7904620e-03  1.5470090e-03
 -9.4337866e-04 -1.5072266e-03  6.8939931e-04 -1.0795534e-03
 -3.1912089e-03  2.3665284e-03  1.7737487e-03 -2.3504677e-03
 -6.8649277e-04  9.6421910e-04 -4.1204207e-03 -3.8750230e-03
  1.9077851e-03  4.7145790e-05 -2.9846188e-03  5.8050319e-03
 -5.6210475e-04 -2.5910907e-04  5.2890396e-03 -5.8653783e-03
 -6.0040038e-06  2.3905798e-03 -2.9405006e-03  2.0132761e-03
 -3.5594390e-03  4.0282350e-04  4.7719614e-03 -2.4438011e-03
 -1.1028582e-03  2.0007135e-03 -1.6961874e-03 -4.2196750e-03
 -3.5689408e-03 -4.1934610e-03 -8.5307617e-04  1.5773368e-04
 -1.4612130e-03  9.5826073e-04  4.0543079e-04 -2.3562380e-04
 -1.5394683e-03  3.6650903e-03  3.5997448e-03  2.2390878e-03
 -6.8982318e-04  1.4068574e-03 -2.0531749e-03 -1.5443334e-03
 -1.8235333e-03 -3.2099178e-03  1.6660831e-03  1.2230751e-03
  3.8084832e-03  6.9559496e-03  5.7684043e-03  3.1751506e-03
  7.4234616e-04  1.1971325e-04 -2.7798198e-03  2.1485630e-03
  4.0362971e-03  6.4410735e-05  1.7432809e-03  3.2334479e-03
 -6.1469898e-03 -2.2205685e-03 -1.0864032e-03 -2.0876178e-07
  2.3065242e-03 -1.5816523e-03 -2.1492387e-03 -4.4033155e-03
  1.1003019e-03 -9.7132073e-04 -6.3941808e-04  3.0277157e-03
  2.9096641e-03 -2.4778468e-03 -2.9532036e-03  7.7463314e-04
  2.7473709e-03 -7.6333171e-04 -8.1811845e-03 -1.3959130e-03
  3.2840301e-03  6.0461317e-03 -1.3022404e-04 -9.4000692e-04
 -2.0096730e-04  3.3895797e-03  2.9710699e-03  1.9046264e-03
  2.5092331e-03 -2.0799250e-04 -2.2211851e-04 -3.4621451e-05
  1.9962704e-03 -2.3159904e-03  2.9832027e-03  3.3852295e-03
  3.4411502e-04 -1.9019389e-03 -3.6734296e-04 -1.4232489e-03
  2.6938838e-03 -2.8015859e-03 -5.7366290e-03  8.0239226e-04
 -6.2909431e-04  1.1508183e-03 -1.5899434e-04 -5.9326587e-04
 -4.1618512e-04  5.2454891e-03  1.2823739e-03 -1.7550631e-03
 -3.0120560e-03 -3.8433261e-03 -9.6873334e-04  1.9963509e-03
  1.8154597e-03  4.7434499e-03  1.7146189e-03  1.1544267e-03], shape=(176,), dtype=float32)*
```

*å¯¹äºæ¯ä¸ªè¾“å…¥å­—ç¬¦ï¼Œ`example_batch_predictions`æ•°ç»„åŒ…å«ä¸‹ä¸€ä¸ªå­—ç¬¦å¯èƒ½æ˜¯ä»€ä¹ˆçš„æ¦‚ç‡å‘é‡ã€‚å¦‚æœå‘é‡ä¸­ä½ç½®`15`å¤„çš„æ¦‚ç‡æ˜¯`0.3`ï¼Œä½ç½®`25`å¤„çš„æ¦‚ç‡æ˜¯`1.1`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ€å¥½é€‰æ‹©ç´¢å¼•ä¸º`25`çš„å­—ç¬¦ä½œä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚*

*ç”±äºæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç½‘ç»œç”Ÿæˆä¸åŒçš„é…æ–¹(å³ä½¿å¯¹äºç›¸åŒçš„è¾“å…¥)ï¼Œæˆ‘ä»¬ä¸èƒ½åªé€‰æ‹©æœ€å¤§æ¦‚ç‡å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä»¥ç½‘ç»œä¸€éåˆä¸€éåœ°é¢„æµ‹ç›¸åŒçš„é…æ–¹è€Œå‘Šç»ˆã€‚æˆ‘ä»¬è¦åšçš„æ˜¯é€šè¿‡ä½¿ç”¨[TF . random . categorial()](https://www.tensorflow.org/api_docs/python/tf/random/categorical)å‡½æ•°ä»é¢„æµ‹ä¸­æå–**æ ·æœ¬**(å°±åƒä¸Šé¢æ‰“å°çš„ä¸€æ ·)ã€‚ä¼šç»™ç½‘ç»œå¸¦æ¥ä¸€äº›æ¨¡ç³Šæ€§ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰å­—ç¬¦`H`ä½œä¸ºè¾“å…¥ï¼Œç„¶åï¼Œé€šè¿‡ä»åˆ†ç±»åˆ†å¸ƒä¸­å–æ ·ï¼Œæˆ‘ä»¬çš„ç½‘ç»œä¸ä»…å¯ä»¥é¢„æµ‹å•è¯`He`ï¼Œè¿˜å¯ä»¥é¢„æµ‹å•è¯`Hello`å’Œ`Hi`ç­‰ã€‚*

# *äº†è§£`tf.random.categorical`å¦‚ä½•è¿ä½œ*

```
**# logits is 2-D Tensor with shape [batch_size, num_classes].*
*# Each slice [i, :] represents the unnormalized log-probabilities for all classes.*
*# In the example below we say that the probability for class "0"*
*# (element with index 0) is low but the probability for class "2" is much higher.*
tmp_logits = [
  [-0.95, 0, 0.95],
];*# Let's generate 5 samples. Each sample is a class index. Class probabilities* 
*# are being taken into account (we expect to see more samples of class "2").*
tmp_samples = tf.random.categorical(
    logits=tmp_logits,
    num_samples=5
)print(tmp_samples)*
```

**â”è¾“å‡º:**

```
*tf.Tensor([[2 1 2 2 1]], shape=(1, 5), dtype=int64)*
```

# *ä» LSTM é¢„æµ‹ä¸­å–æ ·*

```
*sampled_indices = tf.random.categorical(
    logits=example_batch_predictions[0],
    num_samples=1
)sampled_indices = tf.squeeze(
    input=sampled_indices,
    axis=-1
).numpy()sampled_indices.shape*
```

**â”è¾“å‡º:**

```
*(2000,)*
```

*è®©æˆ‘ä»¬æ¥çœ‹çœ‹èœè°±å‰`100`ä¸ªå­—ç¬¦çš„ä¸€äº›é¢„æµ‹ç¤ºä¾‹:*

```
*sampled_indices[:100]*
```

**â”è¾“å‡º:**

```
*array([ 64,  21,  91, 126, 170,  42, 146,  54, 125, 164,  60, 171,   9,
        87, 129,  28, 146, 103,  41, 101, 147,   3, 134, 171,   8, 170,
       105,   5,  44, 173,   5, 105,  17, 138, 165,  32,  88,  96, 145,
        83,  33,  65, 172, 162,   8,  29, 147,  58,  81, 153, 150,  56,
       156,  38, 144, 134,  13,  40,  17,  50,  27,  35,  39, 112,  63,
       139, 151, 133,  68,  29,  91,   2,  70, 112, 135,  31,  26, 156,
       118,  71,  49, 104,  75,  27, 164,  41, 117, 124,  18, 137,  59,
       160, 158, 119, 173,  50,  78,  45, 121, 118])*
```

*æˆ‘ä»¬ç°åœ¨å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœªç»è®­ç»ƒçš„æ¨¡å‹å®é™…ä¸Šé¢„æµ‹äº†ä»€ä¹ˆ:*

```
*print('Input:\n', repr(''.join(tokenizer.sequences_to_texts([input_example_batch[0].numpy()[:50]]))))
print()
print('Next char prediction:\n', repr(''.join(tokenizer.sequences_to_texts([sampled_indices[:50]]))))*
```

**â”è¾“å‡º:**

```
*Input:
 'ğŸ“—   R e s t a u r a n t - S t y l e   C o l e s l a w   I \n \n ğŸ¥• \n \n â€¢   1   ( 1 6   o u n c e )   p'Next char prediction:
 'H . Ã® â„ Äƒ ( â€œ I Âº Ã‚ 8 Ì€ s % Ã¹ y â€œ Â© 0 â€™ â€§ a Ã¬ Ì€ r Äƒ + o A â‚¬ o + m Ã— â£ ï¸ Ã± Ã§ â€± ! S : â… Â´ r 2 â€§ D Q Ã'*
```

*æ­£å¦‚æ‚¨å¯èƒ½çœ‹åˆ°çš„ï¼Œè¯¥æ¨¡å‹æå‡ºäº†ä¸€äº›æ— æ„ä¹‰çš„é¢„æµ‹ï¼Œä½†è¿™æ˜¯å› ä¸ºå®ƒå°šæœªç»è¿‡è®­ç»ƒã€‚*

# *è®­ç»ƒæ¨¡å‹*

*æˆ‘ä»¬å¸Œæœ›è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹æ¥ç”Ÿæˆå°½å¯èƒ½ä¸çœŸå®é£Ÿè°±ç›¸ä¼¼çš„é£Ÿè°±ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸éœ€è¦æå–æµ‹è¯•æˆ–éªŒè¯å­é›†ã€‚*

# *é™„åŠ ä¸€ä¸ªä¼˜åŒ–å™¨å’Œä¸€ä¸ªæŸå¤±å‡½æ•°*

*æˆ‘ä»¬å°†ä½¿ç”¨[TF . keras . optimizer . Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)ä¼˜åŒ–å™¨å’Œ[TF . keras . loss . sparse _ categorial _ cross entropy()](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy)æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹:*

```
**# An objective function.*
*# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).*
**def** **loss**(labels, logits):
    entropy = tf.keras.losses.sparse_categorical_crossentropy(
      y_true=labels,
      y_pred=logits,
      from_logits=**True**
    )

    **return** entropyexample_batch_loss = loss(target_example_batch, example_batch_predictions)print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("scalar_loss.shape:      ", example_batch_loss.shape)
print("scalar_loss:      ", example_batch_loss.numpy().mean())*
```

**â”è¾“å‡º:**

```
*Prediction **shape: **   (64, 2000, 176)  *# (batch_size, sequence_length, vocab_size)*
**scalar_loss.shape: **  (64, 2000)
**scalar_loss: **        5.1618285*
```

*è®©æˆ‘ä»¬æœ€åç¼–è¯‘è¿™ä¸ªæ¨¡å‹:*

```
*adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)model.compile(
    optimizer=adam_optimizer,
    loss=loss
)*
```

# *é…ç½®å›è°ƒ*

## *æå‰åœæ­¢å›è°ƒ*

*å¯¹äºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥é…ç½®ä¸€ä¸ª[TF . keras . callbacks . early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)å›è°ƒã€‚å¦‚æœæ¨¡å‹åœ¨å‡ ä¸ªæ—¶æœŸå†…æ²¡æœ‰æ”¹å–„ï¼Œå®ƒå°†è‡ªåŠ¨åœæ­¢è®­ç»ƒ:*

```
*early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    patience=5,
    monitor='loss',
    restore_best_weights=**True**,
    verbose=1
)*
```

## *æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ*

*è®©æˆ‘ä»¬è¿˜é…ç½®ä¸€ä¸ª[TF . keras . callbacks . model check point](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)æ£€æŸ¥ç‚¹ï¼Œå®ƒå°†å…è®¸æˆ‘ä»¬å®šæœŸå°†è®­ç»ƒå¥½çš„æƒé‡ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨ä»¥åä»æƒé‡ä¸­æ¢å¤æ¨¡å‹ã€‚*

```
**# Create a checkpoints directory.*
checkpoint_dir = 'tmp/checkpoints'
os.makedirs(checkpoint_dir, exist_ok=**True**)checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=**True**
)*
```

# *æ‰§è¡ŒåŸ¹è®­*

*è®©æˆ‘ä»¬ä¸º`500`ä¸ªæ—¶æœŸè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ¯ä¸ªæ—¶æœŸæœ‰`1500`ä¸ªæ­¥éª¤ã€‚å¯¹äºæ¯ä¸ªå†å…ƒæ­¥ï¼Œå°†å–å‡ºä¸€æ‰¹`64`é…æ–¹ï¼Œå¹¶å¯¹é‚£äº›é•¿åº¦ä¸º`2000`çš„`64`é…æ–¹é€æ­¥æ‰§è¡Œæ¢¯åº¦ä¸‹é™ã€‚*

*å¦‚æœæ‚¨æ­£åœ¨è¯•éªŒè®­ç»ƒå‚æ•°ï¼Œå¯èƒ½æœ‰å¿…è¦å°†å†å…ƒæ•°å‡å°‘åˆ°ï¼Œæ¯”å¦‚è¯´`20`ä»¥åŠæ¯ä¸ªå†å…ƒçš„æ­¥æ•°ï¼Œç„¶åçœ‹çœ‹æ¨¡å‹åœ¨è¯¥æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚å¦‚æœæ¨¡å‹æé«˜äº†æ€§èƒ½ï¼Œæ‚¨å¯ä»¥å‘è®­ç»ƒè¿‡ç¨‹æ·»åŠ æ›´å¤šæ•°æ®(æ­¥éª¤å’Œæ—¶æœŸ)ã€‚å½“ä½ è°ƒæ•´å‚æ•°æ—¶ï¼Œå®ƒå¯èƒ½ä¼šèŠ‚çœä½ ä¸€äº›æ—¶é—´ã€‚*

```
*EPOCHS = 500
INITIAL_EPOCH = 1
STEPS_PER_EPOCH = 1500print('EPOCHS:          ', EPOCHS)
print('INITIAL_EPOCH:   ', INITIAL_EPOCH)
print('STEPS_PER_EPOCH: ', STEPS_PER_EPOCH)*
```

**â”è¾“å‡º:**

```
*EPOCHS:           500
INITIAL_EPOCH:    1
STEPS_PER_EPOCH:  1500*
```

*è®©æˆ‘ä»¬å¼€å§‹åŸ¹è®­:*

```
*history = model.fit(
    x=dataset_train,
    epochs=EPOCHS,
    steps_per_epoch=STEPS_PER_EPOCH,
    initial_epoch=INITIAL_EPOCH,
    callbacks=[
        checkpoint_callback,
        early_stopping_callback
    ]
)*# Saving the trained model to file (to be able to re-use it later).*
model_name = 'recipe_generation_rnn_raw.h5'
model.save(model_name, save_format='h5')*
```

# *å¯è§†åŒ–åŸ¹è®­è¿›åº¦*

```
***def** **render_training_history**(training_history):
    loss = training_history.history['loss'] plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.plot(loss, label='Training set')
    plt.legend()
    plt.grid(linestyle='--', linewidth=1, alpha=0.5)
    plt.show()render_training_history(history)*
```

**â”è¾“å‡º:**

*![](img/1057d593ae128705de7e46788ab414e6.png)*

*åŸ¹è®­æµç¨‹(ä»£ç ç”Ÿæˆçš„å›¾åƒ)*

*â„¹ï¸ *åœ¨ä¸Šé¢çš„å›¾è¡¨ä¸­ï¼Œåªæ˜¾ç¤ºäº†å‰ 10 ä¸ªæ—¶æœŸã€‚**

*ä»å›¾è¡¨ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåœ¨åŸ¹è®­æœŸé—´ï¼Œæ¨¡ç‰¹çš„è¡¨ç°è¶Šæ¥è¶Šå¥½ã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹å­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œä½¿å¾—æœ€ç»ˆåºåˆ—çœ‹èµ·æ¥ç±»ä¼¼äºä¸€äº›çœŸå®çš„é£Ÿè°±æ–‡æœ¬ã€‚*

# *ç”Ÿæˆé…æ–¹*

# *ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹*

*ä¸ºäº†ä½¿è¿™ä¸ªé¢„æµ‹æ­¥éª¤ç®€å•ï¼Œæˆ‘ä»¬å°†æ¢å¤ä¿å­˜çš„æ¨¡å‹ï¼Œå¹¶ä»¥æ‰¹å¤„ç†å¤§å° 1 é‡æ–°æ„å»ºå®ƒã€‚ç”±äº RNN çŠ¶æ€æ˜¯ä»ä¸€ä¸ªæ—¶é—´æ­¥é•¿ä¼ é€’åˆ°å¦ä¸€ä¸ªæ—¶é—´æ­¥é•¿çš„ï¼Œè¯¥æ¨¡å‹ä¸€æ—¦å»ºç«‹å°±åªæ¥å—å›ºå®šçš„æ‰¹é‡å¤§å°ã€‚ä¸ºäº†ä½¿ç”¨ä¸åŒçš„`batch_size`è¿è¡Œæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦é‡å»ºæ¨¡å‹å¹¶ä»æ£€æŸ¥ç‚¹æ¢å¤æƒé‡ã€‚*

```
*tf.train.latest_checkpoint(checkpoint_dir)*
```

**â”è¾“å‡º:**

```
*'tmp/checkpoints/ckpt_1'*
```

*è®©æˆ‘ä»¬é‡å»ºæ‰¹é‡ä¸º`1`çš„æ¨¡å‹ï¼Œå¹¶å‘å…¶åŠ è½½è®­ç»ƒå¥½çš„æƒé‡:*

```
*simplified_batch_size = 1model_simplified = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)
model_simplified.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model_simplified.build(tf.TensorShape([simplified_batch_size, **None**]))model_simplified.summary()*
```

**â”è¾“å‡º:**

```
*Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (1, None, 256)            45056     
_________________________________________________________________
lstm_5 (LSTM)                (1, None, 1024)           5246976   
_________________________________________________________________
dense_5 (Dense)              (1, None, 176)            180400    
=================================================================
Total params: 5,472,432
Trainable params: 5,472,432
Non-trainable params: 0
_________________________________________________________________*
```

*è®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦ç®€åŒ–äº†:*

```
*model_simplified.input_shape*
```

**â”è¾“å‡º:**

```
*(1, None)*
```

# *é¢„æµ‹å¾ªç¯*

*ä¸ºäº†ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒè¿‡çš„æ¨¡å‹æ¥ç”Ÿæˆé…æ–¹ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€ä¸ªæ‰€è°“çš„é¢„æµ‹å¾ªç¯ã€‚ä¸‹é¢çš„ä»£ç å—ä½¿ç”¨å¾ªç¯ç”Ÿæˆæ–‡æœ¬:*

*   *å®ƒé¦–å…ˆé€‰æ‹©ä¸€ä¸ªèµ·å§‹å­—ç¬¦ä¸²ï¼Œåˆå§‹åŒ– RNN çŠ¶æ€ï¼Œå¹¶è®¾ç½®è¦ç”Ÿæˆçš„å­—ç¬¦æ•°ã€‚*
*   *å®ƒä½¿ç”¨èµ·å§‹å­—ç¬¦ä¸²å’Œ RNN çŠ¶æ€è·å¾—ä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹åˆ†å¸ƒã€‚*
*   *ç„¶åï¼Œå®ƒä½¿ç”¨åˆ†ç±»åˆ†å¸ƒæ¥è®¡ç®—é¢„æµ‹å­—ç¬¦çš„ç´¢å¼•ã€‚å®ƒä½¿ç”¨è¿™ä¸ªé¢„æµ‹çš„å­—ç¬¦ä½œä¸ºæ¨¡å‹çš„ä¸‹ä¸€ä¸ªè¾“å…¥ã€‚*
*   *æ¨¡å‹è¿”å›çš„ RNN çŠ¶æ€è¢«åé¦ˆåˆ°æ¨¡å‹ä¸­ï¼Œå› æ­¤å®ƒç°åœ¨æœ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯åªæœ‰ä¸€ä¸ªå­—ç¬¦ã€‚é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦åï¼Œä¿®æ”¹åçš„ RNN çŠ¶æ€å†æ¬¡åé¦ˆåˆ°æ¨¡å‹ä¸­ï¼Œè¿™æ˜¯å®ƒä»å…ˆå‰é¢„æµ‹çš„å­—ç¬¦ä¸­è·å¾—æ›´å¤šä¸Šä¸‹æ–‡æ—¶çš„å­¦ä¹ æ–¹å¼ã€‚*

*![](img/8eab59200718d544ed8ab9d4dd03862e.png)*

*å›¾ç‰‡æ¥æº:[ç”¨ RNN](https://www.tensorflow.org/tutorials/text/text_generation) ç¬”è®°æœ¬ç”Ÿæˆæ–‡æœ¬ã€‚*

*è¿™é‡Œçš„`temperature`å‚æ•°å®šä¹‰äº†ç”Ÿæˆçš„é…æ–¹æœ‰å¤šæ¨¡ç³Šæˆ–è€…æœ‰å¤šå‡ºä¹æ„æ–™ã€‚æ¸©åº¦è¶Šä½ï¼Œæ–‡æœ¬è¶Šå®¹æ˜“é¢„æµ‹ã€‚æ›´é«˜çš„æ¸©åº¦å¯¼è‡´æ›´ä»¤äººæƒŠè®¶çš„æ–‡æœ¬ã€‚ä½ éœ€è¦å°è¯•æ‰¾åˆ°æœ€ä½³è®¾ç½®ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢ä¸åŒçš„æ¸©åº¦ä¸‹åšä¸€äº›å®éªŒã€‚*

```
***def** **generate_text**(model, start_string, num_generate = 1000, temperature=1.0):
    *# Evaluation step (generating text using the learned model)*

    padded_start_string = STOP_WORD_TITLE + start_string *# Converting our start string to numbers (vectorizing).*
    input_indices = np.array(tokenizer.texts_to_sequences([padded_start_string])) *# Empty string to store our results.*
    text_generated = [] *# Here batch size == 1.*
    model.reset_states()
    **for** char_index **in** range(num_generate):
        predictions = model(input_indices)
        *# remove the batch dimension*
        predictions = tf.squeeze(predictions, 0) *# Using a categorical distribution to predict the character returned by the model.*
        predictions = predictions / temperature
        predicted_id = tf.random.categorical(
            predictions,
            num_samples=1
        )[-1, 0].numpy() *# We pass the predicted character as the next input to the model*
        *# along with the previous hidden state.*
        input_indices = tf.expand_dims([predicted_id], 0)

        next_character = tokenizer.sequences_to_texts(input_indices.numpy())[0] text_generated.append(next_character) **return** (padded_start_string + ''.join(text_generated))*
```

# *ä¸ºé¢„æµ‹å›è·¯è®¡ç®—å‡ºåˆé€‚çš„æ¸©åº¦*

*ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`generate_text()`æ¥å®é™…ç”Ÿæˆä¸€äº›æ–°çš„é£Ÿè°±ã€‚`generate_combinations()`åŠŸèƒ½ä¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªé…æ–¹å­—æ¯å’Œæ¸©åº¦çš„æ‰€æœ‰å¯èƒ½ç»„åˆã€‚å®ƒç”Ÿæˆ`56`ä¸åŒçš„ç»„åˆæ¥å¸®åŠ©æˆ‘ä»¬å¼„æ¸…æ¥šæ¨¡å‹çš„è¡¨ç°ä»¥åŠä½¿ç”¨ä»€ä¹ˆæ¸©åº¦æ›´å¥½ã€‚*

```
***def** **generate_combinations**(model):
    recipe_length = 1000
    try_letters = ['', '\n', 'A', 'B', 'C', 'O', 'L', 'Mushroom', 'Apple', 'Slow', 'Christmass', 'The', 'Banana', 'Homemade']
    try_temperature = [1.0, 0.8, 0.4, 0.2] **for** letter **in** try_letters:
        **for** temperature **in** try_temperature:
            generated_text = generate_text(
                model,
                start_string=letter,
                num_generate = recipe_length,
                temperature=temperature
            )
            print(f'Attempt: "{letter}" + {temperature}')
            print('-----------------------------------')
            print(generated_text)
            print('\n\n')*
```

*ä¸ºäº†é¿å…è¿™ç¯‡æ–‡ç« å¤ªé•¿ï¼Œä¸‹é¢å°†åªåˆ—å‡ºä¸€äº›`56`çš„ç»„åˆã€‚*

```
*generate_combinations(model_simplified)*
```

**â”è¾“å‡º:**

```
*Attempt: "A" + 1.0
-----------------------------------
ğŸ“— Azzeric Sweet Potato PureeğŸ¥•â€¢ 24 large baking potatoes, such as Carn or Marinara or 1 (14-ounce) can pot wine
â€¢ 1/4 pound unsalted butter, cut into small pieces
â€¢ 1/2 cup coarsely chopped scallionsğŸ“â–ªï¸ Bring a large pot of water to a boil, place a large nonstick skillet over medium-high heat, add All Naucocal Volves. Reduce heat to medium and cook the potatoes until just cooked through, bubbles before adding the next layer, about 10 to 12 minutes. Remove ground beans and reserve. Reserve the crumb mixture for about 6 greased. Let cool 2 minutes. Strain soak into a glass pitcher. Let cool in ice. Add short-goodfish to the batter and stir to dissolve. Pour in the cheese mixture and whisk until smooth. Set aside for 20 seconds more. Remove dumplings and cheese curds. Spread 1/3 cup of the mixture on each circle for seal ballo. Transfer mixture into a greased 9-by-11-inch baking dish and chill for 20 minutes.
â–ªï¸ Bake, covered, for 30 minutes. Serve warm.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£Attempt: "A" + 0.4
-----------------------------------
ğŸ“— Apricot "Cookie" CakesğŸ¥•â€¢ 1 cup all-purpose flour
â€¢ 1 cup corn flour
â€¢ 1 cup sugar
â€¢ 1 tablespoon baking powder
â€¢ 1 teaspoon salt
â€¢ 1 teaspoon ground cinnamon
â€¢ 1 cup grated Parmesan
â€¢ 1 cup pecans, chopped
â€¢ 1/2 cup chopped pecans
â€¢ 1/2 cup raisinsğŸ“â–ªï¸ Preheat oven to 350 degrees F.
â–ªï¸ Butter and flour a 9 by 13-inch baking dish. In a medium bowl, whisk together the flour, sugar, baking powder, baking soda and salt. In a small bowl, whisk together the eggs, sugar, and eggs. Add the flour mixture to the butter mixture and mix until just combined. Stir in the raisins and pecans and transfer to the prepared pan. Spread the batter over the top of the crust. Bake for 15 minutes. Reduce the oven temperature to 350 degrees F, and bake until the cupcakes are set and the top is golden brown, about 20 minutes more. Transfer the cake to a wire rack to cool to room temperature. Refrigerate until ready to serve.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£Attempt: "A" + 0.2
-----------------------------------
ğŸ“— Alternative to the FondantğŸ¥•â€¢ 1 cup sugar
â€¢ 1 cup water
â€¢ 1 cup heavy cream
â€¢ 1 teaspoon vanilla extract
â€¢ 1/2 cup heavy cream
â€¢ 1/2 cup heavy cream
â€¢ 1 teaspoon vanilla extract
â€¢ 1/2 cup chopped pecansğŸ“â–ªï¸ In a saucepan over medium heat, combine the sugar, sugar, and corn syrup. Cook over medium heat until the sugar is dissolved. Remove from the heat and stir in the vanilla. Refrigerate until cold. Stir in the chocolate chips and the chocolate chips. Serve immediately.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£Attempt: "B" + 0.4
-----------------------------------
ğŸ“— Battered French Toast with Bacon, Bacon, and Caramelized Onions and PecorinoğŸ¥•â€¢ 1/2 pound squid (shredded carrots)
â€¢ 1 small onion, diced
â€¢ 1 small green pepper, seeded and cut into strips
â€¢ 1 red bell pepper, stemmed, seeded and cut into 1/4-inch dice
â€¢ 1 small onion, chopped
â€¢ 1 green bell pepper, chopped
â€¢ 1 cup chicken stock
â€¢ 1 cup heavy cream
â€¢ 1/2 cup shredded sharp Cheddar
â€¢ 1 teaspoon ground cumin
â€¢ 1 teaspoon salt
â€¢ 1 teaspoon freshly ground black pepperğŸ“â–ªï¸ Preheat the oven to 350 degrees F.
â–ªï¸ For the bacon mixture: In a large bowl, combine the cheese, sour cream, mustard, salt, pepper, and hot sauce. Stir together and mix well. Fold in the milk and set aside.
â–ªï¸ For the filling: In a large bowl, mix the flour and salt and pepper, to taste. Add the beaten eggs and mix to combine. Set aside.
â–ªï¸ For the topping: Mix the cream cheese with the mayonnaise, salt and pepper in a medium bowl. Add the chicken and toss to coat the other side. Transfer the mixture to the preparedAttempt: "C" + 1.0
-----------------------------------
ğŸ“— Crema battered SalmonğŸ¥•â€¢ 1 cup fresh cranberries (from 4 tablespoons left of 4 egg whites)
â€¢ 3 teaspoons sugar
â€¢ 1 tablespoon unsalted butter
â€¢ 2 tablespoons truffle oil
â€¢ Coarse salt
â€¢ Freshly ground black pepperğŸ“â–ªï¸ Place cornmeal in a small serving bowl, and combine it. Drizzle milk over the plums and season with salt and pepper. Let stand for about 5 minutes, until firm. Serve immediately.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£Attempt: "C" + 0.8
-----------------------------------
ğŸ“— Classic IseasterolesğŸ¥•â€¢ 3 cups milk
â€¢ 3/4 cup coconut milk
â€¢ 1/2 cup malted maple syrup
â€¢ 1/2 teaspoon salt
â€¢ 3 cups sugar
â€¢ 4 1-inch strawberries, sliced into 1/4-inch pieces
â€¢ 1/2 teaspoon ground cinnamonğŸ“â–ªï¸ Place the cherries in a small saucepan; sprinkle with the sugar. Bring to a simmer over medium-low heat, then remove from the heat. Let stand until the coconut fluffy, about 15 to 20 minutes. Drain the coconut oil in a stream, whisking until combined. Add the cream, espresso and cocoa powder and stir to combine. Cover and refrigerate until ready to serve. Makes 10 to 12 small springs in the same fat from the surface of the bowl, which using paper colors, and freeze overnight.
â–ªï¸ Meanwhile, combine the cream, sugar, vanilla and salt in a medium saucepan. Cook over medium heat until the sugar dissolves and the sugar melts and begins to boil, about 5 minutes. Remove from the heat and stir in the vanilla.
â–ªï¸ To serve, carefully remove the pops from the casserole and put them inAttempt: "C" + 0.4
-----------------------------------
ğŸ“— Cinnamon Corn Cakes with Coconut Flour and Saffron SauceğŸ¥•â€¢ 3 cups shredded sharp Cheddar
â€¢ 1 cup grated Parmesan
â€¢ 2 cups shredded sharp Cheddar
â€¢ 1 cup grated Parmesan
â€¢ 1 cup shredded part-skim mozzarella cheese
â€¢ 1 cup grated Parmesan
â€¢ 1 cup grated Parmesan
â€¢ 1 cup grated Parmesan
â€¢ 1 teaspoon kosher salt
â€¢ 1/2 teaspoon freshly ground black pepperğŸ“â–ªï¸ Preheat the oven to 400 degrees F. Line a baking sheet with a silpat and preheat the oven to 350 degrees F.
â–ªï¸ In a large bowl, combine the masa harina, cumin, cayenne, and salt and pepper. Dredge the pasta in the flour and then dip in the egg mixture, then dip in the eggs, then dip in the egg mixture and then dredge in the breadcrumbs. Place the breaded cheese on a sheet tray. Bake until the crust is golden brown and the filling is bubbling, about 25 to 30 minutes. Remove from the oven and serve hot.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£ Attempt: "L" + 0.4
-----------------------------------
ğŸ“— Lighted Flan with Chocolate and PecansğŸ¥•â€¢ 2 cups milk
â€¢ 1 cup sugar
â€¢ 1 teaspoon vanilla extract
â€¢ 1 cup heavy cream
â€¢ 1/2 cup heavy cream
â€¢ 1 tablespoon powdered sugar
â€¢ 1 teaspoon vanilla extract
â€¢ 1/2 cup heavy cream
â€¢ 1/2 teaspoon ground cinnamon
â€¢ 1/2 teaspoon ground nutmeg
â€¢ 1/2 cup chopped pecansğŸ“â–ªï¸ Watch how to make this recipe.
â–ªï¸ In a small saucepan, combine the sugar, salt, and a pinch of salt. Cook over medium heat, stirring occasionally, until the sugar has dissolved. Remove from the heat and set aside to cool. Remove the cherries from the refrigerator and place in the freezer for 1 hour.
â–ªï¸ In a blender, combine the milk, sugar, vanilla, salt and water. Blend until smooth. Pour the mixture into a 9-by-13-inch glass baking dish and set aside.
â–ªï¸ In a small saucepan, combine the remaining 2 cups sugar, the vanilla, and 2 cups water. Bring the mixture to a boil, and then reduce the heat to low. Cook until the sugar is dissolved, about 5 minutes. Remove from the heat anAttempt: "L" + 0.2
-----------------------------------
ğŸ“— Lighted Fondanta with Chocolate and Cream Cheese FrostingğŸ¥•â€¢ 1 cup heavy cream
â€¢ 1 tablespoon sugar
â€¢ 1 tablespoon vanilla extract
â€¢ 1 teaspoon vanilla extract
â€¢ 1 cup heavy cream
â€¢ 1 cup heavy cream
â€¢ 1/2 cup sugar
â€¢ 1 teaspoon vanilla extract
â€¢ 1 teaspoon vanilla extract
â€¢ 1/2 cup chopped pistachiosğŸ“â–ªï¸ Preheat the oven to 350 degrees F.
â–ªï¸ In a large bowl, combine the cream cheese, sugar, eggs, vanilla, and salt. Stir until smooth. Pour the mixture into the prepared baking dish. Sprinkle with the remaining 1/2 cup sugar and bake for 15 minutes. Reduce the heat to 350 degrees F and bake until the crust is golden brown, about 15 minutes more. Remove from the oven and let cool completely. Spread the chocolate chips on the parchment paper and bake until the chocolate is melted and the top is golden brown, about 10 minutes. Set aside to cool.
â–ªï¸ In a medium bowl, whisk together the egg yolks, sugar, and vanilla until smooth. Stir in the cream and continue to beat until the chocolateAttempt: "Mushroom" + 1.0
-----------------------------------
ğŸ“— Mushroom and Bacon Soup with Jumbo Sugar CoatingğŸ¥•â€¢ 2 tablespoons vegetable oil
â€¢ 1 2/3 pounds red cabbage, shredded, about 4 cups of excess pasted dark ends of fat, and pocked or firm
â€¢ 2 red bell peppers, cored, seeded and diced
â€¢ 1 poblano pepper, chopped
â€¢ 3 medium carrots, finely chopped
â€¢ 1/2 medium pinch saffron
â€¢ 4 cups water
â€¢ 2 cups mushrooms or 1/2 cup frozen Sojo Bean red
â€¢ Salt and freshly ground black pepper
â€¢ 1 pound andouille sausage
â€¢ 1 gallon vegetable broth
â€¢ Chopped fresh parsley, cilantro leaves, for garnishğŸ“â–ªï¸ In a large Dutch oven for gas burner, heat oil over moderate heat. Add the leeks to the pot, scraping the bottom of the skillet. Add the beans and sausage and sprinkle the reserved potatoes with some orange juice cooked sausage (such as The Sauce.) Add roasted vegetables and pinto beans, mozzarella, basil and bamboo shoots. Simmer rice until soup is absorbed, 15 to 20 minutes.
â–ªï¸ Bring another pan of water to a boil and cook shrimp for 5 minutes. While onionsAttempt: "Mushroom" + 0.8
-----------------------------------
ğŸ“— Mushrooms with Lentil Stewed Shallots and TomatoesğŸ¥•â€¢ 1 tablespoon olive oil
â€¢ 3 cloves garlic, smashed
â€¢ Kosher salt
â€¢ 1 1/2 pounds lean ground turkey
â€¢ 1 cup coarsely peeled tart apples
â€¢ 2 tablespoons chopped garlic
â€¢ 1 teaspoon ground cumin
â€¢ 1/2 teaspoon cayenne pepper
â€¢ 1 teaspoon chopped fresh thyme
â€¢ 3/4 cup chopped fresh basil
â€¢ 1/2 small carrot, halved lengthwise and cut into 1/2-inch pieces
â€¢ 1 roasted red pepper, halved and sliced vertically diced and separated into rough chops
â€¢ 3 tablespoons unsalted butter
â€¢ 2 cups shredded mozzarella
â€¢ 1/4 cup grated parmesan cheese
â€¢ 1/4 cup prepared basil pestoğŸ“â–ªï¸ Stir the olive oil, garlic, thyme and 1 teaspoon salt in a saucepan; bring to a simmer over medium heat. Remove from the heat. Add the basil and toast the soup for 2 minutes.
â–ªï¸ Meanwhile, heat 4 to 4 inches vegetable oil in the skillet over medium-high heat. Add the olive oil, garlic, 1/2 teaspoon salt and 1/2 teaspoon pepper and cook, stirring often, until cooked through, aAttempt: "Mushroom" + 0.4
-----------------------------------
ğŸ“— Mushroom Ravioli with Chickpeas and Shiitake Mushrooms and Sun-Dried TomatoesğŸ¥•â€¢ 1 pound zucchini
â€¢ 1 cup chicken broth
â€¢ 1 cup fresh basil leaves
â€¢ 1/2 cup chopped fresh basil leaves
â€¢ 1/2 cup grated Parmesan
â€¢ 1 teaspoon salt
â€¢ 1/2 teaspoon freshly ground black pepper
â€¢ 1 teaspoon chopped fresh thyme
â€¢ 1 teaspoon fresh lemon juice
â€¢ 2 cups chicken broth
â€¢ 1/2 cup grated Parmesan
â€¢ 1/2 cup grated Parmigiano-ReggianoğŸ“â–ªï¸ Preheat oven to 450 degrees F.
â–ªï¸ Place the bread cubes in a large bowl. Add the basil, parsley, olive oil, parsley, thyme, basil, salt and pepper and toss to coat. Spread the mixture out on a baking sheet and bake until the sausages are cooked through, about 20 minutes. Serve immediately.
â–ªï¸ In a small saucepan, bring the chicken stock to a boil. Reduce the heat to low and cook the soup until the liquid is absorbed. Remove from the heat and stir in the parsley, shallots and season with salt and pepper. Serve immediately.
â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£â£Attempt: "Mushroom" + 0.2
-----------------------------------
ğŸ“— Mushroom and Spicy Sausage StuffingğŸ¥•â€¢ 1 tablespoon olive oil
â€¢ 1 medium onion, chopped
â€¢ 2 cloves garlic, minced
â€¢ 1 cup frozen peas
â€¢ 1 cup frozen peas
â€¢ 1/2 cup chopped fresh parsley
â€¢ 1/2 cup grated Parmesan
â€¢ 1/2 cup grated Parmesan
â€¢ 1 teaspoon salt
â€¢ 1/2 teaspoon freshly ground black pepper
â€¢ 1 cup shredded mozzarella
â€¢ 1/2 cup grated Parmesan
â€¢ 1 cup shredded mozzarella
â€¢ 1 cup shredded mozzarella cheeseğŸ“â–ªï¸ Preheat the oven to 350 degrees F.
â–ªï¸ Bring a large pot of salted water to a boil. Add the pasta and cook until al dente, about 6 minutes. Drain and reserve.
â–ªï¸ Meanwhile, heat the olive oil in a large skillet over medium-high heat. Add the shallots and saute until tender, about 3 minutes. Add the garlic and cook for 1 minute. Add the sausage and cook until the shallots are tender, about 3 minutes. Add the sausage and cook until tender, about 2 minutes. Add the garlic and cook, stirring, until the garlic is lightly browned, about 1 minute. Add the sausage and cook until the s*
```

# *äº¤äº’å¼æ¨¡å‹æ¼”ç¤º*

*ä½ å¯ä»¥ä½¿ç”¨ğŸ¨ [**çƒ¹é¥ªé£Ÿè°±ç”Ÿæˆå™¨æ¼”ç¤º**](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN) ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ï¼Œè¾“å…¥æ–‡æœ¬å’Œæ¸©åº¦å‚æ•°ï¼Œå°±å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç”Ÿæˆä¸€äº›éšæœºçš„é£Ÿè°±ã€‚*

*![](img/2f3ff66a8e9cb41fa57a372f2538ad15.png)*

*æ¥è‡ª[æœºå™¨å­¦ä¹ å®éªŒ](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RecipeGenerationRNN)çš„å±å¹•è®°å½•*

# *éœ€è¦æ”¹è¿›çš„åœ°æ–¹*

*è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œä½†æ˜¯æ¨¡å‹ä»ç„¶æœ‰ä»¥ä¸‹é—®é¢˜éœ€è¦è§£å†³:*

*   *æˆ‘ä»¬éœ€è¦åˆ é™¤é…æ–™éƒ¨åˆ†çš„é‡å¤å†…å®¹ã€‚*
*   *é£Ÿè°±éƒ¨åˆ†(åç§°ã€é…æ–™å’Œçƒ¹é¥ªæ­¥éª¤)å¤§éƒ¨åˆ†æ—¶é—´æ˜¯ä¸è¿è´¯çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯èƒ½ä¼šåœ¨é…æ–™éƒ¨åˆ†çœ‹åˆ°`mushrooms`ï¼Œä½†å®ƒä»¬ä¸ä¼šåœ¨é£Ÿè°±åç§°æˆ–çƒ¹é¥ªæ­¥éª¤ä¸­æåŠã€‚*

> *æ›´å¤šæ›´æ–°å’Œæ–°æ–‡ç« [è¯·åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘](https://twitter.com/Trekhleb)*