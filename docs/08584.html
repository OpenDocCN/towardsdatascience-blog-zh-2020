<html>
<head>
<title>Pytorch vs Tensorflow in 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年Pytorch vs Tensorflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-vs-tensorflow-in-2020-fe237862fae1?source=collection_archive---------4-----------------------#2020-06-22">https://towardsdatascience.com/pytorch-vs-tensorflow-in-2020-fe237862fae1?source=collection_archive---------4-----------------------#2020-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dcf8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">两种流行的框架是如何融合的</h2></div><p id="37f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pytorch和Tensorflow是目前最流行的两个深度学习框架。学习和适应一个新的框架总是需要做大量的工作，所以很多人面临着从两个框架中选择哪一个的困境</p><p id="d1b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个框架在设计、范式、语法等方面有很大的不同，但是它们已经发展了很多，都从对方那里吸取了好的特性，不再有那么大的不同。</p><p id="d36d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很多比较这两者的网上文章都有点过时了，而且没有恰当地抓住当前的情况。事实上，PyTorch的原始作者之一Soumith Chintala最近也在推特上说这两个框架现在非常相似。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><h1 id="1d8e" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">简史</h1><p id="21e4" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">Tensorflow来自谷歌，于2015年发布，PyTorch由脸书于2017年发布。</p><p id="a793" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Tensorflow出现较早，因此在用户数量、采用率等方面领先，但Pytorch在过去几年中大大缩小了差距</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/74e4cc95e30ede86c81a9eabc6cd8fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsaBkifkc5P7ihRA8IKQ8Q.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源:<a class="ae mq" href="https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=%2Fg%2F11bwp1s2k3,%2Fg%2F11gd3905v1" rel="noopener ugc nofollow" target="_blank">谷歌趋势</a></p></figure><p id="7885" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两者都致力于被称为<em class="mr">张量</em>的基本数据类型，它只不过是多维数组，适合高性能计算。两者都将计算表示为有向无环图，通常称为<em class="mr">计算图。</em></p><h1 id="17e9" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">功能比较:过去和现在</h1><p id="01d1" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">让我们来看看这些框架的一些重要方面，开始时的主要区别以及目前的情况。</p><p id="6c17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">编程API </strong></p><p id="433d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Tensorflow API一开始非常神秘，感觉就像在学习一种新的编程语言，此外，由于其静态计算图方法，它也很难调试(下面将详细介绍)。另一方面，Pytorch (python) API从一开始就非常python化，感觉就像编写原生Python代码一样，非常容易调试。</p><p id="ecb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Tensorflow使用Tensorflow 2.0对其API进行了重大清理，并将高级编程API Keras集成到主API本身中。这两者的结合极大地减少了过去编写张量流代码时必须承受的认知负荷:-)。</p><p id="a47b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编程API(tensor flow和PyTorch)实际上现在看起来非常相似，以至于两者很多时候都无法区分(见最后的例子)</p><h2 id="1424" class="ms lj iq bd lk mt mu dn lo mv mw dp ls ko mx my lu ks mz na lw kw nb nc ly nd bi translated"><strong class="ak">计算图</strong></h2><p id="97cb" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">计算图是两个框架之间的主要设计差异。</p><p id="30ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Tensorflow采用了一种静态计算图方法，在这种方法中，我们定义了想要执行的计算序列，并为数据预留了占位符。之后，为了训练/运行模型，你输入数据。静态计算图对于性能和在不同设备(cpu / gpu / tpu)上运行的能力非常重要，但是调试起来非常麻烦。</p><p id="71e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，Pytorch采用了动态计算图方法，在解释代码时逐行进行计算。这使得调试代码变得容易得多，并且还提供了其他好处——例如在像RNN这样的模型中支持可变长度输入。</p><p id="eec8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快进到今天，Tensorflow引入了通过其“渴望”模式构建动态计算图的工具，PyTorch允许构建静态计算图，所以现在两个框架中都有静态/动态模式。</p><h2 id="d700" class="ms lj iq bd lk mt mu dn lo mv mw dp ls ko mx my lu ks mz na lw kw nb nc ly nd bi translated"><strong class="ak">分布式计算</strong></h2><p id="4663" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">这两个框架都提供了在单/多/分布式CPU或GPU上运行的工具。在早期，让Tensorflow在多个GPU上工作是一件痛苦的事情，因为人们必须在多个设备上手动编码和微调性能，从那时起事情发生了变化，现在使用这两个框架进行分布式计算几乎毫不费力。</p><p id="0a4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谷歌定制的硬件加速器张量处理单元(TPU)可以以极快的速度运行计算，甚至比GPU快得多，将于2018年供第三方使用。由于Tensorflow和TPU都来自谷歌，使用Tensorflow在TPU上运行代码要比使用PyTorch容易得多，因为PyTorch使用第三方库(如<a class="ae mq" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank"> XLA </a>)在TPU上运行代码的方式有点混乱</p><h2 id="daa9" class="ms lj iq bd lk mt mu dn lo mv mw dp ls ko mx my lu ks mz na lw kw nb nc ly nd bi translated">部署/生产</h2><p id="8e5a" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">与PyTorch相比，Tensorflow在部署方面有很多优势，部分原因是其静态计算图方法带来了更好的性能，但也是因为有助于在云、浏览器或移动设备上快速部署的包/工具。这就是很多公司在生产时更喜欢Tensorflow的原因。</p><p id="2751" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PyTorch试图在1.5+版本中用TorchServe来弥补这一差距，但它尚未成熟</p><h2 id="f97e" class="ms lj iq bd lk mt mu dn lo mv mw dp ls ko mx my lu ks mz na lw kw nb nc ly nd bi translated">代码比较</h2><p id="7bd0" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">有趣的是，很多东西的API是如此的相似，以至于代码几乎无法区分。下面是Tensorflow和Pytorch的<a class="ae mq" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST数字识别</a>(计算机视觉中众所周知的“Hello World”问题)的核心组件的代码片段，试着<strong class="kh ir">猜猜</strong>哪个是哪个</p><p id="7b4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的Tensorflow和Pytorch代码可在我的<a class="ae mq" href="https://github.com/moizsaifee/TF-vs-PyTorch" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>获得</p><p id="f34a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">数据加载器</strong></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="e8ea" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Download the MNIST Data<br/></em>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0<br/><em class="mr"># Add a channels dimension<br/></em>x_train = x_train[..., tf.newaxis]<br/>x_test = x_test[..., tf.newaxis]<br/><br/>train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)<br/>test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)</span></pre><p id="063f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哪一个是PyTorch代码-上面还是下面？</p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="abf5" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Download the MNIST Data and create dataloader<br/></em>transform = transforms.Compose([transforms.ToTensor()])<br/>xy_train = datasets.MNIST(<strong class="nf ir">'./'</strong>, download=True, train=True, transform=transform)<br/>xy_test = datasets.MNIST(<strong class="nf ir">'./'</strong>, download=True, train=False, transform=transform)<br/><br/>train_ds = DataLoader(xy_train, batch_size=32, shuffle=True)<br/>test_ds = DataLoader(xy_test, batch_size=32, shuffle=True)</span></pre><p id="db37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好的，加载数据的方法看起来有点不同，但是我保证从现在开始会变得相似:-)</p><p id="b12a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">模型定义</strong></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="2025" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Model Definition<br/></em>class MyModel(nn.Module):<br/>    def __init__(self):<br/>        super(MyModel, self).__init__()<br/>        self.conv1 = Conv2d(in_channels=1, out_channels=32, kernel_size=3)<br/>        self.flatten = Flatten()<br/>        self.d1 = Linear(21632, 128)<br/>        self.d2 = Linear(128, 10)<br/><br/>    def forward(self, x):<br/>        x = F.relu(self.conv1(x))<br/>        x = self.flatten(x)<br/>        x = F.relu(self.d1(x))<br/>        x = self.d2(x)<br/>        output = F.log_softmax(x, dim=1)<br/>        return output</span></pre><p id="f67d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mr">哪一个是PyTorch码——上面还是下面？</em></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="32ff" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Model Definition<br/></em>class MyModel(Model):<br/>    def __init__(self):<br/>        super(MyModel, self).__init__()<br/>        self.conv1 = Conv2D(filters=32, kernel_size=3, activation=<strong class="nf ir">'relu'</strong>)<br/>        self.flatten = Flatten()<br/>        self.d1 = Dense(128, activation=<strong class="nf ir">'relu'</strong>)<br/>        self.d2 = Dense(10)<br/><br/>    def call(self, x):<br/>        x = self.conv1(x)<br/>        x = self.flatten(x)<br/>        x = self.d1(x)<br/>        output = self.d2(x)<br/>        return output</span></pre><p id="0c5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例化模型、损失、优化器</strong></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="94bf" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Instantiate Model, Optimizer, Loss<br/></em>model = MyModel()<br/>optimizer = Adam()<br/>loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction=<strong class="nf ir">'sum'</strong>)</span></pre><p id="8cf7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mr">哪个是PyTorch码——上面还是下面？</em></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="8359" class="ms lj iq nf b gy nj nk l nl nm"><em class="mr"># Instantiate Model, Optimizer, Loss<br/></em>model = MyModel()<br/>optimizer = Adam(model.parameters())<br/>loss_object = CrossEntropyLoss(reduction=<strong class="nf ir">'sum'</strong>)</span></pre><p id="c18b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">训练循环</strong></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="ae2c" class="ms lj iq nf b gy nj nk l nl nm">for epoch in range(2):<br/>    <em class="mr"># Reset the metrics at the start of the next epoch<br/>    </em>train_loss = 0<br/>    train_n = 0<br/>    for images, labels in train_ds:<br/>        with GradientTape() as tape:<br/>            <em class="mr"># training=True is only needed if there are layers with different<br/>            # behavior during training versus inference (e.g. Dropout).<br/>            </em>predictions = model(images, training=True)<br/>            loss = loss_object(labels, predictions)<br/>            train_loss += loss.numpy()<br/>            train_n += labels.shape[0]<br/>        gradients = tape.gradient(loss, model.trainable_variables)<br/>        optimizer.apply_gradients(zip(gradients, model.trainable_variables))<br/>    train_loss /= train_n</span></pre><p id="b9a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mr">哪个是PyTorch码——上面还是下面？</em></p><pre class="lb lc ld le gt ne nf ng nh aw ni bi"><span id="de39" class="ms lj iq nf b gy nj nk l nl nm">for epoch in range(2):<br/>    <em class="mr"># Train<br/>    </em>model.train()<br/>    train_loss = 0<br/>    train_n = 0<br/>    for image, labels in train_ds:<br/>        predictions = model(image).squeeze()<br/>        loss = loss_object(predictions, labels)<br/>        train_loss += loss.item()<br/>        train_n += labels.shape[0]<br/>        loss.backward()<br/>        optimizer.step()<br/>        optimizer.zero_grad()<br/>    train_loss /= train_n</span></pre><p id="3d53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">真的很有趣(而且方便！)现在的API看起来是多么的相似。</p><h1 id="9d68" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">结论</h1><p id="9033" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">Tensorflow和PyTorch是深度学习应用研发的两个优秀框架。他们在计算、管理底层硬件方面做着繁重的工作，并拥有庞大的社区，这使得站在巨人的肩膀上开发定制应用程序变得容易得多。</p><p id="84e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直到不久前，这两个框架还存在一些主要的差异，从那以后，它们都从对方那里吸取了好的特性，并在这个过程中变得更好。仍然有一些东西在一个中比另一个稍微容易一些，但是由于相似性的增加，现在在两者之间来回切换也比以前更容易了。</p><h1 id="ccfc" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">参考</h1><ul class=""><li id="e19c" class="nn no iq kh b ki ma kl mb ko np ks nq kw nr la ns nt nu nv bi translated"><a class="ae mq" href="https://github.com/moizsaifee/TF-vs-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github.com/moizsaifee/TF-vs-PyTorch</a></li><li id="4a04" class="nn no iq kh b ki nw kl nx ko ny ks nz kw oa la ns nt nu nv bi translated"><a class="ae mq" href="https://www.tensorflow.org/guide/effective_tf2" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/effective_tf2</a></li><li id="b332" class="nn no iq kh b ki nw kl nx ko ny ks nz kw oa la ns nt nu nv bi translated"><a class="ae mq" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/index.html</a></li></ul></div></div>    
</body>
</html>