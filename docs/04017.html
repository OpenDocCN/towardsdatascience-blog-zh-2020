<html>
<head>
<title>COVID-19 FineTuned Bert Literature Search Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新冠肺炎微调伯特文学搜索引擎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/covid-19-finetuned-bert-literature-search-engine-93ff9755a502?source=collection_archive---------35-----------------------#2020-04-13">https://towardsdatascience.com/covid-19-finetuned-bert-literature-search-engine-93ff9755a502?source=collection_archive---------35-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8e37eb218427f5a30a33aa6064280107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTUA701WBp9MdIKWS7xKig.jpeg"/></div></div></figure><p id="bdb4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在过去的几周内，研究工作和研究论文的数量在持续增加，以对抗这种冠状新冠肺炎邪恶病毒。组织如此庞大的数据现在至关重要，这就是这项工作(<a class="ae kz" href="https://www.kaggle.com/theamrzaki/covid-19-finetune-bert-researchpapers-semantic-sea" rel="noopener ugc nofollow" target="_blank"> kaggle代码</a>、<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search/blob/master/COVID_19_FineTune_BERT_ResearchPapers_Semantic_Search.ipynb" rel="noopener ugc nofollow" target="_blank"> github代码</a>优化运行在谷歌实验室、<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">数据</a>)试图实现的，作为<a class="ae kz" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568" rel="noopener ugc nofollow" target="_blank">Kaggle竞赛</a>的一部分，该竞赛试图找到智能解决方案来组织大量不断增加的研究知识。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/fe3423754cd7130aa23193c98cf7fb1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*mK8jQzL7YPI_ocRfCLkWuw.gif"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">微调BERT，然后嵌入研究论文来构建搜索引擎</p></figure><p id="e86e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们的方法是</p><ol class=""><li id="6d82" class="lj lk it kd b ke kf ki kj km ll kq lm ku ln ky lo lp lq lr bi translated">使用<a class="ae kz" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变压器</a>包微调BERT架构</li><li id="e4e9" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky lo lp lq lr bi translated">然后嵌入<strong class="kd iu">研究论文</strong>和<strong class="kd iu">查询</strong>，并使用余弦相似度在它们之间进行比较，有效地构建了一个搜索引擎，这里我们将使用<a class="ae kz" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换包</a></li></ol><p id="c28a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这篇教程是我们上一篇教程 ( <a class="ae kz" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">代码</a>，<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">数据</a>)的延续，在这篇教程中，我们使用了一个已经训练好的BERT模型，并用它来构建我们的搜索引擎，不同的是，今天我们将微调我们的BERT模型以适应研究论文本身，所以让我们开始吧！！</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="48ef" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">A.为什么是伯特</h1><p id="8581" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">BERT已经证明了它是语言建模的架构，在使用Transformers之前，文献使用seq2seq编码器-解码基于递归的模型(在我们的博客<a class="ae kz" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">系列</a>中阅读更多)</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/f47d3a36d905500007637a8fb648fd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XKNX_ldxN6ESn1rbjyGyyA.gif"/></div></div></figure><p id="2877" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，使用<a class="ae kz" href="https://medium.com/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener"> LSTM </a>限制了该架构处理长句的能力，所以这就是为什么在他们的论文<a class="ae kz" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">中引入了变形金刚【注意力是你所需要的全部】</a>，它们依赖于注意力模型，特别是<strong class="kd iu">自我注意力</strong>，这是一种神经网络，旨在理解如何关注输入句子中的特定单词，变形金刚也内置于编码器/解码器结构中(在贾马尔的精彩博客<a class="ae kz" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a>中了解更多信息)</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/3fb8579af0397dcc0ad431ba3257de95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAaU4E4q--8qQlNFeq_F7g.jpeg"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">来自<a class="ae kz" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></p></figure><p id="3f01" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">结果是，<a class="ae kz" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">我们不需要整个Transformer采用一个可微调的语言模型来完成NLP任务</a>，我们可以只使用解码器，就像<a class="ae kz" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"> OpenAI提出的那样</a>，然而，由于它使用解码器，模型只训练一个正向模型，而不考虑前面和后面(因此是双向的)，这就是为什么引入了BERT，我们只使用Transformer编码器。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/90a45bba17739d50961c6156acf3c84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0RyusmMkDe6gHzTu.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">来自<a class="ae kz" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></p></figure><h1 id="b762" class="me mf it bd mg mh nk mj mk ml nl mn mo mp nm mr ms mt nn mv mw mx no mz na nb bi translated">B.微调伯特</h1><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0d183c162f10227671fe4bcc2de70d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*6M87UPeyCPBs7vPs868GLA.jpeg"/></div></figure><p id="0c6f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了获得巨大的语言理解，BERT在巨大的数据集上被训练，然而，我们能够进一步训练BERT到我们自己的数据集，(这是<a class="ae kz" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank"> covid19研究论文</a>)这一步被称为微调，因为你微调BERT以适应我们自己的数据</p><p id="c777" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">1-首先我们将把<a class="ae kz" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank">研究论文</a>(处理过的数据集在这里找到<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">)以一个大文件的形式传递，其中每一段都在它自己的一行上</a></p><p id="5925" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2-然后我们将使用<a class="ae kz" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变压器</a>包来微调BERT</p><p id="987e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以让我们进入细节</p><p id="867a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">1-处理<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">数据</a>，我们采用了一些处理技术来构建一个csv文件，其中每一行都是一篇论文中的一个段落，我们的搜索引擎会尝试获取与查询最相似的段落，您可以从<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">这里下载数据</a>(了解更多关于如何将数据从google drive连接到google colab <a class="ae kz" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">这里</a>)</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="e4e5" class="nv mf it nr b gy nw nx l ny nz">import pandas as pd<br/>from tqdm import tqdm</span><span id="6ca6" class="nv mf it nr b gy oa nx l ny nz">#read csv<br/>df_sentences = pd.read_csv("/content/drive/My Drive/BertSentenceSimilarity/Data/covid_sentences.csv")<br/>df_sentences = df_sentences.set_index("Unnamed: 0")</span><span id="c403" class="nv mf it nr b gy oa nx l ny nz">#load column to list<br/>df_sentences = df_sentences["paper_id"].to_dict()<br/>df_sentences_list = list(df_sentences.keys())<br/>df_sentences_list = [str(d) for d in tqdm(df_sentences_list)]</span><span id="6820" class="nv mf it nr b gy oa nx l ny nz">#process data to file<br/>file_content = "\n".join(df_sentences_list)<br/>with open("input_text.txt","w") as f:<br/>    f.write(file_content)</span></pre><p id="70d8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2-现在我们将使用<a class="ae kz" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>套装来微调BERT</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="e401" class="nv mf it nr b gy nw nx l ny nz">!pip install transformers<br/>!git clone https://github.com/huggingface/transformers.git</span></pre><p id="44f1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后运行微调</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="2e84" class="nv mf it nr b gy nw nx l ny nz">!python "/content/transformers/examples/run_language_modeling.py" \<br/>--output_dir="/content/drive/My Drive/BertSentenceSimilarity/BERTfine" \<br/>--model_type=bert \<br/>--model_name_or_path=google/bert_uncased_L-2_H-128_A-2 \<br/>--do_train \<br/>--block_size=512 \<br/>--train_data_file="/content/input_text.txt" \<br/>--mlm</span></pre><h1 id="8ddb" class="me mf it bd mg mh nk mj mk ml nl mn mo mp nm mr ms mt nn mv mw mx no mz na nb bi translated">C.构建搜索引擎</h1><p id="ab18" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">既然我们已经构建了自己的微调过的BERT，让我们将嵌入应用到我们的数据中(使用<a class="ae kz" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>包)</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="250b" class="nv mf it nr b gy nw nx l ny nz">!pip install -U sentence-transformers</span></pre><p id="7dae" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后加载您微调过的BERT模型</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="b3b2" class="nv mf it nr b gy nw nx l ny nz">#https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py</span><span id="816a" class="nv mf it nr b gy oa nx l ny nz">from sentence_transformers import SentenceTransformer<br/>from sentence_transformers import models, losses<br/>import scipy.spatial<br/>import pickle as pkl</span><span id="8892" class="nv mf it nr b gy oa nx l ny nz">word_embedding_model = models.BERT("/content/drive/My Drive/BertSentenceSimilarity/BERTfine")</span><span id="d3bd" class="nv mf it nr b gy oa nx l ny nz"># Apply mean pooling to get one fixed sized sentence vector<br/>pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),<br/>pooling_mode_mean_tokens=True,pooling_mode_cls_token=False,pooling_mode_max_tokens=False)</span><span id="bed7" class="nv mf it nr b gy oa nx l ny nz">model = SentenceTransformer(modules=[word_embedding_model, pooling_model])</span></pre><p id="ef08" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后应用嵌入，并将结果保存到pkl临时文件中</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/749e81916342348af919b1eb764b33be.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*--swioOowlYRUAteCqKdmg.gif"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">使用微调的BERT来嵌入我们的数据集</p></figure><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="c569" class="nv mf it nr b gy nw nx l ny nz">corpus = df_sentences_list<br/>corpus_embeddings = model.encode(corpus,show_progress_bar=True)</span><span id="3889" class="nv mf it nr b gy oa nx l ny nz">with open("/content/drive/My Drive/BertSentenceSimilarity/Pickles/corpus_finetuned_embeddings.pkl" , "wb") as f:<br/>    pkl.dump(corpus_embeddings,f)</span></pre><p id="7938" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们即将完成，我们只需要嵌入查询本身，然后使用<strong class="kd iu">余弦相似度</strong>来获得研究论文中最相似的段落，有效地构建一个搜索引擎</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi la"><img src="../Images/0ff4aba3dcfa3e64e5fe9380f1229bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*j_9FTDHs4RipZBNCsm40Ow.gif"/></div></div></figure><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="55b8" class="nv mf it nr b gy nw nx l ny nz"># Query sentences:</span><span id="b3c3" class="nv mf it nr b gy oa nx l ny nz">queries = ['What has been published about medical care?',<br/>'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest',<br/>'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually',<br/>'Resources to support skilled nursing facilities and long term care facilities.',<br/>'Mobilization of surge medical staff to address shortages in overwhelmed communities .',<br/>'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies .']</span><span id="4348" class="nv mf it nr b gy oa nx l ny nz">query_embeddings = model.encode(queries,show_progress_bar=True)</span></pre><p id="3451" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们应用余弦相似度</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="ab34" class="nv mf it nr b gy nw nx l ny nz"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span><span id="92c1" class="nv mf it nr b gy oa nx l ny nz">closest_n = 5 <br/>print("\nTop 5 most similar sentences in corpus:")</span><span id="5c0e" class="nv mf it nr b gy oa nx l ny nz"><em class="ob"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</em><br/>closest_n = 5<br/>print("<strong class="nr iu">\n</strong>Top 5 most similar sentences in corpus:")<br/><strong class="nr iu">for</strong> query, query_embedding <strong class="nr iu">in</strong> zip(queries, query_embeddings):<br/>    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, "cosine")[0]<br/><br/>    results = zip(range(len(distances)), distances)<br/>    results = sorted(results, key=<strong class="nr iu">lambda</strong> x: x[1])<br/><br/>    <strong class="nr iu">for</strong> idx, distance <strong class="nr iu">in</strong> results[0:closest_n]:<br/>        print("Score:   ", "(Score: <strong class="nr iu">%.4f</strong>)" % (1-distance) , "<strong class="nr iu">\n</strong>" )<br/>        print("Paragraph:   ", corpus[idx].strip(), "<strong class="nr iu">\n</strong>" )<br/>        row_dict = df.loc[df.index== corpus[idx]].to_dict()<br/>        print("paper_id:  " , row_dict["paper_id"][corpus[idx]] , "<strong class="nr iu">\n</strong>")<br/>        print("Title:  " , row_dict["title"][corpus[idx]] , "<strong class="nr iu">\n</strong>")<br/>        print("Abstract:  " , row_dict["abstract"][corpus[idx]] , "<strong class="nr iu">\n</strong>")<br/>        print("Abstract_Summary:  " , row_dict["abstract_summary"][corpus[idx]] , "<strong class="nr iu">\n</strong>")</span></pre><h1 id="3e30" class="me mf it bd mg mh nk mj mk ml nl mn mo mp nm mr ms mt nn mv mw mx no mz na nb bi translated">D.结果是</h1><p id="279d" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">(与我们上一个<a class="ae kz" rel="noopener" target="_blank" href="/covid-19-bert-literature-search-engine-4d06cdac08bd">教程</a>的预训练BERT相比)</p><h2 id="ae21" class="nv mf it bd mg oc od dn mk oe of dp mo km og oh ms kq oi oj mw ku ok ol na om bi translated">示例1:</h2><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="28f2" class="nv mf it nr b gy nw nx l ny nz">==========================Query==========================<br/>=== What has been published about medical care? =========<br/>=========================================================</span><span id="92b0" class="nv mf it nr b gy oa nx l ny nz">==========================OLD (pretrained)=======================<br/>Score:    (Score: 0.8296)<br/>Paragraph:    how may state authorities require persons to undergo medical treatment<br/>Title:    Chapter 10 Legal Aspects of Biosecurity-------------------</span><span id="769c" class="nv mf it nr b gy oa nx l ny nz">---------------Score:    (Score: 0.8220) <br/>Paragraph:    to identify how one health has been used recently in the medical literature<br/>Title:    One Health and Zoonoses: The Evolution of One&lt;br&gt;Health and Incorporation of Zoonoses</span><span id="ab25" class="nv mf it nr b gy oa nx l ny nz">==========================NEW (finetuned)=========================<br/>---------------Score:    (Score: 0.8779)</span><span id="4b9d" class="nv mf it nr b gy oa nx l ny nz">Paragraph:    what is already known about this topic what are the new findings   paper_id:   f084dcc7e442ab282deb97670e1843e347cf1fd5   </span><span id="df96" class="nv mf it nr b gy oa nx l ny nz">Title:    Ebola Holding Units at government hospitals in&lt;br&gt;Sierra Leone: evidence for a flexible and effective&lt;br&gt;model for safe isolation, early treatment&lt;br&gt;initiation, hospital safety and health system functioning</span><span id="753b" class="nv mf it nr b gy oa nx l ny nz">---------------Score:    (Score: 0.8735)<br/>Paragraph:    to identify how one health has been used recently in the medical literature</span><span id="07f1" class="nv mf it nr b gy oa nx l ny nz">Title:    One Health and Zoonoses: The Evolution of One&lt;br&gt;Health and Incorporation of Zoonoses</span></pre><h2 id="39ff" class="nv mf it bd mg oc od dn mk oe of dp mo km og oh ms kq oi oj mw ku ok ol na om bi translated">示例2:</h2><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="8f78" class="nv mf it nr b gy nw nx l ny nz">==========================Query==============================<br/>=== Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest =====<br/>=========================================================</span><span id="c478" class="nv mf it nr b gy oa nx l ny nz">==========================OLD (pretrained)=======================<br/>--------------Score:    (Score: 0.8139) <br/>Paragraph:    clinical signs in hcm are explained by leftsided chf complications of arterial thromboembolism ate lv outflow tract obstruction or arrhythmias capable of<br/>Title:    Chapter 150 Cardiomyopathy<br/></span><span id="df27" class="nv mf it nr b gy oa nx l ny nz">--------------Score:    (Score: 0.7966) <br/>Paragraph:    the term arrhythmogenic cardiomyopathy is a useful expression that refers to recurrent or persistent ventricular or atrial arrhythmias in the setting of a normal echocardiogram the most commonly observed rhythm disturbances are pvcs and ventricular tachycardia vt however atrial rhythm disturbances may be recognized including atrial fibrillation paroxysmal or sustained atrial tachycardia and atrial flutter<br/>Title:    Chapter 150 Cardiomyopathy</span><span id="2055" class="nv mf it nr b gy oa nx l ny nz">==========================NEW (finetuned)=========================<br/>--------------Score:    (Score: 0.8942)<br/>Paragraph:    echocardiography and cardiac catheterization are common cardiac imaging modalities both modalities have drawbacks the limitations of echocardiography include operator dependence limited acoustic shadows a small field of view and poor evaluation of pulmonary veins the limitations of cardiac .......</span><span id="5f10" class="nv mf it nr b gy oa nx l ny nz">Title:    Trends in the utilization of computed&lt;br&gt;tomography and cardiac catheterization among children&lt;br&gt;with congenital heart disease</span><span id="d508" class="nv mf it nr b gy oa nx l ny nz">--------------Score:    (Score: 0.8937)<br/>Paragraph:    classic physical examination features of dcm include soft heart sounds from reduced contractility or pleural effusion gallop rhythm with or without a systolic murmur hypokinetic arterial pulses dull left apical impulse and clinical signs of profound chf exceptional cases are seen prior to onset of chf</span><span id="7686" class="nv mf it nr b gy oa nx l ny nz">Title:    Chapter 150 Cardiomyopathy</span></pre><p id="9d3f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如我们所看到的，新的微调伯特已经获得了经验丰富的知识，特别是优化和定制的新冠肺炎研究论文</p><p id="b4da" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">完整的结果请参考<a class="ae kz" href="https://www.kaggle.com/theamrzaki/covid-19-finetune-bert-researchpapers-semantic-sea" rel="noopener ugc nofollow" target="_blank">我们的代码笔记本</a> (kaggle)或<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search/blob/master/COVID_19_FineTune_BERT_ResearchPapers_Semantic_Search.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a> (github优化为在google colab上运行)</p><h1 id="0998" class="me mf it bd mg mh nk mj mk ml nl mn mo mp nm mr ms mt nn mv mw mx no mz na nb bi translated">电子评论</h1><p id="2a0e" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">我们真的被深深打动了，</p><ul class=""><li id="7649" class="lj lk it kd b ke kf ki kj km ll kq lm ku ln ky on lp lq lr bi translated">变形金刚包的易用性使得微调BERT变得非常容易，只需提供一个输入文本文件，每行包含一个句子(在我们的例子中是研究论文中的段落)</li><li id="9739" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated"><a class="ae kz" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>库，这使得应用BERT嵌入和提取相似性变得极其容易。</li><li id="ceb1" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">结果的质量给我们留下了深刻的印象，因为与我们上一次教程中<a class="ae kz" rel="noopener" target="_blank" href="/covid-19-bert-literature-search-engine-4d06cdac08bd">的一般预训练的BERT相比，微调的BERT被证明更适合我们自己的数据集</a></li><li id="caf3" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们相信，通过使用段落本身，而不仅仅是论文的摘要，我们不仅能够返回最相似的论文，而且能够返回论文中最相似的部分。</li><li id="52ce" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们希望通过这一点，我们正在帮助构建一个不断增加的文献研究工作的世界，以对抗这种冠状新冠肺炎病毒。</li></ul><h1 id="1ec1" class="me mf it bd mg mh nk mj mk ml nl mn mo mp nm mr ms mt nn mv mw mx no mz na nb bi translated">电子参考文献</h1><ul class=""><li id="da10" class="lj lk it kd b ke nc ki nd km oo kq op ku oq ky on lp lq lr bi translated">我们使用由<a class="ae kz" href="https://github.com/huggingface" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>提供的叫做<a class="ae kz" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的库，这个库使得微调伯特变得非常容易</li><li id="a19d" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们使用由<a class="ae kz" href="https://github.com/UKPLab" rel="noopener ugc nofollow" target="_blank"> UKPLab </a>提供的名为<a class="ae kz" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>的库，这个库使得使用BERT和其他架构如ALBERT、XLNet进行句子嵌入变得非常容易，它们还提供了一个简单的接口来查询和聚集数据。</li><li id="66ba" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们已经使用来自<a class="ae kz" href="https://www.kaggle.com/maksimeren/covid-19-literature-clustering" rel="noopener ugc nofollow" target="_blank"> maksimeren </a>的代码进行数据处理，我们真心感谢他。</li><li id="0817" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们使用了绘画伯特的概念，在这里讨论<a class="ae kz" href="http://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉姆马</a>在说明我们的建筑如何工作时，我们还参考了他所做的多个插图和解释，他的博客信息量极大，易于理解。</li><li id="3ea2" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们使用Conneau等人在2017年讨论的预训练模型，在InferSent-Paper(从自然语言推理数据中监督学习通用句子表示)中显示，对自然语言推理(NLI)数据的训练可以产生通用句子嵌入。</li><li id="25d9" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated"><a class="ae kz" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">注意就是你所需要的</strong></a><strong class="kd iu"/>变压器纸</li><li id="37a2" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated"><a class="ae kz" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>，<a class="ae kz" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特代码</a></li><li id="d370" class="lj lk it kd b ke ls ki lt km lu kq lv ku lw ky on lp lq lr bi translated">我们使用了一些来自<a class="ae kz" href="http://freepik.com/" rel="noopener ugc nofollow" target="_blank"> Freepic </a>的矢量(图像)元素</li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="a3f5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本教程的代码可以在<a class="ae kz" href="https://www.kaggle.com/theamrzaki/covid-19-finetune-bert-researchpapers-semantic-sea" rel="noopener ugc nofollow" target="_blank">这里</a>找到，或者从<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search/blob/master/COVID_19_FineTune_BERT_ResearchPapers_Semantic_Search.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>这里找到(如果你需要在google colab上运行它)，代码被构建为在google colab上无缝运行，使用其免费使用的GPU，我们也使用kaggle API将数据直接下载到google colab，所以既不需要在本地运行代码，也不需要在本地下载数据集。</p><p id="a2ac" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们还提供了所有处理过的数据集和嵌入的段落(219MB ),这些都需要一些时间来嵌入(所以你可以简单地使用它而不需要再次运行嵌入)，链接<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search" rel="noopener ugc nofollow" target="_blank">这里</a>，这些都托管在google drive上，我们已经构建了<a class="ae kz" href="https://www.kaggle.com/theamrzaki/covid-19-finetune-bert-researchpapers-semantic-sea" rel="noopener ugc nofollow" target="_blank">代码</a>或<a class="ae kz" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search/blob/master/COVID_19_FineTune_BERT_ResearchPapers_Semantic_Search.ipynb" rel="noopener ugc nofollow" target="_blank"> github代码</a>来无缝连接到google drive，(了解更多关于连接google drive的信息<a class="ae kz" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">这里</a></p><blockquote class="or os ot"><p id="80da" class="kb kc ob kd b ke kf kg kh ki kj kk kl ou kn ko kp ov kr ks kt ow kv kw kx ky im bi translated"><strong class="kd iu"> <em class="it">编者注:</em> </strong> <em class="it"> </em> <a class="ae kz" href="http://towardsdatascience.com/" rel="noopener" target="_blank"> <em class="it">走向数据科学</em> </a> <em class="it">是一份以数据科学和机器学习研究为主的中型刊物。我们不是健康专家或流行病学家，本文的观点不应被解释为专业建议。想了解更多关于疫情冠状病毒的信息，可以点击</em> <a class="ae kz" href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports" rel="noopener ugc nofollow" target="_blank"> <em class="it">这里</em> </a> <em class="it">。</em></p></blockquote></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="fc85" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们真的相信，通过本教程，您已经对BERT有了更多的了解，以及如何根据您自己的数据集对其进行微调，我们也希望通过本教程，我们能够帮助研究社区对抗冠状病毒(新冠肺炎)。</p><p id="a9a3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">希望你们都平安无事。</p></div></div>    
</body>
</html>