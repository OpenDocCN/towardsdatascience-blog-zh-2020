<html>
<head>
<title>Illustrated Guide to Transformers- Step by Step Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器图解指南-逐步解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?source=collection_archive---------0-----------------------#2020-04-30">https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?source=collection_archive---------0-----------------------#2020-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/30364bf3d447c17c480149f95670bcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lKxMJupvor8t4CaavK7PYQ.png"/></div></div></figure><p id="201a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">变形金刚正在席卷自然语言处理世界。这些令人难以置信的模型打破了多项 NLP 记录，推动了技术的发展。它们被用于许多应用程序，如机器语言翻译、会话聊天机器人，甚至是更好的搜索引擎。变形金刚是当今深度学习的热门，但它们是如何工作的呢？为什么它们胜过了之前的序列问题之王，比如递归神经网络、GRU 和 LSTM？你可能听说过不同的著名变形金刚模型，如伯特、GPT 和 GPT2。在本帖中，我们将重点关注引发这一切的一篇文章，“你所需要的是注意力”。</p><p id="108e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你想看视频版本，请点击下面的链接。</p><figure class="kx ky kz la gt jr"><div class="bz fp l di"><div class="lb lc l"/></div></figure><h1 id="6974" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">注意机制</h1><p id="46f7" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">为了理解变形金刚，我们首先必须理解注意力机制。注意机制使变形金刚具有极长的记忆。转换器模型可以“关注”或“关注”所有先前已经生成的令牌。</p><p id="9158" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看一个例子。比方说我们要用生成式变形金刚写一篇短篇科幻小说。使用<a class="ae kw" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">的拥抱脸</a>的<a class="ae kw" href="https://transformer.huggingface.co/" rel="noopener ugc nofollow" target="_blank">用变形金刚</a>写的应用程序，我们就可以做到这一点。我们将用我们的输入来填充模型，模型将生成其余的。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/4f9cc238ba29c3d3b70bea7d33950ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CoVkXTogdhlfjSw_.png"/></div></div></figure><p id="ec1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">我们的输入:</strong> <em class="mh">“随着外星人进入我们的星球”。</em></p><p id="2ba9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">变形金刚输出:</strong> <em class="mh">“开始殖民地球，某一群外星人开始操纵我们的社会，通过他们对一定数量精英的影响来保持对平民的铁腕控制。”</em></p><p id="2216" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好吧，这个故事有点黑暗，但有趣的是这个模型是如何产生它的。当模型逐字生成文本时，它可以“注意”或“关注”与生成的单词相关的单词。知道参加什么单词的能力也是通过反向传播在训练中学会的。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/ece6e042822cc9c88a173150b10c6bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ODlgeguKzjyzjuuJ.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">在一个接一个地生成单词时关注不同标记的注意机制</p></figure><p id="51be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">递归神经网络(RNN)也能够查看以前的输入。但是注意力机制的力量在于它不会受到短期记忆的影响。RNN 的参考窗口更短，所以当故事变长时，RNN 不能访问序列中更早生成的单词。对于门控循环单位(GRU 氏)和长短期记忆(LSTM 氏)网络来说仍然如此，尽管它们实现长期记忆的能力更大，因此具有更长的参考窗口。从理论上讲，注意机制在给定足够的计算资源的情况下，具有无限的参考窗口，因此能够在生成文本时使用故事的整个上下文。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/3019dc087328476419d78fcea50c1e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*w2OJ4jfjxmlcqu6k4BdgrA.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">假设的注意参照窗，RNN 的，GRU 的和 LSTM 的</p></figure><h1 id="8406" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">注意力是你所需要的-一步一步的演练</h1><p id="06f4" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">在论文“<a class="ae kw" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”中展示了注意力机制的力量，其中作者介绍了一种新的新型神经网络，称为变压器，这是一种基于注意力的编码器-解码器类型的架构。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/cbcf7e2a8afca9bb5502e90df62082f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/0*1Q2FlNcFND8amq3m.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">变压器模型</p></figure><p id="fd7a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在高层次上，编码器将输入序列映射到抽象的连续表示中，该表示保存该输入的所有学习信息。然后，解码器获取该连续表示，并逐步生成单个输出，同时还被馈送先前的输出。</p><p id="9652" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看一个例子。本文将变压器模型应用于神经机器翻译问题。在本帖中，我们将展示它如何适用于对话聊天机器人。</p><p id="b34e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">我们的输入:</strong> <em class="mh">【嗨你好】</em></p><p id="2ad1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">变压器输出:</strong> <em class="mh">【我很好】</em></p><h1 id="adab" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">输入嵌入</h1><p id="7a33" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">第一步是将输入输出到单词嵌入层。单词嵌入层可以被认为是获取每个单词的学习矢量表示的查找表。神经网络通过数字进行学习，因此每个单词都映射到一个具有连续值的向量来表示该单词。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/604117f94390c9cd85fc064701d1ea3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/0*6MnniQMOBPu4kFq3.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">将单词转换为输入嵌入</p></figure><h1 id="b369" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">位置编码</h1><p id="b60c" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">下一步是在嵌入中注入位置信息。因为变换器编码器不像递归神经网络那样具有递归性，所以我们必须将一些关于位置的信息添加到输入嵌入中。这是使用位置编码完成的。作者想出了一个使用正弦和余弦函数的聪明绝招。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mp"><img src="../Images/ad8a5a77c4cbc9699b46284a317eff5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PuV87IVkFin98EZW.png"/></div></div></figure><p id="cde2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们不会深入位置编码的数学细节，但这里有一些基础知识。对于输入向量的每个奇数索引，使用 cos 函数创建一个向量。对于每个偶数索引，使用 sin 函数创建一个向量。然后将这些向量添加到它们相应的输入嵌入中。这成功地给出了关于每个矢量位置的网络信息。选择正弦和余弦函数是因为它们具有线性属性，模型可以很容易地学会处理。</p><h1 id="1711" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">编码器层</h1><p id="c0f0" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">现在我们有了编码器层。编码器层的工作是将所有输入序列映射到一个抽象的连续表示中，该表示保存整个序列的学习信息。它包含 2 个子模块，多头关注，其次是一个完全连接的网络。在两个子图层的每一个周围还有剩余连接，随后是图层归一化。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3f9d8c8295a97b70f5bdce010e54a922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/0*gxx0-uUpZfAmKmPa.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">编码器层子模块</p></figure><p id="ed09" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要分解这一点，我们先来看看多头注意力模块。</p><h1 id="a6f4" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">多头注意力</h1><p id="64be" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">编码器中的多头注意力应用了一种称为自我注意力的特定注意力机制。自我注意允许模型将输入中的每个单词与其他单词联系起来。所以在我们的例子中，有可能我们的模型可以学会将单词“你”，与“怎么样”和“是”联系起来。也有可能模型知道以这种模式构建的单词通常是一个问题，因此做出适当的响应。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/491beac62a3d28e6c7c8a60c1cbfdcea.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*mupwYy99Watj1GJT.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">编码器自关注操作。参考下面的插图。</p></figure><h2 id="016d" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">查询、键和值向量</h2><p id="a097" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">为了实现自我关注，我们将输入提供给 3 个不同的完全连接的层，以创建查询、键和值向量。</p><p id="7657" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些向量到底是什么？我在栈交换上找到了一个很好的解释。</p><blockquote class="ne nf ng"><p id="10b2" class="jy jz mh ka b kb kc kd ke kf kg kh ki nh kk kl km ni ko kp kq nj ks kt ku kv ij bi translated"><em class="iq">“查询关键字和值概念来自检索系统。例如，当你键入一个查询来搜索 Youtube 上的一些视频时，搜索引擎会将你的</em> <strong class="ka ir"> <em class="iq">查询</em> </strong> <em class="iq">映射到一组</em> <strong class="ka ir"> <em class="iq">键</em> </strong> <em class="iq">(视频标题、描述等)。)与数据库中的候选视频相关联，然后呈现给你最匹配的视频(</em> <strong class="ka ir"> <em class="iq">值</em> </strong> <em class="iq">)。</em></p></blockquote><h2 id="1805" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">查询和关键字的点积</h2><p id="bb0f" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">在通过线性层馈送查询、关键字和值向量之后，查询和关键字经历点积矩阵乘法以产生得分矩阵。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/aa828a7c696802b62cea23bced20cad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*kxR_DjBgFw7LTTN-Ut34Pw.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">查询和密钥的点积乘法</p></figure><p id="2e0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">得分矩阵决定了一个单词在其他单词上的关注程度。因此每个单词都有一个与时间步长中的其他单词相对应的分数。分数越高，注意力越集中。这就是查询映射到键的方式。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/faeb3caf322afed623acffe74728b5f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*D8GA7_DsjTfudI0-.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">点积的注意力分数。</p></figure><h2 id="904e" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">降低注意力分数</h2><p id="be83" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">然后，分数通过除以查询和键的维度的平方根而缩小。这是为了获得更稳定的渐变，因为倍增值可能会产生爆炸效果。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8d45d5fb1dea2f53c4319418cb4e3488.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*VKdRa5TC6jJmKZ-0.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">降低注意力分数</p></figure><h2 id="7d9d" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">标度分数的最大值</h2><p id="0520" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">下一步，你取标度分数的 softmax 来得到注意力权重，它给你 0 到 1 之间的概率值。通过做 softmax，高分得到提高，低分受到抑制。这允许模型对哪些单词也要参加更有信心。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/80e012675454047a7b5206f943513b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qSKUxncfQVhUJeCr.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">取标度分数的软最大值以获得概率值</p></figure><h2 id="dedc" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">将 Softmax 输出乘以值向量</h2><p id="7ef4" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">然后你把注意力权重乘以你的价值向量，得到一个输出向量。softmax 分数越高，模型学习的单词值就越重要。较低的分数会淹没无关的单词。然后你把它的输出输入到一个线性层进行处理。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e6291b42ee3f4ebe871a46ac7cbc0ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/0*dv4q_I1gubq2xua1.png"/></div></figure><h1 id="9536" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">计算多头注意力</h1><p id="d321" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">要使这成为多头注意力计算，您需要在应用自我注意力之前将查询、键和值分成 N 个向量。分裂的向量然后单独经历自我注意过程。每一个自我关注的过程被称为一个头。每个头产生一个输出向量，在通过最终的线性层之前，该向量被连接成一个向量。理论上，每个头将学习不同的东西，因此给编码器模型更多的表示能力。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/45d6373e1f7e7a69eb443f0698ab771b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*X0c962yMhgRKfMTD.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">在应用自我关注之前，分裂 Q，K，V，N 次</p></figure><p id="b126" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总而言之，多头注意力是 transformer 网络中的一个模块，它计算输入的注意力权重，并产生一个输出向量，其中包含关于每个单词应该如何关注序列中所有其他单词的编码信息。</p><h1 id="04dd" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">剩余连接、层标准化和前馈网络</h1><p id="98cf" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">多头注意力输出向量被添加到原始位置输入嵌入中。这被称为剩余连接。剩余连接的输出经过层标准化。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/923167d29744d9c82007608d65f6d09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*RRWR2BsH5SQgMGo3.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">位置输入嵌入与多头注意输出的剩余连接</p></figure><p id="eebf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">归一化残差输出通过逐点前馈网络进行投影，以便进一步处理。点式前馈网络是一对线性层，中间有一个 ReLU 激活。然后，其输出再次添加到逐点前馈网络的输入，并进一步归一化。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/5cc3ffa1781555c52425161cccbb50d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*-fdpoPbN-BHAMRnr.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">逐点前馈层的输入和输出的剩余连接。</p></figure><p id="c30e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">剩余连接通过允许梯度直接流过网络来帮助网络训练。使用层标准化来稳定网络，从而大大减少所需的训练时间。逐点前馈层用于投射注意力输出，潜在地给予它更丰富的表示。</p><h1 id="e8fe" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">编码器总结</h1><p id="07e1" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">这就完成了编码器层。所有这些操作都是将输入编码成具有注意力信息的连续表示。这将有助于解码器在解码过程中关注输入中的适当单词。您可以将编码器堆叠 N 次，以进一步对信息进行编码，其中每一层都有机会学习不同的注意力表示，因此有可能提高 transformer 网络的预测能力。</p><h1 id="1985" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">解码器层</h1><p id="49e1" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">解码器的工作是生成文本序列。解码器具有与编码器相似的子层。它有两个多头注意力层，一个逐点前馈层，和残差连接，以及每个子层之后的层归一化。这些子层的行为类似于编码器中的层，但是每个多头注意力层有不同的工作。解码器以线性层作为分类器，以 softmax 获得单词概率。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/7ecc7e0197ebb2b030009a4d753cedfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bXI7F0OiEtbtyy3k.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">解码器层。阅读时参考这张图表。</p></figure><p id="332a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解码器是自回归的，它以一个开始标记开始，并接受一个先前输出的列表作为输入，以及包含来自输入的注意信息的编码器输出。解码器在生成令牌作为输出时停止解码。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1aafb1becfe7b6d37a48beaf449fd636.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*u8nSpT8Z8ITwzNLV.gif"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">解码器是自回归的，因为它在先前的输出中被馈送时一次生成一个令牌 1。</p></figure><p id="1e6e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们来看一下解码步骤。</p><h1 id="778d" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">解码器输入嵌入和位置编码</h1><p id="f673" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">解码器的开始与编码器非常相似。输入经过嵌入层和位置编码层以获得位置嵌入。位置嵌入被送入第一个多头注意力层，该层计算解码器输入的注意力分数。</p><h1 id="aa32" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">解码器第一多头关注</h1><p id="9b1a" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">这个多头注意力层的运作方式略有不同。由于解码器是自回归的，并且一个字一个字地生成序列，所以需要防止它适应未来的标记。例如，当计算单词“am”的注意力分数时，您不应该访问单词“fine ”,因为该单词是在之后生成的未来单词。单词“am”应该只能访问它自己和它前面的单词。这对于所有其他的词都是如此，在那里他们只能注意前面的词。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/76543d6888ec6f6f0492783813000e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/0*0pqSkWgSPZYr_Sjx.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">对解码器的第一个多头注意力标度注意力分数的描述。“am”这个词，不应该为“fine”这个词取任何值。其他所有单词都是如此。</p></figure><p id="328f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要一种方法来防止计算未来单词的注意力分数。这种方法叫做掩蔽。为了防止解码器查看将来的令牌，您可以应用一个先行掩码。在计算 softmax 之前和缩放分数之后添加掩码。让我们来看看这是如何工作的。</p><h2 id="24e0" class="ms le iq bd lf mt mu dn lj mv mw dp ln kj mx my lr kn mz na lv kr nb nc lz nd bi translated">前瞻掩模</h2><p id="6ca5" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">掩码是一个矩阵，其大小与填充有 0 值和负无穷大值的注意力分数相同。当你把面具加到缩放的注意力分数上时，你会得到一个分数矩阵，右上角的三角形填充了否定的无穷大。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/8b51cfb83ac9cda685c0c2086ecc0213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QYFua-iIKp5jZLNT.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">向缩放后的分数添加前瞻掩码</p></figure><p id="ef59" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">屏蔽的原因是因为一旦你取屏蔽分数的软最大值，负的无穷大被清零，为未来的标记留下零注意分数。正如您在下图中所看到的，“am”的关注度得分本身及其之前的所有单词都有值，但单词“fine”的关注度得分为零。这实际上是告诉模型不要关注这些单词。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/8c0ca2fca61ee610d756fe019e6bcbdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3ykVCJ9okbgB0uUR.png"/></div></div></figure><p id="4594" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种屏蔽是在第一个多头注意力层中如何计算注意力分数的唯一区别。这一层仍然有多个头部，蒙版被应用，然后连接并通过一个线性层进行进一步处理。第一多头注意力的输出是带有关于模型应该如何关注解码器输入的信息的屏蔽输出向量。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/36d6dd003d33193cf30874c03605daee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Qos9ymoz4LW1pmP.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">带掩蔽的多头注意力</p></figure><h1 id="90dc" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">解码器二次多头关注，逐点前馈层</h1><p id="d682" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">第二个多头关注层。对于这一层，编码器的输出是查询和键，第一个多头关注层的输出是值。这个过程将编码器的输入与解码器的输入相匹配，允许解码器决定关注哪个编码器输入。第二多头注意力的输出通过逐点前馈层进行进一步处理。</p><h1 id="3454" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">输出概率的线性分类器和最终 Softmax</h1><p id="cabb" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">最终的逐点前馈层的输出通过最终的线性层，该线性层充当分类器。分类器和你拥有的类的数量一样大。例如，如果您有 10，000 个包含 10，000 个单词的类，则该分类器的输出大小为 10，000。分类器的输出然后被输入到 softmax 层，该层将产生 0 到 1 之间的概率分数。我们取最高概率分数的索引，它等于我们预测的单词。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a27ed0ab6f414b4cecbb92afadb8977e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/0*1OyVUO-s-uBh8EV2.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">使用 Softmax 获得输出概率的线性分类器</p></figure><p id="5956" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，解码器获取输出，将其添加到解码器输入列表中，并再次继续解码，直到预测到令牌。对于我们的情况，最高概率预测是分配给结束令牌的最后一个类。</p><p id="7c92" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解码器也可以堆叠 N 层，每层接收来自编码器及其前几层的输入。通过堆叠这些层，该模型可以学习从其注意力头部提取并关注不同的注意力组合，从而潜在地提高其预测能力。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/84c22bf34464b9e80d169ebffb666c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e6ZYxwBRleLgzfw8.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">堆叠编码器和解码器</p></figure><h1 id="d3fc" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">就是这样！</h1><p id="3820" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">就是这样！这就是变形金刚的机制。变形金刚利用注意力机制的力量做出更好的预测。递归神经网络试图实现类似的事情，但因为它们患有短期记忆。如果您想要编码或生成长序列，转换器可能会更好。因为有了 transformer 架构，自然语言处理行业才能取得前所未有的成果。</p><p id="6ecf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看<a class="ae kw" href="https://www.michaelphi.com/" rel="noopener ugc nofollow" target="_blank">michaelphi.com</a>了解更多类似的内容。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="1b3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">✍🏽想要更多内容？在 https://www.michaelphi.com 查看我的博客<a class="ae kw" href="https://www.michaelphi.com/build-your-own-deep-learning-machine-what-you-need-to-know/" rel="noopener ugc nofollow" target="_blank"/></p><p id="0dc4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">📺喜欢看基于项目的视频？看看我的<a class="ae kw" href="https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir"> Youtube </strong> </a>！</p><p id="a5d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">🥇注册我的<a class="ae kw" href="http://eepurl.com/gwy3hj" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">电子邮件简讯</strong> </a> <strong class="ka ir">，了解最新文章和视频！</strong></p></div></div>    
</body>
</html>