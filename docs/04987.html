<html>
<head>
<title>How to Write TensorFlow 2 Custom Loops</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何编写TensorFlow 2自定义循环</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/writing-tensorflow-2-custom-loops-438b1ab6eb6c?source=collection_archive---------19-----------------------#2020-04-30">https://towardsdatascience.com/writing-tensorflow-2-custom-loops-438b1ab6eb6c?source=collection_archive---------19-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e86f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从Keras到TensorFlow 2的分步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4bf518f53522dacaa2f9f01ace9e1380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5aCSxVE1bFpaNGhf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@georgeiermann?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Georg Eiermann </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">我使用Keras已经有一段时间了，我从来没有找到一个好的理由去其他地方。TensorFlow 1笨拙，PyTorch虽然性感，但不太懂我。我倾向于做原型的时候学的最多，Keras是这里的王者。快进到2019年9月，TensorFlow 2发布。不久之后，我把所有的<code class="fe me mf mg mh b">import keras</code>通话都切换到了<code class="fe me mf mg mh b">import tensorflow.keras</code>。</p><p id="1d2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果这篇文章只是关于从Keras迁移到TensorFlow 2，它可以在这里结束。真的。然而，在TensorFlow 2附带的所有功能中，自定义训练循环无疑是Keras用户的最佳新功能。在本文中，我解释了它们为什么重要以及如何实现。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="333d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">免责声明:</strong>我已经将所有代码添加为图片，因为它在智能手机上显示得更好。<a class="ae ky" href="https://gist.github.com/nuzrub/f1527654572b3e2da5125d0581e7bdad" rel="noopener ugc nofollow" target="_blank">所有代码都可以作为GitHub gist</a>获得。如果你知道或者更喜欢发布代码的其他替代方法，请写信给我。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="0f99" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">张量流2</h1><p id="d3ee" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">对于外行人来说，TensorFlow 2的问题在于，它抛弃了TensorFlow 1的大多数习惯用法和Keras上的几个API，Keras是唯一一个用于定义神经网络的API。此外，他们热衷于执行死刑。</p><p id="ef43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个变化一起解决了早期张量流的许多批评:</p><ol class=""><li id="cf96" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">不再有几个API。</strong></li><li id="4a15" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">不需要手动管理变量。</strong></li><li id="eddb" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">会话没了。</strong></li><li id="310a" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">可以调试执行流程。</strong></li><li id="d37c" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">动态模型现在是可能的。</li></ol><p id="6f57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与此同时，他们成功实现了这一点，同时保留了使TensorFlow 1闻名遐迩的部署能力和速度。</p><p id="ee86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于Keras用户来说，新的API意味着四件事:</p><ol class=""><li id="23f4" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">我们不再处于抽象层。</li><li id="8b5a" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">创作新技术要容易得多。</li><li id="6076" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">编写定制的训练循环现在是可行的。</li><li id="71e8" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">执行速度大大加快。</li></ol><p id="3db3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，自定义循环是TensorFlow 2对Keras用户如此重要的原因。定制循环提供了对训练的最终控制，同时使训练速度提高了约30%。</p><blockquote class="oa ob oc"><p id="5905" class="kz la od lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated">老实说，TensorFlow 2更好的名字应该是Keras 3。</p></blockquote><p id="d69c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道一些谷歌开发人员可能会在我写这篇文章的时候死去，但这是事实。TensorFlow 1是Keras的后端，在升级后基本保持不变。大多数新事物都比张量流更经典。</p><h1 id="4941" class="mp mq it bd mr ms oh mu mv mw oi my mz jz oj ka nb kc ok kd nd kf ol kg nf ng bi translated">训练循环</h1><p id="08b9" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">本质上，所有Keras计划都遵循以下结构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/816d0e18a2da839d260e569d492df16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phhW0uRLuCnhZW5Xn6RH3w.png"/></div></div></figure><p id="cb29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一切都从导入开始，然后是数据集加载和准备、模型加载/创建，最后是编译和拟合调用。Keras独有的编译方法将模型与损失函数和优化器相关联，而拟合函数执行所谓的“训练循环”</p><p id="ec65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练循环是将整个训练集逐批提供给算法、计算损失、其梯度并应用优化器的代码。然后，验证集被用于计算验证损失和验证指标。最后，整个过程重复几个“时期”</p><p id="bd59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练循环期间，fit方法还执行其他功能，例如操作几个工作线程、对模型进行检查点操作以及将结果记录到磁盘。自定义代码也可以通过回调在特定事件中插入。</p><blockquote class="oa ob oc"><p id="f426" class="kz la od lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated">Keras用户习惯于根本不用编写任何训练循环。fit函数完美地完成了这一切，并允许对大多数用例进行适当的定制</p></blockquote><p id="9c92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">固定训练循环的问题是，当不同的用例出现时，你就不走运了。最初的Keras很少提供定制的训练循环。</p><p id="543d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例子可以很容易地在GAN教程中找到。为了训练敌对网络，你必须交错他们的训练。为此，<code class="fe me mf mg mh b">train_on_batch </code>方法是交错生成器和鉴别器训练的最佳方法，让您手动批处理和混洗数据集，编写您的进度条形码，并将其打包到每个历元的For循环中。</p><p id="2511" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在2017年，<a class="ae ky" href="https://arxiv.org/abs/1701.07875" rel="noopener ugc nofollow" target="_blank"> Wasserstein GAN </a>被提出，它要求在训练中加入梯度裁剪，这是一个简单的要求。为了保持文明，Keras中的WGAN实现是臃肿的。</p><p id="ea60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些都不是孤立的例子。任何多网络设置、渐变技巧或开箱即用的解决方案都可能需要您编写大量代码。这就是为什么Keras在研究者中如此不受欢迎(也是为什么PyTorch如此受欢迎)。</p><h1 id="fdfc" class="mp mq it bd mr ms oh mu mv mw oi my mz jz oj ka nb kc ok kd nd kf ol kg nf ng bi translated">自定义循环</h1><p id="dc35" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">TensorFlow 2为Keras用户带来的是打开<code class="fe me mf mg mh b">train_on_batch </code>调用的能力，公开损失、梯度和优化器调用。然而，要使用它，您必须放弃编译和拟合功能。</p><p id="bbd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从好的方面来看，Keras不再是TensorFlow上的抽象。它现在是它的一部分了。这意味着我们不再需要在Keras中创建定制逻辑的所有奇怪的东西。一切兼容。我们不再需要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/30d562a102a0e2e5b03cf90fd09fa105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAjz82JaKSDjr0IZINmbcg.png"/></div></div></figure><p id="718b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，您必须自己获得损失和优化器对象。然后，定义train_on_batch调用。在我看来这很像PyTorch代码。你只需调用传递x的方法来得到ŷ，然后与y比较，得到损失值。</p><p id="7080" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一切都发生在梯度带的环境中，梯度带只是一种跟踪应该区分哪些操作的方法。使用磁带，我们可以得到关于每个训练变量损失的梯度。然后，梯度变量对被馈送到优化器，优化器将更新网络。</p><p id="5aad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一行只是一个例子，说明如何提取一批128个样本来调试我们的新方法。</p><p id="c403" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它的孪生兄弟<code class="fe me mf mg mh b">validate_on_batch</code>，只是它的一个简单版本。我们只需要摆脱胶带和梯度逻辑。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/b9880d1c0a77b21072938f54b4ec860c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWmKC7dXiCnCQiYA6iTsSw.png"/></div></div></figure><p id="dfe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，它是我们在渐变磁带中找到的相同代码，但是没有磁带(和渐变/优化器逻辑)。</p><p id="31a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">关于“训练=真”和“训练=假”参数的说明:</strong>这改变了一些层的行为。例如，<em class="od"> dropout </em>层在训练期间会丢弃一些连接，但在测试期间不会。这不会直接影响优化或任何其他与训练相关的任务。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="b40f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整训练循环的最小示例如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/8d47c8c88d7617b07851f2a4812577b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNTPtq7D08L6YCVLis9i3g.png"/></div></div></figure><p id="328f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至此，我们有了一个最小的工作示例。然而，它缺少几个基本特征:</p><ol class=""><li id="cfd1" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">当前的批处理逻辑笨拙且容易出错。</li><li id="3bff" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">除了每个时期打印一张以外，没有进度指示器。</strong></li><li id="c434" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">没有模型检查点逻辑。</strong></li></ol><p id="750a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这使我们想到以下一点:</p><blockquote class="oa ob oc"><p id="b9e2" class="kz la od lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated">做自己的循环很好，但是它也需要你重新编码一些你认为理所当然的特性</p></blockquote><p id="8737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谢天谢地，改进它并不难:)</p><h1 id="47ba" class="mp mq it bd mr ms oh mu mv mw oi my mz jz oj ka nb kc ok kd nd kf ol kg nf ng bi translated">改善环路</h1><p id="4fa6" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">首先要注意的是<code class="fe me mf mg mh b">tf.data</code>包。它包含tf.data.Dataset类，该类封装了几个数据集任务。</p><p id="e729" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集API使用“流畅的风格”这意味着对Dataset对象的所有调用都返回另一个Dataset对象。这使得链接调用成为可能。下面是一个用它来解决我们的问题的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/318ca3d84ad5ffedb79bf284aef4bef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmILJir9ZsK0oqCIYnYJQg.png"/></div></div></figure><p id="d1d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些调用为定型和测试数据创建一个dataset对象，为洗牌做准备，并对实例进行批处理。在我们的循环中，笨拙的批处理代码如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/f497dd8dc954ae319d2fe33d29a3df75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0d6Cn8j_hNoNUxYJzrqOw.png"/></div></div></figure><p id="1144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整洁多了。新代码只需枚举数据集中的所有批次。这也为我们处理洗牌，这总是一个好主意。</p><p id="9fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了让我们的代码看起来更加生动，我们可以添加一些打印语句。我们还可以使我们的验证逻辑更短一些:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/082ca4ef1072bd9a01d8c3026642d16e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWb-uGHSbNOdTncjYKFeZw.png"/></div></div></figure><p id="c46f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们打印当前的纪元和批次，以及一些点来保持它的移动。为了验证，我们创建了一个每批平均损耗的列表，然后打印出最终的平均损耗。这是它在控制台中的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/471f66827394873d6ffc257bb28fa1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzBJKf5351LGPBdsvQn97A.png"/></div></div></figure><p id="b2d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于模型检查点，我们必须跟踪到目前为止最好的验证准确性，以便我们只保存提高了我们性能的模型。这要求我们跟踪最佳损失，并将新的损失与之进行比较。就代码而言，这变成如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/8569909ece8b77fb7b94378c7fb0f4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKA2EDUVBFanLpusqyFFqw.png"/></div></div></figure><p id="2c6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这完成了我们的目标，即改进数据集处理，看到更频繁的屏幕更新，并在训练时保存模型检查点。</p><p id="148e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一步是让它具有性能。</p><h1 id="1612" class="mp mq it bd mr ms oh mu mv mw oi my mz jz oj ka nb kc ok kd nd kf ol kg nf ng bi translated">tf .函数</h1><p id="66ab" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">到目前为止，我们正在所谓的“渴望模式”下运行这意味着如果你写“a + b ”,求和的结果会立即计算出来。虽然这简化了调试，但不是性能最好的方法。</p><p id="a8f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习发生在GPU上。每个“求和”和“相乘”命令都有成本。CPU必须调用GPU并告诉哪些变量需要操作，等待操作完成，并取回结果。<em class="od">这是慢</em>。一种更快的方法是给GPU一大堆要做的事情，并且只等待一次。</p><p id="e885" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种替代方法被称为“延迟模式”或“计算图”这个想法是让TensorFlow把你的网络变成一组对数据进行运算的数学步骤。这个命令列表再发送到GPU，整体处理，速度快很多。</p><p id="1c39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建命令列表对于优化也是至关重要的。InceptionNet等模型有多条路径，可以并行计算。简单的运算可以合并，例如乘法后接加法，等等。</p><p id="acff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要让TensorFlow为您构建这个图，您只需要用@tf.function注释来注释<code class="fe me mf mg mh b">train_on_batch </code>和<code class="fe me mf mg mh b">validate_on_batch </code>调用。就这么简单:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/92401a1d4599f85a009073e21a1fb368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjUqwcAsV1Fa-5QZRnycag.png"/></div></div></figure><p id="5ee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一次调用这两个函数时，TensorFlow将解析其代码并构建关联的图。这将比平常花费更长的时间，但会使所有后续的调用明显更快。</p><p id="33cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个很好的方法是在里面放一个print语句。在计算图形构建期间，打印将仅执行一次或两次。然后，它将不再打印，因为该函数不再被调用。</p><p id="79f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在数值上，使用RTX 2070 GPU，原始Keras fit函数需要18秒，自定义循环需要40秒，优化循环需要20秒。</strong>这个简单的注释使它的速度比急切模式快了一倍。与Keras fit相比，它慢了2秒，显示了原始fit的优化程度。对于较大的问题和网络，优化的自定义循环优于原始拟合。在实践中，我发现使用定制循环可以提高30%的纪元速度。</p><p id="a34b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用@tf.function注释的缺点是它的错误消息很糟糕。经验法则是先在没有它的情况下开发，然后添加它只是为了验证，然后再添加到培训中。这样，您可以更容易地确定错误。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="0cf7" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">后续步骤</h1><p id="b1b4" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">切换到自定义循环的用户正在不断改进他们的工作流程。例如，我已经在我的自定义代码中实现了回调和度量，以及一个漂亮的进度条。在互联网上，你可以找到一些软件包，如<a class="ae ky" href="https://pypi.org/project/tqdm/" rel="noopener ugc nofollow" target="_blank"> TQDM </a>或非常Keras <a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/Progbar" rel="noopener ugc nofollow" target="_blank">进度条</a>。</p><p id="2a1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实现自定义循环看起来像是额外的工作，但是它很快就有了回报，并且您只需要做一次。它允许您完全控制何时进行验证、计算哪些指标、复杂的培训计划等等。就模型而言，篡改训练过程要容易得多。您可以添加梯度惩罚，训练几个模型，或轻松创建虚拟批次。</p><p id="55e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，PyTorch用户多年来一直在这么做。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="0b50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我把如何实现这些留给了你。如果你有任何问题，欢迎在评论区提问。</p><p id="28bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我离开之前，有一个好消息即将发布。T <strong class="lb iu"> ensorflow 2.2将允许您将自己的</strong> <code class="fe me mf mg mh b"><strong class="lb iu">train_on_batch </strong></code> <strong class="lb iu">和</strong> <code class="fe me mf mg mh b"><strong class="lb iu">validate_on_batch </strong></code> <strong class="lb iu">函数反馈给原始函数。安装API </strong>。这意味着我们将两全其美。fit调用将更加模块化，同时我们保留从头开始实现它的可能性。</p><p id="32e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的代码可以在<a class="ae ky" href="https://gist.github.com/nuzrub/f1527654572b3e2da5125d0581e7bdad" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="626a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新:我最近发表了一份关于TensorFlow和Keras的技巧列表。在那里，我描述了如何创建定制的训练循环，而不需要重新实现整个。适合功能。</strong></p><div class="ow ox gp gr oy oz"><a rel="noopener follow" target="_blank" href="/taking-keras-and-tensorflow-to-the-next-level-c73466e829d3"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">让Keras和TensorFlow更上一层楼</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">充分利用Keras和TensorFlow的11个技巧和诀窍</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">towardsdatascience.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ks oz"/></div></div></a></div><p id="1c5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对本文有任何问题，请随时发表评论或与我联系。如果你刚接触媒体，我强烈推荐<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">订阅</a>。对于数据和IT专业人士来说，中型文章是<a class="ae ky" href="https://stackoverflow.com/" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>的完美搭档，对于新手来说更是如此。注册时请考虑使用<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">我的会员链接。</a></p><p id="8fbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读:)</p></div></div>    
</body>
</html>