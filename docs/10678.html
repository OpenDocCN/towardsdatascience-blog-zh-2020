<html>
<head>
<title>A Quick Guide to Activation Functions In Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中激活函数的快速指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-guide-to-activation-functions-in-deep-learning-4042e7addd5b?source=collection_archive---------13-----------------------#2020-07-26">https://towardsdatascience.com/a-quick-guide-to-activation-functions-in-deep-learning-4042e7addd5b?source=collection_archive---------13-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="94d9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习中所有主要激活功能的快速讨论。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/137b8ddf79edb40b8215f739fc616a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZ31SfQM3BjuWIbdJcXF_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="f4cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将给出一个在神经网络中使用的一些最常见的激活函数的快速概述。但在讨论它们之前，让我们先讨论一些与激活函数相关的基本概念。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="07a1" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">内容-</h1><ol class=""><li id="7e54" class="mt mu it la b lb mv le mw lh mx ll my lp mz lt na nb nc nd bi translated">为什么我们需要激活函数？</li><li id="9436" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">为什么我们总是在神经网络中选择非线性激活函数？</li><li id="16b0" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">通用逼近定理</li><li id="3deb" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">消失和爆炸梯度问题</li><li id="7b41" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">激活功能-</li></ol><ul class=""><li id="c7bb" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt nm nb nc nd bi translated">Sigmoid 函数</li><li id="8184" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">Softmax 函数</li><li id="5946" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">双曲正切值</li><li id="e134" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">热卢</li><li id="43c3" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">泄漏 Relu</li><li id="0491" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">嗖嗖</li><li id="f02d" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">参数化 Relu</li><li id="efaf" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">ELU</li><li id="6cea" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">Softplus 和 Softsign</li><li id="d3c5" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">卢瑟</li><li id="24ee" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">格鲁</li><li id="eb35" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">线性激活函数</li></ul><p id="a044" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.如何决定应该使用哪个激活函数？</p><p id="d054" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.结论</p><p id="6f95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8.信用</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="97ed" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">为什么我们需要激活函数？</h1><p id="ecb1" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">正如我们所知，在人工神经元中，输入和权重是给定的，由此计算出输入的加权和，然后将其提供给激活函数，该函数将其转换为输出。所以基本上激活函数被用来将输入映射到输出。这种激活功能有助于神经网络学习数据中的复杂关系和模式。现在的问题是，如果我们不使用任何激活函数，让一个神经元给出输入的加权和作为输出，会怎么样。在这种情况下，计算将非常困难，因为输入的加权和没有任何范围，根据输入，它可以取任何值。因此，激活功能的一个重要用途是将输出限制在特定范围内。激活函数的另一个用途是在数据中添加非线性。我们总是选择非线性函数作为激活函数。让我们看看，为什么它很重要。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="468f" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">为什么我们总是在神经网络中选择非线性激活函数？</h1><p id="3306" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">非线性意味着不能从输入的线性组合中产生输出。非线性在神经网络中很重要，因为线性激活函数不足以形成通用函数逼近器。如果我们在深度神经网络中使用线性激活函数，无论我们的网络有多深，它都将等同于没有隐藏层的线性神经网络，因为这些线性激活函数可以组合起来形成另一个单一的线性函数。因此，基本上我们的整个网络将被简化为一个单一的神经元，以组合的线性函数作为其激活函数，而这个单一的神经元将无法学习数据中的复杂关系。由于大多数现实世界的问题非常复杂，我们需要神经网络中的非线性激活函数。没有非线性激活函数的神经网络将只是一个简单的线性回归模型。</p><p id="9609" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在神经网络的最后一层，我们可以选择线性激活函数。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="5be6" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">通用逼近定理</h1><p id="3ef3" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">直接引用维基百科-</p><blockquote class="nq nr ns"><p id="1481" class="ky kz nt la b lb lc ju ld le lf jx lg nu li lj lk nv lm ln lo nw lq lr ls lt im bi translated">普适逼近定理指出，由人工神经元构成的前馈网络可以很好地逼近 Rⁿ.紧集上的任意实值连续函数</p></blockquote><p id="dd58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简而言之，这个定理表明神经网络可以学习任何连续函数。现在的问题是是什么让它这样做。答案是激活函数的非线性。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="7011" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">消失和爆炸梯度问题-</h1><p id="a664" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">在反向传播期间的神经网络中，每个权重接收与误差函数的偏导数成比例的更新。在某些情况下，这个衍生项非常小，以至于更新非常小。特别是在神经网络的深层，通过各种偏导数的相乘来获得更新。如果这些偏导数非常小，那么总的更新变得非常小，接近于零。在这种情况下，权重将不能更新，因此将有缓慢或没有收敛。这个问题被称为消失梯度问题。</p><p id="9064" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，如果导数项非常大，那么更新也将非常大。在这种情况下，算法将超过最小值，并且不能收敛。这个问题被称为爆炸梯度问题。</p><p id="b8e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有各种方法可以避免这些问题。选择合适的激活函数就是其中之一</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="6c36" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">激活功能-</h1><h1 id="66db" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">1.Sigmoid 函数-</h1><p id="ce6c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">Sigmoid 是一个“S”形数学函数，其公式为-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/24c0e0aa76aff34327eacc39634f1549.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*wBSZtcLmG8HVtUZRGIIllA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="fe11" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是 sigmoid 函数的图表。你一定在学习逻辑回归的时候遇到过这个函数。尽管 sigmoid 函数非常受欢迎，但由于以下原因，它并不常用</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/e460e37fc1774fbbd6572f2f2dc92756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5IxgCMTQlym0Q9zk1PtmcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乙状结肠函数(来源:维基百科)</p></figure><p id="8cf5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="7b1b" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">Sigmoid 函数是连续且可微的。</li><li id="0f62" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">它将输出限制在 0 和 1 之间</li><li id="77b4" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">非常明确的二进制分类预测。</li></ol><p id="efa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="7e05" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">它会导致渐变消失的问题。</li><li id="9093" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">它不是以零为中心。</li><li id="1aa9" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">计算成本高</li></ol><p id="7452" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f740d8ac203e1d608384aa503f971890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*O1behrlyt_wV5HCIyvibSg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="7469" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">2.Softmax 函数-</h1><p id="b0e0" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">Softmax 函数是 sigmoid 函数对多类设置的推广。它通常用于多类分类的最后一层。它取一个“k”实数的向量，然后将其归一化为一个概率分布，该概率分布由对应于输入数的指数的“k”个概率组成。在应用 softmax 之前，一些向量分量可能是负的，或者大于 1，并且可能不等于 1，但是在应用 softmax 之后，每个分量将在 0-1 的范围内，并且将等于 1，因此它们可以被解释为概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/22af106a49feffaac2eaccde4b319836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O08KhNyCbhH_p-OWACfVFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="1f23" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><p id="748e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它可用于多类分类，因此可用于神经网络的输出层。</p><p id="3798" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><p id="4d8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这在计算上是昂贵的，因为我们必须计算大量的指数项。</p><p id="d1fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8a7364561263f24067d149598701ca10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*EXDgiAzdD1VHPy1V1SJ9Fw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="5576" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">3.双曲正切值</h1><p id="92d4" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">双曲正切或简称为“tanh”由以下各项表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/86dd96a032b0068edb068099d266372f.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*ULrk38EawEuoIjRzjD6TQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/fbf86d3245974960e4eefee4bf2de171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*g1vNhvRrkYa2On8kKvR-9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6951" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它非常类似于 sigmoid 函数。它以零为中心，范围在-1 和+1 之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/fc6ef94f163bab258ae7712eafe7c878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*AAqhecH32SUIFsQDPqK78g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="888c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="ac13" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">它在任何地方都是连续且可微的。</li><li id="682a" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">它以零为中心。</li><li id="0c9f" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">它会将输出限制在-1 到+1 的范围内。</li></ol><p id="55ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="9f28" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">它会导致渐变消失的问题。</li><li id="837a" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">计算开销很大。</li></ol><p id="3708" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/c529403e7ca9bf9a8a49fb853f304e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UymxFEQpw5k5xD2F_4kWVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="3449" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">4.Relu-</h1><p id="304a" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">整流线性单元通常称为整流器或 relu</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5d195eb4fa398aaf6ab3aa31536413c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*oZSX8ld1oVP4HNTS1c0-tA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/20f621b5cc3b1689d113a7b74f5947d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0sRHFlV5bPxtVdFkT_YYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像<a class="ae om" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8dad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="978a" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">容易计算。</li><li id="36a7" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不会导致渐变消失的问题</li><li id="11b1" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">由于不是所有的神经元都被激活，这造成了网络的稀疏性，因此它将是快速而有效的。</li></ol><p id="8882" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="cde7" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">导致爆炸梯度问题。</li><li id="6251" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不在零中心。</li><li id="c70b" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">可以永远杀死一些神经元，因为它总是给负值 0。</li></ol><p id="4e99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/79a7c481461108d7bd1dd09c5011f344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*O-oalZMsWPUUdJN2w4ebQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ce2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了克服 relu 激活中的爆炸梯度问题，我们可以设置它的饱和阈值，即函数将返回的最大值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b596f805726031798e11d5756eda29e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*IDXzYS6lvajEaomJLhXGdA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/bf2619bda9f71c497ac202b95211d6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*gK_o5A3p8cH2Qxsdf0WAKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片<a class="ae om" href="https://pin.it/7KHrknq" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="e706" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">5.泄漏的 Relu-</h1><p id="140c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">漏 relu 是 relu 函数的改进。Relu 函数在每次迭代中会杀死一些神经元，这就是所谓的死亡 relu 条件。Leaky relu 可以克服这个问题，它不会给负值 0，而是使用相对较小的输入分量来计算输出，因此它永远不会杀死任何神经元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f241190695deb6bcc2a4dddce30d52cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*pNY9VG7pNRnc9Pm0IoVTOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ff355cdf8b2858c2c21f57f487b64636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*YRcV4kjXInKG_o6k2M3gSg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片<a class="ae om" href="https://www.i2tutorials.com/explain-step-threshold-and-leaky-relu-activation-functions/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0793" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="c7e2" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">容易计算。</li><li id="0973" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不会导致渐变消失的问题</li><li id="ce30" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不会导致将死的问题。</li></ol><p id="1826" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="6566" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">导致爆炸梯度问题。</li><li id="83c2" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不在零中心</li></ol><p id="810c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/83abde91a04c6c7071bf916289e83f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kHyTu54Bt1l5QxdJAsh9ew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="fbd6" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">6.参数化 Relu-</h1><p id="ffbd" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">在参数化 relu 中，它不是为负轴固定一个速率，而是作为一个新的可训练参数传递，网络可以自行学习以实现更快的收敛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/975ef2af44a2448b48d4d68843b264ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*EJF4Y4rzCEFReO7eUk1nGA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="429e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="42cc" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">网络将自己学习最合适的α值。</li><li id="2384" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不会导致渐变消失的问题</li></ol><p id="efcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="8d54" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">难以计算。</li><li id="49a5" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">性能取决于问题。</li></ol><p id="8511" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 Tensorflow 中，参数化 relu 被实现为自定义层。示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/0a627c8d5fa0d47bc088a47d6103a470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGDQ_N3dxd529qxgSXJk2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="2f91" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">7.嗖嗖-</h1><p id="a868" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">swish 函数是通过 x 乘以 sigmoid 函数得到的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/7082f9e4734dc222eff7ee0579390a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*l7hp_RkOCyQ9aIi929PCIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/61c41ea99e7c4763a85d960cd21068ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*bCO5p7a1BPvE9BQCoxss0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像<a class="ae om" href="https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820" rel="noopener">来源</a></p></figure><p id="35b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">swish 功能是谷歌大脑团队提出的。他们的实验表明，在几个具有挑战性的数据集上，swish 往往比 Relu 的深度模型工作得更快。</p><p id="02b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="797e" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">不会导致渐变消失的问题。</li><li id="ae52" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">被证明比 relu 略胜一筹。</li></ol><p id="66a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><p id="0684" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">计算成本高</p><h1 id="bf42" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">8.ELU-</h1><p id="29f0" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">指数线性单元(ELU)是 relu 的另一种变体，它试图使激活接近零，从而加快学习速度。它比 relu 具有更好的分类精度。elu 具有负值，使得激活的平均值更接近于零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/37b6cee3055ab902b5e74859f6215796.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*2UgI4CJ8whHq5jyya_ZCCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="a3f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="cddd" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">不会导致将死的问题。</li></ol><p id="b642" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="5675" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">计算开销很大。</li><li id="a6c5" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">没有避免爆炸梯度问题。</li><li id="5b83" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">阿尔法值需要决定。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/37d359ed9b4564f35d14aad89da4854b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vG39S7lVQY7jkqT10vhM5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="e92d" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">9.Softplus 和 Softsign</h1><p id="bd5a" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">Softplus 功能是-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2d6b024f713dd869b3b53914f8c9a5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*ZKMIazoF7TYVwBrIGsw7wQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="bb55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它的导数是一个 sigmoid 函数。</p><p id="5495" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">软设计功能是-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/172e29c3267965ecad1df9efabe51949.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*WfXujWzrEiaMBtBZaqMvfA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像<a class="ae om" href="https://keras.io/api/layers/activations/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="497f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Softplus 和 Softsign 用得不多，一般而言，relu 及其变体比它们更受欢迎。</p><p id="8b92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="b4b6" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">不会导致渐变消失的问题。</li></ol><p id="e48c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点是</p><ol class=""><li id="5250" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">计算开销很大。</li><li id="64ba" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">比 Relu 慢。</li></ol><p id="8cfe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/ed963e1bbdefecf8021b784c9f810d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38gMiAQJuctT04CwSBrCdg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/01a0f6eff484c120d94c5ef19fa479e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*ohV1gIhkzz5mmffT34RzdQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="0b7f" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">10.卢瑟-</h1><p id="c21b" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">卢瑟代表比例指数线性单位。卢瑟被定义为-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/7d2040b80b67d14f9ee5379d4200251c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXG3d332isdKP3PHpAZHHA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片<a class="ae om" href="https://keras.io/api/layers/activations/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8a53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中 alpha 和 scale 是常量，其值分别为 1.673 和 1.050。选择 alpha 和 scale 的值，使得只要权重被正确初始化，输入的均值和方差在两个连续层之间保持不变。卢瑟被证明比 relu 更好，并具有以下优点。</p><p id="07c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优点-</p><ol class=""><li id="a88c" class="mt mu it la b lb lc le lf lh nj ll nk lp nl lt na nb nc nd bi translated">不会导致渐变消失的问题。</li><li id="1d98" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">不会导致死亡的问题</li><li id="edeb" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">比其他激活功能更快更好。</li></ol><p id="3378" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/dcf4eaa3e521c53041936d3656f0f78b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vx1yRIcTtPDZrp8Hq4ZQwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="2750" class="mb mc it bd md me nx mg mh mi ny mk ml jz nz ka mn kc oa kd mp kf ob kg mr ms bi translated">11.线性激活-</h1><p id="7b52" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">正如我们之前讨论的，我们应该在神经网络中使用非线性激活函数。然而，在用于回归问题的神经网络的最后一层，我们可以使用线性激活函数。</p><p id="f7b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/03a56e9d03d5ee636978026cf5de8287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*X4zTjTwWecnorS4vWD5X8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="10ae" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">如何决定应该使用哪个激活函数</h1><ul class=""><li id="2adc" class="mt mu it la b lb mv le mw lh mx ll my lp mz lt nm nb nc nd bi translated">由于消失梯度问题，应避免使用 Sigmoid 和 tanh。</li><li id="bc8f" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">还应避免使用 Softplus 和 Softsign，因为 Relu 是更好的选择。</li><li id="8498" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">对于隐藏层，应该首选 Relu。如果它导致了将死的 relu 问题，那么应该使用它的修改版本，如 leaky relu、elu、SELU 等。</li><li id="5aa6" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">对于深度网络，swish 比 relu 表现更好。</li><li id="939f" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt nm nb nc nd bi translated">对于回归的最终图层，线性函数是正确的选择，对于二元分类，sigmoid 是正确的选择，对于多类分类，softmax 是正确的选择。在自动编码器中应该使用相同的概念。</li></ul></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="05ae" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">结论-</h1><p id="d9ec" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh nn lj lk ll no ln lo lp np lr ls lt im bi translated">我们已经讨论了所有流行的激活函数及其优缺点。还有很多激活功能，但我们并不经常使用。我们也可以定义我们的激活函数。这里讨论的一些激活函数从未用于解决现实世界的问题。他们只是为了知识。大多数情况下，对于隐藏层，我们使用 relu 及其变体，对于最终层，我们根据问题的类型使用 softmax 或线性函数。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="99b4" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">学分-</h1><ol class=""><li id="99f9" class="mt mu it la b lb mv le mw lh mx ll my lp mz lt na nb nc nd bi translated"><a class="ae om" href="https://keras.io/api/layers/activations/#layer-activation-functions" rel="noopener ugc nofollow" target="_blank">https://keras . io/API/layers/activations/# layer-activation-functions</a></li><li id="3509" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" href="https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/</a></li><li id="5061" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" rel="noopener" target="_blank" href="/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253">https://towards data science . com/everything-you-need-know-to-know-about-activation-functions-in-deep-learning-models-84ba 9 f 82 c 253</a></li><li id="9591" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" rel="noopener" target="_blank" href="/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a">https://towards data science . com/comparison-of-activation-functions-for-deep-neural-networks-706 AC 4284 c8 a</a></li><li id="d1c4" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" rel="noopener ugc nofollow" target="_blank">https://missing link . ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/</a></li><li id="eab0" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Rectifier _(neural _ networks)</a></li><li id="fd94" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" href="https://mlfromscratch.com/activation-functions-explained/#/" rel="noopener ugc nofollow" target="_blank">https://mlfromscratch.com/activation-functions-explained/#/</a></li><li id="94c6" class="mt mu it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated"><a class="ae om" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Universal _ approximation _ theory</a></li></ol></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/0c360aed485eff919f6ae4bab1abf42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xl8DCBSCMbwjGHIeFJZQaA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="3bbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那都是我这边的。感谢阅读这篇文章。使用的少数图片的来源被提及，其余的是我的创作。请随意发表评论，建议改正和改进。通过 Linkedin 与我联系，或者你可以给我发邮件，地址是 sahdevkansal02@gmail.com。我期待听到您的反馈。查看更多这样的文章我的媒体简介。</p></div></div>    
</body>
</html>