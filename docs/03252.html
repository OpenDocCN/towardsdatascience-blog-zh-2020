<html>
<head>
<title>An Artificial Intelligence Learns to Play Battleship</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个人工智能学会玩战舰</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01?source=collection_archive---------16-----------------------#2020-03-28">https://towardsdatascience.com/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01?source=collection_archive---------16-----------------------#2020-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1dc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在你训练计算机玩游戏之后，看它玩游戏可能比使用机器学习模型来进行预测更有价值。在强化学习的背景下，我们可以训练一个人工智能(一台笔记本电脑)来做决定，并赋予它能力，使其成为适应不断变化的情况的“终身学习者”。在这篇文章中，我分享了我在强化学习领域的早期步骤，其中包括从头开始编码，以理解使用OpenAI Gym进行更高级和标准化的强化学习方法的基本概念。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/934fefc000ede44ae90ad6b3373ef0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/0*fLd5AC3RRkQWCBa6"/></div></figure><figure class="kq kr ld kt ku kv kw paragraph-image"><img src="../Images/6c647d5df4b0a7342f7b01181aded999.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/1*hic3ECF1DGu7F63EhR1aFQ.gif"/><p class="le lf gj gh gi lg lh bd b be z dk li di lj lk translated">一台电脑在一个简化的5x5网格上玩战舰游戏，其中一艘隐藏的船占据3个单元。顶部网格是计算机的计分板。底部的格子对电脑来说是隐藏的，它显示了船在每场游戏开始时被随机放置的位置。“X”代表得分板上的命中，而“O”代表未命中。</p></figure></div></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="cd36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个前所未有的生活在冠状病毒统治下的时期，我被困在家里，这给了我完成这篇文章的机会。这篇文章是关于我学习和编码强化学习(RL)元素的经验，这是教一台机器如何玩<a class="ae ls" href="https://en.wikipedia.org/wiki/Battleship_(game)" rel="noopener ugc nofollow" target="_blank">战舰</a>所需要的。我选择战舰的原因是因为在我看来，编写它的框架很好地解决了第一次尝试RL时可能遇到的挑战。此外，编写人类可能用来赢得游戏的策略代码，然后将这个策略与机器提出的策略进行比较，也相对容易。当我第一次开始阅读强化学习时，我并不期望它与机器学习有那么大的不同，但事实是，设置教机器在特定情况下行动需要不同的方法和一些编程工作。事实上，用于预测建模的机器学习总会有一个库来帮助。相反，机器“教学”需要你思考如何适当地奖励你的机器，以便未来采取的任何行动都更像人类。此外，在大多数情况下，你没有任何数据可以开始，你设计机器所处“情况”的方式和机器采取的行动将在尽可能快地教会机器时产生重大影响。</p><p id="77c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将遵循两种方法:</p><ul class=""><li id="6714" class="lt lu iq jp b jq jr ju jv jy lv kc lw kg lx kk ly lz ma mb bi translated">从头开始编码并使用线性模型来近似代理学习；</li><li id="1f0d" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">使用OpenAI健身房库和更先进的近似模型。</li></ul><p id="b65f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在这里保留了第一种方法，因为它是我学习经验的一部分，理解任何更高级的RL方法的基础是很重要的。读者可以随意跳过它。同样，对于RL更详细的回顾，读者可以看<a class="ae ls" rel="noopener" target="_blank" href="/reinforcement-learning-101-e24b50e1d292">这里</a>或者<a class="ae ls" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-dp-0262039249/dp/0262039249/ref=mt_hardcover?_encoding=UTF8&amp;me=&amp;qid=" rel="noopener ugc nofollow" target="_blank">这里</a>。在本文中，我将只涉及几个关键术语和方程。最后，我没有花太多时间来改进本文发布的代码，而是把它作为我在RL的学习经历。</p><h2 id="5254" class="mh mi iq bd mj mk ml dn mm mn mo dp mp jy mq mr ms kc mt mu mv kg mw mx my mz bi translated">目录。</h2><ul class=""><li id="d42e" class="lt lu iq jp b jq na ju nb jy nc kc nd kg ne kk ly lz ma mb bi translated"><a class="ae ls" href="#0fbd" rel="noopener ugc nofollow">战舰快速强化学习。</a></li><li id="7ca1" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">类人策略与完全随机策略。</li><li id="72cd" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated"><a class="ae ls" href="#4331" rel="noopener ugc nofollow">问——学习最佳行动。</a></li><li id="5742" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated"><a class="ae ls" href="#99a4" rel="noopener ugc nofollow">使用Q学习和线性模型的代理训练。</a></li><li id="fffd" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated"><a class="ae ls" href="#673a" rel="noopener ugc nofollow">使用OpenAI Gym通过神经网络进行Q学习。</a></li><li id="e4be" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated"><a class="ae ls" href="#3080" rel="noopener ugc nofollow">结论。</a></li><li id="60a1" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated"><a class="ae ls" href="#89b2" rel="noopener ugc nofollow">代码。</a></li></ul></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="05ee" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">战舰快速强化学习。</h1><p id="3cce" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">在每场比赛或<strong class="jp ir">一集</strong>的开始和比赛期间，每个球员或<strong class="jp ir">经纪人</strong>看到自己的计分板。计分板代表玩家的<strong class="jp ir">环境</strong>。在任何时候，具有给定数量的命中、未命中和空单元的板配置代表代理“感知”的<strong class="jp ir">状态</strong>。当所有船单元被击中时，代理处于<strong class="jp ir">终止状态</strong>。代理从当前状态做出的任何可能的移动都是一个<strong class="jp ir">动作</strong>。代理因采取任何行动而获得<strong class="jp ir">奖励</strong>。这可以是一个正数或负数，它的定义需要在算法上使代理学习。在我的第一个方法中，我使用了下面的定义来分配奖励。当我使用第二种方法时，我将改变这个奖励方案。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/5a72aeb59b1f3229c3b28c23755a2618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsNQttRN_55MLVskEIT5GQ.png"/></div></div></figure><p id="e57d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了避免懒惰的学习者，代理人在进行非法移动时利用的每个随机动作都会被分配一个惩罚。对于一次击中，会分配一个正的奖励，但会根据自上次击中以来这次击中所花费的数量进行折扣。事实上，这个想法是让代理人学会在击中目标后关注哪里，并防止它四处游荡。</p><p id="c354" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">行动价值函数</strong> Q(s，a)是代理人通过执行行动𝑎从𝑠.获得的预期收益(或未来报酬)q用于学习代理赢得游戏所需遵循的最佳<strong class="jp ir">政策</strong>或策略。最好的(真正的)政策是未知的<em class="kl">先验</em>。实际上，近似方法是用来获得一个最优策略的，它可能离最优策略很远。近似Q的模型可以被认为是代理“大脑”。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="151d" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">类人策略与完全随机策略。</h1><p id="80dc" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">为了简单起见，我们先问一个问题:“在一艘船(叫做巡洋舰)占据一个棋盘上三个格子的情况下，我会怎么玩战舰？”我随机呼叫一些位置，直到我击中一个船细胞或缩小位置的数量，这样我就可以比随机移动更聪明。</p><p id="33bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以对这种方法进行编码，并为训练有素的代理创建一个基准:</p><ul class=""><li id="3dfc" class="lt lu iq jp b jq jr ju jv jy lv kc lw kg lx kk ly lz ma mb bi translated">循环遍历所有空单元，并制作一个计数矩阵，描述在每个位置有效装运单元可能出现的次数。具有最高计数的单元将是下一个动作。</li><li id="95a9" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">如果命中已经存在，只考虑它周围的非空单元格。</li></ul><div class="km kn ko kp gt ab cb"><figure class="kq kr oa kt ku kv kw paragraph-image"><img src="../Images/d84cfe6539fac4d0298a2acffecaf31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*DbDPGCcnOsiIJyIaj5qI3w.png"/></figure><figure class="kq kr ob kt ku kv kw paragraph-image"><img src="../Images/4daea06cc3245d8524530f1545818c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*cj8gC8obOwCiNU2DQ5Y93Q.png"/><p class="le lf gj gh gi lg lh bd b be z dk oc di od lk translated">(左)一个完全随机的代理人需要在5x5的棋盘上走25步才能在12%的游戏中完成一个游戏(击中一艘有3个单元的船)(在这个例子中是100，000)。(右)一个类似人类的代理人平均需要在10x10的棋盘上走18步才能完成一场游戏(击中3个细胞的船)6-8%的时间。代理将从不需要超过大约40次移动(0.4个网格大小)。</p></figure></div><p id="c2e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据经验，一个类似人类的策略平均需要大约0.2倍的网格单元数量来完成一个游戏:10x10的网格需要20步，8x8的网格需要13步等等。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="cabe" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">q-学习最佳行动。</h1><p id="d9b5" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">一个代理从它与环境的交互历史中学习最优策略。代理学习需要在算法中实现。在本文中，我们将使用Q-learning，这是一种基于值的学习算法。Q值是假设代理处于状态<em class="kl"> s </em>并执行动作<em class="kl"> a. </em>的整体预期回报的度量</p><p id="40bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q-learning是<em class="kl">非策略</em>，即代理评估并改进一个<em class="kl">估计</em> <em class="kl">策略</em>，同时使用一个<em class="kl">行为</em> <em class="kl">策略</em>(例如ε-贪婪)来驱动进一步的学习，从而从当前策略之外的动作中学习。估计策略的核心是逼近Q的函数，该函数使用下一状态的Q值来更新。这个下一个状态是通过做出有最大回报的(贪婪)行为而达到的。如果我们回忆一下Q学习方程式</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/d14c75e5d1941a5a6803be7c53e30745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPOGY-JbVaoDqv_-tjuLuw.png"/></div></div></figure><p id="14a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该术语</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/6216fa8c63ef3d7d10af7274379424a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*go__XhdHlSFMCuh0lcNI5w.png"/></div></div></figure><p id="e5f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑠′州的𝑎′行动是q(𝑠′最大化吗？正如我们所看到的，我们下一步采取什么行动并不重要，目标是一样的:采取使q最大化的行动。</p><p id="0d56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像SARSA这样的<em class="kl">基于策略的</em>学习方法意味着在更新Q函数之后继续遵循当前策略。如果我们回忆一下萨尔萨方程</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/4dd97a129da144fe43c8443b717673ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*leD90mWlC8FYhHo_VDD6DQ.png"/></div></div></figure><p id="c50c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑎′下一步的行动将遵循目前的政策。</p><p id="ca8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据Q-learning训练代理的伪代码是</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/48474f305924f658616a0a4476d6da8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8VPSPD1_dgEB5Vmwy6SMew.png"/></div></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="da48" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">使用Q学习和线性模型的智能体训练。</h1><p id="b2a4" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">一个经过最佳训练的代理人将遵循一个估计的策略，同时受行为(ε贪婪)策略的“残余贪婪”的驱动，这是保证足够的探索和缩小产生最大回报的可能行动所需要的。ε-贪婪策略允许以固定速率ε对下一个动作进行随机猜测。遵循估计策略的行动预计会产生最高的回报。在训练代理时为epsilon找到合适的值有助于代理在训练结束后获得最佳策略。</p><p id="5f78" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当用ε-贪婪策略训练代理时，有两种选择:</p><ol class=""><li id="98e1" class="lt lu iq jp b jq jr ju jv jy lv kc lw kg lx kk og lz ma mb bi translated">将ε固定为所有训练集的值；</li><li id="7dc9" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk og lz ma mb bi translated">使ε随时间衰减，直到达到最小值。</li></ol><p id="f783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第一种情况下，当训练结束时，代理将优化其策略，保持探索的速率ε。因此，即使它的估计策略可能是完美的，代理也将以速率1-ε使用它，同时以速率ε维持随机选择。</p><p id="8f25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第二个场景中，我们依赖于这样一个事实，即随着时间的推移，代理将已经学习到足够多的知识，以便它可以做出越来越好的估计移动，以有限次数的随机猜测击中敌船(获得更高的总奖励)，其概率保持在最小ε值。需要这个最小ε来保证对代理的足够探索。事实上，随机移动有助于缩小敌舰的可能位置。有人可能会说，未经训练的代理人的估计移动也可以被认为是随机移动，那么为什么不让代理人从错误的估计移动中学习呢？随着时间的推移，这些动作会变得更好，对吗？现实是，如果我们让代理在每场比赛中从自己的错误中学习，而不让它进行足够的探索，它的策略永远不会是最好的。换句话说，估计策略的损失函数最终将被最小化，但是更低的最小值可以通过更好的策略来定位。此外，也有可能完全错过最小值。</p><p id="7e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重现下面结果所需的代码可以在<a class="ae ls" href="https://gist.github.com/alessiot/0a3ec05a1bc4ec499a5a837beaceb1ff" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我用10万集的时间训练了一个代理，让他在5x5的棋盘上玩战舰，并把战舰的数量限制为一个由三个单元组成。我使用线性模型来近似Q函数，并且随机梯度下降，学习速率为0.001，动量为0.9，以最小化损失，这是Q值的均方误差函数。我让epsilon在前半集衰减，直到它达到0.01的值。</p><p id="b082" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了确保算法按预期运行，我做了一个初步测试，在每集开始时保持飞船位置不变。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr oh kt ku kv kw paragraph-image"><img src="../Images/d7c30c00c53669243b90011a2b2d1fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*KfCaWN-8Ke1rVFj1cptbxw.png"/></figure><figure class="kq kr oi kt ku kv kw paragraph-image"><img src="../Images/c13d11b5b797f440f30f1857d73e1e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*Z8VN7ZkVb-Uub2RXVFPv6w.png"/></figure></div><div class="ab cb"><figure class="kq kr oj kt ku kv kw paragraph-image"><img src="../Images/5e840a32f12043ee02ce0a5cd4cee2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*yOkYS7hRQc9iWrzYr9HB5Q.png"/></figure><figure class="kq kr oj kt ku kv kw paragraph-image"><img src="../Images/91bd7c51a3a39953b20e40636bf8ff7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*kRcacTzrCDW3NguKyxZ1cg.png"/></figure></div><p id="3d3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如预期的那样，估计策略线性模型在大约10，000集之后过度拟合:代理了解了船的位置。</p><p id="d30b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个初始测试之后，我做了一个更真实的实验，让隐藏的飞船在每集开始时随机重新定位。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr oh kt ku kv kw paragraph-image"><img src="../Images/7a769de87ec7b688798367e44b06988d.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*BlWcE_9gyU9b7U4r6z0EPQ.png"/></figure><figure class="kq kr oi kt ku kv kw paragraph-image"><img src="../Images/7ba827ebfe38b919139fa1f81c29a63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*oKurCt72WtVA8sLalzR3Gg.png"/></figure></div><div class="ab cb"><figure class="kq kr oa kt ku kv kw paragraph-image"><img src="../Images/e0da29169c123ed6821c8c0e97219bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*YjRCdd3lrTXODh8aFEw-qQ.png"/></figure><figure class="kq kr ob kt ku kv kw paragraph-image"><img src="../Images/650a199715166a6541ef6c96bdc69ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*2CoadIX6fXew2bsAw_zI4Q.png"/></figure></div><p id="11e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在100，000集的情况下，代理平均需要大约13步才能赢得一场游戏(0.5个棋盘大小)。我们能做得更好吗？</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="ee77" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">使用OpenAI Gym的神经网络进行q学习。</h1><p id="dfcf" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">学习如何为战舰游戏实现目标的代理的先前实现可以被修改以使用最新的RL算法，这些算法可以通过OpenAI <a class="ae ls" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">稳定基线</a>库获得。使用这些库的主要改变是让战舰环境与开放的<a class="ae ls" href="http://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">体育馆</a>库环境兼容。OpenAI gym标准化了任何RL项目中需要的所有步骤。OpenAI健身房所需的基本组件如下图。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/bafac5de31e2153f80668137abd8bfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDc2cLioB5WPL3wSm_rPAw.png"/></div></div></figure><p id="9e52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整的战舰环境在这里<a class="ae ls" href="#89b2" rel="noopener ugc nofollow">可用</a>以及重现本文中描述的所有结果所需的代码。</p><p id="466c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我用OpenAI库执行的实验，我修改了奖励方案，如下所示</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/206d6928a502d398d488de6c350f6e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04M8xtjvX0ieszEOcXX4xA.png"/></div></div></figure><p id="7fc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这转化为下面的代码。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/e564e1367a9cf5592077fb4956f14b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGzJNyVKiIj7XPL9t1H6HA.png"/></div></div></figure><p id="84f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">修改奖励方案的原因是使其适应于处理重复的非法行为(移至非空牢房)。我尝试了许多版本，然后提出了这个简化版本，只取决于网格边长。在OpenAI健身房，我发现处理非法动作并不简单。事实上，非法移动需要随着时间的推移改变动作空间，这是我没有找到一个很好的解决方案。</p><p id="95cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦实现了环境，我们就可以使用稳定基线提供的环境检查器来检查它。我的实现允许在每个游戏开始时随机生成敌人的棋盘或者用户定义的敌人的棋盘。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/559d05acccb0e067a5b0e61ba3ba3c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Er9PY7TNT396Utx3k-jCZg.png"/></div></div></figure><p id="a66f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们可以观察游戏是如何进行的，并查看奖励、动作和下一个状态，以确保一切按预期进行。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/427945e37f71c69f3803ce8565129e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaoXfhKz9CQnCoCBPdKNTw.png"/></div></div></figure><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/b4c8f3fdbc4c6e9fcaeff738840c558b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-VDKCA3cn4EfeDKEQzJWCQ.png"/></div></div></figure><p id="9de6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的例子显示了一个完全随机的代理在工作。</p><p id="d84e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我做的第一个实验是训练一个代理人在一个5x5的板子和一艘巡洋舰上玩1，000，000集的战舰，它只由三个单元组成。代理人使用同步的、确定性的<a class="ae ls" href="https://arxiv.org/abs/1602.01783" rel="noopener ugc nofollow" target="_blank">异步优势行动者评论家(A3C) </a>算法变体进行训练，称为<a class="ae ls" href="https://stable-baselines.readthedocs.io/en/master/modules/a2c.html" rel="noopener ugc nofollow" target="_blank"> A2C </a>。我选择了使用一个2层64节点的多层感知器网络(称为“MlpPolicy”)来实现。<a class="ae ls" rel="noopener" target="_blank" href="/understanding-actor-critic-methods-931b97b6df3f">演员评论方法</a>背后的主要思想是使用两个神经网络，称为评论家和演员。评论家估计Q值；参与者按照评论家建议的方向更新策略分布。A2C <strong class="jp ir"> </strong>是A3C的变体，但是没有异步部分。经验表明，A2C的性能与A3C相当，但效率更高。</p><p id="a31b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练一个有稳定基线的代理很简单。一旦实现了有效的环境，策略训练就可以无缝处理，我们只需要担心选择算法和优化其超参数。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/76bfb7d0467ce80c9a97cab9fda94047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-yepYgIIrX2pPvQyGsb8g.png"/></div></div></figure><p id="0987" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我创建了回调函数来监控训练并保存最佳策略。他们可以在我的代码完整版<a class="ae ls" href="#89b2" rel="noopener ugc nofollow">这里</a>找到。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr or kt ku kv kw paragraph-image"><img src="../Images/f873c53b7d4029fd5dd715fc72c9919e.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*aOo9UsHasHjxL0kUBRjLag.png"/></figure><figure class="kq kr os kt ku kv kw paragraph-image"><img src="../Images/eda99b5f151bb8f35b34401150d3ee0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*WCAXSC4g1Xp0exniqSnRUg.png"/><p class="le lf gj gh gi lg lh bd b be z dk oc di od lk translated">代理人在5x5棋盘上用一艘巡洋舰玩战舰游戏的学习曲线。1000集的滑动窗口用于平滑曲线。</p></figure></div><p id="7398" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在步骤900，000保存了最佳模型。该策略平均需要7步左右才能完成一局。</p><p id="2605" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个实验，我用的是6x6的板，500万集。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/25c82d5d2dd857ecd55cc40b5c57610b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZFjllgXoIpK6ATdUcqB42Q.png"/></div></div></figure><p id="f55d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最佳模型保存在第4，789，994步。该策略平均需要10步左右才能完成一局。如果我们考虑到一个类似人类的策略平均需要大约8步，那么这肯定不是一个完全优化的策略。这也可以在下图中看到，其中曲线没有达到饱和。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr ou kt ku kv kw paragraph-image"><img src="../Images/bcc54876454b99f02ceba5c1924d0fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*JwG8xXCHlUUq6IdWqIXxDg.png"/></figure><figure class="kq kr ov kt ku kv kw paragraph-image"><img src="../Images/ec32579a7335b825ca7e1fa1c78aa39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*QOlGY8e3nmq5lmZOpXuz9w.png"/></figure><figure class="kq kr ow kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/5b21995a751b57f8a2a6193dadd77f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*Wmtp9CKGTabatO7fgxwSZQ.gif"/></div><p class="le lf gj gh gi lg lh bd b be z dk ox di oy lk translated">代理人在6x6的棋盘上用一艘巡洋舰玩战舰游戏的学习曲线。1000集的滑动窗口用于平滑曲线。训练有素的代理人演奏也显示在渲染板上。顶部网格是代理的计分板。底部的格子对代理是隐藏的，它显示了巡洋舰在每个游戏开始时被随机放置的位置。在一个案例中(第三集)，代理人的行为出乎意料。</p></figure></div><p id="c318" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">增加董事会规模会让学习变得更加困难，或者至少需要更长时间的培训。我的目标是看看代理是否能在第一次击中后学习下一步该做什么，以及在稍大的网格中学习正确的动作需要多长时间。</p><p id="db98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在我的第二个实验中，我使用了7x7板和10，000，000集。然而，这一次，在为这许多集进行训练之前，我使用Python库<a class="ae ls" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> hyperopt </a>优化了模型超参数。事实上，像以前一样使用默认的超参数会使模型难以收敛。我只优化了与神经网络相关的超参数，特别是批量大小(n_steps)、学习速率、学习速率调度程序类型、学习速率衰减(alpha)、RMSProp的ε因子，将所有其他超参数保留为默认值。优化空间如下所示。更多细节可以在代码中看到。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/7676537a2b6fbf7e45d5a16e71007a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDO7bzynbfxKcKq-7uF6qA.png"/></div></div></figure><p id="53a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">发现超参数的最佳值是α= 0.99，n_steps=10，learning_rate=0.007，lr _ schedule =‘常数’，<br/>ε= 0.001。然而，我只有耐心等待20次试验，每一次都训练代理人20万集。根据想要尝试的试验次数(超参数组合)，找到最佳超参数可能需要很长时间。我把这个留给读者去玩。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr pa kt ku kv kw paragraph-image"><img src="../Images/3695bec327ccc3fa707a89a1d9bdf2a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*EhkWgNnQF_sOg5whO3N_GQ.png"/></figure><figure class="kq kr pb kt ku kv kw paragraph-image"><img src="../Images/7c358310c7edb5c39e80d5dd5229c19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*VDoaCBXcxQQbmfQIms1eTA.png"/></figure><figure class="kq kr pc kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/0da3dba9bb1427e192d2d0ec0b80c28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*nZGl2IQvFDlMIsZO_0XPhQ.gif"/></div><p class="le lf gj gh gi lg lh bd b be z dk pd di pe lk translated">代理人在7x7棋盘上用一艘巡洋舰玩战舰游戏的学习曲线。1000集的滑动窗口用于平滑曲线。训练有素的代理人演奏也显示在渲染板上。顶部网格是代理的计分板。底部的格子对代理是隐藏的，它显示了巡洋舰在每个游戏开始时被随机放置的位置。如前所述，在某些情况下，由于策略没有完全优化，代理的行为会出乎意料。</p></figure></div><p id="9ddd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练有素的特工平均需要14步才能完成一局游戏。对代理人进行更长时间的培训可能会带来更好的政策。</p><p id="61f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在8x8、9x9或10x10的网格上训练一个代理需要多少集？使用来自5x5、6x6和7x7网格的结果进行粗略的外推表明，我们可能很容易需要数亿个，并且可能永远不会达到近似模型的损失函数的最小值。同样，我的目标是学习RL，我没有花时间来解决这个特殊的问题。</p><p id="330c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更快地收敛到最优策略，可以尝试的事情很少:</p><ul class=""><li id="1e10" class="lt lu iq jp b jq jr ju jv jy lv kc lw kg lx kk ly lz ma mb bi translated">修改奖励方案；</li><li id="cbbd" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">使用不同的算法；</li><li id="2d99" class="lt lu iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">优化算法超参数。</li></ul><p id="ceda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这三者中，找到一个更好的奖励方案将使收敛到最优政策的速度明显加快。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="fcfe" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">结论。</h1><p id="ece6" class="pw-post-body-paragraph jn jo iq jp b jq na js jt ju nb jw jx jy nw ka kb kc nx ke kf kg ny ki kj kk ij bi translated">在这篇文章中，我描述了我使用战舰游戏进行强化学习的经历。对于任何愿意学习RL的人，我会建议同样的方法:选择一个游戏，编写它的框架，让它准备好应用RL技术，用它来实验RL的美丽和局限性。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="f412" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我要感谢<a class="pf pg ep" href="https://medium.com/u/b61b6673cdce?source=post_page-----ebd2cf9adb01--------------------------------" rel="noopener" target="_blank"> Sundar Krishnan </a>所有有益的讨论。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="6bab" class="nf mi iq bd mj ng nh ni mm nj nk nl mp nm nn no ms np nq nr mv ns nt nu my nv bi translated">代码。</h1><figure class="km kn ko kp gt kr"><div class="bz fp l di"><div class="ph pi l"/></div></figure></div></div>    
</body>
</html>