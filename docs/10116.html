<html>
<head>
<title>Spamilton: Text Generation with LSTMs and Hamilton Lyrics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spamilton:带 LSTMs 和 Hamilton 歌词的文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spamilton-text-generation-with-lstms-and-hamilton-lyrics-ec7938ae830c?source=collection_archive---------50-----------------------#2020-07-16">https://towardsdatascience.com/spamilton-text-generation-with-lstms-and-hamilton-lyrics-ec7938ae830c?source=collection_archive---------50-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e7dc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Tensorflow 和 Keras 的简单文本生成教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/03d852903fde9d7ccc3988599d1830cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JOgiJQnp_qPOQSBZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">苏丹欧阳在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2876" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本生成是计算语言学和自动生成自然语言文本的 AI 之间的桥梁。在深度学习中，rnn 已经被证明可以非常好地处理文本等序列数据。在这个例子中，我将演示应用 LSTMs 和单词嵌入来生成 Hamilton 歌词。许多想法来自卡帕西和班萨尔。所有的代码都可以在我的<a class="ae ky" href="https://github.com/perkdrew/text-generation" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="2085" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从 Tensorflow 和 Keras 导入所需的库:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9f13" class="ma mb it lw b gy mc md l me mf">from <strong class="lw iu">keras.preprocessing.sequence</strong> import <strong class="lw iu">pad_sequences </strong><br/>from <strong class="lw iu">keras.models</strong> import <strong class="lw iu">Sequential</strong><br/>from <strong class="lw iu">keras.layers</strong> import <strong class="lw iu">Embedding, LSTM, Bidirectional, Dense, Dropout</strong><br/>from <strong class="lw iu">keras.preprocessing.text</strong> import <strong class="lw iu">Tokenizer </strong><br/>from <strong class="lw iu">keras.callbacks</strong> import <strong class="lw iu">EarlyStopping</strong><br/>import <strong class="lw iu">keras.utils</strong> as <strong class="lw iu">ku</strong><br/>import <strong class="lw iu">numpy</strong> as <strong class="lw iu">np</strong></span></pre><p id="54fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们提供一条通向单词嵌入的路径:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="dec8" class="ma mb it lw b gy mc md l me mf">glove_path = 'glove.twitter.27B/glove.twitter.27B.200d.txt'</span></pre><p id="f18f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">歌词是从网上搜集来的，放在一个纯文本文件中:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8b0d" class="ma mb it lw b gy mc md l me mf">text = open('ham_lyrics.txt', encoding='latin1').read()</span></pre><p id="0e58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语料库被小写化和标记化。输入序列是使用标记列表创建的，并被填充以匹配最大序列长度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a84b" class="ma mb it lw b gy mc md l me mf">tokenizer = Tokenizer()<br/>corpus = text.lower().split("\n") <br/>tokenizer.fit_on_texts(corpus) <br/>total_words = len(tokenizer.word_index) + 1 <br/>input_seq = [] <br/>for line in corpus:  <br/>  token_list = tokenizer.texts_to_sequences([line])[0]  <br/>  for i in range(1, len(token_list)):   <br/>    n_gram_seq = token_list[:i+1]   <br/>    input_seq.append(n_gram_seq)</span></pre><p id="ca43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将输入序列分成预测器和标签，用于我们的学习算法。这被视为分类任务，类别的数量反映了记号赋予器识别的单词总数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d18e" class="ma mb it lw b gy mc md l me mf">max_seq_len = max([len(x) for x in input_seq]) <br/>input_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_len, padding='pre')) <br/>predictors, label = input_seq[:,:-1],input_seq[:,-1] <br/>label = ku.to_categorical(label, num_classes=total_words)</span></pre><p id="5535" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要打开我们的 word 嵌入文件，以便可以在我们的嵌入层中正确访问。嵌入索引是嵌入矩阵的预备步骤。这里应用了手套嵌入:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1f03" class="ma mb it lw b gy mc md l me mf">embeddings_index = dict()<br/>with open(glove_path, encoding="utf8") as glove:  <br/>  for line in glove:    <br/>    values = line.split()    <br/>    word = values[0]    <br/>    coefs = np.asarray(values[1:], dtype='float32') <br/>    embeddings_index[word] = coefs  <br/>glove.close()</span></pre><p id="7054" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嵌入矩阵是我们将实际输入到网络中的内容:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7e48" class="ma mb it lw b gy mc md l me mf">embedding_matrix = np.zeros((total_words, 200))<br/>for word, index in tokenizer.word_index.items():    <br/>  if index &gt; total_words - 1:        <br/>    break    <br/>  else:        <br/>    embedding_vector = embeddings_index.get(word)        <br/>    if embedding_vector is not None:   <br/>      embedding_matrix[index] = embedding_vector</span></pre><p id="0761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然数据和单词嵌入已经准备好了，我们可以开始设置 RNN 的图层了。我们首先添加嵌入层，然后添加 256 个单位的双向 LSTM 和 128 个单位的 LSTM:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ab5d" class="ma mb it lw b gy mc md l me mf">model = Sequential() <br/>model.add(Embedding(total_words, 200, weights = [embedding_matrix],                     input_length=max_seq_len-1)) <br/>model.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences = True))) <br/>model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))</span></pre><p id="01e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们用一个丢弃层来移除一次性神经元，防止过度拟合，而不会降低我们任务的性能；经常性断开“断开”经常性单元之间的连接，而常规断开“断开”到一般输入/输出的连接。激活 softmax 后的最终致密层会关闭模型。如果损失函数开始膨胀，我们称之为提前止损。由于运行时间可能会很长，因此将时期设置得比较低:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9e63" class="ma mb it lw b gy mc md l me mf">model.add(Dropout(0.2)) <br/>model.add(Dense(total_words, activation=’softmax’)) model.compile(loss=’categorical_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’]) <br/>earlystop = EarlyStopping(monitor=’val_loss’, min_delta=0, patience=5, verbose=0, mode=’auto’) <br/>model.fit(predictors, label, epochs=25, verbose=1, callbacks=[earlystop])<br/>model.save('hamilton_model.h5')</span></pre><p id="2652" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，添加了一个助手函数来显示生成的文本:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="60fa" class="ma mb it lw b gy mc md l me mf">def generate_text(seed_text, next_words, max_seq_len):<br/>  for _ in range(next_words):  <br/>    token_list = tokenizer.texts_to_sequences([seed_text])[0] <br/>    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')  <br/>    predicted = model.predict_classes(token_list, verbose=0)  <br/>    output_word = ""  <br/>    for word, index in tokenizer.word_index.items():   <br/>      if index == predicted:    <br/>        output_word = word    <br/>        break  <br/>    seed_text += " " + output_word</span></pre><p id="46a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数将种子文本、后续单词的数量和最大序列长度作为参数。种子文本是我们使用的文本，作为我们的学习算法进行预测的基础，我们选择希望跟随文本的单词数。</p><p id="4a24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们运行管道并打印结果:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e3f8" class="ma mb it lw b gy mc md l me mf">print(generate_text("These United States", 3, max_seq_len))</span></pre><p id="4f7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成几行文本后，我们可以预期如下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mg"><img src="../Images/0e3bada8a05dbc7844fe96392680e608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1CpJceKiebx0uadil9Dkw.png"/></div></div></figure><p id="6c53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过更多的文本预处理、特征工程和健壮的建模，我们可以期望减轻上面的语法和句法错误。LSTMs 可以用 GRUs 替换，以获得更快的运行时间，代价是在较长的文本序列中精度较低。带有字符嵌入或值的文本生成也值得探索。正如 Aaron Burr 所指出的，对于不同的建模方法来说，世界是足够广阔的。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><p id="9c7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]:安德烈·卡帕西。(2015 年 5 月 21 日)。<em class="mo">递归神经网络的不合理有效性</em>【http://karpathy.github.io/2015/05/21/rnn-effectiveness/ T2】</p><p id="23c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]:希瓦姆·班萨尔。(2018 年 3 月 26 日)。<em class="mo">使用 LSTMs 的语言建模文本生成—用于 NLP 的深度学习</em><a class="ae ky" href="https://mc.ai/language-modelling-text-generation-using-lstms-deep-learning-for-nlp/?fbclid=IwAR2mR7QkpnwzCzszwN1mOXUWHBhIGOtfvxGA4AapS52RJZW6wSpKhckI1HY" rel="noopener ugc nofollow" target="_blank">https://MC . ai/Language-modeling-Text-Generation-using-lst ms-Deep-Learning-for-NLP/？FB clid = iwar 2 Mr 7 qkpnwzczzwn 1 moxuwhbhigotfvxga 4 aaps 52 rjzw 6 wspkhcki 1 hy</a></p></div></div>    
</body>
</html>