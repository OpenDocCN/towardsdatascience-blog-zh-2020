<html>
<head>
<title>Algorithms from Scratch: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的算法:逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-logistic-regression-7bacdfd9738e?source=collection_archive---------29-----------------------#2020-07-15">https://towardsdatascience.com/algorithms-from-scratch-logistic-regression-7bacdfd9738e?source=collection_archive---------29-----------------------#2020-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0838" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="c3ba" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始详述和构建逻辑回归模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f28a70dab702b7b7c62b5d892f80587a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ra3DOde-w_kO1ikb"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@marcinjozwiak?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马尔辛·乔兹维亚克</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b141" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与普遍的看法相反，我在此声明，逻辑回归是<strong class="lk jd">而不是</strong>一种分类算法(<em class="me">本身是</em> ) —事实上，逻辑回归实际上是一种回归模型，所以不要对其命名中出现“<em class="me">回归</em>”感到惊讶。回归分析是一套统计过程，用于估计因变量与一个或多个自变量之间的关系(来源:<a class="ae lh" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank">维基百科</a>)。如上所述，逻辑回归不是一种分类算法，它不执行统计分类，因为它只是估计逻辑模型的参数。</p><blockquote class="mf"><p id="8312" class="mg mh it bd mi mj mk ml mm mn mo md dk translated">逻辑回归是一种统计模型，其最基本的形式是使用逻辑函数来模拟二元因变量，尽管存在许多更复杂的扩展。(来源:<a class="ae lh" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></blockquote><p id="1262" class="pw-post-body-paragraph li lj it lk b ll mp kd ln lo mq kg lq lr mr lt lu lv ms lx ly lz mt mb mc md im bi translated">允许将逻辑回归用作分类算法的是使用<em class="me">阈值</em>(也可称为截止或决策边界)，这反过来会将概率大于阈值的输入分类为一个类别，而将概率低于阈值的输入分类为另一个类别，正如我们在机器学习中通常所做的那样。</p><p id="6efc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">参见</em> </strong> <a class="ae lh" href="https://en.wikipedia.org/wiki/Multiclass_classification#:~:text=%2Drest-,One%2Dvs.,all%20other%20samples%20as%20negatives." rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="me">这一环节</em> </strong> </a> <strong class="lk jd"> <em class="me">来看我们如何处理多类分类问题。</em> </strong></p><p id="8525" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们把注意力从零开始回到<a class="ae lh" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">算法</a>系列的目的。</p><p id="72b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">链接到 Github 存储库…</p><div class="mu mv gp gr mw mx"><a href="https://github.com/kurtispykes/ml-from-scratch/blob/master/logistic_regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jd gy z fp nc fr fs nd fu fw jc bi translated">kurtispykes/ml-从零开始</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl lb mx"/></div></div></a></div><blockquote class="nm nn no"><p id="8882" class="li lj me lk b ll lm kd ln lo lp kg lq np ls lt lu nq lw lx ly nr ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">注</em> </strong> <em class="it">:有很多机器学习框架的代码经过高度优化，这使得从头开始编写机器学习算法在实际设置中成为一项多余的任务。然而，当我们从零开始构建算法时，它有助于我们对模型中正在发生的事情获得更深入的直觉，这可能会在尝试改进我们的模型时带来高回报。</em></p></blockquote><h2 id="5205" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">二元分类的线性回归</h2><p id="31a2" class="pw-post-body-paragraph li lj it lk b ll ok kd ln lo ol kg lq lr om lt lu lv on lx ly lz oo mb mc md im bi translated">在《从零开始的算法:线性回归》的最后一集里，我说“<em class="me">它通常是第一次学习机器学习时学习的第一批算法之一，因为它很简单，而且它如何构建到其他算法里，如逻辑回归和神经网络</em>”——你现在会明白我的意思了。</p><p id="4ec7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">我们如何从预测连续变量到伯努利变量(即“成功”或“失败”)？</strong>由于响应数据(我们试图预测的数据)是二元的(取值 0 和 1)，因此仅由 2 个值组成，我们可以假设我们的响应变量的分布现在来自<a class="ae lh" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank">二项式分布</a>——这需要一个完美的时间来引入由约翰·内尔德和罗伯特·威德伯恩制定的广义线性模型(GLM)。</p><p id="e04a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GLM 模型允许响应变量具有不同于正态分布的误差分布。在我们的情况下，我们现在有一个二项分布，通过使用 GLM 模型，我们可以通过链接函数将线性模型与响应变量相关联，并允许每个测量的方差大小是其预测值的函数，从而推广线性回归(来源:<a class="ae lh" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">维基百科</a>)。</p><blockquote class="mf"><p id="73a0" class="mg mh it bd mi mj mk ml mm mn mo md dk translated">长话短说，逻辑回归是具有二项式条件响应和 logit 链接的 GLM 的特例。</p></blockquote><p id="1703" class="pw-post-body-paragraph li lj it lk b ll mp kd ln lo mq kg lq lr mr lt lu lv ms lx ly lz mt mb mc md im bi translated"><strong class="lk jd">停止… </strong></p><p id="a078" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在继续之前，我们应该弄清楚一些统计术语(用外行人的术语来说):</p><ul class=""><li id="ac9b" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><strong class="lk jd">几率</strong>——某事发生与某事未发生的比率。例如，切尔西赢得接下来 4 场比赛的赔率是 1 比 3。发生的事情(切尔西赢得比赛)1 与没有发生的事情(切尔西没有赢得比赛)3 的比值可以写成一个分数，1/3。</li><li id="cca2" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated">概率——发生的事情与可能发生的事情的比率。使用上面的例子，正在发生的事情(切尔西获胜)1 与所有可能发生的事情(切尔西获胜和失败)4 的比率也可以写成分数 1/4，这是获胜的概率——因此失败的概率是 1–1/4 = 3/4。</li></ul><p id="47d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">概率的范围在 0 和 1 之间，而赔率并不局限于 0 和 1 之间，而是可以取 0 到无穷大之间的任何值。</p><p id="7c52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以通过将获胜概率的比率(用我们的例子来说是 1/4)除以失败概率的比率(3/4)来得到概率的赔率，从而得到 1/3，即赔率。参见<em class="me">图 1 </em>了解我们如何用数学方法表达它。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/76ea167a284686ef54b449372732d2e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*BgzrkwJf7jdxth4DoG2_Gg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 1:从概率中得出的赔率。</p></figure><p id="ec7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果切尔西是一支糟糕的足球队(难以想象，我知道)，他们获胜的几率会在 0 到 1 之间。然而，由于我们都知道切尔西是世界上最伟大的球队之一(毫无疑问是伦敦最好的球队)，因此，切尔西获胜的几率将在 1 到无穷大之间。不对称使得很难比较支持或反对切尔西获胜的几率，所以我们采用几率的对数来使一切对称。</p><p id="2c5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">图 1 </em>向我们展示了我们可以用概率计算赔率，既然如此，我们也可以使用<em class="me">图 1 </em>中的公式计算赔率的对数。概率比的对数被称为 logit 函数，它构成了逻辑回归的基础。让我们通过考虑具有给定参数的逻辑模型来更好地理解这一点，并看看如何从数据中估计系数。</p><p id="0e80" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">注:下面这个例子来源于逻辑回归的例子部分</em> </strong> <a class="ae lh" href="https://en.wikipedia.org/wiki/Logistic_regression#Logistic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="me">维基百科</em> </strong> </a> <strong class="lk jd"> <em class="me">。</em>T19】</strong></p><p id="b173" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑一个有两个独立变量(<strong class="lk jd"> X </strong> 1 和<strong class="lk jd"> X </strong> 2)和一个伯努利响应变量<strong class="lk jd"> Y </strong>的模型，我们用 p = <strong class="lk jd"> P </strong> (Y=1)表示。我们假设独立变量和 Y=1 事件的对数概率之间存在线性关系，可以用数学方法表示为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/3f63b482c719567cb604baf59fe8b223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVVopARnDIZxo1J1is2j9g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 2:独立变量与 Y=1 事件的对数比数之间的线性关系表达式。</p></figure><p id="f0b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过对对数赔率求幂，我们恢复赔率如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/4bf486410dfa1fcf5588586195845f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*Ea65DvT5n9aBDF3mPn5iKQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 3:对对数赔率求幂以恢复赔率。</p></figure><p id="e476" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过简单的代数运算，Y=1 的概率为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/d74628588bb77766de533d211d31f59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*bbkRBBsKD-SVgJg6D39gag.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 4:代数操作</p></figure><p id="23f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑到这一点，<em class="me">图 4 </em>向我们展示了，如果我们有线性模型的参数，我们可以很容易地计算给定观察值的对数优势或 Y= 0 的概率。逻辑回归仍然是一个回归模型，没有与使反应的预测概率二分法的阈值结合使用。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h2 id="cd52" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">分块算法</h2><ol class=""><li id="c504" class="op oq it lk b ll ok lo ol lr po lv pp lz pq md pr ov ow ox bi translated">随机初始化假设函数的参数</li><li id="7e28" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pr ov ow ox bi translated">将逻辑函数应用于线性假设函数</li><li id="042a" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pr ov ow ox bi translated">计算偏导数(<a class="ps pt ep" href="https://medium.com/u/102f526f83de?source=post_page-----7bacdfd9738e--------------------------------" rel="noopener" target="_blank"> Saket Thavanani </a>写了一篇很好的文章，题目是<a class="ae lh" href="https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d" rel="noopener"> <em class="me">逻辑回归成本函数的导数</em> </a>)</li><li id="ca91" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pr ov ow ox bi translated">更新参数</li><li id="23c1" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pr ov ow ox bi translated">重复 2-4，重复<em class="me"> n </em>次迭代(直到成本函数最小化)</li><li id="c657" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md pr ov ow ox bi translated">推理</li></ol><p id="b0b2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">实现</strong></p><p id="8874" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本节中，我使用了 3 个 Python 框架:NumPy 用于线性代数，Pandas 用于数据操作，Scikit-Learn 用于机器学习工具。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="0caf" class="ns nt it pv b gy pz qa l qb qc"><strong class="pv jd">import</strong> <strong class="pv jd">numpy</strong> <strong class="pv jd">as</strong> <strong class="pv jd">np</strong> <br/><strong class="pv jd">import</strong> <strong class="pv jd">pandas</strong> <strong class="pv jd">as</strong> <strong class="pv jd">pd</strong> <br/><strong class="pv jd">from</strong> <strong class="pv jd">sklearn.metrics</strong> <strong class="pv jd">import</strong> accuracy_score<br/><strong class="pv jd">from</strong> <strong class="pv jd">sklearn.datasets</strong> <strong class="pv jd">import</strong> load_breast_cancer<br/><strong class="pv jd">from</strong> <strong class="pv jd">sklearn.linear_model</strong> <strong class="pv jd">import</strong> LogisticRegression <br/><strong class="pv jd">from</strong> <strong class="pv jd">sklearn.model_selection</strong> <strong class="pv jd">import</strong> train_test_split</span></pre><p id="6e04" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们需要一个数据集。我使用的是经典的二元分类数据集<code class="fe qd qe qf pv b">sklearn.datasets.load_breast_cancer</code>——参见<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="a0a6" class="ns nt it pv b gy pz qa l qb qc"><em class="me"># loading the data set</em><br/>dataset = load_breast_cancer(as_frame=<strong class="pv jd">True</strong>)<br/>df= pd.DataFrame(data= dataset.data)<br/>df["target"] = dataset.target<br/><br/>df.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/0b6889a0a582c980b423f4f02b15cafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQG2bJuF6BKPLFuRwCGBfA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 5:上面代码单元的输出。<strong class="bd qh">注意:数据帧有 31 列，太大而无法显示，因此出现省略号(仍有一些列看不到)。</strong></p></figure><p id="1704" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将预测变量和响应变量分开，然后创建一个训练和测试集。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="c062" class="ns nt it pv b gy pz qa l qb qc"><em class="me"># Seperating to X and Y </em><br/>X = df.iloc[:, :-1]<br/>y = df.iloc[:, -1]<br/><br/><em class="me"># splitting training and test</em><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=<strong class="pv jd">True</strong>, random_state=24)</span></pre><p id="2dd4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们从头开始构建线性回归所做的大量工作(见下面的链接)可以借用一些微小的变化来使用逻辑回归调整我们的分类模型。</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/algorithms-from-scratch-linear-regression-c654353d1e7c"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jd gy z fp nc fr fs nd fu fw jc bi translated">从头开始的算法:线性回归</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">从头开始详述和构建线性回归模型</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="qi l ni nj nk ng nl lb mx"/></div></div></a></div><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="8482" class="ns nt it pv b gy pz qa l qb qc"><strong class="pv jd">def</strong> param_init(X): <br/>    <em class="me">"""</em><br/><em class="me">    Initialize parameters</em><br/><em class="me">    __________________ </em><br/><em class="me">    Input(s)</em><br/><em class="me">    X: Training data</em><br/><em class="me">    __________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    params: Dictionary containing coefficients</em><br/><em class="me">    """</em><br/>    params = {} <em class="me"># initialize dictionary </em><br/>    _, n_features = X.shape <em class="me"># shape of training data</em><br/><br/>    <em class="me"># initializing coefficents to 0 </em><br/>    params["W"] = np.zeros(n_features)<br/>    params["b"] = 0<br/>    <strong class="pv jd">return</strong> params</span><span id="af3c" class="ns nt it pv b gy qj qa l qb qc"><strong class="pv jd">def</strong> get_z(X, W, b): <br/>    <em class="me">"""</em><br/><em class="me">    Calculates Linear Function</em><br/><em class="me">    __________________</em><br/><em class="me">    Input(s)</em><br/><em class="me">    X: Training data</em><br/><em class="me">    W: Weight coefficients</em><br/><em class="me">    b: bias coefficients</em><br/><em class="me">    __________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    z: a Linear function</em><br/><em class="me">    """</em><br/>    z = np.dot(X, W) + b<br/>    <strong class="pv jd">return</strong> z</span><span id="7173" class="ns nt it pv b gy qj qa l qb qc"><strong class="pv jd">def</strong> sigmoid(z):<br/>    <em class="me">"""</em><br/><em class="me">    Logit model</em><br/><em class="me">    _________________</em><br/><em class="me">    Input(s)</em><br/><em class="me">    z: Linear model </em><br/><em class="me">    _________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    g: Logit function applied to linear model</em><br/><em class="me">    """</em><br/>    g = 1 / (1 + np.exp(-z))<br/>    <strong class="pv jd">return</strong> g</span><span id="2245" class="ns nt it pv b gy qj qa l qb qc"><strong class="pv jd">def</strong> gradient_descent(X, y, params, lr, n_iter): <br/>    <em class="me">"""</em><br/><em class="me">    Gradient descent to minimize cost function</em><br/><em class="me">    __________________ </em><br/><em class="me">    Input(s)</em><br/><em class="me">    X: Training data</em><br/><em class="me">    y: Labels</em><br/><em class="me">    params: Dictionary contatining coefficients</em><br/><em class="me">    lr: learning rate</em><br/><em class="me">    __________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    params: Dictionary containing optimized coefficients</em><br/><em class="me">    """</em><br/>    W = params["W"] <br/>    b = params["b"]<br/>    m = X.shape[0] <em class="me"># number of training instances </em><br/><br/>    <strong class="pv jd">for</strong> _ <strong class="pv jd">in</strong> range(n_iter): <br/>        <em class="me"># prediction with random weights</em><br/>        g = sigmoid(get_z(X, W, b))<br/>        <em class="me"># calculate the loss</em><br/>        loss = -1/m * np.sum(y * np.log(g)) + (1 - y) * np.log(1-g)<br/>        <em class="me"># partial derivative of weights </em><br/>        dW = 1/m * np.dot(X.T, (g - y))<br/>        db = 1/m * np.sum(g - y)<br/>        <em class="me"># updates to coefficients</em><br/>        W -= lr * dW<br/>        b -= lr * db <br/>    <br/>    params["W"] = W<br/>    params["b"] = b<br/>    <strong class="pv jd">return</strong> params</span><span id="7f29" class="ns nt it pv b gy qj qa l qb qc"><strong class="pv jd">def</strong> train(X, y, lr=0.01, n_iter=1000):<br/>    <em class="me">"""</em><br/><em class="me">    Train Linear Regression model with Gradient decent</em><br/><em class="me">    __________________ </em><br/><em class="me">    Input(s)</em><br/><em class="me">    X: Training data</em><br/><em class="me">    y: Labels</em><br/><em class="me">    lr: learning rate</em><br/><em class="me">    n_iter: Number of iterations </em><br/><em class="me">    __________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    params: Dictionary containing optimized coefficients</em><br/><em class="me">    """</em> <br/>    init_params = param_init(X)<br/>    params = gradient_descent(X, y, init_params, lr, n_iter)<br/>    <strong class="pv jd">return</strong> params</span><span id="75b2" class="ns nt it pv b gy qj qa l qb qc"><strong class="pv jd">def</strong> predict(X_test, params):<br/>    <em class="me">"""</em><br/><em class="me">    Train Linear Regression model with Gradient decent</em><br/><em class="me">    __________________ </em><br/><em class="me">    Input(s)</em><br/><em class="me">    X: Unseen data</em><br/><em class="me">    params: Dictionary contianing optimized weights from training</em><br/><em class="me">    __________________</em><br/><em class="me">    Output(s)</em><br/><em class="me">    prediction of model</em><br/><em class="me">    """</em>  <br/>    z = np.dot(X_test, params["W"]) + params["b"]<br/>    y_pred = sigmoid(z) &gt;= 0.5<br/>    <strong class="pv jd">return</strong> y_pred.astype("int")</span></pre><p id="5b36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">值得注意的区别是，我们现在将 logit 函数应用于我们的线性模型，根据推断，我们将 logit 模型中大于 0.5 的每个输出都分类为第一类(否则为第 0 类)，并且我们使用不同的成本函数来工作于我们的分类模型，因为 MSE 会使我们的损失函数为非凸的-要了解更多关于所使用的成本函数的信息，您一定要阅读<a class="ae lh" href="https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d" rel="noopener"> <em class="me">逻辑回归的成本函数的导数</em> </a> <em class="me">。</em></p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="54b8" class="ns nt it pv b gy pz qa l qb qc">params = train(X_train, y_train) <em class="me"># train model</em><br/>y_pred = predict(X_test, params) <em class="me"># inference</em></span><span id="f791" class="ns nt it pv b gy qj qa l qb qc">lr = LogisticRegression(C=0.01)<br/>lr.fit(X_train, y_train)<br/>sklearn_y_pred = lr.predict(X_test)</span><span id="9ac5" class="ns nt it pv b gy qj qa l qb qc">print(f"My Implementation: {accuracy_score(y_test, y_pred)}<strong class="pv jd">\n</strong>Sklearn Implementation: {accuracy_score(y_test, sklearn_y_pred)}")</span><span id="28d8" class="ns nt it pv b gy qj qa l qb qc">&gt;&gt;&gt;&gt; My Implementation: 0.9300699300699301<br/>Sklearn Implementation: 0.9300699300699301</span></pre><p id="000b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很好，我们获得了与 Scikit-Learn 实现相同的精度。</p><p id="369c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们将用面向对象编程来重复这一点，面向对象编程被认为更适合协作。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="1a2d" class="ns nt it pv b gy pz qa l qb qc"><strong class="pv jd">class</strong> <strong class="pv jd">LogReg</strong>(): <br/>    <em class="me">"""</em><br/><em class="me">    Custom made Logistic Regression class</em><br/><em class="me">    """</em><br/>    <strong class="pv jd">def</strong> __init__(self, lr=0.01, n_iter= 1000): <br/>        self.lr = lr<br/>        self.n_iter = n_iter <br/>        self.params = {}<br/>    <br/>    <strong class="pv jd">def</strong> param_init(self, X_train): <br/>        <em class="me">"""</em><br/><em class="me">        Initialize parameters </em><br/><em class="me">        __________________ </em><br/><em class="me">        Input(s)</em><br/><em class="me">        X: Training data</em><br/><em class="me">        """</em><br/>        _, n_features = self.X.shape <em class="me"># shape of training data</em><br/><br/>        <em class="me"># initializing coefficents to 0 </em><br/>        self.params["W"] = np.zeros(n_features)<br/>        self.params["b"] = 0<br/>        <strong class="pv jd">return</strong> self<br/><br/>    <strong class="pv jd">def</strong> get_z(X, W, b): <br/>        <em class="me">"""</em><br/><em class="me">        Calculates Linear Function</em><br/><em class="me">        __________________</em><br/><em class="me">        Input(s)</em><br/><em class="me">        X: Training data</em><br/><em class="me">        W: Weight coefficients</em><br/><em class="me">        b: bias coefficients</em><br/><em class="me">        __________________</em><br/><em class="me">        Output(s)</em><br/><em class="me">        z: a Linear function</em><br/><em class="me">        """</em><br/>        z = np.dot(X, W) + b<br/>        <strong class="pv jd">return</strong> z<br/>        <br/>    <strong class="pv jd">def</strong> sigmoid(z):<br/>        <em class="me">"""</em><br/><em class="me">        Logit model</em><br/><em class="me">        _________________</em><br/><em class="me">        Input(s)</em><br/><em class="me">        z: Linear model </em><br/><em class="me">        _________________</em><br/><em class="me">        Output(s)</em><br/><em class="me">        g: Logit function applied to linear model</em><br/><em class="me">        """</em><br/>        g = 1 / (1 + np.exp(-z))<br/>        <strong class="pv jd">return</strong> g <br/>        <br/><br/>    <strong class="pv jd">def</strong> gradient_descent(self, X_train, y_train): <br/>        <em class="me">"""</em><br/><em class="me">        Gradient descent to minimize cost function</em><br/><em class="me">        __________________ </em><br/><em class="me">        Input(s)</em><br/><em class="me">        X: Training data</em><br/><em class="me">        y: Labels</em><br/><em class="me">        params: Dictionary contatining random coefficients</em><br/><em class="me">        alpha: Model learning rate</em><br/><em class="me">        __________________</em><br/><em class="me">        Output(s)</em><br/><em class="me">        params: Dictionary containing optimized coefficients</em><br/><em class="me">        """</em><br/>        W = self.params["W"] <br/>        b = self.params["b"] <br/>        m = X_train.shape[0]<br/><br/>        <strong class="pv jd">for</strong> _ <strong class="pv jd">in</strong> range(self.n_iter): <br/>            <em class="me"># prediction with random weights</em><br/>            g = sigmoid(get_z(X, W, b))<br/>            <em class="me"># calculate the loss</em><br/>            loss = -1/m * np.sum(y * np.log(g)) + (1 - y) * np.log(1 - g)<br/>            <em class="me"># partial derivative of weights </em><br/>            dW = 1/m * np.dot(X.T, (g - y))<br/>            db = 1/m * np.sum(g - y)<br/>            <em class="me"># updates to coefficients</em><br/>            W -= self.lr * dW<br/>            b -= self.lr * db <br/>        <br/>        self.params["W"] = W<br/>        self.params["b"] = b<br/>        <strong class="pv jd">return</strong> self<br/><br/>    <strong class="pv jd">def</strong> train(self, X_train, y_train):<br/>        <em class="me">"""</em><br/><em class="me">        Train model with Gradient decent</em><br/><em class="me">        __________________ </em><br/><em class="me">        Input(s)</em><br/><em class="me">        X: Training data</em><br/><em class="me">        y: Labels</em><br/><em class="me">        alpha: Model learning rate</em><br/><em class="me">        n_iter: Number of iterations </em><br/><em class="me">        __________________</em><br/><em class="me">        Output(s)</em><br/><em class="me">        params: Dictionary containing optimized coefficients</em><br/><em class="me">        """</em> <br/>        self.params = param_init(X_train)<br/>        gradient_descent(X_train, y_train, self.params , self.lr, self.n_iter)<br/>        <strong class="pv jd">return</strong> self <br/><br/>    <strong class="pv jd">def</strong> predict(self, X_test):<br/>        <em class="me">"""</em><br/><em class="me">        Inference </em><br/><em class="me">        __________________ </em><br/><em class="me">        Input(s)</em><br/><em class="me">        X: Unseen data</em><br/><em class="me">        params: Dictionary contianing optimized weights from training</em><br/><em class="me">        __________________</em><br/><em class="me">        Output(s)</em><br/><em class="me">        y_preds: Predictions of model</em><br/><em class="me">        """</em>  <br/>        g = sigmoid(np.dot(X_test, self.params["W"]) + self.params["b"])<br/>        <strong class="pv jd">return</strong> g</span></pre><p id="c423" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了检查我们是否正确地实现了它，我们可以看看预测是否与我们的过程实现相同，因为我们已经知道这大约等于 Scikit-learn 的实现。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="9a11" class="ns nt it pv b gy pz qa l qb qc">logreg = LogReg()<br/>logreg.train(X_train, y_train)<br/>oop_y_pred = logreg.predict(X_test)</span><span id="85d8" class="ns nt it pv b gy qj qa l qb qc">oop_y_pred == y_preds</span></pre><p id="1aa6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将返回一个对每个值都为真的数组。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h2 id="f963" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">假设</h2><ul class=""><li id="6ce8" class="op oq it lk b ll ok lo ol lr po lv pp lz pq md ou ov ow ox bi translated"><strong class="lk jd">二进制或序数</strong> —响应变量在二进制逻辑回归中要求为二进制，在序数逻辑回归中要求为序数</li><li id="6717" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd">独立性</strong> —要求观测值相互独立</li><li id="ea58" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd">多重共线性</strong>-预测变量之间很少或没有多重共线性。</li><li id="4938" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd">线性</strong> —自变量和对数优势的线性</li></ul></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h2 id="3df2" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">赞成的意见</h2><ul class=""><li id="7c1b" class="op oq it lk b ll ok lo ol lr po lv pp lz pq md ou ov ow ox bi translated">低方差</li><li id="bc30" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated">提供概率</li><li id="2ade" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated">易于实施</li></ul><h2 id="3823" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">骗局</h2><ul class=""><li id="4608" class="op oq it lk b ll ok lo ol lr po lv pp lz pq md ou ov ow ox bi translated">高偏差</li></ul></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h2 id="c84a" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">包裹</h2><p id="fe3f" class="pw-post-body-paragraph li lj it lk b ll ok kd ln lo ol kg lq lr om lt lu lv on lx ly lz oo mb mc md im bi translated">在大多数在线课程中，逻辑回归往往是在线性回归之后教授的内容。虽然通常逻辑回归用于不同领域的回归，但通过将其与阈值相结合，我们能够将其用作非常有用、易于实现的分类器，这证明是在处理分类问题时实现的良好的第一模型。</p><p id="a60a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢您花时间阅读这个故事。如果有什么我错过了，说错了，或者你想让我澄清的，请在评论中留下你的回复。另外，如果你想和我联系，我在 LinkedIn 上是最容易联系到的。</p><div class="mu mv gp gr mw mx"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jd gy z fp nc fr fs nd fu fw jc bi translated">Kurtis Pykes -人工智能作家-走向数据科学| LinkedIn</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">在世界上最大的职业社区 LinkedIn 上查看 Kurtis Pykes 的个人资料。Kurtis 有一个工作列在他们的…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">www.linkedin.com</p></div></div><div class="ng l"><div class="qk l ni nj nk ng nl lb mx"/></div></div></a></div><p id="fbb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以从这里访问从头开始系列的完整算法:</p><div class="mu mv gp gr mw mx"><a href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jd gy z fp nc fr fs nd fu fw jc bi translated">从零开始的算法——走向数据科学</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">阅读《走向数据科学》中关于算法的文章。分享概念、想法和…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="ql l ni nj nk ng nl lb mx"/></div></div></a></div></div></div>    
</body>
</html>