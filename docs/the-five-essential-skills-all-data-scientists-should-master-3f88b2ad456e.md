# 所有数据科学家都应该掌握的五项技能

> 原文：<https://towardsdatascience.com/the-five-essential-skills-all-data-scientists-should-master-3f88b2ad456e?source=collection_archive---------27----------------------->

## 和练习来让你坚持每一项

![](img/a8bd4043cb3563cdefe63a3d8db5f205.png)

乔纳森·博尔巴在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

W 当大多数人想到数据科学时，他们会想到超复杂的模型、万亿字节大小的数据库和复杂的多显示器仪表板。实际上，数据科学的任务比大多数数据科学家愿意承认的要“平凡”得多。“数据”比“科学”更重要。这意味着收集、清理、可视化、关联以及最终预测数据。在本文中，我列出了所有数据科学家都应该熟悉的五项技能。

在很大程度上，这些更多的是与 python 和脚本有关，而不是训练和推理。对于每项技能，我都列出了几个你应该能够解决/回答的“问题”。大多数都很短，不需要很长时间就能完成。有些需要一个下午，可能需要一些咖啡。尽管如此，除了我的建议之外，还是要花时间去发展它们。

虽然本文主要面向初学者，但有些任务甚至对有经验的用户来说也可能很有趣。此外，这可以作为一个检查表来衡量你在数据科学方面的进展。

# 大纲:

1.  **图像挖掘**
2.  **文本挖掘**
3.  **数据集预处理**
4.  **使用预先训练的模型**
5.  **可视化数据**

# 排名第一的图像挖掘

大多数数据科学问题都始于寻找与任务相关的数据集。然而，大多数时候，并不存在针对您的问题的数据集。我们必须建立自己的。对于那些计划使用计算机视觉的人来说，熟悉图像抓取是一项必要的技能。此外，当你制作第一个数据集时，你会意识到这比你想象的要难。

**练习 1:** 构建一个 python 脚本，使用 google images 自动获取与 Q 个查询匹配的 N 个图像。每次查询都将它们保存在一个子文件夹中。使用 N = 150 和以下十个查询:猫、狗、蜥蜴、鱼、鸟、鲨鱼、青蛙、牛、恐龙和怪物。如果你有创意，你也可以使用 instaloader 包从 Instagram 下载图片。代替动物，你可以做一个你最喜欢的影响者的数据集。

**练习 2:** 手动检查每个文件夹，删除所有与查询不完全匹配的图像，如 memes 和 drawings。统计每次查询删除了多少张照片以及删除的原因。有什么查询是重叠的吗，比如狗文件夹里的猫图片？

**练习 3:** 修改练习 1 中的脚本，为每个查询下载更多的图像，以补偿您在练习 2 中删除的图像。执行这个脚本得到 N = 1000。

**练习 4:** 对于练习 2，手动检查每个文件夹，删除重叠和不需要的图像。这一次，假图像出现的频率是更高还是更低？构建每个类一万幅图像的数据集需要多长时间？

# #2 文本挖掘

对于图像挖掘，我们经常需要收集大量的文本。虽然存在通用数据集，但我们经常处理特定的词汇表或不太常见的语言。除此之外，很多时候我们还要自己做文字清理，这是一个非常容易出错的工作。

为了磨练我们的文本挖掘技能，我们将看看从维基百科抓取文本。文字嘈杂；几篇文章可能包含一千多个独特的单词。有些可能拼错了。一些将被连字符连接。清洁图像是自然的；清理文本没有那么多。

**练习 1:** 创建一个脚本，可以从维基百科获取 N 篇文章，并提取其标题和文本。有很多方法可以做到这一点，REST APIs、pip 包、美汤等等。要获得随机页面，您可以使用“[随机页面](https://en.wikipedia.org/wiki/Special:Random)”链接。试着下载一百篇文章。

**练习 2:** 创建一个脚本，将所有文章组合成一个字符串，统计每个唯一单词的频率，按频率排序，并打印所有内容。数据集中最少/最常用的一百个单词是什么？有多少单词出现一百次以上？

**练习 3:** 使用绘图库，创建词频线图。得到的曲线是什么形状？使用对数标度重复绘图。曲线现在看起来怎么样？

**练习 4:** 手动计算每个文档每个单词的 [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) 。这是自然语言处理中最广泛使用的度量之一，用于确定哪些单词与每个文档相关。按顺序，按 TF-IDF 值对单词进行排序，并打印前 10 个单词。将它们与文章的标题进行比较。它们是相关的词吗？

**练习 5:** 要清理文本语料库，您必须将所有内容转换为小写，删除停用词，删除数字，修复错别字，取消缩写，并使每个单词正常化(词干化和词条化)。此过程有效地删除了所有冠词、介词、性别标记、复数标记，并将所有动词转换为不定式。最后，通常会删除 10%或 20%最不常用的单词。这整个过程是做好自然语言处理最关键的任务之一。寻找如何执行此操作的教程。 [NLTK](https://www.nltk.org) 是一个优秀的工具[大量教程](https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae)。

**练习 6:** 如果你在清理文集时幸存下来，重复练习 4。每个文档最相关的单词有变化吗？

**练习 7:** 有些词放在一起有不同的意思。“The”、“United”和“States”在单独使用时有各自的含义，在一起使用时表示“美国”。找到这样的二元模型和三元模型。在高层次上，这可以通过将两个词随机一起出现的概率与它们在数据集中一起出现的实际频率进行比较来实现。

# #3 数据集预处理

有时我们会找到一个可以使用的数据集。然而，除了主流的，到达“X_train，y_train，X_test，y_test”可能没有你想的那么简单。在实践中，数据科学家必须每天驯服数据野兽。这里有一些数据集，你必须做一些手工工作才能让它们为深度学习做好准备。

**练习 1:** 处理原始的 [CIFAR-10 和 CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) 数据集。这包括下载它，用 pickle 打开文件，对加载的数组进行整形，以及将图像规范化为[0，1]范围内的浮点数。

**练习 2:** 处理原始 [Flower-17 数据集](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/)。执行与练习 1 中相同的任务。这一次，下载中没有标签信息；你必须想办法得到它。

**练习 3:** 处理原始的 [Flower-102 数据集](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)。这次有一个 [Matlab 文件](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat)，你可以用它来弄清楚如何打开和提取相关数据。另一个选项是使用[这个页面](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/categories.html)来获得每个类的实例计数。

**练习 4:** 之前我们手动下载了一些维基百科的文章来做一些 NLP 的任务。这次，你的任务是下载整个英文维基百科。换句话说，所有文章的所有文本。幸运的是，维基百科本身提供了许多下载其全部目录的方法。不要试图一条一条地做这件事。您的工作是找到一个转储，下载、读取并解析它。这意味着要完成整个清洁程序。

**练习 5:** 有时，我们必须增强数据集。CIFAR-100 有一百个类别，但它只有图像数据。你的任务是使用你的维基百科搜集技能为每个 CIFAR-100 类找到 50 个相关单词。衡量这 50 个词在类别和子类别中有多少重叠。

# #4 使用预先训练的模型

对于大多数概念验证，预先训练的模型就足够了。 [TensorFlow Hub](https://tfhub.dev/) 是一个很好的地方，可以感受到外面有什么可以使用。存在许多用于图像分类、对象检测、情感分析、文本处理等的模型。知道如何应用、微调和重新训练一些最著名的模型是快速执行数据任务的必要技能。

这些练习旨在让您使用一些最常用的模型来完成一些最常见的任务。

**练习 1:** 为 ImageNet 数据集获取预训练版本的 ResNet50。它的输出是一个 1000 元素的向量，其类别置信度为 ImageNet 挑战中使用的 1000 个类别。您的工作是创建一个脚本，该脚本使用 ResNet 模型获取图像并打印其类。您还可以让它编写五个最可能的类，取五个最高的输出值。

**练习 2:** 针对 MobileNetV3 模型重复练习 1。下载 1000 张图片，测量每个模型处理这些图片需要多长时间。精度是否受到显著影响？哪款比较好？

**练习 3:** 启动并运行 YOLO 和更快的 R-CNN 模型。您可以使用 [COCO 2017 验证图像](http://cocodataset.org/#download)、一些 Youtube 视频或您的网络摄像头作为检测的数据流。找出它们支持的对象类型。哪个更快更准确，准确多少？

**练习 4:** 对于文本，大多数模型会给每个单词分配数值来表示它们的含义。这就是所谓的单词嵌入。使用 BERT 模型对 [IMDb 评论数据集](http://ai.stanford.edu/~amaas/data/sentiment/)进行情感分析。为此，您需要微调 BERT。情况如何？

# #5 可视化

数据可视化的重要性怎么强调都不为过。没有它，我们就看不到我们的数据所能提供的所有机会。我们可以用它来完成模型解释、错误分析和数据理解等任务。在我在本文中介绍的所有技能中，可视化是最需要独创性和想象力的一种。俗话说，一图胜千言。在下文中，我提出了一些问题，你必须找出一个或多个回答这些问题的视觉化图像。

**练习 1:** 考虑一下 [IMDb 评论数据集](http://ai.stanford.edu/~amaas/data/sentiment/)。你能想出一个好方法来形象化哪些单词代表积极和消极的类别吗？修剪最频繁的单词是否提高了你的视觉化？如果你训练 BERT 来解决这个问题，试着把它出错的评论可视化。这些评论有什么特别之处吗？

**练习 2:** 下载 [CelebA 数据集](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)。画出每个属性的频率。哪些是最频繁/最不频繁的？你如何快速总结每堂课对非英语人士的意义？有些标签非常主观，是不是所有“有吸引力”类别下的面孔都对你有吸引力？其他一些不属于这一类的图片有吸引力吗？

**练习 3:** 微调 ResNet50 和 MobileNetV3，以求解 [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) 数据集，并保存它们对所有测试图像的预测。两个模型在预测中同意/不同意的频率是多少？哪些是他们比较认同/不认同的阶层？他们都搞错的图片有哪些？

**练习 4:** 下载 [Flower-102 数据集](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/)。每种的原色是什么？有多色的花吗？花可以按颜色分组，也可以按名称有意义地分组吗？

**练习 5:** 下载针对 ImageNet 问题训练过的 VGG 网络，应用到某个图像上。检查其第一个卷积，每个滤波器可以识别哪些模式？第二层和第三层可以这样回答吗？

这些是有抱负的数据科学家需要掌握的五项最基本的技能。如果您完成了这些练习，您将熟悉如何获取文本和图像数据、利用现有数据集、使用预先训练的模型以及使用可视化技术来深入了解您的数据、模型和错误。经验丰富的开发人员应该已经熟悉这些任务，并对大多数问题的结果有一个有根据的猜测。

我希望这些能让你感兴趣，并能帮助你锻炼数据科学的肌肉:)

对于研究型的乡亲们，我也写了一篇文章，对 2020 年 AI 论文阅读的十点建议。看看这个。

[](/ai-papers-to-read-in-2020-ac0e4e91d915) [## 2020 年要读的 AI 论文

### 阅读建议让你了解人工智能和数据科学的最新经典突破

towardsdatascience.com](/ai-papers-to-read-in-2020-ac0e4e91d915) 

如果您对本文有任何问题，请随时发表评论或与我联系。如果你是中新，我强烈推荐[订阅](https://ygorserpa.medium.com/membership)。对于数据和 IT 专业人员来说，中型文章是 StackOverflow 的完美搭档，对于新手来说更是如此。注册时请考虑使用[我的会员链接。](https://ygorserpa.medium.com/membership)

感谢阅读:)