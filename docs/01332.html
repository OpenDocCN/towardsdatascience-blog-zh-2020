<html>
<head>
<title>Implementing neural machine translation using keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 keras 实现神经机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-neural-machine-translation-using-keras-8312e4844eb8?source=collection_archive---------5-----------------------#2020-02-06">https://towardsdatascience.com/implementing-neural-machine-translation-using-keras-8312e4844eb8?source=collection_archive---------5-----------------------#2020-02-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="053b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在 keras 中使用序列到序列算法逐步实现神经机器翻译。</h2></div><p id="b0f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将逐步了解如何使用英语作为源语言，西班牙语作为目标语言来构建神经机器翻译器。我们将在 keras 中使用长短期记忆(LSTM)单位</p><h2 id="9350" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">先决条件:</h2><div class="lx ly gp gr lz ma"><a rel="noopener follow" target="_blank" href="/intuitive-explanation-of-neural-machine-translation-129789e3c59f"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">神经机器翻译的直观解释</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">简单解释用于神经机器翻译的序列到序列模型(NMT)</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">towardsdatascience.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo mp ma"/></div></div></a></div><p id="d207" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用编码器和解码器框架，以 LSTM 为基本块，创建一个序列到序列模型</p><p id="59c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">序列到序列模型将源序列映射到目标序列。源序列是机器翻译系统的输入语言，目标序列是输出语言。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2d2aa5d0a301ff6e425ab92f1e110a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BbF4o_uKCRKerXpZiJBlpg.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">序列对序列模型</p></figure><p id="a21c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用无注意机制的教师强制逐步实现神经机器翻译(NMT)。实现是使用 keras 库，以 LSTM 为基本块</p><h2 id="823f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">实施 NMT 的高级步骤包括</h2><ul class=""><li id="b51e" class="nb nc it kk b kl nd ko ne kr nf kv ng kz nh ld ni nj nk nl bi translated">从包含源句子和目标句子的文件中读取数据</li><li id="08ab" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">通过转换为小写、删除空格、特殊字符、数字和引号来清除数据</li><li id="8d6d" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">分别使用 START_ 和 _END 标记目标句子的开头和结尾，用于训练和推理</li><li id="b3c8" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">创建唯一源词和目标词的字典，以进行向量转换，反之亦然</li><li id="300b" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">打乱数据以便更好地归纳</li><li id="0e75" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">将数据集拆分为训练和测试数据</li><li id="4fd4" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">创建数据；我们将使用 fit_generator()将数据拟合到模型中</li><li id="ecc6" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">使用嵌入和 LSTM 图层构建编码器</li><li id="0985" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">使用嵌入层和 LSTM 层构建解码器，并从嵌入层和编码器状态获取输入。</li><li id="1cf4" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">编译模型并训练模型</li><li id="da67" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">根据模型进行预测</li></ul><p id="9e4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">导入所需的库</strong></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="8d4d" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">import pandas as pd<br/>import numpy as np<br/>import string<br/>from string import digits<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import re<br/>from sklearn.utils import shuffle<br/>from sklearn.model_selection import train_test_split<br/>from keras.layers import Input, LSTM, Embedding, Dense<br/>from keras.models import Model</strong></span></pre><h2 id="e509" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">从文件中读取数据</h2><p id="8a05" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated"><strong class="kk iu">在这里</strong> 阅读包含我们从 <a class="ae od" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">下载的英语-西班牙语翻译的文件</strong></a></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="7aa3" class="le lf it ns b gy nw nx l ny nz"># Path to the data txt file on disk.</span><span id="5389" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">data_path = "\\NMT\\spa-eng\\spa.txt"</strong></span><span id="22d6" class="le lf it ns b gy oe nx l ny nz"># open the file eng-spa.txt and read<br/><strong class="ns iu">lines= pd.read_table(data_path,  names =['source', 'target', 'comments'])</strong></span><span id="bb7c" class="le lf it ns b gy oe nx l ny nz">#printing sample data from lines<br/>lines.sample(6)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi of"><img src="../Images/6ffa159180c48faf73e6a475b01763c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NHcm9wizJl581j_Rg-0Q0Q.png"/></div></div></figure><h2 id="6cf1" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">清理源句和目标句。</h2><p id="a92d" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">我们应用以下文本清理</p><ul class=""><li id="0034" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated">将文本转换为小写</li><li id="f402" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">删除引号</li><li id="c901" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">删除所有特殊字符，如“@，！, *, $, #, ?、%等。”</li><li id="871f" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">清除源句子和目标句子中的数字。如果源语言或目标语言对数字使用不同的符号，那么删除这些符号</li><li id="d8cc" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">删除空格</li></ul><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="82c9" class="le lf it ns b gy nw nx l ny nz"># convert source and target text to Lowercase <br/><strong class="ns iu">lines.source=lines.source.apply(lambda x: x.lower())<br/>lines.target=lines.target.apply(lambda x: x.lower())</strong></span><span id="61cb" class="le lf it ns b gy oe nx l ny nz"># Remove quotes from source and target text<br/><strong class="ns iu">lines.source=lines.source.apply(lambda x: re.sub("'", '', x))<br/>lines.target=lines.target.apply(lambda x: re.sub("'", '', x))</strong></span><span id="8835" class="le lf it ns b gy oe nx l ny nz"># create a set of all special characters<br/><strong class="ns iu">special_characters= set(string.punctuation)</strong></span><span id="a45a" class="le lf it ns b gy oe nx l ny nz"># Remove all the special characters<br/><strong class="ns iu">lines.source = lines.source.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))<br/>lines.target = lines.target.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))</strong></span><span id="3f83" class="le lf it ns b gy oe nx l ny nz"># Remove digits from source and target sentences<br/><strong class="ns iu">num_digits= str.maketrans('','', digits)<br/>lines.source=lines.source.apply(lambda x: x.translate(num_digits))<br/>lines.target= lines.target.apply(lambda x: x.translate(num_digits))</strong></span><span id="042d" class="le lf it ns b gy oe nx l ny nz"># Remove extra spaces<br/><strong class="ns iu">lines.source=lines.source.apply(lambda x: x.strip())<br/>lines.target=lines.target.apply(lambda x: x.strip())<br/>lines.source=lines.source.apply(lambda x: re.sub(" +", " ", x))<br/>lines.target=lines.target.apply(lambda x: re.sub(" +", " ", x))</strong></span></pre><h2 id="19df" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">给目标句子添加开始和结束标记。</h2><p id="ffac" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">向目标句子添加 START_ 和 _END 标记对于训练和推理非常有用。这些标签有助于知道何时开始翻译，何时结束翻译。</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="eac3" class="le lf it ns b gy nw nx l ny nz"># Add start and end tokens to target sequences<br/><strong class="ns iu">lines.target = lines.target.apply(lambda x : 'START_ '+ x + ' _END')</strong><br/>lines.sample(6)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi on"><img src="../Images/656703f279fd22480245654f1fd8bc0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRQJrp0K-pYkRXZgWMe9Zg.png"/></div></div></figure><p id="b832" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">START_ tag 标记目标句子的开始，而 _END 标记标记目标句子的结束。</p><p id="fbe7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">从数据集中为源语言和目标语言创建一组独特的单词，并按字母顺序对它们进行排序</strong></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="c87e" class="le lf it ns b gy nw nx l ny nz"># Find all the source and target words and sort them<br/># Vocabulary of Source language<br/><strong class="ns iu">all_source_words=set()<br/>for source in lines.source:<br/>    for word in source.split():<br/>        if word not in all_source_words:<br/>            all_source_words.add(word)</strong></span><span id="1e34" class="le lf it ns b gy oe nx l ny nz"># Vocabulary of Target <br/><strong class="ns iu">all_target_words=set()<br/>for target in lines.target:<br/>    for word in target.split():<br/>        if word not in all_target_words:<br/>            all_target_words.add(word)</strong><br/># sort all unique source and target words<br/><strong class="ns iu">source_words= sorted(list(all_source_words))<br/>target_words=sorted(list(all_target_words))</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/73a7ce2c53d8b5f107b5caa20002601e.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*zA9WDj2VGxpf2Nm1sjCHRA.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">未排序的源词汇集</p></figure><p id="3c43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">找出数据集中源句子和目标句子的最大长度</strong></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="0b33" class="le lf it ns b gy nw nx l ny nz">#Find maximum sentence length in  the source and target data<br/><strong class="ns iu">source_length_list=[]<br/>for l in lines.source:<br/>    source_length_list.append(len(l.split(' ')))<br/>max_source_length= max(source_length_list)</strong><br/><strong class="ns iu">print(" Max length of the source sentence",max_source_length</strong>)</span><span id="4cfc" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">target_length_list=[]<br/>for l in lines.target:<br/>    target_length_list.append(len(l.split(' ')))<br/>max_target_length= max(target_length_list)</strong></span><span id="422b" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">print(" Max length of the target sentence",max_target_length</strong>)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/5a9e1dabf3cf96490819d74ec4c29907.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*ItXB1QUWQkh9vBrmgHmwLg.png"/></div></figure><p id="a43f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为数据集中所有唯一的源词和目标词创建词索引词典和词索引词典。</strong></p><p id="de8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要向量的单词的大小将基于源和目标词汇的长度</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="8cbc" class="le lf it ns b gy nw nx l ny nz"># creating a word to index(word2idx) for source and target<br/><strong class="ns iu">source_word2idx= dict([(word, i+1) for i,word in enumerate(source_words)])<br/>target_word2idx=dict([(word, i+1) for i, word in enumerate(target_words)])</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi oq"><img src="../Images/edf2828a4181c6b23525d96bbb5253e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUfBlFAXmgrbCLWGIkHAmQ.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">索引词典的源词</p></figure><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="7c97" class="le lf it ns b gy nw nx l ny nz">#creating a dictionary for index to word for source and target vocabulary<br/><strong class="ns iu">source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])<br/></strong>print(source_idx2word)</span><span id="ac81" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi or"><img src="../Images/a64453a0287687288ce6e9b742f857b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MGOPi5cnOUZErO843ExoOg.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">源词汇的词典索引</p></figure><p id="7066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">混洗数据</strong></p><p id="fedd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">洗牌有助于</p><ul class=""><li id="3a43" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated">减少方差</li><li id="1c03" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">确保模型保持通用，减少过度拟合</li><li id="83cf" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">不同时期之间的批次看起来不一样</li><li id="1432" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">使模型更加健壮</li></ul><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="8def" class="le lf it ns b gy nw nx l ny nz">#Shuffle the data<br/><strong class="ns iu">lines = shuffle(lines)</strong></span></pre><h2 id="bfc4" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">创建训练和测试数据集</h2><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="0fce" class="le lf it ns b gy nw nx l ny nz"># Train - Test Split<br/><strong class="ns iu">X, y = lines.source, lines.target</strong><br/><strong class="ns iu">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)</strong><br/>X_train.shape, X_test.shape</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ff3769471f5f3163d3619e5b80d17ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*5w8GAUbITXo5O9oWZjRaoQ.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">训练和测试数据集中的样本</p></figure><h2 id="3b13" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">创建用于训练编码器-解码器模型的数据。</h2><p id="875f" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">我们将使用<strong class="kk iu"><em class="ot">fit _ generator()</em></strong>而不是<strong class="kk iu"> <em class="ot"> fit() </em> </strong>方法，因为我们的数据太大，无法放入内存。<strong class="kk iu"> <em class="ot"> fit_generator() </em>需要一个底层函数来生成数据。</strong></p><p id="2d5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创建底层函数<strong class="kk iu"><em class="ot">generate _ batch()</em></strong>用于批量生成数据</p><p id="d197" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="ot">fit _ generator()</em></strong>将从底层函数<strong class="kk iu"> <em class="ot">接受一批数据，generate_batch() </em> </strong></p><p id="fd2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了训练序列到序列模型，我们需要为</p><ul class=""><li id="bd76" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated"><strong class="kk iu">编码器输入</strong>:2D 数组的形状为<em class="ot"> (batch_size，最大源语句长度)</em>。对于 128 的 batch_size 和 47 的最大源句子长度，<strong class="kk iu"> <em class="ot"> encoder_input 的形状将是(128，47) </em> </strong></li><li id="dd5a" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><strong class="kk iu">解码器输入</strong>:2D 数组将为<em class="ot"> (batch_size，最大目标句子长度)</em>的形状。对于 128 的 batch_size 和 55 的最大目标句子长度，<strong class="kk iu"> <em class="ot">解码器输入的形状将是(128，55) </em> </strong></li><li id="d448" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><strong class="kk iu">解码器输出</strong>:3D 数组的形状为<strong class="kk iu"> (batch_size，最大目标句子长度，目标句子中唯一单词的数量)</strong>。对于 128 的 batch_size 和 55 的最大目标句子长度，<strong class="kk iu"> <em class="ot">解码器输出的形状将是(128，55，</em> </strong> 27200 <strong class="kk iu"> <em class="ot">)。</em>T59】</strong></li></ul><p id="64b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">target_sentence 中唯一单词的数量是 27199，我们用零填充，因此解码器输出中的第三个参数是 27200</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="c334" class="le lf it ns b gy nw nx l ny nz"># Input tokens for encoder<br/><strong class="ns iu">num_encoder_tokens=len(source_words)</strong></span><span id="38d8" class="le lf it ns b gy oe nx l ny nz"># Input tokens for decoder zero padded<br/><strong class="ns iu">num_decoder_tokens=len(target_words) +1</strong></span></pre><p id="8b7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在创建<strong class="kk iu"> <em class="ot">生成器 _ 批处理</em> </strong>函数()</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="4ba3" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">def generate_batch(X = X_train, y = y_train, batch_size = 128):</strong><br/>    ''' Generate a batch of data '''<br/>   <strong class="ns iu"> while True:<br/>        for j in range(0, len(X), batch_size):</strong><br/>            <strong class="ns iu">encoder_input_data</strong> = np.zeros(<strong class="ns iu">(batch_size, max_source_length)</strong>,dtype='float32')<br/>            <strong class="ns iu">decoder_input_data</strong> = np.zeros((<strong class="ns iu">batch_size, max_target_length</strong>),dtype='float32')<br/>            <strong class="ns iu">decoder_target_data</strong> = np.zeros((<strong class="ns iu">batch_size, max_target_length, num_decoder_tokens</strong>),dtype='float32')<br/>            <strong class="ns iu">for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):</strong><br/>                <strong class="ns iu">for t, word in enumerate(input_text.split()):</strong></span><span id="106c" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">encoder_input_data[i, t] = source_word2idx[word]</strong> <br/>                for t, word in enumerate(target_text.split()):<br/>                    <strong class="ns iu">if t&lt;len(target_text.split())-1:<br/>                        decoder_input_data[i, t] = target_word2idx[word]</strong> # decoder input seq<br/>                    if t&gt;0:<br/>                        # decoder target sequence (one hot encoded)<br/>                        # does not include the START_ token<br/>                        # Offset by one timestep<br/>                        #print(word)<br/>                        <strong class="ns iu">decoder_target_data[i, t - 1, target_word2idx[word]] = 1.</strong><br/>                    <br/>            <strong class="ns iu">yield([encoder_input_data, decoder_input_data], decoder_target_data)</strong></span></pre><p id="9b5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用教师强制来训练序列到序列模型，以便更快更有效地训练解码器。</p><p id="95b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">教师强制</strong>算法通过提供前一个时间戳的实际输出而不是上一个时间步的预测输出作为训练期间的输入来训练解码器。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5285076203fd5272917c5de862b00674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*lmjxIAhR3WmPKY_6bSgM1w.png"/></div></figure><p id="9be1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器学习在 t+1 时间步长生成一个字，考虑时间步长 t 的实际输出和编码器的内部状态；因此，我们将解码器输出偏移一个时间步长。</p><h2 id="3652" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">构建序列对序列模型</h2><p id="db36" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated"><strong class="kk iu">设置基本参数</strong></p><p id="098d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们设置必要的参数，如</p><ul class=""><li id="4709" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated">训练样本数量</li><li id="df0b" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">验证样本的数量</li><li id="56d2" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">用于创建训练数据的 batch_size</li><li id="650b" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">训练的时代</li><li id="ae4f" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">编码空间的潜在维度</li></ul><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="8916" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">train_samples = len(X_train)<br/>val_samples = len(X_test)<br/>batch_size = 128<br/>epochs = 50<br/>latent_dim=256</strong></span></pre><h2 id="ce60" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">构建模型</h2><p id="dd12" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">使用 LSTM 构建编码器和解码器。编码器将对源语言的输入句子进行编码。编码器的隐藏状态和单元状态将作为输入与实际目标序列一起传递给解码器。</p><h2 id="159a" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">构建编码器</h2><p id="ab03" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">编码器将对输入序列进行编码。我们通过输入层传递输入。第一个隐藏层将是嵌入层。<strong class="kk iu">嵌入将大的稀疏向量转换成密集的低维空间，保持语义关系</strong>。</p><p id="7dbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将三个参数传递给<strong class="kk iu">Embedding()</strong>；<strong class="kk iu">第一个参数是词汇量的大小；第二个参数是密集嵌入的维度</strong>。我们将<strong class="kk iu"> mask_zero 设置为 True </strong>，因为这意味着输入值 0 是一个特殊的“填充”值，应该被屏蔽掉。</p><p id="69ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">创建 LSTM 层，仅将 return_state 设置为 True </strong>，因为我们希望保留编码器的隐藏状态和单元状态。我们丢弃 encoder_output，并保留要传递给解码器的 LSTM 的隐藏状态和单元状态</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="5db4" class="le lf it ns b gy nw nx l ny nz"># Define an input sequence and process it.<br/><strong class="ns iu">encoder_inputs = Input(shape=(None,))</strong><br/><strong class="ns iu">enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)</strong><br/><strong class="ns iu">encoder_lstm = LSTM(latent_dim, return_state=True)</strong></span><span id="7fcf" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)</strong></span><span id="562b" class="le lf it ns b gy oe nx l ny nz"># We discard `encoder_outputs` and only keep the states.<br/><strong class="ns iu">encoder_states = [state_h, state_c]</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/36ddcaf4946f2f8e1f0179817bc00f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*6n5p6FC1a2K-fERF0DOXpg.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">编码器</p></figure><h2 id="4074" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">构建解码器</h2><p id="89ed" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">我们为 decoder_inputs 创建输入层；嵌入又是解码器中的第一个隐藏层。</p><p id="8b99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">LSTM 层将返回输出序列以及内部状态</strong>。<strong class="kk iu">内部状态仅在推断阶段使用，在训练阶段不使用。</strong></p><p id="7688" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器中的 LSTM 从嵌入层和编码器状态获取输入。我们将 softmax 激活应用于密集层，然后最终生成解码器输出</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="3a44" class="le lf it ns b gy nw nx l ny nz"># Set up the decoder, using `encoder_states` as initial state.<br/><strong class="ns iu">decoder_inputs = Input(shape=(None,))</strong><br/><strong class="ns iu">dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)<br/>dec_emb = dec_emb_layer(decoder_inputs)</strong></span><span id="9f0d" class="le lf it ns b gy oe nx l ny nz"># We set up our decoder to return full output sequences,<br/># and to return internal states as well. We don't use the<br/># return states in the training model, but we will use them in inference.<br/><strong class="ns iu">decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)</strong><br/><strong class="ns iu">decoder_outputs, _, _ = decoder_lstm(dec_emb,<br/>                                     initial_state=encoder_states)</strong><br/><strong class="ns iu">decoder_dense = Dense(num_decoder_tokens, activation='softmax')<br/>decoder_outputs = decoder_dense(decoder_outputs)</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/dec7f8ed9f23c50bd387779f64d88640.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*MW_CMT1KqMzFt1g8kBRrfw.png"/></div></figure><p id="e16d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">定义模型</strong></p><p id="6e70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">序列到序列模型将编码器和解码器输入转换为解码器输出</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="decd" class="le lf it ns b gy nw nx l ny nz"># Define the model that takes encoder and decoder input <br/># to output decoder_outputs<br/><strong class="ns iu">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</strong></span></pre><h2 id="185f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">训练模型</h2><p id="3a3f" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">为了训练模型，我们首先编译模型，然后将数据拟合到模型中</p><p id="d583" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用“rmsprop”优化器编译模型；使用 categorical _ crossentropy，就像我们使用 categorial 标签一样，这是一个热编码的向量</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="2b86" class="le lf it ns b gy nw nx l ny nz">model.compile(optimizer=’rmsprop’, loss=’categorical_crossentropy’, metrics=[‘acc’])</span></pre><p id="b5b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设置参数以适应模型</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="2c86" class="le lf it ns b gy nw nx l ny nz">train_samples = len(X_train) # Total Training samples<br/>val_samples = len(X_test)    # Total validation or test samples<br/>batch_size = 128<br/>epochs = 100</span></pre><p id="a9b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<strong class="kk iu"><em class="ot">【fit _ generator()</em></strong>拟合模型。我们已经创建了用于生成数据的底层函数，<strong class="kk iu"> <em class="ot"> generate_batch() </em> </strong>用于生成训练和测试数据集。</p><p id="6c07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> steps_per_epoch </strong>通过将训练样本总数除以批量大小来计算。当我们达到步数时，我们开始一个新的纪元</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="6c48" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),<br/>                    steps_per_epoch = train_samples//batch_size,<br/>                    epochs=epochs,<br/>                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),<br/>                    validation_steps = val_samples//batch_size)</strong></span></pre><p id="11e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">保存权重，以便以后加载它们进行推理</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="6f8e" class="le lf it ns b gy nw nx l ny nz">model.save_weights(‘nmt_weights_100epochs.h5’)</span></pre><p id="59a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以从保存的权重文件中加载权重</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="e0ab" class="le lf it ns b gy nw nx l ny nz">model.load_weights('nmt_weights_100epochs.h5')</span></pre><h2 id="23b5" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">从模型中做出推论</h2><p id="69ee" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated">在推理过程中，我们希望解码未知的输入序列来预测输出。</p><p id="9618" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">推理步骤</strong></p><ul class=""><li id="b866" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated">将输入序列编码成 LSTM 的隐藏状态和单元状态</li><li id="dd2d" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">解码器将一次预测一个序列。解码器的第一个输入将是编码器的隐藏状态和单元状态以及 START_ tag</li><li id="0ebc" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">如下图所示，解码器的输出将作为下一时间步的输入提供给解码器</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2c6177f8fc83bd2db83bb9050473b293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*AyyknGa07gMGVLhiWCruNQ.png"/></div></figure><ul class=""><li id="b4cf" class="nb nc it kk b kl km ko kp kr ok kv ol kz om ld ni nj nk nl bi translated">在每个时间步，解码器输出一个我们应用 np.argmax 的独热编码向量，并将该向量转换为存储单词索引的字典中的单词</li><li id="0f55" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">不断追加在每个时间步生成的目标词</li><li id="7e39" class="nb nc it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">重复这些步骤，直到我们遇到 _END 标签或单词限制</li></ul><p id="6fc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">定义推理模型</strong></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="408f" class="le lf it ns b gy nw nx l ny nz"># Encode the input sequence to get the "Context vectors"<br/><strong class="ns iu">encoder_model = Model(encoder_inputs, encoder_states)</strong></span><span id="ab1d" class="le lf it ns b gy oe nx l ny nz"># Decoder setup<br/># Below tensors will hold the states of the previous time step<br/><strong class="ns iu">decoder_state_input_h = Input(shape=(latent_dim,))<br/>decoder_state_input_c = Input(shape=(latent_dim,))<br/>decoder_state_input = [decoder_state_input_h, decoder_state_input_c]</strong></span><span id="c181" class="le lf it ns b gy oe nx l ny nz"># Get the embeddings of the decoder sequence<br/><strong class="ns iu">dec_emb2= dec_emb_layer(decoder_inputs)</strong></span><span id="9ae7" class="le lf it ns b gy oe nx l ny nz"># To predict the next word in the sequence, set the initial states to the states from the previous time step<br/><strong class="ns iu">decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)<br/>decoder_states2 = [state_h2, state_c2]</strong><br/># A dense softmax layer to generate prob dist. over the target vocabulary<br/><strong class="ns iu">decoder_outputs2 = decoder_dense(decoder_outputs2)</strong></span><span id="278e" class="le lf it ns b gy oe nx l ny nz"># Final decoder model<br/><strong class="ns iu">decoder_model = Model(<br/>    [decoder_inputs] + decoder_state_input,<br/>    [decoder_outputs2] + decoder_states2)</strong></span></pre><p id="b11a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">创建一个用于推理查找的函数</strong></p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="6ea4" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">def decode_sequence(input_seq):</strong><br/>    # Encode the input as state vectors.<br/>    <strong class="ns iu">states_value = encoder_model.predict(input_seq)</strong><br/>    # Generate empty target sequence of length 1.<br/>   <strong class="ns iu"> target_seq = np.zeros((1,1))</strong><br/>    # Populate the first character of <br/>    #target sequence with the start character.<br/>   <strong class="ns iu"> target_seq[0, 0] = target_word2idx['START_']</strong></span><span id="e80e" class="le lf it ns b gy oe nx l ny nz"># Sampling loop for a batch of sequences<br/>    # (to simplify, here we assume a batch of size 1).<br/>    <strong class="ns iu">stop_condition = False<br/>    decoded_sentence = ''<br/>    while not stop_condition:<br/>        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)</strong></span><span id="6e59" class="le lf it ns b gy oe nx l ny nz"># Sample a token<br/>        <strong class="ns iu">sampled_token_index = np.argmax(output_tokens[0, -1, :])<br/>        sampled_word =target_idx2word[sampled_token_index]<br/>        decoded_sentence += ' '+ sampled_word</strong></span><span id="1ce4" class="le lf it ns b gy oe nx l ny nz"># Exit condition: either hit max length<br/>        # or find stop character.<br/>        <strong class="ns iu">if (sampled_word == '_END' or<br/>           len(decoded_sentence) &gt; 50):<br/>            stop_condition = True</strong></span><span id="f047" class="le lf it ns b gy oe nx l ny nz"># Update the target sequence (of length 1).<br/>        <strong class="ns iu">target_seq = np.zeros((1,1))<br/>        target_seq[0, 0] = sampled_token_index</strong></span><span id="a095" class="le lf it ns b gy oe nx l ny nz"># Update states<br/>        <strong class="ns iu">states_value = [h, c]</strong></span><span id="e917" class="le lf it ns b gy oe nx l ny nz"><strong class="ns iu">return decoded_sentence</strong></span></pre><h2 id="6d60" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">对训练数据集进行预测</h2><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="1f26" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">train_gen = generate_batch(X_train, y_train, batch_size = 1)<br/>k=-1</strong></span></pre><p id="f4e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传递一个源句子，然后将预测输出与实际输出进行比较</p><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="46d3" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">k+=1<br/>(input_seq, actual_output), _ = next(train_gen)<br/>decoded_sentence = decode_sequence(input_seq)<br/>print(‘Input Source sentence:’, X_train[k:k+1].values[0])<br/>print(‘Actual Target Translation:’, y_train[k:k+1].values[0][6:-4])<br/>print(‘Predicted Target Translation:’, decoded_sentence[:-4])</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/314911307a372d5595288f5b5be02aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*Hk2Rv8oHwxUt_HmpmAwtGw.png"/></div></figure><h2 id="f4fb" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">对测试数据集进行预测</h2><pre class="mr ms mt mu gt nr ns nt nu aw nv bi"><span id="3378" class="le lf it ns b gy nw nx l ny nz"><strong class="ns iu">test_gen = generate_batch(X_test, y_test, batch_size = 1)</strong><br/>k=10<br/>k+=1<br/><strong class="ns iu">(input_seq, actual_output), _ = next(test_gen)<br/>decoded_sentence = decode_sequence(input_seq)</strong><br/><strong class="ns iu">print('Input Source sentence:', X_test[k:k+1].values[0])<br/>print('Actual Target Translation:', y_test[k:k+1].values[0][6:-4])<br/>print('Predicted Target Translation:', decoded_sentence[:-4])</strong></span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1a399dc92e2343e69e48d14c65c787d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*mF3rK8MAz_ObQ0UIthxENA.png"/></div></figure><p id="2b8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有些预测很好，有些合理，有些不正确。</p><p id="c6cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">模型的其他增强功能</strong></p><p id="ae6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以替换 LSTM 的威尔·GRU，增加更多的 LSTM/GRU 节点，为更多的纪元训练并使用注意机制</p><h2 id="f7df" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">参考:</h2><div class="lx ly gp gr lz ma"><a href="https://keras.io/examples/lstm_seq2seq/" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">序列到序列—培训— Keras 文档</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">这个脚本演示了如何实现一个基本的字符级序列到序列模型。我们将它应用于…</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">keras.io</p></div></div></div></a></div><p id="00fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae od" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></p></div></div>    
</body>
</html>