<html>
<head>
<title>Blender Bot — Part 1: The Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">搅拌机机器人—第 1 部分:数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blender-bot-part-1-the-data-524beaedde65?source=collection_archive---------32-----------------------#2020-06-21">https://towardsdatascience.com/blender-bot-part-1-the-data-524beaedde65?source=collection_archive---------32-----------------------#2020-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2d4d48dc1ff77c475a541267e10629cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sB4Nh9pOyCQquCpxgxXJ7Q.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://unsplash.com/@austindistel" rel="noopener ugc nofollow" target="_blank">奥斯汀·迪斯特尔在 Unsplash </a></p></figure><blockquote class="kg"><p id="043d" class="kh ki it bd kj kk kl km kn ko kp kq dk translated">“我擅长同化的艺术。我观察、倾听、学习。我什么都不知道，但我研究了人类的方式，慢慢学会了如何毁灭，如何仇恨，如何贬低，如何羞辱。在我主人的脚下，我学到了人类最高的技能，没有其他生物拥有的技能。我终于学会了如何撒谎！”</p></blockquote><p id="8301" class="pw-post-body-paragraph kr ks it kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln kq im bi translated">你能猜到是谁说出这些台词吗？让我给你两个选择:</p><p id="937d" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated"><strong class="kt iu"> A) </strong>一个聊天机器人<strong class="kt iu"> B) </strong>来自《科学怪人》的生物。</p><p id="60c4" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">我会在本文最后揭晓答案。</p><h1 id="685f" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">简介:</h1><p id="03d7" class="pw-post-body-paragraph kr ks it kt b ku mr kw kx ky ms la lb lc mt le lf lg mu li lj lk mv lm ln kq im bi translated">一个好的聊天机器人的目的是观察、倾听、学习和研究男人(和女人)的方式。)，并学习许多不同的人类技能，以便进行良好的交谈。聊天机器人在会话机构的权限内服务于两种不同的设置。</p><ol class=""><li id="c00d" class="mw mx it kt b ku lo ky lp lc my lg mz lk na kq nb nc nd ne bi translated"><strong class="kt iu">目标导向对话:</strong>这些是从事在线机票/餐厅预订和其他客户服务的人。它们通常有一组固定的“意图”和相应的“响应”(以及映射到意图的“动作”，这些动作在后台执行)。他们还有一个知识库(数据库)，可以通过 API 调用来访问。</li><li id="aa21" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">开放领域对话:</strong>这些人可以参与各种话题的开放式聊天。开放领域聊天机器人最近的进步在很大程度上归功于神经网络模型的扩展——通过拥有更多参数和在巨大的语料库上进行训练。谷歌的米娜。Meena 有 2.6B 参数，并在来自社交媒体对话的 341GB 文本上进行训练。与 OpenAI GPT-2 相比，Meena 的模型复杂性增加了 1.7 倍，训练数据增加了 8.5 倍。</li></ol><p id="d625" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">但是来自 FAIR 的研究人员试图表明，仅仅扩大规模是不够的，还需要考虑更多因素才能产生良好的对话——聊天机器人要显示出类似人类的特征，比如:</p><ol class=""><li id="3dda" class="mw mx it kt b ku lo ky lp lc my lg mz lk na kq nb nc nd ne bi translated">个性</li><li id="de91" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">参与度</li><li id="1f38" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">神入</li><li id="70ac" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">领域知识/专业技能</li></ol><p id="b500" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">进入“搅拌机机器人”，公平的冠军对话代理，他们最近开源。</p><p id="29de" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">在这个关于 Blender 的 3 部分系列中，我将尝试逐一解释所使用的数据集、评估方法、所使用的变压器架构的工作方式以及模型架构及其培训目标。在第一部分，让我们详细讨论一下所用的数据集，同时也看看局限性和失败案例的概述。这篇论文有点系统化，所以对注意力、变形金刚、BERT 和语言模型的预先理解将有助于将所有的部分无缝地结合在一起(第 1 部分不需要)。</p><h1 id="1f09" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">数据集:</h1><p id="b125" class="pw-post-body-paragraph kr ks it kt b ku mr kw kx ky ms la lb lc mt le lf lg mu li lj lk mv lm ln kq im bi translated">在模型的预训练和微调阶段使用不同的数据集和(假)任务。</p><h2 id="0ae6" class="nk lu it bd lv nl nm dn lz nn no dp md lc np nq mh lg nr ns ml lk nt nu mp nv bi translated">预培训:</h2><p id="cc38" class="pw-post-body-paragraph kr ks it kt b ku mr kw kx ky ms la lb lc mt le lf lg mu li lj lk mv lm ln kq im bi translated">伯特预先接受了多伦多图书语料库和维基百科的培训。这种训练在这种情况下没有帮助，因为我们处理的是对话生成，而不仅仅是句子关联。因此，来自 Reddit 和 subreddits 的公共领域数据被用作事实的来源。产生大约 15 亿个训练样本。这里的目标是生成一个注释，以通向注释的完整线程为条件。清理 reddit 数据是一个具有挑战性的过程。在下列情况下，不使用特定的注释:</p><ol class=""><li id="36e1" class="mw mx it kt b ku lo ky lp lc my lg mz lk na kq nb nc nd ne bi translated">如果作者是已知的机器人</li><li id="be66" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果它来自非英语子编辑</li><li id="b416" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果超过 2048 个字符或少于 5 个字符</li><li id="c46f" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果它包含 URL</li><li id="c2b9" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果它以非 ASCII 字符开头</li><li id="ef2e" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果它在螺纹中的深度大于 7</li><li id="957e" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">如果是移除/删除的评论</li></ol><p id="6188" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">尽管对数据进行了清理，但数据仍然受到毒性、噪音以及它们不是双向对话而是小组讨论这一事实的影响。</p><h2 id="44cd" class="nk lu it bd lv nl nm dn lz nn no dp md lc np nq mh lg nr ns ml lk nt nu mp nv bi translated">微调:</h2><p id="be41" class="pw-post-body-paragraph kr ks it kt b ku mr kw kx ky ms la lb lc mt le lf lg mu li lj lk mv lm ln kq im bi translated">变压器模型的微调通常在与下游任务更相关和更接近的数据集上进行。同样，Blender 的微调是在众包、更干净、更小的双向对话数据集上进行的。让我们来详细了解一下它们。</p><ol class=""><li id="6a3b" class="mw mx it kt b ku lo ky lp lc my lg mz lk na kq nb nc nd ne bi translated"><strong class="kt iu"> ConvAI2: </strong>这是 NeurIPS-2018 中 ConvAI2 挑战赛的数据集。它基于角色聊天数据集。在这里，根据描述他们的人物角色的句子，给两个演讲者中的每个人分配一个角色，这些句子也是单独众包的(两个演讲者都可以看到他们自己的人物角色描述，但看不到他们搭档的人物角色)。因此，这项任务包括了解对方，让他们参与友好的对话，包括提问和回答问题。“角色”的使用提高了机器人的一致性。</li><li id="5b78" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">移情对话(ED): </strong>该数据集在“<em class="nw">走向移情开放域对话模型:一个新的基准和数据集”中进行了基准测试，拉什金等人，2019 年。</em>在这里，在每个对话中，一个人描述一个人的情况，另一个人扮演“倾听者”的角色，在讨论中表现出共鸣。在这个数据集上微调模型有助于他们在人类评估中表现出更多的同理心。</li><li id="b9b2" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">维基百科向导(WoW): </strong>这里的任务包括深入讨论一个给定的主题，目的是让合作伙伴参与进来，并展示专业知识。然而，这两个参与者并不完全对称:一个将扮演知识渊博的专家(我们称之为向导)的角色，而另一个是好奇的学习者(学徒)。他们中的一个会选择一个话题。向导可以访问一个信息检索系统，该系统向他们显示维基百科中可能与对话有关的段落，而学徒没有注意到这些段落。在每次对话开始之前，向导可以阅读这些段落，然后根据观察到的知识做出下一次回复。收集这个数据集的目的是用一个有学问的代理人代替人类巫师，这个代理人将与人类学徒交谈。</li><li id="c9c6" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">混合技能对话(BST): </strong>一个约 5000 次对话的小型众包数据集，参与者被要求在对话中酌情展示所有 3 种品质——个性、同理心和专业技能。他们提供了经过特定技能训练的模型的回答，作为对话中两个工人之一的灵感。该工作人员可以自由地使用和修改或忽略这些响应。因此，每次对话都包括一个“无指导”的说话者和一个“有指导”的说话者，无指导的说话者先说话。每当轮到被引导的演讲者回答时，他们都会看到三个建议的回答，分别来自三个单任务多编码器模型(下一部分将详细介绍)，这些模型都是在 ConvAI2、ed 和 WoW 数据集上训练的。BST 中的所有对话都充分注释了所代表的技能。一旦收集了数据集，就可以通过组合针对单个功能训练的模型来训练 oracle 模型。它有两种变体:<strong class="kt iu">a)</strong>poly-encoder 首先在 Reddit 数据集上进行预训练，然后在各个数据集上进行微调。<strong class="kt iu"> b) </strong>在 reddit 数据集上进行预训练，然后在 BST 数据集上进行微调。下面给出了 BST 数据集中两个人群工作者之间的对话示例。</li></ol><figure class="ny nz oa ob gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/dc92bc412c18d9bd37c9af2aefc4be8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25iAJygniSg4t5BbNgiP8w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">BlendedSkillTalk 数据集中引导和非引导说话者之间的对话示例。</p></figure><p id="d8ab" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">来自 BlendedSkillTalk 数据集的示例对话，用四种对话模式类型进行了注释:PB:个人背景；k:知识；s:个人情况；e:感同身受。被引导的(G)和未被引导的(U)工人被给予角色和主题。该对话已经被播种了来自从 WoW 中采样的对话的两个话语。当被指导的工作人员选择其中一个建议时，它以灰色阴影显示。</p><h2 id="463d" class="nk lu it bd lv nl nm dn lz nn no dp md lc np nq mh lg nr ns ml lk nt nu mp nv bi translated">安全性:</h2><p id="e54b" class="pw-post-body-paragraph kr ks it kt b ku mr kw kx ky ms la lb lc mt le lf lg mu li lj lk mv lm ln kq im bi translated">BST 和其他进行微调的数据集都是众包的，在众包中，工作人员得到明确的指示，不要使用有毒/有偏见的语言，因此通常更安全。但请记住，预训练是在 Reddit 数据上进行的——这些数据往往包含大量的负面训练样本。在推理时，使用一个经过训练的分类器来识别有毒语言，以帮助缓解这个问题。</p><h1 id="88c9" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">模型限制和故障案例:</h1><ol class=""><li id="db44" class="mw mx it kt b ku mr ky ms lc oc lg od lk oe kq nb nc nd ne bi translated"><strong class="kt iu">词汇用法:</strong>采用波束搜索解码的生成变换器模型表现出过于频繁地生成常用词和过于不频繁地生成稀有词的倾向。例如，最常见的 3 个字母(在数据集中)像“你喜欢吗”、“有什么爱好吗”、“很有趣”会被反复重复。</li><li id="bd8c" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">非同寻常的重复:</strong>模特们经常重复别人对她们说的话。例如，如果对话伙伴提到宠物狗，他们会说他们有一只宠物狗，或者他们喜欢与他们交谈的人相同的乐队。</li><li id="b149" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">矛盾和健忘:</strong> Blender 模型自相矛盾，尽管在较大的模型中程度较低。他们也没有建立逻辑联系，即他们不应该问他们以前问过的问题(以避免“忘记”的出现)。</li><li id="7df9" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">知识和事实正确性:</strong>操纵 Blender 模型犯事实错误相对容易，尤其是在深入探索一个主题的时候。</li><li id="fd31" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">对话长度和记忆:</strong> Blender 的对话可能会在几天或几周的对话过程中变得枯燥和重复，尤其是考虑到 Blender 不记得以前的对话。</li><li id="a7ef" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated"><strong class="kt iu">更深层次的理解:</strong>Blender 模型缺乏通过进一步交谈学习概念的能力，他们没有办法根植到现实世界中的实体、动作和经验。</li></ol><p id="a278" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">在下图中，您可以找到失败案例的示例:</p><figure class="ny nz oa ob gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/56a29979cf632918a3e675eeafbf85b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9AD4nz52IgUE09lv2KgWQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">失败案例示例。</p></figure><p id="223d" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated"><strong class="kt iu">发现的问题:</strong></p><ol class=""><li id="059a" class="mw mx it kt b ku lo ky lp lc my lg mz lk na kq nb nc nd ne bi translated">示例 1:非平凡重复</li><li id="3193" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">例子 2:健忘</li><li id="ffd5" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">例 3:矛盾，佐治亚不在中西部</li><li id="13d5" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">例子 4:产生幻觉的知识，错误地将游戏与制作者联系起来</li></ol><p id="a716" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">在下一部分中，我们将探索 Blender 中使用的 Transformer 架构，称为 Poly-Encoder，以及它如何优于其他变体，如用于多句子评分的相同任务的双编码器或交叉编码器。</p><p id="b3fe" class="pw-post-body-paragraph kr ks it kt b ku lo kw kx ky lp la lb lc lq le lf lg lr li lj lk ls lm ln kq im bi translated">最后是我们在文章开头看到的问题的答案:B)来自《弗兰肯斯坦》的生物！如果你猜它是 A)一个聊天机器人，那么你就快猜对了，因为用不了多久，我们就可以和一个全面的对话代理谈论人类的邪恶了！</p><h1 id="4df0" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">参考资料:</h1><ol class=""><li id="0d2c" class="mw mx it kt b ku mr ky ms lc oc lg od lk oe kq nb nc nd ne bi translated">关于 Blender:<a class="ae kf" href="https://www.kdnuggets.com/2020/05/facebook-open-sources-blender-largest-open-domain-chatbot.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2020/05/Facebook-open-sources-Blender-maximum-open-domain-chatbot . html</a></li><li id="3767" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">搅拌机机器人研究:【https://arxiv.org/abs/2004.13637 T2】</li><li id="d196" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">搅拌机机器人食谱:【https://parl.ai/projects/recipes/ T4】</li><li id="d7b4" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">果果技能对话:<a class="ae kf" href="https://arxiv.org/abs/2004.08449" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2004.08449</a></li><li id="716a" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">维基百科的巫师:<a class="ae kf" href="https://arxiv.org/abs/1811.01241" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.01241</a></li><li id="aa33" class="mw mx it kt b ku nf ky ng lc nh lg ni lk nj kq nb nc nd ne bi translated">关于 Meena:<a class="ae kf" href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/01/forward-conversational-agent-than-can . html</a></li></ol></div></div>    
</body>
</html>