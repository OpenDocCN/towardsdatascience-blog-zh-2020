<html>
<head>
<title>Decision Tree: build, prune and visualize it using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树:使用Python构建、修剪和可视化决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-build-prune-and-visualize-it-using-python-12ceee9af752?source=collection_archive---------2-----------------------#2020-02-05">https://towardsdatascience.com/decision-tree-build-prune-and-visualize-it-using-python-12ceee9af752?source=collection_archive---------2-----------------------#2020-02-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aaf9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">构建和调整机器学习模型，一步一步地解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/eed90c7f48ab21e1a5e7917262b6a5c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hb96Eu6iu4Pqv3vy8HAVg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布兰登·格林的照片</p></figure><p id="3454" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> B </span>二叉树是计算世界中最常见和最强大的数据结构之一。机器学习工程师如此喜欢决策树的主要原因是，它的处理成本很低，而且真的很容易理解(它是透明的，与神经网络的“黑箱”相反)。它使用分支方法来检查每个特性及其不同的结果，以找到目标值的模式。关于决策树和不同分支方法的更详细的解释，请看托马斯·普拉平格的<a class="ae kv" rel="noopener" target="_blank" href="/what-is-a-decision-tree-22975f00f3e1">这篇文章。</a></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="9ee6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们开始吧！</p><h1 id="3146" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">决策树模型</h1><p id="0221" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们将使用来自卡格尔的泰坦尼克号数据集来预测幸存者。我们将导入数据并选择一些要使用的要素。“幸存”是我们的目标价值。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="b427" class="nk mj iq ng b gy nl nm l nn no">data = pd.read_csv(‘path_or_link/train.csv’)<br/>data = data.loc[:,(‘Survived’,’Pclass’,’Sex’,’Age’,’SibSp’,’Parch’,’Fare’)]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/cf16328eb01a63422c19464b1f6b8bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*y71BnQMjScIaQwk8hmQNNQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据样本</p></figure><p id="0dfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">缺失值<br/> </strong>我们有一些“年龄”的缺失值。为了解决这个问题，我们可以运行一些预测分析并填充这些点，但这不是本文的目标，所以我们将在以后的文章中讨论。现在，让我们只删除那些缺少值的行。</p><p id="c5f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">编码分类特征<br/> </strong>为了将所有特征保持为数字格式，我们将对“性别”进行编码，这意味着将值从“女性”和“男性”转换为0和1。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="c004" class="nk mj iq ng b gy nl nm l nn no">data.dropna(inplace=True)<br/>#'inplace=True' applies the code to the 'data' object.</span><span id="97c7" class="nk mj iq ng b gy nq nm l nn no">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>data.Sex = le.fit_transform(data.Sex)</span></pre><p id="987a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">将数据集分为训练集和测试集</strong></p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="90ab" class="nk mj iq ng b gy nl nm l nn no">x = data.iloc[:,1:]   # Second column until the last column<br/>y = data.iloc[:,0]    # First column (Survived) is our target</span><span id="459f" class="nk mj iq ng b gy nq nm l nn no">from sklearn.model_selection import train_test_split<br/>#this function randomly split the data into train and test sets</span><span id="f551" class="nk mj iq ng b gy nq nm l nn no">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3)<br/>#test_size=.3 means that our test set will be 30% of the train set.</span></pre><p id="342a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">构建决策树分类器<br/> </strong>来自sklearn的decision tree classifier()是一个很好的现成的机器学习模型，可供我们使用。它有fit()和predict()方法。<br/><strong class="ky ir">fit()</strong>方法是建模过程的“训练”部分。它找到算法的系数。然后，<strong class="ky ir"> predict() </strong>方法将使用训练好的模型对一组新数据(测试集)进行预测。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="dfe1" class="nk mj iq ng b gy nl nm l nn no">dtree = DecisionTreeClassifier()<br/>dtree.fit(x_train, y_train)  #train parameters: features and target<br/>pred = dtree.predict(x_test)  #parameter: new data to predict</span></pre><p id="abdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">准确性<br/> </strong>为了确定我们的模型有多好，我们使用sklearn的另一个包来给它一个度量:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="7de9" class="nk mj iq ng b gy nl nm l nn no">from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test, pred)</span><span id="d2c4" class="nk mj iq ng b gy nq nm l nn no">#parameters: targets to be predicted and predictions from new data used before</span></pre><p id="5655" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">准确度:0.75 <br/>这意味着我们的预测有75%的准确度。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="8aa9" class="mi mj iq bd mk ml nr mn mo mp ns mr ms jw nt jx mu jz nu ka mw kc nv kd my mz bi translated">可视化树</h1><p id="e5e0" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们可以绘制树来查看它的根、分支和节点。我们将使用一组新的库来完成这项工作。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="73d1" class="nk mj iq ng b gy nl nm l nn no">from sklearn.tree import export_graphviz<br/>from sklearn.externals.six import StringIO<br/>from IPython.display import Image  <br/>import pydotplus</span><span id="f4df" class="nk mj iq ng b gy nq nm l nn no">dot_data = StringIO()<br/>export_graphviz(dtree, out_file=dot_data)<br/>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br/>graph.write_png('/tree.png')<br/>Image(graph.create_png())</span></pre><p id="7831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> StringIO() </strong>:创建一个对象(本例中为空)来接收一个点(图形描述语言)格式的字符串缓冲区(树将首先创建为字符串，然后创建为图像)。<br/> <strong class="ky ir"> export_graphviz() </strong>:该函数以点格式导出树，生成决策树的表示，并写入‘out _ file’。<br/><strong class="ky ir">graph _ from _ dot _ data()</strong>:将使用DOT对象创建图形。<br/><strong class="ky ir">【Image()</strong>:使用原始数据返回图像(png、jpeg、gif)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/0fd58535f7003a9ad3b4d27efc79128d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YPf-8ccElDjYYPbo2RNgA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">决策图表</p></figure><p id="dbff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这棵树似乎很长。让我们改变几个参数，看看对精度是否有任何影响，并使树更短。</p><ul class=""><li id="e41f" class="nx ny iq ky b kz la lc ld lf nz lj oa ln ob lr oc od oe of bi translated"><strong class="ky ir">标准</strong>:定义将使用什么函数来测量分割的质量。选项有“基尼”和“熵”。</li><li id="7ba4" class="nx ny iq ky b kz og lc oh lf oi lj oj ln ok lr oc od oe of bi translated"><strong class="ky ir"> Max_depth </strong>:定义树的最大深度。如果它是“无”，树将尽可能长，这时所有的叶子都是纯的(过度拟合模型的风险)。</li></ul><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="c309" class="nk mj iq ng b gy nl nm l nn no">dtree = DecisionTreeClassifier(criterion='gini')<br/>dtree.fit(x_train, y_train)<br/>pred = dtree.predict(x_test)<br/>print('Criterion=gini', accuracy_score(y_test, pred))</span><span id="eaae" class="nk mj iq ng b gy nq nm l nn no">dtree = DecisionTreeClassifier(criterion='entropy')<br/>dtree.fit(x_train, y_train)<br/>pred = dtree.predict(x_test)<br/>print('Criterion=entropy', accuracy_score(y_test, pred))</span></pre><p id="ecba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">准确度结果:<br/>标准=基尼:0.735 <br/>标准=熵:0.716</p><p id="ef1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基尼系数法比熵值法略有改进。现在，让我们检查使用max_depth修剪树是否能给我们带来更好的结果。在下面的代码块中，我创建了一个简单的函数，使用不同的max_depth值(从1到30)来运行我们的模型，并可视化其结果，以查看每个值的精确度有何不同。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="fb13" class="nk mj iq ng b gy nl nm l nn no">max_depth = []<br/>acc_gini = []<br/>acc_entropy = []<br/>for i in range(1,30):<br/> dtree = DecisionTreeClassifier(criterion=’gini’, max_depth=i)<br/> dtree.fit(x_train, y_train)<br/> pred = dtree.predict(x_test)<br/> acc_gini.append(accuracy_score(y_test, pred))<br/> ####<br/> dtree = DecisionTreeClassifier(criterion=’entropy’, max_depth=i)<br/> dtree.fit(x_train, y_train)<br/> pred = dtree.predict(x_test)<br/> acc_entropy.append(accuracy_score(y_test, pred))<br/> ####<br/> max_depth.append(i)</span><span id="493c" class="nk mj iq ng b gy nq nm l nn no">d = pd.DataFrame({‘acc_gini’:pd.Series(acc_gini), <br/> ‘acc_entropy’:pd.Series(acc_entropy),<br/> ‘max_depth’:pd.Series(max_depth)})</span><span id="e46f" class="nk mj iq ng b gy nq nm l nn no"># visualizing changes in parameters<br/>plt.plot(‘max_depth’,’acc_gini’, data=d, label=’gini’)<br/>plt.plot(‘max_depth’,’acc_entropy’, data=d, label=’entropy’)<br/>plt.xlabel(‘max_depth’)<br/>plt.ylabel(‘accuracy’)<br/>plt.legend()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a00c8cf0671ebb325ff34c0b2efe5db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*r_pr8mMpQxfQ0uixQJjV-A.png"/></div></figure><p id="951e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，gini似乎对较长的树效果最好(正如我们在前面的精确度中看到的)，但熵对较短的树效果更好，而且更精确(接近0.82)。所以我们来画max_depth=7，criterion=entropy的树，看看是什么样子。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="e890" class="nk mj iq ng b gy nl nm l nn no">dtree = DecisionTreeClassifier(criterion=’entropy’, max_depth=7)<br/>dtree.fit(x_train, y_train)</span><span id="7fe7" class="nk mj iq ng b gy nq nm l nn no">pred = dtree.predict(x_test)<br/>accuracy_score(y_test, pred)</span><span id="3daf" class="nk mj iq ng b gy nq nm l nn no">#Accuracy: 0.814</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/16aea756a82fbdad83c515677692e972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kqNLwvgiqpRjyi8djdOWmw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标准= '熵'，最大深度=7</p></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="47c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们通过改变函数来创建分支并限制树以避免过度拟合，从而从0.75的准确度提高到0.81。每个案例都是不同的，测试不同的参数以找到最适合您的场景是一个很好的实践。当你学习新的函数和机器学习模型时，不要犹豫阅读文档。它将为您提供更广泛的选项来根据您的需求定制模型。</p><p id="e366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，去吧，使用你的新技能，通过种一棵树来拯救地球。</p></div></div>    
</body>
</html>