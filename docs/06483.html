<html>
<head>
<title>Linear Regression from Scratch with Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow 2从头开始线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0?source=collection_archive---------16-----------------------#2020-05-23">https://towardsdatascience.com/linear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0?source=collection_archive---------16-----------------------#2020-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1579" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">纯用TensorFlow 2.0写线性回归算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/200106306d8bd6446939dba83f672a34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tcRSkzIsdGOEcmX9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@andriklangfield?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Andrik Langfield </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cd43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是最基本的，也许是最常用的机器学习算法之一，初学者和专家都应该烂熟于心。在本文中，我将带你了解如何只用Tensorflow实现线性回归。讨论将分为两部分，第一部分解释线性回归的概念，第二部分是如何在Tensorflow中实现线性回归。</p><h1 id="6028" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">一.概念</h1><p id="a1cc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">线性回归试图通过拟合线性方程来模拟因变量和自变量之间的关系。假设我们有100名学生的测验分数和学习时间长度的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/fcbe9335e74d7836e5b347c80ff3feb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*fDfhpbBM67W-wPsyfX886g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">100名学生的测验分数和学习时间</p></figure><p id="6420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过观察散点图，我们可以很容易地用公式<code class="fe mt mu mv mw b">y=mx+b</code>画一条线，其中<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">b</code>分别是斜率和y轴截距。从图上可以看出，<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">b</code>分别大约为40和0。</p><p id="3e79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们在<code class="fe mt mu mv mw b">m=40</code>和<code class="fe mt mu mv mw b">b=0</code>之间划一条线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/845cbf4f35ea8fcc84d9082243c2e536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*J0dPncPRUdNQxgs7TgL60g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用公式y=40x拟合直线</p></figure><p id="7e46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">y=40x</code>线好看！然后，我们可以估计学生的分数是40乘以学生学习的小时数。</p><p id="0593" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归的工作原理与此完全相同，只是它不能从散点图中直观地检查斜率和y截距。相反，它首先猜测斜率和y轴截距，然后衡量其猜测的准确性。如果不够好，它会调整斜率和y轴截距，直到直线与数据吻合。</p><p id="8de1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是一个三步算法:</p><ol class=""><li id="696f" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated">初始化线性方程的参数(斜率和y轴截距的第一次猜测)。</li><li id="1ae9" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">基于某个函数测量拟合优度。</li><li id="2084" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">调整参数，直到步骤2中的测量看起来不错。</li></ol><h2 id="3c7c" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated"><strong class="ak"> 1。线性方程和初始化</strong></h2><p id="486f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，我们已经围绕线性回归建立了我们的直觉，让我们从数学的角度来讨论每一步。</p><p id="bedf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归模型的第一步是初始化一个线性方程，是的，我们将使用<code class="fe mt mu mv mw b">y=mx+b</code>但是我们必须推广我们的方法。这样做的原因是，我们可能会面临多个独立变量的数据。可以把它看作是在我们的测验分数数据中增加了另一个变量，比如学习时喝的咖啡量。拥有这个<code class="fe mt mu mv mw b">coffee</code>维度将使线性方程看起来像这样:<code class="fe mt mu mv mw b">y=m1x1+m2x2+b</code>，其中m1和m2分别是学习时间和咖啡维度的斜率，x1和x2是学习时间和咖啡变量。</p><p id="701f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用点积来表示矩阵、<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">x,</code>的乘积，而不是为每个新变量写一个更长的方程。注意，使用术语张量也是有效的，因为它是矩阵的推广。黑体字母用来表示矩阵，所以线性方程应该写成y =<strong class="lb iu">m</strong>⋅<strong class="lb iu">x</strong>+<strong class="lb iu">b</strong>。</p><p id="d8b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多方法可以初始化方程的参数，最常见的是使用随机值、0或1。你可以自由使用任何类型的初始化，这个选择将决定你的学习算法终止的速度。在算法的下一次迭代中，这些参数将基于步骤2中讨论的某个函数进行更新。</p><h2 id="75e5" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">2.损失函数</h2><p id="aa4f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在假设你为m和b设置的初始值都是1，那么你的方程就是<code class="fe mt mu mv mw b">y=1x+1</code>。初始预测将看起来像下图中的橙色点。这显然是一个非常糟糕的预测，我们需要一个数字来量化这些预测是好是坏。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/599f15037773b387603cd384142289f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*X-1q5DK6xYxBrWxNjTdelA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">初始预测</p></figure><p id="2680" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多方法可以衡量我们预测的好坏，我们将使用其中一种叫做<em class="ny">均方误差(MSE)的方法。</em>在这种情况下，误差意味着差异，所以MSE字面意思是取实际值和预测值之差的平方，然后取平均值。它在数学上写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/2d9d290926844b76618f82bd4ebcb3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*L2cG_FchQ00t06yk.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://www.researchgate.net/figure/Mean-Squared-Error-formula-used-to-evaluate-the-user-model_fig1_221515860" rel="noopener ugc nofollow" target="_blank">researchgate.net</a></p></figure><p id="1e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像MSE这样的函数叫做损失函数或者目标函数。这些是算法想要最小化的函数。如果我们的线性回归模型完美地预测了测验分数，它的MSE将等于0。因此，在算法的每次迭代中，它应该更新参数，以使MSE更接近0，而不会过度拟合。过度拟合本身是一个完整的主题，但它的本质含义是，我们不希望我们的学习算法对训练数据如此好，却在测试集上惨败。</p><p id="6760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。梯度下降</strong></p><p id="90df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们可以继续猜测参数，直到我们足够接近零MSE，但这将需要时间和努力— <em class="ny">梯度下降</em>将为我们做到这一点。如果你不熟悉这个术语，有大量的文章和视频解释它的概念。</p><p id="e2f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降是人工智能的基石之一。就是机器学习中的学习。像梯度下降这样的算法允许学习算法在没有被如此明确地告知的情况下学习。(你需要温习一下微积分，了解梯度下降是如何工作的。)</p><p id="2787" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降是一种优化算法，我们将使用它来最小化我们的损失函数(在这种情况下为MSE)。它通过在每次迭代中用小的变化来更新参数，这个变化也可以很大，这取决于你的偏好(学习率)。</p><p id="6200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每次新的迭代中，更新的参数将是<strong class="lb iu"><em class="ny">p _ new = p _ old-(l * dL/DP)</em></strong>，其中<strong class="lb iu"> <em class="ny"> p </em> </strong>是参数，它可以是斜率、<code class="fe mt mu mv mw b">m</code>或y轴截距、<code class="fe mt mu mv mw b">b</code>。新变量，<strong class="lb iu"> <em class="ny"> l </em> </strong>和<em class="ny"/><strong class="lb iu"><em class="ny">dL/DP</em></strong>，<em class="ny"> </em>是损失函数相对于参数的学习率和偏导数。</p><p id="93c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过足够的迭代，斜率和y轴截距将更接近40°和0°，我们认为这些值“足够接近”以符合我们的数据。正如您可能已经观察到的，如果您恰好将参数初始化为接近40和0，比如35和0.5，那么算法将需要较少的迭代。</p><p id="6989" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想更深入地研究梯度下降的数学，这篇文章很有帮助。</p><h2 id="e54e" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">停止标准</h2><p id="fee8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以下是终止算法的一些可能方式:</p><ol class=""><li id="19e7" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated">一旦达到指定的迭代次数，就终止算法。</li><li id="22d2" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">一旦满足指定的MSE，就终止算法。</li><li id="0a18" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">如果MSE在下一次迭代中没有改善，则终止该算法。您可以指定一个精度，如0.001，如果两个连续MSEs之间的差值小于该精度，则停止算法。</li></ol><h1 id="b226" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">II TensorFlow2实现</h1><p id="4787" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本演示中，我们将遵循上面讨论的线性回归算法的三步方法，并使用停止标准1终止算法。</p><h2 id="3877" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">导入库</h2><p id="ca67" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这些是我们在这个演示中需要的唯一的库。TensorFlow用于构建算法，pyplot用于可视化，而<code class="fe mt mu mv mw b">boston_housing</code>作为我们的玩具数据集。</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="2a70" class="nm lw it mw b gy oe of l og oh">import tensorflow as tf<br/>import matplotlib.pyplot as plt</span><span id="4f43" class="nm lw it mw b gy oi of l og oh">from tensorflow.keras.datasets import boston_housing</span></pre><h2 id="1206" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">1.初始化一个线性方程</h2><p id="8331" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们从创建一个带有初始化选项的<code class="fe mt mu mv mw b">SimpleLinearRegression</code>类开始。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c8cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我指定了三个初始化选项，<code class="fe mt mu mv mw b">ones</code>、<code class="fe mt mu mv mw b">zeros</code>和<code class="fe mt mu mv mw b">random</code>(默认)。<code class="fe mt mu mv mw b">tf.random.uniform</code>将从范围<code class="fe mt mu mv mw b">minval</code>和形状<code class="fe mt mu mv mw b">shape</code>的<code class="fe mt mu mv mw b">maxval</code>内的均匀分布中产生随机值的张量。我将<code class="fe mt mu mv mw b">m</code>定义为一个没有特定形状的变量，因此它可以足够灵活地接受任意数量的独立变量，这可以通过设置<code class="fe mt mu mv mw b">shape=tf.TensorShape(None)</code>来实现。</p><h2 id="72cf" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">2.损失函数</h2><p id="187b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来是实现我们的损失函数，MSE。概括地说，MSE在数学上写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/2d9d290926844b76618f82bd4ebcb3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*L2cG_FchQ00t06yk.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自researchgate.net<a class="ae ky" href="https://www.researchgate.net/figure/Mean-Squared-Error-formula-used-to-evaluate-the-user-model_fig1_221515860" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="29a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是我在TensorFlow中实现的函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="fb4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在Tensorflow中写起来相当容易。首先取<code class="fe mt mu mv mw b">true</code>和<code class="fe mt mu mv mw b">predicted</code>值的差，用<code class="fe mt mu mv mw b">tf.square</code>求差的平方，然后用<code class="fe mt mu mv mw b">tf.reduce_mean</code>求差的平方的平均值。</p><p id="b174" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数接受<code class="fe mt mu mv mw b">true</code>和<code class="fe mt mu mv mw b">predicted</code>值，前者来自数据本身，但后者必须经过计算。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="dc3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">predict</code>方法是通过简化线性方程来完成的。首先我们取<code class="fe mt mu mv mw b">m</code>(斜率张量)和<code class="fe mt mu mv mw b">x</code>(特征张量)的点积，加上y轴截距<code class="fe mt mu mv mw b">b</code>。我必须指定<code class="fe mt mu mv mw b">reduction_sum</code>中的缩减将被计算到<code class="fe mt mu mv mw b">1</code>的轴，否则它将把张量缩减为单个和。</p><h2 id="88ce" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">3.更新参数</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="ee2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要梯度下降来更新每次迭代的参数。没有必要从头开始创建这个算法，因为Tensorflow已经为此建立了一个函数，<code class="fe mt mu mv mw b">tf.GradientTape</code>。默认情况下，<code class="fe mt mu mv mw b">GradientTape</code>将<code class="fe mt mu mv mw b">persistent</code>设置为<code class="fe mt mu mv mw b">False</code>，这意味着最多可以对该对象中的<code class="fe mt mu mv mw b">gradient()</code>方法进行一次调用。因为我们使用它来计算每次迭代的两个梯度(一个用于<code class="fe mt mu mv mw b">m</code>，另一个用于<code class="fe mt mu mv mw b">b</code>，所以我们必须将其设置为<code class="fe mt mu mv mw b">True</code>。然后，我们指定将要计算梯度的损失函数，在这种情况下是带有参数<code class="fe mt mu mv mw b">y</code>和<code class="fe mt mu mv mw b">self.predict(X)</code>的<code class="fe mt mu mv mw b">mse</code>，它们分别代表<code class="fe mt mu mv mw b">true</code>和<code class="fe mt mu mv mw b">predicted</code>值。</p><p id="73cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个参数将通过减去学习率和参数梯度的乘积来更新。</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="7411" class="nm lw it mw b gy oe of l og oh"><strong class="mw iu"><em class="ny">p_new = p_old - (l*dL/dp)</em></strong></span></pre><p id="934e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习是一个超参数，应该在训练线性回归时指定。使用接受损失函数和参数的<code class="fe mt mu mv mw b">gradient()</code>方法计算梯度<code class="fe mt mu mv mw b">dL/dp</code>。这个操作只是求解损失函数相对于参数的偏导数。我们必须在每次迭代中计算两个梯度，一个用于<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">b</code>。</p><p id="27e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了用新值更新参数，新值只是旧值减去<code class="fe mt mu mv mw b">l*dL/dp</code>，我们简单地使用<code class="fe mt mu mv mw b">tf.Variable</code>的<code class="fe mt mu mv mw b">assign_sub()</code>方法。</p><h2 id="97dc" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">训练方法</h2><p id="3c0a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们用一个<code class="fe mt mu mv mw b">train</code>方法把所有的东西放在一起。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="5c23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它做的第一件事是，检查数据是否只包含一个独立变量，如果是，那么它会把它变成一个2D张量。</p><p id="5c64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">self.m.assign([self.var]*X.shape[-1])</code>将使用我们在初始化过程中设置的初始值初始化<code class="fe mt mu mv mw b">m</code>,其形状遵循数据中独立变量的数量。</p><p id="1175" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们算法的停止标准是迭代次数，由<code class="fe mt mu mv mw b">epoch</code>定义。对于每次迭代，它将调用<code class="fe mt mu mv mw b">update</code>方法。</p><h2 id="fc0f" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">这是线性回归的全部代码。</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><h2 id="b9ef" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">测试我们的算法</h2><p id="9078" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">是时候使用波士顿房屋数据集来测试我们的算法了。</p><p id="e87b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe mt mu mv mw b">keras.datasets</code>加载数据集:</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="4959" class="nm lw it mw b gy oe of l og oh">(x_train, y_train), (x_test, y_test) = boston_housing.load_data()</span></pre><p id="2ec2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准化数据:</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="8947" class="nm lw it mw b gy oe of l og oh">mean_label = y_train.mean(axis=0)<br/>std_label = y_train.std(axis=0)</span><span id="13c4" class="nm lw it mw b gy oi of l og oh">mean_feat = x_train.mean(axis=0)<br/>std_feat = x_train.std(axis=0)</span><span id="6ef8" class="nm lw it mw b gy oi of l og oh">x_train = (x_train-mean_feat)/std_feat<br/>y_train = (y_train-mean_label)/std_label</span></pre><p id="85b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建并训练一个<code class="fe mt mu mv mw b">SimpleLinearRegression</code>对象。</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="0280" class="nm lw it mw b gy oe of l og oh">linear_model = SimpleLinearRegression('zeros')<br/>linear_model.train(x_train_norm, y_train_norm, learning_rate=0.1, epochs=50)</span></pre><p id="9d83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最近五次迭代的损失:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/36dd68f1a6b3683f9265ac2ccc90fda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*H19dLANs2126Wcffy-LXBQ.png"/></div></figure><p id="e470" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用测试集来预测:</p><pre class="kj kk kl km gt oa mw ob oc aw od bi"><span id="0463" class="nm lw it mw b gy oe of l og oh"># standardize<br/>x_test = (x_test-mean_feat)/std_feat</span><span id="3151" class="nm lw it mw b gy oi of l og oh"># reverse standardization<br/>pred = linear_model.predict(x_test)<br/>pred *= std_label<br/>pred += mean_label</span></pre><p id="fa70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须首先将输入标准化，然后一旦我们有了预测，就逆转这个过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e245c1c68de1d598008ef2e93c720745.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*Lf6EOAYFhL6TcVfZhGW_dQ.png"/></div></figure><h1 id="03e9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="31c5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">尽管简单，但线性回归是业内最常用的机器学习算法之一，一些公司会测试你对它的理解程度。虽然有更简单的方法来实现这个算法，比如使用scikit-learn甚至TensorFlow的<code class="fe mt mu mv mw b">LinearRegressor</code>，但我们从头开始实现了整个算法，目的是体验TensorFlow的功能。当你进入神经网络时，有像<code class="fe mt mu mv mw b">Keras</code>这样的高级库，它简化了使用TensorFlow作为后端构建神经网络的过程。但最终，特别是如果你是一名研究人员，你会想要定制你的模型，无论你想要什么，这就是TensorFlow的低级功能非常有用的地方。</p></div></div>    
</body>
</html>