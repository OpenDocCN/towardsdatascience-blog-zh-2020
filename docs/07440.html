<html>
<head>
<title>Text Mining Tweets on Religion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于宗教的文本挖掘推文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/religion-on-twitter-5f7b84062304?source=collection_archive---------25-----------------------#2020-06-05">https://towardsdatascience.com/religion-on-twitter-5f7b84062304?source=collection_archive---------25-----------------------#2020-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="863a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python的主题建模、情感分析和仇恨言论检测模型</h2></div><p id="80e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">有人声称科技革命的光芒将会削弱宗教在人类生活中的存在。然而，现在是2020年，有一件事似乎是不可避免的，<em class="ln">宗教</em>仍然在这里，并且会一直存在下去！也就是说，值得研究一下现代公众对宗教的看法。</p><p id="0f0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章全面介绍了如何使用<em class="ln"> Python </em>来提取、预处理和分析关于宗教的推文。该分析是关于实现<strong class="kk iu">主题建模</strong>(LDA)<strong class="kk iu">情感分析</strong> (Gensim)和<strong class="kk iu">仇恨言论检测</strong> (HateSonar)模型。下面的分步教程与代码和结果一起展示。</p><blockquote class="lo lp lq"><p id="ca54" class="ki kj ln kk b kl km ju kn ko kp jx kq lr ks kt ku ls kw kx ky lt la lb lc ld im bi translated">感谢Johannes Schneider博士和Joshua Handali理学硕士在列支敦士登大学指导这项工作。完整的代码可以在GitHub的<a class="ae lu" href="https://github.com/HurmetNoka/religion_on_twitter" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></blockquote><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi lv"><img src="../Images/d1166d55da4b04a775bc297256535922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAMvSzbxvPhTvFnwZH7OQw.jpeg"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">列支敦士登瓦杜兹城堡</p></figure><h1 id="2409" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated"><strong class="ak">数据提取&amp;预处理</strong></h1><p id="0b0b" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">为了获取tweets，我们使用了一个公共python脚本，该脚本能够捕获旧的tweets，从而绕过Twitter API的7天期限限制。该脚本是免费的，可以在GitHub的<a class="ae lu" href="https://github.com/Jefferson-Henrique/GetOldTweets-python" rel="noopener ugc nofollow" target="_blank">这里</a>找到。你所需要做的就是调整搜索过滤器并运行程序。在我们的研究中，我们提取了包含短语<em class="ln">“宗教是”的推文</em>为了减少某些影响对宗教感受的孤立事件(如《查理周刊》袭击)的偏见，我们将时间范围延长至大约五年。我们从2015年1月开始到2019年10月，每月提取1000条推文。这产生了大约57，351条推文，然后被加载到数据帧中，准备进行预处理。</p><p id="5603" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是整个分析过程的示意图</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ni"><img src="../Images/787a419f0e2386c9e7f39f08fa212b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpwKGovOAXLKH17EWh0YNw.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">分析步骤</strong></p></figure><h2 id="bdff" class="nk mm it bd mn nl nm dn mr nn no dp mv kr np nq mx kv nr ns mz kz nt nu nb nv bi translated"><strong class="ak">预处理</strong></h2><p id="c734" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">这些是执行预处理阶段所必需的包。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="fdaa" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu">import</strong> pandas <strong class="nx iu">as</strong> pd<br/><strong class="nx iu">import</strong> numpy <strong class="nx iu">as</strong> np<br/><strong class="nx iu">import</strong> re</span><span id="dc3c" class="nk mm it nx b gy of oc l od oe"># Plotting<br/><strong class="nx iu">import</strong> seaborn <strong class="nx iu">as</strong> sns<br/><strong class="nx iu">import</strong> matplotlib.pyplot <strong class="nx iu">as</strong> plt</span><span id="3f86" class="nk mm it nx b gy of oc l od oe"># Gensim<br/><strong class="nx iu">import</strong> gensim<br/><strong class="nx iu">from</strong> gensim.utils <strong class="nx iu">import</strong> simple_preprocess</span><span id="f436" class="nk mm it nx b gy of oc l od oe"># NLTK<br/><strong class="nx iu">import</strong> nltk<br/><strong class="nx iu">from</strong> nltk.corpus <strong class="nx iu">import</strong> stopwords</span><span id="9486" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">from</strong> collections <strong class="nx iu">import</strong> Counter<br/><strong class="nx iu">from</strong> wordcloud <strong class="nx iu">import</strong> WordCloud</span><span id="5902" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">import</strong> warnings <br/>warnings.filterwarnings("ignore", category=DeprecationWarning)</span><span id="725e" class="nk mm it nx b gy of oc l od oe">%matplotlib inline</span></pre><p id="334d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据加载</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="43c1" class="nk mm it nx b gy ob oc l od oe">df = pd.read_csv('full_data.csv', index_col=[0])<br/>df.head()</span></pre><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi og"><img src="../Images/fac46bea27fe98c73a0d98dae5e06939.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*UHcrqFq3CVPMxsNSiMrEyg.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">数据概述</p></figure><p id="090c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在任何进一步的步骤之前，我们删除重复的内容，因为在twitter上，人们复制粘贴不同的引用并转发某些内容是很常见的。删除重复后，我们总共得到53，939条独特的推文。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="0c8b" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># DROP DUPLICATES</strong><br/>df.drop_duplicates(subset=['tweet_text'], keep='first',inplace=<strong class="nx iu">True</strong>)<br/>df.shape<br/>Out: (53939, 1)</span></pre><p id="77b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们生成一些一般的描述性统计数据:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oh"><img src="../Images/411808cbe08f0d66e73c853b2ac652ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5FibVLRGuScb8U5LowL1A.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">描述统计学</p></figure><p id="b3ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Twitter允许通过“@”符号在推文中包含用户名。这些对我们的分析没有任何价值；因此，使用函数将它们从数据集中移除。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="a99c" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE '@USER'<br/>def</strong> remove_users(tweet, pattern1, pattern2):<br/>    r = re.findall(pattern1, tweet)<br/>    <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> r:<br/>        tweet = re.sub(i, '', tweet)<br/>  <br/>    r = re.findall(pattern2, tweet)<br/>    <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> r:<br/>        tweet = re.sub(i, '', tweet)<br/>    <strong class="nx iu">return</strong> tweet</span><span id="7752" class="nk mm it nx b gy of oc l od oe">df['tidy_tweet'] = np.vectorize(remove_users)(df['tweet_text'],     "@ [\w]*", "@[\w]*")</span></pre><p id="7630" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">规范化，一种将所有tweets转换成小写的方法，这样“token”和“Token”就不会被认为是两个不同的单词。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="808d" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># NORMALIZATION</strong><br/>df['tidy_tweet'] = df['tidy_tweet'].str.lower()</span></pre><p id="99b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与用户名一样，标签也被认为对主题建模分析没有重要价值，因此被移除。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oi"><img src="../Images/f328cd5e8d8f05534ba604c388a630d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjQmEZ-FwO_0QXMwhuzBxw.png"/></div></div></figure><p id="2bbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上是该数据集的前25个标签，我们注意到<strong class="kk iu"> #islam </strong>是使用最多的标签，这表明它是讨论最多的宗教。使用以下函数移除标签:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="c2f5" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE HASHTAGS</strong><br/><strong class="nx iu">def</strong> remove_hashtags(tweet, pattern1, pattern2):<br/>    r = re.findall(pattern1, tweet)<br/>    <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> r:<br/>        tweet = re.sub(i, '', tweet)<br/>    <br/>    r = re.findall(pattern2, tweet)<br/>    <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> r:<br/>        tweet = re.sub(i, '', tweet)<br/>    <strong class="nx iu">return</strong> tweet</span><span id="d4bc" class="nk mm it nx b gy of oc l od oe">df['tidy_tweet'] = np.vectorize(remove_hashtags)(df['tidy_tweet'], "# [\w]*", "#[\w]*")</span></pre><p id="1e9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来要删除的是URL:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="6c18" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE LINKS</strong><br/><strong class="nx iu">def</strong> remove_links(tweet):<br/>    tweet_no_link = re.sub(r"http\S+", "", tweet)<br/>    <strong class="nx iu">return</strong> tweet_no_link</span><span id="db3f" class="nk mm it nx b gy of oc l od oe">df['tidy_tweet'] = np.vectorize(remove_links)(df['tidy_tweet'])</span></pre><p id="988f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用与链接相同的功能，我们还删除了<em class="ln">收集词，</em>那些首先用于过滤推文的词，在这种情况下:<em class="ln">宗教，“宗教”</em></p><p id="aa26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，需要去掉数字、标点符号(仅用于主题建模)，以及特殊字符(@、&amp;、#、%).</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="b559" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE Punctuations, Numbers, and Special Characters</strong><br/>df['tidy_tweet'] = df['tidy_tweet'].str.replace("[^a-zA-Z#]", " ")</span></pre><p id="4dae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，从数据集中移除少于三个字符的单词(短单词)，从而简化用于分析的特征提取。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="61b6" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE SHORT WORDS</strong><br/>df['tidy_tweet'] = df['tidy_tweet'].apply(<strong class="nx iu">lambda</strong> x:' '.join([w <strong class="nx iu">for</strong> w <strong class="nx iu">in</strong> x.split() <strong class="nx iu">if</strong> len(w)&gt;3]))</span></pre><p id="c7e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预处理的一个基本步骤被称为<em class="ln">标记化。</em>它是根据空格分割文本的过程，每个单词和标点都保存为单独的标记。我们使用来自<em class="ln"> Gensim的<em class="ln"> simple_preprocess </em>方法来执行这个步骤。</em></p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="f6f4" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># TOKENIZATION</strong><br/><strong class="nx iu">def</strong> tokenize(tweet):<br/>    <strong class="nx iu">for</strong> word <strong class="nx iu">in</strong> tweet:<br/>        <strong class="nx iu">yield</strong>(gensim.utils.simple_preprocess(<strong class="nx iu">str</strong>(word), deacc=<strong class="nx iu">True</strong>))  </span><span id="4429" class="nk mm it nx b gy of oc l od oe">df['tidy_tweet_tokens'] = <strong class="nx iu">list</strong>(tokenize(df['tidy_tweet']))</span></pre><p id="c8c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们删除没有分析价值的<em class="ln">停用词</em>，通常是冠词、介词或代词，例如，“a”、“and”、“the”等。默认列表可以根据需要进行调整和扩展。我们向包含179个单词的自然语言工具包(NLTK)的预定义列表中添加了一些新单词。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="7cf2" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Prepare Stop Words</strong><br/>stop_words = stopwords.words('english')<br/>stop_words.<strong class="nx iu">extend</strong>([<em class="ln">'from', 'https', 'twitter', 'religions',     'pic','twitt'</em>,])</span><span id="4bee" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># REMOVE STOPWORDS</strong><br/><strong class="nx iu">def</strong> remove_stopwords(tweets):<br/>    <strong class="nx iu">return</strong> [[word <strong class="nx iu">for</strong> word <strong class="nx iu">in</strong> simple_preprocess(str(tweet)) <strong class="nx iu">if</strong> word    <strong class="nx iu">not</strong> <strong class="nx iu">in</strong> stop_words] <strong class="nx iu">for</strong> tweet <strong class="nx iu">in</strong> tweets]</span><span id="2be5" class="nk mm it nx b gy of oc l od oe">df['tokens_no_stop'] = remove_stopwords(df['tidy_tweet_tokens'])</span></pre><p id="5e17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在预处理的最后一步，我们删除少于三个令牌的tweets这导致总共有<strong class="kk iu"> 49，458条推文</strong>需要在分析阶段进一步考虑。对于主题建模和情感分析，具有少于三个标记的文档不适合生成足够的信息。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="efa6" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># REMOVE TWEETS LESS THAN 3 TOKENS</strong><br/>df['length'] = df['tokens_no_stop'].apply(<strong class="nx iu">len</strong>)<br/>df = df.drop(df[df['length']&lt;<strong class="nx iu">3</strong>].index)<br/>df = df.drop(['length'], axis=<strong class="nx iu">1</strong>)<br/>df.shape</span><span id="b59e" class="nk mm it nx b gy of oc l od oe">df.reset_index(drop=True, inplace=<strong class="nx iu">True</strong>)</span></pre><p id="3208" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，出于识别和可视化数据集的最常用单词的说明性目的，我们生成200个最常用单词的单词云。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/209d9a363f2704e60eb2a3b7698173db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*Kdk9-gznCxxFUcOCzNuS4Q.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">Wordcloud (250字)</p></figure><p id="a6c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预处理阶段的效果如下图所示，清理后的tweets长度大大减少。如图所示，大多数推文在预处理后都不到10个单词，不像原始推文那样有大约20个单词。在第二组图表中，随着推文长度从大部分推文的大约150个字符变为清理阶段后的大约50个字符，效果更加明显。这一阶段至关重要，因为它减少了维度，并为模型产生了非常有价值的表征，这将在接下来的部分中解释。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ok"><img src="../Images/10d4809914fd8c56c3603d8634c54389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAuWOxiggbQxAq2SStZIuQ.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ol"><img src="../Images/fc15773d91e5de556bbcdc260fa5eed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W81FFH6wIiG2UUtgEWPhEw.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">清洁效果</strong></p></figure><p id="af40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将预处理后的数据帧保存为<em class="ln"> pickle </em>，然后用于<strong class="kk iu">主题建模</strong>阶段。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="14a2" class="nk mm it nx b gy ob oc l od oe">df.to_pickle('pre-processed.pkl')</span></pre></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="8a8d" class="ml mm it bd mn mo ot mq mr ms ou mu mv jz ov ka mx kc ow kd mz kf ox kg nb nc bi translated">主题建模</h1><p id="d6ab" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">这些是实现<strong class="kk iu"> LDA </strong>(潜在狄利克雷分配)算法所需的包。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="ee30" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># IMPORTS</strong><br/><strong class="nx iu">import</strong> pandas <strong class="nx iu">as</strong> pd<br/><strong class="nx iu">import</strong> numpy <strong class="nx iu">as</strong> np<br/><strong class="nx iu">import</strong> networkx <strong class="nx iu">as</strong> nx<br/><strong class="nx iu">import</strong> itertools<br/><strong class="nx iu">import</strong> collections<br/><strong class="nx iu">import</strong> spacy<br/><strong class="nx iu">from</strong> pprint <strong class="nx iu">import</strong> pprint</span><span id="fe0d" class="nk mm it nx b gy of oc l od oe"># Plotting<br/><strong class="nx iu">import</strong> matplotlib.pyplot <strong class="nx iu">as</strong> plt<br/><strong class="nx iu">import</strong> seaborn <strong class="nx iu">as</strong> sns<br/><strong class="nx iu">import</strong> pyLDAvis<br/><strong class="nx iu">import</strong> pyLDAvis.gensim</span><span id="a085" class="nk mm it nx b gy of oc l od oe"># Gensim<br/><strong class="nx iu">import</strong> gensim<br/><strong class="nx iu">import</strong> gensim.corpora <strong class="nx iu">as</strong> corpora<br/><strong class="nx iu">from</strong> gensim.utils <strong class="nx iu">import</strong> simple_preprocess<br/><strong class="nx iu">from</strong> gensim.models <strong class="nx iu">import</strong> CoherenceModel<br/><strong class="nx iu">from</strong> gensim.models.wrappers <strong class="nx iu">import</strong> LdaMallet</span><span id="3702" class="nk mm it nx b gy of oc l od oe"># NLTK<br/><strong class="nx iu">from</strong> nltk <strong class="nx iu">import</strong> bigrams<br/><strong class="nx iu">from</strong> nltk.stem <strong class="nx iu">import</strong> PorterStemmer</span><span id="9814" class="nk mm it nx b gy of oc l od oe">sns.set(font_scale=<strong class="nx iu">1.5</strong>)<br/>sns.set_style("whitegrid")</span><span id="4840" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">import</strong> warnings <br/>warnings.filterwarnings("ignore", category=DeprecationWarning)</span><span id="8bd4" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">%</strong>matplotlib inline</span></pre><p id="db19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不打算深入解释LDA是如何工作的；详细内容可在[1]的原始论文中找到。LDA算法的核心思想可以理解为一个生成过程，其中文档由一组主题<em class="ln"> T、</em>的概率分布定义，而离散单词的概率分布反过来建立每个主题。说到这里，我们加载预处理的数据:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="41b0" class="nk mm it nx b gy ob oc l od oe">df = pd.read_pickle('pre-processed.pkl')<br/>df.head()</span></pre><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oy"><img src="../Images/633cd4916c925f64af61f41d3d253e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxbZgeAa9lowBpHJPZuyNA.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">预处理数据概述</strong></p></figure><p id="6d6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据加载后，我们继续添加<strong class="kk iu">二元模型</strong>和<strong class="kk iu">三元模型</strong>。表达特定意思时经常一起出现的一系列单词。N个单词的序列被称为N元语法，因为理论上N可以是任意长度；最常见的是成对的单词(二元单词)和一系列三个单词(三元单词)。首先，我们需要对<em class="ln"> no_stop_joined </em>列进行标记化，并将其转换为一个包含每条tweet的标记的列表；我们将这个列表命名为<em class="ln"> data_words </em>，如下所示:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="7e75" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># TOKENIZE</strong><br/><strong class="nx iu">def</strong> sent_to_words(sentences):<br/>    <strong class="nx iu">for</strong> sentence <strong class="nx iu">in</strong> sentences:<br/>        yield(gensim.utils.simple_preprocess(str(sentence), deacc=<strong class="nx iu">True</strong>))  # deacc=True removes punctuations</span><span id="5d72" class="nk mm it nx b gy of oc l od oe">data_words = list(sent_to_words(data))</span></pre><p id="cef0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们准备将二元模型和三元模型添加到我们的语料库中。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="2e4e" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Build the bigram and trigram model</strong><br/>bigram = gensim.models.Phrases(data_words, min_count=10, threshold=100)<br/>trigram = gensim.models.Phrases(bigram[data_words], threshold=100)</span><span id="c8e7" class="nk mm it nx b gy of oc l od oe"># Faster way to get a sentence clubbed as a bigram<br/>bigram_mod = gensim.models.phrases.Phraser(bigram)<br/>trigram_mod = gensim.models.phrases.Phraser(trigram)</span><span id="8611" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">def</strong> make_bigrams(texts):<br/>    <strong class="nx iu">return</strong> [bigram_mod[doc] <strong class="nx iu">for</strong> doc <strong class="nx iu">in</strong> texts]</span><span id="eb5b" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">def</strong> make_trigrams(texts):<br/>    <strong class="nx iu">return</strong> [trigram_mod[bigram_mod[doc]] <strong class="nx iu">for</strong> doc <strong class="nx iu">in</strong> texts]</span><span id="2a88" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Form Bigrams</strong><br/>data_words_bigrams = make_bigrams(data_words)</span></pre><p id="c1b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了我们数据集中的二元模型。我们可以注意到伊斯兰-和平-和平、耶稣-基督、科学-气候变化等的组合。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oz"><img src="../Images/e12a09ccbc7b1de4b66f8a4d8d9d3c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAblmC1Yu074SFwLl2lhyA.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">二元模型可视化</strong></p></figure><p id="209e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步是<em class="ln">词汇化</em>，这是许多文本挖掘应用的基本步骤。词汇化考虑了上下文，并将单词转换为其基本形式；例如，术语“<em class="ln">拥抱</em>”被转换为“<em class="ln">拥抱</em>”,<em class="ln">最好的</em>”被转换为“<em class="ln">好的</em>”对于词汇化任务，使用的包是<em class="ln"> spaCy，</em>一个开源库，其中有许多用于自然语言处理的预建模型。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="2250" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># LEMMATIZATION<br/>def</strong> lemmatization(tweets, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):<br/>    """<a class="ae lu" href="https://spacy.io/api/annotation" rel="noopener ugc nofollow" target="_blank">https://spacy.io/api/annotation</a>"""<br/>    tweets_out = []<br/>    <strong class="nx iu">for</strong> sent <strong class="nx iu">in</strong> tweets:<br/>        doc = nlp(" ".join(sent)) <br/>        tweets_out.append([token.lemma_ <strong class="nx iu">for</strong> token <strong class="nx iu">in</strong> doc <strong class="nx iu">if</strong> token.pos_ in allowed_postags])<br/>    <strong class="nx iu">return</strong> tweets_out</span><span id="b2b5" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Initialize spacy 'en' model, keeping only tagger component<br/># python3 -m spacy download en</strong><br/>nlp = spacy.load('en', disable=['parser', 'ner'])</span><span id="6401" class="nk mm it nx b gy of oc l od oe"># <strong class="nx iu">Lemmatization keeping only noun, adj, vb, adv</strong><br/>df['lemmatized'] = pd.Series(lemmatization(data_words_bigrams, allowed_postags=['<em class="ln">NOUN</em>', '<em class="ln">ADJ</em>', '<em class="ln">VERB</em>', '<em class="ln">ADV</em>']))</span></pre><p id="2bf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一步之后，我们再次删除了重复项，因为由很少几个标记组成的短tweets在被词条化后可能会导致重复行。在这一步之后，我们有48'013个唯一的行。</p><p id="d066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个有用的技术是<em class="ln">词干</em>，这是将一个单词转换成其词根形式的过程。与前面提到的词干化不同，词干化是一种更激进的方法，因为后缀剪切经常导致无意义的英语单词。例如，单词“<em class="ln">动物</em>”将被词条解释为“<em class="ln">动物</em>”，但是搬运工斯特梅尔给出了“<em class="ln">动画</em>”我们决定实现这两者来帮助降维。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="5599" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># STEMMING</strong><br/>stemmer = PorterStemmer()<br/>df['stemmed'] = df['lemmatized'].apply(<strong class="nx iu">lambda</strong> x : [stemmer.stem(y) <strong class="nx iu">for</strong> y <strong class="nx iu">in</strong> x])</span></pre><p id="a534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在构建LDA模型之前，我们必须创建两个主要输入:字典和语料库，它们是使用Gensim 包中的函数创建的。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="393d" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Create Dictionary</strong><br/>id2word_stemmed = corpora.Dictionary(df['stemmed'])<br/><strong class="nx iu">IN:</strong> print(id2word_stemmed)<br/><strong class="nx iu">OUT: </strong>Dictionary(26748 unique tokens: ['also', 'bless', 'blood', 'deed', 'fact']...)\</span><span id="c039" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Create Corpus</strong><br/>tweets_stemmed = df['stemmed']<br/><strong class="nx iu">IN:</strong> df['stemmed'][1]<br/><strong class="nx iu">OUT: </strong>['piti', 'antonym', 'disast', 'human', 'live', 'piti']</span></pre><p id="a088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是语料库的样子，它的长度为48'013:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pa"><img src="../Images/a6f5e71e49802a687f3679fcd248c29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBnx_04Sup5gfWgLcSXkog.png"/></div></div></figure><p id="0ec2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ln"> Gensim </em>给每个单词分配一个唯一的Id，然后将语料库表示为一个元组<em class="ln"> (word_id，word_frequency </em>)。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="84c4" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Term Document Frequency</strong><br/>corpus_stemmed = [id2word_stemmed.doc2bow(tweet) for tweet in tweets_stemmed]</span></pre><p id="bd4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，下面这条未经处理的推文:</p><blockquote class="lo lp lq"><p id="a517" class="ki kj ln kk b kl km ju kn ko kp jx kq lr ks kt ku ls kw kx ky lt la lb lc ld im bi translated">@ wagner_claire宗教和怜悯是反义词。一个是灾难，另一个是人性。我们可以没有宗教，但不能没有怜悯。然而我们拥抱着……”</p></blockquote><p id="8852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在语料库中预处理之后，将呈现为元组列表:</p><blockquote class="lo lp lq"><p id="3967" class="ki kj ln kk b kl km ju kn ko kp jx kq lr ks kt ku ls kw kx ky lt la lb lc ld im bi">[(11, 1), (12, 1), (13, 1), (14, 1), (15, 2)]</p></blockquote><h2 id="ee39" class="nk mm it bd mn nl nm dn mr nn no dp mv kr np nq mx kv nr ns mz kz nt nu nb nv bi translated"><strong class="ak">建立LDA模型</strong></h2><p id="245d" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在是我们初始化话题数量<em class="ln"> k=10，</em>的时候了，这些话题将会被调整。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="f99e" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Build LDA model</strong><br/>lda_model_stemmed = gensim.models.ldamodel.LdaModel(corpus=<strong class="nx iu">corpus_stemmed</strong>,<br/>                                           id2word=<strong class="nx iu">id2word_stemmed</strong>,<br/>                                           num_topics=<strong class="nx iu">10</strong>, <br/>                                           random_state=<strong class="nx iu">100</strong>,<br/>                                           update_every=<strong class="nx iu">1</strong>,<br/>                                           chunksize=<strong class="nx iu">100</strong>,<br/>                                           passes=<strong class="nx iu">15</strong>,<br/>                                           alpha='<strong class="nx iu">auto</strong>',<br/>                                           per_word_topics=<strong class="nx iu">True</strong>)</span></pre><p id="0465" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此步骤之后，我们可以直接生成主题或搜索最佳模型，使用<strong class="kk iu">一致性分数</strong>作为具有不同主题数量的每个模型的度量。</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="4619" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># OPTIMAL MODEL</strong><br/><strong class="nx iu">def</strong> compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):<br/>    <br/>    coherence_values = []<br/>    model_list = []<br/>    <strong class="nx iu">for</strong> num_topics <strong class="nx iu">in</strong> <strong class="nx iu">range</strong>(start, limit, step):<br/>        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_stemmed, num_topics=num_topics, id2word=id2word_stemmed)<br/>        model_list.append(model)<br/>        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')<br/>        coherence_values.append(coherencemodel.get_coherence())</span><span id="3b5a" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">return</strong> model_list, coherence_values</span><span id="7927" class="nk mm it nx b gy of oc l od oe">model_list, coherence_values = compute_coherence_values(dictionary=id2word_stemmed, corpus=corpus_stemmed, texts=df['stemmed'], start=<strong class="nx iu">2</strong>, limit=<strong class="nx iu">26</strong>, step=<strong class="nx iu">3</strong>)</span></pre><p id="f006" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是生成的每个模型的一致性分数，我们选择的最佳模型有<strong class="kk iu"> k=8个主题。</strong></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d427cd8c9f2457bf6a4cea18fcdd6e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*vdgk5T3q57t_KspkFbzsbw.png"/></div></figure><p id="cb96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建立模型并运行最佳模型后，我们将讨论以下主题:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pc"><img src="../Images/419c7ce8439799013a4edcb2792a58cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qKqdU8HsR3wuNFQlYdBCg.png"/></div></div></figure><p id="488d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解释:</p><p id="bb65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Topic [0] <em class="ln">宗教&amp;政治</em> </strong> <em class="ln"> </em>不言而喻是用政治、控制、政府、民族、川普等术语来表示的。，这说明了宗教是一个敏感的方面，它的作用与政治有关。</p><p id="f046" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">主题【1】<em class="ln">基督教</em> </strong> <em class="ln"> </em>由包含基督教作为讨论主题及其代表性关键词(如教会、天主教、基督教等)的推文组成。对于主要属于西方世界、以基督教为主要宗教的说英语的观众来说，这样的话题是意料之中的。</p><p id="48ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">话题【2】<em class="ln">宗教&amp;科学</em> </strong> <em class="ln"> </em>关注的是宗教与科学之间永无止境的争论，这场争论在由技术革命引领的现代世界愈演愈烈。</p><p id="8624" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">话题【3】<em class="ln">宗教教义</em>，【6】<em class="ln">个人信仰</em>，【7】<em class="ln">众说纷纭</em> </strong> <em class="ln"> </em>以关键词为基准，似乎彼此更接近。</p><p id="8415" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">话题【5】<em class="ln">伊斯兰教</em> </strong> <em class="ln"> </em>是以伊斯兰教为主要讨论主题的推文集合。从标签数量和词频统计可以看出，伊斯兰教因其不同方面在社交媒体上受到高度讨论，引发了人们的不同反应。</p><p id="b887" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面给出了推文在各主题中的分布，我们可以注意到前三个主题更占主导地位:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d9e4554fc43c9e0228c31a75f1d4994f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*nSgK7aC3OHj_uhrg-s8hcQ.png"/></div></figure></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="9579" class="ml mm it bd mn mo ot mq mr ms ou mu mv jz ov ka mx kc ow kd mz kf ox kg nb nc bi translated">情感分析(VADER)</h1><p id="bc9b" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">该零件所需的包装如下:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="a7d4" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># IMPORTS<br/>import</strong> pandas <strong class="nx iu">as</strong> pd<br/><strong class="nx iu">import</strong> numpy <strong class="nx iu">as</strong> np<br/><strong class="nx iu">import</strong> seaborn <strong class="nx iu">as</strong> sns<br/><strong class="nx iu">import</strong> matplotlib.pyplot <strong class="nx iu">as</strong> plt<br/><strong class="nx iu">import</strong> spacy<br/><strong class="nx iu">import</strong> re<br/><strong class="nx iu">from</strong> pprint <strong class="nx iu">import</strong> pprint</span><span id="324d" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">import</strong> nltk<br/>nltk.download('vader_lexicon')<br/><strong class="nx iu">from</strong> nltk.sentiment.vader <strong class="nx iu">import</strong> SentimentIntensityAnalyzer<br/><strong class="nx iu">from</strong> nltk.corpus <strong class="nx iu">import</strong> stopwords</span><span id="98ab" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">from</strong> collections <strong class="nx iu">import</strong> Counter<br/><strong class="nx iu">from</strong> wordcloud <strong class="nx iu">import</strong> WordCloud</span><span id="4e93" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">import</strong> warnings <br/>warnings.filterwarnings("ignore", category=DeprecationWarning)</span><span id="42d4" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">%</strong>matplotlib inline</span></pre><p id="1d07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> VADER </strong>代表用于情感推理的效价感知词典，由[2]开发为基于规则的情感分析模型。关于如何开发该模型及其特征的详细信息可在原始文件中找到(见参考文献)。它考虑到了标点符号，大写，程度修饰语，连词，在分配情感值时前面的三个字母:<em class="ln">消极，积极，中性</em>。这些特性使得VADER情感分析器在对推文等社交媒体文本进行分类时取得了显著的效果，并使其成为进行我们分析的合适工具。</p><p id="91c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于VADER情感分析器的特性，预处理阶段与主题建模阶段略有不同，下面列出了所采取的步骤，而代码与预处理阶段相同。<strong class="kk iu">也可以在这里</strong>  <strong class="kk iu">访问所有代码</strong> <a class="ae lu" href="https://github.com/HurmetNoka/religion_on_twitter" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">。</strong></a></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pe"><img src="../Images/c0b588f50368f923d4b03e4face15453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTmIxP0Vdqqvgmp-1WDX2A.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">情感分析的预处理步骤</strong></p></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pf"><img src="../Images/b74542619bb814c67c731d472ac56858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FzBOaS82rDp2sJ8qNpq46g.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">删除不必要的列后的数据概览</strong></p></figure><p id="da46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> VADER模式</strong></p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="33b7" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Create an object of Vader Sentiment Analyzer</strong><br/>vader_analyzer = SentimentIntensityAnalyzer()</span><span id="2b44" class="nk mm it nx b gy of oc l od oe">negative = []<br/>neutral = []<br/>positive = []<br/>compound = []</span><span id="9aac" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">def</strong> sentiment_scores(df, negative, neutral, positive, compound):<br/>    <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> df['tweet_text_p']:<br/>        sentiment_dict = vader_analyzer.polarity_scores(i)<br/>        negative.append(sentiment_dict['neg'])<br/>        neutral.append(sentiment_dict['neu'])<br/>        positive.append(sentiment_dict['pos'])<br/>        compound.append(sentiment_dict['compound'])</span><span id="95bb" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Function calling </strong><br/>sentiment_scores(df, negative, neutral, positive, compound)</span><span id="6e2a" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Prepare columns to add the scores later</strong><br/>df["negative"] = negative<br/>df["neutral"] = neutral<br/>df["positive"] = positive<br/>df["compound"] = compound</span><span id="1273" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Fill the overall sentiment with encoding:<br/># (-1)Negative, (0)Neutral, (1)Positive</strong><br/>sentiment = []<br/><strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> df['compound']:<br/>    <strong class="nx iu">if</strong> i &gt;= 0.05 : <br/>        sentiment.append(1)<br/>  <br/>    <strong class="nx iu">elif</strong> i &lt;= - 0.05 : <br/>        sentiment.append(-1) <br/>        <br/>    <strong class="nx iu">else</strong> : <br/>        sentiment.append(0)<br/>df['sentiment'] = sentiment</span></pre><p id="9a56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将模型应用于我们的数据后，我们为每条推文分配了以下情感分数。最后一列<strong class="kk iu"><em class="ln"/></strong>表示最终分类(<strong class="kk iu"> 1 </strong> -正，<strong class="kk iu"> 0 </strong> -中性，<strong class="kk iu"> -1 </strong> -负)。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pg"><img src="../Images/e59cde11a738ec7b7929536a6e352e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJYI-Pu_I17ONvyrRL5ApA.png"/></div></div></figure><p id="8dc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图给出了推文对情感类别的总体分类结果，其中42.2% (20'512)的推文被归类为<em class="ln">正面。</em> 41.1% (19'957)被归类为<em class="ln">负面，</em>，其余16.7% (8'109)的推文被认为是<em class="ln">中性</em>(既不正面也不负面)。就社交媒体上关于宗教讨论的情绪而言，公众意见似乎是平衡的。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/70e6dfe2e2af4a72755f72f9859df3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*DePHHurxg5ZNQcUPFJKZ9Q.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">根据情感对推文进行分类</strong></p></figure><p id="9011" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们可以使用词云来查看负面和正面的推文，以说明每个类别的主导词。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/31846284b00ead53523b7085a5f53a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*dDrjnX_Ka9ebQ2KLqLrcbg.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">字云正面推文</strong></p></figure><p id="0ce4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最常见的词是与领域相关的词，如<em class="ln">伊斯兰教、穆斯林、基督教、基督教</em>，而一些与情感相关的词是:<em class="ln">爱、和平、支持、伟大、尊重、善良</em>等。，这自然意味着该推文的正面情绪得分。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8d30ca2dc7dc1a35b429cd614c635d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*g5S0nFmINQZy3nJrnIuM0A.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">词云负面推文</p></figure><p id="be88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在负面推文的情况下，除了与领域相关的词之外，一些携带负面情绪的词有:<em class="ln">、杀戮、仇恨、攻击、愚蠢、暴力、问题、邪恶、操、狗屎、</em>等..</p><p id="17a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于积极和消极情绪的观点几乎各半，积极的推文除了轻微的数字优势外，在复合得分方面也有微小的负差异，如下图所示。正面推文的平均值为0.525122，而负面推文的平均值为-0.541433。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pi"><img src="../Images/c96b30d8a8c1098941831197cf72f95a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKOC2fNg7Jj4LFVJvyVWOw.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">复合得分分布</strong></p></figure><p id="b18f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当谈到具体的宗教时，大多数讨论都是针对伊斯兰教的，推文数量最高，经过预处理步骤后有679条推文，其次是基督教，有127条推文，然后是其他流行的宗教，如印度教、佛教和犹太教。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/97844b2ae63c28523eb63d50bc0c4749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*cmPx-69OaKkIrT0OFl-0Ag.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">讨论最多的宗教</strong></p></figure><p id="76ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">伊斯兰教</strong>是社交媒体上讨论最多的宗教。因此，我们特别仔细地观察了对伊斯兰教的看法。对谈论伊斯兰的推文进行情感分析得出了以下结果:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/22a2c9fc552631523135bc2992882612.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*xQ8TOpbOruVu0jyVvUJ4nQ.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">对伊斯兰教的感悟</strong></p></figure><p id="b27f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用相同的方法(VADER)对推文进行分类，分析结果显示44.2% (295条)的推文是正面的，而38.7% (258条)的推文是负面的，而17.0% (113条)被认为是中性的。</p><p id="7579" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下表列出了关于伊斯兰教正面和负面推文中最常见的词汇。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pl"><img src="../Images/90bac9c49be72c46a8f28b74bbc42e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aHBzIybACn5bBk35xN39KQ.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">基于伊斯兰情感的词频</strong></p></figure><p id="ab3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们呈现10条最积极的推文:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pm"><img src="../Images/0778e26ee5df95a874df863c889b4ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JsYGjkqf7C962FF2uEDMWA.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pn"><img src="../Images/d6eed14594454876a082af7c3ffb2a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HfVy7iQRMla8R--AsoRDKQ.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi po"><img src="../Images/9efedef3952c5fa0f3239c79f58b6d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJrpQNhCkizLdhLpqkwjSg.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">十大正面推文</strong></p></figure><p id="4833" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面显示的推文被高度评为正面，其中第10条推文的复合得分为0.9810，第1条推文的复合得分为0.989。分类器产生了正确的标签，因为推文显然是积极的，没有隐藏潜在的讽刺或讽刺模式，这些模式破坏了句子的意思。有趣的是，在10条推文中，只有4条提到了<em class="ln">上帝，</em>这个词，而<em class="ln">爱</em>这个词在10条最积极的推文中有7条被提到了16次。一条是关于伊斯兰教的，两条是关于基督教的。其余的是关于宗教的一般观点，集中在与他人的关系上，即彼此相爱是一个人应该遵循的核心宗教原则。大部分推文(6/10)的评分为负0.000；其他推文的负分非常小，因为这些词:<em class="ln">教条、种族、不请自来的</em>等。个人正面得分最高的推文是第4条，得分为0.683，因为它七次提到了<em class="ln">和平</em>这个词，在不包含否定词的同时增加了句子的正面度。</p><p id="20e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，以下是最负面的10条推文及其各自的得分:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pp"><img src="../Images/a37d15e9b79211336a730e6defabf745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LO5tULR_mw5-1VJATcEUBA.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pq"><img src="../Images/fe3ffac092650c5e2d2c7fc4c64a7fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Qn_FhzB33Pj00kqNGRJZg.png"/></div></div></figure><p id="a4b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面显示的推文被评为非常负面(负面意义上)，其中第10条推文的复合得分为-0.982，第1条最负面的推文的复合得分为-0.988。很明显，分类器产生了正确的标签，因为推文是负面的，表达了对宗教或宗教团体(即穆斯林)的拒绝、厌恶和负面情绪。这张表上的一些主导词及其各自的频率是死亡(8)、伊斯兰教(6)、强奸(4)等。被提及最多的宗教是伊斯兰教，在表达负面情绪时，10条推文中有8条关注伊斯兰教，而一条是关于基督教，另一条是关于印度的佛教/印度教。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="1a9c" class="ml mm it bd mn mo ot mq mr ms ou mu mv jz ov ka mx kc ow kd mz kf ox kg nb nc bi translated">仇恨言论检测(声纳)</h1><p id="63dd" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">用于对讨论宗教的推文进行分类的模型是基于[3]工作的预训练模型。这个模型被认为对我们的任务有用，调查关于宗教的推特上仇恨言论的存在，并对其进行测量。这将有助于对在线环境中的宗教景观有一个总体的了解。</p><p id="cb6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此任务所需的软件包有:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="7edc" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># IMPORTS</strong><br/><strong class="nx iu">import</strong> pandas <strong class="nx iu">as</strong> pd<br/><strong class="nx iu">import</strong> numpy <strong class="nx iu">as</strong> np<br/><strong class="nx iu">import</strong> seaborn <strong class="nx iu">as</strong> sns<br/><strong class="nx iu">import</strong> matplotlib.pyplot as<strong class="nx iu"> </strong>plt</span><span id="7e15" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">from</strong> hatesonar <strong class="nx iu">import</strong> Sonar # This is the hate speech detection library</span><span id="7045" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">import</strong> warnings<br/>warnings.filterwarnings('ignore')</span></pre><p id="0716" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该过程从在笔记本上加载数据帧格式的数据开始。这些数据由大约48，500条推文组成，这些推文没有经过预处理，因为模型被训练来处理输入以提取必要的特征。模型中使用的一些预处理步骤有:移除标签、移除链接、移除提及、标记化和使用波特斯特梅尔的词干化。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pr"><img src="../Images/c9545d7d99f4356a37466ff18fc2b637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8_DvaX4YK4TguAsbDfkRQ.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">数据概述</p></figure><p id="9862" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">声纳型号</strong></p><p id="2b7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将这个分类模型应用于我们的数据:</p><pre class="lw lx ly lz gt nw nx ny nz aw oa bi"><span id="e88e" class="nk mm it nx b gy ob oc l od oe"><strong class="nx iu"># Create an object of Sonar Hate Speech Detection</strong><br/>sonar = Sonar()</span><span id="89f0" class="nk mm it nx b gy of oc l od oe">Class = []<br/>hate = []<br/>offensive = []<br/>neither = []</span><span id="ca6a" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu">def</strong> hate_speech_classifier(df, Class, hate, offensive, neither):<br/> <strong class="nx iu">for</strong> i <strong class="nx iu">in</strong> df['tweet_text']:<br/>        sonar_dict = sonar.ping(text=i)<br/>        Class.append(list(sonar_dict.values())[1])<br/> hate.append(list(list(sonar_dict.values())[2][0].values())[1])<br/> offensive.append(list(list(sonar_dict.values())[2][1].values())[1])<br/> neither.append(list(list(sonar_dict.values())[2][2].values())[1])</span><span id="0b0e" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Function calling </strong><br/>hate_speech_classifier(df, Class, hate, offensive, neither)</span><span id="3db9" class="nk mm it nx b gy of oc l od oe"><strong class="nx iu"># Prepare columns to add the scores later</strong><br/>df["Class"] = Class<br/>df["hate"] = hate<br/>df["offensive"] = offensive<br/>df["neither"] = neither</span></pre><p id="a341" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">生成的数据帧如下所示:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ps"><img src="../Images/ec2c7af0da25e576a502b9e6e50dbe2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_73T3n1VTeFUs1ukz9C0g.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">结果概述</p></figure><p id="7647" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型应用后，由48'528条推文组成的数据集被拆分为三类<strong class="kk iu"> <em class="ln">仇恨言论、</em>攻击性语言、<em class="ln">既不</em> </strong> <em class="ln"> </em>(表示既不仇恨也不攻击)。下图中的第一个图表给出了推文在这些类别中分布的总体结果。仇恨和攻击性的推文加在一起总共产生了3802条推文，占数据集的7.83%。因此，没有问题的推文总共有44726条，占数据集的92.16%。第二张图表显示了所谓的有问题推文的分布情况，仇恨言论导致了总共232条推文(占有问题推文的6.10%)，因为攻击性语言在这一类别中占主导地位，共有3，570条(93.89%)。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pt"><img src="../Images/2b403811317ef87486135c770736319a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DChVqeM5snumZWcvdC1uUQ.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">仇恨言论检测结果</strong></p></figure><p id="dd7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是观察情感特征如何与仇恨言论分类结果相关联。下图显示了仇恨言论、攻击性语言、和<em class="ln">【非T3】<em class="ln">负面、正面、</em>或<em class="ln">中性</em>等分类的推文分布。正如预期的那样，仇恨言论更多地出现在负面推文中，从数字上讲，60%(232条中的145条)包含仇恨言论的推文是负面的。在攻击性推文的情况下，比例类似于仇恨言论，正如习惯上预期的那样，大多数推文:58.8%(3570条推文中的2089条)包含负面情绪。从其余部分来看，1106条或30.98%是正面的，375条或10.50%被归类为冒犯性的推文是情绪中性的。</em></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pu"><img src="../Images/d2386efa273d07b35539e67b03494fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aY-5GaClfmOeLARc5XegBA.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">通过情绪检测仇恨言论</strong></p></figure><p id="bcc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了基于复合情绪得分的仇恨推文的分布。与负面推文相对应的左侧人口更多(60%的推文)，这使得使用仇恨言论的推文在情绪上是负面的。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/d5354003013a12d0a6291b82c459aea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*q6Eo2jmmh5gW2BCK0FRoGA.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">仇恨言论按复合情绪评分</strong></p></figure><p id="b535" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们呈现十大最可恶的推文:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pw"><img src="../Images/3b7847b23dd262bf4bd67ce5717ae4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5BUsNepvNk-fwq9c_Vk3A.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi px"><img src="../Images/d25fd1c7f377dcfba0ace29cc1d473b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XiAN_zMiIftnIgoZP0YMig.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">十大最可恶的推文</strong></p></figure><p id="fd1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在被归类为仇恨言论的232条推文中，上述推文的仇恨得分最高，第10条推文的仇恨得分为0.676，第1条推文的仇恨得分为0.870。从一般的人工分析来看，我们看到这些推文被攻击性的词语占据，如推文编号[4]。该模型被训练为通过关注仇恨词汇来克服攻击性语言和仇恨言论之间的细微界限。仇恨最主要的方面是种族。10条最可恶的推文中有8条提到了白人，使用了像<em class="ln">白人女权主义者、美国白人种族主义者、白人、白人民族主义、欧洲白人血统、白人男性和白人男孩俱乐部这样的词语。</em>第[2]条推文表达了对穆斯林的仇恨，称他们的宗教<em class="ln">过时、性别歧视、愚蠢、</em>而第[7]条和第[8]条推文提到了犹太人。第6条推特是带有种族主义动机的仇恨言论，指的是黑人使用俚语词<em class="ln"> niggas。</em></p><p id="9190" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们也展示最具攻击性的推文:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi py"><img src="../Images/98ca6ac4458c9fb5ce6f57bfa63f7b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23EvVdvalwyYayjeuY-SsA.png"/></div></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi pp"><img src="../Images/4633d51fc207e1eba7ee5dab6dba5b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3F30Atac2WOMlc9sxRJmQg.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><strong class="bd nj">十大最具攻击性的推文</strong></p></figure><p id="f78f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">冒犯性语言评分在榜单中占据主导地位，因为第一条推文的评分从96.9%到98.9%不等。只有第10条推文提到了伊斯兰教，第5条推文使用了希伯来语，但没有明确的证据表明提到了犹太教。攻击性的推文中充斥着大量攻击性的词语，如<em class="ln"> bitch，</em>出现在列表中10条推文中的8条。Tweets [3，5，6]包含强烈的性相关词汇，而只有tweets [1，2]以比喻的方式使用短语“是我的宗教”。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="adde" class="ml mm it bd mn mo ot mq mr ms ou mu mv jz ov ka mx kc ow kd mz kf ox kg nb nc bi translated">结论</h1><p id="0523" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在清理和预处理步骤之后，数据被提取到LDA算法以执行主题建模。这一阶段旨在调查公众关心的与宗教相关的话题，从而调查宗教的多维性是如何在网上讨论中体现的。然后使用VADER情绪分析工具对这些推文进行情绪分析，以揭示公众对宗教的情绪，无论是<em class="ln">负面、正面、</em>还是<em class="ln">中立</em>。最后，声纳是一种仇恨言论检测工具，用于调查和测量宗教讨论中是否存在仇恨言论和攻击性语言。</p><p id="9796" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果显示，与宗教相关的公共讨论最相关的话题是<em class="ln">宗教&amp;政治</em>、<em class="ln">宗教&amp;科学</em>、<em class="ln">基督教</em>和<em class="ln">伊斯兰教</em>。情绪分析显示，公众对宗教的情绪存在分歧，42.2% (20'512)的推文被归类为积极的。41.1%(19957例)为阴性，其余为中性。这些结果可以用来论证现代社会已经接受的世俗化和宗教归属的水平。仇恨言论检测工具提供的证据表明，社交媒体上确实存在以宗教为动机的仇恨言论和攻击性语言。总共有3802条推文或7.83%的数据集包含仇恨言论(232条推文)或攻击性语言(3570条推文)。这表明宗教是一个公众意见分歧的领域，可能会在他们的讨论中激发仇恨和攻击性语言。因此，社交媒体平台应该做出更多努力来防止他们的用户在他们的在线讨论中的这种滥用行为。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><blockquote class="lo lp lq"><p id="46d6" class="ki kj ln kk b kl km ju kn ko kp jx kq lr ks kt ku ls kw kx ky lt la lb lc ld im bi translated">谢谢你</p></blockquote><p id="eb81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考文献</strong></p><p id="0e30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]布莱博士，Ng，A. Y .，&amp;乔丹，M. I. (2003年，第1期)。潜在狄利克雷分配。(拉弗蒂编辑。)<em class="ln">机器学习研究杂志</em>，993–1022。</p><p id="71f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]休顿，C. J .，&amp;吉尔伯特，E. (2014年)。VADER:基于规则的社交媒体文本情感分析的简约模型。<em class="ln">第八届AAAI网络日志和社交媒体国际会议。密歇根州:AAAI出版社。</em></p><p id="d6a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]t .戴维森、d .瓦姆斯利、m .梅西和I .韦伯(2017年)。自动仇恨言论检测和攻击性语言问题。第十一届AAAI国际网络和社交媒体会议。蒙特利尔:AAAI出版社。</p></div></div>    
</body>
</html>