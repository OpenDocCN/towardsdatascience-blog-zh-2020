<html>
<head>
<title>Binary Classification Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二元分类示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c?source=collection_archive---------16-----------------------#2020-05-24">https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c?source=collection_archive---------16-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测阿片类药物的使用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d8c948ead8d5dc8fb68b0515f68b720c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G5avFk8iG3FPas_u"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">里卡多·罗查在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9e6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这场全球危机以这样或那样的方式影响了我们所有人的生活，但这是一个磨练你的手艺的绝佳机会。我碰巧更新了我的Coursera账户，专门尝试网络分析。加密货币/区块链媒介文章已经成为我日常生活的一部分。最后，我想发布一个ML分类的例子来帮助那些寻找一个详细的“从开始到结束”的用例的人，并从整个社区中寻求建设性的反馈。这将是一个漫长而详细的旅程，我希望你已经准备好了。</p><p id="3d17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将利用一个保险数据集，该数据集概述了一系列以患者为中心的特征，最终目标是正确预测阿片类药物滥用是否已经发生。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a0d5" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">功能定义</h1><p id="a8f9" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><strong class="lb iu">ClaimID Unique:</strong> Identifier for a claim<br/><strong class="lb iu">Accident DateID:</strong> Number of days since the accident occurred from an arbitrary date<br/><strong class="lb iu">Claim Setup DateID:</strong> Number of days since the Resolution Manager sets up the claim from an arbitrary date<br/><strong class="lb iu">Report To DateID:</strong> Number of days since the employer notifies insurance of a claim from an arbitrary date<br/><strong class="lb iu">Employer Notification DateID:</strong> Number of days since the claimant notifies employer of an injury from an arbitrary date<br/><strong class="lb iu">Benefits State:</strong> The jurisdiction whose benefits are applied to a claim<br/><strong class="lb iu">Accident State:</strong> State in which the accident occurred<br/><strong class="lb iu">Industry ID:</strong> Broad industry classification categories<br/><strong class="lb iu">Claimant Age:</strong> Age of the injured worker <br/><strong class="lb iu">Claimant Sex:</strong> Sex of the injured worker <br/><strong class="lb iu">Claimant State:</strong> State in which the claimant resides<br/><strong class="lb iu">Claimant Marital Status:</strong> Marital status of the injured worker <br/><strong class="lb iu">Number Dependents:</strong> Number of dependents the claimant has<br/><strong class="lb iu">Weekly Wage:</strong> An average of the claimant’s weekly wages as of the injury date.<br/><strong class="lb iu">Employment Status Flag:</strong><br/> F — Regular full-time employee<br/> P — Part-time employee<br/> U — Unemployed<br/> S — On strike<br/> D — Disabled<br/> R — Retired<br/> O — Other<br/> L — Seasonal worker<br/> V — Volunteer worker<br/> A — Apprenticeship full-time<br/> B — Apprenticeship part-time<br/> C — Piece worker<br/><strong class="lb iu">RTW Restriction Flag:</strong> A Y/N flag, used to indicate whether the employees responsibilities upon returning to work were limited as a result of his/her illness or injury.<br/><strong class="lb iu">Max Medical Improvement DateID:</strong> DateID of Maximum Medical Improvement, after which further recovery from or lasting improvements to an injury or disease can no longer be anticipated based on reasonable medical probability.<br/><strong class="lb iu">Percent Impairment:</strong> Indicates the percentage of anatomic or functional abnormality or loss, for the body as a whole, which resulted from the injury and exists after the date of maximum medical improvement<br/><strong class="lb iu">Post Injury Weekly Wage:</strong> The weekly wage of the claimant after returning to work, post-injury, and/or the claim is closed.<br/><strong class="lb iu">NCCI Job Code:</strong> A code that is established to identify and categorize jobs for workers’ compensation.<br/><strong class="lb iu">Surgery Flag:</strong> Indicates if the claimant’s injury will require or did require surgery<br/><strong class="lb iu">Disability Status:</strong><br/> — Temporary Total Disability (TTD)<br/> — Temporary Partial Disability (TPD)<br/> — Permanent Partial Disability (PPD)<br/> — Permanent Total Disability (PTD)<br/><strong class="lb iu">SIC Group:</strong> Standard Industry Classification group for the client<br/><strong class="lb iu">NCCI BINatureOfLossDescription</strong>: Description of the end result of the bodily injury (BI) loss occurrence<br/><strong class="lb iu">Accident Source Code:</strong> A code identifying the object or source which inflicted the injury or damage.<br/><strong class="lb iu">Accident Type Group:</strong> A code identifying the general action which occurred resulting in the loss<br/><strong class="lb iu">Neurology Payment Flag:</strong> Indicates if there were any payments made for diagnosis and treatment of disorders of the nervous system without surgical intervention<br/><strong class="lb iu">Neurosurgery Payment Flag:</strong> Indicates if there were any payments made for services by physicians specializing in the diagnosis and treatment of disorders of the nervous system, including surgical intervention if needed<br/><strong class="lb iu">Dentist Payment Flag:</strong> Indicates if there were any payments made for prevention, diagnosis, and treatment of diseases of the teeth and gums<br/><strong class="lb iu">Orthopedic Surgery Payment Flag:</strong> Indicates if there were any payments made for surgery dealing with the skeletal system and preservation and restoration of its articulations and structures.<br/><strong class="lb iu">Psychiatry Payment Flag:</strong> Indicates if there were any payments made for treatment of mental, emotional, or behavioral disorders.<br/><strong class="lb iu">Hand Surgery Payment Flag:</strong> Indicates if there were any payments made for surgery only addressing one or both hands.<br/><strong class="lb iu">Optometrist Payment Flag:</strong> Indicates if there were any payments made to specialists who examine the eye for defects and faults of refraction and prescribe correctional lenses or exercises but not drugs or surgery<br/><strong class="lb iu">Podiatry Payment Flag:</strong> Indicates if there were any payments made for services from a specialist concerned with the care of the foot, including its anatomy, medical and surgical treatment, and its diseases.<br/><strong class="lb iu">HCPCS A Codes — HCPCS Z Codes:</strong> Count of the number of HCPCS codes that appear on the claim within each respective code group<br/><strong class="lb iu">ICD Group 1 — ICD Group 21:</strong> Count of the number of ICD codes that appear on the claim w/in each respective code group<br/><strong class="lb iu">Count of the number of codes on the claim</strong><br/> — CPT Category — Anesthesia<br/> — CPT Category — Eval_Mgmt<br/> — CPT Category — Medicine<br/> — CPT Category — Path_Lab<br/> — CPT Category — Radiology<br/> — CPT Category — Surgery<br/><strong class="lb iu">Count of the number of NDC codes on the claim within each respective code class</strong><br/> — NDC Class — Benzo<br/> — NDC Class — Misc (Zolpidem)<br/> — NDC Class — Muscle Relaxants<br/> — NDC Class — Stimulants<br/><strong class="lb iu">Opioids Used:</strong> A True (1) or False (0) indicator for whether or not the claimant abused an opioid</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1d44" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">探索性数据分析</h1><p id="e852" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们从导入所有需要的库和数据集开始。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a103" class="ne md it na b gy nf ng l nh ni">from math import sqrt<br/>import pandas as pd<br/>import numpy as np<br/>import scipy.stats as stats<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from IPython.display import display</span><span id="15c0" class="ne md it na b gy nj ng l nh ni">from sklearn.model_selection import train_test_split<br/>from sklearn.pipeline import Pipeline</span><span id="b167" class="ne md it na b gy nj ng l nh ni">from feature_engine import missing_data_imputers as mdi<br/>from feature_engine import categorical_encoders as ce<br/>from sklearn.preprocessing import RobustScaler<br/>from imblearn.over_sampling import SMOTE</span><span id="5232" class="ne md it na b gy nj ng l nh ni">from sklearn.model_selection import cross_val_score<br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.metrics import roc_curve<br/>from sklearn.metrics import recall_score<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.model_selection import GridSearchCV</span><span id="eeee" class="ne md it na b gy nj ng l nh ni">from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn import svm</span><span id="fd64" class="ne md it na b gy nj ng l nh ni">%matplotlib inline<br/>pd.pandas.set_option('display.max_columns', None)</span><span id="7868" class="ne md it na b gy nj ng l nh ni">with open('opioid_use_data.csv') as f:<br/>    df = pd.read_csv(f)<br/>f.close()</span><span id="3a9f" class="ne md it na b gy nj ng l nh ni">SEED = 42</span><span id="c487" class="ne md it na b gy nj ng l nh ni">df.shape<br/>df.head(10)<br/>df.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9fbbb04e8048fc8c868a3257af0f8755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRf0pN1D2P6u9VsnR2aW8g.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/aa73c3ffbe9be89bd2e2a31ab13c4054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*Bn3jotVJoJnwWY_i4Q8igg.png"/></div></figure><p id="04d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据集包含刚刚超过16，000个观察值以及92个特征，包括目标(即使用的阿片类药物)。我们还有各种各样的特征类型，包括整数、浮点、字符串、布尔和混合类型。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="7659" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">删除初始特征</h2><p id="5dd3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在我们处理缺失数据、离群值或基数之前，让我们看看是否可以快速删除任何功能以简化我们的进一步分析。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b26e" class="ne md it na b gy nf ng l nh ni">for var in df.columns:<br/>    print(var, df[var].nunique(), len(df))</span><span id="772f" class="ne md it na b gy nj ng l nh ni">df.drop('ClaimID', axis=1, inplace=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/30214640ad9f21671cf57a896fc478b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*rxFR2Xt47hRW78Xmmj0sZw.png"/></div></figure><p id="4e0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们滚动输出时，可以看到每个要素的唯一值的数量以及整个数据集的总长度。具有与数据帧总长度相似数量的唯一值的特征可以被移除，因为它们不提供太多的预测能力(即方差)。“ClaimID”是唯一符合此标准的功能，可以删除。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="10bd" class="ne md it na b gy nf ng l nh ni">df_copy = df.copy()<br/>corr_matrix = df_copy.corr().abs()<br/>upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))</span><span id="6443" class="ne md it na b gy nj ng l nh ni">upper</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/2e44c2ec8688ef58a49bcec5055e0a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ds9_brJuc6xYcY2gg4gZpw.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="404c" class="ne md it na b gy nf ng l nh ni">to_drop = [var for var in upper.columns if any(upper[var] &gt; .90)]<br/>to_drop</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/088d9ed7e7fd489ee5e1da8911cd8b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*A-szG_YG5hSo4LbOezAABA.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8b3a" class="ne md it na b gy nf ng l nh ni">df.drop(df[to_drop], axis=1, inplace=True)</span></pre><p id="bcd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们可以检查数字特征之间的相关性，并删除相关性非常高的特征。由您决定什么被认为是“高度相关”,但在这种情况下，我们将选择90及以上的相关性。请注意，在代码中，我们构建了一个相关矩阵，并将相关性转换为绝对值，以便处理负相关性。可以删除索赔设置日期标识、报告日期标识、雇主通知日期标识和最大医疗改善日期标识。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c1ae" class="ne md it na b gy nf ng l nh ni">for var in df.columns:<br/>    print(var, 'percent of missing values', df[var].isnull().mean().round(3))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6b66230f318f1ab13f31e761bce52115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*gsP5dQy7-YEi4KCilGZShA.png"/></div></figure><p id="f8f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以检查剩余要素的缺失值百分比，并移除任何缺失数据过多的要素。一个特性“事故源代码”有超过50%的缺失值，但这不足以保证删除。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c614" class="ne md it na b gy nf ng l nh ni">states = pd.DataFrame(df[['Claimant State', 'Accident State', 'Benefits State']])</span><span id="7104" class="ne md it na b gy nj ng l nh ni">states</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ad37e0e30c837206656dfb192af211f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*9xm2ZBaJWE2Oz3vwJEVHoA.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b107" class="ne md it na b gy nf ng l nh ni">df.drop(['Claimant State', 'Accident State'], axis=1, inplace=True)</span></pre><p id="2abf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考察“索赔国”、“事故国”和“福利国”，我们发现绝大多数值是相同的。我们将保留“福利状态”,因为它包含最少的缺失值。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="80af" class="ne md it na b gy nf ng l nh ni">for var in df.columns:<br/>    print(var, df[var].unique(), '\n')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/d762b55e0f606c87534000c54127eddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CF4QE62NlheWqpJh8ylReQ.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a5d9" class="ne md it na b gy nf ng l nh ni"># Converting nulls to np.nan<br/>for var in df.columns:<br/>    df[var].replace(to_replace='  ', value=np.nan, inplace=True)</span><span id="7e50" class="ne md it na b gy nj ng l nh ni"># Converting values of "X" to np.nan<br/>for var in df.columns:<br/>    df[var].replace(to_replace='X', value=np.nan, inplace=True)</span><span id="acd7" class="ne md it na b gy nj ng l nh ni"># Splitting out "Accident Type Group" (ie. mixed type feature) into separate features<br/>df['Accident Type Group num'] = df['Accident Type Group'].str.extract('(\d+)')<br/>df['Accident Type Group str'] = df['Accident Type Group'].str[0:5]<br/>df.drop(['Accident Type Group'], axis=1, inplace=True)</span></pre><p id="3713" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们检查每个特性的唯一值时，我们可以开始看到一些需要我们注意的差异。首先，我们注意到未转换为Np.nan的空白值或空值。接下来，我们看到许多要素的“X”值，这似乎是记录差异，记录数据的个人记录了带有“X”的缺失值。最后，名为“事故类型组”的特征是我们所说的混合类型，因为它包含字符串和数值。让我们将字符串和数值分离成各自的特征，并删除原来的“事故类型组”特征。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="86e4" class="ne md it na b gy nf ng l nh ni">for var in df.columns:<br/>    print(var,'\n', df[var].value_counts()/len(df))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e82b9aff202f58bd1ac59f9462a76299.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*FqA290_SfxIj3LgXeL89Ng.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cd58" class="ne md it na b gy nf ng l nh ni">df.drop(['Neurology Payment Flag', 'Neurosurgery Payment Flag', 'Dentist Payment Flag',<br/>         'Psychiatry Payment Flag', 'Hand Surgery Payment Flag', 'Optometrist Payment Flag',<br/>         'Podiatry Payment Flag', 'Accident Type Group str', 'Post Injury Weekly Wage'], axis=1, inplace=True)</span><span id="7f52" class="ne md it na b gy nj ng l nh ni">df.drop(['HCPCS B Codes','HCPCS C Codes', 'HCPCS D Codes', 'HCPCS F Codes', 'HCPCS H Codes', 'HCPCS I Codes',<br/>'HCPCS K Codes', 'HCPCS M Codes', 'HCPCS N Codes', 'HCPCS O Codes', 'HCPCS P Codes',<br/>'HCPCS Q Codes', 'HCPCS R Codes', 'HCPCS S Codes', 'HCPCS T Codes', 'HCPCS U Codes', <br/>'HCPCS V Codes', 'HCPCS X Codes', 'HCPCS Y Codes', 'HCPCS Z Codes', 'ICD Group 1',<br/>'ICD Group 2', 'ICD Group 3', 'ICD Group 4', 'ICD Group 5', 'ICD Group 7', 'ICD Group 8', <br/>'ICD Group 9', 'ICD Group 10', 'ICD Group 11', 'ICD Group 12', 'ICD Group 14', 'ICD Group 15', <br/>'ICD Group 16', 'ICD Group 17', 'ICD Group 20', 'NDC Class - Benzo', 'NDC Class - Misc (Zolpidem)',<br/>'NDC Class - Stimulants'], axis=1, inplace=True)</span></pre><p id="f91e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们把注意力转向基数或每个特性的唯一值/类别的数量。像“周工资”这样的连续特征无疑会有数百甚至数千个独特的类别。标称和离散特征(即性别和受抚养人人数)的种类数量要少得多。本练习的目标是确定是否有任何类别拥有大多数(90%+)的值。如果一个特性包含一个或两个包含90%以上值的类别，那么数据中就没有足够的可变性来保留该特性。最终由你来决定截止值，但我们认为90%或更高是一个安全的假设。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="bd18" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">特征特性</h1><p id="3667" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">现在，我们已经成功地消除了许多由于高度相关性、重复值和缺乏可变性而导致的特征，我们可以专注于检查特征特性并决定如何解决每个问题。</p><p id="0a89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oe">你会注意到，在这一部分，我们只是简单地识别问题并做一个心理记录。我们将不会实际应用讨论的变化，直到笔记本结束到一个功能工程管道。</em></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e88e" class="ne md it na b gy nf ng l nh ni">categorical = ['Benefits State', 'Industry ID', 'Claimant Sex', 'Claimant Marital Status', 'Employment Status Flag', 'RTW Restriction Flag', 'NCCI Job Code', <br/>'Surgery Flag', 'Disability Status', 'SIC Group', 'NCCI BINatureOfLossDescription', 'Accident Source Code',<br/>'Orthopedic Surgery Payment Flag','Accident Type Group num']</span><span id="a99d" class="ne md it na b gy nj ng l nh ni">discrete = ['Claimant Age', 'Number Dependents', 'Percent Impairment', 'HCPCS A Codes',<br/>'HCPCS E Codes', 'HCPCS G Codes', 'HCPCS J Codes','HCPCS L Codes', 'HCPCS W Codes', <br/>'ICD Group 6', 'ICD Group 13','ICD Group 18', 'ICD Group 19', 'ICD Group 21', <br/>'CPT Category - Anesthesia', 'CPT Category - Eval_Mgmt', 'CPT Category - Medicine', <br/>'CPT Category - Path_Lab', 'CPT Category - Radiology', 'CPT Category - Surgery', <br/>'NDC Class - Muscle Relaxants']</span><span id="d6ec" class="ne md it na b gy nj ng l nh ni">continuous = ['Accident DateID', 'Weekly Wage']</span><span id="f86d" class="ne md it na b gy nj ng l nh ni">df.columns,'Number of Features:',len(df.columns)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f4e79d884f72b42963b0405d984369b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*PnIKSpdvVOSvkg-N5eOEhw.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="934e" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">缺少值</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="56bb" class="ne md it na b gy nf ng l nh ni">for var in df.columns:<br/>    if df[var].isnull().sum() &gt; 0:<br/>        print(var, df[var].isnull().mean())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b59594e56fd1b9663c2e8f58f0d0066c.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*DeWX69xWFA5BozbXwSFJag.png"/></div></figure><p id="bcf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据我们之前对缺失值的观察，我们发现只有一个要素包含超过50%的缺失值，而绝大多数要素不包含任何缺失数据。也就是说，我们确实有一些包含缺失数据的要素，我们需要确定如何处理这个问题，因为许多ML算法需要完全干净的数据集。这是一个广泛的话题，我们不希望在这个博客中涵盖错综复杂的内容，但读者应该熟悉这个话题。首先，人们必须适应各种类型的缺失数据，如“完全随机缺失”、“随机缺失”和“非随机缺失”。这些主题有助于确定应该如何以及何时处理丢失的数据。此外，进一步解读插补技术，如均值/中值/众数、任意值插补、添加缺失数据指标、随机样本插补、最大似然插补等。将提供一个很好的概述。</p><p id="35b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有9个缺失数据的特征。对于缺失值少于5%的特征(即申请人性别、申请人婚姻状况、就业状况标志、环球旅行限制标志)我们将用其分布模式替换缺失值。由于缺失值的百分比较低，模式插补不会对分布造成太大的改变。“事故日期”是我们唯一具有缺失数据的连续特征，我们将使用任意数字-99999来估算缺失值。所有其他特征本质上都是分类的，由于它们有超过5%的缺失值，我们将用字符串“missing”来估算缺失值。这最终不会改变发行版，只会给他们的发行版增加一个新的类别。</p><p id="750c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故日期ID:连续w/ -99999 <br/>索赔人性别:分类w/模式<br/>索赔人婚姻状况:分类w/模式<br/>就业状况标志:分类w/模式<br/> RTW限制标志:分类w/模式<br/>残疾状况:分类w/‘失踪’<br/>NCCI binatureofloss描述:分类w/‘失踪’<br/>事故来源代码:分类w/‘失踪’<br/>事故类型组号:分类w/‘失踪’</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="819b" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">分类和离散特征的基数</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f092" class="ne md it na b gy nf ng l nh ni">for var in categorical:<br/>    print(var, 'has', df[var].nunique(), 'unique categories')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/3a5ac9f841aefff3fe4307a87a1718d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*7Y7uB-lWtPRag9M1kL0Qog.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="26b8" class="ne md it na b gy nf ng l nh ni">for var in discrete:<br/>    print(var, 'has', df[var].nunique(), 'unique categories')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/03a6b62cfd2656683d06f21ec9136ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*KS4mGeXhpQgnlTwLcFskew.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ff2d" class="ne md it na b gy nf ng l nh ni">for var in categorical:<br/>    print(df[var].nunique(),(df.groupby(var)[var].count()/len(df)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eb9915b63949601e53155f08577c1594.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*dVHurtfo9SRPQANyOFiUmA.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9174" class="ne md it na b gy nf ng l nh ni">for var in discrete:<br/>    print(var, 'has', df[var].nunique(), 'unique categories')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5908180a1e9bd5106f3e2c583fe1366c.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*0bs_5SvRnnom6v_VZFCUrA.png"/></div></figure><p id="6729" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上一节中，我们查看了基数，以便删除可变性低的特性(即具有包含大部分数据的类别的特征)。我们需要更深入地检查基数，并识别“稀有”类别。换句话说，哪些类别只包含非常小百分比的数据(=&lt;1%)。我们将把所有的类别聚合成一个“稀有”的类别，从而减少每个特性的基数，简化模型。当我们讨论编码分类和离散特征时，这种方法将非常有用。</p><p id="2871" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，功能“就业状态标志”目前有13个类别(包括np.nan ),但正如您所见,“F =全职”和“P =兼职”类别几乎占了数据的96%。所有其他类别仅出现0.5%的时间，它们都将被汇总为“罕见”类别。任何类别出现次数少于1%的分类或离散特征都将这些类别编码为“稀有”。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="6c46" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">分布和异常值</h2><p id="735c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">该数据集仅包含两个连续特征“事故日期”和“周工资”。我们需要确定这些特征是否包含偏斜分布，以及它们是否包含任何异常值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8d09" class="ne md it na b gy nf ng l nh ni">for var in continuous:<br/>    plt.figure(figsize=(10,5))<br/>    plt.subplot(1,2,1)<br/>    fig = df.boxplot(column=var)<br/>    fig.set_title('outliers')<br/>    fig.set_ylabel(var)<br/>    <br/>    plt.subplot(1,2,2)<br/>    fig = df[var].hist(bins=20)<br/>    fig.set_ylabel('number of cases')<br/>    fig.set_xlabel(var)<br/>    <br/>    plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4ef561d39f71180856b9152175796872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*9Uk-9_u-HDdZBWJug-KpYA.png"/></div></figure><p id="b1e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个特征当然保持了偏态分布，但只有“周工资”包含任何异常值。最大似然算法对数据有某些假设，我们需要遵循这些假设来增加它们的预测能力。例如，线性回归假设预测值和目标值之间的关系是线性的(线性)。它还假设数据中没有异常值。对于高度相关的要素(多重共线性)，这尤其困难。最后，它假设您的要素呈正态分布。</p><p id="2f74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正态分布的要素遵循高斯分布，您可能还记得高中统计课程中的类似钟形的分布。如你所见,“事故日期”和“周工资”都不是正态分布的。有几种常用的方法来修复偏斜分布，如对数、倒数、平方根、box-cox和yeo-johnson变换。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="dcec" class="ne md it na b gy nf ng l nh ni">df_copy = df.copy()</span><span id="f8ab" class="ne md it na b gy nj ng l nh ni">df_copy['Accident DateID'].skew(skipna = True)</span><span id="86f7" class="ne md it na b gy nj ng l nh ni">df_copy['Accident DateID_log'] = df_copy['Accident DateID'].map(lambda x: np.log(x))</span><span id="7092" class="ne md it na b gy nj ng l nh ni">df_copy['Accident DateID_rec'] = df_copy['Accident DateID'].map(lambda x: np.reciprocal(x))</span><span id="87b6" class="ne md it na b gy nj ng l nh ni">df_copy['Accident DateID_sqrt'] = df_copy['Accident DateID'].map(lambda x: np.sqrt(x))</span><span id="47d8" class="ne md it na b gy nj ng l nh ni">df_copy['Accident DateID_log'].skew(skipna = True)<br/>df_copy['Accident DateID_rec'].skew(skipna = True)<br/>df_copy['Accident DateID_sqrt'].skew(skipna = True)</span><span id="7bd9" class="ne md it na b gy nj ng l nh ni">df['Accident DateID_rec'] = df['Accident DateID'].map(lambda x: np.reciprocal(x))</span><span id="361e" class="ne md it na b gy nj ng l nh ni">df.drop(['Accident DateID'], axis=1, inplace=True)</span></pre><p id="caf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“事故日期”的校正偏差如下:</p><p id="721f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故日期:0.137</p><p id="343a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故日期_日志:0.111</p><p id="b254" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故日期_记录:0.00</p><p id="9e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故日期ID_sqrt: 0.124</p><p id="c9b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到“事故日期”的初始偏差为0.137，从技术上讲，这并不是很大的偏差，因为正态分布的偏差为零(0)。也就是说，应用倒数变换将我们的偏斜调整为零(0)。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cd4d" class="ne md it na b gy nf ng l nh ni">def diagnostic_plot(df, variable):<br/>    plt.figure(figsize=(16, 4))<br/>    sns.distplot(df[variable], bins=30)<br/>    plt.title('Histogram')</span><span id="73b0" class="ne md it na b gy nj ng l nh ni">diagnostic_plot(df_copy, 'Accident DateID_rec')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/bed52142127be05fe968fe5820c10607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pKJkYvPzeXWUkv5AGLMhTQ.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4493" class="ne md it na b gy nf ng l nh ni">df_copy['Weekly Wage'].skew(skipna = True)</span><span id="01a5" class="ne md it na b gy nj ng l nh ni">df_copy['Weekly Wage_log'] = df_copy['Weekly Wage'].map(lambda x: np.log(x) if x &gt; 0 else 0)</span><span id="73e3" class="ne md it na b gy nj ng l nh ni">df_copy['Weekly Wage_rec'] = df_copy['Weekly Wage'].map(lambda x: np.reciprocal(x) if x &gt; 0 else 0)</span><span id="7890" class="ne md it na b gy nj ng l nh ni">df_copy['Weekly Wage_sqrt'] = df_copy['Weekly Wage'].map(lambda x: np.sqrt(x))<br/>df_copy['Weekly Wage_log'].skew(skipna = True)<br/>df_copy['Weekly Wage_rec'].skew(skipna = True)<br/>df_copy['Weekly Wage_sqrt'].skew(skipna = True)</span><span id="2963" class="ne md it na b gy nj ng l nh ni">df['Weekly Wage_sqrt'] = df['Weekly Wage'].map(lambda x: np.sqrt(x))</span><span id="0c9e" class="ne md it na b gy nj ng l nh ni">df.drop(['Weekly Wage'], axis=1, inplace=True)</span></pre><p id="13f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">修正后的“周工资”偏差如下:</p><p id="a23e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">周工资:2.57英镑</p><p id="40ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">周工资_日志:-3.74</p><p id="4b03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">周工资_收入:54.37英镑</p><p id="ba11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">周工资_sqrt: 0.407</p><p id="6508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“周工资”的初始偏差要大得多，为2.56，但平方根变换使偏差显著下降(0.40)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/8c56803205bc20977184784a57d10c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Js9eKPPud2XJMgGzPZRAYg.png"/></div></div></figure><p id="225e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经修复了偏斜度，让我们来解决位于“Weekly Wage_sqrt”中的异常值，因为我们已经放弃了原来的特性。由于“周工资_sqrt”呈正态分布，我们可以使用“平均值的3倍标准差”规则来识别异常值。如果您的分布是偏态的，您最好先计算分位数，然后计算IQR来确定您的上下边界。</p><p id="9af6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，连续特征，如“事故日期”和“每周工资_sqrt”通常可以从离散化或宁滨中受益。离散化需要将特征值分成组或箱。这种方法也是处理异常值的有效方法，因为它们通常更接近分布的平均值。这将最终将特征从连续变为离散，因为最终结果将是每个箱中的观察数量(即0–1000、1000–2000、2000–5000等。).</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="74b6" class="ne md it na b gy nf ng l nh ni">def find_boundaries(df, variable):</span><span id="6d31" class="ne md it na b gy nj ng l nh ni"># calculate the boundaries anything outside the upper and lower boundaries is an outlier<br/>    upper_boundary = df[variable].mean() + 3 * df[variable].std()<br/>    lower_boundary = df[variable].mean() - 3 * df[variable].std()</span><span id="25e2" class="ne md it na b gy nj ng l nh ni">return upper_boundary, lower_boundary</span><span id="93e2" class="ne md it na b gy nj ng l nh ni">upper_boundary, lower_boundary = find_boundaries(df, 'Weekly Wage_sqrt')</span><span id="a212" class="ne md it na b gy nj ng l nh ni">upper_boundary, lower_boundary</span></pre><p id="0f03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上限为51.146，下限为-0.763。换句话说，任何超出这些边界的值都将被视为异常值。请注意，我们使用三(3)标准偏差规则来确定异常值。我们可以将该值更改为2，我们的边界会缩小，从而导致更多的异常值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a3ae" class="ne md it na b gy nf ng l nh ni">print('We have {} upper boundary outliers:'.format(len(df[df['Weekly Wage_sqrt'] &gt; upper_boundary])))</span><span id="605c" class="ne md it na b gy nj ng l nh ni">print('We have {} lower boundary outliers:'.format(len(df[df['Weekly Wage_sqrt'] &lt; lower_boundary])))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/253ffe9766eb22cf07c5753f58805bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*1Od4OogN5XxZ7-Ck4mCcMg.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e05f" class="ne md it na b gy nf ng l nh ni"># Identify the outliers in 'weekly wage_sqrt'<br/>outliers_weekly_wage = np.where(df['Weekly Wage_sqrt'] &gt; upper_boundary, True, np.where(df['Weekly Wage_sqrt'] &lt; lower_boundary, True, False))</span><span id="3e53" class="ne md it na b gy nj ng l nh ni"># trim the df of the outliers<br/>df = df.loc[~(outliers_weekly_wage)]</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="852b" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">不平衡的目标分布</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c2e0" class="ne md it na b gy nf ng l nh ni">df['Opiods Used'].value_counts()/len(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6ae75cdb970a43933fe75b0c38b40ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*P_UT4-vbsF06yxYVz6_UNg.png"/></div></figure><p id="5b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是“使用过的Opiods ”,和大多数分类问题一样，就纯粹的数量而言，错误类往往占大多数。正如你在上面看到的，几乎90%的案例都是假的或者没有滥用阿片类药物。一些最大似然算法如决策树倾向于使它们的预测偏向多数类(即在我们的情况下为假)。有许多技术可以用来解决这个问题。过采样是一种尝试将少数类的随机副本添加到数据集直到不平衡消除的技术。欠采样与过采样相反，因为它需要移除多数类观测值。主要的缺点是删除数据的想法会导致模型的欠拟合。最后但同样重要的是，合成少数过采样技术(SMOTE)使用KNN算法生成新的观测值来消除不平衡。我们将在这个用例中使用SMOTE技术来生成新的(合成的)观察结果。</p><blockquote class="oq or os"><p id="5b6f" class="kz la oe lb b lc ld ju le lf lg jx lh ot lj lk ll ou ln lo lp ov lr ls lt lu im bi translated">需要注意的是，所使用的任何技术都应该仅用于生成或消除训练集中的观察值。</p></blockquote></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ca10" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">预处理流水线</h1><p id="62e7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们需要将我们的数据分成训练和测试数据集，但首先让我们将我们的特征转换成适当的格式，以便符合我们管道的要求。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="15e5" class="ne md it na b gy nf ng l nh ni">df[discrete] = df[discrete].astype('O')<br/>df['Industry ID'] = df['Industry ID'].astype('O')<br/>df['Surgery Flag'] = df['Surgery Flag'].astype('O')<br/>df['Accident Source Code'] = df['Accident Source Code'].astype('O')<br/>df['Orthopedic Surgery Payment Flag'] = df['Orthopedic Surgery Payment Flag'].astype('O')<br/>df['Orthopedic Surgery Payment Flag'] *= 1<br/>df['Opiods Used'] *= 1</span><span id="b837" class="ne md it na b gy nj ng l nh ni">X_train, X_test, y_train, y_test = train_test_split(<br/>    df.drop('Opiods Used', axis=1),<br/>    df['Opiods Used'],<br/>    test_size=0.3,<br/>    random_state=SEED)</span></pre><p id="220f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简化缺失数据、稀有值、基数和编码的数据处理任务，我们将利用Scikit-Learn的make_pipeline库。管道允许我们将多个进程应用到一段代码中，这段代码将依次运行每个进程。使用管道使我们的代码更容易理解，可重复性更好。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="205e" class="ne md it na b gy nf ng l nh ni"><strong class="na iu"># Imputing missing values for continuous features with more than 5% missing data with -99999.</strong><br/>feature_transform = make_pipeline(mdi.ArbitraryNumberImputer(arbitrary_number = -99999, variables='Accident DateID_rec'),<br/>                                  <br/><strong class="na iu"># Imputing categorical (object) features w/ more than 5% of nulls as 'missing'<br/></strong>mdi.CategoricalVariableImputer(variables=['Disability Status',<br/>'NCCI BINatureOfLossDescription','Accident Source Code','Accident Type Group num'], imputation_method='missing'),<br/>                                  <br/><strong class="na iu"># Imputing categorical features w/less than 5% of missing values with the mode<br/></strong>mdi.CategoricalVariableImputer(variables=['Claimant Sex', 'Claimant Marital Status','Employment Status Flag','RTW Restriction Flag'],<br/>imputation_method='frequent'),<br/>                                  <br/><strong class="na iu"># Encoding rare categories for categorical and discrete features (Less than 1% is rare)<br/></strong>ce.RareLabelCategoricalEncoder(tol=0.01, n_categories=6, <br/>variables=['Benefits State', 'Industry ID', 'NCCI Job Code','Employment Status Flag', 'SIC Group', 'NCCI BINatureOfLossDescription','Accident Type Group num', 'Claimant Age', 'Number Dependents', 'Percent Impairment', 'HCPCS A Codes', 'HCPCS E Codes', 'HCPCS G Codes', 'HCPCS J Codes', 'HCPCS L Codes','HCPCS W Codes', 'ICD Group 6', 'ICD Group 13','ICD Group 18', 'ICD Group 19', 'ICD Group 21','CPT Category - Anesthesia', 'CPT Category - Eval_Mgmt','CPT Category - Medicine', 'CPT Category - Path_Lab','CPT Category - Radiology', 'CPT Category - Surgery', <br/>'NDC Class - Muscle Relaxants']),<br/>                                  <br/><strong class="na iu"># We will use one_hot_encoding for categorical features</strong><br/>ce.OneHotCategoricalEncoder(variables=['Benefits State', 'Industry ID', 'Claimant Sex','Claimant Marital Status','Employment Status Flag','RTW Restriction Flag','Disability Status','SIC Group','NCCI Job Code','NCCI BINatureOfLossDescription','Accident Source Code','Accident Type Group num'],drop_last=True),<br/>                                  <br/><strong class="na iu"># We are going to use ordinal encoding according to the target mean to the target feature<br/></strong>ce.OrdinalCategoricalEncoder(encoding_method='ordered',variables=['Claimant Age','Number Dependents','Percent Impairment','HCPCS A Codes', 'HCPCS E Codes','HCPCS G Codes','HCPCS J Codes', 'HCPCS L Codes', 'HCPCS W Codes','ICD Group 6', 'ICD Group 13', 'ICD Group 18', 'ICD Group 19','ICD Group 21', 'CPT Category - Anesthesia', <br/>'CPT Category - Eval_Mgmt','CPT Category - Medicine','CPT Category - Path_Lab','CPT Category - Radiology','CPT Category - Surgery','NDC Class - Muscle Relaxants']))</span></pre><p id="6ee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据处理管道广泛使用了“特征引擎”库。我们可以使用Scikit-Learn来完成这些任务，但是我们想指出的是，feature-engine具有某些优势。首先，基于scikit-learn、pandas、numpy和SciPy构建的feature-engine能够返回pandas数据帧，而不是像scikit-learn那样返回Numpy数组。其次，特征引擎转换器能够学习和存储训练参数，并使用存储的参数转换您的测试数据。</p><p id="8224" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于特征引擎已经说得够多了，让我们更详细地讨论一下管道。理解流水线中的步骤是连续运行的，从顶部的变压器开始，这一点很重要。学生通常会犯这样的错误，即应用第一步最终会改变数据的结构或第二步无法识别的特征的名称。</p><p id="d25d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个转换器“ArbitraryNumberImputer”使用值-99999对丢失数据超过5%的连续要素进行估算。第二个转换器“CategoricalVariableImputer”使用字符串值“missing”对缺失数据超过5%的分类数据进行估算。第三个转换器“FrequentCategoryImputer”用特征的模式估算缺失数据少于5%的分类数据。第四个转换器“RareLabelCategoricalEncoder”将出现时间不到1%的分类和离散特征观察值编码到一个名为“稀有”的新类别中。</p><p id="7e96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第五个转换器“OneHotCategoricalEncoder”将每个分类要素的每个唯一值转换为存储在新要素中的二进制形式。比如“性别”有“M”、“F”、“U”的值。一个热编码将产生三个(或两个“k-1”取决于您的设置)新功能(即性别_M，性别_F，性别_U)。如果在原始“性别”特征下观察值为“M ”,则“性别_M”的值为1,“性别_F”和“性别_U”的值为0。由于这种方法极大地扩展了特征空间，现在您理解了为什么将稀有观察值(&lt; 1%)归类为“稀有”非常重要。</p><p id="e085" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终的转换器“OridinalCategoricalEncoder”专门用于对离散特征进行编码，以保持它们与目标特征的有序关系。利用该编码器将会为测试集中存在的、没有在训练集中编码的类别生成缺失值或抛出错误。这也是在编码序数/离散特征之前处理稀有值的另一个原因。为了更好地理解这个顺序编码器，让我们检查一下“申请人年龄”特性。我们有三个潜在值“F”，“M”和“U”。让我们假设“F”的阿片类药物滥用率平均为10%，“M”为25%，“U”为5%。编码器将根据他们平均阿片类药物滥用的程度将“M”编码为1，“F”编码为2，“U”编码为3。</p><p id="6c7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oe">注意扩展到155个特征的特征空间</em></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0d90" class="ne md it na b gy nf ng l nh ni">feat_transform.fit(X_train, y_train)<br/>X_train_clean = feat_transform.transform(X_train)<br/>X_test_clean = feat_transform.transform(X_test)</span><span id="97dd" class="ne md it na b gy nj ng l nh ni">X_train_clean.head()</span></pre><p id="2407" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，将管道安装到X_train和y_train上，并转换X_train和X_test。请注意，我们的特征空间已经大大增加到155个特征，这是由于我们对分类特征使用了一次性编码器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/beb740df9606d0e7582f4530f7984f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*0LNjZTUKyYRQdNzFM8eIIw.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8cbe" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">特征缩放</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/00d42ec8bcd4dcb68472b3d11d19ba90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUxomSobjzCuW0o6zK1Www.png"/></div></div></figure><p id="8b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们必须缩放特征，以使它们的所有值都在相同的范围或量级上。这个步骤必须完成，因为一些ML分类器使用欧几里德距离，并且具有更高幅度或范围的特征将对预测有更大的影响。例如温度，32华氏度等于273.15开尔文，如果我们在模型中使用这两个特征，开尔文将具有更大的权重或影响预测。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="91c7" class="ne md it na b gy nf ng l nh ni">scaler = StandardScaler()<br/>scaler.fit(X_train_clean)</span><span id="c5a5" class="ne md it na b gy nj ng l nh ni">X_train_std = scaler.transform(X_train_clean)<br/>X_test_std = scaler.transform(X_test_clean)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6c3ea4d34e2b7e265c85d23cc849f96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*4_LqUWNjU0vTly_EMH5HPA.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2f92" class="ne md it na b gy nf ng l nh ni">X_train_std_df = pd.DataFrame(X_train_std, columns=col_names)<br/>X_test_std_df = pd.DataFrame(X_test_std, columns=col_names)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/3090e000c61e1f4de46b3b5c17aae8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEjdE_7sk_C9LnKov4ithw.png"/></div></div></figure><h1 id="9fc2" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">基线模型</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7ce3" class="ne md it na b gy nf ng l nh ni">models = []<br/>models.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))<br/>models.append(('rf_classifer', RandomForestClassifier(random_state=42)))<br/>models.append(('bayes', GaussianNB()))<br/>models.append(('gbc', GradientBoostingClassifier()))</span><span id="426b" class="ne md it na b gy nj ng l nh ni">base_model_train = []<br/>base_model_test = []</span><span id="ef9f" class="ne md it na b gy nj ng l nh ni">for name, classifier in models:<br/>    scores = cross_val_score(classifier, X_train_std_df, y_train, cv=5, scoring='recall')<br/>    base_model_train.append(scores.mean().round(4))<br/>    print(scores)<br/>    print('{}: Avg CV recall using all features on training data: {}'.format(name, scores.mean().round(4)))<br/>    <br/>    classifier.fit(X_train_std_df, y_train)<br/>    y_preds = classifier.predict(X_test_std_df)<br/>    test_recall = recall_score(y_test, y_preds, average='binary')<br/>    test_class = classification_report(y_test, y_preds)<br/>    cnf_matrix = confusion_matrix(y_test, y_preds)<br/>    base_model_test.append(test_recall.round(4))<br/>    print('{}: Recall w/all features on test data {}:'.format(name, test_recall.round(4)))<br/>    print(test_class)<br/>    print(cnf_matrix)<br/>    print('-------------------------------------------------------')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f9f03dca95bc550833ed05709de2f62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*xy18tK4jT5VTu8Jk3nVNBw.png"/></div></figure><p id="ee35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将比较四个不同分类器的相对召回分数。我们使用召回是因为我们想尽量减少假阴性(即滥用阿片类药物但预测不会滥用)。首先，我们希望建立一个基线，可以与分类器的额外迭代进行比较，以确定相对的改进。基线模型包括具有不平衡目标的整个特征空间。为了正确评估我们的分类器，我们将使用仅应用于训练数据集的5重分层交叉验证，从而大大减少数据泄漏。换句话说，每个分类器将在训练数据的五个唯一分割上被训练和测试5次。将为每个分类器计算五个独特的召回分数，并一起平均以产生最终召回分数。分类器在训练期间不会“看到”任何测试数据。最后，每个分类器将在保留的测试数据集上进行测试，以确定可推广性和过度拟合。</p><p id="1c49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归、随机森林和梯度推进分类器已经实现了很高的整体准确性。然而，朴素贝叶斯设法实现了最高的召回率，因为它只有331个假阴性预测。</p><blockquote class="pg"><p id="3eeb" class="ph pi it bd pj pk pl pm pn po pp lu dk translated"><strong class="ak"> <em class="pq">如果您希望阅读有关分类指标的更多信息(</em> </strong> <a class="ae ky" href="https://medium.com/@kamilmysiak/classification-metrics-thresholds-explained-caff18ad2747" rel="noopener"> <strong class="ak"> <em class="pq">链接</em></strong></a><strong class="ak"><em class="pq">)</em></strong></p></blockquote><h1 id="2a54" class="mc md it bd me mf pa mh mi mj pb ml mm jz pr ka mo kc ps kd mq kf pt kg ms mt bi translated">模型1:完整的功能集和平衡的SMOTE</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="19ae" class="ne md it na b gy nf ng l nh ni">sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>X_train_std_sm, y_train_sm = sm.fit_resample(X_train_std_df, y_train)</span><span id="f3ff" class="ne md it na b gy nj ng l nh ni">model1_train = []<br/>model1_test = []</span><span id="da6c" class="ne md it na b gy nj ng l nh ni">for name, classifier in models:<br/>    pipeline = make_pipeline(sm, classifier)<br/>    scores = cross_val_score(pipeline, X_train_std_df, y_train, cv=5, scoring='recall')<br/>    model1_train.append(scores.mean().round(4))<br/>    print(scores)<br/>    print('{}: Avg CV Recall w/All Reatures: {}'.format(name, scores.mean().round(4)))<br/>    <br/>    classifier.fit(X_train_std_sm, y_train_sm)<br/>    y_preds = classifier.predict(X_test_std_df)<br/>    test_recall = recall_score(y_test, y_preds)<br/>    test_class = classification_report(y_test, y_preds)<br/>    cnf_matrix = confusion_matrix(y_test, y_preds)<br/>    model1_test.append(test_recall.round(4))<br/>    print('{}: Recall w/All Features on test data {}:'.format(name, test_recall.round(4)))<br/>    print(test_class)<br/>    print(cnf_matrix)<br/>    print('-------------------------------------------------------')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/b1ae849d0ac4e16975758b32a4354c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*qdXD3OmPfwmh_LpZb3S0OA.png"/></div></figure><p id="9091" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不平衡的目标给我们的预测强加了偏见。正如预期的那样，逻辑回归得到了很大的改进，因为该算法在平衡目标的情况下表现得更好。</p><p id="5e53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谈SMOTE及其方法论。如果你还记得我们最初对数据集的检查，目标变量是不平衡的。与导致阿片类药物滥用的观察结果(10%)相比，我们有更多未导致阿片类药物滥用的观察结果(89%)。我们还决定使用SMOTE方法，因为它创建了少数类的新的合成观测值，而不是复制现有的观测值。SMOTE使用KNN(通常k=5 ),其中从少数类中选择一个随机观察值，并找到最近邻居的<em class="oe"> k </em>。然后，随机选择k个邻居中的一个，并且从原始观察和随机选择的邻居之间的随机选择的点构建合成样本。</p><h1 id="9368" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">特征选择</h1><h2 id="b656" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">随机森林特征重要性</h2><p id="2b5e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">自然语言处理和物联网等领域的现代数据集通常是高度多维的。看到数千甚至数百万个特征并不罕见。知道如何将特征缩小到选定的几个，不仅增加了我们找到可概括模型的机会，还减少了我们对昂贵的计算能力的依赖。通过精确减少数据中的特征/维度数量，我们最终从数据中去除了不必要的噪声。</p><p id="ccf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，特征选择不仅包括特征空间的缩减，还包括特征创建。了解如何发现数据集中的趋势和要素之间的关系(即多项式特征)需要多年的实践，但在预测能力方面有很大的好处。有整个大学的课程致力于特性选择/工程，但那些对这个主题更感兴趣的人请研究过滤器、包装器和嵌入式方法作为介绍。</p><p id="878b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-Learn的RandomForestClassifier具有“feature_importances_”属性，该属性用于确定数据集中每个要素的相对重要性。由于随机森林在平衡目标下往往表现更好，我们将使用SMOTE平衡X_train_std_sm和y_train_sm数据集。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="34a9" class="ne md it na b gy nf ng l nh ni">rf_selector = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)</span><span id="3b6a" class="ne md it na b gy nj ng l nh ni">rf_selector.fit(X_train_std_sm, y_train_sm)</span><span id="540d" class="ne md it na b gy nj ng l nh ni">feature_imp = pd.Series(rf_selector.feature_importances_, index=X_train_std_df.columns).sort_values(ascending=False)</span><span id="651f" class="ne md it na b gy nj ng l nh ni">feature_imp[:30]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/e29fb226d6e26f6b1f670b96b24910ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*QuHVXUJNIRe8jv4GTFtpjQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">30大特性</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e18c" class="ne md it na b gy nf ng l nh ni">sum(feature_imp[0:30]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/eab252775248209f174537bea38ac2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*lOtUgWD9qlrHEJ9vkheS6Q.png"/></div></figure><p id="b3b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能够将大多数原始特性减少到30个，这占了性能差异的91 <strong class="lb iu"> % </strong>。任何额外的特征只会增加非常小的额外预测能力。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a539" class="ne md it na b gy nf ng l nh ni">X_train_rf = X_train_std_df[feature_imp[:30].index]<br/>X_test_rf = X_test_std_df[feature_imp[:30].index]</span><span id="069f" class="ne md it na b gy nj ng l nh ni">plt.figure(figsize=(12,8))<br/>sns.barplot(x=feature_imp[0:30], y=feature_imp.index[0:30])<br/>plt.xlabel('Feature Importance Score')<br/>plt.ylabel('Features')<br/>plt.title("Visualizing Important Features")<br/>plt.grid(b=False, which='major', color='#666666', linestyle='-', alpha=0.2)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/73523dfe9505f76d195b494749fbfc58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EpIohmMerDA4HBy2ENtjQ.png"/></div></div></figure><h1 id="6cb5" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">模型2:射频特征和不平衡目标</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2bec" class="ne md it na b gy nf ng l nh ni">models = []<br/>models.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))<br/>models.append(('rf_classifer', RandomForestClassifier(random_state=42)))<br/>models.append(('bayes', GaussianNB()))<br/>models.append(('gbc', GradientBoostingClassifier()))</span><span id="b6a2" class="ne md it na b gy nj ng l nh ni">model2_train = []<br/>model2_test = []</span><span id="4ff8" class="ne md it na b gy nj ng l nh ni">for name, classifier in models:<br/>    scores = cross_val_score(classifier, X_train_rf, y_train, cv=5, scoring='recall')<br/>    model2_train.append(scores.mean().round(3))<br/>    print(scores)<br/>    print('{}: Avg CV Recall on RF Features: {}'.format(name, scores.mean().round(3)))<br/>    <br/>    classifier.fit(X_train_rf, y_train)<br/>    y_preds = classifier.predict(X_test_rf)<br/>    test_recall = recall_score(y_test, y_preds, average='binary')<br/>    test_class = classification_report(y_test, y_preds)<br/>    cnf_matrix = confusion_matrix(y_test, y_preds)<br/>    model2_test.append(test_recall.round(3))<br/>    print('{}: Recall w/RF features on test data {}:'.format(name, test_recall.round(3)))<br/>    print(test_class)<br/>    print(cnf_matrix)<br/>    print('-------------------------------------------------------')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/73c4fbc61b0800705fe4e5056fb6fee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*ROis8MzeDocfB-idfzkazQ.png"/></div></figure><h1 id="768c" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">模型3: RF特性和平衡w/SMOTE</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0d14" class="ne md it na b gy nf ng l nh ni">X_train_rf_sm, y_train_sm = sm.fit_resample(X_train_rf, y_train)</span><span id="daac" class="ne md it na b gy nj ng l nh ni">models = []<br/>models.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))<br/>models.append(('rf_classifer', RandomForestClassifier(random_state=42)))<br/>models.append(('bayes', GaussianNB()))<br/>models.append(('gbc', GradientBoostingClassifier()))</span><span id="e59c" class="ne md it na b gy nj ng l nh ni">sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>skf = StratifiedKFold(n_splits=5)</span><span id="b24f" class="ne md it na b gy nj ng l nh ni">model3_train = []<br/>model3_test = []</span><span id="1f3c" class="ne md it na b gy nj ng l nh ni">for name, classifier in models:<br/>    pipeline = make_pipeline(sm, classifier)<br/>    scores = cross_val_score(pipeline, X_train_rf, y_train, cv=skf, scoring='recall')<br/>    model3_train.append(scores.mean().round(3))<br/>    print(scores)<br/>    print('{}: Avg CV Recall w/RF Reatures: {}'.format(name, scores.mean().round(3)))<br/>    <br/>    classifier.fit(X_train_rf_sm, y_train_sm)<br/>    y_preds = classifier.predict(X_test_rf)<br/>    test_recall = recall_score(y_test, y_preds, average='binary')<br/>    test_class = classification_report(y_test, y_preds)<br/>    cnf_matrix = confusion_matrix(y_test, y_preds)<br/>    model3_test.append(test_recall.round(3))<br/>    print('{}: Recall w/RF on test data {}:'.format(name, test_recall.round(3)))<br/>    print(test_class)<br/>    print(cnf_matrix)<br/>    print('-------------------------------------------------------')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1b2786e448bd8a6aee510d852f80f3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*J_0KFisAZgL9Q1P5WLvDKw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/becfe99e7f34d2543ddebf5f7fe3c26f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*GOdrtLZg0t7NGf4l0eWFsw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/cd9b81e3432af725fc39cfe4200d79ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*PySVSsC78XBKvsEBuIdCPA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/f50b84e9b330d08525c7666b168516cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*6pk5vNRU1GN8AaqWoH2pqg.png"/></div></figure><h1 id="2a00" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">摘要</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="316e" class="ne md it na b gy nf ng l nh ni">classifiers = ['Log_Regression', 'Random_Forest', 'Naive_Bayes', 'Gradient_Boosting_clf']</span><span id="184e" class="ne md it na b gy nj ng l nh ni">idx = ['All_Feat_Imbalance_Train', 'All_Feat_Imbalance_Test','All_Feat_Smote_Train',<br/>'All_Feat_Smote_Test','RF_Imbalance_Train', <br/>'RF_Imbalance_Test','RF_Smote_Train','RF_Smote_Test']</span><span id="9a72" class="ne md it na b gy nj ng l nh ni">combined_results = pd.DataFrame([base_model_train,base_model_test,<br/>model1_train, model1_test, model2_train,<br/>model2_test, model3_train, model3_test],<br/>columns=classifiers, index=idx)</span><span id="1bb1" class="ne md it na b gy nj ng l nh ni">test_results = pd.DataFrame([base_model_test, model1_test, model2_test, model3_test], columns=classifiers, index=idx[1:8:2])</span><span id="c8ad" class="ne md it na b gy nj ng l nh ni">print(test_results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/453290d109f027827278f2dbb80e98ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*KT_0PRYz_RSkiEh0kpGvyA.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0b49" class="ne md it na b gy nf ng l nh ni">plt.figure(figsize=(15,10))<br/>sns.lineplot(data=test_results[['Log_Regression', 'Random_Forest',<br/>                             'Naive_Bayes', 'Gradient_Boosting_clf']])<br/>plt.xlabel('Iterations', fontsize=17, labelpad=15)             <br/>plt.ylabel('Recall Scores', fontsize=17, labelpad=15)<br/>plt.title('Classifier Recall Scores on Test Data',fontsize=25, pad=15)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qc"><img src="../Images/a1e0cea7e39ab941e18fe0969121e73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWQjSFlwaFEz-MmNyVP0vA.png"/></div></div></figure><p id="bcff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征的减少导致召回性能的轻微下降。这是意料之中的，因为仅使用30个特性就造成了91%的性能差异。增加功能的数量肯定会提高召回率。只有朴素贝叶斯没有受到特征减少的影响。其次，我们数据集中的不平衡也影响了分类器的召回性能。一般来说，一旦不平衡得到纠正，大多数分类器都会提高它们的性能。逻辑回归受不平衡的影响最大。</p><p id="2fa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就整体分类器性能而言，必须说所有分类器在特征减少以及目标平衡的情况下表现最佳。当然，有人可能会认为朴素贝叶斯表现最好，因为它设法实现了最好的测试召回率(0.949)，但我会认为是逻辑回归胜过了领域。它实现了非常相似的召回率0.945，相比之下，朴素贝叶斯的召回率为0.949，而朴素贝叶斯仅解释了14%的假阴性增加。更令人印象深刻的是，与朴素贝叶斯相比，它多了2276个正确的真阴性预测。在下一节中，我们将尝试超参数调优，看看我们能否提高逻辑回归模型的分类召回率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1b2786e448bd8a6aee510d852f80f3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*J_0KFisAZgL9Q1P5WLvDKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有RF特征和SMOTE的逻辑回归</p></figure><h1 id="2a4b" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">超参数调谐</h1><h2 id="5d34" class="ne md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">什么是超参数？</h2><p id="bf5d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">认为超参数是“调谐”旋钮。想象你正在编辑一张图片，以达到某种效果。您可以使用曝光、高光、阴影、对比度、亮度、饱和度、温暖度、色调等“旋钮”来调节画面。好吧，超参数是相同类型的“旋钮”,但用于分类器。</p><p id="4c89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了求解器、惩罚和c之外，逻辑回归没有太多的超参数需要调整。</p><p id="df38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解算器</strong>试图优化参数权重或θ，这将引出最小误差或成本函数。Sklearn自带了几个解算器:newton-cg，lbfgs，liblinear。</p><p id="ca23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">惩罚</strong>参数也被称为“正则化”，其目的是帮助模型避免过度拟合训练数据，从而产生更通用的模型。它通过“惩罚”被认为是噪声或对模型贡献很小的特征来做到这一点。</p><ul class=""><li id="7bf3" class="qd qe it lb b lc ld lf lg li qf lm qg lq qh lu qi qj qk ql bi translated"><strong class="lb iu"> L1惩罚或套索:</strong>将惩罚的绝对幅度或特征的权重之和添加到成本函数中。L1将噪声特征减少到零，从而简化了模型。</li><li id="a647" class="qd qe it lb b lc qm lf qn li qo lm qp lq qq lu qi qj qk ql bi translated"><strong class="lb iu"> L2惩罚或岭:</strong>将惩罚的平方幅度或特征权重的平方和添加到成本函数中。L2也降低了噪声特征的影响，但是它们的权重或θ非常接近于零，但并不完全是零。</li></ul><p id="872a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<strong class="lb iu">“C”参数</strong>决定正则化惩罚的强度。C参数越大，正则化程度越低，模型变得越复杂，过拟合增加。C参数越小，应用的正则化越多，欠拟合增加。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ab33" class="ne md it na b gy nf ng l nh ni">clf_lr = LogisticRegression(max_iter=10000, random_state=SEED)</span><span id="5220" class="ne md it na b gy nj ng l nh ni">penalty = ['l1','l2']<br/>C = [0.001,0.002,0.003,0.005,1,10,100,1000]</span><span id="0160" class="ne md it na b gy nj ng l nh ni">skf = StratifiedKFold(n_splits=5)<br/>pipeline = make_pipeline(sm, clf_lr)<br/>param_grid = dict(logisticregression__penalty=penalty,<br/>                  logisticregression__C=C)</span><span id="2e4c" class="ne md it na b gy nj ng l nh ni">grid = GridSearchCV(pipeline, <br/>                    param_grid=param_grid,<br/>                    scoring='recall', <br/>                    verbose=1, cv=skf)</span><span id="d3d4" class="ne md it na b gy nj ng l nh ni">grid_results = grid.fit(X_train_rf, y_train) <br/>print('Best Score: ', grid_results.best_score_)<br/>print('Best Params: ', grid_results.best_params_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qr"><img src="../Images/3d07a07adad38864c4d8e923e4e9042e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qn8CFlC6-lJ_bk5KG3N8Wg.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9324" class="ne md it na b gy nf ng l nh ni">X_train_rf_sm, y_train_sm = sm.fit_resample(X_train_rf, y_train)<br/>clf_lr = LogisticRegression(max_iter=10000, penalty='l2', C=0.0001, random_state=42)</span><span id="9d26" class="ne md it na b gy nj ng l nh ni">clf_lr.fit(X_train_rf_sm, y_train_sm)<br/>y_preds = clf_lr.predict(X_test_rf)<br/>test_recall = recall_score(y_test, y_preds).round(3)<br/>test_class = classification_report(y_test, y_preds)<br/>cm = confusion_matrix(y_test, y_preds)</span><span id="7f5d" class="ne md it na b gy nj ng l nh ni">print('Log Regression Recall w/RF on test data {}:'.format(test_recall.round(3)))<br/>print(test_class)<br/>print(cm)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/e4f63de354c35cf18236d5795374f9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*pRLNhHcphDVV8rofq89_0A.png"/></div></figure><p id="c460" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了减少训练时间和警告数量，我们将重点调整“惩罚”和“C”参数。不仅要了解每个超参数的作用，还要了解参数之间如何相互作用，这对它们的调整至关重要。此外，调整过程通常是迭代的，因为我们从每个参数的宽范围开始，然后开始缩小范围，直到选择特定的参数值。</p><p id="fb25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GridSearchCV应用了一种穷举方法，因为它考虑了所提供参数的所有组合。由于平衡目标在分类器评估期间产生了最佳的召回，我们选择将其与传递到gridsearchcv的对数回归分类器一起包含到我们的管道中。这样，每个交叉验证训练/测试分割仅与其数据平衡。换句话说，在交叉验证期间，SMOTE和分类器都没有遭受数据泄漏。</p><p id="b669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，我们没有增加我们的训练召回率，但是我们能够将我们的测试召回率从0.945(对数回归w/RF特征和SMOTE)增加到0.954。此外，我们将假阴性计数减少了42。我们的假阳性计数增加了269，但是再一次，当你实际上没有使用阿片类药物时，预测使用阿片类药物更好。</p><h1 id="d79f" class="mc md it bd me mf pa mh mi mj pb ml mm jz pc ka mo kc pd kd mq kf pe kg ms mt bi translated">结论</h1><p id="1997" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">作为一名数据科学家，理解数据科学分类问题的全部本质是你走向成熟的关键。我希望你发现这个教程信息丰富，容易理解。我欢迎任何反馈和建议，因为我们都只是在磨练我们的手艺。</p><p id="fac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有代码都可以在我的GitHub上找到。<a class="ae ky" href="https://github.com/Kmysiak/Opioid_Classification" rel="noopener ugc nofollow" target="_blank">链接</a></p></div></div>    
</body>
</html>