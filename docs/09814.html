<html>
<head>
<title>Introduction to Text Representations for Language Processing — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言处理的文本表示介绍—第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-1-dc6e8068b8a4?source=collection_archive---------13-----------------------#2020-07-12">https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-1-dc6e8068b8a4?source=collection_archive---------13-----------------------#2020-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d4572fbe3ad47896132e8e9b3716c3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JiXRl1KxneTR31Il"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jd" href="https://unsplash.com/@jaredd_craig?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jaredd Craig </a>拍照</p></figure><div class=""/><div class=""><h2 id="4662" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">计算机是如何理解和解释语言的？</h2></div><p id="15a7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">计算机在处理数字时很聪明。它们在计算和解码模式方面比人类快很多个数量级。但是如果数据不是数字呢？如果是语言呢？当数据是字符、单词和句子时会发生什么？我们如何让计算机处理我们的语言？Alexa、Google Home &amp;很多其他智能助手是如何理解&amp;回复我们的发言的？如果你正在寻找这些问题的答案，这篇文章将是你走向正确方向的垫脚石。</p><p id="4198" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自然语言处理是人工智能的一个子领域，致力于使机器理解和处理人类语言。大多数自然语言处理(NLP)任务的最基本步骤是将单词转换成数字，以便机器理解和解码语言中的模式。我们称这一步为文本表示。这一步虽然是迭代的，但在决定机器学习模型/算法的特征方面起着重要作用。</p><p id="9063" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">文本表示可以大致分为两部分:</p><ul class=""><li id="72e0" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">离散文本表示</li><li id="1500" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">分布式/连续文本表示</li></ul><p id="a476" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文将关注离散文本表示&amp;我们将深入研究一些基本 Sklearn 实现中常用的表示。</p><h1 id="e57b" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">离散文本表示:</h1><p id="f4c6" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">这些表示中，单词由它们在字典中的位置的对应索引来表示，该索引来自更大的语料库。</p><p id="9e38" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">属于这一类别的著名代表有:</p><ul class=""><li id="0799" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">一键编码</li><li id="7f47" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">词袋表示法(BOW)</li><li id="6a61" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">基本 BOW 计数矢量器</li><li id="1292" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">先进弓-TF-IDF</li></ul><h1 id="b28c" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">一键编码:</h1><p id="6c67" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">这是一种将 0 赋给向量中所有元素的表示形式，只有一个元素的值为 1。该值表示元素的类别。</p><p id="a01e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如:</p><p id="e858" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我有一个句子，“我爱我的狗”，句子中的每个单词将表示如下:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="05cb" class="nl mg jg nh b gy nm nn l no np">I → [1 0 0 0], love → [0 1 0 0], my → [0 0 1 0], dog → [0 0 0 1]</span></pre><p id="61dc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，整个句子表示为:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="af73" class="nl mg jg nh b gy nm nn l no np">sentence = [ [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1] ]</span></pre><blockquote class="nq nr ns"><p id="ae9a" class="kv kw nt kx b ky kz kh la lb lc kk ld nu lf lg lh nv lj lk ll nw ln lo lp lq ij bi translated"><em class="jg">一键编码背后的直觉是每个比特代表一个可能的类别&amp;如果一个特定的变量不能归入多个类别，那么一个比特就足以代表它</em></p></blockquote><p id="58a2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如您可能已经理解的，单词数组的长度取决于词汇量。这对于可能包含多达 100，000 个唯一单词甚至更多的非常大的语料库来说是不可扩展的。</p><p id="b707" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们使用 Sklearn 来实现它:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="e656" class="nl mg jg nh b gy nm nn l no np">from sklearn.preprocessing import OneHotEncoder<br/>import itertools</span><span id="c3bf" class="nl mg jg nh b gy nx nn l no np"># two example documents<br/>docs = ["cat","dog","bat","ate"]</span><span id="310e" class="nl mg jg nh b gy nx nn l no np"># split documents to tokens<br/>tokens_docs = [doc.split(" ") for doc in docs]</span><span id="61d3" class="nl mg jg nh b gy nx nn l no np"># convert list of of token-lists to one flat list of tokens<br/># and then create a dictionary that maps word to id of word,<br/>all_tokens = itertools.chain.from_iterable(tokens_docs)<br/>word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}</span><span id="fdb7" class="nl mg jg nh b gy nx nn l no np"># convert token lists to token-id lists<br/>token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]</span><span id="0fbf" class="nl mg jg nh b gy nx nn l no np"># convert list of token-id lists to one-hot representation<br/>vec = OneHotEncoder(categories="auto")<br/>X = vec.fit_transform(token_ids)</span><span id="78b3" class="nl mg jg nh b gy nx nn l no np">print(X.toarray())</span></pre><h1 id="1564" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">输出:</h1><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="87c7" class="nl mg jg nh b gy nm nn l no np">[[0. 0. 1. 0.]<br/> [0. 1. 0. 0.]<br/> [0. 0. 0. 1.]<br/> [1. 0. 0. 0.]]</span></pre><h1 id="fad1" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">Sklearn 文档:</h1><div class="ip iq gp gr ir ny"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jh gy z fp od fr fs oe fu fw jf bi translated">sk learn . preprocessing . onehotencoder-sci kit-learn 0 . 23 . 1 文档</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">将分类特征编码为一个独热数值数组。这个转换器的输入应该是一个类似数组的…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">scikit-learn.org</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ix ny"/></div></div></a></div><h1 id="f58b" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">一键编码的优势:</h1><ul class=""><li id="5747" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">易于理解和实施</li></ul><h1 id="871c" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">一键编码的缺点:</h1><ul class=""><li id="be2c" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">如果类别数量非常多，特征空间会爆炸</li><li id="ce7b" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">单词的矢量表示是正交的，并且不能确定或测量不同单词之间的关系</li><li id="aa04" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">无法衡量一个单词在句子中的重要性，但可以理解一个单词在句子中是否存在</li><li id="1389" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">高维稀疏矩阵表示可能是存储器和计算昂贵的</li></ul><h1 id="8fef" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">词袋表示法</h1><p id="40da" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">顾名思义，单词包表示法将单词放在一个“包”中，并计算每个单词的出现频率。它不考虑文本表示的词序或词汇信息</p><blockquote class="nq nr ns"><p id="73eb" class="kv kw nt kx b ky kz kh la lb lc kk ld nu lf lg lh nv lj lk ll nw ln lo lp lq ij bi translated"><em class="jg">BOW 表示背后的直觉是，具有相似单词的文档是相似的，而不管单词的位置如何</em></p></blockquote><h1 id="9b34" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">基本 BOW 计数矢量器</h1><p id="cc9d" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">CountVectorizer 计算一个单词在文档中出现的频率。它将多个句子的语料库(比如产品评论)转换成评论和单词的矩阵，并用每个单词在句子中的出现频率填充它</p><p id="7a7f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看如何使用 Sklearn 计数矢量器:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="7d27" class="nl mg jg nh b gy nm nn l no np">from sklearn.feature_extraction.text import CountVectorizer</span><span id="03fd" class="nl mg jg nh b gy nx nn l no np">text = ["i love nlp. nlp is so cool"]</span><span id="36ed" class="nl mg jg nh b gy nx nn l no np">vectorizer = CountVectorizer()</span><span id="6421" class="nl mg jg nh b gy nx nn l no np"># tokenize and build vocab<br/>vectorizer.fit(text)<br/>print(vectorizer.vocabulary_)<br/># Output: {'love': 2, 'nlp': 3, 'is': 1, 'so': 4, 'cool': 0}</span><span id="acf5" class="nl mg jg nh b gy nx nn l no np"># encode document<br/>vector = vectorizer.transform(text)</span><span id="a430" class="nl mg jg nh b gy nx nn l no np"># summarize encoded vector<br/>print(vector.shape) # Output: (1, 5)<br/>print(vector.toarray())</span></pre><h1 id="63b0" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">输出:</h1><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="7310" class="nl mg jg nh b gy nm nn l no np">[[1 1 1 2 1]]</span></pre><p id="4efd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如你所见，单词“nlp”在句子中出现了两次&amp;也在索引 3 中。我们可以将其视为最终打印语句的输出</p><blockquote class="nq nr ns"><p id="524b" class="kv kw nt kx b ky kz kh la lb lc kk ld nu lf lg lh nv lj lk ll nw ln lo lp lq ij bi translated"><em class="jg">一个词在句子中的“权重”是它的出现频率</em></p></blockquote><p id="b2ba" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为 CountVectorizer 的一部分，可以调整各种参数来获得所需的结果，包括小写、strp_accents、预处理器等文本预处理参数</p><p id="eee6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可以在下面的 Sklearn 文档中找到完整的参数列表:</p><div class="ip iq gp gr ir ny"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jh gy z fp od fr fs oe fu fw jf bi translated">sk learn . feature _ extraction . text . count vectorizer-sci kit-learn 0 . 23 . 1 文档</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">class sk learn . feature _ extraction . text . count vectorizer(*，input='content '，encoding='utf-8 '，decode_error='strict'…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">scikit-learn.org</p></div></div><div class="oh l"><div class="oq l oj ok ol oh om ix ny"/></div></div></a></div><h1 id="3caf" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">CountVectorizer 的优势:</h1><ul class=""><li id="2857" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">CountVectorizer 还能给出文本文档/句子中单词的频率，这是一键编码所不能提供的</li><li id="cf63" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">编码向量的长度就是字典的长度</li></ul><h1 id="e06f" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">CountVectorizer 的缺点:</h1><ul class=""><li id="87a3" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">此方法忽略单词的位置信息。从这种表述中不可能领会一个词的意思</li><li id="aff0" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">当遇到像“is，The，an，I”这样的停用词时&amp;当语料库是特定于上下文的时，高频词更重要或提供关于句子的更多信息的直觉失效了。例如，在一个关于新冠肺炎的语料库中，冠状病毒这个词可能不会增加很多价值</li></ul><h1 id="58cc" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">高级弓</h1><p id="d42f" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">为了抑制非常高频率的单词&amp;忽略低频率的单词，需要相应地标准化单词的“权重”</p><p id="27d2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> TF-IDF 表示:</strong>TF-IDF 的完整形式是术语频率-逆文档频率是 2 个因子的乘积</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/3fd6d7d83cd000c097f21a5e33997b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4RH4kNsNuc6uGVTfKaTPQ.png"/></div></div></figure><p id="ce77" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中，TF(w，d)是单词“w”在文档“d”中的频率</p><p id="77da" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">IDF(w)可以进一步细分为:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/c1fa34d12c15a2e70abcbef99e390b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUv2f2MsNI5LzDwNKngsFw.png"/></div></div></figure><p id="689b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中，N 是文档总数，df(w)是包含单词“w”的文档的频率</p><blockquote class="nq nr ns"><p id="91b6" class="kv kw nt kx b ky kz kh la lb lc kk ld nu lf lg lh nv lj lk ll nw ln lo lp lq ij bi translated">TF-IDF 背后的直觉是，分配给每个单词的权重不仅取决于单词频率，还取决于特定单词在整个语料库中的出现频率</p></blockquote><p id="468f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它采用上一节中讨论的 CountVectorizer 并将其乘以 IDF 分数。对于非常高频率的词(如停用词)和非常低频率的词(噪声项)，从该过程得到的词的输出权重很低</p><p id="7c74" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们尝试使用 Sklearn 来实现这一点</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="110c" class="nl mg jg nh b gy nm nn l no np">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="f760" class="nl mg jg nh b gy nx nn l no np">text1 = ['i love nlp', 'nlp is so cool', <br/>'nlp is all about helping machines process language', <br/>'this tutorial is on baisc nlp technique']</span><span id="0e44" class="nl mg jg nh b gy nx nn l no np">tf = TfidfVectorizer()<br/>txt_fitted = tf.fit(text1)<br/>txt_transformed = txt_fitted.transform(text1)<br/>print ("The text: ", text1)</span><span id="7fa2" class="nl mg jg nh b gy nx nn l no np"># Output: The text:  ['i love nlp', 'nlp is so cool', <br/># 'nlp is all about helping machines process language', <br/># 'this tutorial is on basic nlp technique']</span><span id="dbb2" class="nl mg jg nh b gy nx nn l no np">idf = tf.idf_<br/>print(dict(zip(txt_fitted.get_feature_names(), idf)))</span></pre><h1 id="219b" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">输出:</h1><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="d1d0" class="nl mg jg nh b gy nm nn l no np">{'about': 1.916290731874155, 'all': 1.916290731874155, <br/>'basic': 1.916290731874155, 'cool': 1.916290731874155, <br/>'helping': 1.916290731874155, 'is': 1.2231435513142097, <br/>'language': 1.916290731874155, 'love': 1.916290731874155, <br/>'machines': 1.916290731874155, 'nlp': 1.0, 'on': 1.916290731874155, <br/>'process': 1.916290731874155, 'so': 1.916290731874155, <br/>'technique': 1.916290731874155, 'this': 1.916290731874155, <br/>'tutorial': 1.916290731874155}</span></pre><p id="f526" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意单词“nlp”的权重。因为它出现在所有的句子中，所以它被赋予 1.0 的低权重。同样，停用词“is”的权重也相对较低，为 1.22，因为它在给出的 4 个句子中有 3 个出现。</p><p id="00a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与 CountVectorizer 类似，可以调整各种参数来获得所需的结果。一些重要的参数(除了像小写、strip_accent、stop_words 等文本预处理参数。)是 max_df，min_df，norm，ngram_range &amp; sublinear_tf。这些参数对输出权重的影响超出了本文的范围，将单独讨论。</p><p id="bdf4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以在下面找到 TF-IDF 矢量器的完整文档:</p><div class="ip iq gp gr ir ny"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jh gy z fp od fr fs oe fu fw jf bi translated">sk learn . feature _ extraction . text . tfidf vectorizer-sci kit-learn 0 . 23 . 1 文档</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">class sk learn . feature _ extraction . text . tfidf vectorizer(*，input='content '，encoding='utf-8 '，decode_error='strict'…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">scikit-learn.org</p></div></div><div class="oh l"><div class="ot l oj ok ol oh om ix ny"/></div></div></a></div><h1 id="5fb1" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">TF-IDF 代表的优势:</h1><ul class=""><li id="9ba3" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">简单、易于理解和解释的实施</li><li id="fbf6" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">构建 CountVectorizer 来惩罚语料库中的高频词和低频词。在某种程度上，IDF 降低了我们矩阵中的噪声。</li></ul><h1 id="0d76" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">TF-IDF 代表的缺点:</h1><ul class=""><li id="d89f" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">这个单词的位置信息仍然没有在这个表示中被捕获</li><li id="1c05" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">TF-IDF 高度依赖语料库。由板球数据生成的矩阵表示不能用于足球或排球。因此，需要有高质量的训练数据</li></ul><h1 id="b90b" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">让我们总结一下</h1><p id="40f4" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">离散表示是每个单词都被认为是唯一的&amp;根据我们上面讨论的各种技术转换成数字。我们已经看到了各种离散表示的一些重叠的优点和缺点，让我们将其作为一个整体来总结</p><h1 id="7252" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">离散表示的优点:</h1><ul class=""><li id="6019" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">易于理解、实施和解释的简单表示</li><li id="dd36" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">像 TF-IDF 这样的算法可以用来过滤掉不常见和不相关的单词，从而帮助模型更快地训练和收敛</li></ul><h1 id="813c" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">离散表示的缺点:</h1><ul class=""><li id="ec0b" class="lr ls jg kx b ky mx lb my le on li oo lm op lq lw lx ly lz bi translated">这种表现与词汇量成正比。词汇量大会导致记忆受限</li><li id="13f4" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">它没有利用单词之间的共现统计。它假设所有的单词都是相互独立的</li><li id="3a43" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">这导致具有很少非零值的高度稀疏的向量</li><li id="7d45" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">它们没有抓住单词的上下文或语义。它不认为幽灵和恐怖是相似的，而是两个独立的术语，它们之间没有共性</li></ul><p id="13d7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">离散表示广泛用于经典的机器学习和深度学习应用，以解决复杂的用例，如文档相似性、情感分类、垃圾邮件分类和主题建模等。</p><p id="634e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下一部分中，我们将讨论文本的分布式或连续文本表示&amp;它如何优于(或劣于)离散表示。</p><p id="447f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望你喜欢这篇文章。回头见！</p><p id="db2d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">编辑:该系列的第二部分:<a class="ae jd" href="https://medium.com/@sundareshchandran/introduction-to-text-representations-for-language-processing-part-2-54fe6907868" rel="noopener">https://medium . com/@ sundareshchandran/introduction-to-text-representations-for-language-processing-Part-2-54fe 6907868</a></p><h1 id="3fc6" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">回购链接</h1><div class="ip iq gp gr ir ny"><a href="https://github.com/SundareshPrasanna/Introduction-to-text-representation-for-nlp/tree/master" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jh gy z fp od fr fs oe fu fw jf bi translated">SundareshPrasanna/自然语言处理的文本表示介绍</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">github.com</p></div></div><div class="oh l"><div class="ou l oj ok ol oh om ix ny"/></div></div></a></div><p id="8ffe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">喜欢我的文章？给我买杯咖啡</p><div class="ip iq gp gr ir ny"><a href="https://www.buymeacoffee.com/sundaresh" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jh gy z fp od fr fs oe fu fw jf bi translated">sundaresh 正在创作与数据科学相关的文章，并且热爱教学</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">嘿👋我刚刚在这里创建了一个页面。你现在可以给我买杯咖啡了！</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">www.buymeacoffee.com</p></div></div></div></a></div></div></div>    
</body>
</html>