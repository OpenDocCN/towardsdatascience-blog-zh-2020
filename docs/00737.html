<html>
<head>
<title>Optimisation Algorithm — Adaptive Moment Estimation(Adam)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化算法—自适应矩估计(Adam)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimisation-algorithm-adaptive-moment-estimation-adam-92144d75e232?source=collection_archive---------13-----------------------#2020-01-21">https://towardsdatascience.com/optimisation-algorithm-adaptive-moment-estimation-adam-92144d75e232?source=collection_archive---------13-----------------------#2020-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d0d0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自动协调机制的实施</h2></div><p id="a2ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你曾经使用过任何一种深度学习的包，你一定使用过亚当作为优化者。我记得有一段时间，我有一个想法，每当你试图优化一些东西，就用亚当，仅仅因为它是最好的。在这篇文章中，让我们打开黑盒子，看看是什么让亚当如此特别和强大。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/355aaf09f1479ec0360f53f2e1d50309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_U4cEB8qUSez-hK1lJ9eg.jpeg"/></div></div></figure></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="ba2d" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="77e7" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">每一种算法都不是凭空出现的，它要么是从另一种算法演化而来，要么是克服了另一种算法的某些不足。这同样适用于亚当。优化算法大致通过以下方式发展:</p><pre class="lf lg lh li gt mu mv mw mx aw my bi"><span id="d4cb" class="mz ly it mv b gy na nb l nc nd">vanilla gradient descent<!-- --> =&gt; <!-- -->sgd + momentum<!-- --> =&gt; <!-- -->adaptive gradient</span></pre><p id="5300" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(解释与实现:<a class="ae ne" rel="noopener" target="_blank" href="/gradient-descent-explanation-implementation-c74005ff7dd1">梯度下降</a>、<a class="ae ne" rel="noopener" target="_blank" href="/stochastic-gradient-descent-momentum-explanation-8548a1cd264e"> SGD+动量</a>、<a class="ae ne" rel="noopener" target="_blank" href="/introduction-and-implementation-of-adagradient-rmsprop-fad64fe4991">自适应梯度</a>)</p><p id="2b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD为梯度下降增加了随机性，momentum加速了收敛，而adaptive gradient，顾名思义，适应不同参数的不同学习速率，Adam在某种程度上集合了其他算法的优点，使之成为一种算法。</p><blockquote class="nf ng nh"><p id="2201" class="ki kj ni kk b kl km ju kn ko kp jx kq nj ks kt ku nk kw kx ky nl la lb lc ld im bi translated">自适应矩估计(Adam)是另一种计算每个参数的自适应学习率的方法。除了存储过去平方梯度<code class="fe nm nn no mv b">s</code>的指数衰减平均值(如Adadelta和rms prop ), Adam还保存过去梯度<code class="fe nm nn no mv b">v</code>的指数衰减平均值，类似于momentum。动量可以被视为一个沿斜坡向下运行的球，而亚当的行为就像一个有摩擦力的重球，因此更喜欢误差面上平坦的极小值。</p></blockquote><p id="99a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看它的更新过程:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6320208a944f4b1f122b9ed138921647.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*_g4vT8GL66VJk52CU4c6SQ.png"/></div></figure><p id="2ce8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe nm nn no mv b">v</code>是第一时刻，它类似于记录过去标准化梯度的动量。<code class="fe nm nn no mv b">s</code>为二阶矩，与自适应梯度下降&amp; RMSprop中引入的相同。这个术语有助于给出不同参数的不同学习率。</p><blockquote class="nf ng nh"><p id="a056" class="ki kj ni kk b kl km ju kn ko kp jx kq nj ks kt ku nk kw kx ky nl la lb lc ld im bi translated">由于<code class="fe nm nn no mv b">v</code>和<code class="fe nm nn no mv b">s</code>被初始化为0的向量，Adam的作者观察到它们偏向于0，尤其是在初始时间步长期间，尤其是当衰减率较小时。他们通过计算偏差修正的一阶和二阶矩估计来抵消这些偏差:</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/075e80ccb48d72a20efc5083f6e623af.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*6qj3yj4ip0B_vN3U0K-tVw.png"/></div></figure><p id="450b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后的更新是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/95777f2342e2bec1bcd717956b30bc2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*rvpOco_1i9shqfcHWFMEnQ.png"/></div></figure><p id="a026" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最终的更新过程实际上与自适应梯度下降中引入的公式相同，只是命名器中的梯度被替换为<code class="fe nm nn no mv b">v</code>，因此现在应该清楚的是，Adam结合了其先例的思想，并演变成了更好的版本。</p><h1 id="1f0c" class="lx ly it bd lz ma ns mc md me nt mg mh jz nu ka mj kc nv kd ml kf nw kg mn mo bi translated">履行</h1><p id="719a" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">为了与之前的介绍保持一致，我们仍将使用相同的优化任务:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl nx"><img src="../Images/b8f6b3571ff3b20b6ac9a32241e2db9a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SnND_aiqdSnGZPkpAtEgmw.png"/></div></figure><p id="7129" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们试图用两个参数<code class="fe nm nn no mv b">a, b</code>最小化<code class="fe nm nn no mv b">y — f(x)</code>的损失，上面计算了它们的梯度。</p><p id="086e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">样本生成将是:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6f2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们生成了100个<code class="fe nm nn no mv b">x</code>和<code class="fe nm nn no mv b">y</code>的样本，我们将使用它们来找到参数的实际值。</p><p id="b44d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现将遵循上面的公式:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="2985" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们设置<code class="fe nm nn no mv b">β1 = 0.9, β2 = 0.999</code>，学习过程会是这样的:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ac803709090713807bc7b3af256211a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*a8JC4QHOBv4nd2Tbz_t7Sw.png"/></div></figure><p id="09dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这里展示的例子并没有显示Adam的明显优势，然而，在更复杂的场景中，Adam似乎是目前最好的选择。</p><p id="59be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的实现有点冗长，因为我们一直在为每个参数重复相同的过程。下面给出了一个更简洁的实现:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3ef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，你可以在这里实现<a class="ae ne" href="https://github.com/MJeremy2017/Machine-Learning-Models/tree/master/Optimisation" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7a2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考:</p><ol class=""><li id="da48" class="ob oc it kk b kl km ko kp kr od kv oe kz of ld og oh oi oj bi translated"><a class="ae ne" href="https://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent" rel="noopener ugc nofollow" target="_blank">https://ruder . io/optimizing-gradient-descent/index . html # stochasticgradientdescence</a></li></ol></div></div>    
</body>
</html>