# 从零开始的 ML(决策树)

> 原文：<https://towardsdatascience.com/ml-from-scratch-decision-tree-c6444102436a?source=collection_archive---------25----------------------->

## 从头开始构建决策树

![](img/ee17d5cc90a0e0aa4670e0c440b3ce62.png)

来源:https://unsplash.com/photos/t1PaIbMTJIM

在这篇和接下来的两篇文章中，我将尝试从代码的角度解释经典的机器学习算法。我总觉得如果你能编写一个 ML 算法的简单实现，那么你的理解就很扎实了。

由于这是 ML 从零开始系列的第一篇文章，我将从分类的角度开始讨论 DT(决策树),因为它非常流行且易于理解。本文的结构是，首先我们将从代码和理论的角度来理解 DT 的构造块，然后最后，我们将这些构造块组装成一个简单的 DT，我们将在 Iris 数据集上测试它。

> 基尼指数

在构建 DT 时，最重要的选择之一是分割节点的标准，尽管我们有几个选择，但我们将使用基尼指数进行演示。
描述如下:

对于给定的节点，它被定义为

> 基尼系数= 1 -总和(prob[i]^2)

因此，在计算了左右节点的基尼指数之后，我们对左右节点进行加权平均，这就成为了父节点的基尼指数，它将用于分割节点。

即 ***左节点值/左右节点总值*左节点 gini 杂质+右节点值/左右节点总值*右节点 gini 杂质***

我们已经完成了分割部分，现在我们将使用它来构建一棵树，因为我们首先创建一个函数，它将根据给定的数据生成一个新的节点。

**创建节点**

这个函数检查每个特性的每个值以找到最佳分割值，我们也可以使用随机分割技术。在所有这些计算之后，它返回一个使用节点类创建的节点(左边提到了完整的代码)。

该函数还检查一些其他约束，如最小基尼值，即节点基尼值应该至少大于用户设置的最小基尼值，以继续分裂，如果不是，则该节点被认为是叶节点。它还检查存在的样本的最小数量，即，如果存在的总数据点小于最小样本，则该节点也被认为是叶节点。

**构建树**

现在让我们转向构建树方法，它以递归的方式构建树。这个函数还使用了一些标准，例如最大深度(由用户设置，以确保我们只在特定深度构建树)

**遍历树**

只有一种方法可以打印整个树结构。只是对 DT 的一个有序遍历。

因此，我们现在已经制定了 DT 的所有构建模块，现在让我们结合起来，尝试理解 DT 的完整流程以及代码。

完整流程-

1.  创建根
2.  以递归方式为根构建左子树
3.  以递归方式为根构建右子树
4.  存储->预测

这个简单的实现不需要改变默认参数，就能给出 95%的准确率*。虽然我没有检查过度拟合和其他问题，因为这篇文章并不关注这一点。一个模块一个模块的运行，破解它。如果有错误，请告诉我。希望你们玩得开心！*