<html>
<head>
<title>Why Is Logistic Regression the Spokesperson of Binomial Regression Models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么 Logistic 回归是二项式回归模型的代言人？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-is-logistic-regression-the-spokesperson-of-binomial-regression-models-54a65a3f368e?source=collection_archive---------20-----------------------#2020-09-17">https://towardsdatascience.com/why-is-logistic-regression-the-spokesperson-of-binomial-regression-models-54a65a3f368e?source=collection_archive---------20-----------------------#2020-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f34d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">广义线性模型</h2><div class=""/><div class=""><h2 id="0e64" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">浅谈二项式回归模型及其连接函数</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/04b1a7ad97194ee50a938b06ef658267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K7N6U6_j__cqR1L8"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">菲尔·博塔在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cd83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">我们日常生活中的许多事件都遵循<a class="ae lh" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank">二项式分布</a>，该分布描述了一系列独立<a class="ae lh" href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="noopener ugc nofollow" target="_blank">伯努利</a>实验的成功次数。</span></p><p id="6b55" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，假设<a class="ae lh" href="https://en.wikipedia.org/wiki/James_Harden" rel="noopener ugc nofollow" target="_blank">詹姆斯·哈登</a>投篮的概率是恒定的，并且每次投篮都是独立的，那么投篮命中率就遵循二项式分布。</p><p id="eac5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们想找出一个二项分布变量 Y 的成功概率(<em class="mn"> p </em>)与一系列自变量 x<em class="mn">s 之间的关系，二项式回归模型是我们的首选。</em></p><p id="1e1a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">链接函数是二项式回归和线性回归模型之间的主要区别。具体来说，线性回归模型直接使用<em class="mn"> p </em>作为响应变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/94d050da506cd1c8fa13a9d2f4a9221d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*hH8LoN2ZnDBaRqqzbEweIQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">线性回归</p></figure><p id="3c34" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线性回归的问题是它的响应值没有界限。然而，二项式回归使用<em class="mn"> p </em>的链接函数(<em class="mn"> l </em>)作为响应变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/cd145b953cd499b46fa938c76ef74e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*RV9f-dIgXK27zutH3s_d9A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">带连接函数的二项式回归</p></figure><p id="d8a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">链接函数将<em class="mn"> x </em> s 的线性组合映射到一个介于 0 和 1 之间但不会达到 0 或 1 的值。基于这样的标准，主要有三种常见的选择:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/fb97569a23b94ea587d323afdf4bfc24.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*sj3fBXeUrFiK1JZgrvbrdA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">二项式回归链接函数</p></figure><p id="94c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当链接函数是 logit 函数时，二项式回归就变成了众所周知的<a class="ae lh" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> logistic 回归</a>。作为数据科学书籍中最早的分类器之一，逻辑回归无疑成为了二项式回归模型的代言人。主要有三个原因。</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="c5f0" class="mw mx it ms b gy my mz l na nb">1. Applicable to more general cases.</span><span id="14e9" class="mw mx it ms b gy nc mz l na nb">2. Easy interpretation.</span><span id="6827" class="mw mx it ms b gy nc mz l na nb">3. Works in retrospective studies.</span></pre><p id="4b3e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们详细检查一下。</p><h2 id="1598" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">极值分布？不完全是。</h2><p id="348b" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">互补对数-对数模型的一个主要区别是，概率单位和对数函数是对称的，但互补对数-对数函数是非对称的。</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="781d" class="mw mx it ms b gy my mz l na nb"><strong class="ms jd"># R code</strong><br/>eq = function(x){log(x/(1-x))}<br/>curve(eq, xlab="probability", ylab="l(probability)",col="red",lwd=2)<br/>curve(qnorm,add = T,lty="dashed",col="blue",lwd=2)<br/>curve(cloglog,add=T,lty="dotted",col="green",lwd=2)<br/>abline(v=0.5,lty="dashed",col="grey")<br/>legend("topleft",legend=c("logit","probit","cloglog"),lty = c("solid","dashed","dotted"),col = c("red","blue","green"),lwd=2)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/635ffa8b0a76ad12ff121f21bfd397bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*u26Q-pxTKm5jyFRQjyOgZw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">链接函数的曲线。对称:logit(红色)和 probit(蓝色)；不对称:绿色</p></figure><p id="bf07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具体来说，我们可以看到对称函数(logit 和 probit)在<em class="mn"> p </em> =0.5 的位置相交。但是 cloglog 函数在概率上有不同的趋近 0 和 1 的速率。有了这样一个特性，cloglog link 函数总是用在极端事件上，在这种情况下，事件的概率接近 0 或 1。</p><p id="f7fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事实上，我们几乎不可能用有限的数据做出极值分布的假设。因此，在大多数情况下，二项式回归模型不选择 cloglog 链接函数。</p><p id="e98c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">换句话说，logit 和 probit 模型可以应用于比 cloglog 模型更普遍的情况。接下来让我们看看 logit 模型相对于 probit 模型的优势。</p><h2 id="2767" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">Z 评分增加β单位？？奇怪。</h2><p id="b63d" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">逻辑回归比概率回归应用更广泛，因为它易于解释，这得益于优势的概念。</p><p id="7334" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有趣的是，在我们的日常生活中，赔率有时比概率更频繁地被使用，因为它更好地代表了<em class="mn">机会，例如</em>NBA 季后赛的赌注。赔率(<em class="mn"> o </em>)和概率(<em class="mn"> p </em>)之间的关系可以用下面的等式来描述。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0a54909090977d88580fb4fdc855fcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*iua8dMBVNoOI-nTVloBXUA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">赔率和概率</p></figure><p id="c8a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">逻辑回归可以改写为，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a93bea02f812ff68363a4a0d5fcb3e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*hqYUl_-wsTa5Z8a0OMd2ag.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">带优势的 logistic 回归模型。</p></figure><p id="9588" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的模型解读起来超级简单。例如，在所有其他独立变量固定的情况下，x1 增加一个单位会使成功的对数几率增加β1。</p><p id="52f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，概率单位模型的解释并不简单。概率单位链接函数计算概率的 Z 值<a class="ae lh" href="https://www.statisticshowto.com/probability-and-statistics/z-score/" rel="noopener ugc nofollow" target="_blank"/>。因此，对同一示例的解释应该是，在所有其他自变量固定的情况下，x1 增加一个单位会使 Z 得分增加β1。</p><p id="0f1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果季后赛投注网站说“湖人赢得半决赛系列赛的 Z 分是 1.65！”，很少有篮球迷会因为他/她的统计学专业而称赞网站跑者。</p><h2 id="df36" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">固定预测因子和观察结果？事实并非如此。</h2><p id="52cf" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">在大多数实际情况下，数据来自<a class="ae lh" href="https://en.wikipedia.org/wiki/Retrospective_cohort_study" rel="noopener ugc nofollow" target="_blank">回顾性抽样</a>而不是<a class="ae lh" href="https://en.wikipedia.org/wiki/Prospective_cohort_study" rel="noopener ugc nofollow" target="_blank">前瞻性抽样</a>。在回顾性研究中，结果是固定的，预测因子是观察和收集的，然而，在前瞻性研究中，预测因子是固定的，结果是观察的。</p><p id="3eaa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，如果我们对狗癌症和只吃狗粮之间的关系感兴趣，我们首先收集 10 只不同品种的狗，给一半的狗只喂狗粮，另一半喂混合食物。然后我们跟踪五年后这些狗的健康状况。这种类型的研究是前瞻性的研究，非常理想，但速度缓慢。</p><p id="e190" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，一个更便宜和更快的方法应该是，我们找到一些患有癌症的狗，我们检查过去五年的喂养记录，并在我们的二项式模型中将它们与一组健康的狗进行比较。这种类型的研究是回顾性研究。</p><p id="89c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设<em class="mn"> α1 </em>是狗没有患癌症时被纳入回顾性研究的概率，而<em class="mn"> α0 </em>是狗患癌症时被纳入的概率。在前瞻性研究中，我们总是可以假设<em class="mn"> α1 = α0 </em>，因为我们还没有看到结果。</p><p id="0a72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">但在回顾性研究中，α1 通常远大于α0，情况并非如此。</em></p><p id="61e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们用贝叶斯定理来描述一只狗得癌症的条件概率(<em class="mn"> p* </em>)假设它被纳入研究，和这只狗得癌症的无条件概率(<em class="mn"> p </em>)之间的关系。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e1bc53c6246fe58fcade9466e0f6daca.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*gMdapjXUwb8QxfkJ7sTY3Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">假设研究中包括了狗患癌症的条件概率</p></figure><p id="d215" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">应用 logit link 函数后，我们有</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/926d57c73e7b6c3ebd0a58b4da6f0455.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*4p3GmylyeIDGyl5eCzWPlA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">条件概率的 logit 函数</p></figure><p id="fe2a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的等式清楚地表明，前瞻性研究和回顾性研究之间的差异是 log( <em class="mn"> α1/α0 </em>)，它只影响二项式模型的截距项。</p><p id="4a0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">logit 链接函数的优势在于，即使在回顾性研究中，我们也不需要担心截距项，因为我们关注的是独立变量对几率的相对影响。</p><p id="6f83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">换句话说，即使在回顾性研究中不可能知道 log( <em class="mn"> α1/α0 </em>)(或对β0 的调整)，它也不能阻止我们估计 logistic 模型的系数(其他β)。此功能对于概率单位链接函数无效。</p><h2 id="de9a" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">摘要</h2><p id="72e3" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">逻辑回归是二项式回归模型中的一种，它使用 logit 作为其连接函数。它优于其他链接功能，因为它的<strong class="lk jd">易于解释</strong>并且在<strong class="lk jd">回顾性研究</strong>中有用。</p><h2 id="2673" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">奖金</h2><p id="eca0" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">其他链接功能都没用吗？不，用下面的代码试试挑战者号的灾难数据，你会发现 cloglog 模型在有链接函数的二项式模型中获得了最小的 AIC。</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="df7a" class="mw mx it ms b gy my mz l na nb"><strong class="ms jd"># R code<br/></strong>library(faraway)<br/>data(orings)<br/>logit_m = glm(cbind(damage, 6-damage) ~ temp, family=binomial(link = logit), orings)<br/>probit_m = glm(cbind(damage, 6-damage) ~ temp, family=binomial(link=probit), orings)<br/>clog_m = glm(cbind(damage, 6-damage) ~ temp, family=binomial(link=cloglog), orings)</span><span id="ddf8" class="mw mx it ms b gy nc mz l na nb">summary(logit_m)<br/>summary(probit_m)<br/>summary(clog_m)</span></pre><p id="9290" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢阅读这篇文章。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="17ba" class="mw mx it bd nd ne nf dn ng nh ni dp nj lr nk nl nm lv nn no np lz nq nr ns iz bi translated">参考</h2><p id="4245" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">遥远，Julian J. <em class="mn">用 R 扩展线性模型:广义线性、混合效应和非参数回归模型</em>。CRC 出版社，2016。</p><div class="ok ol gp gr om on"><a href="https://stats.idre.ucla.edu/stata/dae/probit-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">概率回归| Stata 数据分析示例</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">样本大小:概率和对数模型都比 OLS 回归需要更多的案例，因为它们使用最大似然法…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">stats.idre.ucla.edu</p></div></div></div></a></div><div class="ok ol gp gr om on"><a href="https://stats.idre.ucla.edu/r/dae/logit-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">Logit 回归| R 数据分析示例</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">逻辑回归，也称为 logit 模型，用于模拟二分结果变量。在 logit 模型中…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">stats.idre.ucla.edu</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a href="https://www.datasklr.com/logistic-regression/probit-and-complimentary-log-log-models-for-binary-regression" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">二元回归的概率单位和互补双对数模型</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">Logit:误差具有标准的逻辑分布 Probit:误差具有标准的正态分布…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">www.datasklr.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb lb on"/></div></div></a></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/3d4368c77b138d6c2f01ca0b206a9611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EFzbUAoba8feADko"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">彼得·罗伊德在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>