<html>
<head>
<title>The Family of Unfamiliar Regression Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">陌生回归算法家族</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-family-of-unfamiliar-regression-algorithms-ae7eff9e9463?source=collection_archive---------59-----------------------#2020-05-15">https://towardsdatascience.com/the-family-of-unfamiliar-regression-algorithms-ae7eff9e9463?source=collection_archive---------59-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e738c8cda8de477ae18f7f798424b641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2TdvGiRjGdheWpYVPdseZQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3502290" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae jg" href="https://pixabay.com/users/Hurca-8968775/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3502290" rel="noopener ugc nofollow" target="_blank">米尔科·格里森迪</a>拍摄</p></figure><div class=""/><div class=""><h2 id="4e25" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">介绍一些隐藏的回归方法</h2></div><p id="e727" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单的线性回归和逻辑回归通常是人们在机器学习旅程中学习的第一个算法。由于它们的流行，许多初学者甚至认为它们是唯一的回归形式。然而，有无数的回归算法可以用来建立ML模型。</p><p id="400b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将讨论一些常用的回归算法，即多项式回归，逐步回归，套索回归，岭回归和弹性网回归。</p><h1 id="4058" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">多项式回归算法</h1><p id="f251" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">如果自变量的幂大于1，则回归方程是多项式回归方程。当线性回归线不能恰当地拟合数据时，即模型欠拟合或过拟合数据时，使用多项式回归。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6ad260e777e1c35002ab87d226a3b79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*wdWYslXmAyu7xteXoqRZpQ.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://blog.adafruit.com/2019/12/30/machine-learning-from-scratch/" rel="noopener ugc nofollow" target="_blank"> Adafruit博客</a></p></figure><p id="f4b2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免这种情况，我们需要增加模型的复杂性。为了生成高阶方程，我们可以添加原始特征的幂作为新特征。多项式回归算法的形式为:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/fe5a6ab6e2b75f265873e353d7c4a8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*KPmpSy6cRSe5HE7KBfxs6Q.jpeg"/></div></figure><p id="61fc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种回归技术中，最佳拟合线不是直线。而是一条符合数据点的曲线。这条曲线是二次曲线，比直线更适合数据。</p><h1 id="e2bd" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">逐步回归算法</h1><p id="a183" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">逐步回归是一种拟合回归模型的方法，其中预测变量的选择是通过自动程序进行的。在每一步中，基于一些预先指定的标准，考虑将一个变量添加到解释变量集或从解释变量集中减去，并查看哪一个具有最低的p值。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2c915520f48c7769fa9ea027abc6b9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/1*xELUcEuVOgoFLhISqKtrqQ.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://www.sciencedirect.com/science/article/pii/S0002929707612889" rel="noopener ugc nofollow" target="_blank">科学指导</a></p></figure><p id="b854" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，p值或概率值是在假设零假设正确的情况下，获得至少与测试期间实际观察到的结果一样极端的测试结果的概率。</p><p id="e049" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种建模技术的目的是用最少数量的预测变量最大化预测能力。</p><h1 id="2386" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">Lasso回归算法</h1><p id="4fe9" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">Lasso(最小绝对收缩和选择运算符)是一种回归分析方法，它执行变量选择和正则化，以提高其生成的统计模型的预测精度和可解释性。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/f431a209be5bcc14318f62daf27d08e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xd4rQ449L_TM1zK6.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://towardsdatascience.com/@saptashwa?source=post_page-----e20e34bcbf0b----------------------" rel="noopener" target="_blank"> Saptashwa Bhattacharyya </a>从<a class="ae jg" rel="noopener" target="_blank" href="/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">走向数据科学</a></p></figure><p id="8c6f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与岭回归不同，套索回归将系数缩小为零(正好为零)，这无疑有助于特征选择。该算法在数学上表示为:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/edbe50a0268e0f33795bd64ce7921709.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*y4hQJ8QqAHIow1mu.png"/></div></figure><p id="48d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这也是一种正则化方法，使用l1正则化。套索回归的假设与最小二乘回归相同，只是不假设正态性。</p><h1 id="cffe" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">岭回归算法</h1><p id="4007" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">数据分析师在建立回归模型时面临的最大问题之一是数据存在多重共线性。多重共线性是独立变量高度相关时出现的情况。当我们对数据应用岭回归时，就是这种情况。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a7905c27821f367315beb5e83c249218.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*ba2vYhhWth6a5cYllFAPrQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://towardsdatascience.com/@qshickkim?source=post_page-----2f19b3a202db----------------------" rel="noopener" target="_blank">kyo sik Kim</a>在<a class="ae jg" rel="noopener" target="_blank" href="/ridge-regression-for-better-usage-2f19b3a202db">走向数据科学</a></p></figure><p id="7d90" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">岭回归算法背后的主要思想是找到一条也不符合训练数据的新直线。换句话说，我们引入了一个小偏差，使新的线与数据相适应。这是一种缩小系数值并使用l2正则化的正则化方法。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6ff4308cd0a4e15d4cd29b1305637dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*dCQiL9Cybg_Suo2tXK_Xag.jpeg"/></div></figure><p id="fda2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中，β是系数，λ是收缩参数。岭回归通过收缩参数λ解决了多重共线性问题。该算法的等式如下所示:</p><h1 id="69c4" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">ElasticNet回归算法</h1><p id="54d5" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">ElasticNe回归是一种正则化的回归方法，它线性地结合了套索法和岭法的L1和L2罚函数。当有多个相关的特征时，弹性网是有用的。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/ed8d8a1ffd431b491ff698cd3f80b088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXPEN97r_qTV3DlisylNWA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://www.i2tutorials.com/machine-learning-tutorial/elasticnet-regression/" rel="noopener ugc nofollow" target="_blank">I2教程</a></p></figure><p id="b89b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在变量高度相关的情况下，ElasticNet回归鼓励群体效应。除了设置和选择λ值，弹性网还允许我们调整alpha参数，其中𝞪 = 0对应于山脊，𝞪 = 1对应于套索。这些回归技术的应用应该考虑到数据的条件。</p><p id="1442" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就这样，我们来到了本文的结尾。我希望这能帮助你对不同的回归算法有一个直觉。我们将在后面的文章中研究这些算法。如果你有任何问题，或者如果你认为我有任何错误，请联系我！您可以通过<a class="ae jg" href="http://rajashwin812@gmail.com/" rel="noopener ugc nofollow" target="_blank">邮箱</a>或<a class="ae jg" href="http://linkedin.com/in/rajashwin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>