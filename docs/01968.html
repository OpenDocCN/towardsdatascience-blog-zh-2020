<html>
<head>
<title>How to deploy interpretable models on Google Cloud Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在谷歌云平台上部署可解释模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-deploy-interpretable-models-on-google-cloud-platform-8da93f2e131d?source=collection_archive---------24-----------------------#2020-02-24">https://towardsdatascience.com/how-to-deploy-interpretable-models-on-google-cloud-platform-8da93f2e131d?source=collection_archive---------24-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="97be" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">获取生产张量流模型的本地和全球解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3dc4669a7c995ebba8effd319c1fab33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WbiMH9n_Mhqxpxe7.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源于<a class="ae kv" href="https://pixabay.com/photos/splashing-splash-aqua-water-165192/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>。</p></figure><p id="b220" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">由Chris Rawles，Michael Munn和Michael Abel发布。</em></p><p id="5e53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现代机器学习和人工智能已经证明了在解决非常复杂的问题方面令人印象深刻的结果。然而，更复杂的问题往往意味着更复杂的数据，这必然导致更复杂的模型。真正理解一个模型为什么做出某种预测，可以和原问题本身一样复杂！</p><p id="1c95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可能会有问题，因为许多这样的ML系统已经影响到医疗干预、自主运输、刑事司法、风险管理和许多其他社会领域的用例。在许多情况下，这些人工智能系统的有用性和公平性受到我们理解、解释和控制它们的能力的限制。因此，相当多的努力和研究已经进入了解开强大而复杂的ML模型的黑盒，如深度神经网络。</p><p id="82d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可解释的人工智能指的是方法和技术的集合，这些方法和技术使人类能够理解为什么一个模型会给出特定的结果。模型可解释性是我们在谷歌云的<a class="ae kv" href="https://cloud.google.com/asl" rel="noopener ugc nofollow" target="_blank">高级解决方案实验室</a>教给客户的一个关键话题，在这篇文章中，我们将展示如何使用谷歌云的<a class="ae kv" href="https://cloud.google.com/explainable-ai/" rel="noopener ugc nofollow" target="_blank">可解释人工智能</a>来部署可解释和包容的机器学习模型。</p><p id="f722" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章中使用的所有代码都可以在<a class="ae kv" href="https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/explainable_ai" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="6b47" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">可解释方法的分类</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/ba7261bb32d18815c0fd0d9f7da71c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/0*rgH1H-dEciw3nTUe"/></div></figure><p id="2e9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">(事后)模型可解释性方法的概述和不同技术的例子。</em></p><p id="f17b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数可解释性方法可以沿着三个轴分开[ <a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">来源</a> ]:</p><p id="1fa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">内在与事后。</strong>所谓内在，我们指的是内在可解释的模型。也就是说，它们在结构上足够简单，我们可以通过简单地查看模型本身来理解模型是如何进行预测的。例如，线性模型的学习权重或通过决策树学习的拆分可用于解释模型做出预测的原因。</p><p id="793e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事后方法包括使用经过训练的模型和数据来理解为什么做出某些预测。在某些情况下，事后方法也可以应用于具有内在可解释性的模型。</p><p id="019d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将关注后特设模型的可解释性，因为许多先进的方法，如梯度推进和神经网络，是最好的理解使用这些方法。</p><p id="0701" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模型不可知与模型特定。</strong>模型不可知意味着可解释性方法可以应用于<em class="ls">任何</em>模型，而特定于模型的方法只能用于某些模型类型。例如，如果该方法只适用于神经网络，那么它将被认为是特定于模型的。相反，如果一个可解释性方法将训练好的模型视为一个黑盒，那么它将被认为是模型不可知的。</p><p id="b930" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">局部与全局</strong>:局部可解释性方法旨在解释单个数据点或预测，而全局方法试图提供模型整体表现的综合解释。通过使用局部结果的集合，所有局部方法都可以变成全局技术。</p><h1 id="481c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">在谷歌云上部署可解释的模型</h1><p id="4e6c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">你可以使用可解释的人工智能在GCP部署可解释的模型，并使用<code class="fe mr ms mt mu b">gcloud beta ai-platform explain</code>命令进行预测。</p><p id="6af1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用解释进行训练和预测的步骤是</p><ol class=""><li id="aabb" class="mv mw iq ky b kz la lc ld lf mx lj my ln mz lr na nb nc nd bi translated">训练一个模型并将其部署在GCP上。</li><li id="2bfb" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">将一个包含基线特征值的JSON文件上传到一个云存储桶。</li><li id="cd8e" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">使用这个JSON文件创建模型的一个版本，并指定<code class="fe mr ms mt mu b">explanation-method</code>。</li><li id="e2d0" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">致电<code class="fe mr ms mt mu b">gcloud beta ai-platform explain</code>获取解释。</li></ol><p id="f682" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面我们将更详细地展示这些步骤。</p><p id="77dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，你需要一个在谷歌云人工智能平台(CAIP)上训练和部署的模型。我们将查看纽约市出租车数据集。你可以看看这篇博文，看看如何在CAIP轻松训练一个模特。在撰写本文时，AI解释仅支持TensorFlow 1.x，因此无论您构建什么模型，请确保您使用TensorFlow 1.x。一旦您将模型保存为<a class="ae kv" href="https://www.google.com/search?q=tensorflow+model+format&amp;oq=tensorflow+model+format&amp;aqs=chrome.0.0l2j69i61j69i65j69i60l4.2431j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank"> SavedModel </a>格式，我们将在CAIP上创建一个新模型:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="bfe7" class="nn lu iq mu b gy no np l nq nr">gcloud ai-platform models create taxifare</span></pre><p id="7fed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在部署我们的模型之前，我们必须配置一个<code class="fe mr ms mt mu b">explanations_metadata.json</code>文件，并将其复制到模型目录中。在这个JSON文件中，我们需要告诉AI解释我们的模型所期望的输入和输出张量的名称。</p><p id="70da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，在这个文件中，我们需要设置<code class="fe mr ms mt mu b">input_baselines</code>，它告诉解释服务我们的模型的基线输入应该是什么。理解基线对于许多模型解释技术的有效使用是很重要的。两种支持的技术，采样Shapley和综合梯度，将预测与基线特征值和预测进行比较。选择合适的基线很重要，因为本质上您是在比较模型的预测与基线值的比较。要阅读更多关于基线的内容，请查看<a class="ae kv" href="https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf" rel="noopener ugc nofollow" target="_blank">可解释的人工智能白皮书。</a></p><p id="d031" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，对于数字数据，我们建议您选择一个简单的基线，如平均值或中值。对于本例，我们将使用每个要素的中值-这意味着此模型的基线预测将是我们的模型使用数据集中每个要素的中值预测的出租车费用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="b3df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将这个Python字典写出到一个JSON文件中:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="0dbb" class="nn lu iq mu b gy no np l nq nr"># Write the json to a local file<br/>with open(‘explanation_metadata.json’, ‘w’) as output_file:<br/>    json.dump(explanation_metadata, output_file)<br/>    # Copy the json to the model directory.</span></pre><p id="cce2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后在bash中，我们使用<code class="fe mr ms mt mu b">gsutil.</code>将JSON文件复制到您的模型目录中</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="1bc3" class="nn lu iq mu b gy no np l nq nr">$ gsutil cp explanation_metadata.json $model_dir</span></pre><p id="dd0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经创建了我们的<code class="fe mr ms mt mu b">explanations_metadata.json</code>文件，我们将部署模型的新版本。这段代码非常类似于使用<code class="fe mr ms mt mu b">gcloud</code>创建模型版本的通常过程，但是有一些额外的标志:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="5171" class="nn lu iq mu b gy no np l nq nr">gcloud beta ai-platform versions create $VERSION_IG \<br/>  -- model $MODEL \<br/>  --origin $model_dir \<br/>  --runtime-version 1.15 \<br/>  --framework TENSORFLOW \<br/>  --python-version 3.5 \<br/>  --machine-type n1-standard-4 \<br/>  <strong class="mu ir">--explanation-method ‘integrated-gradients’ \<br/>  --num-integral-steps 25</strong></span></pre><p id="9698" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用`<strong class="ky ir">解释方法`</strong>标志，您可以指定解释方法——目前支持<code class="fe mr ms mt mu b"><a class="ae kv" href="https://github.com/ankurtaly/Integrated-Gradients" rel="noopener ugc nofollow" target="_blank">integrated-gradients</a></code>和<code class="fe mr ms mt mu b"><a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/shapley.html" rel="noopener ugc nofollow" target="_blank">sampled-shapley</a></code>。</p><p id="fd06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意:在综合渐变和采样Shapley之间做出决定时，我们引用了白皮书</em><a class="ae kv" href="https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ls"/></a><em class="ls">:</em></p><blockquote class="nu nv nw"><p id="65f7" class="kw kx ls ky b kz la jr lb lc ld ju le nx lg lh li ny lk ll lm nz lo lp lq lr ij bi translated">一般来说，对于神经网络和可微分模型，推荐使用积分梯度。它提供了计算优势，特别是对于大的输入特征空间(例如，具有数千个输入像素的图像)。对于不可微模型，建议使用采样Shapley，这是由树和神经网络的集成组成的AutoML表模型的情况。</p></blockquote><p id="3613" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">此外，对于那些想知道采样的Shapley方法与流行的</em><a class="ae kv" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"><em class="ls">【SHAP】</em></a><em class="ls">库有何不同的人，我们还引用了白皮书:</em></p><blockquote class="nu nv nw"><p id="8000" class="kw kx ls ky b kz la jr lb lc ld ju le nx lg lh li ny lk ll lm nz lo lp lq lr ij bi translated">应用Shapley值的方法有很多种，不同之处在于它们引用模型、定型数据和解释上下文的方式。这导致了用于解释模型预测的Shapley值的多样性，<br/>考虑到Shapley的唯一性，这是有点不幸的。Mukund Sundararajan和Amir Najmi的《模型解释的多个Shapley值》中对该主题进行了全面的讨论。我们的方法属于基线Shapley类别，并支持跨各种输入数据模态的多个同步基线。</p></blockquote><p id="b75d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们的模型已经部署好了，我们可以从Jupyter笔记本中获得本地属性:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="debb" class="nn lu iq mu b gy no np l nq nr">resp_obj = !gcloud beta ai-platform explain — model $MODEL \<br/>           — version $VERSION_IG — json-instances=’taxi-data.txt’<br/>response_IG = json.loads(resp_obj.s)</span><span id="7c4b" class="nn lu iq mu b gy oa np l nq nr"># Analyze individual example.<br/>explanations_IG = response_IG[‘explanations’][0][‘attributions_by_label’][0]</span></pre><p id="3582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将这些加载到Pandas数据框架中，并绘制各个示例的属性:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="e81e" class="nn lu iq mu b gy no np l nq nr">df = pd.DataFrame(explanations_IG)<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/3697c6f973cfe58d2b25aac308ecaf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ees37aLibLBUvcuH"/></div></div></figure><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="e97d" class="nn lu iq mu b gy no np l nq nr">row = df.iloc[0] # First example.<br/>row.plot(kind=’barh’)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0ac433bce9a302f68ca42d27d3bfb254.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*AcszE5eINR9shYOo"/></div></figure><p id="4447" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以通过聚集局部属性来获得全局模型可解释性:</p><pre class="kg kh ki kj gt nj mu nk nl aw nm bi"><span id="0b67" class="nn lu iq mu b gy no np l nq nr">df.mean(axis=0).plot(kind=’barh’, color=’orange’)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/8678955a162025717951c2798353bd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*f-L625uR2m9uPdoe"/></div></figure><p id="c722" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关使用采样Shapley全局属性的更多信息，请参考<a class="ae kv" href="https://arxiv.org/pdf/1908.08474.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>。</p><h1 id="dd51" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="1e3d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">就是这样！在这篇文章中，我们展示了如何使用可解释的人工智能在谷歌云平台上部署可解释的模型。可解释的人工智能工具允许用户从已部署的模型中获得本地解释。这些解释可以组合在一起，以提供全局可解释性。除了上述步骤，您还可以查看<a class="ae kv" href="https://pair-code.github.io/what-if-tool/index.html#features" rel="noopener ugc nofollow" target="_blank">假设工具</a>来检查和解释您的模型。</p><h1 id="81f5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">额外资源</h1><ul class=""><li id="37b7" class="mv mw iq ky b kz mm lc mn lf od lj oe ln of lr og nb nc nd bi translated"><a class="ae kv" href="https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data" rel="noopener ugc nofollow" target="_blank">用谷歌人工智能平台解释模型预测</a></li><li id="5ee6" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr og nb nc nd bi translated"><a class="ae kv" href="https://sararobinson.dev/2020/01/15/fraud-detection-tensorflow.html" rel="noopener ugc nofollow" target="_blank">欺诈检测和可解释人工智能</a></li><li id="db93" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr og nb nc nd bi translated"><a class="ae kv" href="https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf" rel="noopener ugc nofollow" target="_blank">谷歌人工智能可解释性白皮书</a></li><li id="4bd6" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr og nb nc nd bi translated"><a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/index.html" rel="noopener ugc nofollow" target="_blank">克里斯多佛·莫尔纳尔的可解释ML </a></li></ul></div></div>    
</body>
</html>