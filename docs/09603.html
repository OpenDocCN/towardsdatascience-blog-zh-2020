<html>
<head>
<title>Algorithms From Scratch: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的算法:线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-linear-regression-c654353d1e7c?source=collection_archive---------27-----------------------#2020-07-08">https://towardsdatascience.com/algorithms-from-scratch-linear-regression-c654353d1e7c?source=collection_archive---------27-----------------------#2020-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cc76" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="0d04" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始详述和构建线性回归模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bc14d5473320bbdbd40c6d007ab0ae45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uOMo-OrOXNaCzWif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">艾萨克·史密斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="75f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression." rel="noopener ugc nofollow" target="_blank">线性回归</a>是一种流行的线性机器学习算法，用于基于回归的问题。由于它的简单性以及它如何构建到逻辑回归和神经网络等其他算法中，它通常是第一次学习机器学习时学习的第一批算法之一。在这个故事中，我们将从头开始实现它，这样我们就可以对线性回归模型中发生的事情建立直觉。</p><p id="dae3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">链接到 Github Repo…</p><div class="me mf gp gr mg mh"><a href="https://github.com/kurtispykes/ml-from-scratch/blob/master/linear_regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">kurtispykes/ml-从零开始</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">github.com</p></div></div></div></a></div><blockquote class="mq mr ms"><p id="8a5d" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated">注意:许多框架都有高度优化的代码，如 Scikit-Learn、Tensorflow 和 PyTorch，因此通常没有必要从头构建自己的算法。然而，当我们从零开始构建模型时，它为我们对模型中正在发生的事情的直觉提供了一个很好的目的，这有助于我们尝试改进我们的模型性能。</p></blockquote><p id="1b3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线性模型是一种算法，它通过简单地计算输入要素加上偏差项(也称为截距项)的加权和来进行预测。考虑到这一点，当我们使用线性回归模型时，我们希望解释因变量(即房价)和一个或多个自变量(即位置、卧室、面积等)之间的关系。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mx"><img src="../Images/d36287d7074df9d19f16c60b4706e8e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bk4QBQnazcAar4iMWN6YQw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 1:多元线性回归</p></figure><p id="fe75" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我们训练一个模型时，我们试图设置参数以得到一条最适合训练数据的线。因此，当我们训练线性回归模型时，我们试图找到最能最小化<a class="ae lh" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">成本函数</a>的θ值。回归模型最常见的成本函数是<a class="ae lh" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank"> RMSE </a>，然而，最小化<a class="ae lh" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>要容易得多，因为它会导致相同的结果。</p><h2 id="3d20" class="my mz it bd na nb nc dn nd ne nf dp ng lr nh ni nj lv nk nl nm lz nn no np iz bi translated">创建模型</h2><p id="31e1" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">如果你从未从头开始编写过机器学习算法，我非常鼓励你这样做。<a class="nv nw ep" href="https://medium.com/u/c831f584bf94?source=post_page-----c654353d1e7c--------------------------------" rel="noopener" target="_blank">约翰·苏利文</a>写了一个非常有用的故事，名为<a class="ae lh" rel="noopener" target="_blank" href="/6-steps-to-write-any-machine-learning-algorithm-from-scratch-perceptron-case-study-335f638a70f3"> <em class="mt">从零开始编写任何机器学习算法的 6 个步骤:感知机案例研究</em> </a> <em class="mt"> w </em>这是我在互联网上找到的关于从零开始编写算法的最好建议。</p><p id="d645" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">分块算法</strong></p><ol class=""><li id="f432" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">随机初始化假设函数的参数</li><li id="b640" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">计算偏导数(点击阅读更多关于这个<a class="ae lh" href="https://math.stackexchange.com/questions/3152235/partial-derivative-of-mse-cost-function-in-linear-regression" rel="noopener ugc nofollow" target="_blank">的信息)</a></li><li id="6c73" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">更新参数</li><li id="746c" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">重复 2-3，重复<em class="mt"> n </em>次迭代(直到成本函数最小化)</li><li id="811e" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">推理</li></ol><p id="051a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">实施</strong></p><p id="d21a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于本节，我将利用 3 个 Python 包。NumPy 用于线性代数，Scikit-Learn 是一个流行的机器学习框架，Matplotlib 用于可视化我们的数据。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="8a40" class="my mz it om b gy oq or l os ot"><strong class="om jd">import</strong> <strong class="om jd">numpy</strong> <strong class="om jd">as</strong> <strong class="om jd">np</strong> <br/><strong class="om jd">import</strong> <strong class="om jd">matplotlib.pyplot</strong> <strong class="om jd">as</strong> <strong class="om jd">plt</strong> <br/><strong class="om jd">from</strong> <strong class="om jd">sklearn.datasets</strong> <strong class="om jd">import</strong> make_regression <br/><strong class="om jd">from</strong> <strong class="om jd">sklearn.linear_model</strong> <strong class="om jd">import</strong> LinearRegression <br/><strong class="om jd">from</strong> <strong class="om jd">sklearn.model_selection</strong> <strong class="om jd">import</strong> train_test_split <br/><strong class="om jd">from</strong> <strong class="om jd">sklearn.metrics</strong> <strong class="om jd">import</strong> mean_squared_error</span></pre><p id="671c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们需要一个数据集。为此，我将<code class="fe ou ov ow om b">sklearn.datasets.make_regression</code>允许您生成一个随机回归问题—参见<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" rel="noopener ugc nofollow" target="_blank">文档</a>。接下来，我将用<code class="fe ou ov ow om b">sklearn.model_selection.train_test_split</code> — <a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">文档</a>将我的数据分成训练集和测试集。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="564e" class="my mz it om b gy oq or l os ot"># creating the data set<br/>X, y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=20, random_state=24)<br/><br/># splitting training and test<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=24)</span></pre><p id="121a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们用<code class="fe ou ov ow om b">matplotlib.pyplot</code>来看看我们的数据是什么样子的— <a class="ae lh" href="https://matplotlib.org/tutorials/introductory/pyplot.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="d338" class="my mz it om b gy oq or l os ot"><em class="mt"># visualize </em><br/>plt.scatter(X, y)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/270d19553be1b97fa7a3d658da7581c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*YAug_2ASpyZ3gZ12Xven1g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 2:我们生成的回归问题</p></figure><p id="60de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们可以开始实施线性回归了。我们程序块的第一步是为我们的假设函数随机初始化参数。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="2aa1" class="my mz it om b gy oq or l os ot"><strong class="om jd">def</strong> param_init(X): <br/>    <em class="mt">"""</em><br/><em class="mt">    Initialize parameters for linear regression model</em><br/><em class="mt">    __________________ </em><br/><em class="mt">    Input(s)</em><br/><em class="mt">    X: Training data</em><br/><em class="mt">    __________________</em><br/><em class="mt">    Output(s)</em><br/><em class="mt">    params: Dictionary containing coefficients</em><br/><em class="mt">    """</em><br/>    params = {} <em class="mt"># initialize dictionary </em><br/>    _, n_features = X.shape <em class="mt"># shape of training data</em><br/><br/>    <em class="mt"># initializing coefficents to 0 </em><br/>    params["W"] = np.zeros(n_features)<br/>    params["b"] = 0<br/>    <strong class="om jd">return</strong> params</span></pre><p id="307c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常好。接下来我们要计算偏导数并更新我们的参数。我们使用一种非常重要的叫做<a class="ae lh" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>的机器学习算法来做到这一点。因此，我们可以用梯度下降实现步骤 2-4。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="c8f2" class="my mz it om b gy oq or l os ot"><strong class="om jd">def</strong> gradient_descent(X, y, params, alpha, n_iter): <br/>    <em class="mt">"""</em><br/><em class="mt">    Gradient descent to minimize cost function</em><br/><em class="mt">    __________________ </em><br/><em class="mt">    Input(s)</em><br/><em class="mt">    X: Training data</em><br/><em class="mt">    y: Labels</em><br/><em class="mt">    params: Dictionary contatining random coefficients</em><br/><em class="mt">    alpha: Model learning rate</em><br/><em class="mt">    __________________</em><br/><em class="mt">    Output(s)</em><br/><em class="mt">    params: Dictionary containing optimized coefficients</em><br/><em class="mt">    """</em><br/>    W = params["W"] <br/>    b = params["b"]<br/>    m = X.shape[0] <em class="mt"># number of training instances </em><br/><br/>    <strong class="om jd">for</strong> _ <strong class="om jd">in</strong> range(n_iter): <br/>        <em class="mt"># prediction with random weights</em><br/>        y_pred = np.dot(X, W) + b<br/>        <em class="mt"># taking the partial derivative of coefficients</em><br/>        dW = (2/m) * np.dot(X.T, (y_pred - y)) <br/>        db = (2/m) * np.sum(y_pred -  y)<br/>        <em class="mt"># updates to coefficients</em><br/>        W -= alpha * dW<br/>        b -= alpha * db <br/>    <br/>    params["W"] = W<br/>    params["b"] = b<br/>    <strong class="om jd">return</strong> params</span></pre><p id="b906" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这些函数，我们可以根据训练数据训练我们的线性回归模型，以获得我们进行推理所需的模型参数。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="3bce" class="my mz it om b gy oq or l os ot"><strong class="om jd">def</strong> train(X, y, alpha=0.01, n_iter=1000):<br/>    <em class="mt">"""</em><br/><em class="mt">    Train Linear Regression model with Gradient decent</em><br/><em class="mt">    __________________ </em><br/><em class="mt">    Input(s)</em><br/><em class="mt">    X: Training data</em><br/><em class="mt">    y: Labels</em><br/><em class="mt">    alpha: Model learning rate</em><br/><em class="mt">    n_iter: Number of iterations </em><br/><em class="mt">    __________________</em><br/><em class="mt">    Output(s)</em><br/><em class="mt">    params: Dictionary containing optimized coefficients</em><br/><em class="mt">    """</em> <br/>    init_params = param_init(X)<br/>    params = gradient_descent(X, y, init_params, alpha, n_iter)<br/>    <strong class="om jd">return</strong> params</span></pre><p id="4156" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，当我们运行这个函数时，我们将从我们的训练数据中获得优化的权重，我们将使用这些权重对我们的测试数据进行推断。接下来，我们需要使用我们存储的权重为我们的推断创建一个预测函数。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="fffd" class="my mz it om b gy oq or l os ot"><strong class="om jd">def</strong> predict(X_test, params):<br/>    <em class="mt">"""</em><br/><em class="mt">    Train Linear Regression model with Gradient decent</em><br/><em class="mt">    __________________ </em><br/><em class="mt">    Input(s)</em><br/><em class="mt">    X: Unseen data</em><br/><em class="mt">    params: Dictionary contianing optimized weights from training</em><br/><em class="mt">    __________________</em><br/><em class="mt">    Output(s)</em><br/><em class="mt">    y_preds: Predictions of model</em><br/><em class="mt">    """</em>  <br/>    y_preds = np.dot(X_test, params["W"]) + params["b"]<br/>    <strong class="om jd">return</strong> y_preds</span></pre><p id="7af5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">太好了！让我们运行这些函数并绘制它们，看看会发生什么…</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="8780" class="my mz it om b gy oq or l os ot">params = train(X_train, y_train) <em class="mt"># train model</em><br/>y_preds = predict(X_test, params) <em class="mt"># inference</em></span><span id="6370" class="my mz it om b gy oy or l os ot">plt.scatter(X_test, y_test)<br/>plt.plot(X_test, y_preds, color="red")<br/>plt.title("Predictions Dummy Regression Data")<br/>plt.xlabel("X axis")<br/>plt.ylabel("Y axis")<br/><br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/5cf941553c5824340de3db9d09ccb884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*JTGah1XDhyjDkB65_Czklw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图 3:来自自定义线性回归模型的预测。</p></figure><p id="16c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的最佳系列似乎相当不错。为了完全确定我们的实现，我们很幸运有许多机器学习库，带有优化的代码，可以用来比较我们的实现。为了进行比较，我将简单地检查我们的实现与 Scikit-learn 的<a class="ae lh" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank"> RMSE </a>。</p><blockquote class="mq mr ms"><p id="53c9" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated">注意:在同一个图上绘制它们的实现也是值得的，如果它们的最佳拟合线覆盖了你的，那么你就在正确的轨道上。</p></blockquote><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="2577" class="my mz it om b gy oq or l os ot">lin_reg = LinearRegression()<br/>lin_reg.fit(X_train, y_train)<br/>sklearn_y_preds = lin_reg.predict(X_test)</span><span id="c4f1" class="my mz it om b gy oy or l os ot">print(f"My implementation: {np.sqrt(mean_squared_error(y_test, y_preds))}<strong class="om jd">\n</strong>Sklearn implementation: {np.sqrt(mean_squared_error(y_test, sklearn_y_preds))}")</span><span id="de7f" class="my mz it om b gy oy or l os ot">&gt;&gt;&gt;&gt; My implementation: 20.986105292320207<br/>Sklearn implementation: 20.986105292320207</span></pre><p id="03ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">匹配了！</p><p id="2ecb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于这种实现，我们使用了过程化编程，这种编程维护起来会变得非常复杂和麻烦，并且没有最大限度地发挥 Python 的潜力，Python 是一种<a class="ae lh" href="https://en.wikipedia.org/wiki/Object-oriented_programming" rel="noopener ugc nofollow" target="_blank">面向对象编程(OOP) </a>语言。关于这一点，这里是我们的线性回归模型的 OOP 实现。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="e184" class="my mz it om b gy oq or l os ot"><strong class="om jd">class</strong> <strong class="om jd">LinReg</strong>(): <br/>    <em class="mt">"""</em><br/><em class="mt">    Custom made Linear Regression class</em><br/><em class="mt">    """</em><br/>    <strong class="om jd">def</strong> __init__(self, alpha=0.01, n_iter= 1000): <br/>        self.alpha = alpha<br/>        self.n_iter = n_iter <br/>        self.params = {}<br/>    <br/>    <strong class="om jd">def</strong> param_init(self, X_train): <br/>        <em class="mt">"""</em><br/><em class="mt">        Initialize parameters for linear regression model</em><br/><em class="mt">        __________________ </em><br/><em class="mt">        Input(s)</em><br/><em class="mt">        X: Training data</em><br/><em class="mt">        """</em><br/>        _, n_features = self.X.shape <em class="mt"># shape of training data</em><br/><br/>        <em class="mt"># initializing coefficents to 0 </em><br/>        self.params["W"] = np.zeros(n_features)<br/>        self.params["b"] = 0<br/>        <strong class="om jd">return</strong> self<br/>        <br/><br/>    <strong class="om jd">def</strong> gradient_descent(self, X_train, y_train): <br/>        <em class="mt">"""</em><br/><em class="mt">        Gradient descent to minimize cost function</em><br/><em class="mt">        __________________ </em><br/><em class="mt">        Input(s)</em><br/><em class="mt">        X: Training data</em><br/><em class="mt">        y: Labels</em><br/><em class="mt">        params: Dictionary contatining random coefficients</em><br/><em class="mt">        alpha: Model learning rate</em><br/><em class="mt">        __________________</em><br/><em class="mt">        Output(s)</em><br/><em class="mt">        params: Dictionary containing optimized coefficients</em><br/><em class="mt">        """</em><br/>        W = self.params["W"] <br/>        b = self.params["b"] <br/>        m = X_train.shape[0]<br/><br/>        <strong class="om jd">for</strong> _ <strong class="om jd">in</strong> range(self.n_iter): <br/>            <em class="mt"># prediction with random weights</em><br/>            y_pred = np.dot(X_train, W) + b<br/>            <em class="mt"># taking the partial derivative of coefficients</em><br/>            dW = (2/m) * np.dot(X_train.T, (y_pred - y_train)) <br/>            db = (2/m) * np.sum(y_pred -  y_train)<br/>            <em class="mt"># updates to coefficients</em><br/>            W -= self.alpha * dW<br/>            b -= self.alpha * db <br/>        <br/>        self.params["W"] = W<br/>        self.params["b"] = b<br/>        <strong class="om jd">return</strong> self<br/><br/>    <strong class="om jd">def</strong> train(self, X_train, y_train):<br/>        <em class="mt">"""</em><br/><em class="mt">        Train Linear Regression model with Gradient decent</em><br/><em class="mt">        __________________ </em><br/><em class="mt">        Input(s)</em><br/><em class="mt">        X: Training data</em><br/><em class="mt">        y: Labels</em><br/><em class="mt">        alpha: Model learning rate</em><br/><em class="mt">        n_iter: Number of iterations </em><br/><em class="mt">        __________________</em><br/><em class="mt">        Output(s)</em><br/><em class="mt">        params: Dictionary containing optimized coefficients</em><br/><em class="mt">        """</em> <br/>        self.params = param_init(X_train)<br/>        gradient_descent(X_train, y_train, self.params , self.alpha, self.n_iter)<br/>        <strong class="om jd">return</strong> self <br/><br/>    <strong class="om jd">def</strong> predict(self, X_test):<br/>        <em class="mt">"""</em><br/><em class="mt">        Train Linear Regression model with Gradient decent</em><br/><em class="mt">        __________________ </em><br/><em class="mt">        Input(s)</em><br/><em class="mt">        X: Unseen data</em><br/><em class="mt">        params: Dictionary contianing optimized weights from training</em><br/><em class="mt">        __________________</em><br/><em class="mt">        Output(s)</em><br/><em class="mt">        y_preds: Predictions of model</em><br/><em class="mt">        """</em>  <br/>        y_preds = np.dot(X_test, self.params["W"]) + self.params["b"]<br/>        <strong class="om jd">return</strong> y_preds</span></pre><p id="e10d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们调用这个类，让它对我们的测试数据进行预测…</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="00d3" class="my mz it om b gy oq or l os ot">linreg = LinReg()<br/>linreg.train(X_train, y_train)<br/>linreg.predict(X_test)</span><span id="9ca7" class="my mz it om b gy oy or l os ot">&gt;&gt;&gt;&gt; <br/>array([   4.73888182,  -90.06369632,   80.39799712,   66.76983607,<br/>        -49.97207144,   93.77905208,   34.30778991,  -38.2209702 ,<br/>         78.03331698,   53.81416352,  102.96993005,  151.71946744,<br/>         95.52801857,  104.82707085,   98.0492089 ,   45.05150211,<br/>         -7.29917923,  -78.41675446,  -27.14118529,  -98.52923336,<br/>        170.75840972, -106.22126739,   24.86194847,  -21.39127805,<br/>         50.24074837])</span></pre><p id="9591" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为健全性检查，我们将测试预测是否与我们的过程实现相同(因为我们知道这已经与 scikit-learn 实现类似)。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="2994" class="my mz it om b gy oq or l os ot">linreg.predict(X_test) == y_preds</span><span id="a0a7" class="my mz it om b gy oy or l os ot">&gt;&gt;&gt;&gt; <br/>array([ True,  True,  True,  True,  True,  True,  True,  True,  True,<br/>        True,  True,  True,  True,  True,  True,  True,  True,  True,<br/>        True,  True,  True,  True,  True,  True,  True])</span></pre><p id="8c9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在你知道了！</p><blockquote class="mq mr ms"><p id="bbb7" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated">注意:我们作为例子的问题是简单线性回归问题(一元线性回归)，y = WX + b，其中 W 是权重，b 是偏倚。</p></blockquote><h2 id="a76c" class="my mz it bd na nb nc dn nd ne nf dp ng lr nh ni nj lv nk nl nm lz nn no np iz bi translated">假设</h2><p id="2a8d" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">在决定我们要使用线性回归之前，了解模型对我们的数据所做的假设是很重要的，这样我们就可以执行必要的特征工程来与我们的模型保持一致。</p><p id="a4d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于特征工程的更多信息，请参阅我以前关于这个主题的文章…</p><div class="me mf gp gr mg mh"><a rel="noopener follow" target="_blank" href="/feature-engineering-for-numerical-data-e20167ec18"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">数字数据的特征工程</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">工程数值的技巧</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf lb mh"/></div></div></a></div><p id="a465" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Linearity" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">线性</strong> </a> —线性回归假设特征和我们想要预测的目标之间存在线性关系。我们可以通过绘制与目标变量相关的特征散点图来检查这一点的有效性。</p><p id="9ae3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Multicollinearity#:~:text=Multicollinearity%20refers%20to%20a%20situation,equal%20to%201%20or%20%E2%88%921." rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">多重共线性</strong></a>——指多元回归模型中两个或两个以上解释变量高度相关的情况(来源:维基百科)。当我们实施线性回归时，我们假设很少或没有多重共线性，因为多重共线性的存在会削弱回归模型的统计能力。要确定数据中是否存在多重共线性，我们可以使用相关矩阵来查看哪些要素高度相关，并移除其中一个高度相关的要素。</p><p id="7560" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Homoscedasticity" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">同方差</strong> </a> —在统计学中，一个随机变量序列(或向量)如果其所有随机变量都具有相同的有限方差，则该序列(或向量)是同方差的(来源:维基百科)。线性回归假设所有观测值中的噪声是相同的(也可称为误差项或随机扰动),并且不依赖于独立变量的值。我们可以用残差值与预测值的散点图来检验同方差性。我们应该期望分布中没有模式，因为如果有模式，那么数据就是异方差的。</p><p id="6b73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">误差分布的正态性</strong> —数据点正态分布在回归线周围。换句话说，我们假设残差遵循<a class="ae lh" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>。我们可以使用直方图或 QQ 图来检查这一点——我们可以使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">广义线性模型(GLM) </a>来克服这一点。</p><p id="e1a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Autocorrelation#:~:text=Autocorrelation%2C%20also%20known%20as%20serial,the%20time%20lag%20between%20them." rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">自相关</strong></a>——信号与自身延迟副本的相关性，作为延迟的函数——通俗地说，就是观测值之间的相似性，作为它们之间时滞的函数(来源:维基百科)。简而言之，当残差彼此不独立时，就会出现自相关。我们可以使用散点图来直观地检查自相关性，或者使用 Durbin-Watsons 检验来测试残差不是线性自相关的零假设。</p><h2 id="2c53" class="my mz it bd na nb nc dn nd ne nf dp ng lr nh ni nj lv nk nl nm lz nn no np iz bi translated">赞成的意见</h2><ul class=""><li id="f833" class="nx ny it lk b ll nq lo nr lr pg lv ph lz pi md pj od oe of bi translated">易于实施</li><li id="4252" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md pj od oe of bi translated">易于解释输出系数</li><li id="e1a1" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md pj od oe of bi translated">不太复杂</li></ul><h2 id="a041" class="my mz it bd na nb nc dn nd ne nf dp ng lr nh ni nj lv nk nl nm lz nn no np iz bi translated">骗局</h2><ul class=""><li id="1a20" class="nx ny it lk b ll nq lo nr lr pg lv ph lz pi md pj od oe of bi translated">对异常值敏感</li><li id="2c73" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md pj od oe of bi translated">假设自变量和因变量之间呈线性关系</li><li id="2489" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md pj od oe of bi translated">假设特征之间相互独立</li></ul><p id="fb13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于线性回归的更多信息，你可以阅读维基百科页面。</p><div class="me mf gp gr mg mh"><a href="https://en.wikipedia.org/wiki/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression." rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">线性回归</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">在统计学中，线性回归是一种建模标量响应(或变量)之间关系的线性方法</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">en.wikipedia.org</p></div></div><div class="pa l"><div class="pk l pc pd pe pa pf lb mh"/></div></div></a></div><h2 id="db80" class="my mz it bd na nb nc dn nd ne nf dp ng lr nh ni nj lv nk nl nm lz nn no np iz bi translated">包裹</h2><p id="67f0" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">线性回归是最简单的机器学习模型之一，也最有可能是你将要学习或者应该学习的第一个模型。它是一个非常有用的分析变量之间关系的工具，但是由于模型的假设很多，现实问题的简化，所以在大多数实际应用中不推荐使用。</p><p id="e172" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢您花时间阅读这个故事。如果我错过了什么或者你想让我澄清什么，请在评论中回复。另外，如果你想和我联系，我在 LinkedIn 上是最容易联系到的。</p><div class="me mf gp gr mg mh"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">Kurtis Pykes -人工智能作家-走向数据科学| LinkedIn</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">在世界上最大的职业社区 LinkedIn 上查看 Kurtis Pykes 的个人资料。Kurtis 有一个工作列在他们的…</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">www.linkedin.com</p></div></div><div class="pa l"><div class="pl l pc pd pe pa pf lb mh"/></div></div></a></div></div></div>    
</body>
</html>