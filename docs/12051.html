<html>
<head>
<title>K-Nearest Neighbors Classification From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻分类从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-classification-from-scratch-6b31751bed9b?source=collection_archive---------58-----------------------#2020-08-19">https://towardsdatascience.com/k-nearest-neighbors-classification-from-scratch-6b31751bed9b?source=collection_archive---------58-----------------------#2020-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4595" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">没有第三方库的 R 中的分步示例</h2></div><p id="7700" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章旨在探索一步一步的方法来创建一个<strong class="kk iu">K-最近邻算法</strong>而不需要任何第三方库的帮助。在实践中，这种算法应该足够有用，每当我们已经进行了分类(在这种情况下，颜色)时，我们就可以对我们的数据进行分类，这将作为查找邻居的起点。</p><p id="d93a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将使用一个特定的数据集，它可以在这里下载<a class="ae le" href="https://www.kaggle.com/tenaciousjay/knn-fromscratch?select=RGB.csv" rel="noopener ugc nofollow" target="_blank">。它包含 539 个二维数据点，每个数据点都有特定的颜色分类。我们的目标是将他们分成两组(训练和测试)，并根据我们的算法建议尝试猜测我们的测试样本颜色。</a></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="15b1" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">训练和测试样本生成</h1><p id="e1fd" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们将创建两个不同的样品组:</p><ul class=""><li id="0d74" class="mj mk it kk b kl km ko kp kr ml kv mm kz mn ld mo mp mq mr bi translated"><strong class="kk iu">训练集:</strong>这将包含我们 75%的工作数据，随机选择。这个集合将用于生成我们的模型。</li><li id="4e20" class="mj mk it kk b kl ms ko mt kr mu kv mv kz mw ld mo mp mq mr bi translated"><strong class="kk iu">测试集:</strong>我们剩余的 25%工作数据将用于测试我们模型的样本外准确性。一旦我们做出了 25%的预测，我们将通过比较预测值和实际值来检查“<em class="mx">正确分类的百分比</em>”。</li></ul><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="a78d" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># Load Data</em><br/>library(readr)<br/>RGB &lt;- as.data.frame(read_csv("RGB.csv"))<br/>RGB$x &lt;- <strong class="nd iu">as.numeric</strong>(RGB$x)<br/>RGB$y &lt;- <strong class="nd iu">as.numeric</strong>(RGB$y)<br/>print("Working data ready")</span><span id="46ca" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Training Dataset</em><br/>smp_siz = <strong class="nd iu">floor</strong>(0.75*nrow(RGB))<br/>train_ind = sample(<strong class="nd iu">seq_len</strong>(nrow(RGB)),size = smp_siz)<br/>train =RGB[train_ind,]</span><span id="c83b" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Testting Dataset</em><br/>test=RGB[-train_ind,]<br/>OriginalTest &lt;- test<br/>paste("Training and test sets done")</span></pre><h1 id="79bb" class="lm ln it bd lo lp nn lr ls lt no lv lw jz np ka ly kc nq kd ma kf nr kg mc md bi translated">培训用数据</h1><p id="a089" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们可以看到，我们的训练数据根据颜色分为 3 类。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="3487" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># We plot test colored datapoints</em><br/>library(ggplot2)<br/>colsdot &lt;- <strong class="nd iu">c</strong>("Blue" = "blue", "Red" = "darkred", "Green" = "darkgreen")<br/>ggplot() + <br/>  geom_tile(data=train,mapping=aes(x, y), alpha=0) +<br/>  <em class="mx">##Ad tiles according to probabilities</em><br/>  <em class="mx">##add points</em><br/>  geom_point(data=train,mapping=aes(x,y, colour=Class),size=3 ) + <br/>  scale_color_manual(values=colsdot) +<br/>  <em class="mx">#add the labels to the plots</em><br/>  xlab('X') + ylab('Y') + ggtitle('Train Data')+<br/>  <em class="mx">#remove grey border from the tile</em><br/>  scale_x_continuous(expand=<strong class="nd iu">c</strong>(0,.05))+scale_y_continuous(expand=<strong class="nd iu">c</strong>(0,.05))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1f5811bda4017d0d36c227491604191f.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*BrlfcAkRElSlUkgW.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">训练数据:我们可以观察 3 个类别(蓝色、绿色和红色)</p></figure><h1 id="cb68" class="lm ln it bd lo lp nn lr ls lt no lv lw jz np ka ly kc nq kd ma kf nr kg mc md bi translated">测试数据</h1><p id="8acb" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">即使我们知道测试数据的原始颜色分类，我们也将尝试创建一个模型，该模型可以仅根据有根据的猜测来猜测它的颜色。为此，我们将删除它们的原始颜色，保存它们只是为了测试的目的；一旦我们的模型做出预测，我们将能够通过比较原始预测和我们的预测来计算我们的<strong class="kk iu">模型精度</strong>。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="23bc" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># We plot test colored datapoints</em><br/>colsdot &lt;- <strong class="nd iu">c</strong>("Blue" = "blue", "Red" = "darkred", "Green" = "darkgreen")<br/>ggplot() + <br/>  geom_tile(data=test,mapping=aes(x, y), alpha=0) +<br/>  <em class="mx">##Ad tiles according to probabilities</em><br/>  <em class="mx">##add points</em><br/>  geom_point(data=test,mapping=aes(x,y),size=3 ) + <br/>  scale_color_manual(values=colsdot) +<br/>  <em class="mx">#add the labels to the plots</em><br/>  xlab('X') + ylab('Y') + ggtitle('Test Data')+<br/>  <em class="mx">#remove grey border from the tile</em><br/>  scale_x_continuous(expand=<strong class="nd iu">c</strong>(0,.05))+scale_y_continuous(expand=<strong class="nd iu">c</strong>(0,.05))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1f51b7ad9a50d74b748217e32803b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*rYXxQ18pXmkJ0Nf3.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">测试数据:我们删除并故意忘记了它的分类颜色，以创建一个能够猜测它们的模型。</p></figure><h1 id="b9b7" class="lm ln it bd lo lp nn lr ls lt no lv lw jz np ka ly kc nq kd ma kf nr kg mc md bi translated">k-最近邻算法</h1><p id="8210" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">下面是实现该算法的分步示例。我们想要实现的是，对于上面每个选定的灰点(我们的测试值)，据称我们不知道它们的实际颜色，从我们的训练值中找到最近的邻居或最近的彩色数据点，并分配与此相同的颜色。</p><p id="ae98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">具体来说，我们需要:</strong></p><ul class=""><li id="4a06" class="mj mk it kk b kl km ko kp kr ml kv mm kz mn ld mo mp mq mr bi translated"><strong class="kk iu">归一化数据:</strong>即使在这种情况下不需要，因为所有值都是相同的标度(0 到 1 之间的小数)，建议归一化，以便有一个“标准距离度量”。</li><li id="48d1" class="mj mk it kk b kl ms ko mt kr mu kv mv kz mw ld mo mp mq mr bi translated"><strong class="kk iu">定义我们如何度量距离:</strong>我们可以将这个二维数据集中两点之间的距离定义为它们之间的欧氏距离。我们将计算 L1(绝对差之和)和 L2(平方差之和)距离，尽管最终结果将使用 L2 计算，因为它比 L1 更不宽容。</li><li id="a1c5" class="mj mk it kk b kl ms ko mt kr mu kv mv kz mw ld mo mp mq mr bi translated"><strong class="kk iu">计算距离:</strong>我们需要计算每个测试数据点和训练数据集中每个值之间的距离。标准化在这里是至关重要的，因为在身体结构的情况下，体重(1 公斤)和身高(1 米)的距离是不可比的。我们可以预见到公斤比米的偏差更大，导致总距离不正确。</li><li id="cbfe" class="mj mk it kk b kl ms ko mt kr mu kv mv kz mw ld mo mp mq mr bi translated"><strong class="kk iu">排序距离:</strong>一旦我们计算出每个测试点和训练点之间的距离，我们就需要按降序对它们进行排序。</li><li id="3b05" class="mj mk it kk b kl ms ko mt kr mu kv mv kz mw ld mo mp mq mr bi translated"><strong class="kk iu">选择前 K 个最近的邻居:</strong>我们将选择前 K 个最近的训练数据点，以检查它们属于哪个类别(颜色)，以便将该类别分配给我们的测试点。由于我们可能使用多个邻居，我们可能会有多个类别，在这种情况下，我们应该计算一个概率。</li></ul><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6630" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># We define a function for prediction</em><br/>KnnL2Prediction &lt;- function(x,y,K) {<br/>    <br/>  <em class="mx"># Train data</em><br/>  Train &lt;- train<br/>  <em class="mx"># This matrix will contain all X,Y values that we want test.</em><br/>  Test &lt;- data.frame(X=x,Y=y)<br/>    <br/>  <em class="mx"># Data normalization</em><br/>  Test$X &lt;- (Test$X - <strong class="nd iu">min</strong>(Train$x))/(<strong class="nd iu">min</strong>(Train$x) - <strong class="nd iu">max</strong>(Train$x))<br/>  Test$Y &lt;- (Test$Y - <strong class="nd iu">min</strong>(Train$y))/(<strong class="nd iu">min</strong>(Train$y) - <strong class="nd iu">max</strong>(Train$y))<br/>  Train$x &lt;- (Train$x - <strong class="nd iu">min</strong>(Train$x))/(<strong class="nd iu">min</strong>(Train$x) - <strong class="nd iu">max</strong>(Train$x))<br/>  Train$y &lt;- (Train$y - <strong class="nd iu">min</strong>(Train$y))/(<strong class="nd iu">min</strong>(Train$y) - <strong class="nd iu">max</strong>(Train$y))</span><span id="0d64" class="nh ln it nd b gy nm nj l nk nl">  <em class="mx"># We will calculate L1 and L2 distances between Test and Train values.</em><br/>  VarNum &lt;- ncol(Train)-1<br/>  L1 &lt;- 0<br/>  L2 &lt;- 0<br/>  for (i in 1:VarNum) {<br/>    L1 &lt;- L1 + (Train[,i] - Test[,i])<br/>    L2 &lt;- L2 + (Train[,i] - Test[,i])^2<br/>  }<br/>    <br/>  <em class="mx"># We will use L2 Distance</em><br/>  L2 &lt;- <strong class="nd iu">sqrt</strong>(L2)<br/>  <br/>  <em class="mx"># We add labels to distances and sort</em><br/>  Result &lt;- data.frame(Label=Train$Class,L1=L1,L2=L2)<br/>  <br/>  <em class="mx"># We sort data based on score</em><br/>  ResultL1 &lt;-Result[order(Result$L1),]<br/>  ResultL2 &lt;-Result[order(Result$L2),]<br/>  <br/>  <em class="mx"># Return Table of Possible classifications</em><br/>  a &lt;- prop.table(table(head(ResultL2$Label,K)))<br/>  b &lt;- as.data.frame(a)<br/>  <strong class="nd iu">return</strong>(<strong class="nd iu">as.character</strong>(b$Var1[b$Freq == <strong class="nd iu">max</strong>(b$Freq)]))<br/>}</span></pre><h1 id="cb40" class="lm ln it bd lo lp nn lr ls lt no lv lw jz np ka ly kc nq kd ma kf nr kg mc md bi translated">使用交叉验证找到正确的 K 参数</h1><p id="6384" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">为此，我们将使用一种叫做“交叉验证”的方法。这意味着，我们将在训练数据本身中进行预测，并针对数据的许多不同折叠或排列，对许多不同的 K 值进行迭代。一旦我们完成，我们将平均我们的结果，并为我们的“K-最近邻”算法获得最佳 K。</p><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/24954c1fd3983d263182e3ad85d28b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/0*ryF8siec8o4l15qy.jpg"/></div></figure><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="9ad4" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># We will use 5 folds</em><br/>FoldSize = <strong class="nd iu">floor</strong>(0.2*nrow(train)) </span><span id="1c17" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Fold1</em><br/>piece1 = sample(<strong class="nd iu">seq_len</strong>(nrow(train)),size = FoldSize ) <br/>Fold1 = train[piece1,]<br/>rest = train[-piece1,] </span><span id="6e8d" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Fold2</em><br/>piece2 = sample(<strong class="nd iu">seq_len</strong>(nrow(rest)),size = FoldSize)<br/>Fold2 = rest[piece2,]<br/>rest = rest[-piece2,] </span><span id="a679" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Fold3</em><br/>piece3 = sample(<strong class="nd iu">seq_len</strong>(nrow(rest)),size = FoldSize)<br/>Fold3 = rest[piece3,]<br/>rest = rest[-piece3,] </span><span id="d195" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Fold4</em><br/>piece4 = sample(<strong class="nd iu">seq_len</strong>(nrow(rest)),size = FoldSize)<br/>Fold4 = rest[piece4,]<br/>rest = rest[-piece4,] </span><span id="79d3" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># Fold5</em><br/>Fold5 &lt;- rest</span><span id="d82e" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># We make folds</em><br/>Split1_Test &lt;- rbind(Fold1,Fold2,Fold3,Fold4)<br/>Split1_Train &lt;- Fold5</span><span id="6dd2" class="nh ln it nd b gy nm nj l nk nl">Split2_Test &lt;- rbind(Fold1,Fold2,Fold3,Fold5)<br/>Split2_Train &lt;- Fold4</span><span id="f3a6" class="nh ln it nd b gy nm nj l nk nl">Split3_Test &lt;- rbind(Fold1,Fold2,Fold4,Fold5)<br/>Split3_Train &lt;- Fold3</span><span id="4515" class="nh ln it nd b gy nm nj l nk nl">Split4_Test &lt;- rbind(Fold1,Fold3,Fold4,Fold5)<br/>Split4_Train &lt;- Fold2</span><span id="ca70" class="nh ln it nd b gy nm nj l nk nl">Split5_Test &lt;- rbind(Fold2,Fold3,Fold4,Fold5)<br/>Split5_Train &lt;- Fold1</span><span id="ac7a" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># We select best K</em><br/>OptimumK &lt;- data.frame(K=<strong class="nd iu">NA</strong>,Accuracy=<strong class="nd iu">NA</strong>,Fold=<strong class="nd iu">NA</strong>)<br/>results &lt;- train</span><span id="c177" class="nh ln it nd b gy nm nj l nk nl">for (i in 1:5) {<br/>  if(i == 1) {<br/>    train &lt;- Split1_Train<br/>    test &lt;- Split1_Test<br/>  } else if(i == 2)  {<br/>    train &lt;- Split2_Train<br/>    test &lt;- Split2_Test<br/>  } else if(i == 3)  {<br/>    train &lt;- Split3_Train<br/>    test &lt;- Split3_Test<br/>  } else if(i == 4)  {<br/>    train &lt;- Split4_Train<br/>    test &lt;- Split4_Test<br/>  } else if(i == 5)  {<br/>    train &lt;- Split5_Train<br/>    test &lt;- Split5_Test<br/>  }<br/>    for(j in 1:20) {<br/>      results$Prediction &lt;- mapply(KnnL2Prediction, results$x, results$y,j)<br/>      <em class="mx"># We calculate accuracy</em><br/>      results$Match &lt;- ifelse(results$Class == results$Prediction, 1, 0)<br/>      Accuracy &lt;- <strong class="nd iu">round</strong>(<strong class="nd iu">sum</strong>(results$Match)/nrow(results),4)<br/>      OptimumK &lt;- rbind(OptimumK,data.frame(K=j,Accuracy=Accuracy,Fold=paste("Fold",i)))<br/>    <br/>    }<br/>}</span><span id="98f9" class="nh ln it nd b gy nm nj l nk nl">OptimumK &lt;- OptimumK [-1,]<br/>MeanK &lt;- aggregate(Accuracy ~ K, OptimumK, mean)<br/>ggplot() + <br/>  geom_point(data=OptimumK,mapping=aes(K,Accuracy, colour=Fold),size=3 ) +<br/>  geom_line(aes(K, Accuracy, colour="Moving Average"), linetype="twodash", MeanK) +<br/>  scale_x_continuous(breaks=seq(1, <strong class="nd iu">max</strong>(OptimumK$K), 1))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d443713b90229d5f9c956420ff898698.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*XrXQdGY17C5ntSF-.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">20 个不同 K 值的 5 倍</p></figure><p id="02b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上图所示，我们可以观察到，对于所有褶皱，我们算法的预测精度在 88%-95%的范围内，并且从 K=3 开始下降。我们可以在 K=1 (3 也是一个很好的选择)上观察到最高的一致精度结果。</p><h1 id="49b2" class="lm ln it bd lo lp nn lr ls lt no lv lw jz np ka ly kc nq kd ma kf nr kg mc md bi translated">基于前 1 个最近邻进行预测。</h1><h2 id="5198" class="nh ln it bd lo ob oc dn ls od oe dp lw kr of og ly kv oh oi ma kz oj ok mc ol bi translated">模型精度</h2><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="ab49" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># Predictions over our Test sample</em><br/>test &lt;- OriginalTest<br/>K &lt;- 1<br/>test$Prediction &lt;- mapply(KnnL2Prediction, test$x, test$y,K)<br/>head(test,10)</span><span id="588e" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># We calculate accuracy</em><br/>test$Match &lt;- ifelse(test$Class == test$Prediction, 1, 0)<br/>Accuracy &lt;- <strong class="nd iu">round</strong>(<strong class="nd iu">sum</strong>(test$Match)/nrow(test),4)<br/>print(paste("Accuracy of ",Accuracy*100,"%",sep=""))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/15fc38aa7a94d8552b264ad11b3c8169.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*GDwIg92ZXzgDuVep.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">使用 K=1 的前 10 个预测</p></figure><p id="f1b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的结果可以看出，我们有望在 93%的时间里“猜出正确的类别或颜色”。</p><h2 id="5a3e" class="nh ln it bd lo ob oc dn ls od oe dp lw kr of og ly kv oh oi ma kz oj ok mc ol bi translated"><strong class="ak">原始颜色</strong></h2><p id="9c67" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">下面我们可以观察测试样本的原始颜色或类别。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6dce" class="nh ln it nd b gy ni nj l nk nl">ggplot() + <br/>  geom_tile(data=test,mapping=aes(x, y), alpha=0) +<br/>  geom_point(data=test,mapping=aes(x,y,colour=Class),size=3 ) + <br/>  scale_color_manual(values=colsdot) +<br/>  xlab('X') + ylab('Y') + ggtitle('Test Data')+<br/>  scale_x_continuous(expand=<strong class="nd iu">c</strong>(0,.05))+scale_y_continuous(expand=<strong class="nd iu">c</strong>(0,.05))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8dd8a17a02ba29e122c4b81a6dd7d8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*NME7L_5BImnRw31o.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">这是我们测试样品的原始颜色/等级</p></figure><h2 id="8e63" class="nh ln it bd lo ob oc dn ls od oe dp lw kr of og ly kv oh oi ma kz oj ok mc ol bi translated"><strong class="ak">预测颜色</strong></h2><p id="80a0" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">使用我们的算法，我们为最初无色的样本数据集获得以下颜色。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="bb3d" class="nh ln it nd b gy ni nj l nk nl">ggplot() + <br/>  geom_tile(data=test,mapping=aes(x, y), alpha=0) +<br/>  geom_point(data=test,mapping=aes(x,y,colour=Prediction),size=3 ) + <br/>  scale_color_manual(values=colsdot) +<br/>  xlab('X') + ylab('Y') + ggtitle('Test Data')+<br/>  scale_x_continuous(expand=<strong class="nd iu">c</strong>(0,.05))+scale_y_continuous(expand=<strong class="nd iu">c</strong>(0,.05))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e14c151383df093682dc6d4cd861bdef.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*19tvtYc09wJrqboG.jpg"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在红圈里，我们标出了不同或不正确的分类。</p></figure><p id="7ce6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的图中可以看出，尽管我们的算法对大多数数据点进行了正确的分类，但其中一些数据点还是失败了(用红色标记)。</p><h2 id="8d07" class="nh ln it bd lo ob oc dn ls od oe dp lw kr of og ly kv oh oi ma kz oj ok mc ol bi translated"><strong class="ak">决策限制</strong></h2><p id="155e" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">最后，我们可以可视化我们在原始测试数据集上的“决策限制”。这为我们的模型如何对数据进行分类以及其分类空间的限制提供了一个极好的可视化近似值。</p><p id="af55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，我们将在原始数据集的范围内模拟 160，000 个数据点(400x400 矩阵)，当稍后绘制时，将使用颜色填充大部分空白空间。这将帮助我们详细地表达我们的模型将如何在其学习的颜色类别中分类这个 2D 空间。我们生成的点数越多，我们的“分辨率”就越好，就像电视上的像素一样。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="e648" class="nh ln it nd b gy ni nj l nk nl"><em class="mx"># We calculate background colors</em><br/>x_coord = seq(<strong class="nd iu">min</strong>(train[,1]) - 0.02,<strong class="nd iu">max</strong>(train[,1]) + 0.02,length.out = 40)<br/>y_coord = seq(<strong class="nd iu">min</strong>(train[,2]) - 0.02,<strong class="nd iu">max</strong>(train[,2]) + 0.02, length.out = 40)<br/>coord = expand.grid(x = x_coord, y = y_coord)<br/>coord[['prob']] = mapply(KnnL2Prediction, coord$x, coord$y,K)</span><span id="1cf9" class="nh ln it nd b gy nm nj l nk nl"><em class="mx"># We calculate predictions and plot decition area</em><br/>colsdot &lt;- <strong class="nd iu">c</strong>("Blue" = "blue", "Red" = "darkred", "Green" = "darkgreen")<br/>colsfill &lt;- <strong class="nd iu">c</strong>("Blue" = "#aaaaff", "Red" = "#ffaaaa", "Green" = "#aaffaa")<br/>ggplot() + <br/>  geom_tile(data=coord,mapping=aes(x, y, fill=prob), alpha=0.8) +<br/>  geom_point(data=test,mapping=aes(x,y, colour=Class),size=3 ) + <br/>  scale_color_manual(values=colsdot) +<br/>  scale_fill_manual(values=colsfill) +<br/>  xlab('X') + ylab('Y') + ggtitle('Decision Limits')+<br/>  scale_x_continuous(expand=<strong class="nd iu">c</strong>(0,0))+scale_y_continuous(expand=<strong class="nd iu">c</strong>(0,0))</span></pre><figure class="my mz na nb gt nt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7a5191532cb4fc7ce5fb39fe57a77a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*7VLh2dUKRCWzQWWk.jpg"/></div></figure><p id="6eb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上所述，彩色区域代表我们的算法将定义为“彩色数据点”的区域。显而易见，为什么它未能对其中一些进行正确分类。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="4e92" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">最后的想法</h1><p id="6ece" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">k-最近邻是一种简单的算法，似乎可以提供很好的结果。尽管在这里我们可以用肉眼对物品进行分类，但是这个模型也适用于我们不能仅仅用肉眼观察的高维情况。为了实现这一点，我们需要一个具有现有分类的训练数据集，我们稍后将使用它来对周围的数据进行分类，这意味着它是一个由 T2 监督的机器学习算法。</p><p id="4177" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遗憾的是，这种方法在一些情况下存在困难，例如在存在无法用简单的直线距离表示的复杂模式的情况下，比如在放射状或嵌套式集群的情况下。它还存在性能问题，因为对于新数据点的每个分类，我们需要将其与训练数据集中的每个点进行比较，这是资源和时间密集型的，因为它需要复制和迭代整个集合。</p></div></div>    
</body>
</html>