# 探索正则化背后简单而令人满意的数学

> 原文：<https://towardsdatascience.com/exploring-the-simple-satisfying-math-behind-regularization-2c947755d19f?source=collection_archive---------20----------------------->

![](img/f291ec37a1b59b00922bec64afe9d8a5.png)

来源: [Unsplash](https://unsplash.com/photos/F7Sive0fwIg)

## 对过度拟合的迷人防御

正则化常用于机器学习，从简单的回归算法到复杂的神经网络，以防止算法过拟合。让我们通过简单数学的透镜来探索正则化控制模型的令人难以置信的令人满意和美丽的方式。

考虑一个非常简单的回归任务。有六个参数(系数)表示为β和，五个输入表示为 *x* 。模型的输出只是每个系数乘以输入(加上一个截距项β-0)。

为了使这个问题人性化，我们假设我们正试图基于五个因素来预测学生在一次测试中的分数(`*f*(*x*)`的输出):1)他们花了多少时间做作业(`hw`)；2)他们睡了多少小时(`sleep`)；3)他们上次测试的分数(`score`)；4)他们当前班级的 GPA(`gpa`)；以及 5)他们在测试前是否吃了食物(`food`)。

回归模型的目标是最小化“损失”，试图量化误差。这通常通过均方误差来实现。例如，如果我们有一个输入 *x* 和目标 *y* 的数据集，对于每个数据点，我们将评估预测目标和实际目标的差异，对其求平方，并在所有项目中取平均值。为了简洁起见，可以用这样的简写来表达( *E* 表示“期望值”或平均值)。

因此，线性回归被训练成沿着这个损失函数 *l* 优化其参数。然而，请注意，我们的模型有许多特点，这通常与更高的复杂性相关，类似于增加多项式的次数。所有的特征都相关吗？如果不是，它们可能只是为模型过度拟合提供了另一个自由度。

假设一个学生当前的 GPA 和他们那天早上是否吃了食物，结果提供了最小的好处，同时导致模型过度拟合。如果这些系数被设置为 0，那么这些特征将从模型中完全消除。

这导致了一个更简单的模型，该模型对数据过度拟合的能力较弱。在某种意义上，这种将系数降低到零的行为类似于特征选择。

正则化基于这样的想法，即较大的参数通常会导致过拟合，因为它们 a)不为零，所以它向生态系统中添加了另一个变量/自由度 b)会导致预测中的大波动和不自然的波动。这种高方差是过度拟合的警示信号。

让我们探讨两种类型的正则化:L1 正则化和 L2 正则化。

L1 正则化通过添加“正则化项”略微改变了线性回归中使用的损失函数，在这种情况下是“ *λE* [| *β|* ]”。我们来分解一下: *E* [| *β|* 的意思是‘参数绝对值的平均值’，而 *λ* 是一个比例因子，它决定了参数的平均值应该在总损失上招致多少损失。

总的来说，这鼓励更小的参数。当模型基于损失调整其系数以达到降低其值的总体目标时，它必须确定某个特征是否有足够的价值来保留，因为它比正则化项更多地增加了预测能力*。从根本上来说，保留这些特性更有利可图。*

*然后，丢弃不太有用的特征，模型变得更简单。*

*L2 正则化本质上是相同的，但是参数在被平均之前被平方。因此，正则化是参数平方的平均值。这试图完成同样的任务，即鼓励模型降低系数的整体值，但结果不同。*

*值得注意的是，常规线性回归是 L1/L2 归一化的特例，其中λ参数等于 0，正则项基本上被抵消。*

*为了探究 L1 和 L2 归一化的不同效果，我们需要看看它们的导数，或者它们的函数在某一点的斜率。为了简化，我们可以表示为:*

*   *L1 正则化为`y = x`。导数是 1。*
*   *L2 正规化为`y = x²`。导数是 2 *x* 。*

*这意味着:*

*   *在 L1 正则化中，如果参数从 5 减少到 4，相应的正则化减少 5–4 = 1。*
*   *在 L2 正则化中，如果参数从 5 减少到 4，相应的正则化减少 25–16 = 9。*

*在 L1 正则化中，减少参数的回报是恒定的，而在 L2 正则化中，随着参数接近零，回报变得越来越小。从参数值 5 到 4 会产生 9 的下降，但是从 1 到 0 只会产生 1 的提高。*

*注意，记住模型只关心相对回报。奖励的绝对值与我们无关，因为 lambda 参数总是可以放大或缩小。重要的是模型将从参数的某个变化中获得多少减少或增加。*

*因此，在使用 L2 时，模型可能决定值得“保留一个特征”(不丢弃，或将参数减少到 0)，因为:*

*   *它提供了大量的预测能力(减少损失中的第一项，`*E*[(*f*(*x*)−*y*)²]`)。*
*   *降低参数值不会有太大的好处，因为它已经接近 0 了。*
*   *减小参数将消除损失函数第一项中的增益，而第二项中的增益要小得多(`*λE*[*β²*]`)。*

*所以，总的来说:*

*   *L1 正则化将产生具有更少特征的更简单的模型，因为它为减少参数值提供了一致的回报。它可以被认为是一种变量选择的“自然”方法，例如，它可以去除[多重共线性](/multicollinearity-impacts-your-data-science-project-more-than-you-know-8504efd706f)变量。*
*   *L2 正则化将产生参数*接近*但很可能*不在*零点的更复杂的模型，因为它提供递减的奖励来减少参数值。它能够学习更复杂的数据模式。*

*两种正则化方法都通过防止每个参数对最终结果产生太大影响来降低模型过度拟合的能力，但会导致两种不同的结果。*

*如果你正在寻找一个简单和轻量级的模型，L1 正则化是一个不错的选择。它采取了一种严肃的方法来消除对输出没有深刻影响的变量。在回归中，这被称为“套索回归”，可以在像`sci-kit learn`这样的标准库中找到。*

*另一方面，如果您的任务更复杂，例如神经网络中的正则化，使用 L1 可能是一个坏主意，它可以通过将大量超参数设置为零来杀死它们。L2 正则化通常在神经网络中被推荐，因为它充当护栏，但不会过多地干扰神经元的复杂工作。*

*L2 回归被称为“岭回归”，可以在标准库中实现。在神经网络中，丢弃比 L2 正则化更“自然”，但在许多用例中，应该使用后者或两者都使用。*

# *要点*

*   *正则化通过减少任何特征对结果的整体影响来防止机器学习模型中的过度拟合。*
*   *L1 正则化和 L2 正则化都不断地给模型施加压力以减少它们的参数。前者给出恒定的奖励，但是后者根据参数接近 0 的程度给出递减的奖励。*
*   *正则化迫使模型反复比较某个特征带来的预测能力与其增加正则化项的程度，这导致模型选择保留更重要的特征。*
*   *L1 将导致许多不太相关的变量被一起消除(系数设置为 0)，而 L2 将导致不太相关的变量仍然存在，但系数较小。*

*感谢阅读！*