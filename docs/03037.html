<html>
<head>
<title>Exploring DenseNets: From Paper To Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索DenseNets:从纸到Keras</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-densenets-from-paper-to-keras-dcc01725488b?source=collection_archive---------17-----------------------#2020-03-23">https://towardsdatascience.com/exploring-densenets-from-paper-to-keras-dcc01725488b?source=collection_archive---------17-----------------------#2020-03-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fe1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">拆开DenseNets看看里面有什么“密”！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ac4b06b618431c645c4fd0a292c33920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b-9VR35o4RZ5IpUy"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="845f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNets </a>广泛用于图像分类任务、分割、图像重建等。如果你是一个经验丰富的TF开发者，你可能会在<code class="fe lv lw lx ly b">tf.keras.applications</code>模块中看到它们。什么是DenseNets？它们里面真的有<em class="lz">密</em>的东西吗？来吧，让我们一起探索！</p><p id="0ab0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在本笔记本中查看TF的实现，</p><div class="ma mb gp gr mc md"><a href="https://colab.research.google.com/drive/1v2p228o-_PRtecU0vYUXuGlG_VierqcP#scrollTo=wbkXMkrTgXiF&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd iu gy z fp mi fr fs mj fu fw is bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">编辑描述</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ks md"/></div></div></a></div><h1 id="97d7" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">致密网应该是致密的。对吗？</h1><p id="47b9" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">绝对的，DenseNets是密集连接的卷积神经网络。我们将通过比较普通的CNN(你会在大多数在线博客中看到)和密集块(DenseNet的构建块)来理解这一点。).</p><p id="f92c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩形代表卷积层，红线表示连接或信息流。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/846c0ccfcc8e150a88197fa16ba66774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbJAd7XVXPijX15Dg-NlCw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">常规CNN(左)和密集区块(右)。</p></figure><p id="7031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意上面两个网络中的卷积层3。在第一网络中，第三层接收来自前一层即第二层的信息。这似乎很正常。</p><p id="3356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看一看第二网络。这就是<em class="lz">密集块</em>的作用。第二网络中的第三卷积层接收来自所有先前层的信息。它接收3个输入，</p><ul class=""><li id="476e" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated">红色箭头来自图像。这是第一层的输入。</li><li id="21f5" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">另一个红色箭头来自前一层(第二层)。</li><li id="67d0" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">来自第一层的黑色箭头。这是第一层的输出。</li></ul><p id="29d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看，我们得到了所有先前层(第1层和第2层)的输出以及输入。这就是我们普通CNN和密块的区别。密集块中的每一层都与该块中的每一个后续层相连。一个<em class="lz">块</em>只是一组具有<em class="lz">密集连接</em>的层的别称。</p><p id="3bf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了密集块，我们还有所谓的<em class="lz">过渡</em>层。它们基本上对特征地图进行下采样。这些层执行1 × 1卷积和2 × 2平均合并。</p><h1 id="5e95" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">每一层都从所有前面的层获得输入。听起来是不是很疯狂？</h1><p id="4923" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">这个主意被证明是一个绝妙的诡计。在我们传统的CNN中，每一层只知道从前一层接收的特征地图。较差的卷积层对网络的输入一无所知。它只学习以前的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/8824049933bba897a9417b46c8802189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cFyXKXT8xG4clj1Rz25_Sw.png"/></div></div></figure><p id="7819" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，密集块中的每一层都从所有层接收输入或信息。简而言之，在全球范围内，整个网络都可以获得这些信息。这一点甚至在<a class="ae ky" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNets </a>的惊人论文中也有提及，</p><blockquote class="oe of og"><p id="5728" class="kz la lz lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">“除了更好的参数效率，DenseNets的一大优势是改善了整个网络的信息流和梯度，这使得它们易于训练。”</p></blockquote><p id="6ec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而且，</p><blockquote class="oe of og"><p id="8277" class="kz la lz lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">“为了进一步改善各层之间的信息流，我们提出了一种不同的连接模式:我们引入了从任何一层到所有后续层的直接连接。”</p></blockquote><p id="d74e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可能包括<em class="lz">瓶颈</em>层来提高计算效率。由于许多输入的串联，我们有大量的特征图，这可能会减慢训练。因此，我们对输入执行1 × 1卷积来解决这个问题。</p><p id="8200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决上述问题的另一种方法是使用<em class="lz">压缩率</em>。它从区间<em class="lz"> ( 0，1】</em>中选择，并决定过渡层将产生的特征图的数量。如果θ是<em class="lz">压缩因子</em>并且<em class="lz"> m </em>是输入特征图的数量，则过渡层产生由下式给出的<em class="lz"> p </em>特征图，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/caad46391b06f9d108fc77dd63f3b9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*Qh2VUAIwEpevllFMl8H5vg.png"/></div></figure><h1 id="98ea" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">数学</h1><p id="fe61" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">我们确实可以用一些数学来理解稠密连通性的概念！别担心，这很容易理解。考虑一个输入图像<em class="lz"> x₀ </em>给我们的密集块。</p><p id="e3ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lz"> H </em>是一个复合函数，由三个运算组成:<em class="lz">批量归一化→ ReLU →卷积</em></p><blockquote class="oe of og"><p id="a2f2" class="kz la lz lb b lc ld ju le lf lg jx lh oh lj lk ll oi ln lo lp oj lr ls lt lu im bi translated">我们将H()定义为三个连续操作的复合函数:批量归一化(BN)，然后是校正线性单元(ReLU)和3×3卷积(Conv)。</p></blockquote><p id="9866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一层(x₁)的输出将由，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a4bac022b4e3a191f69bce46577629af.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*Acidhu5R2LtJl8WeAeLk6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">1.第一层的输出。</p></figure><p id="49a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一般对于常规CNN(非DenseNets)，</strong>第<em class="lz">l</em>层的输出将由第<em class="lz">l-1</em>层的输入产生。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/82664870bdbb5aca690f918ccdbac679.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*Y9aMl7NJVVznSHBujGQEjA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(1)的一般表达式。</p></figure><p id="afaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一个DenseNet ，正如我们之前讨论的，一个层从前面的层接收输入。它看起来会像，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a092bcdd8c9982d851c8da08bb7cd3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*OH30Ewa33A7Mp4gao0ez0Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方括号表示x₀,x₁,…,xₗ₋₁的连接</p></figure><p id="8508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是DenseBlock的数学表达式。</p><h1 id="2bb8" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">在TensorFlow里看起来怎么样？(请用Keras！)</h1><p id="8f14" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">我们都在急切地等待这个，对吗？我会坚持让你跟着Colab笔记本去看看DenseNets的实现。首先，我们实现前面讨论过的<em class="lz"> H </em>函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="52e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后是<em class="lz">过渡层</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="7f98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，实现了密集块。我们将调用<code class="fe lv lw lx ly b">H()</code>方法，将之前输出的<code class="fe lv lw lx ly b">conv_outputs</code>和<code class="fe lv lw lx ly b">inputs</code>连接起来。这将会循环发生，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="713d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将所有的密集块组装在一起，并用过渡层将它们连接起来，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="0add" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们做了一个漂亮的<code class="fe lv lw lx ly b">tf.keras.models.Model()</code>，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="2fe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">培训和评估包含在<a class="ae ky" href="https://colab.research.google.com/drive/1v2p228o-_PRtecU0vYUXuGlG_VierqcP#scrollTo=wbkXMkrTgXiF&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中。恭喜你，你已经从零开始实现了一个DenseNet！</p><h1 id="f79c" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">更多关于ML的资源和博客</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><h1 id="24bb" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">就这样</h1><p id="1e07" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">DenseNets的想法令人印象深刻。请确保您也阅读了ResNets。浏览报纸，你会了解更多关于训练的细节。感谢阅读。</p></div></div>    
</body>
</html>