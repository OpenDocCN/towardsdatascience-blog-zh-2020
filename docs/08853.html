<html>
<head>
<title>2 Easy Ways To Avoid Racial Discrimination in Your Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在你的模型中避免种族歧视的两个简单方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/2-easy-ways-to-avoid-racial-discrimination-in-your-model-868ed43593c9?source=collection_archive---------40-----------------------#2020-06-25">https://towardsdatascience.com/2-easy-ways-to-avoid-racial-discrimination-in-your-model-868ed43593c9?source=collection_archive---------40-----------------------#2020-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e53a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于机器学习公平性的 Python 教程。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a32854a0f94aa765845f324101954970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LU_yX9B0cWvg3CAd_z8akw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">火星·马丁内斯在 Unsplash 拍摄的照片</p></figure><p id="c809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">许多人工智能项目的一个高层次目标是沿着公平和歧视的路线解决算法的伦理含义。</p><h1 id="030d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我们为什么要关心公平？</h1><p id="8488" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">众所周知，<a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-and-discrimination-2ed1a8b01038">算法会助长非法歧视</a>。</p><p id="5649" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，每个投资者都希望将更多的资本投入到高投资回报、低风险的贷款中，这可能并不奇怪。</p><p id="d632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个现代的想法是使用机器学习模型，根据关于过去贷款结果的少量已知信息，决定未来的贷款请求给借款人最大的机会全额偿还贷款，同时实现高回报(高利率)的最佳权衡。</p><p id="a8e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一个问题:该模型是根据历史数据训练的，贫穷的未受教育的人，通常是少数族裔或工作经验较少的人，比一般人更有可能屈服于贷款冲销的历史趋势。</p><p id="9f33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果我们的模型试图最大化投资回报，它也可能针对白人，特定邮政编码的人，有工作经验的人，事实上剥夺了其余人口公平贷款的机会。</p><p id="45a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种行为是违法的。</p><p id="1919" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里可能有两个失败点:</p><ul class=""><li id="458c" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">基于对数据的有偏见的探索，我们可能无意中将偏见编码到模型中，</li><li id="8163" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">数据本身可以对人类决策产生的偏差进行编码。</li></ul><p id="73a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，反对不同的治疗方法很容易。</p><h1 id="ae5e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">方法 1:不同的治疗检查</h1><p id="9cad" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然没有一个定义被广泛认同为公平的好定义，但我们可以使用<strong class="lb iu">统计奇偶性</strong>来测试关于受保护属性(如种族)的公平假设。这是一个完全不同的治疗检查。</p><p id="82af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一下申请贷款的被称为<strong class="lb iu"> <em class="ng"> P </em> </strong>的借款人群体，在该群体中有一个已知的黑人借款人子集<strong class="lb iu"> <em class="ng"> B </em> </strong>。</p><p id="a70b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们假设在<strong class="lb iu"><em class="ng"/></strong>上有某种分布<strong class="lb iu"> <em class="ng"> D </em> </strong>，这代表了我们的模型将选择这些借款人进行评估的概率。</p><p id="c896" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模型是一个分类器<em class="ng"> m </em> : <em class="ng"> X </em> → <em class="ng"> 0，1 </em>给借款人贴标签。如果 m = 1，那么这个人将注销他的贷款，如果 m = 0，这个人将全额偿还他的贷款。</p><p id="5a6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ng"> m </em>在<em class="ng"> B </em>上相对于<em class="ng"> X </em>，<em class="ng"> D </em>的偏差或<strong class="lb iu">统计不平等性</strong>是随机黑人借款人被标记为 1 的概率与随机非黑人借款人被标记为 1 的概率之间的差异。</p><p id="8dd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果统计上的不平等很小，那么我们可以说我们的模型具有统计上的奇偶性。该指标描述了我们的模型对于受保护子集群体<em class="ng"> B </em>的公平程度。</p><p id="9263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数的输入是一个二进制值数组(如果样本是黑人申请的贷款，则为 1，否则为 0)和第二个二进制值数组(如果模型预测贷款将被冲销，则为 1，否则为 0)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="f79c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">方法 2:不同的影响检查</h1><p id="1061" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">完全不同的待遇通常被认为是故意的。另一方面，<strong class="lb iu">不同的影响</strong>是无意的。在美国劳动法中，完全不同的影响指的是“在就业、住房和其他领域中，对具有受保护特征的一组人的不利影响大于对另一组人的不利影响，即使雇主或房东适用的规则在形式上是中立的。”</p><p id="0a68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完全不同的影响衡量多数群体和受保护群体获得特定结果的条件概率的比率。法律定义提到了 80%的门槛。</p><p id="b26a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">if P(White | charge off)/P(Black | charge off)&lt;= 80% then the definition of disparate impact is satisfied.</p><p id="94bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The input of the function is an array of binary values (1 if the sample is a loan requested by a Black person, 0 else) and a second array of binary values (1 if the model predicted that the loan will Charge Off, 0 else).</p><p id="9667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The output is True if the model demonstrates discrimination, False else. The degree of discrimination is also provided between 0 and 1.</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="f2f7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Conclusion</h1><p id="2c02" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">In this article, we introduced statistical parity as a metric that characterizes the degree of discrimination between groups, where groups are defined concerning some protected class (e.g. Black population). We also covered the 80 percent rule to measure disparate impact.</p><p id="8aff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Both methods make an easy starting point to check fairness for a classifier model. An advanced understanding is offered in this <a class="ae ky" rel="noopener" target="_blank" href="/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb">机器学习中的公平性教程</a>。</p><p id="789b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在我下面的后续文章中了解更多关于人工智能的不确定性:</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/wild-wide-ai-responsible-data-science-16b860e1efe9"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">野生人工智能:负责任的数据科学</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">谁先开枪——新种族还是人类？</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/my-deep-learning-model-says-sorry-i-dont-know-the-answer-that-s-absolutely-ok-50ffa562cb0b"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">深度学习的不确定性。如何衡量？</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">使用 Keras 对认知和任意不确定性进行贝叶斯估计的实践教程。走向社会…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa ks nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://medium.com/an-injustice/money-is-not-black-its-colorful-34fd1ba7e43d" rel="noopener follow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">钱不是黑色的，是彩色的</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">红线如何仍然阻止美国黑人今天的财富</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">medium.com</p></div></div><div class="nv l"><div class="oc l nx ny nz nv oa ks nm"/></div></div></a></div></div></div>    
</body>
</html>