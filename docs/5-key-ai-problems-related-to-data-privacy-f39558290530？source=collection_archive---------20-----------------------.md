# 与数据隐私相关的 5 个关键人工智能问题

> 原文：<https://towardsdatascience.com/5-key-ai-problems-related-to-data-privacy-f39558290530?source=collection_archive---------20----------------------->

## 隐私

## 解决机器学习模型的问题将如何提高数据隐私合规性

![](img/ed3fdce10e980182db3d2bc9f8519633.png)

[连浩曲](https://unsplash.com/@lianhao?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/privacy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍照

隐私不仅是与人工智能(AI)相关的问题，也是任何数据相关领域的问题。它是关于人们控制他们的个人数据和基于这些数据做出的决定。

在欧洲，2018 年生效的《一般数据保护条例》( GDPR)规范了个人数据的收集和使用。[1]数据保护法并未明确提及人工智能或机器学习，但重点关注个人数据的大规模自动化处理和自动化决策。这意味着，如果人工智能使用个人数据，它就属于该条例的范围，并适用 GDPR 原则。这可以通过使用个人数据来训练、测试或部署人工智能系统。不遵守 GDPR 可能会导致相关公司受到巨额处罚。

一些个人数据的例子包括出生日期、邮政编码、性别，甚至是用户的 IP 地址。

具体来说，GDPR 教赋予个人不受完全自动化的决定支配的权利。对人工智能专家来说，关键问题是:当你在人工智能的帮助下对一个人做出决定时，你如何证明你公平、透明地对待了他，或者给他们机会质疑这样的决定？

尽管 GDPR 与欧洲和英国最为相关，但其主要原则和思想应该与全世界相关。

# 从人工智能的角度考虑什么？

虽然模型的公平性和可解释性是人工智能中活跃的研究主题，但在数据隐私方面，你可能已经考虑了至少 5 个因素。

## **1)阶层失衡**

当你的培训标签不成比例地偏向某个特定的班级时，就会出现班级失衡。换句话说，在二进制分类问题中，输出为 0 的例子很多，但输出为 1 的例子很少，反之亦然。这可能是由于数据收集过程中的偏差，例如仅从本地分支机构收集的数据，或者是域的固有属性，例如在制造过程中识别异常数据点。

类别不平衡是模型偏差最常见的原因之一，但它经常被数据科学家忽略。这是因为，通常，微小的不平衡不会造成巨大的风险，因为模型可以同样好地学习所有类的特征。然而，当出现严重的阶级失衡时，事情就变得棘手了。具体来说，少数阶级会更难预测，所以你的模型偏向多数阶级。

例如，当你训练一个人工智能系统来识别图像时，你可能会面临许多潜在的问题，类别不平衡可能就是其中之一。想想一组 100，000 张图片，其中只有 100 张是猫的图片，99，900 张是狗的图片。你训练的人工智能系统更有可能预测一只狗，因为它被训练得更频繁；它没有足够的反面案例来准确区分这两种类型的图像。

这个潜在的问题并不像错误地给猫和狗分类那样无辜。假设您正在训练一个接受或拒绝个人贷款的模型，而大多数历史贷款都被拒绝了(出于某种原因)。你猜怎么着。您的模型可能会拒绝大多数或所有未来的贷款申请，因为它更多地暴露于此类信息，并且可能没有学会区分这两种情况。从数据隐私的角度来看，这是一个问题，因为该模型不会产生公平的结果。

缓解措施:

首先，尽早确定这是否是一个问题非常重要。你可以通过了解属于你的每个类的数据点的数量来检查:`df[‘your_label’].value_counts()`

如果您确实注意到了类别不平衡，但是您仍然对您的数据运行一个简单的模型，那么您可能会在准确性方面得到很好的结果。您的测试数据可能与您的训练数据遵循相同的分布，因此，如果大多数实例主要来自一个类，则总是预测该类会给出良好的准确度分数。

但是不要被这个愚弄了。通常情况下，混淆矩阵会让你更好地了解你的模型实际在做什么。减轻相关问题的一个主要且最简单的策略是随机重采样。您可以减少多数类以匹配少数类(欠采样)或过采样少数类。

实际上，

```
count_0, count_1 = df.target.value_counts()
df_class_0 = df[df[‘your_label’] == 0].sample(count_1)#under sample
```

或者

```
df_class_1 = df_class_1.sample(count_0, replace=True)#over sample
```

你同样可以像 Imblearn 一样使用 NumPy 或其他库对数据进行采样。此外，还有很多方法可以解决这个问题，比如 SMOTE 和 Tomek 链接。

## **2)易受敌对攻击**

对一个人工智能系统的对抗性攻击可以完全扰乱系统。例如，图像识别系统被证明有弱点或容易受到恶意攻击。研究人员已经表明，即使人工智能系统在成千上万张图像上进行训练，图像中精心放置的像素也可以从根本上改变人工智能系统对它的感知，从而导致错误的预测。[2]这可能会对涉及个人身份识别的实际应用产生严重影响。想象一个安全摄像头拍摄的场景，其中 AI 系统因为这种类型的攻击而错误地识别了罪犯。

缓解措施:

我们需要让我们的深度学习模型更加健壮。不幸的是，这个问题很棘手。目前，全球顶尖大学正在对这一问题进行研究。然而，在理论上，你应该能够测试你的模型，不仅仅是在一个看不见的测试数据集上，还可以模拟这些敌对的攻击来评估它的健壮性。深度学习模型中被错误激活的神经元可能会被丢弃，以提高鲁棒性。【2019 年的这篇文章讨论了这样的方法。

## **3)再现性和一致性**

人工智能中的一个常见问题是，复制我们获得的结果或我们生成的模型有多容易。许多算法在训练它们的模型时具有随机元素。因此，不同的训练运行产生不同的模型(假设不同的随机种子)，并且不同的模型可能具有不同的预测结果。我们如何确保与个人有关的预测不会被基于相同数据训练的下一个模型逆转？

此外，在本地机器上使用我们的数据表现良好的系统，在现场测试时可能表现不佳。我们如何确保我们最初拥有的性能传播到部署的应用程序？我们如何确保系统的性能不会随着时间的推移而恶化，从而影响对个人的决策？

缓解措施:

这些是多重相关的问题，需要采取多种方法。为了确保结果的一致性，您通常应该采用交叉验证技术，以确保您的结果不是基于训练集和测试集的幸运分割。[参见这篇文章获得实践指导。](/complete-guide-to-pythons-cross-validation-with-examples-a9676b5cac12)

此外，对于预测模型，您可以进行反向测试，并评估如果在过去的某个时间点部署它，并且仅给出到该时间点的训练数据，性能会如何。

此外，在具有相似输入的完全不同的数据集上评估您的模型，以检查其在给定数据集之外的泛化能力，这是一个好主意。不过，重要的是，当您在现实世界中部署模型时，数据应该遵循与您的训练模型相同的分布。在任何其他情况下，性能都会不可预测地下降。

最后，监视已部署的模型并评估其在新数据上的性能始终是一个好的实践。在性能突然下降或漂移的情况下，这可能是模型需要重新训练的信号。当然，这也将取决于具体的应用。根据应用程序的不同，您可能有一个适当的再培训策略，每天、每周、每季度、每年等等都有一个新的模型。

## **4)评估指标**

构建人工智能系统的关键问题应该是“我们如何评估系统？”。最常见的指标之一是准确性。换句话说，无论你的模型在所有测试样本中预测正确与否。但是准确性是一个好的衡量标准吗？想想一个问题，你有 100 个女人，其中 10 个怀孕了。假设你有一些关于这些妇女的信息，你试图建立一个模型来预测谁怀孕了，谁没有。你这样做，你的模型有 85%的准确率。这是否意味着你有一个好的模型？另一方面，让我们假设你没有模型，你宁愿做的是预测所有女性都没有怀孕。令人惊讶的是，这个准确率高达 90%,因为你 100 次中有 90 次是正确的。这比你上面创建的实际模型更好吗？那么，我们使用什么指标，我们如何评估模型的性能？对于影响个人的决策，你会仅仅依靠准确性吗？

缓解措施:

答案显然是否定的。事实上，通常情况下，最好的方法是比较多个指标，并仔细检查混淆矩阵，以了解您的模型的优点和缺点。

因此，对于上述有 90%准确率的简单方法，F1 分数实际上是 0，因为没有真阳性(只有真阴性)。相反，您 85%准确性的模型实际上可能有 67%的 F1 值，这在特定应用中可能是可接受的，也可能是不可接受的。要寻找的其他度量是接收器操作特性(ROC)的曲线下面积(AUC)、精确度、召回率和特异性，仅举几个例子。

## **5)黑天鹅事件**

依靠历史数据来预测未来并不总是可行的。一个很好的例子是试图预测股票市场。由于多种原因，这在本质上是困难的。利用长期以来具有某种结果的数据，可以创造出在其历史范围内有效的模型。这意味着，如果你在一个没有市场崩溃的时期训练一个模型，这个模型是不可能预测到市场崩溃的。即使你在市场崩盘期间训练了它，由于事件的罕见性和缺乏指向该方向的明确信号，该模型仍然不太可能知道何时会发生崩盘。现在，想想在全球疫情时代做出影响个人决策的模型。因为所有的模型在过去都没有类似的数据，所以不太可能像在疫情之前那样准确地对个人做出决策。

缓解措施:

在这种情况下，为了在新的现实中操作，模型可能需要利用从新的情况中获取的数据进行重新训练。这可能会暂时奏效，直到行为再次转变到旧的标准。如果重新培训是不可能的，那么就不应该自动做出决定，因为它们很可能是错误的。这需要测试和验证。

总而言之，在我们的模型运行的当前假设下，预测黑天鹅事件是不可能的。对个人进行预测和决策，因为你知道你预测的数据与你训练的数据不遵循相同的分布，这样做是不负责任的。这并不是说模型不能作为咨询工具。除了“所有的模型都是错的，只有一些有用”——乔治·博克斯。

[1][https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri = CELEX:32016 r 0679&from = EN](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679&from=EN)

[2]https://arxiv.org/pdf/1710.08864.pdf