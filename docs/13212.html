<html>
<head>
<title>An Intuitive Guide to Auto-Encoders: Theory, Code and Visualization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器直观指南:理论、代码和可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-guide-to-auto-encoders-theory-code-and-visualization-3cc2a6a30d2c?source=collection_archive---------49-----------------------#2020-09-10">https://towardsdatascience.com/an-intuitive-guide-to-auto-encoders-theory-code-and-visualization-3cc2a6a30d2c?source=collection_archive---------49-----------------------#2020-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b4d0b28ffbe454aba46b26a81df99af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-Foet4BRO2DJl5R-PCF-w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://www.pexels.com/photo/ball-ball-shaped-blur-bubble-302743/" rel="noopener ugc nofollow" target="_blank">像素</a>上的<a class="ae kf" href="https://www.pexels.com/@pixabay" rel="noopener ugc nofollow" target="_blank"> pixabay </a>拍摄</p></figure><p id="ba47" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自动编码器是最有用的非监督算法之一，可以洞察数据，从而优化训练算法的学习算法。</p><h1 id="3ad5" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">什么是自动编码器？</h1><p id="f38a" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">自动编码器是一种神经网络，它将数据作为输入，将数据作为输出。起初，这可能看起来很荒谬:一个数和它本身之间的关系仅仅是 1，为什么神经网络是必要的？这是真的，但是自动编码器已经创造了一种方法来绕过它。</p><p id="e1b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用瓶颈。瓶颈，从其常见的定义来看，意味着玻璃瓶颈部的狭窄缝隙。在这种情况下，瓶颈是指减少隐藏层中神经元的数量。因此，这就迫使数据只能由更少数量的神经元来表示。默认情况下，网络将学习压缩和解压缩数据的方法，将数据编码和解码为更大和更小的数据表示。</p><h1 id="de62" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">为什么自动编码器如此有用？</h1><p id="ea99" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在一个数据集高达万亿字节的世界里，我们需要找到一种方法来有效地减少数据占用的内存。此外，处理信息的计算能力有限。由自动编码器创建的较小的表示变得如此重要，因为它占用的数据少得多。</p><p id="e0ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，通过我将向你展示的可视化技术，你实际上可以可视化每个神经元在寻找什么。</p><h1 id="5373" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">代码:</h1><p id="8879" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">既然自动编码器的理论已经很清楚了，我们可以继续看代码了。对于实际的自动编码器，我将使用 Keras，但是对于每一层所寻找的可视化，我将使用我自己的神经网络框架。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a55e" class="mq lf it mm b gy mr ms l mt mu">from keras.layers import Dense<br/>from keras.models import Sequential<br/>import requests<br/>import json</span></pre><p id="7c8b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了标准的 keras 层和模型，我还将使用 requests 和 json，通过 alpha-vantage API 访问财务数据。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="d586" class="mq lf it mm b gy mr ms l mt mu">API_KEY = 'XXXXXXX'<br/>from_symbol = 'EUR'<br/>to_symbol = 'USD'</span><span id="3c20" class="mq lf it mm b gy mv ms l mt mu">close_price = [<br/>r = requests.get(<br/>        '<a class="ae kf" href="https://www.alphavantage.co/query?function=FX_INTRADAY&amp;from_symbol='" rel="noopener ugc nofollow" target="_blank">https://www.alphavantage.co/query?function=FX_INTRADAY&amp;from_symbol='</a> +<br/>        from_symbol + '&amp;to_symbol=' + to_symbol +<br/>        '&amp;interval=1min&amp;outputsize=full&amp;apikey=' + API_KEY)<br/>jsondata = json.loads(r.content)<br/>pre_data = list(jsondata['Time Series FX (1min)'].values())<br/>fx_data = []<br/>for data in pre_data: <br/>    fx_data.append(list(data.values()))<br/>fx_data.reverse()<br/>for term in fx_data:<br/>    close_price.append(float(term[-1]))</span></pre><p id="67d2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我用来从 alpha-vantage 访问数据集的脚本。然而，要使用这个脚本，将 API 密钥更改为一个有效的密钥，您可以在这里得到<a class="ae kf" href="http://alphavantage.co" rel="noopener ugc nofollow" target="_blank"/>。为了简单起见，脚本也只选择收盘价。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a845" class="mq lf it mm b gy mr ms l mt mu">model = Sequential() <br/>model.add(Dense(10,input_shape = (None,10),activation = 'relu'))<br/>model.add(Dense(1))<br/>model.compile(optimizer = 'adam', loss = 'mse')<br/>model.fit(close_price,close_price,epochs = 100)</span></pre><p id="c3e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个脚本创建了模型，只是使用密集层。你可以使用 KL-Divergence，一种防止模型仅仅记忆输入数据的方法。它通过激活神经元的数量来惩罚网络，从而减少表示数据所需的神经元数量。</p><p id="917e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是完整的程序！简单吧？</p><h1 id="8e4c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">可视化:</h1><p id="f554" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">自动编码器的主要优势是能够可视化每个神经元在寻找什么。首先，将这个框架复制到你的程序中。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="f3b0" class="mq lf it mm b gy mr ms l mt mu">import numpy as np<br/>from matplotlib import pyplot as plt</span><span id="c372" class="mq lf it mm b gy mv ms l mt mu">def sigmoid(x):<br/>    return 1/(1+np.exp(-x))</span><span id="0679" class="mq lf it mm b gy mv ms l mt mu">def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))</span><span id="f230" class="mq lf it mm b gy mv ms l mt mu">def relu(x):<br/>    return np.maximum(x, 0)</span><span id="4e30" class="mq lf it mm b gy mv ms l mt mu">def relu_p(x):<br/>    return np.heaviside(x, 0)</span><span id="efb9" class="mq lf it mm b gy mv ms l mt mu">def tanh(x):<br/>    return np.tanh(x)</span><span id="3ff9" class="mq lf it mm b gy mv ms l mt mu">def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2</span><span id="0388" class="mq lf it mm b gy mv ms l mt mu">def flatten(itr):<br/>    t = tuple()<br/>    for e in itr:<br/>        try:<br/>            t += flatten(e)<br/>        except:<br/>            t += (e,)<br/>    return t</span><span id="77f6" class="mq lf it mm b gy mv ms l mt mu">def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)</span><span id="e624" class="mq lf it mm b gy mv ms l mt mu">class NeuralNetwork:<br/>    def __init__(self):<br/>        self.layers = []<br/>        self.weights = []<br/>        self.loss = []<br/>    def add(self,layer_function):<br/>        self.layers.append(layer_function)<br/>        <br/>    def initialize_weights(self):<br/>        for layer in self.layers:<br/>            index = self.layers.index(layer)<br/>            weights = layer.initialize_weights(self.layers,index)<br/>            self.weights.append(weights)<br/>            <br/>    def propagate(self,X):<br/>        As,Zs = [],[]<br/>        input_data = X<br/>        for layer in self.layers:<br/>            a,z = layer.propagate(input_data)<br/>            As.append(a)<br/>            Zs.append(z)<br/>            input_data = a<br/>        return As,Zs<br/>    <br/>    def train(self,X,y,iterations):<br/>        loss = []<br/>        for i in range(iterations):<br/>            As,Zs = self.propagate(X)<br/>            loss.append(sum(np.square(flatten(y - As[-1]))))<br/>            As.insert(0,X)<br/>            g_wm = [0] * len(self.layers)<br/>            for i in range(len(g_wm)):<br/>                pre_req = (y-As[-1])*2<br/>                a_1 = As[-(i+2)]<br/>                z_index = -1<br/>                w_index = -1<br/>                if i == 0:<br/>                    range_value = 1<br/>                else:<br/>                    range_value = 2*i<br/>                for j in range(range_value):<br/>                    if j% 2 == 0:<br/>                        pre_req = pre_req * sigmoid_p(Zs[z_index])<br/>                        z_index -= 1<br/>                    else:<br/>                        pre_req = np.dot(pre_req,self.weights[w_index].T)<br/>                        w_index -= 1<br/>                gradient = np.dot(a_1.T,pre_req)<br/>                g_wm[-(i+1)] = gradient<br/>                for i in range(len(self.layers)):<br/>                    self.layers[i].network_train(g_wm[i])<br/>        <br/>        return loss<br/>        <br/>    class Perceptron:<br/>        def __init__(self,nodes,input_shape= None,activation = None):<br/>            self.nodes = nodes<br/>            self.input_shape = input_shape<br/>            self.activation = activation<br/>        def initialize_weights(self,layers,index):<br/>            if self.input_shape:<br/>                self.weights = np.random.randn(self.input_shape[-1],self.nodes)<br/>            else:<br/>                self.weights = np.random.randn(layers[index-1].weights.shape[-1],self.nodes)<br/>            return self.weights<br/>        def propagate(self,input_data):<br/>            z = np.dot(input_data,self.weights)<br/>            if self.activation:<br/>                a = self.activation(z)<br/>            else:<br/>                a = z<br/>            return a,z<br/>        def network_train(self,gradient):<br/>            self.weights += gradient<br/>                <br/>model = NeuralNetwork()</span><span id="ec69" class="mq lf it mm b gy mv ms l mt mu">Perceptron = model.Perceptron</span></pre><p id="c6e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不会深入讨论框架的代码，但它本质上是 Keras 的透明版本，可以给出每一层的传播值。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="248d" class="mq lf it mm b gy mr ms l mt mu">X = np.array([[1, 1, 1, 1, 0],<br/>   [1, 1, 0, 0, 1],<br/>   [0, 0, 0, 1, 0],<br/>   [1, 0, 0, 0, 0],<br/>   [0, 0, 1, 1, 1],<br/>   [1, 1, 0, 0, 0],<br/>   [1, 0, 0, 1, 0],<br/>   [1, 0, 1, 1, 0],<br/>   [0, 0, 0, 0, 0],<br/>   [1, 1, 0, 0, 1]])</span><span id="70e4" class="mq lf it mm b gy mv ms l mt mu">model.add(Perceptron(2,input_shape = (None,5),activation = sigmoid))<br/>model.add(Perceptron(5,activation = sigmoid))</span><span id="a9c9" class="mq lf it mm b gy mv ms l mt mu">model.initialize_weights()<br/>loss = model.train(X,X,1000)</span></pre><p id="24d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个自动编码器仅用 MSE 训练了 1000 个时期。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="660d" class="mq lf it mm b gy mr ms l mt mu">As,Zs = model.propagate(X)<br/>full_set = []<br/>for a in range(len(As)):<br/>    a_set_values = []<br/>    for z in range(5):<br/>        zeroes =np.zeros(5)<br/>        zeroes[z] = 1<br/>        As,Zs = model.propagate(zeroes)<br/>        a_set_values.append(As[a])<br/>    full_set.append(a_set_values)<br/>full_set</span><span id="e34c" class="mq lf it mm b gy mv ms l mt mu">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="85b2" class="mq lf it mm b gy mv ms l mt mu">plt.subplot(212)<br/>for sets in full_set:<br/>    plt.imshow(sets, cmap='Greys',  interpolation='nearest')<br/>    plt.show()</span><span id="767d" class="mq lf it mm b gy mv ms l mt mu">full_set</span></pre><p id="370b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，用数据传播网络。由于这里的数据很简单，我们遍历数据中的每一项，将其更改为 1，并查看它如何更改数据的每一层。然后我们绘制它的黑白图像，看看效果</p><p id="087b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你看我的文章！</p><h1 id="7544" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">我的链接:</h1><p id="a665" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">如果你想看更多我的内容，点击这个<a class="ae kf" href="https://linktr.ee/victorsi" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">链接</strong> </a>。</p></div></div>    
</body>
</html>