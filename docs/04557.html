<html>
<head>
<title>Walking Through a Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">走过线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/walking-through-a-linear-regression-dca9942111e4?source=collection_archive---------6-----------------------#2020-04-23">https://towardsdatascience.com/walking-through-a-linear-regression-dca9942111e4?source=collection_archive---------6-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="32e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习过程的全面分解</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cdfa565e3cfe8bcdd4508bc172a32a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EoTJU-ub4XEJ3lii"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡·布拉沃在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6863" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">处理数据时，线性回归是基础算法。它被广泛使用，是现有的最适用的监督机器学习算法之一。通过执行线性回归，我们试图捕捉预测自变量(<strong class="lb iu"> X1，X2等)</strong>和预测因变量(<strong class="lb iu"> Y) </strong>之间的最佳线性关系。让我们一起来看看Python中的线性回归模型过程。包含所有代码和数据的存储库可以在<a class="ae ky" href="https://github.com/andrewcole33/carseat_sales_linreg" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="bf61" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">该设置</h1><p id="79a1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将使用一个模拟数据集来代表一家公司的汽车座椅销售。该公司希望他们的数据团队建立一个模型，在给定一个包含多个预测独立变量的数据集的情况下，该模型能够准确地捕捉和预测他们商店所在地的汽车座椅销售活动。变量描述如下:</p><ul class=""><li id="2f86" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">销售额:每个地点的单位销售额</strong></li><li id="92fc" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">CompPrice:在每个地点最接近的竞争对手收取的价格</li><li id="b74e" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">收入:社区收入水平</li><li id="150f" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">广告:公司在各地的本地广告预算</li><li id="d260" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">人口:该地区的人口规模(以千计)</li><li id="f89a" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">价格:每个站点汽车座位的收费价格</li><li id="1d16" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">搁置位置:现场搁置位置的质量(好|差|中)</li><li id="6c06" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">年龄:当地人口的平均年龄</li><li id="139c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">教育:每个地点的教育水平</li><li id="560a" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">城市:商店位于城市还是农村</li><li id="25ca" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">美国:无论商店是否在美国</li></ul><p id="8514" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">销售将是我们的因变量(或目标),其余10个变量(特征)将被处理和操纵，以协助我们的回归。在这一点上，注意我们的变量由什么组成是很重要的。我们有某些变量，如收入、广告或人口，这些都是基于整数的变量。然而，我们也有像Urban、USA和ShelveLoc这样的变量。这些变量的值代表分类值(是/否；好/差/中等；等等。).随着我们的进展，我们将会看到如何解释这些差异，在这个阶段，重要的是要注意到它们是不同的。(利用Python中的‘df . info()’命令将有助于识别变量值的构成。</p><p id="1ec5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们数据帧的快照:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/434c55d00eaf8eec58a1f12bb1b6dd5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnt-hNVHVqZfwl_2ASRYFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">df.head()</p></figure><h1 id="9f0c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预处理</h1><p id="6400" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">令人震惊的是，预处理阶段是我们在构建模型之前准备数据框架及其内容的阶段。在此阶段，我们将执行训练-测试-分割，使用编码处理上述分类变量，最后处理可能出现的任何缩放问题。</p><p id="6474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要做的第一件事是将数据框架分成目标系列和预测系列。x将是我们的独立变量的数据框架，而y将是我们的目标特征。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="03dd" class="nm lw it ni b gy nn no l np nq">X = df.drop('Sales', axis = 1)<br/>y = df.Sales</span></pre><p id="2fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经有了每个分割，我们可以执行训练-测试-分割。这一步对我们的机器学习模型至关重要。我们将数据分为“训练”组和“测试”组。训练集将用于我们的模型进行实际学习，我们的测试集将用于验证输出，因为我们已经知道这些值。为了让我们访问这个特性，我们必须从Sklearn导入它。我们有400个数据条目，这是一个足够大的数据量，所以我们将对我们的训练量和测试量分别使用70/30的比率(对于函数中的test_size参数)。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="96e9" class="nm lw it ni b gy nn no l np nq">from sklearn.preprocessing import train_test_split</span><span id="a625" class="nm lw it ni b gy nr no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 33)</span></pre><p id="4951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要进行一些快速清洁程序。让我们从训练数据集中删除任何重复值或缺失值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="ac60" class="nm lw it ni b gy nn no l np nq">X_train.drop_duplicates(inplace = True)<br/>X_train.dropna(inplace = True)</span></pre><h1 id="d1f5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">处理不同的数据类型</h1><p id="72f2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如前所述，区分数据集中的数据类型至关重要。这要求我们将数字数据(整数或连续数)与分类数据(二进制结果、用数字表示的位置等)分开。).至此，<strong class="lb iu">所有的</strong>数据类型都包含在我们的<em class="ns"> X_train中。</em></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8322" class="nm lw it ni b gy nn no l np nq">X_train.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/82ec9027fb57411f69fb051b028d6dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*aJDoEeGwHNcBMoipovxbvQ.png"/></div></figure><p id="9b3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在第三列中看到了数据类型，其中除了三个特性之外，所有特性的数据类型都是“int64”，这意味着我们的数据帧将该数据解释为整数。这些将被视为我们的数字数据。剩下的三个分类变量将被暂时排除。</p><h1 id="ccb0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数字数据</h1><p id="c1cf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">下面的代码块将选择所有数据类型的特性，不包括我们的分类数据“object”。第二行和第三行代码只是从我们的<em class="ns"> X_train </em>中带来了列标题。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7bfa" class="nm lw it ni b gy nn no l np nq">X_train_numerics = X_train.select_dtypes(exclude = 'object')<br/>X_train_cols = X_train.columns<br/>X_train_numerics.columns = X_train_cols</span></pre><p id="ac7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了验证这一点，下面左边是我们新的纯数值型数据集的数据信息。下图右侧是我们的数字数据帧的快照。</p><div class="kj kk kl km gt ab cb"><figure class="nu kn nv nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7c9f528bd0db1955410b6a2834484615.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*XpAON_Yf4rtK5YqEsQO8sw.png"/></div></figure><figure class="nu kn oa nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c96ed8b2acafc7e92c254b547136dbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*a75th5EF4YFVupnxpFLkig.png"/></div></figure></div><p id="426d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我们现在已经收集了所有的数字数据，但是我们的预处理工作还没有完成。现在我们必须了解每个数字序列包含什么。例如，“CompPrice”代表来自另一个竞争对手的美元数字，而“Education”代表每个地点的教育水平，但我们没有任何进一步的信息。人口代表(以千计)一个地区有多少人。即使这些都是数字数据的形式，我们也不能真正比较它们，因为单位都是不同的！我们如何处理这个难题？缩放！</p><h2 id="4dcc" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated"><strong class="ak">缩放我们的数据</strong></h2><p id="425d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">缩放将允许我们所有的数据转换成一个更正常的分布。我们将使用Sklearn的StandardScaler方法。该函数将通过移除平均值并缩放至单位方差来有效地标准化各自的特征。此函数将允许我们移除某个要素(比如CompPrice，由于其较大的值)对预测变量产生不正确影响的任何影响。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="22e0" class="nm lw it ni b gy nn no l np nq">from sklearn.preprocessing import StandardScaler<br/>from scipy import stats</span><span id="b2e4" class="nm lw it ni b gy nr no l np nq">ss = StandardScaler()</span><span id="95dd" class="nm lw it ni b gy nr no l np nq">X_train_numeric = pd.DataFrame(ss.fit_transform(X_train_numeric))<br/>X_train_numeric.set_index(X_train.index, inplace = True)</span><span id="b6c3" class="nm lw it ni b gy nr no l np nq">X_train_numeric.columns = X_numeric_cols<br/>X_train_numeric.head()</span></pre><p id="df66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们导入并实例化一个StandardScaler对象。然后，我们拟合并转换我们的数字数据帧，并将索引特征设置为等于我们的原始X_train的索引特征。这将确保我们对正确的数据条目进行操作。我们必须再次设置列名，因为它们在通过StandardScaler对象时被删除了。我们得到的数据如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/6aca4e5a50dd76cc75423c81cb36b301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BYloM_YANjAwg4S-Ea8_5w.png"/></div></div></figure><p id="3a46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们注意到一些事情。我们的数据值不再像以前那样了。我们已经去除了原始特征的任何和所有大小差异，它们现在都是一致的尺度，没有任何单个特征对我们的预测变量有不准确的影响。</p><h2 id="a16e" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated"><strong class="ak">去除异常值</strong></h2><p id="30b3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们处理数字数据的最后一步是移除任何异常值。异常值可能会导致要素数据的不准确表示，因为少量的输入值可能会对另一个变量产生不准确的影响。我们将通过使用“stats”包并过滤任何z得分值大于平均值2.5倍标准差的值来实现这一点。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c122" class="nm lw it ni b gy nn no l np nq">X_train_numeric = X_train_numeric[(np.abs(stats.zscore(X_train_numeric)) &lt; 2.5).all(axis = 1)]</span></pre><h1 id="b058" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类数据</h1><p id="6cf5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们已经准备好了数字数据，我们想处理分类数据。让我们把我们的分类数据隔离到它自己的数据框架中，看看我们在处理什么。这与使用numeric时的第一步相同，我们只需将“select_dtypes”方法中的参数从“exclude”更改为“include”。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="31ac" class="nm lw it ni b gy nn no l np nq">X_train_cat = X_train.select_dtypes(include = 'object')</span></pre><div class="kj kk kl km gt ab cb"><figure class="nu kn on nw nx ny nz paragraph-image"><img src="../Images/f3180385eb864dced1ec5534f41448b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*YoEBZYKUGA8M0nc9mDgAOw.png"/></figure><figure class="nu kn oo nw nx ny nz paragraph-image"><img src="../Images/eef5c45636d54b3e63ad210a58cd3066.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*yVfGbP2LCmFTAiXbkU6AuQ.png"/></figure></div><p id="39f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">右图向我们展示了我们的分类数据框架实际上看到了什么。如果我们的计算机能够理解如何解释“好”、“中等”或“是”，这将是非常酷的，但不幸的是，它还没有那么智能——我们将稍后保存NLP模型；).此时，我们需要通过一个称为编码的过程将这些值转换成我们的计算机可以理解的东西。</p><h2 id="cceb" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated">编码分类数据</h2><p id="2398" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们开始处理我们的城市和美国变量。这些变量都是二进制的(“是”或“否”)，所以我们想把它们用二进制值表达给我们的计算机，而不是“是”或“否”。我们将使用sklearn的LabelBinarizer函数来实现。要使用该函数，我们首先为每个特性实例化一个LabelBinarizer()对象，然后对每个特性执行fit_transform。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="b094" class="nm lw it ni b gy nn no l np nq">from sklearn.preprocessing import LabelBinarizer</span><span id="895a" class="nm lw it ni b gy nr no l np nq">urban_bin = LabelBinarizer()<br/>us_bin = LabelBinarizer()</span><span id="3d9b" class="nm lw it ni b gy nr no l np nq">X_train_cat.Urban = urban_bin.fit_transform(X_train_cat.Urban)<br/>X_train_cat.US = us_bin.fit_transform(X_train_cat.US)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0c7efeaa79e947034e728b4e58c8348d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*bszTA_K9Wjph2bLiD9ezWQ.png"/></div></figure><p id="9997" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到的数据帧现在用1表示“是”,用0表示“否”。现在让我们把注意力转向“ShelveLoc”特性。因为这个特性不再有二进制值集，所以我们必须使用不同类型的编码。我们将为每个值创建一个新的二进制变量，并删除原来的特性。这将允许计算机解释哪个变量具有“真”二进制值，然后给它分配一个整数。我们实际上是为每个值创建虚拟变量。这听起来有点令人困惑，但是让我们来看看它的实际应用。我们将使用熊猫。get_dummies()函数将每个值分配给一个虚拟变量。<em class="ns">注意:我们通过“drop_first = True”参数来避免我们的数据帧中的自相关，这是</em> <strong class="lb iu"> <em class="ns">基本的</em> </strong> <em class="ns">。</em></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="be0f" class="nm lw it ni b gy nn no l np nq">X_cat_prepped = X_train_cat.merge(pd.get_dummies(X_train_cat.ShelveLoc, drop_first=True), left_index=True, right_index=True)</span><span id="89d2" class="nm lw it ni b gy nr no l np nq">X_cat_prepped.drop('ShelveLoc', axis = 1, inplace=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/a02118f22c2cdf7bd6ab8d0fef1e5be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*vp_LB9Yy7Flu067HJiMS3A.png"/></div></figure><p id="0916" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以很容易地看到分类值选项是如何被转换成新的二进制变量供我们的计算机解释的。</p><p id="7e4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经完成了数字和分类数据类型的预处理，我们将它们合并回一个完整的准备好的数据帧。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="1b32" class="nm lw it ni b gy nn no l np nq">X_train_prep = pd.merge(X_cat_prepped, X_train_numeric, left_index = True, right_index = True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/8f6402d7c4d10b244df1ccca9dc06324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ox8MZ8MQJXLYM8lvzSFqg.png"/></div></div></figure><p id="dc6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的最后一步是设置我们的<em class="ns"> y_train </em>来包含正确的条目。请记住，当我们考虑到重复、缺失值和异常值时，我们在训练测试分割后从我们的<em class="ns"> X_train </em>中删除了几个条目。该步骤确保我们的<em class="ns"> y_train </em>与我们的<em class="ns"> X_train </em>长度相等且准确。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="054a" class="nm lw it ni b gy nn no l np nq">y_train = y_train.loc[X_train_prep.index]</span></pre><h1 id="1286" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">建立线性回归模型</h1><p id="8109" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们已经处理了模型的本质，即预处理。一旦我们有了准备好的数据，构建实际的线性回归模型就相当简单了。我们将首先导入一个线性回归模型，并从Sklearn实例化它。然后，我们将<em class="ns"> X_train_prep </em>和<em class="ns"> y_train </em>拟合到我们的线性回归对象。最后，我们将利用。predict()方法来预测我们未来的值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="352d" class="nm lw it ni b gy nn no l np nq">from sklearn.linear_model import LinearRegression</span><span id="32ab" class="nm lw it ni b gy nr no l np nq">lr = LinearRegression()</span><span id="7664" class="nm lw it ni b gy nr no l np nq">lr.fit(X_train_prep, y_train)</span><span id="130f" class="nm lw it ni b gy nr no l np nq">y_hat_train = lr.predict(X_train_prep)</span></pre><p id="5090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模型现在已经完全建立和预测好了。为了了解我们的模型表现如何，我们将利用两个指标:一个<strong class="lb iu"> r平方值</strong>和<strong class="lb iu">均方根(rmse)。</strong></p><ul class=""><li id="b289" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu"> R平方:</strong>数据与我们拟合的回归线接近程度的统计度量(也称为“拟合优度”)。范围是[0，1]，其中r2 = 1.00意味着我们的模型解释了响应数据围绕其平均值的所有可变性，而r2 = 0意味着我们的模型解释不了任何可变性。<strong class="lb iu">R2越高，模型越好。</strong></li><li id="f0ce" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><strong class="lb iu"> RMSE: </strong>预测值向量和观测值向量之间的归一化相对距离。“小”值表示误差项更接近预测的回归线，因此证明模型更好。<strong class="lb iu">RMSE越低，模型越好。</strong></li></ul><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2ded" class="nm lw it ni b gy nn no l np nq">from sklearn.metrics import r2_score, mean_squared_error</span><span id="529c" class="nm lw it ni b gy nr no l np nq">print(f"r^2: {r2_score(y_train, y_hat_train)}")<br/>print(f"rmse: {np.sqrt(mean_squared_error(y_train, y_hat_train))}")</span></pre><p id="0150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述函数将通过我们的训练集和预测集(<em class="ns"> y_hat_train </em>)来建立模型性能的度量分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a927c5820621763d55a7d73dfdc163df.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*MdPBunf3_NirsPHvUrPLrQ.png"/></div></figure><p id="e4bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的两个指标看起来都非常好！我们的r2值0.88告诉我们，我们的模型已经捕获了我们的平均投影周围的88.6%的可变性。对于我们的RMSE，我们可能需要多个模型进行比较，因为它们是一个相对的衡量标准。如果我们运行另一个模型并返回RMSE = 1.5，我们的第一个模型将是两个模型中较好的。</p><h1 id="9e6d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">线性回归假设</h1><p id="5fcd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当执行OLS线性回归时，在模型建立和预测之后，我们剩下最后一步。这些回归有一些模型假设，如果它们不满足，那么我们的结果就不能被认为是稳健和可信的。这些假设之所以存在，是因为OLS是一种<strong class="lb iu">参数</strong>技术，这意味着它使用从数据中学到的参数。这使得我们的基础数据必须满足这些假设。</p><h2 id="aed8" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated"><strong class="ak"> 1。线性度</strong></h2><p id="c918" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">线性假设要求我们的目标变量(销售额)与其预测变量之间存在线性关系。试图将非线性数据集拟合到线性模型会失败。最好用散点图来测试。由于离群值的存在会产生重大影响，因此删除离群值对于这一假设非常重要。因为这个数据集是根据线性假设随机生成的，所以我们不需要为我们的数据测试这个假设。</p><h2 id="2b55" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated">2.常态</h2><p id="736b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正态假设要求模型<strong class="lb iu">残差</strong>应遵循正态分布。检验这一假设最简单的方法是通过直方图或分位数-分位数图(Q-Q-Plot)。</p><ul class=""><li id="7337" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu"> QQ图</strong>:通过绘制两个概率分布的分位数来比较它们的概率图。这张图有助于我们了解样本是否呈正态分布。该图应显示一条正直线，剩余点在直线上。如果我们的残差线有失真，那么我们的数据表明是非正态分布。</li></ul><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4eb5" class="nm lw it ni b gy nn no l np nq">from statsmodels.graphics.gofplots import qqplot</span><span id="97a6" class="nm lw it ni b gy nr no l np nq">qqplot(residuals, line = 'q')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7089cb55de1090e60dfbc2668d84ddb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*qGkEbI7Y3_9IEUzcXcHNng.png"/></div></figure><p id="f1dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在上面看到的，蓝色的观测点始终以线性模式下降，向我们证明了我们的残差是正态分布的。这个假设现在得到了验证。</p><h2 id="42e1" class="nm lw it bd lx ob oc dn mb od oe dp mf li of og mh lm oh oi mj lq oj ok ml ol bi translated">3.同方差性</h2><p id="49fb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">同方差性表示因变量在自变量的值之间具有相等的方差。我们的残差在回归线上应该是相等的。如果不满足这一假设，我们的数据将是异方差的，我们的变量散点图将是圆锥形的，表明我们的独立变量值的方差不相等。要查看我们的残差:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8694" class="nm lw it ni b gy nn no l np nq">residuals = y_hat_train - y_train</span><span id="6092" class="nm lw it ni b gy nr no l np nq">plt.scatter(y_hat_train, residuals)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/3c7bc908d7d3ee2077f4163ad20ed2da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z56sv8pPf5q3L_F71G5OAQ.png"/></div></div></figure><p id="2086" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们没有看到任何类似圆锥曲线的形状，也没有证据表明所有值的方差不相等，所以我们可以说这个假设也得到验证。</p><h1 id="e73e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">总结</h1><p id="26b8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们现在已经完成了线性回归的整个过程。希望这个笔记本可以为一个非常强大和适应性强的机器学习模型提供一个良好的开端。同样，<a class="ae ky" href="https://github.com/andrewcole33/carseat_sales_linreg" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>可用于所有代码。在我的下一篇博客中，我将通过一个非常强大的数据科学工具——特征工程来改进这个模型！</p></div></div>    
</body>
</html>