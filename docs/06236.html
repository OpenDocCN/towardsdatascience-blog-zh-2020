<html>
<head>
<title>Effects of Outliers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">异常值的影响</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/effect-of-outliers-in-classification-ed7e8b6d39f8?source=collection_archive---------43-----------------------#2020-05-19">https://towardsdatascience.com/effect-of-outliers-in-classification-ed7e8b6d39f8?source=collection_archive---------43-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/04ebb156de1979303b146ab698379c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1pbuz7eSWAxuTvls"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">威尔·梅尔斯在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="d054" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="5aab" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当我们开始处理数据时，我们(通常总是)观察到数据中几乎没有错误，比如缺失值、异常值、没有适当的格式等等。简而言之，我们称之为不一致性。这种一致性或多或少会扭曲数据，并妨碍机器学习算法进行正确预测。</p><p id="156f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在本文中，我们将尝试了解异常值如何影响机器学习算法的准确性，以及缩放如何帮助或影响我们的学习。为了简化目标，我们使用了两种非参数算法，k-NN和决策树。</p><h1 id="c3b1" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">关于数据集</h1><p id="d765" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将使用从UCI机器学习库获得的埃及患者的丙型肝炎病毒(HCV)数据集。可从以下渠道获得:</p><blockquote class="mh mi mj"><p id="1e8a" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated"><a class="ae kf" href="http://archive.ics.uci.edu/ml/datasets/Hepatitis+C+Virus+%28HCV%29+for+Egyptian+patients" rel="noopener ugc nofollow" target="_blank">http://archive . ics . UCI . edu/ml/datasets/肝炎+丙型+病毒+% 28 HCV % 29+for+埃及+患者</a></p></blockquote><p id="eb07" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">该数据包含了接受了约18个月HCV治疗剂量的埃及患者。共有1385名患者具有29种属性。这些属性包括年龄、白细胞计数、红细胞计数、血小板计数等。</p><h1 id="c316" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">处理数据</h1><p id="c412" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首要的事情是用python加载数据和所需的库。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c18a" class="mx kh it mt b gy my mz l na nb">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline<br/>pd.set_option('display.max_columns', None) # This will help yoy view all the columns rather than the sample when using <em class="mk">dataframe.head()</em></span><span id="d105" class="mx kh it mt b gy nc mz l na nb">df = pd.read_csv('HCV-Egy-Data.csv')<br/>df.head()</span></pre><p id="ad03" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一旦我们通过数据集，它几乎建议检查是否有任何我们前面提到的不一致。为此，我们将使用python的info函数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8bad" class="mx kh it mt b gy my mz l na nb">df.info()</span></pre><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/498c2fc5616cf3544a08a41acf158ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*589VrEj-BhAjARSQqLjcjQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">df.info()结果</p></figure><p id="23a1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这里，我们观察到我们没有任何丢失的值，并且由于数据是数字数据，我们可以确定所有的属性值都是数字的，即int64或float64类型。此外，没有空值，因此我们可以使用我们的数据来建模。</p><p id="6592" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们还想看看是否有异常值，在pandas库中的一个快速检查是使用<em class="mk"> describe() </em>函数。它为我们提供了所需的统计数据，如<em class="mk">最小-最大值、分位数、标准偏差等。</em></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="1c73" class="mx kh it mt b gy my mz l na nb">df.describe()<br/></span></pre><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3c1e491f20e94a4e4c1b6e1eaa118838.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*XOkZjdyUql4dDKSYOfTzJQ.png"/></div></figure><p id="1a67" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这里，我们观察到RNA EOT的最小值为5，这与我们的<em class="mk">平均值相差甚远。</em>现在我们确定至少有一个异常值。因此，我们将看到如何处理异常值影响我们的模型。</p><h2 id="44aa" class="mx kh it bd ki nf ng dn km nh ni dp kq lp nj nk ku lt nl nm ky lx nn no lc np bi translated">目标变量:基线组织学分期</h2><blockquote class="mh mi mj"><p id="945a" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated">一点背景，如果活检获得的组织足够大，观察者之间的一致是非常好的，特别是对于纤维化。为了帮助标准化病理学家之间的组织学评估，特别是提高从事不同临床试验的不同研究者之间的组织学测量的客观性，几个小组已经提出了用于分级活动和分期纤维化的组织学方案。在Metavir系统中，将坏死性炎症活性分级为0-3级。对于与疾病阶段的预处理评估问题更相关的量化纤维化，HAI、改良HAI和Metavir系统依赖于0至4或0至6的标度。</p></blockquote><p id="0438" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们将使用Metavir系统，因为它给了我们4个多级分类，并取消了“基线组织学分级”,因为它们符合相同的逻辑。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="77a6" class="mx kh it mt b gy my mz l na nb">df = df.drop('Baseline histological Grading',axis=1)</span></pre><h1 id="40b6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">建模</h1><p id="11a0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于我们的数据在不同的尺度上，我们执行了缩放以将每个属性放在一个共同的尺度上。这将进一步减少异常值的影响。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="44d5" class="mx kh it mt b gy my mz l na nb">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scaler.fit(df.drop('Baselinehistological staging',axis=1))<br/>scaled_features = scaler.transform(df.drop('Baselinehistological staging',axis=1))<br/>df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])<br/>df_feat.head()</span></pre><p id="b3ec" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们将利用scikit-learn库对我们的数据进行拆分和建模。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b582" class="mx kh it mt b gy my mz l na nb">from sklearn.model_selection import train_test_split<br/>X = df.drop('Baselinehistological staging',axis=1)<br/>y = df['Baselinehistological staging']<br/>X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['Baselinehistological staging'],<br/>                                                    test_size=0.20)</span><span id="13c2" class="mx kh it mt b gy nc mz l na nb"># here test_size = 20 tells us that the data is split into train 80/20 ratio. </span></pre><h1 id="ee6a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak"> K-最近邻(k-NN) </strong></h1><p id="4396" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一旦我们将数据分成训练和测试数据集，我们就可以运行k = 1或(n_neighbors = 1)的k-NN算法，并检查数据的准确性。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="eb82" class="mx kh it mt b gy my mz l na nb">from sklearn.neighbors import KNeighborsClassifier<br/>knn = KNeighborsClassifier(n_neighbors=1)<br/>knn.fit(X_train,y_train)<br/>pred = knn.predict(X_test)<br/>from sklearn.metrics import classification_report,confusion_matrix<br/>print(confusion_matrix(y_test,pred))<br/>print(classification_report(y_test,pred))</span></pre><p id="6371" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">该模型的精确度为24%,总体来说很差，但这不是我们的目标。我们可以运行肘法来选择最佳的<em class="mk"> k </em>值。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="5d65" class="mx kh it mt b gy my mz l na nb">error_rate = []</span><span id="074d" class="mx kh it mt b gy nc mz l na nb"># Will take some time<br/>for i in range(1,40,2):<br/>    <br/>    knn = KNeighborsClassifier(n_neighbors=i)<br/>    knn.fit(X_train,y_train)<br/>    pred_i = knn.predict(X_test)<br/>    error_rate.append(np.mean(pred_i != y_test))</span><span id="b630" class="mx kh it mt b gy nc mz l na nb">plt.figure(figsize=(10,6))<br/>plt.plot(range(1,40,2),error_rate,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)<br/>plt.title('Error Rate vs. K Value')<br/>plt.xlabel('K')<br/>plt.ylabel('Error Rate')</span></pre><p id="00cf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">其中我们看到最小误差率为k = 35，精度为31 %。</p><h1 id="c028" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">决策树</h1><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0faf" class="mx kh it mt b gy my mz l na nb"># decision Tree classifier <br/>from sklearn.tree import DecisionTreeClassifier<br/>dtree = DecisionTreeClassifier()<br/>dtree.fit(X_train,y_train)<br/>pred = dtree.predict(X_test)</span><span id="b6be" class="mx kh it mt b gy nc mz l na nb">print(confusion_matrix(y_test,pred))<br/>print(classification_report(y_test,pred))</span></pre><p id="a3e7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">运行上述代码，我们观察到准确率为25 %,未能超过k-NN。</p><h1 id="719f" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">移除异常值</h1><p id="f4ed" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将移除超出数据的2 %和98 %的异常值。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ec75" class="mx kh it mt b gy my mz l na nb"># Remove Outliers <br/>df1 = df<br/>col = list(df1.drop('Baselinehistological staging',axis=1).columns)<br/>for i in col:<br/>    y = df[i]<br/>    removed_outliers = y.between(y.quantile(.02), y.quantile(.98))<br/>    index_names = df[~removed_outliers].index <br/>    df.drop(index_names, inplace=True)</span></pre><p id="edb6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们观察到k-NN在k =1时的准确度为28%,提高了约16%。类似地，在k = 21时，给出了33%,这大约增加了6%,但是，对于决策树，我们的准确度几乎降低了4%。</p><h1 id="e377" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="6333" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以得出结论，去除异常值增加了模型的准确性。尽管它在k-NN中显著提高了准确性，但在决策树中却降低了。这就引出了我们分析的下一步，即参数调整。我们将深入研究参数调整，以实现更高的精度。</p><h1 id="b9f2" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h1><ul class=""><li id="7dae" class="nq nr it lg b lh li ll lm lp ns lt nt lx nu mb nv nw nx ny bi translated"><a class="ae kf" href="https://aasldpubs.onlinelibrary.wiley.com/doi/pdf/10.1002/hep.1840360720" rel="noopener ugc nofollow" target="_blank">https://aasldpubs . online library . Wiley . com/doi/pdf/10.1002/hep . 1840360720</a></li><li id="b578" class="nq nr it lg b lh nz ll oa lp ob lt oc lx od mb nv nw nx ny bi translated">Dua d .和Graff c .(2019年)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</li></ul></div></div>    
</body>
</html>