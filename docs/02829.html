<html>
<head>
<title>Complete Guide to Advanced CNNs in Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 2中的高级CNN完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-the-standard-cnn-in-tensorflow-2-a7562d25ca2d?source=collection_archive---------29-----------------------#2020-03-18">https://towardsdatascience.com/beyond-the-standard-cnn-in-tensorflow-2-a7562d25ca2d?source=collection_archive---------29-----------------------#2020-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7826" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用复杂的架构生成更深层次的模型，并了解不同的层应该使模型更好。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/380909dd1f08205dce73b590461da4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dPl54MxKG-ulRDmA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="08c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我构建的第一个ConvNet模型是为了识别狗或猫，它具有一个序列模型的基本结构，该模型具有Conv2d层，以及这里和那里的批处理规范化和最大池层的混合，并且不要忘记一个丢弃层以对抗过度拟合😏。所有这一切之后是平坦层，最终导致致密层。使用add将它们添加到模型中。它工作得很好，给出了足够好的精度。然后我尝试了著名的MNIST数据集，并使用了相同的架构。我所要做的就是将我的密集层中的损失从二元交叉熵改变为分类交叉熵，tada我觉得我已经掌握了CNN。但是，孩子，我错了！还有更多的东西，比如用不同类型的Conv2d层构建稀疏连接的架构，应用不同的过拟合技术等等。</p><h1 id="2476" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目录</h1><ol class=""><li id="81e1" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">高级Conv2D图层</strong></li></ol><ul class=""><li id="dbfd" class="mn mo it lb b lc ld lf lg li my lm mz lq na lu nb mv mw mx bi translated">深度可分卷积</li><li id="c184" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated">扩张的回旋</li></ul><p id="268b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu">计数器过拟合</strong></p><ul class=""><li id="abea" class="mn mo it lb b lc ld lf lg li my lm mz lq na lu nb mv mw mx bi translated">空间缺失</li><li id="9828" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated">高斯漏失</li><li id="48cf" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated">活动正规化</li></ul><p id="f9ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.<strong class="lb iu">复杂架构</strong></p><ul class=""><li id="f6c1" class="mn mo it lb b lc ld lf lg li my lm mz lq na lu nb mv mw mx bi translated">稀疏连接的架构</li><li id="f8c1" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated">跳过连接</li></ul><p id="dbbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.<strong class="lb iu">结论</strong></p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="a557" class="lv lw it bd lx ly no ma mb mc np me mf jz nq ka mh kc nr kd mj kf ns kg ml mm bi translated">高级Conv2D层</h1><h2 id="98c4" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">深度可分卷积</h2><p id="2d5f" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">它们效率更高，需要更少的内存和更少的计算，在某些情况下甚至可以提供更好的结果。由于这些特征，当我们的模型必须部署在边缘/物联网设备上时，通常使用它们<em class="oi">,因为它们的CPU和RAM有限。它将正常卷积层的过程分为两个过程，即<em class="oi">深度方向卷积和</em>点方向卷积。在深度方向卷积中，内核一次迭代一个通道。逐点卷积使用1x1内核来增加通道数量。这样就减少了所需的乘法总数，从而使我们的网络速度更快。这是一篇很棒的<a class="ae ky" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728">文章</a>来了解更多。</em></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="719c" class="nt lw it ok b gy oo op l oq or">tensorflow.keras.layers.SeparableConv2D(32, (3, 3), padding="same"))</span></pre><h2 id="0f22" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">扩张的回旋</h2><p id="3f7e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">可以在正常卷积层以及深度方向可分离的卷积层中实现扩展卷积。这是一个带有间隙的正常卷积运算。除了提供更大的感受域、高效的计算和更少的内存消耗，它还<em class="oi">保持了数据</em>的分辨率和顺序。因此，它通常会提高模型的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/bbee2e7a878a519699155b59e594e193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PXb0Zo6CMVBkTuHS8woIHA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片取自<a class="ae ky" href="http://www.erogol.com/dilated-convolution/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></figure><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1b21" class="nt lw it ok b gy oo op l oq or">#In Normal Conv2D<br/>tf.keras.layers.Conv2D(32, (3, 3), padding='same', dilation_rate=(2, 2))</span><span id="3385" class="nt lw it ok b gy ot op l oq or">#In Seperable Conv2D<br/>tf.keras.layers.SeparableConv2D(no_of_filters, (3, 3), padding='same', dilation_rate=(2, 2))</span></pre><h1 id="547a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">反过度拟合</h1><h2 id="0d90" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">空间缺失</h2><p id="84ad" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在正常退出中，一些随机选择的神经元在训练期间被忽略。这些神经元在前向传递中没有贡献，并且它们的权重在后向传递中没有更新。这导致网络学习多个独立的内部表示，并使其不太可能过度适应训练数据。<em class="oi">在空间删除中，我们没有删除神经元，而是删除了整个特征图</em>。如<a class="ae ky" href="https://keras.rstudio.com/reference/layer_spatial_dropout_2d.html" rel="noopener ugc nofollow" target="_blank">空间文档的Keras文档所述</a>:</p><blockquote class="ou ov ow"><p id="3315" class="kz la oi lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">如果特征图中的相邻像素是强相关的(在早期卷积层中通常是这种情况),那么常规的丢弃不会使激活正则化，否则只会导致有效学习率的降低。在这种情况下，空间下降将有助于提高要素地图之间的独立性，因此应改为使用空间下降。</p></blockquote><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="2d56" class="nt lw it ok b gy oo op l oq or">tf.keras.layers.SpatialDropout2D(0.5)</span></pre><h2 id="c69a" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">高斯漏失</h2><p id="e60b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">它是压差和高斯噪声的组合。这意味着这一层连同丢弃的一些神经元也应用了以1为中心的乘法高斯噪声。和正常退学者一样，它也采用论证率。来自其<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianDropout" rel="noopener ugc nofollow" target="_blank">文档</a>:</p><blockquote class="ou ov ow"><p id="a68c" class="kz la oi lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">Float，drop概率(和dropout一样)。乘法噪声将具有标准差sqrt(rate / (1 — rate))。</p></blockquote><p id="702f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于它的更深入的阅读，你可以参考这篇文章<a class="ae ky" href="https://medium.com/@maksutov.rn/deep-study-of-a-not-very-deep-neural-network-part-5-dropout-and-noise-29d980ece933" rel="noopener">这里</a>。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="77ba" class="nt lw it ok b gy oo op l oq or">tf.keras.layers.GaussianDropout(0.5)</span></pre><h2 id="0f14" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">活动正规化</h2><p id="24b0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">正则化会对网络进行细微的更改，以便更好地进行概化。它鼓励神经网络<em class="oi">学习稀疏特征或原始观察值的内部表示</em>，这使得模型在看不见的数据上表现得更好。支持三种类型的正则化技术:</p><ul class=""><li id="96c4" class="mn mo it lb b lc ld lf lg li my lm mz lq na lu nb mv mw mx bi translated"><strong class="lb iu"> l1 </strong>:活动计算为绝对值之和。</li><li id="62dc" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated"><strong class="lb iu"> l2 </strong>:活动计算为平方值的总和。</li><li id="3c62" class="mn mo it lb b lc nc lf nd li ne lm nf lq ng lu nb mv mw mx bi translated"><strong class="lb iu"> l1_l2 </strong>:活动被计算为绝对值之和以及平方值之和。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/9e29df731e8bb350f6e8b4d7127f6b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*nTOtemYBWGIUESA4WUlkLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">L1和L2正则化公式。λ是正则化参数。图片取自<a class="ae ky" href="https://www.google.com/url?sa=i&amp;url=http%3A%2F%2Flaid.delanover.com%2Fdifference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow%2F&amp;psig=AOvVaw2-BdRWRvBXmAsVNFdu14WF&amp;ust=1584432566455000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCOC__rjFnugCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></figure><p id="4faa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于正则化项的增加，权重矩阵的值降低，并且导致减少过拟合的更简单的模型。有关正则化技术的更多信息，请参考此处的<a class="ae ky" href="https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-ker" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="48fa" class="nt lw it ok b gy oo op l oq or">#L1 regularization<br/>tf.keras.layers.ActivityRegularization(l1=0.001)</span><span id="2781" class="nt lw it ok b gy ot op l oq or">#L2 regularizaton<br/>tf.keras.layers.ActivityRegularization(l2=0.001)</span><span id="61ce" class="nt lw it ok b gy ot op l oq or">#L1_L2 regularization<br/>tf.keras.layers.ActivityRegularization(l1=0.001, l2=0.001)</span></pre><h1 id="1759" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">复杂架构</h1><h2 id="c810" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">稀疏连接的架构</h2><p id="72a9" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">有两种类型的架构，密集连接和稀疏连接。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/1dc02b5bc50f0a1a6b731290bf633d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPcHHnLQkSLDix-VrcxMgg.jpeg"/></div></div></figure><p id="91ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它因<a class="ae ky" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank"> Inception </a>网络而流行起来。那么对这些类型的架构有什么需求呢？难道我们不能继续增加层次，网络越深，结果越好吗？一点也不。<em class="oi">模型越大，就越容易过度拟合</em> <em class="oi">，尤其是对于较小的数据集，这也增加了训练模型所需的计算能力</em>。<em class="oi">有时甚至训练误差甚至变得更严重。</em>稀疏连接的架构有助于我们增加模型的深度和宽度，同时不会超出所需的计算能力。它们可以具有不同大小的卷积核，这在测试对象大小不同时也有助于建模。</p><p id="f151" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般我们用<code class="fe pc pd pe ok b">tf.keras.layers.add</code>做密集连接的架构。我们可以使用<code class="fe pc pd pe ok b">tf.keras.utils.plot_model</code>来可视化我们的模型。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="3b2b" class="nt lw it ok b gy oo op l oq or">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Conv2D, add, Input<br/>from tensorflow.keras.utils import plot_model</span><span id="f71b" class="nt lw it ok b gy ot op l oq or">model = Sequential()<br/>model.add(Conv2D(64, (3, 3), activation = 'relu', padding='same', input_shape=(128, 128, 3)))<br/>model.add(Conv2D(64, (3, 3), activation = 'relu', padding='same'))<br/>model.add(Conv2D(128, (3, 3), activation = 'relu', padding='same'))<br/>plot_model(model)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/9002845b9d86ce4ff5274447eef82ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*Kpv52rdR724Kel_ZHwerVQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制的模型</p></figure><p id="cc00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以创建这样的模型，方法是将它们分配给变量，并将它们的变量名放在它最后连接的层上。输入形状在<code class="fe pc pd pe ok b">tf.keras.layers.Input</code>中指定，<code class="fe pc pd pe ok b">tf.keras.models.Model</code>用于强调输入和输出，即我们模型的第一层和最后一层。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1ccd" class="nt lw it ok b gy oo op l oq or">from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Conv2D, Input<br/>from tensorflow.keras.utils import plot_model</span><span id="1499" class="nt lw it ok b gy ot op l oq or">input_flow = Input(shape=(128, 128, 3))<br/>x = Conv2D(64, (3, 3), activation = 'relu', padding='same')(input_flow)<br/>x = Conv2D(64, (3, 3), activation = 'relu', padding='same')(x)<br/>x = Conv2D(128, (3, 3), activation = 'relu', padding='same')(x)<br/>model = Model(inputs=input_flow, outputs=x)<br/>plot_model(model)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/f528b9c0ef62612ab8fe529e8fffb690.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*ZO3YDVYnOL5o6uTkGZciFg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制的模型</p></figure><p id="c528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，要构建稀疏连接的架构，我们只需要将一层分配给多层，然后使用<code class="fe pc pd pe ok b">tf.keras.layers.concatenate</code>将它们连接起来。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="a8e8" class="nt lw it ok b gy oo op l oq or">from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Conv2D, Input, concatenate<br/>from tensorflow.keras.utils import plot_model</span><span id="a351" class="nt lw it ok b gy ot op l oq or">input_flow = Input(shape=(128, 128, 3))<br/>x = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3))(input_flow)<br/>one = Conv2D(64, (3, 3), activation='relu', padding='same')(x)<br/>two = Conv2D(64, (5, 5), activation='relu', padding='same')(x)<br/>three = Conv2D(64, (7, 7), activation='relu', padding='same')(x)<br/>x = concatenate([one, two, three])<br/>x = Conv2D(128, (3, 3), activation = 'relu', padding='same')(x)<br/>model = Model(inputs=input_flow, outputs=x)<br/>plot_model(model)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/f4e1ba890b0dcf334825e8fcff18931a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*wKpYOo69ihzAo_8a2UOwnQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型已绘制。</p></figure><h2 id="90f1" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">跳过连接</h2><p id="261b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">这被网络<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>推广开来。跳过连接背后的主要思想是解决微软研究院在他们的论文<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">图像识别的深度剩余学习</a>中给出的这个问题。</p><blockquote class="ou ov ow"><p id="a70b" class="kz la oi lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">当更深的网络能够开始收敛时，一个<em class="it">退化</em>问题就暴露出来了:随着网络深度的增加，精确度达到饱和(这可能不足为奇)，然后迅速退化。出乎意料的是，这种退化<em class="it">不是由过度拟合</em>引起的，并且向适当深度的模型添加更多的层导致<em class="it">更高的训练误差……</em></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/772b5b660c9f3ad226107963503ea5ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*7CvoJmdqQKV_VCu3F3QdBg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像取自此处的<a class="ae ky" href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fkharshit.github.io%2Fimg%2Fresnet_block.png&amp;imgrefurl=https%3A%2F%2Fkharshit.github.io%2Fblog%2F2018%2F09%2F07%2Fskip-connections-and-residual-blocks&amp;tbnid=YqX4hhjHTgLN5M&amp;vet=12ahUKEwii846BlqLoAhWCHysKHfEuDmkQMygAegUIARDdAQ..i&amp;docid=vDE83FK50zkAbM&amp;w=574&amp;h=312&amp;q=skip%20connections&amp;ved=2ahUKEwii846BlqLoAhWCHysKHfEuDmkQMygAegUIARDdAQ" rel="noopener ugc nofollow" target="_blank"/>。</p></figure><p id="f3ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它的优点是可以更容易地估计权重的良好值，并且模型可以更好地概括。</p><p id="6e72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你了解如何构建稀疏连接的架构，你可以很容易地猜到这是如何工作的，但是在这种情况下，为了连接它们，我们将使用<code class="fe pc pd pe ok b">tf.keras.layers.Add</code>。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b8a1" class="nt lw it ok b gy oo op l oq or">from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Conv2D, Input, Add<br/>from tensorflow.keras.utils import plot_model</span><span id="d9c9" class="nt lw it ok b gy ot op l oq or">input_flow = Input(shape=(128, 128, 3))<br/>x = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3))(input_flow)<br/>one = Conv2D(64, (3, 3), activation='relu', padding='same')(x)<br/>two = Conv2D(64, (5, 5), activation='relu', padding='same')(one)<br/>three = Conv2D(64, (7, 7), activation='relu', padding='same')(two)<br/>x = Add()([x, three])<br/>x = Conv2D(128, (3, 3), activation = 'relu', padding='same')(x)<br/>model = Model(inputs=input_flow, outputs=x)<br/>plot_model(model)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ac54dbd0d18a857cf102d7cfb4c737ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*oKxy7Hkuy7-7aXCyU_xRpQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制的模型</p></figure><h1 id="2c57" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="7aae" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">使用这些技术可能会改进我们的模型，但情况并非总是如此。它们的性能甚至可能比普通Conv2D层的简单密集连接架构更差。你需要试着看看哪种策略能产生最好的结果。</p><p id="af84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于修订的目的，我使用上面提供的概念创建了一个模型。这只是为了修改的目的，我不保证这个模型会给出更好的结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pk pl l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/09ce2be575246b770ae65b54fec67ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRgfSca-Bv9d45nZb1MuTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">绘制的模型</p></figure></div></div>    
</body>
</html>