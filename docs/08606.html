<html>
<head>
<title>Do Decision Trees need Feature Scaling?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树需要特征缩放吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-decision-trees-need-feature-scaling-97809eaa60c6?source=collection_archive---------26-----------------------#2020-06-22">https://towardsdatascience.com/do-decision-trees-need-feature-scaling-97809eaa60c6?source=collection_archive---------26-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/176998814dad7fd9c54503f6e71df745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o76A4qNIkr-rPa6xJKRzDg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">亚里沙·戈洛文斯卡的照片</p></figure><div class=""/><div class=""><h2 id="9825" class="pw-subtitle-paragraph kh jj jk bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">决策树和集成方法需要特征缩放吗？</h2></div><p id="d2b1" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习算法从一开始就一直走在进化的道路上。今天，这个领域已经从数学建模发展到集合建模等等。这种进化已经看到了更强大的 SOTA 模型，几乎弥合了人类和人工智能潜在能力之间的差距。集合模型给了我们一个 SOTA 模型 XGBoost。</p><h2 id="6964" class="lv lw jk bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">招聘挑战</h2><p id="d866" class="pw-post-body-paragraph kz la jk lb b lc mo kl le lf mp ko lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最近我碰巧参加了一个机器学习招聘挑战，其中的问题陈述是一个分类问题。虽然问题陈述是一个入门级的问题，但我想在这里分享一下竞赛中的一些关键收获。</p><p id="59aa" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的日常工作是从数据清理、数据预处理、EDA 和类上采样开始的(因为训练数据集是不平衡的)。作为 EDA 的一部分，我发现数据集的特征具有很高的方差。因此，我计划用 MinMax Scaler 进行特征缩放。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="6e35" class="lv lw jk my b gy nc nd l ne nf"><em class="ng">#Feature Preprocessing and Feature Transformation:<br/><br/></em><strong class="my jl">def </strong>feature_preprocess_transformation(data):<br/>    <strong class="my jl">global </strong>train,test<br/><em class="ng">#   data['X_12']=data['X_12'].fillna(value=1.0).astype(int)                                                    <br/>#   scaler=MinMaxScaler(feature_range=(0,4))<br/>#   for j in to_normalize:<br/>#        data[j]=scaler.fit_transform(data[j].values.reshape(-1,1)).astype(int)<br/><br/><br/><br/>#Calling the preprocessing function for Train and Test Data<br/><br/></em>feature_preprocess_transformation(train)<br/>feature_preprocess_transformation(test)</span></pre><p id="ca71" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成所有先决条件后，我决定尝试集成算法，因为它基于决策树的逻辑以及从弱学习者学习的能力，预计它在分类问题上表现更好。所以我决定使用 XGBoost。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ab6f" class="lv lw jk my b gy nc nd l ne nf">xgb=XGBClassifier()<br/>xgb.fit(X_train,y_train)<br/>y_pred=xgb.predict(X_test)<br/><br/>tn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()<br/>recall=recall_score(y_test,y_pred)<br/>precision=precision_score(y_test,y_pred)<br/>print(<strong class="my jl">'\n'</strong>,<strong class="my jl">'Recall : '</strong>+str(recall))<br/>print(<strong class="my jl">'\n'</strong>,<strong class="my jl">'TN : '</strong>+str(tn),<strong class="my jl">'\n'</strong>,<strong class="my jl">'FP : '</strong>+str(fp),<strong class="my jl">'\n'</strong>,<strong class="my jl">'FN : '</strong>+str(fn),<strong class="my jl">'\n'</strong>,<strong class="my jl">'TP : '</strong>+str(tp))</span></pre><p id="b152" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如下所示，该模型的召回分数为 0.9961，并且高假阳性计数表明该模型在对阴性类别进行分类时的难度。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/397f3b77eb779eafd24493fa31c49145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ECHJCH0yV5gwfdwlJIXUSw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">具有最小最大标度模型的召回度量</p></figure><p id="a845" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">后来，我删除了特性缩放的预处理部分，并尝试应用相同的 XGBoost 模型。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="6a82" class="lv lw jk my b gy nc nd l ne nf"><strong class="my jl">def </strong>feature_preprocess_transformation(data):<br/>    <strong class="my jl">global </strong>train,test<br/>    data[<strong class="my jl">'X_12'</strong>]=data[<strong class="my jl">'X_12'</strong>].fillna(value=1.0).astype(int)                                                   <em class="ng"><br/>#   scaler=MinMaxScaler(feature_range=(0,4))<br/>#   for j in to_normalize:<br/># data[j]=scaler.fit_transform(data[j].values.reshape(-1,1)).astype(int)</em></span><span id="ab74" class="lv lw jk my b gy ni nd l ne nf">xgb=XGBClassifier()<br/>xgb.fit(X_train,y_train)<br/>y_pred=xgb.predict(X_test)<br/><br/>tn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()<br/>recall=recall_score(y_test,y_pred)<br/>precision=precision_score(y_test,y_pred)<br/>print(<strong class="my jl">'\n'</strong>,<strong class="my jl">'Recall : '</strong>+str(recall))<br/>print(<strong class="my jl">'\n'</strong>,<strong class="my jl">'TN : '</strong>+str(tn),<strong class="my jl">'\n'</strong>,<strong class="my jl">'FP : '</strong>+str(fp),<strong class="my jl">'\n'</strong>,<strong class="my jl">'FN : '</strong>+str(fn),<strong class="my jl">'\n'</strong>,<strong class="my jl">'TP : '</strong>+str(tp))</span></pre><p id="5461" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下面可以看出，召回分数提高到 1.0，并且该模型能够更有效地对阴性类别进行分类，而假阳性更少。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/4d2b977d4afb5e569b112575ae93025c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xFhA4q0yTqwQ_ju59Qa4uw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">具有最小最大标度模型的召回度量</p></figure><h2 id="0900" class="lv lw jk bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">直觉</h2><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/49176bffabc68b3097ddc82c89c1fac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBCSo6DfZhi9xcaJLdDn6A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">礼貌:【https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf】T2</p></figure><ul class=""><li id="ff58" class="nm nn jk lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">以上节选自 XGBoost 作者的一篇演讲。</li><li id="5d00" class="nm nn jk lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">决策树分类不受数据中异常值的影响，因为数据是使用分数分割的，分数是使用所得数据点的同质性计算的。</li></ul><h2 id="bf9b" class="lv lw jk bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">外卖食品</h2><ul class=""><li id="6fcf" class="nm nn jk lb b lc mo lf mp li oa lm ob lq oc lu nr ns nt nu bi translated">决策树和集成方法不需要执行特征缩放，因为它们对数据中的方差不敏感。</li></ul><h2 id="afa8" class="lv lw jk bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="8827" class="pw-post-body-paragraph kz la jk lb b lc mo kl le lf mp ko lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae nl" href="https://paperswithcode.com/paper/xgboost-a-scalable-tree-boosting-system" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/paper/xgboost-a-scalable-tree-boosting-system</a></p><div class="is it gp gr iu od"><a href="https://github.com/dmlc/xgboost/issues/357" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd jl gy z fp oi fr fs oj fu fw jj bi translated">正常化有必要吗？问题#357 dmlc/xgboost</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">解散 GitHub 是超过 5000 万开发者的家园，他们一起工作来托管和审查代码，管理项目，以及…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">github.com</p></div></div></div></a></div></div></div>    
</body>
</html>