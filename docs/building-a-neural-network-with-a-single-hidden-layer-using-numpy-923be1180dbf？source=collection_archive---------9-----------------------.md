# ä½¿ç”¨ Numpy æ„å»ºå•éšå±‚ç¥ç»ç½‘ç»œ

> åŸæ–‡ï¼š<https://towardsdatascience.com/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf?source=collection_archive---------9----------------------->

ä½¿ç”¨ Numpy å®ç°å…·æœ‰å•ä¸ªéšè—å±‚çš„ä¸¤ç±»åˆ†ç±»ç¥ç»ç½‘ç»œ

åœ¨[ä¸Šä¸€ç¯‡](/build-a-simple-neural-network-using-numpy-2add9aad6fc8?source=your_stories_page---------------------------)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨ NumPy åˆ¶ä½œä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åˆ¶ä½œä¸€ä¸ªå…·æœ‰éšè—å±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚

1.  **å¯¼å…¥åº“**

æˆ‘ä»¬å°†å¯¼å…¥ä¸€äº›åŸºæœ¬çš„ python åº“ï¼Œå¦‚ numpyã€matplotlib(ç”¨äºç»˜åˆ¶å›¾å½¢)ã€sklearn(ç”¨äºæ•°æ®æŒ–æ˜å’Œåˆ†æå·¥å…·)ç­‰ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„ã€‚

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
```

**2ã€‚æ•°æ®é›†**

æˆ‘ä»¬å°†ä½¿ç”¨é’ç¥¨æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶‰åŠåœ¨ç»™å®šä»ç…§ç‰‡ä¸­è·å–çš„å‡ ä¸ªæµ‹é‡å€¼çš„æƒ…å†µä¸‹é¢„æµ‹ç»™å®šé’ç¥¨æ˜¯å¦æ˜¯çœŸå®çš„ã€‚è¿™æ˜¯ä¸€ä¸ªäºŒå…ƒ(2 ç±»)åˆ†ç±»é—®é¢˜ã€‚æœ‰ 1ï¼Œ372 ä¸ªå…·æœ‰ 4 ä¸ªè¾“å…¥å˜é‡å’Œ 1 ä¸ªè¾“å‡ºå˜é‡çš„è§‚å¯Ÿå€¼ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è§[é“¾æ¥ã€‚](http://archive.ics.uci.edu/ml/datasets/banknote+authentication)

```
data = np.genfromtxt(â€˜data_banknote_authentication.txtâ€™, delimiter = â€˜,â€™)
X = data[:,:4]
y = data[:, 4]
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•£ç‚¹å›¾æ¥å¯è§†åŒ–æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ç±»(çœŸå®å’ŒéçœŸå®)æ˜¯å¯åˆ†çš„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæ¨¡å‹æ¥æ‹Ÿåˆè¿™äº›æ•°æ®ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›å»ºç«‹ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹æ¥å®šä¹‰åŒºåŸŸæ˜¯çœŸå®çš„è¿˜æ˜¯ä¸çœŸå®çš„ã€‚

```
plt.scatter(X[:, 0], X[:, 1], alpha=0.2,
 c=y, cmap=â€™viridisâ€™)
plt.xlabel(â€˜variance of waveletâ€™)
plt.ylabel(â€˜skewness of waveletâ€™);
```

![](img/0fdfb456843706e089ebf9127861bd56.png)

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è¿™å¯ä»¥ä½¿ç”¨ sk learn*train _ test _ split()*å‡½æ•°æ¥å®Œæˆã€‚é€‰æ‹© 20%çš„æ•°æ®ç”¨äºæµ‹è¯•ï¼Œ80%ç”¨äºè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¤§å°ã€‚è¿™å°†æœ‰åŠ©äºä»¥åè®¾è®¡æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)X_train = X_train.T
y_train = y_train.reshape(1, y_train.shape[0])X_test = X_test.T
y_test = y_test.reshape(1, y_test.shape[0])print (â€˜Train X Shape: â€˜, X_train.shape)
print (â€˜Train Y Shape: â€˜, y_train.shape)
print (â€˜I have m = %d training examples!â€™ % (X_train.shape[1]))

print ('\nTest X Shape: ', X_test.shape)
```

![](img/6771a4ab036427284afd5be1b5171553.png)

**3ã€‚ç¥ç»ç½‘ç»œæ¨¡å‹**

æ„å»ºç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•æ˜¯:

```
1\. Define the neural network structure ( # of input units,  # of hidden units, etc). 
2\. Initialize the model's parameters
3\. Loop:
    - Implement forward propagation
    - Compute loss
    - Implement backward propagation to get the gradients
    - Update parameters (gradient descent)
```

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå…·æœ‰å•ä¸€éšè—å±‚çš„ç¥ç»ç½‘ç»œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

![](img/7199eb5af9eac8c65207db9ff1e5ec2f.png)

**3.1 å®šä¹‰ç»“æ„**

æˆ‘ä»¬éœ€è¦å®šä¹‰è¾“å…¥å•å…ƒçš„æ•°é‡ã€éšè—å•å…ƒçš„æ•°é‡å’Œè¾“å‡ºå±‚ã€‚è¾“å…¥å•ä½ç­‰äºæ•°æ®é›†ä¸­çš„è¦ç´ æ•°é‡(4)ï¼Œéšè—å±‚è®¾ç½®ä¸º 4(ä¸ºæ­¤)ï¼Œé—®é¢˜æ˜¯æˆ‘ä»¬å°†ä½¿ç”¨å•ä¸€å›¾å±‚è¾“å‡ºçš„äºŒè¿›åˆ¶åˆ†ç±»ã€‚

```
def **define_structure**(X, Y):
    input_unit = X.shape[0] # size of input layer
    hidden_unit = 4 #hidden layer of size 4
    output_unit = Y.shape[0] # size of output layer
    return (input_unit, hidden_unit, output_unit)(input_unit, hidden_unit, output_unit) = **define_structure**(X_train, y_train)
print("The size of the input layer is:  = " + str(input_unit))
print("The size of the hidden layer is:  = " + str(hidden_unit))
print("The size of the output layer is:  = " + str(output_unit))
```

![](img/097c15de3a1f0efd693ae1d416dcdfcd.png)

**3.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°**

æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–æƒé‡çŸ©é˜µå’Œåç½®å‘é‡ã€‚å½“åå·®è®¾ç½®ä¸ºé›¶æ—¶ï¼Œæƒé‡è¢«éšæœºåˆå§‹åŒ–ã€‚è¿™å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°æ¥å®Œæˆã€‚

```
def **parameters_initialization**(input_unit, hidden_unit, output_unit):
    np.random.seed(2) 
    W1 = np.random.randn(hidden_unit, input_unit)*0.01
    b1 = np.zeros((hidden_unit, 1))
    W2 = np.random.randn(output_unit, hidden_unit)*0.01
    b2 = np.zeros((output_unit, 1))
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
```

**3.3.1 æ­£å‘ä¼ æ’­**

å¯¹äºæ­£å‘ä¼ æ’­ï¼Œç»™å®šä¸€ç»„è¾“å…¥ç‰¹å¾(X)ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ¯ä¸€å±‚çš„æ¿€æ´»å‡½æ•°ã€‚å¯¹äºéšè—å±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ **tanh** æ¿€æ´»å‡½æ•°:

![](img/f3767c6521f7b1684a5d8139a99b3b6f.png)![](img/eba1afaf39f8eed185d50f4fe2f192ef.png)

åŒæ ·ï¼Œå¯¹äºè¾“å‡ºå±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ã€‚

![](img/a934d9cdb6fa5dc275881f81d1dbce56.png)![](img/8f99b142e777a89456f84c29b2d284cb.png)

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥å®ç°å‘å‰ä¼ æ’­ã€‚

```
def **sigmoid**(z):
    return 1/(1+np.exp(-z))def **forward_propagation**(X, parameters):
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    cache = {"Z1": Z1,"A1": A1,"Z2": Z2,"A2": A2}

    return A2, cache
```

3.3.2 è®¡ç®—æˆæœ¬

æˆ‘ä»¬å°†è®¡ç®—äº¤å‰ç†µæˆæœ¬ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† A2ã€‚ä½¿ç”¨ A2ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—äº¤å‰ç†µæˆæœ¬ã€‚

![](img/d22632929230d0e612f9287c0890480c.png)

```
def **cross_entropy_cost**(A2, Y, parameters):
    # number of training example
    m = Y.shape[1] 
    # Compute the cross-entropy cost
    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))
    cost = - np.sum(logprobs) / m
    cost = float(np.squeeze(cost))

    return cost
```

**3.3.3 åå‘ä¼ æ’­**

æˆ‘ä»¬éœ€è¦è®¡ç®—ä¸åŒå‚æ•°çš„æ¢¯åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](img/95a52f8673fa6251252af73885a06132.png)

å›¾ç‰‡æä¾›:[å´æ©è¾¾](https://www.coursera.org/learn/neural-networks-deep-learning/)

```
def **backward_propagation**(parameters, cache, X, Y):
    #number of training example
    m = X.shape[1]

    W1 = parameters['W1']
    W2 = parameters['W2']
    A1 = cache['A1']
    A2 = cache['A2']

    dZ2 = A2-Y
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = (1/m) * np.dot(dZ1, X.T) 
    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)

    grads = {"dW1": dW1, "db1": db1, "dW2": dW2,"db2": db2}

    return grads
```

**3.3.4 æ¢¯åº¦ä¸‹é™(æ›´æ–°å‚æ•°)**

æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™è§„åˆ™æ›´æ–°å‚æ•°ï¼Œå³

![](img/bbb2ea5678acb752985381225c5f947d.png)

å…¶ä¸­ **ğ›¼** æ˜¯å­¦ä¹ ç‡ **ğœƒ** æ˜¯å‚æ•°ã€‚

```
def **gradient_descent**(parameters, grads, learning_rate = 0.01):
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2'] W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2

    parameters = {"W1": W1, "b1": b1,"W2": W2,"b2": b2}

    return parameters
```

**4ã€‚ç¥ç»ç½‘ç»œæ¨¡å‹**

æœ€åï¼ŒæŠŠæ‰€æœ‰çš„åŠŸèƒ½æ”¾åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªåªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

```
def **neural_network_model**(X, Y, hidden_unit, num_iterations = 1000):
    np.random.seed(3)
    input_unit = **define_structure**(X, Y)[0]
    output_unit = **define_structure**(X, Y)[2]

    parameters = **parameters_initialization**(input_unit, hidden_unit, output_unit)

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    for i in range(0, num_iterations):
        A2, cache = **forward_propagation**(X, parameters)
        cost = **cross_entropy_cost**(A2, Y, parameters)
        grads = **backward_propagation**(parameters, cache, X, Y)
        parameters = **gradient_descent**(parameters, grads)
        if i % 5 == 0:
            print ("Cost after iteration %i: %f" %(i, cost)) return parametersparameters = **neural_network_model**(X_train, y_train, 4, num_iterations=1000)
```

![](img/1efa51d0becb001cbe722d4c6de49c0b.png)

**5ã€‚é¢„æµ‹**

ä½¿ç”¨å­¦ä¹ åˆ°çš„å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å‰å‘ä¼ æ’­æ¥é¢„æµ‹æ¯ä¸ªç¤ºä¾‹çš„ç±»ã€‚

```
def **prediction**(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)

    return predictions
```

å¦‚æœ*æ¿€æ´»> 0.5ï¼Œ*åˆ™é¢„æµ‹ä¸º 1 å¦åˆ™ä¸º 0ã€‚

```
predictions = **prediction**(parameters, X_train)
print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')predictions = **prediction**(parameters, X_test)
print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')
```

![](img/0376f1544d550ae36b986e0314a53ca8.png)

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè®­ç»ƒç²¾åº¦çº¦ä¸º 97%ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹æ­£åœ¨å·¥ä½œï¼Œå¹¶ä¸”ä»¥é«˜æ¦‚ç‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚æµ‹è¯•å‡†ç¡®ç‡åœ¨ 96%å·¦å³ã€‚ç»™å®šç®€å•çš„æ¨¡å‹å’Œå°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªå¥½çš„æ¨¡å‹ã€‚

> åœ¨è¿™é‡Œæˆä¸º Medium ä¼šå‘˜[ï¼Œæ”¯æŒç‹¬ç«‹å†™ä½œï¼Œæ¯æœˆ 5 ç¾å…ƒï¼Œå¯ä»¥å®Œå…¨è®¿é—® Medium ä¸Šçš„æ¯ä¸ªæ•…äº‹ã€‚](https://medium.com/@rmesfrmpkr/membership)