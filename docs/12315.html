<html>
<head>
<title>Train GPT-2 in your own language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用您自己的语言培训 GPT-2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171?source=collection_archive---------3-----------------------#2020-08-25">https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171?source=collection_archive---------3-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一步一步的指导，训练你自己的 GPT-2 文本生成模型在你选择的语言从零开始</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/69cad891053171b921902300cb2eda49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oaE4DFjHcdlYWaiy"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Jr Korpa 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="42eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们都知道，随着<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力网络</a>和《变形金刚》的发展，现代自然语言处理(NLP)在过去几年里取得了突飞猛进的进步。它为大量的新算法铺平了道路，为 NLP 的不同任务实现了最先进的(SOTA)。</p><p id="6376" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OpenAI 一直是提供自己的语言模型(现已发布 GPT-3)的领导者之一，该模型是在巨大的互联网数据语料库上训练的。因为，GPT-3 是最近的现象，目前是英语，只能通过 OpenAI 提供的 API 访问，我们将注意力转移到它的早期版本，即<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>。要了解 GPT-2 的内部螺母和螺栓，我建议你通过这个<a class="ae ky" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">链接</a>。关于注意力和变形金刚的更多深度，这里有一些很好的链接:</p><ul class=""><li id="8b1c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">杰伊·阿拉玛的《变形金刚》</li><li id="b09a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">哈佛 NLP 的注释变压器</li></ul><p id="d38e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPT-2 也发布了英语版本，这使得人们很难用不同的语言生成文本。</p><p id="f304" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，为什么不在你最喜欢的文本生成语言上训练你自己的 GPT-2 模型呢？这正是我们要做的。所以，事不宜迟，让我们开始吧。</p><p id="d4ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于演示，我已经考虑了非拉丁字母脚本(孟加拉语在这里)，因为为什么不！！我已经为模型使用了<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>的实现。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="58c1" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">1.收集数据。</h1><p id="110f" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">所有数据科学家都同意，收集高质量的数据是最重要的阶段之一。因此，我们假设您已经有了一个包含。txt 文件，清除并存储所有数据。为方便起见，您可以使用 Wikipedia 文章数据，它是可用的，可以通过以下代码下载。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="3650" class="nu mr it nq b gy nv nw l nx ny">python wikipedia_download.py --lang bn</span></pre><p id="4188" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将创建一个包含所有维基百科文件的文件夹，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dad68d6e57cfc81826bc58c15c47669f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*lV-7WQ0D8sxsEaD7bF5jHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文件列表截图</p></figure><blockquote class="oa ob oc"><p id="a176" class="kz la od lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>由于资源限制，同时出于演示目的，我在<a class="ae ky" href="https://en.wikipedia.org/wiki/Satyajit_Ray" rel="noopener ugc nofollow" target="_blank">萨耶吉特·雷伊</a>的一小部分书籍中训练了模型，尤其是他的侦探<a class="ae ky" href="https://en.wikipedia.org/wiki/Feluda" rel="noopener ugc nofollow" target="_blank">费鲁达</a>系列。</p></blockquote><h1 id="7b97" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">2.标记化</h1><p id="6bd5" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在，第二步是将数据标记化。为此，我们使用下面的类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h2 id="7627" class="nu mr it bd ms om on dn mw oo op dp na li oq or nc lm os ot ne lq ou ov ng ow bi translated">关于标记化的一些注释:</h2><ul class=""><li id="b750" class="lv lw it lb b lc ni lf nj li ox lm oy lq oz lu ma mb mc md bi translated">我们使用<strong class="lb iu"> BPE </strong>(字节对编码)，这是一种子字编码，这通常不会将不同形式的字视为不同。(例如，最大将被视为两个标记:“大”和“est”，这是有利的，因为它保留了大和最大之间的相似性，而“最大”添加了另一个标记“est”，使其不同)。此外，它不像字符级编码那样低级，字符级编码不保留特定单词的任何值。</li><li id="387e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">另一个小而微妙的地方是第 13 行代码中的<strong class="lb iu"> NFKC </strong>(规范化形式兼容性组合)。它是标准的 Unicode 兼容格式之一。如果语言是英语，这没有多大关系，但是因为我们使用孟加拉语，它包含一种不同的字符形式，所以我们使用这种特殊的语言。更多信息请点击<a class="ae ky" rel="noopener" target="_blank" href="/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c">链接</a></li></ul><p id="97d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们在这里做的是将我们的数据标记化，并将其保存在一个文件夹中。将在指定目录下创建两个文件(<em class="od"> merges.txt </em>和<em class="od"> vocab.json </em>)。要运行该文件，请使用以下代码:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="6cca" class="nu mr it nq b gy nv nw l nx ny">from tokenise import BPE_token<br/>from pathlib import Path<br/>import os</span><span id="7203" class="nu mr it nq b gy pa nw l nx ny"># the folder 'text' contains all the files<br/>paths = [str(x) for x in Path("./text/").glob("**/*.txt")]</span><span id="5c96" class="nu mr it nq b gy pa nw l nx ny">tokenizer = BPE_token()</span><span id="bbf3" class="nu mr it nq b gy pa nw l nx ny"># train the tokenizer model<br/>tokenizer.bpe_train(paths)</span><span id="ee6f" class="nu mr it nq b gy pa nw l nx ny"># saving the tokenized data in our specified folder <br/>save_path = 'tokenized_data'<br/>tokenizer.save_tokenizer(save_path)</span></pre><h1 id="a558" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">3.模型初始化</h1><p id="5ab9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在真正的魔法开始之前，我们需要确保炮兵准备好了。让我们从一些初始化开始。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="2a2f" class="nu mr it nq b gy nv nw l nx ny">import tensorflow as tf<br/>from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer</span><span id="6df9" class="nu mr it nq b gy pa nw l nx ny"># loading tokenizer from the saved model path<br/>tokenizer = GPT2Tokenizer.from_pretrained(save_path)</span><span id="037e" class="nu mr it nq b gy pa nw l nx ny">tokenizer.add_special_tokens({<br/>  "eos_token": "&lt;/s&gt;",<br/>  "bos_token": "&lt;s&gt;",<br/>  "unk_token": "&lt;unk&gt;",<br/>  "pad_token": "&lt;pad&gt;",<br/>  "mask_token": "&lt;mask&gt;"<br/>})</span><span id="44de" class="nu mr it nq b gy pa nw l nx ny"># creating the configurations from which the model can be made<br/>config = GPT2Config(<br/>  vocab_size=tokenizer.vocab_size,<br/>  bos_token_id=tokenizer.bos_token_id,<br/>  eos_token_id=tokenizer.eos_token_id<br/>)</span><span id="03a8" class="nu mr it nq b gy pa nw l nx ny"># creating the model<br/>model = TFGPT2LMHeadModel(config)</span></pre><p id="f31e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还从所有文档中创建一个字符串，并对其进行标记。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="5e0e" class="nu mr it nq b gy nv nw l nx ny">single_string = ''<br/>for filename in paths:<br/>  with open(filename, "r", encoding='utf-8') as f:<br/>   x = f.read()</span><span id="d0f6" class="nu mr it nq b gy pa nw l nx ny">  single_string += x + tokenizer.eos_token</span><span id="4409" class="nu mr it nq b gy pa nw l nx ny">string_tokenized = tokenizer.encode(single_string)</span></pre><p id="48aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们对整个字符串进行编码后，我们现在继续制作 TensorFlow 数据集，将数据分割成相等的间隔，以便我们的模型可以学习。这里我们使用的块大小为 100(每个示例中的令牌长度),批量大小为 16。这是保持低，否则我们可以在 RTX 2060 GPU 上轻松运行它。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="2776" class="nu mr it nq b gy nv nw l nx ny">examples = []<br/>block_size = 100<br/>BATCH_SIZE = 12<br/>BUFFER_SIZE = 1000</span><span id="e170" class="nu mr it nq b gy pa nw l nx ny">for i in range(0, len(string_tokenized) - block_size + 1, block_size):<br/>  examples.append(string_tokenized[i:i + block_size])<br/>inputs, labels = [], []</span><span id="10e0" class="nu mr it nq b gy pa nw l nx ny">for ex in examples:<br/>  inputs.append(ex[:-1])<br/>  labels.append(ex[1:])</span><span id="db1c" class="nu mr it nq b gy pa nw l nx ny">dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))<br/>dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)</span></pre><h1 id="810f" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">4.模特培训</h1><p id="acbb" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在是我们期待已久的部分，制作模型和训练。因此，我们定义了优化器、损失函数和指标，并开始训练。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="61f2" class="nu mr it nq b gy nv nw l nx ny"># defining our optimizer<br/>optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)</span><span id="0ac8" class="nu mr it nq b gy pa nw l nx ny"># definining our loss function<br/>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="a029" class="nu mr it nq b gy pa nw l nx ny"># defining our metric which we want to observe<br/>metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')</span><span id="39ef" class="nu mr it nq b gy pa nw l nx ny"># compiling the model<br/>model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])</span></pre><p id="4dc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们训练模型</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="dbc9" class="nu mr it nq b gy nv nw l nx ny">num_epoch = 10<br/>history = model.fit(dataset, epochs=num_epoch)</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1e69" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">5.预言；预测；预告</h1><p id="a6f2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">为了进行预测，我们只需要简单地对输入文本进行编码，并将其传递给模型</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="3ae5" class="nu mr it nq b gy nv nw l nx ny">text = "লালমোহনবাবু "</span><span id="4ba4" class="nu mr it nq b gy pa nw l nx ny"># encoding the input text<br/>input_ids = tokenizer.encode(text, return_tensors='tf')</span><span id="e577" class="nu mr it nq b gy pa nw l nx ny"># getting out output<br/>beam_output = model.generate(<br/>  input_ids,<br/>  max_length = 50,<br/>  num_beams = 5,<br/>  temperature = 0.7,<br/>  no_repeat_ngram_size=2,<br/>  <!-- -->num_return_sequences=5<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/14c87e3d72a1e7d2b5a15f440a30157e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*yt38iZgMKoOyPhSlRmHwZw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出的屏幕截图</p></figure><p id="73f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果你是一个孟加拉人，那么你可以指出，虽然句子的语法是正确的，但它看起来不连贯。没错，但是对于这个演示，我已经尽可能地简化了这个演示。</p><h1 id="361d" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">6.保存模型</h1><p id="bc3b" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">好吧，在长时间的训练之后，如果我们结束了我们的训练，所有我们训练过的模型都丢失了，我们又需要从头开始训练它，这有什么好处呢？因此，让我们保存模型和记号化器，以便我们可以从我们停止的地方重新训练</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="f903" class="nu mr it nq b gy nv nw l nx ny">from transformers import WEIGHTS_NAME, CONFIG_NAME<br/>import os</span><span id="5a1b" class="nu mr it nq b gy pa nw l nx ny">output_dir = './model_bn_custom/'</span><span id="c438" class="nu mr it nq b gy pa nw l nx ny"># creating directory if it is not present<br/>if not os.path.exists(output_dir):<br/>  os.mkdir(output_dir)</span><span id="8925" class="nu mr it nq b gy pa nw l nx ny">model_to_save = model.module if hasattr(model, 'module') else model<br/>output_model_file = os.path.join(output_dir, WEIGHTS_NAME)<br/>output_config_file = os.path.join(output_dir, CONFIG_NAME)</span><span id="0cfd" class="nu mr it nq b gy pa nw l nx ny"># save model and model configs<br/>model.save_pretrained(output_dir)<br/>model_to_save.config.to_json_file(output_config_file)</span><span id="8f21" class="nu mr it nq b gy pa nw l nx ny"># save tokenizer<br/>tokenizer.save_pretrained(output_dir)</span></pre><h1 id="7672" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">奖金</h1><p id="3d71" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们已经完成了所有的艰苦工作，所以要加载保存的模型和标记器，我们只需要执行两行代码，一切都准备好了。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="e45b" class="nu mr it nq b gy nv nw l nx ny">tokenizer = GPT2Tokenizer.from_pretrained(output_dir)<br/>model = TFGPT2LMHeadModel.from_pretrained(output_dir)</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="a8d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊。现在你可以用你自己的语言训练你自己的模型。创造出可以与任何语言的最佳文学作品相媲美的内容。</p><h1 id="8b4b" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">未来范围:</h1><p id="c757" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">这个博客给出了如何用任何语言训练 GPT-2 模型的框架。这与现有的一些预训练模型不相上下，但要达到这种状态，我们需要大量的训练数据和计算能力。</p><h1 id="f81c" class="mq mr it bd ms mt oh mv mw mx oi mz na jz oj ka nc kc ok kd ne kf ol kg ng nh bi translated">参考资料:</h1><div class="pc pd gp gr pe pf"><a href="https://huggingface.co/blog/how-to-train" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">如何使用转换器和记号赋予器从零开始训练一个新的语言模型</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">在过去的几个月里，我们对我们的 transformers 和 tokenizers 库做了一些改进，目标是…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">huggingface.co</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a href="https://huggingface.co/blog/how-to-generate" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">如何生成文本:使用不同的解码方法通过转换器生成语言</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">近年来，由于大规模语言生成的兴起，人们对开放式语言生成越来越感兴趣。</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">huggingface.co</p></div></div><div class="po l"><div class="pu l pq pr ps po pt ks pf"/></div></div></a></div></div></div>    
</body>
</html>