<html>
<head>
<title>Reinforcement Learning— An Introduction to Gradient Temporal Difference Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——梯度时间差分学习算法介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e?source=collection_archive---------17-----------------------#2020-02-07">https://towardsdatascience.com/reinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e?source=collection_archive---------17-----------------------#2020-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a8a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对政策评估中一些最新算法的全面介绍，包括GTD和GTD2。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2d4417784ceeb7adc29eff16fbe3dbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ynkGw6vIvtLp5WS-GtMVA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Rafif Prawira 在<a class="ae kv" href="https://unsplash.com/s/photos/maze?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="d724" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">目录:</strong></p><ul class=""><li id="8490" class="lz ma iq lf b lg lh lj lk lm mb lq mc lu md ly me mf mg mh bi translated">介绍</li><li id="9ed4" class="lz ma iq lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">线性函数逼近</li><li id="ef28" class="lz ma iq lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">推导GTD2算法</li><li id="a5f7" class="lz ma iq lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">结论</li><li id="2c68" class="lz ma iq lf b lg mi lj mj lm mk lq ml lu mm ly me mf mg mh bi translated">鸣谢和资源</li></ul></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="3808" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">介绍</h1><p id="5b62" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">强化学习是目前最热门的领域之一，具体的应用正在以令人难以置信的速度增长，从击败视频游戏到机器人。从本质上来说，<strong class="lf ir">强化学习(RL) </strong>处理的是决策——也就是说，它试图回答一个主体在给定环境中应该如何行动的问题。</p><blockquote class="nk"><p id="709c" class="nl nm iq bd nn no np nq nr ns nt ly dk translated">强化学习处理决策</p></blockquote><p id="e658" class="pw-post-body-paragraph ld le iq lf b lg nu jr li lj nv ju ll lm nw lo lp lq nx ls lt lu ny lw lx ly ij bi translated">不严格地说，所有的RL都归结为发现或评估一个<strong class="lf ir">策略</strong>，这只是一种行为方式。例如，策略可以是下棋的策略。</p><blockquote class="nz oa ob"><p id="4250" class="ld le oc lf b lg lh jr li lj lk ju ll od ln lo lp oe lr ls lt of lv lw lx ly ij bi translated">策略接受一个状态(在国际象棋的例子中，是棋盘上所有棋子的位置)并为其分配一个动作。</p></blockquote><p id="8789" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">例如，给定你的棋盘的状态，你的策略可能要求你向前移动你的皇后。当你退出一个状态，你会得到一些奖励。这个很直观。例如，如果我们用快乐/痛苦来衡量我们作为人类的回报，那么把我们的手放在沸水中(退出之前手很冷的状态)会产生一些痛苦(因此，会有回报)。</p><p id="c26d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在本文中，我们将关注一个叫做<strong class="lf ir">策略评估</strong>的东西，它可以归结为评估策略。这是什么意思？给定一个策略，我们希望找到其关联的<strong class="lf ir">值函数</strong>。这意味着如果我们遵循一些给定的策略，就能够给每个状态赋值。但是我们所说的<em class="oc">值</em>是什么意思呢？如果我们处于某种状态，那种状态的值就是我们从这种状态中得到的预期回报。</p><blockquote class="nk"><p id="3510" class="nl nm iq bd nn no np nq nr ns nt ly dk translated">TD不一定在非政策环境下收敛</p></blockquote><p id="e61b" class="pw-post-body-paragraph ld le iq lf b lg nu jr li lj nv ju ll lm nw lo lp lq nx ls lt lu ny lw lx ly ij bi translated">由Richard Sutton在20世纪80年代提出的TD学习的变体是一些最健壮和最常用的策略评估算法，尤其是在<strong class="lf ir"> on-policy </strong>设置中，即当您遵循您正在评估的策略时。然而，在<strong class="lf ir">非策略</strong>设置中——当遵循不同于您正在评估的策略时——TD学习算法的收敛性在某些情况下无法保证(例如，当使用线性近似时，我们将在下一节中讨论)。这使得萨顿等人在2009年引入了GTD和GTD 2(GTD的改进版本)，以保证在非政策设置下的收敛性[1] [2]。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="defd" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">线性函数逼近</h1><p id="4f61" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">当你第一次开始学习RL时，你可能会开始学习马尔可夫链、马尔可夫奖励过程(MRP)，最后是<strong class="lf ir">马尔可夫决策过程(MDP) </strong>。然后，您通常会转到典型的策略评估算法，如蒙特卡罗(MC)和时间差异(TD)学习，然后是控制算法，如SARSA和Q-learning。</p><blockquote class="nk"><p id="e0b9" class="nl nm iq bd nn no np nq nr ns nt ly dk translated">对于大多数RL应用程序来说，表查找并不实用</p></blockquote><p id="480b" class="pw-post-body-paragraph ld le iq lf b lg nu jr li lj nv ju ll lm nw lo lp lq nx ls lt lu ny lw lx ly ij bi translated">在所有这些情况下，您可能一直在使用状态值和动作值函数的<strong class="lf ir">表格表示</strong>。这意味着您直接更新了每个状态的值。实际上，这对于具有大量状态的MDP或者在我们使用连续状态空间的情况下是非常不切实际的。例如，如果你试图在三维空间中引导一架直升机，你的状态可能是它的当前位置、速度、角速度、加速度等。正如您所猜测的，状态的数量是无限的，这意味着没有办法在一个表中存储所有的东西。</p><p id="99a2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这个问题的解决方法是什么？我们将使用一个函数(由某个权重向量<strong class="lf ir"> θ </strong>参数化)来逼近真实值函数，而不是直接更新每个状态。这个函数可以是任何东西(例如，您可以使用神经网络)，但我们将重点关注线性函数逼近。数学上，我们在寻找一个函数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/355a32ac702990d292931b2ec564e119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u38x_OtKA1BHDizZ_P2YxA.png"/></div></div></figure><p id="9b84" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其中<strong class="lf ir"> ϕ(s) </strong>是包含给定状态信息(例如:速度、加速度等)的特征向量。)和<strong class="lf ir"> θ </strong>是我们正在努力学习的权重向量。在某些状态下，价值函数的估计就是<strong class="lf ir">ϕ(s</strong>和<strong class="lf ir"> θ </strong>之间的点积。<strong class="lf ir"> </strong>上式中，<strong class="lf ir"> γ ∈ (0，1)</strong>称为折现率；直觉上，这意味着我们更重视获得短期回报，而不是长期回报——这与我们人类的工作方式是一致的，除了数学上的便利。</p><blockquote class="nz oa ob"><p id="ea13" class="ld le oc lf b lg lh jr li lj lk ju ll od ln lo lp oe lr ls lt of lv lw lx ly ij bi translated">我们的目标是找到权重向量<strong class="lf ir"> θ </strong>，这将允许我们的价值函数逼近尽可能精确。</p></blockquote><p id="1b99" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果我们用<strong class="lf ir">s’</strong>表示下一个状态，我们可以用<strong class="lf ir">贝尔曼方程</strong>写出<strong class="lf ir"> TD误差</strong>如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/81ed8bba09349495db998d30cc7cf37f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SBCx_Q3V4-QokIQIt8cMHg.png"/></div></div></figure><p id="2368" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们记得，我们将在后面使用的贝尔曼算子被定义为<strong class="lf ir"> TV = R + γ PV，</strong>其中<strong class="lf ir"> P </strong>是转移矩阵。任何给定值函数<strong class="lf ir"> V </strong>必须满足贝尔曼方程，即<strong class="lf ir"> V = TV </strong>。利用TD误差，我们可以获得传统的TD更新:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/5de6c09118934fd7d8fb244a4c458a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OlsJcwRBrsMZDkWsJkQBag.png"/></div></div></figure><p id="cf96" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于某些学习率<strong class="lf ir"> α(k) </strong>。如果这对你来说是新的，这可能看起来很复杂，但这并不新奇！的确，这只是一个简单的随机梯度下降！在线性近似器的情况下，我们注意到梯度减少到<strong class="lf ir"> ϕ(k) </strong>，这使得一切都变得美好而简单。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="96be" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">目标函数</h1><p id="8cae" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">现在我们有了一个价值函数逼近器，问题就变成了:我们如何让这个逼近器尽可能好？为此，我们首先需要定义某种误差函数。</p><blockquote class="nz oa ob"><p id="fec4" class="ld le oc lf b lg lh jr li lj lk ju ll od ln lo lp oe lr ls lt of lv lw lx ly ij bi translated">一个<strong class="lf ir">目标函数</strong>只是一个<strong class="lf ir"> θ </strong>的函数，我们将努力使其相对于<strong class="lf ir"> θ </strong>最小化。</p></blockquote><p id="e418" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在做TD学习的时候，最自然的测量误差的方式就是通过测量<strong class="lf ir"> V(θ) </strong>符合贝尔曼方程的程度。这被称为<strong class="lf ir">均方贝尔曼误差(MSBE) </strong>，其定义为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1d1f1a9bedacf71ce23da2ba10685fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*Jyfgz63wNhgCdyQnxBDlnw.png"/></div></figure><p id="40d9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其中<strong class="lf ir"> D </strong>是一个对角矩阵，包含衡量每种状态出现频率的权重</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/24d7850632d1dea3344ad2868d6fb70b.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*mK6qNGYNcy-SbB49jX3Fvw.png"/></div></figure><p id="995b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于某些矢量<strong class="lf ir"> v </strong>。</p><blockquote class="nk"><p id="d914" class="nl nm iq bd nn no np nq nr ns nt ly dk translated">对于任何<strong class="ak"> θ </strong>，TV(θ)通常不能表示为<strong class="ak"> V(θ) </strong></p></blockquote><p id="563a" class="pw-post-body-paragraph ld le iq lf b lg nu jr li lj nv ju ll lm nw lo lp lq nx ls lt lu ny lw lx ly ij bi translated">这个目标函数在许多先前的研究中被使用，但是大多数TD算法不收敛到MSBE的最小值。这是由于贝尔曼算子遵循马尔可夫链的基本状态动力学，这意味着对于任何<strong class="lf ir"> θ </strong>，通常不能用<strong class="lf ir"> V(θ) </strong>来表示<strong class="lf ir"> TV(θ) </strong>(例如，如果我们考虑线性函数逼近器，我们不能精确地表示非线性函数是有意义的)。然后，通常所做的是将最优解投影到最近的可表示值函数，其中运算符<strong class="lf ir">π</strong>定义为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/b47ad5ec2ca3140dc16898122a6b085c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HycRVNWdh9J8fSJ7zGk3tA.png"/></div></div></figure><p id="e027" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果我们考虑一个线性架构，其中对于某个矩阵<strong class="lf ir">φ</strong>，其行是<strong class="lf ir"> ϕ(s) </strong>，我们可以独立于<strong class="lf ir"> θ </strong>将<strong class="lf ir">π</strong>写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e4803c4c3711f79af7fc104e0870df7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*dhPcPrPbI7l8aeCtEwkfPQ.png"/></div></figure><p id="d26b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">到目前为止，所有的TD算法都收敛到值<strong class="lf ir">θ</strong>(TD固定点)，使得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/043f5aec54fba165698fc1449060509a.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*c-g3Bz4fl7rlp7PxXZOqBA.png"/></div></figure><p id="8697" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">GTD2的推导使用了不同的目标函数，即<strong class="lf ir">均方投影贝尔曼误差(MSPBE) </strong>，它考虑了投影<strong class="lf ir">π</strong>，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b31514abb0e2ccc95f9e975b08939446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*ZdcKW8Ay6DPr1uhzRJ0NRg.png"/></div></figure><p id="b801" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">MSBE和MSPE之间有一种非常简洁的几何关系，我们可以在下图中看到(摘自萨顿关于GTD2的原始论文):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/95ca2f2b044881043bb999e4fc9cc4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WckfkTunw50T7XbKeRXLpA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两个贝尔曼误差目标函数之间的几何关系[1]</p></figure><p id="4e6d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有趣的是，对于GTD来说，使用的目标函数是预期TD更新(NEU)的标准，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7c29a13d267fd2dc6134e3a109df119e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*iTfYIbKN5fc5_6G8mlg-mw.png"/></div></figure><p id="156e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们很快就会看到，这两个方程确实非常相似。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="cd26" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">推导GTD2算法</h1><p id="5316" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">我们现在将推导GDT2算法，这是由Sutton等人在2009年引入的算法，它在偏离策略的情况下收敛。我们首先承认下列重要的关系，这将有助于以后简化某些方程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/583109e494a255472a133a9d0e97b5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WZGlaqCMz5SHMzCdGihRg.png"/></div></div></figure><p id="3ca6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在，我们可以根据期望将MSPBE写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/c7669f10c7f68dd32a019ca77e1081c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGcHsuqlIDp9wp5E4Gb2oA.png"/></div></div></figure><p id="0966" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们应该在这里看到一些有趣的东西！</p><blockquote class="nz oa ob"><p id="43c4" class="ld le oc lf b lg lh jr li lj lk ju ll od ln lo lp oe lr ls lt of lv lw lx ly ij bi translated">我们看到，MSPBE与NEU的不同之处仅在于包含了特征协方差矩阵的逆。</p></blockquote><p id="c2c3" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了避免需要两个独立的样本，我们可以使用一个可修改的参数<strong class="lf ir"> w ∈ Rⁿ </strong>来形成目标函数梯度中除了一个期望之外的所有期望的准平稳估计</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/204b5f0b99906e60fe7210b37c40b925.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*A27HxUkoAP_fwgIh14zGyw.png"/></div></figure><p id="cb6a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">利用这一点，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/243a66db8f1d292fd521cd1c9fb42752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vWbLHNXzqvEB4KLWhDrORw.png"/></div></div></figure><p id="c65b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其可以被直接采样。由此，我们得到GTD2算法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/0dd6e7f8e5c9c396221fcb5d132f4191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kjiwgVS7Iu8iaDVYe1YHg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GTD2算法的θ更新</p></figure><p id="ba16" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在，我们想为<strong class="lf ir"> w </strong>找到一个迭代更新。首先，我们意识到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/b9fac8860ee2928e8cff6c5e4630d996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dea4wG3EfYYwlMV72A0Q_g.png"/></div></div></figure><p id="e161" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们看到上面的右边是下面最小二乘问题的解决方案:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2650290e3249afe92db8908e7cf8ee83.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*4z_tTg9AFj-xDiaQyLb1cw.png"/></div></figure><p id="50ac" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这可以通过具有以下更新的随机梯度下降来解决:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/3990144f084953c5460f571a23346eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4aIyLVDOGb015yH3DmALkA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oz"> w </strong>为GTD2算法更新</p></figure><p id="ca0c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir"> θ </strong>和<strong class="lf ir"> w </strong>更新一起组成了GTD2算法，可以很容易地用你喜欢的编程语言实现。注意，为了让算法收敛，我们需要</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/2b79e06b092b7d88453150a91564d7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rK978vAmYlVpb6uUD_lRvQ.png"/></div></div></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="2fc1" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">结论</h1><p id="c5c4" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">谢谢你走了这么远！我希望您现在对策略评估有了更好的理解，更具体地说，对强化学习中的一些新算法有了更好的理解，比如梯度时间差异学习。此外，我希望你对算法背后的数学有所了解，并对它们是如何推导出来的有更多的直觉。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="4772" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">鸣谢和资源</h1><p id="fd71" class="pw-post-body-paragraph ld le iq lf b lg nf jr li lj ng ju ll lm nh lo lp lq ni ls lt lu nj lw lx ly ij bi translated">这篇文章主要依赖于Sutton等人在2009年发表的GTD2论文，以及最初的GTD论文。艾尔。</p><p id="b330" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[1] Sutton，R. S .，Maei，H. R .，Precup，d .，Bhatnagar，s .，Silver，d .，Szepesvari，Cs .和Wiewiora，E. (2009年)。线性函数逼近的时差学习快速梯度下降法。在<em class="oc">第26届机器学习国际会议记录</em>中，第993–1000页。全媒体。</p><p id="e7d6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[2]萨顿、塞佩瓦里、Cs。，Maei，H. R. (2009年)。线性函数逼近的非策略时差学习收敛O(n)算法。<em class="oc">神经信息处理系统进展21 </em>。麻省理工出版社。</p></div></div>    
</body>
</html>