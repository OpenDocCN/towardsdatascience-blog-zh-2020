<html>
<head>
<title>How to Wrap Your Head Around Spark NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何理解 Spark NLP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-wrap-your-head-around-spark-nlp-a6f6a968b7e8?source=collection_archive---------36-----------------------#2020-08-25">https://towardsdatascience.com/how-to-wrap-your-head-around-spark-nlp-a6f6a968b7e8?source=collection_archive---------36-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="47b9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">*跟进<em class="ki">“如何在两周内开始 SparkNLP 第一部分”</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/6d42e11d0b410a71ed65bc2a85ec123b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y0id1KPSHlieo2QN"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.pexels.com/photo/pine-trees-by-lake-in-forest-against-sky-247474/" rel="noopener ugc nofollow" target="_blank">摄影:Pixabay </a></p></figure><p id="6de5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">欢迎阅读 Spark NLP 文章的第二部分。在第一部分中，目标是为 NLP 从业者提供一个破冰的平台，并让他们对 Spark NLP 有所了解。对任何基于 Spark 的库的最大偏见来自于这样一种思想流派<em class="lw">“Spark 代码与常规 Python 脚本有点不同”</em>。为了消除这种偏见，我们分享了学习策略，如果你遵循了这些策略，你就为下一阶段做好了准备。</p><p id="6f59" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文的这一部分，我们将比较 spaCy 和 Spark NLP，并深入探讨 Spark NLP 模块和管道。我创建了一个笔记本，使用 spaCy 和 Spark NLP 来做同样的事情，进行完美的图片比较。SpaCy 表现不错，但是在速度、内存消耗、精度方面 Spark NLP 胜过它。因此，就易用性而言，一旦 Spark 的初始摩擦被克服，我发现它至少与 spaCy 不相上下，这要归功于管道带来的便利。</p><p id="8217" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由 Spark NLP 创建者准备的 smart、<a class="ae kz" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public" rel="noopener ugc nofollow" target="_blank">comprehensive notebook</a>的使用，提供了大量真实场景的示例，以及我为实践而创建的<a class="ae kz" href="https://github.com/aytugkaya/spark_nlp" rel="noopener ugc nofollow" target="_blank"> repo </a>强烈建议在所需的技能集方面表现出色。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="d70c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第 8/9 天:了解 Spark NLP 中的注释器/转换程序和使用 Spark 的</strong> <a class="ae kz" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">文本预处理</strong></a><strong class="lc iu"/></p><p id="ba90" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Spark NLP 库原生构建于 Apache Spark 和<a class="ae kz" href="https://www.analyticsindiamag.com/a-hands-on-primer-to-tensorflow/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>之上，为机器学习管道提供了简单、高效且准确的 NLP 符号，可在分布式环境中轻松扩展。这个库重用了 Spark ML 管道以及集成的 NLP 功能。</p><p id="f2fc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">该库涵盖了许多常见的 NLP 任务，包括标记化、词干化、词汇化、词性标注、情感分析、拼写检查、命名实体识别，所有这些都是开源的，可以由训练模型使用您的数据。Spark NLP 的注释器利用基于规则的算法、机器学习和一些在引擎盖下运行的<a class="ae kz" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>来支持特定的深度学习实现。</p><p id="7e48" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在 Spark NLP 中，所有的<strong class="lc iu">标注器</strong>要么是估算器，要么是变换器，就像 Spark ML 一样，由两种类型组成:<strong class="lc iu">标注器方法</strong>和<strong class="lc iu">标注器模型。任何在数据帧上产生模型的标注器训练都是标注器方法。那些通过一些模型将一个数据帧转换成另一个数据帧的是<em class="lw">注释者模型</em>(例如<em class="lw"> WordEmbeddingsModel </em>)。通常，如果注释器在转换数据帧时不依赖于预先训练的注释器(例如<em class="lw">标记器</em>)，它就不会使用<em class="lw">模型</em>后缀。下面是注释器及其描述的列表:</strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="me mf l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Spark NLP 提供的注释器列表，来源:<a class="ae kz" rel="noopener" target="_blank" href="/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59">Spark NLP 介绍—基础和基本组件</a></p></figure><p id="6688" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了完成自然语言处理过程，我们需要对原始数据进行预处理。除了 SQL 过滤器、转换和用户定义的函数之外，Spark NLP 还附带了用于该任务的强大工具。<strong class="lc iu"><em class="lw">Document assembler</em></strong>是一个特殊的转换器，它创建文档类型的第一个注释，这个注释可能会被管道中的后续注释器使用。</p><p id="79b7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"/></p><p id="7230" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> <em class="lw"> Doc2Chunk </em> </strong>将文档类型注释转换为带有块 Col 内容的块类型，而<strong class="lc iu"> <em class="lw"> Chunk2Doc </em> </strong>将块类型列转换回文档。在尝试对块结果进行重新标记或进一步分析时非常有用。</p><p id="14cd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> <em class="lw">修整器</em> </strong>将注释值输出到一个字符串中，以方便使用。一旦我们的 NLP 管道准备就绪，我们可能希望在其他实际可用的地方使用注释结果。</p><p id="818f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如前所述，管道被指定为一系列阶段，每个阶段要么是转换器，要么是估计器。这些阶段按顺序运行，输入数据帧在通过每个阶段时会发生转换。</p><p id="3343" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面是这条管道在 Spark NLP 中的编码方式。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="2fca" class="ml mm it mh b gy mn mo l mp mq">from pyspark.ml import Pipeline</span><span id="cd66" class="ml mm it mh b gy mr mo l mp mq">document_assembler = DocumentAssembler()\<br/> .setInputCol(“text”)\<br/> .setOutputCol(“document”)</span><span id="6d49" class="ml mm it mh b gy mr mo l mp mq">sentenceDetector = SentenceDetector()\<br/> .setInputCols([“document”])\<br/> .setOutputCol(“sentences”)tokenizer = Tokenizer() \<br/> .setInputCols([“sentences”]) \<br/> .setOutputCol(“token”)normalizer = Normalizer()\<br/> .setInputCols([“token”])\<br/> .setOutputCol(“normal”)</span><span id="1b2e" class="ml mm it mh b gy mr mo l mp mq">word_embeddings=WordEmbeddingsModel.pretrained()\<br/> .setInputCols([“document”,”normal”])\<br/> .setOutputCol(“embeddings”)</span><span id="3138" class="ml mm it mh b gy mr mo l mp mq">nlpPipeline = Pipeline(stages=[<br/> document_assembler, <br/> sentenceDetector,<br/> tokenizer,<br/> normalizer,<br/> word_embeddings,<br/> ])</span><span id="2937" class="ml mm it mh b gy mr mo l mp mq">pipelineModel = nlpPipeline.fit(df)</span></pre><p id="8afa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当我们在具有 Spark 数据帧(df)的管道上<code class="fe ms mt mu mh b">fit()</code>时，它的<em class="lw">文本</em>列首先被送入<code class="fe ms mt mu mh b">DocumentAssembler()</code>转换器，然后在<em class="lw">文档</em>类型(<em class="lw">注释者类型</em>)中创建一个新列“<em class="lw">文档</em>”。如前所述，这个转换器是任何 Spark 数据帧的 Spark NLP 的初始入口点。然后将它的文档列送入<code class="fe ms mt mu mh b">SentenceDetector()</code> <em class="lw"> </em> ( <em class="lw">注释者方法</em>)中，将文本拆分成一个句子数组，并在文档类型<em class="lw"> </em>中创建一个新列“<em class="lw">句子”。然后将“句子”列输入到<code class="fe ms mt mu mh b">Tokenizer()</code> ( <em class="lw">注释器模型</em>)中，对每个句子进行标记化，并在标记</em>类型<em class="lw"> </em>中创建新列“<em class="lw">标记”。诸如此类。请参考<a class="ae kz" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb" rel="noopener ugc nofollow" target="_blank"> Spark NLP workshop 预处理笔记本</a>和<a class="ae kz" href="https://github.com/aytugkaya/spark_nlp" rel="noopener ugc nofollow" target="_blank"> my repo </a>查看注释器和转换器的工作情况。</em></p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="ab93" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第 10 天:Regex </strong> <br/> <em class="lw">正则表达式本质上是一种嵌入在 Python 中的微小的、高度专门化的编程语言，并通过</em> <code class="fe ms mt mu mh b"><a class="ae kz" href="https://docs.python.org/3/library/re.html#module-re" rel="noopener ugc nofollow" target="_blank"><em class="lw">re module</em></a></code> <em class="lw">可用。</em> <em class="lw">使用这个小语言，你为你想要匹配的可能字符串集合指定规则；这个集合可能包含英语句子，或者电子邮件地址，临床实体，或者任何你喜欢的东西。您还可以使用正则表达式来修改字符串或以各种方式拆分它。</em></p><p id="7034" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Regex 是 NLP 的主要构件之一，我们不仅将使用它进行预处理，还将它作为 Spark NLP 管道中<code class="fe ms mt mu mh b">RegexMatcher</code>的一部分。由于有时可能相当复杂，我推荐 Maria Eugenia Inzaugarat 的这个<a class="ae kz" href="https://learn.datacamp.com/courses/regular-expressions-in-python" rel="noopener ugc nofollow" target="_blank">课程</a>，以便快速掌握高级主题，请参考<a class="ae kz" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank">官方 python 文档</a>以及精彩的 python <a class="ae kz" href="https://docs.python.org/3/howto/regex.html#regex-howto" rel="noopener ugc nofollow" target="_blank"> regex how to </a>教程。我用来掌握 regex 的另一个关键组件是<a class="ae kz" href="http://regex101.com" rel="noopener ugc nofollow" target="_blank"> Regex101 </a>，这是一个非常强大的 API，可以让你在文本块上测试几乎任何正则表达式。</p><p id="9237" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">大多数时候正则表达式被忽略了。相信我:你会需要它，你会使用它。你知道的越多越好。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="d0ee" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第 11 天:预训练模型</strong></p><p id="0e37" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">前面提到过，经过训练的标注器被称为<em class="lw">标注器模型</em>，这里的目标是通过指定的模型(经过训练的标注器)将一个数据帧转换成另一个数据帧。Spark NLP 提供多种语言的预训练模型，您需要做的就是加载预训练模型并相应地配置其参数。当您可以通过一种<code class="fe ms mt mu mh b">transform()</code>方法直接应用预先训练好的 SOTA 算法时，为什么还要担心从头开始训练新模型呢？在<a class="ae kz" href="https://nlp.johnsnowlabs.com/docs/en/models" rel="noopener ugc nofollow" target="_blank">官方文档</a>中，您可以找到关于如何使用哪些算法和数据集来训练这些模型的详细信息。<a class="ae kz" href="https://www.johnsnowlabs.com/spark-nlp-in-action/" rel="noopener ugc nofollow" target="_blank">笔记本样本可在此处找到</a>。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="87e0" class="ml mm it mh b gy mn mo l mp mq">MODEL_NAME='sentimentdl_use_twitter'</span><span id="5a21" class="ml mm it mh b gy mr mo l mp mq">documentAssembler = DocumentAssembler()\<br/>    .setInputCol("text")\<br/>    .setOutputCol("document")</span><span id="8f29" class="ml mm it mh b gy mr mo l mp mq">use = UniversalSentenceEncoder.pretrained(name="tfhub_use", lang="en")\<br/> .setInputCols(["document"])\<br/> .setOutputCol("sentence_embeddings")</span><span id="3208" class="ml mm it mh b gy mr mo l mp mq">sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang="en")\<br/>    .setInputCols(["sentence_embeddings"])\<br/>    .setOutputCol("pr_sentiment")</span><span id="9af4" class="ml mm it mh b gy mr mo l mp mq">nlpPipeline = Pipeline(<br/>      stages = [<br/>          documentAssembler,<br/>          use,<br/>          sentimentdl<br/>      ])</span><span id="9de7" class="ml mm it mh b gy mr mo l mp mq">empty_df = spark.createDataFrame([['']]).toDF("text")</span><span id="82b1" class="ml mm it mh b gy mr mo l mp mq">pipelineModel = nlpPipeline.fit(empty_df)</span><span id="8e1a" class="ml mm it mh b gy mr mo l mp mq">result = pipelineModel.transform(spark_df1)</span></pre><p id="f4a5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第 12/13 天:文本分类</strong></p><p id="d0d9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Spark NLP 中有几个文本分类选项:</p><ul class=""><li id="2004" class="mv mw it lc b ld le lg lh lj mx ln my lr mz lv na nb nc nd bi translated">Spark NLP 中的文本预处理和使用来自<a class="ae kz" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark ML </a>的 ML 算法</li><li id="4ec0" class="mv mw it lc b ld ne lg nf lj ng ln nh lr ni lv na nb nc nd bi translated">Spark ML 的 Spark NLP 和 ML 算法中的文本预处理和单词嵌入(Glove、Bert、Elmo)</li><li id="a3dd" class="mv mw it lc b ld ne lg nf lj ng ln nh lr ni lv na nb nc nd bi translated">Spark ML 的 Spark NLP 和 ML 算法中的文本预处理和句子嵌入(通用句子编码器)</li><li id="cd31" class="mv mw it lc b ld ne lg nf lj ng ln nh lr ni lv na nb nc nd bi translated">Spark 自然语言处理中的文本预处理和分类器</li></ul><p id="325c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如在关于 Spark NLP 的文章<a class="ae kz" rel="noopener" target="_blank" href="/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59">中所讨论的，在<code class="fe ms mt mu mh b">ClassifierDL</code>之前的所有这些文本处理步骤都可以在一个流水线中实现，这个流水线被指定为一系列阶段，每个阶段要么是一个转换器，要么是一个估计器。这些阶段按顺序运行，输入数据帧在通过每个阶段时会发生转换。也就是说，数据按顺序通过拟合的管道。每个阶段的<code class="fe ms mt mu mh b">transform()</code> <em class="lw"> </em>方法更新数据集并将其传递给下一个阶段。在<code class="fe ms mt mu mh b">Pipelines</code>的帮助下，我们可以确保训练和测试数据经过相同的特征处理步骤。</a></p><p id="a40c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">你可以用这个<code class="fe ms mt mu mh b">ClassiferDL</code>在 Spark NLP 中构建一个包含<code class="fe ms mt mu mh b">Bert</code>、<code class="fe ms mt mu mh b">Elmo</code>、<code class="fe ms mt mu mh b">Glove</code>和<code class="fe ms mt mu mh b">Universal Sentence Encoders</code>的文本分类器。样本分类器笔记本可以在<a class="ae kz" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.Text_Classification_with_ClassifierDL.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae kz" href="https://github.com/aytugkaya/spark_nlp/tree/master/toxicity" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="06a8" class="ml mm it mh b gy mn mo l mp mq">document_assembler = DocumentAssembler() \<br/>    .setInputCol("comment_text") \<br/>    .setOutputCol("document")\<br/>    .setCleanupMode('shrink')</span><span id="9366" class="ml mm it mh b gy mr mo l mp mq">tokenizer = Tokenizer() \<br/>  .setInputCols(["document"]) \<br/>  .setOutputCol("token")<br/>    <br/>normalizer = Normalizer() \<br/>    .setInputCols(["token"]) \<br/>    .setOutputCol("normalized")\<br/>    .setLowercase(True)</span><span id="5381" class="ml mm it mh b gy mr mo l mp mq">stopwords_cleaner = StopWordsCleaner()\<br/>      .setInputCols("normalized")\<br/>      .setOutputCol("cleanTokens")\<br/>      .setCaseSensitive(False)</span><span id="343a" class="ml mm it mh b gy mr mo l mp mq">lemma = LemmatizerModel.pretrained('lemma_antbnc') \<br/>    .setInputCols(["cleanTokens"]) \<br/>    .setOutputCol("lemma")</span><span id="72d1" class="ml mm it mh b gy mr mo l mp mq">bert = BertEmbeddings.pretrained('bert_base_uncased', 'en') \<br/>      .setInputCols("document", "lemma") \<br/>      .setOutputCol("embeddings")\<br/>      .setPoolingLayer(0) # default 0</span><span id="3b0a" class="ml mm it mh b gy mr mo l mp mq">embeddingsSentence = SentenceEmbeddings() \<br/>      .setInputCols(["document", "embeddings"]) \<br/>      .setOutputCol("sentence_embeddings") \<br/>      .setPoolingStrategy("AVERAGE")</span><span id="a614" class="ml mm it mh b gy mr mo l mp mq">classsifierdl = ClassifierDLApproach()\<br/>  .setInputCols(["sentence_embeddings"])\<br/>  .setOutputCol("prediction")\<br/>  .setLabelColumn("toxic")\<br/>  .setMaxEpochs(5)\<br/>  .setEnableOutputLogs(True)\<br/>  .setBatchSize(32)\<br/>  .setValidationSplit(0.1)\<br/>  .setDropout(0.75)\<br/>  .setLr(0.0035)\<br/>  #.setOutputLogsPath('logs')</span><span id="ea75" class="ml mm it mh b gy mr mo l mp mq">clf_pipeline = Pipeline(<br/>    stages=[document_assembler, <br/>            tokenizer,<br/>            normalizer,<br/>            stopwords_cleaner, <br/>            lemma, <br/>            bert,<br/>            embeddingsSentence,<br/>            classsifierdl<br/>           ])</span><span id="5213" class="ml mm it mh b gy mr mo l mp mq">clf_pipelineModel = clf_pipeline.fit(sdf)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nj"><img src="../Images/aab720f8029b66c3d6f8487bcae32da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kx7OVEyRvBQk02zbKgfnow.png"/></div></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="91b5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第 13–14 天:SpaCy 或 Spark NLP —基准比较</strong></p><p id="7aa0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本节中，我们将获得大量的练习编码！我们将使用从 Gutenberg.org 下载的七部不同经典著作组成的图书馆。该语料库包括大约 490 万个字符和 9.7 万个句子。我们将使用 spaCy 和 Spark NLP 的类似过程构建相同的输出数据帧，然后比较结果。</p><p id="3bea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Spacy 是我见过的记录最好的图书馆之一，我惊喜地发现现在他们包括了一门免费课程，我以前上过。如果需要的话，这是一个提升你技能的好机会。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/13635ab196660303c3fe91667f158353.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*pU88CEHtKCV5kRmbr5AAkg.png"/></div></figure><p id="44b2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">比较的整个笔记本和语料数据可以在我的<a class="ae kz" href="https://github.com/aytugkaya/spark_nlp/tree/master/spaCy_SparkNLP_comparison" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到。让我们从检查空间方式开始。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="6e36" class="ml mm it mh b gy mn mo l mp mq">### 2.1 Clean Tabs and Whitespace<br/>clean_shrink = lambda text : text.replace(r'\n|\t|\s+',' ').replace('\s+',' ').strip()<br/>df.loc[:,'document']=df.text.map(clean_shrink)</span><span id="2248" class="ml mm it mh b gy mr mo l mp mq">### 2.2 Tokenizer<br/>sentence_tokenizer = lambda sent : [token for token in nlp(sent)]<br/>df.loc[:,'token']=df.document.map(sentence_tokenizer)</span><span id="cde1" class="ml mm it mh b gy mr mo l mp mq">### 2.3 Normalizer<br/>normalizer = lambda tokens : [re.sub(punct,'',token.text)  for token in tokens if re.sub(punct,'',token.text) != '']<br/>df.loc[:,'normalized']=df.token.map(normalizer)</span><span id="992c" class="ml mm it mh b gy mr mo l mp mq">### 2.4 Remove Stop Words<br/>normalizer_and_stop = lambda tokens : [re.sub(punct,'',token.text)  for token in tokens if re.sub(punct,'',token.text) != '' and not token.is_stop]<br/>df.loc[:,'cleanTokens']=df.token.map(normalizer_and_stop)</span><span id="1264" class="ml mm it mh b gy mr mo l mp mq">### 2.5 Lemmatize<br/>normalizer_and_stop_lemma = lambda tokens : [re.sub(punct,'',token.lemma_)  for token in tokens if re.sub(punct,'',token.text) != '' and not token.is_stop]<br/>df.loc[:,'lemma']=df.token.map(normalizer_and_stop_lemma)</span><span id="c43a" class="ml mm it mh b gy mr mo l mp mq">### 2.6 Stemmer<br/>stemmer = PorterStemmer()<br/>stems = lambda tokens : [stemmer.stem(token.text) if len(tokens)&gt;0 else [] for token in tokens]<br/>df.loc[:,'stem']=df.token.map(stems)</span><span id="2e4f" class="ml mm it mh b gy mr mo l mp mq">### 2.7 Part of Speech Tagging<br/>normalizer_and_stop_pos = lambda tokens : [re.sub(punct,'',token.pos_)  for token in tokens if re.sub(punct,'',token.text) != '' and not token.is_stop]<br/>df.loc[:,'pos']=df.cleanTokens.map(normalizer_and_stop_pos)</span><span id="1359" class="ml mm it mh b gy mr mo l mp mq">### 2.8 Token Assembler<br/>token_assembler = lambda tokens : " ".join(tokens)<br/>df.loc[:,'clean_text']=df.cleanTokens.map(token_assembler)</span><span id="00d9" class="ml mm it mh b gy mr mo l mp mq">### 2.9 Tagger<br/>tagger = lambda text : [(ent.text, ent.label_) for ent in nlp(text).ents]<br/>df.loc[:,'ner_chunks']=df.loc[:,'document'].map(tagger)</span><span id="2c8b" class="ml mm it mh b gy mr mo l mp mq">### 2.10 Regex Parser<br/>noun_chunker = lambda text : [(chnk,(chnk[0].pos_,chnk[1].pos_,chnk[2].tag_ ))for chnk in nlp(text).noun_chunks if len(chnk.text.split())==3\<br/>                              and  chnk.text.replace(' ','').isalpha()   and chnk[0].pos_ == 'DET'and chnk[1].pos_ == 'ADJ' and chnk[2].tag_ in ['NN','NNP']<br/>                             ]<br/>df.loc[:,'RegexpParser'] =df.loc[:,'document'].map(noun_chunker)</span></pre><p id="8b07" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们暂停一下，观察正则表达式解析器的输出。我们要求名词组块器返回由一个<strong class="lc iu">限定词</strong>、一个<strong class="lc iu">形容词</strong>和一个<strong class="lc iu">名词</strong>(专有、单数或复数)组成的组块。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="d80a" class="ml mm it mh b gy mn mo l mp mq">[chunk for chunk in df.RegexpParser.values if chunk!=[]]</span></pre><p id="163d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">结果看起来相当不错。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="9037" class="ml mm it mh b gy mn mo l mp mq">[[(My dear Robinson, ('DET', 'ADJ', 'NNP'))],<br/> [(this accidental souvenir, ('DET', 'ADJ', 'NN'))],<br/> [(a great deal, ('DET', 'ADJ', 'NN'))],<br/> [(a great amount, ('DET', 'ADJ', 'NN'))],<br/> [(the local hunt, ('DET', 'ADJ', 'NN')),<br/>  (some surgical assistance, ('DET', 'ADJ', 'NN'))],<br/> [(a remarkable power, ('DET', 'ADJ', 'NN'))],<br/> [(a good deal, ('DET', 'ADJ', 'NN'))],<br/> [(a fresh basis, ('DET', 'ADJ', 'NN')),<br/>  (this unknown visitor, ('DET', 'ADJ', 'NN'))],<br/> [(the obvious conclusion, ('DET', 'ADJ', 'NN'))],<br/> [(their good will, ('DET', 'ADJ', 'NN'))],<br/> [(my dear Watson, ('DET', 'ADJ', 'NNP')),<br/>  (a young fellow, ('DET', 'ADJ', 'NN'))],<br/> [(a favourite dog, ('DET', 'ADJ', 'NN'))],<br/> [(the latter part, ('DET', 'ADJ', 'NN'))],<br/> [(that local hunt, ('DET', 'ADJ', 'NN'))],<br/> [(a heavy stick, ('DET', 'ADJ', 'NN'))],<br/> [(a professional brother, ('DET', 'ADJ', 'NN'))],<br/> [(the dramatic moment, ('DET', 'ADJ', 'NN'))],<br/> [(a long nose, ('DET', 'ADJ', 'NN'))],<br/>................</span></pre><p id="a50e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们继续搭积木吧。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="a0bb" class="ml mm it mh b gy mn mo l mp mq">### 2.11 N-Gram Generator<br/>ngram_generator = lambda input_list: [*zip(*[input_list[i:] for i in range(n)])]<br/>n=3<br/>df.loc[:,'triGrams'] = df.loc[:,'token'].map(ngram_generator)</span><span id="ec48" class="ml mm it mh b gy mr mo l mp mq">### 2.12 Word2Vec Embeddings<br/>vector = lambda tokens: [(token.text, token.has_vector, token.vector, token.is_oov) for token in tokens]<br/>df.loc[:,'vectors'] = df.loc[:,'token'].map(vector)</span><span id="44cf" class="ml mm it mh b gy mr mo l mp mq">### 2.13 Regex Matcher<br/>rules = r'''\b[A-Z]\w+ly\b|Stephen\s(?!Proto|Cardinal)[A-Z]\w+|Simon\s[A-Z]\w+'''<br/>regex_matchers = lambda text : re.findall(rules,text)<br/>df.loc[:,'Regex_matches'] =df.loc[:,'document'].map(regex_matchers)</span></pre><p id="8875" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们来看看正则表达式匹配</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="19e3" class="ml mm it mh b gy mn mo l mp mq">df.Regex_matches[df.Regex_matches.map(len)&gt;1]</span></pre><p id="bf20" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">结果如下:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="be9d" class="ml mm it mh b gy mn mo l mp mq">13123                      [Polly, Polly]<br/>25669                      [Sally, Sally]<br/>27262                      [Sally, Sally]<br/>27273                      [Polly, Sally]<br/>27340                      [Polly, Sally]<br/>28311                      [Pelly, Dolly]<br/>42016                      [Feely, Feely]<br/>49802                    [Finally, Emily]<br/>52129                    [Lively, Lively]<br/>58295                      [Silly, Milly]<br/>62141                      [Silly, Milly]<br/>64811                       [Only, Molly]<br/>71650                        [Hely, Daly]<br/>74427                      [Healy, Dolly]<br/>77404                      [Molly, Milly]<br/>77437                      [Milly, Molly]<br/>81557                     [Molly, Reilly]<br/>84023          [Szombathely, Szombathely]<br/>89594                       [Healy, Joly]<br/>92206    [Simon Dedalus, Stephen Dedalus]<br/>92980      [Firstly, Nelly, Nelly, Nelly]<br/>93046               [Szombathely, Karoly]<br/>94402       [Reilly, Simon Dedalus, Hely]<br/>94489    [Stephen Dedalus, Simon Dedalus]</span></pre><p id="53c5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们冒险进入角色…</p><p id="85b5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然我们有了一个包含许多要素的数据集，我们就有了大量的选项可供选择。让我们检查一下书中的人物……让我们找到带有<strong class="lc iu">“人”</strong>标签的<strong class="lc iu"> NER 语块</strong>，由两个单词组成。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="4cf1" class="ml mm it mh b gy mn mo l mp mq">flatlist = lambda l : [re.sub("[^a-zA-Z\s\']","",item[0]).title().strip()  for sublist in l for item in sublist if item[1]=='PERSON' and len(item[0].split())==2]<br/>ner_chunks = df.ner_chunks.to_list()<br/>names=(flatlist(ner_chunks))<br/>len(sorted(names))</span></pre><p id="8456" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面的代码返回了 4832 个名字，这看起来有点可疑，因为这个数字很高。让我们检查一下<code class="fe ms mt mu mh b">Counter</code>对象的结果:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="02a5" class="ml mm it mh b gy mn mo l mp mq">('St Clare', 306),<br/> ('Buck Mulligan', 96),<br/> ('Aunt Chloe', 76),<br/> ('Martin Cunningham', 74),<br/> ('Masr George', 45),<br/> ('Ned Lambert', 44),<br/> ('Solomon Northup', 43),<br/> ('Tom Sawyer', 42),<br/> ('Aunt Sally', 37),<br/> ('Uncle Tom', 37),<br/> ('Ben Dollard', 36),<br/> ('Myles Crawford', 35),<br/> ('Blazes Boylan', 28),<br/> ('Uncle Abram', 24),<br/> ('J J', 22),<br/>...... <br/> ('L L', 5),<br/> ('Mark Twain', 5),<br/> ("Tom Sawyer'S", 5),<br/> ('Chapter Xi', 5),<br/> ('Chapter Xix', 5),<br/> ('Dey Wuz', 5),<br/> ('George Jackson', 5),<br/> ('Levi Bell', 5),<br/> ('King Lear', 5),<br/> ('Simon Legree', 5),<br/> ('Garrett Deasy', 5),<br/> ('A E', 5),<br/> ('S D', 5),<br/> ('Josie Powell', 5),<br/> ('Mrs Purefoy', 5),<br/> ('Ben Howth', 5),<br/> ('Bald Pat', 5),<br/> ('Barney Kiernan', 5),<br/> ('Michael Gunn', 5),<br/> ('C C', 5),</span></pre><p id="6c15" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">不幸的是，许多标签是不准确的。请注意一些章节标题以及大写首字母被错误地返回为<code class="fe ms mt mu mh b">PER</code>标签。</p><p id="b587" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在编写上面的代码时，使用了映射来确保快速调整，并且模拟了下面将要实现的 Spark NLP 管道。一旦我们在 Spark NLP 中运行类似的代码，我们将在内存使用、速度和准确性方面比较结果。</p><p id="6ffb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">是时候用 NLP 的方式做事了！</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="057d" class="ml mm it mh b gy mn mo l mp mq">documentAssembler = DocumentAssembler()\<br/>.setInputCol(“text”)\<br/>.setOutputCol(“document”)\<br/>.setCleanupMode(“shrink”)</span><span id="fe7f" class="ml mm it mh b gy mr mo l mp mq">sentenceDetector = SentenceDetector().\<br/>    setInputCols(['document']).\<br/>    setOutputCol('sentences')</span><span id="e29b" class="ml mm it mh b gy mr mo l mp mq">tokenizer = Tokenizer() \<br/>    .setInputCols(["sentences"]) \<br/>    .setOutputCol("token")</span><span id="2bf2" class="ml mm it mh b gy mr mo l mp mq">ngrams = NGramGenerator() \<br/>            .setInputCols(["token"]) \<br/>            .setOutputCol("ngrams") \<br/>            .setN(3) \<br/>            .setEnableCumulative(False)\<br/>            .setDelimiter("_") # Default is space</span><span id="5b54" class="ml mm it mh b gy mr mo l mp mq">normalizer = Normalizer() \<br/>    .setInputCols(["token"]) \<br/>    .setOutputCol("normalized")\<br/>    .setLowercase(False)\<br/>    .setCleanupPatterns(["[^\w\d\s\.\!\?]"])</span><span id="4283" class="ml mm it mh b gy mr mo l mp mq">stopwords_cleaner = StopWordsCleaner()\<br/>      .setInputCols("normalized")\<br/>      .setOutputCol("cleanTokens")\<br/>      .setCaseSensitive(False)\</span><span id="b9a0" class="ml mm it mh b gy mr mo l mp mq">lemma = LemmatizerModel.pretrained('lemma_antbnc') \<br/>    .setInputCols(["cleanTokens"]) \<br/>    .setOutputCol("lemma")</span><span id="95ed" class="ml mm it mh b gy mr mo l mp mq">stemmer = Stemmer() \<br/>    .setInputCols(["token"]) \<br/>    .setOutputCol("stem")</span><span id="721d" class="ml mm it mh b gy mr mo l mp mq">pos = PerceptronModel.pretrained("pos_anc", 'en')\<br/>      .setInputCols("clean_text", "cleanTokens")\<br/>      .setOutputCol("pos")</span><span id="afbc" class="ml mm it mh b gy mr mo l mp mq">chunker = Chunker()\<br/>    .setInputCols(["sentences", "pos"])\<br/>    .setOutputCol("chunk")\<br/>    .setRegexParsers(["&lt;DT&gt;+&lt;JJ&gt;*&lt;NN&gt;"])  ## Determiner - adjective - singular noun</span><span id="f862" class="ml mm it mh b gy mr mo l mp mq">tokenassembler = TokenAssembler()\<br/>    .setInputCols(["sentences", "cleanTokens"]) \<br/>    .setOutputCol("clean_text")\</span><span id="c5ff" class="ml mm it mh b gy mr mo l mp mq">tokenizer2 = Tokenizer() \<br/>    .setInputCols(["clean_text"]) \<br/>    .setOutputCol("token2")</span><span id="e2dc" class="ml mm it mh b gy mr mo l mp mq">glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\<br/>          .setInputCols(["document", "lemma"])\<br/>          .setOutputCol("embeddings")\<br/>          .setCaseSensitive(False)</span><span id="5dd6" class="ml mm it mh b gy mr mo l mp mq">onto_ner = NerDLModel.pretrained("onto_100", 'en') \<br/>          .setInputCols(["document", "token", "embeddings"]) \<br/>          .setOutputCol("ner")</span><span id="5c4d" class="ml mm it mh b gy mr mo l mp mq">ner_converter = NerConverter() \<br/>  .setInputCols(["sentences", "token", "ner"]) \<br/>  .setOutputCol("ner_chunk")</span><span id="d8b9" class="ml mm it mh b gy mr mo l mp mq">rules = r'''<br/>\b[A-Z]\w+ly\b, starting with a capital letter ending with 'ly'<br/>Stephen\s(?!Proto|Cardinal)[A-Z]\w+, followed by "Stephen"<br/>Simon\s[A-Z]\w+, followed by "Simon"<br/>'''</span><span id="1fb7" class="ml mm it mh b gy mr mo l mp mq">with open('ulyses_regex_rules.txt', 'w') as f:<br/>    <br/>    f.write(rules)</span><span id="bb36" class="ml mm it mh b gy mr mo l mp mq">regex_matcher = RegexMatcher()\<br/>    .setInputCols('sentences')\<br/>    .setStrategy("MATCH_ALL")\<br/>    .setOutputCol("regex_matches")\<br/>    .setExternalRules(path='./ulyses_regex_rules.txt', delimiter=',')</span></pre><p id="3a8a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然我们的零件都准备好了，让我们来定义装配线。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="3549" class="ml mm it mh b gy mn mo l mp mq">nlpPipeline = Pipeline(stages=[<br/> documentAssembler,<br/> sentenceDetector,<br/> tokenizer,<br/> ngrams,<br/> normalizer,<br/> stopwords_cleaner,<br/> lemma,<br/> stemmer,<br/> tokenassembler,<br/> tokenizer2,<br/> pos,<br/> chunker,<br/> glove_embeddings,<br/> onto_ner,<br/> ner_converter,<br/> regex_matcher<br/> <br/> ])</span><span id="9d07" class="ml mm it mh b gy mr mo l mp mq">empty_df = spark.createDataFrame([[‘’]]).toDF(“text”)</span><span id="92cb" class="ml mm it mh b gy mr mo l mp mq">pipelineModel = nlpPipeline.fit(empty_df)</span><span id="904d" class="ml mm it mh b gy mr mo l mp mq">lib_result = pipelineModel.transform(library)</span></pre><p id="be74" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们根据我们的搜索标准检查 regex 匹配:<br/> -以大写字母开头并以' ly '结尾的整个单词，<br/> - 'Stephen '后面没有' Cardinal '或' Proto '，但后面有以大写字母开头的单词。--“Simon”后面是一个以大写字母开头的单词。我们希望每个句子中至少出现两次…</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="20b6" class="ml mm it mh b gy mn mo l mp mq">match_df.filter(F.size('finished_regex_matches')&gt;1).show(truncate = 50)</span></pre><p id="2882" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们来看看结果…</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/04598e3584869627028e3601f082076c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*ngIEMgZ0Ol6AODRJIXALjg.png"/></div></figure><p id="cda3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们管道中的 chunker 注释器将返回由限定词、形容词和单数名词组成的组块。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="1f4f" class="ml mm it mh b gy mn mo l mp mq">lib_result.withColumn(<br/>    "tmp", <br/>    F.explode("chunk")) \<br/>    .select("tmp.*").select("begin","end","result","metadata.sentence").show(20,truncate = 100)</span></pre><p id="c668" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">以下是 chunker 结果的前 20 行。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9820b3eb0693c2be5d29d4f5e75be342.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*wJmnuRapGnbwX4PTl4FiLg.png"/></div></figure><p id="3760" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们冒险进入角色…火花 NLP 方式。</p><p id="9046" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这里，我们检查书中的角色…这一次我们将使用 Spark NLP 机制。请注意准确性与空间的差异。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="10c6" class="ml mm it mh b gy mn mo l mp mq">l = result_ner.filter(result_ner.ner_label == "PERSON").select(F.expr("ner_chunk")).collect()</span><span id="dcd9" class="ml mm it mh b gy mr mo l mp mq">names = list([re.sub("[^a-zA-Z\s\']","",l_[0]).title() for l_ in l if l_[0].replace(' ','').isalpha() and len(l_[0].strip().split())==2 and "’" not in l_[0]])</span><span id="6d5c" class="ml mm it mh b gy mr mo l mp mq">len(set(names))</span></pre><p id="00a5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这一次，我们的字符数被限制在 1284。让我们来看看最常见的 350 个名字。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="2da2" class="ml mm it mh b gy mn mo l mp mq">('Buck Mulligan', 93),<br/> ('Aunt Chloe', 82),<br/> ('Martin Cunningham', 71),<br/> ('Bayou Boeuf', 48),<br/> ('Aunt Sally', 39),<br/> ('Ned Lambert', 39),<br/> ('Mary Jane', 38),<br/> ('Solomon Northup', 36),<br/> ('John Thornton', 34),<br/> ('Myles Crawford', 33),<br/> ('Ben Dollard', 31),<br/> ('Sherlock Holmes', 30),<br/> ('Tom Sawyer', 30),<br/> ('John Eglinton', 29),<br/> ('Nosey Flynn', 28),<br/> ('Corny Kelleher', 27),<br/> ('Mrs Breen', 27),<br/> ('Father Conmee', 26),<br/> ('Uncle Tom', 25),<br/> ('John Wyse', 24),<br/> ('Henry Baskerville', 23),<br/> ('Uncle Abram', 22),<br/> ('Blazes Boylan', 19),<br/> ('Bob Doran', 18),<br/> ('Davy Byrne', 18),<br/> ('Coombe Tracey', 17),<br/> ('Aunt Phebe', 17),<br/> ('Simon Dedalus', 17),<br/>........................<br/> ('Sandy Hill', 8),<br/> ('Theophilus Freeman', 8),<br/> ('Father Cowley', 8),<br/> ('Gregory B', 7),<br/> ('George Harris', 7),<br/> ('Rachel Halliday', 7),<br/> ('George Shelby', 7),<br/> ('Anne Hampton', 7),<br/> ('Peter Tanner', 7),<br/> ('Almidano Artifoni', 7),<br/> ('Hugo Baskerville', 6),<br/> ('Laura Lyons', 6),<br/> ('Aunt Polly', 6),<br/> ('Peter Wilks', 6),<br/>.........................</span></pre><p id="41a6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">名单看起来更准确。难怪 Spark NLP 是企业首选！</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/40baa9fe9b51f2e0bbf4455e163e953b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*yyXS8XGlokTVH7tstakqCw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">SpaCy 不错，但是 Spark NLP 更好…</p></figure><p id="e7c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">我们来谈谈资源消耗:</strong> <br/>本次研究使用的系统是一个 8 核英特尔酷睿 i7–9700k CPU @ 3.60 GHz，32820MB 内存。操作系统是 Ubuntu 20.04。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="2d20" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">与 spaCy 相比，Spark NLP 使用的内存更少，运行速度是 spaCy 的两倍。这个事实，再加上 Spark NLP 更高的准确性，为掌握这个库提供了很好的理由！</em></p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="c600" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">目前就这些。本文的主要目的是给中级水平的从业者一个如何使用 Spark 在 NLP 领域实现目标的指南。一些 Spark NLP 概念在开始时可能很难学习，但是，本文中介绍的文章、课程和示例笔记本将提供一个很好的起点。spaCy-Spark NLP 的初始开销可能很高，但比较显示了差异。掌握 Spark NLP 并不容易，但也不太难…如果你的目标是企业领域，这绝对是一项让你在简历上大放异彩的技能！</p><p id="2124" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">请点击<a class="ae kz" rel="noopener" target="_blank" href="/how-to-get-started-with-sparknlp-in-2-weeks-cb47b2ba994d">链接进入文章的第一部分。</a></p></div></div>    
</body>
</html>