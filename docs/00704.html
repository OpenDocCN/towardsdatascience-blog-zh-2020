<html>
<head>
<title>ResNet: The Most Popular Network in the Computer-Vision Era</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ResNet:计算机视觉时代最流行的网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/resnet-the-most-popular-network-in-computer-vision-era-973df3e92809?source=collection_archive---------24-----------------------#2020-01-20">https://towardsdatascience.com/resnet-the-most-popular-network-in-computer-vision-era-973df3e92809?source=collection_archive---------24-----------------------#2020-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a081" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">计算机视觉工作者的必备知识</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9c54dd5b039156a57962a8559b801829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wN5t9ILU0fpnhbMX2vtng.jpeg"/></div></div></figure><p id="e3da" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用计算机算法对图像进行分类似乎很有挑战性。令人惊讶的是，最近在计算机视觉领域的一项研究取得了成功，在名为ImageNet的数据集上出现了1.3%的前5名错误。2020年，最先进的图像分类技术变成了由谷歌研究团队发布的EfficientNet。然而，在很长一段时间内，名为ResNet的网络在图像分类领域表现出色。此外，许多研究人员使用ResNet作为他们的网络主干，以提高他们的性能。这篇文章将帮助你直观地理解什么是ResNet以及它是如何被激发的。</p><p id="9134" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">链接:<a class="ae ln" href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/sota/image-class ification-on-imagenet</a></p><h1 id="f522" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">退化问题</h1><p id="ae03" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">深度神经网络在学习过程中遇到许多困难。计算机视觉研究人员提出了它们的解决方案，例如用批量标准化解决消失/爆炸梯度问题。(【https://arxiv.org/pdf/1502.03167.pdf】T4)ResNet论文介绍了一个具有挑战性的问题，名为“<strong class="kt ir">退化问题</strong>”在阅读之前，我们先思考一下下面的问题。</p><blockquote class="ml mm mn"><p id="dbac" class="kr ks mo kt b ku kv jr kw kx ky ju kz mp lb lc ld mq lf lg lh mr lj lk ll lm ij bi translated">层数越多，准确度越高？</p></blockquote><p id="e762" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在网络上添加图层会扩大输出的多样性，这似乎很直观。如果每个添加的图层都是身份映射，则新网络可以输出与原始网络相同的值。因此，训练有素的网络层数越多，分类精度越高，这是有说服力的。不幸的是，事实并非如此。</p><p id="aadb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当您使用普通网络(在ResNet之前)估计精度时，<strong class="kt ir">随着模型复杂性的增加，其精度会迅速下降</strong>。这个问题是一个<strong class="kt ir">退化问题</strong>。这不是一个过度拟合的问题；然而，随着模型复杂性的增加，网络的性能下降。作者声称平面网络不适合近似身份映射；因此，添加层并不保证添加层的网络能够表达添加层之前网络的所有值。ResNet的动机是<strong class="kt ir">建立一个适合身份映射的网络。</strong></p><h1 id="9919" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">快捷连接</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8b2abc0ce7c0b25e41fe517e830b83fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*KI4lTehNPG9uulJbNz3fXQ.png"/></div></figure><p id="d530" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了使身份映射适用于网络，作者使用了一个名为<strong class="kt ir">的快捷连接</strong>的方法。这种方法的主要直觉是与其学习函数F(x)，不如学习函数F(x) + x .学习一个恒等式映射更容易；由于层权重都调整为0，它将产生一个身份映射，而不是零映射。此外，它是可区分的，因此端到端是可训练的。</p><p id="2427" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">快捷连接的另一个考虑是在恒等式中增加<strong class="kt ir">投影</strong>。由于快捷方式连接的层之间的尺寸可以不同，因此有三个考虑因素。a)在增加的尺寸上补零，B)投影快捷方式仅用于尺寸改变的部分，C)所有快捷方式都是投影。下表是对每种情况的估计。(ResNet-34后面的A、B和C表示在ResNet-34中应用的A)、B)和C)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/9b70217859b9877cc3e4ced5c629ff7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*AwbUbsJlEAP1srog93AVEw.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">关注第二行框</p></figure><p id="0c2a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">结果显示<strong class="kt ir">在身份上进行投射并不会严重影响绩效</strong>。改变参数的数量使得与普通网络的比较更加困难。因此，作者简单地在网络中使用了身份映射。</p><h1 id="bfb0" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">整体主干</h1><p id="9a40" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">要参考网络的详细结构，请参考论文。</p><p id="d30a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">链接:【https://arxiv.org/pdf/1512.03385.pdf T4】</p><h1 id="316f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">实验</h1><p id="4a1d" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">他们比较了两个网络:普通网络和ResNet。两个网络使用相同的层；然而，只有ResNet具有快捷连接。他们在两个数据集上进行了实验:ImageNet和CIFAR-10。下面的图表是实验的结果。</p><p id="f036" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">(细曲线表示训练错误，粗曲线表示验证错误)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/cd41d4125606e495ef2237ee4c17a4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*kLg0w80Wy_W3hj-PWX5RFg.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">ImageNet上平面网络的性能</p></figure><p id="b2e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从图中可以看出，训练误差随着层数的增加而增加。这意味着<strong class="kt ir">普通网络正遭受退化问题</strong>。ResNet怎么样？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7fdce8480b5716ccf0cfe507c0a72e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*WnCxOJ7uQ_HYHXf6TnzoRw.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">ResNet在ImageNet上的性能</p></figure><p id="3985" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="mo">不再有退化问题</em> </strong>。随着层数的增加，它们的训练误差减小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8a1cfe7da8a33db9cdd39f71eb94b57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*xN-iFiRr5KiKYtb5IABVTA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">ImageNet上的实验结果</p></figure><p id="e79b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作者在ResNet中添加了更多层，以制作更复杂的模型。正如所料，增加层数可以提高性能。当在CIFAR-10上进行实验时，这种趋势是相似的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/4596aa185183076fdc29839a5f177e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*y1MJ6gARQ94mbWVpE4GssQ.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">CIFAR-10的实验结果</p></figure><p id="2c48" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是，我们可以观察到，在网络上使用1202层，性能会显著下降。论文认为这是由于过度拟合造成的。即使性能有显著下降，它仍然优于原始方法。</p><h1 id="c78c" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">结论</h1><p id="9a44" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">ResNet的动机是解决退化问题。通过直观的方法，他们设计了适合身份映射近似的网络。实验表明，<strong class="kt ir"> ResNet很好地解决了退化问题，</strong>然而，它对于极深的网络工作不佳。</p><p id="e85b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我很感激对我的文章的任何反馈，对于任何讨论，欢迎你给我发电子邮件。如果有什么不对或者误解的地方，请告诉我。:)</p><p id="a542" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">联系我:jeongyw12382@postech.ac.kr </strong></p><p id="e702" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">进一步阅读</strong></p><p id="7d8f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">D2匹配解释:</p><p id="3608" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" href="https://medium.com/towards-artificial-intelligence/d2-net-matching-problem-among-images-under-an-extreme-appearance-changes-9f059f33a2ef" rel="noopener">https://medium . com/forward-artificial-intelligence/D2-net-matching-problem-in-images-under-a extreme-appearance-changes-9f 059 f 33 a2 ef</a></p></div></div>    
</body>
</html>