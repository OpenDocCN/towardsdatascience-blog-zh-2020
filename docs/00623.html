<html>
<head>
<title>Lightning Fast XGBoost on Multiple GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多个GPU上的闪电般快速XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lightning-fast-xgboost-on-multiple-gpus-32710815c7c3?source=collection_archive---------9-----------------------#2020-01-18">https://towardsdatascience.com/lightning-fast-xgboost-on-multiple-gpus-32710815c7c3?source=collection_archive---------9-----------------------#2020-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/71b3720e0e04b753ecf3b0d618fce6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_zcIX9q-1wMjEzdzgSDag.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67646" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/WikiImages-1897/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67646" rel="noopener ugc nofollow" target="_blank">维基图片</a></p></figure><div class=""/><div class=""><h2 id="4e62" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">无需大量的代码更改</h2></div><p id="b5c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">XGBoost是数据科学中最常用的库之一。</p><p id="f352" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在XGBoost出现的时候，它比它最接近的竞争对手Python的Scikit-learn GBM快得多。但随着时间的推移，它已经被一些令人敬畏的库如LightGBM和Catboost所竞争，无论是在速度还是准确性上。</p><p id="ae1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就我而言，我使用LightGBM来处理大多数我刚刚得到CPU进行培训的用例。但是当我拥有一个或多个GPU时，我仍然喜欢用XGBoost进行训练。</p><p id="02d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为什么？</p><p id="9ae4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我可以将XGBoost提供的优秀GPU功能与Dask结合使用，在单GPU和多GPU模式下使用XGBoost。</p><p id="4d8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">怎么会？</p><p id="2ff0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">这个帖子是关于在多GPU机器上运行XGBoost的。</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="15e0" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据集:</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mu"><img src="../Images/47d69c3994c5273c49db93e8e89c25e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4DN-CG5IC7PC6g7ptBsow.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">UCI·希格斯</p></figure><p id="8334" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用<a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/HIGGS" rel="noopener ugc nofollow" target="_blank"> UCI希格斯数据集</a>。这是一个具有11M行和29列的二进制分类问题，可能需要相当长的时间来解决。</p><p id="29a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从UCI网站:</p><blockquote class="mz na nb"><p id="2433" class="ky kz lu la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated">这些数据是使用蒙特卡罗模拟产生的。前21个特征(第2-22列)是由加速器中的粒子探测器测量的运动特性。后7个特征是前21个特征的函数；这些是由物理学家得出的高级特征，有助于区分这两个类别。人们有兴趣使用深度学习方法来消除物理学家手动开发这些特征的需要。基准测试结果使用贝叶斯决策树从一个标准的物理包和5层神经网络提出了在原来的文件。最后500，000个示例用作测试集。</p></blockquote><p id="4374" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过使用我从NVidia post 的<a class="ae jg" href="https://devblogs.nvidia.com/gradient-boosting-decision-trees-xgboost-cuda/" rel="noopener ugc nofollow" target="_blank">借用的漂亮函数将这个数据集加载到内存中。</a></p><figure class="mv mw mx my gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="034a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个函数下载Higgs数据集，并创建Dmatrix对象供以后XGBoost使用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a4bd" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:CPU方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/aba79c1dac1bbbd217465d6ae7f94f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*QR5K7Fo-AL2LZVhQ.jpg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/illustrations/processor-cpu-computer-chip-board-2217771/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="46a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们已经加载了数据，所以我们可以用CPU来训练XGBoost模型，以便进行基准测试。</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="0171" class="nn md jj nj b gy no np l nq nr">print("Training with CPU ...")<br/>param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silent'] = 1<br/>param['tree_method'] = 'hist'</span><span id="1014" class="nn md jj nj b gy ns np l nq nr">tmp = time.time()<br/>cpu_res = {}<br/>xgb.train(param, dtrain, num_round, evals=[(dtest, "test")], <br/>          evals_result=cpu_res)<br/>cpu_time = time.time() - tmp<br/>print("CPU Training Time: %s seconds" % (str(cpu_time)))</span><span id="7d32" class="nn md jj nj b gy ns np l nq nr">---------------------------------------------------------------</span><span id="8c72" class="nn md jj nj b gy ns np l nq nr">CPU Training Time: <strong class="nj jk">717.6483490467072 seconds</strong></span></pre><p id="09fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这段代码耗时717秒，大约需要12分钟才能完成。这很好，值得称赞，但我们能做得更好吗？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cb8e" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:单一GPU方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/bdf34baa8bc295cb3401b43e41f0857f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fGmypzPwH_0lRUpI.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.google.com/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiKwMPphI3nAhXtzzgGHeLNB1QQjhx6BAgBEAI&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;psig=AOvVaw3QFPz_lTQrqraHJQz_ewwh&amp;ust=1579433051897517" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="fe77" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最棒的是，我们不需要对上面的代码做太多修改，就可以使用单个GPU来构建模型。</p><blockquote class="nu"><p id="d3e0" class="nv nw jj bd nx ny nz oa ob oc od lt dk translated">我们可以用GPU为什么要用CPU？</p></blockquote><p id="8201" class="pw-post-body-paragraph ky kz jj la b lb oe kk ld le of kn lg lh og lj lk ll oh ln lo lp oi lr ls lt im bi translated">我们将<code class="fe oj ok ol nj b">tree_method</code>改为<code class="fe oj ok ol nj b">gpu_hist</code></p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="2b70" class="nn md jj nj b gy no np l nq nr">print("Training with Single GPU ...")</span><span id="73b9" class="nn md jj nj b gy ns np l nq nr">param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silent'] = 1<br/><strong class="nj jk">param['tree_method'] = 'gpu_hist'</strong><br/>tmp = time.time()<br/>gpu_res = {}</span><span id="d4d3" class="nn md jj nj b gy ns np l nq nr">xgb.train(param, dtrain, num_round, evals=[(dtest, "test")], <br/>          evals_result=gpu_res)<br/>gpu_time = time.time() - tmp<br/>print("GPU Training Time: %s seconds" % (str(gpu_time)))</span><span id="87f7" class="nn md jj nj b gy ns np l nq nr">----------------------------------------------------------------<br/>GPU Training Time: 78.2187008857727 seconds</span></pre><p id="6010" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">我们实现了10倍的加速</em> </strong>，我们的模型现在只需1.3分钟就能完成。这很好，但是如果我们有多个GPU，我们能做得更好吗？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6d52" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:多GPU方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/8e86a18055ccdec11acc9ea156a62c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*KCE7f875hrIFeRv5.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.nvidia.com%2Fcontent%2Fdam%2Fen-zz%2FSolutions%2Ftitan%2Ftitan-rtx%2Fnvidia-titan-rtx-nvlink-300-t%402x.jpg&amp;imgrefurl=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;docid=LXUW4PiXbaOvoM&amp;tbnid=2aobaeCTl1m0CM%3A&amp;vet=10ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg..i&amp;w=600&amp;h=408&amp;client=ubuntu&amp;bih=2025&amp;biw=1879&amp;q=gpu%20titan%20rtx%20dual&amp;ved=0ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg&amp;iact=mrc&amp;uact=8" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="488a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，我的机器中有2个GPU，而上面的代码只使用了1个GPU。随着GPU现在变得越来越便宜，集群拥有4个以上的GPU并不罕见。那么我们可以同时使用多个GPU吗？</p><blockquote class="nu"><p id="40ad" class="nv nw jj bd nx ny nz oa ob oc od lt dk translated">两个GPU总比一个好</p></blockquote><p id="4d99" class="pw-post-body-paragraph ky kz jj la b lb oe kk ld le of kn lg lh og lj lk ll oh ln lo lp oi lr ls lt im bi translated">要使用MultiGPUs，过程并不是像上面那样加一点参数那么简单，涉及到几个步骤。</p><p id="d4c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先是数据加载的差异:</p><figure class="mv mw mx my gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="320a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据加载有多个步骤，因为我们需要dask DMatrix对象来训练XGBoost使用多个GPU。</p><ol class=""><li id="261d" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated">使用熊猫阅读CSV文件。</li><li id="eafb" class="on oo jj la b lb ow le ox lh oy ll oz lp pa lt os ot ou ov bi translated">从Pandas数据帧创建Dask数据帧，以及</li><li id="1835" class="on oo jj la b lb ow le ox lh oy ll oz lp pa lt os ot ou ov bi translated">使用dask数据帧创建Dask数据矩阵对象。</li></ol><p id="1bd3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使用多GPU来训练XGBoost，我们需要使用Dask来创建一个GPU集群。这个命令创建了一个GPU集群，dask稍后可以通过使用<code class="fe oj ok ol nj b">client</code>对象来使用它。</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="340c" class="nn md jj nj b gy no np l nq nr">cluster = LocalCUDACluster()<br/>client = Client(cluster)</span></pre><p id="059b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以加载Dask Dmatrix对象并定义训练参数。注意<code class="fe oj ok ol nj b">nthread</code>被设置为<code class="fe oj ok ol nj b">one</code>，而<code class="fe oj ok ol nj b">tree_method</code>被设置为<code class="fe oj ok ol nj b">gpu_hist</code></p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="7245" class="nn md jj nj b gy no np l nq nr">ddtrain, ddtest = load_higgs_for_dask(client)</span><span id="18d4" class="nn md jj nj b gy ns np l nq nr">param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silence'] = 1<br/><strong class="nj jk">param['tree_method'] = 'gpu_hist'<br/>param['nthread'] = 1</strong></span></pre><p id="6e60" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以在多个GPU上训练，使用:</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="09c9" class="nn md jj nj b gy no np l nq nr">print("Training with Multiple GPUs ...")<br/>tmp = time.time()<br/>output = <strong class="nj jk">xgb.dask.train</strong>(client, param, ddtrain, num_boost_round=1000, evals=[(ddtest, 'test')])<br/>multigpu_time = time.time() - tmp<br/>bst = output['booster']<br/>multigpu_res = output['history']<br/>print("Multi GPU Training Time: %s seconds" % (str(multigpu_time)))<br/>---------------------------------------------------------------<br/>Multi GPU Training Time: 50.08211898803711 seconds</span></pre><p id="91f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意对<code class="fe oj ok ol nj b">xgb.train </code>的调用如何变为<code class="fe oj ok ol nj b">xgb.dask.train</code>，以及它如何也需要dask客户端来工作。</p><p id="1ed4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这大约需要0.8分钟，比单个GPU的速度提高了1.5倍。我只有2个GPU可供我使用，所以我不能测试它，但我相信它是线性增长的，即更多的GPU和更多的时间减少。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e084" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结果</h1><p id="6b78" class="pw-post-body-paragraph ky kz jj la b lb pb kk ld le pc kn lg lh pd lj lk ll pe ln lo lp pf lr ls lt im bi translated">以下是所有三种设置的结果:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/67c8592a165c4e4955c1000b4eeab31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzeChCwmlxbRAiTf6yJXUg.png"/></div></div></figure><p id="f578" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然多CPU和单CPU之间的区别现在看起来有些多余，但在运行手头的<a class="ae jg" rel="noopener" target="_blank" href="/automate-hyperparameter-tuning-for-your-models-71b18f819604">多个超参数调优任务</a>时，可能需要运行多个具有不同超参数的GBM模型，这种区别将是相当可观的。</p><p id="3853" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，当我们将其扩展到许多GPU时，这个结果可能会发生变化。</p><p id="1e82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以继续缩放。</p><p id="1a7a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/blob/master/xgb_dask/XGB%20with%20Dask%2BGPU.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到这篇文章的完整代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="31c4" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">继续学习</h1><p id="0a75" class="pw-post-body-paragraph ky kz jj la b lb pb kk ld le pc kn lg lh pd lj lk ll pe ln lo lp pf lr ls lt im bi translated">如果你对深度学习感兴趣，并想为此使用你的GPU，我想推荐这门关于计算机视觉中的<a class="ae jg" href="https://coursera.pxf.io/7mKnnY" rel="noopener ugc nofollow" target="_blank">深度学习的优秀课程。</a></p><p id="c6a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">中</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过Twitter<a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">@ mlwhiz</strong></a>联系</p><p id="c023" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>