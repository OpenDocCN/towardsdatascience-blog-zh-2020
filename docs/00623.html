<html>
<head>
<title>Lightning Fast XGBoost on Multiple GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多个 GPU 上的闪电般快速 XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lightning-fast-xgboost-on-multiple-gpus-32710815c7c3?source=collection_archive---------9-----------------------#2020-01-18">https://towardsdatascience.com/lightning-fast-xgboost-on-multiple-gpus-32710815c7c3?source=collection_archive---------9-----------------------#2020-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/71b3720e0e04b753ecf3b0d618fce6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_zcIX9q-1wMjEzdzgSDag.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67646" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/WikiImages-1897/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67646" rel="noopener ugc nofollow" target="_blank">维基图片</a></p></figure><div class=""/><div class=""><h2 id="4e62" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">无需大量的代码更改</h2></div><p id="b5c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">XGBoost 是数据科学中最常用的库之一。</p><p id="f352" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 XGBoost 出现的时候，它比它最接近的竞争对手 Python 的 Scikit-learn GBM 快得多。但随着时间的推移，它已经被一些令人敬畏的库如 LightGBM 和 Catboost 所竞争，无论是在速度还是准确性上。</p><p id="ae1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就我而言，我使用 LightGBM 来处理大多数我刚刚得到 CPU 进行培训的用例。但是当我拥有一个或多个 GPU 时，我仍然喜欢用 XGBoost 进行训练。</p><p id="02d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为什么？</p><p id="9ae4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我可以将 XGBoost 提供的优秀 GPU 功能与 Dask 结合使用，在单 GPU 和多 GPU 模式下使用 XGBoost。</p><p id="4d8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">怎么会？</p><p id="2ff0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">这个帖子是关于在多 GPU 机器上运行 XGBoost 的。</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="15e0" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据集:</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mu"><img src="../Images/47d69c3994c5273c49db93e8e89c25e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4DN-CG5IC7PC6g7ptBsow.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">UCI·希格斯</p></figure><p id="8334" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用<a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/HIGGS" rel="noopener ugc nofollow" target="_blank"> UCI 希格斯数据集</a>。这是一个具有 11M 行和 29 列的二进制分类问题，可能需要相当长的时间来解决。</p><p id="29a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从 UCI 网站:</p><blockquote class="mz na nb"><p id="2433" class="ky kz lu la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated">这些数据是使用蒙特卡罗模拟产生的。前 21 个特征(第 2-22 列)是由加速器中的粒子探测器测量的运动特性。后 7 个特征是前 21 个特征的函数；这些是由物理学家得出的高级特征，有助于区分这两个类别。人们有兴趣使用深度学习方法来消除物理学家手动开发这些特征的需要。基准测试结果使用贝叶斯决策树从一个标准的物理包和 5 层神经网络提出了在原来的文件。最后 500，000 个示例用作测试集。</p></blockquote><p id="4374" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过使用我从 NVidia post 的<a class="ae jg" href="https://devblogs.nvidia.com/gradient-boosting-decision-trees-xgboost-cuda/" rel="noopener ugc nofollow" target="_blank">借用的漂亮函数将这个数据集加载到内存中。</a></p><figure class="mv mw mx my gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="034a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个函数下载 Higgs 数据集，并创建 Dmatrix 对象供以后 XGBoost 使用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a4bd" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:CPU 方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/aba79c1dac1bbbd217465d6ae7f94f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*QR5K7Fo-AL2LZVhQ.jpg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/illustrations/processor-cpu-computer-chip-board-2217771/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="46a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们已经加载了数据，所以我们可以用 CPU 来训练 XGBoost 模型，以便进行基准测试。</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="0171" class="nn md jj nj b gy no np l nq nr">print("Training with CPU ...")<br/>param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silent'] = 1<br/>param['tree_method'] = 'hist'</span><span id="1014" class="nn md jj nj b gy ns np l nq nr">tmp = time.time()<br/>cpu_res = {}<br/>xgb.train(param, dtrain, num_round, evals=[(dtest, "test")], <br/>          evals_result=cpu_res)<br/>cpu_time = time.time() - tmp<br/>print("CPU Training Time: %s seconds" % (str(cpu_time)))</span><span id="7d32" class="nn md jj nj b gy ns np l nq nr">---------------------------------------------------------------</span><span id="8c72" class="nn md jj nj b gy ns np l nq nr">CPU Training Time: <strong class="nj jk">717.6483490467072 seconds</strong></span></pre><p id="09fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这段代码耗时 717 秒，大约需要 12 分钟才能完成。这很好，值得称赞，但我们能做得更好吗？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cb8e" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:单一 GPU 方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/bdf34baa8bc295cb3401b43e41f0857f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fGmypzPwH_0lRUpI.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.google.com/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiKwMPphI3nAhXtzzgGHeLNB1QQjhx6BAgBEAI&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;psig=AOvVaw3QFPz_lTQrqraHJQz_ewwh&amp;ust=1579433051897517" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="fe77" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最棒的是，我们不需要对上面的代码做太多修改，就可以使用单个 GPU 来构建模型。</p><blockquote class="nu"><p id="d3e0" class="nv nw jj bd nx ny nz oa ob oc od lt dk translated">我们可以用 GPU 为什么要用 CPU？</p></blockquote><p id="8201" class="pw-post-body-paragraph ky kz jj la b lb oe kk ld le of kn lg lh og lj lk ll oh ln lo lp oi lr ls lt im bi translated">我们将<code class="fe oj ok ol nj b">tree_method</code>改为<code class="fe oj ok ol nj b">gpu_hist</code></p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="2b70" class="nn md jj nj b gy no np l nq nr">print("Training with Single GPU ...")</span><span id="73b9" class="nn md jj nj b gy ns np l nq nr">param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silent'] = 1<br/><strong class="nj jk">param['tree_method'] = 'gpu_hist'</strong><br/>tmp = time.time()<br/>gpu_res = {}</span><span id="d4d3" class="nn md jj nj b gy ns np l nq nr">xgb.train(param, dtrain, num_round, evals=[(dtest, "test")], <br/>          evals_result=gpu_res)<br/>gpu_time = time.time() - tmp<br/>print("GPU Training Time: %s seconds" % (str(gpu_time)))</span><span id="87f7" class="nn md jj nj b gy ns np l nq nr">----------------------------------------------------------------<br/>GPU Training Time: 78.2187008857727 seconds</span></pre><p id="6010" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">我们实现了 10 倍的加速</em> </strong>，我们的模型现在只需 1.3 分钟就能完成。这很好，但是如果我们有多个 GPU，我们能做得更好吗？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6d52" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">XGBoost:多 GPU 方法</h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/8e86a18055ccdec11acc9ea156a62c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*KCE7f875hrIFeRv5.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.nvidia.com%2Fcontent%2Fdam%2Fen-zz%2FSolutions%2Ftitan%2Ftitan-rtx%2Fnvidia-titan-rtx-nvlink-300-t%402x.jpg&amp;imgrefurl=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;docid=LXUW4PiXbaOvoM&amp;tbnid=2aobaeCTl1m0CM%3A&amp;vet=10ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg..i&amp;w=600&amp;h=408&amp;client=ubuntu&amp;bih=2025&amp;biw=1879&amp;q=gpu%20titan%20rtx%20dual&amp;ved=0ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg&amp;iact=mrc&amp;uact=8" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="488a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，我的机器中有 2 个 GPU，而上面的代码只使用了 1 个 GPU。随着 GPU 现在变得越来越便宜，集群拥有 4 个以上的 GPU 并不罕见。那么我们可以同时使用多个 GPU 吗？</p><blockquote class="nu"><p id="40ad" class="nv nw jj bd nx ny nz oa ob oc od lt dk translated">两个 GPU 总比一个好</p></blockquote><p id="4d99" class="pw-post-body-paragraph ky kz jj la b lb oe kk ld le of kn lg lh og lj lk ll oh ln lo lp oi lr ls lt im bi translated">要使用 MultiGPUs，过程并不是像上面那样加一点参数那么简单，涉及到几个步骤。</p><p id="d4c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先是数据加载的差异:</p><figure class="mv mw mx my gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="320a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据加载有多个步骤，因为我们需要 dask DMatrix 对象来训练 XGBoost 使用多个 GPU。</p><ol class=""><li id="261d" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated">使用熊猫阅读 CSV 文件。</li><li id="eafb" class="on oo jj la b lb ow le ox lh oy ll oz lp pa lt os ot ou ov bi translated">从 Pandas 数据帧创建 Dask 数据帧，以及</li><li id="1835" class="on oo jj la b lb ow le ox lh oy ll oz lp pa lt os ot ou ov bi translated">使用 dask 数据帧创建 Dask 数据矩阵对象。</li></ol><p id="1bd3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使用多 GPU 来训练 XGBoost，我们需要使用 Dask 来创建一个 GPU 集群。这个命令创建了一个 GPU 集群，dask 稍后可以通过使用<code class="fe oj ok ol nj b">client</code>对象来使用它。</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="340c" class="nn md jj nj b gy no np l nq nr">cluster = LocalCUDACluster()<br/>client = Client(cluster)</span></pre><p id="059b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以加载 Dask Dmatrix 对象并定义训练参数。注意<code class="fe oj ok ol nj b">nthread</code>被设置为<code class="fe oj ok ol nj b">one</code>，而<code class="fe oj ok ol nj b">tree_method</code>被设置为<code class="fe oj ok ol nj b">gpu_hist</code></p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="7245" class="nn md jj nj b gy no np l nq nr">ddtrain, ddtest = load_higgs_for_dask(client)</span><span id="18d4" class="nn md jj nj b gy ns np l nq nr">param = {}<br/>param['objective'] = 'binary:logitraw'<br/>param['eval_metric'] = 'error'<br/>param['silence'] = 1<br/><strong class="nj jk">param['tree_method'] = 'gpu_hist'<br/>param['nthread'] = 1</strong></span></pre><p id="6e60" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以在多个 GPU 上训练，使用:</p><pre class="mv mw mx my gt ni nj nk nl aw nm bi"><span id="09c9" class="nn md jj nj b gy no np l nq nr">print("Training with Multiple GPUs ...")<br/>tmp = time.time()<br/>output = <strong class="nj jk">xgb.dask.train</strong>(client, param, ddtrain, num_boost_round=1000, evals=[(ddtest, 'test')])<br/>multigpu_time = time.time() - tmp<br/>bst = output['booster']<br/>multigpu_res = output['history']<br/>print("Multi GPU Training Time: %s seconds" % (str(multigpu_time)))<br/>---------------------------------------------------------------<br/>Multi GPU Training Time: 50.08211898803711 seconds</span></pre><p id="91f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意对<code class="fe oj ok ol nj b">xgb.train </code>的调用如何变为<code class="fe oj ok ol nj b">xgb.dask.train</code>，以及它如何也需要 dask 客户端来工作。</p><p id="1ed4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这大约需要 0.8 分钟，比单个 GPU 的速度提高了 1.5 倍。我只有 2 个 GPU 可供我使用，所以我不能测试它，但我相信它是线性增长的，即更多的 GPU 和更多的时间减少。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e084" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结果</h1><p id="6b78" class="pw-post-body-paragraph ky kz jj la b lb pb kk ld le pc kn lg lh pd lj lk ll pe ln lo lp pf lr ls lt im bi translated">以下是所有三种设置的结果:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/67c8592a165c4e4955c1000b4eeab31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzeChCwmlxbRAiTf6yJXUg.png"/></div></div></figure><p id="f578" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然多 CPU 和单 CPU 之间的区别现在看起来有些多余，但在运行手头的<a class="ae jg" rel="noopener" target="_blank" href="/automate-hyperparameter-tuning-for-your-models-71b18f819604">多个超参数调优任务</a>时，可能需要运行多个具有不同超参数的 GBM 模型，这种区别将是相当可观的。</p><p id="3853" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，当我们将其扩展到许多 GPU 时，这个结果可能会发生变化。</p><p id="1e82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以继续缩放。</p><p id="1a7a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/blob/master/xgb_dask/XGB%20with%20Dask%2BGPU.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到这篇文章的完整代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="31c4" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">继续学习</h1><p id="0a75" class="pw-post-body-paragraph ky kz jj la b lb pb kk ld le pc kn lg lh pd lj lk ll pe ln lo lp pf lr ls lt im bi translated">如果你对深度学习感兴趣，并想为此使用你的 GPU，我想推荐这门关于计算机视觉中的<a class="ae jg" href="https://coursera.pxf.io/7mKnnY" rel="noopener ugc nofollow" target="_blank">深度学习的优秀课程。</a></p><p id="c6a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">中</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter<a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">@ mlwhiz</strong></a>联系</p><p id="c023" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>