<html>
<head>
<title>Consider Multicollinearity in My Model or Not?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是否在我的模型中考虑多重共线性？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/consider-multicollinearity-in-my-model-or-not-7aca16e74773?source=collection_archive---------31-----------------------#2020-05-16">https://towardsdatascience.com/consider-multicollinearity-in-my-model-or-not-7aca16e74773?source=collection_archive---------31-----------------------#2020-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b1fa" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="e397" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">简要讨论是否有必要修复特征空间中的多重共线性。希望对你设计下一个计算实验有帮助。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fd207da001c2a795107330f31f43ffde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MIwPNXnISdDgo_cD"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">马丁·桑切斯在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d371" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">作为一个超级篮球迷，我曾经写过一篇博客，讲述如何使用线性回归模型根据一名 NBA 球员的场均数据来预测他的工资。我被我的一个粉丝挑战了。</span></p><p id="ed9a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">他坚持认为我的管道是错误的，因为独立变量之间的多重共线性在将它们汇集到回归模型之前没有得到解决。</p><p id="9bf6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我会说，他一定是一个好学生，记得统计课程中所教的内容。但是无论何时建模，我们真的应该关心我们的特征空间中的多重共线性吗？</p><p id="ea1b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看情况。</p><p id="3a09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我想通过一个小而有趣的实验来讨论这个话题。在这个例子中，我不会从理论统计的角度来评论任何事情。让我们只阅读代码并检查结果。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="5825" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">问题。</h2><p id="ab82" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">谁是 NBA 2019-2020 赛季的年度最佳新秀(罗伊)？</p><h2 id="eb08" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">特征空间。</h2><p id="3ee3" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">NBA 新秀从 1990 年<em class="nr">到 2019 年</em>的基本数据，包括新秀赛季的比赛(' G ')、场均上场时间(' MP ')、场均得分(' PTS ')、场均篮板总数(' AST ')、场均助攻数(' STL ')、场均盖帽数(' BLK ')、投篮命中率(' FG% ')、三分球命中率(' 3P% ')和罚球命中率(' FT% ')。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/995b6758246b706127c0d346f840bb5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJGjqPU0xO0wpUC6cmL-aA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">玩具数据的头</p></figure><h2 id="f9f4" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">因变量。</h2><p id="dd84" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我用<strong class="lk jd">罗伊投票份额</strong>作为因变量，范围从 0 到 1。投票份额越高，成为<strong class="lk jd">罗伊</strong>的几率就越高。我没有把它框架到一个分类问题上，为了避免一个不平衡的训练集。对如何从头到尾生成一个 ML 项目的细节感兴趣的可以参考<a class="ae lh" rel="noopener" target="_blank" href="/whos-the-mvp-of-nba-this-season-3e347c66a40a">我的另一个帖子</a>。</p><h2 id="3613" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">回归模型。</h2><p id="2f22" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我尝试了两个机器学习回归模型(<strong class="lk jd">弹性网</strong>和<strong class="lk jd">随机森林</strong>)和一个经典线性回归模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/8c44ef4717bbea49921454de216992b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*boLEGIQc3khJKu-DtyY5qw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">超参数调谐功能。</p></figure><p id="69eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的函数显示了超参数调整函数，其中我可以使用交叉验证来确定每个模型的超参数。调整后的超参数将用于最终的模型训练。</p><p id="dd79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基本线性回归在管道内定义如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/df1432e8fbfc9dd12fbc122b7cbb029a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aosj_kMZbJvm2S4JHYE_yw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">基本线性回归模型</p></figure><h2 id="97d4" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">超参数调整和模型训练。</h2><p id="e8f2" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我使用分割数据集训练上述模型。数据分割和超参数调整代码如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/bb9c717093970c1c06e5531cbd4ea91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhCQvPXxRyXx4ow32uigLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据分割代码。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/88eab15d1d737202e904896d14f01a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rvoX4oqIUAKvwnBBEEwfQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">调谐过程。</p></figure><p id="349b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面代码中调整过的超参数然后用于训练最终的模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/b34aba23c13c4518b38237fdf6ba838f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQAuVbSk-28hte8H5YAHtw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最终模型训练函数。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/576a07928d4aa52998fe76cf4c2d19ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOLEl3_q19tz2BugGUeziw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练最终模型。</p></figure><p id="6830" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我也训练了基本的线性回归模型如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/3a7ba3c03003552c2acdde7b0c0846c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*yStBLpcepODQ8KSJ926oFQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">拟合基本线性回归模型。</p></figure><h2 id="6498" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">绩效评估。</h2><p id="5fbf" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我在测试数据上评估了三个模型的性能。我计算回归模型的 MSE。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/0b07db41e6eb3ce34d360c7118aa999c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gzX4_J8BFOQPJYKBVEOhg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有所有特征的三个模型的 MSE。</p></figure><p id="1c6b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我发现模型 2(随机森林回归)是所有三个模型中最好的，测试数据 MSE 等于 0.055。</p><p id="70aa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">整个建模过程已经完成。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="9211" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">从特征空间中移除相关变量。</h2><p id="cd8c" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">现在，让我们处理数据，以便排除彼此相关的特征。我简单地检查了下面的相关性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/8ab7ff0aa2e157143449adbef1cd8eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9YDV1lMGD-mEIYsnIs-MJA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据集相关性。</p></figure><p id="b811" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">似乎每场比赛的得分(PTS)和上场时间(MP)与皮尔森的相关系数等于 82%高度相关。因此，我首先从数据集中删除 MP，然后再次重复整个建模过程。以下是测试数据 MSE:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/04f3bfb451d271dc1493bd4f0f190eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUqenempNoOrPAL0T4kyKw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从特征集中移除 MP 后三个模型的 MSE。</p></figure><p id="7a6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我还从整个特征空间中移除 PTS，并重复整个建模过程。这是测试数据的 MSE 结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e3a9523fab1628f2dc6b175a8b7b955c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*FYgUfQHGS6vX3hK_L8ltUw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从特征集中移除 PTS 后三个模型的 MSE。</p></figure><p id="3bf8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上图所示，在解决了特征集中的共线性后，三个模型的性能几乎与完整模型相同。</p><p id="599e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些结果表明，从特征空间中移除共线性对我们数据集中的<strong class="lk jd">罗伊</strong>的<strong class="lk jd">预测</strong>没有帮助。</p><p id="ad70" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">顺便说一句，所有的模型对我们的问题给出了相同的答案。预测<a class="ae lh" href="https://en.wikipedia.org/wiki/Ja_Morant" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Ja 莫兰特</strong> </a>将是 NBA 2019–2020 赛季的<strong class="lk jd">罗伊</strong>。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="564f" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">何时考虑多重共线性？</h2><p id="1905" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">然而，为什么人们强调解决回归模型中多重共线性的重要性，而它对预测没有影响呢？这与模型的解释有关。</p><p id="284a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设我们感兴趣的是检测球员的哪些基本数据对预测贡献最大。然后，我们需要小心多重共线性。</p><p id="8166" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">高度相关的变量可以相互竞争来解释模型中因变量的方差。具体来说，这种竞争将体现在弹性净回归和线性回归中的系数上，或者体现在随机森林回归中的特征重要性上。</p><p id="8c72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于第二个模型(随机森林回归器)是我实验中最好的模型，我将用它作为例子来解释。</p><p id="9152" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我分别画出了随机森林模型<em class="nr"> 1) </em>具有所有特征、<em class="nr"> 2) </em>具有除 MP 之外的所有特征和<em class="nr"> 3) </em>具有除 PTS 之外的所有特征的特征重要性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/31a9b18e3658322322d590bf5b6d860b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Buy_EMTRSG9IRAjdDrdmkw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">所有变量的特征重要性。</p></figure><p id="c7fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我使用所有特征时，我发现 PTS 是模型中最重要的变量，而 MP 看起来一点也不重要。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/c32335d28b1b06dc39d926fa2b75f222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0FMnRmpLcw8s-0SBHlxGDQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">没有 MP 的特性重要性。</p></figure><p id="e301" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我从特征空间中移除 MP，模型仍然给出 PTS 作为最重要的变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/e824827d043fa7fa62581923351696db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEDahWVjqfywC0q-2fpo6w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">不含 PTS 的特征重要性。</p></figure><p id="5a44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如果我把 PTS 从特征空间中去掉，如上图所示，MP 就成了最重要的变量。</p><p id="2e48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从这些结果中我们可以看出，MP 和 PTS 对于预测都是重要的，但是如果将它们作为独立变量一起放入模型中，它们在特征重要性上是相互竞争的。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="97da" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">结论</h2><p id="b7e9" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我们需要修复回归模型中的多重共线性吗？</p><p id="ff69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看情况。</p><p id="186b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果回归器的目的是<strong class="lk jd">纯粹的预测</strong>，数据集<strong class="lk jd">中存在的多重共线性根本没有害处</strong>。它的预测能力不受相关变量的影响。</p><p id="2b28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果回归变量的目的是<strong class="lk jd">了解自变量和因变量之间的关系</strong>，则需要解决<strong class="lk jd">多重共线性问题</strong>，因为相关变量会在解释因变量时相互竞争。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="5cf1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">希望这篇文章能对你的机器学习项目有所帮助。如果你喜欢读这篇文章，请在媒体上<a class="ae lh" href="https://medium.com/@jianan.jay.lin" rel="noopener"> <strong class="lk jd">跟随我</strong> </a>。如果您对数据科学文章感兴趣，这些帖子可能会对您有所帮助:</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/pheatmap-draws-pretty-heatmaps-483dab9a3cc"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">Pheatmap 绘制了漂亮的热图</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">一个关于如何在 r 中用 pheatmap 生成漂亮的热图的教程。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow lb oi"/></div></div></a></div><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/present-the-feature-importance-of-the-random-forest-classifier-99bb042be4cc"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">给出了随机森林分类器的特征重要性</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">如何建立一个随机森林分类器，提取特征重要性，并漂亮地呈现出来。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow lb oi"/></div></div></a></div><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/a-practical-suggestion-in-linear-regression-cb639fd5ccdb"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">线性回归中的一个实用建议</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">从弹性网开始，记得调好定义 l1 范数之比的超参数。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="oy l ot ou ov or ow lb oi"/></div></div></a></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/313d6e89f77fc4f6b3bfdd3ada6f9060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HAWCGyJQPc0jbRdi"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">内森·杜姆劳在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>