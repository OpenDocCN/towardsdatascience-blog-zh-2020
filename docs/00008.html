<html>
<head>
<title>An Introduction to Decision Trees with Python and scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python和scikit决策树简介-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f?source=collection_archive---------7-----------------------#2020-01-01">https://towardsdatascience.com/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f?source=collection_archive---------7-----------------------#2020-01-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4044" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是一个完整的指南，帮助你对决策树有一个直观的理解和数学上的理解，从而用scikit实现你的第一个模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1303982916c18d179b8954c54a396348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGX0_gacojVa6-njlCrWZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最大深度为2、准确率为79.1%的简单决策树</p></figure><p id="e9a7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树是一组强大的监督机器学习模型，可用于分类和回归。随着<em class="lr"> XGBoost </em>库的兴起，决策树已经成为在竞争中提供最佳结果的一些机器学习模型。</p><p id="398a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种模型存在于许多不同变体中；<em class="lr">随机森林，极度随机树，自适应增强，梯度增强</em>等等。然而，在本文中，我们将只探索非常标准的决策树的基础。</p><h1 id="5c6e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">决策树的主要思想</h1><p id="06c0" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们从决策树背后非常基本的思想开始，即根据一些特征分割我们的数据，以获得尽可能好的数据分组。那么这到底意味着什么呢？让我们考虑以下数据集:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="6ecc" class="mu lt iq mq b gy mv mw l mx my"><strong class="mq ir">      Pclass      Age      Fare      Survived</strong><br/>0          3       22       7,2             0<br/>1          1       38      71,3             1<br/>2          3       26       7,9             1<br/>3          1       35      53,1             1<br/>4          3       35       8,0             0</span></pre><p id="acd6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是来自经典的<em class="lr">泰坦尼克号数据集</em>的数据，你可以根据诸如阶级、年龄、费用等特征来预测这个人是否幸存。对于我们拥有的小样本数据，我们可以看到60% (3/5)的乘客幸存，40% (2/5)的乘客没有幸存。我们基本上想做的是，看看我们是否能在任何特征上分割这些数据，这样我们就能更好地预测结果。让我们看一看。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c931d55797da58dc43957cba625d12d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*MvQyjgsk8Y4IyZe0VQlQow.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">只有5个数据观察的基本决策树</p></figure><p id="be16" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好的，看起来，只要知道乘客的等级，我们就可以区分他们是一等舱还是三等舱，现在做一个预测，我们只得到一个错误(在右边，我们预测一个乘客没有生还，但那个乘客实际上幸存了)。这意味着我们现在只需进行一次分割就有80%的准确率(比我们猜测所有人都幸存下来的60%要好)。这显然是一个非常简单的解释，然而，这确实是决策树的主要思想:将组分成更多的<em class="lr">【纯】</em>子组(即，使得它们包含一个组的大多数)。然而，您可能会想:我们如何决定拆分哪个特性呢？我们如何决定在这个特性的什么地方进行分割？我们如何决定分几次？可能还有很多其他问题。让我们试着挖掘更多的细节。</p><h1 id="5c0b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">如何衡量分割是否良好</strong></h1><p id="e352" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们确实可以像上面看到的那样根据特征分割观察结果，但是我们如何真正决定一个分割是否产生了比初始数据更“纯”的两组数据。要理解如何衡量这一点，我们必须了解基尼系数<em class="lr">。基尼系数在数学上被描述为:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/efcce0a1079e01114faea460804183d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*uqN1EOvnfU3RgkyMOOt-4w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基尼系数的数学符号</p></figure><p id="3185" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，如果你，像我一样，并不真正喜欢数学符号，它真正的意思是，你把每组观察值的平方除以总观察值。在这种情况下，我们有两组(幸存和未幸存)，所以我们只需做两次，然后从一个中减去所有内容。此示例计算决策树初始节点的基尼系数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/384e45a1711f3b89c073883da051b82f.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*wwYUmU8xbuQMl1Vrw2iEyQ.png"/></div></figure><p id="7b89" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么这种基尼杂质实际上是如何起作用的，我们为什么要使用它呢？首先，重要的是要知道，这不是唯一的措施，但它的使用相当广泛。基尼系数有一个很好的特点，当分割变得非常不相等时(例如，仅包含“<em class="lr">幸存”</em>观察值)，基尼系数接近0，当两组包含完全相同数量的“<em class="lr">幸存</em>和“<em class="lr">未幸存</em>观察值时，基尼系数为0.50。因此，我们的想法是，在新的分裂中，我们希望降低两个节点的基尼系数。尝试回到上面的基本决策树的表示，并看到两个新节点的基尼系数实际上更低(尝试看看您是否可以自己计算两个节点的基尼系数)。还要注意，左分裂的基尼系数是0.00，因为我们在这一侧只有一个类(幸存)，因此我们不能使这一侧更加'<em class="lr">纯粹'。</em>对于左侧，它只是从0.48到0.44变得稍微更纯。</p><h1 id="69b4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">理解决策树算法</h1><p id="328d" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">既然我们已经知道了一个分裂的好坏分类，让我们深入研究一下分裂树的算法实际上是如何工作的。毫不奇怪，该算法实际上正如我们所预期的那样工作，即遍历数据中每个单一特征的每个可能的分割，计算每种情况下的基尼系数，然后根据找到的最低加权基尼系数进行分割。然后，根据数据被分割成的每组数据重复这些步骤，当不再有能够减少<em class="lr">基尼系数杂质</em>的分割时，停止这些步骤。这个算法叫做CART，是sklearn已经实现的，</p><p id="0d32" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这实际上相当简单，但手动操作起来相当困难，因为潜在的拆分数量太大，手动操作会花费太多时间。如果我们回到之前缩短的数据集，这实际上意味着我们必须在<em class="lr"> Pclass </em>上模拟一个拆分(尽管这很容易，因为只有一个潜在的拆分)，然后在<em class="lr">年龄上模拟一个拆分(因为35年发生了两次，所以总共有3个潜在的拆分)，</em>最后我们必须模拟票价的拆分(然后有4个不同的潜在拆分，因为所有的值都是唯一的—记住，我们没有5个拆分，因为我们需要在每个组中至少放置1个观察值)。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="de95" class="mu lt iq mq b gy mv mw l mx my"><strong class="mq ir">      Pclass      Age      Fare      Survived</strong><br/>0          3       22       7,2             0<br/>1          1       38      71,3             1<br/>2          3       26       7,9             1<br/>3          1       35      53,1             1<br/>4          3       35       8,0             0</span></pre><p id="9234" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于我们模拟的每一个分裂，我们显然必须计算两边的基尼系数。值得注意的是，我们根据两个<em class="lr">基尼系数的<em class="lr">加权平均值</em>来评估后分割。</em>假设我们有500个观察值，很可能我们会以某种方式进行拆分，在一组中只有1个观察值，而在另一组中有499个观察值。在只有一个观察值的组中，我们的基尼系数为0.00(请记住，这是当所有数据都被正确分类时的数据)，另一方面，根据数据的分布，我们的基尼系数可能接近0.50。如果我们只取正态平均值，那么基尼系数的平均值为0.25，而加权平均值为(0.00 * 1 + 0.50 * 499) / 500 = 0.499。至少对我来说，我们取一个加权平均值似乎更合理，而不是给一个分裂以相同的权重，我们只忽略一个观察值。</p><p id="4f16" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们实际上可以将所有分割的模拟放入一个表中，因为数据量并不是很大。我们看到两个最好的分割将是<em class="lr"> Pclass &lt; 3 </em>和<em class="lr"> Fare &lt; 53.1 — </em>这确实是我们在上面的图像<em class="lr">中看到的。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/88d69d50710deec8dad8908131d85c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kcLBcUsLZjeU5pGbuS5Ng.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">寻找决策树的最佳分割</p></figure><p id="7ac5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于决策树，值得注意的一点是，即使我们进行了最优分割，我们也不一定知道这是否会导致后续节点的最优分割。因此，当我们决定在<em class="lr"> Pclass </em>上进行拆分时，这是局部最优的拆分，我们不知道这种拆分是否会导致树中进一步的最优拆分。由于这个原因，决策树算法通常被称为'<em class="lr">贪婪'</em>算法，因为我们不计算全局最优分裂。如果我们必须这样做，算法会慢得多(对于相当大的数据集，甚至可能无法执行)。</p><h1 id="54d4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">用Python实现算法</h1><p id="c763" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">CART算法实际上可以很容易地用Python实现，我在下面的GitHub Gist中提供了这一点。代码没有以任何方式优化，只是为了与泰坦尼克号数据集一起工作。如果可以在其他数据集上运行，您可以尝试对se进行一些调整。此外，它只是计算最佳的第一次分裂。我特意决定这样做，因为更多的代码函数只会使它变得更大，我只是想传达一个单独的拆分是如何进行的，因为这实际上是进行下一个拆分的相同过程。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="6c4a" class="mu lt iq mq b gy mv mw l mx my"><strong class="mq ir">Out [1]:</strong></span><span id="2e84" class="mu lt iq mq b gy nf mw l mx my">Best split-feature     : Sex<br/>Value to split on      : 0.0<br/>Weighted gini impurity : 0.33454</span></pre><p id="66e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们试着回到用sklearn DecisionTreeClassifier训练的决策树的第一张图片，我没有提到它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1303982916c18d179b8954c54a396348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGX0_gacojVa6-njlCrWZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最大深度为2、准确率为79.1%的简单决策树</p></figure><p id="1381" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们看到，该决策树的加权<em class="lr">基尼系数</em>正好是0.33454，我们可以通过以下方式计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/2a11ef07ad29735b260113625dc3cc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*nO765TnqNVzRkg8wDJe_-g.png"/></div></figure><p id="8d0d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望这篇文章对你更深入的了解决策树，理解算法的工作原理有所帮助。请随意留下您的评论或问题。我目前还在撰写一篇关于兰登森林的文章，一旦准备好，我会马上链接到这里。</p></div></div>    
</body>
</html>