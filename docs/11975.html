<html>
<head>
<title>How Much Juice Is There In Your Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的数据有多少价值？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-much-juice-is-there-in-your-data-d3e76393ca9d?source=collection_archive---------51-----------------------#2020-08-18">https://towardsdatascience.com/how-much-juice-is-there-in-your-data-d3e76393ca9d?source=collection_archive---------51-----------------------#2020-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4959" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">应用于 Kaggle 的信息论答案，带代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/85236992025a85d9304de43ba58391ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1U8GQIVN9oLO-oc4cZD7Pw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b98c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 97 </span> %。根据 Gartner 的数据，这是组织未使用的数据的百分比，构成了所谓的“<a class="ae md" href="https://www.gartner.com/en/information-technology/glossary/dark-data" rel="noopener ugc nofollow" target="_blank">黑暗数据</a>”。</p><p id="530d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据已经取代石油成为世界上最有价值的资源，但几乎所有的数据仍未被组织使用。Gartner 估计，87%的组织“商业智能和分析成熟度较低”。</p><p id="7d42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对这一悖论的一种可能解释是，并非所有数据都是生而平等的。从一个组织到另一个组织，以及在同一个组织内从一个项目到另一个项目，数据的价值可能有很大的不同。</p><p id="2b1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了在商业智能和分析方面进行必要的投资，组织应该能够准确地预测这样做的业务影响，并且这样的未来投资应该能够产生足够高的 ROI。</p><blockquote class="me"><p id="d142" class="mf mg it bd mh mi mj mk ml mm mn lt dk translated">如果没有勘探地球物理学的机器学习等价物，数据就不可能成为新的石油</p></blockquote><p id="4821" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">假设数据总是有价值的，并试图通过试错法提取这种价值作为人工智能项目的一部分，无论是由 AutoML 平台驱动，充其量都是非常浪费的，在最坏的情况下会产生负 ROI。</p><p id="caec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据石油类比，这就相当于假设无论在哪里看都有石油在地下<em class="mt">，并且驱动从地下提取的石油量的唯一因素是使用的提取技术</em>。</p><p id="216e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多年来，地球物理学和经济学交叉的整个研究领域，即<a class="ae md" href="https://en.wikipedia.org/wiki/Exploration_geophysics" rel="noopener ugc nofollow" target="_blank">勘探地球物理学</a>，一直致力于降低石油生产中的商业风险。勘探地球物理学依靠<strong class="la iu">归纳推理</strong>(与演绎推理相反)来检测给定位置有价值地质矿床的存在并估计其数量，而不会产生建设开采场地的前期成本和风险。</p><p id="ec2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，为了降低投资人工智能项目的商业风险，在进行任何预测建模之前并独立于预测建模，开发量化数据价值的归纳推理方法至关重要，这一阶段我们称为<strong class="la iu">预学习</strong>。</p><p id="1ce6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评估数据集的演绎方法包括首先分配资源以使用数据集，然后监控一段时间的业务影响，而归纳方法包括使用数学推理从感兴趣的数据集推断任何预测模型可以实现的最高性能，成本低廉，且无需训练任何预测模型。</p><p id="436a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们总结了使<a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/problem_formulation.html" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">能够预学习</strong> </a>的理论基础，并且我们说明了如何使用开源的<a class="ae md" href="https://github.com/kxytechnologies/kxy-python" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> kxy </strong> </a> python 包来量化各种数据集中的汁液。</p><blockquote class="me"><p id="f8c0" class="mf mg it bd mh mi mj mk ml mm mn lt dk translated">果汁是从什么开始的？</p></blockquote><p id="4dbf" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu"> <em class="mt">数据中的汁液指的是数据中对解决手头特定问题有用的信息量。</em>T11】</strong></p><div class="kj kk kl km gt ab cb"><figure class="mu kn mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/493c5ff3e5137ddf4da0f7549dffe2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*1OekGc_48g_baLVj"/></div></figure><figure class="mu kn na mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/526646a2a6465fa5e889e58af00dd0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*S4kH95HYYTCOZQZNl9LVLA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nb di nc nd translated">(左)Unsplash 上 Rinck Content Studio 的照片|(右)Instagram 用户@skwoodlekids 的插画</p></figure></div><p id="cca9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就像桔子汁(或地下的油)独立存在一样，无论是否被提取，也无论如何被提取，重要的是要认识到每个数据集在概念上都可以被认为是一个(可能是空的)部分，可以用来解决手头的问题，以及无用的剩余物。</p><p id="9935" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这方面，有两点值得强调。首先，什么是有用的(resp。没用)是问题特定的。数据集对解决特定问题无用的部分可能对解决另一个问题有用。</p><p id="dbc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其次，数据集中对解决给定问题有用的东西并不依赖于解决问题的特定方法。同样，给定橙子中包含的果汁总量是可以从橙子中提取的最大液体量，不管它是如何压榨的，数据集中的果汁总量是可以从数据集提取的最大效用量，以解决特定问题，不管使用什么机器学习模型来解决问题。</p><p id="0097" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了奠定预学习的基础，我们需要正式定义“问题”和“有用”的含义。</p><p id="4c0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们关心的问题是分类和回归问题，对输入或输出的类型没有限制。具体来说，我们考虑使用输入向量<strong class="la iu"><em class="mt">×输入向量</em> </strong>来预测业务结果<strong class="la iu"> <em class="mt"> y </em> </strong> <em class="mt"> </em>。选择<strong class="la iu"> <em class="mt"> y </em> </strong>是我们有兴趣解决的业务问题的固有属性，而输入<strong class="la iu"> <em class="mt"> x </em> </strong>代表我们考虑用来解决问题的数据集。</p><p id="73eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像往常一样，我们通过将它们建模为随机变量来表示我们对<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>的值的不确定性。说我们的数据集对于解决感兴趣的问题是有用的，相当于说输入<strong class="la iu"> <em class="mt"> x </em> </strong>是关于标签/输出<strong class="la iu"> <em class="mt"> y </em> </strong>的信息。</p><p id="49a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，信息量和关联的概念被信息论完全形式化了。你会在这里找到一本关于信息论的初级读本。出于本文的目的，如果我们将随机变量的熵表示为<strong class="la iu"> <em class="mt"> h(z) </em> </strong>连续变量的熵表示为微分，分类变量的熵表示为香农，就足以回忆起关于<strong class="la iu"><em class="mt"/></strong>x<strong class="la iu"><em class="mt">y</em></strong>信息量的规范度量是它们的<strong class="la iu"> <em class="mt">互信息，</em> </strong>定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/acbac73fc17364e62fdbe521b96db186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyGpY3qhhm-lR_uP4Mt4RA.png"/></div></div></figure><blockquote class="me"><p id="4e6d" class="mf mg it bd mh mi nf ng nh ni nj lt dk translated">互信息的一些关键性质</p></blockquote><p id="4ed0" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu">互信息<em class="mt">I(y；x) </em> </strong>明确定义了输出是分类的还是连续的，以及输入是连续的、分类的还是两者的组合。关于为什么会出现这种情况的一些背景阅读，请查看本书的<a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/quantifying_informativeness.html" rel="noopener ugc nofollow" target="_blank">和其中的参考文献。</a></p><p id="48e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它总是非负的，当且仅当<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>是统计独立的(即<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>之间没有任何关系)。</p><p id="a026" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，互信息通过无损特征变换是不变的。的确，如果<strong class="la iu"> <em class="mt"> f </em> </strong>和<strong class="la iu"> <em class="mt"> g </em> </strong>是两个一一对应的映射，那么</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9fdc69badf8e8a6973bb2fa1a7fa9c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*3fuu_RMAgvRcV6C-YIjOxg.png"/></div></div></figure><p id="5702" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个更一般的结果，被称为<a class="ae md" href="https://en.wikipedia.org/wiki/Data_processing_inequality" rel="noopener ugc nofollow" target="_blank">数据处理不等式</a>，陈述了应用于<strong class="la iu"> <em class="mt"> x </em> </strong>的变换只能减少其与<strong class="la iu"> <em class="mt"> y </em> </strong>的互信息。具体来说，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/012ce9a58a24b19bf006135e6baa1cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*2Q8MVBqguYkzuL0YqkZTGg.png"/></div></figure><p id="9c81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且当<strong class="la iu"> <em class="mt"> f </em> </strong>或者是一对一映射，或者<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>是统计独立给定的<strong class="la iu"> <em class="mt"> f(x) </em> </strong>时，等式成立</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ff0299d4ff8ebf3fc58506ff0317df13.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*pz2oU3P4lNGIYhA-gnUOUg.png"/></div></figure><p id="1146" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着包含在<strong class="la iu"> <em class="mt"> x </em> </strong>中的关于<strong class="la iu"> <em class="mt"> y </em> </strong>的所有信息都完全反映在<strong class="la iu"> <em class="mt"> f(x) </em> </strong>中，或者换句话说，转换<strong class="la iu"> <em class="mt"> f </em> </strong>保留了所有果汁，尽管有所损失。</p><p id="3e1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，当互信息被用作量化数据集中汁液量的代理时，有效的特征工程既不减少也不增加汁液量，这是相当直观的。特征工程只是将输入和/或输出转换成一种表示，使训练特定的机器学习模型变得更容易。</p><blockquote class="me"><p id="4ced" class="mf mg it bd mh mi mj mk ml mm mn lt dk translated">从互信息到最大可实现的性能</p></blockquote><p id="d64c" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">虽然它反映了数据集中果汁量的本质，但通常以比特或 NAT 表示的互信息值很难与业务分析师或决策者交流。</p><p id="a49a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，它可以用来计算使用<strong class="la iu"> <em class="mt"> x </em> </strong>预测<strong class="la iu"><em class="mt"/></strong>y 时所能达到的最高性能，对于各种性能指标(R、RMSE、分类精度、每样本对数似然等。)，进而可以转化为业务成果。我们在下面提供了一个简短的总结，但是你可以在这里找到更多的<a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="12c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们考虑一个具有预测概率的预测模型<em class="mt"> M </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/606c0f2822477cab0b3ec41e808f1229.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*SVa11dHeDW6Mt6n4ugoV-w.png"/></div></div></figure><p id="b531" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu"> <em class="mt"> f(x) </em> </strong>是模型对与输入<strong class="la iu"> <em class="mt"> x </em> </strong>相关联的输出的预测。当<strong class="la iu"> <em class="mt"> y </em> </strong>为分类时，该模型为分类器，当<strong class="la iu"> <em class="mt"> y </em> </strong>为连续时，该模型为回归模型。</p><h2 id="64ed" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#a-achievable-r-2" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">最大可实现 R </strong> </a></h2><p id="5ca8" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在加法回归模型中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0bb03870cf0c3d2007d756291b8e7990.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*ZZ0KfGcCzL9g-FZPKl9RIQ.png"/></div></figure><p id="5205" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">R 的总体版本定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/c30a3f1021acf166ba8f40bc1bac4fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2vDQV7_Ujj7qa0WWCy-hA.png"/></div></div></figure><p id="36e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的模型下，上面公式中的比率代表了不能用输入来解释的输出方差的分数。</p><p id="1f69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然方差是高斯分布不确定性的良好度量，但与熵不同，它是其他分布不确定性的弱度量。</p><p id="ddc3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到<strong class="la iu">熵</strong>(在 nats 中)<strong class="la iu">与</strong>(自然)<strong class="la iu"/><strong class="la iu">标准差</strong>的对数具有相同的单位/标度，我们将 R 概括如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/09dc3db3a6f1fc76344f470053cbe3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ewEMxMaiZSI1b8SchJAK0Q.png"/></div></figure><p id="826b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，当<strong class="la iu"> <em class="mt"> (y，f(x)) </em> </strong>为联合高斯(如高斯过程回归，包括带有高斯加性噪声的线性回归)时，上述信息调整后的 R 与原始 R 相同。</p><p id="c128" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="mt">更一般地，这种信息调整的 R 适用于回归和分类，具有连续输入、分类输入或两者的组合。</em>T15】</strong></p><p id="0b35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直接应用<em class="mt">数据处理不等式</em>给出了任何模型预测<strong class="la iu"><em class="mt"/></strong>与<strong class="la iu"><em class="mt"/></strong>所能达到的最大 R:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8fc2d0e4d8c4f4c86ee6769cbd10dfdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*9MvQ-8PRpTUksBRbqhCFlQ.png"/></div></figure><p id="4beb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">重要的是要强调这个最优 R 不仅仅是一个上限；通过任何预测分布为真(数据生成)条件分布<strong class="la iu"> <em class="mt"> p(y|x) </em> </strong>的模型来实现。</p><h2 id="d033" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#d-regression-achievable-rmse" rel="noopener ugc nofollow" target="_blank">最小可实现 RMSE </a></h2><p id="fb82" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">上述模型的均方根误差的总体版本为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/30aad9c165b80296a94e602a12233a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UIRmnaS897KcuPvADG_P2w.png"/></div></div></figure><p id="b536" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，我们可以把它的信息调整概括定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a1632ca5d8d28990956c939d44bb67f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBXROIvni83z8_S6e_83hQ.png"/></div></div></figure><p id="df98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据处理不等式<em class="mt">的直接应用</em>给出了最小的 RMSE 任何模型使用<strong class="la iu"><em class="mt"/></strong>x<strong class="la iu"><em class="mt"/></strong>预测都能达到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/18ce33321d70a60e6e1cf09965b32a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmCWWMyrmrvYflSI2M-LHg.png"/></div></div></figure><h2 id="b0d2" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#b-achievable-true-log-likelihood" rel="noopener ugc nofollow" target="_blank">最大可达真实对数似然</a></h2><p id="1f41" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">类似地，我们模型的每次观察的样本对数似然性可以定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3f6ff0f25f77ee09c7fd057142e0537f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*5QwuFbJtPYcNrbntIxdyEw.png"/></div></figure><p id="db72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其人口当量，我们称之为<em class="mt">每个观测值的真实对数似然</em>，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ca9c686df8e526241aae87fab0f456a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*B4f8QweR4KdmcWWyjfsnrg.png"/></div></figure><p id="8da0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">满足不等式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/6501bbd3682699f0e823354077a56d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*kasSpkWb6pgONeXHcgVM8A.png"/></div></figure><p id="e287" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个不等式源于吉布斯不等式和数据处理不等式。更多详情见<a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#b-achievable-true-log-likelihood" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="690a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，上述不等式适用于回归和分类问题，并且上限是通过使用真条件<strong class="la iu"><em class="mt">【p(y | x)】</em></strong>作为其预测分布的模型来实现的。</p><p id="8ebb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">术语<strong class="la iu"> <em class="mt"> -h(y) </em> </strong>表示在没有任何数据的情况下可以实现的每个观测的最佳真实对数似然，并且可以被视为天真的对数似然基准，而互信息术语<strong class="la iu"><em class="mt">I(y；x) </em> </strong>代表可归因于我们数据的提升。</p><h2 id="5b4f" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#c-achievable-classification-accuracy" rel="noopener ugc nofollow" target="_blank">可达到的最大分类精度</a></h2><p id="6363" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在输出<strong class="la iu"> <em class="mt"> y </em> </strong>可以取高达<strong class="la iu"> <em class="mt"> q </em> </strong>的不同值的分类问题中，也可以通过使用<strong class="la iu"> <em class="mt"> x </em> </strong>来预测<strong class="la iu"> <em class="mt"> y. </em> </strong>的模型来表达可以实现的最高分类精度</p><p id="17ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑一下功能</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d7a38361eea6512e0eaec55520bbf24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvOOryqsfuY6-UgeJtrWqg.png"/></div></div></figure><p id="6bde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于给定的熵值<strong class="la iu"> <em class="mt"> h </em> </strong>，通过预测采用<strong class="la iu"> <em class="mt"> q </em> </strong>不同值的任何离散分布的结果可以实现的最佳精度，并且具有熵值<strong class="la iu"> <em class="mt"> h </em> </strong>由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/23fbf350c976a44b3b216ba16a495db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*_a_agwQde64kCH2pqz9d2g.png"/></div></figure><p id="102e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2a58fdd54a694c0ed7cec0eaef4d1854.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*2cK9cIEkrBowaeLVHuvhDg.png"/></div></figure><p id="ba39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是的反函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/57ab25ccc7532444d22d701b7a40dcee.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*tUe7NXql5-d9CMmdd2bEiQ.png"/></div></figure><p id="422a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且很容易进行数值评估。你可以在这里找到更多细节<a class="ae md" href="https://www.kxy.ai/reference/latest/theoretical_foundation/memoryless/applications.html#c-achievable-classification-accuracy" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="9c43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图为各种<strong class="la iu"><em class="mt"/></strong><em class="mt">q 提供了上述功能的说明。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/aeb10892c3565732a84dbe9be42b5744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toAtr8RE7WHz1YU6CNtXgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="pb">图 1:预测具有 q 个可能结果的离散分布的结果时可达到的精度。</em></p></figure><p id="b9fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更一般地，使用<strong class="la iu"><em class="mt"/></strong>预测分类输出<strong class="la iu"> <em class="mt"> y </em> </strong>取<strong class="la iu"> <em class="mt"> q </em> </strong>不同值的分类模型<strong class="la iu"> <em class="mt">可以达到的精度满足不等式:</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/43632356db70c8b259ab129dff75dea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*U96QSzWOtmS3OtiNOdZlRg.png"/></div></figure><p id="1aa0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">熵项<strong class="la iu"> <em class="mt"> h(y) </em> </strong>反映了由总是预测最频繁结果组成的简单策略的准确性，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0c860f539e949ee14f4abc4f90f70985.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*neHvUVpRBvHrwrK2eI4guA.png"/></div></figure><p id="522b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">鉴于互信息项<strong class="la iu"><em class="mt">I(y；x) </em></strong></p><blockquote class="pe pf pg"><p id="7acc" class="ky kz mt la b lb lc ju ld le lf jx lg ph li lj lk pi lm ln lo pj lq lr ls lt im bi translated">总而言之，在分类和回归问题中，几乎任何基于群体的性能度量所能达到的最高值都可以表示为真实数据生成分布<em class="it"/><strong class="la iu"><em class="it">I(y；x) </em> </strong>和输出的可变性的度量(如其熵<strong class="la iu"><em class="it">【h(y)</em></strong>、方差或标准差)<strong class="la iu"> <em class="it"> </em> </strong>)当天真(无输入)预测策略不具有无效性能时。</p></blockquote><blockquote class="me"><p id="d852" class="mf mg it bd mh mi nf ng nh ni nj lt dk translated">无模型互信息估计</p></blockquote><p id="5d79" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">最后，我们可以解决房间里的大象。显然，这都归结为估计互信息<strong class="la iu"><em class="mt">I(y；x) </em> </strong>在真实数据下生成分布。然而，我们并不知道真正的联合分布<strong class="la iu"><em class="mt">【y，x】</em></strong>。如果我们知道它，我们就能获得最好的预测模型——具有预测分布的模型——真条件<strong class="la iu"> <em class="mt"> p(y|x) </em> </strong>！</p><p id="064a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，我们不需要知道或学习真正的联合分布<strong class="la iu"><em class="mt">【y，x】</em></strong>；这就是我们前面提到的归纳推理方法发挥作用的地方。</p><p id="73b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们采用的归纳方法包括测量足够宽范围的数据属性，间接揭示其中的结构/模式，并推断与观察到的属性一致的交互信息，而不做任何额外的任意假设。我们凭经验观察的属性越灵活，我们在数据中捕获的结构就越多，我们的估计就越接近真实的互信息。</p><p id="e318" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了有效地做到这一点，我们依靠一些技巧。</p><h2 id="bb4e" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><strong class="ak">招数一:</strong>在 copula-uniform 对偶空间中工作。</h2><p id="d16e" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">首先，我们回忆一下，<strong class="la iu"><em class="mt"/></strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>之间的互信息是一对一映射不变的，特别地，当<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>为序数时，<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>之间的互信息等于</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/8a66383bdade7a0de6df5a6f26a2b1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*DyQl9SRQI2R0zCm_PZfLvg.png"/></div></figure><p id="2ce1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不是直接估计<strong class="la iu"> <em class="mt"> y </em> </strong>和<strong class="la iu"> <em class="mt"> x </em> </strong>之间的互信息，而是估计它们的 copula-uniform 对偶表示之间的互信息；我们称之为在 copula-uniform 对偶空间中工作的<em class="mt">。</em></p><p id="e5f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这允许我们完全绕过边际分布，并以一种单位/比例/表示自由的方式进行推断——在对偶空间中，所有边际分布在[0，1]上都是一致的！</p><h2 id="891d" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated">技巧二:通过成对 Spearman 等级相关性揭示模式。</h2><p id="ade2" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">我们通过估计原始空间中的所有成对 Spearman 等级相关性来揭示我们的数据中的结构，为两个序数标量<strong class="la iu"> <em class="mt"> x </em> </strong>和<strong class="la iu"> <em class="mt"> y </em> </strong>定义如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/3687440e29dd36e723d0109e2980c856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfy4diaGuVRACxPTRBD-pQ.png"/></div></div></figure><p id="b28b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它衡量两个变量单调相关的倾向。它的人口版本仅仅是 copula 的函数——统一的对偶表示法<strong class="la iu"><em class="mt"/></strong>和<strong class="la iu"><em class="mt">v</em></strong><strong class="la iu"><em class="mt">x</em></strong>和<strong class="la iu"> <em class="mt"> y </em> </strong>并读作</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/bea7153e01771e0c83dee38e242dabd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoeewsj-qT-AoNUC-XlzJg.png"/></div></div></figure><p id="5328" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，使用 Spearman 的秩相关，我们可以在对偶空间中工作，同时有效地估计原始空间中感兴趣的属性。</p><p id="5712" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果没有这个技巧，我们将需要估计边际 CDF，并明确地将概率积分变换应用于输入，以便能够在对偶空间中工作，这将违背第一个技巧的目的。</p><p id="5307" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">毕竟，假设两个变量之间的互信息不依赖于它们的边际分布，那么不得不估计边际分布来计算它将是一种耻辱。</p><h2 id="de57" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated"><strong class="ak"> <em class="pb">招数三:扩大输入空间捕捉非单调模式。</em> </strong></h2><p id="1393" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">对于回归问题，成对 Spearman 秩相关完全捕获“输出随着特定输入减少/增加”类型的模式，对于分类问题，“我们可以根据特定输入取大值还是小值来判断编码输出的一位是 0 还是 1”。</p><p id="59e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了捕捉这些单调关联类型之外的模式，我们需要求助于另一个技巧。我们注意到，对于任何不是内射的函数<strong class="la iu"> <em class="mt"> f </em> </strong>，我们有</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c8ddff0ef26b6be97263702e67cbb018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*b6wpyJnvjGPP9Q5Zd5UTKA.png"/></div></figure><p id="e79c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由此可见，与其估计<strong class="la iu"><em class="mt">I(y；x) </em> </strong>，我们可以估计<strong class="la iu"><em class="mt">I(y；x，f(x)) </em> </strong>对于任意内射函数<strong class="la iu"> <em class="mt"> f </em> </strong>。</p><p id="8362" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而<strong class="la iu"> <em class="mt"> y </em> </strong>与<strong class="la iu"> <em class="mt"> x </em> </strong>之间的成对 Spearman 秩相关性揭示了<strong class="la iu"> <em class="mt"> y </em> </strong>与<strong class="la iu"> <em class="mt"> x </em> </strong>、<strong class="la iu"> <em class="mt"> f </em> </strong>之间的单调关系，因此可以选择<strong class="la iu"> <em class="mt"> y </em> </strong>与<strong class="la iu"> <em class="mt"> f(x) </em> </strong>之间的成对 Spearman 相关性</p><p id="20a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="mt"> f </em> </strong>的一个很好的例子就是函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/81ecf297be10d6ba61b50e4f5c12c3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*l-Rr10YCdl5Cq6-_e2CVkw.png"/></div></figure><p id="8054" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<em class="mt"> m </em>可以选择为样本均值、中值或众数。</p><p id="c95d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">的确，如果<strong class="la iu"><em class="mt">y =</em><em class="mt">x</em></strong>对于某个均值为零且偏斜为零的随机变量<strong class="la iu"><em class="mt"/></strong>x，那么<strong class="la iu"><em class="mt"/></strong>y 与<strong class="la iu"><em class="mt">【x，</em> </strong>之间的 Spearman 秩相关关系可以通过对称性<em class="mt">，</em>发现为<em class="mt"/><em class="mt">，</em>不能揭示其中的结构<em class="mt"> </em>另一方面，<em class="mt"/><strong class="la iu"><em class="mt">【y】</em></strong><em class="mt"/>和<strong class="la iu"><em class="mt">【x】</em></strong>(同<strong class="la iu"> <em class="mt"> m=0 </em> </strong>)，也就是<strong class="la iu"> <em class="mt"> </em> 1 </strong>，更能体现出<strong class="la iu">的信息量有多大</strong></p><p id="cf35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">选择<strong class="la iu"> <em class="mt"> f </em> </strong>捕捉“当输入偏离标准值时，输出趋于减少/增加”类型的模式。使用相同的技巧可以捕获更多类型的模式，包括周期性/季节性等。</p><h2 id="bc3d" class="no np it bd nq nr ns dn nt nu nv dp nw lh nx ny nz ll oa ob oc lp od oe of og bi translated">诀窍四:用最大熵原理把所有东西放在一起，以此来避免武断的假设。T101】</h2><p id="941d" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">综上，我们定义<strong class="la iu"> <em class="mt"> z=(x，f(x)) </em> </strong>，我们估计向量<strong class="la iu"> <em class="mt"> (y，z)</em></strong>的 Spearman 秩自相关矩阵即<strong class="la iu"> <em class="mt"> S(y，z)。</em>T113】</strong></p><p id="aec5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们使用与通过斯皮尔曼秩自相关矩阵<strong class="la iu"><em class="mt">【S(y，z)】、</em> </strong>观察到的模式相匹配的所有 copula 密度中具有最高熵的 copula 密度作为<strong class="la iu"> <em class="mt"> (y，z) </em> </strong>的 copula 统一表示的密度，即，除了通过<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>观察到的模式之外，对于每个模式最不确定。</p><p id="ca47" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设<strong class="la iu"> <em class="mt"> (y，z) </em> </strong>是<em class="mt"> d </em>维的，得到的变分优化问题为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/98a5fbbf06b2176935cc2f580254b6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3VpIJMHoRMdilKlINcFQA.png"/></div></div></figure><p id="50b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们使用学习的联合 pdf 来估计所需的互信息，如</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/3ffcaf975b4d4cf12e3dadee2055a1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*j90sn-o0OvPzG5vRudPAiw.png"/></div></figure><blockquote class="me"><p id="7897" class="mf mg it bd mh mi nf ng nh ni nj lt dk translated">理解最大熵变分问题</p></blockquote><p id="0db9" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">在没有 Spearman 秩相关约束的情况下，上述最大熵问题的解是标准均匀分布的 pdf，对应于假设<strong class="la iu"><em class="mt"/></strong>y<strong class="la iu"><em class="mt">x</em></strong>统计独立，且具有<em class="mt"> 0 </em>互信息。这很直观，因为我们没有理由相信<strong class="la iu"> <em class="mt"> x </em> </strong>是关于<strong class="la iu"> <em class="mt"> y </em> </strong>的信息，直到我们收集到经验证据。</p><p id="2633" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们观察<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>时，变分最大熵问题的新解偏离均匀分布刚好足以反映<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>所捕捉到的模式。因此，不应该期望我们的方法过高估计真实的互信息<strong class="la iu"><em class="mt">I(y；x) </em> </strong>。</p><p id="3a12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，只要<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>足够有表现力，我们可以通过选择函数<strong class="la iu"> <em class="mt"> f </em> </strong>来控制，所有类型的模式都将反映在<strong class="la iu"><em class="mt">【S(y，z) </em> </strong>中，并且我们估计的互信息不应被期望低估真实的互信息<strong class="la iu"><em class="mt">I(y；x) </em> </strong>。</p><blockquote class="me"><p id="bcc6" class="mf mg it bd mh mi mj mk ml mm mn lt dk translated">应用于正在进行的 Kaggle 竞赛</p></blockquote><p id="9931" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们构建了<a class="ae md" href="https://www.kxy.ai/" rel="noopener ugc nofollow" target="_blank"> KxY 平台</a>，以及附带的开源<a class="ae md" href="https://github.com/kxytechnologies/kxy-python" rel="noopener ugc nofollow" target="_blank"> python 包</a>，通过专注于高 ROI 项目和实验，帮助各种规模的组织削减其人工智能项目的风险和成本。特别令人感兴趣的是本文中描述的方法的实现，以量化数据中的果汁量。</p><p id="4e21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> kxy </strong>包可以从 PyPi ( <strong class="la iu"> pip install kxy </strong>)或 Github 安装，也可以通过我们在 DockerHub 上预先配置的 Docker 映像来访问——有关如何开始的更多详细信息，请阅读<a class="ae md" href="https://www.kxy.ai/reference/latest/quickstart/getting_started.html#Getting-Started" rel="noopener ugc nofollow" target="_blank"> this </a>。一旦安装完毕，<strong class="la iu"> kxy </strong>包需要一个 API 密匙来运行。您可以通过填写我们的<a class="ae md" href="https://www.kxy.ai/request-a-trial/" rel="noopener ugc nofollow" target="_blank">联系表</a>或发送电子邮件至 demo@kxy.ai 来申请。</p><p id="7947" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们以<a class="ae md" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">房价高级回归技术</a> Kaggle 竞赛为例。该问题包括使用 79 个解释变量的综合列表预测房屋销售价格，其中 43 个是分类变量，36 个是顺序变量。</p><p id="c526" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们发现，当使用独热编码方法来表示分类变量时，可以实现近乎完美的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/43b0490c858cfb3e172a1366f37f0d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XY5YKUpPMjOsNloeB_0DOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于房价高级回归 Kaggle 竞争的可实现性能分析的代码片段和输出。</p></figure><p id="fa82" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，当我们以贪婪的方式一次选择一个解释变量，总是在尚未选择的变量中选择产生最高增量果汁量的变量时，我们发现仅用 79 个变量中的 17 个，我们就可以实现近乎完美的预测——在 R 意义上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/7290385a75b85c9f76491f3889c30067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZq9J9yQpDRCTWxMnxPN-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贪婪变量选择分析的代码片段。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/64be7bff3568489f986cd19a7cd1b448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TE3C-3B8EwNErA5f0ijCQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">写这篇文章时 Kaggle 排行榜截图。</p></figure><p id="88bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图说明了全变量选择分析的结果。有趣的是，当前 Kaggle 排行榜的榜首已经设法产生了 0.00044 的 RMSE，这是使用前 15 个变量可以实现的最佳结果和使用前 16 个变量可以实现的最佳结果之间的某个位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/48c8c69e321be4c8666a605e6158a84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*88JE5y78aP04DFV5ppblPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签。Kaggle 房价高级回归技术竞赛 KxY 无模型变量选择分析结果。</p></figure><p id="78a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在这里找到用于生成上述结果的代码。</p><p id="5235" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">脚注:</p><p id="b152" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【】假设观测值(输入、输出)可以看作是从同一个随机变量<strong class="la iu"> <em class="mt"> (y，x) </em> </strong>中独立抽取的。特别是，问题不应该表现出任何时间依赖性，或者观察值(输入、输出)应该被视为来自静态和遍历时间序列的样本，在这种情况下，样本大小应该足够长，以跨越多个系统的内存。</p><p id="d716" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[ ]前一个脚注的要求具有非常实际的意义。整个推理管道依赖于对 Spearman 秩相关矩阵<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>的精确估计。当观测值可以被视为同分布样本时，对<strong class="la iu"><em class="mt">【y，z】</em></strong>的可靠估计只需要很小的样本量，就能指示真实的潜在现象。另一方面，当观察呈现时间依赖性时，如果使用我们的数据的不相交子集来估计<strong class="la iu"><em class="mt">【y，z】</em></strong>产生非常不同的值，则时间序列不是平稳的和遍历的，或者它是平稳的和遍历的，但是我们没有足够长的历史来表征<strong class="la iu"> <em class="mt"> S(y，z)；</em> </strong>无论哪种方式，估计的<strong class="la iu"> <em class="mt"> S(y，z) </em> </strong>都不会准确地表征真实的潜在现象，并且不应该应用分析。</p></div></div>    
</body>
</html>