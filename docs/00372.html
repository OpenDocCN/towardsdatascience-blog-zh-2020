<html>
<head>
<title>Can We Generate High-Quality Movie Reviews Using Language Models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们能用语言模型生成高质量的电影评论吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-we-generate-high-quality-movie-reviews-using-language-models-5158f494aea7?source=collection_archive---------25-----------------------#2020-01-11">https://towardsdatascience.com/can-we-generate-high-quality-movie-reviews-using-language-models-5158f494aea7?source=collection_archive---------25-----------------------#2020-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eed1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">微调IMDB电影评论的语言模型，并使用各种不同的方法生成电影评论。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9d3c9b053dd84a6b686b9eb15cdfbf25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7MjPotIIGaIGUkaa"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Ahmet Yal nkaya在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="afa1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="00c9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最近，语言模型——试图预测句子中的下一个单词的模型，通常是深度神经网络——引起了严重的轰动，因为OpenAI宣布，他们已经成功训练了一个名为“<a class="ae kv" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">【GPT-2】</a>的15亿参数语言模型，他们最初认为发布这个模型太危险了。它已经被释放了，现在你甚至可以和它说话了。</p><p id="d1df" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">上面链接的OpenAI博客和“与变形金刚对话”网站上的例子真的让我震惊，我开始想知道我们是否能够以某种方式生成高质量的电影评论，类似于<a class="ae kv" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDb数据集</a>，其中包括100，000条电影评论，分为25，000条正面评论，25，000条负面评论和50，000条未标记评论。然而，据我所知，完整的GPT-2模型——包含15亿个参数——如果不付出大量努力，就无法在单个GPU中安装，而且肯定也不适合我的个人GPU。所以我很好奇——标准语言模型在IMDb数据集上训练后产生的评论质量会怎么样？</p><h1 id="737d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设置</h1><p id="b97b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我决定使用fastai的<a class="ae kv" href="https://docs.fast.ai/text.html" rel="noopener ugc nofollow" target="_blank">提供的语言模型</a>进行一些实验，该语言模型已经在维基百科上进行了预训练(关于该模型的更多细节，请参见ULMFiT论文<a class="ae kv" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">这里</a>)。然后，我对整个IMDb数据集进行了微调，因为我不关心电影评论中表达的情感，我对情感分析也不感兴趣，所以我不需要维护数据集中提供的训练/测试分割。这种微调的目的是使模型专注于电影评论的领域，因为能够准确预测评论中的下一个词对于能够真实地生成电影评论中的下一个词是非常有用的，这是我们的目标。如果您对从另一个领域生成文本感兴趣，请随意使用您选择的语料库重复这些实验！</p><p id="b63f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我最近看到了<a class="ae kv" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank">“神经文本退化的奇怪案例”</a>，它深入探讨了现有的文本生成策略，强调了它们的不足，并提出了一些新的动机良好的成功方法，因此我决定比较每种方法生成的电影评论的质量。所有方法都有相同的提示(如下图)和复习长度(100字)，使用相同的模型进行推理。</p><h1 id="d0d7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">方法和结果</h1><p id="1c51" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这里，我将简要介绍评估的方法，并提供一些示例(为图像质量道歉)——这些示例是对每种方法运行一次模型的结果，没有经过挑选或篡改。</p><p id="997b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了增加可读性，实现细节和更多的例子，请查看底部的Github repo链接。</p><ul class=""><li id="1e7f" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">贪婪的top-1方法——由于语言模型输出一个概率向量，其大小与我们的词汇量相当，我们可以简单地总是获取具有最高概率的标记并将其返回，通过重复这个过程并不断地将生成的标记添加到现有提示中，我们可以生成完整的评论。</li><li id="c837" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">贪婪多项式方法-在这个版本中，我根据概率分布对令牌进行采样，而不是总是选择最可能的一个-这个过程仍然是贪婪的(仅考虑当前令牌的概率分布)，但导致重复结果较少。</li></ul><p id="ef07" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如我们在例子中看到的，贪婪的top-1的评论是重复的，不像自然语言。在我看来，贪婪多项式方法产生的评论出奇的好，但还不太好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/2eeffa7a968d7483ec5c1879a1d8a690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r__3LsO2BMw9_NkUZ8Xbeg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用“贪婪的前1名”方法生成评论的几个例子。方括号中的文字是给定的提示，“xxbos”代表一个评审的开始。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/c4606940c9104911646c87b9980766b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K91Lo_dURGzo51OEMzU76Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用“贪婪多项式”方法生成评论的几个例子。方括号中的文字是给定的提示，“xxbos”代表一个评审的开始。</p></figure><ul class=""><li id="694c" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">波束搜索方法—在这种方法中，不是最初只挑选具有最高概率的记号(top-1)，而是挑选具有最高概率的top-k <em class="nf"> </em>记号。该参数也称为“波束宽度”。在每一步，我们将k个标记中的每一个添加到现有的提示中，并且只保留到目前为止的前k个组合。理论上，这允许我们不仅基于它们之前的标记，而且基于它们之后的标记来挑选标记。然而，在实践中，这种方法经常失败，并最终生成极其重复的文本-所有top-k结果似乎都是模型的重复和“安全”选择的链，因此即使是最终top-k的多项式分布在大多数情况下也不会产生好的结果。有关这种特殊故障模式的更深入的分析，请参考论文。正如我们在例子中看到的，这种方法倾向于产生短期意义的句子，但非常重复，总体上不能产生高质量的评论。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/61d7b544f1cb131e23b753f52f0670d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCZOBtLePZ07qxFWcOqk0Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用“波束搜索”方法生成评论的几个例子。方括号中的文字是给定的提示，“xxbos”代表一个评审的开始。</p></figure><ul class=""><li id="fc67" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">Top-k方法——与波束搜索类似，我们现在选择具有最高概率的top-k <em class="nf"> </em>记号，而不是总是选择具有最高概率的记号。正如我们在例子中看到的，结果可能是偶然的——有时整个评论是连贯的，似乎是现实的，但在其他时候，句子放在一起没有多大意义。总的来说，在我看来，它似乎比以前的方法产生了更有趣和更多样的结果。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/a8687244eb42f0df1f01219d4a149e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPVkcalFEDjO7F-ShseIEA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用“top-k”方法生成评论的几个例子。方括号中的文字是给定的提示，“xxbos”代表一个评审的开始。</p></figure><ul class=""><li id="1f16" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">Top-p nucleus方法——本文提出这种方法是为了解决top-k方法中的“猜测k的数字”问题。与使用固定的k相比，根据概率分布来决定k更有意义— k应该是所需的任何数字，以便前k个令牌包含大部分概率质量。因此，在top-p nucleus方法中，我们为函数提供概率p，并且对于每个单词，它决定一个数字k，然后像以前一样执行top-k采样。总的来说，对我来说，质量似乎与top-k非常相似，但它可能更好地处理边缘情况(其中概率密度相对均匀，因此当它相对达到峰值时，我们会想要一个高k，而我们会想要一个小k)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/6b461690b953026b2c849a3f9307c311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txUxiZqxsAFUsNeG2JUhEA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用“top-p”方法生成评论的几个例子。方括号中的文字是给定的提示，“xxbos”代表一个评审的开始。</p></figure><p id="2acd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">更多的实现细节——我已经选择从生成中删除代表任何未知单词的特殊标记，并添加了对最小标记概率和概率分布温度的支持(除了每个方法采用的参数之外)。我还没有机会调整这些参数，所以通过一些实验，更好的文本生成是完全可能的。</p><h1 id="2cc8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="70d7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">总之，我们观察到文本生成的质量不仅取决于所用模型的大小，还取决于生成方法本身，不同的方法生成的评论质量大不相同——贪婪的top-1文本生成似乎产生低质量的文本，beam-search似乎产生极度重复的文本，top-k和top-p似乎产生比top-1质量更高的非重复文本。希望文本生成方法仍然是一个活跃的研究领域，不久我们将不再需要1.5B参数模型来生成真实的文本！</p><h1 id="5ab4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><ul class=""><li id="5787" class="mp mq iq lq b lr ls lu lv lx ni mb nj mf nk mj mu mv mw mx bi translated"><a class="ae kv" href="https://github.com/orendar/imdb_text_generation" rel="noopener ugc nofollow" target="_blank"> Github repo </a>提供完整代码和更多示例。</li><li id="4d3f" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank">霍尔茨曼，a .，买斯，j .，福布斯，m .，&amp;崔，Y. (2019)。神经文本退化的奇特案例。<em class="nf"> arXiv预印本arXiv:1904.09751 </em>。</a></li><li id="7cb2" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="noopener ugc nofollow" target="_blank"> Fast.ai的代码优先NLP课程</a>，启发了我写这篇文章！视频播放列表<a class="ae kv" href="https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" rel="noopener ugc nofollow" target="_blank">这里</a>。</li></ul></div></div>    
</body>
</html>