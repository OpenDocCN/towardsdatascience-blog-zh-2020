<html>
<head>
<title>Bayesian logistic regression with PyMC3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyMC3 的贝叶斯逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-logistic-regression-with-pymc3-8e17c576f31a?source=collection_archive---------11-----------------------#2020-09-02">https://towardsdatascience.com/bayesian-logistic-regression-with-pymc3-8e17c576f31a?source=collection_archive---------11-----------------------#2020-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2c08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一系列文章中的另一篇(参见<a class="ae kl" rel="noopener" target="_blank" href="/introduction-to-pymc3-a-python-package-for-probabilistic-programming-5299278b428">此处</a>和<a class="ae kl" rel="noopener" target="_blank" href="/a-b-testing-with-probabilistic-programming-and-pymc3-part-i-7ae52d45bc41">此处</a>的其他相关文章),一般来说是关于概率编程的，特别是关于 PyMC3 的。在我们之前的文章中，我们解释了 PyMC3 如何帮助统计推断。在本文中，我们将使用 PyMC3 解决一个端到端的分类问题。更准确地说，我们将使用 PyMC3 通过以下公共数据集进行贝叶斯逻辑回归:</p><p id="f423" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Occupancy+Detection+</a></p><p id="f677" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该数据集包含几个变量，如光线、温度、湿度和二氧化碳水平。目标是从这些变量中检测房间的占用情况。</p><p id="a9d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们需要加载几个相关的包。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="87f1" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">import</strong> <strong class="kr ir">arviz</strong> <strong class="kr ir">as</strong> <strong class="kr ir">az</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">matplotlib.pyplot</strong> <strong class="kr ir">as</strong> <strong class="kr ir">plt</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">numpy</strong> <strong class="kr ir">as</strong> <strong class="kr ir">np</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">pandas</strong> <strong class="kr ir">as</strong> <strong class="kr ir">pd</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">pymc3</strong> <strong class="kr ir">as</strong> <strong class="kr ir">pm</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">seaborn</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">theano.tensor</strong> <strong class="kr ir">as</strong> <strong class="kr ir">tt</strong><br/><strong class="kr ir">import</strong> <strong class="kr ir">warnings</strong><br/><strong class="kr ir">from</strong> <strong class="kr ir">IPython.core.pylabtools</strong> <strong class="kr ir">import</strong> figsize<br/><strong class="kr ir">import</strong> <strong class="kr ir">seaborn</strong> <strong class="kr ir">as</strong> <strong class="kr ir">sns</strong><br/><strong class="kr ir">from</strong> <strong class="kr ir">sklearn.metrics</strong> <strong class="kr ir">import</strong> (roc_curve, roc_auc_score, confusion_matrix, accuracy_score, f1_score, <br/>                             precision_recall_curve) <br/><strong class="kr ir">from</strong> <strong class="kr ir">sklearn.metrics</strong> <strong class="kr ir">import</strong> confusion_matrix</span></pre><p id="049e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们加载数据集。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="306d" class="kv kw iq kr b gy kx ky l kz la">df=pd.read_csv('datatest.txt')<br/>df.sample(5)</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/3e98499797e822e815035d94b8fad5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*gqYLqoI0v0BAZ4vNup1hIg.jpeg"/></div></figure><p id="a496" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了方便起见，我们将日期变量转换为日期时间对象。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="8e3a" class="kv kw iq kr b gy kx ky l kz la">df['date']=pd.to_datetime(df['date'])</span></pre><h1 id="3a55" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">第 1 部分:探索性数据分析</h1><p id="c5b6" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">首先，让我们看一下数据集的概况。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="49fc" class="kv kw iq kr b gy kx ky l kz la">df.describe()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/9c21610929784d77c61e22bfe84f74f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KkKaL_ppiQp4jlTeBSsJw.jpeg"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">数据集概述</p></figure><p id="da3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到这个数据集中有 2655 个样本。此外，没有丢失值。让我们也看看这个数据集的时间框架。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="6f5a" class="kv kw iq kr b gy kx ky l kz la">df['date'].describe()</span><span id="5bc1" class="kv kw iq kr b gy mq ky l kz la">count                    2665<br/>unique                   2665<br/>top       2015-02-03 07:25:59<br/>freq                        1<br/>first     2015-02-02 14:19:00<br/>last      2015-02-04 10:43:00<br/>Name: date, dtype: object</span></pre><p id="a25c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们的数据只在三天内就收集好了。接下来，我们将探讨我们的变量及其关系。首先，让我们画出温度变量。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="6e73" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 4)<br/>plt.hist(df['Temperature'], bins=40, density=<strong class="kr ir">True</strong>, label='Temperature')<br/>plt.xlabel('Temperature')<br/>plt.title('Distribution of temperature')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mr"><img src="../Images/7aebc6451c756f67b40e3cba78bc00bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKYymKLbDovYXEU_NDAoSw.png"/></div></div></figure><p id="d13b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该图显示温度具有重尾分布。湿度变量呢？</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="577f" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 4)<br/>plt.hist(df['Humidity'], bins=50, density=<strong class="kr ir">True</strong>, label='Humidity')<br/>plt.xlabel('Humidity')<br/>plt.title('Distribution of Humidity')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/d6e30c6aafc52fba9812e9bd02cef3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nt9LhGzdLVv8ouMmAECA0g.png"/></div></div></figure><p id="a45c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，在 22.5 和 25 附近有两个峰值。我们还对不同日子里的光照变化感兴趣。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="fa33" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 4)<br/>sns.boxplot(x=df['date'].dt.day,y=df['Light'], orient='v')<br/>plt.xlabel('Day')<br/>plt.title('Boxplot for Light during different days')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/a8a3f4711a410d9b655fd327d7350186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cUXmGrXfDGeLRr9_1SHIw.jpeg"/></div></div></figure><p id="4272" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到光的分布在这三天中几乎是相同的。接下来，让我们看看二氧化碳水平。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="bc97" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 4)<br/>sns.boxplot(x=df['date'].dt.day,y=df['CO2'], orient='v')<br/>plt.xlabel('Day')<br/>plt.title('Boxplot for CO2 level during different days')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mu"><img src="../Images/5582b739e1f3e227f8d84beb12b00cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHMbHT8D3y446vjD_2Ulqg.jpeg"/></div></div></figure><p id="93a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些分布明显不同。2015 年 2 月 4 日有很多异常值。最后，我们将深入研究湿度比变量。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="2c98" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 4)<br/>sns.boxplot(x=df['date'].dt.day,y=df['HumidityRatio'], orient='v')<br/>plt.xlabel('Day')<br/>plt.title('Boxplot for Humidity Ratio level during different days')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mv"><img src="../Images/9a8f94e82fdd512f4aa262b9fc670027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQ9_r-Dsxu2e2rLm02RY-g.jpeg"/></div></div></figure><p id="dcab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这看起来非常类似于二氧化碳水平的箱线图。也许，二氧化碳水平和湿度之间有很强的相关性。我们可以用这两个变量的散点图来验证。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="1247" class="kv kw iq kr b gy kx ky l kz la">ax=sns.scatterplot(df['CO2'], df['HumidityRatio'], style=df['date'].dt.day)</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mw"><img src="../Images/45cb38a80311d9c5a39aad4f1af53a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYBe1DJXIpqvr-34JD9u-Q.jpeg"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">二氧化碳水平和湿度比的散点图</p></figure><p id="8283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，二氧化碳水平和湿度比之间有很强的线性关系。让我们看一下变量之间的关系。这可以通过 seaborn 的 pair plot 函数来完成。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="165b" class="kv kw iq kr b gy kx ky l kz la">ax=seaborn.pairplot(df)</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mx"><img src="../Images/0c2eb0f33a82f87aa4fc78f0b93ceb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7twc2ZF4N_zTI64VVjpvA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">数据集中所有要素的配对图</p></figure><p id="cee5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这直观地显示了在以下对之间存在强线性关系:CO2 和温度、CO2 和湿度、湿度和湿度比、湿度比和 CO2。我们甚至可以通过绘制热图来量化这些关系。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="59ca" class="kv kw iq kr b gy kx ky l kz la">corr=df.iloc[:, 1:-1].corr()<br/><em class="my"># Generate a mask for the upper triangle</em><br/>mask = np.triu(np.ones_like(corr, dtype=np.bool))<br/><em class="my"># Set up the matplotlib figure</em><br/>f, ax = plt.subplots(figsize=(11, 9))</span><span id="3248" class="kv kw iq kr b gy mq ky l kz la"><em class="my"># Draw the heatmap with the mask and correct aspect ratio</em><br/>ax=sns.heatmap(corr, mask=mask,<br/>            square=<strong class="kr ir">True</strong>, linewidths=.5, cbar_kws={"shrink": .5})</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7235973628dd8a65e5931d47a2eb4766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*Uo9Hr6eVO4pNjwtZ8WM47A.png"/></div></figure><p id="3243" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">清楚地表明，两对湿度-湿度比和湿度比-CO2 表现出最强的线性关系。</p><h1 id="f91b" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">基于 PyMC3 的贝叶斯逻辑模型</h1><p id="2d84" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们将建立几个机器学习模型，根据其他变量对入住情况进行分类。</p><p id="a773" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，我们有一个二元决策问题。换句话说，我们的目标变量假设遵循伯努利随机变量，p 由下式给出:</p><figure class="km kn ko kp gt lc gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f17e07fc06bcca0f4ae479e1a6abb573.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*f7v_HMgB07xfg_IMzoBW0A.jpeg"/></div></figure><p id="a763" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，var 是我们在模型中使用的所有变量的集合，logit 是逻辑函数。</p><p id="6cb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了建立贝叶斯逻辑回归模型，我们首先要对每个参数进行先验分布。这些先验的选择将影响结果(尽管有更多的数据，它们可能会“收敛”到相同的分布。)</p><p id="94ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们的先验被指定，PyMC3 将使用<a class="ae kl" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">马尔可夫链蒙特卡罗</a>模拟及其推广在数值上近似后验分布。然后，我们可以使用这些后验样本进行推断。</p><h1 id="5e9f" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">先验分布均匀的第一个模型</h1><p id="b536" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">由于我们没有关于这些参数的先验知识，我们可以假设它们可以是任何东西。换句话说，我们假设所有的β_var 服从一个上下界都很大的均匀分布。为了捕捉一张大网，我们对均匀分布使用大的上下界。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="f913" class="kv kw iq kr b gy kx ky l kz la"><br/>lower=-10**6<br/>higher=10**6<br/><strong class="kr ir">with</strong> pm.Model() <strong class="kr ir">as</strong> first_model:<br/>    <em class="my">#priors on parameters</em><br/>    beta_0=pm.Uniform('beta_0', lower=lower, upper= higher)<br/>    beta_temp=pm.Uniform('beta_temp', lower, higher)<br/>    beta_humid=pm.Uniform('beta_humid', lower, higher)<br/>    beta_light=pm.Uniform('beta_light', lower, higher)<br/>    beta_co2=pm.Uniform('beta_co2', lower, higher)<br/>    beta_humid_ratio=pm.Uniform('beta_humid_ration', lower, higher)<br/>    <br/>    <em class="my">#the probability of belonging to class 1</em><br/>    p = pm.Deterministic('p', pm.math.sigmoid(beta_0+beta_temp*df['Temperature']+<br/>                               beta_humid*df['Humidity']+<br/>                               beta_light*df['Light']+<br/>                               beta_co2*df['CO2']+<br/>                               beta_humid_ratio*df['HumidityRatio']))<br/><strong class="kr ir">with</strong> first_model:<br/>    <em class="my">#fit the data </em><br/>    observed=pm.Bernoulli("occupancy", p, observed=df['Occupancy'])<br/>    start=pm.find_MAP()<br/>    step=pm.Metropolis()<br/>    <br/>    <em class="my">#samples from posterior distribution </em><br/>    trace=pm.sample(25000, step=step, start=start)<br/>    burned_trace=trace[15000:]</span></pre><p id="2fcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可能需要一段时间来运行。一旦完成，我们就可以绘制样本。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="a647" class="kv kw iq kr b gy kx ky l kz la">pm.traceplot(burned_trace)<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nb"><img src="../Images/025beb20de668328117cba73f1a99a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X1Urt318OJvXz2jTcVdPDw.png"/></div></div></figure><p id="ffc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们得出结论，我们的算法确实收敛。我们可以计算这些后验分布的平均值。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="0be5" class="kv kw iq kr b gy kx ky l kz la">coeffs=['beta_0', 'beta_temp', 'beta_humid', 'beta_light', 'beta_co2', 'beta_humid_ration']<br/>d=dict()<br/><strong class="kr ir">for</strong> item <strong class="kr ir">in</strong> coeffs:<br/>    d[item]=[burned_trace[item].mean()]<br/>    <br/>result_coeffs=pd.DataFrame.from_dict(d)    <br/>result_coeffs<br/><em class="my">#coeff_result=pd.DataFrame(d)    </em><br/><em class="my">#coeff_result</em></span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e924e2c86d856be3b13dee0ff44afaa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*IzHdV-7OkajOt1v4W9eTyA.jpeg"/></div></figure><p id="1388" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与频率统计相比，贝叶斯统计的一个优势是我们拥有的不仅仅是平均值。特别是，我们可以计算这些参数的 95% <a class="ae kl" href="https://www.sciencedirect.com/topics/mathematics/highest-density-interval#:~:text=Second%20Edition)%2C%202015-,4.3.,highest%20density%20interval%2C%20abbreviated%20HDI.&amp;text=Moreover%2C%20the%20probability%20density%20of,any%20x%20outside%20those%20limits." rel="noopener ugc nofollow" target="_blank">高密度区间</a>。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="62ea" class="kv kw iq kr b gy kx ky l kz la">pm.stats.hpd(burned_trace['beta_0'])<br/>coeffs=['beta_0', 'beta_temp', 'beta_humid', 'beta_light', 'beta_co2', 'beta_humid_ration']<br/>interval=dict()</span><span id="aae7" class="kv kw iq kr b gy mq ky l kz la"><strong class="kr ir">for</strong> item <strong class="kr ir">in</strong> coeffs:<br/>    interval[item]=pm.stats.hpd(burned_trace[item]) <em class="my">#compute 95% high density interval</em><br/>    <br/>result_coeffs=pd.DataFrame.from_dict(interval).rename(index={0: 'lower', 1: 'upper'})<br/>result_coeffs</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nd"><img src="../Images/7d6b0d620eeb0e6c153184b852b4b11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MIBwiZPMlJNEt-lG1OLmcQ.jpeg"/></div></div></figure><p id="a674" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，湿度比的系数明显大于其他系数。这并不一定意味着这个变量更重要。如果我们仔细观察数据，我们会发现这个变量取的值非常小。</p><p id="09fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们可以探索不同参数之间的关系。例如，让我们来看看β_ CO2 和β_ wet _ ratio 系数。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="7a15" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5, 12.5)<br/>seaborn.jointplot(burned_trace['beta_co2'], burned_trace['beta_humid_ration'], kind="hex") <em class="my">#color="#4CB391")</em><br/>plt.xlabel("beta_co2")<br/>plt.ylabel("beta_humid_ratio");</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/58656050c6d45e8b243099497596f57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*klDJjjqYwPNo-Zy_HG3I6g.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">CO2 和湿度比系数的散点图</p></figure><p id="42e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">曲线图显示这两个系数负相关。注意，CO2 水平和湿度比是正相关的。</p><p id="2ddb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，在经典的逻辑回归中，我们通过<a class="ae kl" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验估计</a> (MAP 解)来寻找最佳参数。换句话说，最佳拟合参数由下式给出</p><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nf"><img src="../Images/e31ff5a2dab7e67974461fda5b2d60bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Wsi9HojELWE_E2NmvDbxg.jpeg"/></div></div></figure><p id="78c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，𝑝(𝜃|𝐷)p(θ|D 是给定数据时θ的后验分布，p(D|θ)是似然函数，p(θ)是θ的先验分布。</p><p id="aea7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，由于我们在第一个模型中使用均匀分布，我们可以预期我们的 MAP 解决方案应该与 MLE 解决方案(<a class="ae kl" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>)一致，后者对应于 frequentist logistic 回归。我们可以使用 Scikit-Learn 库来测试这个语句。首先，我们使用 MAP 计算系数。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="83c4" class="kv kw iq kr b gy kx ky l kz la">coeffs=['beta_0', 'beta_temp', 'beta_humid', 'beta_light', 'beta_co2', 'beta_humid_ration']<br/>d=dict()<br/><strong class="kr ir">for</strong> item <strong class="kr ir">in</strong> coeffs:<br/>    d[item]=[float(start[item])]<br/>    <br/>map_coeffs=pd.DataFrame.from_dict(d)    <br/>map_coeffs</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ng"><img src="../Images/401553c9a92c7662a7825f56ff5d3fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ld2heI3QP5_z3IPrYlBdcw.jpeg"/></div></div></figure><p id="3f0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们使用经典的逻辑回归计算β系数。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="aca1" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">from</strong> <strong class="kr ir">sklearn.linear_model</strong> <strong class="kr ir">import</strong> LogisticRegression</span><span id="a460" class="kv kw iq kr b gy mq ky l kz la">X=df.iloc[:, 1: -1]<br/>y=df['Occupancy']<br/>logit=LogisticRegression()<br/>logit_model=logit.fit(X,y)<br/>pd.DataFrame(logit_model.coef_, columns=X.columns)</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nh"><img src="../Images/929f6a2ea765cd1ae7cc5f5ce17d95c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*sDSWGnAyUIyzExCXgY6GqQ.jpeg"/></div></div></figure><p id="b647" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是啊！两种方法的系数几乎相同。</p><p id="308c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们讨论了我们的模型的预测能力，并与经典的逻辑回归进行了比较。我们用经典方法记录预测。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="4074" class="kv kw iq kr b gy kx ky l kz la">logit_prediction=logit_model.predict(X)</span></pre><p id="2506" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了使用贝叶斯逻辑模型进行预测，我们通过对样本值进行平均来计算𝑦_score。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="6bfc" class="kv kw iq kr b gy kx ky l kz la"><em class="my">#compute the average probability of predicting 1. </em><br/>y_score = np.mean(burned_trace['p'], axis=0)</span><span id="65c7" class="kv kw iq kr b gy mq ky l kz la"><em class="my">#histogram of the distribution</em><br/>figsize(12.5,4)<br/>plt.hist(y_score, bins=40, density=<strong class="kr ir">True</strong>)<br/>plt.xlabel('Probability')<br/>plt.ylabel('Frequency')<br/>plt.title('Distribution of $y_score$')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ni"><img src="../Images/00abac1d42e5fa940c2ef04cddfe1d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SeBWK1i0MPYxnUEhXiByw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">y 分数的分布</p></figure><p id="73f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，p 的大部分集中在 0 和 1 附近。我们也可以使用 y_score 进行预测。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="044e" class="kv kw iq kr b gy kx ky l kz la">first_model_prediction=[1 <strong class="kr ir">if</strong> x &gt;0.5 <strong class="kr ir">else</strong> 0 <strong class="kr ir">for</strong> x <strong class="kr ir">in</strong> y_score]</span></pre><p id="812d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们通过计算混淆矩阵来评估我们的模型的性能。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="52ac" class="kv kw iq kr b gy kx ky l kz la">first_model_confussion_matrix =confusion_matrix(df['Occupancy'], first_model_prediction)<br/>first_model_confussion_matrix</span><span id="dd05" class="kv kw iq kr b gy mq ky l kz la">array([[1639,   54],<br/>       [   3,  969]])</span></pre><p id="2b84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很好。我们甚至还可以通过其他指标来量化性能。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="11cd" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">import</strong> <strong class="kr ir">sklearn</strong><br/><strong class="kr ir">from</strong> <strong class="kr ir">sklearn.metrics</strong> <strong class="kr ir">import</strong> classification_report</span><span id="234c" class="kv kw iq kr b gy mq ky l kz la">print(sklearn.metrics.classification_report(y, first_model_prediction))</span><span id="c3c6" class="kv kw iq kr b gy mq ky l kz la">precision    recall  f1-score   support</span><span id="341e" class="kv kw iq kr b gy mq ky l kz la">           0       1.00      0.97      0.98      1693<br/>           1       0.95      1.00      0.97       972</span><span id="52b3" class="kv kw iq kr b gy mq ky l kz la">    accuracy                           0.98      2665<br/>   macro avg       0.97      0.98      0.98      2665<br/>weighted avg       0.98      0.98      0.98      2665</span></pre><p id="bd33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以计算曲线下的面积。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="b555" class="kv kw iq kr b gy kx ky l kz la">pred_scores = dict(y_true=df['Occupancy'],y_score=y_score)<br/>roc_auc_score(**pred_scores)</span><span id="0928" class="kv kw iq kr b gy mq ky l kz la">0.99358530283253</span></pre><p id="0a41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，我们的模型表现得相当好。我们来和经典的 logistic 回归比较一下。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="938b" class="kv kw iq kr b gy kx ky l kz la">print(sklearn.metrics.classification_report(y, logit_prediction))</span><span id="2d37" class="kv kw iq kr b gy mq ky l kz la">precision    recall  f1-score   support</span><span id="1b6a" class="kv kw iq kr b gy mq ky l kz la">           0       1.00      0.97      0.98      1693<br/>           1       0.95      1.00      0.97       972</span><span id="2347" class="kv kw iq kr b gy mq ky l kz la">    accuracy                           0.98      2665<br/>   macro avg       0.97      0.98      0.98      2665<br/>weighted avg       0.98      0.98      0.98      2665</span></pre><p id="c821" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们是一样的！然而，使用贝叶斯模型，我们获得了更多的信息，因此我们对自己的估计更有信心。</p><h1 id="f45f" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">先验分布为正态分布的不同模型。</h1><p id="c607" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">现在，让我们使用一组不同的先验知识来训练我们的模型。例如，我们可以假设系数遵循正态分布。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="92cd" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">with</strong> pm.Model() <strong class="kr ir">as</strong> second_model: <br/>    #priors with normal distribution</span><span id="a0fb" class="kv kw iq kr b gy mq ky l kz la">    beta_0=pm.Normal('beta_0', mu=0, sd=10**4)<br/>    beta_temp=pm.Normal('beta_temp', mu=0, sd=10**4)<br/>    beta_humid=pm.Normal('beta_humid', mu=0, sd=10**4)<br/>    beta_light=pm.Normal('beta_light', mu=0, sd=10**4)<br/>    beta_co2=pm.Normal('beta_co2', mu=0, sd=10**4)<br/>    beta_humid_ratio=pm.Normal('beta_humid_ration', mu=0, sd=10**4)</span><span id="d02f" class="kv kw iq kr b gy mq ky l kz la">    #probability of belonging to class 1</span><span id="af8e" class="kv kw iq kr b gy mq ky l kz la">    p = pm.Deterministic('p', pm.math.sigmoid(beta_0+beta_temp*df['Temperature']+<br/>                               beta_humid*df['Humidity']+<br/>                               beta_light*df['Light']+<br/>                               beta_co2*df['CO2']+<br/>                               beta_humid_ratio*df['HumidityRatio']))</span><span id="346d" class="kv kw iq kr b gy mq ky l kz la">#fit observed data into the model</span><span id="dd17" class="kv kw iq kr b gy mq ky l kz la"><strong class="kr ir">with</strong> second_model:<br/>    observed=pm.Bernoulli("occupancy", p, observed=df['Occupancy'])<br/>    start=pm.find_MAP()<br/>    step=pm.Metropolis()<br/>    second_trace=pm.sample(25000, step=step, start=start)<br/>    second_burned_trace=second_trace[15000:]<br/>pm.traceplot(second_burned_trace)<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nb"><img src="../Images/d8c124f5f86e4e8bfb394a8896ffda3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBiX44ebTfgcUAcR_ACTKw.png"/></div></div></figure><p id="67c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们再次看到算法确实收敛了。让我们计算 MAP 解的β系数。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="f485" class="kv kw iq kr b gy kx ky l kz la">coeffs=['beta_0', 'beta_temp', 'beta_humid', 'beta_light', 'beta_co2', 'beta_humid_ration']<br/>d=dict()<br/><strong class="kr ir">for</strong> item <strong class="kr ir">in</strong> coeffs:<br/>    d[item]=[float(start[item])]<br/>    <br/>second_map_coeffs=pd.DataFrame.from_dict(d)    <br/>second_map_coeffs</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nj"><img src="../Images/b79bcb3f76ff470ad8f2308bf4971e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XjkRsY65sJvyGIZxZ6wABQ.jpeg"/></div></div></figure><p id="4821" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它们与我们在第一个模型中得到的非常接近。更进一步，因为我们在贝叶斯框架中，我们甚至可以比较两个模型的后验分布。例如，让我们看看截距变量。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="3ef1" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.hist(burned_trace['beta_0']-second_burned_trace['beta_0'], bins=40, density=<strong class="kr ir">True</strong>)<br/>plt.title('Distribution of the difference between beta_0')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/ffb15ab92144455feee376e564c44575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfkKNxJbqk1M-gPHH2kYvw.png"/></div></div></figure><p id="dd59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然 MAP 解对β_0 给出了相同的估计，但我们看到两个后验概率相当不同。让我们也比较一下这两个模型之间β_temp 的后验分布。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="866c" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.hist(burned_trace['beta_temp'], label='First model', bins=40, density=<strong class="kr ir">True</strong>)<br/>plt.hist(second_burned_trace['beta_temp'], bins=40, label='Second model', density=<strong class="kr ir">True</strong>)<br/>plt.title('Distribution of of beta_temp')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mr"><img src="../Images/02e44df7a107fe79ab0f27e11efc3791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kL0vw4YBgVVfWqQlL9kZew.png"/></div></div></figure><p id="96ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实际上差别很小。接下来，让我们计算第二个模型的预测能力。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="7e71" class="kv kw iq kr b gy kx ky l kz la">second_y_score = np.mean(second_burned_trace['p'], axis=0)<br/>second_model_prediction=[1 <strong class="kr ir">if</strong> x &gt;0.5 <strong class="kr ir">else</strong> 0 <strong class="kr ir">for</strong> x <strong class="kr ir">in</strong> second_y_score]<br/>second_model_confussion_matrix =confusion_matrix(df['Occupancy'], second_model_prediction)<br/>second_model_confussion_matrix</span><span id="6fe5" class="kv kw iq kr b gy mq ky l kz la">array([[1639,   54],<br/>       [   3,  969]])</span></pre><p id="0d2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这与我们从第一个模型中得到的结果相同。我们可以检查一下 y_score 和 second_y_score 几乎是一样的。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="b48e" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.hist(y_score-second_y_score, bins=40)<br/>plt.title('Distribution of the difference between y_score and second_y_score')<br/>plt.ylabel('Frequency')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nk"><img src="../Images/ed9a0d4dd155f35d7f6dddf39a14b0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JStfEfdPTySzXvXonciAVQ.png"/></div></div></figure><h1 id="4336" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">内置广义线性模型(GLM)的模型。</h1><p id="5d4e" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">在前面的章节中，我们使用实际操作的方法来构建我们的模型。这很容易，因为我们只有几个变量。当变量数量很大的时候，就不会很实用了。幸运的是，PyMC3 有一个内置的广义线性模型，在这个模型中，一切都将自动化。让我们使用这个内置模型来拟合我们的数据。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="471f" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">with</strong> pm.Model() <strong class="kr ir">as</strong> third_model:<br/>    pm.glm.GLM.from_formula('Occupancy ~ Temperature + Humidity + Light + CO2 + HumidityRatio',<br/>                            df,<br/>                            family=pm.glm.families.Binomial())<br/>    third_trace = pm.sample(25000, tune=10000, init='adapt_diag')</span><span id="004f" class="kv kw iq kr b gy mq ky l kz la">pm.traceplot(third_trace)<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nb"><img src="../Images/d2e3ed3f6ef08366283891f8ca4e302e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zq4gmJbc46fXgNXD5umu4A.png"/></div></div></figure><p id="5a4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与以前的模型不同，在这种情况下，我们的参数的后验分布是单峰的。</p><p id="26f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们总结一下这些后验分布。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="8985" class="kv kw iq kr b gy kx ky l kz la">pm.summary(third_trace)</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nl"><img src="../Images/1ee0cffdcfb115706c6c096f028ca0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdB3aW8Xa5ML71gw19mzIA.jpeg"/></div></div></figure><p id="c446" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了查看汇总的统计数据，我们还可以查看地图解决方案。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="fe62" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">with</strong> third_model:<br/>    map_solution=pm.find_MAP()<br/>d=dict()<br/><strong class="kr ir">for</strong> item <strong class="kr ir">in</strong> map_solution.keys():<br/>    d[item]=[float(map_solution[item])]<br/>    <br/>third_map_coeffs=pd.DataFrame.from_dict(d)    <br/>third_map_coeffs</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nm"><img src="../Images/9f26faa10df21140233f133962793fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*po-AVu3ULBkUjpQ_09Vfpg.jpeg"/></div></div></figure><p id="821a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到，第二种和第三种模型的地图解决方案之间存在显著差异。预测呢？</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="34e5" class="kv kw iq kr b gy kx ky l kz la"><strong class="kr ir">with</strong> third_model:<br/>    ppc = pm.sample_posterior_predictive(third_trace, samples=15000)</span><span id="c5a8" class="kv kw iq kr b gy mq ky l kz la"><em class="my">#compute y_score </em><br/><strong class="kr ir">with</strong> third_model:<br/>    third_y_score = np.mean(ppc['y'], axis=0)<br/><em class="my">#convert y_score into binary decisions    </em><br/>third_model_prediction=[1 <strong class="kr ir">if</strong> x &gt;0.5 <strong class="kr ir">else</strong> 0 <strong class="kr ir">for</strong> x <strong class="kr ir">in</strong> third_y_score]</span><span id="aab1" class="kv kw iq kr b gy mq ky l kz la"><em class="my">#compute confussion matrix </em><br/>third_model_confussion_matrix =confusion_matrix(df['Occupancy'], third_model_prediction)<br/>third_model_confussion_matrix</span><span id="9744" class="kv kw iq kr b gy mq ky l kz la">array([[1639,   54],<br/>       [   3,  969]])</span></pre><p id="a69b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个混淆矩阵与前两个模型中的相同。第二个和第三个模型的 y 分数分布如何？</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="95a6" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.hist(third_y_score-second_y_score, bins=40)<br/>plt.title('Distribution of the difference between y_score and second_y_score')<br/>plt.ylabel('Frequency')<br/>plt.show(</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nk"><img src="../Images/87c0842bec07e798a82fb3f2c517b6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U9VCQJyUQh6FntHoGNnM9Q.png"/></div></div></figure><p id="2dd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种分布集中在 0 左右。换句话说，y_scores 在不同模型间的分布几乎相同。系数呢，比如温度系数？</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="0f3e" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.hist(third_trace['Temperature'][-40000:]-second_burned_trace['beta_temp'], bins=40, density=<strong class="kr ir">True</strong>)<br/>plt.title('Difference between the temperature coefficients for the second and the third model')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nn"><img src="../Images/f3e9f2303816185031728d221715d0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QnU5F0lUbXSN7eGDOVPGxw.png"/></div></div></figure><p id="a738" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">差值近似遵循具有小平均值的正态分布。</p><p id="39c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也检查一下湿度系数之间的差异。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="4de5" class="kv kw iq kr b gy kx ky l kz la">figsize(12.5,4)<br/>plt.boxplot(third_trace['Humidity'][-40000:]-second_burned_trace['beta_humid'])<br/>plt.title('Difference between the humidity coefficients for the second and the third model')<br/>plt.show()</span></pre><figure class="km kn ko kp gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi no"><img src="../Images/40cd0f34c46c3553b04878c0038cb5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-xonSPJnI4oMbKYdQBYAQ.png"/></div></div></figure><p id="f4be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，这种差异很小。</p><h1 id="06c2" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">3.结论</h1><p id="4ac6" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们看到，即使我们的模型使用不同的先验，预测性能是相似的。这证实了我们的信念，随着我们的数据集变大，它们应该收敛到同一个解决方案。</p><p id="66d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们希望我们的项目将帮助 PyMC3 的初学者学习它的语法。我们发现 PyMC3 的代码相当直观，我们希望我们的代码清楚地证明了这一点。</p><h1 id="99c0" class="lf kw iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">4.参考文献。</h1><p id="1577" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">[1]<a class="ae kl" href="https://docs.pymc.io/notebooks/GLM-logistic.html" rel="noopener ugc nofollow" target="_blank">https://docs.pymc.io/notebooks/GLM-logistic.html</a>官方 PyMC3 文档</p><p id="88d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]<a class="ae kl" href="https://goldinlocks.github.io/Bayesian-logistic-regression-with-pymc3/" rel="noopener ugc nofollow" target="_blank">https://Goldin locks . github . io/Bayesian-logistic-regression-with-pymc 3/</a></p></div></div>    
</body>
</html>