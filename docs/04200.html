<html>
<head>
<title>Evaluating the Accuracy of My Video Search Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估我的视频搜索引擎的准确性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluating-the-accuracy-of-my-video-search-engine-1235f375bd5?source=collection_archive---------43-----------------------#2020-04-16">https://towardsdatascience.com/evaluating-the-accuracy-of-my-video-search-engine-1235f375bd5?source=collection_archive---------43-----------------------#2020-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7769" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我训练了一个ResNet-50模型，让它在一个视频帧中找到一个乒乓球。我的模型好吗，或者我只是训练了一个“白色斑点”检测器？</h2></div><p id="fa05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/the-video-search-engine-my-journey-into-computer-vision-9789824e76bb">我在“视频搜索引擎”</a>上的帖子之后的几个星期里，我兴奋地向任何愿意听的人展示我的进步。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/9389f54b8b0b92fe670c5aa6d78eb466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*2Ae2LrecK8FqYQJy6YRpFw.gif"/></div></figure><p id="a4ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当ML社区的朋友们认为这很棒时，一个问题困扰着我，</p><blockquote class="ln"><p id="a798" class="lo lp it bd lq lr ls lt lu lv lw ld dk translated">我的模型好吗？</p><p id="79fd" class="lo lp it bd lq lr ls lt lu lv lw ld dk translated">我刚刚训练了一个白色斑点探测器吗？</p></blockquote><p id="9a7d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">有时我的模特会发现一个乒乓球，我会高兴地大叫。但其他时候，模型是在推断球员短裤上的乒乓球，光头，等等。没什么了不起的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mc"><img src="../Images/1972d1fb15d2f55b7fd333a5af5e963a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RfcdBt150t0cPuAu"/></div></div></figure><p id="0cfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我旅程的下一步是测量模型的准确性。</p><p id="8aa8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用平均精度(mAP)的变量来测量对象检测精度。有无数的<a class="ae le" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">帖子</a>描述了这一指标的变体，所以我不会在这里赘述。但是有必要总结一下将在下面的代码示例中出现的要点。</p><h1 id="9880" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">背景和方法</h1><p id="0a45" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated"><a class="ae le" href="https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/" rel="noopener ugc nofollow" target="_blank">Mean average Precision(mAP)</a>是每个类的<em class="ne">平均精度</em> (AP)的平均值，其被测量为每个类的精度-召回曲线下的面积:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nf"><img src="../Images/5f6f53a3f08874abb81798e6f789676a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*00k8jqtT6c6yCVWU"/></div></div></figure><p id="ea65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设一个物体检测器可以识别20个不同的类别。地图计算每个类的平均精度，然后对结果进行平均。</p><p id="aebb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们需要为给定的<em class="ne">召回</em>值计算<em class="ne">精度</em>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ng"><img src="../Images/2e3caf476ac1e7a1350f6459c2f957bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4corhfSo5GEejMrw"/></div></div></figure><p id="8ec0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后测量精确-召回对的曲线下的面积。为了简单起见，精确召回曲线的之字形被平滑了，就像这样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ng"><img src="../Images/f2a461c2bc9440479d01e838f0ee5ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pOaNKIEQOJX-yNbx"/></div></div></figure><p id="81fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们需要计算精度和召回率。这些公式信息丰富:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nh"><img src="../Images/c553c7d0292bf16b144f09fb6a6881b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JkAz-OAeiy9nukr1"/></div></div></figure><p id="90b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="ne">精度</em>指的是准确度(所有预测中正确的预测的百分比)，而<em class="ne">召回</em>指的是真阳性率(真阳性与所有真实阳性的比率)。</p><p id="c55b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000" rel="noopener ugc nofollow" target="_blank"> PASCAL VOC challenge </a>(包括基准对象检测模型的评估指标)中，如果IoU大于或等于0.5 ，则<strong class="kk iu">预测为真。小于0.5的IoU是“误报”(就像<a class="ae le" rel="noopener" target="_blank" href="/breaking-down-mean-average-precision-map-ae462f623a52">重复的边界框</a>)。</strong></p><p id="ec70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">IoU是一种评估指标，用于计算地面实况边界框和预测边界框之间的重叠。来自Jonathan Hui 的这张图是一个精彩的总结:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ni"><img src="../Images/3ce52861d274d6f7b408d9468a529182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M8LhhUhNMmlPVZcY"/></div></div></figure><p id="402c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">IoU相当<a class="ae le" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">容易计算</a>:</p><ul class=""><li id="41e4" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">在分子中，我们计算预测边界框和真实边界框之间的重叠区域。</li><li id="56ee" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">分母是联合区域，由预测边界框和实际边界框包围。</li><li id="cd8f" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">将重叠面积除以并集面积得到分数“交集超过并集”。</li></ul><p id="41b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了计算我的模型准确性所需的一切。以下部分显示了实现上述步骤的代码片段。</p><h1 id="67f3" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">我的视频搜索引擎的平均精度</h1><p id="40de" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">我们从导入本例所需的包开始:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="f601" class="oc mi it ny b gy od oe l of og"># import the necessary packages<br/>import json<br/>import os<br/>import boto3<br/>from collections import namedtuple<br/>import cv2<br/>import matplotlib.image as mpimg<br/>import pandas as pd<br/># set Pandas table options for viewing data on screen<br/>pd.set_option(‘display.max_columns’, 500)<br/>pd.set_option(‘display.width’, 1000)</span></pre><p id="301b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遵循<a class="ae le" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank"> Adrian Rosebrock的示例</a>，我们定义一个将存储三个属性的检测对象:</p><ul class=""><li id="a68e" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated"><code class="fe oh oi oj ny b">image_path</code>:驻留在磁盘上的输入图像的路径。</li><li id="c3ab" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated"><code class="fe oh oi oj ny b">gt</code>:地面实况包围盒。</li><li id="f969" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated"><code class="fe oh oi oj ny b">pred</code>:来自我们模型的预测包围盒。</li></ul><p id="30d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们创建一个函数来计算并集上的交集:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="1bd0" class="oc mi it ny b gy od oe l of og"># Define the `Detection` object<br/>Detection = namedtuple("Detection", ["image_path", "gt", "pred"])</span><span id="9914" class="oc mi it ny b gy ok oe l of og">def bb_intersection_over_union(boxA, boxB):<br/>  # determine the (x, y)-coordinates of the intersection rectangle<br/>  xA = max(boxA[0], boxB[0])<br/>  yA = max(boxA[1], boxB[1])<br/>  xB = min(boxA[2], boxB[2])<br/>  yB = min(boxA[3], boxB[3])<br/>  # compute the area of intersection rectangle<br/>  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)<br/>  # compute the area of both the prediction and ground-truth<br/>  # rectangles<br/>  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)<br/>  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)<br/>  # compute the intersection over union by taking the intersection<br/>  # area and dividing it by the sum of prediction + ground-truth<br/>  # areas - the interesection area<br/>  iou = interArea / float(boxAArea + boxBArea - interArea)<br/>  # return the intersection over union value<br/>  return iou</span><span id="b7a3" class="oc mi it ny b gy ok oe l of og">def bb_intersection_over_union(boxA, boxB):<br/> # determine the (x, y)-coordinates of the intersection rectangle<br/> xA = max(boxA[0], boxB[0])<br/> yA = max(boxA[1], boxB[1])<br/> xB = min(boxA[2], boxB[2])<br/> yB = min(boxA[3], boxB[3])<br/> # compute the area of intersection rectangle<br/> interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)<br/> # compute the area of both the prediction and ground-truth<br/> # rectangles<br/> boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)<br/> boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)<br/> # compute the intersection over union by taking the intersection<br/> # area and dividing it by the sum of prediction + ground-truth<br/> # areas - the interesection area<br/> iou = interArea / float(boxAArea + boxBArea - interArea)<br/> # return the intersection over union value<br/> return iou</span></pre><p id="4544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理想情况下，我们对验证集中的每个图像重复这种计算。因此，让我们通过(1)遍历每个验证图像来构建检测元组的数组；(2)调用对象检测端点；以及(3)存储预测边界框、地面实况边界框和图像路径的结果。这就完成了工作:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="0e3f" class="oc mi it ny b gy od oe l of og">for filename in os.listdir(directory):<br/> if filename.endswith(".png"):<br/>  file_with_path = (os.path.join(directory, filename))<br/>  img = mpimg.imread(file_with_path)<br/>  height = img.shape[0]<br/>  width = img.shape[1]<br/>  with open(file_with_path, 'rb') as image:<br/>   f = image.read()<br/>   b = bytearray(f)<br/>   ne = open('n.txt', 'wb')<br/>   ne.write(b)</span><span id="b901" class="oc mi it ny b gy ok oe l of og">response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType='image/png', Body=b)<br/>  result = response['Body'].read().decode('ascii')<br/>  detections = json.loads(result)<br/>  best_detection = detections['prediction'][0] # ordered by max confidence; take the first one b/c only one ping pong ball in play ever<br/>  print(best_detection)</span><span id="a31e" class="oc mi it ny b gy ok oe l of og">(klass, score, x0, y0, x1, y1) = best_detection<br/>  xmin = int(x0 * width)<br/>  ymin = int(y0 * height)<br/>  xmax = int(x1 * width)<br/>  ymax = int(y1 * height)<br/>  pred_pixels = [xmin, ymin, xmax, ymax]<br/>  gt_pixels = find_gt_bbox_for_image(annotation_filename, filename)<br/>  det = Detection(filename, gt_pixels, pred_pixels)<br/>  det_array.append(det)<br/>  mAP_df.loc[filename, 'Confidences'] = score<br/>  continue<br/> else:<br/>  continue</span></pre><p id="96a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，上面的代码片段通过使用图像文件名作为关键字引用annotations.json文件来获取基本事实边界框，使用如下方式:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="e930" class="oc mi it ny b gy od oe l of og">def find_gt_bbox_for_image(annotation_filename, image_filename):<br/> with open(annotation_filename) as f:<br/>  js = json.load(f)<br/>  images = js['images']<br/>  categories = js['categories']<br/>  annotations = js['annotations']<br/>  for i in images:<br/>   if i['file_name'] == image_filename:<br/>    line = {}<br/>    line['file'] = i['file_name']<br/>    line['image_size'] = [{<br/>     'width': int(i['width']),<br/>     'height': int(i['height']),<br/>     'depth': 3<br/>    }]<br/>    line['annotations'] = []<br/>    line['categories'] = []<br/>    for j in annotations:<br/>     if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:<br/>      line['annotations'].append({<br/>       'class_id': int(j['category_id']),<br/>       'top': int(j['bbox'][1]),<br/>       'left': int(j['bbox'][0]),<br/>       'width': int(j['bbox'][2]),<br/>       'height': int(j['bbox'][3])<br/>      })<br/>      class_name = ''<br/>      for k in categories:<br/>       if int(j['category_id']) == k['id']:<br/>        class_name = str(k['name'])<br/>      assert class_name is not ''<br/>      line['categories'].append({<br/>       'class_id': int(j['category_id']),<br/>       'name': class_name<br/>      })<br/>    if line['annotations']:<br/>     x0 = line['annotations'][0]['left']<br/>     y0 = line['annotations'][0]['top']<br/>     x1 = int(line['annotations'][0]['left'] + line['annotations'][0]['width'])<br/>     y1 = int(line['annotations'][0]['top'] + line['annotations'][0]['height'])<br/>     gt = [x0, y0, x1, y1]<br/> return gt</span></pre><p id="51c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们取检测数组<code class="fe oh oi oj ny b">det_array</code>中的每个<code class="fe oh oi oj ny b">Detection</code>元组，并计算它的IoU，此外还输出一个图像供我们检查:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="7397" class="oc mi it ny b gy od oe l of og">for detection in det_array:<br/> # load the image<br/> image_with_path = (os.path.join(directory, detection.image_path))<br/> image = cv2.imread(image_with_path)<br/> # draw the ground-truth bounding box and predicted bounding box<br/> cv2.rectangle(image, tuple(detection.gt[:2]),<br/>  tuple(detection.gt[2:]), (0, 255, 0), 1)<br/> cv2.rectangle(image, tuple(detection.pred[:2]),<br/>  tuple(detection.pred[2:]), (0, 0, 255), 1)<br/> # compute the intersection over union and display it<br/> iou = bb_intersection_over_union(detection.gt, detection.pred)<br/> mAP_df.loc[detection.image_path, 'IoU'] = iou # add to df for PR-curve calc.<br/> cv2.putText(image, "IoU: {:.4f}".format(iou), (10, 30),<br/>  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)<br/> print("{}: {:.4f}".format(detection.image_path, iou))<br/> if not cv2.imwrite('./IoU/{}'.format(detection.image_path), image):<br/>  raise Exception("Could not write image")</span></pre><p id="72c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是上面脚本中的一个示例图像:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1e8644bec115fb5e05b153ab1aafa950.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/0*P_Qj0Mqw7FR1cTcZ"/></div></figure><p id="4cbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们的工作还没有完成。我们仍然需要计算精确回忆曲线，并测量曲线下的面积。<a class="ae le" rel="noopener" target="_blank" href="/breaking-down-mean-average-precision-map-ae462f623a52">根据谭</a>的这个例子，我做了一个熊猫表，按照各自的推理得分排序。我承认熊猫对于这个简单的任务来说火力太大了，但是我想在这篇文章中利用熊猫的表格布局和可视化:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi om"><img src="../Images/0f61382fdd6c009d147d3dd3cec15489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_DD6w-SuzMHf1bmg"/></div></div></figure><p id="1b3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们通过在上面的<code class="fe oh oi oj ny b">det_array</code>中的每次迭代期间将IoU结果存储在具有image_path索引的数据帧中来计算精度召回表:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="3d4f" class="oc mi it ny b gy od oe l of og">mAP_df.loc[detection.image_path, 'IoU']</span></pre><p id="758c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们可以根据置信度对数据帧进行排序:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="77ef" class="oc mi it ny b gy od oe l of og">mAP_df = mAP_df.sort_values(by='Confidences', ascending=False)</span></pre><p id="de10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并计算数据帧中每一行的真阳性、假阳性、精度和召回值:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="9e82" class="oc mi it ny b gy od oe l of og">def calc_TP(row):<br/> iou = row['IoU']<br/> if iou &gt;= 0.5:<br/>  result = 1<br/> else:<br/>  result = 0<br/> return result</span><span id="c476" class="oc mi it ny b gy ok oe l of og">def calc_FP(row):<br/> iou = row['IoU']<br/> if iou &lt; 0.5:<br/>  result = 1<br/> else:<br/>  result = 0<br/> return result</span><span id="c8b9" class="oc mi it ny b gy ok oe l of og">mAP_df['TP'] = mAP_df.apply(calc_TP, axis=1)<br/>mAP_df['FP'] = mAP_df.apply(calc_FP, axis=1)<br/>mAP_df['Acc TP'] = mAP_df['TP'].cumsum(axis=0)<br/>mAP_df['Acc FP'] = mAP_df['FP'].cumsum(axis=0)</span><span id="b592" class="oc mi it ny b gy ok oe l of og">def calc_Acc_Precision(row):<br/> precision = row['Acc TP'] / (row['Acc TP'] + row['Acc FP'])<br/> return precision</span><span id="9126" class="oc mi it ny b gy ok oe l of og">def calc_Acc_Recall(row):<br/> recall = row['Acc TP'] / (mAP_df.count()[0])<br/> return recall</span><span id="adf4" class="oc mi it ny b gy ok oe l of og">mAP_df['Precision'] = mAP_df.apply(calc_Acc_Precision, axis=1)<br/>mAP_df['Recall'] = mAP_df.apply(calc_Acc_Recall, axis=1)</span></pre><p id="6ea7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个方便的绘图函数揭示了PR曲线，考虑到我有限的验证示例数量，它看起来相当不足:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="8efc" class="oc mi it ny b gy od oe l of og">import matplotlib.pyplot as plt<br/>mAP_df.plot(kind='line',x='Recall',y='Precision',color='red')<br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi on"><img src="../Images/485869b5225e8065e11b98a0f6b536c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zOk1wVVJJYKKb3yM"/></div></div></figure><p id="e1c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">剩下的就是计算曲线下的面积了。该函数在给定一系列(x，y)坐标的情况下执行数值积分:</p><pre class="lg lh li lj gt nx ny nz oa aw ob bi"><span id="3eb1" class="oc mi it ny b gy od oe l of og">def calc_PR_AUC(x, y):<br/>   sm = 0<br/>   for i in range(1, len(x)):<br/>       h = x[i] - x[i-1]<br/>       sm += h * (y[i-1] + y[i]) / 2</span><span id="e7c9" class="oc mi it ny b gy ok oe l of og">return sm</span><span id="3a0f" class="oc mi it ny b gy ok oe l of og">calc_PR_AUC(mAP_df['Recall'], mAP_df['Precision'])</span></pre><p id="7aa4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么我的模型的平均精度是多少呢？</p><p id="911d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请敲鼓…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/76d5ba899f6e569482619427955686e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*WNlt0UfnLt1WvIz0"/></div></figure><h1 id="1d3d" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated"><strong class="ak"> &gt; &gt; &gt; 0.57 </strong></h1><p id="7484" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">万岁…？没有上下文，很难对这个数字感到兴奋。让我们将我的平均精度与其他模型进行比较。</p><h1 id="7814" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">将我的平均精确度联系起来</h1><p id="25d9" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">“自2006年以来，Pascal视觉对象类(VOC)挑战赛一直是一项年度活动。挑战由两部分组成:(一)从Flickr网站(2013年)获得的公开图像数据集，以及地面实况注释和标准化评估软件；(二)年度竞赛和研讨会。”(Everingham，m .，Eslami，S.M.A .，Van Gool，l .等人，<a class="ae le" href="https://doi.org/10.1007/s11263-014-0733-5" rel="noopener ugc nofollow" target="_blank">《帕斯卡视觉对象类挑战:回顾》</a> (2015)。)如这里，该论文中的一个任务是对象检测，它询问“图像中特定对象类的实例(如果有的话)在哪里？”(<em class="ne"> Id。</em>)</p><p id="08a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看2012年PASCAL VOC挑战赛，我们得到了来自几个世界级团队的结果表:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi op"><img src="../Images/643fdb0a663e8ebafd3e03eaa12cb3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0gOnqzsXbQtSiq6s"/></div></div></figure><p id="f9dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在那里，每个团队都在VOC2012数据上训练了一个对象检测器，该表报告了每个对象类别和提交的AP分数。每列中的金色条目表示相应类别的最大AP，银色条目表示排名第二的结果(<em class="ne"> Id。</em>)我们看到最高AP是65。该论文还显示了通过任何方法获得的最佳AP(最大值)和所有方法的中值AP(中值):</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d018440c0336b2bd93ec869f05c0c8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/0*G6rXwL3gqIU6hCmn"/></div></figure><p id="87b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">放在这样的背景下，我57的AP看起来还不错！</p><p id="a5f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但如果你像我一样，你会想，</p><blockquote class="ln"><p id="600e" class="lo lp it bd lq lr ls lt lu lv lw ld dk translated">等等，这太好了，不可能是真的…</p></blockquote><p id="a9c9" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">将我的结果与2012年的OC挑战进行比较<strong class="kk iu">是善意的，但有严重的局限性</strong>。各种因素使我的模型看起来比实际要好:</p><ul class=""><li id="2f4f" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">我的模型只预测了一个类。要求模型预测许多类肯定会降低我的整体地图。</li><li id="f9e0" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">给定我的领域特定的任务，假设在任何时候都只有“一个球在玩”，所以我的计算采用了每帧的最高得分检测，这排除了重复的边界框。查看<em class="ne">所有的预测</em>都会降低我的地图质量。</li><li id="e5c6" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">我的模型利用了一种新颖的深度学习架构ResNet-50(一种具有50层的卷积神经网络，在ILSVRC和COCO 2015比赛中获得了第一名)，相对于PASCAL VOC challenge中提供的技术，这代表了最先进技术的一次重大飞跃。</li><li id="dae0" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">我的模型使用了“迁移学习”，利用ImageNet数据库中的100多万张图像对模型进行预训练，然后为它提供更多针对我的任务的数据</li><li id="953b" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">PASCAL VOC的方法并不是计算目标探测任务平均精度的最新方法。Jonathan Hui 很好地总结了这一点:“最新的研究论文往往只给出COCO数据集的结果。。。。对于COCO来说，AP是多个IoU的平均值。。。。AP@[.5:.95]对应于0.5至0.95范围内IoU的平均AP，步长为0.05。<a class="ae le" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"> YOLOv3 </a>(一个来自2018年的最先进的深度学习神经网络)，在COCO test-dev数据集上以每秒30帧的速度运行，在80个类上获得了57.9%的mAP，而我的模型预测一个类每秒约0.3帧！</li></ul><p id="5e9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是说，我对在<em class="ne">环境中学习和校准</em>我对这一指标的理解的AP计算感到满意。我谦卑地补充一点，上面的比较并不意味着我的模型很棒，或者它的表现与世界级研究人员的模型相似。</p><h1 id="38e5" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">未来方向</h1><p id="10a4" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">鉴于模型的平均精度，我开始梦想组装整个系统:</p><ul class=""><li id="ed7b" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">给定一个乒乓球视频，</li><li id="2d6f" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">通过在每一帧上调用我的模型来识别球何时在运动</li><li id="5fe0" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">当球在比赛时存储时间戳</li><li id="3eac" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">向用户提供web UI界面，以及</li><li id="78f1" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">允许用户查看球在比赛时的精彩镜头。</li></ul><p id="993c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但我想知道我的模型将如何推广到不同的乒乓球比赛。不同的拍摄角度会放大我的模型吗？不同的桌面颜色或球员的服装会影响表现吗？我惊喜地看到这样的结果:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ng"><img src="../Images/511befe9d6226ca5fb9aee356c6a6e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ipYhlg7lRpN9KPHr"/></div></div><p class="or os gj gh gi ot ou bd b be z dk translated">计算各种匹配帧的IoU。</p></figure><p id="ed13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">满怀希望，我潦草地画了这张图，以指引我旅程的下一步:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ng"><img src="../Images/35b00481ae6d91fbb629b2fa3662666f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DCqR9wh7JKPznTCN"/></div></div></figure><p id="86a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请继续关注这方面的更多内容！</p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="a201" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然上面的帖子解释了各种代码片段，但最好有完整的要点，这样您就可以自己探索代码了:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="or os gj gh gi ot ou bd b be z dk translated">对象检测的计算平均精度. py</p></figure></div></div>    
</body>
</html>