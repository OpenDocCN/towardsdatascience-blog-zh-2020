<html>
<head>
<title>An Overview for Text Representations in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的文本表示综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-overview-for-text-representations-in-nlp-311253730af1?source=collection_archive---------13-----------------------#2020-03-04">https://towardsdatascience.com/an-overview-for-text-representations-in-nlp-311253730af1?source=collection_archive---------13-----------------------#2020-03-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4ab2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">讨论自然语言处理中三种最常用的输入类型。</h2></div><p id="9cc2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">当要阐明一个人对某个特定话题的理解时，写作总是一个不错的选择。通过把想法写在纸上，想法会被澄清，困惑会被暴露。虽然这可能不是最舒服的事情，但这确实是一种学习和提高的有效方式。</p><p id="f0ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你发现自己很难向朋友解释一些事情，一些你已经研究了一段时间，但不知何故仍然无法清晰直观地描述的事情，你应该试着把它写下来。</p><p id="e398" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我试图总结NLP中文本表示的一些想法，旨在为未来的复杂概念奠定基础，并希望对您的学习贡献我的<em class="lk"> granito de arena </em>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/b244f86ba8356f73b4ee0b17e004461e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DocMTV7nTAomKxcu3m-tyw.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">NLP输入的预处理步骤</p></figure><p id="1fe9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图总结了一个<strong class="kh ir">文本</strong> <strong class="kh ir">语料库</strong>转化为不同输入格式的过程，供一个<strong class="kh ir">机器</strong> <strong class="kh ir">学习</strong> <strong class="kh ir">模型</strong>使用。从左起，语料库经过几个步骤才获得<strong class="kh ir">记号</strong>，一组文本<strong class="kh ir">构建</strong> <strong class="kh ir">组块</strong>即<strong class="kh ir">词</strong>、<strong class="kh ir">子词</strong>、<strong class="kh ir">字</strong>等。由于ML模型只能够处理<strong class="kh ir">数字</strong> <strong class="kh ir">值</strong>，所以通过在<strong class="kh ir">关联</strong> <strong class="kh ir">数组</strong>(词汇)中查找或者使用<strong class="kh ir">散列</strong> <strong class="kh ir">技巧</strong>，句子中的记号被它们对应的<strong class="kh ir">id、</strong>所替换。一旦完成，它们就被转换成不同的<strong class="kh ir">输入</strong> <strong class="kh ir">格式</strong>，如右图所示。这些格式中的每一种都有自己的优点、缺点，应该根据手头任务的特点有策略地选择。</p><p id="c654" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将研究上面显示的每一个输入表示，直接从标记开始，忽略前面的步骤。</p><h2 id="9e39" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">内容:</h2><ul class=""><li id="f2ee" class="mu mv iq kh b ki mw kl mx ko my ks mz kw na la nb nc nd ne bi translated">一键编码</li><li id="9ae5" class="mu mv iq kh b ki nf kl ng ko nh ks ni kw nj la nb nc nd ne bi translated">计数向量/特征散列/Tf-idf</li><li id="a4ae" class="mu mv iq kh b ki nf kl ng ko nh ks ni kw nj la nb nc nd ne bi translated">单词嵌入/训练嵌入/语境化嵌入</li></ul></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><h2 id="f215" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">1.一键编码</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nr"><img src="../Images/178ddfa3036b53c51cf2cf390b6d7c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArM6Z5jeptCQ082DYn9nDQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">单词“The”、“cat”和“on”在词汇表中分别具有索引值0、1和2，只有在这些位置它们的独热向量将具有值1。向量的长度由语料库中唯一标记的数量决定。图片来自<a class="ae ns" href="https://www.kdnuggets.com/2019/10/introduction-natural-language-processing.html" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="e810" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当在一个数据集中遇到一个<strong class="kh ir"/><strong class="kh ir">特征</strong>时，一键编码可能是一个人的首选(显然，如果你认为这样的特征对模型有用的话)。这是一种简单而直接的技术，它通过用充满<strong class="kh ir">零</strong>的<strong class="kh ir">向量</strong>替换每个类别来工作，除了其对应的<strong class="kh ir">索引</strong>值的位置，其值为<strong class="kh ir"> 1 </strong>。</p><p id="b0e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当对文本文档应用独热编码时，标记被它们的独热向量所代替，并且给定的句子又被转换成形状为(<strong class="kh ir"><em class="lk">【n，m】</em></strong>)的<strong class="kh ir"> 2D矩阵</strong>，其中<strong class="kh ir"> <em class="lk"> n </em> </strong>是句子中 <strong class="kh ir">标记</strong>的<strong class="kh ir">号</strong> <strong class="kh ir">以及<strong class="kh ir"> <em class="lk"> m </em> </strong>根据一个句子有多少个标记，它的<strong class="kh ir">形状</strong>将<strong class="kh ir">不同</strong>。</strong></p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="b583" class="mb mc iq nu b gy ny nz l oa ob">As an example, say that we were to use one-hot encoding for the following sentences:</span><span id="89d8" class="mb mc iq nu b gy oc nz l oa ob">s1. <strong class="nu ir"><em class="lk">"This is sentence one."</em></strong><br/>s2. <strong class="nu ir"><em class="lk">"Now, here is our sentence number two."</em></strong></span><span id="df96" class="mb mc iq nu b gy oc nz l oa ob">The vocabulary from the two sentences is:</span><span id="7e1a" class="mb mc iq nu b gy oc nz l oa ob"><strong class="nu ir">vocabulary = {"here": 0, "is": 1, "now": 2, "number": 3, "one": 4, "our": 5, "sentence": 6, "this": 7, "two": 8}</strong></span><span id="5093" class="mb mc iq nu b gy oc nz l oa ob">The two sentences represented by one-hot vectors are:</span><span id="39e6" class="mb mc iq nu b gy oc nz l oa ob"><strong class="nu ir">indices</strong>                             <strong class="nu ir">words<br/>     </strong> <em class="lk">0  1  2  3  4  5  6  7  8</em><br/>s1: <strong class="nu ir">[[0, 0, 0, 0, 0, 0, 0, 1, 0],</strong> - "this"<br/>     <strong class="nu ir">[0, 1, 0, 0, 0, 0, 0, 0, 0],</strong> - "is"<br/>     <strong class="nu ir">[0, 0, 0, 0, 0, 0, 1, 0, 0],</strong> - "sentence"<br/>     <strong class="nu ir">[0, 0, 0, 0, 1, 0, 0, 0, 0]]</strong> - "one"</span><span id="9868" class="mb mc iq nu b gy oc nz l oa ob">s2: <strong class="nu ir">[[0, 0, 1, 0, 0, 0, 0, 0, 0],</strong> - "now"<br/>     <strong class="nu ir">[1, 0, 0, 0, 0, 0, 0, 0, 0],</strong> - "here"<br/>     <strong class="nu ir">[0, 1, 0, 0, 0, 0, 0, 0, 0],</strong> - "is"<br/>     <strong class="nu ir">[0, 0, 0, 0, 0, 1, 0, 0, 0],</strong> - "our"<br/>     <strong class="nu ir">[0, 0, 0, 0, 0, 0, 1, 0, 0],</strong> - "sentence"<br/>     <strong class="nu ir">[0, 0, 0, 1, 0, 0, 0, 0, 0],</strong> - "number"<br/>     <strong class="nu ir">[0, 0, 0, 0, 0, 0, 0, 0, 1]]</strong> - "two"</span></pre><h2 id="d924" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">改变形状和不认识的单词</h2><p id="969c" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">随着训练库<strong class="kh ir">变得越来越大，词汇的大小只会<strong class="kh ir">增长</strong>，结果，每个记号将由具有越来越大<strong class="kh ir">长度</strong>的向量来表示，使得矩阵更加<strong class="kh ir">稀疏</strong>。代替单词级表示，更常见的方法是使用<strong class="kh ir">字符</strong>作为标记，因为它将<strong class="kh ir">限制</strong>向量的长度。</strong></p><p id="cd4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是无论是使用单词级还是字符级表示，不同的句子矩阵都不可避免地会有不同的<strong class="kh ir">形状</strong>(不同的行数)。对于大多数ML模型来说，这可能是一个问题，因为它们的输入形状应该是相同的。<strong class="kh ir">另一方面，基于RNN </strong>的模型，如果设置正确，由于其'<strong class="kh ir">重现</strong>'的性质，不会有这种担心，但是同一个<strong class="kh ir">批次</strong>中的所有实例仍有望共享一个<strong class="kh ir">统一的</strong>形状。</p><p id="52a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这个问题，一个解决方案是为所有实例固定一个长度<strong class="kh ir"><em class="lk"/></strong>——<strong class="kh ir">截断</strong>较长的实例，而<strong class="kh ir">填充</strong>。对于像短文本的<strong class="kh ir">情感</strong> <strong class="kh ir">分析</strong>这样的任务(尽管使用一键处理不会产生好的结果)，通常使用前300个字符就足够了。在<strong class="kh ir"> Keras </strong>中，<strong class="kh ir">填充</strong>令牌可以被<strong class="kh ir">屏蔽</strong>，因此它们不会影响损失。</p><p id="5e41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练语料库的规模可以想有多大就有多大，使词汇量越来越丰富，但在<strong class="kh ir">推理</strong>时间里，总有可能遇到<strong class="kh ir">未知</strong> <strong class="kh ir">单词</strong>。处理这个问题的一个方法是在初始化时在词汇表中保留一些空间，这样当一个不在词汇表中的单词弹出时，它可以被分配到其中一个保留位置(一个<strong class="kh ir">oov</strong>T26】bucket)。</p><p id="23df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种特征化文本的方法是通过<strong class="kh ir"> n-gram计数矢量器</strong>，让我们来看看。</p><h2 id="c6d9" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">2.计数矢量器</h2><p id="37d5" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">一键编码的使用允许我们实现句子的<strong class="kh ir">记号</strong> - <strong class="kh ir">级</strong>表示，这是通过用向量替换其记号，同时保持其原始的<strong class="kh ir">顺序</strong> <strong class="kh ir">排列</strong>来实现的。另一方面，计数矢量器基于词频，能够<strong class="kh ir">将整个句子</strong>压缩成一个<strong class="kh ir">单个</strong>矢量。如前所述，计数向量的每个位置被分配给一个特定的标记，其值代表该标记在句子中的 <strong class="kh ir">出现</strong>的<strong class="kh ir">号</strong> <strong class="kh ir">。</strong></p><p id="737f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先从语料库中生成标记，并且构建词汇以将标记映射到它们相应的id。计数向量不是为每个单词构建一个向量，而是简单地<strong class="kh ir">计算</strong>每个单词在句子中出现的次数，并将该数字放在向量中它们对应的<strong class="kh ir">位置</strong>。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="bcfa" class="mb mc iq nu b gy ny nz l oa ob">If we were to represent the sentences from the previous example with count vectors, they would look like this:</span><span id="e396" class="mb mc iq nu b gy oc nz l oa ob">s1: [0, 1, 0, 0, 1, 0, 1, 1, 0]</span><span id="4368" class="mb mc iq nu b gy oc nz l oa ob">s2: [1, 1, 1, 1, 0, 1, 1, 0, 1]</span><span id="de37" class="mb mc iq nu b gy oc nz l oa ob">vocabulary = {"here": 0, "is": 1, "now": 2, "number": 3, "one": 4, "our": 5, "sentence": 6, "this": 7, "two": 8}</span></pre><p id="e792" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在所有的句子都由共享相同长度<strong class="kh ir"> <em class="lk"> l </em> </strong>的向量表示，长度由词汇表中<strong class="kh ir">唯一</strong>标记的数量定义，默认为<strong class="kh ir">但也可以手动选择<strong class="kh ir"/>。这种表示句子的方式将无法提供原句子的任何标记<strong class="kh ir">排序</strong> <strong class="kh ir">信息</strong>，并且其关联的<strong class="kh ir">上下文</strong>信息丢失——只有<strong class="kh ir">术语</strong> <strong class="kh ir">频率</strong>反映在计数向量中。</strong></p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="ce25" class="mb mc iq nu b gy ny nz l oa ob">For illustration purposes, let's vectorize sentence s3:</span><span id="22ca" class="mb mc iq nu b gy oc nz l oa ob">s3: <strong class="nu ir"><em class="lk">"this this this is is one one one one"</em></strong><br/>--&gt; feature counts: "this" x 3, "is" x 2, "one" x 4</span><span id="78a4" class="mb mc iq nu b gy oc nz l oa ob">s3: [0, 2, 0, 0, 4, 0, 0, 3, 0]</span></pre><p id="7172" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了使用单个单词/字符作为标记，这可以被视为<strong class="kh ir">单字母</strong>计数矢量器，我们还可以使用两个或更多连续的单词/字符来形成标记，获得所谓的2字母、3字母或更一般的<strong class="kh ir"> n字母</strong>矢量器。</p><h2 id="b2f9" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated"><strong class="ak">垃圾邮件检测中的用法</strong></h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi og"><img src="../Images/7101acd7f444fa5808cda3b13cccdd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*st-80BqMyU5WXakNkR9CZg.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated"><a class="ae ns" href="https://unsplash.com/@webaroo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Webaroo </a>在<a class="ae ns" href="https://unsplash.com/s/photos/spam-email?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b822" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计数向量的一个用例是，例如，<strong class="kh ir">垃圾邮件</strong> <strong class="kh ir">电子邮件</strong> <strong class="kh ir">检测</strong>与<strong class="kh ir">朴素</strong> <strong class="kh ir">贝叶斯</strong>模型。</p><p id="5aac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">邮件过滤的主要<strong class="kh ir">标准</strong>是每类邮件中的<strong class="kh ir">术语</strong> <strong class="kh ir">频率</strong>，即一个单词在垃圾邮件和非垃圾邮件中出现的次数。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="d3f7" class="mb mc iq nu b gy ny nz l oa ob">The model learns through training about what words are more spam-like words, and what words are not. It assumes <strong class="nu ir">conditional</strong> <strong class="nu ir">independence</strong> for each email’s content words, meaning they are mutually independent and their appearances are only conditioned by the email’s <strong class="nu ir">label</strong>(spam, Y=1, or non-spam, Y=0).</span><span id="cf98" class="mb mc iq nu b gy oc nz l oa ob">The probability of a given email being a Spam(Y=1), given its content words(X), is calculated using <strong class="nu ir">Bayes</strong> <strong class="nu ir">rule</strong>. According to which, such probability is determined by the probability of seeing these words in those Spam emails, P(X|Y=1), multiplied by the portion of Spam emails the model has seen during training, P(Y=1), and divided by P(X).</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oh"><img src="../Images/101e6b196e2d9c8c8ef9d8e5148d060c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtDCY46Eicd25LU8GHVnGg.png"/></div></div></figure><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="74b6" class="mb mc iq nu b gy ny nz l oa ob">With X being the collection of words from an email, {x1, x2, …, xm}, by assuming <strong class="nu ir">conditional</strong> <strong class="nu ir">independence</strong>, P(X|Y=1) is calculated as the product of the probabilities of seeing these words in Spam emails:</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oi"><img src="../Images/a065846195540d983b5bd0508ef850dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5XLr9KsQrxFGONbyrh2Vg.png"/></div></div></figure><p id="f8b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里使用计数向量的一个<strong class="kh ir">缺点</strong>是<strong class="kh ir">未知的</strong>单词被<strong class="kh ir">扔掉</strong><strong class="kh ir"/>。如果你有一个句子有m个<strong class="kh ir"><em class="lk"/></strong>单词和n个<strong class="kh ir"><em class="lk"/></strong>单词，而这些单词在模型中从未出现过，那么只有m-n个 单词会被考虑进去。这被狡猾的垃圾邮件发送者用于<strong class="kh ir">过滤</strong> <strong class="kh ir">规避</strong>——修改垃圾邮件关键词，使它们不为模型所知，但仍然对用户可读。该模型将看到的内容仅包含<strong class="kh ir">中性</strong>单词，并且该电子邮件可以被分类为<strong class="kh ir">非垃圾邮件</strong>。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="6dff" class="mb mc iq nu b gy ny nz l oa ob">- Sentence from a Spam email:</span><span id="2c14" class="mb mc iq nu b gy oc nz l oa ob"><em class="lk">"come </em><strong class="nu ir"><em class="lk">make</em></strong><em class="lk"> some </em><strong class="nu ir"><em class="lk">money</em></strong><em class="lk">, ten </em><strong class="nu ir"><em class="lk">thousand</em></strong><em class="lk"> </em><strong class="nu ir"><em class="lk">dollar</em></strong><em class="lk"> a week for doing nothing"</em></span><span id="d9f6" class="mb mc iq nu b gy oc nz l oa ob">- With modified words:</span><span id="39c0" class="mb mc iq nu b gy oc nz l oa ob"><em class="lk">"come </em><strong class="nu ir"><em class="lk">mayke</em></strong><em class="lk"> some </em><strong class="nu ir"><em class="lk">m0ney</em></strong><em class="lk">, ten </em><strong class="nu ir"><em class="lk">th0usand</em></strong><em class="lk"> </em><strong class="nu ir"><em class="lk">dollar$</em></strong><em class="lk"> a week for doing nothing"</em></span><span id="ec86" class="mb mc iq nu b gy oc nz l oa ob">- The sentence the model will see:</span><span id="6518" class="mb mc iq nu b gy oc nz l oa ob"><em class="lk">"come some, ten a week for doing nothing"</em></span></pre><p id="4148" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了其他优点之外，特征散列有助于减轻损害。</p><h2 id="bacf" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">特征散列</h2><p id="2c6e" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">随着模型用越来越大的语料库训练，它产生的词汇需要越来越多的存储空间来存储。为了提高效率，这些查找表被存储在<strong class="kh ir"> RAM </strong>中，用于快速<strong class="kh ir">令牌id </strong> <strong class="kh ir">映射</strong>，并且一旦它们的大小变得太大，能够<strong class="kh ir">减慢</strong> <strong class="kh ir">操作。</strong></p><p id="6e41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用<strong class="kh ir">散列</strong> <strong class="kh ir">技巧</strong>，我们可以完全摆脱这种<strong class="kh ir">消耗内存的</strong>词汇，而代之以使用<strong class="kh ir">散列</strong> <strong class="kh ir">函数</strong>进行令牌-id映射。对于一个给定的字符串，哈希函数能够返回一个数值<strong class="kh ir">、</strong>一个<strong class="kh ir">哈希</strong>、<strong class="kh ir">值</strong>，也就是<strong class="kh ir">对该字符串唯一的</strong>，并将其作为<strong class="kh ir">令牌</strong>、<strong class="kh ir"> id </strong>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi gj"><img src="../Images/c74686930d7205c9160fdc50ee8f4164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hNpaXBMYtDfvfDXpapoYbA.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">使用散列函数进行令牌-id映射。</p></figure><p id="01d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于不涉及固定大小的词汇，所有的记号现在都可以分配给一个数字，不管模型之前是否见过它。在<strong class="kh ir">垃圾信息</strong> <strong class="kh ir">检测</strong>的情况下，<strong class="kh ir">通过让垃圾信息词变得对模型陌生来绕过</strong>过滤器的方式不再那么有效。任何给定的字，“已知”或“未知”，都可以被馈送到<strong class="kh ir">散列</strong>T42函数并输出一个在<strong class="kh ir">预定义的</strong> <strong class="kh ir">范围</strong>内的<strong class="kh ir">数字</strong> <strong class="kh ir">值</strong>。它在计数向量中的相应位置将加1，而不是丢弃未知单词，因此电子邮件中的所有单词都将被考虑在内，而不仅仅是中性单词。</p><p id="2489" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与词汇表不同，特性哈希是一种<strong class="kh ir"> one </strong> - <strong class="kh ir"> way </strong>操作——我们无法通过哈希函数使用其<strong class="kh ir">哈希</strong> <strong class="kh ir">值</strong>找到<strong class="kh ir">初始</strong> <strong class="kh ir">特性</strong>。相同的输入将总是产生相同的输出，但是两个不同的特征可能碰巧被映射到相同的散列值(如果向量大小很大，即2个⁴).，这种可能性可以忽略不计)</p><h2 id="b86c" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">Tf-idf术语权重</h2><p id="eab0" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">如前所述，当表示<strong class="kh ir">句子</strong> <strong class="kh ir">上下文</strong>时，计数向量是有缺陷的，因为它们不反映任何初始的<strong class="kh ir">标记</strong> <strong class="kh ir">排序</strong>。当诸如“like”、“a”或“and”等几乎没有意义的<strong class="kh ir">上下文</strong> <strong class="kh ir">信息</strong>的术语出现得太频繁时，事情会变得更糟，通过降低它们的频率，将模型的<strong class="kh ir">注意力</strong>从那些<strong class="kh ir">不太频繁但更有趣的</strong>术语上移开。</p><p id="f112" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Tf-idf </strong>代表<strong class="kh ir">术语</strong> - <strong class="kh ir">频率</strong> (tf)乘以<strong class="kh ir">逆</strong> <strong class="kh ir">文档</strong> - <strong class="kh ir">频率</strong> (idf)，它用于根据每个标记出现在多少个不同的句子中，通过<strong class="kh ir">重新加权</strong>计数特征来解决这个问题。假设一个<strong class="kh ir">术语的</strong> <strong class="kh ir">相关性</strong>与其在不同文档中出现<strong class="kh ir">的</strong>的<strong class="kh ir">号</strong>之间存在<strong class="kh ir">逆</strong> <strong class="kh ir">关系</strong>，则<strong class="kh ir">惩罚</strong>。</p><p id="6f80" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个例子，让我们看看<strong class="kh ir"> Scikit-Learn的</strong> tf-idf实现、<strong class="kh ir"> TfidfTransformer </strong>，在默认设置中:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oj"><img src="../Images/3c86713b79f1979b1b9a5ebe680b4ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFloqKJO5d7m9CmImy8PRw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">Scikit-Learn默认设置中的TfidfTransformer。</p></figure><h2 id="b051" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">3.单词嵌入</h2><p id="346e" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">到目前为止，我们已经看到了两种类型的表示:<strong class="kh ir"> One-hot </strong>编码，这是一种标记级别的表示，允许保留初始句子中的标记顺序；以及<strong class="kh ir">计数向量</strong>，这是一种更紧凑的<strong class="kh ir">句子级别的</strong>表示，它依赖于术语频率。</p><p id="fb19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于NLP任务，如<strong class="kh ir">文本生成</strong>或<strong class="kh ir">分类</strong>，一键表示或计数向量可能<strong class="kh ir">能够</strong>足以表示模型做出明智决策所需的<strong class="kh ir">信息。但是，它们的用法对于其他任务来说就不那么有效了，例如<strong class="kh ir">情感</strong> <strong class="kh ir">分析</strong>、<strong class="kh ir">神经</strong> <strong class="kh ir">机器</strong> <strong class="kh ir">翻译</strong>以及<strong class="kh ir">问题</strong> <strong class="kh ir">回答</strong>，这些任务需要对<strong class="kh ir">上下文</strong>有更深入的理解才能获得很好的结果。</strong></p><p id="305b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以One-hot编码为例，使用它不会导致这些任务的<strong class="kh ir">良好概括的</strong>模型，因为任何两个给定单词之间都不能进行<strong class="kh ir">比较</strong>。所有矢量都是彼此<strong class="kh ir">正交的</strong>，任意两个矢量的<strong class="kh ir">内积</strong> <strong class="kh ir">积</strong>为零，它们的<strong class="kh ir">相似度</strong>不能用<strong class="kh ir">距离</strong>或<strong class="kh ir">余弦相似度</strong>来度量。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="79fc" class="mb mc iq nu b gy ny nz l oa ob">Say that we were to train a language model for text generation and we've provided the model with the following sentence: <strong class="nu ir"><em class="lk">"I want to have an orange", </em></strong>expecting it to output <strong class="nu ir"><em class="lk">"juice"</em></strong>.</span><span id="ef52" class="mb mc iq nu b gy oc nz l oa ob">--&gt; <strong class="nu ir"><em class="lk">"I want to have an orange juice"</em></strong></span><span id="3351" class="mb mc iq nu b gy oc nz l oa ob">If done right, the model should learn that <strong class="nu ir"><em class="lk">"orange juice"</em></strong> is a common thing, or the entire sentence is a common sentence.</span><span id="c93f" class="mb mc iq nu b gy oc nz l oa ob">If we were asked to complete the following sentence with one word: <strong class="nu ir"><em class="lk">"I want to have an apple"</em></strong>. Having seen the first sentence, our common sense will tell us to use <strong class="nu ir"><em class="lk">"juice"</em></strong>, provided that we have a notion on the similarities between <strong class="nu ir"><em class="lk">"orange"</em></strong> and <strong class="nu ir"><em class="lk">"apple"</em></strong>.</span><span id="6d6b" class="mb mc iq nu b gy oc nz l oa ob">But the model we just trained won't be able to mimic this since it doesn't have any clue about how similar these two words are. In fact, it doesn't have any clue on how similar any two given words are, so it won't be able to generalized from <strong class="nu ir"><em class="lk">"orange juice"</em></strong> to <strong class="nu ir"><em class="lk">"apple juice".</em></strong></span></pre><p id="26c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们求助于<strong class="kh ir">单词</strong> <strong class="kh ir">嵌入</strong>，一种能够<strong class="kh ir">捕获</strong>单词的<strong class="kh ir">语义</strong> <strong class="kh ir">含义</strong>的特征化单词级表示。</p><p id="74ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过嵌入，每个单词由一个固定大小的<strong class="kh ir">密集</strong> <strong class="kh ir">向量</strong>表示(通常范围从50到300)，其值对应于一组<strong class="kh ir">特征</strong>，即<em class="lk">男性</em>、<em class="lk">女性</em>、<em class="lk">年龄</em>等。如下图所示，这些特征被视为一个词的语义的<strong class="kh ir">不同的</strong> <strong class="kh ir">方面</strong>，它们的值由<strong class="kh ir">随机</strong> <strong class="kh ir">初始化</strong>得到，并在训练过程中<strong class="kh ir">更新</strong>，就像模型的<strong class="kh ir">参数</strong>一样。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a7b0e29514729a24863c8d8e23f0f706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*-a8qPvHHyfI852o2J3NTAQ.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">嵌入“国王”、“王后”、“女人”和“公主”的向量。图片来自<a class="ae ns" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/text-embedding" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="bcd6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当训练嵌入时，我们不告诉模型这些特征应该是什么，而是由模型来决定什么是学习任务的最佳特征。在建立一个<strong class="kh ir">嵌入</strong> <strong class="kh ir">矩阵</strong>(一组单词嵌入)时，我们只定义它的<strong class="kh ir">形状</strong>——单词的数量和每个向量的长度。每个特征所代表的东西通常很难解释。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ol"><img src="../Images/f25552608c468587f765455740c8396f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xh0wkGPELt5ec_F8uP958w.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">单词嵌入的可视化。图片来自<a class="ae ns" href="http://www.vima.co.za/2017/03/21/a-dive-into-word2vec-tsne-for-visualisation/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="67a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单词嵌入捕捉<strong class="kh ir">语义</strong> <strong class="kh ir">含义</strong>的能力可以通过将这些高维向量通过<strong class="kh ir"> t-SNE </strong>投影到2D空间进行可视化来说明。如果成功地获得了嵌入，用t-SNE绘制这些向量将展示具有相似含义的单词如何最终变得更加接近。</p><p id="0d0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同嵌入向量之间的<strong class="kh ir">语义</strong> <strong class="kh ir">关系</strong>也可以用下面的例子来说明。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="71cc" class="mb mc iq nu b gy ny nz l oa ob">Say that we have the embedding vectors of the word "<strong class="nu ir">king</strong>", "<strong class="nu ir">queen"</strong>, "<strong class="nu ir">man</strong>", and "<strong class="nu ir">woman"</strong>. The resulting vectors of subtracting "<strong class="nu ir">queen</strong>" from "<strong class="nu ir">king</strong>" and "<strong class="nu ir">woman</strong>" from "<strong class="nu ir">man</strong>" would be very similar direction-wise since they both carry similar values for that <strong class="nu ir"><em class="lk">Gender</em></strong> feature.</span><span id="4e42" class="mb mc iq nu b gy oc nz l oa ob">"<strong class="nu ir">king</strong>" - "<strong class="nu ir">queen</strong>" ≃ "<strong class="nu ir">man</strong>" - "<strong class="nu ir">woman</strong>"</span><span id="eb00" class="mb mc iq nu b gy oc nz l oa ob">or equivalently,</span><span id="5e88" class="mb mc iq nu b gy oc nz l oa ob">"<strong class="nu ir">king</strong>" - "<strong class="nu ir">queen</strong>" + "<strong class="nu ir">woman</strong>" ≃ "<strong class="nu ir">man</strong>"</span></pre><h2 id="66ec" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">训练词嵌入</h2><p id="927b" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">前面我们提到过<strong class="kh ir">嵌入</strong>向量可以像另一个<strong class="kh ir">层</strong>(在神经网络中)一样被训练，它们也可以被单独训练<strong class="kh ir"/>并在以后通过<strong class="kh ir">转移</strong> <strong class="kh ir">学习</strong>用于不同的任务。</p><p id="5c10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有不同的方法来训练嵌入，但是原理大致相同。这里我们简单讨论两种<strong class="kh ir"> Word2Vec </strong>方法，即<strong class="kh ir">连续</strong> <strong class="kh ir">包字</strong> (CBOW)和<strong class="kh ir">跳字</strong>。</p><p id="25b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Word2Vec </strong>是浅层的两层<strong class="kh ir">神经</strong> <strong class="kh ir">网络</strong>，被训练来重构单词的语言<strong class="kh ir">上下文</strong>。我们从大型语料库中提取<strong class="kh ir">上下文/目标</strong>单词的<strong class="kh ir">对</strong>进行训练，其中<strong class="kh ir">目标</strong>单词是随机选择的<strong class="kh ir">单词，而<strong class="kh ir">上下文</strong>单词是位于目标单词周围给定<strong class="kh ir">窗口</strong>内的那些单词。</strong></p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="afca" class="mb mc iq nu b gy ny nz l oa ob">There are different varieties of context, they can be 1)all the words within the window, 2) n words before or 3) after the target word, or 4) simply a random word located within the window.</span><span id="e570" class="mb mc iq nu b gy oc nz l oa ob">Sentence: "<em class="lk">Learning Spanish </em><strong class="nu ir"><em class="lk">is</em></strong><em class="lk"> hard but also fun</em>"<br/> <br/>- target word: "is"<br/>- windows length is 2, for both directions<br/>- context:<br/> 1. "learning", "Spanish", "hard", "but"<br/> 2. "learning", "Spanish"<br/> 3. "hard", "but"<br/> 4. "learning"</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4ab33a27dd8fd321c58ecd929187a6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JdBwJjvcn2XvE5Hwl0z_JA.jpeg"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">CBOW(左)和Skip-gram(右)的模型架构图。图片来自<a class="ae ns" href="https://www.researchgate.net/publication/328373466_Training_Neural_Language_Models_with_SPARQL_queries_for_Semi-Automatic_Semantic_Mapping" rel="noopener ugc nofollow" target="_blank">本文</a>。</p></figure><p id="c1e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于<strong class="kh ir"> CBOW </strong>，将<strong class="kh ir">上下文</strong>字以其嵌入形式(随机初始化)输入到模型中，并且期望模型使用softmax，P(目标|上下文)输出<strong class="kh ir">目标</strong>字。<strong class="kh ir"> Skip-gram </strong>，另一方面，与相反的<strong class="kh ir">，它接受<strong class="kh ir">目标</strong>单词，并预期输出<strong class="kh ir">上下文</strong>单词。</strong></p><p id="89a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，这些听起来可能很难<strong class="kh ir">学习</strong> <strong class="kh ir">任务</strong>，但是请记住，目标不是做好任务本身，而是学习好<strong class="kh ir">嵌入</strong>，这些模型可以做到这一点。</p><h2 id="9640" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">语境化的单词嵌入</h2><p id="0ed3" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated"><strong class="kh ir">一词多义</strong>代表书写相同但根据<strong class="kh ir">上下文</strong>它们可以有<strong class="kh ir">完全</strong> <strong class="kh ir">不同</strong>的意思。如上所述的单词嵌入将使<strong class="kh ir">无法</strong>处理这个问题。</p><pre class="lm ln lo lp gt nt nu nv nw aw nx bi"><span id="e7da" class="mb mc iq nu b gy ny nz l oa ob">The word "<strong class="nu ir">bank</strong>" in the following two sentences has <strong class="nu ir">different</strong> <strong class="nu ir">meanings</strong>, but since they are assigned to the <strong class="nu ir">same</strong> <strong class="nu ir">token</strong> <strong class="nu ir">id</strong>, their word <strong class="nu ir">embedding</strong> vectors are the <strong class="nu ir">same</strong>.</span><span id="0c36" class="mb mc iq nu b gy oc nz l oa ob">1. "I went to the <strong class="nu ir">bank</strong> to withdraw some money."<br/>2. "I went to the other side of the river <strong class="nu ir">bank</strong>."</span></pre><p id="bf1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这个问题，每个工作嵌入必须考虑到找到单词的<strong class="kh ir">上下文</strong>并相应地修改它的值。携带一个一般化的<strong class="kh ir">嵌入</strong> <strong class="kh ir">矩阵</strong>并将其插入到模型中对于我们正在尝试做的任务来说是不充分的，相反，一个更加复杂的<strong class="kh ir">结构</strong> <strong class="kh ir">必须被包含到模型的下部才能找到<strong class="kh ir"/><strong class="kh ir">嵌入</strong>。</strong></p><p id="f09e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于<strong class="kh ir">神经</strong> <strong class="kh ir">机器</strong> <strong class="kh ir">翻译</strong> (NMT)的基于<strong class="kh ir">编解码</strong>的架构是一个很好的例子来介绍<strong class="kh ir">预训练</strong> + <strong class="kh ir">微调</strong>方法论，这是近年来NLP中最新的<strong class="kh ir">突破</strong>的原因。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/851b47fd0e761d8bbe79d4df518ea8e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbx7o2BGt6ZJBJkPgjElhA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">a)用于机器翻译的序列到序列模型的预训练。b)将模型的编码器用于其他任务。图片来自<a class="ae ns" href="https://arxiv.org/abs/1708.00107" rel="noopener ugc nofollow" target="_blank">封面纸</a>。</p></figure><p id="b1e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这张图来自<strong class="kh ir"> <em class="lk">学习翻译:语境化的</em> </strong> <em class="lk"> </em> <strong class="kh ir"> <em class="lk">字</em> </strong> <em class="lk"> </em> <strong class="kh ir"> <em class="lk">向量</em> </strong> (CoVe)描述了在<strong class="kh ir">预训练</strong>中获得<strong class="kh ir"/><strong class="kh ir">嵌入</strong>的过程，以及它们如何用于<strong class="kh ir">下游<strong class="kh ir"/></strong></p><p id="2a92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在左边，一个基于编码器-解码器的模型为<strong class="kh ir"> NMT </strong>训练，来自原始语言的句子首先由<strong class="kh ir">编码器</strong>处理，然后其输出被传递到<strong class="kh ir">解码器</strong>进行最终翻译。这个<strong class="kh ir">预训练</strong>过程是受监督的，目标是让编码器学会如何捕捉单词的<strong class="kh ir">句法</strong>和<strong class="kh ir">语义</strong>含义，并输出<strong class="kh ir">上下文化的</strong> <strong class="kh ir">嵌入</strong>。编码器基于两层<strong class="kh ir">双向</strong> <strong class="kh ir"> LSTM </strong>架构，解码器基于<strong class="kh ir">注意力</strong> <strong class="kh ir">单向</strong> <strong class="kh ir"> LSTMs </strong>。</p><p id="98a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在右边，一个预先训练的<strong class="kh ir">编码器</strong>接收输入、<strong class="kh ir">手套</strong>嵌入，以找到<strong class="kh ir">上下文化的</strong> <strong class="kh ir">嵌入</strong>，然后它们<strong class="kh ir">与原始输入</strong>组合，用于一个<strong class="kh ir">下游</strong>任务。</p><p id="3b9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> CoVe </strong>的局限性在于:1)预训练是由<strong class="kh ir">监督</strong>的，因此它受到标注为 <strong class="kh ir">的<strong class="kh ir">数据</strong>的数量的限制，2)<strong class="kh ir">任务特定的</strong>模型的<strong class="kh ir">架构</strong>还有待定义，要找到一个能够实现出色结果的架构并不是一件轻而易举的事情。通过克服这两个障碍，我们希望找到一个模型，它可以1)用<strong class="kh ir">无限</strong>数据进行预训练——<strong class="kh ir">无监督</strong>训练，2) <strong class="kh ir">对有限的标记数据进行微调</strong>，并对其架构进行一些小的修改，以实现不同NLP任务的<strong class="kh ir"> SOTA </strong>结果。</strong></p><p id="1777" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面简单介绍一下<strong class="kh ir">埃尔莫</strong>、<strong class="kh ir">奥纳尔GPT </strong>和<strong class="kh ir">伯特</strong>是如何解决上述局限性的。更详细的解释请看这篇<strong class="kh ir">惊人的</strong>帖子:<a class="ae ns" href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="lk">广义语言模型</em></strong></a><strong class="kh ir"><em class="lk"/></strong>作者<em class="lk"> Lilian Weng。</em></p><p id="1142" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语言模型</strong>的嵌入(ELMo)通过以<strong class="kh ir">无监督</strong>方式训练<strong class="kh ir">语言</strong> <strong class="kh ir">模型</strong>来获得<strong class="kh ir">上下文化</strong>嵌入——接受一系列标记，并学习<strong class="kh ir">预测</strong>给定<strong class="kh ir">历史</strong>的下一个标记。其结构基于<strong class="kh ir">双向</strong><strong class="kh ir">LSTMs</strong>，其中<strong class="kh ir"> <em class="lk"> L </em> </strong>层的lstm是<strong class="kh ir">一层一层的堆叠</strong>，每一层输出一个<strong class="kh ir">不同的</strong>序列表示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oo"><img src="../Images/4d45c35d43f8a596e1d54fc85a2636fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eFh5yDpdy9nInndCC0V0uw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">双向LSTM。图片来自Lilian Weng的博客。</p></figure><p id="372d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同的层次侧重于单词的句法/语义的不同方面。<strong class="kh ir"> ELMo </strong>较高层更关注语义方面，而较低层可以捕捉更好的句法方面。</p><p id="a920" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">双向LSTMs用于确保模型不仅学习预测给定令牌的<strong class="kh ir">未来</strong>，还学习预测其<strong class="kh ir">过去</strong>。</p><p id="d269" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ELMo仅用于查找<strong class="kh ir">上下文化的</strong>嵌入。对于给定的任务，仍然需要找到特定的模型<strong class="kh ir">架构</strong>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi op"><img src="../Images/988c4411b976a3ecfbb635c684c26565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJymmx5uwcqAPqRjHHYETQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">OpenAI GPT用于不同的下游任务。图片来自<a class="ae ns" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">原纸</a>。</p></figure><p id="b895" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> OpenAI GPT </strong>，基于<strong class="kh ir">变压器的解码器</strong>，可直接用于<strong class="kh ir">所有</strong>终端任务。如图所示，不同的任务有不同的<strong class="kh ir">预处理</strong>步骤，但只需对<strong class="kh ir"> GPT的变形金刚模型</strong>稍加修改即可完成任务。</p><p id="e55d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与ELMo不同，<strong class="kh ir"> GPT </strong>只被训练来预测<strong class="kh ir">未来</strong>，但是为了更好地理解给定令牌的上下文，来自其<strong class="kh ir">左</strong>和<strong class="kh ir">右</strong>的项目都应该被考虑。</p><p id="2d0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> BERT </strong>基于<strong class="kh ir"> Transformer的编码器</strong>，被训练预测来自<strong class="kh ir">左</strong>和<strong class="kh ir">右</strong>的上下文。它的预培训包括两项任务:</p><ol class=""><li id="f618" class="mu mv iq kh b ki kj kl km ko oq ks or kw os la ot nc nd ne bi translated"><strong class="kh ir">屏蔽语言模型</strong>，用<strong class="kh ir">【屏蔽】"</strong>随机替换标记，该模型必须通过查看其上下文来输出其原始单词。通过这样做，它迫使模型更好地捕捉缺少的单词的<strong class="kh ir">句法</strong>和<strong class="kh ir">语义</strong>含义。</li><li id="69d7" class="mu mv iq kh b ki nf kl ng ko nh ks ni kw nj la ot nc nd ne bi translated"><strong class="kh ir">下一句预测</strong>。BERT不是每次只取一个句子，而是取其中的一对，A和B，作为输入，它必须预测B是否需要A。经过这项任务的训练，模型将更有能力理解 <strong class="kh ir">句子</strong>之间的<strong class="kh ir">关系</strong> <strong class="kh ir">。</strong></li></ol><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ou"><img src="../Images/53ab3a7095f2b0a07b989b231711706e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8RQ2bSb4VEvlwBx_mui7NA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">伯特的输入。图片来自<a class="ae ns" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特论文</a>。</p></figure><p id="9412" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">BERT使用<strong class="kh ir">单词块</strong> <strong class="kh ir">标记化</strong> <strong class="kh ir">嵌入</strong>作为它们的输入。它不生成正常单词标记，而是使用<strong class="kh ir">子单词</strong> <strong class="kh ir">标记化</strong>来更好地处理<strong class="kh ir">罕见</strong>和<strong class="kh ir">未知</strong>单词，因为它们中的大部分可以使用子单词重构<strong class="kh ir">。两个输入句子有不同的<strong class="kh ir">句子</strong> <strong class="kh ir">嵌入</strong>，并且使用特殊字符来分隔它们。<strong class="kh ir">位置</strong> <strong class="kh ir">嵌入</strong>也被使用。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e18e0d116bb53a8f5491fadddad89e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*z9SfuXhc47ENSpDsvykYUQ.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">用于句子分类的微调BERT。图片来自<a class="ae ns" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特论文</a>。</p></figure><p id="8bc3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当使用BERT进行下游任务时，就像GPT一样，只需添加几个新参数。以文本分类为例，我们需要做的是从最后一层取出<strong class="kh ir">【CLS】</strong>token的嵌入，并传递给一个<strong class="kh ir"> softmax </strong>。</p><h2 id="80a2" class="mb mc iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">结论:</h2><p id="554c" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko od kq kr ks oe ku kv kw of ky kz la ij bi translated">如果你从头到尾都读完了，这就不是一篇短文，但是如果你没有，这里有一个快速回顾:</p><p id="efba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文讨论了三种类型的<strong class="kh ir">文本</strong> <strong class="kh ir">表示</strong>，其中前两种是稀疏向量:<strong class="kh ir">一键</strong> <strong class="kh ir">编码</strong>一方面是一种<strong class="kh ir">标记级</strong>表示，其中一个句子在用作输入之前被转换成一个矩阵，其行数与其标记数相同。<strong class="kh ir">计数矢量器</strong>则可以把一个句子作为一个整体，挤压成<strong class="kh ir">一个</strong> <strong class="kh ir">单个</strong> <strong class="kh ir">矢量</strong>。它依赖于统计<strong class="kh ir">项</strong> <strong class="kh ir">频率</strong>，而这是以丢失关于句子的<strong class="kh ir">标记</strong> <strong class="kh ir">排序</strong>的信息为代价的。</p><p id="8b0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<strong class="kh ir">散列</strong> <strong class="kh ir">技巧</strong>可以帮助解决大<strong class="kh ir">词汇表</strong>的<strong class="kh ir">内存</strong> <strong class="kh ir">消耗</strong>的问题，并且它还缓解了在垃圾邮件检测的情况下<strong class="kh ir">过滤器</strong> <strong class="kh ir">规避</strong>的问题。<strong class="kh ir"> Tf-idf </strong>用于<strong class="kh ir">重新加权</strong> <strong class="kh ir">术语</strong> <strong class="kh ir">频率</strong>用于计数向量，因此<strong class="kh ir">较少</strong> <strong class="kh ir">频繁</strong>但<strong class="kh ir">上下文</strong> - <strong class="kh ir">揭示</strong>术语不会被“忽略”。</p><p id="5cd3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">单词</strong> <strong class="kh ir">嵌入</strong>，一种更高级的技术，在需要更好地捕捉单词的<strong class="kh ir">语义</strong> <strong class="kh ir">含义</strong>时使用。使用更复杂的结构来处理输入，以获得<strong class="kh ir">上下文化的</strong> <strong class="kh ir">嵌入</strong>，这又用于解决<strong class="kh ir">多义性</strong>的问题。</p><p id="7864" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文章最后对最新的自然语言处理模型进行了简要的讨论。目的是提供一些基本的想法，需要投入更多的努力来进一步理解这些模型。</p></div></div>    
</body>
</html>