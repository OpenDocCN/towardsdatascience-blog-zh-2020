<html>
<head>
<title>What links linear regression, ridge regression, and PCA?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归，岭回归，主成分分析有什么联系？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-what-links-ols-ridge-regression-and-pca-b64fcaf37b33?source=collection_archive---------34-----------------------#2020-09-17">https://towardsdatascience.com/under-the-hood-what-links-ols-ridge-regression-and-pca-b64fcaf37b33?source=collection_archive---------34-----------------------#2020-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9b6f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/under-the-hood" rel="noopener" target="_blank">引擎盖下</a></h2><div class=""/><div class=""><h2 id="a9a6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从保存病态回归问题到启用正则化路径的快速计算，这是引擎盖下的链接。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1efa1fda461fbfefb45c9914674f590f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4nbgRVET9bRl_NMNhwrUQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3011368" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae lh" href="https://pixabay.com/users/doria150-7337031/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3011368" rel="noopener ugc nofollow" target="_blank"> doria150 </a></p></figure><p id="c590" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我正在写<a class="ae lh" href="https://towardsdatascience.com/tagged/under-the-hood" rel="noopener" target="_blank">一个新的系列</a>(相对较短)帖子，围绕统计学习中的基础话题。特别是，这一系列将会有意想不到的发现，很少被谈论的联系，和统计学习的幕后概念。</p><p id="d8eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的第一篇文章从岭正则化开始，<a class="ae lh" href="https://www.tandfonline.com/doi/abs/10.1080/00401706.2020.1791959" rel="noopener ugc nofollow" target="_blank">数据科学中的一个基本概念</a>。<strong class="lk jd">普通最小二乘(OLS)估计、岭估计和 PCA 之间简单而优雅的关系可以通过光谱分解的透镜找到。</strong>我们通过<a class="ae lh" href="https://books.google.com.sg/books/about/Multivariate_Analysis.html?id=bxjvAAAAMAAJ&amp;source=kp_book_description&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank">多元分析</a>的练习 8.8.1 看到了这些关系。</p><p id="6caa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章改编自我的一篇博文，省略了所有的证明。如果你喜欢 LaTex 格式的数学和 HTML 风格的页面，你可以在我的博客上阅读这篇文章。</p><h1 id="6eb2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">设置</h1><p id="bfa8" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">给定以下回归模型:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/01341f92b00f9db0f0ffcb658b21a536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17B1A6G2fyBsZ6629UfyaA.png"/></div></div></figure><p id="3864" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑一下<strong class="lk jd"> X`X </strong>(在本文中，`和上标 T 都表示转置)的列已经标准化为均值为 0，方差为 1。那么<strong class="lk jd"><em class="nc"/></strong>的岭估计为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/555fbea9220f01bcbb5953004685f8e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*xXWGjlwHxxJSb1rILJX8cA.png"/></div></figure><p id="8aff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中对于给定的<strong class="lk jd"> X </strong>，λ≥0 为小的固定脊正则化参数。注意，当λ=0 时，它只是 OLS 公式。另外，考虑 var-cov 矩阵的谱分解<strong class="lk jd"> X`X </strong> = <strong class="lk jd"> GLG` </strong>。设<strong class="lk jd"> W </strong> = <strong class="lk jd"> XG </strong>为原始数据矩阵的主成分变换。</p><h1 id="3893" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结果 1.1</h1><p id="879a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">如果<strong class="lk jd"><em class="nc">α</em></strong>=<strong class="lk jd">g`<em class="nc">β</em></strong>表示主分量的参数向量，那么我们可以表明，通过简单地用岭正则化参数对它们进行缩放，岭估计<strong class="lk jd"> <em class="nc"> α* </em> </strong>可以从 OLS 估计 hat( <strong class="lk jd"> <em class="nc"> α </em> </strong>)中获得:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/ae9da7c2deba2a8a9ced21e59c2ef4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CirACamjCJWUkGIfNwJ0Ww.png"/></div></div></figure><p id="5ca5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个结果向我们展示了两个重要的见解:</p><ol class=""><li id="68aa" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">对于 PC 转换的数据，<strong class="lk jd">我们可以通过 OLS 估计值的简单元素式缩放来获得岭估计值</strong>。</li><li id="b73c" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">脊正则化的<strong class="lk jd">收缩效果取决于λ和相应 PC 的特征值</strong>:<br/>-较大的λ对应于每个参数的较大收缩。<br/>——然而，给定相同的λ，对应于较大特征值的主成分得到最小的收缩。</li></ol><h2 id="a578" class="nt mf it bd mg nu nv dn mk nw nx dp mo lr ny nz mq lv oa ob ms lz oc od mu iz bi translated">形象化</h2><p id="a81e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了演示这两种收缩效应，我绘制了收缩百分比(1-hat(<strong class="lk jd"><em class="nc">α</em></strong>)与有序主分量以及脊正则化参数值的函数关系。从该图中可以清楚地看到两种收缩效应。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/c28302a3a3eb44a705f2f7205de6e97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*yszpOcD5LH5A63VIlG71aw.png"/></div></figure><h1 id="c34f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结果 1.2</h1><p id="f856" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">从结果 1.1 可以得出，通过 var-cov 矩阵的谱分解，我们可以在 OLS 估计 hat(<strong class="lk jd"><em class="nc">【β】</em></strong>)和岭估计<strong class="lk jd"><em class="nc">【β*</em></strong>之间建立直接联系(因此有了本文的标题)。具体来说，我们有</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/6ce852083724e0fea3f723e9858c68e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-evKG0ozCCNNQ5OCgt5Xg.png"/></div></div></figure><h1 id="2eb4" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结果 1.3</h1><p id="a2f7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">估计量<strong class="lk jd"><em class="nc">【β*】</em></strong>质量的一个度量是跟踪均方误差(MSE):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9679a58cd6ce4f7a9c99a3588eb30cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*4H0oUJuz-Y1BW6orCmiJFg.png"/></div></figure><p id="05b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，从前面的两个结果可以看出，岭估计的迹 MSE 可以分解为两部分:<strong class="lk jd">方差</strong>和<strong class="lk jd">偏差</strong>，并得到它们的显式公式。MSE 的精确公式的可用性允许诸如正则化路径之类的东西被容易地计算。具体来说，我们有</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/0790c9c078decca7c59de30d83a41e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GtmgUoT0DIFvoxUeHccAGw.png"/></div></div></figure><p id="1f48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中第一个分量是方差之和:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/c67b94e2b4541289494f85323976e4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raGgAEP3r7YYsG5gnooJMQ.png"/></div></div></figure><p id="9be3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二部分是偏差平方和:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/df6b246fc1156b42e0bb8a915ba7c818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*qQ-o1titB3QAwD0jLDKCdA.png"/></div></figure><h1 id="86b4" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结果 1.4</h1><p id="b8c7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这是从结果 1.3 得出的一个快速但有启发性的结果。对轨迹 MSE 函数对λ取偏导数，并取λ=0，我们得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/2f38071cf860ff5966cfdcb8008b896f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wdxsZ2ZHFgQbp_TPro_02A.png"/></div></div></figure><p id="72a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，当λ为 0 时，跟踪 MSE 函数的梯度为负。这告诉我们两件事:</p><ol class=""><li id="1140" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">我们可以通过采用非零λ值来降低轨迹 MSE。特别是，我们<strong class="lk jd">用一点偏差换取方差</strong>的减少，因为方差函数在λ中单调递减。然而，我们需要在方差和偏差之间找到正确的平衡，以便使总的跟踪 MSE 最小。</li><li id="ba6c" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">当一些特征值(l_i)较小时，通过脊正则化的道 MSE 的减少较高。<strong class="lk jd">也就是说，当预测量之间存在相当大的共线性时，岭正则化可以实现比 OLS 小得多的迹 MSE。</strong></li></ol><h2 id="7194" class="nt mf it bd mg nu nv dn mk nw nx dp mo lr ny nz mq lv oa ob ms lz oc od mu iz bi translated">形象化</h2><p id="4db6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">使用<code class="fe ol om on oo b">sklearn.metrics.make_regression</code>函数，我生成了一个包含 50 个样本和 10 个特征的噪声回归数据集。特别是，我要求大多数方差(在 PCA 意义上)只能由这 10 个特征中的 5 个来解释，即最后 5 个特征值相对较小。这是正则化路径和系数误差图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/31a4a51baac2860aa0624f7a39e9e9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9iuUQ4DtFLiE_o5QhIIiA.png"/></div></div></figure><p id="31b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从图中，我们可以清楚地看到:</p><ol class=""><li id="ae8b" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">增加λ会使每个系数向 0 收缩。</li><li id="9082" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">OLS 过程(两个图的左侧)产生错误的(并且具有大的方差)估计。估计量 MSE 明显大于岭回归的 MSE。</li><li id="393c" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">在岭估计系数的均方误差最小的 1 附近发现一个最佳λ。</li><li id="0b57" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated"><strong class="lk jd">另一方面，大于和小于 1 的λ </strong> <strong class="lk jd">值是次优的，因为在这种情况下它们会导致过度正则化和欠正则化。</strong></li></ol><p id="8588" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">绘制该图的脚本附后。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oq or l"/></div></figure><h1 id="5e81" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="057c" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">通过光谱分解的镜头，我们看到线性回归估计、岭回归估计和 PCA 之间有一个简单而优雅的联系。特别是:</p><ol class=""><li id="4144" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">结果 1.2 表明，当你有数据 var-cov 矩阵的谱分解和通常的线性回归估计时，你可以通过简单的矩阵计算获得每个岭正则化参数值的岭估计。我们刚刚避免了很多矩阵求逆！</li><li id="0608" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">我们对岭估计的迹 MSE 进行了偏差-方差分解。很明显，我们总是可以通过增加正则化强度来减小估计量的方差。</li><li id="d83f" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">虽然轨迹 MSE 函数在λ=0 处的梯度是负的，但是我们上面的可视化表明λ的值需要仔细选择，因为它可能导致过度正则化或欠正则化。</li></ol><p id="cbf2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢您阅读本文！如果你有兴趣了解更多关于统计学习或数据科学的知识，你可以看看我下面的其他文章。尽情享受吧！</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/linear-discriminant-analysis-explained-f88be6c1e00b"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">线性判别分析，已解释</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">直觉、插图和数学:它如何不仅仅是一个降维工具，为什么它在现实世界中如此强大…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj lb ov"/></div></div></a></div></div></div>    
</body>
</html>