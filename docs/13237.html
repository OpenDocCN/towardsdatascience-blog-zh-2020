<html>
<head>
<title>Machine Learning With Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 的机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-spark-f1dbc1363986?source=collection_archive---------20-----------------------#2020-09-11">https://towardsdatascience.com/machine-learning-with-spark-f1dbc1363986?source=collection_archive---------20-----------------------#2020-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="08cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">综合指南</h2><div class=""/><div class=""><h2 id="9ea8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">分布式机器学习框架</h2></div><p id="ace1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated">这是一个关于使用 Spark 分布式机器学习框架构建可扩展 ML 数据管道的综合教程。我将介绍在 Spark MLlib 库中实现的基本机器学习算法，通过本教程，我将在 python 环境中使用 PySpark。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/bc7629b98415bdc8c3c79c71ab909b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxN0vU5hAouRD0god6Undg.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">作者使用 Canva.com 的图片</p></figure><p id="10a6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在几乎每个商业领域，机器学习在解决现实世界的问题方面越来越受欢迎。它有助于解决使用数据的问题，这些数据通常是非结构化的、有噪声的和巨大的。随着数据大小和各种数据源的增加，使用标准技术解决机器学习问题带来了巨大的挑战。Spark 是一个分布式处理引擎，使用 MapReduce 框架来解决与大数据及其处理相关的问题。</p><p id="b58c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Spark framework 有自己的机器学习模块，名为 MLlib。在本文中，我将使用 pyspark 和 spark MLlib 来演示使用分布式处理的机器学习。读者将能够通过真实的例子学习下面的概念。</p><ul class=""><li id="d3f1" class="mm mn it kt b ku kv kx ky la mo le mp li mq lm mr ms mt mu bi translated">在谷歌联合实验室中建立火花</li><li id="ae12" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">机器学习的基本概念</li><li id="273e" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 Spark 进行预处理和数据转换</li><li id="4121" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 pySpark 进行 spark 聚类</li><li id="4418" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">pyspark 分类</li><li id="d80c" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">pyspark 回归方法</li></ul><blockquote class="na nb nc"><p id="81c1" class="kr ks nd kt b ku kv kd kw kx ky kg kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated">将提供一个工作的 google colab 笔记本来重现结果。</p></blockquote><p id="5806" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于本文是一个实践教程，在一个会话中涵盖了使用 pyspark 进行转换、分类、聚类和回归，因此本文的长度比我以前的文章要长。一个好处是您可以一口气完成基本概念和实现。</p><h1 id="3b02" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">什么是阿帕奇火花？</h1><p id="8657" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">据<a class="ae oe" href="https://databricks.com/p/ebook/apache-spark-under-the-hood" rel="noopener ugc nofollow" target="_blank">阿帕奇星火和三角洲引擎盖下的湖泊</a></p><blockquote class="na nb nc"><p id="cfea" class="kr ks nd kt b ku kv kd kw kx ky kg kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated"><em class="it"> Apache Spark 是一个统一的计算引擎和一组用于在计算机集群上进行并行数据处理的库。截至本文撰写之时，Spark 是针对这一任务开发的最活跃的开源引擎；使其成为任何对大数据感兴趣的开发人员或数据科学家的实际工具。Spark 支持多种广泛使用的编程语言(Python、Java、Scala 和 R)，包括从 SQL 到流和机器学习等各种任务的库，可以在从笔记本电脑到数千个服务器集群的任何地方运行。这使它成为一个易于启动和扩展到大数据处理或超大规模的系统。</em></p></blockquote><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi of"><img src="../Images/c476ce90c4773abd90b2ab0ce1ec13ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Dy9w0lUXIeH6WHALkQC-g.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">作者图片</p></figure><h1 id="d5b4" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">在 Google 联合实验室中设置 Spark 3.0.1</h1><p id="a379" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">作为第一步，我用 spark 安装配置 google colab 运行时。详细内容，读者可以在 Google Colab  om medium 阅读我的文章<a class="ae oe" href="https://medium.com/analytics-vidhya/getting-started-spark3-0-0-with-google-colab-9796d350d78" rel="noopener">入门 Spark 3.0.0。</a></p><p id="2cc7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将安装以下程序</p><ul class=""><li id="89c9" class="mm mn it kt b ku kv kx ky la mo le mp li mq lm mr ms mt mu bi translated">Java 8</li><li id="4d8f" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">火花-3.0.1</li><li id="550b" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">Hadoop3.2</li><li id="9cfc" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated"><a class="ae oe" href="https://github.com/minrk/findspark" rel="noopener ugc nofollow" target="_blank"> Findspark </a></li></ul><p id="ddb1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您可以使用下面的命令集安装最新版本的 Spark。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="8fa8" class="ol ni it oh b gy om on l oo op"># Run below commands<br/>!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null<br/>!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz<br/>!tar xf spark-3.0.1-bin-hadoop3.2.tgz<br/>!pip install -q findspark</span></pre><h2 id="5d36" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">环境变量</h2><p id="6099" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">安装完 spark 和 Java 之后，设置安装 Spark 和 Java 的环境变量。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="2ff2" class="ol ni it oh b gy om on l oo op">import os<br/>os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"<br/>os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop3.2"</span></pre><h2 id="db9e" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">火花安装试验</h2><p id="5cc4" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">让我们在 google colab 环境中测试 spark 的安装。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="2ac7" class="ol ni it oh b gy om on l oo op">import findspark<br/>findspark.init()<br/><br/>from pyspark.sql import SparkSession<br/><br/>spark = SparkSession.builder.master("local[*]").getOrCreate()<br/># Test the spark <br/>df = spark.createDataFrame([{"hello": "world"} for x in range(1000)])<br/><br/>df.show(3, False)</span><span id="2037" class="ol ni it oh b gy pa on l oo op">/content/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead<br/>  warnings.warn("inferring schema from dict is deprecated,"<br/><br/><br/>+-----+<br/>|hello|<br/>+-----+<br/>|world|<br/>|world|<br/>|world|<br/>+-----+<br/>only showing top 3 rows</span><span id="d30d" class="ol ni it oh b gy pa on l oo op"># make sure the version of pyspark<br/>import pyspark<br/>print(pyspark.__version__)</span><span id="d45c" class="ol ni it oh b gy pa on l oo op">3.0.1</span></pre><h1 id="8251" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">机器学习</h1><p id="469d" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">一旦我们在 google colab 中设置了 spark，并确保它运行的是正确的版本，即 3.0.1。在这种情况下，我们可以开始探索基于 Spark 开发的机器学习 API。PySpark 是一个更高级的 Python API，可以将 Spark 与 Python 结合使用。对于本教程，我假设读者对机器学习和用于模型构建和训练的 SK-Learn 有基本的了解。Spark MLlib 使用了与 SK-Learn 中相同的拟合和预测结构。</p><p id="c0f7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了重现结果，我已经把数据上传到我的 GitHub 上，可以方便地访问。</p><blockquote class="na nb nc"><p id="59e0" class="kr ks nd kt b ku kv kd kw kx ky kg kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated">边做边学:使用 colab 笔记本自己运行它</p></blockquote><h1 id="e2ea" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">Spark 中的数据准备和转换</h1><p id="d8d0" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">本节涵盖将输入要素数据转换为机器学习算法接受的格式所涉及的基本步骤。我们将讨论 SparkML 库带来的转换。要了解或阅读更多关于 3.0.3 中可用的 spark 转换，请点击下面的链接。</p><div class="pb pc gp gr pd pe"><a href="https://spark.apache.org/docs/3.0.1/ml-features.html" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd jd gy z fp pj fr fs pk fu fw jc bi translated">提取、转换和选择特征</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">本节介绍处理要素的算法，大致分为以下几组:提取:提取…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">spark.apache.org</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps mg pe"/></div></div></a></div><h2 id="1047" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">规范化数字数据</h2><p id="be3a" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">MinMaxScaler 是大多数机器学习库中最受欢迎的类之一。它在 0 和 1 之间缩放数据。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="39c3" class="ol ni it oh b gy om on l oo op">from pyspark.ml.feature import MinMaxScaler<br/>from pyspark.ml.linalg import Vectors</span><span id="44e1" class="ol ni it oh b gy pa on l oo op"># Create some dummy feature data<br/>features_df = spark.createDataFrame([<br/>    (1, Vectors.dense([10.0,10000.0,1.0]),),<br/>    (2, Vectors.dense([20.0,30000.0,2.0]),),<br/>    (3, Vectors.dense([30.0,40000.0,3.0]),),<br/>    <br/>],["id", "features"] )</span><span id="b2e6" class="ol ni it oh b gy pa on l oo op">features_df.show()</span><span id="fe19" class="ol ni it oh b gy pa on l oo op">+---+------------------+<br/>| id|          features|<br/>+---+------------------+<br/>|  1|[10.0,10000.0,1.0]|<br/>|  2|[20.0,30000.0,2.0]|<br/>|  3|[30.0,40000.0,3.0]|<br/>+---+------------------+</span><span id="55a3" class="ol ni it oh b gy pa on l oo op"># Apply MinMaxScaler transformation<br/>features_scaler = MinMaxScaler(inputCol = "features", outputCol = "sfeatures")<br/>smodel = features_scaler.fit(features_df)<br/>sfeatures_df = smodel.transform(features_df)</span><span id="9d4c" class="ol ni it oh b gy pa on l oo op">sfeatures_df.show()</span><span id="23b0" class="ol ni it oh b gy pa on l oo op">+---+------------------+--------------------+<br/>| id|          features|           sfeatures|<br/>+---+------------------+--------------------+<br/>|  1|[10.0,10000.0,1.0]|           (3,[],[])|<br/>|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|<br/>|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|<br/>+---+------------------+--------------------+</span></pre><h2 id="626e" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">标准化数字数据</h2><p id="026d" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">StandardScaler 是另一个著名的用机器学习库编写的类。它将-1 和 1 之间的数据标准化，并将数据转换为钟形数据。你可以贬低数据和规模的一些差异。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="fd2a" class="ol ni it oh b gy om on l oo op">from pyspark.ml.feature import  StandardScaler<br/>from pyspark.ml.linalg import Vectors</span><span id="959d" class="ol ni it oh b gy pa on l oo op"># Create the dummy data<br/>features_df = spark.createDataFrame([<br/>    (1, Vectors.dense([10.0,10000.0,1.0]),),<br/>    (2, Vectors.dense([20.0,30000.0,2.0]),),<br/>    (3, Vectors.dense([30.0,40000.0,3.0]),),<br/>    <br/>],["id", "features"] )</span><span id="649c" class="ol ni it oh b gy pa on l oo op"># Apply the StandardScaler model<br/>features_stand_scaler = StandardScaler(inputCol = "features", outputCol = "sfeatures", withStd=True, withMean=True)<br/>stmodel = features_stand_scaler.fit(features_df)<br/>stand_sfeatures_df = stmodel.transform(features_df)</span><span id="27fb" class="ol ni it oh b gy pa on l oo op">stand_sfeatures_df.show()</span><span id="80af" class="ol ni it oh b gy pa on l oo op">+---+------------------+--------------------+<br/>| id|          features|           sfeatures|<br/>+---+------------------+--------------------+<br/>|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|<br/>|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|<br/>|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|<br/>+---+------------------+--------------------+</span></pre><h2 id="3002" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">存储数字数据</h2><p id="43c2" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">真实的数据集具有不同的范围，有时在插入机器学习算法之前，最好将数据转换为定义明确的桶。</p><p id="8c25" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Bucketizer 类可以方便地将数据转换成不同的存储桶。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="2570" class="ol ni it oh b gy om on l oo op">from pyspark.ml.feature import  Bucketizer<br/>from pyspark.ml.linalg import Vectors</span><span id="441b" class="ol ni it oh b gy pa on l oo op"># Define the splits for buckets<br/>splits = [-float("inf"), -10, 0.0, 10, float("inf")]<br/>b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]<br/>b_df = spark.createDataFrame(b_data, ["features"])</span><span id="f7ab" class="ol ni it oh b gy pa on l oo op">b_df.show()</span><span id="0c68" class="ol ni it oh b gy pa on l oo op">+--------+<br/>|features|<br/>+--------+<br/>|  -800.0|<br/>|   -10.5|<br/>|    -1.7|<br/>|     0.0|<br/>|     8.2|<br/>|    90.1|<br/>+--------+</span><span id="3961" class="ol ni it oh b gy pa on l oo op"># Transforming data into buckets<br/>bucketizer = Bucketizer(splits=splits, inputCol= "features", outputCol="bfeatures")<br/>bucketed_df = bucketizer.transform(b_df)</span><span id="a02e" class="ol ni it oh b gy pa on l oo op">bucketed_df.show()</span><span id="b279" class="ol ni it oh b gy pa on l oo op">+--------+---------+<br/>|features|bfeatures|<br/>+--------+---------+<br/>|  -800.0|      0.0|<br/>|   -10.5|      0.0|<br/>|    -1.7|      1.0|<br/>|     0.0|      2.0|<br/>|     8.2|      2.0|<br/>|    90.1|      3.0|<br/>+--------+---------+</span></pre><h2 id="9420" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">标记文本数据</h2><p id="92c0" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">自然语言处理是机器学习的主要应用之一。自然语言处理的第一步是将文本标记成单词或标记。我们可以利用 SparkML 中的 Tokenizer 类来执行这项任务。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="a6ca" class="ol ni it oh b gy om on l oo op">from pyspark.ml.feature import  Tokenizer</span><span id="1a56" class="ol ni it oh b gy pa on l oo op">sentences_df = spark.createDataFrame([<br/>    (1, "This is an introduction to sparkMlib"),<br/>    (2, "Mlib incluse libraries fro classfication and regression"),<br/>    (3, "It also incluses support for data piple lines"),<br/>    <br/>], ["id", "sentences"])</span><span id="0633" class="ol ni it oh b gy pa on l oo op">sentences_df.show()</span><span id="a009" class="ol ni it oh b gy pa on l oo op">+---+--------------------+<br/>| id|           sentences|<br/>+---+--------------------+<br/>|  1|This is an introd...|<br/>|  2|Mlib incluse libr...|<br/>|  3|It also incluses ...|<br/>+---+--------------------+</span><span id="bc31" class="ol ni it oh b gy pa on l oo op">sent_token = Tokenizer(inputCol = "sentences", outputCol = "words")<br/>sent_tokenized_df = sent_token.transform(sentences_df)</span><span id="2214" class="ol ni it oh b gy pa on l oo op">sent_tokenized_df.take(10)</span><span id="291e" class="ol ni it oh b gy pa on l oo op">[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib']),<br/> Row(id=2, sentences='Mlib incluse libraries fro classfication and regression', words=['mlib', 'incluse', 'libraries', 'fro', 'classfication', 'and', 'regression']),<br/> Row(id=3, sentences='It also incluses support for data piple lines', words=['it', 'also', 'incluses', 'support', 'for', 'data', 'piple', 'lines'])]</span></pre><h2 id="5b91" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">TF-IDF</h2><p id="ebd8" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">词频-逆文档频率(TF-IDF)是一种广泛应用于文本挖掘的特征矢量化方法，用于反映语料库中某个词对文档的重要性。使用上面标记化的数据，让我们应用 TF-IDF</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="bbb1" class="ol ni it oh b gy om on l oo op">from pyspark.ml.feature import HashingTF, IDF</span><span id="1714" class="ol ni it oh b gy pa on l oo op">hashingTF = HashingTF(inputCol = "words", outputCol = "rawfeatures", numFeatures = 20)<br/>sent_fhTF_df = hashingTF.transform(sent_tokenized_df)</span><span id="5653" class="ol ni it oh b gy pa on l oo op">sent_fhTF_df.take(1)</span><span id="a991" class="ol ni it oh b gy pa on l oo op">[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}))]</span><span id="6526" class="ol ni it oh b gy pa on l oo op">idf = IDF(inputCol = "rawfeatures", outputCol = "idffeatures")<br/>idfModel = idf.fit(sent_fhTF_df)<br/>tfidf_df = idfModel.transform(sent_fhTF_df)</span><span id="0d54" class="ol ni it oh b gy pa on l oo op">tfidf_df.take(1)</span><span id="d2de" class="ol ni it oh b gy pa on l oo op">[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}), idffeatures=SparseVector(20, {6: 0.5754, 8: 0.6931, 9: 0.0, 10: 0.6931, 13: 0.2877}))]</span></pre><blockquote class="na nb nc"><p id="7780" class="kr ks nd kt b ku kv kd kw kx ky kg kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated">用户可以根据手头问题的要求进行各种变换。</p></blockquote><h1 id="340c" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">使用 PySpark 进行聚类</h1><p id="c875" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">聚类是一种机器学习技术，其中使用输入特征将数据分组到合理数量的类中。在本节中，我们将使用 spark ML 框架研究集群技术的基本应用。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="116e" class="ol ni it oh b gy om on l oo op">from pyspark.ml.linalg import Vectors<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.clustering import KMeans, BisectingKMeans<br/>import glob</span><span id="d53f" class="ol ni it oh b gy pa on l oo op"># Downloading the clustering dataset<br/>!wget -q 'https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/clustering_dataset.csv'</span></pre><p id="108c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用 spark 加载以 csv 格式存储的聚类数据</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="a061" class="ol ni it oh b gy om on l oo op"># Read the data.<br/>clustering_file_name ='clustering_dataset.csv'<br/>import pandas as pd<br/># df = pd.read_csv(clustering_file_name)<br/>cluster_df = spark.read.csv(clustering_file_name, header=True,inferSchema=True)</span></pre><p id="92b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用<code class="fe pt pu pv oh b">VectorAssembler</code>将表格数据转换成矢量化格式</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="3c2a" class="ol ni it oh b gy om on l oo op"># Coverting the input data into features column<br/>vectorAssembler = VectorAssembler(inputCols = ['col1', 'col2', 'col3'], outputCol = "features")<br/>vcluster_df = vectorAssembler.transform(cluster_df)</span><span id="4bcc" class="ol ni it oh b gy pa on l oo op">vcluster_df.show(10)</span><span id="d4fb" class="ol ni it oh b gy pa on l oo op">+----+----+----+--------------+<br/>|col1|col2|col3|      features|<br/>+----+----+----+--------------+<br/>|   7|   4|   1| [7.0,4.0,1.0]|<br/>|   7|   7|   9| [7.0,7.0,9.0]|<br/>|   7|   9|   6| [7.0,9.0,6.0]|<br/>|   1|   6|   5| [1.0,6.0,5.0]|<br/>|   6|   7|   7| [6.0,7.0,7.0]|<br/>|   7|   9|   4| [7.0,9.0,4.0]|<br/>|   7|  10|   6|[7.0,10.0,6.0]|<br/>|   7|   8|   2| [7.0,8.0,2.0]|<br/>|   8|   3|   8| [8.0,3.0,8.0]|<br/>|   4|  10|   5|[4.0,10.0,5.0]|<br/>+----+----+----+--------------+<br/>only showing top 10 rows</span></pre><p id="89c3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦数据被准备成 MLlib 可以用于模型的格式，现在我们可以定义和训练聚类算法，例如 K-Means。我们可以定义集群的数量并初始化种子，如下所示。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="d5d3" class="ol ni it oh b gy om on l oo op"># Applying the k-means algorithm<br/>kmeans = KMeans().setK(3)<br/>kmeans = kmeans.setSeed(1)<br/>kmodel = kmeans.fit(vcluster_df)</span></pre><p id="cbf4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">培训结束后，让我们打印中心。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="a1e9" class="ol ni it oh b gy om on l oo op">centers = kmodel.clusterCenters()<br/>print("The location of centers: {}".format(centers))</span><span id="64fe" class="ol ni it oh b gy pa on l oo op">The location of centers: [array([35.88461538, 31.46153846, 34.42307692]), array([80.        , 79.20833333, 78.29166667]), array([5.12, 5.84, 4.84])]</span></pre><p id="0507" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">MLlib 中实现了各种聚类算法。对分 K-均值聚类是另一种流行的方法。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="3812" class="ol ni it oh b gy om on l oo op"># Applying Hierarchical Clustering<br/>bkmeans = BisectingKMeans().setK(3)<br/>bkmeans = bkmeans.setSeed(1)</span><span id="eae5" class="ol ni it oh b gy pa on l oo op">bkmodel = bkmeans.fit(vcluster_df)<br/>bkcneters = bkmodel.clusterCenters()</span><span id="68ad" class="ol ni it oh b gy pa on l oo op">bkcneters</span><span id="bbec" class="ol ni it oh b gy pa on l oo op">[array([5.12, 5.84, 4.84]),<br/> array([35.88461538, 31.46153846, 34.42307692]),<br/> array([80.        , 79.20833333, 78.29166667])]</span></pre><p id="3e22" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">要阅读更多关于在 MLlib 中实现的集群方法，请点击下面的链接。</p><div class="pb pc gp gr pd pe"><a href="https://spark.apache.org/docs/3.0.1/ml-clustering.html" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd jd gy z fp pj fr fs pk fu fw jc bi translated">使聚集</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">本页描述 MLlib 中的群集演算法。基于 RDD 的 API 中的集群指南也有相关的…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">spark.apache.org</p></div></div><div class="pn l"><div class="pw l pp pq pr pn ps mg pe"/></div></div></a></div><h1 id="fcef" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">使用 PySpark 分类</h1><p id="2e97" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">分类是广泛使用的机器算法之一，几乎每个数据工程师和数据科学家都必须了解这些算法。一旦加载并准备好数据，我将演示三种分类算法。</p><ol class=""><li id="1032" class="mm mn it kt b ku kv kx ky la mo le mp li mq lm px ms mt mu bi translated">朴素贝叶斯分类</li><li id="157b" class="mm mn it kt b ku mv kx mw la mx le my li mz lm px ms mt mu bi translated">多层感知器分类</li><li id="03b0" class="mm mn it kt b ku mv kx mw la mx le my li mz lm px ms mt mu bi translated">决策树分类</li></ol><p id="811c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们使用<a class="ae oe" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">虹膜数据</a>探索监督分类算法。我已经把数据上传到我的 GitHub 来重现结果。用户可以使用下面的命令下载数据。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="7aee" class="ol ni it oh b gy om on l oo op"># Downloading the clustering data<br/>!wget -q "https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv"</span><span id="fe17" class="ol ni it oh b gy pa on l oo op">df = pd.read_csv("https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv", header=None)</span><span id="8f87" class="ol ni it oh b gy pa on l oo op">df.head()</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="ab gu cl py"><img src="../Images/e8a3dcaed75267031f42f6f68c9c634b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*eBq-77Z2wBZKSaTfVlmjUg.png"/></div></figure><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="8fcd" class="ol ni it oh b gy om on l oo op">spark.createDataFrame(df, columns)</span><span id="3f60" class="ol ni it oh b gy pa on l oo op">DataFrame[c_0: double, c_1: double, c_2: double, c_3: double, c4 : string]</span></pre><h2 id="fddc" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">预处理虹膜数据</h2><p id="9742" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">在本节中，我们将使用 IRIS 数据来理解分类。为了执行 ML 模型，我们对输入数据应用预处理步骤。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="6683" class="ol ni it oh b gy om on l oo op">from pyspark.sql.functions import *<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.feature import  StringIndexer</span><span id="d11f" class="ol ni it oh b gy pa on l oo op"># Read the iris data<br/>df_iris = pd.read_csv("https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv", header=None)<br/>iris_df = spark.createDataFrame(df_iris)</span><span id="8be2" class="ol ni it oh b gy pa on l oo op">iris_df.show(5, False)</span><span id="34b3" class="ol ni it oh b gy pa on l oo op">+------------+-----------+------------+-----------+-----------+<br/>|sepal_length|sepal_width|petal_length|petal_width|species    |<br/>+------------+-----------+------------+-----------+-----------+<br/>|5.1         |3.5        |1.4         |0.2        |Iris-setosa|<br/>|4.9         |3.0        |1.4         |0.2        |Iris-setosa|<br/>|4.7         |3.2        |1.3         |0.2        |Iris-setosa|<br/>|4.6         |3.1        |1.5         |0.2        |Iris-setosa|<br/>|5.0         |3.6        |1.4         |0.2        |Iris-setosa|<br/>+------------+-----------+------------+-----------+-----------+<br/>only showing top 5 rows</span><span id="86bc" class="ol ni it oh b gy pa on l oo op"># Rename the columns<br/>iris_df = iris_df.select(col("0").alias("sepal_length"),<br/>                         col("1").alias("sepal_width"),<br/>                         col("2").alias("petal_length"),<br/>                         col("3").alias("petal_width"),<br/>                         col("4").alias("species"),<br/>                        )</span><span id="4e37" class="ol ni it oh b gy pa on l oo op"># Converting the columns into features<br/>vectorAssembler = VectorAssembler(inputCols = ["sepal_length", "sepal_width", "petal_length", "petal_width"],<br/>                                  outputCol = "features")<br/>viris_df = vectorAssembler.transform(iris_df)</span><span id="64db" class="ol ni it oh b gy pa on l oo op">viris_df.show(5, False)</span><span id="5946" class="ol ni it oh b gy pa on l oo op">+------------+-----------+------------+-----------+-----------+-----------------+<br/>|sepal_length|sepal_width|petal_length|petal_width|species    |features         |<br/>+------------+-----------+------------+-----------+-----------+-----------------+<br/>|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|<br/>|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|<br/>|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|<br/>|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|<br/>|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|<br/>+------------+-----------+------------+-----------+-----------+-----------------+<br/>only showing top 5 rows</span><span id="5401" class="ol ni it oh b gy pa on l oo op">indexer = StringIndexer(inputCol="species", outputCol = "label")<br/>iviris_df = indexer.fit(viris_df).transform(viris_df)</span><span id="3a30" class="ol ni it oh b gy pa on l oo op">iviris_df.show(2, False)</span><span id="4a4e" class="ol ni it oh b gy pa on l oo op">+------------+-----------+------------+-----------+-----------+-----------------+-----+<br/>|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|<br/>+------------+-----------+------------+-----------+-----------+-----------------+-----+<br/>|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|0.0  |<br/>|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|0.0  |<br/>+------------+-----------+------------+-----------+-----------+-----------------+-----+<br/>only showing top 2 rows</span></pre><h2 id="ab5e" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">朴素贝叶斯分类</h2><p id="548b" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">一旦数据准备好了，我们就可以应用第一个分类算法了。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="4832" class="ol ni it oh b gy om on l oo op">from pyspark.ml.classification import NaiveBayes<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator</span><span id="8b18" class="ol ni it oh b gy pa on l oo op"># Create the traing and test splits<br/>splits = iviris_df.randomSplit([0.6,0.4], 1)<br/>train_df = splits[0]<br/>test_df = splits[1]</span><span id="46c7" class="ol ni it oh b gy pa on l oo op"># Apply the Naive bayes classifier<br/>nb = NaiveBayes(modelType="multinomial")<br/>nbmodel = nb.fit(train_df)<br/>predictions_df = nbmodel.transform(test_df)</span><span id="23d2" class="ol ni it oh b gy pa on l oo op">predictions_df.show(1, False)</span><span id="142b" class="ol ni it oh b gy pa on l oo op">+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+<br/>|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|rawPrediction                                               |probability                                                 |prediction|<br/>+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+<br/>|4.3         |3.0        |1.1         |0.1        |Iris-setosa|[4.3,3.0,1.1,0.1]|0.0  |[-9.966434726497221,-11.294595492758821,-11.956012812323921]|[0.7134106367667451,0.18902823898426235,0.09756112424899269]|0.0       |<br/>+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+<br/>only showing top 1 row</span></pre><p id="1d25" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们评估训练好的分类器</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="30a0" class="ol ni it oh b gy om on l oo op">evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")<br/>nbaccuracy = evaluator.evaluate(predictions_df)<br/>nbaccuracy</span><span id="865d" class="ol ni it oh b gy pa on l oo op">0.8275862068965517</span></pre><h2 id="b836" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">多层感知器分类</h2><p id="fbb4" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">我们将研究的第二个分类器是一个多层感知器。在本教程中，我不打算详细介绍这个问题的最优 MLP 网络，但是在实践中，你可以研究适合手头问题的最优网络。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="936c" class="ol ni it oh b gy om on l oo op">from pyspark.ml.classification import MultilayerPerceptronClassifier</span><span id="7993" class="ol ni it oh b gy pa on l oo op"># Define the MLP Classifier<br/>layers = [4,5,5,3]<br/>mlp = MultilayerPerceptronClassifier(layers = layers, seed=1)<br/>mlp_model = mlp.fit(train_df)<br/>mlp_predictions = mlp_model.transform(test_df)</span><span id="6c17" class="ol ni it oh b gy pa on l oo op"># Evaluate the MLP classifier<br/>mlp_evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")<br/>mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)<br/>mlp_accuracy</span><span id="ac34" class="ol ni it oh b gy pa on l oo op">0.9827586206896551</span></pre><h2 id="edfe" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">决策树分类</h2><p id="4e96" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">ML 家族中另一个常见的分类器是决策树分类器，在本节中，我们将探讨这个分类器。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="005b" class="ol ni it oh b gy om on l oo op">from pyspark.ml.classification import DecisionTreeClassifier</span><span id="7130" class="ol ni it oh b gy pa on l oo op"># Define the DT Classifier <br/>dt = DecisionTreeClassifier(labelCol="label", featuresCol="features")<br/>dt_model = dt.fit(train_df)<br/>dt_predictions = dt_model.transform(test_df)</span><span id="4673" class="ol ni it oh b gy pa on l oo op"># Evaluate the DT Classifier<br/>dt_evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")<br/>dt_accuracy = dt_evaluator.evaluate(dt_predictions)<br/>dt_accuracy</span><span id="2367" class="ol ni it oh b gy pa on l oo op">0.9827586206896551</span></pre><p id="69c5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">除了上面三个演示的分类算法，Spark MLlib 还有许多其他分类算法的实现。实现的分类算法的细节可以在下面的链接中找到</p><div class="pb pc gp gr pd pe"><a href="https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd jd gy z fp pj fr fs pk fu fw jc bi translated">分类和回归</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">\newcommand{\R}</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">spark.apache.org</p></div></div><div class="pn l"><div class="pz l pp pq pr pn ps mg pe"/></div></div></a></div><p id="0ecf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">强烈建议尝试一些分类算法，以便动手操作。</p><h1 id="d5c9" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">使用 PySpark 进行回归</h1><p id="d21f" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">在本节中，我们将使用 pyspark 探索回归问题的机器学习模型。回归模型有助于使用过去的数据预测未来的值。</p><p id="8354" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将使用<a class="ae oe" href="https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant" rel="noopener ugc nofollow" target="_blank">联合循环发电厂</a>数据集来预测每小时净电力输出(EP)。我已经把数据上传到我的 GitHub 上，这样用户就可以重现结果了。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="9ae8" class="ol ni it oh b gy om on l oo op">from pyspark.ml.regression import LinearRegression<br/>from pyspark.ml.feature import VectorAssembler</span><span id="bcdf" class="ol ni it oh b gy pa on l oo op"># Read the iris data<br/>df_ccpp = pd.read_csv("https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/ccpp.csv")<br/>pp_df = spark.createDataFrame(df_ccpp)</span><span id="b90d" class="ol ni it oh b gy pa on l oo op">pp_df.show(2, False)</span><span id="82f4" class="ol ni it oh b gy pa on l oo op">+-----+-----+-------+-----+------+<br/>|AT   |V    |AP     |RH   |PE    |<br/>+-----+-----+-------+-----+------+<br/>|14.96|41.76|1024.07|73.17|463.26|<br/>|25.18|62.96|1020.04|59.08|444.37|<br/>+-----+-----+-------+-----+------+<br/>only showing top 2 rows</span><span id="07ba" class="ol ni it oh b gy pa on l oo op"># Create the feature column using VectorAssembler class<br/>vectorAssembler = VectorAssembler(inputCols =["AT", "V", "AP", "RH"], outputCol = "features")<br/>vpp_df = vectorAssembler.transform(pp_df)</span><span id="1b78" class="ol ni it oh b gy pa on l oo op">vpp_df.show(2, False)</span><span id="7d58" class="ol ni it oh b gy pa on l oo op">+-----+-----+-------+-----+------+---------------------------+<br/>|AT   |V    |AP     |RH   |PE    |features                   |<br/>+-----+-----+-------+-----+------+---------------------------+<br/>|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|<br/>|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|<br/>+-----+-----+-------+-----+------+---------------------------+<br/>only showing top 2 rows</span></pre><h2 id="81f5" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">线性回归</h2><p id="d57b" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">我们从最简单的回归技术开始，即线性回归。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="08c8" class="ol ni it oh b gy om on l oo op"># Define and fit Linear Regression<br/>lr = LinearRegression(featuresCol="features", labelCol="PE")<br/>lr_model = lr.fit(vpp_df)</span><span id="1559" class="ol ni it oh b gy pa on l oo op"># Print and save the Model output<br/>lr_model.coefficients<br/>lr_model.intercept<br/>lr_model.summary.rootMeanSquaredError</span><span id="a177" class="ol ni it oh b gy pa on l oo op">4.557126016749486</span><span id="caa8" class="ol ni it oh b gy pa on l oo op">#lr_model.save()</span></pre><h2 id="2d1e" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">决策树回归</h2><p id="8628" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">在本节中，我们将探讨机器学习中常用的决策树回归。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="c312" class="ol ni it oh b gy om on l oo op">from pyspark.ml.regression import DecisionTreeRegressor<br/>from pyspark.ml.evaluation import RegressionEvaluator</span><span id="a41b" class="ol ni it oh b gy pa on l oo op">vpp_df.show(2, False)</span><span id="1cbc" class="ol ni it oh b gy pa on l oo op">+-----+-----+-------+-----+------+---------------------------+<br/>|AT   |V    |AP     |RH   |PE    |features                   |<br/>+-----+-----+-------+-----+------+---------------------------+<br/>|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|<br/>|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|<br/>+-----+-----+-------+-----+------+---------------------------+<br/>only showing top 2 rows</span><span id="5e62" class="ol ni it oh b gy pa on l oo op"># Define train and test data split<br/>splits = vpp_df.randomSplit([0.7,0.3])<br/>train_df = splits[0]<br/>test_df = splits[1]</span><span id="3fd2" class="ol ni it oh b gy pa on l oo op"># Define the Decision Tree Model <br/>dt = DecisionTreeRegressor(featuresCol="features", labelCol="PE")<br/>dt_model = dt.fit(train_df)<br/>dt_predictions = dt_model.transform(test_df)</span><span id="50a6" class="ol ni it oh b gy pa on l oo op">dt_predictions.show(1, False)</span><span id="f7b9" class="ol ni it oh b gy pa on l oo op">+----+-----+-------+-----+------+--------------------------+-----------------+<br/>|AT  |V    |AP     |RH   |PE    |features                  |prediction       |<br/>+----+-----+-------+-----+------+--------------------------+-----------------+<br/>|3.31|39.42|1024.05|84.31|487.19|[3.31,39.42,1024.05,84.31]|486.1117703349283|<br/>+----+-----+-------+-----+------+--------------------------+-----------------+<br/>only showing top 1 row</span><span id="caca" class="ol ni it oh b gy pa on l oo op"># Evaluate the Model<br/>dt_evaluator = RegressionEvaluator(labelCol="PE", predictionCol="prediction", metricName="rmse")<br/>dt_rmse = dt_evaluator.evaluate(dt_predictions)<br/>print("The RMSE of Decision Tree regression Model is {}".format(dt_rmse))</span><span id="cc8e" class="ol ni it oh b gy pa on l oo op">The RMSE of Decision Tree regression Model is 4.451790078736588</span></pre><h2 id="074f" class="ol ni it bd nj oq or dn nn os ot dp nr la ou ov nt le ow ox nv li oy oz nx iz bi translated">梯度推进决策树回归</h2><p id="fe8c" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">梯度推进是 ML 专业人士的另一个普遍选择。让我们在本节中尝试一下 GBM。</p><pre class="lx ly lz ma gt og oh oi oj aw ok bi"><span id="c5ed" class="ol ni it oh b gy om on l oo op">from pyspark.ml.regression import GBTRegressor</span><span id="4061" class="ol ni it oh b gy pa on l oo op"># Define the GBT Model<br/>gbt = GBTRegressor(featuresCol="features", labelCol="PE")<br/>gbt_model = gbt.fit(train_df)<br/>gbt_predictions = gbt_model.transform(test_df)</span><span id="1074" class="ol ni it oh b gy pa on l oo op"># Evaluate the GBT Model<br/>gbt_evaluator = RegressionEvaluator(labelCol="PE", predictionCol="prediction", metricName="rmse")<br/>gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)<br/>print("The RMSE of GBT Tree regression Model is {}".format(gbt_rmse))</span><span id="e92c" class="ol ni it oh b gy pa on l oo op">The RMSE of GBT Tree regression Model is 4.035802933864555</span></pre><p id="ab34" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">除了上面演示的回归算法，Spark MLlib 还有许多其他回归算法的实现。实现回归算法的细节可以在下面的链接中找到。</p><div class="pb pc gp gr pd pe"><a href="https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#regression" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd jd gy z fp pj fr fs pk fu fw jc bi translated">分类和回归</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">\newcommand{\R}</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">spark.apache.org</p></div></div><div class="pn l"><div class="qa l pp pq pr pn ps mg pe"/></div></div></a></div><p id="e002" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">强烈建议您尝试一些回归算法，亲自动手操作并使用这些参数。</p><h1 id="4c8b" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">一个有效的 Google Colab</h1><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="qb qc l"/></div></figure><h1 id="01be" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">结论</h1><p id="ce5c" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">在本教程中，我试图让读者有机会学习和使用 PySpark 实现基本的机器学习算法。Spark 不仅提供了分布式处理的好处，还可以处理大量的待处理数据。总之，我们已经讨论了以下主题/算法</p><ul class=""><li id="13eb" class="mm mn it kt b ku kv kx ky la mo le mp li mq lm mr ms mt mu bi translated">在 Google Colab 中设置 Spark 3.0.1</li><li id="ca32" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 PySpark 进行数据转换概述</li><li id="c32e" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 PySpark 的聚类算法</li><li id="36c8" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 PySpark 的分类问题</li><li id="d79d" class="mm mn it kt b ku mv kx mw la mx le my li mz lm mr ms mt mu bi translated">使用 PySpark 的回归问题</li></ul><h1 id="f01f" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">参考资料/阅读/链接</h1><ol class=""><li id="7924" class="mm mn it kt b ku nz kx oa la qd le qe li qf lm px ms mt mu bi translated">https://spark.apache.org/docs/latest/ml-features.html<a class="ae oe" href="https://spark.apache.org/docs/latest/ml-features.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="6bec" class="mm mn it kt b ku mv kx mw la mx le my li mz lm px ms mt mu bi translated"><a class="ae oe" href="https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#regression" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/3 . 0 . 1/ml-class ification-regression . html # regression</a></li><li id="26dc" class="mm mn it kt b ku mv kx mw la mx le my li mz lm px ms mt mu bi translated"><a class="ae oe" href="https://spark.apache.org/docs/3.0.1/ml-clustering.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/3.0.1/ml-clustering.html</a></li><li id="80de" class="mm mn it kt b ku mv kx mw la mx le my li mz lm px ms mt mu bi translated"><a class="ae oe" href="https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/3 . 0 . 1/ml-class ification-regression . html # class ification</a></li></ol></div></div>    
</body>
</html>