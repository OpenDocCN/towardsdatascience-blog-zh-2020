<html>
<head>
<title>Sequence-to-Sequence Models: Encoder-Decoder using Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列到序列模型:使用 Tensorflow 2 的编码器-解码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639?source=collection_archive---------17-----------------------#2020-09-07">https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639?source=collection_archive---------17-----------------------#2020-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="95d5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第 1 部分:序列到序列模型:从 RNN 到变压器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/740273e24a5aa7c0cc871c306c7d015d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Y3jXYVU93s8CaznAd_vEQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1661731" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/MonikaP-2515080/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1661731" rel="noopener ugc nofollow" target="_blank">莫尼卡普</a>的图片</p></figure><p id="1a4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">序列到序列模型是对序列数据进行操作的基本深度学习技术。它将一个结构域的序列转换成另一个结构域的序列[1]。这些模型可以是基于 RNN 的简单编码器-解码器网络或高级的基于注意力的编码器-解码器 RNN 或最先进的变压器模型。序列到序列模型有许多应用，例如机器翻译、语音识别、文本摘要、问题回答、需求预测等等。</p><p id="1897" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文是关于序列到序列模型的三篇文章中的第 1 篇，在这篇文章中，我们将重点构建一个机器翻译系统。在这一部分中，我们将关注基于 RNN 的编码器-解码器网络的内部工作。为了举例说明，我们将构建一个西班牙语到英语的翻译模型。</p><p id="c524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的重点是模型架构、训练和使用 Tensorflow 2.0 的推理过程。因此，我们将省略关于数据准备的讨论。作为参考，数据准备部分可以按照 Tensorflow 教程[2]进行。如果您是 Tensorflow 2.0 的新手，您可能需要特别关注<code class="fe lv lw lx ly b">create a tf.data dataset</code>部分[2]</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="822f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你觉得这篇文章很有趣，请随时联系 LinkedIn。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="ab15" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">编码器-解码器模型</h1><p id="f673" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">直观上，编码器对输入进行编码，解码器将其解码到所需的域。在我们的例子中，编码器将对输入的西班牙语句子进行编码，解码器将它们解码成英语。在基于递归神经网络(RNN)的架构中，编码器和解码器都是 RNN 或其变体之一，如 LSTM 或 GRU。在本文中，我们将使用 GRU 单元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c90f3317618177a3b4686f0b79f30c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*ygsUF7WwXCrGO1AIjYCjCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:机器翻译的编码器-解码器模型</p></figure><h2 id="3d68" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">编码器</h2><ul class=""><li id="0669" class="nq nr it lb b lc my lf mz li ns lm nt lq nu lu nv nw nx ny bi translated">编码器的输入是英语单词。在每个时间步长，一个英语单词作为输入与前一个时间步长(称为隐藏状态)的 RNN 的输出一起传递给 RNN。一开始，这个隐藏状态被初始化为零。</li><li id="2f31" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">编码器通过这个 RNN 网络对输入数据进行编码</li></ul><h2 id="0fb6" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">解码器</h2><ul class=""><li id="3a0a" class="nq nr it lb b lc my lf mz li ns lm nt lq nu lu nv nw nx ny bi translated">在训练阶段，解码器的输入是移动一步的西班牙语单词。也就是说，给解码器一个它<code class="fe lv lw lx ly b">should have</code>预测的输入字，而不管它实际预测的是什么。</li><li id="84aa" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">对于第一个时间步长，解码器被赋予<code class="fe lv lw lx ly b">start-of-sequence (SOS)</code>。解码器应该以一个<code class="fe lv lw lx ly b">end-of-sequence (EOS)</code>标记结束句子。</li></ul><h2 id="0111" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">推理过程:</h2><p id="9bee" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">序列对序列模型中的一个关键概念是理解训练和推理模型的不同之处。它是关于解码器的。我们在推断时间和训练时间如何将输入提供给解码器是不同的。你大概可以直观的理解，在推理时间里，我们不知道目标翻译的单词。因此解码器将被馈送前一时间步的输出。当然，对于第一个时间步长，解码器被赋予<code class="fe lv lw lx ly b">start-of-sequence (SOS)</code>。</p><p id="c9fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们深入研究一下实现。</p><h2 id="9be1" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">编码器型号</h2><p id="b10e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">我们将使用 Tensorflow 2 构建一个<code class="fe lv lw lx ly b">Encoder</code>类。首先，确保您导入了必要的库</p><p id="37ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">import tensorflow as tf</code></p><p id="6774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">Encoder</code>和<code class="fe lv lw lx ly b">Decoder</code>类都将继承自<code class="fe lv lw lx ly b">tf.keras.Model</code>。至少，这些类将有两个方法——一个初始化器<code class="fe lv lw lx ly b">__init__</code>方法和一个<code class="fe lv lw lx ly b">call</code>方法。<code class="fe lv lw lx ly b">call</code>方法在网络的正向传递过程中执行。如果您熟悉 Pytorch，那么您很可能熟悉这种定义模型的风格。</p><p id="aa79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe lv lw lx ly b">Encoder</code>类中，我们还将定义一个初始化隐藏状态的方法。</p><p id="5e82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe lv lw lx ly b">__init__</code>方法中，我们需要定义层——例如<code class="fe lv lw lx ly b">Embedding</code>层、<code class="fe lv lw lx ly b">GRU</code>层或完全连接的<code class="fe lv lw lx ly b">Dense</code>层。我们还需要初始化定义这些层所需的变量。具体来说，我们需要定义</p><ul class=""><li id="d86a" class="nq nr it lb b lc ld lf lg li oe lm of lq og lu nv nw nx ny bi translated"><code class="fe lv lw lx ly b">vocab_size</code>:训练数据中的唯一单词数</li><li id="defb" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe lv lw lx ly b">embedding_dim</code>:您希望嵌入的尺寸。一般来说，越高越好，但这也带来了计算和内存成本。</li><li id="5e4e" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe lv lw lx ly b">enc_units</code>:GRU 单位的数量。例如，在<code class="fe lv lw lx ly b">figure-1</code>中，我们有<code class="fe lv lw lx ly b">n</code>个编码器单元</li><li id="88d5" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe lv lw lx ly b">batch_size</code>:你希望你的模型在每个时期训练的数据量。</li></ul><p id="9a52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe lv lw lx ly b">call</code>方法中，你想做你认为你的模型在网络的前向传播中应该做的操作。在编码器的情况下，我们需要得到嵌入的输入字，并通过 GRU 层传递它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="b4f9" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">解码器模型</h2><p id="fe92" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated"><code class="fe lv lw lx ly b">Decoder</code>类与<code class="fe lv lw lx ly b">Encoder</code>类非常相似。除此之外，您需要通过完全连接的<code class="fe lv lw lx ly b">Dense</code>层传递 GRU 单元的输出，以便从网络中获得预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="9553" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">训练编码器-解码器网络</h2><p id="7db2" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">首先，我们将定义网络的优化器和损失函数。我们将使用 Adam 优化器。既然是分类问题，我们就用<code class="fe lv lw lx ly b">CrossEntropy</code>损失作为损失函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="8e33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练过程中—</p><ul class=""><li id="937a" class="nq nr it lb b lc ld lf lg li oe lm of lq og lu nv nw nx ny bi translated">我们通过返回<code class="fe lv lw lx ly b">encoder output</code>和<code class="fe lv lw lx ly b">encoder hidden state</code>的编码器输入。</li><li id="5884" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">然后将<code class="fe lv lw lx ly b">encoder output</code>、<code class="fe lv lw lx ly b">encoder hidden state</code>和<code class="fe lv lw lx ly b">decoder input</code>传递给解码器。<code class="fe lv lw lx ly b">decoder input</code>以<code class="fe lv lw lx ly b">&lt;SOS&gt;</code>(句首)标记开始。</li><li id="1da6" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">解码器输出<code class="fe lv lw lx ly b">prediction</code>和<code class="fe lv lw lx ly b">decoder hidden state</code>。</li><li id="b169" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe lv lw lx ly b">prediction</code>用于计算损失</li><li id="6842" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">我们根据当前时间步的输入为下一个时间步创建<code class="fe lv lw lx ly b">decoder input</code>。这个迫使解码器学习目标输出的过程称为<code class="fe lv lw lx ly b">teacher forcing</code>。</li><li id="ed13" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">当前时间步的<code class="fe lv lw lx ly b">decoder hidden state</code>被送至下一个时间步。</li><li id="c78f" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">接下来，我们将计算梯度。随着 Tensorflow 进入急切执行模式[6]，我们将使用<code class="fe lv lw lx ly b">tf.GradientTape</code>来跟踪计算梯度的操作。梯度计算相对于模型的可训练参数发生。因此，在下面的行<code class="fe lv lw lx ly b">19</code>中，你会发现我们正在总结编码器和解码器的可训练变量。</li><li id="fecf" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">当在<code class="fe lv lw lx ly b">tf.GradientTape</code>的上下文中执行操作时，它们被记录。默认情况下会记录可训练参数[7]。如果你想记录张量，你必须通过在<code class="fe lv lw lx ly b">tf.GradientTape</code>的上下文中调用<code class="fe lv lw lx ly b">watch</code>方法来手动完成。</li><li id="bb6a" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">最后，我们将对优化器应用梯度，优化器将更新模型参数——aka。反向传播。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="0f44" class="ne mh it bd mi nf ng dn mm nh ni dp mq li nj nk ms lm nl nm mu lq nn no mw np bi translated">推理</h2><p id="b281" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">您可能已经注意到，在培训时，我们保存了模型检查点。保存检查点是保存模型的一种方式。或者，如果您有一个<code class="fe lv lw lx ly b">keras.Model</code>对象，那么您可以使用<code class="fe lv lw lx ly b">saved_model.save</code>来保存模型[8]。要运行推理，您需要重新加载检查点[9]。</p><p id="9311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，推理过程与训练过程非常相似，只是我们如何馈送给解码器。这里，解码器的输入是解码器在前一时间步的输出——无论它预测了什么，而不是目标。如果你理解训练过程，代码是简单的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="9d7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你已经理解到这一点，可以写一个基于 RNN 的编码器-解码器，可以做训练和编码推理方法-祝贺你！</p><p id="46b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即将推出！第 2 部分—关注基于 RNN 的编码器-解码器网络</p><h1 id="b978" class="mg mh it bd mi mj oj ml mm mn ok mp mq jz ol ka ms kc om kd mu kf on kg mw mx bi translated">参考:</h1><ol class=""><li id="2d54" class="nq nr it lb b lc my lf mz li ns lm nt lq nu lu oo nw nx ny bi translated">十分钟序列对序列学习介绍<a class="ae ky" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></li><li id="e221" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">关于数据处理/清理的一切<a class="ae ky" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></li><li id="0419" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">原序对序纸<a class="ae ky" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1409.3215.pdf</a></li><li id="feef" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated"><a class="ae ky" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/</a></li><li id="02a9" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated"><a class="ae ky" href="https://youtu.be/QuELiw8tbx8?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;t=1190" rel="noopener ugc nofollow" target="_blank">理查德·索契的机器翻译讲座</a></li><li id="0bdf" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">张量流急切执行<a class="ae ky" href="https://www.tensorflow.org/guide/eager" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/eager</a></li><li id="e306" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">张量流梯度带<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a></li><li id="cad5" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">https://www.tensorflow.org/guide/saved_model</li><li id="132d" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu oo nw nx ny bi translated">张量流检查点<a class="ae ky" href="https://www.tensorflow.org/guide/checkpoint" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/checkpoint</a></li></ol></div></div>    
</body>
</html>