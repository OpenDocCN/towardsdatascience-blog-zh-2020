<html>
<head>
<title>Understanding Latent Space in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的潜在空间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d?source=collection_archive---------0-----------------------#2020-02-04">https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d?source=collection_archive---------0-----------------------#2020-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="746b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习深度学习的一个基本的，但经常是“隐藏的”概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cd84f30d0142dcf4e169d52ce7e300d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7A0qNVuOjgGh6pj4WUq5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Hackernoon，<a class="ae ky" href="https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/latent-space-visualization-deep-learning-bits-2-bd09a 46920 df</a></p></figure><h1 id="6a39" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是潜在空间？</h1><blockquote class="lr"><p id="0c14" class="ls lt it bd lu lv lw lx ly lz ma mb dk translated">如果我必须用一句话来描述潜在空间，它仅仅意味着压缩数据的表示。</p></blockquote><p id="f686" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mb im bi translated">想象一个大型的手写数字(0–9)数据集，如上图所示。与其他不同数字的图像(即 3 对 7)相比，相同数字的手写图像(即 3 的图像)彼此最相似。但是我们能训练一个算法来识别这些相似之处吗？<em class="mx">如何？</em></p><p id="6029" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">如果你训练一个模型<em class="mx">对数字</em>进行分类，那么<em class="mx">也</em>训练这个模型学习图像之间的“结构相似性”。事实上，这就是模型首先能够对数字进行分类的方式——通过学习每个数字的特征。</p><p id="42b4" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">如果看起来这个过程对你是“隐藏的”,那是因为它确实是。潜伏，顾名思义，就是“隐藏”的意思</p><p id="a59c" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">“潜在空间”的概念是<em class="mx">重要的</em>，因为它的效用是“深度学习”的核心——<em class="mx">学习数据的特征并简化数据表示，以找到模式。</em></p><p id="5da0" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">好奇吗？让我们一点一点地打破潜在空间:</p><p id="3804" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu"> <em class="mx">我们为什么要用 ML 压缩数据？</em>T15】</strong></p><p id="7337" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu">数据压缩</strong>定义为使用比原始表示更少的比特对信息进行编码的过程。这就像取一个 19D 的数据点(需要 19 个值来定义唯一的点)并将所有信息压缩到一个 9D 的数据点中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e937b1c82252a062f43ec385282c1db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/0*zG3k_ciZomNRaO-K.jpg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">压缩的图解。来源:浮士德 2013</p></figure><p id="b567" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">通常情况下，数据在机器学习中被压缩，以<em class="mx">学习关于数据点的重要信息</em>。我举个例子解释一下。</p><p id="d873" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">假设我们想要训练一个模型来使用完全卷积神经网络(FCN)对图像进行分类。(即输出给定图像的数字位数)。当模型“学习”时，它只是学习每一层(边缘、角度等)的<em class="mx">特征</em>。)并将特征的组合归因于特定的输出。</p><p id="9c40" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">但是每次模型通过数据点学习时，图像的<em class="mx">维度</em>在最终增加之前首先<em class="mx">减少</em>。(参见下面的编码器和瓶颈)。当维数减少时，我们认为这是一种有损压缩。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1df334bcd02c8871418cb956019c584b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kHJ_LsPi-jz_CreZ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卷积神经网络的描述。来源:来源:Hackernoon 潜在空间可视化。</p></figure><p id="ee7e" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">因为要求模型然后<em class="mx">重建</em>压缩数据(见解码器)，它必须学会存储<em class="mx">所有相关信息</em>并忽略噪声。这就是压缩的价值——它允许我们去掉任何无关的信息，只关注最重要的特征。</p><p id="3331" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu">这种“压缩状态”是我们数据的潜在空间表示。</strong></p><p id="a648" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu"> <em class="mx">我说的空间是什么意思？</em>T9】</strong></p><p id="0c29" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">你可能想知道为什么我们称它为潜在空间。毕竟，乍一看，压缩数据可能不会引起任何形式的“空间”</p><p id="33c8" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">但这里有个相似之处。</p><p id="38c8" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">在这个相当简单的例子中，假设我们的原始数据集是尺寸为 5 x 5 x 1 的图像。我们将我们的潜在空间维度设置为 3×1，这意味着我们的压缩数据点是一个三维向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f6decb8dda13614e13d7b1c03ee37d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/0*rZn-ksyRTUpDUxh7"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例 5x5x1 数据</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/376bec266d6a802fd3e212a162ad3948.png" data-original-src="https://miro.medium.com/v2/resize:fit:74/0*GRY4C_Ov5RYMn6Vi"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“潜在空间”中的压缩 3x1 数据示例</p></figure><p id="a186" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">现在，每个压缩数据点仅由 3 个数字唯一定义。这意味着我们可以在 3D 平面上绘制这些数据(一个数字是 x，另一个是 y，另一个是 z)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/259281f6ecab3ae166456554ab1c3b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djUvYgePXdjMbdbGsDq2eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在 3D 空间中绘制的点(0.4，0.3，0.8)</p></figure><p id="c5fb" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">这就是我们所说的“空间”。</p><blockquote class="nh ni nj"><p id="e837" class="mc md mx me b mf my ju mh mi mz jx mk nk na mn mo nl nb mr ms nm nc mv mw mb im bi translated">每当我们在潜在空间中绘制点或思考点时，我们可以将它们想象为空间中的坐标，在该坐标中，<em class="it">【相似】</em>的<strong class="me iu">点在图上靠得更近。</strong></p></blockquote><p id="39c7" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">一个自然出现的问题是，我们如何想象 4D 点或 n 维点的空间，或者甚至是非向量的空间(因为潜在空间表示不需要是 2 维或 3 维向量，并且经常不是，因为太多的信息将丢失)。</p><p id="a520" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">不满意的答案是，<em class="mx">我们不能</em>。我们是三维生物，无法理解 n 维空间(比如 n &gt; 3)。然而，有一些工具，比如 t-SNE，可以将我们更高维度的潜在空间表征转化为我们能够可视化的表征(2D 或 3D)。(参见下面的<strong class="me iu">可视化潜在空间</strong>部分。)</p><p id="97d5" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">但是你可能想知道，什么是<em class="mx"/>“相似”图像，为什么减少我们数据的维度会使相似的图像在空间上“更接近”？</p><p id="33f3" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu"> <em class="mx">我说的相似是什么意思？</em> </strong></p><p id="14be" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">如果我们看三张图片，两张椅子和一张桌子，我们会很容易地说这两张椅子图片最相似，而桌子与这两张椅子图片最不同。</p><div class="kj kk kl km gt ab cb"><figure class="nn kn no np nq nr ns paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/320aff16b7a64f8eeffeb94839588b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/0*BUSdEVlaSwZoYeGt.jpg"/></div></figure><figure class="nn kn nt np nq nr ns paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3f33fcda731023a1584920475c5d026d.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*XajTWbn1wZFtQ3ZT.JPG"/></div></figure><figure class="nn kn nt np nq nr ns paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/95316913c95c951903fe4eac9667ccf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*xifBa5mzuIpQglvm.JPG"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nu di nv nw translated">两把椅子和一张桌子。</p></figure></div><p id="6051" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">但是是什么让这两张椅子的图像“更相似呢？”椅子具有<em class="mx">可区分的特征</em>(即靠背、无抽屉、腿间连接)。我们的模型可以通过学习边缘、角度等模式来“理解”这些。</p><p id="e392" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">如前所述，这些特征被打包在数据的潜在空间表示中。</p><p id="5a24" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">因此，随着维度的减少，对每个图像(即椅子颜色)不同的“无关”信息从我们的潜在空间表示中“移除”，因为只有每个图像的最重要的<em class="mx">特征存储在潜在空间表示中。</em></p><p id="d52c" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">结果，当我们降低维度时，两把椅子的表现变得不那么明显，而更相似。如果我们在太空中想象它们，它们会离得更近。</p><p id="06c4" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">*请注意，我在整篇文章中提到的“接近度”是一个模糊的术语，<em class="mx">不是</em>一个确定的欧几里德距离，因为空间距离有多种定义。</p><h1 id="b063" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么潜在空间很重要？</h1><p id="f650" class="pw-post-body-paragraph mc md it me b mf nx ju mh mi ny jx mk ml nz mn mo mp oa mr ms mt ob mv mw mb im bi translated">潜在空间的概念绝对是耐人寻味。但是<em class="mx">怎么用</em>呢？我们什么时候使用它？而且最重要的是，<em class="mx">为什么是</em>？</p><p id="2623" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">我们会发现，潜在空间“隐藏”在许多我们最喜欢的图像处理网络、生成模型等中。</p><p id="1ab6" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">虽然潜在空间对大多数人来说是隐藏的，但是在某些任务中，了解潜在空间不仅是有益的，而且是必要的。</p><h2 id="2d20" class="oc la it bd lb od oe dn lf of og dp lj ml oh oi ll mp oj ok ln mt ol om lp on bi translated">表征学习</h2><p id="45ea" class="pw-post-body-paragraph mc md it me b mf nx ju mh mi ny jx mk ml nz mn mo mp oa mr ms mt ob mv mw mb im bi translated">数据的潜在空间表示包含了表示原始数据点所需的所有重要信息。</p><p id="8997" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">该表示法<em class="mx">必须</em>表示原始数据的<strong class="me iu">特征</strong>。</p><p id="2dea" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">换句话说，模型<em class="mx">学习</em>数据特征并简化其表示，使其更容易分析。</p><p id="2b44" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">这是一个名为<strong class="me iu">表示学习的概念的核心，</strong>定义为一套技术，允许系统从<em class="mx">原始数据中发现<em class="mx">特征检测</em>或<em class="mx">分类</em>所需的<em class="mx">表示</em>。</em></p><p id="8f0d" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">在这个用例中，我们的<strong class="me iu">潜在空间表示</strong>用于将更复杂形式的原始数据(即图像、视频)转换为“更便于处理”和分析的更简单表示。</p><p id="935a" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">下面列出的是表征学习的具体例子。</p><h2 id="2d5c" class="oc la it bd lb od oe dn lf of og dp lj ml oh oi ll mp oj ok ln mt ol om lp on bi translated">多支管</h2><p id="27ac" class="pw-post-body-paragraph mc md it me b mf nx ju mh mi ny jx mk ml nz mn mo mp oa mr ms mt ob mv mw mb im bi translated">潜在空间是表征学习的子领域<strong class="me iu">流形学习</strong>中的一个基本概念。</p><p id="0dd1" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu">数据科学中的流形</strong>可以理解为在某些方面“相似”的数据组或子集。</p><p id="962e" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><em class="mx">这些相似性，通常在高维空间中不易察觉或模糊不清，一旦我们的数据在潜在空间中被表示出来，就可以被发现。</em></p><p id="0218" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">以下面的“瑞士卷”为例。</p><div class="kj kk kl km gt ab cb"><figure class="nn kn oo np nq nr ns paragraph-image"><img src="../Images/8dca20b110584212e57586fc0140954c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*Emrh5MYcuMD0ADwB.png"/></figure><figure class="nn kn op np nq nr ns paragraph-image"><img src="../Images/1f99b5087958fc270640ad892138bd60.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*4VJXQFucdJcMYoo9.png"/><p class="ku kv gj gh gi kw kx bd b be z dk oq di or nw translated">瑞士卷的 3D 表示与相同数据的 2D 表示。来自<a class="ae ky" href="https://datascience.stackexchange.com/a/5698" rel="noopener ugc nofollow" target="_blank">https://datascience.stackexchange.com/a/5698</a>的例子</p></figure></div><p id="f53e" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">在 3D 中，我们知道存在<em class="mx">组类似的数据点，但是用更高维度的数据来描绘这样的组要困难得多。</em></p><blockquote class="nh ni nj"><p id="620f" class="mc md mx me b mf my ju mh mi mz jx mk nk na mn mo nl nb mr ms nm nc mv mw mb im bi translated">通过将我们的数据维度减少到 2D，在这种情况下可以认为是一种“潜在空间”表示，我们能够更容易地区分我们数据集中的流形(相似数据的组)。</p></blockquote><p id="6de9" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">要了解更多关于流形和流形学习的知识，我推荐以下文章:</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/manifolds-in-data-science-a-brief-overview-2e9dde9437e5"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">数据科学中的流形——概述</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">流形是在更高维度中表示数据的基本工具。但是什么是流形，它们是如何…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div><div class="os ot gp gr ou ov"><a href="https://scikit-learn.org/stable/modules/manifold.html" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">2.2.流形学习-sci kit-学习 0.22.1 文档</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">寻找最基本的必需品简单的最基本的必需品忘记你的忧虑和冲突我是说最基本的…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">scikit-learn.org</p></div></div></div></a></div><h2 id="5fab" class="oc la it bd lb od oe dn lf of og dp lj ml oh oi ll mp oj ok ln mt ol om lp on bi translated">自动编码器和生成模型</h2><p id="b5ff" class="pw-post-body-paragraph mc md it me b mf nx ju mh mi ny jx mk ml nz mn mo mp oa mr ms mt ob mv mw mb im bi translated">一种常见类型的深度学习模型操纵潜在空间中数据的“接近度”，这种模型是<strong class="me iu">自动编码器— </strong>一种充当身份函数的神经网络。换句话说，自动编码器学习输出任何输入的内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/eb4217f2eb99f539f5dcdb6db61745bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dknVUIxkjQ3ZV8y0.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器的一般结构</p></figure><p id="d619" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">现在，如果你是这个领域的新手，你可能会想，为什么我们需要这样的模型？如果它输出的都是它自己，那似乎就没什么用了…</p><p id="f3bd" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">虽然这个推理是有效的，但是我们并不太关心模型<em class="mx">输出什么</em>。我们更关心模型<em class="mx">在这个过程中学到了什么</em>。</p><p id="d248" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">当我们强迫一个模型成为一个身份函数时，我们是在强迫它将所有数据的相关特征存储在一个压缩的表示中，这样在那个压缩的形式中就有足够的信息，使得模型能够“准确地”重建它。<em class="mx">听起来熟悉吗？应该是，因为这个压缩的表象是我们的潜在空间表象(上图中的红色块)。</em></p><p id="2eac" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">我们已经看到了如何在潜在空间中更容易地发现模式，因为相似的数据点往往会聚集在一起，但是我们还没有看到如何从这个潜在空间的中对点<em class="mx">进行采样，以似乎生成“新”数据。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/27d7e9700809c94e30068850d6ef8ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6Nrb168aK7xoaV8b.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过潜在空间插值生成图像。来源:<em class="pm">随机噪声向量的潜在空间上的双线性插值。</em>图 20</p></figure><p id="6615" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">在上面的例子中，我们可以通过<em class="mx">对潜在空间</em>进行插值来生成不同的面部结构，并使用我们的模型解码器将潜在空间表示重建为与我们的原始输入具有相同维度的 2D 图像。</p><p id="1c2c" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated"><strong class="me iu"> <em class="mx">我说的在潜空间上插值是什么意思？</em>T19】</strong></p><p id="f4da" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">假设我已经将前一部分的椅子图像压缩成以下 2D 向量，[0.4，0.5]和[0.45，0.45]。假设桌子被压缩到[0.6，0.75]。如果我要对潜在空间进行插值，我将在潜在空间中的“椅子”聚类和“桌子”聚类之间的<em class="mx">采样点。</em></p><p id="0025" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">我们可以将这些采样的 2D 向量输入模型的解码器，瞧！我们得到的“新”图像看起来像是介于椅子和桌子之间的变体。*new 在引号中，因为这些生成的图像在技术上并不独立于原始数据样本。</p><p id="76b2" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">下面是潜在空间中两种椅子之间的线性插值示例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/d27ac304e4e5c323187c3775ab612732.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*cYaaF2pFLECohCaI.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜在空间插值。来源:Hackernoon 潜在空间可视化。</p></figure><p id="3165" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">图像生成仍然是一个活跃的研究领域，潜在空间是一个必须理解的基本概念。有关生成模型的更多用例，以及使用 GAN(生成对抗网络，另一种使用潜在空间表示的生成模型)的潜在空间插值的实践示例，请参见以下文章。</p><div class="os ot gp gr ou ov"><a href="https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">生成性对抗网络(GANs)的 18 个令人印象深刻的应用</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">生成式对抗网络(GAN)是一种用于生成式建模的神经网络架构。生成…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">machinelearningmastery.com</p></div></div><div class="pe l"><div class="po l pg ph pi pe pj ks ov"/></div></div></a></div><div class="os ot gp gr ou ov"><a href="https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">生成人脸时如何挖掘 GAN 潜在空间</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">如何利用插值和矢量运算探索 GAN 潜在空间？生成性对抗网络，或…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">machinelearningmastery.com</p></div></div><div class="pe l"><div class="pp l pg ph pi pe pj ks ov"/></div></div></a></div><h1 id="ae68" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">可视化潜在空间</h1><p id="c32c" class="pw-post-body-paragraph mc md it me b mf nx ju mh mi ny jx mk ml nz mn mo mp oa mr ms mt ob mv mw mb im bi translated">关于潜在空间可视化的更多内容，我推荐 Hackernoon 的文章，它提供了一个使用 t-SNE 算法可视化 2D 空间中数字图像之间相似性的实践示例。</p><div class="os ot gp gr ou ov"><a href="https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">潜在空间可视化—深度学习比特#2</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">特色:插值，t-SNE 投影(有 gif 和例子！)</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">hackernoon.com</p></div></div><div class="pe l"><div class="pq l pg ph pi pe pj ks ov"/></div></div></a></div><h1 id="7c36" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关键要点</h1><ul class=""><li id="d671" class="pr ps it me b mf nx mi ny ml pt mp pu mt pv mb pw px py pz bi translated"><strong class="me iu">潜在空间</strong>是压缩数据的简单表示，其中相似的数据点在空间上更加靠近。</li><li id="2c0f" class="pr ps it me b mf qa mi qb ml qc mp qd mt qe mb pw px py pz bi translated">潜在空间对于学习数据特征和寻找用于分析的更简单的数据表示是有用的。</li><li id="20ff" class="pr ps it me b mf qa mi qb ml qc mp qd mt qe mb pw px py pz bi translated">我们可以通过分析潜在空间中的数据来了解数据点之间的模式或结构相似性，无论是通过流形、聚类等。</li><li id="0ffd" class="pr ps it me b mf qa mi qb ml qc mp qd mt qe mb pw px py pz bi translated">我们可以在潜在空间中插入数据，并使用我们模型的解码器来“生成”数据样本。</li><li id="771d" class="pr ps it me b mf qa mi qb ml qc mp qd mt qe mb pw px py pz bi translated">我们可以使用 t-SNE 和 LLE 等算法来可视化潜在空间，这些算法将我们的潜在空间表示转化为 2D 或 3D。</li></ul><p id="ad59" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mb im bi translated">在学习“潜在空间”的时候，我被这个“隐藏”却又重要的概念迷住了。我希望这篇文章揭开了潜在空间表示的神秘面纱，并提供了我作为新手所渴望的对深度学习的“更深入的理解”。</p></div></div>    
</body>
</html>