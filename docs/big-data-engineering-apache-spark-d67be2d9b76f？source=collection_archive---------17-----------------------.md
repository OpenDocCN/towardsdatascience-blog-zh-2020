# 大数据工程— Apache Spark

> 原文：<https://towardsdatascience.com/big-data-engineering-apache-spark-d67be2d9b76f?source=collection_archive---------17----------------------->

## [理解大数据](https://towardsdatascience.com/tagged/making-sense-of-big-data)

## 为什么 Apache Spark 非常适合各种 ETL 工作负载。

![](img/412f6e445c626ed209b66456ab159cba.png)

由 [Seika I](https://unsplash.com/@seiseisei?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

这是大数据环境中的数据工程系列的第 2 部分。它将反映我个人的经验教训之旅，并在我创建的开源工具 [Flowman](https://flowman.readthedocs.io) 中达到高潮，以承担在几个项目中一遍又一遍地重新实现所有 boiler plate 代码的负担。

*   [第 1 部分:大数据工程—最佳实践](https://medium.com/@kupferk/big-data-engineering-best-practices-bfc7e112cf1a)
*   第 2 部分:大数据工程— Apache Spark
*   [第 3 部分:大数据工程——声明性数据流](/big-data-engineering-declarative-data-flows-3a63d1802846)
*   [第 4 部分:大数据工程— Flowman 启动并运行](/big-data-engineering-flowman-up-and-running-cd234ac6c98e)

# 期待什么

本系列是关于用 Apache Spark 构建批处理数据管道的。但是有些方面对于其他框架或流处理也是有效的。最后，我将介绍 [Flowman](https://flowman.readthedocs.io) ，这是一个基于 Apache Spark 的应用程序，它简化了批处理数据管道的实现。

# 介绍

第二部分强调了为什么 Apache Spark 非常适合作为实现数据处理管道的框架。还有许多其他的选择，尤其是在流处理领域。但是从我的角度来看，当在批处理世界中工作时(有很好的理由这样做，特别是如果涉及许多需要大量历史记录的非平凡转换，如分组聚合和巨大连接)，Apache Spark 是一个几乎无可匹敌的框架，在批处理领域表现尤为突出。

本文试图揭示 Spark 提供的一些功能，这些功能为批处理提供了坚实的基础。

# 数据工程的技术要求

我已经在第一部分中评论了数据处理管道的典型部分。让我们重复这些步骤:

1.  **提取。从某个源系统读取数据(可以是像 HDFS 这样的共享文件系统，也可以是像 S3 这样的对象存储，或者像 MySQL 或 MongoDB 这样的数据库)**
2.  **转型。**应用一些转换，如数据提取、过滤、连接甚至聚合。
3.  **装货。**将结果再次存储到某个目标系统中。同样，这可以是共享文件系统、对象存储或某个数据库。

现在，我们可以通过将这些步骤中的每一步映射到期望的功能，并在最后添加一些额外的需求，来推导出用于数据工程的框架或工具的一些需求。

1.  **广泛的连接器。**我们需要一个能够从广泛的数据源读入数据的框架，比如分布式文件系统中的文件、关系数据库或列存储中的记录，甚至是键值存储。
2.  **广泛而可扩展的变换范围。为了“应用转换”,框架应该明确地支持和实现转换。典型的转换是简单的列式转换，如字符串操作、过滤、连接、分组聚合——所有这些都是传统 SQL 提供的。最重要的是，框架应该提供一个简洁的 API 来扩展转换集，特别是列式转换。这对于实现不能用核心功能实现的定制逻辑非常重要。**
3.  **广泛的连接器。**同样，我们需要各种各样的连接器来将结果写回到所需的目标存储系统中。
4.  **扩展性。**我已经在上面的第二个需求中提到了这一点，但是我觉得这一点对于一个明确的点来说足够重要。可扩展性可能不仅限于转换类型，还应该包括新输入/输出格式和新连接器的扩展点。
5.  **可扩展性。**无论选择哪种解决方案，都应该能够处理不断增长的数据量。首先，在许多情况下，你应该准备好处理比 RAM 所能容纳的更多的数据。这有助于避免完全被数据量卡住。其次，如果数据量使处理速度太慢，您可能希望能够将工作负载分布到多台机器上。

# 什么是 Apache Spark

Apache Spark 为上述所有需求提供了良好的解决方案。Apache Spark 本身是一个库集合，是一个开发定制数据处理管道的框架。这意味着 Apache Spark 本身并不是一个成熟的应用程序，而是需要您编写包含转换逻辑的程序，而 Spark 负责以一种高效的方式在集群中的多台机器上执行逻辑。

Spark 最初于 2009 年在加州大学伯克利分校的 AMPLab 启动，并于 2010 年开源。最终在 2013 年，这个项目被捐赠给了 Apache 软件基金会。该项目很快就受到了关注，尤其是以前使用 Hadoop Map Reduce 的人。最初，Spark 围绕所谓的 rdd(弹性分布式数据集)提供了核心 API，与 Hadoop 相比，rdd 提供了更高层次的抽象，从而帮助开发人员更高效地工作。

后来，添加了更新的 on preferred DataFrame API，它实现了一个关系代数，其表达能力可与 SQL 相媲美。这个 API 提供了与数据库中的表非常相似的概念，这些表具有命名的和强类型的列。

虽然 Apache Spark 本身是用 Scala(一种在 JVM 上运行的混合函数式和面向对象的编程语言)开发的，但它提供了使用 Scala、Java、Python 或 r 编写应用程序的 API。当查看[官方示例](https://spark.apache.org/examples.html)时，您会很快意识到该 API 非常有表现力和简单。

1.  **连接器。**由于 Apache Spark 只是一个处理框架，没有内置的持久层，它一直依赖于通过 JDBC 连接到 HDFS、S3 或关系数据库等存储系统。这意味着从一开始就建立了清晰的连接设计，特别是随着数据帧的出现。如今，几乎每一种存储或数据库技术都只需要*为 Apache Spark 提供一个适配器，而 Apache Spark 被认为是许多环境中的一个可能选择。*
2.  **变换。**原始核心库为 RDD 抽象提供了许多常见的转换，如过滤、连接和分组聚合。但是现在更新的 DataFrame API 更受欢迎，它提供了大量模仿 SQL 的转换。这应该足够满足大多数需求了。
3.  **扩展性。**新的转换可以通过所谓的*用户定义函数*(UDF)轻松实现，在这种情况下，您只需提供一小段处理单个记录或列的代码，Spark 将它包装起来，以便该函数可以并行执行并分布在一个计算机集群中。
    由于 Spark 具有非常高的代码质量，您甚至可以深入一两层，使用内部开发人员 API 实现新的功能。这可能有点困难，但是对于那些无法使用 UDF 实现的罕见情况来说，这是非常有益的。
4.  **可扩展性。** Spark 从一开始就被设计成一个大数据工具，因此它可以扩展到不同类型集群(当然是 Hadoop YARN、Mesos 和最近的 Kubernetes)中的数百个节点。它可以处理比内存大得多的数据。一个非常好的方面是，Spark 应用程序也可以在没有任何集群基础设施的单个节点上非常高效地运行，从开发人员的角度来看，这是一个很好的测试，但这也使 Spark 能够用于不太大的数据量，并且仍然受益于 Sparks 的特性和灵活性。

从这四个方面来看，Apache Spark 非常适合以前由 Talend 或 Informatica 等供应商提供的专用且昂贵的 ETL 软件完成的典型数据转换任务。通过使用 Spark，您可以获得生动的开源社区的所有好处，并且可以根据您的需求自由定制应用程序。

尽管 Spark 在创建时就考虑到了海量数据，但我总是会考虑使用它，即使是少量数据，因为它非常灵活，可以随着数据量无缝增长。

# 可供选择的事物

当然，Apache Spark 并不是实现数据处理管道的唯一选择。像 Informatica 和 Talend 这样的软件供应商也为喜欢购买完整生态系统的人提供非常可靠的产品(包括所有的优点和缺点)。

但是，即使在大数据开源世界中，有些项目乍看起来似乎是备选方案。

首先，我们仍然有 Hadoop。但是 Hadoop 实际上由三个组件组成，这三个组件被清晰地划分:首先，我们有分布式文件系统 HDFS，它能够存储非常大量的数据(比如说 Pb)。接下来，我们有运行分布式应用程序的集群调度器。最后，我们有一个 Map Reduce 框架，用于开发非常特殊类型的分布式数据处理应用程序。虽然前两个组件 HDFS 和 YARN 仍然被广泛使用和部署(尽管他们感到来自云存储的压力，Kubernetes 是可能的替代品)，但 Map Reduce 框架现在根本不应该被任何项目使用。编程模型太复杂了，编写重要的转换会变得非常困难。所以，是的，HDFS 和 YARN 作为基础设施服务(存储和计算)很好，Spark 与这两者很好地集成在一起。

其他备选方案可以是 SQL 执行引擎(没有集成的持久层)，如 Hive、Presto、Impala 等。虽然这些工具通常也提供了与不同数据源的广泛连接，但它们都局限于 SQL。首先，对于带有许多公共表表达式(cte)的长链转换，SQL 查询本身会变得非常棘手。其次，用新特性扩展 SQL 通常更加困难。我不会说 Spark 在总体上比这些工具更好，但我认为 Spark 更适合数据处理管道。这些工具在*查询*现有数据方面大放异彩。但是我不想用这些工具为*创建*数据——那从来都不是他们的主要工作范围。另一方面，虽然你*可以*通过 [Spark Thrift Server](https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html) 使用 Spark 来执行 SQL 以提供数据，但它并不是真正为这种场景而创建的。

# 发展

我经常听到的一个问题是，应该使用什么编程语言来访问 Spark 的功能。正如我在上面写的，Spark out of the box 提供了 Scala、Java、Python 和 R 的绑定——所以这个问题真的很有意义。

我的建议是根据任务使用 Scala 或 Python(也许是 R——我没有这方面的经验)。千万不要用 Java(感觉真的比干净的 Scala API 复杂多了)，投资点时间学点基础的 Scala 吧。

现在留给我们的问题是“Python 还是 Scala”。

*   如果你做的是数据工程(读取、转换、存储)，那么我强烈建议使用 Scala。首先，因为 Scala 是一种静态类型语言，所以实际上比 Python 更容易编写正确的程序。第二，每当您需要实现 Spark 中没有的新功能时，最好使用 Spark 的本地语言。尽管 Spark well 支持 Python 中的 UDF，但您将付出性能代价，并且无法再深入研究。用 Python 实现新的连接器或文件格式将非常困难，甚至是不可能的。
*   如果你正在研究数据科学(这不在本系列文章的讨论范围之内)，那么 Python 是更好的选择，包括所有那些 Python 包，比如 Pandas、SciPy、SciKit Learn、Tensorflow 等等。

除了上面两个场景中的不同库之外，典型的开发工作流也有很大的不同:数据工程师开发的应用程序通常每天甚至每小时都在生产中运行。另一方面，数据科学家经常与数据交互工作，一些见解是最终的成果。因此，生产就绪性对数据工程师来说比对数据科学家来说更重要。即使许多人不同意，Python 或任何其他动态类型语言的“生产就绪”要困难得多。

# 框架的缺点

既然 Apache Spark 对于复杂的数据转换来说是一个非常好的框架，我们可以简单地开始实现我们的管道。在几行代码中，我们可以指示 Spark 执行所有的魔法，将我们的多 TB 数据集处理成更容易访问的东西。

等等，别这么快！我在过去为不同的公司多次这样做，过了一段时间后，我发现许多方面必须反复实现。虽然 Spark 擅长数据处理本身，但我在本系列的第一部分中指出，健壮的数据工程不仅仅是处理本身。日志、监控、调度、模式管理都出现在我的脑海中，所有这些方面都需要在每个严肃的项目中得到解决。

那些非功能方面通常需要编写重要的代码，其中一些可能是非常低级和技术性的。因此，Spark 不足以实现生产质量数据管道。由于这些问题的出现与特定的项目和公司无关，我建议将应用程序分成两层:一个顶层包含编码在数据转换中的业务逻辑以及数据源和数据目标的规范。一个较低的层应该负责执行整个数据流，提供相关的日志记录和监控指标，负责模式管理。

# 最后的话

这是关于使用 Apache Spark 构建健壮的数据管道的系列文章的第二部分。我们非常关注为什么 Apache Spark 非常适合取代传统的 ETL 工具。下一次我将讨论为什么另一个抽象层将帮助您关注业务逻辑而不是技术细节。