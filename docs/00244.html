<html>
<head>
<title>A Guide To The Data Lake — Modern Batch Data Warehousing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据湖指南——现代批量数据仓库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-modern-batch-data-warehousing-extraction-f63bfa6ef878?source=collection_archive---------10-----------------------#2020-01-08">https://towardsdatascience.com/a-guide-to-modern-batch-data-warehousing-extraction-f63bfa6ef878?source=collection_archive---------10-----------------------#2020-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e9df428b923241482024963c460ce7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xHr9gIBsauLWCykbfp7kQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://www.pexels.com/@chetanvlad?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jh">弗拉德Chețan </strong> </a>发自<a class="ae jg" href="https://www.pexels.com/photo/water-flowing-down-on-mossy-rock-2957464/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jh">像素</strong> </a></p></figure><div class=""/><div class=""><h2 id="85b3" class="pw-subtitle-paragraph kh jj jk bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">使用“功能数据工程”重新定义批量数据提取模式和数据湖</h2></div><p id="80c6" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过去几十年，数据分析领域发生了巨大的变革。随着存储成本的降低和云计算的采用，指导数据工具设计的限制变得过时，因此—数据工程工具和技术<strong class="lb jl">必须</strong>发展。</p><p id="203a" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我怀疑许多数据团队正在进行复杂的项目，以使他们的数据工程栈现代化，并使用他们所掌握的新技术。许多其他公司正在从零开始设计新的数据生态系统，这些公司正在寻求机器学习和云计算的进步所带来的新商机。</p><p id="ecb5" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">pre-</em><a class="ae jg" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Hadoop</em></a>批处理数据基础架构通常由与其存储紧密耦合的数据仓库(DW)设备(例如Oracle或Teradata DW)、提取转换加载(ETL)工具(例如SSIS或Informatica)和商业智能(BI)工具(例如Looker或MicroStrategy)组成。在这种情况下，数据组织的哲学和设计原则是由诸如Ralph Kimball的<em class="lv">The Data Warehouse Toolkit</em>(1996)或比尔·恩门的<em class="lv">Building The Data Warehouse</em>(1992)等书中概述的成熟方法所驱动的。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="48bc" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将这种方法与其现代版本进行了对比，后者诞生于云技术创新和降低存储成本。在现代堆栈中，由数据仓库设备处理的角色现在由专门的<strong class="lb jl">组件处理，如文件格式(如<a class="ae jg" href="http://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Parquet </a>、<a class="ae jg" href="https://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Avro </a>、<a class="ae jg" href="https://hudi.incubator.apache.org/" rel="noopener ugc nofollow" target="_blank">胡迪</a>)、廉价云存储(如<a class="ae jg" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> AWS S3 </a>、<a class="ae jg" href="https://cloud.google.com/storage/" rel="noopener ugc nofollow" target="_blank"> GS </a>)、元数据引擎(如<a class="ae jg" href="https://hive.apache.org/" rel="noopener ugc nofollow" target="_blank"> Hive </a> metastore)、查询/计算引擎(如Hive、<em class="lv">拖放</em> ETL工具不太常见，取而代之的是一个调度器/指挥器(例如<a class="ae jg" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Airflow </a>、<a class="ae jg" href="https://luigi.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> Luigi </a>)和“特设”软件逻辑来承担这个角色。“ad-hoc”ETL软件有时出现在单独的应用程序中，有时出现在调度程序框架中，该框架通过设计<strong class="lb jl">是可扩展的</strong>(气流中的操作符，Luigi中的任务)。它通常依赖Spark clusters或DWs等外部计算系统进行大量转换。BI方面也看到了称为<a class="ae jg" href="https://superset.incubator.apache.org/" rel="noopener ugc nofollow" target="_blank">超集</a>的开源替代方案的兴起，有时由<a class="ae jg" href="https://druid.apache.org/" rel="noopener ugc nofollow" target="_blank"> Druid </a>补充，以创建汇总、在线分析处理(OLAP)立方体，并提供快速只读存储和查询引擎。</strong></p><h1 id="faff" class="md me jk bd mf mg mh mi mj mk ml mm mn kq mo kr mp kt mq ku mr kw ms kx mt mu bi translated">存在的理由</h1><p id="4103" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我发现自己正在从事从<em class="lv">前Hadoop </em>堆栈到现代堆栈的迁移。这不仅是一次技术转变，也是一次重大的<strong class="lb jl">范式转变</strong>。在某些情况下，在建模和架构决策中，我们应该远离<em class="lv">过时的</em>智慧和最佳实践，理解我们为什么这样做是很重要的。我发现<a class="na nb ep" href="https://medium.com/u/9f4d525c99e2?source=post_page-----f63bfa6ef878--------------------------------" rel="noopener" target="_blank"> Maxime Beauchemin </a>的资源非常有帮助，学习/理解Apache Airflow的设计选择在实现他提倡的方法(Airflow是由Maxime创建的)时带来了很多实际的理解。本指南旨在采用一种<em class="lv">固执己见的</em>方法来定义和设计数据湖。</p><p id="eaf8" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我挑选了一些特定的技术来使本指南更加实用，我希望其中的大部分技术也适用于现代堆栈中的其他工具。选择一种技术而不是另一种技术的动机通常是我的经验(或缺乏经验)的结果。例如，我会提到AWS工具，因为这是我有经验的云提供商。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="5f38" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇博文定义了ETL的E，并描述了<strong class="lb jl">数据湖</strong>的角色。</p><h1 id="129e" class="md me jk bd mf mg mh mi mj mk ml mm mn kq mo kr mp kt mq ku mr kw ms kx mt mu bi translated">提取，血统</h1><p id="a523" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在数据工程中，提取应该是在给定时间点<em class="lv">实体</em>状态的<strong class="lb jl">未改变快照</strong>。</p><p id="2553" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，它通常涉及调用API、抓取网站、从安全文件传输协议(SFTP)服务器获取文件、定期对运行数据库的副本运行查询以及从S3帐户复制文件。提取的结果存储在一个便宜的、可扩展的、高可用性的云存储中，比如S3，可以永久保存<em class="lv"/>——或者只要合规允许就保存。这些提取产生的数据构成了数据湖。</p><h2 id="8b0c" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">数据湖</h2><p id="184b" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">不同的作家、博客作者和专业人士对“数据湖”和“数据仓库”有不同的定义。有时，他们的角色没有被清楚地陈述或者有重叠——以至于造成混乱，这两个词可以互换使用。数据湖的以下定义很简单，它清楚地将数据湖从数据仓库中分离出来，并将原始数据(数据湖的一部分)从派生的数据集(数据仓库/数据集市的一部分)中分离出来。</p><p id="c160" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据湖是一个存储库，包含所有由业务产生或收集的未处理的数据。因为此时没有业务逻辑应用于数据，<strong class="lb jl">保持不变</strong>，如果业务需求发生变化，任何分析(表格、数据科学模型)都可以从该来源重新创建。从不同来源提取数据是必要的，因为从来源获取数据通常是昂贵的(API调用、缓慢的SFTP、操作数据库转储)，有时是不可能的(API发展、sftp变空、操作数据库就地改变记录)。</p><h2 id="21ca" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">结构</h2><p id="5c35" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">随着数据的民主化和分析的去中心化，发现<strong class="lb jl">湖泊变得非常重要</strong>，考虑以下结构:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="3151" class="nc me jk nt b gy nx ny l nz oa">s3://myorg-data-lake<br/>├── s3://myorg-data-lake/tweets_mentioning_myorg<br/>└── s3://myorg-data-lake/salesforce_clients</span></pre><p id="15e6" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据湖消费者希望任何摘录都是“my org-Data-Lake”S3存储桶中的顶级前缀，以便于浏览。然而，数据不需要放在同一个桶中，因为我们可以使用Hive metastore将任何提取注册为同一个模式中的表，提供一个到数据湖的<strong class="lb jl">中央接口</strong>。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="62c1" class="nc me jk nt b gy nx ny l nz oa">data_lake<br/>├── data_lake.tweets_mentioning_myorg → s3://myorg-twitter-extracts/tweets_mentioning_myorg<br/>└── data_lake.salesforce_clients → s3://myorg-salesforce-extracts/salesforce_clients</span></pre><h2 id="bf58" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">格式</h2><p id="fdb2" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">半结构化和非结构化数据经常被认为是数据湖的一个特征。然而，我相信在提取过程中，转换成一种嵌入了模式的文件格式(比如Parquet和Avro)有很大的好处。模式是定义数据集接口的一种方式，使得多个团队在需要最少通信的情况下更容易使用它。这也是一种执行轻量级验证的方式，验证源系统仍然在生成预期的数据。当模式不再有效时，提取<em class="lv">中断</em>，但是它降低了产生错误分析或转换过程中隐藏错误的风险。提取的数据可能已经有了某种模式(数据库和API提取)，存储为JSON / CSV将意味着丢失一些有价值的元数据。</p><p id="d911" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在源系统使以前的数据很快不可用的情况下，提取一次<strong class="lb jl">而不进行任何转换</strong>，并在原始副本上运行转换。然后，配置单元表可以指向转换后的副本。例如，如果we文件来自SFTP服务器，并且这些文件在大约1小时后消失:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c599" class="nc me jk nt b gy nx ny l nz oa">SFTP<br/>├── file1.csv<br/>└── file2.csv</span><span id="1140" class="nc me jk nt b gy ob ny l nz oa">s3://myorg-data-lake<br/>├── s3://myorg-data-lake/sftp_clients/raw/file1.csv<br/>├── s3://myorg-data-lake/sftp_clients/raw/file2.csv<br/>└── s3://myorg-data-lake/sftp_clients/parquet/…</span></pre><p id="9c90" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，文件在S3上，模式可以被修复，而不用担心丢失任何数据。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="0546" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于大数据框架利用文件元数据和特殊数据布局来优化计算，因此生成的文件处理速度通常会更快。使用这种文件格式的另一个好处是<strong class="lb jl">减小了文件大小</strong>。模式、编码技术和特殊的<em class="lv">数据布局</em>，如列存储，允许这些库避免冗余——减少表示相同信息所需的字节量。</p><h2 id="93c2" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">文件大小</h2><p id="c9fb" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">许多Hadoop系列框架在处理少量大文件时比处理大量小文件时效率更高。原因是读取每个文件的元数据有开销，启动并行下载文件的进程也有开销。因此，<strong class="lb jl">合并从数据库的多个查询、多个API调用或多个SFTP文件中提取的数据</strong>以减少文件数量，可以为下游转换节省大量时间。像Snappy这样的<strong class="lb jl">压缩</strong>库也可以用来减少通过网络的数据量，并且通常值得这样做，因为引入的CPU负载可以忽略不计。</p><h2 id="cd91" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">没有变化</h2><p id="3b28" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">数据湖应该包含在给定时刻<strong class="lb jl">的<em class="lv">状态</em>的不变<strong class="lb jl">真值</strong> — <strong class="lb jl"> </strong>未修改快照。</strong>此时任何转型都是不可取的— <strong class="lb jl"> </strong>合规流程除外(如匿名化)。</p><p id="1401" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果数据在被复制到数据湖之前被更改，它将偏离其在源系统中捕获的状态。如果源系统使以前的数据不可用，并且应用的逻辑需要撤销，数据将不得不进一步变异。这是危险的，因为它可能会导致<strong class="lb jl">无法回滚更改</strong>来获取原始提取。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="6369" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时，源系统中的错误会导致产生不正确的数据。在这些方面，我们可能希望它反映在我们的捕获(和分析)中，或者我们可能希望它得到纠正。在后一种情况下，重新运行相关时间窗口的提取过程可能就足够了，或者可能需要人工干预，但是在这两种情况下，<em class="lv">物理分区</em>仅用于<strong class="lb jl">覆盖</strong>相关数据。</p><h2 id="04e8" class="nc me jk bd mf nd ne dn mj nf ng dp mn li nh ni mp lm nj nk mr lq nl nm mt nn bi translated">物理分区</h2><p id="74c0" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">对提取的数据进行分区对于实现幂等性和优化提取大小非常重要。Maxime Beauchemin 在他的<a class="ae jg" href="https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a" rel="noopener">功能数据工程博客</a>中为幂等ETL提供了一个强有力的案例。给定一个“预定的执行日期”，提取应该总是产生相同的结果。这有时<em class="lv">是不可能的</em>，因为我们依赖于我们无法控制的外部资源，但是这些提取仍然应该被分组到分区中，以便下游的转换可以是等幂的。</p><p id="94de" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个幂等每日摘录的简单示例:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="03c5" class="nc me jk nt b gy nx ny l nz oa">SELECT * FROM customers<br/>WHERE last_modified_date &gt;= 2020–01–01<br/>  AND last_modified_date &lt;  2020–01–02</span></pre><p id="61ff" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实践中，我们将参数化上面的SQL以从执行日期导出下限和上限</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7aac" class="nc me jk nt b gy nx ny l nz oa">SELECT * FROM customers<br/>WHERE last_modified_date &gt;= {{ execution date - 1 day }}<br/>  AND last_modified_date &lt;  {{ execution date }}</span></pre><p id="af67" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将其与下面的<strong class="lb jl">错误示例</strong>进行比较，其中结果将根据外部因素(今天的日期)而变化:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0779" class="nc me jk nt b gy nx ny l nz oa">SELECT * FROM customers<br/>WHERE last_modified_date &gt; 2019–01–01</span></pre><p id="39a7" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Airflow中，<a class="ae jg" href="https://airflow.apache.org/docs/stable/macros.html#default-variables" rel="noopener ugc nofollow" target="_blank">执行日期</a>通常用于实现幂等。执行日期是一个强大的概念，它是一个<strong class="lb jl">不可变的日期</strong>在运行时赋予一个<em class="lv">管道</em>(气流中的有向无环图)。如果DAG计划在<em class="lv">2019–01–01 00:00:00</em>运行，这就是执行日期。如果该管道失败，或者需要重新运行，则可以清除该特定运行的状态，然后它将获得<strong class="lb jl">相同的执行日期</strong>并产生相同的提取，条件是提取基于执行日期和幂等。这使得并行运行多个提取过程成为可能，并且通常用于回填数据。</p><p id="b114" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我使用相同的(简单的)技术从API回填数据，并使用Airflow的特性来限制并行性，并自动重试或超时调用。这是气流的一个常见用例，通过幂等(非重叠)提取使其成为可能。</p><p id="3461" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我们可以运行端到端ETL的多个实例。当试图重新处理一个<em class="lv">不寻常的</em>时间范围内的数据时，这尤其有用。例如，如果转换代码和定义的资源是针对每天的数据量进行测试的，那么很难知道相同的设置对于整个月的数据量会有什么样的表现。相反，ETL的一个实例可以在被重新处理的一个月中的每一天启动——可能并行执行(这在Airflow中很容易实现)。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="fa6e" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提取的物理分区应基于提取处理的计划运行日期。最简单的例子是每日批量处理的每日提取物:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0723" class="nc me jk nt b gy nx ny l nz oa">s3://myorg-data-lake/sftp_clients/parquet/ds=2020-01-01<br/>s3://myorg-data-lake/sftp_clients/parquet/ds=2020-01-02<br/>s3://myorg-data-lake/sftp_clients/parquet/ds=2020-01-03</span></pre><p id="a832" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> ds代表日期戳，这里:执行日期</em></p><p id="25f4" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意表示Hadoop生态系统中的物理分区的<code class="fe oc od oe nt b">key=value</code>格式——物理分区被转换为数据集的一列，当在WHERE子句中使用时，允许跳过所选分区之外的所有文件(分区修剪)。</p><p id="2b5a" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提取过程应该以这样的方式编写，即根据给定的执行日期覆盖分区<strong class="lb jl">。然后，转换过程可以使用<code class="fe oc od oe nt b">ds</code>作为过滤器来获取它们需要处理的数据。</strong></p><p id="f359" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他时间框架遵循相同的原则，但在每小时的情况下，我们可能要考虑<em class="lv">流</em>——批处理系统往往会有开销，使它们无法用于非常短的批处理。</p><p id="d1dc" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个有趣且常见的用例是，我们在<em class="lv">提取数据的频率高于我们在</em>转换数据的频率。当从源中提取一个大的时间范围使其超负荷时，就会出现这种需要，因为数据有以后不可用的风险，或者在转换之前一次提取所有数据会延迟下游过程。例如，我们可能希望创建每小时的提取，但是每12小时处理一次数据。为了设计这样一个需要以不同节奏调度任务的流水线，我们可以使用<a class="ae jg" href="https://airflow.apache.org/docs/stable/concepts.html?highlight=branch#branching" rel="noopener ugc nofollow" target="_blank">逻辑分支</a> <strong class="lb jl"> </strong>到<strong class="lb jl"> </strong>在我们需要的时候选择性地运行转换过程。</p><p id="261b" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种较高的提取频率会产生不希望的小文件，在这种情况下，在转换之前，我们合并要处理的批处理。当文件非常小，以至于将元数据(模式、统计数据)写入每一个文件会产生很大的开销时，应该在文件合并后转换为模式嵌入文件。在这种情况下，请考虑以下结构:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="35ac" class="nc me jk nt b gy nx ny l nz oa">s3://myorg-data-lake/sftp_clients/raw/ds=2020-01-01&lt;space&gt;00:00:00<br/>s3://myorg-data-lake/sftp_clients/raw/ds=2020-01-01&lt;space&gt;01:00:00<br/>s3://myorg-data-lake/sftp_clients/raw/ds=2020-01-01&lt;space&gt;02:00:00<br/>...<br/>s3://myorg-data-lake/sftp_clients/raw/ds=2020-01-01&lt;space&gt;12:00:00<br/>--<br/>s3://myorg-data-lake/sftp_clients/parquet/ds=2020-01-01&lt;space&gt;12:00:00</span></pre><p id="7bca" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中合并+转换+压缩过程将把12个分区变成1个，并且<code class="fe oc od oe nt b">sftp_clients</code>表将指向拼花版本而不是原始副本。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="5317" class="md me jk bd mf mg of mi mj mk og mm mn kq oh kr mp kt oi ku mr kw oj kx mt mu bi translated">最后</h1><p id="76e9" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在这篇文章中，我试图给数据湖和提供数据的提取过程添加一些结构。希望你觉得有用！</p></div></div>    
</body>
</html>