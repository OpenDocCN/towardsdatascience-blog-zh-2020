<html>
<head>
<title>Data Science and Machine Learning with Scala and Spark (Episode 02/03)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Scala 和 Spark 实现数据科学和机器学习(第 2/03 集)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-and-machine-learning-with-scala-and-spark-episode-02-03-be74f0590f20?source=collection_archive---------22-----------------------#2020-07-04">https://towardsdatascience.com/data-science-and-machine-learning-with-scala-and-spark-episode-02-03-be74f0590f20?source=collection_archive---------22-----------------------#2020-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f317" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">SCALA SPARK 机器学习</h2><div class=""/><div class=""><h2 id="2b22" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Scala API 的 Spark</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/9cedd149456eab6e538bf743da06b20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*DslnppBYqokMiUsCEMkSbg.png"/></div></figure><p id="b887" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark 的发明者选择 Scala 来编写底层模块。在<a class="ae ls" rel="noopener" target="_blank" href="/data-science-and-machine-learning-with-scala-and-spark-episode-01-03-23864e07fdb3">使用 Scala 和 Spark 的数据科学和机器学习(第 01/03 集)</a>中，我们在使用 Google Colab 环境时讲述了 Scala 编程语言的基础知识。在本文中，我们将了解 Spark 生态系统及其面向 Scala 用户的高级 API。和以前一样，我们仍然使用 Spark 3.0.0 和 Google Colab 来练习一些代码片段。</p><h2 id="2109" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk iw bi translated">什么是阿帕奇火花？</h2><p id="8bbf" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">据<a class="ae ls" href="https://databricks.com/p/ebook/apache-spark-under-the-hood" rel="noopener ugc nofollow" target="_blank">阿帕奇星火和三角洲引擎盖下的湖泊</a></p><blockquote class="mq mr ms"><p id="32b7" class="kw kx mt ky b kz la ka lb lc ld kd le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">Apache Spark 是一个统一的计算引擎和一组用于在计算机集群上进行并行数据处理的库。截至本文撰写之时，Spark 是针对这一任务开发的最活跃的开源引擎；使其成为任何对大数据感兴趣的开发人员或数据科学家的实际工具。Spark 支持多种广泛使用的编程语言(Python、Java、Scala 和 R)，包括从 SQL 到流和机器学习等各种任务的库，可以在从笔记本电脑到数千个服务器集群的任何地方运行。这使它成为一个易于启动和扩展到大数据处理或超大规模的系统。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/c476ce90c4773abd90b2ab0ce1ec13ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Dy9w0lUXIeH6WHALkQC-g.png"/></div></div></figure><p id="e8c3" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我强烈推荐阅读<a class="ae ls" href="https://databricks.com/p/ebook/apache-spark-under-the-hood" rel="noopener ugc nofollow" target="_blank"> Apache Spark 和 Delta Lake Under the Hood </a>快速参考手册。这是一个 45 页的文档，包含 Scala 和 pyspark APIs 中的示例，阅读时间不会超过 30 分钟。</p><h2 id="e227" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk iw bi translated">为什么要用 Spark 进行机器学习？</h2><p id="cfaa" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">作为一名数据科学家，我们可以提出这样一个问题“Spark 对于机器学习任务的重要性是什么？”随着电子设备、社交媒体平台和高级 IT 系统使用的增加，数据正以前所未有的水平产生。此外，由于廉价的数据存储，客户很乐意收集大数据来提取价值。机器学习模型已经得到证明，并为企业更好地理解和制定未来扩张战略而工作。就获取大数据而言，Spark 是机器学习的事实上的选择，可以通过收集大量数据来建立模型。</p><blockquote class="mq mr ms"><p id="053a" class="kw kx mt ky b kz la ka lb lc ld kd le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">DataFrame API 是机器学习最重要的高层 API。Spark 的 MLlib 放弃了对 RDD 的支持，转而支持 DataFrame API。</p></blockquote><h2 id="c3ed" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk iw bi translated">Spark 中的 Scala API</h2><p id="897f" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Scala 被选为编写 Spark 引擎的核心语言。但是，Apache Spark 提供了 Java、Scala、Python 和 r 的高级 API，在本文中，我们将使用 spark 3.0.0。</p><p id="da99" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是在 Google colab 中获取 Scala 和 Spark 模块的命令</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="b8a5" class="lt lu iq nd b gy nh ni l nj nk">// Run below commands</span><span id="3737" class="lt lu iq nd b gy nl ni l nj nk">import $ivy.`org.apache.spark::spark-sql:3.0.0`<br/>import org.apache.spark.sql._</span><span id="29a7" class="lt lu iq nd b gy nl ni l nj nk">import $ivy.`sh.almond::ammonite-spark:0.3.0`</span></pre><p id="f0c4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark 有两个重要的抽象，Spark 上下文和 Spark 会话。使用下面的代码，我们创建一个 spark 会话和上下文。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="dcac" class="lt lu iq nd b gy nh ni l nj nk">import org.apache.spark.SparkContext</span><span id="f493" class="lt lu iq nd b gy nl ni l nj nk">import org.apache.spark.SparkConf</span></pre><blockquote class="mq mr ms"><p id="3008" class="kw kx mt ky b kz la ka lb lc ld kd le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">通常，当运行 spark 时会有很多警告，使用下面的命令，你可以关闭它们。</p></blockquote><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="92d0" class="lt lu iq nd b gy nh ni l nj nk">// TIP: Turn off the millions lines of logs</span><span id="a3cb" class="lt lu iq nd b gy nl ni l nj nk">import org.apache.log4j.{Level, Logger}</span><span id="e9f2" class="lt lu iq nd b gy nl ni l nj nk">Logger.getLogger(“org”).setLevel(Level.OFF)</span><span id="8860" class="lt lu iq nd b gy nl ni l nj nk">// define spark session<br/>val spark = {</span><span id="c525" class="lt lu iq nd b gy nl ni l nj nk">SparkSession.builder()</span><span id="1939" class="lt lu iq nd b gy nl ni l nj nk">.master("local[*]")</span><span id="18b1" class="lt lu iq nd b gy nl ni l nj nk">.getOrCreate()</span><span id="d045" class="lt lu iq nd b gy nl ni l nj nk">}</span><span id="c334" class="lt lu iq nd b gy nl ni l nj nk">// Define Spark Context<br/>def sc = spark.sparkContext</span></pre><h2 id="282e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk iw bi translated">用 Scala 点燃 RDD</h2><p id="55ab" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">弹性分布式数据集(RDD)是与 Scala 密切相关的 Spark 数据结构的最常见抽象。它非常类似于 Scala 的原生并行特性。让我们用 Scala 写一些关于 RRD 的片段。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="b677" class="lt lu iq nd b gy nh ni l nj nk">// Spark RDD</span><span id="8290" class="lt lu iq nd b gy nl ni l nj nk">import scala.util.Random</span><span id="5f79" class="lt lu iq nd b gy nl ni l nj nk">// Define variable in Scala<br/>val bigRng = scala.util.Random.shuffle(1 to 100000)</span><span id="9d4b" class="lt lu iq nd b gy nl ni l nj nk">// convert Scala variable to spark RDD<br/>val bigPRng = sc.parallelize(bigRng)</span></pre><p id="e980" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在<code class="fe nm nn no nd b">bigPRng</code>上应用很多操作，它会在 Spark 上运行。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="cc7b" class="lt lu iq nd b gy nh ni l nj nk">// calculate the mean of the population<br/>bigPRng.mean</span><span id="6af8" class="lt lu iq nd b gy nl ni l nj nk">// Find the min of the population<br/>bigPRng.min</span><span id="4203" class="lt lu iq nd b gy nl ni l nj nk">// Find the stanndard deviation of the population<br/>bigPRng.popStdev</span></pre><p id="5b19" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 RDD 的每个元素上应用函数非常类似于 Scala 并行<code class="fe nm nn no nd b">map</code>函数。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="9948" class="lt lu iq nd b gy nh ni l nj nk">// Map function on RDD, similar to Paralell in Scala</span><span id="f18f" class="lt lu iq nd b gy nl ni l nj nk">val bigPRng2 = bigPRng.map(_ * 2)</span><span id="4da6" class="lt lu iq nd b gy nl ni l nj nk">// Scala function function to apply on RDD</span><span id="5f36" class="lt lu iq nd b gy nl ni l nj nk">def div3(x:Int) : Boolean = {val y:Int=(x%3); return(y==0)}<br/>val bigBool = bigPRng2.map(div3(_))</span></pre><h1 id="faec" class="np lu iq bd lv nq nr ns ly nt nu nv mb kf nw kg me ki nx kj mh kl ny km mk nz bi translated">Scala 中的 Spark DataFrame API</h1><p id="0c38" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在前面的章节中，我们已经包括了关于 RDD 以及如何使用 RDD 进行并行计算的例子。spark 中另一个流行的 API 是 Dataframe。从数据科学家的背景来看，DataFrame API 可能更有意义。然而，有一个主要的区别是如何火花数据帧和熊猫数据帧在引擎盖下操作。</p><blockquote class="mq mr ms"><p id="3f45" class="kw kx mt ky b kz la ka lb lc ld kd le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">Spark 数据帧可以存在于多个物理机器上，因此它们的计算是以分布式方式执行的，这与 pandas 数据帧相反。</p></blockquote><p id="a1c4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark MLlib 将在 DataFrame API 中实现。</p><p id="87ce" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们学习一些 Spark DataFrame API 的基本技巧，它们对机器学习任务非常有用。</p><p id="7df2" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通读 Google colab 的第 0 部分，了解以下片段的数据准备。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="c127" class="lt lu iq nd b gy nh ni l nj nk">// Read the .txt file<br/>val df_emps = spark.read.option("header", "true").csv(data_dir + "employee.txt")</span><span id="8566" class="lt lu iq nd b gy nl ni l nj nk">// print the schema<br/>df_emps.printSchema()</span><span id="680c" class="lt lu iq nd b gy nl ni l nj nk">// show top 10 records similar to df.head(10) in pandas<br/>df_emps.show(10, false)</span></pre><p id="9c0b" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读第二张表格</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="4001" class="lt lu iq nd b gy nh ni l nj nk">// Read the .txt file<br/>val df_cr = spark.read.option("header", "true").csv(data_dir + "country_region.txt")</span><span id="045c" class="lt lu iq nd b gy nl ni l nj nk">// print the schema<br/>df_cr.printSchema()</span><span id="68b5" class="lt lu iq nd b gy nl ni l nj nk">// show top 10 records similar to df.head(10) in pandas<br/>df_cr.show(10, false)</span></pre><p id="a6b4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读第三张表格</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="e108" class="lt lu iq nd b gy nh ni l nj nk">// Read the .txt file<br/>val df_dd = spark.read.option("header", "true").csv(data_dir + "dept_div.txt")</span><span id="7e1b" class="lt lu iq nd b gy nl ni l nj nk">// print the schema<br/>df_dd.printSchema()</span><span id="0561" class="lt lu iq nd b gy nl ni l nj nk">// show top 10 records similar to df.head(10) in pandas<br/>df_dd.show(10, false)</span></pre><p id="7a6a" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">合并是 Spark 相比熊猫最常见最高效的操作之一。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="45f9" class="lt lu iq nd b gy nh ni l nj nk">// Merge all three tables<br/>val df_joined = df_emps.join(df_cr, "region_id").join(df_dd, "department")</span></pre><h1 id="b0ee" class="np lu iq bd lv nq nr ns ly nt nu nv mb kf nw kg me ki nx kj mh kl ny km mk nz bi translated">谷歌 Colab 笔记本</h1><p id="7234" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我准备了一个功能性的 Google colab 笔记本。请随意使用笔记本进行练习。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><blockquote class="mq mr ms"><p id="cda5" class="kw kx mt ky b kz la ka lb lc ld kd le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">有关更多示例，请参考 Spark 官方文档</p></blockquote><h1 id="27ce" class="np lu iq bd lv nq nr ns ly nt nu nv mb kf nw kg me ki nx kj mh kl ny km mk nz bi translated">现实项目结论</h1><p id="0828" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在这一集中，我们学习了 Scala 的 Spark 基础知识，并通过练习涵盖了以下关键概念。</p><ul class=""><li id="a974" class="oc od iq ky b kz la lc ld lf oe lj of ln og lr oh oi oj ok bi translated">在 Google Colab 中运行 Scala</li><li id="eb01" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated">火花的基础</li><li id="72d1" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated">Spark 的 RDD 与 Scala</li><li id="7b94" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated">Sparks 的 Scala 数据框架 API</li></ul><p id="4480" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一集，我们将学习使用 Google Colab 运行时的 Spark 和 Scala 的机器学习模型。</p><h1 id="a935" class="np lu iq bd lv nq nr ns ly nt nu nv mb kf nw kg me ki nx kj mh kl ny km mk nz bi translated">参考资料/阅读/链接</h1><p id="abbc" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae ls" href="https://medium.com/@shadaj/machine-learning-with-scala-in-google-colaboratory-e6f1661f1c88" rel="noopener">在谷歌联合实验室用 Scala 进行机器学习</a></p><p id="c3ba" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scala Docs-【https://docs.scala-lang.org/tour/tour-of-scala.html T4】</p><p id="8673" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae ls" href="https://www.lynda.com/Scala-tutorials/Scala-Essential-Training-Data-Science/559182-2.html" rel="noopener ugc nofollow" target="_blank">https://www . Lynda . com/Scala-tutorials/Scala-Essential-Training-Data-Science/559182-2 . html</a></p><div class="oq or gp gr os ot"><a href="http://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd ja gy z fp oy fr fs oz fu fw iz bi translated">入门指南</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">Spark 2.0 中的 SparkSession 提供了对 Hive 特性的内置支持，包括使用…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">spark.apache.org</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ku ot"/></div></div></a></div><p id="38e4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae ls" href="https://databricks.com/p/ebook/apache-spark-under-the-hood" rel="noopener ugc nofollow" target="_blank">https://databricks.com/p/ebook/apache-spark-under-the-hood</a></p></div></div>    
</body>
</html>