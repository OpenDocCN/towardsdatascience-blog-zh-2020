<html>
<head>
<title>Building a One-shot Learning Network with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch构建一次性学习网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a?source=collection_archive---------4-----------------------#2020-05-30">https://towardsdatascience.com/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a?source=collection_archive---------4-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c913" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们如何用每门课这么少的样本建立一个深度网络？</h2></div><p id="9106" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">近年来，深度学习由于其高性能而在图像识别和分类任务中非常流行。然而，传统的深度学习方法通常需要一个大型数据集来训练模型，以区分非常少的不同类别，这与人类能够从甚至非常少的例子中学习的方式截然不同。</p><p id="802c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">少击或单击学习是一个分类问题，旨在只给定有限数量的样本对对象进行分类，最终目标是创建一个更像人类的学习算法。在本文中，我们将通过使用一种特殊的网络结构:暹罗网络，深入研究解决一次性学习问题的深度学习方法。我们将使用PyTorch构建网络，并在Omniglot手写字符数据集上对其进行测试，并使用一次性学习评估指标进行多次实验，以比较不同网络结构和超参数的结果。</p><h1 id="14b8" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak"> Omniglot数据集</strong></h1><p id="25d2" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">Omniglot手写字符数据集是由Lake等人提出的用于一次性学习的数据集。它包含来自50个不同系列的字母表的1623个不同的手写字符，其中每个字符由20个不同的人手写。每个图像的大小为105x105像素。这50个字母被分成30:20的比例进行训练和测试，这意味着测试集是在一个全新的字符集上，这是以前从未见过的。</p><h1 id="2bed" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">计算环境</h1><p id="1df1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">训练和实验完全是通过谷歌实验室完成的，使用了包括特斯拉K80和P100在内的一系列GPU。我们使用的库包括Numpy、Matplotlib和PyTorch。</p><h1 id="851d" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">方法</h1><p id="3b7c" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">传统的深度网络通常不能很好地进行一次或几次学习，因为每类很少的样本很可能导致过拟合。为了防止过度拟合问题，并将其扩展到看不见的字符，我们建议使用暹罗网络。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/475e3984e8bbb43e1a30170b876410c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILChjgpuRoa9BG5OMTrGRQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图一。卷积暹罗网络架构</p></figure><p id="4d6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图一。是卷积暹罗网络的主干架构。与传统的CNN采用1幅图像的输入来生成暗示图像所属类别的独热向量不同，Siamese网络采用2幅图像并将它们馈入具有相同结构的2个CNN。输出将被合并在一起，在这种情况下是通过它们的绝对差异，并被馈送到完全连接的层，以输出一个表示两幅图像相似性的数字。数字越大，表示两幅图像越相似。</p><p id="f19a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">暹罗网络不是学习哪个图像属于哪个类，而是学习如何确定两个图像之间的“相似性”。在训练之后，给定一个全新的图像，网络然后可以将该图像与来自每个类别的图像进行比较，并确定哪个类别与给定的图像最相似。</p><h1 id="cd2f" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">数据集预处理和生成</h1><h2 id="b1ea" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">训练和验证数据加载器</strong></h2><p id="380f" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">为了训练暹罗网络，我们必须首先生成适当的输入(成对)并为模型定义基础事实标签。</p><p id="5562" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先定义两幅图像，它们来自相同字母表中的相同字符，相似度为1，否则为0，如图3所示。然后，我们根据dataloader迭代中索引的奇偶性，随机选择一对图像输入到网络中。换句话说，如果当前迭代是奇数，我们从同一个字符检索一对图像，反之亦然。这确保了我们的训练数据集对于两种类型的输出都是平衡的。两幅图像经历相同的图像转换，因为目标是确定两幅图像的相似性，所以将它们输入不同的图像转换没有意义。</p><p id="37fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是生成训练集的代码:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="a57e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创建了10000对这些数据作为我们的训练集，然后以80:20的比例随机地进一步分成训练和验证。</p><h2 id="7516" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">测试加载器</strong></h2><p id="dc5e" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">对网络在一次学习中的性能的评估可以通过n路一次学习评估度量来完成，其中我们找到代表n个类别的n个图像和属于n个类别之一的一个主图像。对于我们的暹罗网络，我们计算了主图像相对于所有n个图像的相似性，具有最高相似性的对意味着主图像属于该类。</p><p id="0e48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试加载器是以支持上述评估的方式构建的，其中随机获取了一个主图像，并且还检索了代表<em class="nf"> n </em>个类别的<em class="nf"> n </em>个图像，其中一个图像来自主图像的相同类别。</p><p id="8bd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是用于生成测试集的代码:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="299a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的最终测试，我们将我们的网络扩展到4路一次性学习，测试集大小为1000，20路为200。</p><h1 id="c89d" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">实验</h1><h2 id="078f" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">实验一。用于一次性学习的传统连体网络</strong></h2><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ng"><img src="../Images/4e6efe4c4181a76a249d060a4358b6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRzEp0SnqyIgGfLz33HwUw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图二。Koch等人的暹罗网络架构。</p></figure><p id="13fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">暹罗网络的主要部分是前面显示的双卷积架构。我们将尝试构建的第一个卷积架构来自Koch等人的论文“用于一次性图像识别的暹罗神经网络”，如图2所示。需要注意的一点是，在展平之后，两个卷积分支之间的绝对差异被馈送到全连接层，而不仅仅是一个图像的输入。</p><p id="ffc5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyTorch中的网络构建如下:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用以下功能进行培训:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="a668" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">超参数设置</strong></p><p id="ca01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批量大小:因为我们正在学习两幅图像有多相似，所以批量大小需要相当大，以便模型具有通用性，特别是对于像这样有许多不同类别的数据集。因此，我们使用的批量大小为128。</p><p id="05f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习率:我们测试了从0.001到0.0005的几个学习率，并选择了0.0006，它提供了最好的损失减少率。</p><p id="5e0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优化器和损耗:我们采用传统的Adam优化器来优化该网络，并使用logits进行二进制交叉熵(BCE)损耗。</p><p id="3829" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结果</strong></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nh"><img src="../Images/f7d551ee67375db5a5cb7dc7a5cc3233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qBD-m-TY5Wd_WgwcN6TNag.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图3。原始网络的训练和验证损失</p></figure><p id="ac3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该网络被训练30个时期。图3。是每个时期后的训练和验证损失图，正如我们所看到的，它显示了接近结束时的急剧下降和收敛。验证损失通常随着训练损失而减少，表明在整个训练中没有发生过拟合。在训练期间，将保存具有最低验证损失的模型。我们使用验证损失而不是训练损失，因为它是模型不仅仅对训练集表现良好的指标，这可能是过度拟合的情况。</p><h2 id="7c07" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">实验二。添加批量标准化</strong></h2><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ni"><img src="../Images/18a22da74b0982a89ae587c5da98537b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5xI4i2QBxtWY9MDNGZ3Ng.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图4。使用BatchNorm的模型架构</p></figure><p id="8ab1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进一步改善网络，我们可以添加批量标准化，这应该会使收敛过程更快、更稳定。图4是更新后的架构，每个卷积层后都有一个BatchNorm2d。</p><p id="edf4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结果</strong></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1e2c2615c0114546881e337daebcd86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*vHvK-Ho-I62oXr80MiUptw.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图4。10个时期后的训练结果</p></figure><p id="fac5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如预期的那样，与原始网络相比，训练损失和验证损失都下降得更快。有了更好的结果，我们决定也训练模型更多的时期，看看它是否会比实验1表现得更好。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bdb2fbc78adce469e6ac519fe8424b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*lgSeSVlEj7On-9HAeGK-YA.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图5。50个时期后的训练结果</p></figure><p id="d1ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如损失图所示，结果略好于实验1的原始结果。由于损失在40和50代之间缓慢收敛，我们在第50代停止了训练。这是目前我们取得的最好成绩。</p><h2 id="5db4" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">实验三。用轻型VGG16 </strong>替换ConvNet</h2><p id="ca62" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在让原来的网络工作得相当好之后，我们还可以为我们的暹罗网络测试不同的成熟CNN，看看我们是否能取得更好的结果。对于105x105的小图像大小，我们希望使用一个相对较小的网络，没有太多的层，但仍能产生不错的结果，因此我们借用了VGG16的网络架构。</p><p id="b752" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初的VGG16对于我们的尺寸来说仍然有点太大，其中最后5个卷积层只处理单个像素，所以我们取消了它们，最终网络如下:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="497c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结果</strong></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1b433efa0ad3b68c548aae4d3115e668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*MNENvDg5tgpEUQcULtTOwg.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图6。VGG16暹罗网络结果</p></figure><p id="935a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如损失图所示，训练损失比先前的实验下降得慢得多。这可能是由于卷积层的内核大小相当小(3×3)，这给出了小的感受域。对于计算两个图像之间的相似性的问题，查看两个图像的“更大画面”而不是关注小细节可能是有益的，因此在原始网络中提出的更大的感受野工作得更好。</p><h2 id="681d" class="mr lf it bd lg ms mt dn lk mu mv dp lo kr mw mx lq kv my mz ls kz na nb lu nc bi translated"><strong class="ak">对模型的评价</strong></h2><p id="eae3" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">评估网络的代码实现如下:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="d23a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">四路一次学习</strong></p><p id="adc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先使用一组全新的图像来测试4向一次学习进行评估，其中所有的测试图像在训练期间都没有使用，并且模型也不知道任何角色。结果显示大约90%的准确率，这表明该模型可以很好地推广到看不见的数据集和类别，实现了我们在Omniglot数据集上一次性学习的目标。</p><p id="ff07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 20路一次学习</strong></p><p id="cbf6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我们对200组进行了20路单镜头学习评估。结果仍然是86%左右。我们将结果与Lake等人提供的基线进行了比较:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/3f2481e58ec048994929ee3cb2a09902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*dX5heQphkHkYcvNE3IM8Mw.jpeg"/></div></figure><p id="d48b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我们没有超过或复制论文提出的92%的准确性(可能是由于细节，如变化的层学习率)，但我们实际上非常接近它。</p><p id="70e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们的模型实际上比许多其他模型表现得更好，包括正常的暹罗网络和最近邻居。</p><h1 id="f924" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="0180" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">所以你有它！这就是如何为一次性学习Omniglot数据集构建卷积暹罗网络。完整代码也发布在Github的以下目录中:</p><div class="nn no gp gr np nq"><a href="https://github.com/ttchengab/One_Shot_Pytorch.git" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">ttchengab/One_Shot_Pytorch</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">设置:从https://github.com/brendenlake/omniglot下载数据集，转到/python并提取文件夹…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">github.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ml nq"/></div></div></a></div><p id="9811" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">感谢您坚持到现在</em>🙏！<em class="nf">我将发布更多关于计算机视觉/深度学习不同领域的文章，请务必查看我关于3D重建的另一篇文章！</em></p></div></div>    
</body>
</html>