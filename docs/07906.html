<html>
<head>
<title>BERT Text Classification Using Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch的BERT文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b?source=collection_archive---------0-----------------------#2020-06-12">https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b?source=collection_archive---------0-----------------------#2020-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9355" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Huggingface库提供的BERT分类任何文本</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/5b6fefebec3b2b5a4e872b666e6a8404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9sCzwwfNgvhKFNSH"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">照片由<a class="ae lf" href="https://unsplash.com/@clemhlrdt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克莱门特·H</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="0e0d" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="213a" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><strong class="ma iu">文本分类</strong>是自然语言处理中最常见的任务之一。它被广泛应用于各种应用中，包括情感分析、垃圾邮件过滤、新闻分类等。在这里，我们向您展示如何使用最先进的模型来检测假新闻(将文章分类为真实或虚假)，这是一个可以扩展到任何文本分类任务的教程。</p><p id="20b3" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae lf" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu">变压器</strong> </a>是当前最先进的NLP架构的基本构建模块。它的主要优势是它的多头注意力机制，与以前的竞争模型(如递归神经网络)相比，它可以提高性能并显著提高并行性。在本教程中，我们将使用预先训练好的<a class="ae lf" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu"> BERT </strong> </a>，这是最流行的变形金刚模型之一，并在假新闻检测上对其进行微调。在后面的教程中，我还使用了一个<a class="ae lf" rel="noopener" target="_blank" href="/lstm-text-classification-using-pytorch-2c6c657f8fc0"> <strong class="ma iu"> LSTM </strong>来完成同样的任务，如果感兴趣的话，请检查一下！</a></p><p id="4cd2" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这篇文章的<strong class="ma iu">主要源代码</strong>可以在<a class="ae lf" href="https://colab.research.google.com/drive/1P4Hq0btDUDOTGkCHGzZbAx1lb0bTzMMa?usp=sharing" rel="noopener ugc nofollow" target="_blank">这个Google Colab笔记本</a>中找到。</p><p id="0992" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma iu">预处理代码</strong>在<a class="ae lf" href="https://colab.research.google.com/drive/1xqkvuNDg0Opk-aZpicTVPNY4wUdC7Y7v?usp=sharing" rel="noopener ugc nofollow" target="_blank">这款Google Colab笔记本</a>中也有。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="b789" class="lg lh it bd li lj mz ll lm ln na lp lq jz nb ka ls kc nc kd lu kf nd kg lw lx bi translated"><strong class="ak">入门</strong></h1><p id="adf0" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><a class="ae lf" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>是用Python实现最新变形器的最知名的库。它提供了清晰的文档和教程，介绍了如何为各种不同的任务实现几十种不同的转换器。我们将使用<a class="ae lf" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>，因此请确保Pytorch已安装。在确保安装了相关库之后，您可以通过以下方式安装transformers库:</p><p id="ebae" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><code class="fe ne nf ng nh b">pip install transformers</code></p><p id="69fa" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于数据集，我们将使用来自Kaggle的真实和虚假新闻数据集<a class="ae lf" href="https://www.kaggle.com/nopdev/real-and-fake-news-dataset" rel="noopener ugc nofollow" target="_blank"><strong class="ma iu">。</strong></a></p><h1 id="a99c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤1:导入库</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="8141" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这里要注意的最重要的库是，我们导入了BERTokenizer和BERTSequenceClassification，以便稍后构造记号化器和模型。</p><h1 id="67e4" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤2:预处理和准备数据集</h1><p id="b8b3" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在原始数据集中，我们添加了一个额外的TitleText列，它是标题和文本的串联。我们想用标题和正文来检验一篇文章是否是假的。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="ac1c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于记号赋予器，我们使用“bert-base-uncased”版本的BertTokenizer。使用<a class="ae lf" href="https://pytorch.org/text/" rel="noopener ugc nofollow" target="_blank"> TorchText </a>，我们首先创建文本字段和标签字段。文本字段将用于包含新闻文章，标签是真正的目标。我们将每篇文章限制为BERT输入的前128个标记。然后，我们从数据集csv文件中创建一个TabularDataset，使用这两个字段来生成训练集、验证集和测试集。然后我们创建迭代器来批量准备它们。</p><p id="2566" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><em class="nk">注意:为了在TorchText中使用BERT tokenizer，我们必须设置</em> <code class="fe ne nf ng nh b"><em class="nk">use_vocab=False</em></code> <em class="nk">和</em> <code class="fe ne nf ng nh b"><em class="nk">tokenize=tokenizer.encode</em></code> <em class="nk">。这将让TorchText知道，我们不会使用我们的数据集从头开始构建我们自己的词汇表，而是使用预先训练的BERT标记器及其相应的单词到索引的映射。</em></p><h1 id="e77e" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤3:构建模型</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="7a31" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们使用的是“bert-base-uncased”版本的bert，这是一个针对小写英文文本训练的较小模型(具有12层、768个隐藏、12个头、110M参数)。查看<a class="ae lf" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank"> Huggingface的文档</a>了解其他版本的BERT或其他变压器模型。</p><h1 id="d41e" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第四步:培训</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="c685" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们分别为模型检查点和训练指标编写保存和加载函数。请注意，模型检查点的保存功能并不保存优化器。我们不保存优化器，因为优化器通常会占用非常大的存储空间，并且我们假设不需要从以前的检查点进行训练。训练度量存储训练损失、验证损失和全局步骤，以便以后可以进行关于训练过程的可视化。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="db4c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们使用Adam优化器和合适的学习率来调整5个时期的BERT。</p><p id="28b6" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们使用二进制交叉熵作为损失函数，因为假新闻检测是两类问题。在计算目标与其自身之间的损耗之前，确保输出通过Sigmoid。</p><p id="9253" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在训练期间，我们根据验证集评估我们的模型参数。每次验证损失减少时，我们保存模型，以便我们最终得到具有最低验证损失的模型，该模型可以被认为是最佳模型。以下是培训期间的输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nl"><img src="../Images/0c88e3d0e46419b9a51c4eee51650399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGvaNX5NN6tGtLR-y7vXbA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="nm">作者图片</em></p></figure><p id="0061" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">训练之后，我们可以使用下面的代码绘制一个图表:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nn"><img src="../Images/73bc90b1f0021f736376a295e08cf365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9RNf-f_sU6FxIDJcTV18A.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="nm">作者图片</em></p></figure><h1 id="466a" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第五步:评估</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="c4e4" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于评估，我们使用我们训练的模型预测文章，并针对真实标签进行评估。我们打印出分类报告，其中包括测试准确度，精确度，召回，F1分数。我们还打印出混淆矩阵，以查看我们的模型对每个类正确和不正确地预测了多少数据。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nl"><img src="../Images/b085ac87063b92341393dfec1dfb1207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ha5MmEDKWtMadx0eNnXjSQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="nm">作者图片</em></p></figure><p id="6531" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在对我们的模型进行评估后，我们发现我们的模型达到了令人印象深刻的96.99%的准确率！</p><h1 id="f1be" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论</h1><p id="b635" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们发现微调BERT在我们的数据集上表现非常好，并且由于开源的Huggingface Transformers库，实现起来非常简单。这可以毫无困难地扩展到任何文本分类数据集。</p><p id="2814" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这里是我写的其他文章，如果感兴趣的话😊：</p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/lstm-text-classification-using-pytorch-2c6c657f8fc0"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">基于Pytorch的LSTM文本分类</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">一步一步的指导你如何在Pytorch中建立一个双向LSTM！</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of kz nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">使用Pytorch微调用于文本生成的GPT2</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">使用Pytorch和Huggingface微调用于文本生成的GPT2。我们在CMU图书摘要数据集上进行训练，以生成…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="og l oc od oe oa of kz nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/controlling-text-generation-from-language-models-6334935e80cf"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">控制语言模型的文本生成</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">控制机器生成文本的样式和内容的实际操作方法</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="oh l oc od oe oa of kz nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/@itsuncheng/best-free-resources-that-computer-science-students-should-definitely-know-d148c51b956e" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">计算机专业学生应该知道的最佳免费资源</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">有效学习的最重要的事情之一是拥有合适的资源，这不是一个简单的过程</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="oi l oc od oe oa of kz nr"/></div></div></a></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="35ed" class="lg lh it bd li lj mz ll lm ln na lp lq jz nb ka ls kc nc kd lu kf nd kg lw lx bi translated">参考</h1><p id="faa1" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">[1] A .瓦斯瓦尼，n .沙泽尔，n .帕尔马等。、<a class="ae lf" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)，第31届神经信息处理系统会议</p><p id="d5c7" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[2] J. Devlin，M. Chang，K. Lee和K. Toutanova，<a class="ae lf" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2019)，计算语言学协会北美分会2019年年会</p></div></div>    
</body>
</html>