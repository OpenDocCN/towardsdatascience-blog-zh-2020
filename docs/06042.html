<html>
<head>
<title>k-NN on Iris Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">虹膜数据集上的 k-NN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nn-on-iris-dataset-3b827f2591e?source=collection_archive---------12-----------------------#2020-05-17">https://towardsdatascience.com/k-nn-on-iris-dataset-3b827f2591e?source=collection_archive---------12-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ce9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">k-最近邻(k-NN)是一种基于实例的监督学习算法，它通过将一个新实例与在训练中已经看到的存储器中已经存储的实例进行比较来对该新实例进行分类。</h2></div><p id="9d78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用以下步骤计算未知实例的类:</p><ol class=""><li id="1705" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">计算未知实例和所有其他训练实例之间的距离。</li><li id="6f05" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">k 个最近邻居被识别。</li><li id="4870" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">k 个最近邻居的类别标签被用于通过使用像多数投票这样的技术来确定未知实例的类别标签。</li></ol><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/2f619780c163f0d45ed9dadc2e633adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*0Pqqx6wGDfFm_7GLebg2Hw.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">k-NN 分类示例<a class="ae me" href="https://twitter.com/DataCamp/status/1037324501757976577/photo/1" rel="noopener ugc nofollow" target="_blank">(图像源)</a></p></figure><p id="42c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，在上面的图像中，如果 k 值为 3，分类器可以将未知实例分类为 B 类，如果 k 值为 7，则分类为 A 类。</p><p id="a5f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章关注的是使用 Iris 数据集进行 kNN 的超参数调优。然后，使用最佳超参数对测试集实例进行分类，并计算模型的最终精度。实现是从零开始的，不依赖于现有的 python 数据科学库。</p><p id="1a9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">调整的超参数有:</p><ol class=""><li id="1d23" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">距离度量:欧几里德、归一化欧几里德和余弦相似性</li><li id="c595" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">k 值:1、3、5 和 7</li></ol><p id="a74d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">欧几里德距离<br/> </strong>欧几里德空间中两点 p 和 q 之间的欧几里德距离计算如下:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/cc34c4aff8ca1bf1c0e1f6e5b2d9f4bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIWCm1mhqDaF57iNTQBVCg.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">(图像来源)</a></p></figure><p id="de12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">归一化欧几里德距离</strong> <br/>归一化欧几里德距离是指点经过归一化后，点与点之间的欧几里德距离。</p><p id="775d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">余弦相似度<br/> </strong>余弦相似度是两个非零向量之间的相似性度量。两个向量 A 和 B 之间的余弦相似度计算如下:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mk"><img src="../Images/15bdaa27a9f6842f515c7a61e0172091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ru5YapsWJTBQ6VrXwsIvAA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">(图片来源)</a></p></figure></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/34b35a30c4ab24bcc191d9f725f6e67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDMA0SDoM8efwYK0B6rWOQ.jpeg"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Iris_(plant)" rel="noopener ugc nofollow" target="_blank">(图片来源)</a></p></figure><p id="e937" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们进入用 Python 实现的分类器。为了便于理解，本节分为不同的步骤。</p><p id="f0db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第一步</strong>。</p><p id="2dc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">加载所需的库。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="be1c" class="my mz it mu b gy na nb l nc nd"><strong class="mu iu">import</strong> <strong class="mu iu">pandas</strong> <strong class="mu iu">as</strong> <strong class="mu iu">pd</strong><br/><strong class="mu iu">import</strong> <strong class="mu iu">numpy</strong> <strong class="mu iu">as</strong> <strong class="mu iu">np</strong><br/><strong class="mu iu">import</strong> <strong class="mu iu">operator</strong><br/><strong class="mu iu">import</strong> <strong class="mu iu">matplotlib.pyplot</strong> <strong class="mu iu">as</strong> <strong class="mu iu">plt</strong></span></pre><p id="11dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第二步。</strong></p><p id="6091" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将虹膜数据集加载到 Jupyter 笔记本中。数据集是 csv 格式的，可以很容易地使用<strong class="kk iu">熊猫</strong>库读入数据帧。数据集有四个属性萼片长度、萼片宽度、花瓣长度和花瓣宽度，以及每个实例的类标签。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="8852" class="my mz it mu b gy na nb l nc nd">data = pd.read_csv('iris.data', header=<strong class="mu iu">None</strong>, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])</span></pre><p id="2dc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据帧可以显示如下:</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="3a23" class="my mz it mu b gy na nb l nc nd">print(data)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ne"><img src="../Images/6ef339d2bae397242498c095f0046484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpUZnxXz9yomVJrXxnekUw.png"/></div></div></figure><p id="241c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第三步。</strong></p><p id="9bed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将数据集分为开发集和测试集。它可以通过随机化索引然后根据索引分割数据帧来划分。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="566e" class="my mz it mu b gy na nb l nc nd">indices = np.random.permutation(data.shape[0])<br/>div = int(0.75 * len(indices))<br/>development_id, test_id = indices[:div], indices[div:]<br/><br/>development_set, test_set = data.loc[development_id,:], data.loc[test_id,:]<br/>print("Development Set:<strong class="mu iu">\n</strong>", development_set, "<strong class="mu iu">\n\n</strong>Test Set:<strong class="mu iu">\n</strong>", test_set)</span></pre><p id="8e98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算开发集和测试集的平均值和标准偏差，以计算归一化的欧几里德距离。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="603e" class="my mz it mu b gy na nb l nc nd">mean_development_set = development_set.mean()<br/>mean_test_set = test_set.mean()<br/>std_development_set = development_set.std()<br/>std_test_set = test_set.std()</span></pre><p id="b0dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从开发和测试集中检索“class”列，并将其存储在单独的列表中。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="b87c" class="my mz it mu b gy na nb l nc nd">test_class = list(test_set.iloc[:,-1])<br/>dev_class = list(development_set.iloc[:,-1])</span></pre><p id="1bcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第四步。</strong></p><p id="d677" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义计算距离度量值的函数:欧几里德、归一化欧几里德和余弦相似性</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="baed" class="my mz it mu b gy na nb l nc nd"><strong class="mu iu">def</strong> euclideanDistance(data_1, data_2, data_len):<br/>    dist = 0<br/>    <strong class="mu iu">for</strong> i <strong class="mu iu">in</strong> range(data_len):<br/>        dist = dist + np.square(data_1[i] - data_2[i])<br/>    <strong class="mu iu">return</strong> np.sqrt(dist)<br/><br/><strong class="mu iu">def</strong> normalizedEuclideanDistance(data_1, data_2, data_len, data_mean, data_std):<br/>    n_dist = 0<br/>    <strong class="mu iu">for</strong> i <strong class="mu iu">in</strong> range(data_len):<br/>        n_dist = n_dist + (np.square(((data_1[i] - data_mean[i])/data_std[i]) - ((data_2[i] - data_mean[i])/data_std[i])))<br/>    <strong class="mu iu">return</strong> np.sqrt(n_dist)<br/><br/><strong class="mu iu">def</strong> cosineSimilarity(data_1, data_2):<br/>    dot = np.dot(data_1, data_2[:-1])<br/>    norm_data_1 = np.linalg.norm(data_1)<br/>    norm_data_2 = np.linalg.norm(data_2[:-1])<br/>    cos = dot / (norm_data_1 * norm_data_2)<br/>    <strong class="mu iu">return</strong> (1-cos)</span></pre><p id="fe58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义返回 k 个最近邻的函数</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="227e" class="my mz it mu b gy na nb l nc nd"><strong class="mu iu">def</strong> knn(dataset, testInstance, k, dist_method, dataset_mean, dataset_std): <br/>    distances = {}<br/>    length = testInstance.shape[1]<br/>    <strong class="mu iu">if</strong> dist_method == 'euclidean':<br/>        <strong class="mu iu">for</strong> x <strong class="mu iu">in</strong> range(len(dataset)):<br/>            dist_up = euclideanDistance(testInstance, dataset.iloc[x], length)<br/>            distances[x] = dist_up[0]<br/>    <strong class="mu iu">elif</strong> dist_method == 'normalized_euclidean':<br/>        <strong class="mu iu">for</strong> x <strong class="mu iu">in</strong> range(len(dataset)):<br/>            dist_up = normalizedEuclideanDistance(testInstance, dataset.iloc[x], length, dataset_mean, dataset_std)<br/>            distances[x] = dist_up[0]<br/>    <strong class="mu iu">elif</strong> dist_method == 'cosine':<br/>        <strong class="mu iu">for</strong> x <strong class="mu iu">in</strong> range(len(dataset)):<br/>            dist_up = cosineSimilarity(testInstance, dataset.iloc[x])<br/>            distances[x] = dist_up[0]<br/>    <em class="nf"># Sort values based on distance</em><br/>    sort_distances = sorted(distances.items(), key=operator.itemgetter(1))<br/>    neighbors = []<br/>    <em class="nf"># Extracting nearest k neighbors</em><br/>    <strong class="mu iu">for</strong> x <strong class="mu iu">in</strong> range(k):<br/>        neighbors.append(sort_distances[x][0])<br/>    <em class="nf"># Initializing counts for 'class' labels counts as 0</em><br/>    counts = {"Iris-setosa" : 0, "Iris-versicolor" : 0, "Iris-virginica" : 0}<br/>    <em class="nf"># Computing the most frequent class</em><br/>    <strong class="mu iu">for</strong> x <strong class="mu iu">in</strong> range(len(neighbors)):<br/>        response = dataset.iloc[neighbors[x]][-1] <br/>        <strong class="mu iu">if</strong> response <strong class="mu iu">in</strong> counts:<br/>            counts[response] += 1<br/>        <strong class="mu iu">else</strong>:<br/>            counts[response] = 1<br/>    <em class="nf"># Sorting the class in reverse order to get the most frequest class</em><br/>    sort_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=<strong class="mu iu">True</strong>)<br/>    <strong class="mu iu">return</strong>(sort_counts[0][0])</span></pre><p id="4857" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第五步。</strong></p><p id="249f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用开发数据集，迭代所有开发数据实例，并计算每个 k 值和每个距离度量的类。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="99c1" class="my mz it mu b gy na nb l nc nd"><em class="nf"># Creating a list of list of all columns except 'class' by iterating through the development set</em><br/>row_list = []<br/><strong class="mu iu">for</strong> index, rows <strong class="mu iu">in</strong> development_set.iterrows():<br/>    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       <br/>    row_list.append([my_list])<br/><em class="nf"># k values for the number of neighbors that need to be considered</em><br/>k_n = [1, 3, 5, 7]<br/><em class="nf"># Distance metrics</em><br/>distance_methods = ['euclidean', 'normalized_euclidean', 'cosine']<br/><em class="nf"># Performing kNN on the development set by iterating all of the development set data points and for each k and each distance metric</em><br/>obs_k = {}<br/><strong class="mu iu">for</strong> dist_method <strong class="mu iu">in</strong> distance_methods:<br/>    development_set_obs_k = {}<br/>    <strong class="mu iu">for</strong> k <strong class="mu iu">in</strong> k_n:<br/>        development_set_obs = []<br/>        <strong class="mu iu">for</strong> i <strong class="mu iu">in</strong> range(len(row_list)):<br/>            development_set_obs.append(knn(development_set, pd.DataFrame(row_list[i]), k, dist_method, mean_development_set, std_development_set))<br/>        development_set_obs_k[k] = development_set_obs<br/>    <em class="nf"># Nested Dictionary containing the observed class for each k and each distance metric (obs_k of the form obs_k[dist_method][k])</em><br/>    obs_k[dist_method] = development_set_obs_k<br/><em class="nf">#print(obs_k)</em></span></pre><p id="173b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算开发集的准确性</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="2714" class="my mz it mu b gy na nb l nc nd"><em class="nf"># Calculating the accuracy of the development set by comparing it with the development set 'class' list created earlier</em><br/>accuracy = {}<br/><strong class="mu iu">for</strong> key <strong class="mu iu">in</strong> obs_k.keys():<br/>    accuracy[key] = {}<br/>    <strong class="mu iu">for</strong> k_value <strong class="mu iu">in</strong> obs_k[key].keys():<br/>        <em class="nf">#print('k = ', key)</em><br/>        count = 0<br/>        <strong class="mu iu">for</strong> i,j <strong class="mu iu">in</strong> zip(dev_class, obs_k[key][k_value]):<br/>            <strong class="mu iu">if</strong> i == j:<br/>                count = count + 1<br/>            <strong class="mu iu">else</strong>:<br/>                <strong class="mu iu">pass</strong><br/>        accuracy[key][k_value] = count/(len(dev_class))<br/><br/><em class="nf"># Storing the accuracy for each k and each distance metric into a dataframe</em><br/>df_res = pd.DataFrame({'k': k_n})<br/><strong class="mu iu">for</strong> key <strong class="mu iu">in</strong> accuracy.keys():<br/>    value = list(accuracy[key].values())<br/>    df_res[key] = value<br/>print(df_res)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ng"><img src="../Images/7ccd242a3f7bfe9acf11dce0c8d394c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-IuwufyaRXcvA2abLTE5Q.png"/></div></div></figure><p id="c7f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">绘制条形图以比较超参数的性能。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="e16b" class="my mz it mu b gy na nb l nc nd"><em class="nf"># Plotting a Bar Chart for accuracy</em><br/>draw = df_res.plot(x='k', y=['euclidean', 'normalized_euclidean', 'cosine'], kind="bar", colormap='YlGnBu')<br/>draw.set(ylabel='Accuracy')</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/7aacfc851c7962a36b440aee9689f90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00hiknvnZD0X1KEzulaILg.png"/></div></div></figure><p id="4f9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="nf">注意</em> </strong> <em class="nf">:如果 k=1 的精度值为 100%，则忽略 k=1，因为这通常意味着过拟合。将其替换为 numpy.nan. </em>的值</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="dcfe" class="my mz it mu b gy na nb l nc nd">df_res.loc[df_res['k'] == 1.0, ['euclidean', 'normalized_euclidean', 'cosine']] = np.nan</span></pre><p id="7ef7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第六步。</strong></p><p id="b3a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">找到最佳超参数。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="015a" class="my mz it mu b gy na nb l nc nd"><em class="nf"># In case the accuracy is the same for different k and different distance metric selecting the first of all the same</em><br/>column_val = [c <strong class="mu iu">for</strong> c <strong class="mu iu">in</strong> df_res.columns <strong class="mu iu">if</strong> <strong class="mu iu">not</strong> c.startswith('k')]<br/>col_max = df_res[column_val].max().idxmax(1)<br/>best_dist_method = col_max<br/>row_max = df_res[col_max].argmax()<br/>best_k = int(df_res.iloc[row_max]['k'])<br/><strong class="mu iu">if</strong> df_res.isnull().values.any():<br/>    print('<strong class="mu iu">\n\n\n</strong>Best k value is<strong class="mu iu">\033</strong>[1m', best_k, '<strong class="mu iu">\033</strong>[0mand best distance metric is<strong class="mu iu">\033</strong>[1m', best_dist_method, '<strong class="mu iu">\033</strong>[0m. Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting')<br/><strong class="mu iu">else</strong>:<br/>    print('<strong class="mu iu">\n\n\n</strong>Best k value is<strong class="mu iu">\033</strong>[1m', best_k, '<strong class="mu iu">\033</strong>[0mand best distance metric is<strong class="mu iu">\033</strong>[1m', best_dist_method, '<strong class="mu iu">\033</strong>[0m.')</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ni"><img src="../Images/25bf8272b314b6ac91eb129f5a6a474d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*plbLuCPtm_yPKe3MuJtziQ.png"/></div></div></figure><p id="aec3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第七步。</strong></p><p id="a260" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用测试集和最佳超参数来计算最终精度。</p><pre class="lt lu lv lw gt mt mu mv mw aw mx bi"><span id="3f8f" class="my mz it mu b gy na nb l nc nd"><em class="nf"># Creating a list of list of all columns except 'class' by iterating through the development set</em><br/>row_list_test = []<br/><strong class="mu iu">for</strong> index, rows <strong class="mu iu">in</strong> test_set.iterrows(): <br/>    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       <br/>    row_list_test.append([my_list])<br/>test_set_obs = []<br/><strong class="mu iu">for</strong> i <strong class="mu iu">in</strong> range(len(row_list_test)):<br/>    test_set_obs.append(knn(test_set, pd.DataFrame(row_list_test[i]), best_k, best_dist_method, mean_test_set, std_test_set))<br/><em class="nf">#print(test_set_obs)</em><br/><br/>count = 0<br/><strong class="mu iu">for</strong> i,j <strong class="mu iu">in</strong> zip(test_class, test_set_obs):<br/>    <strong class="mu iu">if</strong> i == j:<br/>        count = count + 1<br/>    <strong class="mu iu">else</strong>:<br/>        <strong class="mu iu">pass</strong><br/>accuracy_test = count/(len(test_class))<br/>print('Final Accuracy of the Test dataset is ', accuracy_test)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nj"><img src="../Images/98d706e5ff42bcf94e8550b446edda43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_ygAILLRegZfs-D_jWE2Q.png"/></div></div></figure><p id="ad7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用余弦相似性作为距离度量并且 k 值为 5，实现了大约 97%的最终精度。这个模型给出了非常好的结果。</p><p id="1211" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完整的代码可以在 GitHub <a class="ae me" href="https://github.com/ishita-kapur/data-mining/blob/master/kNN_iris.ipynb" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="b374" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]:余弦相似度<br/><a class="ae me" href="https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists" rel="noopener ugc nofollow" target="_blank"><em class="nf">https://stack overflow . com/questions/18424228/Cosine-Similarity-between-2-number-lists</em></a></p><p id="af8f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]:作为参考<br/><em class="nf">https://machine learning mastery . com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/</em></p></div></div>    
</body>
</html>