<html>
<head>
<title>A supervised or semi-supervised ULMFit model to Twitter US Airlines Sentiment Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Twitter 美国航空公司情感数据集的监督或半监督 ULMFit 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-supervised-or-semi-supervised-ulmfit-model-to-twitter-us-airlines-sentiment-dataset-db3a6550abdf?source=collection_archive---------29-----------------------#2020-08-15">https://towardsdatascience.com/a-supervised-or-semi-supervised-ulmfit-model-to-twitter-us-airlines-sentiment-dataset-db3a6550abdf?source=collection_archive---------29-----------------------#2020-08-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="64e7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Twitter 美国航空公司情感数据集的监督或半监督 ULMFit 模型</h2></div><p id="7cf4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">我们的任务是将类似 ULMFit (Ruder 等人，2018 年)的监督/半监督技术应用于 Twitter 美国航空公司情绪分析数据。<br/>这个问题是半监督的原因是，它首先是一种非监督的训练方式，然后通过在网络顶部添加一个分类器网络来微调网络。</p><blockquote class="lk ll lm"><p id="6b90" class="kf kg ln kh b ki kj jr kk kl km ju kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kh ir">我们使用 Twitter 美国航空公司数据集(</strong><a class="ae lr" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">https://www . ka ggle . com/crowd flower/Twitter-airline-情操</strong> </a> <strong class="kh ir"> ) </strong></p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/797f0ccd737111dcfd51e986ab85050e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*431w4-u_NtDMIlGo"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated"><a class="ae lr" href="https://unsplash.com/photos/rf6ywHVkrlY" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/rf6ywHVkrlY</a></p></figure></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><p id="d6b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我们将从</span> <strong class="kh ir">开始:</strong></p><ul class=""><li id="c283" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">浏览数据集，为模型进行预处理和准备</li><li id="c7cb" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">探索情感分析的历史</li><li id="5c3f" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">探索语言模型及其重要性</li><li id="0e56" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">设置基线模型</li><li id="3c46" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">探索文本分类技术</li><li id="735f" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">ULMFit 概述</li><li id="7128" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">将 ULMFIT 应用于 Twitter 美国航空公司数据</li><li id="6e84" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">结果和预测</li><li id="f825" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">结论和未来方向</li></ul><h1 id="e643" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">数据集</h1><p id="c250" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我们</span>将从探索数据集统计数据和执行所有强制特征转换开始。</p><ul class=""><li id="b38a" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">由于这是一个多类分类问题，我们将对目标变量进行编码。</li><li id="e731" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们将改变列的显示顺序</li><li id="0536" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们将执行基本的统计以从数据中获得一些洞察力</li><li id="14f3" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">最后，我们将把新的数据帧分成 df_train、df_val 和 df_test</li></ul><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="611e" class="of ne iq ob b gy og oh l oi oj"># Loading dataset</span><span id="39dd" class="of ne iq ob b gy ok oh l oi oj">df = pd.read_csv(DATA_DIR)</span><span id="6fb2" class="of ne iq ob b gy ok oh l oi oj"># LabelEncoder to change positive, negative and neutral to numbers (classes)</span><span id="1391" class="of ne iq ob b gy ok oh l oi oj">labelEncoder = LabelEncoder()</span><span id="c6f1" class="of ne iq ob b gy ok oh l oi oj">def cleanAscii(text):</span><span id="69d9" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="8af0" class="of ne iq ob b gy ok oh l oi oj">Remove Non ASCII characters from the dataset.</span><span id="eef1" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="a3db" class="of ne iq ob b gy ok oh l oi oj">text: str</span><span id="9806" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="3c0f" class="of ne iq ob b gy ok oh l oi oj">return ''.join(i for i in text if ord(i) &lt; 128)</span><span id="9cf6" class="of ne iq ob b gy ok oh l oi oj">def gather_texts_and_labels(df=None, test_size=0.15,random_state=42):</span><span id="b79e" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="595b" class="of ne iq ob b gy ok oh l oi oj">Gathers the text and the corresponding labels from the dataset and splits it.</span><span id="c9c7" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="07cc" class="of ne iq ob b gy ok oh l oi oj">df: Pandas DataFrame</span><span id="4e13" class="of ne iq ob b gy ok oh l oi oj">test_size: represents the test size</span><span id="3158" class="of ne iq ob b gy ok oh l oi oj">random_state: represents the random state</span><span id="0a1d" class="of ne iq ob b gy ok oh l oi oj">Returns:</span><span id="d7a2" class="of ne iq ob b gy ok oh l oi oj">(x_train, x_test, y_train, y_test, new_df)</span><span id="43a9" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="3e83" class="of ne iq ob b gy ok oh l oi oj"># texts</span><span id="8dff" class="of ne iq ob b gy ok oh l oi oj">texts = df["text"].values</span><span id="d953" class="of ne iq ob b gy ok oh l oi oj"># encoding labels (positive, neutral, negative)</span><span id="bcd5" class="of ne iq ob b gy ok oh l oi oj">df['airline_sentiment'] = labelEncoder.fit_transform(df['airline_sentiment'])</span><span id="3a8e" class="of ne iq ob b gy ok oh l oi oj">labels = df['airline_sentiment'].values</span><span id="2259" class="of ne iq ob b gy ok oh l oi oj"># changing the order for fastai tokenizers to capture data.</span><span id="f255" class="of ne iq ob b gy ok oh l oi oj">new_df = pd.DataFrame(data={"label":labels, "text":texts})</span><span id="cf06" class="of ne iq ob b gy ok oh l oi oj">df_train, df_test = train_test_split(new_df, stratify = new_df['label'], test_size=test_size, random_state = random_state)</span><span id="a7f6" class="of ne iq ob b gy ok oh l oi oj">df_train, df_val = train_test_split(df_train, stratify = df_train['label'], test_size = test_size,</span><span id="e280" class="of ne iq ob b gy ok oh l oi oj">random_state = random_state)</span><span id="07a1" class="of ne iq ob b gy ok oh l oi oj">print("Training: {}, Testing: {}, Val: {}".format(len(df_train), len(df_test), len(df_val)))</span><span id="2557" class="of ne iq ob b gy ok oh l oi oj">return df_train, df_test, df_val,new_df</span><span id="4d63" class="of ne iq ob b gy ok oh l oi oj">def describe_dataset(df=None):</span><span id="2c9c" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="bb87" class="of ne iq ob b gy ok oh l oi oj">Describes the dataset</span><span id="3bac" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="76a8" class="of ne iq ob b gy ok oh l oi oj">df: Pandas Dataframe</span><span id="e684" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="c03a" class="of ne iq ob b gy ok oh l oi oj">print(df["airline_sentiment"].value_counts())</span><span id="d40b" class="of ne iq ob b gy ok oh l oi oj">print(df["airline"].value_counts())</span><span id="274d" class="of ne iq ob b gy ok oh l oi oj">print("\nMean airline_sentiment_confidence is {}".format(df.airline_sentiment_confidence.mean()))</span><span id="dc8d" class="of ne iq ob b gy ok oh l oi oj"># Optional</span><span id="bc91" class="of ne iq ob b gy ok oh l oi oj">def add_negativereason_to_text(df=None):</span><span id="2d9f" class="of ne iq ob b gy ok oh l oi oj"># change negativereason to "" if NaN else remain as is.</span><span id="7b27" class="of ne iq ob b gy ok oh l oi oj">df['negativereason'] = df['negativereason'].apply(lambda x: "" if pd.isna(x) else x)</span><span id="1aab" class="of ne iq ob b gy ok oh l oi oj"># add negativereason to text</span><span id="6468" class="of ne iq ob b gy ok oh l oi oj">df['text'] = df['text'] + df['negativereason']</span><span id="91d0" class="of ne iq ob b gy ok oh l oi oj">add_negativereason_to_text(df)</span><span id="4ac4" class="of ne iq ob b gy ok oh l oi oj">df['text'] = df['text'].apply(cleanAscii)</span><span id="88be" class="of ne iq ob b gy ok oh l oi oj">describe_dataset(df)</span><span id="30f3" class="of ne iq ob b gy ok oh l oi oj">df_train, df_test, df_val, new_df = gather_texts_and_labels(df)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7d5503bb4bb6621322caca254aca2d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*015NyYg1wqZ1fzVPH-uMLw.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">数据的统计</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi om"><img src="../Images/1840dd010fdb86ca4b5cafb1bfea961d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GwXxznoOOsHrwiUZR4_aA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">一些基本功能</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi on"><img src="../Images/53bbb209230d5ba8ab13b254a6716be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*mwB-giTNHTJF7RmcVgRIQg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">一些视觉统计</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oo"><img src="../Images/68f2401ee878885516ba3a465a266696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A1HD7k8AZkTjBLa-Ehp3w.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">更多统计数据</p></figure><blockquote class="op"><p id="5c44" class="oq or iq bd os ot ou ov ow ox oy la dk translated"><strong class="ak">我们将依靠不同的指标来衡量模型的性能(</strong>精度<strong class="ak">、召回率、F1 得分)</strong>。</p></blockquote><p id="7b7d" class="pw-post-body-paragraph kf kg iq kh b ki oz jr kk kl pa ju kn ko pb kq kr ks pc ku kv kw pd ky kz la ij bi translated">历史</p><h1 id="9086" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">历史</h1><p id="a430" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> B </span>在 ULMFit (2018)或 NLP 中的迁移学习之前准确地说，我们使用 word2Vec 或 GLove 等单词嵌入将单词表示为密集稀疏向量表示。通常，我们使用嵌入层作为模型中的第一层，然后根据我们的需要附加一个分类器。这使得该系统很难训练，因为它需要大量的数据。这些语言模型是早期的统计 LMs，使用概率分布来表示单词。(《由公司一言保管》)。</p><ul class=""><li id="e6a7" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">ULMfit，BERT，Universal sentence encoder，OpenAI GPT-2 使用一种叫做神经语言模型的东西来以分布式方式表示单词，并允许微调一个大型预训练语言模型来帮助我们完成任务。</li><li id="d5e1" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">具体来说，ULMfit (2018)引入了三种新技术来微调预训练语言模型</li><li id="7636" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">微调是计算机视觉中的一种流行方法，虽然这种方法在 NLP 上尝试过，但结果证明这种方法在 ULMFit 之前是错误的。</li></ul><blockquote class="lk ll lm"><p id="f1c9" class="kf kg ln kh b ki kj jr kk kl km ju kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">在本文的后面，我们将看到语言模型和分类器的概述。</p></blockquote><h1 id="c476" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">设定基线</h1><p id="e5f9" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> B </span>在任何机器学习实验之前，我们都应该设立一个基线，并与我们的结果进行比较。</p><p id="dd0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了建立基线，<strong class="kh ir">我们将使用 word2vec 嵌入矩阵来尝试预测情绪。</strong></p><ul class=""><li id="25df" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">为了加载我们的 word2vec，我们将使用嵌入层，然后是基本的前馈神经网络来预测情绪。</li></ul><p id="c9a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">我们也可以加载一个预先训练好的 word2vec 或 glove 嵌入，并输入到我们的嵌入层中。<br/>我们可以在嵌入层之后使用 LSTM 或 CNN，然后激活 softmax。</strong></p><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="78a4" class="of ne iq ob b gy og oh l oi oj"># The word2vec requires sentences as list of lists.</span><span id="8400" class="of ne iq ob b gy ok oh l oi oj">texts = df['text'].apply(cleanAscii).values</span><span id="ab80" class="of ne iq ob b gy ok oh l oi oj">tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000, oov_token='&lt;OOV&gt;')</span><span id="b307" class="of ne iq ob b gy ok oh l oi oj"># fitting</span><span id="51ec" class="of ne iq ob b gy ok oh l oi oj">tokenizer.fit_on_texts(texts)</span><span id="5986" class="of ne iq ob b gy ok oh l oi oj">vocab_size = len(tokenizer.word_index) + 1</span><span id="970f" class="of ne iq ob b gy ok oh l oi oj"># max length to be padded (batch_size, 100)</span><span id="359d" class="of ne iq ob b gy ok oh l oi oj">max_length = 100</span><span id="aa44" class="of ne iq ob b gy ok oh l oi oj">train_text = tokenizer.texts_to_sequences(df_train['text'].values)</span><span id="1995" class="of ne iq ob b gy ok oh l oi oj">test_text = tokenizer.texts_to_sequences(df_test['text'].values)</span><span id="23da" class="of ne iq ob b gy ok oh l oi oj"># getting the padded length of 100</span><span id="8e8c" class="of ne iq ob b gy ok oh l oi oj">padded_train_text = keras.preprocessing.sequence.pad_sequences(train_text, max_length, padding='post')</span><span id="a361" class="of ne iq ob b gy ok oh l oi oj">padded_test_text = keras.preprocessing.sequence.pad_sequences(test_text, max_length, padding='post')</span><span id="3bd0" class="of ne iq ob b gy ok oh l oi oj">labels_train = keras.utils.to_categorical(df_train['label'].values, 3)</span><span id="1ede" class="of ne iq ob b gy ok oh l oi oj">labels_test = keras.utils.to_categorical(df_test['label'].values, 3)</span><span id="d795" class="of ne iq ob b gy ok oh l oi oj">metrics = [</span><span id="a1c4" class="of ne iq ob b gy ok oh l oi oj">keras.metrics.Accuracy()</span><span id="5c6f" class="of ne iq ob b gy ok oh l oi oj">]</span><span id="90ea" class="of ne iq ob b gy ok oh l oi oj">net = Sequential()</span><span id="01eb" class="of ne iq ob b gy ok oh l oi oj"># return 50 dimension embedding representation with input_length as 100</span><span id="f595" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Embedding(vocab_size, 50, input_length=max_length))</span><span id="6127" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Flatten())</span><span id="ba5c" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Dense(512, activation='relu'))</span><span id="e20f" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Dense(3, activation='softmax'))</span><span id="d5cd" class="of ne iq ob b gy ok oh l oi oj">net.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=metrics)</span><span id="1037" class="of ne iq ob b gy ok oh l oi oj">net.summary()</span><span id="3861" class="of ne iq ob b gy ok oh l oi oj"># The word2vec requires sentences as list of lists.</span><span id="f31d" class="of ne iq ob b gy ok oh l oi oj">texts = df['text'].apply(cleanAscii).values</span><span id="ae21" class="of ne iq ob b gy ok oh l oi oj">tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000, oov_token='&lt;OOV&gt;')</span><span id="c83c" class="of ne iq ob b gy ok oh l oi oj"># fitting</span><span id="d9b7" class="of ne iq ob b gy ok oh l oi oj">tokenizer.fit_on_texts(texts)</span><span id="16d5" class="of ne iq ob b gy ok oh l oi oj">vocab_size = len(tokenizer.word_index) + 1</span><span id="6596" class="of ne iq ob b gy ok oh l oi oj"># max length to be padded (batch_size, 100)</span><span id="8211" class="of ne iq ob b gy ok oh l oi oj">max_length = 100</span><span id="0670" class="of ne iq ob b gy ok oh l oi oj">train_text = tokenizer.texts_to_sequences(df_train['text'].values)</span><span id="fcb5" class="of ne iq ob b gy ok oh l oi oj">test_text = tokenizer.texts_to_sequences(df_test['text'].values)</span><span id="c567" class="of ne iq ob b gy ok oh l oi oj"># getting the padded length of 100</span><span id="6422" class="of ne iq ob b gy ok oh l oi oj">padded_train_text = keras.preprocessing.sequence.pad_sequences(train_text, max_length, padding='post')</span><span id="c1ef" class="of ne iq ob b gy ok oh l oi oj">padded_test_text = keras.preprocessing.sequence.pad_sequences(test_text, max_length, padding='post')</span><span id="d1dc" class="of ne iq ob b gy ok oh l oi oj">labels_train = keras.utils.to_categorical(df_train['label'].values, 3)</span><span id="e199" class="of ne iq ob b gy ok oh l oi oj">labels_test = keras.utils.to_categorical(df_test['label'].values, 3)</span><span id="1f7b" class="of ne iq ob b gy ok oh l oi oj">metrics = [</span><span id="3482" class="of ne iq ob b gy ok oh l oi oj">keras.metrics.Accuracy()</span><span id="7413" class="of ne iq ob b gy ok oh l oi oj">]</span><span id="55fd" class="of ne iq ob b gy ok oh l oi oj">net = Sequential()</span><span id="89f0" class="of ne iq ob b gy ok oh l oi oj"># return 50 dimension embedding representation with input_length as 100</span><span id="cc9f" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Embedding(vocab_size, 50, input_length=max_length))</span><span id="dd88" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Flatten())</span><span id="9d7c" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Dense(512, activation='relu'))</span><span id="4b84" class="of ne iq ob b gy ok oh l oi oj">net.add(keras.layers.Dense(3, activation='softmax'))</span><span id="578d" class="of ne iq ob b gy ok oh l oi oj">net.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=metrics)</span><span id="e30d" class="of ne iq ob b gy ok oh l oi oj">net.summary()</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d7b440770a2c6710c3fb2d95498253ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*EC_GlUhxZ2C7ufoTWBlLeA.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">模型摘要</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi pf"><img src="../Images/dcc52e570e966e11b38c2135274455eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zMZFYXAhihOaxd9ydEADlg.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">培养</p></figure><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="8775" class="of ne iq ob b gy og oh l oi oj"># test the baseline model<br/>def test_baseline_sentiment(text):</span><span id="f07c" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="6434" class="of ne iq ob b gy ok oh l oi oj">Test the baseline model</span><span id="54c2" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="105a" class="of ne iq ob b gy ok oh l oi oj">text:str</span><span id="a5c5" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="6390" class="of ne iq ob b gy ok oh l oi oj">padded_text = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([text]), max_length, padding='post')</span><span id="166e" class="of ne iq ob b gy ok oh l oi oj">print(net.predict(padded_text).argmax(axis=1))</span><span id="cef6" class="of ne iq ob b gy ok oh l oi oj">net.evaluate(padded_test_text, labels_test)</span><span id="5957" class="of ne iq ob b gy ok oh l oi oj">preds = net.predict(padded_test_text).argmax(axis=1)</span></pre><blockquote class="lk ll lm"><p id="69c4" class="kf kg ln kh b ki kj jr kk kl km ju kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kh ir">如您所见，使用简单的前馈神经网络和嵌入层，我们很难达到 12%的精度</strong></p></blockquote><h1 id="32b8" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">加载语言模型和微调</h1><p id="5117" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated">astAI 为我们提供了一个易于使用的基于维基文本(AWD)的语言模型。</p><p id="34cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将从加载 LM 数据并使用所需的数据初始化它开始。</p><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="6324" class="of ne iq ob b gy og oh l oi oj">data_lm = TextLMDataBunch.from_df(train_df = df_train, valid_df = df_val, path = "")</span><span id="8c06" class="of ne iq ob b gy ok oh l oi oj"># Saving the data_lm as backup</span><span id="9718" class="of ne iq ob b gy ok oh l oi oj">data_lm.save("data_lm_twitter.pkl") # saving as a back stop</span><span id="e28f" class="of ne iq ob b gy ok oh l oi oj"># Loading the language model (AWD_LSTM)</span><span id="2d3f" class="of ne iq ob b gy ok oh l oi oj">learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)</span><span id="113a" class="of ne iq ob b gy ok oh l oi oj">print(learn)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi pg"><img src="../Images/344c7f3b8b45024f378a9bcd86025ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DwnoGaLoQsxd8iLI3heqw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">我们的样本数据</p></figure><blockquote class="op"><p id="5e21" class="oq or iq bd os ot ou ov ow ox oy la dk translated">正如你所看到的，fastai 库使用了 spacy tokenizer，所以除了删除 asci 字符之外，我们不对数据进行任何预处理。ULMFit 的作者在经验上很好地检验了标记化过程。</p></blockquote><h1 id="fbbf" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw ph jx np jz pi ka nr kc pj kd nt nu bi translated">培养</h1><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="3d92" class="of ne iq ob b gy og oh l oi oj"># Finding the optimal learning rate</span><span id="0684" class="of ne iq ob b gy ok oh l oi oj">learn.lr_find(start_lr=1e-8, end_lr=1e2)</span><span id="2138" class="of ne iq ob b gy ok oh l oi oj">learn.recorder.plot()</span><span id="fd2c" class="of ne iq ob b gy ok oh l oi oj"># Fit using one cycle policy</span><span id="c98a" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(1, 1e-2)</span><span id="9ebb" class="of ne iq ob b gy ok oh l oi oj"># Unfreeze all layers</span><span id="2865" class="of ne iq ob b gy ok oh l oi oj">learn.unfreeze()</span><span id="562f" class="of ne iq ob b gy ok oh l oi oj"># fit one cycle for 10 epochs</span><span id="8cbd" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))</span><span id="efba" class="of ne iq ob b gy ok oh l oi oj"># save the encoder</span><span id="3a63" class="of ne iq ob b gy ok oh l oi oj">learn.save_encoder('fine_tuned_enc') # we need the encoder in particular..FOr classifier</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/bf3dc0992003254be287bbe89b9cc9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*DfmnK9yzwlnx-3Qn_mQcJw.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">模型进度</p></figure><h1 id="c611" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">文本分类</h1><p id="6a10" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我们</span>现在创建添加我们的分类器在下面的网络(微调)。这是将指定的任务分类器添加到预训练语言模型中的最后一步</p><p id="0ffa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是<strong class="kh ir">逐步冻结步骤。</strong></p><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="1102" class="of ne iq ob b gy og oh l oi oj"># Preparing the classifier data</span><span id="96c4" class="of ne iq ob b gy ok oh l oi oj">data_clas = TextClasDataBunch.from_df(path = "", train_df = df_train, valid_df = df_val, test_df=df_test, vocab=data_lm.train_ds.vocab)</span><span id="352b" class="of ne iq ob b gy ok oh l oi oj"># Building the classifier</span><span id="fffc" class="of ne iq ob b gy ok oh l oi oj">learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)</span><span id="7bd9" class="of ne iq ob b gy ok oh l oi oj"># loading the saved encoder</span><span id="c2ec" class="of ne iq ob b gy ok oh l oi oj">learn.load_encoder('fine_tuned_enc') # load th encoder from the LM</span><span id="1b77" class="of ne iq ob b gy ok oh l oi oj"># slanted learning rate scheduler</span><span id="58da" class="of ne iq ob b gy ok oh l oi oj"># fine tuning the whole network</span><span id="ab52" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(3, 1e-2, moms=(0.8,0.7))  # you can of course train more, Jeremy promises its hard to over fit here :D</span><span id="7e3b" class="of ne iq ob b gy ok oh l oi oj"># fine tuning the network layer by layer to preserve as much information is possible.</span><span id="01ad" class="of ne iq ob b gy ok oh l oi oj">learn.freeze_to(-2) # unfreeze last 2 layers</span><span id="1e07" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(2, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))</span><span id="ee66" class="of ne iq ob b gy ok oh l oi oj">learn.freeze_to(-3) # unfreeze last 3 layers</span><span id="7b3d" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(2, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))</span><span id="5188" class="of ne iq ob b gy ok oh l oi oj">learn.freeze_to(-4) # unfreeze last 4 layers</span><span id="35c0" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(2, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))</span><span id="7940" class="of ne iq ob b gy ok oh l oi oj">learn.freeze_to(-5) # unfreeze last 5 layers</span><span id="5685" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(2, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))</span><span id="80aa" class="of ne iq ob b gy ok oh l oi oj"># Unfreezing all the layers and training</span><span id="32e7" class="of ne iq ob b gy ok oh l oi oj">learn.unfreeze() # unfreze all</span><span id="d6cc" class="of ne iq ob b gy ok oh l oi oj">learn.fit_one_cycle(3, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi pl"><img src="../Images/750318966a6eb5c2669c355fd8f64dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*4r-sRD9FwGCtdg1zoxLe6Q.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">我们达到了 94%的准确率</p></figure><h1 id="979c" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">ULMFit 概述</h1><p id="29b9" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated">ULMfit 流程的回顾</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/2ed2611a6a7026451d4144ae7a8d5b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*01L0lV-E_EDUBibV.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated"><a class="ae lr" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.06146</a></p></figure><p id="eeda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">不同类型的流程如下:</p><ul class=""><li id="b6eb" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">LM 预训练:这是我们遵循无监督学习来捕获大型语料库的语义和概率表示的步骤。(维基文本-103)</li><li id="8c00" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">LM 微调:这是我们通过使用某些新技术来微调 LM 的步骤。由于 AWD-LSTM(预训练模型)的每一层都捕获关于语料库的不同信息，我们首先微调最后一层，因为它包含最少的信息，而所有其他层都被冻结。然后，我们解冻所有其他层，用指定的任务重新训练模型。这样，我们不会丢失信息。通过使用倾斜的三角形学习率(模式为三角形的循环学习率)来完成训练。</li><li id="a017" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">最后一步是分类器微调，其中分类器模型附加到模型的顶部，并通过使用逐步解冻来训练，我们通过逐层解冻来训练模型。</li></ul><p id="3c7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> T </span> <em class="ln">这些技术是:</em></p><ul class=""><li id="4c17" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">区别微调</li><li id="8bd4" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">倾斜三角形学习率</li><li id="7859" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">逐渐冻结</li></ul><h1 id="5b45" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">乌尔菲特在推特上表达了美国航空公司的观点。(预测和准确性)</h1><pre class="lt lu lv lw gt oa ob oc od aw oe bi"><span id="fbf9" class="of ne iq ob b gy og oh l oi oj">def get_sentiment(text:str):</span><span id="d3d8" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="ab5a" class="of ne iq ob b gy ok oh l oi oj">Get the sentiment of text.</span><span id="9e57" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="a05f" class="of ne iq ob b gy ok oh l oi oj">text: the text sentiment to be predicted</span><span id="9617" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="c116" class="of ne iq ob b gy ok oh l oi oj">index = learn.predict("This was a great movie!")[2].numpy().argmax()</span><span id="fb84" class="of ne iq ob b gy ok oh l oi oj">print("Predicted sentiment: {}".format(mapping[index]))</span><span id="1b18" class="of ne iq ob b gy ok oh l oi oj">def evaluate():</span><span id="6927" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="f675" class="of ne iq ob b gy ok oh l oi oj">Evaluates the network</span><span id="1e73" class="of ne iq ob b gy ok oh l oi oj">Arguments:</span><span id="cef3" class="of ne iq ob b gy ok oh l oi oj">None</span><span id="c290" class="of ne iq ob b gy ok oh l oi oj">Returns:</span><span id="523f" class="of ne iq ob b gy ok oh l oi oj">accuracy: float</span><span id="d2f2" class="of ne iq ob b gy ok oh l oi oj">"""</span><span id="56a9" class="of ne iq ob b gy ok oh l oi oj">texts = df_test['text'].values</span><span id="c09a" class="of ne iq ob b gy ok oh l oi oj">labels = df_test['label'].values</span><span id="622a" class="of ne iq ob b gy ok oh l oi oj">preds = []</span><span id="d24e" class="of ne iq ob b gy ok oh l oi oj">for t in texts:</span><span id="dd05" class="of ne iq ob b gy ok oh l oi oj">preds.append(learn.predict(t)[1].numpy())</span><span id="6f45" class="of ne iq ob b gy ok oh l oi oj">acc = (labels == preds).mean() * 100</span><span id="24ef" class="of ne iq ob b gy ok oh l oi oj">print("Test Accuracy: {}".format(acc))</span><span id="be20" class="of ne iq ob b gy ok oh l oi oj">return preds, labels</span><span id="6c45" class="of ne iq ob b gy ok oh l oi oj">get_sentiment("This is amazing")</span><span id="35c1" class="of ne iq ob b gy ok oh l oi oj">preds, labels = evaluate()</span><span id="1191" class="of ne iq ob b gy ok oh l oi oj">print(classification_report(labels, preds, labels=[0,1,2]))</span><span id="7a61" class="of ne iq ob b gy ok oh l oi oj">print(confusion_matrix(labels, preds))</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/67018df2af706524fb5c24f6d21ba6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*9CCy8upNutlO-RL7PqnZLg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">模型结果</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi pn"><img src="../Images/c61477f6929462f9ac2f68361043097e.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*B-3QQk3PGAeC9C0AZynpsw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">混淆矩阵</p></figure><ul class=""><li id="aaae" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">如你所见，我们的模型很好，但可以通过试验超参数来改进。</li><li id="deb3" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">如果我们看到混淆矩阵，我们可以看到我们的模型对大多数类别进行了正确的分类。</li><li id="15ba" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">黑色代表 0，从图上看，我们得到的大多数预测都是黑色的</li></ul><h1 id="b1d2" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">结论和未来方向</h1><p id="fb14" class="pw-post-body-paragraph kf kg iq kh b ki nv jr kk kl nw ju kn ko nx kq kr ks ny ku kv kw nz ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">到</span>得出结论，我们取得以下结果:</p><ul class=""><li id="d8d6" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">我们使用美国航空公司的推文数据库训练一个模型来预测推文的情绪。</li><li id="a7ab" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们使用 ULMFit (Ruder 等人，2018 年)通过上面给出的新技术来训练我们的模型。</li><li id="849d" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们使用流行的 fastai 库来训练模型，因为它包含 AWD-LSTM 的预训练权重。</li><li id="ec42" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们实现了 94 的测试准确度，由于我们的数据集不平衡，我们使用 F1 分数等指标。</li><li id="bf7e" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们得到一个 F1 分，即<strong class="kh ir"> 89 </strong>的准确度。</li><li id="df4f" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">我们使用混淆矩阵进一步检验我们的模型的性能。</li></ul><blockquote class="op"><p id="75cd" class="oq or iq bd os ot ou ov ow ox oy la dk translated">为了建立一个更好的模型，我们还可以使用其他语言模型和技术，如 BERT、use、Transformers、XLNet 等。</p></blockquote><p id="b69f" class="pw-post-body-paragraph kf kg iq kh b ki oz jr kk kl pa ju kn ko pb kq kr ks pc ku kv kw pd ky kz la ij bi translated"><strong class="kh ir"> Colab 笔记本:</strong><a class="ae lr" href="https://colab.research.google.com/drive/1eiSmiFjg1aeNgepSfSEB55BJccioP5PQ?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1 eismifjg 1 aengepsfseb 55 bjcciop 5 pq？usp =共享</a></p></div></div>    
</body>
</html>