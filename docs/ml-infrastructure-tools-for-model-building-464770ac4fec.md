# 用于模型构建的 ML 基础设施工具

> 原文：<https://towardsdatascience.com/ml-infrastructure-tools-for-model-building-464770ac4fec?source=collection_archive---------17----------------------->

## 建模——ML 工作流的第二阶段

![](img/6241402a3ba37538bcfb1df6c18a0b70.png)

作者的 ML 基础设施平台图

几乎每个行业的企业都在采用人工智能(AI)和机器学习(ML)。许多企业都在期待 ML 基础设施平台，以推动他们在业务中利用人工智能的运动。了解各种平台和产品可能是一项挑战。**ML 基础设施空间拥挤、混乱且复杂**。有许多平台和工具跨越模型构建工作流程中的各种功能。

为了理解生态系统，我们大致将机器学习工作流分为三个阶段——数据准备、模型构建和生产。了解工作流程每个阶段的目标和挑战有助于做出明智的决策，确定最适合您业务需求的 ML 基础架构平台。

![](img/9a2aa3b037a203c4032292bd9c42d23f.png)

作者的 ML 基础设施平台图

机器学习工作流程的每个广泛阶段(数据准备、模型构建和生产)都有许多垂直功能。其中一些功能是更大的端到端平台的一部分，而一些功能是一些平台的主要焦点。

在我们的上一篇文章中，我们深入探讨了 ML 工作流程的数据准备部分。在那篇文章中，我们讨论了专注于数据准备功能的 ML 基础设施平台。在这篇文章中，我们将更深入地研究模型构建。

# 什么是模型构建？

建模的第一步是从理解业务需求开始。该模型解决了什么样的业务需求？这一步从 ML 工作流程的计划和构思阶段开始。在这个阶段，类似于软件开发生命周期，数据科学家收集需求，考虑可行性，并为数据准备、模型构建和生产创建一个计划。在这一阶段，他们使用数据来探索他们在规划阶段考虑过的各种模型构建实验。

# 特征探索和选择

作为这个实验过程的一部分，数据科学家探索各种数据输入选项来选择特征。特征选择是为机器学习模型寻找特征输入的过程。对于新模型，这可能是一个理解可用数据输入、输入的重要性以及不同候选特征之间的关系的漫长过程。这里可以做出许多决策，以获得更易解释的模型、更短的训练时间、获取特征的成本以及减少过拟合。找出正确的特征是一个不断迭代的过程。

*特征提取领域的 ML 基础设施公司:Alteryx/Feature Labs，Paxata(DataRobot)*

# 模型管理

数据科学家可以尝试多种建模方法。某些类型的模型比其他模型更适合某些任务(例如，基于树的模型更容易解释)。作为构思阶段的一部分，模型是否有监督、无监督、分类、回归等都是显而易见的。然而，决定什么类型的建模方法、什么超参数和什么特征取决于实验。一些 AutoML 平台会尝试许多具有不同参数的不同模型，这有助于建立基线方法。即使手动完成，探索各种选项也可以为模型构建者提供关于模型可解释性的见解。

# 实验跟踪

虽然在各种类型的模型中有许多优点和折衷，但一般来说，这个阶段涉及许多实验。有许多平台可以跟踪这些实验、建模依赖和模型存储。这些功能被广泛地归类为模型管理。一些平台主要关注实验跟踪。其他拥有培训和/或服务组件的公司拥有模型管理组件，用于比较各种模型的性能、跟踪培训/测试数据集、调整和优化超参数、存储评估指标以及实现详细的沿袭和版本控制。与 Github for software 类似，这些模型管理平台应该支持版本控制、历史传承和可再现性。

这些不同的模型管理平台之间的权衡是集成的成本。一些更轻量级的平台仅提供实验跟踪，但可以轻松地与当前环境集成，并导入到数据科学笔记本中。其他的需要一些更繁重的集成，需要模型构建者转移到他们的平台上，这样就有了集中的模型管理。

在机器学习工作流程的这一阶段，数据科学家通常会花时间在笔记本上构建模型、训练模型、将模型权重存储在模型存储中，然后在验证集上评估模型的结果。有许多平台可以为培训提供计算资源。根据团队希望如何存储模型对象，模型也有许多存储选项。

*ML 基础架构 AutoML: H20，SageMaker，DataRobot，Google Cloud ML，微软 ML*

*实验跟踪中的 ML 基础设施公司:Weights and Biases、Comet ML、ML Flow、Domino、Tensorboard*

*模型管理领域的 ML 基础设施公司:Domino 数据实验室、SageMaker*

*超参数选项中的 ML 基础设施公司。:Sigopt、权重和偏差、SageMaker*

# 模型评估

一旦在具有所选特征的训练数据集上训练了实验模型，就在测试集上评估该模型。该评估阶段涉及数据科学家试图了解模型的性能和需要改进的地方。一些更高级的 ML 团队将有一个自动化的回溯测试框架，用于评估历史数据上的模型性能。

每个实验都试图超越基线[模型的性能](https://arize.com/blog/monitor-your-model-in-production/)，并考虑计算成本、可解释性和归纳能力之间的权衡。在一些更受监管的行业中，该评估过程还可以包括外部评审者的合规性和审计，以确保模型的可再现性、性能和要求。

*ML 基础设施模型评测:Fiddler AI，Tensorboard，Stealth Startups*

*ML 基础设施预发布验证* : Fiddler AI， [Arize AI](https://arize.com/platform-overview/\)

# 一个平台来统治他们

许多以汽车或模型制作为中心的公司，为每件事推销一个单一的平台。它们正在竞争成为企业在数据准备、模型构建和生产中使用的单一人工智能平台。这些公司包括 DataRobot、H20、SageMaker 和其他一些公司。

这一组分为低代码和以开发人员为中心的解决方案。Datarobot 似乎专注于无代码/低代码选项，允许 BI 或财务团队从事数据科学项目。这与 SageMaker 和 H20 形成了鲜明的对比，它们似乎迎合了数据科学家或开发者优先团队的需求，而后者是当今更常见的数据科学组织。这两种情况下的市场都很大，可以共存，但值得注意的是，并非所有的 ML 基础设施公司都向相同的人或团队销售。

该领域的许多新进入者可以被认为是 ML 基础设施食物链特定部分的最佳解决方案。最好的类比是软件工程领域，在这里你的软件解决方案 GitHub、IDE、生产监控都不是相同的端到端系统。它们是不同的软件是有原因的；它们提供非常不同的功能，具有明显的区别。

# 挑战

与软件开发并行不同，模型的可再现性通常被认为是一个挑战。这主要是由于缺乏对模型训练数据的版本控制。

理解该模型的性能有许多挑战。如何比较实验并确定哪个版本的模型是性能和权衡的最佳平衡？一个折衷可能是想要性能稍差的模型，但是更容易理解。一些数据科学家使用内置的[模型可解释性](https://arize.com/blog/model-explainability-primer/)特征，或者使用 SHAP/莱姆探索特征重要性。

另一个性能挑战是不知道你在这个实验阶段的模型性能将如何转化为现实世界。这可以通过确保训练数据集中的数据是模型在生产中可能看到的数据的代表性分布来最好地缓解，以防止过度适应训练数据集。这就是交叉验证和回溯测试框架有用的地方。

# 接下来会发生什么？

对于数据科学家来说，确定模型何时可以投入生产的标准非常重要。如果在生产中部署了一个预先存在的模型，那么可能是在新版本的性能更高的时候。无论如何，设定一个标准对于将实验实际转移到真实世界环境中是很重要的。

一旦模型被训练，模型图像/权重被存储在模型存储中。这通常是负责将模型部署到产品中的数据科学家或工程师可以获取模型并用于服务的时候。在某些平台中，这种部署甚至可以更简单，可以用外部服务可以调用的 REST API 来配置部署的模型。

# 下一个

我们将在 ML 工作流程的生产环节对 ML 基础设施公司进行更深入的研究。

## 联系我们

如果这个博客引起了你的注意，并且你渴望了解更多关于[机器学习可观察性](https://arize.com/platform-overview/)和[模型监控](https://arize.com/model-monitoring/)，请查看我们的其他[博客](https://arize.com/blog/)和 [ML 监控](https://arize.com/ml-monitoring/)上的资源！如果您有兴趣加入一个有趣的 rockstar 工程团队，帮助模型成功生产，请随时[联系](https://arize.com/contact/)我们，并在此[找到我们的空缺职位](https://arize.com/careers/)！