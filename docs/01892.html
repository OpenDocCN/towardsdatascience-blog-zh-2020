<html>
<head>
<title>Deploy Keras Models using TensorFlow Serving — TF 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用tensor flow Serving-TF 2 . x部署Keras模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serving-keras-models-locally-using-tensorflow-serving-tf-2-x-8bb8474c304e?source=collection_archive---------13-----------------------#2020-02-22">https://towardsdatascience.com/serving-keras-models-locally-using-tensorflow-serving-tf-2-x-8bb8474c304e?source=collection_archive---------13-----------------------#2020-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3b88" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用TensorFlow Serving和Docker在本地主机上部署自己的Keras模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0f4032edc595f5fce56e1b680482c1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrvMKrWMhi3HomoiTLPsfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">张量流— 2.1.0</p></figure><h1 id="77ec" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">概观</h1><p id="138b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">创造了一个很棒的深度学习模型？恭喜你！</p><p id="b413" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">不知道如何使用Keras创建深度学习模型？别担心！</p><p id="3ebd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我将介绍如何创建一个基本的和超级简单的Keras模型，以及如何在您的本地机器上部署它。这个模型肯定比您计划部署的任何模型都要简单(嗯，我希望如此！).但是由于这篇文章的目标是使您能够部署您的Keras模型，所以我想确保我们不会偏离我们的目标。</p><p id="6bb5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在，我们为什么需要在本地服务器上部署它呢？验证我们用模型想要解决的任何问题都可以在现实世界中得到解决。您会期望您的模型以相同的方式工作，并在真实世界的数据上提供与您在本地IDE或笔记本上的测试输入相同的性能。但事实是，它很难做到这一点。如果是的话，太好了！但是，在您部署了供消费的模型之后，您不会希望意识到您的测试数据和真实世界数据之间的差距。</p><p id="e11a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果真实世界的图像不同于您用来训练、验证和测试模型的白色背景工作室图像，该怎么办？如果由于延迟问题，在网络调用中处理数据的时间比在IDE中长，该怎么办？尽早发现这些问题是将你的模型从舒适的笔记本电脑成功过渡到现实世界的关键。</p><h1 id="71f1" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">张量流服务</h1><h2 id="f25c" class="mr kz it bd la ms mt dn le mu mv dp li lz mw mx lk md my mz lm mh na nb lo nc bi translated">TensorFlow提供的是什么？</h2><p id="37e5" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae nd" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serving </a>是一个面向机器学习模型的灵活、高性能的服务系统，专为生产环境而设计。TensorFlow服务可以轻松部署新的算法和实验，同时保持相同的服务器架构和API。TensorFlow服务提供了与TensorFlow模型的现成集成，但可以轻松扩展为服务于其他类型的模型和数据。</p><h2 id="3ed2" class="mr kz it bd la ms mt dn le mu mv dp li lz mw mx lk md my mz lm mh na nb lo nc bi translated">为什么TensorFlow服务？</h2><p id="c2a2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果您习惯于使用TensorFlow或Keras构建模型，那么部署模型最简单的方法就是使用TensorFlow服务器。</p><p id="71db" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">大多数TensorFlow文档是为TensorFlow-1.0编写的，遗憾的是，它不能像TensorFlow-2.0那样工作。因此需要这个博客。</p><h1 id="6833" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">构建一个超级简单的Keras模型</h1><p id="c780" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">你可以在这里找到我的笔记本<a class="ae nd" href="https://github.com/bankarpranit26/deep-learning/blob/master/learnings/TFServing_Local_LinearModel.ipynb" rel="noopener ugc nofollow" target="_blank">:(你也可以在</a><a class="ae nd" href="https://colab.research.google.com/drive/1OaGLgnP7QXy0_C0wq0ndq7hwOVr2pEBk" rel="noopener ugc nofollow" target="_blank">谷歌实验室</a>运行)</p><p id="0836" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">[如果您已经以SavedModel格式准备好了模型，请跳过这一部分。]</p><p id="3aaf" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这里，我将创建一个预测线性关系的模型:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="ccc4" class="mr kz it nf b gy nj nk l nl nm">y = 2x + 1</span></pre><p id="fba5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我使用了Google Colab，但是你可以使用你选择的任何工具，只要生成的模型保持不变。</p><p id="7896" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们开始吃吧。</p><p id="9322" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们首先加载必要的库</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="c238" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然后我们创建数据集</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="adb3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然后我们建立我们的模型。既然是简单的线性关系，单个神经元就够了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="ab01" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们现在为我们的模型训练20个纪元。经过20个时期后，我们的模型在训练和验证时的损失约为2e-12。然后，我们在测试数据上评估我们的模型。我们的测试损失大约是2e-12。你可以尝试预测一些值来验证结果，就像我在这里做的那样。我收到的输出是15.866。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="bade" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一切看起来都很棒。现在让我们以SavedModel格式保存并下载我们的模型。</p><p id="62bd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从这一步开始，该过程将独立于所使用的输入和输出数据的类型。它可以是数字数组、文本、图像、音频或视频。在Colab中，您应该能够看到在您的目录中创建了一个名为“linear_model”的文件夹。这里的“export_path”变量表示我们的模型被命名为“linear_model ”,这是它的第一个版本。使用TensorFlow服务进行部署时，必须有版本号，因此请确保您的“export_path”采用{MODEL}/{VERSION}的形式，其中版本是不带任何字母或特殊字符的数字。</p><p id="f0af" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在要下载这个模型，我们将压缩这个文件夹，然后使用“google.colab.files”来下载压缩文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="061d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一旦您在本地机器上解压缩这个zip文件，您应该能够看到一个名为' linear_model '的文件夹，其中包含一个名为' 1 '的文件夹，该文件夹包含您的变量和模型架构。</p><h1 id="495f" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">设置您的机器</h1><p id="3ce8" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这是一次性设置活动。</p><p id="9393" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">要启动本地服务器，我们需要在本地机器上安装一个TensorFlow服务实例。我们将使用推荐的Docker使用方式，而不是下载并安装所有必要的库。</p><p id="fbbf" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可以在指南<a class="ae nd" href="https://www.tensorflow.org/tfx/serving/docker" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多细节。</p><p id="7f6e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们不会遵循指南中提到的所有步骤，因为有些事情是特定于TF 1.0的，有些事情是特定于重用已经可用的模型的。</p><p id="067c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一旦你从<a class="ae nd" href="https://www.docker.com/products/docker-desktop" rel="noopener ugc nofollow" target="_blank">这里</a>下载Docker到你的系统上，继续完成安装步骤。您需要重新启动系统，以便保存您的所有工作。</p><p id="3f2a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">安装成功完成后，进入命令提示符(Mac和Linux用户，请使用适当的工具)并键入</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="d594" class="mr kz it nf b gy nj nk l nl nm">docker pull tensorflow/serving</span></pre><p id="6b04" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">就是这样！现在让我们开始部署我们的模型。</p><h1 id="b3d2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">在本地主机上部署Keras模型</h1><p id="4867" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果我告诉您，部署模型只是一行命令脚本，会怎么样呢？</p><p id="b56d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你所需要的是<strong class="ls iu">到你的“线性模型”文件夹的绝对路径</strong>。不要忘记使用绝对路径，因为这将导致错误，你需要花时间和打破你的头来解决。</p><p id="1516" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我的“线性模型”保存在“D:/my_own_models/”中。所以我的命令看起来像:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="caa5" class="mr kz it nf b gy nj nk l nl nm">docker run -p 8038:8501 — mount type=bind,source=D:/my_own_models/linear_model,target=/models/linear_model -e MODEL_NAME=linear_model -t tensorflow/serving</span></pre><p id="c72f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这都是一条线。对于您的后续模型，您只需要更改您的“源”路径。更改您的“目标”和MODEL_NAME是可选的，但是，根据上下文，这当然是必要的。</p><p id="353b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在，让我们试着理解上面的命令脚本意味着什么。这个脚本的一般形式是</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="f565" class="mr kz it nf b gy nj nk l nl nm">docker run -p {LOCAL_PORT}:8501 — mount type=bind,source={ABSOLUTE_PATH},target=/models/{MODEL_NAME} -e MODEL_NAME={MODEL_NAME} -t tensorflow/serving</span></pre><p id="dc7d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">{LOCAL_PORT}:这是您机器的本地端口，所以请确保您没有在那里运行任何其他程序。我们将它映射到TensorFlow Serving为REST API调用公开的8501端口</p><p id="bb22" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">{绝对路径}:这是模型的绝对路径。这告诉TensorFlow服务你的模型位于哪里(很明显)。</p><p id="78b0" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">{MODEL_NAME}:这是带有前缀“/models/”的REST API调用的服务端点。不要更改目标变量中的“/models/”前缀，只根据您的需要更改{MODEL_NAME}部分</p><p id="3849" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">当您在命令窗口中看到以下消息时，您的模型已成功托管。您还可以在Docker仪表板中看到一个成功运行的容器。</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="3ba2" class="mr kz it nf b gy nj nk l nl nm">[evhttp_server.cc : 238] NET_LOG: Entering the event loop …</span></pre><h1 id="813a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">测试我们的模型</h1><p id="17bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我使用Postman来测试我的查询，但是您可以使用任何形式的API调用。</p><p id="17ae" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">[注意:您将无法从浏览器或任何其他主机直接调用您的REST API，因为TensorFlow服务不支持CORS。然而，有一些方法可以实现从浏览器到您的模型的调用，我将在另一篇文章中介绍。]</p><p id="46c7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">您需要一个POST查询来测试我们的模型。我们的请求URL看起来像:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="918d" class="mr kz it nf b gy nj nk l nl nm"><a class="ae nd" href="http://localhost:8509/v1/models/linear_model:predict" rel="noopener ugc nofollow" target="_blank">http://localhost:8509/v1/models/linear_model:predict</a></span></pre><p id="394b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">同样，一般形式是:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="807a" class="mr kz it nf b gy nj nk l nl nm"><a class="ae nd" href="http://localhost:{LOCAL_PORT}}/v1/models/{MODEL_NAME}:predict" rel="noopener ugc nofollow" target="_blank">http://localhost:{LOCAL_PORT}}/v1/models/{MODEL_NAME}:predict</a></span></pre><p id="b596" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在标题中添加“内容类型”作为“应用程序/json ”,在正文中添加:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="d885" class="mr kz it nf b gy nj nk l nl nm">{<br/>     "instances": [[<br/>          0<br/>     ]]<br/>}</span></pre><p id="56eb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">请确保您的JSON键是“instances ”,并且您的值在一个数组中。因为我们的输入是[0]，所以我们把它写成[[0]]。</p><p id="f1af" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">请记住，我们创建了一个模型来预测<strong class="ls iu"> y = 2x + 1 </strong>，这意味着对于输入值0，我们的预测值应该为1(或接近1)。</p><p id="370f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们发送我们的查询。响应看起来像:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="7a65" class="mr kz it nf b gy nj nk l nl nm">{<br/>     "predictions": [[<br/>          0.999998748<br/>     ]]<br/>}</span></pre><p id="04b8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">对我来说这看起来很像1。通过进行POST查询来摆弄您的模型。</p><h1 id="b28a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><p id="a22a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们能够从头开始创建一个模型，并将其部署在本地服务器上。这是一个很好的方法，可以看看您的模型在真实世界中的工作方式是否和在您的IDE中一样。</p><p id="916c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，正如我前面提到的，除了Postman(或类似的工具)，您将无法调用这个模型。如何克服这一点？我将在另一篇文章中讨论这个问题，因为这篇文章已经太长了。</p><p id="14f7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果您已经到达这里，感谢您的阅读。如果你有任何疑问，建议或意见，请随时评论这篇文章。这是我第一个关于机器学习的博客，我会很高兴地感谢所有的反馈。</p></div></div>    
</body>
</html>