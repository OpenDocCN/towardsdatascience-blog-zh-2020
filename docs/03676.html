<html>
<head>
<title>Two Challenges of K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-均值聚类的两个挑战</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/two-challenges-of-k-means-clustering-72e90bdeb0da?source=collection_archive---------37-----------------------#2020-04-06">https://towardsdatascience.com/two-challenges-of-k-means-clustering-72e90bdeb0da?source=collection_archive---------37-----------------------#2020-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ae58" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何“明智地”选择 k 和初始质心</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5ed504427d6aa528d8d6526152045067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuyodPAxwdsJuKo1FDjO2Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@ryoji__iwata?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">岩田良治</a>在<a class="ae ky" href="https://unsplash.com/s/photos/random?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3a6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means 聚类算法旨在将数据划分为 k 个聚类，使得同一聚类中的数据点相似，而不同聚类中的数据点相距较远。两点的相似性是由它们之间的距离决定的。在这篇文章中，我将讨论在使用 k-means 聚类时需要记住的两个关键点。如果您不熟悉 k-means 聚类，您可能希望先阅读 k-means 算法的详细说明。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/k-means-clustering-explained-4528df86a120"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">k-均值聚类—已解释</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">详细的理论解释和 scikit-learn 实现</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="1d0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了充分利用 k-means 聚类算法，需要明智地应对两个挑战:</p><ul class=""><li id="98ff" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated">定义聚类的数量</li><li id="8687" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">确定初始质心</li></ul><h1 id="ebb0" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated"><strong class="ak">定义集群数量</strong></h1><p id="d31a" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li nv lk ll lm nw lo lp lq nx ls lt lu im bi translated">在运行 k-means 聚类算法之前，我们需要声明聚类的数量。它不能确定聚类的最佳数量。K-means 将数据集划分为我们预先确定的聚类数。对于我们来说，找到最佳的集群数量也是一项具有挑战性的任务。我们不能只看数据集就找出我们应该有多少分区。</p><blockquote class="ny nz oa"><p id="eb6a" class="kz la ob lb b lc ld ju le lf lg jx lh oc lj lk ll od ln lo lp oe lr ls lt lu im bi translated">K-means 聚类试图最小化聚类内的距离。该距离被定义为聚类内距离和(WCSS)。</p></blockquote><p id="5e8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看 WCSS 随着不同的集群数量而变化。假设我们有以下非常简单的数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/127d25db62e4974d9f12725df7683e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*EIMZ84meUtI9Jo7UfJrENQ.png"/></div></figure><p id="78d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们现实一点，有一个集群:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/273969cd3926cf2fda056d7d165ce335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*f4pqE2ijAWKQ1OlDWBj2og.png"/></div></figure><p id="0f70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们有一个集群，质心将是红色方块。WCSS 计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a2536e641b9caf4c492f4cf69482bc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*677RATnYkOvJ9eLFXMCrBA.png"/></div></figure><p id="051a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个数据点和平均值之间的平方距离之和(红色方块)。随着聚类数量的增加，数据点和质心之间的平均距离减小，因此 WCSS 减小。</p><p id="77fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看两个集群的情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d1f5564bb4092be95c437f0fcce255cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*YWzMFqqTpAtjMxcjyVcsFQ.png"/></div></figure><p id="f652" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，数据点和质心之间的平均距离减小了。请记住，距离的数量与集群的数量无关。无论存在多少个聚类，我们都将计算 n 个距离，其中 n 是数据点的数量。所以只能着眼于平均距离。</p><p id="5e88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">WCSS 在 3 个集群中进一步下降:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/46686e9138a8ad1b1d7868fa63d3f286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*wxX2X47Hxe7a6E6YtFdRdg.png"/></div></figure><p id="2c50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经看到了 1、2 和 3 簇的 WCSS，其计算如下:</p><ul class=""><li id="da30" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated">k=1 &gt; WCSS = 35，16 单位</li><li id="f72e" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">k=2 &gt; WCSS = 20，91 单位</li><li id="d95d" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">k=3 &gt; WCSS = 2，95 单位</li></ul><p id="45bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">WCSS 还会下降多远？嗯，最终会是零。我们可以拥有的最大聚类数等于数据点的数量。虽然没有用，但是我们可以为每个数据点建立一个单独的集群。那么数据点和它的质心之间的距离变为零，因为数据点的质心就是它本身。</p><p id="f542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，为什么不对每个数据点进行聚类，并使 WCSS 为零呢？因为，在这种情况下，我们将有一个更大的问题，即过度拟合。过了一段时间后，我们将通过增加集群的数量来获得一点点 WCSS。下图显示了 WCSS 随着集群数量的增加而发生的变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9a756d8f33aeb41c34ef77cb388dabe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*PEbRgB_kqcYZeyVoUDxv6g.png"/></div></figure><p id="582e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，在某一点之后，WCSS 并没有减少多少。我们应该寻找斜率急剧变化锐边。红色方块标记的边是我们的最佳聚类数。</p><p id="9476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们选择大于该点的 k，WCSS 仍会降低，但不值得冒过度拟合的风险。</p><h1 id="b22e" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated"><strong class="ak">确定初始质心</strong></h1><p id="6214" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li nv lk ll lm nw lo lp lq nx ls lt lu im bi translated">K-means 是一个迭代过程。它建立在<a class="ae ky" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>算法的基础上。确定集群数量后，它通过执行以下步骤来工作:</p><ol class=""><li id="58a7" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ol mt mu mv bi translated"><strong class="lb iu">为每个簇随机选择质心(簇的中心)。</strong></li><li id="c973" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ol mt mu mv bi translated">计算所有数据点到质心的距离。</li><li id="c545" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ol mt mu mv bi translated">将数据点分配给最近的聚类。</li><li id="3f7d" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ol mt mu mv bi translated">通过取聚类中所有数据点的平均值，找到每个聚类的新质心。</li><li id="4df9" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ol mt mu mv bi translated">重复步骤 2、3 和 4，直到所有点收敛并且聚类中心停止移动。</li></ol><p id="9ed6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在专注于第一步。根据数据集的底层结构，不同的初始质心可能最终形成不同的聚类。除此之外，完全随机选择质心可能会增加运行时间，因此算法需要更多的时间来收敛。</p><p id="c494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可能想寻找一种聪明的方法，而不是以完全随机的方式选择初始质心。那个聪明的办法就是<strong class="lb iu"> k-means++ </strong>。</p><p id="dfd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means++确保了一种更智能的方式来初始化集群。如维基百科<a class="ae ky" href="https://en.wikipedia.org/wiki/K-means%2B%2B" rel="noopener ugc nofollow" target="_blank">所述，</a></p><blockquote class="ny nz oa"><p id="92f1" class="kz la ob lb b lc ld ju le lf lg jx lh oc lj lk ll od ln lo lp oe lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> k </em> -means++ </strong>是一种为<em class="it"> k </em> -means 聚类算法选择初始值(或“种子”)的算法。</p></blockquote><p id="3466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k-means 和 k-means++的区别只是选择初始质心。剩下的步骤完全相同。K-means++从数据集中的数据点随机均匀地选择第一个质心。每个随后的质心从剩余的数据点中选择，其概率与其到该点最近的现有质心的距离的平方成比例。</p><p id="4c39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，在 scikit-learn 中实现了 k-means++。由<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> sklearn.cluster.KMean </a> s 的“<strong class="lb iu"> init </strong>参数指定，默认值为 k-means++。init 参数的另一个选项是“random ”,它随机初始化质心。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><p id="09a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="650a" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="1621" class="mn mo it lb b lc nt lf nu li ot lm ou lq ov lu ms mt mu mv bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/K-means%2B%2B" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/K-means%2B%2B</a></li></ul></div></div>    
</body>
</html>