<html>
<head>
<title>Binary and Multiclass Text Classification (auto detection in a model test pipeline)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二进制和多类文本分类(模型测试管道中的自动检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/binary-and-multiclass-text-classification-auto-detection-in-a-model-test-pipeline-938158854943?source=collection_archive---------18-----------------------#2020-05-25">https://towardsdatascience.com/binary-and-multiclass-text-classification-auto-detection-in-a-model-test-pipeline-938158854943?source=collection_archive---------18-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/cf2011dff638eabc50647eb39ad21ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2oV7Ez_YvKfc0BBe9RjpA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</p></figure><div class=""/><h1 id="4c7c" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="3dd4" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在我之前的文章(<a class="ae ly" rel="noopener" target="_blank" href="/model-selection-in-text-classification-ac13eedf6146">文本分类中的模型选择</a>)中，我针对二进制文本分类问题，提出了一种通过比较经典机器学习和深度学习来选择模型的方法。</p><p id="8170" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">笔记本的结构是自动运行，交叉验证所有算法，并显示不同指标的结果，让用户根据自己的需要自由选择算法。</p><p id="de37" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">在这里，<a class="ae ly" href="https://github.com/Christophe-pere/Text-classification" rel="noopener ugc nofollow" target="_blank">笔记本</a>被创建来显示二元或多类问题的不同度量和学习曲线。笔记本将自动定义数据集的配置。</p><blockquote class="me mf mg"><p id="498b" class="la lb mh lc b ld lz lf lg lh ma lj lk mi mb ln lo mj mc lr ls mk md lv lw lx ij bi translated">笔记本和脚本可以在这里找到:<a class="ae ly" href="https://github.com/Christophe-pere/Text-classification" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></blockquote><h1 id="1127" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">信息</h1><p id="0a8b" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">目标是使用不同的模型，并在<strong class="lc jg">训练</strong>和<strong class="lc jg">测试</strong>期间观察它们的行为。当可能时，算法执行<strong class="lc jg">提前停止</strong>以<strong class="lc jg">避免过拟合</strong>。对于深度学习，可以在训练期间使用<strong class="lc jg">预训练模型</strong>来减少训练时间。</p><p id="88fe" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">开头有两件事很重要:<br/>-<strong class="lc jg">文本列名</strong>归类<br/>-<strong class="lc jg">标签列名</strong></p><blockquote class="ml"><p id="113f" class="mm mn jf bd mo mp mq mr ms mt mu lx dk translated">创建流水线是为了考虑到<strong class="ak">二进制分类或多类分类，而无需人工参与</strong>。管道提取标签的数量，并确定它是二元问题还是多类问题。所有算法和指标将自动从一个切换到另一个。</p></blockquote><p id="9528" class="pw-post-body-paragraph la lb jf lc b ld mv lf lg lh mw lj lk ll mx ln lo lp my lr ls lt mz lv lw lx ij bi translated">笔记本以用于测试您想要的模型的参数列表开始。下面的<strong class="lc jg"> <em class="mh">要诀</em> </strong>显示了这个列表。<strong class="lc jg"> <em class="mh"> Lang </em> </strong>决定笔记本是否需要从<em class="mh"> Google </em>调用<a class="ae ly" href="https://pypi.org/project/googletrans/" rel="noopener ugc nofollow" target="_blank"><em class="mh">Translator()</em></a>API 来检测数据集的语言(默认为英语)。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="79f9" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">相应的 python 函数:</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="8c17" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">I —数据清理、文本处理</h1><p id="b96a" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这篇文章中使用的数据与上一篇文章相同，IMDB 数据集(因为它是一个开源数据集，更多详细信息<a class="ae ly" rel="noopener" target="_blank" href="/model-selection-in-text-classification-ac13eedf6146">在此</a>)。</p><p id="2c46" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">数据集不是完全干净的，在自然语言处理任务中，你需要为你的问题清理你的数据。这一步将影响算法的性能。</p><p id="fe63" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">关于 IMDB 数据集，我做了什么？</p><ul class=""><li id="46c8" class="ng nh jf lc b ld lz lh ma ll ni lp nj lt nk lx nl nm nn no bi translated">去掉一个单词的所有大写字母<strong class="lc jg">,只保留第一个字母(这对 NER 提取很重要)</strong></li><li id="7198" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">移除<strong class="lc jg">网址</strong>(如果有)</li><li id="827a" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">移除<strong class="lc jg"> html </strong>应答器</li><li id="ce53" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">删除<strong class="lc jg">表情符号</strong></li><li id="3050" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">用<em class="mh">有</em>有代替<strong class="lc jg">有<em class="mh">有</em></strong></li><li id="a84b" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">用<em class="mh">而非</em> 替换<em class="mh">n</em>的缩写</li></ul><h1 id="2428" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">II —极性</h1><p id="e146" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">使用库<a class="ae ly" href="https://textblob.readthedocs.io/en/dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc jg"> TextBlob </strong> </a>估计文本的极性，并绘制成图表(在 5000 raws 的样本上制作)。</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7ca455820e5f54a7ce4d3446bfd36b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*OX4AwrGEaM1f5GsXSJp9Ww.png"/></div></figure><h1 id="f4cb" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">III —文本信息</h1><p id="b77a" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">所有的评论都没有相同的长度，相同的字数。研究这些信息很有趣:</p><ul class=""><li id="970c" class="ng nh jf lc b ld lz lh ma ll ni lp nj lt nk lx nl nm nn no bi translated">提取字数</li><li id="e7f3" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">提取字符数</li><li id="5632" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">计算密度(字符数/字数)</li><li id="d39c" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">提取首字母大写的单词数</li></ul><p id="8c71" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">我们可以很容易地用图表标出 char 的数量:</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/da4c45162cddcf093757b0340f648da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5rxIGvJFsNgI4iCXttEIg.png"/></div></div></figure><p id="73db" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">评论大多由 550-600 个字符组成。</p><p id="4df4" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">看类(标签)的分布也很有意思。</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8144d6bbc45ab8dfa5988ec80f01e835.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*WVO6oW0F3QLWYG47QqcQwQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标签</p></figure><p id="28a3" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">数据集是平衡的，大约有 2500 个标签类别的评论(二进制)。</p><h1 id="5f72" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">六元语法</h1><p id="41ba" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">N-grams 是一种将句子分割成 n 个单词的技术，例如:</p><p id="8e54" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated"><em class="mh">我学机器学习是为了成为一名数据科学家</em></p><ul class=""><li id="07f9" class="ng nh jf lc b ld lz lh ma ll ni lp nj lt nk lx nl nm nn no bi translated"><strong class="lc jg"> Unigram </strong>:【我，学习，机器，学习，到，成为，a，数据，科学家】</li><li id="8256" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated"><strong class="lc jg">二元模型</strong> : [(我学习)，(学习机)，(机器学习)，(学习到)，(成为)，(成为一个)，(一个数据)，(数据科学家)]</li><li id="02cc" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated"><strong class="lc jg">三元组</strong> : [(我学习机器)，(学习机器学习)，(机器学习到)，(学习成为)，(成为一个)，(成为一个数据)，(一个数据科学家)]</li><li id="2e68" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi">…</li></ul><p id="1748" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">我是 n 元语法的粉丝，因为我可以通过它们展示预处理是否正确。在笔记本上，你会发现单词重要性对于单个词、两个词、有和没有停用词的三个词(一个没有重要性的单词)以及对 5 个词的测试。</p><p id="f863" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">对于带停用词的三元模型:</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/7aa294e7914c9fbd4a99bfc30951a857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwWxezYF7ZayoH8ZXGDNeA.png"/></div></div></figure><p id="76a8" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">没有停止词:</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/da12ce48786fa723893630d22e041995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMlVpToKqoPuJNAIHONBig.png"/></div></div></figure><p id="8d62" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">这种差异非常重要。一般来说，文本包含许多停用词，这对于像<strong class="lc jg"> <em class="mh"> TF-IDF </em> </strong>或<strong class="lc jg"> <em class="mh">单词嵌入</em> </strong>这样的方法并不重要，但是对于<strong class="lc jg"> <em class="mh"> One-Hot 编码</em> </strong>来说却是如此，因为每个单词在一个句子中具有相同的重要性。</p><h1 id="b2ef" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">我们使用什么模型和指标？</h1><p id="1f5c" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">管道被配置为使用不同的模型。表 1 给出了机器学习和深度学习算法以及使用的指标。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="d4aa" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">看起来怎么样</h1><p id="c7bb" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我给大家看两个例子，第一个，<strong class="lc jg"> <em class="mh">随机梯度推进</em> </strong>和<strong class="lc jg"> <em class="mh">浅层神经网络</em> </strong>。</p><p id="3da9" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">训练经典分类器代码如下:</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="53b8" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">该函数输入分类器和数据以拟合模型。</p><p id="2547" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">对于指标，已经创建了相同类型的函数:</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="5be9" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">该函数将显示不同的曲线(精确召回率、真假阳性率、ROC AUC)、混淆矩阵、Cohen's Kappa(模型之间的比较以及两个标注器将如何做)和准确度。</p><p id="cf3a" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated"><strong class="lc jg"> <em class="mh">随机梯度推进:</em> </strong></p><p id="1d05" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">用 TF-IDF 方法获得了 SGD 算法(函数 1)的最佳结果。</p><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="f9f9" class="oc kd jf ny b gy od oe l of og">if sgd: # does the computation if sgd = True<br/>    print("\nStochastic Gradient Descent with early stopping for TF-IDF\n")<br/>    print("Early Stopping : 10 iterations without change")<br/>    metrics_ML(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ),xtrain_tfidf, train_y, xvalid_tfidf, valid_y, gb=True)</span></pre><p id="622e" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">metrics_ML()函数将调用 classifier_model()函数来训练模型并计算指标。训练分类器和指标的最简单方法。</p><p id="7bfe" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">结果是:</p><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="bf5a" class="oc kd jf ny b gy od oe l of og">Stochastic Gradient Descent with early stopping for TF-IDF<br/><br/>Early Stopping : 10 iterations without change<br/>Execution time : 0.060 s<br/>Score : 84.7 %<br/><br/>Classification Report<br/><br/>              precision    recall  f1-score   support<br/><br/>    negative       0.87      0.81      0.84       490<br/>    positive       0.83      0.88      0.85       510<br/><br/>    accuracy                           0.85      1000<br/>   macro avg       0.85      0.85      0.85      1000<br/>weighted avg       0.85      0.85      0.85      1000</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f468c55ef86253a1e12d46691be3789b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*V-jJ6ylC67uhdAw2IG4hjw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">混淆矩阵</p></figure><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="fc0e" class="oc kd jf ny b gy od oe l of og">Model: f1-score=0.855 AUC=0.923</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7db811a6f2abdc4be06d0a6c4b3c7c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*IPsZXlefR2lZLz1pMXOXEg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">精确召回曲线</p></figure><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="d067" class="oc kd jf ny b gy od oe l of og">ROC AUC=0.919</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/227dacad45d816a668c32ccb7b81c90f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*MN-sAawcHErgEd303inbHA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">真假阳性率</p></figure><p id="99d8" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">还不错，84.7%的分数。现在我们能做得更好吗？</p><p id="546f" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated"><strong class="lc jg"> <em class="mh">浅层神经网络:</em> </strong></p><p id="ad58" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">浅层神经网络的代码已在上一篇文章中介绍过。这里再说一遍:</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="01ae" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">每个深度学习算法都以相同的方式实现。所有的代码都可以在笔记本和对应的 GitHub 中找到。</p><p id="e1a1" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">如何使用它:</p><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="5c23" class="oc kd jf ny b gy od oe l of og">es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=3)<br/>if shallow_network:<br/>    model_shallow = shallow_neural_networks(word_index, pre_trained=pre_trained)<br/>    history = model_shallow.fit(train_seq_x, train_y,<br/>                    epochs=1000, callbacks=[es],<br/>                    validation_split=0.2, verbose=True)<br/>    results = model_shallow.evaluate(valid_seq_x, valid_y)</span></pre><p id="ac8f" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">输出:</p><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="6f40" class="oc kd jf ny b gy od oe l of og">Train on 3200 samples, validate on 800 samples<br/>Epoch 1/1000<br/>3200/3200 [==============================] - 2s 578us/sample - loss: 0.7117 - accuracy: 0.4837 - val_loss: 0.7212 - val_accuracy: 0.5175<br/>...<br/>Epoch 98/1000<br/>3200/3200 [==============================] - 1s 407us/sample - loss: 0.4991 - accuracy: 0.9991 - val_loss: 0.5808 - val_accuracy: 0.8850</span><span id="b42a" class="oc kd jf ny b gy ok oe l of og">1000/1000 [==============================] - 0s 383us/sample - loss: 0.5748 - accuracy: 0.8590</span></pre><p id="2735" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">指标:</p><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/575591a74dfec293c5b7cd18eb58814b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qs6RV0CxGP-qyvWLpQAqzw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">浅层神经网络的历史</p></figure><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="5eb2" class="oc kd jf ny b gy od oe l of og">                precision    recall  f1-score   support<br/><br/>    negative       0.86      0.85      0.86       490<br/>    positive       0.86      0.87      0.86       510<br/><br/>    accuracy                           0.86      1000<br/>   macro avg       0.86      0.86      0.86      1000<br/>weighted avg       0.86      0.86      0.86      1000<br/><br/><br/>The balanced accuracy is : 85.88%<br/><br/><br/>The Zero-one Loss is : 14.1%<br/><br/><br/>Explained variance score: 0.436<br/><br/><br/>ROC AUC=0.931</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2653f636453651a693013df2b4c28a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*j2pKZ53jlbZ9P_KTkokOew.png"/></div></figure><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="bc65" class="oc kd jf ny b gy od oe l of og">Model: f1-score=0.863 AUC=0.932</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/b173fba83ec283089c66495d96f339da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*s9y_Y3t6O7nWFmxEfQ4eJw.png"/></div></div></figure><pre class="na nb nc nd gt nx ny nz oa aw ob bi"><span id="e39e" class="oc kd jf ny b gy od oe l of og">Cohen's kappa: 71.78%</span></pre><figure class="na nb nc nd gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/167d5d1150eae4c0e0ac7292702617f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*425oEtfUXdHOiPHR1e0SKA.png"/></div></figure><p id="56ea" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">所以，神经网络的结果更好。但是，不能对每种方法的单次运行进行比较。要正确选择模型，必须使用交叉验证等评估方法(参见:<a class="ae ly" rel="noopener" target="_blank" href="/model-selection-in-text-classification-ac13eedf6146">文本分类中的模型选择</a>)。</p><h1 id="f41d" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结论</h1><p id="cac6" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="lc jg">流水线是自动的</strong>，你只需要在开始时配置<strong class="lc jg">参数</strong>，<strong class="lc jg">选择你想要测试的不同算法</strong>并等待结果。</p><p id="f693" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">目标是<strong class="lc jg">通过算法和方法(One-Hot encoding、TF-IDF、TF-IDF n-grams、TF-IDF char n-grams 和单词嵌入)显示不同的指标</strong>,并<strong class="lc jg">选择您希望用于解决问题的一类算法</strong>。下一步将是调整超参数并享受结果。</p><p id="12ab" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">这项工作有助于在不了解类的情况下，针对<strong class="lc jg">文本分类</strong>、<strong class="lc jg">二元或多元类</strong>快速<strong class="lc jg">测试 NLP 用例</strong>。管道可以接受<strong class="lc jg">法语文本或者</strong>英语文本。</p><p id="52ba" class="pw-post-body-paragraph la lb jf lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">笔记本和课程可在<a class="ae ly" href="https://github.com/Christophe-pere/Text-classification" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><h1 id="9cbd" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">后续步骤</h1><ul class=""><li id="72f2" class="ng nh jf lc b ld le lh li ll oo lp op lt oq lx nl nm nn no bi translated">实现不平衡的方法来自动平衡数据集</li><li id="d196" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">实现变压器分类模型</li><li id="5740" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">实施预先培训的变压器</li><li id="789a" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">用强化学习测试 NLP</li><li id="29c5" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">知识图谱</li><li id="71b0" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">使用分布式深度学习</li><li id="c5c9" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">使用 TensorNetwork 加速神经网络</li><li id="ffcd" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">用正确的方法选择一类模型并进行超参数调整</li><li id="24f9" class="ng nh jf lc b ld np lh nq ll nr lp ns lt nt lx nl nm nn no bi translated">使用量子 NLP (QNLP)</li></ul></div></div>    
</body>
</html>