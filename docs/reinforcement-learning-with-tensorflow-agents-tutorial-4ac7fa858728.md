# ä½¿ç”¨å¼ é‡æµä»£ç†çš„å¼ºåŒ–å­¦ä¹ â€”æ•™ç¨‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728?source=collection_archive---------13----------------------->

## ç”¨è¿™ä¸ªç®€å•çš„æ•™ç¨‹è¯•è¯• TF-Agents for RLï¼Œå®ƒä»¥ Google colab ç¬”è®°æœ¬çš„å½¢å¼å‘å¸ƒï¼Œæ‰€ä»¥ä½ å¯ä»¥ç›´æ¥ä»ä½ çš„æµè§ˆå™¨ä¸Šè¿è¡Œå®ƒã€‚

![](img/c67ccb32d8794b70100da02248bb18df.png)

å‡ å‘¨å‰ï¼Œæˆ‘å†™äº†ä¸€ç¯‡æ–‡ç« ï¼Œåˆ—ä¸¾äº†å¯ä»¥ç”¨æ¥åœ¨é¡¹ç›®ä¸­å®ç°å¼ºåŒ–å­¦ä¹ (RL)çš„ä¸åŒæ¡†æ¶ï¼Œå±•ç¤ºäº†æ¯ä¸ªæ¡†æ¶çš„ç››è¡°ï¼Œå¹¶æƒ³çŸ¥é“æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªæ¡†æ¶ä¼šåœ¨æŸä¸ªæ—¶å€™ç»Ÿæ²»æ‰€æœ‰æ¡†æ¶ã€‚ä»é‚£ä»¥åï¼Œæˆ‘å¼€å§‹çŸ¥é“äº† [TF Agents](https://github.com/tensorflow/agents) ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº TensorFlow çš„ RL åº“ï¼Œå¹¶å¾—åˆ°äº†å…¶ç¤¾åŒºçš„å…¨åŠ›æ”¯æŒ(æ³¨æ„ï¼ŒTF Agents ä¸æ˜¯ Google çš„å®˜æ–¹äº§å“ï¼Œä½†å®ƒæ˜¯ä½œä¸º Github ä¸Šçš„å®˜æ–¹ TensorFlow å¸æˆ·çš„å­˜å‚¨åº“å‘å¸ƒçš„)ã€‚

æˆ‘ç›®å‰æ­£åœ¨ä¸€ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨ TF ä»£ç†ï¼Œç”±äºå®ƒçš„è‰¯å¥½æ–‡æ¡£ï¼ŒåŒ…æ‹¬[æ•™ç¨‹](https://github.com/tensorflow/agents/tree/master/docs/tutorials)ï¼Œå¼€å§‹ä½¿ç”¨å®ƒå¾ˆå®¹æ˜“ã€‚å®ƒå®šæœŸæ›´æ–°ï¼Œæœ‰è®¸å¤šè´¡çŒ®è€…ï¼Œè¿™ä½¿æˆ‘è®¤ä¸ºæˆ‘ä»¬æœ‰å¯èƒ½åœ¨ä¸ä¹…çš„å°†æ¥çœ‹åˆ° TF ä»£ç†ä½œä¸ºå®ç° RL çš„æ ‡å‡†æ¡†æ¶ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œæˆ‘å†³å®šå†™è¿™ç¯‡æ–‡ç« ç»™ä½ ä¸€ä¸ªå¿«é€Ÿçš„ä»‹ç»ï¼Œè¿™æ ·ä½ ä¹Ÿå¯ä»¥ä»è¿™ä¸ªåº“å—ç›Šã€‚æˆ‘å·²ç»[å‘å¸ƒäº†è¿™é‡Œä½¿ç”¨çš„æ‰€æœ‰ä»£ç ï¼Œä½œä¸ºä¸€ä¸ª Google colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing)ï¼Œæ‰€ä»¥ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨çº¿è¿è¡Œå®ƒã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° Github çš„æ‰€æœ‰ä»£ç å’Œæ–‡æ¡£ã€‚ä½ ä¸éœ€è¦å…‹éš†ä»–ä»¬çš„åº“ï¼Œä½†æ˜¯æœ‰å®˜æ–¹çš„ Github ä½œä¸ºå‚è€ƒæ€»æ˜¯æœ‰ç”¨çš„ã€‚æˆ‘å®ç°äº†ä¸‹é¢çš„ä¾‹å­ï¼Œéƒ¨åˆ†éµå¾ªä»–ä»¬çš„æ•™ç¨‹(1_dqn_tutorial ),ä½†æˆ‘è¿›ä¸€æ­¥ç®€åŒ–äº†å®ƒï¼Œå¹¶åœ¨æœ¬æ–‡ä¸­ç”¨å®ƒæ¥ç© Atari æ¸¸æˆã€‚è®©æˆ‘ä»¬åŠ¨æ‰‹å§ã€‚

# å®‰è£… TF ä»£ç†å’Œä¾èµ–é¡¹

å¦‚å‰æ‰€è¿°ï¼ŒTF-Agents è¿è¡Œåœ¨ TensorFlow ä¸Šï¼Œæ›´å…·ä½“åœ°è¯´æ˜¯ TensorFlow 2.2.0ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨è¿˜æ²¡æœ‰ä»¥ä¸‹è½¯ä»¶åŒ…ï¼Œæ‚¨éœ€è¦å®‰è£…å®ƒä»¬:

```
pip install tensorflow==2.2.0
pip install tf-agents
```

# ä¸º CartPole å®ç° DQN ä»£ç†

æˆ‘ä»¬å°†å®ç°ä¸€ä¸ª DQN ä»£ç†( [Mnih et al. 2015](https://daiwk.github.io/assets/dqn.pdf) )ï¼Œå¹¶å°†å…¶ç”¨äºç»å…¸æ§åˆ¶é—®é¢˜ CartPoleã€‚å¦‚æœä½ æƒ³è§£å†³ä¸€äº›æ›´ä»¤äººå…´å¥‹çš„äº‹æƒ…ï¼Œæ¯”å¦‚è¯´ï¼Œä¸€ä¸ª Atari æ¸¸æˆï¼Œä½ åªéœ€è¦ä»æ‰€æœ‰å¯ç”¨çš„ OpenAI ç¯å¢ƒä¸­é€‰æ‹©ä¸€ä¸ªä½ æƒ³è¦çš„ç¯å¢ƒåã€‚

æˆ‘ä»¬ä»æ‰€æœ‰å¿…è¦çš„è¿›å£å¼€å§‹ã€‚æ­£å¦‚ä½ åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ä» TF-Agents å®ç°äº†ç›¸å½“å¤šçš„å¯¹è±¡ã€‚è¿™äº›éƒ½æ˜¯æˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„å®ç°å®šåˆ¶å’Œåˆ‡æ¢çš„ä¸œè¥¿ã€‚

```
from __future__ import absolute_import, division, print_functionimport base64
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tffrom tf_agents.agents.dqn import dqn_agent
from tf_agents.drivers import dynamic_step_driver
from tf_agents.environments import suite_gym
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import q_network
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory
from tf_agents.utils import common
```

## ç¯å¢ƒ

![](img/5d6e42c1f7a68272a4bbd026306c707b.png)

OpenAI å¥èº«æˆ¿çš„ç¿»ç­‹æ–—ç¯å¢ƒ[GIF from [ç§¯è°·åº·](https://github.com/jaekookang)/[RL-ç¿»ç­‹æ–—](https://github.com/jaekookang/RL-cartpole)ã€‚]

ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹åˆ›é€ æˆ‘ä»¬çš„ç¯å¢ƒã€‚åœ¨ CartPole ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé¡¶éƒ¨æœ‰ä¸€æ ¹æ†å­çš„æ¨è½¦ï¼Œä»£ç†çš„ä»»åŠ¡æ˜¯å­¦ä¹ ä¿æŒæ†å­ï¼Œå·¦å³ç§»åŠ¨æ¨è½¦ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å·²ç»åŒ…å«åœ¨ TF-Agents ä¸­çš„ suite_gym ä¸­çš„ e ç¯å¢ƒï¼Œè¿™æ˜¯ OpenAI Gym ç¯å¢ƒçš„ä¸€ä¸ªç¨å¾®å®šåˆ¶(å¹¶é’ˆå¯¹å…¶ä¸ TF-Agents çš„ä½¿ç”¨è¿›è¡Œäº†æ”¹è¿›)çš„ç‰ˆæœ¬(å¦‚æœæ‚¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹ä¸ OpenAI çš„å®ç°[è¿™é‡Œ](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/suite_gym.py)çš„å·®å¼‚)ã€‚æˆ‘ä»¬è¿˜å°†ä¸ºæˆ‘ä»¬çš„ç¯å¢ƒä½¿ç”¨ä¸€ä¸ªåä¸º TFPyEnvironment çš„åŒ…è£…å™¨ï¼Œå®ƒå°†ç”¨äºçŠ¶æ€è§‚å¯Ÿã€æ“ä½œå’Œå¥–åŠ±çš„ numpy æ•°ç»„è½¬æ¢ä¸º TensorFlow å¼ é‡ã€‚åœ¨å¤„ç†å¼ é‡æµæ¨¡å‹(å³ç¥ç»ç½‘ç»œ)æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ é‡ï¼Œå› æ­¤é€šè¿‡ä½¿ç”¨è¿™ä¸ªåŒ…è£…å™¨ï¼Œæˆ‘ä»¬å¯ä»¥èŠ‚çœä¸€äº›è½¬æ¢è¿™äº›æ•°æ®æ‰€éœ€çš„å·¥ä½œã€‚

```
env = suite_gym.load('CartPole-v1')
env = tf_py_environment.TFPyEnvironment(env)
```

## ä»£ç†äºº

TF ä¸­æœ‰ä¸åŒçš„è¯å‰‚â€”â€”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„è¯å‰‚: [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) ã€[å¢æ´](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)ã€ [DDPG](https://arxiv.org/pdf/1509.02971.pdf) ã€ [TD3](https://arxiv.org/pdf/1802.09477.pdf) ã€ [PPO](https://arxiv.org/abs/1707.06347) å’Œ [SAC](https://arxiv.org/abs/1801.01290) ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ DQNã€‚ä»£ç†çš„ä¸€ä¸ªä¸»è¦å‚æ•°æ˜¯å®ƒçš„ Q(ç¥ç»)ç½‘ç»œï¼Œå®ƒå°†ç”¨äºè®¡ç®—æ¯ä¸€æ­¥ä¸­åŠ¨ä½œçš„ Q å€¼ã€‚q_network æœ‰ä¸¤ä¸ªå¼ºåˆ¶å‚æ•°:å®šä¹‰è§‚å¯Ÿå½¢çŠ¶å’ŒåŠ¨ä½œå½¢çŠ¶çš„ input_tensor_spec å’Œ action_specã€‚æˆ‘ä»¬å¯ä»¥ä»æˆ‘ä»¬çš„ç¯å¢ƒä¸­å¾—åˆ°è¿™ä¸€ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬å°†æˆ‘ä»¬çš„ q_network å®šä¹‰å¦‚ä¸‹:

```
q_net = q_network.QNetwork(env.observation_spec(), 
                           env.action_spec())
```

æ­£å¦‚ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„ q_network å®šåˆ¶æ›´å¤šçš„å‚æ•°ï¼Œä½†æ˜¯ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é»˜è®¤çš„å‚æ•°ã€‚ä»£ç†è¿˜éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨æ¥æŸ¥æ‰¾ q_network å‚æ•°çš„å€¼ã€‚è®©æˆ‘ä»¬ä¿æŒç»å…¸ï¼Œç”¨äºšå½“ã€‚

```
optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)
```

æœ€åï¼Œæˆ‘ä»¬ç”¨ä»¥ä¸‹å‚æ•°å®šä¹‰å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„ä»£ç†:

*   time_step_specï¼Œå®ƒæ˜¯æˆ‘ä»¬ä»ç¯å¢ƒä¸­è·å¾—çš„ï¼Œå®šä¹‰äº†æˆ‘ä»¬çš„æ—¶é—´æ­¥é•¿æ˜¯å¦‚ä½•å®šä¹‰çš„ã€‚
*   action_specï¼Œä¸ q_network ç›¸åŒã€‚
*   æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ Q ç½‘ã€‚
*   æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ä¼˜åŒ–å™¨ã€‚
*   TD è¯¯å·®æŸå¤±å‡½æ•°ï¼Œç±»ä¼¼äºç¥ç»ç½‘ç»œä¸­çš„æŸå¤±å‡½æ•°ã€‚
*   åˆ—è½¦æ­¥æ•°è®¡æ•°å™¨ï¼Œè¿™åªæ˜¯ä¸€ä¸ª 0 é˜¶å¼ é‡(ä¹Ÿç§°ä¸ºæ ‡é‡)ï¼Œå®ƒå°†è®¡ç®—æˆ‘ä»¬åœ¨ç¯å¢ƒä¸­çš„æ­¥æ•°ã€‚

```
train_step_counter = tf.Variable(0)agent = dqn_agent.DqnAgent(env.time_step_spec(),
                           env.action_spec(),
                           q_network=q_net,
                           optimizer=optimizer,
                           td_errors_loss_fn= 
                                  common.element_wise_squared_loss,
                           train_step_counter=train_step_counter)agent.initialize()
```

## è¾…åŠ©æ–¹æ³•:å¹³å‡ç´¯ç§¯å›æŠ¥å’Œæ”¶é›†æ•°æ®

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›è¾…åŠ©æ–¹æ³•ã€‚ç¬¬ä¸€ä¸ªå°†åœ¨ç¯å¢ƒä¸­è¿­ä»£è‹¥å¹²é›†ï¼Œåº”ç”¨ç­–ç•¥æ¥é€‰æ‹©è¦éµå¾ªçš„æ“ä½œï¼Œå¹¶è¿”å›è¿™äº›é›†ä¸­çš„å¹³å‡ç´¯ç§¯å¥–åŠ±ã€‚è¿™å°†æœ‰åŠ©äºè¯„ä¼°æˆ‘ä»¬çš„ä»£ç†äº†è§£åˆ°çš„ç­–ç•¥ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­å°è¯•è¯¥æ–¹æ³• 10 é›†ã€‚

```
def compute_avg_return(environment, policy, num_episodes=10):
    total_return = 0.0
    for _ in range(num_episodes):
        time_step = environment.reset()
        episode_return = 0.0 while not time_step.is_last():
            action_step = policy.action(time_step)
            time_step = environment.step(action_step.action)
            episode_return += time_step.reward
        total_return += episode_return avg_return = total_return / num_episodes
    return avg_return.numpy()[0]# Evaluate the agent's policy once before training.
avg_return = compute_avg_return(env, agent.policy, 5)
returns = [avg_return]
```

æˆ‘ä»¬è¿˜å°†åœ¨åŸ¹è®­æˆ‘ä»¬çš„ä»£ç†æ—¶å®ç°ä¸€ç§æ”¶é›†æ•°æ®çš„æ–¹æ³•ã€‚DQN çš„çªç ´ä¹‹ä¸€æ˜¯ç»éªŒå›æ”¾ï¼Œæˆ‘ä»¬å°†ä»£ç†äººçš„ç»éªŒ(çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±)å­˜å‚¨èµ·æ¥ï¼Œç”¨å®ƒåœ¨æ¯ä¸€æ­¥æ‰¹é‡è®­ç»ƒ Q ç½‘ç»œã€‚è¿™é€šè¿‡ä½¿å­¦ä¹ æ›´å¿«å’Œæ›´ç¨³å®šæ¥æ”¹è¿›å­¦ä¹ ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼ŒTF-Agents åŒ…å«äº† TFUniformReplayBuffer å¯¹è±¡ï¼Œå®ƒå­˜å‚¨äº†è¿™äº›ç»éªŒä»¥ä¾¿ä»¥åé‡ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆåˆ›å»ºè¿™ä¸ªæˆ‘ä»¬ä»¥åä¼šç”¨åˆ°çš„å¯¹è±¡ã€‚

åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªç¯å¢ƒã€ä¸€ä¸ªç­–ç•¥å’Œä¸€ä¸ªç¼“å†²åŒºï¼Œé‡‡ç”¨ç”±å®ƒçš„çŠ¶æ€è§‚å¯Ÿå’Œå¥–åŠ±å½¢æˆçš„å½“å‰æ—¶é—´æ­¥é•¿ã€ç­–ç•¥é€‰æ‹©çš„åŠ¨ä½œä»¥åŠä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é•¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å®ƒå­˜å‚¨åœ¨é‡æ”¾ç¼“å†²åŒºä¸­ã€‚æ³¨æ„ï¼Œé‡æ”¾ç¼“å†²åŒºå­˜å‚¨äº†ä¸€ä¸ªåä¸º Trajectory çš„å¯¹è±¡ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨å‰é¢å‘½åçš„å…ƒç´ åˆ›å»ºäº†è¿™ä¸ªå¯¹è±¡ï¼Œç„¶åä½¿ç”¨ add_batch æ–¹æ³•å°†å®ƒä¿å­˜åˆ°ç¼“å†²åŒºã€‚

```
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
                                data_spec=agent.collect_data_spec,                                                                
                                batch_size=env.batch_size,                                                              
                                max_length=100000)def collect_step(environment, policy, buffer):
    time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = trajectory.from_transition(time_step, 
                                      action_step, 
                                      next_time_step)# Add trajectory to the replay buffer
    buffer.add_batch(traj)
```

## åˆ—è½¦ä»£ç†

æˆ‘ä»¬ç»ˆäºå¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„ç‰¹å·¥äº†ã€‚æˆ‘ä»¬å®šä¹‰äº†æˆ‘ä»¬åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­çš„æ­¥æ•°ï¼Œåœ¨è¿™ä¸ªæ­¥æ•°ä¹‹åï¼Œæˆ‘ä»¬å°†åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ï¼Œä¿®æ”¹å®ƒçš„ç­–ç•¥ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¯æ¬¡è¿­ä»£åªä½¿ç”¨ä¸€æ­¥ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†æˆ‘ä»¬çš„ Q ç½‘ç»œå°†è¢«è®­ç»ƒçš„æ‰¹é‡å¤§å°å’Œä¸€ä¸ªè¿­ä»£å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬è¿­ä»£ä»£ç†çš„ç»éªŒã€‚

ç„¶åï¼Œæˆ‘ä»¬å°†ä¸ºç¼“å†²å™¨æ”¶é›†ä¸€äº›ç»éªŒï¼Œå¹¶ä»å¸¸è§çš„ RL ç¯è·¯å¼€å§‹ã€‚é€šè¿‡å¯¹ç¯å¢ƒé‡‡å–è¡ŒåŠ¨ã€åŸ¹è®­æ”¿ç­–å’Œé‡å¤æ¥è·å¾—ç»éªŒã€‚æˆ‘ä»¬å¦å¤–æ‰“å°æŸå¤±ï¼Œå¹¶åˆ†åˆ«æ¯ 200 å’Œ 1000 æ­¥è¯„ä¼°ä»£ç†çš„æ€§èƒ½ã€‚

```
collect_steps_per_iteration = 1
batch_size = 64
dataset = replay_buffer.as_dataset(num_parallel_calls=3, 
                                   sample_batch_size=batch_size, 
                                   num_steps=2).prefetch(3)
iterator = iter(dataset)
num_iterations = 20000
env.reset()for _ in range(batch_size):
    collect_step(env, agent.policy, replay_buffer)for _ in range(num_iterations):
    # Collect a few steps using collect_policy and save to the replay buffer.
    for _ in range(collect_steps_per_iteration):
        collect_step(env, agent.collect_policy, replay_buffer) # Sample a batch of data from the buffer and update the agent's network.
    experience, unused_info = next(iterator)
    train_loss = agent.train(experience).loss step = agent.train_step_counter.numpy() # Print loss every 200 steps.
    if step % 200 == 0:
        print('step = {0}: loss = {1}'.format(step, train_loss)) # Evaluate agent's performance every 1000 steps.
    if step % 1000 == 0:
        avg_return = compute_avg_return(env, agent.policy, 5)
        print('step = {0}: Average Return = {1}'.format(step, avg_return))
        returns.append(avg_return)
```

## æƒ…èŠ‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ç”»å‡ºå½“æˆ‘ä»¬è®­ç»ƒä»£ç†äººæ—¶ï¼Œç´¯ç§¯çš„å¹³å‡å›æŠ¥æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ matplotlib åˆ¶ä½œä¸€ä¸ªéå¸¸ç®€å•çš„ç»˜å›¾ã€‚

```
iterations = range(0, num_iterations + 1, 1000)
plt.plot(iterations, returns)
plt.ylabel('Average Return')
plt.xlabel('Iterations')
```

![](img/4fd44b8335a11188f95cc6508376ba62.png)

ã€ŠDQN ç‰¹å·¥ã€‹5 é›†çš„å¹³å‡å›æŠ¥ç‡ã€‚éšç€ä»£ç†å˜å¾—æ›´æœ‰ç»éªŒï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ€§èƒ½æ˜¯å¦‚ä½•éšç€æ—¶é—´çš„æ¨ç§»è€Œæé«˜çš„ã€‚

# å®Œå…¨ç 

æˆ‘å·²ç»æŠŠæœ¬æ–‡ä¸­çš„[æ‰€æœ‰ä»£ç ä½œä¸º Google Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing)è¿›è¡Œäº†åˆ†äº«ã€‚ä½ å¯ä»¥ç›´æ¥è¿è¡Œæ‰€æœ‰ä»£ç ï¼Œå¦‚æœä½ æƒ³æ”¹å˜å®ƒï¼Œä½ å¿…é¡»æŠŠå®ƒä¿å­˜åœ¨ä½ è‡ªå·±çš„ Google drive è´¦æˆ·ä¸Šï¼Œç„¶åä½ å¯ä»¥åšä»»ä½•ä½ æƒ³åšçš„äº‹æƒ…ã€‚å¦‚æœä½ æ„¿æ„ï¼Œä½ ä¹Ÿå¯ä»¥ä¸‹è½½å®ƒåœ¨ä½ çš„æœ¬åœ°è®¡ç®—æœºä¸Šè¿è¡Œã€‚

# ä»è¿™é‡Œå»å“ªé‡Œ

*   ä½ å¯ä»¥è·Ÿéš Github ä¸Š TF-Agents åº“ä¸­çš„[æ•™ç¨‹](https://github.com/tensorflow/agents/tree/master/docs/tutorials)
*   å¦‚æœä½ æƒ³æŸ¥çœ‹ RL çš„å…¶ä»–å¥½æ¡†æ¶ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æˆ‘ä»¥å‰çš„å¸–å­:

[](/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18) [## Python å¼ºåŒ–å­¦ä¹ çš„ 5 ä¸ªæ¡†æ¶

### ä»å¤´å¼€å§‹ç¼–ç¨‹ä½ è‡ªå·±çš„å¼ºåŒ–å­¦ä¹ å®ç°å¯èƒ½ä¼šæœ‰å¾ˆå¤šå·¥ä½œï¼Œä½†æ˜¯ä½ ä¸éœ€è¦åšâ€¦

towardsdatascience.com](/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18) 

*   ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘å‰æ®µæ—¶é—´å†™çš„å¦ä¸€ç¯‡æ–‡ç« çš„[ä¸­æŸ¥çœ‹å…¶ä»–å¯ä»¥å°è¯• TF-Agents(æˆ–è€…ä»»ä½•ä½ é€‰æ‹©çš„ RL ç®—æ³•)çš„ç¯å¢ƒã€‚](https://medium.com/@mauriciofadelargerich/reinforcement-learning-environments-cff767bc241f)

è€è§„çŸ©ï¼Œæ„Ÿè°¢é˜…è¯»ï¼è¯·åœ¨å›å¤ä¸­å‘Šè¯‰æˆ‘ä½ å¯¹ TF-Agents çš„çœ‹æ³•ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å‘ç°ä»»ä½•é—®é¢˜ï¼Œä¹Ÿè¯·å‘Šè¯‰æˆ‘ğŸ›åœ¨ä»£ç ä¸­ã€‚