<html>
<head>
<title>Overview of various Optimizers in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中各种优化器综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5?source=collection_archive---------12-----------------------#2020-06-09">https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5?source=collection_archive---------12-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f44" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解各种优化器之间的关系以及它们的优缺点。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5b448d6c36e18b57600b83fc4a7dbecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ejugShlI63T6-TO4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@hiteshchoudhary?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Hitesh Choudhary </a>拍摄的照片</p></figure><p id="9e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化器是用于改变神经网络属性(如权重和学习速率)以减少损失的算法或方法。优化器用于通过最小化函数来解决优化问题。</p><blockquote class="lv lw lx"><p id="99d3" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">优化人员是如何工作的？</strong></p></blockquote><p id="d4fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你应该如何改变你的神经网络的权重或学习速率来减少损失是由你使用的优化器定义的。优化算法负责减少损失，并尽可能提供最准确的结果。</p><p id="37a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用一些初始化策略来初始化权重，并且根据更新等式在每个时期更新权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/d847d6ccc178a43fb20742ea346c9a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*L2odkoBwwLXF5W14sG2qTg.png"/></div></figure><p id="a74e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的等式是更新等式，使用它来更新权重以达到最精确的结果。使用一些称为优化器的优化策略或算法可以获得最佳结果。</p><p id="4477" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在过去的几年里，人们研究了各种各样的优化器，每一种都有其优缺点。阅读整篇文章，了解算法的工作原理、优点和缺点。</p><blockquote class="lv lw lx"><p id="0e9e" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">梯度下降(物品):</strong></p></blockquote><p id="8cd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降是最基本的一阶优化算法，它依赖于损失函数的一阶导数。它计算权重应该以何种方式改变，以便函数可以达到最小值。通过反向传播，损耗从一层转移到另一层，并且模型的参数(也称为权重)根据损耗进行修改，以便损耗可以最小化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/d847d6ccc178a43fb20742ea346c9a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*L2odkoBwwLXF5W14sG2qTg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/27a3d71dec9e75264f3710ebcca0f0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*SULbXWO7ZV205bk82NHElg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片 1</p></figure><p id="9692" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图(图 1)可以看出，权重被更新以收敛到最小值。</p><p id="8671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ol class=""><li id="bacd" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">实现起来非常简单。</li></ol><p id="d63c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="c421" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">该算法一次采用 n 个点的整个数据集来计算导数，以更新需要大量存储器的权重。</li><li id="b488" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">极小值是在很长一段时间后达到的，或者是永远达不到的。</li><li id="9993" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">该算法可能会卡在局部最小值或鞍点:</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/af8097ddb820d1cb77cadc5619d14a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*fnwz411OXtbCWVz0bTgjYw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片 2</p></figure><p id="005d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图(图 2)中，梯度下降可能会卡在局部最小值或鞍点，永远不会收敛到最小值。为了找到最佳解决方案，算法必须达到全局最小值。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="c1a4" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">随机梯度下降(SGD): </strong></p></blockquote><p id="5141" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SGD 算法是 GD 算法的扩展，它克服了 GD 算法的一些缺点。GD 算法有一个缺点，它需要大量的内存来一次加载 n 点的整个数据集到计算机导数。在 SGD 算法的情况下，每次取一个点来计算导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/30af7928ff7d2f8bfe9c8d7d83ea88af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*yyncuXzeq0wAV9BigplxwQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3</p></figure><p id="c42f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图(图 3)可以看出，与梯度下降相比，更新需要更多的迭代次数才能达到最小值。在图 3 的右边部分，GD 算法达到最小值需要较少的步骤，但是 SGD 算法噪声更大，迭代次数更多。</p><p id="fe7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优势:</strong></p><ol class=""><li id="1fd2" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">与 GD 算法相比，内存需求更少，因为每次只取一个点来计算导数。</li></ol><p id="431a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="a460" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">与 GD 算法相比，完成 1 个历元所需的时间很长。</li><li id="9be2" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">需要很长时间才能收敛。</li><li id="d69c" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">可能会陷入局部最小值。</li></ol></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="c976" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">小批量随机梯度下降(MB-SGD): </strong></p></blockquote><p id="a228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MB-SGD 算法是 SGD 算法的扩展，它克服了 SGD 算法时间复杂度大的问题。MB-SGD 算法从数据集中取出一批点或点的子集来计算导数。</p><p id="151e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以观察到，MB-SGD 的损失函数的导数与 GD 的损失函数的导数在若干次迭代之后几乎相同。但是与 GD 相比，MB-SGD 实现最小值的迭代次数很大，并且计算成本也很高。</p><p id="8f8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">权重的更新取决于一批点的损失的导数。在 MB-SGD 的情况下，更新的噪声更大，因为导数并不总是朝向最小值。</p><p id="e064" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ol class=""><li id="5245" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">与标准 SGD 算法相比，收敛的时间复杂度更低。</li></ol><p id="98a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="b406" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">与 GD 算法的更新相比，MB-SGD 的更新噪音更大。</li><li id="ac6f" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">比 GD 算法需要更长的时间来收敛。</li><li id="438e" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">可能会陷入局部最小值。</li></ol></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="c667" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> SGD 带动量:</strong></p></blockquote><p id="bf10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MB-SGD 算法的一个主要缺点是权重的更新非常嘈杂。具有动量的 SGD 通过对梯度去噪克服了这个缺点。权重的更新依赖于有噪声的导数，并且如果我们以某种方式对导数去噪，那么收敛时间将会减少。</p><p id="6b03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想法是使用指数加权平均来对导数进行去噪，即与先前更新相比，给予最近更新更多的权重。</p><p id="d16a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SGD 更新公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/d847d6ccc178a43fb20742ea346c9a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*L2odkoBwwLXF5W14sG2qTg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/cbc02866a3d67e0cf9d7859faa20c80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*awmpuCMoir96qwG8532Y6A.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/67902d4d9f78c760207e05150ba547d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*-z_TVT9ZHe-Ot6HJ39vSuw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c1fb2f308dc4da70f82df77de6e274be.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*-k6unBOpzTpP4Wz-of8CLQ.png"/></div></figure><p id="9557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用所有先前的更新来计算时间“t”处的动量，与先前的更新相比，给予最近的更新更大的权重。这导致收敛速度加快。</p><p id="57bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">动量如何加速收敛？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f84a144afabec0f70b1ae4574f24f37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*O3ae7dnwHMjX8CBlfhL2sg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4</p></figure><p id="5036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当通过仅减去具有先前权重的梯度项来计算新权重时，更新在方向 1 上移动，并且如果通过减去具有先前权重的动量项来计算新权重，则更新在方向 2 上移动(图 5)。</p><p id="7373" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们结合这两个方程，通过用动量和梯度项之和减去先前的权重来计算新的权重，那么更新将向方向 3 移动，这导致对更新去噪。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2fb50c95a39dc89da6c3942d4dc3a9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*toJFo3oZ6wzISAtcY3LT-A.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5</p></figure><p id="e382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图(图 5)总结了 SGD+动量降噪梯度，与 SGD 相比收敛更快。</p><p id="da4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ol class=""><li id="5227" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">具有 SGD 算法的所有优点。</li><li id="7114" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">比 GD 算法收敛得更快。</li></ol><p id="f0ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="784c" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">我们需要为每次更新多计算一个变量。</li></ol></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="3915" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">内斯特罗夫加速梯度(NAG): </strong></p></blockquote><p id="dfe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NAG 算法的思想非常类似于带有动量的 SGD，只是略有不同。在具有动量算法的 SGD 的情况下，动量和梯度是在先前更新的权重上计算的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8900ad1a2ac9105940aa784c584a6595.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*pv2GOZfsF5x_q2QfigD4YQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/deaad9357a8b217afe9b73048ce20db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*JySvsrHm7-6g4guOwtYCFg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6364aaa84646c814e28162153abcaf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*lBODYGirQt9Jh_TP4UGojQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4476099052515487f5f4a8893a77bc34.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*Zb0RH-gxSstih3IUkPi9Lw.png"/></div></figure><p id="e88d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据 NAG 算法，首先计算点 W_(t-1)的动量 V_t，并沿该方向移动以到达 W_dash，然后计算新的更新权重 W_dash 处的梯度，并再次向梯度移动(图 6)。净运动导致朝向最小值的方向。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b9116aa1bffce6a2c2e151b2f9f425cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*ODOgISGzT4RLQfMjSpyRaw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6</p></figure><p id="bfed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NAG 和带动量算法的 SGD 工作得一样好，并且具有相同的优点和缺点。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="33fd" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">自适应梯度(AdaGrad): </strong></p></blockquote><p id="c4d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于前面讨论的所有算法，学习率保持不变。因此，AdaGrad 的关键思想是为每个权重设定一个自适应的学习速率。权重的学习速率将随着迭代次数而降低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5a9694a97b9ff08d7d43c33d62fffa4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*CLkms777IWa89_rTWYybBg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6b355dcdff584486d3949d117dff0c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*RuEs9Nf0x3pF8_56tORBLA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8e78b1c7bed220117a52e63f40beba3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*A28mdurbrT9tPyYM0M0flA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8259627f698a54fd23c1c99f8f4a9b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*RP3NcNsKdK5HcP4n_I_0Yg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fe76aded9d4f8112a9797e3dee079069.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*U7SP8nphtW9UpO_7-QJ7zA.png"/></div></figure><p id="0147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，随着迭代次数(t)的增加，学习率α增加，这导致学习率自适应地降低。</p><p id="b0a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优势:</strong></p><ol class=""><li id="9b33" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">无需手动更新学习率，因为它会随着迭代自适应地变化。</li></ol><p id="040c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="76fc" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">当迭代次数变得非常大时，学习率降低到非常小的数值，这导致收敛缓慢。</li></ol></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="d9a1" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">阿达德尔塔:</strong></p></blockquote><p id="2a4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以前的算法 AdaGrad 的问题是学习率随着大量迭代而变得非常小，这导致收敛缓慢。为了避免这种情况，AdaDelta 算法有一个取指数衰减平均值的想法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8e78b1c7bed220117a52e63f40beba3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*A28mdurbrT9tPyYM0M0flA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a8a3f966397165bc3d1866e2c985d8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*E0y1fEc3kuHU-xnOo_RFkA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/908c6b9a80216ad931d03a59e5384e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*BeVWN4Dw4Lfc53F1cInbgA.png"/></div></div></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="23fe" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">亚当:</p></blockquote><p id="e7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 AdaDelta 算法的情况下，我们存储梯度平方的指数衰减平均值来修改学习速率。对于 Adam optimizer，想法是存储梯度的一阶矩(g_t)和二阶矩(g_t 的平方)。</p><p id="ea12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一阶矩的 EDA:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a6405e4c46c92b61608e40b99284ec3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*MyIm8lg0jfgI0dJqFxuLSw.png"/></div></figure><p id="10a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二阶矩的 EDA:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3e0a372de9f2a34da173e1553a0eeb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*8kkqKVZSg20DU_5rKsKiig.png"/></div></figure><p id="1c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">偏差校正:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b7288c7c267c104481176bd9bc86dada.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*1W8Fufs7wEX6aT8CfJg2PQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2d9c43b83cd2ac7540dfc6c846cbf306.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*M5j8-Wn6snGt7xgAHB7Dxw.png"/></div></figure><p id="75e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1ce5e959f80bb3b72f1dc344bb8c8db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*457Vx-wwJ0fJ3e1bLeFrtA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/96dee89ba6a552c72b5e557a29578ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*jfwYdsgkeNC3fjHrV-w-PQ.png"/></div></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="f041" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">优化器的结论和比较:</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6f88abbbc7f15cc32c055a4577c03cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*g2RIZTalueDjGcJQ7YsXzw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://arxiv.org/pdf/1412.6980.pdf</p></figure><p id="e523" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图是本文中讨论的算法的迭代次数与训练损失的关系图。</p><p id="aab0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于给定的迭代次数(假设 100)，从图中可以观察到 adam 在所有其他算法中收敛最快。</p><blockquote class="lv lw lx"><p id="ef39" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">什么时候选择哪种算法？</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5ced5286be46c059022fec7161e0f4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*SjtKOauOXFVjWRR7iCtHiA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif" rel="noopener ugc nofollow" target="_blank">https://ruder . io/content/images/2016/09/saddle _ point _ evaluation _ optimizer . gif</a></p></figure><ul class=""><li id="3d07" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu nz mk ml mm bi translated">在上面的动画中，可以看到 SGD 算法(红色)停滞在一个鞍点上。所以 SGD 算法只能用于浅层网络。</li><li id="52f5" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu nz mk ml mm bi translated">除 SGD 之外的所有其他算法最终一个接一个地收敛，AdaDelta 是最快的，其次是动量算法。</li><li id="6625" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu nz mk ml mm bi translated">AdaGrad 和 AdaDelta 算法可用于稀疏数据。</li><li id="e9c9" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu nz mk ml mm bi translated">动量和 NAG 在大多数情况下工作良好，但速度较慢。</li><li id="9cea" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu nz mk ml mm bi translated">Adam 的动画不可用，但是从上面的图中可以看出，它是收敛到最小值的最快算法。</li><li id="a861" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu nz mk ml mm bi translated">Adam 被认为是上面讨论的所有算法中最好的算法。</li></ul><blockquote class="oa"><p id="3858" class="ob oc it bd od oe of og oh oi oj lu dk translated">感谢您的阅读！</p></blockquote></div></div>    
</body>
</html>