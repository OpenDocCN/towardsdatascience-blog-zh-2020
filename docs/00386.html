<html>
<head>
<title>Intuitive explanation of Neural Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f?source=collection_archive---------8-----------------------#2020-01-12">https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f?source=collection_archive---------8-----------------------#2020-01-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3850" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简单解释用于神经机器翻译的序列到序列模型(NMT)</h2></div><p id="8237" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">什么是神经机器翻译？</strong></p><p id="f27f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经机器翻译是一种将一种语言翻译成另一种语言的技术。例如，将英语转换为印地语。让我们考虑一下，如果你在一个印度村庄，那里的大多数人不懂英语。你打算毫不费力地与村民沟通。在这种情况下，你可以使用神经机器翻译。</p><p id="ec25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">神经机器翻译是使用深度神经网络将源语言(如英语)的单词序列转换为目标语言(如印地语或西班牙语)的单词序列的任务。</strong></p><p id="c84b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">神经机器翻译需要什么特征？</em> </strong></p><ul class=""><li id="2283" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated"><strong class="kk iu">能够在几个时间步骤内保存顺序数据</strong></li></ul><p id="388d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">NMT使用顺序数据，这种数据需要在几个时间步骤中保持不变。人工神经网络(ANN)不会将数据持续几个时间步。像LSTM(长短期记忆)或GRU(门控递归单元)这样的递归神经网络(RNN)能够在几个时间步长上保存数据</p><ul class=""><li id="a8e7" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated"><strong class="kk iu">处理可变长度输入和输出向量的能力</strong></li></ul><p id="006a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ANN和CNN需要一个固定的输入向量，您可以对其应用一个函数来产生固定大小的输出。将一种语言翻译成另一种语言包括源语言和目标语言中长度可变的单词序列。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/d5a34330f9e4534d75901be2295b6e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K497YsMhZ10VTno-8-yntg.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">源语言和可变长度的目标句子</p></figure><p id="cbe3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="le">RNN的像LSTM或者</em></strong><a class="ae me" href="https://medium.com/datadriveninvestor/multivariate-time-series-using-gated-recurrent-unit-gru-1039099e545a" rel="noopener"><strong class="kk iu"><em class="le">GRU</em></strong></a><strong class="kk iu"><em class="le">对顺序数据处理有什么帮助？</em>T25】</strong></p><p id="cdc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae me" href="https://medium.com/datadriveninvestor/recurrent-neural-network-rnn-52dd4f01b7e8" rel="noopener"> <strong class="kk iu"> RNN的</strong> </a> <strong class="kk iu">是具有循环的神经网络，用于保存信息</strong>。它们<strong class="kk iu">对序列中的每个元素执行相同的任务，并且输出元素依赖于先前的元素或状态</strong>。这正是我们处理顺序数据所需要的</p><p id="6a83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNN可以有一个或多个输入，也可以有一个或多个输出。这是处理顺序数据(即可变输入和可变输出)的另一个要求</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mf"><img src="../Images/c047e50590d2ba3ff6b2d879da0e01b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qmd-gH-fo3gq636vQ0w8A.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">来源:<a class="ae me" href="http://karpathy.github.io/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/</a></p></figure><p id="f281" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">为什么我们不能用RNN进行神经机器翻译？</em> </strong></p><p id="f3ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在人工神经网络中，我们不在网络的不同层之间共享权重，因此，我们不需要对梯度求和。RNN的份额权重，我们需要对W在每一步的梯度求和，如下所示。</p><p id="b7f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在时间步长t =0计算h的梯度涉及W的许多因子，因为我们需要通过每个RNN单元反向传播。即使我们忘记了权重矩阵，一次又一次地乘以相同的标量值，比如说100个时间步长，这也是一个挑战。</p><p id="fc17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果最大奇异值大于1，那么梯度会爆炸，称为<strong class="kk iu">爆炸梯度</strong>。</p><p id="3c45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果最大奇异值小于1，那么梯度将消失，称为<strong class="kk iu">消失梯度。</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mg"><img src="../Images/c759771bd9632f43728a2caec2982c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qa4rdvwbVldZP6cCJmz2Kw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">向前和向后进给和权重在所有层上共享，导致爆炸或消失渐变</p></figure><p id="d893" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">爆炸渐变</strong>是通过使用<strong class="kk iu">渐变裁剪解决的，在渐变裁剪中我们</strong>为渐变设置了一个阈值。如果梯度值大于阈值，我们就对其进行剪裁。</p><p id="29f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">消失梯度</strong>问题通过使用LSTM(长短期记忆)或门控循环单元(GRU)解决。</p><p id="0c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">什么是LSTM和GRU？</em>T13】</strong></p><p id="6681" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> LSTM是长短期记忆，GRU </strong>是门控循环单位。他们能够很快学会长期依赖。LSTM可以学会跨越超过1000步的时间间隔。这是通过有效的基于梯度的算法实现的，该算法使用通过内部状态的恒定误差流。</p><p id="e8e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTM和GRU记忆信息的时间跨度很长。他们通过决定记住什么和忘记什么来做到这一点。</p><p id="2a49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTM用4个门来决定我们是否需要记住之前的状态。细胞状态在LSTM扮演着重要角色。LSTM可以使用4个调节门来决定是否要添加或删除单元状态的信息。这些闸门就像水龙头一样，决定着应该通过多少信息。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mh"><img src="../Images/b82c4f794d260adc3cb23221bc1a059e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_X6YCQ_DIvB7F-7v7E6Kw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">LSTM的三个步骤</p></figure><p id="695c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GRU是LSTM解决消失梯度问题的更简单的变体</p><p id="784a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它使用两个门:<strong class="kk iu">复位门和一个更新门</strong>不像LSTM的三个步骤。GRU没有内部记忆</p><p id="55d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重置门决定如何将新输入与先前时间步长的记忆相结合。更新门决定应该保留多少先前的内存。</p><p id="5902" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GRU有更少的参数，所以他们在计算上更有效率，比LSTM需要更少的数据来概括</p><p id="bb9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我们如何使用LSTM或GRU进行神经机器翻译？</em>T25】</strong></p><blockquote class="mi"><p id="678b" class="mj mk it bd ml mm mn mo mp mq mr ld dk translated"><strong class="ak">我们使用以LSTM或GRU为基本块的编码器和解码器框架来创建Seq2Seq模型</strong></p></blockquote><p id="63d0" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">序列到序列模型将源序列映射到目标序列。源序列是机器翻译系统的输入语言，目标序列是输出语言。</p><p id="a91a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">编码器</strong>:从源语言中读取单词的输入序列，并将该信息编码成实值向量，也称为隐藏状态或思想向量或上下文向量。思维向量将输入序列的“意义”编码成一个向量。编码器输出被丢弃，只有隐藏或内部状态作为初始输入被传递给解码器</p><p id="25e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">解码器:</strong>将来自编码器的思想向量作为输入，将字符串开始标记<code class="fe mx my mz na b">&lt;START&gt;</code>作为初始输入，产生输出序列。</p><p id="44da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编码器逐字读取输入序列，类似地，解码器逐字生成输出序列。</p><p id="eb65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器在训练和推断阶段的工作方式不同，而编码器在训练和推断阶段的工作方式相同</p><p id="0a49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">解码器的训练阶段</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nb"><img src="../Images/0360f14c63100b6b2de10c2a67ea94f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RfFciCR6CVTf-MSXo6EHA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">使用教师强制的编码器-解码器训练阶段</p></figure><p id="1f02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用<strong class="kk iu">教师强制</strong>来更快更有效地训练解码器。</p><p id="d67d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">老师的强迫就像老师在学生接受新概念训练时纠正学生一样。由于在培训过程中老师给了学生正确的输入，学生将更快更有效地学习新概念。 </p><p id="d056" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">教师强制</strong>算法通过提供先前时间戳的实际输出而不是先前时间的预测输出作为训练期间的输入来训练解码器。</p><p id="40f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们添加一个标记<code class="fe mx my mz na b">&lt;START&gt;</code>来表示目标序列的开始，并添加一个标记<code class="fe mx my mz na b">&lt;END&gt;</code>作为目标序列的最后一个字。<code class="fe mx my mz na b">&lt;END&gt;</code>标记稍后在推断阶段用作停止条件，表示输出序列的结束。</p><p id="ac13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">解码器的推断阶段</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nc"><img src="../Images/6e03c2424e0a9ffb4d7ffbc7ed5d1e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lk9hwavG8VyGCBTsO4t-0A.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">编码器-解码器推理阶段</p></figure><p id="f254" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在推理或预测阶段，我们没有实际的输出序列或单词。在推断阶段，我们将前一时间步的预测输出作为输入连同隐藏状态一起传递给解码器。</p><p id="7e0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器预测阶段的第一个时间步长将来自编码器和<code class="fe mx my mz na b">&lt;START&gt;</code>标签的最终状态作为输入。</p><p id="0665" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于随后的时间步长，解码器的输入将是来自前一解码器的隐藏状态以及来自前一解码器的输出。</p><p id="63eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们达到最大目标序列长度或<code class="fe mx my mz na b">&lt;END&gt;</code>标记时，预测阶段停止。</p><p id="38f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注:</strong>这只是对Seq2Seq的直观解释。我们为输入语言单词和目标语言单词创建单词嵌入。<a class="ae me" rel="noopener" target="_blank" href="/word-embeddings-for-nlp-5b72991e01d4"> <strong class="kk iu">嵌入</strong> </a> <strong class="kk iu">提供了单词及其相对意义的密集表示。</strong></p><p id="428e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何提高seq2seq模型的性能？</em>T11】</strong></p><ul class=""><li id="cedb" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">大型训练数据集</li><li id="4c0e" class="lf lg it kk b kl nd ko ne kr nf kv ng kz nh ld lk ll lm ln bi translated">超参数调谐</li><li id="e4c6" class="lf lg it kk b kl nd ko ne kr nf kv ng kz nh ld lk ll lm ln bi translated">注意机制</li></ul><p id="556a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">什么是注意机制？</em>T15】</strong></p><p id="894c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编码器将上下文向量或思想向量传递给解码器。思想或上下文向量是概括整个输入序列的单个向量。由于输入的单词对翻译的影响，可能需要更多的注意。</p><p id="49dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意机制基本思想是避免试图学习每个句子的单一向量表示。注意力机制基于注意力权重关注输入序列的某些输入向量。这允许解码器网络“聚焦”在编码器输出的不同部分。它使用一组<em class="le">注意力权重</em>对解码器自身输出的每一步执行此操作。</p><p id="debc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在下一篇文章中详细讨论注意力机制、光束搜索和BLEU评分</p><h2 id="95cf" class="ni nj it bd nk nl nm dn nn no np dp nq kr nr ns nt kv nu nv nw kz nx ny nz oa bi translated"><a class="ae me" href="https://arxiv.org/pdf/1703.01619.pdf" rel="noopener ugc nofollow" target="_blank">参考文献:</a></h2><div class="ob oc gp gr od oe"><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">从零开始的NLP:从序列到序列网络和注意力的翻译- PyTorch教程…</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">作者:Sean Robertson这是第三篇也是最后一篇关于“从零开始NLP”的教程，在这里我们编写自己的类…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">pytorch.org</p></div></div><div class="on l"><div class="oo l op oq or on os ly oe"/></div></div></a></div><p id="edc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae me" href="https://arxiv.org/pdf/1703.01619.pdf" rel="noopener ugc nofollow" target="_blank">神经机器翻译和序列对序列模型:教程格雷厄姆·纽比格</a></p><p id="9188" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae me" rel="noopener" target="_blank" href="/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7">https://towards data science . com/word-level-English-to-Marathi-neural-machine-translation-using-seq 2 seq-encoder-decoder-lstm-model-1a 913 F2 DC 4a 7</a></p><p id="70a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae me" href="https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf</a></p></div></div>    
</body>
</html>