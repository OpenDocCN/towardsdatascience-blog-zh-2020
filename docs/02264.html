<html>
<head>
<title>Named Entity Recognition (NER) with BERT in Spark NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark自然语言处理中基于BERT的命名实体识别(NER)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77?source=collection_archive---------2-----------------------#2020-03-04">https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77?source=collection_archive---------2-----------------------#2020-03-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a399" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用BERT在Spark NLP中的几行代码训练一个NER，获得SOTA精度。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a1ac62bbbfe69f19b9e8ca551a41225e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kLlLjLbezBtETU6z"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@jasminnb?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jasmin Ne </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5797" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NER是信息提取的一个子任务，它试图定位非结构化文本中提到的命名实体并将其分类成预定义的类别，例如人名、组织、位置、医疗代码、时间表达式、数量、货币值、百分比等。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/eb7497e915e5e4e0f4c9a4d15dbfc127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yg9wTrnX5sVS2otU"/></div></div></figure><p id="45bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NER在自然语言处理(NLP)的很多领域都有应用，它可以帮助回答很多现实世界的问题，比如:</p><ul class=""><li id="da21" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">新闻中提到了哪些公司？</li><li id="a362" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">哪些测试适用于患者(临床报告)？</li><li id="4ece" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">推文中有提到产品名称吗？</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/4a38fa0038e714447a04c0c3130c6335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DtoyvvlR3d8_8Xk3"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Spark自然语言处理库中的临床NER模型</p></figure><h1 id="6366" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">NER系统公司</h1><p id="ccac" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">NER是NLP和<a class="ae kv" href="https://arxiv.org/abs/1910.11470" rel="noopener ugc nofollow" target="_blank">的一个活跃话题，多年来研究工作一直在进行</a>。我们可以将这些努力总结为以下主题:</p><ol class=""><li id="ecf8" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nf lz ma mb bi translated">传统方法(基于规则)</li><li id="60e8" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nf lz ma mb bi translated">ML方法</li></ol><ul class=""><li id="1602" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">多类分类</li><li id="43e8" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">条件随机场</li></ul><p id="02ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.DL方法</p><ul class=""><li id="38eb" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1508.01991v1.pdf" rel="noopener ugc nofollow" target="_blank">双向LSTM-CRF </a></li><li id="007f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1511.08308.pdf" rel="noopener ugc nofollow" target="_blank">双向LSTM有线电视新闻网</a></li><li id="b5e0" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1603.01354.pdf" rel="noopener ugc nofollow" target="_blank">双向LSTM-CNN-CRF</a></li><li id="4bac" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">预先训练的语言模型(<a class="ae kv" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> Elmo </a>和<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a></li></ul><p id="6597" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<a class="ae kv" href="https://www.aclweb.org/anthology/P19-1527/" rel="noopener ugc nofollow" target="_blank">混合方法</a> (DL + ML)</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="eafb" class="mi mj iq bd mk ml nn mn mo mp no mr ms jw np jx mu jz nq ka mw kc nr kd my mz bi translated">NER和伯特在星火NLP</h1><p id="2de1" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">在本文中，我们将尝试向您展示如何使用BERT在<a class="ae kv" href="https://nlp.johnsnowlabs.com/" rel="noopener ugc nofollow" target="_blank"> Spark NLP库中构建一个最先进的NER模型。</a>我们要实现的模型受到了NER以前的艺术模型的启发:<a class="ae kv" href="https://arxiv.org/abs/1511.08308" rel="noopener ugc nofollow" target="_blank"> Chiu &amp; Nicols，<em class="ns">使用双向LSTM的命名实体识别-CNN </em> </a> <em class="ns"> </em>并且它已经嵌入在Spark NLP <a class="ae kv" href="https://nlp.johnsnowlabs.com/docs/en/annotators#ner-dl" rel="noopener ugc nofollow" target="_blank"> NerDL注释器</a>中。这是一种新颖的神经网络架构，使用混合双向LSTM和CNN架构自动检测单词和字符级特征，无需大多数特征工程。</p><p id="4366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开发该模型时，没有类似BERT的语言模型，并且在原始论文中使用了Glove单词嵌入。但是在本文中，我们将使用BERT替换Glove。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/063039f4ef1ab81ebb133c4eab44ae3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qmTNEKuZUXqudfkJdodifg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Spark NLP NER DL基准</p></figure><p id="0808" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Spark NLP中，有三种不同的预训练单词嵌入模型(及其变体),您可以根据您的使用情况即插即用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/6fd395e4cb4477053893eb0598eb7102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RLBbLg9JhdTx9VVZVc0fA.png"/></div></div></figure><p id="3294" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了简单和节省您的时间，我们将跳过安装步骤。我们已经有另一篇文章详细介绍了Spark NLP。而我们会做好<a class="ae kv" href="https://colab.research.google.com/notebooks/intro.ipynb#recent=true" rel="noopener ugc nofollow" target="_blank"> Colab </a>里的所有工作，让你直接跳到Spark NLP。</p><p id="438c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们开始吧！</p><p id="24b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从下载训练和测试集开始。我们将使用官方的CoNLL2003数据集，这是一个几乎在所有NER论文中都使用过的基准数据集。你可以在这里下载这个数据集<a class="ae kv" href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/resources/conll2003" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="42b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们简单解释一下什么是CoNLL。作为“<em class="ns">自然语言学习会议”的缩写，</em> CoNLL也是用于训练集以训练NER模型的注释的标准格式。这是它的样子。您可以在CONLL中注释自己的数据，然后在Spark NLP中训练一个自定义NER。还有一些免费的注释工具<a class="ae kv" href="https://medium.com/dida-machine-learning/the-best-free-labeling-tools-for-text-annotation-in-nlp-844525c5c65b" rel="noopener">可以用来标记你自己的数据。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/f3de7f61708f8bdeaa2f948354c85ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUmGvxuQ1ronRq5O8MvNnw.png"/></div></div></figure><p id="6b90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是我们要一一讲解的流程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/a523ee30b796ac1ffa950ce3a3e1bb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHZpqLbekQ8yocbBqrymjg.png"/></div></div></figure><p id="74af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们导入相关的包，然后开始编码。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="0973" class="oc mj iq ny b gy od oe l of og">from pyspark.sql import SparkSession<br/>from pyspark.ml import Pipeline</span><span id="8b21" class="oc mj iq ny b gy oh oe l of og">from sparknlp.annotator import *<br/>from sparknlp.common import *<br/>from sparknlp.base import *</span><span id="46b7" class="oc mj iq ny b gy oh oe l of og">import sparknlp<br/>spark = sparknlp.start()</span><span id="33ad" class="oc mj iq ny b gy oh oe l of og">print("Spark NLP version: ", sparknlp.version())<br/>print("Apache Spark version: ", spark.version)</span><span id="6067" class="oc mj iq ny b gy oh oe l of og"><em class="ns">&gt;&gt; Spark NLP version:  2.4.1 <br/>&gt;&gt; Apache Spark version:  2.4.4</em></span></pre><p id="2b09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想在GPU上训练你的模型，你可以用<code class="fe oi oj ok ny b">spark = sparknlp(gpu=True)</code> <em class="ns">开始你的Spark会话。</em></p><p id="3ad6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将CoNLL文件转换为Spark数据帧，并生成所有附加字段供以后使用。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="2c46" class="oc mj iq ny b gy od oe l of og">from sparknlp.training import CoNLL<br/>training_data = CoNLL().readDataset(spark, './eng.train')<br/>training_data.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/f6acfc1918d6cc2079453510cac10177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRueCqGu-xpkBmfnKIgvBQ.png"/></div></div></figure><p id="7393" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，输入数据集只有线和标注，现在这里有几列。Spark NLP中的CoNLL helper类将数据集作为输入，并准备我们将使用的NER算法所需的令牌、词性和句子。您需要做的就是准备或下载一个CoNLL格式的数据集。</p><p id="3074" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ns">(注意:尽管原文建议使用POS，但在Spark NLP培训中不会使用。因为POS标签是CoNLL格式的，所以CoNLL helper类从作为输入给出的CONLL文件中提取它们。因此，我们甚至可以用-X-来代替在源文件中张贴标签。)</em></p><p id="56f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是通过BERT获得单词嵌入。我们将使用名为<code class="fe oi oj ok ny b">BertEmbeddings()</code> <em class="ns">的Spark NLP注释器。</em></p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="b456" class="oc mj iq ny b gy od oe l of og">bert = BertEmbeddings.pretrained('bert_base_cased', 'en') \    .setInputCols(["sentence",'token'])\<br/>.setOutputCol("bert")\<br/>.setCaseSensitive(False)\<br/>.setPoolingLayer(0) # default 0</span></pre><p id="b632" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Spark NLP中，我们有四种预先训练好的BERT变体:<code class="fe oi oj ok ny b">bert_base_uncased</code>、<code class="fe oi oj ok ny b">bert_base_cased</code>、<code class="fe oi oj ok ny b">bert_large_uncased</code>、<code class="fe oi oj ok ny b">bert_large_cased</code>。使用哪一个取决于您的用例、训练集以及您试图建模的任务的复杂性。</p><p id="c7e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码片段中，我们基本上从Spark NLP公共资源加载了<code class="fe oi oj ok ny b">bert_base_cased</code>版本，并将<code class="fe oi oj ok ny b">sentence</code>和<code class="fe oi oj ok ny b">token</code>列指向<code class="fe oi oj ok ny b">setInputCols().</code>简而言之，<code class="fe oi oj ok ny b">BertEmbeddings()</code>注释器将获取<code class="fe oi oj ok ny b">sentence</code>和<code class="fe oi oj ok ny b">token</code>列，并在<code class="fe oi oj ok ny b">bert</code>列中填充Bert嵌入。一般来说，每个单词被翻译成一个768维的向量。参数<code class="fe oi oj ok ny b">setPoolingLayer()</code>可以设置为<code class="fe oi oj ok ny b">0</code>为第一层最快，<code class="fe oi oj ok ny b">-1</code>为最后一层，<code class="fe oi oj ok ny b">-2</code>为<code class="fe oi oj ok ny b">second-to-last-hidden layer</code>。</p><p id="d203" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ns">正如官方</em> <a class="ae kv" href="https://arxiv.org/pdf/1810.04805v2.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> BERT论文</em> </a> <em class="ns">的作者所解释的，不同的BERT层捕捉不同的信息。最后一层在预训练期间太接近目标函数(即，掩蔽语言模型和下一句预测)，因此它可能偏向那些目标。如果你想使用最后隐藏层，请随意设置</em> <code class="fe oi oj ok ny b"><em class="ns">pooling_layer=-1</em></code> <em class="ns">。直观上，</em> <code class="fe oi oj ok ny b"><em class="ns">pooling_layer=-1</em></code> <em class="ns">接近训练输出，所以可能会偏向训练目标。如果你不微调模型，那么这可能会导致一个坏的表现。也就是说，这是模型准确性和您拥有的计算资源之间的权衡问题。</em></p><p id="3c99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们导入<code class="fe oi oj ok ny b">NerDLApproach()</code>注释器，它是负责训练NER模型的主要模块。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="f4b0" class="oc mj iq ny b gy od oe l of og">nerTagger = NerDLApproach()\<br/>.setInputCols(["sentence", "token", "bert"])\<br/>.setLabelColumn("label")\<br/>.setOutputCol("ner")\<br/>.setMaxEpochs(1)\<br/>.setRandomSeed(0)\<br/>.setVerbose(1)\<br/>.setValidationSplit(0.2)\<br/>.setEvaluationLogExtended(True)\<br/>.setEnableOutputLogs(True)\<br/>.setIncludeConfidence(True)\<br/>.setTestDataset("test_withEmbeds.parquet")</span></pre><p id="f7bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们解释一下每个参数:</p><p id="2f8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setInputCols([“sentence”, “token”, “bert”])</code>:NER模型将用来生成特征的列。</p><p id="f628" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setLabelColumn(“label”)</code>:目标列</p><p id="4666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setOutputCol(“ner”)</code>:预测将被写入<code class="fe oi oj ok ny b">ner</code>栏</p><p id="325f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setMaxEpochs(1)</code>:训练的历元数</p><p id="7502" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setVerbose(1)</code>:训练时的日志级别</p><p id="b35a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setValidationSplit(0.2)</code>:每个历元上要对照模型验证的训练数据集的比例。该值应介于0.0和1.0之间，默认情况下为0.0和关闭。</p><p id="5881" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setEvaluationLogExtended(True)</code>:是否延长验证日志:显示每个标签的时间和评估。默认值为false。</p><p id="3c64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setEnableOutputLogs(True)</code>:是否输出到日志文件夹。当设置为真时，日志和训练指标将被写入主文件夹中的文件夹。</p><p id="6f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setIncludeConfidence(True)</code>:注释元数据中是否包含置信度得分。</p><p id="c8ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oi oj ok ny b">.setTestDataset(“test_withEmbeds.parquet”)</code>:测试数据集的路径。如果设置了，它将用于在训练期间对其进行统计。这也是一种CoNLL格式，但是嵌入像以前一样通过添加并保存到磁盘。如果你不需要在Spark中一个不可见的测试集上评估你的模型，你就不需要设置这个。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="564c" class="oc mj iq ny b gy od oe l of og">test_data = CoNLL().readDataset(spark, './eng.testa')<br/>test_data = bert.transform(test_data)<br/>test_data.write.parquet("test_withEmbeds.parquet")</span></pre><p id="577a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还可以设置学习率(<code class="fe oi oj ok ny b">setLr</code>)、学习率衰减系数(<code class="fe oi oj ok ny b">setPo</code>)、<code class="fe oi oj ok ny b">setBatchSize</code>和<code class="fe oi oj ok ny b">setDropout</code>速率。完整名单请见<a class="ae kv" href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach.scala" rel="noopener ugc nofollow" target="_blank">官方回购</a>。</p><p id="f4f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以在管道中添加这两个注释器。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="0806" class="oc mj iq ny b gy od oe l of og">ner_pipeline = Pipeline(stages = [bert, nerTagger])</span><span id="a4f4" class="oc mj iq ny b gy oh oe l of og">ner_model = ner_pipeline.fit(training_data)</span></pre><p id="a8ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据数据集大小和您设置的历元数完成拟合后，它就可以使用了。让我们看看仅仅一个时期后的日志是什么样子的(在你的主文件夹的<code class="fe oi oj ok ny b">annotators_log</code>文件夹中)。</p><p id="5e02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在<code class="fe oi oj ok ny b">Colab</code>实例的根目录下找到这个文件夹。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/53d61e8594524d97b717a49f26585c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9c5O0CwsMohSSNhV_AiDkA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拟合时，训练日志被写入磁盘</p></figure><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="3643" class="oc mj iq ny b gy od oe l of og"><em class="ns">Please check </em><a class="ae kv" href="https://nlp.johnsnowlabs.com/docs/en/graph" rel="noopener ugc nofollow" target="_blank"><em class="ns">here</em></a><em class="ns"> in case you get an IllegalArgumentException error with a description such as: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Generate graph by python code in python/tensorflow/ner/create_models before usage and use setGraphFolder Param to point to output.</em></span></pre><p id="046c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，我们可以用Spark NLP中的几行代码在SOTA算法中训练一个自定义的NER。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/55daff1491542a3c2a86dc472884e734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-ileni26iSa3J6U8d2kOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在Spark NLP中训练NER的整个流程</p></figure><p id="9797" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也可以将这个拟合的NER模型保存到磁盘上。由于NERDL模型是管道中的第二个阶段(第一个阶段是BertEmbedding annotator)，我们可以通过在阶段中建立索引来引用它。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="0511" class="oc mj iq ny b gy od oe l of og">ner_model.stages[1].write().save('NER_bert_20200221')</span></pre><p id="f14b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后使用<code class="fe oi oj ok ny b">NerDLModel </code>注释器加载回来。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="7bd1" class="oc mj iq ny b gy od oe l of og">loaded_ner_model = NerDLModel.load("NER_bert_20200221")\<br/>   .setInputCols(["sentence", "token", "bert"])\<br/>   .setOutputCol("ner")</span></pre><p id="38e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并在另一个流水线(即预测流水线)中使用</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/616efc15534fbe35aac77b0f22534351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyJu-5x9s9uU8BDgpkfRPA.png"/></div></div></figure><p id="1e98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拟合管道并获得预测。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="e101" class="oc mj iq ny b gy od oe l of og">text = "Peter Parker is a nice man and lives in New York"</span><span id="4f94" class="oc mj iq ny b gy oh oe l of og">prediction_data = spark.createDataFrame([[text]]).toDF("text")</span><span id="70c9" class="oc mj iq ny b gy oh oe l of og">prediction_model = custom_ner_pipeline.fit(prediction_data)</span><span id="3e39" class="oc mj iq ny b gy oh oe l of og">preds = prediction_model.transform(prediction_data)</span><span id="05bf" class="oc mj iq ny b gy oh oe l of og">preds.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/9e0e220ab8c616aa97b1f7c376534883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ApYT5cLgi_ONDL9JLwlJ-w.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/9c54786213e79fb63be6d243d743f626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvr8wnECpM_qWsaNhO1QLQ.png"/></div></div></figure><p id="34ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以编写一些PySpark函数，在NerConverter的<code class="fe oi oj ok ny b">ner_span</code>的帮助下，使输出更加漂亮和有用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/9f1491dff8fa2bccad817368fb03bb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLdGRhCagU1uyZb7XCd02g.png"/></div></div></figure><p id="7be5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也可以将此管道转换为LightPipeline，以消除开销中的火花，并进行光速推断。一旦您将任何拟合的管道转换为LightPipeline，您可以更快地轻松获得字符串或字符串列表的预测。你可以在这里找到更多关于光线管道的细节。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/95f105f514d9ca7fe5d18cd55a588878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FKF58H4xAq6JNLAB54EKg.png"/></div></div></figure><p id="3ec4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有代码都可以在<a class="ae kv" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>获得。在我们通过Github分享的笔记本中，你甚至会看到带有各种其他嵌入的附加实现，以及如何准备自己的CoNLL文件来训练定制的NER模型。</p><h1 id="3f2e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结论</h1><p id="2abd" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">目前就这些。在这篇文章中，我们讨论了如何在Spark NLP中与BERT(NLP镇上最酷的孩子)一起训练最先进的NER模型。我希望你喜欢！</p><p id="0bee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还举办了一场关于这个主题的网络研讨会，录音<a class="ae kv" href="https://register.gotowebinar.com/recording/8789202788019102477" rel="noopener ugc nofollow" target="_blank">可在此处</a>获得！</p><p id="9a81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们希望您已经阅读了我们的<a class="ae kv" href="https://medium.com/spark-nlp" rel="noopener">官方媒体页面</a>上的前几篇文章，并开始使用Spark NLP。以下是其他文章的链接。别忘了关注我们的页面，敬请期待！</p><p id="c923" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb#scrollTo=FcOwTCgB-VnC" rel="noopener ugc nofollow" target="_blank"> NER与伯特在Spark NLP笔记本</a>(本文)</p><p id="3bc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/4.NERDL_Training.ipynb#scrollTo=8NFG_NK6G67o" rel="noopener ugc nofollow" target="_blank">如何使用其他单词嵌入在Spark NLP中训练NER模型(Colab) </a></p><p id="20af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/spark-nlp/installing-spark-nlp-and-spark-ocr-in-air-gapped-networks-offline-mode-f42a1ee6b7a8" rel="noopener">在空气间隙网络中安装Spark NLP和Spark OCR(离线模式)</a></p><p id="7846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/spark-nlp/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59" rel="noopener">Spark NLP简介:基础和基本组件(第一部分)</a></p><p id="99d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/spark-nlp/introduction-to-spark-nlp-installation-and-getting-started-part-ii-d009f7a177f3?source=collection_home---6------0-----------------------" rel="noopener">Spark NLP简介:安装和入门(第二部分)</a></p><p id="d99d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos" rel="noopener ugc nofollow" target="_blank"> Spark NLP Youtube频道</a></p></div></div>    
</body>
</html>