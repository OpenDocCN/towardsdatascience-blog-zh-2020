<html>
<head>
<title>Improving upon Rosenblatt’s perceptron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">罗森布拉特感知机的改进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939?source=collection_archive---------42-----------------------#2020-05-25">https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939?source=collection_archive---------42-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ef0e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自适应线性神经元和Delta规则</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1457e4485364b32bcc22166ebe42afa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3yza9S1OCTAANFXGkgYQg.jpeg"/></div></div></figure><p id="6c02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习和人工智能已经在许多领域产生了变革性的影响，从医学科学(例如<a class="ae lq" href="https://arxiv.org/abs/1811.10052" rel="noopener ugc nofollow" target="_blank">成像和MRI </a>)到实时战略视频游戏(例如<a class="ae lq" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank">星际争霸2 </a>)。这些成功的关键促成因素是<em class="lr">深度神经网络</em>，其特点是所谓的隐藏层和人工神经元的数量不断增加。然而，必须强调的是，神经网络最初有着卑微的开端:当弗兰克·罗森布拉特(1928–1971)在1957年介绍他的感知机时，它只有一层由单个计算神经元组成，与今天可能有数百层和数千个神经元的神经网络相去甚远。尽管自20世纪50年代末以来发生了很多变化，但这些系列旨在引导学生了解神经网络的起源。这样做将更容易最终掌握深度学习所依赖的数学，以及现代神经网络是如何成为今天的样子的。为此，今天的主题是<em class="lr">自适应线性神经元和Delta规则</em>，这是由<a class="ae lq" href="https://en.wikipedia.org/wiki/Bernard_Widrow" rel="noopener ugc nofollow" target="_blank"> Bernard Widrow </a>和他的学生<a class="ae lq" href="https://en.wikipedia.org/wiki/Marcian_Hoff" rel="noopener ugc nofollow" target="_blank"> Ted Hoff </a>于1960年提出的。在开始之前，让我们快速回顾一下罗森布拉特的感知机。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a62c3ead36432776419b684ecb3f2888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*a1_7_BFQ40qRBOgAmjpP9A.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">左图:伯纳德·维德罗。右:马尔西安“特德”霍夫。</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="8434" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">快速回顾罗森布拉特的感知机(1957)</h1><div class="mw mx gp gr my mz"><a rel="noopener follow" target="_blank" href="/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">罗森布拉特的感知机，第一个神经网络</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">深度学习快速入门。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><p id="7c6a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当Rosenblatt在1957年首次为二进制分类问题引入感知器时，他的机器使用了一个单一的计算神经元，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/e14e35bf1ea7630d174f38e5d52e2819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BM8HuBXy9XYLvSGlBZwL0g.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">罗森布拉特感知机</p></figure><p id="4dc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定输入向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/a213efec2998aa0e3b9d875525fc2574.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*E_7dwaK5nN-_-KVfYcnk3Q.png"/></div></div></figure><p id="0de4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感知器首先计算加权和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6bdee91b54420fd5a35a8a0ad0855903.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*BL2rNyDSDwJr5JNGK7wP6g.png"/></div></figure><p id="5e30" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后将其传递给Heaviside函数以生成其输出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/76447df2c20dc5039b987888e2437461.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*uabYTVopTiUx7dIO1R9HNw.png"/></div></figure><p id="92b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最终输出是<strong class="kw iu"> <em class="lr"> x </em> </strong>所属的预测类(0或1)。尽管这种人工神经元的数学简单，罗森布拉特的主要成就是设计了一种算法，使它能够直接从训练数据中实际学习一组权重<strong class="kw iu"><em class="lr"/></strong>和偏差<em class="lr"> b </em>。今天，这种监督学习算法被称为<em class="lr">感知器学习算法</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/05e829e78c05060787333ddd5a4d56ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wje5-soLhLjDApnuPMwm2A.png"/></div></div></figure><h2 id="b3ff" class="nt mf it bd mg nu nv dn mk nw nx dp mo ld ny nz mq lh oa ob ms ll oc od mu oe bi translated">感知器训练程序的局限性</h2><p id="6c55" class="pw-post-body-paragraph ku kv it kw b kx of ju kz la og jx lc ld oh lf lg lh oi lj lk ll oj ln lo lp im bi translated">罗森布拉特的感知机并非没有局限性。其中一些根源于数学模型本身，已经在明斯基于1969年出版的一本臭名昭著的书中指出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/322d8c12873d04ee2f74d56099f67e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*TCro1Bla5_VOpUreyOujqg.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated"><strong class="bd ol">图1: </strong>玩具问题考虑。</p></figure><p id="c080" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了说明我们的观点，让我们考虑图1中的二元分类玩具问题。这和介绍<a class="ae lq" rel="noopener" target="_blank" href="/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a">罗森布拉特的感知机</a>时考虑的是一样的。然而，一个单独的训练示例被故意贴错了标签。使用标准的机器学习工具，人们会期望这一个错误标记的训练示例不会显著恶化分类模型的性能。虽然这种直觉对最近的模型可能是正确的，但我们会看到它不适用于罗森布拉特的感知机。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/de579e0baa5ee89c79d7fff032c1b9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*dVFMs2nE6T6QYlvEpU8new.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated"><strong class="bd ol">图2: </strong>罗森布拉特感知器的决策边界在100个历元上的演变。学习率设置为1。</p></figure><p id="b02c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图2描绘了当历元的数量从1到100变化时，感知机的决策边界的演变(即，我们循环通过整个训练数据集以学习权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>的次数)。很明显，决策边界到处都是。巧合的是，错误分类点的数量从1一直到100个数据点中的69。这种不稳定的行为不能通过降低学习速率(默认设置为1)来缓解，这是一种经典的建议。事实上，回头看看感知器学习算法(见算法1)，只要感知器误分类哪怕是一个点，权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>就会不断更新。然而，给定我们的训练数据集，感知器实际上不可能正确地分类所有的点，因此学习过程永远继续下去，永远不会收敛！然而，感知器学习算法的这种基本限制和缺乏鲁棒性可以通过稍微修改学习过程来消除。我们将在下文讨论的小修改产生了现在被称为<em class="lr"> ADALINE </em>的单层感知器。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="ca61" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">适应性线性神经元(1960年)</h1><p id="f723" class="pw-post-body-paragraph ku kv it kw b kx of ju kz la og jx lc ld oh lf lg lh oi lj lk ll oj ln lo lp im bi translated">像罗森布拉特的感知器一样，ADALINE(又名<em class="lr">自适应线性元素</em>或<em class="lr"> Widrow-Hoff规则</em>)是一个单层感知器。然而，它与它的不同之处在于它如何从数据中学习权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/678b75113127c76e28ceb9c80bf414af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQxszJbVie_AdBXthhQ0qg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">阿达林。与罗森布拉特感知器的图表进行比较。</p></figure><p id="ce23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">主要区别来自用于调整两个感知器的权重和偏差的反馈误差。Rosenblatt使用分类误差(即二进制值)，ADALINE引入了所谓的<em class="lr">损失函数</em>(有时也称为<em class="lr">成本函数</em>或<em class="lr">目标函数</em>)的概念，它依赖于量化前人工神经元的输出(即连续值)。虽然这可能看起来是一个很小的区别，但我们很快就会看到，当涉及到最终的模型时，这实际上是一个很大的区别！</p><p id="5f03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">损失函数的引入使得ADALINE比Rosenblatt的感知器更接近现代机器学习方法。给定我们的集合<em class="lr"> m </em>例<em class="lr">(</em><strong class="kw iu"><em class="lr">x</em></strong><em class="lr">【yₘ】</em>用<em class="lr"> yₘ </em> ∈ {0，1}表示类<strong class="kw iu"><em class="lr"/></strong><em class="lr">x</em>所属，ADALINE的损失函数定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/0a88fee74103f8032bcb17f20ef53a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*6uFYqzVoIhUxvs3XB9HZrA.png"/></div></figure><p id="baab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="lr"> φ(z) </em>为激活函数(即这里的恒等函数)。基于该定义，获得权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>作为最小化问题的解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c374134a1b7a961a4cc87c33b76f60c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*j_QCx9iM7y8Q1f9QWiyw-A.png"/></div></figure><p id="853a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">即，它们被选择为使得我们的示例集上的误差平方和尽可能小。</p><h2 id="8d25" class="nt mf it bd mg nu nv dn mk nw nx dp mo ld ny nz mq lh oa ob ms ll oc od mu oe bi translated">损失函数如何最小化？</h2><p id="6335" class="pw-post-body-paragraph ku kv it kw b kx of ju kz la og jx lc ld oh lf lg lh oi lj lk ll oj ln lo lp im bi translated">让我们看看如何找到最小化损失函数的权重和偏差集。为此，我们将依赖于损失函数是二次的以及相关的最小化问题是<a class="ae lq" href="https://en.wikipedia.org/wiki/Convex_optimization" rel="noopener ugc nofollow" target="_blank">凸</a>的事实。如果你不知道什么是凸优化问题，简单来说就是我们知道如何高效求解的优化问题。我们还需要一些基本的高中微积分。</p><p id="8392" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个凸函数ℒ <em class="lr"> (z) </em>只有一个最小值。它位于ℒ <em class="lr"> (z) </em>的斜率为零的点<em class="lr"> z </em>，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/417f8b1c2fedf0d99a89b005f35c285b.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*WXMOPRr9z2gs_B_UvkAOLQ.png"/></div></figure><p id="68cb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的目标是找到满足这个条件的一组权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>。这些是导致误差平方和尽可能小的权重和偏差(给定我们的数据)。</p><p id="aa77" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">德尔塔法则:</strong>记住一个函数的梯度表示正斜率最大的方向。因此，寻找函数最小值点的一个简单的启发式方法是向与梯度相反的方向移动。因此，让我们首先计算损失函数的梯度。对于一般激活函数<em class="lr"> φ(z) </em>，它由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/933f556ec3b263ef28a2dd58298547db.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*iByXpKFFpYk5rF2fUoMPTg.png"/></div></figure><p id="6a7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/1397f95c1b989da9b596d31334e417fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*ZmNdZdpnqyj7Tn7KGvUnIQ.png"/></div></figure><p id="311c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用<em class="lr">φ’(z)</em>激活函数相对于<em class="lr"> z </em>的导数。注意ADALINE代表ADA <em class="lr">感受性线性神经元。</em>它的激活是恒等式，即<em class="lr"> φ(z) = z </em>从而<em class="lr"> φ'(z) = 1 </em>。因此，损失函数的梯度简化为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d626d0343b267661d27aa3d81b7983d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*3wuL78qvYM_ub0iS95qzSQ.png"/></div></figure><p id="cb9f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8fe5b0d7635d63591865fb51eb73ec87.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*el-qxyX3QFsRqtUiM23iTQ.png"/></div></figure><p id="e30c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从给定的一组权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>开始，delta规则规定，为了减少误差平方和，这些权重和偏差需要更新如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e384d9279b87cd0f93e15e59b90a9018.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*2YmxpssFaEQOuU6nyuj5rQ.png"/></div></figure><p id="dd8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中更新∈<strong class="kw iu"><em class="lr">w</em></strong>和∈<em class="lr">b</em>由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3276b7f4705d23b2fe3b7a36780d9d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*4sd2Ah5q84iKiOQ511Jgfg.png"/></div></figure><p id="6ef0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4c5d382859deae5afe39030ff39638d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*kYxAsMHeUZ698XCPQddllA.png"/></div></figure><p id="9e94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，α是一个标量，通常称为<em class="lr">学习率</em>或<em class="lr">步长</em>。最简单的形式是，德尔塔法则假设α常数。变量<strong class="kw iu"> <em class="lr"> w </em> </strong>和<em class="lr"> b </em>持续更新，直到执行了规定的迭代次数，或者更新的范数小于用户定义的容差。这就是事情的全部。</p><p id="349e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，德尔塔法则背后的数学和哲学都很简单。尽管如此，它仍然是用于训练更深层次神经网络的更一般的反向传播算法的一个特例，因此它在本系列中很重要。不要犹豫重新推导所有的数学作为练习。同时，现在让我们转移到有趣的东西，用Python实现ADALINE。假设您已经熟悉Python，下面的代码应该是不言自明的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="c673" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了清晰和易用，我们将在整个系列中坚持使用scikit-learn API。这段代码的扩展版本也可以在我的TowardsDataScience Github repo上获得(<a class="ae lq" href="https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><h2 id="ba8e" class="nt mf it bd mg nu nv dn mk nw nx dp mo ld ny nz mq lh oa ob ms ll oc od mu oe bi translated"><strong class="ak"> ADALINE vs. Rosenblatt的感知器</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6476232c5c2ce7e11af6c6b7fa3aa76e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*W6_m7Ek_nejGIYt1bDv2iA.gif"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated"><strong class="bd ol">图4: </strong>随着训练的进行，训练数据集中误分类点的数量。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e7a73cd54e140d8dcb15ab8ff54bca5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*PDoj0V8BEeNOrfGnUed0pg.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated"><strong class="bd ol">图5: </strong>罗森布拉特感知器(浅灰色)和ADALINE(黑色)的决策边界在100个历元上的演变。在这两种情况下，学习率都设置为0.1。</p></figure><p id="f186" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图4显示了Rosenblatt的感知器和ADALINE在训练过程中错误分类点的数量，而图5描述了两个模型的决策界限的演变。罗森布拉特的感知器无处不在，在某个特定时期的准确率接近99%，而在下一个时期的准确率不到50%。这种鲁棒性的缺乏来自于一个错误标记的数据点阻止罗森布拉特的感知机学习任何东西。</p><p id="b698" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相比之下，随着训练的进行，ADALINE的误分类点的数量单调减少。看ADALINE的决策边界的演化，可以看出它比Rosenblatt的感知器要平滑得多。这种更平滑的演变来自于成本函数的定义，使得训练过程能够基于对当前预测的错误程度的一些评估，而不是简单地基于它是否错误(如罗森布拉特的感知器的情况)，在每一步对权重<strong class="kw iu"><em class="lr">【w】</em></strong>和偏差<em class="lr"> b </em>执行小的调整。由损失函数编码的信息因此使得ADALINE的训练更加健壮和良好。最后要注意的是，即使这两个类不是线性可分的(由于一个错误标记的数据点)，ADALINE的最终线性决策边界仍然是基于最小化问题的公式以有原则的方式选择的，从而确保它在某种意义上是最优的。关于Rosenblatt的感知器的决策边界，没有这样的事情可以说，因为对于这里考虑的问题，它根本不存在，感知器学习算法不能收敛。这是这两个单层感知器的主要区别…</p><h2 id="f235" class="nt mf it bd mg nu nv dn mk nw nx dp mo ld ny nz mq lh oa ob ms ll oc od mu oe bi translated">ADALINE普通最小二乘法是变相的吗？</h2><p id="105b" class="pw-post-body-paragraph ku kv it kw b kx of ju kz la og jx lc ld oh lf lg lh oi lj lk ll oj ln lo lp im bi translated">在结束之前，让我们试着更好地理解ADALINE实际上在学习什么。虽然它用于分类目的，但它与最简单的回归模型之一的<a class="ae lq" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘法</a> (OLS)密切相关。当观察损失函数时，这种紧密的联系清晰可见</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a0ea78ee3e48b8d47e4eedf2804bdd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*fVUG7xUcmRVBioOsad52kg.png"/></div></figure><p id="5824" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用简单的变量变化，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c905c79f3018b5ed4b3c769897e56154.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*vHx5ERNADUlFj8p96tm7kw.png"/></div></figure><p id="13b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/a54820d9930c2c88f3720095167fab4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*Lmvk02L9gYZbFSSu1vNXcQ.png"/></div></figure><p id="83dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个损失函数可以重写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/83ce9a7239545d6f6e42cb6689d4420d.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*IXZw84uCH4OQ9o7mrozNFg.png"/></div></figure><p id="30ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> X </strong>的每一行都是给定的训练示例。这个损失函数不过是普通最小二乘最小化的二次损失函数。最优解的封闭形式是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/afad085648c0ff2f9a8a26c79d64a49c.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*5yajin9dyLkUVEZ1rYaiBw.png"/></div></figure><p id="b1e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，两个模型之间的这种等价允许我们将ADALINE的权重<strong class="kw iu"> <em class="lr"> w </em> </strong>和偏差<em class="lr"> b </em>理解为出现在超平面的方程中的系数，该方程在最小二乘意义上最好地分离了两个类别。</p><p id="5947" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管ADALINE改进了Rosenblatt的感知器，但我们将在接下来的帖子中看到<em class="lr">在最小二乘意义上解决问题</em>并不是一个人可以为分类问题做的最合适的事情…</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="1c0f" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="ef6e" class="pw-post-body-paragraph ku kv it kw b kx of ju kz la og jx lc ld oh lf lg lh oi lj lk ll oj ln lo lp im bi translated">这篇文章是我的深度学习初学者系列的第二篇。这本书应该更早出版，但生活另有决定(新生儿、新房子、工作等)。至于罗森布拉特的感知机的第一篇文章，我知道将自适应线性神经元的文章标记为深度学习可能有些牵强，但相信我，我们会实现的。与此同时，不要犹豫地玩代码(见<a class="ae lq" href="https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>)，如果有兴趣，自己重新推导所有的数学。没那么复杂。如前所述，ADALINE比Rosenblatt的感知器更接近现代机器学习方法。因此，当我们转向现代神经网络时，很好地理解它所依赖的数学将是非常有价值的！</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="493e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在接下来的几篇文章中，我们将讨论以下主题:</p><ul class=""><li id="98f5" class="pe pf it kw b kx ky la lb ld pg lh ph ll pi lp pj pk pl pm bi translated">感知器收敛定理。</li><li id="4bc3" class="pe pf it kw b kx pn la po ld pp lh pq ll pr lp pj pk pl pm bi translated">XOR:深度学习的卑微开端。</li></ul><p id="592b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，您会在下面找到一个与ADALINE相关主题的附加在线资源列表。不要犹豫，看看这些，因为它们可能会处理一些我们可能会忽略的方面！</p><p id="9cc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">PS:如果你知道任何其他相关链接，不要犹豫给我发消息，我会编辑帖子来添加它:]</p><h2 id="3ab9" class="nt mf it bd mg nu nv dn mk nw nx dp mo ld ny nz mq lh oa ob ms ll oc od mu oe bi translated">其他在线资源</h2><ul class=""><li id="5850" class="pe pf it kw b kx of la og ld ps lh pt ll pu lp pj pk pl pm bi translated">Sebastian Raschka关于<a class="ae lq" href="https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#references" rel="noopener ugc nofollow" target="_blank">单层神经网络和梯度下降</a>的博客文章以及他的<a class="ae lq" href="https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html" rel="noopener ugc nofollow" target="_blank">机器学习常见问题</a>。</li><li id="c1c2" class="pe pf it kw b kx pn la po ld pp lh pq ll pr lp pj pk pl pm bi translated">如果你想更多地了解泰德霍夫:<a class="ae lq" href="https://computerhistory.org/profile/ted-hoff/" rel="noopener ugc nofollow" target="_blank">https://computerhistory.org/profile/ted-hoff/</a></li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><blockquote class="pv pw px"><p id="df51" class="ku kv lr kw b kx ky ju kz la lb jx lc py le lf lg pz li lj lk qa lm ln lo lp im bi translated"><em class="it">想要阅读更多此类内容？</em>查看我其他关于<a class="ae lq" href="https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea" rel="noopener">低秩结构和数据驱动建模</a> <em class="it">的文章或者干脆我的</em> <a class="ae lq" href="https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5" rel="noopener"> <em class="it">机器学习基础知识</em> </a>！</p></blockquote><div class="mw mx gp gr my mz"><a rel="noopener follow" target="_blank" href="/binary-cross-entropy-and-logistic-regression-bf7098e75559"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">二元交叉熵和逻辑回归</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">有没有想过我们为什么使用它，它来自哪里，如何有效地优化它？这里有一个解释(代码…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div></div></div>    
</body>
</html>