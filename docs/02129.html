<html>
<head>
<title>K-Nearest Neighbors (kNN) — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻(kNN)-解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-knn-explained-cbc31849a7e3?source=collection_archive---------6-----------------------#2020-02-29">https://towardsdatascience.com/k-nearest-neighbors-knn-explained-cbc31849a7e3?source=collection_archive---------6-----------------------#2020-02-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0cc0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细的理论解释和 scikit-learn 实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/17056d1f4865235ae0e59ae497969ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*f5CmwipUB_I8BvDFwY26hg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://questioneverything39.wordpress.com/2018/04/16/no-matter-what-group-you-join-its-all-group-think/" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="8516" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">k-最近邻(kNN)是一种受监督的机器学习算法，可用于解决分类和回归任务。我认为 kNN 是一种来自<em class="lr">现实生活</em>的算法。人们往往会受到周围人的影响。我们的行为受到从小一起长大的朋友的指导。我们的父母也在某些方面塑造了我们的性格。如果你和热爱运动的人一起长大，你很可能最终也会热爱运动。当然也有例外。kNN 的工作原理类似。</p><blockquote class="ls"><p id="2f64" class="lt lu it bd lv lw lx ly lz ma mb lq dk translated"><em class="mc">一个数据点的价值是由其周围的数据点决定的。</em></p></blockquote><ul class=""><li id="dc65" class="md me it kx b ky mf lb mg le mh li mi lm mj lq mk ml mm mn bi translated">如果你有一个非常亲密的朋友，并且大部分时间都和他/她在一起，你们最终会分享相似的兴趣和享受相同的东西。也就是 k=1 的 kNN。</li><li id="979b" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">如果你总是和一群<em class="lr"> 5 人在一起，</em>这个群体中的每个人都会对你的行为产生影响，你最终会成为 5 人中的平均水平。那就是 kNN，其中<em class="lr"> k=5 </em>。</li></ul><p id="c17c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">kNN 分类器通过多数表决原则确定数据点的类别。如果 k 设置为 5，则检查 5 个最近点的类。根据多数类进行预测。类似地，kNN 回归取 5 个最近点的平均值。</p><p id="bdea" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们观察接近的人，但是数据点是如何确定接近的？测量数据点之间的距离。测量距离的方法有很多。<a class="ae ku" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>(闵可夫斯基距离与 p=2)是最常用的距离度量之一。下图显示了如何计算二维空间中两点之间的欧几里德距离。它是使用点的 x 和 y 坐标之差的平方来计算的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a62ac7b480e5f1010ea21f13ce9796c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*mFNKqdk8OEoRuZTD6Gnepw.png"/></div></figure><p id="be1f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在上面的例子中，欧几里德距离是(16 + 9)的平方根，也就是 5。二维中的欧几里得距离让我们想起了著名的<a class="ae ku" href="https://en.wikipedia.org/wiki/Pythagorean_theorem" rel="noopener ugc nofollow" target="_blank">勾股定理</a>。</p><p id="087b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于二维空间中的两个点来说，这似乎非常简单。每个维度代表数据集中的一个未来。我们通常有许多具有许多特征的样品。为了能够清楚地解释这个概念，我将回顾一个二维空间中的例子(即 2 个特征)。</p><blockquote class="mu mv mw"><p id="aca9" class="kv kw lr kx b ky kz ju la lb lc jx ld mx lf lg lh my lj lk ll mz ln lo lp lq im bi translated">你可以访问这个 github <a class="ae ku" href="https://github.com/SonerYldrm/kNN_Algorithm" rel="noopener ugc nofollow" target="_blank"> repo </a>中的所有代码。放心用吧！</p></blockquote><p id="665d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们从导入库开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d284a1d7e4754a3c3d473da3ddea8fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*5UJ8J6Wjr5p022KFs5SnRw.png"/></div></figure><blockquote class="mu mv mw"><p id="c8a6" class="kv kw lr kx b ky kz ju la lb lc jx ld mx lf lg lh my lj lk ll mz ln lo lp lq im bi translated">Scikit-learn 提供了许多有用的功能来创建合成数据集，这对实践机器学习算法非常有帮助。我将使用<strong class="kx iu"><em class="it">make _ blobs</em></strong><em class="it">函数。</em></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f7316ee37bf397f8c7b7b4197c741c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*x9TXSH2pvTgkKkGYsXDCdw.png"/></div></figure><p id="9dfb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">此代码创建了一个数据集，包含 100 个样本，分为 4 类，特征数为 2。使用相关参数可以轻松调整样本、特征和类别的数量。我们还可以调整每个集群(或类)的分布程度。让我们想象一下这个合成数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7cd4dbaf5778709c331e2fbc57a12abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*mIZIT4gU00epZDGczf5csw.png"/></div></figure><p id="4f8f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于任何有监督的机器学习算法，将数据集划分为训练集和测试集是非常重要的。我们首先训练模型，并使用数据集的不同部分对其进行测试。如果这种分离没有完成，我们基本上是用模型已经知道的一些数据来测试模型。我们可以使用<strong class="kx iu"> train_test_split </strong>函数轻松实现这种分离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ed001ee0389caf508f5cc44b507e4f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*FdaWpR7lVALxwVYY6D-i0w.png"/></div></figure><p id="0211" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以分别使用<strong class="kx iu"> train_size </strong>或<strong class="kx iu"> test_size </strong>参数指定有多少原始数据用于训练或测试集。训练集的默认分离度为 75%，测试集的默认分离度为 25%。</p><p id="7f15" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后我们创建一个 kNN 分类器对象。为了显示 k 值的重要性之间的差异，我创建了 k 值为 1 和 5 的两个分类器。然后使用训练集训练这些模型。<strong class="kx iu"> n_neighbors </strong>参数用于选择 k 值。默认值为 5，因此不必显式写入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/b9cf2b7ce47afa04592a7869f77fc0ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*liV2Z3nVikWSLwUzjDXfTg.png"/></div></div></figure><p id="9f0d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后我们预测测试集中的目标值，并与实际值进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b3d71f7d80fc2f384216218dad45f657.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*yqCc4NTu8cIBV650tyIxkg.png"/></div></figure><p id="8028" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了查看 k 值的影响，让我们可视化 k=5 和 k=1 的测试集和预测值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5254eafd8711597c3b6e5abc97e47b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*atqT9EydQN3YGTxtCLU-RA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/6f8cb96fb392454349cc9bbdddadab8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*0fhvGdmPoECczxRGFW7Vlg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/007d834cfe04116ddf8b895d8635c146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*kYtAkcvW1GoOa8Eieqm2Ow.png"/></div></figure><p id="bb47" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">结果似乎非常相似，因为我们使用了一个非常小的数据集。然而，即使在小数据集上，不同的 k 值对某些点的预测也是不同的。</p><h1 id="63e8" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">如何找到最佳 k 值</h1><ul class=""><li id="1156" class="md me it kx b ky od lb oe le of li og lm oh lq mk ml mm mn bi translated"><strong class="kx iu"> k=1 </strong>:模型太具体，没有很好的概括。它对噪音也很敏感。该模型在训练集上实现了高精度，但是在新的、以前看不到的数据点上将是差的预测器。因此，我们很可能以一个过度拟合的模型而告终。</li><li id="ff92" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><strong class="kx iu"> k=100 </strong>:该模型过于一般化，在训练集和测试集上都不是一个好的预测器。这种情况被称为欠适配。</li></ul><p id="7625" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们如何找到最佳的 k 值？Scikit-learn 提供了<strong class="kx iu"> GridSearchCV </strong>函数，使我们能够轻松检查 k 的多个值。让我们使用 scikit-learn datasets 模块下的数据集来查看一个示例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/bfb2c8d7e3ef74b44da43c4351e1d00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*6DXcH7ypnMZmNXF6uGZhWQ.png"/></div></figure><p id="135d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在导入所需的库并加载数据集之后，我们可以创建一个 GridSearchCV 对象。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d3f67a90d873f43d6b9814f7eb966682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*MflibQv6WVklZtwF6UTcHg.png"/></div></figure><p id="69ca" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们不需要分割数据集，因为<strong class="kx iu"> cv </strong>参数分割数据集。cv 参数的缺省值是 5，但是我明确地写了它来强调为什么我们不需要使用 train_test_split。</p><p id="43a4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">cv=5 基本上将数据集分成 5 个子集。GridSearchCV 进行 5 次迭代，每次使用 4 个子集进行训练，1 个子集进行测试。通过这种方式，我们能够将所有数据点用于训练和测试。</p><p id="e682" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以使用<strong class="kx iu"> best_params_ </strong>方法检查哪些参数能给出最佳结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/167bcbb055664063f2f2a060796ddf2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*xoH9lp6f1HzCuMETwy6ygA.png"/></div></figure><p id="0caa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这种情况下，k 的最佳值是 12。</p><h1 id="1b08" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated"><strong class="ak">k 近邻的利弊</strong></h1><p id="c83b" class="pw-post-body-paragraph kv kw it kx b ky od ju la lb oe jx ld le ol lg lh li om lk ll lm on lo lp lq im bi translated"><strong class="kx iu">优点</strong></p><ul class=""><li id="af3f" class="md me it kx b ky kz lb lc le oo li op lm oq lq mk ml mm mn bi translated">简单易懂</li><li id="3028" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">不做任何假设，因此可以在非线性任务中实现。</li><li id="ccfd" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">适用于多个类别的分类</li><li id="d15b" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">处理分类和回归任务</li></ul><p id="5c52" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">缺点</strong></p><ul class=""><li id="4e39" class="md me it kx b ky kz lb lc le oo li op lm oq lq mk ml mm mn bi translated">随着数据点数量的增加，会变得非常慢，因为模型需要存储所有的数据点。</li><li id="2b96" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">内存效率不高</li><li id="dc76" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated">对异常值敏感。离群也有一票！</li></ul></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><p id="80d1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="9936" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">我关于机器学习算法的其他帖子</h1><ul class=""><li id="644e" class="md me it kx b ky od lb oe le of li og lm oh lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/naive-bayes-classifier-explained-50f9723571ed">朴素贝叶斯分类器—解释</a></li><li id="1694" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-explained-593e9ddb7c6c">逻辑回归—已解释</a></li><li id="2ba2" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd">支持向量机—解释</a></li><li id="8872" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/decision-tree-and-random-forest-explained-8d20ddabc9dd">决策树和随机森林—解释</a></li><li id="93ce" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af">梯度增强决策树—解释</a></li><li id="6663" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/principal-component-analysis-explained-d404c34d76e7">主成分分析—解释</a></li><li id="88a1" class="md me it kx b ky mo lb mp le mq li mr lm ms lq mk ml mm mn bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/predicting-used-car-prices-with-machine-learning-fea53811b1ab">用机器学习预测二手车价格</a></li></ul></div></div>    
</body>
</html>