# [知识蒸馏] FitNets:薄深网的提示

> 原文：<https://towardsdatascience.com/knowledge-distillation-fitnets-hints-for-thin-deep-nets-c64840aa2baa?source=collection_archive---------56----------------------->

![](img/1538ca3fe5845bf577dc90fbdb7c9872.png)

图片由 [emmaws4s](https://pixabay.com/users/emmaws4s-14995841/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4784916) 来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4784916)

**注意——YouTube 上还有一段视频解释了这篇论文**

# 解决的问题

本文首先提出了一个案例，即宽而深的模型通常需要大量的乘法运算，这会导致很高的内存和计算需求。正因为如此，即使该网络在准确性方面是一个最高性能的模型，它在现实世界中的应用也是有限的。

为了解决这种问题，我们需要执行模型压缩(也称为知识蒸馏),将知识从一个繁琐的模型转移到一个参数较少的简单模型。

这篇论文背后的主要驱动力是，到目前为止，知识提炼方案已经考虑了与教师网络规模相同或更小的学生网络。

他们提请注意这样一个事实，即迄今为止，学生网络的深度还没有被考虑。

> 比教师网络层数多但每层神经元数量少的学生网络称为薄深度网络。

# 现有技术及其局限性

现有技术可以从两个不同的角度来看。

第一个视角是知识提炼的技术。自从这篇论文问世几个月后的 [**在神经网络中提炼知识的**](https://arxiv.org/abs/1503.02531) 论文它是唯一的现有技术。

[](/paper-summary-distilling-the-knowledge-in-a-neural-network-dc8efd9813cc) [## 【论文摘要】提取神经网络中的知识

### 知识蒸馏研究领域起步论文的关键见解

towardsdatascience.com](/paper-summary-distilling-the-knowledge-in-a-neural-network-dc8efd9813cc) 

作者在这里提出了一个例子，当更平滑的 softmax 技术起作用时；迄今为止，还没有人注意到学生网络的深度。

他们认为

> **深度是表征学习的一个基本方面，它导致更抽象和不变的表征，因此我们应该考虑深而薄的学生网络**

然而，他们也意识到更深层次网络(尤其是更细的更深层次网络)的训练可能非常具有挑战性。这个挑战是关于优化问题(例如，消失梯度)，因此第二个现有技术观点来自于过去在解决深层网络的优化问题上所做的工作。

也就是说，这篇论文以及推而广之，所有被引用的论文都是 Resnet 时代之前的！

> Resnet 架构在解决消失梯度等问题上发挥了重要作用，并为创建更深层次的模型铺平了道路。因此，对我们来说，试图理解这里过去引用的作品并不重要。

接下来，我们将讨论本文如何试图规避训练更深但更薄的学生网络的挑战

# 关键见解

因此，第一个也是最主要的景象就是我们已经讨论过的，也就是说，建立一个更薄但更深入的学生网络是一个好主意。

本文的重点转移到解决他们在一个更薄更深的学生网络中可能面临的优化问题。

由于初始层的学习是有帮助的，他们决定通过使学生网络的一部分模仿教师网络的一部分来帮助学生网络。例如，如果我们在学生模型中有一层学习预测教师模型中一层的输出，那么我们可能会缓解与培训相关的问题。

![](img/1cd4bd643e7e8951452bd20b14b337c2.png)

来源:从报纸上剪下

学生应该学会预测教师输出的那一层被称为“**提示**层

来自学习的学生网络的层被称为“**引导的**层。

你现在可以考虑有一个目标函数，它接受**提示和引导层**的输出，并最小化损失。

然而，这导致了一个有趣的问题，即在教师模型中选择哪些层作为提示层，同样地，在学生模型中哪一层应该作为引导层。

> **作者指出，如果导向层更靠近输出层，则可能会过度正则化。为什么这种特殊的安排导致过度正规化是没有解释的！**

然后他们提到，他们最终选择了学生的中间层作为引导层，暗示层也是如此，即教师的中间层

这个决定很可能是他们实验的结果。

# 它是如何工作的？

如上所述，您可以选择教师模型的中间层作为提示层，选择学生模型的中间层作为引导层，这样可以最大限度地减少预测中的损失。

这基本上是一个 2 阶段的培训过程。在第一阶段，使用预先训练的教师模型来训练网络直到引导层。

> 也就是说，这创造了一个有趣的情况。由于学生模型应该更薄，引导层的神经元数量将少于提示层。因此，我们无法真正比较这两层的输出

为了解决这个问题，他们在引导层前面放了一个回归量，使维度与提示层的维度相同。下图显示了这一点。

![](img/b6f1fa339b111f0bc9efb60196ffcf92.png)

来源—作者

作者随后在多个数据集(CIFAR 10、CIFAR 100 和 SVHN)上进行了实验，获得了显著的结果。

![](img/69dcbfd22a70f287ea0410688f5b4981.png)

来源—摘自论文

以上结果基于 CIFAR 10 数据集。教师模型有 5 层，900 万个参数。他们用不同的层数训练了 4 个不同的学生网络(称为 FitNet)。从表中可以看出，FitNet 1 只有 250K 参数，精度下降了 1%多一点。

FitNet 2、3 和 4 比教师网络具有更好的准确性，从而证明深度和提示训练都有效。

以下是这篇论文的一些见解/观察-

*   基于提示的机制确实简化了训练过程
*   由于阶段式设置，学生可以更好地进行初始化
*   暗示训练似乎也有助于正规化。结果表明了这一点

# 各种链接和详细信息

## 论文有开源实现吗？

Github 上有多种实现。这里我提供了这篇论文的第一作者 https://github.com/adri-romsor/FitNets 的实现链接

## 这篇论文是在一次会议上发表的吗？

是的。这篇论文被 2015 年 ICLR 会议接受，被引用了 1000 多次

论文链接—【https://arxiv.org/abs/1412.6550 

## 有解释论文的视频吗？

是的。我为这篇论文制作了 YouTube 视频。

# 我的观点、问题和要点

*   一篇写得非常好的论文强调(并证明)了深度在学生网络中的重要性。
*   对**提示和引导层**的选择可以进行更详细的研究。
*   我认为 Resnet 应该关注优化问题(消失梯度等)，但该技术对学生网络的初始化仍然有用。这至少有助于加快收敛速度！

希望你喜欢这个摘要，我可能误解/曲解了论文的某些部分，因此，如果有的话，错误是我的，而不是原论文作者的。