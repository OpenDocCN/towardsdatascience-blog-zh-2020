# 人工智能安全、人工智能伦理和 AGI 辩论

> 原文：<https://towardsdatascience.com/ai-safety-ai-ethics-and-the-agi-debate-d5ffaaca2c8c?source=collection_archive---------58----------------------->

## [苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

## Alayna Kennedy 在 [TDS 播客](https://towardsdatascience.com/podcast/home)

![](img/f918ea4e242e67cfca0a9f851d5fa7e9.png)

*编者按:迈向数据科学播客的“攀登数据科学阶梯”系列由 Jeremie Harris 主持。Jeremie 帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。可以听下面的播客:*

我们大多数人认为，影响我们的决定应该通过推理过程做出，将我们信任的数据与我们认为可以接受的逻辑结合起来。

只要人类在做这些决定，我们就可以探究这个推理，看看我们是否同意它。例如，我们可以问为什么我们被拒绝银行贷款，或者为什么法官宣布了一个特别的判决。

但是今天，机器学习正在自动化处理越来越多的这些重要决策。我们的生活越来越多地被我们无法询问或理解的决策过程所支配。更糟糕的是，机器学习算法可能会出现偏差或犯下严重错误，因此由算法运行的世界有可能成为反乌托邦式的黑箱统治，这可能比我们今天拥有的最不完美的人类设计的系统还要糟糕。

这就是为什么人工智能伦理和人工智能安全近年来吸引了如此多的关注，也是为什么我非常兴奋地与 IBM 的数据科学家 Alayna Kennedy 交谈，她的工作专注于机器学习的伦理，以及与基于 ML 的决策相关的风险。Alayna 曾为美国政府人工智能工作的主要参与者提供咨询，并通过之前在神经网络建模和欺诈检测方面的工作，拥有在行业中应用机器学习的专业知识。

以下是我从对话中获得的一些最大收获:

*   机器学习模型通常带有少数“标准”损失函数，每个人都同意这些函数“工作得很好”(例如，准确性、AUC 分数、分类交叉熵等)。不幸的是，我们已经确定了这些标准指标的事实可能会诱使我们停止批判性地思考优化了什么。有时，具有最佳准确性或最佳 F1 分数的模型只能通过牺牲我们应该关心的其他事情来达到这种性能水平。我们倾向于自动驾驶并接受“标准”度量，因为它们是标准的，这可能导致危险的结果。
*   人工智能伦理面临的最大挑战之一是，我们甚至还没有接近制定出人类伦理。这意味着我们不得不将我们甚至无法达成一致的规则硬编码到我们甚至无法审计其推理的模型中。
*   尽管在关键的伦理问题上缺乏广泛的共识，但许多国家的政府已经制定了非常一致的伦理框架。
*   如今，人工智能安全的一个不太受重视的领域是人工智能失控的风险；我们对人工智能安全的大部分关注都指向更直接、更实际的问题。Alayna 和我不同意这是否是一件好事。你在这个问题上的立场取决于你认为 AGI 在近期或中期发展的可能性有多大(我认为这种可能性令人不安，但 Alayna 不同意)。

你可以在这里的推特上关注阿莱娜，也可以在这里的推特上关注我[。](https://twitter.com/jeremiecharris)

我们正在寻找能与我们的观众分享有价值的东西的客人。如果你碰巧知道谁是合适的人选，请在这里告诉我们:【publication@towardsdatascience.com】T4。