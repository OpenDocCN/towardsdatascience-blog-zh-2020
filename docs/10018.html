<html>
<head>
<title>An Introduction to Artificial Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-artificial-neural-networks-5d2e108ff2c3?source=collection_archive---------10-----------------------#2020-07-15">https://towardsdatascience.com/an-introduction-to-artificial-neural-networks-5d2e108ff2c3?source=collection_archive---------10-----------------------#2020-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="44c5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用人工神经网络提升你的模型性能。Tensorflow 中的一个演练！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/29124c4fa90f1e6971bc92678ae91b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rf8XW5C8yPOXsyvU"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莫里茨·金德勒在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="39e8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">人工神经网络</h1><p id="dc42" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工神经网络(ANN)是一种深度学习算法，它是从人类大脑的<strong class="lt iu">生物神经网络的思想中产生和进化而来的。模拟人脑工作的尝试最终导致了人工神经网络的出现。人工神经网络的工作方式非常类似于生物神经网络，但并不完全类似于它的工作方式。</strong></p><p id="2fa9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">ANN 算法只接受数字和结构化数据作为输入。为了接受非结构化和非数字数据格式，如图像、文本和语音，分别使用<strong class="lt iu">卷积神经网络(CNN)</strong>和<strong class="lt iu">递归神经网络(RNN) </strong>。在这篇文章中，我们只关注人工神经网络。</p><h1 id="fe3c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">生物神经元 vs 人工神经元</h1><h2 id="e0ff" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">生物神经元的结构及其功能</h2><ul class=""><li id="ea7e" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated"><strong class="lt iu">树突</strong>接收输入信号。</li><li id="a612" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated"><strong class="lt iu"> Soma </strong>(细胞体)负责处理输入，携带生化信息。</li><li id="3e70" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated"><strong class="lt iu">轴突</strong>为管状结构，负责信号的传递。</li><li id="fb8d" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated"><strong class="lt iu">突触</strong>存在于轴突末端，负责连接其他神经元。</li></ul><h2 id="2545" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">人工神经元的结构及其功能</h2><ul class=""><li id="f9e5" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">一个单层的神经网络被称为<strong class="lt iu">感知器</strong>。一个多层感知器被称为<strong class="lt iu">人工神经网络。</strong></li><li id="70d7" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">神经网络可以拥有任意数量的层。每层可以有一个或多个神经元或单元。每一个神经元都是相互连接的。每层也可以有不同的<strong class="lt iu">激活功能</strong>。</li><li id="e20c" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">人工神经网络包括两个阶段<strong class="lt iu">正向传播和反向传播。</strong>正向传播包括乘以权重、添加偏差、对输入应用激活函数并将其向前传播。</li><li id="37cb" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">反向传播步骤是最重要的步骤，通常涉及通过在神经网络层的反向传播来寻找模型的最佳参数。反向传播需要<strong class="lt iu">优化函数</strong>找到模型的最佳权重。</li><li id="e03d" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">通过相应地改变输出层的激活函数，人工神经网络可以应用于<strong class="lt iu">回归和分类任务</strong>。(二分类用 Sigmoid 激活函数，多类分类用 Softmax 激活函数，回归用线性激活函数)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/e8409e5d2a8b4185b7091a19e86871df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIg6rUi5Mr0Co8NqcErSMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">感知器。<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Artificial_Neuron.svg" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><h1 id="b5be" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么是神经网络？</h1><ul class=""><li id="b301" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">当数据量增加时，传统的机器学习算法往往表现在相同的水平上，但当数据量巨大时，人工神经网络优于传统的机器学习算法，如下图所示。</li><li id="9c36" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated"><strong class="lt iu">特征学习</strong>。人工神经网络试图以逐层递增的方式进行分级学习。由于这个原因，没有必要明确地执行特征工程。</li><li id="2095" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">神经网络可以处理图像、文本和语音等非结构化数据。当数据包含非结构化数据时，使用诸如 CNN(卷积神经网络)和 RNN(递归神经网络)的神经网络算法。</li></ul><h1 id="bc2e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">安是如何工作的</h1><p id="70dc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工神经网络的工作可以分为两个阶段，</p><ul class=""><li id="cb82" class="ne nf it lt b lu mn lx mo ma nt me nu mi nv mm nj nk nl nm bi translated">正向传播</li><li id="feb4" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">反向传播</li></ul><h2 id="350d" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">正向传播</h2><ul class=""><li id="f8ef" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">前向传播包括将特征值与权重相乘，添加偏差，然后对神经网络中的每个神经元应用激活函数。</li><li id="49d2" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">将特征值乘以权重，并给每个神经元加上偏差，基本上就是应用<strong class="lt iu">线性回归</strong>。如果我们对它应用 Sigmoid 函数，那么每个神经元基本上都在执行一个<strong class="lt iu">逻辑回归。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/814a62546c986fc85ed5abb841a5c9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZyHN5j51J92nwzdVcaFrtQ.png"/></div></div></figure><h2 id="037c" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">激活功能</h2><ul class=""><li id="5bbe" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">激活功能的目的是将<strong class="lt iu">非线性</strong>引入数据。引入非线性有助于识别复杂的潜在模式。它还用于将值缩放到特定的区间。例如，sigmoid 激活函数在 0 和 1 之间缩放该值。</li></ul><h2 id="656b" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">逻辑函数或 Sigmoid 函数</h2><ul class=""><li id="c9b1" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">Logistic/ Sigmoid 函数在 0 和 1 之间调整数值。</li><li id="6f75" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">它用于二进制分类的输出层。</li><li id="302f" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">这可能会在反向传播过程中导致<strong class="lt iu">消失梯度</strong>问题，并减慢训练时间。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9c2572932df6f40938ae3b2087326795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*NSdUKVuBQt5vokwJHCwKCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Sigmoid 函数</p></figure><h2 id="4fa0" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">Tanh 函数</h2><ul class=""><li id="307c" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">Tanh 是<strong class="lt iu">双曲正切</strong>的简称。双曲正切函数在-1 和 1 之间调整数值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b2694e58cbb60cdb332b544c263f6175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*tAE9A9tDlhnAGXq_-5y2JQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双曲正切函数</p></figure><h2 id="09e6" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">ReLU 函数</h2><ul class=""><li id="8b25" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated"><strong class="lt iu"> ReLU(整流线性单元)</strong>当 x &gt;为 0 时输出相同的数字，当 x &lt;为 0 时输出 0。</li><li id="05f2" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">它防止了<strong class="lt iu">消失梯度</strong>问题，但是在反向传播期间引入了<strong class="lt iu">爆炸</strong>梯度问题<strong class="lt iu">。爆炸梯度问题可以通过覆盖梯度来防止。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/aee714eeba2151d84cbcc5fe2d5aefea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*fALttJAfgDC1xxPgHoTpIQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ReLU 函数</p></figure><h2 id="2660" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">泄漏 ReLU 函数</h2><ul class=""><li id="5ec4" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">Leaky ReLU 非常类似于 ReLU，但是当 x &lt;0 it returns (0.01 * x) instead of 0.</li><li id="cb81" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">If the data is normalized using Z-Score it may contain negative values and ReLU would fail to consider it but leaky ReLU overcomes this problem.</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/004a55187f2abf4099d5708e049a9910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Uvovl617zz6eJVjQTVi3hQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Leaky ReLU function</p></figure><h2 id="7f67" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">Backpropagation</h2><ul class=""><li id="2aa7" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">Backpropagation is done to find the <strong class="lt iu">为模型的参数</strong>的最优值时，通过相对于<strong class="lt iu">参数</strong>对损失函数的<strong class="lt iu">梯度进行部分微分来迭代更新参数。</strong></li><li id="4e82" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">应用优化函数来执行反向传播。优化函数的目标是找到参数的最佳值。</li></ul><p id="806a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可用的优化功能有:</p><ul class=""><li id="9ecd" class="ne nf it lt b lu mn lx mo ma nt me nu mi nv mm nj nk nl nm bi translated">梯度下降</li><li id="c050" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">Adam 优化器</li><li id="6c19" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">动量梯度下降</li><li id="fd13" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">均方根支柱(均方根支柱)</li></ul><p id="e479" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">微积分的<strong class="lt iu">链式法则</strong>在反向传播中起着重要的作用。下面的公式表示损失(L)相对于权重/参数(w)的部分微分。</p><p id="3437" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">重量‘w’的微小变化会影响值‘z’的变化(∂𝑧/∂𝑤).值‘z’的微小变化会影响激活‘a’的变化(∂a/∂z).激活‘a’的微小变化会影响损失函数‘l’的变化(∂L/∂a).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/aa4d1512c17d7ae03a1fb5e2dc38ec3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*jV3Ph2feCAOEZDi9-skl1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">链式法则</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/8adffb217cf14a88433d1ba3531578e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PPYx9CIhxDQ5gF6WNE9TA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">链式规则中值的描述</p></figure><h1 id="4e52" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">术语:</strong></h1><h2 id="9c9b" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">韵律学</h2><ul class=""><li id="0e2b" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">指标用于衡量模型的性能。</li><li id="2109" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">度量函数类似于成本函数，只是在训练模型时不使用评估度量的结果。请注意，您可以使用任何成本函数作为度量。</li><li id="c67c" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">我们使用均方对数误差作为度量和成本函数。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/2af4973a98e612b3d4808f36f1a2945e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bgzceS9jybK6YzuRrGTbPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">均方对数误差(MSLE)和均方根对数误差(RMSLE)</p></figure><h2 id="ee13" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">世</h2><ul class=""><li id="3866" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">训练数据的一次传递称为一个时期。训练数据以小批的形式被馈送到模型，并且当训练数据的所有小批被馈送到构成时期的模型时。</li></ul><h2 id="b3e4" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">超参数</h2><p id="f319" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">超参数是不是由模型产生的<strong class="lt iu">可调参数</strong>，这意味着用户必须为这些参数提供一个值。我们提供的超参数值会影响训练过程，因此超参数优化可以提供帮助。</p><p id="43b1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个 ANN 模型中使用的超参数是，</p><ul class=""><li id="08e4" class="ne nf it lt b lu mn lx mo ma nt me nu mi nv mm nj nk nl nm bi translated">层数</li><li id="eded" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">一层中单元/神经元的数量</li><li id="c60c" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">激活功能</li><li id="9134" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">权重的初始化</li><li id="9303" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">损失函数</li><li id="1900" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">公制的</li><li id="ab4c" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">【计算机】优化程序</li><li id="32c3" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">时代数</li></ul><h1 id="0911" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Tensorflow 中的神经网络编码</h1><h2 id="ba04" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">加载预处理的数据</h2><p id="26bf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你输入给人工神经网络的数据必须经过彻底的预处理，以产生可靠的结果。训练数据已经过预处理。所涉及的预处理步骤是，</p><ul class=""><li id="9b91" class="ne nf it lt b lu mn lx mo ma nt me nu mi nv mm nj nk nl nm bi translated">老鼠归罪</li><li id="4de6" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">对数变换</li><li id="6ffc" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">平方根变换</li><li id="52f9" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">顺序编码</li><li id="b635" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">目标编码</li><li id="112a" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">z 分数标准化</li></ul><p id="af96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有关上述步骤的详细实施，请参考我的数据预处理笔记本</p><p id="6d50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://www.kaggle.com/srivignesh/data-preprocessing-for-house-price-prediction" rel="noopener ugc nofollow" target="_blank">笔记本链接</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="b969" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">神经架构</h2><ul class=""><li id="93a0" class="ne nf it lt b lu lv lx ly ma ng me nh mi ni mm nj nk nl nm bi translated">我们将要使用的人工神经网络模型由七层组成，包括一个输入层、一个输出层和五个隐藏层。</li><li id="5841" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">第一层(输入层)由 128 个具有 ReLU 激活功能的单元/神经元组成。</li><li id="7f61" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">第二、第三和第四层由 256 个具有 ReLU 激活功能的隐藏单元/神经元组成。</li><li id="3976" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">第五层和第六层由 384 个具有 ReLU 激活功能的隐藏单元组成。</li><li id="2721" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">最后一层(输出层)由一个单个神经元组成，该神经元输出形状为(1，N)的数组，其中 N 是特征的数量。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="5ccf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">在我的 Kaggle 笔记本里找到这个帖子:</strong><a class="ae ky" href="https://www.kaggle.com/srivignesh/introduction-to-ann-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/srivignesh/introduction-to-ann-in-tensor flow</a></p><p id="bf12" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">参考文献:</strong></p><p id="9274" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[1]吴恩达，<a class="ae ky" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习专业化</a>。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="bcea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="om">联系我上</em><a class="ae ky" href="https://www.linkedin.com/in/srivignesh-rajan-123569151/" rel="noopener ugc nofollow" target="_blank"><em class="om">LinkedIn</em></a><em class="om">，</em><a class="ae ky" href="https://twitter.com/RajanSrivignesh" rel="noopener ugc nofollow" target="_blank"><em class="om">Twitter</em></a><em class="om">！</em></p><p id="5469" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">快乐深度学习！</strong></p><h2 id="0ba6" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">谢谢你！</h2></div></div>    
</body>
</html>