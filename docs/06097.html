<html>
<head>
<title>Perplexity in Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型的困惑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perplexity-in-language-models-87a196019a94?source=collection_archive---------1-----------------------#2020-05-18">https://towardsdatascience.com/perplexity-in-language-models-87a196019a94?source=collection_archive---------1-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/03e213e1d7b656db4efe203bc798f949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7D8RH2qEXhFpCPsiMjPH5Q.png"/></div></div></figure><div class=""/><div class=""><h2 id="56c0" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">使用加权分支因子评估NLP模型</h2></div><p id="aafa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">困惑度是自然语言处理中评估模型的一个有用的度量。本文将介绍它通常的两种定义方式以及它们背后的直觉。</p><h1 id="53a0" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">概述</h1><ol class=""><li id="e2a0" class="mh mi je kv b kw mj kz mk lc ml lg mm lk mn lo mo mp mq mr bi translated"><strong class="kv jf">语言模型的快速回顾</strong></li><li id="7a4f" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo mo mp mq mr bi translated"><strong class="kv jf">评估语言模型</strong></li><li id="3d6c" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo mo mp mq mr bi translated"><strong class="kv jf">作为测试集的归一化逆概率的困惑<br/> </strong> 3.1测试集的概率<br/> 3.2归一化<br/> 3.3将所有这些集合在一起</li><li id="5d1c" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo mo mp mq mr bi translated"><strong class="kv jf">作为交叉熵的指数的困惑<br/> </strong> 4.1 <strong class="kv jf"> </strong>语言模型的交叉熵<br/> 4.2加权分支因子:掷骰子<br/> 4.3加权分支因子:语言模型</li><li id="9dcc" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo mo mp mq mr bi translated"><strong class="kv jf">总结</strong></li></ol></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="efe7" class="lp lq je bd lr ls ne lu lv lw nf ly lz kk ng kl mb kn nh ko md kq ni kr mf mg bi translated">1.语言模型的快速回顾</h1><p id="0219" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated"><strong class="kv jf">语言模型</strong>是给单词和句子分配概率的统计模型。通常，我们可能会试图猜测<strong class="kv jf"> <em class="nm">中的下一个词</em> </strong> w在一个句子中给出的所有前一个词，通常称为<strong class="kv jf"> <em class="nm">【历史】</em> </strong>。<br/>例如，给定历史“晚餐我正在做__ ”,下一个单词是“水泥”的概率是多少？下一个单词是“fajitas”的概率有多大？希望是P(我正在做的晚餐用的fajitas)&gt;P(我正在做的晚餐用的水泥)。</p><p id="c389" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们也经常对我们的模型分配给由单词序列(<em class="nm"> w_1，w_2，…，w_N </em>)组成的完整句子<em class="nm"> W </em>的概率感兴趣。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/d9c4aec23514e16427791bf698e57bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-q6tP_puKuXeKxJHgb-nw.png"/></div></div></figure><p id="cf55" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，我们想要一个模型来分配更高的概率给真正的句子和语法正确的句子。</p><p id="2232" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一个<strong class="kv jf">unigram</strong>T40】模型只在单个单词的层面上起作用。给定单词序列W，单字模型将输出概率:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/1e13faa2f93088a596edf6b242c855e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FErL5jasVdEiVttinHl1mg.png"/></div></div></figure><p id="cda1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">其中个体概率P(w_i)可以例如基于训练语料库中单词的频率来估计。</p><p id="4343" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一个<strong class="kv jf"> n-gram模型</strong>，相反，查看前(n-1)个单词来估计下一个。例如，三元模型会查看前面的两个单词，因此:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/4b8b734db91fca7f257ed60351bd5a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKc2r7crfhGCa2uKhM_o0w.png"/></div></div></figure><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/5cc843b1a6ae1ee4fa8826fa9fb096ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tZibfu0eGz2ZYW-g.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">单字对单字</p></figure><p id="763a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">语言模型可以<strong class="kv jf">嵌入</strong>更复杂的系统中，以帮助执行语言任务，如翻译、分类、语音识别等。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="dae6" class="lp lq je bd lr ls ne lu lv lw nf ly lz kk ng kl mb kn nh ko md kq ni kr mf mg bi translated">2.评估语言模型</h1><p id="7ce2" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated"><strong class="kv jf">困惑</strong>是语言模型的<strong class="kv jf">评估度量</strong>。但是我们为什么要用它呢？为什么不能只看我们最终系统在我们关心的任务上的损失/精度？</p><p id="c341" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">事实上，我们可以使用两种不同的方法来评估和比较语言模型:</p><ul class=""><li id="c854" class="mh mi je kv b kw kx kz la lc nz lg oa lk ob lo oc mp mq mr bi translated"><strong class="kv jf">外在评价</strong>。这包括通过在实际的<strong class="kv jf">任务</strong>(例如机器翻译)中使用这些模型来评估它们，并查看它们的最终损失/准确性。这是最好的选择，因为这是切实看到不同模型如何影响我们感兴趣的任务的唯一方法。然而，由于它需要训练一个完整的系统，所以在计算上可能是昂贵和缓慢的。</li><li id="0dfc" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo oc mp mq mr bi translated"><strong class="kv jf">内在评价</strong>。这包括找到一些度量标准来评估语言模型本身，而不考虑它将被用于的特定任务。虽然内在评估作为最终指标不如外在评估“好”，但它是快速比较模型的有用方式。<strong class="kv jf">困惑是一种内在的评价方法。</strong></li></ul><h1 id="8342" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">3.作为测试集的归一化逆概率的困惑</h1><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/dac59fd79f7ababe9b46f1e9dd04cdff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGceUxPPeIgE-V1m3SF-SA.png"/></div></div></figure><p id="2d87" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这可能是困惑最常见的定义。在这一节中，我们将看到它为什么有意义。</p><h2 id="e929" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">3.1测试集的概率</h2><p id="1c0e" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">首先，什么构成了一个好的语言模型？如前所述，我们希望我们的模型将高概率分配给真实且语法正确的句子，而将低概率分配给虚假、不正确或非常罕见的句子。假设我们的数据集由事实上真实和正确的句子组成，这意味着最佳模型将是为测试集分配<strong class="kv jf">最高概率的模型。直观地说，如果一个模型给测试集分配了一个高概率，这意味着看到它<strong class="kv jf">并不惊讶</strong>(它没有<em class="nm">被它迷惑</em>，这意味着它对语言如何工作有很好的理解。</strong></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/1c6ab00842d43bcb04f0f680473fa01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5kBR7XsQqRiu0p_CZEk1w.png"/></div></div></figure><h2 id="8552" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">3.2标准化</h2><p id="a947" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">然而，值得注意的是数据集可以有<strong class="kv jf">不同数量的句子</strong>，句子可以有不同数量的单词。显然，添加更多的句子会带来更多的不确定性，所以在其他条件相同的情况下，一个较大的测试集比一个较小的测试集有更低的概率。理想情况下，我们希望有一个独立于数据集大小的指标。我们可以通过用总字数对测试集<strong class="kv jf">的概率<strong class="kv jf">进行归一化</strong>来得到这个结果，这将给我们一个<strong class="kv jf">每个单词的度量</strong>。</strong></p><p id="d7be" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们该怎么做？如果我们想要归一化的是一些项的总和，我们可以用它除以单词数来得到每个单词的度量。但是单词序列的概率是由乘积给出的。<br/>例如，让我们来看一个unigram模型:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/3641fa6b22c42c0266c7ec4c1269a64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncwppAoxUEqtfesCKsl1PA.png"/></div></div></figure><p id="8cb1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们如何使这个概率正常化？通过查看对数概率更容易做到这一点，对数概率将乘积转换为总和:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/e4e83c242b38c4beccc76738c6e686da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z87T9Yi2FUIXqjxKo9MQjA.png"/></div></div></figure><p id="7dc1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在可以通过除以N来标准化它，以获得每个单词的对数概率:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/e0a80751611f01b60133bac5eec41b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lVcWf6827EvXDN3-CDT5Q.png"/></div></div></figure><p id="262f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">…然后通过指数运算删除日志:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/e0ec462010a874933315afac8e1228ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XY4A7eN7cLMSaTxs2yHbOg.png"/></div></div></figure><p id="7b4d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以看到，通过取第N个根，我们已经得到了<strong class="kv jf">归一化。</strong></p><h2 id="d685" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">3.3将所有内容整合在一起</h2><p id="c4a6" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">现在回到我们最初的困惑等式，我们可以看到，我们可以将其解释为测试集、<strong class="kv jf">的</strong>、<strong class="kv jf">的<strong class="kv jf">逆概率，通过测试集中的字数</strong>:</strong></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/273e99c9f842b71efc8c2b99ab311c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOHsEeShuNltqImceRakZQ.png"/></div></div></figure><p id="2e77" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">注意:</p><ul class=""><li id="0156" class="mh mi je kv b kw kx kz la lc nz lg oa lk ob lo oc mp mq mr bi translated">由于我们采用的是逆概率，较低的<strong class="kv jf">复杂度</strong>表示更好的<strong class="kv jf">模型</strong></li><li id="3c3b" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo oc mp mq mr bi translated">在这种情况下，W是测试集。它包含了所有句子一个接一个的单词序列，包括句首和句尾标记，<sos>和<eos>。<br/>例如，一个包含两个句子的测试集应该是这样的:<br/> W = ( &lt; SOS &gt;，this，is，the，first，sentence，。，&lt; EOS &gt;，&lt; SOS &gt;，这个，是，那个，第二个，一个，。，&lt; EOS &gt; ) <br/> N是我们测试集中所有记号的计数，包括SOS/ EOS和标点符号。在上面的例子中，N = 16。如果我们愿意，我们也可以计算单个句子的困惑度，在这种情况下，W将仅仅是那一个句子。</eos></sos></li></ul><h1 id="2533" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">4.作为交叉熵指数的困惑</h1><p id="0207" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated"><strong class="kv jf">注</strong>:如果你需要复习一下熵，我强烈推荐<a class="ae ov" href="https://arxiv.org/pdf/1405.2061.pdf" rel="noopener ugc nofollow" target="_blank">这篇由Sriram Vajapeyam撰写的</a>文献。</p><p id="1989" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">困惑也可以定义为交叉熵的指数:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/db727b034b7d62c6b04c3a119853fbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9l_gtwNAup5luu_kdbahQ.png"/></div></div></figure><p id="6a03" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我们可以很容易地检查出这实际上等价于前面的定义:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/16c932983a8cafbdad61b804217663d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGbdURk1kLY4iXWRQl_nvg.png"/></div></div></figure><p id="7b35" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是我们如何根据交叉熵来解释这个定义呢？</p><h2 id="ab2d" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">4.1语言模型的交叉熵</h2><p id="fad9" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">我们知道熵可以解释为在变量中存储信息所需的<em class="nm">平均位数，它由下式给出:</em></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/d22763047ac22f1635827a6f3a99e0f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfY_yQk3LbjoxHC-HcxoDw.png"/></div></div></figure><p id="375c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们还知道，<strong class="kv jf">交叉熵</strong>由下式给出:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/9e80d58224520b180727769adfe32800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QhAXZPsxifZcZnnXmYgtqA.png"/></div></div></figure><p id="a9e9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我们使用一个<strong class="kv jf">估计的分布</strong> q来代替真实的概率分布p，它可以被解释为在一个变量中存储信息所需的平均比特数。</p><p id="36c8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们的例子中，p是我们语言的真实分布，而q是我们的模型在训练集上估计的分布。显然，我们无法知道真正的p，但给定足够长的单词序列W(如此大的N)，我们可以使用香农-麦克米兰-布雷曼定理来近似每个单词的交叉熵(有关更多细节，我推荐<a class="ae ov" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf#page=21" rel="noopener ugc nofollow" target="_blank">【1】</a>和<a class="ae ov" href="http://www.inf.ed.ac.uk/teaching/courses/dil/slides/dil04.pdf#page=4" rel="noopener ugc nofollow" target="_blank">【2】</a>):</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/8287dd60a51b90efbcbd32b972c2f65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hptl4gGI4_8VTXv2AlooQQ.png"/></div></div></figure><p id="f0a1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们重写这段代码，以便与上一节中使用的符号保持一致。给定长度为N的单词序列W和经过训练的语言模型P，我们将交叉熵近似为:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/5ba2fba4588ebf3663d9d7f92f7f81ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Ugpz3UmJgUhGJWa61TUhw.png"/></div></div></figure><p id="e0fb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们再来看看我们对困惑的定义:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/db727b034b7d62c6b04c3a119853fbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9l_gtwNAup5luu_kdbahQ.png"/></div></div></figure><p id="e283" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">根据我们对交叉熵的了解，我们可以说H(W) <strong class="kv jf">是编码每个单词</strong>所需的<em class="nm">平均</em>比特数。这意味着困惑度<em class="nm"> 2^H(W) </em>是可以使用<strong class="kv jf"><em class="nm">【w】</em></strong><strong class="kv jf">位</strong>编码的平均字数。</p><p id="5fac" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对此我们该如何解读？我们可以把困惑看作是<strong class="kv jf">加权分支因子</strong>。</p><h2 id="fc57" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">4.2加权分支系数:滚动模具</h2><p id="99c8" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">所以我们说:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/9fe0856392d9186fa96563a3bd1f28ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1H1KzEwHyqMHMYHNSieVA.png"/></div></div></figure><p id="ee14" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，如果我们发现H(W) = 2，这意味着平均每个字需要2位来编码，使用2位我们可以编码2 = 4个字。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/e117f5897edf2c81dc5fc8ac1d92c392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9FJtu1CQeTbxkEFhpPk5A.png"/></div></div></figure><p id="5ef7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但这意味着什么呢？为了简单起见，让我们暂时忘记语言和文字，想象我们的模型实际上试图预测掷骰子的结果。普通模具有6个面，因此模具的<strong class="kv jf">分支系数</strong>为6。分支因子简单地表明了当我们掷骰子时有多少可能的结果。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/1dcd0e7d129c56a61992d9f6f0fc89ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9tkq1CSiQuxTjUBZT7Azbw.png"/></div></div></figure><p id="75f7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">假设我们在这个公平骰子上训练我们的模型，模型知道我们每次掷骰子都有1/6的概率得到任何一方。然后，假设我们通过滚动骰子10次以上来创建一个测试集，我们获得了(高度缺乏想象力的)结果序列T = {1，2，3，4，5，6，1，2，3，4}。我们的模型在这个测试集上的困惑是什么？</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/5090615ebfe995e5263ea2d284c7ece4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_iUX_dmbeqWPrtRXB2Wrtw.png"/></div></div></figure><p id="dacd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以困惑符合分支因子。</p><p id="2caf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们想象一下，我们有一个<strong class="kv jf">不公平骰子</strong>，它以7/12的概率掷出一个6，所有其他面各有1/12的概率。我们再次在用这个不公平骰子创建的训练集上训练一个模型，以便它学习这些概率。然后，我们通过掷骰子12次来创建一个新的测试集T:我们在7次掷骰子中得到6，在剩下的5次掷骰子中得到其他数字。现在的困惑是什么？</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/1c953aaff47022424d64a05399f53f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKiG6ocldXLF0EXK_El8dQ.png"/></div></div></figure><p id="aba1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">困惑度降低</strong>。这是因为我们的模型现在知道滚动6比任何其他数字都更有可能，所以看到1就不那么“惊讶”了，而且由于测试集中的6比其他数字多，所以与测试集相关联的总体“惊讶”更低。<em class="nm">分支因子</em>仍然是6，因为所有6个数字在任何掷骰子中都是可能的选项。然而，<strong class="kv jf"> <em class="nm">加权</em>分支因子</strong>现在更低，因为一个选项比其他选项更有可能。这就好比说，在这些新的条件下，在每一次掷骰子时，我们的模型都是结果不确定的，就好像它必须在4个不同的选项中做出选择，而不是所有方面都有相同概率的6个选项。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/8a1cf5fad524cc915368197d3680526d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_lEwZgqBP7ybJI92qTRlA.png"/></div></div></figure><p id="fb4b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了进一步澄清这一点，我们把它推到极致。假设我们现在有一个不公平的骰子，给出一个概率为99%的6，其他数字的概率各为1/500。我们再次在这个骰子上训练模型，然后用100个骰子创建一个测试集，其中我们得到6 99次，另一个数字一次。现在的困惑是:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/2672a3211beef9a33ac4903f9fe03fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pw7LgNV00Cuzgou55bG6aQ.png"/></div></div></figure><p id="08ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">分支因子仍然是6，但加权分支因子现在是1，因为在每次掷骰子时，模型几乎可以确定它将是6，这是理所当然的。因此，虽然从技术上来说<em class="nm">在每一次掷骰子时仍有6个可能的选项，但只有一个选项是最受欢迎的。</em></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/b1d6b9896d43c9a040439195da2f909d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU-Ii-Ac2uQGbsM3CbublA.png"/></div></div></figure><h2 id="3148" class="oe lq je bd lr of og dn lv oh oi dp lz lc oj ok mb lg ol om md lk on oo mf op bi translated">4.3加权分支因子:语言模型</h2><p id="4616" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">让我们把它与语言模型和交叉熵联系起来。</p><p id="aa29" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，如果我们有一个试图猜测下一个单词的语言模型，分支因子就是每个点上可能出现的单词的数量，也就是词汇量的大小。</p><p id="2fb7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们之前说过，语言模型中的困惑是使用<em class="nm">【W】</em><em class="nm">位</em>可以编码的平均字数。我们现在可以看到，这仅仅代表了模型的平均分支因子<strong class="kv jf">。正如我们之前所说，如果我们发现交叉熵值为2，这表明困惑度为4，这是“可以编码的平均字数”，这就是平均分支因子。所有这一切意味着，当<strong class="kv jf">试图猜测下一个单词时，我们的模型</strong> <strong class="kv jf"> <em class="nm">和</em>一样困惑，好像它必须在4个不同的单词</strong>之间进行选择。</strong></p><h1 id="9cbf" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">5.摘要</h1><ul class=""><li id="136e" class="mh mi je kv b kw mj kz mk lc ml lg mm lk mn lo oc mp mq mr bi translated">困惑是用来判断一个语言模型有多好的度量标准</li><li id="b869" class="mh mi je kv b kw ms kz mt lc mu lg mv lk mw lo oc mp mq mr bi translated">我们可以将困惑度定义为测试集、<strong class="kv jf">的<strong class="kv jf">逆概率，由</strong>的字数归一化:</strong></li></ul><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/dac59fd79f7ababe9b46f1e9dd04cdff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGceUxPPeIgE-V1m3SF-SA.png"/></div></div></figure><ul class=""><li id="bd27" class="mh mi je kv b kw kx kz la lc nz lg oa lk ob lo oc mp mq mr bi translated">或者，我们可以通过使用<strong class="kv jf">交叉熵</strong>来定义困惑度，其中交叉熵表示编码一个单词所需的平均比特数，而困惑度是可以用这些比特编码的单词数:</li></ul><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/db727b034b7d62c6b04c3a119853fbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9l_gtwNAup5luu_kdbahQ.png"/></div></div></figure><ul class=""><li id="438a" class="mh mi je kv b kw kx kz la lc nz lg oa lk ob lo oc mp mq mr bi translated">我们可以把困惑解释为加权分支因子。如果我们的困惑度是100，这意味着每当模型试图猜测下一个单词时，它就像必须在100个单词之间进行选择一样困惑。</li></ul><h1 id="8f72" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">有用的参考资料</h1><p id="97b3" class="pw-post-body-paragraph kt ku je kv b kw mj kf ky kz mk ki lb lc nj le lf lg nk li lj lk nl lm ln lo im bi translated">[1] Jurafsky，d .和Martin，J. H. <a class="ae ov" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">语音和语言处理。第三章:N元语言模型</a> <em class="nm">(草案)</em> (2019)。<br/>【2】Koehn，P. <a class="ae ov" href="http://www.inf.ed.ac.uk/teaching/courses/dil/slides/dil04.pdf" rel="noopener ugc nofollow" target="_blank">语言建模(二):平滑与回退</a> (2006)。数据密集型语言学<em class="nm">(讲座幻灯片)</em><br/>【3】Vajapeyam，S. <a class="ae ov" href="https://arxiv.org/pdf/1405.2061.pdf" rel="noopener ugc nofollow" target="_blank">理解信息的香农熵度量</a> (2014)。<br/>【4】Iacobelli，F. <a class="ae ov" href="https://www.youtube.com/watch?v=CTYqkWU8cBc" rel="noopener ugc nofollow" target="_blank">困惑</a>(2015)YouTube<br/>【5】Lascarides，A. <a class="ae ov" href="https://www.inf.ed.ac.uk/teaching/courses/fnlp/lectures/04_slides-2x2.pdf" rel="noopener ugc nofollow" target="_blank">语言模型:评估与平滑</a> (2020)。自然语言处理基础<em class="nm">(讲座幻灯片)</em><br/>【6】毛，L. <a class="ae ov" href="https://leimao.github.io/blog/Entropy-Perplexity/" rel="noopener ugc nofollow" target="_blank">熵，困惑及其应用</a> (2019)。毛蕾的日志</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="0b3d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">发现这个故事有用吗？考虑 <a class="ae ov" href="https://chiaracampagnola.medium.com/membership" rel="noopener"> <em class="nm">订阅</em> </a> <em class="nm">到媒体支持写手！</em></p></div></div>    
</body>
</html>