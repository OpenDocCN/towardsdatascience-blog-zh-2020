<html>
<head>
<title>Predicting Housing Prices Using Scikit-Learn’s Random Forest Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Scikit-Learn 的随机森林模型预测房价</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-housing-prices-using-a-scikit-learns-random-forest-model-e736b59d56c5?source=collection_archive---------11-----------------------#2020-06-08">https://towardsdatascience.com/predicting-housing-prices-using-a-scikit-learns-random-forest-model-e736b59d56c5?source=collection_archive---------11-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1c2765312166dd5b80f626382fac7c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oooNTLG5Pf-5MhuWG9-NfA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自 Getty Images</p></figure><h1 id="8305" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">动机</strong></h1><p id="b41e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">拥有一个房价预测模型对买卖双方来说都是非常重要的工具，因为它可以帮助他们做出明智的决定。对卖家来说，它可以帮助他们确定出售房屋的平均价格，而对买家来说，它可以帮助他们找到购买房屋的正确平均价格。</p><h1 id="131b" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">目标</strong></h1><p id="11be" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">建立随机森林回归模型，能够预测房屋的中值。我们还将简要介绍一些探索性数据分析、特征工程和超参数调整，以提高我们的随机森林模型的性能。</p><h1 id="8440" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">我们的机器学习管道</strong></h1><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/da1d95b4f9ac6ad6547ce6bddbc6f193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z2XBjr0Q7PXL-izsG9u9Fw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片作者:简单的机器学习管道</p></figure><p id="3322" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们的机器学习管道可以概括为以下任务:</p><ol class=""><li id="2f58" class="ml mm it lf b lg mg lk mh lo mn ls mo lw mp ma mq mr ms mt bi translated">数据采集</li><li id="1a5c" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">数据预处理和探索性数据分析</li><li id="051f" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">创建基础模型</li><li id="dc5c" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">特征工程</li><li id="8f6d" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">超参数调谐</li><li id="110f" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">最终模型培训和评估</li></ol><h1 id="7bca" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">第一步:数据采集</strong></h1><p id="df97" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们将使用波士顿住房数据集<em class="mz">:</em><a class="ae na" href="https://www.kaggle.com/c/boston-housing/data" rel="noopener ugc nofollow" target="_blank"><em class="mz">https://www.kaggle.com/c/boston-housing/data</em></a><em class="mz">。</em></p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="45e9" class="ng kg it nc b gy nh ni l nj nk">#Importing the necessary libraries we will be using</span><span id="99a3" class="ng kg it nc b gy nl ni l nj nk">%load_ext autoreload<br/>%autoreload 2<br/>%matplotlib inline</span><span id="2aa5" class="ng kg it nc b gy nl ni l nj nk">from fastai.imports import *<br/>from fastai.structured import *</span><span id="8c2f" class="ng kg it nc b gy nl ni l nj nk">from pandas_summary import DataFrameSummary<br/>from sklearn.ensemble import RandomForestRegressor<br/>from IPython.display import display</span><span id="d6a0" class="ng kg it nc b gy nl ni l nj nk">from sklearn import metrics<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="b0a8" class="ng kg it nc b gy nl ni l nj nk">#Loading the Dataset</span><span id="9dbb" class="ng kg it nc b gy nl ni l nj nk">PATH = 'data/Boston Housing Dataset/'<br/>df_raw_train = pd.read_csv(f'{PATH}train.csv',low_memory = False)<br/>df_raw_test = pd.read_csv(f'{PATH}test.csv',low_memory = False)</span></pre><h1 id="bb41" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">步骤 2:数据预处理和探索性数据分析(EDA)</h1><p id="216a" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lf iu"> 2.1 数据缺失和异常值的检查和处理。</strong></p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="c552" class="ng kg it nc b gy nh ni l nj nk">df_raw_train.info</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/c718ae233f5c16721762006bedaf5e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcJu0-AEBvk4P5ZdmGqQig.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="fbfa" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">了解原始数据:</p><p id="4b91" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">根据上面的原始训练数据集:</p><p id="edbf" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">(a)共有<strong class="lf iu"> 14 个变量(13 个自变量—特征和 1 个因变量—目标变量)</strong>。<br/>(b)<strong class="lf iu">数据类型</strong>要么是<strong class="lf iu">整数</strong>要么是<strong class="lf iu">浮点数</strong>。<br/> (c) <strong class="lf iu">不存在分类数据</strong>。<br/> (d)我们的数据集中没有<strong class="lf iu">缺失值</strong>。</p><p id="6e7e" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 2.2 作为 EDA 的一部分，我们将首先尝试确定因变量(MDEV)的分布。</strong></p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="2ad5" class="ng kg it nc b gy nh ni l nj nk">#Plot the distribution of MEDV</span><span id="8247" class="ng kg it nc b gy nl ni l nj nk">plt.figure(figsize=(10, 6))<br/>sns.distplot(df_raw_train['MEDV'],bins=30)</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/63cfcb338996aac27fa03201b12bafa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*aoGVYr8sZTX1Nr2SJH0Q1g.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片:MEDV 分布</p></figure><p id="954d" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">MEDV <strong class="lf iu">的值遵循</strong>正态分布<strong class="lf iu">且平均值约为 22。右边有一些异常值。</strong></p><p id="a4d4" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 2.3 接下来，试着确定:</strong> <br/> <strong class="lf iu"> (i)自变量本身<br/> (ii)自变量与因变量</strong>之间是否有相关性</p><p id="ee52" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">为此，我们来做一个关联热图。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="0a3f" class="ng kg it nc b gy nh ni l nj nk"># Plot the correlation heatmap</span><span id="7c61" class="ng kg it nc b gy nl ni l nj nk">plt.figure(figsize=(14, 8))<br/>corr_matrix = df_raw_train.corr().round(2)<br/>sns.heatmap(data=corr_matrix,cmap='coolwarm',annot=True)</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/5d0ea6c83bb187de3146620952c97f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Nv9eIJItDRNs9aoaaZy-A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片:相关热图</p></figure><p id="7540" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> (i)独立变量之间的相关性</strong> : <br/>我们需要寻找多重共线性的特征(即彼此相关的特征)，因为这将影响我们与独立变量的关系。</p><p id="1095" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">注意到<strong class="lf iu"> RAD </strong>和<strong class="lf iu"> TAX </strong>彼此<strong class="lf iu">高度相关</strong>(相关分数:0.92)，而有几个特征彼此稍微相关，相关分数约为 0.70 (INDUS 和 TAX、NOX 和 INDUS、AGE 和 DIS、AGE 和 INDUS)。</p><p id="dc44" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu">(二)自变量与因变量之间的相关性</strong> : <br/>为了让我们的回归模型表现良好，理想情况下我们需要选择那些与我们的因变量(MEDV)高度相关的特征。</p><p id="e561" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们观察到<strong class="lf iu"> RM </strong>和<strong class="lf iu"> LSTAT </strong>都与和<strong class="lf iu"> MEDV </strong>相关，相关系数分别为 0.66 和 0.74。这也可以通过散点图来说明。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="3388" class="ng kg it nc b gy nh ni l nj nk">#Scatter plot to observe the correlations between the features that are highly correlated with MEDV</span><span id="2d23" class="ng kg it nc b gy nl ni l nj nk">target_var = df_raw_train['MEDV']</span><span id="20f9" class="ng kg it nc b gy nl ni l nj nk">plot1 = plt.figure(1)<br/>plt.scatter(df_raw_train['RM'],target_var)<br/>plt.xlabel('RM')<br/>plt.ylabel('MEDV')</span><span id="ef5e" class="ng kg it nc b gy nl ni l nj nk">plot2 = plt.figure(2)<br/>plt.scatter(df_raw_train['LSTAT'],target_var)<br/>plt.xlabel('LSTAT')<br/>plt.ylabel('MEDV')</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/06f37912ea1731f2c2d28b21f6629efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*hBRe9vQ2RQjXVmb7wadipw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片:RM 与 MEDV 散点图</p></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c621ad491e1e11dd65b781a790987db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*bbq3lqHuygbrBa_shQ1FZg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者提供的图片:RM 与 LSTAT 的散点图</p></figure><p id="1276" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">从上面的散点图来看:<br/> i) <strong class="lf iu"> MEDV 随 RM 线性增加。这是有道理的，因为我们预计随着房间数量的增加，房子的中值价格会更贵。MEDV 随 LSTAT 线性下降。这也是有道理的，因为我们可以预期，在社会地位较低的地方，房子的中值价格通常会更便宜。</strong></p><h1 id="c554" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">步骤 3:创建基础模型</h1><p id="8b07" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">停下来。在我们创建我们的基本随机森林模型之前，选择一个合适的评估标准是非常重要的。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/04f8f25162712b3638b645a4a72bfc1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*RW23Cwl9YDbfW2Q2nlNNJw.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自 imgflip.com</p></figure><p id="aace" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 3.1 选择正确的评估指标</strong> <br/>选择正确的评估指标将有助于我们评估我们的模型性能是否良好。对于回归问题，最常用的评估指标是<strong class="lf iu">均方根误差(RMSE) </strong>或<strong class="lf iu">均方根逻辑误差(RMLSE) </strong>。</p><p id="425b" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> RMSE </strong>:这是我们的模型预测值和实际值之间的平方差。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/88c4e6e118cb09ebdebd487efa4e7f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*D_M3vSQjsuZq75k0L9g8Sw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">均方根误差公式</p></figure><p id="e92b" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><em class="mz">其中 y:预测值，y:实际值</em></p><p id="91c8" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> RMSLE </strong>:它是我们的模型预测的对数和实际值的对数之间的平方差的度量。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/9686ff7957fa1a552041e4f8d11a7ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*JMBZXIRV7xizB-Ob2yjATA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">均方根逻辑误差</p></figure><p id="6d30" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><em class="mz">其中 y:预测值，y:实际值</em></p><p id="7cd9" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">RMSLE 可能是一个更好的评估指标，因为(1)它足够稳健，可以处理我们在数据集中看到的异常值(2) RMSLE 会因低估实际值而招致更大的损失。如果我们站在卖家的角度，我们不想低估价格，因为这会导致损失。然而，对于这个项目，我们不会站在任何一方，我们将选择<strong class="lf iu"> RMSE </strong>作为<strong class="lf iu">评估指标</strong>，因为我们将使用不受异常值影响的随机森林模型。</p><p id="0d59" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 3.2 创建我们的基础随机森林模型</strong></p><p id="696a" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">下一步是首先创建一个基本模型，不进行任何特征工程和超参数调整。在我们完成特征工程和超参数调整之后，我们将使用这个模型的性能作为比较的基准。</p><p id="85ed" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们选择的机器学习模型是随机森林回归模型，原因如下:</p><ol class=""><li id="b8f6" class="ml mm it lf b lg mg lk mh lo mn ls mo lw mp ma mq mr ms mt bi translated">随机森林模型<strong class="lf iu">需要最少的数据准备</strong>。它能够轻松处理分类、数字和二进制特征，无需缩放或标准化。</li><li id="e1ba" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">随机森林模型可以帮助我们执行隐含的特征选择，因为它们提供了重要特征的良好指标。</li><li id="450b" class="ml mm it lf b lg mu lk mv lo mw ls mx lw my ma mq mr ms mt bi translated">随机森林模型<strong class="lf iu">不受离群值</strong>的影响，离群值存在于我们的数据中，它们<strong class="lf iu">完全忽略了统计问题</strong>，因为不像其他机器学习模型在标准化后表现得更好。</li></ol><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/34c44d15a221d02531d5825d358abb2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*B7uGnTh7s7Czkb3EH8UJ4w.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片摄于 frontiersin.org</p></figure><p id="d4cf" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们想创建一些有用的函数:</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="db55" class="ng kg it nc b gy nh ni l nj nk"># A function to split our training data into a training set to train our  model and a validations set, which will be used to validate our model.</span><span id="e643" class="ng kg it nc b gy nl ni l nj nk">def split_vals(a,n):<br/>    return a[:n],a[n:]</span><span id="8454" class="ng kg it nc b gy nl ni l nj nk"># Functions that will help us calculate the RMSE and print the score.</span><span id="cb00" class="ng kg it nc b gy nl ni l nj nk">def rmse(x,y):<br/>    return math.sqrt(((x-y)**2).mean())</span><span id="b329" class="ng kg it nc b gy nl ni l nj nk">def print_score(m):<br/>    res =[rmse(m.predict(X_train),y_train),rmse(m.predict(X_valid),y_valid),m.score(X_train,y_train),m.score(X_valid,y_valid)]<br/>    if hasattr(m,'oob_score_'):res.append(m.oob_score_)<br/>    print(res)</span></pre><p id="9606" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">进一步拆分我们的训练数据—用于训练随机森林模型的训练数据和用于验证模型性能的验证数据。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="0427" class="ng kg it nc b gy nh ni l nj nk">n_valid = 100<br/>n_train = len(df_raw_train)-n_valid<br/>X_train,X_valid = split_vals(df_raw_train.drop('MEDV',axis=1),n_train)<br/>y_train,y_valid = split_vals(df_raw_train['MEDV'],n_train)<br/>X_test = df_raw_test</span></pre><p id="93cb" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">在没有特征选择和超参数调整的情况下创建和拟合我们的随机森林模型<strong class="lf iu">。</strong></p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/d3ce79eef996406eb02bb53905255473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pB39sFJvcITg3XMuBQuH4w.png"/></div></div></figure><p id="8b86" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">从我们的基本随机森林模型中，我们已经得到了一个非常不错的结果，其中<strong class="lf iu">训练 RMSE </strong>为<strong class="lf iu"> 1.394 </strong>，而<strong class="lf iu">验证 RMSE </strong>为<strong class="lf iu"> 3.021 </strong>。但是，请注意，我们的模型似乎<strong class="lf iu">比</strong>更适合，因为验证 RMSE 比训练 RMSE 大约高 3 倍。</p><p id="1fc8" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">因此，击败的<strong class="lf iu">基线分数是<strong class="lf iu">验证 RMSE 3.021 </strong>！</strong></p><h1 id="45d7" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">步骤 4:特征工程</h1><p id="0fdb" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">特征工程和超参数调整是机器学习管道中的<strong class="lf iu">基本步骤</strong>。</p><p id="a6b5" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 4.1 确定重要特征</strong> <br/>我们数据中的特征直接影响着我们的随机森林模型及其实现的结果(即，我们准备和选择的特征越好，我们实现的最终结果就越好！).因此，我们将通过确定哪些特性在之前的基础模型中被认为是重要的，来探索和微调我们的随机森林模型。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="cbfa" class="ng kg it nc b gy nh ni l nj nk">def feat_importance(m,df_train):<br/>    importance = m.feature_importances_<br/>    importance = pd.DataFrame(importance,index=df_train.columns,columns=["Importance"])<br/>    return importance.sort_values(by=['Importance'],ascending=False)</span><span id="1998" class="ng kg it nc b gy nl ni l nj nk">importance = feat_importance(m,X_train)<br/>importance[:]</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d568a411539ef55460aff228a539f60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*cyIlk436VndD_VqxgyLX7g.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">分级特征重要性系数</p></figure><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="1066" class="ng kg it nc b gy nh ni l nj nk">importance.plot(kind='barh')</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/76893ce86cd5826d68f6c56e57a5c3bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*cXLiA8RU7Aq56wr4hDWjBw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">分级特征重要性条形图</p></figure><p id="92ce" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们观察到在预测 MDEV 时最重要的特征是<strong class="lf iu"> LSTAT </strong>和<strong class="lf iu"> RM </strong>。回想一下，之前发现这两个参数与 MDEV 高度相关。</p><p id="2929" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 4.2 丢弃不重要的特征</strong> <br/>下一步将尝试丢弃重要性系数小于 0.01 的特征，并使用它再次为我们的随机森林建模，以查看我们的预测结果是否有改进。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="37a4" class="ng kg it nc b gy nh ni l nj nk">#Discarding features with feature coefficients less than 0.01</span><span id="4468" class="ng kg it nc b gy nl ni l nj nk">to_keep = importance[importance['Importance'] &gt; 0.01].index<br/>df_raw_train_keep = df_raw_train[to_keep].copy()<br/>df_raw_test_keep = df_raw_test[to_keep].copy()</span><span id="0938" class="ng kg it nc b gy nl ni l nj nk">#Splitting data into training and validation set</span><span id="3d0c" class="ng kg it nc b gy nl ni l nj nk">X_train,X_valid = split_vals(df_raw_train_keep,n_train)</span><span id="71a6" class="ng kg it nc b gy nl ni l nj nk"># Fitting our Random Forest Model after discarding the less important features.</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/803d8ca3d1c6fdaa049f0c0683462fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-bzWC31VZ5xQ4hkppCY5Q.png"/></div></div></figure><p id="c252" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们的随机森林模型在移除了一些冗余功能(即 6 个功能)后，表现<strong class="lf iu">稍好</strong>！).我们已经获得了确定因变量 MEDV 的前 9 个最重要的特征(LSTAT、RM、DIS、CRIM、TAX、NOX、PTRATIO、NOX 和 AGE)。</p><p id="aa91" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">接下来，让我们看看对于我们的随机森林模型来说重要的特性的等级是否有任何变化。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="dae0" class="ng kg it nc b gy nh ni l nj nk">def feat_importance(m,df_raw_train_keep):<br/>    importance = m.feature_importances_<br/>    importance =<br/>pd.DataFrame(importance,index=df_train.columns,columns=["Importance"])<br/>    return importance.sort_values(by=['Importance'],ascending=False)</span><span id="f4a4" class="ng kg it nc b gy nl ni l nj nk">importance</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/2c1f7529595d83a10aec7ce9badbda4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*LDoRYPZK2ivTUwP5upEP3Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">去除冗余特征后的分级特征重要性系数</p></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3b1d5443492cd769d9cc70ac55e446dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*S4WZBiezl7gFEMK7MysJUg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">移除冗余特征后的分级特征重要性条形图</p></figure><p id="8bb5" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们观察到，在先前移除冗余特征之后，最重要的特征仍然是 LSTAT 和 RM。我们想探究逐一删除剩余的每个功能会如何影响我们的总体得分。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/71a1533e9e983a138fd39a4ea8621203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFS64M_VGNJNsicrPlQNKg.png"/></div></div></figure><p id="efd6" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">请注意，删除 RM、LSTAT、DIS 和 CRIM 会导致更差的验证 RMSE，而删除 AGE 和 PTRATIO 会给我们一个稍好的分数。因此，在运行最终模型之前，我们将<strong class="lf iu">进一步从数据集中删除 AGE </strong>和<strong class="lf iu"> PTRATIO </strong>。</p><p id="e18d" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> 4.3 去除独立要素间的多重共线性</strong></p><p id="13ba" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">在 2.3(i)中，有一些相互关联的特征。为了提高模型的性能，我们最好是<strong class="lf iu">移除特征之间的多重共线性</strong>。</p><p id="ab92" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">为了了解这些特征是如何相互关联的，我们可以绘制一个树状图。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="fd8e" class="ng kg it nc b gy nh ni l nj nk">#Dendogram plot</span><span id="bfd6" class="ng kg it nc b gy nl ni l nj nk">from scipy.cluster import hierarchy as hc<br/>corr = np.round(scipy.stats.spearmanr(df_raw_train_keep).correlation,4)<br/>corr_condensed = hc.distance.squareform(1-corr)<br/>z = hc.linkage(corr_condensed,method='average')<br/>fig = plt.figure(figsize=(16,10))<br/>dendogram = hc.dendrogram(z,labels=df_raw_train_keep.columns,orientation='left',leaf_font_size=16)<br/>plt.show()</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0292f3b94a65c462276ce19c15f00a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*3tt7ze7min13kcRJy3cnHA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">重要特征之间的树状图</p></figure><p id="7a62" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">从树状图中，<strong class="lf iu">在排名方面</strong>我们可以看到，没有一个特征是衡量同样的东西。排名最接近的是 NOX 和 CRIM。</p><p id="6c61" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">如果在排名方面存在非常接近的特征，下一步将是移除这些特征，一次移除一个，看看我们的模型是否可以进一步简化，而不影响我们的验证 RMSE 分数。然而在这种情况下，我们不需要。</p><h1 id="00b2" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">步骤 5:超参数调整</h1><p id="7eef" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们已经到了最后一步(万岁！)才能建立最终的随机森林模型。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/b73ef97ed93dd05041c0693f4fe23680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMYUZvOebz4gD2KHj5ulaQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自 tech-quantum.com</p></figure><p id="c2d6" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">超参数调整是一个<strong class="lf iu">迭代过程</strong> s，由此我们<strong class="lf iu">选择</strong>超参数的<strong class="lf iu">最佳配置</strong>，这为我们提供了<strong class="lf iu">最佳模型性能输出</strong>。对于随机森林模型，我们将重点调整 3 个超参数:</p><p id="436d" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> (1) n_estimators </strong>:在我们的随机森林中创建和推广的树的数量。很可能，我们创建的树越多越好，因为这将使我们能够对更多的数据集进行采样，并帮助我们更好地进行归纳。然而，会有这样一种情况，即以计算能力为代价，增加树的数量只会对我们的验证 RMSE 产生非常小的改变。</p><p id="9723" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> (2) min_samples_leaf </strong>:表示叶子节点中的样本数。每当我们将 min_sample_leaf 加倍时，我们就从树中删除了一层，并将叶节点的数量减半。因此，增加 min_samples_leaf 的结果是，我们的每个叶节点内将有多个样本，因此当我们计算该叶节点的平均值时，它将更加稳定，尽管我们将获得稍小的深度和较少的节点数。因此，尽管每棵树的预测性和相关性都不会降低，但我们的模型应该能够更好地概括并防止过度拟合。</p><p id="f58c" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated"><strong class="lf iu"> (3) max_features </strong>:表示每次分割要考虑多少个特征。</p><p id="7484" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">为了优化和搜索最佳超参数，我们将使用<strong class="lf iu">随机网格搜索方法</strong>！</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="ca99" class="ng kg it nc b gy nh ni l nj nk">n_estimators = [int(x) for x in np.arange(start = 10, stop = 2000, step = 10)]<br/>max_features = [0.5,'auto', 'sqrt','log2']<br/>min_samples_leaf = [1, 2, 4]<br/>bootstrap = [True, False]<br/>random_grid = {'n_estimators': n_estimators,<br/>               'max_features': max_features,<br/>               'min_samples_leaf': min_samples_leaf,<br/>               'bootstrap': bootstrap}</span><span id="addc" class="ng kg it nc b gy nl ni l nj nk"># First create the base model to tune<br/>m = RandomForestRegressor()<br/># Fit the random search model<br/>m_random = RandomizedSearchCV(estimator = m, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)<br/>m_random.fit(X_train, y_train)<br/>m_random.best_params_</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/3455e3df457599059cd5351f0cd68a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyJJJ1WITu55B8FHspy_ew.png"/></div></div></figure><p id="4edd" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">从我们的随机网格搜索中，我们发现我们的随机森林模型的最佳超参数是上面那些。</p><h1 id="1eb4" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">第六步:最终模型</h1><p id="1378" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">最后一步——构建我们的最终模型。为此，我们将放弃前面讨论过的年龄和比率特性。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/eb5880a42ad6060e96aa37a9e8d2f2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atuWxYFripm4X7gteirCsw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自 gurutzeunzalu.blogspot.com</p></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/373fea2627a213d01b0edeca3ff8af2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cv-o_O-Fqzd_1WSMio2SkQ.png"/></div></div></figure><p id="542e" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们已经获得了 2.847 的<strong class="lf iu">验证 RMSE 分数，这比我们原来的基础模型<strong class="lf iu">验证 RMSE 分数 3.021 </strong>要好！</strong></p><p id="8050" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">另外，<strong class="lf iu"/><strong class="lf iu">最终验证 R 分</strong>为<strong class="lf iu"> 0.87 </strong>，这比<strong class="lf iu">基础款</strong>的<strong class="lf iu">验证 R 分</strong>0.85 要好。R 分数告诉我们我们的模型能够在多大程度上解释数据集中的变化。得分为 0.87 表明我们的模型可以解释数据集中 87%的变化。</p><p id="2bf2" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">因此，我们看到简单的最终模型即使具有较少的输入特性也表现得更好！</p><h1 id="ce75" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">额外的一步:部分独立</h1><p id="ad0d" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">虽然我们已经创建了一个更好的模型，让我们后退一步，进一步探索我们的结果。</p><p id="f435" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">之前，我们观察到房价上涨中值(MDEV)和房间数量(RM) 之间存在<strong class="lf iu">线性关系。</strong></p><p id="d724" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">让我们试着做一个 ggplot。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="fae6" class="ng kg it nc b gy nh ni l nj nk">from pdpbox import pdp<br/>from plotnine import *<br/>ggplot(df_raw_train, aes('RM', 'MEDV'))+stat_smooth(se=True, method='loess')</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ebd9ea89eb220b4f46b1224548ef2d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*v2yg0adw7opsZlbiE9Vkyg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">ggplot</p></figure><p id="d1a8" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">基于上面的 gg 图，我们观察到 RM 和 MEDV 之间的关系是<strong class="lf iu">而不是</strong>我们所期望的。例如，人们通常会认为房价会随着房间数量的增加而上涨。然而，我们看到 4 房和 5 房之间的价格下降，同样 8 房和 9 房之间的价格下降。</p><p id="affc" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">这里的问题是我们正在查看<strong class="lf iu">单变量关系</strong>并且在单变量图中<strong class="lf iu">丢失</strong>的特征之间有<strong class="lf iu">许多相互作用</strong>。例如，为什么 5 间房的价格比 4 间房的价格低，而 6 间房的价格和 4 间房的价格差不多？</p><p id="e9e0" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">因此，要找到 RM 和 MEDV 之间的真实关系，我们需要做一个<strong class="lf iu">部分独立图</strong>(即假设所有其他特征相同，房子的价格如何随房间数量而变化)来查看真实关系。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="c5f1" class="ng kg it nc b gy nh ni l nj nk">def plot_pdp(feat,clusters=None,feat_name=None):<br/>    feat_name = feat_name or feat<br/>    p = pdp.pdp_isolate(m,X_train,feat)<br/>    return pdp.pdp_plot(p,feat_name,plot_lines=True,cluster=clusters is not None,n_cluster_centers = clusters)<br/>plot_pdp('RM')</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/97461446193190c63467afdecc568ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gqP3z8xfUJ3Efi3243YIpw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">PDP 图</p></figure><p id="8c9d" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">从上面的部分依赖图(PDP)中注意到，在去除所有其他外部因素后，我们观察到 RM 和 MEDV 之间的关系几乎是一条直线(即大致线性)，这正是我们所预期的。黄线代表所有交易的平均 MEDV。</p><p id="2cf5" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">现在我们来探讨 LSTAT 和 RM 是如何影响房价中位数的。</p><pre class="mc md me mf gt nb nc nd ne aw nf bi"><span id="6078" class="ng kg it nc b gy nh ni l nj nk">feats = ['LSTAT','RM']<br/>p = pdp.pdp_interact(m,X_train,feats)<br/>pdp.pdp_interact_plot(p,feats)</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/b870a59e9354a88be44de6a528436d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pibklJ2b3Pw0JfsBwNNMog.png"/></div></div></figure><p id="3f22" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">因此，查看 PDP 图，我们可以有把握地得出结论:<strong class="lf iu">房间数量(RM)线性影响中值房价(MEDV)</strong>。人口的状况反过来影响着 MEDV。</p><h1 id="3f76" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">最后的想法和下一步</h1><p id="0521" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们建立了一个随机森林模型，其<strong class="lf iu">验证 RMSE 分数为 3.021 </strong>和<strong class="lf iu">验证 R 分数为 0.87 </strong>。</p><p id="2d72" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们还从我们的随机森林模型中确定了<strong class="lf iu">影响</strong>的<strong class="lf iu">关键特征</strong>。波士顿的<strong class="lf iu">中值房价(MEDV) </strong>是<strong class="lf iu"> (1) LSAT </strong>:低人口状况的百分比<strong class="lf iu"> (2) RM </strong>:每所住宅的平均房间数<strong class="lf iu"> (3) NOX </strong>:氮氧化物浓度<strong class="lf iu"> (4) CRIM </strong>:城镇人均犯罪率。</p><p id="937d" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">我们建造的最终模型远非完美。例如，观察我们最终模型中的<strong class="lf iu">培训 R 分数</strong>比<strong class="lf iu"> 0.87 </strong>的<strong class="lf iu">验证 R 分数</strong>高出<strong class="lf iu">0.98</strong>。这表明最终模型能够解释训练数据中 98%的变化，而仅解释验证数据中 87%的变化(即最终模型仍然<strong class="lf iu">过度拟合</strong>训练数据，并且<strong class="lf iu">没有将</strong> <strong class="lf iu">以及</strong>推广到验证数据)。</p><p id="280b" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">解决过度拟合的一个简单方法是尝试<strong class="lf iu">增加数据集的大小来训练我们的模型。</strong>目前，我们的数据集中只有 406 个条目，<strong class="lf iu">根本不够用</strong>。</p><p id="ed42" class="pw-post-body-paragraph ld le it lf b lg mg li lj lk mh lm ln lo mi lq lr ls mj lu lv lw mk ly lz ma im bi translated">祝贺并感谢你坚持到最后。我真诚地希望你学到了新的东西！快乐学习！😬</p></div></div>    
</body>
</html>