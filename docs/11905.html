<html>
<head>
<title>Graph Deep Learning — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形深度学习—第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-deep-learning-part-1-e9652e5c4681?source=collection_archive---------49-----------------------#2020-08-17">https://towardsdatascience.com/graph-deep-learning-part-1-e9652e5c4681?source=collection_archive---------49-----------------------#2020-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="afb9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fau-lecture-notes" rel="noopener" target="_blank"> FAU 讲座笔记</a>关于深度学习</h2><div class=""/><div class=""><h2 id="2019" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">光谱卷积</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eee09b22c3af5208531c6407b781c8a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nG-xuO5PRquyGanc.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FAU 大学的深度学习。下图<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a></p></figure><p id="2a49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这些是 FAU 的 YouTube 讲座</strong> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">深度学习</strong> </a> <strong class="lk jd">的讲义。这是与幻灯片匹配的讲座视频&amp;的完整抄本。我们希望，你喜欢这个视频一样多。当然，这份抄本是用深度学习技术在很大程度上自动创建的，只进行了少量的手动修改。</strong> <a class="ae lh" href="http://autoblog.tf.fau.de/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">自己试试吧！如果您发现错误，请告诉我们！</strong></a></p><h1 id="a041" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">航行</h1><p id="47fd" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/weakly-and-self-supervised-learning-part-4-2fbfd10280b3"> <strong class="lk jd">上一讲</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" href="https://youtu.be/wcRFkJ5okt4" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">观看本视频</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/all-you-want-to-know-about-deep-learning-8d68dcffc258"> <strong class="lk jd">顶级</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/graph-deep-learning-part-2-c6110d49e63c"> <strong class="lk jd">下一讲</strong> </a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6485cc8af3e7e1bb9aa0d0ee85809db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*_WSpQ7kBlF2-luef.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图形深度学习正在成为学习模拟中的一项关键技术。使用<a class="ae lh" href="https://github.com/vvo/gifify" rel="noopener ugc nofollow" target="_blank"> gifify </a>创建的图像。来源:<a class="ae lh" href="https://youtu.be/2Bw5f4vYL98" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></figure><p id="a53b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">欢迎回到深度学习！所以今天，我们想研究一下如何处理图形，我们将讨论一下图形卷积。让我们看看我为你准备了什么。今天的主题是图形深度学习的介绍。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6ec2ba9805a36d4fec3606e594d3fdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*9D4fA2AYZxqUjpWE.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们在说什么格拉夫？<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="f405" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，什么是图形深度学习呢？你可以说这是一个图表，对吗？我们从数学中知道，我们可以绘制图表。但这不是我们今天要讨论的话题。你也可以说图表就像这样的图。但这些也不是我们今天要讲的情节。那是施特菲·格拉芙吗？不，我们也不是在谈论施特菲·格拉芙。所以我们实际上想看的是更多像这样的东西，比如可以用不同的节点和边连接起来的图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/4861ed75387f4de750ab6c7c6fe072f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*cbeL4KIvyg4gaE7m.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图的定义。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="58e3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个计算机科学家认为一个图是一组节点，它们通过边连接在一起。这就是我们今天要讨论的图表。对数学家来说，图是流形，但却是离散的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9e47c873a50832acf2abc518fb914b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*8TUBXgPrBZwlryve.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">欧几里得空间上的卷积。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="dcf9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，你如何定义卷积？在欧几里得空间，对计算机科学家和数学家来说，这太容易了。这是离散卷积，本质上是一个和。我们记得，当我们为卷积深度模型设置内核时，我们有许多这样的离散卷积。在连续形式中，它实际上有如下形式:它本质上是一个在整个空间上计算的积分，我在这里举了一个例子。因此，如果你想卷积两条高斯曲线，那么你实际上是把它们移动到彼此之上，在每一点上相乘，然后求和。当然，两个高斯函数的卷积也是高斯函数，所以这也很容易。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8a82dbeb39b556b6a2b4789b2c834521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*-tZo897lwChfQgsu.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图形卷积怎么样？来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="c398" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你如何定义图上的卷积？现在，计算机科学家非常努力地思考，但是…管它呢！数学家知道，我们可以用拉普拉斯变换来描述卷积，因此我们研究拉普拉斯算子，这里给出的是梯度的散度。所以在数学上，我们可以更轻松地处理这些事情。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9a8ad7bde74424206568e493c643db80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*UZZSVvUzAG3Q8lfn.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">走向图形卷积的步骤。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="e4a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就把我们带到了这个多方面的观点。我们知道如何卷积流形，我们可以离散卷积，这意味着我们知道如何卷积图形。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e464d4a94f1b5f6529525c01a8b6e2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EUhAbC7jfnf_Yhpx.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图形卷积的一个例子是热扩散。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="b30f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，还是散点热吧！我们知道，我们可以将牛顿冷却定律描述为以下方程。我们还知道，随时间的发展可以用拉普拉斯来描述。所以，f(x，t)是时间 t 点 x 的热量，然后，你需要有一个初始的热量分布。所以，你需要知道热在初始状态下是怎样的。然后，您可以使用拉普拉斯算子来表示系统如何随时间变化。这里可以看到，这本质上是 f(x)和 f 在 x 周围的无限小数小球上的平均值之差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bd3b986ceee267eb9c21281bfafed4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TNVc_5vekPw0vjEE.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">怎样才能得到一个离散的拉普拉斯？<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="2e3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们如何以离散形式表达拉普拉斯算子？这就是 f(x)和 f 在 x 周围无穷小的球上的平均值的差，所以，我们能做的最小的一步就是把当前节点和它的邻居连接起来。因此，我们可以将拉普拉斯表示为边权重 a 下标 I 和 j 的加权和，这是我们的中心节点 f 下标 I 减去 f 下标 j 的差，我们用实际传入 f 下标 I 的连接数来除，这将作为 d 下标 I 给出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/811d37e9ff13a8f901c99c7f1bc3027e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0kf8VfWwh5TlzsiX.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算图的拉普拉斯算子的简单步骤。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="2c99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在有没有另一种表达方式？嗯，是的。我们可以这样做，如果我们在这里看一个例子图。因此，我们有节点 1、2、3、4、5 和 6。我们现在可以使用矩阵<strong class="lk jd"> D </strong>计算拉普拉斯矩阵。<strong class="lk jd"> D </strong>现在仅仅是各个节点的输入连接数。我们可以看到，节点 1 有两个传入连接，节点 2 有三个，节点 3 有两个，节点 4 有三个，节点 5 也有三个，节点 6 只有一个传入连接。我们还需要矩阵<strong class="lk jd"> A </strong>。那是邻接矩阵。这里，每个节点都有一个 1，它与不同的节点相连，你可以看到它可以用上面的矩阵来表示。我们可以取这两个并计算拉普拉斯算子为<strong class="lk jd"> D </strong>减去<strong class="lk jd"> A </strong>。我们简单地按元素减去这两个来得到我们的拉普拉斯矩阵。这很好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fc55f7174b00a998010fb06e5dbf7e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tRyC4oIjWcLMq49X.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">即使对于有向图，也可以得到对称的拉普拉斯算子。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="e471" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到拉普拉斯是一个 N 乘 N 的矩阵，它描述了一个由 N 个节点组成的图或子图。<strong class="lk jd"> D </strong>也是一个 N 乘 N 矩阵，它被称为度矩阵，描述了连接到每个节点的边的数量。<strong class="lk jd"> A </strong>也是一个 N 乘 N 矩阵，它是描述图的连通性的邻接矩阵。所以对于一个有向图，我们的图拉普拉斯矩阵不是对称正定的。所以，为了得到对称的版本，我们需要把它标准化。这可以通过以下方式来实现:我们从原始的拉普拉斯矩阵开始。我们知道<strong class="lk jd"> D </strong>只是一个对角矩阵。因此，我们可以计算平方根倒数，并从左侧和右侧将其相乘。然后，我们可以插入原来的定义，你会发现我们可以稍微重新排列一下。然后我们可以把对称化的版本写成单位矩阵减去<strong class="lk jd"> D </strong>。这里，我们再一次在元素上应用相同矩阵的逆矩阵和平方根乘以<strong class="lk jd">和</strong>。这很有趣，对吧？即使对于有向图，我们也总能得到这个矩阵的对称化版本。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a97315d87eb78d93233ab5ef94e66478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*raWPYLQ66iqMWoyO.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">拉普拉斯算子的特征向量和特征值决定了它的傅立叶变换。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="03c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们感兴趣的是如何实际使用它。我们可以做一些魔术，现在的魔术是，如果我们的矩阵是对称正定的，那么矩阵可以分解为特征向量和特征值。这里我们看到所有的特征向量都集合在<strong class="lk jd"> U </strong>中，特征值在这个对角矩阵<strong class="lk jd">λ</strong>上。现在，这些特征向量被称为图形傅立叶模式。特征值被称为频谱频率。这意味着我们可以使用<strong class="lk jd"> U </strong>和<strong class="lk jd"> U </strong>转置来对图形进行傅里叶变换，我们的<strong class="lk jd">λ</strong>是光谱滤波器系数。因此，我们可以将一个图形转换成一个光谱表示，并观察它的光谱特性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f51523265620493e469b5b02a2627939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5M-j6V45El47G6pc.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们去傅立叶空间。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="fdac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们继续我们的矩阵。现在，让<strong class="lk jd"> x </strong>是某个信号，每个节点的标量。然后，我们可以使用拉普拉斯的特征向量来定义其傅立叶变换。这便是简单的<strong class="lk jd"> x </strong> hat 和<strong class="lk jd"> x </strong> hat 可以表示为<strong class="lk jd"> U </strong>转置次数<strong class="lk jd"> x </strong>。当然，你也可以把这个倒过来。这可以简单地通过应用<strong class="lk jd"> U </strong>来完成。因此，我们也可以为描述节点属性的任何系数集找到各自的谱表示。现在，我们也可以在谱域中用滤波器来描述卷积。因此，我们使用傅里叶表示来表示卷积，因此我们将 g 和<strong class="lk jd"> x </strong>带入傅里叶域，将两者相乘并计算傅里叶逆变换。从信号处理中我们知道，我们也可以在传统信号中做到这一点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2dc36245059e6780ae4a3384b82408bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*L_5eCK2wv0Hu4nyc.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们可以使用多项式进行过滤。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="71ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们构建一个过滤器。该滤波器由系数为<strong class="lk jd"> θ </strong>下标 I 的拉普拉斯 k 阶多项式组成，它们只是实数。所以，我们现在可以找到这种多项式，它是关于谱系数的多项式，并且在系数<strong class="lk jd"> θ </strong>中是线性的。这本质上只是多项式的和。现在，我们可以利用这个滤波器来执行卷积。我们必须像以前一样繁殖。我们有了信号，应用傅里叶变换，然后用多项式进行卷积，最后进行傅里叶逆变换。这就是我们将该滤波器应用于新信号的方式。现在呢？现在，我们可以使用拉普拉斯算子对<strong class="lk jd"> x </strong>进行卷积，因为我们调整了滤波器系数<strong class="lk jd"> θ </strong>。但是<strong class="lk jd"> U </strong>其实真的很重。请记住，我们不能在这里使用快速傅立叶变换的技巧。所以，它总是一个完整的矩阵乘法，如果你想用这种格式来表达你的卷积，计算量可能会很大。但是如果我告诉你一个巧妙的多项式选择可以完全抵消掉<strong class="lk jd"> U </strong>呢？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4c5408c4d4765d3b665071bc9a11bf81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l_y6vndBjrayrQft.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在这个深度学习讲座中，更多令人兴奋的事情即将到来。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="d589" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嗯，这就是我们下节课要讨论的关于深度学习的内容。非常感谢您的收听，下期视频再见。拜拜。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/849cd0ea45dcc1f896da9199edef2982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*r7k9GG9ysjuk4Myf.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更多模拟例子。使用<a class="ae lh" href="https://github.com/vvo/gifify" rel="noopener ugc nofollow" target="_blank"> gifify </a>创建的图像。来源:<a class="ae lh" href="https://youtu.be/2Bw5f4vYL98" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></figure><p id="604b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢这篇文章，你可以在这里找到更多的文章，更多关于机器学习的教育材料，或者看看我们的深度学习讲座。如果你想在未来了解更多的文章、视频和研究，我也会很感激关注<a class="ae lh" href="https://www.youtube.com/c/AndreasMaierTV" rel="noopener ugc nofollow" target="_blank"> YouTube </a>、<a class="ae lh" href="https://twitter.com/maier_ak" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae lh" href="https://www.facebook.com/andreas.maier.31337" rel="noopener ugc nofollow" target="_blank">脸书</a>或<a class="ae lh" href="https://www.linkedin.com/in/andreas-maier-a6870b1a6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。本文以<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/deed.de" rel="noopener ugc nofollow" target="_blank"> Creative Commons 4.0 归属许可</a>发布，如果引用，可以转载和修改。如果你有兴趣从视频讲座中生成文字记录，试试<a class="ae lh" href="http://autoblog.tf.fau.de/" rel="noopener ugc nofollow" target="_blank">自动博客</a>。</p><h1 id="ead3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">谢谢</h1><p id="b5eb" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">非常感谢<a class="ae lh" href="https://www.youtube.com/watch?v=P4yTKhcQqOk&amp;list=PL_VeUGLULXQux1dV4iA3XuMX6AueJmGGa&amp;index=10&amp;t=0s" rel="noopener ugc nofollow" target="_blank"> Michael Bronstein 对 2018 年小姐</a>的精彩介绍，特别感谢 Florian Thamm 准备了这组幻灯片。</p><h1 id="15b1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="4b64" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1]基普夫，托马斯和马克斯韦林。"图卷积网络的半监督分类."arXiv 预印本 arXiv:1609.02907 (2016)。<br/> [2]汉密尔顿、威尔、之桃·英和朱尔·莱斯科维奇。"大型图上的归纳表示学习."神经信息处理系统进展。2017.<br/>[3]沃尔泰林克、杰尔默·m、蒂姆·莱纳和伊万娜·伊什古姆。"用于心脏 CT 血管造影中冠状动脉分割的图形卷积网络."医学成像图形学习国际研讨会。施普林格，查姆，2019。<br/> [4]吴，，等.图神经网络综述 arXiv 预印本 arXiv:1901.00596 (2019)。<br/>【5】布朗斯坦、迈克尔等在 SIAM Tutorial Portland (2018)举办的讲座《图形和流形上的几何深度学习》</p><h1 id="7e7e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">图像参考</h1><p id="f2d9" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[a]<a class="ae lh" href="https://de.serlo.org/mathe/funktionen/funktionsbegriff/funktionen-graphen/graph-funktion" rel="noopener ugc nofollow" target="_blank">https://de . serlo . org/mathe/functionen/funktionsbegriff/funktionen-graphen/graph-function</a>【b】<a class="ae lh" href="https://www.nwrfc.noaa.gov/snow/plot_SWE.php?id=AFSW1" rel="noopener ugc nofollow" target="_blank">【https://www.nwrfc.noaa.gov/snow/plot_SWE.php?id=AFSW1</a><br/>【c】<a class="ae lh" href="https://tennisbeiolympia.wordpress.com/meilensteine/steffi-graf/" rel="noopener ugc nofollow" target="_blank">https://tennis Bei Olympia . WordPress . com/meilensteine/Steffi-graf/</a><br/>【d】<a class="ae lh" href="https://www.pinterest.de/pin/624381935818627852/" rel="noopener ugc nofollow" target="_blank">https://www.pinterest.de/pin/624381935818627852/</a><br/>【e】<a class="ae lh" href="https://www.uihere.com/free-cliparts/the-pentagon-pentagram-symbol-regular-polygon-golden-five-pointed-star-2282605" rel="noopener ugc nofollow" target="_blank">https://www . ui heregif</a><br/>【I】https://www . researchgate . net/publication/306293638/figure/fig 1/AS:396934507450372 @ 1471647969381/Example-of-centline-extracted-left-and-coronal-artery-tree-mesh-construction . png<br/>【j】<a class="ae lh" href="https://www.eurorad.org/sites/default/files/styles/figure_image_teaser_large/public/figure_image/2018-08/0000015888/000006.jpg?itok=hwX1sbCO" rel="noopener ugc nofollow" target="_blank">https://www . eurorad . org/sites/default/files/styles/stylesitok=hwX1sbCO </a></p></div></div>    
</body>
</html>