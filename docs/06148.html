<html>
<head>
<title>Theory of Principal Component Analysis (PCA) and implementation on Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析理论及其在Python上的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/theory-of-principal-component-analysis-pca-and-implementation-on-python-5d4839f9ae89?source=collection_archive---------52-----------------------#2020-05-18">https://towardsdatascience.com/theory-of-principal-component-analysis-pca-and-implementation-on-python-5d4839f9ae89?source=collection_archive---------52-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/7e493ef378506bc3a4ca83dcd3e726d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qY-Ypn8pDcyc8Uf-.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://summitlife.org/sense-of-direction-summit-life-today/" rel="noopener ugc nofollow" target="_blank"> summitlife </a></p></figure><h2 id="7951" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">主成分分析背后的原理和数学以及在Python上的实现</h2><div class=""/><p id="4c0c" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当处理一个包含大量数据的复杂科学项目时，其中每个例子都由许多特征描述，您可能想要<strong class="kr jt">可视化数据</strong>。事实上，在1D、2D或3D中可视化很容易，但如果你想可视化由100个特征组成的数据，你在100D中看不到任何东西。所以你必须<strong class="kr jt">降低维度</strong>，把你的数据放在一个维度等于或者小于3D的空间里。这可以使用主成分分析(PCA)来完成。PCA的另一个非常好的用途是<strong class="kr jt">加速你的机器学习算法的训练过程</strong>。你的特征越多，你的机器学习算法就越复杂，它需要学习的参数就越多。因此，计算时间将越长。现在，很明显，如果你减少数据的大小，学习算法所需的时间将会大大减少。但是，您可能会想，如果我们减少变量的数量，那么我们将会丢失数据中的大量信息，结果也将完全不准确。这是PCA算法的主要挑战，我们将在本文中看到如何确保良好的准确性。</p><p id="1691" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了说明这种情况，我将使用<strong class="kr jt"> MNIST数据库</strong>是一个手写数字的大型数据库。</p><p id="cfa4" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我将向您展示PCA背后的理论以及如何在Python上实现它。我也会给你一些何时使用这种方法的提示。</p><h1 id="f267" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">主成分分析背后的数学</h1><p id="7dc8" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">在主成分分析算法中，目标是找到投影数据的k个向量，以便最小化投影误差。<strong class="kr jt">这k个矢量将是投影数据的k个方向</strong>。这里，k对应于你的最终维度:如果你想在一个2D维度空间中查看你的数据，那么k将等于2。</p><blockquote class="mq mr ms"><p id="ed5f" class="kp kq mt kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated"><strong class="kr jt"> <em class="jj">我们如何找到这些k向量？</em> </strong></p></blockquote><p id="c76a" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们称<strong class="kr jt">为</strong> <strong class="kr jt">描述我们的数据</strong>的矩阵。PCA涉及将相互依赖的变量(在统计学中称为“相关的”)转换成彼此不相关的新变量。这些变量被称为主成分，将描述数据所传达的信息。我们需要查看协方差矩阵，因为<strong class="kr jt">协方差</strong>是两个<a class="ae jg" href="https://en.wikipedia.org/wiki/Random_variable" rel="noopener ugc nofollow" target="_blank">随机变量</a>的联合可变性的度量。但是为什么要协方差呢？让我们想想如何从数据中学习。你看着均值，变量是如何远离或接近均值的。这本质上是协方差:与平均值的偏差。因此，在主成分分析中，我们必须计算数据矩阵的协方差，并寻找收集最多信息的<strong class="kr jt">方向或向量</strong>，以便我们可以保留PI并去除其余的。但事情并没有那么简单，我们将一步一步来看，看看我们是如何做到的。</p><h2 id="0462" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">1.数据的标准化</h2><p id="dd4b" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">该步骤的目的是<strong class="kr jt">标准化连续初始变量</strong>的范围，以便它们中的每一个对分析的贡献相等。</p><p id="64c6" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">更具体地说，在PCA之前进行标准化是至关重要的，因为后者对初始变量的方差非常敏感。也就是说，如果初始变量的范围之间存在较大差异，则范围较大的变量将优于范围较小的变量(例如，范围在0和100之间的变量将优于范围在0和1之间的变量)，这将导致有偏差的结果。因此，将数据转换为可比尺度可以防止这个问题。</p><p id="2965" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">数学上，这可以通过减去每个变量的每个值的平均值并除以标准偏差来完成。回想一下，Ais是描述我们的数据的矩阵，因此每行是一个例子，每列是一个特征。让我们设定<strong class="kr jt"> n等于特征的数量</strong> et <strong class="kr jt"> m等于示例的数量</strong>。因此矩阵A是一个矩阵<strong class="kr jt"> mxn。</strong></p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/42f07d6c1b0fc9d1eb4c953a20d5dc65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-j4sIdd6g_Wz2hcVU4QMCA.png"/></div></div></figure><h2 id="747a" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">2.协方差矩阵和对角化</h2><p id="74f8" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">协方差矩阵由以下公式给出:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/a8745392bebb09c6a1da0e5d408ea9f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PUMK2vf7R4ja9-OGFyBj1A.png"/></div></div></figure><p id="2b07" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们需要对角化协方差矩阵。我们称S为对角矩阵。u和V将是变换矩阵。因此，我们有:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/c4656067e7adbb7c07e9ae7dafceb25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWqieq09jgXjQrTtrGWFHA.png"/></div></div></figure><p id="9980" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你听说过PCA，你可能听说过<a class="ae jg" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank"> SVD </a>。SVD用于奇异值分解，这就是我们在A上应用的内容。SVD理论表明，矩阵A存在以下分解:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/07ca0934eea21a9d72b187a426cf371e.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*s6OFS-oLT1aKpYSEIIfRdA.png"/></div></figure><p id="55db" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中<em class="mt"> U </em>和<em class="mt"> V </em>是分别从aa<strong class="kr jt"><em class="mt">【ᵀ】</em></strong>和a<strong class="kr jt"><em class="mt">【ᵀa】</em></strong>中选择的具有标准正交<strong class="kr jt">特征向量</strong>的<strong class="kr jt">正交矩阵。<strong class="kr jt"><em class="mt">s’</em></strong>是对角矩阵，其中<em class="mt"> r </em>元素等于XX <em class="mt"> ᵀ </em>或X <em class="mt"> ᵀ X. </em>对角元素由奇异值组成。</strong></p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/4c20f3cb08c141fa9d7f6845af01aec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSFEgbWApVegqZpt-tuioA.png"/></div></div></figure><p id="eeeb" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">并且我们有<strong class="kr jt"> <em class="mt"> S' = S. </em> </strong>下一步是重组特征向量以产生U和V. <strong class="kr jt"> <em class="mt"> </em> </strong>为了标准化解决方案，我们对特征向量进行排序，使得具有较高特征值的向量出现在具有较小值的向量之前。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/d42dd29bf49d8bffb742c80c008dc531.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*7XHTmO0NGHJOjtrWNroZzg.png"/></div></figure><p id="ee24" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与特征分解相比，奇异值分解适用于非方阵。<em class="mt"> U </em>和<em class="mt"> V </em>对于SVD中的任何矩阵都是可逆的。</p><h2 id="a07e" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">3.降维</h2><p id="ee04" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">降维的思想是保留k个最能描述数据的特征向量。因此，我们有:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/f495fe3ed5478681087f2c74309daa3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fd2niJpA8_lv_VOPi7BKNA.png"/></div></div></figure><p id="f4fd" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后我们添加零空间，它已经与第一个r v和u正交，从简化SVD到完全SVD。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/5a4c67f86bb7bd2d43b4fd0de5c34127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7RIOqC9FalmQXRCP7QTPA.png"/></div></div></figure><h2 id="24fd" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">4.如何选择k个特征向量</h2><p id="3b71" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">我们需要应用一个公式来保持总方差的某个百分比。假设我们希望保持方差的99%，那么我们有如下结果:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f3fc9eb708d4692877156998db73942c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*94anxjgcLwTlxCTYq02pBg.png"/></div></figure><p id="8eb7" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们已经看到了PCA背后的数学原理，我们将在Python上实现它。</p><h1 id="4868" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">Python上的实现</h1><p id="3c07" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">我们将使用的数据集在这里可访问<a class="ae jg" href="https://www.kaggle.com/c/digit-recognizer/data?select=train.csv" rel="noopener ugc nofollow" target="_blank">。因此，首先我们导入数据和我们需要的不同库。</a></p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="e735" class="mx lo jj nw b gy oa ob l oc od">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pylab as plt</span><span id="c18a" class="mx lo jj nw b gy oe ob l oc od"># import the data<br/>data = pd.read_csv("train.csv")</span><span id="afb2" class="mx lo jj nw b gy oe ob l oc od"># some pre-processing on the data<br/>labels = data['label'].head(1500) # we only keep the first 1500 </span><span id="6983" class="mx lo jj nw b gy oe ob l oc od"><em class="mt"># </em>Drop the label feature and store the pixel data in A and we keep <br/># only the first 1500<br/>A = data.drop("label",axis=1).head(1500)</span></pre><h2 id="d34e" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">1.数据的标准化</h2><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="17c7" class="mx lo jj nw b gy oa ob l oc od">from sklearn.preprocessing import StandardScaler</span><span id="9698" class="mx lo jj nw b gy oe ob l oc od">standardized_data = StandardScaler().fit_transform(A)</span></pre><h2 id="152f" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">2.协方差矩阵和对角化</h2><p id="8ecf" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">首先，我们计算a的<strong class="kr jt">协方差矩阵</strong></p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="be2a" class="mx lo jj nw b gy oa ob l oc od">covar_matrix = np.matmul(standardized_data.T , standardized_data)</span></pre><p id="82f2" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们计算特征值和相应的特征向量。但是这里我们只计算最高的两个。</p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="5b36" class="mx lo jj nw b gy oa ob l oc od"><em class="mt"># the parameter 'eigvals' is defined (low value to heigh value) </em><br/><em class="mt"># eigh function will return the eigen values in ascending order</em><br/><br/>values, vectors = eigh(covar_matrix, eigvals=(782,783))</span></pre><p id="e8a5" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们将原始数据样本投影到由我们刚刚计算的两个主特征向量形成的平面上。</p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="6cab" class="mx lo jj nw b gy oa ob l oc od">new_coordinates = np.matmul(vectors.T, sample_data.T)</span></pre><p id="2ad9" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们将标签附加到2d投影数据，并创建一个数据帧，然后使用seaborn并绘制带标签的点:</p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="3496" class="mx lo jj nw b gy oa ob l oc od">new_coordinates = np.vstack((new_coordinates, labels)).T<br/><br/><em class="mt"># creating a new data frame for plotting the labeled points.</em><br/>dataframe = pd.DataFrame(data=new_coordinates, columns=("1st_principal", "2nd_principal", "label"))</span><span id="1d79" class="mx lo jj nw b gy oe ob l oc od"># plot<br/>sn.FacetGrid(dataframe, hue="label", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()<br/>plt.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/ca586a3d7ae44935ca9ef290d42773c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imMS6KWMrxaFGLFHn2KsHw.png"/></div></div></figure><p id="f887" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">类别之间有很多重叠，这意味着PCA不太适合高维数据集。很少几个阶层可以分开，但大多数都是混合的。<strong class="kr jt"> PCA主要用于降维</strong>，<strong class="kr jt">不用于可视化</strong>。为了可视化高维数据，我们通常使用T-SNE。</p><p id="e5ce" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们也可以使用Python中的PCA模块来实现:</p><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="058a" class="mx lo jj nw b gy oa ob l oc od"><em class="mt"># initializing the pca</em><br/><strong class="nw jt">from</strong> <strong class="nw jt">sklearn</strong> <strong class="nw jt">import</strong> decomposition<br/>pca = decomposition.PCA()</span><span id="89cb" class="mx lo jj nw b gy oe ob l oc od"><em class="mt"># configuring the parameteres</em><br/><em class="mt"># the number of components = 2</em><br/>pca.n_components = 2<br/>pca_data = pca.fit_transform(standardized_data)</span><span id="c854" class="mx lo jj nw b gy oe ob l oc od"><br/>pca_data = np.vstack((pca_data.T, labels)).T<br/><br/><em class="mt"># creating a new data frame which help us in plotting the result data</em><br/>pca_df = pd.DataFrame(data=pca_data, columns=("1st_principal", "2nd_principal", "label"))</span><span id="2617" class="mx lo jj nw b gy oe ob l oc od"># plot the data<br/>sns.FacetGrid(pca_df, hue="label", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()<br/>plt.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/db33f353d9c6bec5a807c726f437d9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZs_4w2iGyKFpD3uxjmXsQ.png"/></div></div></figure><h2 id="0abf" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">3.降维</h2><p id="0c55" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">在这一节中，我们想找到描述我们的数据所需的特征向量的数量。因此，我们需要计算每个特征值的<strong class="kr jt">显著性</strong>，然后计算累积方差。我将特征值的重要性定义为:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ded4d254e07e754b8b774d92fdacd9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*x3uiGDGzFV2nMipEkxabog.png"/></div></figure><pre class="nj nk nl nm gt nv nw nx ny aw nz bi"><span id="addb" class="mx lo jj nw b gy oa ob l oc od"><strong class="nw jt"># initializing the pca</strong><br/>from sklearn import decomposition<br/>pca = decomposition.PCA()<br/><strong class="nw jt"># PCA for dimensionality reduction (non-visualization)</strong></span><span id="ba41" class="mx lo jj nw b gy oe ob l oc od">pca.n_components = 784<br/>pca_data = pca.fit_transform(standardized_data)</span><span id="2c8a" class="mx lo jj nw b gy oe ob l oc od"># then we calculate the significance but the absolute value is not necessary as the eigen values are already positive<br/>significance = pca.explained_variance_/ np.sum(pca.explained_variance_)</span><span id="3921" class="mx lo jj nw b gy oe ob l oc od">cum_var_explained = np.cumsum(significance)</span><span id="a6f1" class="mx lo jj nw b gy oe ob l oc od"><em class="mt"><br/># Plot the PCA spectrum</em><br/>plt.figure(1, figsize=(6, 4))<br/><br/>plt.clf()<br/>plt.plot(cum_var_explained, linewidth=2)<br/>plt.axis('tight')<br/>plt.grid()<br/>plt.xlabel('n_components')<br/>plt.ylabel('Cumulative_explained_variance')<br/>plt.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/6c80cd599e04e498d24b0b662533097f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrIcfOlUaeOvIPdkIweFxA.png"/></div></div></figure><h2 id="f23c" class="mx lo jj bd lp my mz dn lt na nb dp lx la nc nd mb le ne nf mf li ng nh mj jp bi translated">4.如何选择k个特征向量</h2><p id="8204" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">让我们回顾一下我们在第一部分看到的内容:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/979a939e8302b94ced5b808bb44c6da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wn6VMQzgVCC52Ss5A282mg.png"/></div></div></figure><p id="d52b" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，我们绘制了该分量方差的累积和。这里300个成分解释了几乎90%的差异。所以我们可以根据要求的方差来降维。</p><h1 id="c9d7" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">主成分分析法的优点和用途</h1><p id="b328" class="pw-post-body-paragraph kp kq jj kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">PCA是一种<strong class="kr jt">降维</strong>的方法，但是可以要求成分独立:<a class="ae jg" href="https://en.wikipedia.org/wiki/Independent_component_analysis" rel="noopener ugc nofollow" target="_blank">独立成分分析</a> ( <strong class="kr jt"> ICA </strong>)。<br/> PCA是一种<strong class="kr jt">无监督线性方法</strong>，大多数无监督技术并非如此。<br/>由于PCA是一种降维方法，它允许将数据投影到1D、2D或3D中，从而<strong class="kr jt">将数据可视化</strong>。这对<strong class="kr jt">加速训练过程</strong>也非常有用，因为它大大减少了特征的数量，从而减少了参数的数量。</p><p id="2fcb" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我希望你已经找到了你在这篇文章中寻找的东西，并祝你未来的项目好运！</p><p id="42ae" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，考虑注册成为一名灵媒成员。每月5美元，你可以无限制地阅读媒体上的故事。如果你注册使用<a class="ae jg" href="https://medium.com/@jonathan_leban/membership" rel="noopener">我的链接</a>，我会赚一小笔佣金，你仍然要支付5美元。谢谢大家！！</p><div class="is it gp gr iu ok"><a href="https://medium.com/@jonathan_leban/membership" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd jt gy z fp op fr fs oq fu fw js bi translated">通过我的推荐链接加入媒体-乔纳森·莱班</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">阅读乔纳森·莱班的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接支持…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">medium.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ja ok"/></div></div></a></div><p id="89e9" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="mt"> PS:我目前是伯克利的工程硕士学生，如果你想讨论这个话题，请随时联系我。</em> <a class="ae jg" href="http://jonathan_leban@berkeley.edu/" rel="noopener ugc nofollow" target="_blank"> <em class="mt">这里的</em> </a> <em class="mt">是我的邮箱。</em></p></div></div>    
</body>
</html>