<html>
<head>
<title>Gradient Descent, clearly explained in Python, Part 2: The compelling code.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降，在 Python 中有清楚的解释，第 2 部分:引人注目的代码。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-clearly-explained-in-python-part-2-the-compelling-code-c21ee26fbc28?source=collection_archive---------27-----------------------#2020-08-09">https://towardsdatascience.com/gradient-descent-clearly-explained-in-python-part-2-the-compelling-code-c21ee26fbc28?source=collection_archive---------27-----------------------#2020-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="cd2d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">注意</strong> : <em class="kv">这是我上一篇文章的后续，这篇文章讨论了梯度下降的理论方面。如果你想了解一下，请点击</em> <a class="ae kw" rel="noopener" target="_blank" href="/gradient-descent-clearly-explained-in-python-part-1-the-troubling-theory-49a7fa2c4c06?source=your_stories_page-------------------------------------"> <em class="kv">链接</em> </a> <em class="kv">。如果你对这个理论不感兴趣，你可以直接加入！</em></p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/48ca01cddf89f78e9e94a0cb34b3ecab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9calCrrqS9opiytuA--7AA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="42ff" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">梯度下降是数据科学的基础，无论是深度学习还是机器学习。对梯度下降原理的扎实理解，一定会对你以后的工作有所帮助。</p><p id="c1e8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">你将真正理解<em class="kv">这些超参数</em>做什么，<em class="kv">后台发生什么</em>，以及<em class="kv">你</em><em class="kv"/>如何处理你在使用该算法时可能面临的问题，而不是玩弄超参数并希望得到最好的结果。</p><p id="2bc9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，梯度下降并不局限于一种算法。梯度下降的另外两种流行的“味道”(随机和小批量梯度下降)建立在主算法的基础上，并且可能是比普通批量梯度下降更多的算法。因此，我们还必须对这些算法有深入的了解，因为当我们的算法没有达到预期效果时，我们需要了解和分析它们的一些额外的超参数。</p><p id="f94b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">虽然理论对于牢固理解手头的算法至关重要，但梯度下降的实际编码及其不同的“味道”可能会被证明是一项困难但令人满意的任务。为了完成这项任务，文章的格式如下所示:</p><ol class=""><li id="cc8a" class="ln lo it jz b ka kb ke kf ki lp km lq kq lr ku ls lt lu lv bi translated">每种算法的简要概述。</li><li id="420b" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">算法的代码</li><li id="9e6f" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">对代码中不清楚部分的进一步解释</li></ol><p id="243f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将使用著名的波士顿住房数据集，它是 scikit-learn 中预先构建的。我们还将从头开始构建一个线性模型，所以请抓紧，因为你即将进入一个全新的世界！</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mb"><img src="../Images/6a3265b0b4298677843525200c3b2e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nqaz98QjbFA44baGdyn5LA.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">布拉登·科拉姆在<a class="ae kw" href="https://unsplash.com/s/photos/ready?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="1afd" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好的，那么首先让我们做一些基本的导入(通常的东西)。我不打算在这里做 EDA，因为这不是我们这篇文章的真正目的。然而，我将展示一些可视化效果来澄清一些事情。</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="e996" class="mh mi it md b gy mj mk l ml mm">import numpy as np<br/>import pandas as pd <br/>import plotly.express as px<br/>from sklearn.datasets import load_boston<br/>from sklearn.metrics import mean_squared_error</span></pre><p id="d98f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好了，为了让我们看到数据的样子，我将数据转换成 DataFrame 并显示输出。</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="c7e1" class="mh mi it md b gy mj mk l ml mm">data = load_boston()</span><span id="ecc6" class="mh mi it md b gy mn mk l ml mm">df = pd.DataFrame(data['data'],columns=data['feature_names'])<br/>df.insert(13,'target',data['target'])<br/>df.head(5)</span></pre><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mo"><img src="../Images/7cb91f60141ed6104971e32170614033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*845kQFMJd3ngEvhjm0Ppaw.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="e160" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">嗯，这里没什么特别的，我确信如果你在遇到这个之前做过任何 ML 项目。</p><p id="6ed1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，我们将定义我们的特征(X)和目标(y)。我们还将定义我们的参数向量，命名为 thetas，并将其初始化为零。</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="f5f4" class="mh mi it md b gy mj mk l ml mm">X,y = df.drop('target',axis=1),df['target']</span><span id="de44" class="mh mi it md b gy mn mk l ml mm">thetas = np.zeros(X.shape[1])</span></pre><h1 id="6e43" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">价值函数</h1><p id="52bb" class="pw-post-body-paragraph jx jy it jz b ka nm kc kd ke nn kg kh ki no kk kl km np ko kp kq nq ks kt ku im bi translated">回想一下，成本函数是用来衡量模型性能的，也是梯度下降法旨在改进的。我们将使用的成本函数被称为 MSE，或均方误差。公式是这样的:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3ae63d7234e6ab5adb6ad918805b0984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/1*D-7K2fUG8Ev3iA_81svXjw.gif"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="a26f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好，让我们把它编码出来:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="7fe3" class="mh mi it md b gy mj mk l ml mm">def cost_function(X,Y,B):<br/>    predictions = np.dot(X,B.T)<br/>    <br/>    cost = (1/len(Y)) * np.sum((predictions - Y) ** 2)<br/>    return cost</span></pre><p id="322a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里，我们将输入、标签和参数作为输入，并使用线性模型进行预测，获得成本，然后返回。如果第二行让您困惑，请回忆一下线性回归公式:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ns"><img src="../Images/83d02745d3740310766971585383c6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ld1Y1FJmoCUATJErN5irpg.png"/></div></div></figure><p id="03ac" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">因此，我们实际上是在获得我们的每个特征和它们相应的权重之间的点积。如果你仍然不确定我在说什么，看看这个视频。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="8f43" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">很好，现在让我们测试一下我们的成本函数，看看它是否真的有效。为了做到这一点，我们将使用 scikit-learn 的 mean_squared_error，获得结果，并将其与我们的算法进行比较。</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="b9ef" class="mh mi it md b gy mj mk l ml mm">mean_squared_error(np.dot(X,thetas.T),y)</span><span id="ac38" class="mh mi it md b gy mn mk l ml mm">OUT: 592.14691169960474</span><span id="c218" class="mh mi it md b gy mn mk l ml mm">cost_function(X,y,thetas)</span><span id="812d" class="mh mi it md b gy mn mk l ml mm">OUT: 592.14691169960474</span></pre><p id="9b71" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">太棒了，我们的成本函数起作用了！</p><h1 id="6704" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">特征缩放</h1><p id="7355" class="pw-post-body-paragraph jx jy it jz b ka nm kc kd ke nn kg kh ki no kk kl km np ko kp kq nq ks kt ku im bi translated">特征缩放是对线性模型(线性回归、KNN、SVM)至关重要的预处理技术。本质上，特征被缩小到更小的尺度，并且特征也在一定的范围内。将特征缩放想象成这样:</p><ol class=""><li id="b257" class="ln lo it jz b ka kb ke kf ki lp km lq kq lr ku ls lt lu lv bi translated">你有一座非常大的建筑</li><li id="4dc0" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">您想要保持建筑的<strong class="jz iu">形状</strong>，但是想要将其调整为更小的<strong class="jz iu">比例</strong></li></ol><p id="7c4c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">要素缩放通常用于以下情况:</p><ol class=""><li id="9667" class="ln lo it jz b ka kb ke kf ki lp km lq kq lr ku ls lt lu lv bi translated">如果算法使用欧几里德距离，那么特征缩放是必需的，因为欧几里德距离对大的量值是敏感的</li><li id="6a8c" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">特征缩放也可用于归一化具有大范围值的数据</li><li id="d52d" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">特征缩放还可以提高算法的速度</li></ol><p id="dca3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">虽然有许多不同的功能缩放方法可用，但我们将使用以下公式构建 MinMaxScaler 的自定义实现:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi nt"><img src="../Images/c5e3003f9745009e1f3e75e53d5b05d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*F249pj1fvGKGrlQx6gYcTQ.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="f6ec" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于上述原因，我们将使用缩放。</p><p id="86c2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，对于 python 实现:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="6eca" class="mh mi it md b gy mj mk l ml mm">X_norm = (X - X.min()) / (X.max() - X.min())<br/>X = X_norm</span></pre><p id="d3b6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里没什么特别的，我们只是把公式翻译成代码。现在，节目真正开始了:梯度下降！</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi nu"><img src="../Images/50301dfc200a170f3da7a737f80e5dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-P1ZDZSL8yWbShShdii_w.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@dtopkin1?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">戴恩·托普金</a>在<a class="ae kw" href="https://unsplash.com/s/photos/begin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="cbcc" class="mp mi it bd mq mr nv mt mu mv nw mx my mz nx nb nc nd ny nf ng nh nz nj nk nl bi translated">梯度下降</h1><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi oa"><img src="../Images/0a098222d8c9c3188a4fa1cc8ca389cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-O3A_R9Q9Iadff2Yd_-ZQ.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@rustyspear11?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Gavin Spear </a>在<a class="ae kw" href="https://unsplash.com/s/photos/standing-on-mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="2b74" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">具体来说，梯度下降是一种优化算法，通过迭代遍历数据并获得偏导数来寻找函数的最小值(在我们的情况下，MSE)。</p><p id="4fd1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果这看起来有点复杂，试着想象梯度下降，就好像一个人站在山顶上，他们正试图尽可能快地从山上爬下来，重复地向山的负方向“迈步”,直到他们到达底部。</p><p id="9bd9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，梯度下降有不同的版本，但你最常遇到的是:</p><ol class=""><li id="4f9d" class="ln lo it jz b ka kb ke kf ki lp km lq kq lr ku ls lt lu lv bi translated">批量梯度下降</li><li id="e46e" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">随机梯度下降</li><li id="92e1" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">小批量梯度下降</li></ol><p id="182d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们现在将依次讨论、实施和分析其中的每一项，让我们开始吧！</p><h1 id="4f93" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">批量梯度下降</h1><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/46c6b6854b99399ed318292af68d5d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*z9kvl9dugHH-R64-qBiQqw.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图片来自<a class="ae kw" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="c51d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">批量梯度下降可能是你遇到的第一种梯度下降。现在，我在这篇文章中不是很理论化(你可以参考我的<a class="ae kw" href="https://medium.com/@vagifaliyev/gradient-descent-clearly-explained-in-python-part-1-the-troubling-theory-49a7fa2c4c06" rel="noopener">上一篇</a>文章)，但本质上它计算的是<strong class="jz iu">整体(</strong>批次<strong class="jz iu"> ) </strong>数据集上系数的偏导数。这就是为什么你可能已经猜到它在大型数据集上会很慢。</p><p id="bb51" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，关于批量梯度下降的一个很好的想法是，当它达到最小值时，它会自动采取较小的步骤，因此不需要学习计划(我将在后面谈到这一点)。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/77b53f35c3553683026ea5d8f1b01d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*YiAkwR1uRIvoXV68RfdNkw.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="1aab" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们的数据集很小，所以我们可以像这样实现批量梯度下降:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="8358" class="mh mi it md b gy mj mk l ml mm">def batch_gradient_descent(X,Y,theta,alpha,iters):<br/>    cost_history = [0] * iters  # initalize our cost history list<br/>    for i in range(iters):         <br/>        prediction = np.dot(X,theta.T)                  <br/>        theta = theta - (alpha/len(Y)) * np.dot(prediction - Y,X)   <br/>        cost_history[i] = cost_function(X,Y,theta)               <br/>    return theta,cost_history</span></pre><p id="1407" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了澄清一些术语:</p><p id="3e39" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv"> alpha </em>:这是指学习率。</p><p id="20e1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv"> iters: </em>要运行的迭代次数。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="bb3e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">太好了，现在让我们看看结果吧！</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="bd96" class="mh mi it md b gy mj mk l ml mm"><em class="kv">batch_theta,batch_history=batch_gradient_descent(X,y,theta,0.05,500)</em></span></pre><p id="c241" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好吧，不是很快，但也不是很慢。让我们用新的和改进的参数来直观显示和获得我们的成本:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="a008" class="mh mi it md b gy mj mk l ml mm">cost_function(X,y,batch_theta)</span><span id="77a1" class="mh mi it md b gy mn mk l ml mm">OUT: 27.537447130784262</span></pre><p id="5331" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">哇，从 592 到 27！这只是梯度下降威力的一瞥！让我们将迭代次数的成本函数可视化:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="ef69" class="mh mi it md b gy mj mk l ml mm">fig = px.line(batch_history,x=range(5000),y=batch_history,labels={'x':'no. of iterations','y':'cost function'})<br/>fig.show()</span></pre><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi od"><img src="../Images/6777c4dd88c8b00165d9371bc0d47fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwHVTu4s0o2LzsNYLdWRMQ.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="f66d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好的，看这个图表，我们在大约 100 次迭代后达到一个大的下降，从那里开始，它逐渐下降。</p><p id="0782" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">因此，总结批量梯度:</p><h1 id="f12c" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">赞成的意见</h1><ol class=""><li id="3a99" class="ln lo it jz b ka nm ke nn ki oe km of kq og ku ls lt lu lv bi translated">有效且具有平滑曲线，当斜率达到全局最小值时会自动降低。</li><li id="ff15" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">最准确且最有可能达到全局最小值</li></ol><h1 id="be7f" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">骗局</h1><ol class=""><li id="c4ef" class="ln lo it jz b ka nm ke nn ki oe km of kq og ku ls lt lu lv bi translated">在大型数据集上可能会很慢</li><li id="5010" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">计算成本高</li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="f5b4" class="mp mi it bd mq mr nv mt mu mv nw mx my mz nx nb nc nd ny nf ng nh nz nj nk nl bi translated">随机梯度下降</h1><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi oa"><img src="../Images/83201457a5e01c57006442c9f55d5c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmuIZ9WDvv6hN86hUomrUg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="feea" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里，不是计算整个训练集的偏导数，偏导数的计算只在一个<strong class="jz iu">随机</strong>样本上进行(随机意味着随机)。</p><p id="1bd7" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这很好，因为计算只需要在一个训练样本上进行，而不是整个训练集，这使得它更快，更适合大型数据集。</p><p id="934d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，由于其随机性质，随机梯度下降不具有像批量梯度下降那样的平滑曲线，尽管它可能返回良好的参数，但它不能保证达到全局最小值。</p><h2 id="9e33" class="mh mi it bd mq oh oi dn mu oj ok dp my ki ol om nc km on oo ng kq op oq nk or bi translated">学习时间表</h2><p id="437a" class="pw-post-body-paragraph jx jy it jz b ka nm kc kd ke nn kg kh ki no kk kl km np ko kp kq nq ks kt ku im bi translated">解决随机梯度下降不能在最小值上稳定的问题的一种方法是使用被称为<em class="kv">学习计划的东西。</em></p><p id="f74f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">本质上，这逐渐降低了学习率。因此，学习率最初很大(这有助于避免局部最小值)，并随着接近全局最小值而逐渐降低。但是，您必须小心:</p><ol class=""><li id="6579" class="ln lo it jz b ka kb ke kf ki lp km lq kq lr ku ls lt lu lv bi translated">如果学习率降低得太快，那么算法可能会卡在局部最小值，或者它可能会在最小值中途冻结。</li><li id="3cb8" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">如果学习率降低得太慢，您可能会在最小值附近跳很长时间，仍然得不到最佳参数</li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="c4fe" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们现在将使用基本学习计划实施随机梯度下降:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="80c1" class="mh mi it md b gy mj mk l ml mm">t0,t1 = 5,50 # learning schedule hyperparameters</span><span id="9150" class="mh mi it md b gy mn mk l ml mm">def learning_schedule(t):<br/>    return t0/(t+t1)</span><span id="3fba" class="mh mi it md b gy mn mk l ml mm">def stochastic_gradient_descent(X,y,thetas,n_epochs=30):<br/>    c_hist = [0] * n_epochs # Cost history list<br/>    for epoch in range(n_epochs):<br/>        for i in range(len(y)):<br/>            random_index = np.random.randint(len(Y))<br/>            xi = X[random_index:random_index+1]<br/>            yi = y[random_index:random_index+1]<br/>            <br/>            prediction = xi.dot(thetas)<br/>            <br/>            gradient = 2 * xi.T.dot(prediction-yi)<br/>            eta = learning_schedule(epoch * len(Y) + i)<br/>            thetas = thetas - eta * gradient<br/>            c_hist[epoch] = cost_function(xi,yi,thetas)<br/>    return thetas,c_hist</span></pre><p id="f858" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在让我们运行我们的函数:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="0a56" class="mh mi it md b gy mj mk l ml mm">sdg_thetas,sgd_cost_hist = stochastic_gradient_descent(X,Y,theta)</span></pre><p id="afa3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好的，太好了，它起作用了！现在让我们看看结果:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="7fbd" class="mh mi it md b gy mj mk l ml mm">cost_function(X,y,sdg_thetas)</span><span id="28ce" class="mh mi it md b gy mn mk l ml mm">OUT:<br/>29.833230764634493</span></pre><p id="7670" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">哇！我们从 592 到 29，但是注意:我们只做了 30 次迭代。通过批量梯度下降，我们在<strong class="jz iu"> 500 次</strong>迭代后得到了 27 次！这只是随机梯度下降的非凡力量的一瞥。</p><p id="5e32" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们用一个线形图来再次形象化这一点:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi od"><img src="../Images/243ea590436912c089178904a0d98d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57jpJXjXykmqbMoZtGDRkg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure><p id="fd37" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于这是一个小数据集，批量梯度下降就足够了，然而这只是显示了随机梯度下降的威力。</p><p id="13e8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">总结一下随机梯度下降:</p><h1 id="0cab" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">优点:</h1><ol class=""><li id="5a2b" class="ln lo it jz b ka nm ke nn ki oe km of kq og ku ls lt lu lv bi translated">与批量梯度下降相比更快</li><li id="2877" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">更好地处理大型数据集</li></ol><h1 id="9f4f" class="mp mi it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">缺点:</h1><ol class=""><li id="c1f3" class="ln lo it jz b ka nm ke nn ki oe km of kq og ku ls lt lu lv bi translated">很难确定某个最小值</li><li id="d196" class="ln lo it jz b ka lw ke lx ki ly km lz kq ma ku ls lt lu lv bi translated">并不总是有一个清晰的路径，可以在最小值附近反弹，但永远不会达到最优最小值</li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="568b" class="mp mi it bd mq mr nv mt mu mv nw mx my mz nx nb nc nd ny nf ng nh nz nj nk nl bi translated">小批量梯度下降</h1><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi os"><img src="../Images/59f3418a76709c183d6e790fab72cff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1BQMkkFrFq5Mp9ND-_Bgg.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="cf7b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好了，就快到了，只差一个了！现在，在小批量梯度下降中，我们不是计算整个训练集或随机示例的偏导数，而是计算整个训练集的<strong class="jz iu">小子集</strong>的偏导数。</p><p id="ae1f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这给了我们比批量梯度下降快得多的速度，并且因为它不像随机梯度下降那样随机，所以我们更接近最小值。然而，它容易陷入局部极小值。</p><p id="882e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">同样，为了解决陷入局部最小值的问题，我们将在实现中使用一个基本的学习时间表。</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="e3a5" class="mh mi it md b gy mj mk l ml mm">np.random.seed(42) # so we get equal results</span><span id="7c78" class="mh mi it md b gy mn mk l ml mm">t0, t1 = 200, 1000<br/>def learning_schedule(t):<br/>    return t0 / (t + t1)</span><span id="d221" class="mh mi it md b gy mn mk l ml mm">def mini_batch_gradient_descent(X,y,thetas,n_iters=100,batch_size=20):<br/>    t = 0<br/>    c_hist = [0] * n_iters<br/>    for epoch in range(n_iters):<br/>        shuffled_indices = np.random.permutation(len(y))<br/>        X_shuffled = X_scaled[shuffled_indices]<br/>        y_shuffled = y[shuffled_indices]<br/>        <br/>        for i in range(0,len(Y),batch_size):<br/>            t+=1<br/>            xi = X_shuffled[i:i+batch_size]<br/>            yi = y_shuffled[i:i+batch_size]<br/>            <br/>            gradient = 2/batch_size * xi.T.dot(xi.dot(thetas) - yi)<br/>            eta = learning_schedule(t)<br/>            thetas = thetas - eta * gradient<br/>            c_hist[epoch] = cost_function(xi,yi,thetas)<br/>    return thetas,c_hist</span></pre><p id="4546" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们运行并获得结果:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="5941" class="mh mi it md b gy mj mk l ml mm">mini_batch_gd_thetas,mini_batch_gd_cost = mini_batch_gradient_descent(X,y,theta)</span></pre><p id="09ba" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">以及带有新参数的成本函数:</p><pre class="ky kz la lb gt mc md me mf aw mg bi"><span id="a66a" class="mh mi it md b gy mj mk l ml mm">cost_function(X,Y,mini_batch_gd_thetas)</span><span id="be78" class="mh mi it md b gy mn mk l ml mm">OUT: 27.509689139167012</span></pre><p id="d9b1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">又一次令人惊叹。我们运行了批量梯度下降迭代的 1/5，得到了更好的分数！谈效率！</p><p id="5884" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们再次绘制出函数:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi od"><img src="../Images/aee9ba2c0b38b5ba2c8b55bd15275ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAYb-hf3O5ZgXkXwer6UIA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者照片</p></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="ae46" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好了，我的梯度下降系列到此结束！我真的希望你喜欢它，并随时询问任何问题或要求任何澄清！</p><p id="26f2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，我建议你出去休息一下，因为有太多东西要消化了！尽可能地享受你的时间，记住:学习是有趣的，所以每天都要学！</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ot"><img src="../Images/2ca4f118cfd6778d7f9d66e9b68dd581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cwy-tCX8uzLZ53RC6kzFvQ.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">普里西拉·杜·普里兹在<a class="ae kw" href="https://unsplash.com/s/photos/thank-you?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>