<html>
<head>
<title>Machine Learning Algorithms. Here’s the End-to-End.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法。这里是端到端。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-heres-the-end-to-end-a5f2f479d1ef?source=collection_archive---------42-----------------------#2020-05-29">https://towardsdatascience.com/machine-learning-algorithms-heres-the-end-to-end-a5f2f479d1ef?source=collection_archive---------42-----------------------#2020-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9ca8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通用分类算法的端到端运行；包括随机森林、多项式朴素贝叶斯、逻辑回归、kNN 和来自 sklearn 的支持向量机</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5798d0a7b2ce7a4167f19627afac862f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opUFxti5MLr1yP5Dyic79g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·汤纳在<a class="ae ky" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【1】上的照片。</p></figure><h1 id="ba23" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ol class=""><li id="a1ef" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">介绍</li><li id="2022" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">随机森林</li><li id="4209" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">多项式朴素贝叶斯</li><li id="98d1" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">逻辑回归</li><li id="c543" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">支持向量机</li><li id="8f8a" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">k-最近邻</li><li id="fb77" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">摘要</li><li id="3a35" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">参考</li></ol><h1 id="dac4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="ac0f" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">本文的目标是:</p><blockquote class="nb nc nd"><p id="5ae1" class="mo mp ne lt b lu nf ju mq lw ng jx mr nh ni mt mu nj nk mw mx nl nm mz na me im bi translated">1.学习常见的数据科学、分类算法。</p><p id="e470" class="mo mp ne lt b lu nf ju mq lw ng jx mr nh ni mt mu nj nk mw mx nl nm mz na me im bi translated">2.执行这些算法的示例代码。</p></blockquote><p id="a7e2" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">虽然有几篇关于机器学习算法的文档和文章，但我想总结一下作为一名专业数据科学家最常用的算法。此外，我将包含一些带有虚拟数据的示例代码，以便您可以开始执行各种模型！下面，我将从最受青睐的库<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sklearn </a> ( <em class="ne">也称为 scikit-learn</em>)【2】中概述这些主要的分类算法:</p><ul class=""><li id="5084" class="lr ls it lt b lu nf lw ng ly nn ma no mc np me nq mg mh mi bi translated">随机森林(<em class="ne">RandomForestClassifier</em>)—可用于回归</li><li id="0b61" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nq mg mh mi bi translated">多项式朴素贝叶斯(<em class="ne">多项式</em></li><li id="10ed" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nq mg mh mi bi translated">逻辑回归(<em class="ne">逻辑回归</em></li><li id="eed2" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nq mg mh mi bi translated">支持向量机(<em class="ne"> svm </em> ) —可用于回归</li><li id="7ff7" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nq mg mh mi bi translated">k-最近邻(<em class="ne">KNeighborsClassifier</em>)-可用于回归</li></ul><p id="e54a" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><strong class="lt iu">什么是分类？</strong></p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="70ff" class="nw la it ns b gy nx ny l nz oa">Assigning new data into a category or bucket. </span></pre><p id="d3ce" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">而无监督学习，如常用的 K-means 算法，旨在将相似的数据组分组在一起，而无需标签、监督学习或分类——嗯，将数据分类为各种类别。下面描述了一个简单的分类示例。</p><p id="384e" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><em class="ne">标签:水果——香蕉、橘子和苹果</em></p><p id="0fb1" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><em class="ne">特征:形状、大小、颜色、种子数等。</em></p><p id="49f3" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">分类模型从关于水果的特征中学习，以建议输入食物和水果标签。现在，想象一下，对于我将在下面描述的这些分类算法，来自数据的每个观察值，输入，都有关于它的各种特征，然后将基于预定的标签进行分类。类别的其他名称包括标签、类、组(<em class="ne">通常在无监督聚类中引用，但仍然可以描述分类</em>)和桶。</p><p id="0cfb" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">分类的重要性不仅仅在于教育，还在于它在商业世界中极其实用和有益。比方说你要给成千上万的文档分类，给衣服分类，或者检测图像；这些都使用了我将在下面描述的这些流行且强大的分类模型。</p><h1 id="ecdd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">随机森林</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/1fc399ba2e7ad504e0b3a706400c5efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJGR-tSpLsbjxcNijkHtNg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Lukasz Szmigiel 在<a class="ae ky" href="https://unsplash.com/s/photos/trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【3】上拍摄的照片。</p></figure><p id="c5c5" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">这种集成分类器通过在所用数据集的子样本上拟合几个决策树来工作。然后，它对这些决策树的准确性进行平均，并创建一个比仅使用决策树更好的预测准确性，以控制过度拟合。与大多数分类器一样，有大量的参数可以帮助您调整您的模型，使其更差或更好(<em class="ne">希望更好</em>)。下面描述了常见的参数，我将包括默认的参数值，以便您对从哪里开始调优有一个大致的了解:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="ac1b" class="nw la it ns b gy nx ny l nz oa"><strong class="ns iu">n_estimators</strong>: number of trees in your forest (100)</span><span id="ccc4" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">max_depth</strong>: maximum depth of your tree (None) - recommendation, change this parameter to be an actual number because this parameter could cause overfitting from learning your traning data too well</span><span id="b8d9" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">min_samples_split</strong>: minimum samples required to split your node (2)</span><span id="c73a" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">min_samples_leaf</strong>: mimimum number of samples to be at your leaf node (1)</span><span id="5c66" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">max_features</strong>: number of features used for the best split ("auto")</span><span id="25df" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">boostrap</strong>: if you want to use boostrapped samples (True)</span><span id="bc77" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">n_jobs</strong>: number of jobs in parallel run (None) - for using all processors, put -1</span><span id="db44" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">random_state</strong>: for reproducibility in controlling randomness of samples (None)</span><span id="1c4d" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">verbose</strong>: text output of model in process (None)</span><span id="50fb" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">class_weight</strong>: balancing weights of features, n_samples / (n_classes * np.bincount(y)) (None) - recommendation, use 'balanced' for labels that are unbalanced</span></pre><p id="eccb" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">我将在这里提供一个随机森林算法的例子。它将使用虚拟数据(<em class="ne">所以准确性对我来说并不重要</em>)。重点是代码、概念，而不是准确性，除非您的输入数据已经很好地建立。在代码示例中有五个单独的行，您可以注释掉所有的分类器，<em class="ne"> clf </em> <strong class="lt iu">，</strong>除了一个——您正在测试的那个。下面是主代码示例[4]:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9efd" class="nw la it ns b gy nx ny l nz oa"># import libraries<br/>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br/>from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.pipeline import Pipeline, FeatureUnion<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn import svm<br/>from sklearn import metrics<br/>import pandas as pd</span><span id="1048" class="nw la it ns b gy oc ny l nz oa"># text and numeric classes that use sklearn base libaries<br/>class TextTransformer(BaseEstimator, TransformerMixin):<br/>    """<br/>    Transform text features<br/>    """<br/>    def __init__(self, key):<br/>        self.key = key</span><span id="6b8a" class="nw la it ns b gy oc ny l nz oa">def fit(self, X, y=None, *parg, **kwarg):<br/>        return self</span><span id="ca15" class="nw la it ns b gy oc ny l nz oa">def transform(self, X):<br/>        return X[self.key]<br/>    <br/>class NumberTransformer(BaseEstimator, TransformerMixin):<br/>    """<br/>    Transform numeric features<br/>    """<br/>    def __init__(self, key):<br/>        self.key = key</span><span id="c7d0" class="nw la it ns b gy oc ny l nz oa">def fit(self, X, y=None):<br/>        return self</span><span id="0204" class="nw la it ns b gy oc ny l nz oa">def transform(self, X):<br/>        return X[[self.key]]</span><span id="580f" class="nw la it ns b gy oc ny l nz oa"># read in your dataframe<br/>df = pd.read_csv('/Users/data.csv')</span><span id="88a9" class="nw la it ns b gy oc ny l nz oa"># take a look at the first 5 observations<br/>df.head()</span><span id="67ef" class="nw la it ns b gy oc ny l nz oa"># use the term-frequency inverse document frequency vectorizer to transfrom count of text<br/># into a weighed matrix of term importance<br/>vec_tdidf = TfidfVectorizer(ngram_range=(1,1), analyzer='word', norm='l2')</span><span id="8725" class="nw la it ns b gy oc ny l nz oa"># compile both the TextTransformer and TfidfVectorizer <br/># to the text 'Text_Feature' <br/>color_text = Pipeline([<br/>                ('transformer', TextTransformer(key='Text_Feature')),<br/>                ('vectorizer', vec_tdidf)<br/>                ])</span><span id="ac97" class="nw la it ns b gy oc ny l nz oa"># compile the NumberTransformer to 'Confirmed_Test', 'Confirmed_Recovery', <br/># and 'Confirmed_New' numeric features<br/>test_numeric = Pipeline([<br/>                ('transformer', NumberTransformer(key='Confirmed_Test')),<br/>                ])<br/>recovery_numeric = Pipeline([<br/>                ('transformer', NumberTransformer(key='Confirmed_Recovery')),<br/>                ])<br/>new_numeric = Pipeline([<br/>                ('transformer', NumberTransformer(key='Confirmed_New')),<br/>                ])</span><span id="599b" class="nw la it ns b gy oc ny l nz oa"># combine all of the features, text and numeric together<br/>features = FeatureUnion([('Text_Feature', color_text),<br/>                      ('Confirmed_Test', test_numeric),<br/>                      ('Confirmed_Recovery', recovery_numeric),<br/>                      ('Confirmed_New', new_numeric)<br/>                      ])</span><span id="ea3f" class="nw la it ns b gy oc ny l nz oa"># create the classfier from list of algs - choose one only<br/>clf = RandomForestClassifier()<br/>clf = MultinomialNB()<br/>clf = LogisticRegression()<br/>clf = svm.SVC()<br/>clf = KNeighborsClassifier()</span><span id="2386" class="nw la it ns b gy oc ny l nz oa"># unite the features and classfier together<br/>pipe = Pipeline([('features', features),<br/>                 ('clf',clf)<br/>                 ])</span><span id="1109" class="nw la it ns b gy oc ny l nz oa"># transform the categorical predictor into numeric<br/>predicted_dummies = pd.get_dummies(df['Text_Predictor'])</span><span id="1164" class="nw la it ns b gy oc ny l nz oa"># split the data into train and test<br/># isolate the features from the predicted field<br/>text_numeric_features = ['Text_Feature', 'Confirmed_Test', 'Confirmed_Recovery', 'Confirmed_New']<br/>predictor = 'Text_Predictor'</span><span id="10ec" class="nw la it ns b gy oc ny l nz oa">X_train, X_test, y_train, y_test = train_test_split(df[text_numeric_features], df[predictor], <br/>                                                    test_size=0.25, random_state=42)</span><span id="61c0" class="nw la it ns b gy oc ny l nz oa"># fit the model<br/>pipe.fit(X_train, y_train)</span><span id="357e" class="nw la it ns b gy oc ny l nz oa"># predict from the test set<br/>preds = pipe.predict(X_test)</span><span id="b50d" class="nw la it ns b gy oc ny l nz oa"># print out your accuracy!<br/>print("Accuracy:",metrics.accuracy_score(y_test, preds))</span></pre><p id="25bb" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><em class="ne">如果你想看看. py 格式的 GitHub gist 在 python 中是什么样子的，这里有我的 GitHub gist 的代码【4】:</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GitHub gist [4]上作者的整篇文章示例代码。</p></figure><h1 id="f66c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">多项式朴素贝叶斯</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7f93be275a5426ce4c012b8e9e190cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2Q7DkLcmwYUm5mfHDCyUw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">帕特里克·托马索在<a class="ae ky" href="https://unsplash.com/s/photos/words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【5】上拍摄的照片。</p></figure><p id="747e" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">这个分类器是一个朴素贝叶斯分类器，用于多项式模型，正如算法的名字所暗示的那样。它广泛应用于文本分类问题中，其中文本是来自字数统计的特征。使用该算法时，建议使用<em class="ne">术语-频率逆文档频率</em> ( <em class="ne"> td-idf </em>)管道，因为您想要一个分布良好的特性。因为这个分类器是基于<em class="ne">贝叶斯定理</em>的，所以假设特征之间存在独立性。下面是分类器的主要代码:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d6ae" class="nw la it ns b gy nx ny l nz oa">clf = MultinomialNB()</span></pre><p id="6393" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">所用模型的参数和属性如下:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e392" class="nw la it ns b gy nx ny l nz oa">parameters - </span><span id="ba8b" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">alpha</strong>: a paramter for smoothing (1.0)</span><span id="e026" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">class_prior</strong>: looking at the previous class probability (None) </span><span id="8628" class="nw la it ns b gy oc ny l nz oa">attributes -</span><span id="c865" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">feature_count</strong>: number of samples for each class or feature (number of classes, number of features)</span><span id="ff2d" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">n_features</strong>: number of features for sample</span></pre><h1 id="8e2b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">逻辑回归</h1><p id="c4c4" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">逻辑回归，或者更像逻辑分类，是一种使用<em class="ne"> logit </em>函数的算法。估计值通常是二进制值，如真-假、是-否和 0-1。逻辑函数也使用 sigmoid 函数。关于这个模型的细节有很多文章(<em class="ne">见</em>下面有用的链接)，但是我将强调如何执行 sklearn 库中的代码。下面是分类器的主要代码:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="a1ba" class="nw la it ns b gy nx ny l nz oa">clf = LogisticRegression()</span></pre><p id="364b" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">逻辑回归有许多重要的参数。以下是其中的几个例子:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e671" class="nw la it ns b gy nx ny l nz oa"><strong class="ns iu">penalty</strong>: l1 or l2 (lasso or ridge), the normality of penalization (l2)</span><span id="a4e1" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">multi_class</strong>: binary versus multiclass label data ('auto')</span></pre><h1 id="4228" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">支持向量机</h1><p id="e0d0" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">一种监督技术，不仅包括分类，还包括回归。支持向量机(SVMs)的好处是，它比其他构成高维度的算法效果更好。支持向量机的目标是找到在两个分类类别之间划分数据的线(<em class="ne">它们在那条线</em>之间的距离)。支持向量机的代码与上面的算法略有不同:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="42be" class="nw la it ns b gy nx ny l nz oa">clf = svm.SVC()</span></pre><p id="5e58" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">这种代码上的差异引用了增加的<em class="ne"> SVC </em>，也就是 C-支持向量分类。对于多类情况，您可以使用参数:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="c164" class="nw la it ns b gy nx ny l nz oa">parameter<strong class="ns iu"> - </strong></span><span id="120b" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">decision_function_shape</strong>: 'ovr' or 'one-versus-rest' approach</span></pre><h1 id="0d54" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">k-最近邻</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/99ba5beef84c2f9639e97dd190bd815d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lD8WcYSKt6r3nNmg8jTvLg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@woodpecker65?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">克里斯蒂安·斯塔尔</a>在<a class="ae ky" href="https://unsplash.com/s/photos/neighbors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【6】上拍摄。</p></figure><p id="ef64" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">k 近邻法，或称 KNN 法，是通过多数投票来实现的。它根据周围的邻居对新类别进行分类；常用的距离测量函数是欧几里德函数。用于该算法的代码是:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="709e" class="nw la it ns b gy nx ny l nz oa">clf = KNeighborsClassifier()</span></pre><p id="66a3" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">最重要的参数可能是:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="389b" class="nw la it ns b gy nx ny l nz oa">parameter - </span><span id="efe0" class="nw la it ns b gy oc ny l nz oa"><strong class="ns iu">n_neighbors</strong>: number of neighbors (5)</span></pre><h1 id="f801" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="1fde" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">虽然有大量关于广泛使用的机器学习算法的文章；</p><blockquote class="oh"><p id="ece0" class="oi oj it bd ok ol om on oo op oq me dk translated">我希望这篇文章对你有用，因为我已经提供了端到端的代码。</p></blockquote><p id="f34d" class="pw-post-body-paragraph mo mp it lt b lu or ju mq lw os jx mr ly ot mt mu ma ou mw mx mc ov mz na me im bi translated">请查看下面有用的链接以及本文中描述的每种算法的官方文档。感谢您阅读我的文章，如果您有任何问题，请告诉我！</p><h1 id="ad60" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="826f" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">[1]约翰·汤纳在<a class="ae ky" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，(2016)</p><p id="43b5" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">[2] sklearn，<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>，(2020)</p><p id="e634" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">[3]Lukasz Szmigiel 在<a class="ae ky" href="https://unsplash.com/s/photos/trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，(2015)</p><p id="47ab" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">[4] M.Przybyla，<a class="ae ky" href="https://gist.github.com/mprzybyla123/9af0d3f9ff09e8a04ddca8d1e14aa01e" rel="noopener ugc nofollow" target="_blank">分类-模型</a>，(2020)</p><p id="24b5" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">[5]帕特里克·托马索在<a class="ae ky" href="https://unsplash.com/s/photos/words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，(2016)</p><p id="22de" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated">[6]照片由<a class="ae ky" href="https://unsplash.com/@woodpecker65?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Christian Stahl </a>在<a class="ae ky" href="https://unsplash.com/s/photos/neighbors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄，(2017)</p><p id="1a3c" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><strong class="lt iu">有用的 sklearn 链接:</strong></p><p id="85ab" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林</a></p><p id="4274" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html" rel="noopener ugc nofollow" target="_blank">多项式朴素贝叶斯</a></p><p id="87f5" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">逻辑回归</a></p><p id="7a00" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank">支持向量机</a></p><p id="0670" class="pw-post-body-paragraph mo mp it lt b lu nf ju mq lw ng jx mr ly ni mt mu ma nk mw mx mc nm mz na me im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">K-最近邻</a></p></div></div>    
</body>
</html>