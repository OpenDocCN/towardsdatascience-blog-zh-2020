# 如果只训练了批量归一化图层呢？

> 原文：<https://towardsdatascience.com/what-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c?source=collection_archive---------25----------------------->

## 你可能会感到惊讶，这很有效。

![](img/b8faa0237da141536746250eb2d92661.png)

卡西·乔希在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

拿我来说，我绝不会把我的钱押在这上面。

最近，我阅读了由 Jonathan Frankle、David J. Schwab 和 Ari S. Morcos 撰写的论文“*训练 BatchNorm 和 Only BatchNorm:关于 CNN 中随机特征的表达能力”*，该论文最近在 [*arXiv* 平台](https://arxiv.org/abs/2003.00152)上发布。这个想法立刻引起了我的注意。到目前为止，我从未将[批量标准化](https://arxiv.org/abs/1502.03167) (BN)层视为学习过程本身的一部分，只是作为深度网络收敛和稳定的辅助手段。几个实验之后，我大错特错了。在下文中，我将介绍我在复制这篇论文的结果时所采取的措施，以及我从中学到的东西。

更详细地说，我使用 Tensorflow 2 Keras API 成功地再现了论文的主要实验，得出了类似的结论。也就是说，ResNets 可以通过仅训练批量归一化图层的伽马(γ)和贝塔(β)参数，在 CIFAR-10 数据集中获得令人满意的结果。从数字上看，我使用 ResNet-50、101 和 152 架构获得了 45%、52%和 50%的顶级精度，这远远算不上很好，但也远非随机。

在下文中，我概述了批处理规范化的概念、其好处背后的常见解释、我使用的代码以及获得的结果。最后，我对结果及其相关性进行了讨论。

最近，这项工作被刊登在了 deeplearning.ai 的 Batch 新闻简报上。

# 批量标准化

简而言之，批量归一化层估计其输入的均值(μ)和方差(σ)并产生标准化输出，*即，*输出具有零均值和单位方差。在实践中，这种技术有意义地提高了深度网络的收敛性和稳定性。此外，它使用两个参数(γ和β)来转换和调整输出。

作为该层的 *x* 输入和 *z* 输出， *z* 由以下公式给出:

![](img/efe2c58e620425716b0b782a6ff1054c.png)

图 1:批量标准化表达式

虽然μ和σ参数是根据输入数据估计的，但γ和β是可训练的。因此，反向传播算法可以利用它们来优化网络。

如前所述，这种操作可以显著提高网络训练的速度，并改善其对保留数据的性能。而且，它没有禁忌症。出于这个原因，大多数模型大量使用它，经常在所有 Conv-ReLU 操作之间，形成“Conv-BN-ReLU”三重奏(及其变体)。然而，尽管这是最常见的层之一，其好处背后的原因在文献中有很多争论。这里总结了三种主要给出的解释。

*编辑:*在某些情况下，批量定额确实有禁忌症。最近的工作提高了人们的认识，尽管提高了收敛性，批量规范严重影响训练速度。特别是，当跨多个 GPU 和设备进行训练时，在不影响并行性能的情况下计算批量统计数据是一个巨大的挑战。出于这个原因，几位作者提出了替代方案，如[自归一化网络](https://arxiv.org/abs/1706.02515)甚至[无归一化网络](https://arxiv.org/abs/2102.06171)。这些工作旨在保留批处理规范的好处，同时避免训练期间数据点之间的任何依赖。

**内部共变移位:**简单来说，如果输出具有零均值和单位方差，则下一层基于稳定输入进行训练。换句话说，它防止输出变化太大。虽然这是[最初的解释](https://arxiv.org/abs/1502.03167)，后来的著作发现[与证据](https://arxiv.org/abs/1805.11604)相矛盾，否定了这个假设。简而言之，如果你训练 VGG 网络(1)没有 BN，(2)有 BN 和(3)有 BN 加上人工协方差偏移，方法(2)和(3)仍然优于(1)，尽管人工协方差偏移被添加到网络中。

**输出平滑:** BN 也被认为是[平滑优化景观](https://arxiv.org/pdf/1805.11604.pdf)，减少损失函数的变化并限制其梯度。更平滑的目标训练更可预测，更不容易出现数字问题。

**长度方向解耦:**一些作者认为 BN 是优化问题的[改进公式](https://arxiv.org/abs/1805.10694)，因此可以扩展到更传统的优化设置。更详细地说，BN 框架允许独立地优化参数的长度和方向，从而提高收敛性。

总之，这三种解释都集中在批处理规范化的规范化方面。相比之下，我们将着眼于 BN 的移动和缩放点，由γ和β参数实现。

# 复制纸张

如果一个想法是好的，它应该对实现和超参数的选择有弹性。在我的代码中，我使用 Tensorflow 2 和我自己选择的超参数尽可能简单地重新创建了主实验。更详细地说，我测试了以下命题:

> **ResNet** 型号在 **CIFAR-10** 上可以达到**体面的结果**除了批次归一化参数外**所有重量都锁定。**

因此，我将使用 Keras 的 CIFAR-10 和 ResNet 模块以及对 CIFAR-10 数据集的总体建议，即分类交叉熵损失和 Softmax 激活。我的代码下载数据集和随机初始化的 ResNet 模型，冻结不需要的层，并使用 1024 个图像的批量大小训练 50 个时期。您可以检查下面的代码:

在上面的代码中应该注意一些事情:

1.  [Keras API](https://keras.io/applications/#resnet) 只有 ResNet-50、101 和 152 型号。为了简单起见，我只用过这些。如果您想深入了解，请参考本指南[了解整个 ResNet 架构的定制实现。](https://keras.io/examples/cifar10_resnet/)
2.  ResNet 模型对γ参数使用“一”初始化策略。在我们有限的训练场景中，这过于对称，无法通过梯度下降来训练。相反，如论文中所建议的，使用“he_normal”初始化。为此，我们在训练前手动重新初始化批量标准化权重。
3.  作者使用 128 幅图像的批量大小和动量为 0.9 的 SGD 优化器训练了 160 个时期。学习率最初设置为 0.01，并在时期 80 和 120 预定为 0.001 和 0.0001。对于这样一个幼稚的想法，我发现这太具体了。取而代之的是，我使用了 50 个纪元、1024 的批量、普通的 Adam 和 0.01 的固定学习率。如果这个想法是好的，这应该不是一个问题。
4.  作者也使用了数据扩充，而我没有。同样，如果这个想法是好的，这些改变都不会是一个大问题。

## 结果

以下是我使用上述代码获得的结果:

![](img/df598f75729cb5bf57ea8a1b248e44fc.png)

ResNet 模型的训练精度仅训练批量归一化图层

![](img/e82111cf493464baa9ca7e9ee5e02543.png)

ResNet 模型的验证准确性仅训练批处理规范化图层

从数字上看，这三个模型分别达到了 50%、60%和 62%的训练精度以及 45%、52%和 50%的验证精度。

为了更好地理解模型的表现，我们应该始终考虑随机猜测的表现。CIFAR-10 数据集有十个类。因此，随机地，我们有 10%的机会是正确的。以上方法比随机猜测要好五倍左右。因此，我们可以认为他们有*不错的*表现。

有趣的是，验证准确性在十个时期后才开始增加，这是一个明显的迹象，即对于前十个时期，网络只是尽可能地过度拟合数据。随后，验证性能显著提高。然而，它每五个时期变化很大，这表明该模型不是很稳定。

在论文中，图 2 显示他们实现了大约 70%、大约 75%和大约 77%的验证准确性。考虑到作者做了一些调整，使用了定制的训练计划，并采用了数据扩充，这似乎很合理，与我的发现一致，证实了假设。

使用一个 866 层的 ResNet，作者达到了大约 85%的准确率，这仅比通过训练整个架构可达到的大约 91%低几个百分点。此外，他们测试了不同的初始化方案、架构，并测试了解冻最后一层和跳过连接，这导致了一些额外的性能增益。

除了准确性，作者还研究了γ和β参数的直方图，发现通过将γ设置为接近零的值，网络学会了抑制每个 BN 层中大约三分之一的所有激活。

## 讨论

此时，你可能会问:为什么会这样？首先，很好玩:)第二，BN 层是司空见惯的，但我们对它们的作用还是只有肤浅的了解。我们知道的是它们的好处。第三，这种调查让我们更深入地了解我们的模型是如何运作的。

我不相信这本身有实际应用。没有人会冻结他们的层，把这一切都留给 BNs。然而，这可能会激发不同的训练计划。也许像这样训练网络几个时期，然后训练所有权重可能会导致更好的性能。相反，这种技术对于微调预先训练的模型可能是有用的。我也可以看到这个想法被用来削减大型网络的权重。

这项研究最让我困惑的是，我们都忽略了这两个参数。至少我从来不介意这两者。我记得只看到一个关于它的讨论，它认为在 ResNet 块上用“零”初始化γ是好的，这样可以迫使反向传播算法在早期使用更多的跳过连接。

我的第二个问题是关于 [SELU](https://arxiv.org/abs/1706.02515) 和[塞卢](https://arxiv.org/abs/1807.10117)激活函数，它们具有自我规范化的特性。这两个函数都使批处理规范化层变得过时，因为它们在训练期间会自然地规范化它们的输出。现在，我问自己，这是否抓住了批处理规范化层的全部。

最后，假设还是有点原始。它只考虑了 CIFAR-10 数据集和非常深的网络。如果这可以扩展到其他数据集或解决不同的任务，如仅 Batchnorm 的 GAN，则它是开放的。此外，我会很有兴趣看到一篇关于γ和β在完全训练好的网络中的作用的后续文章。

如果你对这篇文章有任何问题，欢迎评论或[联系我](https://www.linkedin.com/in/ygorreboucas/)。如果你是新手，我强烈推荐[订阅](https://ygorserpa.medium.com/membership)。对于数据和 IT 专业人员来说，中型文章是 StackOverflow 的完美组合，对于新手来说更是如此。注册时请考虑使用[我的会员链接。](https://ygorserpa.medium.com/membership)

感谢阅读:)