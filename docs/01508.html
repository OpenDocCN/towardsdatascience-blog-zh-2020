<html>
<head>
<title>Ensemble Learning from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的集成学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca?source=collection_archive---------28-----------------------#2020-02-10">https://towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca?source=collection_archive---------28-----------------------#2020-02-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7570" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">引入集成学习，这是一种通过组合训练好的模型来提高性能的强大工具。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5f955f03faf3c80f8d0b93fb5bc952b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAZn1hGX_u38LGNnXJls0g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://www.technologyreview.com/s/615102/tree-planting-is-a-great-idea-that-could-become-a-dangerous-climate-distraction/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="0ac4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于许多数据科学和应用机器学习的初学者来说，找到符合手头数据的<strong class="ky ir">最佳模型</strong>通常意味着尝试尽可能多的合理选项，<strong class="ky ir">保留最佳模型，而其余的则被丢弃</strong>。集成学习提供了一种替代方案，即通过<strong class="ky ir">所有经过训练的模型</strong>形成一个集成，其性能可以与最好的模型一样好，<strong class="ky ir">如果不是更好</strong>！</p><p id="a915" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集成学习力量的一个合法代表是<strong class="ky ir">随机森林</strong>，在本文中，我们将揭示一些可以用来建立集成模型的<strong class="ky ir">方法</strong>，并直观地解释其背后的<strong class="ky ir">原理</strong>。</p><p id="d13f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的结构遵循<strong class="ky ir"> <em class="ls">第 7 章</em></strong>-<strong class="ky ir">-<em class="ls">集成学习与随机森林</em> </strong>，出自<strong class="ky ir"> <em class="ls">用 Scikit-Learn 的动手机器学习，Keras 和 TensorFlow，第 2 版，</em>作者<em class="ls"> Aurélien Géron (O'Reilly)。</em>T29】</strong></p><h2 id="d94d" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">大数定律</h2><blockquote class="mm mn mo"><p id="cb90" class="kw kx ls ky b kz la jr lb lc ld ju le mp lg lh li mq lk ll lm mr lo lp lq lr ij bi">“三个臭皮匠，顶个诸葛亮”</p></blockquote><p id="6f8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我第一次看到<strong class="ky ir">大数定律</strong>时，我在学校里学到的这句古老的中文谚语突然出现在我的脑海里。它解释了被称为<strong class="ky ir">群众的智慧</strong>——<strong class="ky ir">一大群人的集体智慧可能比一个专家的智慧更好。</strong></p><p id="e11a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集成学习通过组合<strong class="ky ir">独立的</strong>模型(称为<strong class="ky ir">预测器</strong>)来应用该定理，以形成<strong class="ky ir">强学习器</strong>(实现高精度)。它甚至在所有预测者都是<strong class="ky ir">弱学习者</strong>时也能工作(表现仅比<strong class="ky ir">随机猜测</strong>稍好)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/fdaf08ede8fca22d81a1607ccb2768ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agvZ0DyTyrkNYMtI3u1j1w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mt">“用 Scikit-Learn、Keras 和 TensorFlow 实践机器学习，第二版，作者 Aurélien Géron (O'Reilly)。”，第七章。</em></p></figure><p id="6964" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是为什么会这样呢？<strong class="ky ir">群众的智慧</strong>可能听起来很“聪明”，但它仍然有点<strong class="ky ir">违反直觉</strong>。为了回答这个问题，下面的类比可能有助于揭示这个秘密:假设你投掷一枚<strong class="ky ir">略有偏差的硬币</strong>，有 51%的机会正面朝上。如果你做 100 次，不太可能会有<strong class="ky ir">正好</strong> 51 个头，甚至有可能你最后没有<strong class="ky ir">多数个头</strong>。但随着你不断折腾，你会观察到人头的<strong class="ky ir">比</strong> <strong class="ky ir">会越来越接近<strong class="ky ir"> 51% </strong>。您进行的试验越多，平均<strong class="ky ir"/><strong class="ky ir">值</strong>就越接近预期<strong class="ky ir"/><strong class="ky ir">值</strong>，获得多数人头</strong>的概率就越大。</p><p id="81b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集成学习在本质上做着同样的事情——弱学习者在分类任务中只有微小的机会预测正确的类别，但是如果我们使用许多独立的预测器做出同样的预测，聚集的结果将更有可能预测正确的类别。</p><p id="dd8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集合模型的输出是<strong class="ky ir">关于<strong class="ky ir">分类</strong>任务时最频繁的预测</strong>。对于一个<strong class="ky ir">回归</strong>任务，使用<strong class="ky ir">平均值</strong>代替。</p><h2 id="6484" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">集成方法</h2><p id="7a53" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">训练一个集成时的关键要素是，它的所有分类器必须完全独立，这可以通过使用不同的算法来实现，或者使用相同的算法，但在训练集的不同子集上进行训练。在这里，我们探索这两种方法来构建集成分类器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/c5000c037eb5052f591027ec8239bf8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AU_Yuk-8ihw768gwv7hVCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mt">“用 Scikit-Learn，Keras &amp; TensorFlow 进行动手机器学习”，第 7 章。</em></p></figure><h2 id="1c6e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">1.投票分类器</h2><p id="7056" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">训练不同的预测器算法以形成集成被称为<strong class="ky ir">投票分类器</strong>，其机制很简单——一组不同的算法在同一个<strong class="ky ir">训练集</strong>上训练，当进行预测时，集成的输出被<strong class="ky ir">聚集</strong>，取最多<strong class="ky ir">投票的类别</strong>作为集成的预测。</p><p id="6de2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当通过每个预测器的<strong class="ky ir">输出类</strong>进行聚合时，称为<strong class="ky ir">硬投票</strong>，当使用<strong class="ky ir">输出概率</strong>时，称为<strong class="ky ir">软投票</strong>。软投票<strong class="ky ir"> </strong>通常表现更好，因为它捕捉了更多的<strong class="ky ir">细微差别</strong>(逻辑上它要求所有的预测器都能够输出概率)。</p><h2 id="e9a8" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">2.装袋和粘贴</h2><p id="1bd8" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">另一方面，代替训练不同的算法来实现<strong class="ky ir">多样性</strong>，一个替代方案是使用相同类型的预测器，但是在训练集的不同<strong class="ky ir">随机采样子集</strong>上进行训练。</p><p id="c198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">装袋</strong>和<strong class="ky ir">粘贴</strong>是对这些子集进行抽样的两种不同方式，它们的区别在于有无替换的抽样<strong class="ky ir">。</strong></p><p id="d1b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当通过<strong class="ky ir">打包</strong>(带替换)进行采样时，随机<strong class="ky ir">选择一个实例</strong>并立即将其返回给训练集，因此在每次选择时，源数据仍具有其所有实例，并且可以再次<strong class="ky ir">选择相同的实例</strong>。<strong class="ky ir">粘贴</strong>反过来，<strong class="ky ir">同一个实例不能在同一个子集中出现超过一次</strong>。</p><p id="ae1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于打包和粘贴，同一个实例可以出现在不同的子集中，但是只有<strong class="ky ir">打包</strong>允许<strong class="ky ir">同一个实例</strong>在同一个子集<strong class="ky ir">中出现两次或更多次</strong>。由于所有子集都具有相同大小的<strong class="ky ir"/>，具有替换的采样<strong class="ky ir">可以实现更多的<strong class="ky ir">子集多样性</strong>，并且它们对应的预测器将更少<strong class="ky ir">相关</strong>。这种方法的另一个优点是，每个预测器都有不在它们的训练子集中的实例，它们可以直接用于<strong class="ky ir">验证</strong>，而不必保存验证数据。</strong></p><p id="d124" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机森林</strong>是集成学习器家族的著名成员，它由一组在不同子集<strong class="ky ir">采样</strong> <strong class="ky ir">到</strong> <strong class="ky ir">打包</strong>上训练的<strong class="ky ir">决策</strong> <strong class="ky ir">树</strong>(与训练集大小相同)。当分割给定的<strong class="ky ir">节点</strong>时，所使用的<strong class="ky ir">特征</strong>是从随机生成的<strong class="ky ir">特征子空间</strong>中选择的最合适的一个，而不是从所有可用特征的空间中选择的。<strong class="ky ir">多余的</strong> <strong class="ky ir">树</strong>，各种随机森林，更进一步——不仅特征来自随机选择的子空间，而且使用的<strong class="ky ir">阈值</strong>也是<strong class="ky ir">随机生成的</strong><strong class="ky ir"/>。</p><h2 id="15c1" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">3.增强——连续训练</h2><p id="d3ff" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated"><strong class="ky ir">增强</strong>使用与之前相同的<strong class="ky ir"/><strong class="ky ir">预测器</strong>用于集合，但不是在<strong class="ky ir">并行</strong>中采样<strong class="ky ir">不同的</strong> <strong class="ky ir">子集</strong>和训练预测器，而是顺序<strong class="ky ir">完成</strong>，每个预测器试图<strong class="ky ir">纠正</strong>其<strong class="ky ir">前任</strong>的错误。</p><p id="7a87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">再来看看<strong class="ky ir"> AdaBoost </strong>和<strong class="ky ir">渐变提升</strong>。</p><p id="4edc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> — AdaBoost </strong></p><p id="9d9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当对集合中给定的预测器进行训练时，算法<strong class="ky ir">对<strong class="ky ir">实例权重</strong>进行重新加权，使得<strong class="ky ir">错误分类的</strong>实例能够从下一个预测器获得更多的<strong class="ky ir"> <em class="ls">关注</em> </strong>，并且有希望将其正确分类。实例权重定义了实例被选择为子集的一部分的机会-给定实例的实例权重<strong class="ky ir">越高，<strong class="ky ir">越多，</strong> <strong class="ky ir">可能</strong>它会在采样时出现在子集中。</strong></strong></p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d483" class="lt lu iq nb b gy nf ng l nh ni">AdaBoost Algorithm:</span><span id="c89b" class="lt lu iq nb b gy nj ng l nh ni">1. Set the initial <strong class="nb ir">instance weights</strong>(w)to <em class="ls">1/m</em>, with <em class="ls">m</em> being the number of training instances in the training set.</span><span id="c5eb" class="lt lu iq nb b gy nj ng l nh ni">2. Compute the <strong class="nb ir">predictor weight</strong>(⍺) of the current predictor using its <strong class="nb ir">weighted error rate</strong>(r).</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e8e77d28c196d4f38ee8f33d76a831d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcVK7W-bD3PCtfoI1X4Fiw.png"/></div></div></figure><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="a6ed" class="lt lu iq nb b gy nf ng l nh ni">3. To train the next predictor, the instance weights are updated.</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/b71abc0954b92796c7bc2b2e7bb8cb4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg_lFxpewVJK-RylQZtfyw.png"/></div></div></figure><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="8605" class="lt lu iq nb b gy nf ng l nh ni">4. Finally, a new predictor is trained using the updated weights, steps 2 to 4 are repeated until the number of predictors is reached or a perfect predictor is found.</span></pre><p id="0991" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在推断时间期间，集合的预测是其输出的<strong class="ky ir">加权</strong> <strong class="ky ir">平均值</strong>。使用的<strong class="ky ir">权重</strong>是步骤 2 中的<strong class="ky ir">预测权重</strong>。作为随机猜测的，预测器是执行<strong class="ky ir">更好</strong>、<strong class="ky ir">更差</strong>还是<strong class="ky ir">确切地说</strong> <strong class="ky ir">决定了其对集合预测的</strong>贡献的<strong class="ky ir">类型:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/bbca6717b5b7c10b41128c630034ed0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCgq7zTiDQlV8X97DZx1qQ.png"/></div></div></figure><p id="dffa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> —梯度增强</strong></p><p id="2c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">梯度</strong> <strong class="ky ir">增强</strong>处理训练<strong class="ky ir">迭代</strong>作为 AdaBoost，但是每个预测器不是调整实例权重，而是通过拟合其<strong class="ky ir">残差</strong> <strong class="ky ir">误差</strong>来校正其前任。</p><p id="e45d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是为什么要拟合残差呢？假设我们正在用<strong class="ky ir"> <em class="ls"> N </em> </strong> <em class="ls"> </em>预测器<em class="ls">训练一个集成模型。</em>对于每个预测器<strong class="ky ir"> P <em class="ls"> n </em> </strong>，<em class="ls"/><strong class="ky ir"><em class="ls">1&lt;N&lt;N</em></strong>，我们假设它是<strong class="ky ir">不完美的</strong>，它的性能可以通过<strong class="ky ir">增加</strong>一个估计器<strong class="ky ir"> <em class="ls"> h </em> </strong>来改善:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/ba5248ed84b37ed35793bc04d6a2a38a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_qGENMZSGB0lQJ8BNbSQQ.png"/></div></div></figure><p id="f975" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了找到<strong class="ky ir"> <em class="ls"> h </em> </strong>，我们先来看看一个完美的<strong class="ky ir"> <em class="ls"> h </em> </strong>应该是怎样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f7aed079b4391eb1a8948b3b71f8e970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQ7IgH5YUIKimC_eN0ZXGA.png"/></div></div></figure><p id="b363" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个完美的估计器<strong class="ky ir"><em class="ls"/></strong>将<strong class="ky ir">填充</strong><strong class="ky ir"/><strong class="ky ir">目标<strong class="ky ir"><em class="ls"/></strong>和当前预测<strong class="ky ir"><em class="ls">【Pn(x)</em></strong>之间的</strong>。本质上，它试图通过插入一个额外的预测器来使<strong class="ky ir">最小化</strong>一个<strong class="ky ir">成本</strong> <strong class="ky ir">功能</strong>。</p><h2 id="2843" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">4.立桩标界</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/741ab41253d7f292783a913974f1716f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L1mCvUdRB8DqOCZmk0B4_A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://i.pinimg.com/originals/ed/d8/87/edd8873400deea9625ee67b37377d077.jpg" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="3c76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完成拼图的最后一块是<strong class="ky ir">立桩</strong>。如果我们<strong class="ky ir">训练</strong> <strong class="ky ir">一个</strong> <strong class="ky ir">模型</strong>来为我们做这件事，而不是使用一个给定的函数来处理预测器的输出，会不会<strong class="ky ir">增加</strong> <strong class="ky ir">的准确性</strong>？答案是<strong class="ky ir">是的</strong>。这些模型通常被称为<strong class="ky ir">混音器</strong>，它们可以找到方法<strong class="ky ir">校正</strong>和<strong class="ky ir">组合</strong>合奏的输出，以实现<strong class="ky ir">增强的</strong> <strong class="ky ir">性能</strong>。</p><p id="d4db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练搅拌机的常见方法是首先将训练数据分成<strong class="ky ir">训练集</strong>和<strong class="ky ir">保持集</strong>。第一种用于训练预测器，采用任何上述方法。然后，坚持组用于训练<strong class="ky ir">搅拌机</strong>。当预测器被训练时，我们将它们的预测放在保留集上，并将其用作<strong class="ky ir">混合器的输入</strong>。如果集合中有<strong class="ky ir"> <em class="ls"> N </em>个预测值</strong>，那么混合器将在一个<strong class="ky ir"><em class="ls">N</em></strong>维数据加上原始目标上被训练。</p><p id="5cfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当完成训练时，混合器被期望获得集合的输出，并且以最大化整体模型的准确性的方式'<strong class="ky ir"> <em class="ls">混合</em> </strong>'它们。</p><h2 id="3ed6" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">最后的话</h2><p id="2a2c" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">你已经坚持到最后了！我希望你能愉快地阅读这篇文章，也希望你能对<strong class="ky ir">合奏</strong> <strong class="ky ir">学习</strong>有所了解。这是一个强大的工具，关于它的写作帮助我巩固了对这个话题的许多理解。它仅仅触及了集成学习的基础理论的表面，并且没有给出实际的<strong class="ky ir">实现</strong>。我强烈推荐浏览<a class="ae kv" href="https://machinelearningmastery.com/super-learner-ensemble-in-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ls">这篇文章</em> </strong> </a>来自<strong class="ky ir"> <em class="ls">机器学习掌握</em> </strong>，它带你浏览如何从<strong class="ky ir">开始构建一个<strong class="ky ir">超级工薪族合奏</strong>(堆叠)</strong>！</p></div></div>    
</body>
</html>