<html>
<head>
<title>Uncertainty in Deep Learning. How To Measure?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的不确定性。如何衡量？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-deep-learning-model-says-sorry-i-dont-know-the-answer-that-s-absolutely-ok-50ffa562cb0b?source=collection_archive---------1-----------------------#2020-04-26">https://towardsdatascience.com/my-deep-learning-model-says-sorry-i-dont-know-the-answer-that-s-absolutely-ok-50ffa562cb0b?source=collection_archive---------1-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5118" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Keras对认知和任意不确定性进行贝叶斯估计的实践教程。走向人工智能的社会接受。</h2></div><blockquote class="ki"><p id="e079" class="kj kk it bd kl km kn ko kp kq kr ks dk translated">我的深度学习模型说:“对不起，我不知道答案”。这完全没问题。</p></blockquote><figure class="ku kv kw kx ky kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi kt"><img src="../Images/ac2a6a1002002b610aa8962950c0b4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMquBEbjim2H7h2EnXawrA.jpeg"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">苏菲·玛德琳的绘画</p></figure><h1 id="5153" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">动机</h1><p id="b70b" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">虽然深度学习技术有效，但大多数时候不清楚<a class="ae mx" rel="noopener" target="_blank" href="/why-deep-learning-works-289f17cab01a"> <strong class="me iu">为什么深度学习有效</strong> </a>。这使得在航空、司法和医学等高风险领域部署人工智能变得棘手。</p><blockquote class="ki"><p id="6f93" class="kj kk it bd kl km my mz na nb nc ks dk translated">神经网络识别细胞活检是癌性的——它不告诉为什么。</p></blockquote><p id="d966" class="pw-post-body-paragraph mc md it me b mf nd ju mh mi ne jx mk ml nf mn mo mp ng mr ms mt nh mv mw ks im bi translated">通常，分类器模型被迫在两个可能的结果之间做出决定，即使它没有任何线索。它刚刚抛了一枚硬币。在现实生活中，医疗诊断模型不仅要关心准确性，还要关心预测的可信度。如果不确定性太高，医生会在决策过程中考虑这一点。</p><blockquote class="ki"><p id="ba5e" class="kj kk it bd kl km my mz na nb nc ks dk translated">深度学习模型应该能够说:“对不起，我不知道”。</p></blockquote><p id="6df7" class="pw-post-body-paragraph mc md it me b mf nd ju mh mi ne jx mk ml nf mn mo mp ng mr ms mt nh mv mw ks im bi translated">从不够多样化的训练集中学习的自动驾驶汽车模型是另一个有趣的例子。如果汽车不确定路上哪里有行人，我们会希望它让司机来控制。</p><blockquote class="ki"><p id="4850" class="kj kk it bd kl km my mz na nb nc ks dk translated">泛化能力更强的网络更难解释。可解释的网络不能很好地概括。(<a class="ae mx" href="https://medium.com/intuitionmachine/deep-learnings-uncertainty-principle-13f3ffdd15ce" rel="noopener">来源</a>)</p></blockquote><p id="80a4" class="pw-post-body-paragraph mc md it me b mf nd ju mh mi ne jx mk ml nf mn mo mp ng mr ms mt nh mv mw ks im bi translated">有些模型可能不需要解释，因为它们用于低风险应用中，如产品推荐系统。然而，将关键模型整合到我们的日常生活中需要可解释性，以增加人工智能的社会接受度。这是因为人们喜欢把信念、欲望、意图归于事物(<a class="ae mx" href="https://christophm.github.io/interpretable-ml-book/interpretability-importance.html" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p><p id="3c44" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">理解和解释神经网络不知道的东西对最终用户来说至关重要。从业者也寻求更好的可解释性来构建更健壮的模型，这些模型可以抵抗<a class="ae mx" rel="noopener" target="_blank" href="/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa">对抗性攻击</a>。</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nn"><img src="../Images/bd2c000c476881e51c60cd7f3319797e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Gw_Wx88aPe9fapWX.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图片<em class="ns">由Goodfellow等人于2015年在ICLR拍摄。</em> <a class="ae mx" href="https://arxiv.org/abs/1412.6572" rel="noopener ugc nofollow" target="_blank"> <em class="ns">解释和治理反面事例</em> </a> <em class="ns">。</em>给熊猫的照片加一点噪点会导致不正确的分类为长臂猿。</p></figure><p id="40c5" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">在接下来的几节中，我们将更深入地了解不确定性的概念。我们还介绍了如何评估深度学习模型中的不确定性的简单技术。</p><h1 id="c283" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">不确定性的类型</h1><p id="930c" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">深度学习中有两种主要的不确定性:认知不确定性和任意不确定性。这两个术语都不容易脱口而出。</p><p id="5510" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated"><strong class="me iu">认知不确定性</strong>描述了模型不知道什么，因为训练数据不合适。认知的不确定性是由于有限的数据和知识。给定足够的训练样本，认知不确定性将会降低。认知的不确定性可能出现在训练样本较少的领域。</p><p id="179a" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated"><strong class="me iu">随机不确定度</strong>是观测值的自然随机性产生的不确定度。即使提供了更多的数据，也不能减少任意的不确定性。当谈到测量误差时，我们称之为同方差不确定度，因为它对所有样本都是恒定的。依赖于输入数据的不确定性被称为异方差不确定性。</p><p id="42bd" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">下图代表了一个真实的线性过程(<em class="nt"> y=x </em>)，它是在<em class="nt"> x=-2.5 </em>和<em class="nt"> x=2.5 </em>附近采样的。</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/63dbc57bf3ca0940ff150fc358ac6173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vj9r-scd3fEKHRXnqqurg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在线性回归环境中展示不同种类的不确定性(图片由Michel Kana提供)。</p></figure><p id="492f" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">一个传感器故障在左侧云层中引入了噪音。底层过程的噪声测量导致左边云中的<em class="nt">高随机不确定性</em>。这种不确定性不能通过额外的测量来减少，因为传感器在设计<em class="nt">时一直产生大约<em class="nt"> x=-2.5 </em>的误差。</em></p><p id="d2a8" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated"><em class="nt">高认知不确定性</em>出现在很少或没有训练观察的区域。这是因为可以提出太多似是而非的模型参数来解释潜在的地面真实现象。这就是我们云的左、中、右三部分的情况。这里我们不确定哪个模型参数最能描述数据。给定空间中更多的数据，不确定性将会降低。在高风险应用中，识别这样的空间是很重要的。</p><h1 id="6bdd" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">如何使用Dropout获取不确定性</h1><p id="bcd3" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated"><a class="ae mx" rel="noopener" target="_blank" href="/bayesian-nightmare-how-to-start-loving-bayes-1622741fa960">贝叶斯统计</a>允许我们根据数据和我们对潜在现象的先验知识得出结论。一个关键区别是参数是分布而不是固定权重。</p><blockquote class="ki"><p id="456a" class="kj kk it bd kl km my mz na nb nc ks dk translated">如果我们不学习模型的参数，而是学习它们的分布，我们将能够估计权重的不确定性。</p></blockquote><p id="9c6c" class="pw-post-body-paragraph mc md it me b mf nd ju mh mi ne jx mk ml nf mn mo mp ng mr ms mt nh mv mw ks im bi translated">我们怎样才能知道重量的分布呢？<strong class="me iu">深度集成</strong>是一项强大的技术，在该技术中，大量模型或模型的多个副本在各自的数据集上进行训练，它们的结果预测共同构建预测分布。</p><p id="80df" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">因为集成可能需要大量的计算资源，所以提出了一种替代方法:<strong class="me iu">丢弃作为模型集成的贝叶斯近似</strong>。这项技术是由亚林·加尔和邹斌·格拉马尼在他们的<a class="ae mx" href="https://arxiv.org/abs/1506.02142" rel="noopener ugc nofollow" target="_blank"> 2017年论文</a>中介绍的。</p><p id="5fc1" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated"><a class="ae mx" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank"> Dropout </a>是深度学习中一种常用的正则化方法，以避免过度拟合。它由随机抽样的网络节点组成，并在训练过程中将其删除。丢弃根据伯努利分布随机地将神经元清零。</p><p id="7339" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">一般来说，在贝叶斯模型中，正则化和先验分布之间似乎有很强的联系。辍学并不是唯一的例子。经常使用的L2正则化本质上是高斯先验。</p><p id="2cac" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">在他们的论文中，亚林和邹斌表明，在每个权重层之前应用丢失的神经网络在数学上等价于高斯过程的贝叶斯近似。</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/35a3f841853e2dcd56cbe4dd8bbb218e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/0*ROLre4zgQ4yk6Dmd.gif"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图片由谭在<a class="ae mx" href="https://www.yuritan.nl/posts/prediction_uncertainty/" rel="noopener ugc nofollow" target="_blank"> yuritan.nl </a>上提供——在不同的正向传递中，漏失改变了模型架构，从而允许贝叶斯近似。(授权引用的图片来自谭)</p></figure><p id="d6bf" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">使用droupout，每个未被删除的节点子集定义一个新网络。训练过程可以认为是同时训练<em class="nt"> 2^m </em>不同的模型，其中<em class="nt"> m </em>是网络中的节点数。对于每一批，这些模型的随机抽样集被训练。</p><p id="1b35" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">关键的想法是在培训和测试时都要退出。在测试时，论文建议重复预测几百次，随机剔除。所有预测的平均值就是估计值。对于不确定区间，我们简单计算预测的方差。这给了系综的不确定性。</p><h1 id="99c0" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">预测认知不确定性</h1><p id="fd0d" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">我们将使用通过向函数<em class="nt"> y=x </em>添加正态分布噪声生成的数据来评估回归问题的认知不确定性，如下所示:</p><ul class=""><li id="1a61" class="nw nx it me b mf ni mi nj ml ny mp nz mt oa ks ob oc od oe bi translated">在x=-2和x=-3之间的左侧云中产生100个数据点</li><li id="6aa5" class="nw nx it me b mf of mi og ml oh mp oi mt oj ks ob oc od oe bi translated">在x=2和x=3之间的右侧云中产生100个数据点。</li><li id="bbf2" class="nw nx it me b mf of mi og ml oh mp oi mt oj ks ob oc od oe bi translated">噪声以比右云高10倍的方差被添加到左云。</li></ul><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f96ad97bbad360a429c41bc938701924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*g5PQsF-GAITVSGbOghn7iw.png"/></div></figure><p id="af83" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">下面我们设计两个简单的神经网络，一个没有漏失层，另一个在隐藏层之间有漏失层。在每个训练和推理批次期间，丢弃层随机禁用5%的神经元。我们还包括L2正则化应用优化期间层参数的惩罚。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f6009b60d7deb4d9ade4d9d7b8386ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*MQdJrtFamb4VspffbelNZg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">没有脱落层的网络</p></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ca2d52967788b2ea43ec4e4c96050d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*bB88vW9p5j8Mqw_X9_Mwdw.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">带有脱落层的布局</p></figure><p id="fe51" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">rmsprop优化器用于通过最小化均方误差来训练10个点的批次。训练成绩如下所示。两种模型的收敛速度都非常快。有退出的模型表现出稍高的损失和更多的随机行为。这是因为网络的随机区域在训练期间被禁用，导致优化器跳过损失函数的局部最小值。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oo"><img src="../Images/8e036281e589cc60141aef523e54f1f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQdfu7naP5FajJWMxWmO5A.png"/></div></div></figure><p id="eaaa" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">下面，我们展示了模型在测试数据上的表现。没有辍学的模型预测了一条完美的R2分数的直线。包括辍学导致了一条非线性预测线，R2得分为0.79。虽然漏失过度拟合较少，具有较高的偏差和降低的准确性，但它突出了没有训练样本的区域中预测的不确定性。预测线在这些区域具有较高的方差，这可用于计算认知不确定性。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi op"><img src="../Images/59b05081d42f28f403f8d597676fa8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*qcDJ68n3qtQV7-2aiE78CQ.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在没有训练样本的区域中，具有漏失的模型表现出具有高方差的预测。这个性质被用来近似认知的不确定性。</p></figure><p id="f2fa" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">下面，我们在测试数据集上评估两个模型(有和没有脱落)，同时在评估中使用脱落层数百次。这相当于模拟一个高斯过程。我们每次都从测试数据中获得每个输入标量的一系列输出值。这允许我们计算后验分布的标准偏差，并将其显示为认知不确定性的<strong class="me iu">度量</strong>。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f6afe3e89151cbd7def7a8594199352c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*roxLCv9sOS0H1RztS1hPNw.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">即使在没有训练样本的区域中，没有丢失的模型也没有100%确定性地预测固定值。</p></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3e9b1d16b955dfd95a0c1c8e8977aa51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*6duodsQ-Z-xbDglTWtYU8A.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在没有训练样本的地区，带有退出的模型估计了高认知不确定性。</p></figure><p id="37cb" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">正如预期的那样，<em class="nt"> x &lt; -3 </em>和<em class="nt"> x &gt; 3 </em>的数据具有很高的认知不确定性，因为在这些点上没有可用的训练数据。</p><blockquote class="ki"><p id="477d" class="kj kk it bd kl km my mz na nb nc ks dk translated">Dropout允许模型说:“我对<em class="ns"> x &lt; -3 </em>和<em class="ns"> x &gt; 3 </em>的所有预测只是我的最佳猜测。”</p></blockquote><figure class="ku kv kw kx ky kz gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f80983d602fccaa116bcf9fb3f4e970e.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*ZSp03RRR7tryz4YkRj9lpg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图像由OpenClipart-Vectors在<a class="ae mx" href="https://pixabay.com/vectors/lion-animal-confused-bewildered-159448/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上生成</p></figure><h2 id="e4f6" class="ot ll it bd lm ou ov dn lq ow ox dp lu ml oy oz lw mp pa pb ly mt pc pd ma pe bi translated">多项式回归</h2><p id="45ff" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">在这一节中，我们研究如何通过更复杂的任务(如多项式回归)的退出来评估认知不确定性。</p><p id="8d46" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">为此，我们生成一个从正弦函数中随机采样的合成训练数据集，并添加不同幅度的噪声。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="4932" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">下面的结果表明，包括辍学带来了一种方法，在没有数据的地区，甚至对于非线性数据，访问认知的不确定性。虽然遗漏会影响模型性能，但它清楚地表明，在没有足够训练样本的数据区域中，预测不太确定。</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2bc77c55f6f8da063be6bd557df7300f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*zxbxALLIC9OzM-jyQqqmJg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在没有训练数据的区域中进行预测时，没有辍学的模型会过度拟合训练样本并显示出过度自信。</p></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d1fc38988434958b33c88f7819df70af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*aQ0nO75PRGn07VVDmjypXw.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">辍学模型有较高的偏差，但在没有训练数据的地区不太可信。在训练样本缺失的情况下，认知的不确定性更高。</p></figure><h1 id="3212" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">预测任意不确定性</h1><p id="b342" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">认知的不确定性是模型的一个属性，而随机的不确定性是数据的一个属性。任意的不确定性抓住了我们对数据无法解释的信息的不确定性。</p><p id="16e3" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">当随机不确定性为常数，不依赖于输入数据时，称为<strong class="me iu">同方差不确定性</strong>，否则，使用术语<strong class="me iu">异方差不确定性</strong>。</p><p id="3e34" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">异方差不确定性取决于输入数据，因此可以作为模型输出进行预测。同方差不确定性可以作为任务相关的模型参数来估计。</p><p id="7be2" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">学习异方差不确定性是通过用以下等式代替均方误差损失函数来完成的(<a class="ae mx" href="https://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/" rel="noopener ugc nofollow" target="_blank">来源</a>):</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/d6c549cbf481216b4131c88cf0cee55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*YNUdAP_upGUoc4SIDPA22w.png"/></div></figure><p id="bf45" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">该模型预测了均值<em class="nt"> y </em> ^和方差<em class="nt"> σ </em>如果残差非常大，模型将倾向于预测较大的方差。对数项防止方差无限增大。下面提供了这个任意损失函数在Python中的实现。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="bd7b" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">任意损失可用于训练神经网络。下面，我们举例说明一个类似于上一节中用于认知不确定性的架构，有两个不同之处:</p><ul class=""><li id="8562" class="nw nx it me b mf ni mi nj ml ny mp nz mt oa ks ob oc od oe bi translated">隐藏层之间没有脱落层，</li><li id="b6f2" class="nw nx it me b mf of mi og ml oh mp oi mt oj ks ob oc od oe bi translated">输出是2D张量而不是1D张量。这允许网络不仅学习响应<em class="nt"> y^ </em>，还学习方差<em class="nt"> σ </em>。</li></ul><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/b5b78856c542a7ddcfaae4bd234fd903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*L-lfxMt6S75la0VmCtcAOw.png"/></div></figure><p id="ea66" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">学习到的损耗衰减迫使网络在训练过程中寻找最小化损耗的权重和方差，如下所示。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="e0c9" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated"><strong class="me iu">对任意不确定性的推断是在没有遗漏的情况下完成的。</strong>下面的结果证实了我们的预期:左侧数据的随机不确定性高于右侧数据。由于<em class="nt"> x=-2.5 </em>附近的传感器误差，左侧区域有噪声数据。添加更多的样本并不能解决问题。噪声仍将存在于该区域中。通过在损失函数中包括任意的不确定性，该模型将对落在训练样本有噪声的区域中的测试数据进行不太可信的预测。</p><figure class="no np nq nr gt kz"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="no np nq nr gt kz gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/2dc658b0f172d5ad00210819d80e04db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*szn7Twxj6BUXTqc1XVQnjw.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">具有丢失的模型检测具有噪声训练数据的区域。这有助于在这些区域预测更高的随机不确定性。</p></figure><p id="ba96" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">测量任意不确定性在计算机视觉中变得至关重要。图像中的这种不确定性可以归因于当相机不能透过物体看到时的遮挡。图像的过度曝光区域或某些视觉特征的缺乏也可能导致任意的不确定性。</p><p id="75c8" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">认知的不确定性和任意的不确定性可以相加得到总的不确定性。在自动驾驶汽车的预测中包括总的不确定性水平可能非常有用。</p><figure class="no np nq nr gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nn"><img src="../Images/37711e3b11b6a903eea6bb8c9556ceb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HZ0GKLWuyfJ1ZLE8.jpg"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">剑桥大学Alex Kendall的图像，关于计算机视觉中语义分割的任意和认知不确定性。任意不确定性(d)捕捉由于遮挡或距离而导致标签有噪声的对象边界。认知不确定性(e)突出显示模型不熟悉图像特征的区域，例如中断的人行道。</p></figure><h1 id="41d3" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">结论</h1><p id="761c" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw ks im bi translated">在这篇文章中，我们展示了<strong class="me iu">如何在推理时间</strong>使用Dropout等同于使用贝叶斯近似来评估深度学习预测中的不确定性。</p><p id="07b5" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">在商业环境中，了解一个模型对其预测有多有信心是很重要的。优步一直使用这种技术来评估时间序列预测中的<a class="ae mx" href="https://eng.uber.com/neural-networks-uncertainty-estimation/" rel="noopener ugc nofollow" target="_blank">不确定性</a>。</p><p id="8ecc" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">在机器学习中适当地包含不确定性也可以帮助<a class="ae mx" href="https://becominghuman.ai/using-uncertainty-to-interpret-your-model-67a97c28fea5" rel="noopener ugc nofollow" target="_blank">调试模型</a>，使它们在对抗敌对攻击时更加健壮。新的<a class="ae mx" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank"> TensorFlow Probability </a>提供了概率建模，作为深度学习模型的<a class="ae mx" rel="noopener" target="_blank" href="/how-to-deal-with-uncertainty-in-the-era-of-deep-learning-977decdf84b5">插件。</a></p><p id="0350" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">你可以进一步阅读我关于<a class="ae mx" rel="noopener" target="_blank" href="/wild-wide-ai-responsible-data-science-16b860e1efe9">负责任的数据科学</a>的文章，看看当我们过于信任我们的机器学习模型时会出现什么问题。这份<a class="ae mx" rel="noopener" target="_blank" href="/why-deep-learning-works-289f17cab01a">深度学习综合介绍</a>和<a class="ae mx" rel="noopener" target="_blank" href="/introduction-to-bayesian-logistic-regression-7e39a0bae691">贝叶斯推理实用指南</a>可以帮助深化和挑战深度学习的经典方法。</p><p id="6ea4" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">感谢《走向数据科学》的Anne Bonner的编辑笔记。</p><p id="0bb9" class="pw-post-body-paragraph mc md it me b mf ni ju mh mi nj jx mk ml nk mn mo mp nl mr ms mt nm mv mw ks im bi translated">在不确定时期保持安全。</p></div></div>    
</body>
</html>