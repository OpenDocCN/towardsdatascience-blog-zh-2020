<html>
<head>
<title>Contrastive loss for supervised classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督分类的对比损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/contrastive-loss-for-supervised-classification-224ae35692e7?source=collection_archive---------10-----------------------#2020-04-29">https://towardsdatascience.com/contrastive-loss-for-supervised-classification-224ae35692e7?source=collection_archive---------10-----------------------#2020-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="649a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对比交叉熵损失和对比损失</h2></div><p id="aa66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，来自谷歌研究院和麻省理工学院的研究人员(Khosla等人)发表了一篇名为“有监督的对比学习”的论文<a class="ae le" href="https://arxiv.org/abs/2004.11362" rel="noopener ugc nofollow" target="_blank">。在对比学习的基础上，提出了一种新的损失函数“对比损失”来训练有监督的深度网络。他们证明，在</a><a class="ae le" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集上，对比损失在一系列神经架构和数据增强机制的分类中的表现明显优于传统的交叉熵损失。</p><p id="52fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我发现这篇论文特别有趣，因为它的思想非常简洁，可能是监督分类任务甚至回归的一种简单而通用的改进。在进入细节之前，让我们首先回顾一下用于监督分类的损失函数(又名目标函数)的基础知识。</p><h2 id="d7e6" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">交叉熵损失</h2><p id="bf5a" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">交叉熵，也称为对数损失、逻辑损失，可以说是分类中最常用的损失函数。顾名思义，它来自信息论，信息论测量两个概率分布之间的互熵，<em class="md"> p </em>和<em class="md"> q </em>。它也与<a class="ae le" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">kull back-lei bler散度</a>密切相关，可以写成熵<em class="md"> H(p) </em>与从<em class="md"> p </em>到<em class="md"> q </em>的KL散度之和:</p><p id="6889" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="md"> H(p，q) = H(p) + D_{KL}(p||q) </em></p><p id="63f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当用作分类的损失函数时，可以使用交叉熵来度量基础真实类分布和预测类分布之间的差异:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi me"><img src="../Images/b180f3dd9b94d000ee1e3d07e5832010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5pUMMKxjo7g3zocrSiMIA.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">交叉熵损失</p></figure><p id="0715" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="md"> M </em>是类的数量<em class="md"> c </em>和<em class="md"> y_c </em>如果类标签是<em class="md"> c </em>和<em class="md">p(y = c |</em><strong class="kk iu"><em class="md">x</em></strong><em class="md">)</em>是在给定输入特征向量<strong class="kk iu"> <em class="md"> x的情况下分类器认为标签应该是<em class="md"> c </em>的概率</em></strong></p><h2 id="2469" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">对比损失</h2><p id="f34e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">对比损失广泛应用于无监督和自监督学习中。这个损失函数最初是由Yann LeCun的小组的<a class="ae le" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank"> Hadsell等人在2016年</a>开发的，它作用于成对的样本，而不是单个样本。它为每对样本定义了一个二进制指示符<em class="md"> Y </em>，表示它们是否应该被视为不相似(如果<strong class="kk iu"> <em class="md"> x_1 </em> </strong>、<strong class="kk iu"> <em class="md"> x_2 </em> </strong>被视为相似，则Y = 0)；否则Y=1)，以及一个可学习的距离函数<em class="md">D _ W(</em><strong class="kk iu"><em class="md">x _ 1</em></strong><em class="md">，</em><strong class="kk iu"><em class="md">x _ 2</em></strong><em class="md">)</em>在一对样本<strong class="kk iu"> <em class="md"> x_1 </em> </strong>，<strong class="kk iu"> <em class="md"> x_2 </em> </strong>之间，由权重<em class="md"> W </em>参数化对比损失定义为:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mu"><img src="../Images/30df5b83ef8fad432d57bb2120719ebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nu9fZanFQKr_CODZrCgxxw.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">对比度损失函数</p></figure><p id="40db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">，其中<em class="md"> m &gt; 0 </em>为余量。裕度定义了样本嵌入空间周围的半径，使得不同的样本对仅在距离<em class="md"> D_W </em>在裕度内时对对比损失函数有贡献。</p><p id="82b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直观地，该损失函数鼓励神经网络学习嵌入，以将具有相同标签的样本彼此靠近放置，同时在嵌入空间中使具有不同标签的样本远离。</p><h2 id="09c9" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">自我监督学习和监督学习的对比损失</h2><p id="21e0" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">在标签不可用并且目标是学习数据的有用嵌入的自我监督设置中，对比损失与数据扩充技术结合使用，以创建共享相同标签的扩充样本对。就像科斯拉论文中的插图一样，比熊犬图片的不同放大应该彼此靠近，远离任何其他图片，无论是其他狗还是猫。然而，正如作者指出的，这种通过自我监督对比损失学习的嵌入不一定对监督学习者有用，因为引入了假阴性:一只比熊犬的嵌入也应该与其他狗的嵌入在同一邻域，无论是柯基犬还是西班牙猎犬。</p><p id="0421" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使对比损失适应监督学习，科斯拉和他的同事开发了一个两阶段程序，将标签和对比损失的使用结合起来:</p><ul class=""><li id="1941" class="mv mw it kk b kl km ko kp kr mx kv my kz mz ld na nb nc nd bi translated">阶段1:使用对比损失来训练编码器网络，以嵌入由它们的标签引导的样本。</li><li id="d000" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated">阶段2:冻结编码器网络，并在已学习的嵌入之上学习分类器，以使用交叉熵损失来预测标签。</li></ul><p id="794d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初研究中的实验是在ImageNet上进行的。作者还评估了不同的数据增强选项和各种类型的最新架构，如VGG-19和ResNets。</p><h1 id="1b15" class="nj lg it bd lh nk nl nm lk nn no np ln jz nq ka lq kc nr kd lt kf ns kg lw nt bi translated"><strong class="ak">MNIST和时尚MNIST数据集上的实验</strong></h1><p id="769e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我不是计算机视觉专家，我更感兴趣的是，对于通用分类问题，所提出的监督对比损失是否优于交叉熵。为了在我的笔记本电脑上运行比较典型损失函数和对比损失函数的有效性的实验，我选择了小得多的MNIST和时尚MNIST数据集。我还忽略了像素之间的空间关系，避免使用任何卷积层来模拟表格数据集。</p><p id="2b35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在接下来的实验中使用的架构非常简单，两个密集的隐藏层，每个都有256个神经元，通过泄漏的ReLU激活连接。最后一个隐藏层的输出被归一化，以将输出向量定位在单位超球面中。这种架构对于用对比损失和MLP基线训练的编码器是相同的。最后的分类层由对应于10个类别的10个单元组成。</p><p id="cac9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遵循Khosla论文提出的相同过程:编码器在阶段1中使用对比损失进行预训练，然后在阶段2中被冻结以仅训练分类器。如下所示，在两个数据集上，具有监督对比损失的模型确实优于具有交叉熵损失的MLP基线，具有完全相同的架构。这种改善在时尚的MNIST上更高，一个比MNIST更困难的任务。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nu"><img src="../Images/0870338fb18879e04c939e31c9a0ad9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ho5mvr5Vy0WwpH4A39YUgQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd nv">在MNIST和时尚MNIST数据集的保留测试集上的性能(准确性)。</strong></p></figure><p id="76db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个有趣的观察是，对于有监督对比损失的模型，学习曲线更平滑，这可能是由于预先训练的表示。</p><p id="1590" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了证实性能的提高是由于更好的嵌入空间，我使用交叉熵检查了来自MLP基线和使用对比损失学习的嵌入的PCA投影。作为阴性对照，我还检查了没有任何嵌入的原始数据空间的PCA投影。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nw"><img src="../Images/286f433fdacf24e6375ad09ddbabf5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-EcJCzVFc2r82YD2TDaNg.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd nv">模型学习到的嵌入(或无嵌入)的PCA投影。</strong>从左到右:原始数据空间；基线MLP的最后一个隐藏层；使用对比损失学习的投影</p></figure><p id="3084" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如散点图所示，MLP模型和对比模型都比原始数据更好地聚类具有相同标签的样本，但是对比嵌入中的聚类更紧密。为了更加突出这一点，我绘制了PCA投影的密度，从对比嵌入中可以清楚地看到10个不同的聚类:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nx"><img src="../Images/efff962d9dcf052a2e1f5fe15b27aac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMn43vH2oyfyjzrBsDFw7A.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd nv">显示模型学习的PCA投影密度的联合图。</strong>从左到右:原始数据空间；基线MLP的最后一个隐藏层；使用对比损失学习的投影</p></figure><p id="0739" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的图是在MNIST数据上生成的，类似的观察结果也可以在时尚MNIST数据集上持续存在，尽管一些类混合在一起。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi ny"><img src="../Images/5cf0dc7cbb94cfc079c6420986572f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKKVi5l7Iehy1UxYKxoQ6g.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd nv">不同模特在时尚MNIST数据集上学习的表征的主成分分析。</strong></p></figure><p id="ac6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，我的实验清楚地再现了原始研究的发现，即监督对比损失在分类任务中优于交叉熵。事实上，它还可以在更小的数据集上工作，无需数据扩充，也无需使用特定领域的架构，如卷积，这非常令人鼓舞。这表明监督对比损失可以作为任何监督任务的通用技术。我期待将其应用于未来的ML任务，并探索其在回归中的适用性。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><p id="dcbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些实验使用的代码可以在这里找到:【https://github.com/wangz10/contrastive_loss T4】</p><h1 id="5fce" class="nj lg it bd lh nk nl nm lk nn no np ln jz nq ka lq kc nr kd lt kf ns kg lw nt bi translated">参考</h1><ul class=""><li id="077c" class="mv mw it kk b kl ly ko lz kr og kv oh kz oi ld na nb nc nd bi translated"><a class="ae le" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank"> Hadsell等人(2006): <strong class="kk iu">通过学习不变映射进行维数约简。</strong>T9】</a></li><li id="24b5" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated"><a class="ae le" href="https://arxiv.org/abs/2004.11362" rel="noopener ugc nofollow" target="_blank">科斯拉等人(2020): <strong class="kk iu">监督对比学习</strong>。</a></li><li id="58ee" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated"><a class="ae le" href="https://omoindrot.github.io/triplet-loss" rel="noopener ugc nofollow" target="_blank">tensor flow中的三元组丢失和在线三元组挖掘</a></li><li id="b710" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated"><a class="ae le" href="https://sites.icmc.usp.br/moacir/p17sibgrapi-tutorial/2017-SIBGRAPI_Tutorial-DLCV-Part2-Regression-Deep-Learning-Siamese_and_Triplet_Nets.pdf" rel="noopener ugc nofollow" target="_blank">深度学习中的回归:连体和三联体网络</a></li></ul></div></div>    
</body>
</html>