<html>
<head>
<title>Day 106 of #NLP365: NLP Papers Summary — An Unsupervised Neural Attention Model for Aspect Extraction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第106天:NLP论文摘要——用于特征提取的无监督神经注意模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0?source=collection_archive---------55-----------------------#2020-04-15">https://towardsdatascience.com/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0?source=collection_archive---------55-----------------------#2020-04-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/61a393f717c1685f581a48076c22cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDhaS8ed285Bb9L80C5U0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jg" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="770a" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="9741" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="09b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的100天里学到了什么。</p><p id="2100" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天的NLP论文是<strong class="lj jt"> <em class="md">一个用于方面提取的无监督神经注意模型</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="e089" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">目标和贡献</h1><p id="9aa1" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">目标是改进一致方面的发现，因为现有的工作没有产生高度一致的方面。本文提出了一种新的神经模型，基于注意力的方面提取(ABAE ),它通过使用单词嵌入来利用单词共现的分布来提高连贯性。该模型还使用一种注意机制来在训练过程中削弱不相关的单词，这进一步提高了方面的一致性。</p><h2 id="ed41" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">传统LDA的弱点</h2><ul class=""><li id="2b51" class="nt nu jj lj b lk nd ln ne lq nv lu nw ly nx mc ny nz oa ob bi translated">不要直接对单词共现统计进行编码，这很重要，因为它们可以保持主题的连贯性</li><li id="046e" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ny nz oa ob bi translated">LDA模型需要估计每个文档的主题分布。然而，评论文档往往很短，这使得估计每个文档的主题分布更加困难</li></ul><h1 id="5241" class="ml mm jj bd mn mo oh mq mr ms oi mu mv ky oj kz mx lb ok lc mz le ol lf nb nc bi translated">数据集</h1><p id="d655" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">两个真实世界的数据集:Citysearch(餐馆)和BeerAdvocate(啤酒)。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3a14af11900bc122a2906d47c6ffa2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*thjQIrLGxWOiEUHE.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">两个评估数据集的汇总统计数据[1]</p></figure><h2 id="fdf9" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">城市搜索语料库</h2><p id="4bb8" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这是一个餐馆评论文集。有3400个手动标记的方面。这些注释用于方面提取的评估。有6个定义的方面标签:食物、员工、氛围、价格、趣闻和杂项。</p><h2 id="9556" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">BeerAdvocate</h2><p id="3677" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">大约有1000篇评论(9245句话)被标注了5个方面的标签:感觉、外观、气味、味道、整体。</p><h1 id="264d" class="ml mm jj bd mn mo oh mq mr ms oi mu mv ky oj kz mx lb ok lc mz le ol lf nb nc bi translated">基于注意力的特征抽取(ABAE)</h1><p id="c1f7" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">提出的模型，基于注意力的方面提取(ABAE)，使用单词嵌入显式编码单词出现统计，使用降维来提取最重要的方面，并使用注意力机制来移除不相关的单词以进一步提高方面的一致性。</p><p id="9407" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最终目标是学习方面嵌入并将其映射到嵌入空间。以下是ABAE建筑图以及主要步骤的突出显示:</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/b3fa6befa30ff7131974c5635c44a7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F2XPUKHW7nv0iIpb.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ABAE模型架构[1]</p></figure><ol class=""><li id="1224" class="nt nu jj lj b lk ll ln lo lq os lu ot ly ou mc ov nz oa ob bi translated">将我们词汇表中的每个单词映射到它们各自的单词嵌入中。体嵌入是用来逼近我们词汇中的体词的</li><li id="9d7b" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ov nz oa ob bi translated">利用注意机制过滤掉非体貌词并构造一个句子嵌入，𝑧𝑠.注意力机制告诉模型应该对单词I关注多少，以便捕捉句子的主要方面</li><li id="e06e" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ov nz oa ob bi translated">将句子嵌入重构为来自T(方面嵌入矩阵)的方面嵌入的线性组合。𝑝𝑡是k个方面嵌入的权重向量，它告诉模型输入句子与相关方面的相关程度。通过将𝑧𝑠的维数从d维降低到k(方面的数量)维和softmax非线性来获得𝑝𝑡。这个降维和重构的过程保留了嵌入体中的体词的大部分信息</li></ol><p id="c1ca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">培训目标是最大限度地减少重建损失。换句话说，该模型旨在尽量缩小𝑟𝑠和𝑧𝑠.之间的差异</p><h1 id="5108" class="ml mm jj bd mn mo oh mq mr ms oi mu mv ky oj kz mx lb ok lc mz le ol lf nb nc bi translated">实验</h1><h2 id="e159" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">基线模型</h2><ul class=""><li id="46cf" class="nt nu jj lj b lk nd ln ne lq nv lu nw ly nx mc ny nz oa ob bi translated"><em class="md"> LocLDA </em>:标准LDA。每个句子都被视为一个单独的文档</li><li id="fde6" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ny nz oa ob bi translated"><em class="md"> K-means </em>:使用单词嵌入的K-means质心计算初始方面矩阵</li><li id="1896" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ny nz oa ob bi translated"><em class="md"> SAS </em>:提取方面和特定方面意见的混合模型</li><li id="2adb" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ny nz oa ob bi translated">BTM:为短文本设计的双主题模型。它对无序词对的共现进行建模，以解决数据稀疏问题</li></ul><h2 id="a4b3" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">估价</h2><p id="87aa" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">基于现有的工作，作者将餐馆和啤酒语料库的方面数量设置为14个。他们根据两个标准评价ABAE:</p><ol class=""><li id="ba06" class="nt nu jj lj b lk ll ln lo lq os lu ot ly ou mc ov nz oa ob bi translated">架构能识别有意义的和语义一致的方面吗？</li><li id="6ae9" class="nt nu jj lj b lk oc ln od lq oe lu of ly og mc ov nz oa ob bi translated">该架构能提高方面检测的性能吗？</li></ol><h1 id="7e00" class="ml mm jj bd mn mo oh mq mr ms oi mu mv ky oj kz mx lb ok lc mz le ol lf nb nc bi translated">结果</h1><h2 id="39aa" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">外观质量</h2><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/1245f46da10675dc602b63877d171251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jglbxygbx-ulTrrv.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由模型分类并映射到相应黄金方面的推断方面列表[1]</p></figure><p id="a3c6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">推断方面比黄金方面更精细。例如，它可以区分主菜和甜点。为了评估方面的质量，我们使用一致性分数。更高的一致性分数表明更好的方面可解释性，因此更有意义和语义一致性。下面是餐馆和啤酒语料库中每个模型的平均一致性分数。两个发现:1) ABAE优于先前的模型，以及2) k-means在单词嵌入上足以比所有主题模型执行得更好，向我们显示单词嵌入是比LDA更强的用于捕获共现的模型。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e8e982d13abc592a2e396a78b44de65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/0*g_2dk3WSLi6J99uf.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">主题中顶级术语的数量与这些术语的连贯性之间的关系[1]</p></figure><p id="8b2b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">作者还进行了人体评估。首先，人类法官必须评估他们有多少连贯的方面。如果一个方面的前50个术语中的大部分一致地表示该方面，则该方面是一致的。结果如下，ABAE发现了最多的相干态。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/271c8afb8ab7dcefe816b882535dcd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*XDepUCJaHXODu89y.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ABAE模型发现的相干态数目[1]</p></figure><p id="687e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第二，人类裁判必须评估某个方面的顶项是否正确。只有当大多数裁判认为顶部术语反映了相关方面时，它才被认为是正确的。如下图所示。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/069f111abe7fbb1625419a7d3d7c7cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-EaSWRzMfli_Y3iw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ABAE模型对不同主题的术语进行分类的精确度[1]</p></figure><h2 id="dccb" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">方面检测</h2><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/6bd0b4d20fe078b4300871690ba2ac3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*-9RbvyFg1JN1RMl5.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">方面召回餐馆数据集上的结果[1]</p></figure><p id="6d89" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">给定一个评论句子，ABAE首先分配一个推断的方面标签，然后映射到适当的黄金标准标签。上表显示了餐馆语料库的结果。表中的SERBM模型报告了在餐馆语料库上的方面检测的SOTA结果，并且ABAE能够在员工和氛围方面胜过它。</p><p id="1693" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">啤酒语料库的结果如下所示。请注意，作者将味觉和嗅觉结合在一起，因为它们都高度相关。ABAE在各方面都超过了所有车型，除了口味方面。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/c5b1d1e6b6fb2ea4a3e634a16f6eb0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/0*Ghhau_RtXOpeOblF.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">方面召回啤酒数据集上的结果[1]</p></figure><p id="46de" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一个发现是，注意力机制被证明是驱动ABAE表现的关键因素。下表显示了ABAE和ABAE之间的性能比较，其中ABAE是没有注意机制的ABAE模型。ABAE在所有方面，所有指标上都超过了ABAE。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/22c29c5fd03fb08af8a38d963ff46fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*e2KLqvTR7hQr2yom.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ABAE和ABAE——(无注意机制)模型的消融研究[1]</p></figure><h1 id="297d" class="ml mm jj bd mn mo oh mq mr ms oi mu mv ky oj kz mx lb ok lc mz le ol lf nb nc bi translated">结论和未来工作</h1><p id="3044" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">与LDA模型相比，ABAE显式地捕捉单词共现，并克服了数据稀疏的问题。实验结果表明，与以前的模型相比，ABAE学习了更高质量的方面，并且在捕捉评论方面更有效。基于这篇论文，这是第一个用于方面提取的无监督神经技术。ABAE是一个简单而有效的神经注意模型，它的规模很大。</p><h2 id="756d" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">来源:</h2><p id="b094" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">[1]何，r .，李，W.S .，吴，H.T .和达尔迈尔，d .，2017年7月。用于特征提取的无监督神经注意模型。在<em class="md">计算语言学协会第55届年会会议录(第1卷:长篇论文)</em>(第388–397页)。网址:【https://www.aclweb.org/anthology/P17-1036.pdf T2】</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="67b2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【https://ryanong.co.uk】原载于2020年4月15日<a class="ae jg" href="https://ryanong.co.uk/2020/04/15/day-106-nlp-research-papers-an-unsupervised-neural-attention-model-for-aspect-extraction/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div></div>    
</body>
</html>