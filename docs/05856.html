<html>
<head>
<title>Siamese and Dual BERT for Multi Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于多文本分类的连体和双BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533?source=collection_archive---------18-----------------------#2020-05-14">https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533?source=collection_archive---------18-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="316d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在模型中插入变压器的不同方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e7e1e092545d05cfbd1eb00ef2535a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3m3xJSK38fsrOt6k"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗尔夫·诺依曼在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="833a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对NLP的不断研究产生了各种预训练模型的发展。对于各种任务，如文本分类、无监督主题建模和问答，最先进的结果通常会有越来越多的改进。</p><p id="91d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最伟大的发现之一是在神经网络结构中采用了注意力机制。这项技术是所有被称为<strong class="lb iu">变形金刚网络的基础。</strong>他们<strong class="lb iu"> </strong>应用注意力机制提取关于给定单词上下文的信息，然后将其编码到学习向量中。</p><p id="5d0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为数据科学家，我们可以利用许多变压器架构来预测或微调我们的任务。在本帖中，我们喜欢经典的BERT，但同样的推理可以应用于其他任何变压器结构。<strong class="lb iu">我们的范围是在双重和连体结构中使用BERT，而不是将其用作多纹理输入分类的单一特征提取器</strong>。这篇文章的灵感来自于这里的<a class="ae ky" href="https://www.kaggle.com/c/google-quest-challenge/discussion/129978" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="7f93" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><p id="1431" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们从Kaggle收集了一个数据集。<a class="ae ky" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">新闻类别数据集</a>包含从<a class="ae ky" href="https://www.huffingtonpost.com/" rel="noopener ugc nofollow" target="_blank">赫芬顿邮报</a>获得的2012年至2018年约20万条新闻标题。我们的范围是根据两种不同的文本来源对新闻文章进行分类:标题和简短描述。我们总共有40多种不同类型的新闻。为了简单起见，考虑到我们工作流的计算时间，我们只使用8个类的子组。</p><p id="14e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不应用任何类型的预处理清洗；我们让我们的伯特做所有的魔术。我们的工作框架是<strong class="lb iu"> Tensorflow </strong>和巨大的<strong class="lb iu"> Huggingface </strong>变形金刚库。更详细地说，我们利用了裸露的Bert模型转换器，它输出原始的隐藏状态，而没有任何特定的头在上面。它可以像Tensorflow模型子类一样访问，并且可以很容易地在我们的网络架构中进行微调。</p><h1 id="be8a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">单伯特</h1><p id="31fb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">作为第一个竞争对手，我们引入了单一BERT结构。它只接收一个文本输入，这是我们两个文本源连接的结果。这是常态:任何模型都可以接收串联特征的输入。对于变压器，这一过程通过将输入与特殊令牌相结合而得到提升。</p><p id="d296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT期望输入数据具有特定的格式:有特殊的标记来标记句子/文本源的开始([CLS])和结束([SEP])。同时，标记化涉及将输入文本分割成词汇表中可用的标记列表。用词块技术处理未登录词；其中一个单词被逐渐分成子单词，这些子单词是词汇表的一部分。这个过程可以由预先训练好的拥抱脸标记器轻松完成；我们只需要处理填料。</p><p id="4427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们以每个文本源的三个矩阵(令牌、掩码、序列id)结束。它们是我们变压器的输入。在单个BERT的情况下，我们只有一个矩阵元组。这是因为我们同时将两个文本序列传递给我们的标记器，这两个文本序列自动连接在一起(用[SEP]标记)。</p><p id="0bcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们模型的结构非常简单:变压器直接由我们上面构建的矩阵供电。最后，变压器的最终隐藏状态通过平均池操作来减少。概率得分由最终的密集层计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/21c19c2434cd9723388c67de0cdac64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WkDmNywa-aG6xO5O3h1F4Q.png"/></div></div></figure><p id="aa7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们简单的BERT在测试数据上达到了83%的准确率。这些表现记录在下面的混淆矩阵中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/5e47d5046ee75e48a0cc876d6c1d65d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rn87q72xtFB2KES_ne2IuQ.png"/></div></div></figure><h1 id="a4e5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">双重伯特</h1><p id="181f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的第二种结构可以定义为双BERT，因为它使用两个不同的变压器。它们具有相同的组成，但是用不同的输入来训练。第一个接收新闻标题，而另一个接收简短的文字描述。输入被编码为总是产生两个矩阵元组(令牌、掩码、序列id)，每个输入一个。对于两个数据源，我们的转换器的最终隐藏状态随着平均池化而减少。它们被连接起来并通过一个完全连接的层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/6651985b0eb45c1bb93c872e287dfbd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAGuT28ban70hwbqZxEGjw.png"/></div></div></figure><p id="4067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这些设置，我们可以在测试数据上达到84%的准确率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/2ec360c41dac5d19ea2fcb729fdedb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kdv8UGflSDDen3bIHhPM-Q.png"/></div></div></figure><h1 id="0ffe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">暹罗伯特</h1><p id="b1b8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们最后的模型是一种连体建筑。它可以这样定义，因为两个不同的数据源在同一个可训练转换器结构中同时传递。输入矩阵与双BERT的情况相同。对于两个数据源，我们的转换器的最终隐藏状态是通过一个平均操作来汇集的。由此产生的串联在一个完全连接的层中传递，该层组合它们并产生概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/2b57eb71c1161270abf869982b0e3a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8-8INIsJQZ2s3OWtlllBg.png"/></div></div></figure><p id="9b66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的连体结构在测试数据上达到了82%的准确率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/a04125578c53697d655b578641bf5a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mv-dddU_2RWFBoZ1blu3bw.png"/></div></div></figure><h1 id="5081" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="af10" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇文章中，我们应用了BERT结构来执行多类分类任务。我们实验的附加价值是以各种方式使用变压器来处理多个输入源。我们从一个源中所有输入的经典连接开始，并在保持文本输入分离的情况下结束了模型。双体和连体变体能够实现良好的性能。因此，它们可以被视为传统单变压器结构的良好替代品。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="fbee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="fa3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="fce8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="8ede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两张床比一张床好</p><p id="e4fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Kaggle: <a class="ae ky" href="https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer" rel="noopener ugc nofollow" target="_blank">伯特基TF2.0 </a></p></div></div>    
</body>
</html>