<html>
<head>
<title>Top 4 Important Machine Learning and Deep Learning Papers You Should Read in 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021 年你应该阅读的 4 篇重要的机器学习和深度学习论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0?source=collection_archive---------9-----------------------#2020-05-28">https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0?source=collection_archive---------9-----------------------#2020-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="006b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">2021 年机器学习的突破</h2><div class=""/><div class=""><h2 id="b3f0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从数百篇高质量的 ML 研究论文中筛选出来的，这些是最突出的。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/829e8a1717bf34ef161727258356ba00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xG3jeahzmk29hZhF"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">丹·迪莫克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="li"><p id="a7cb" class="lj lk it bd ll lm ln lo lp lq lr ls dk translated">2021 年重要的机器学习和深度学习论文</p></blockquote><p id="dd27" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn ls im bi translated"><a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a>突然成为计算机科学最关键的领域之一，几乎涉及到所有与人工智能相关的领域。</p><p id="fbe7" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">每个公司都在应用<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a> <strong class="lv jd"> </strong>并开发利用这个领域的产品来更有效地解决他们的问题。</p><p id="142f" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">每年都有数千篇与<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a>相关的研究论文发表在 NeurIPS、ICML、ICLR、ACL、MLDS 等热门刊物上。</p><p id="a2e4" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">该标准使用来自三个学术来源的<strong class="lv jd">引用计数</strong>:<a class="ae lh" href="https://scholar.google.com/" rel="noopener ugc nofollow" target="_blank">scholar.google.com</a>；<a class="ae lh" href="https://academic.microsoft.com/" rel="noopener ugc nofollow" target="_blank">academic.microsoft.com</a>；还有 semanticscholar.org 的。</p><blockquote class="li"><p id="8819" class="lj lk it bd ll lm mt mu mv mw mx ls dk translated">自然语言处理、<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank">对话式人工智能、计算机视觉、强化学习</a>和人工智能伦理方面的重要研究论文每年发表一次</p></blockquote><p id="ef9b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn ls im bi translated">几乎所有的论文都提供了在<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a>领域的某种程度的发现。然而，有三篇论文特别站得住脚，它们在<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a>领域，特别是在<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">神经网络</strong> </a>领域提供了一些真正的突破。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="6331" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">用于图像识别的深度残差学习</h1><p id="7cbb" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">arvix:<a class="ae lh" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cv-foundation . org/open access/content _ cvpr _ 2016/papers/He _ Deep _ Residual _ Learning _ CVPR _ 2016 _ paper . pdf</a></p><h2 id="bd2d" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">摘要:</h2><p id="f55b" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated"><em class="oi">越深的神经网络越难训练。我们提出了一个剩余学习框架来简化比以前使用的网络更深入的网络训练。我们明确地将这些层重新表述为参考层输入的学习剩余函数，而不是学习未被参考的函数。</em></p><h2 id="a93e" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">在深度意义的驱动下，一个问题产生了:学习更好的网络是不是和堆叠更多层一样简单？</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/27f0b4742e30b675119c1a4a2567bac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*Fk1gxB6Nl4WRjOMa2xzpCQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有 20 层和 56 层平面网络的 CIFAR-10 上的训练误差(左)和测试误差(右)。</p></figure><p id="4964" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">学习身份函数<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">极其困难</a>，因为权重和偏差的所有可能组合的范围都是巨大的，因此学习<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">身份函数的机会微乎其微</a>。如上所述，<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">给神经网络增加更多的层</a>实际上可能会适得其反:更多的层=更低的精度(收益递减)。</p><p id="f320" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">该文件指出，对此有一个解决办法。也就是通过<strong class="lv jd">将隐藏层的输入与输出</strong>相加</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8ed785141739a79c35bccbd561ccbf32.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*6g78uAVCViTGNaq8uhTcOg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">剩余学习:一个积木。</p></figure><p id="f7f0" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">通过在更深的网络上实现这一想法，他们能够在 COCO 对象检测数据集上获得 28%的相对改善。深度残差网络是我们提交给 ILSVRC 和 COCO 2015 竞赛的基础，在那里他们还在 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割任务中获得了<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">第一名</a>。</p><h1 id="bd4e" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">单头注意力 RNN:停止用你的头脑思考</h1><p id="a2ad" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">arvix:<a class="ae lh" href="https://arxiv.org/pdf/1911.11423.pdf" rel="noopener ugc nofollow" target="_blank"><em class="oi"/></a><em class="oi"><br/></em>作者:<em class="oi"> </em> <a class="ae lh" href="https://scholar.google.com/citations?hl=en&amp;user=AolIi4QAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="noopener ugc nofollow" target="_blank"> <em class="oi">史蒂文</em> </a></p><p id="8052" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">在这篇论文中，哈佛毕业生史蒂文·梅里蒂介绍了一种最新的自然语言处理模型，称为单头注意力 RNN 或沙-RNN。Stephen Merity，独立研究员，主要专注于<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习、NLP 和深度学习</strong> </a>。作者通过一个简单的带有 SHA 的 LSTM 模型进行演示，在<em class="oi"> enwik8 上实现了最先进的字节级语言模型结果。</em></p><p id="b471" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">作者的主要目标是表明，如果我们沉迷于稍微不同的首字母缩写词和稍微不同的结果，整个领域可能会朝着不同的方向发展。</p><div class="ol om gp gr on oo"><a href="https://github.com/smerity/sha-rnn" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">Smerity/sha-rnn</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">要了解全部细节，请参见题为《单头注意力 RNN:停止用你的头脑思考》的论文。总而言之，“停止思考…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc lb oo"/></div></div></a></div><p id="1e90" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">Steven 提出的模型架构的中心概念由一个<strong class="lv jd"> LSTM 架构和一个基于阿沙的网络</strong>组成，该网络具有三个变量(Q、K 和 V)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/e495dbacc006dac23efe47af1d4d52e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FO-HeUI06ixQvfuPhIlawg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:Arvix(<a class="ae lh" href="https://arxiv.org/pdf/1911.11423.pdf" rel="noopener ugc nofollow" target="_blank"><em class="pe">https://arxiv.org/pdf/1911.11423.pdf</em></a><em class="pe">)</em></p></figure><p id="cfe8" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">每个 SHA-RNN 层只包含一个关注点，通过消除更新和维护多个矩阵的需要，有助于将模型的内存消耗保持在最低水平。</p><p id="0166" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">Boom 层与在变形金刚 和其他架构中发现的<strong class="lv jd"> l </strong> <a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd"> arge 前馈层紧密相关。与传统的向下投影层相比，该块通过使用高斯误差线性单元(GeLu)乘法来分解输入以最小化计算，从而减少并移除了整个参数矩阵。</strong></a></p><p id="92ad" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">下面我们来看实际对比。2016 年，<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">RNN 正则化方法 surprise-Driven zone out</a>在 Hutter Prize 数据集<em class="oi"> enwiki8 </em>上取得了 1.313bpc 的优异压缩成绩，该数据集是一个百兆字节的维基百科页面文件。</p><p id="0d2a" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">与 2016 年的车型相比，SHA-RNN 的 bpc 甚至更低。令人印象深刻。每个字符的位数是由 Alex Graves 提出的模型，用于在给定过去字符的情况下近似下一个字符的<a class="ae lh" href="https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html" rel="noopener ugc nofollow" target="_blank">概率分布。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/13acdbfd49a65fa42dbd41e34509154b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KA-q2POhSnClyZKgcM95Cg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/pdf/1911.11423.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.11423.pdf</a></p></figure><p id="f097" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">此外，单头注意力 RNN(沙-RNN)设法实现了强大的最先进的结果，几乎没有超参数调整，并通过使用一个单一的泰坦 V GPU 工作站。此外，他的工作没有经历过密集的超参数调整，完全依赖于一台商用台式机，这使得作者的小工作室公寓有点太温暖了，他不喜欢。这就是对<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a> <strong class="lv jd">的热情。</strong></p></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><h1 id="8ace" class="na nb it bd nc nd pn nf ng nh po nj nk ki pp kj nm kl pq km no ko pr kp nq nr bi translated">EfficientNet:重新思考卷积神经网络的模型缩放</h1><p id="52e4" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">arvix:<a class="ae lh" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/1905.11946】</a><br/>作者:<a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan%2C+M" rel="noopener ugc nofollow" target="_blank"> <em class="oi">谭明兴</em></a><em class="oi"/><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V" rel="noopener ugc nofollow" target="_blank"><em class="oi">郭诉乐</em> </a></p><p id="dff1" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">在本文中，作者系统地研究了模型缩放，并确定仔细平衡网络深度、宽度和分辨率可以获得更好的性能。本文展示了一种新的缩放方法，它使用一个简单而高效的复合系数统一缩放深度、宽度和分辨率的所有维度。</p><div class="ol om gp gr on oo"><a href="https://github.com/narumiruna/efficientnet-pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">narumiruna/efficient net-pytorch</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">“EfficientNet:卷积神经网络模型缩放的再思考”的 PyTorch 实现。…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="ps l oz pa pb ox pc lb oo"/></div></div></a></div><p id="f172" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">论文提出了一种<em class="oi">简单</em>然而<em class="oi">有效</em>的复合缩放方法，描述如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/41679561eed2e1f73875b9a11409b3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9pbS5OyhGLczKcuJf8sEQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:Arvix(<a class="ae lh" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"><em class="pe">)https://arxiv.org/abs/1905.11946</em></a><em class="pe">)</em></p></figure><p id="6590" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">经过尺寸缩放(宽度、深度或分辨率)的网络可提高精度。但是需要注意的是，模型精度随着模型的增大而下降。因此，在 CNN 缩放过程中平衡网络的所有三个维度(宽度、深度和分辨率)对于提高精度和效率至关重要。</p><p id="38c7" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">与传统的缩放方法相比，如上所述的<em class="oi"> </em> <strong class="lv jd"> <em class="oi">复合缩放方法</em> </strong>持续提高了放大现有模型的模型精度和效率，例如<a class="ae lh" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNet </a> (+1.4%图像净精度)和<a class="ae lh" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a> (+0.7%)</p><p id="ad7e" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">缩放不会改变图层操作；相反，他们通过进行神经架构搜索<em class="oi"> </em> (NAS)获得了他们的基本网络，该网络同时优化了准确性和 FLOPS。与 ResNet-50 和 DenseNet-169 等现有 ConvNets 相比，扩展的 EfficientNet 模型始终将参数和 FLOPS 减少了一个数量级(参数减少高达 8.4 倍，FLOPS 减少高达 16 倍)。</p><p id="9adb" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">EfficientNets 还在八个数据集的五个中实现了最先进的准确性，如<a class="ae lh" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-100 </a> (91.7%)和<a class="ae lh" href="http://www.robots.ox.ac.uk/~vgg/data/flowers/" rel="noopener ugc nofollow" target="_blank"> Flowers </a> (98.8%)，参数数量级更少(参数减少高达 21 倍)，这表明 EfficientNets 也传输良好。</p></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><h1 id="ba7d" class="na nb it bd nc nd pn nf ng nh po nj nk ki pp kj nm kl pq km no ko pr kp nq nr bi translated">深度双重下降:更大的模型和更多的数据会造成伤害</h1><p id="455e" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">arvix:<a class="ae lh" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank"><em class="oi"/></a><br/>作者:<em class="oi"> </em> <a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nakkiran%2C+P" rel="noopener ugc nofollow" target="_blank"> <em class="oi">普雷顿·纳克兰</em></a><em class="oi"/><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplun%2C+G" rel="noopener ugc nofollow" target="_blank"><em class="oi">加尔·卡普伦</em></a><em class="oi"/><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bansal%2C+Y" rel="noopener ugc nofollow" target="_blank"><em class="oi">亚米尼·班萨尔</em></a><em class="oi"/><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T" rel="noopener ugc nofollow" target="_blank"><em class="oi">特里斯坦·杨</em></a><em class="oi"/><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barak%2C+B" rel="noopener ugc nofollow" target="_blank"><em class="oi"/></a></p><p id="459e" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">在本文中，作者在<a class="ae lh" href="https://openai.com" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>将<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank">神经网络</a>的一个训练过程的有效模型复杂度<em class="oi"> </em> (EMC) <em class="oi"> </em>定义为它能达到接近零训练误差的最大样本数。进行的实验表明，在<em class="oi">插值阈值</em>附近存在一个临界区间。</p><p id="d37b" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">插值阈值意味着模型随着模型参数的数量、训练的长度、分布中的标签噪声量以及训练样本的数量而变化。临界区域只是参数化不足和参数化过度的风险域之间的一个小区域。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/9d50841d8f7ba79af78e0a42f631d957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMI4UpMca_ilDLkfMRpONw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent" rel="noopener ugc nofollow" target="_blank">https://www . less wrong . com/posts/frv 7 ryoqtvsuqbxut/understanding-deep-double-descent</a></p></figure><p id="3ef7" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">在大多数研究中，<em class="oi">偏差-方差权衡</em>是经典统计学习理论中的一个基本概念。这个想法是，模型越复杂，偏差越小，但方差越大。</p><h2 id="a279" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">一旦模型复杂性超过临界区间，模型会过度拟合，方差项会控制测试误差，因此从这一点开始，增加模型复杂性只会降低<em class="pe"> </em>性能，这称为双下降现象。</h2><div class="ol om gp gr on oo"><a href="https://openai.com/blog/deep-double-descent/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">深度双重下降</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">我们证明了双重下降现象发生在 CNN、ResNets 和 transformers 中:性能首先提高，然后…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">openai.com</p></div></div><div class="ox l"><div class="pv l oz pa pb ox pc lb oo"/></div></div></a></div><p id="71a4" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">本文定义了模型性能降低的三种情况，因为下面的状态变得更加重要。</p><h2 id="f86b" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">(按型号)双重下降—型号越大，伤害越大</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/a760ae786a4aaf9927a657a6ec5734ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gb59rC2GD3kORri1Zr54lA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd px">模范政权</strong>(来源:<a class="ae lh" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank"><em class="pe">【https://arxiv.org/abs/1912.02292】</em></a><em class="pe">)</em></p></figure><p id="8715" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">论文展示了不同架构、数据集、优化器和训练过程中的模型式双下降现象。</p><p id="6215" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">该论文的结论是，通过在训练之前对数据集进行的常规修改(例如，添加标签噪声、使用数据扩充和增加训练样本的数量)，测试误差峰值会向更大的模型移动<strong class="lv jd">。</strong></p><p id="dd88" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">此外，在上面的图表中，测试误差的峰值出现在插值阈值附近，此时模型刚好大到足以适合训练集。</p><h2 id="1ba4" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">(样本方面)非单调性—更多数据会造成伤害</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/56a03640306248c88b52f6511d40def1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wOLe0Ba-hUriUMkDtNkxw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd px">样本政权</strong>(来源:<a class="ae lh" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank"><em class="pe">https://arxiv.org/abs/1912.02292</em></a><em class="pe">)</em></p></figure><p id="6c54" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">在本节中，图表显示了改变固定模型的训练样本数量的效果。增加样本数量会使曲线向较低的测试误差方向下移，但也会使峰值误差向右移动。</p><h2 id="864b" class="nx nb it bd nc ny nz dn ng oa ob dp nk mc oc od nm mg oe of no mk og oh nq iz bi translated">(纪元方式)双重下降-更高的纪元伤害</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/c7f06c940b1d1da8a115ebe83aff3433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbcZFMAhvvOEpzW_vFHHtw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd px">历朝政权</strong>(来源:<a class="ae lh" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/1912.02292】T21</a><em class="pe">)</em></p></figure><p id="6d30" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">对于给定数量的优化步骤(固定的 y 坐标)，测试和训练误差呈现出模型大小的双重下降。对于给定的模型尺寸，随着训练过程的进行，测试和训练误差减小、增大、再减小；我们称这种现象为划时代的双重下降。</p><h1 id="8657" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">增加训练时间会增加 EMC，因此在整个训练阶段，一个足够大的模型会从参数不足过渡到参数过多。</h1><p id="3c9c" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">此外，具有更多宽度参数的较大模型(如 ResNet 架构)可能会经历显著的双下降行为，其中测试误差首先下降(<em class="oi">比其他尺寸模型</em>更快)，然后在插值阈值附近上升，然后再次下降，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/275cdf99d520fa02d286cb4cda8f1e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*V03NIG5cOMJv8xpOENEJcg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://mltheory.org/deep.pdf" rel="noopener ugc nofollow" target="_blank">https://mltheory.org/deep.pdf</a></p></figure><p id="df40" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">对于处于插值阈值的模型，实际上只有一个全局模型适合训练数据，强制它适应甚至小的错误指定的标签将破坏它的全局结构。然后，该论文得出结论，没有好的模型既能对训练集进行插值，又能在测试集上表现良好。</p><p id="323f" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">如上所述，这些关键机制的特征为实践者提供了一种有用的思维方式，<strong class="lv jd">希望很快在机器学习方面取得突破。</strong></p><h1 id="236a" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">参考</h1><div class="ol om gp gr on oo"><a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">理解“深度双重下降”——less wrong 2.0</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">如果你不熟悉双重下降现象，我想你应该熟悉。我认为双重血统是一种…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">www.lesswrong.com</p></div></div><div class="ox l"><div class="qb l oz pa pb ox pc lb oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://openai.com/blog/deep-double-descent/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">深度双重下降</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">我们证明了双重下降现象发生在 CNN、ResNets 和 transformers 中:性能首先提高，然后…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">openai.com</p></div></div><div class="ox l"><div class="qc l oz pa pb ox pc lb oo"/></div></div></a></div><h1 id="f38f" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">最后</h1><p id="f63a" class="pw-post-body-paragraph lt lu it lv b lw ns kd ly lz nt kg mb mc nu me mf mg nv mi mj mk nw mm mn ls im bi translated">随着<a class="ae lh" href="https://www.iunera.com/kraken/fabric/machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jd">机器学习</strong> </a>社区每年的成长，会有越来越多的论文发表。这是我们的一部分，阅读新的和合理的文章，以装备自己在社区的最新和最先进的突破。继续阅读爱好者伙伴！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qd mz l"/></div></figure></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><p id="ed90" class="pw-post-body-paragraph lt lu it lv b lw mo kd ly lz mp kg mb mc mq me mf mg mr mi mj mk ms mm mn ls im bi translated">如果我已经设法让你注意到这一点，请留下评论，如果你对这个系列有任何建议，因为它将大大增加我的知识，改善我的写作方式。<a class="ae lh" href="/@premstroke95" rel="noopener ugc nofollow" target="_blank"><strong class="lv jd"><em class="oi">Prem Kumar</em></strong><em class="oi"/></a><em class="oi">是一个无私的学习者，对我们身边的日常数据充满热情。如果你想谈论这个故事和等待的未来发展，请在</em><a class="ae lh" href="https://www.linkedin.com/in/premstrk/" rel="noopener ugc nofollow" target="_blank"><strong class="lv jd"><em class="oi">LinkedIn</em></strong></a><strong class="lv jd"><em class="oi"/></strong><em class="oi">上与我联系。</em></p></div></div>    
</body>
</html>