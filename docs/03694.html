<html>
<head>
<title>A simple guide to knowing your neural network and activation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解你的神经网络和激活的简单指南。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-guide-to-knowing-your-neural-network-and-activation-8d98ebfcd3fd?source=collection_archive---------55-----------------------#2020-04-06">https://towardsdatascience.com/a-simple-guide-to-knowing-your-neural-network-and-activation-8d98ebfcd3fd?source=collection_archive---------55-----------------------#2020-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="782b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">分析哪个功能似乎合适</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/416d42b78a1c1d43ef579d053026cb95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4p8Ssy8QFeYKaZY-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@stevenwright?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">史蒂夫·赖特</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="6d27" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">生物神经网络一览。</h1><p id="b134" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了更好地理解激活函数的意图和目的，让我们分析一下我们自身的一个等效模型——<em class="mn">神经元</em>(首先是<em class="mn">神经网络</em>的灵感)。一个生物神经网络简单来说就是由一个<strong class="lt iu">细胞</strong> <strong class="lt iu">体</strong>，<strong class="lt iu">树突</strong>(来自神经元的输入)和<strong class="lt iu">轴突</strong>(输出到其他神经元)。一个有用的术语是<strong class="lt iu">突触</strong>，它来自一个神经元与另一个神经元通信的接触点。树突上覆盖着由其他神经元轴突末端形成的突触。来自所有树突的突触在细胞体中累积，细胞体根据累积跨越的<strong class="lt iu">阈值</strong>(类似于各种<strong class="lt iu">激活功能</strong>)确定细胞的状态<strong class="lt iu">激活/未激活</strong>。然后这些突触穿过轴突，到达沿途交流的其他神经元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/55f7f851a0b3ffbd39563dd955893a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7w1w5liaFbN5mBkuRgf3A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经元的简单解剖</p></figure><h1 id="f50d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">人工神经网络</h1><p id="8f7e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工神经网络确实保持了其生物学等效物的完整性，尽管多了一些术语。</p><p id="30d1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">重量<br/>偏差</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/a78b3c581ccc3937d6c3905f498a3e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8YhicvNzminaJpRuoKW7A.jpeg"/></div></div></figure><h2 id="5d45" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">神经元的处理过程</h2><p id="108e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">输出=σ(权重*输入)+偏差</strong></p><p id="3655" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">这遵循线性方程<strong class="lt iu">y = MX+c</strong>T29】其中权重对应于“m ”,偏差对应于“c”</p><h2 id="4dba" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">重量(瓦特)</h2><p id="3916" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">权重，通常是突触权重，代表一个神经元对另一个神经元的强度，使整体连接更强。</p><h2 id="62c5" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">偏见</h2><p id="7f08" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">作为截距，偏差项使网络符合实际情况，而不是理想情况。如果缺少偏差项，训练点将只通过原点，这是一个太简单的分析。一个网络往往是更强大的偏差项，因为它是开放的，以新的方式考虑和拟合。此外，它还决定了激活功能生效的时间点。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="470e" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">选择您的激活功能</h1><p id="e697" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们已经对生物神经网络和人工神经网络进行了基本的比较，我们将看看各种激活函数，并检查它们的优点和局限性。</p><p id="e779" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">每个激活函数取一个值，并对其执行数学运算。</p><h2 id="3356" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">1.乙状结肠的</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/08d1c4eb7c1b3ef6d6b05f3f6fac44f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQM-CtHzZPdSvICOqV7YEQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乙状结肠的</p></figure><ul class=""><li id="72fb" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated">它接受一个实数值，并将其压缩到范围[0，1]内</li><li id="3af0" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">其数学形式:<strong class="lt iu">σ(x)= 1/(1+e x)</strong></li></ul><p id="a6d7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">但是，由于以下限制，它很少实现:</p><ul class=""><li id="83d8" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated"><strong class="lt iu">乙状结肠饱和，从而消除梯度<br/> </strong>当激活在 1，0 达到饱和时，梯度减小到零。<br/>因此，在反向传播期间，该零梯度与梯度输出相乘，产生一个非常小的数字，阻碍信号流过权重和输入。</li><li id="b264" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated"><strong class="lt iu">非零中心输出<br/> </strong>如果进入神经元的数据总是正的，则反向传播期间的权重将产生全正或全负的梯度。这导致梯度更新中不期望的行为。</li></ul><h2 id="26e2" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated"><strong class="ak"> 2。Tanh </strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/41f449a7f254c83032f7f1e42989eb35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gPiOd2RIgqrjX1Aj8nFww.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双曲正切</p></figure><p id="6552" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">虽然与 sigmoid 函数相似，但 tanh 函数解决了它的一个限制:非零中心输出。</p><ul class=""><li id="d662" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated"><strong class="lt iu">tanh(x)= 2σ(2x)1</strong></li><li id="3f49" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">它将一个实数值压缩到[-1，1]范围内</li></ul><p id="e7d1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">局限性:</p><ul class=""><li id="5b15" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated">它在与它的乙状结肠对应物相同的线上饱和。</li></ul><h2 id="209b" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">3.热卢</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/7915f18b2aeb4a97c38b5b93ac6b5ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MampPDjoTyuRYvomF-nag.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">热卢</p></figure><ul class=""><li id="1a90" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated"><strong class="lt iu"> f(x)=max(0，x) </strong></li><li id="3110" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">随机梯度的收敛速度大约加快了 6 倍。</li><li id="e5ec" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">它将激活矩阵的阈值设置为零，并且由于没有指数或缺少指数，所以成本较低。</li><li id="dcde" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">随着 x 的增加，不存在饱和。</li><li id="35e6" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">然而，它提出了“垂死的 ReLU”问题的局限性。<br/>当零处的梯度为零时(当 x &lt;为 0 时)，一个垂死的 ReLU 对任何输入产生相同的输出。梯度学习不会改变权重。</li><li id="d0ca" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">这是由于<br/> -一个非常高的学习率<br/> -一个大的负偏差</li></ul><p id="851d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated"><strong class="lt iu"> 4。泄漏的 ReLU </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/5fc1b6ebd36bb2bfff40b7020cb6bbe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VS-YmAXI-0vvD6eZXaqOFw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">泄漏的 ReLU 和 PReLU</p></figure><p id="6a4f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">泄漏的 ReLU 试图解决垂死的 ReLU 问题的局限性。</p><ul class=""><li id="2c14" class="nu nv it lt b lu mp lx mq ma nw me nx mi ny mm nz oa ob oc bi translated">当 x &lt; 0. <br/>例如 y = 0.01x 时，泄漏 ReLU 具有小的负斜率</li><li id="26d3" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">参数 ReLU ( PReLU)是泄漏 ReLU 的变体，其中斜率被视为要调整的参数。α代表斜率参数。y =α x</li><li id="8322" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">因此，由于负值斜率的存在，它确实会对相应的输入产生一个输出，从而产生变化。</li><li id="6f89" class="nu nv it lt b lu od lx oe ma of me og mi oh mm nz oa ob oc bi translated">然而，泄漏的 ReLU 并不总是一致的。</li></ul><h2 id="b3c0" class="mv la it bd lb mw mx dn lf my mz dp lj ma na nb ll me nc nd ln mi ne nf lp ng bi translated">所以，问题。我应该选择哪个激活功能？</h2><p id="6fd9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">避免使用 sigmoid 函数。您可以选择 ReLU 或泄漏 ReLU(作为 ReLU 函数的替代)。不过，要监控你的学习速度。</p><p id="64d8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">你可以给 Tanh 一个机会，但是它可能产生比 ReLU 更差的结果。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="8d10" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">资源</h1><div class="ol om gp gr on oo"><a href="http://cs231n.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">用于视觉识别的 CS231n 卷积神经网络</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">斯坦福 CS231n 课程材料和笔记:视觉识别的卷积神经网络。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">cs231n.github.ioo</p></div></div></div></a></div><div class="ol om gp gr on oo"><a href="https://www.tinymind.com/learn/terms/relu" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">ReLU -人类的机器学习- TinyMind</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">TL；DR: ReLU 代表整流线性单位，是一种激活函数。数学上定义为 y…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">www.tinymind.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ks oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://news.sophos.com/en-us/2017/09/21/man-vs-machine-comparing-artificial-and-biological-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">人与机器:人工神经网络和生物神经网络的比较</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">学习能力被认为是智慧生命的标志之一。机器学习现在有能力学习和…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">news.sophos.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ks oo"/></div></div></a></div></div></div>    
</body>
</html>