<html>
<head>
<title>Creating Deeper Bottleneck ResNet from Scratch using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow 从头开始创建更深的瓶颈 ResNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-deeper-bottleneck-resnet-from-scratch-using-tensorflow-93e11ff7eb02?source=collection_archive---------13-----------------------#2020-09-02">https://towardsdatascience.com/creating-deeper-bottleneck-resnet-from-scratch-using-tensorflow-93e11ff7eb02?source=collection_archive---------13-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6849" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们将看到如何使用 Tensorflow 2.0 从头开始实现 ResNet50</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dedca3d3d2daa43cc3ccb854a70bddfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6Re5WKe2EfkwqrsTH2sDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。剩余块和跳过连接(来源:图片由作者创建)</p></figure><p id="992d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以看出，通常较深的神经网络比浅的神经网络性能更好。但是，深度神经网络面临一个共同的问题，称为“消失/爆炸梯度问题”。为了克服这个问题，提出了 ResNet 网络。</p><p id="a054" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原文链接—<a class="ae lu" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.03385</a></p><h1 id="6d42" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">剩余块:</h1><p id="f801" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">ResNets 包含<strong class="la iu">剩余块</strong>。如图 1 所示，存在激活‘a<em class="ms">l’</em>，随后是具有 ReLU 非线性的线性层，‘a<em class="ms">l+1</em>’。接着是另一个线性层，带有另一个非线性，“a <em class="ms"> l+2 </em>”。这是正常或普通神经网络的样子。ResNet 增加了<strong class="la iu">跳过连接</strong>。在 ResNet 中，来自‘a<em class="ms">l</em>的信息被快进并复制到‘a<em class="ms">l+1</em>之后的线性层之后，ReLU 非线性之前。现在具有跳跃连接的整个块被称为残余块。图 2 显示了 ResNet 原始论文中看到的残余块。跳跃连接有助于跳过几乎两层，将其信息传递到神经网络的更深处。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0dffc62e826e28a30018cc6658f14ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*sUd2c4yaFVjrADItevMS-A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。ResNet 论文中显示的残余块(来源:ResNet 论文原文)</p></figure><p id="2c84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用残余块可以训练更深层次的神经网络。因此，ResNet 论文的作者将这些残差块一个接一个地堆叠起来，形成一个深度残差神经网络，如图 3 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/8cbc3a59f75b024a884a1e7acf476244.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ArJcY5iVvrmK5uktGvUhbQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。34 层平面网络与 34 层残差网络的图像片段(来源:原始 ResNet 论文)</p></figure><p id="971b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以看出，当使用诸如 SGD 的优化算法来训练深度平面网络时，理论上训练误差应该随着神经网络中层数的增加而不断减小，但实际情况并非如此。在减少到某一层之后，训练误差又开始增加。但是使用残余块可以克服这个问题，并且即使当层数显著增加时，训练误差也保持减小。这可以在图 4 中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/711b5a12ec372eccd80eef153302f197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zu5oRch4ouRAgCQqVTbW6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4。普通网络与 ResNet 的训练误差。X 轴表示层数的增加，y 轴表示训练误差(来源:图片由作者提供)</p></figure><h1 id="9af0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">更深的瓶颈 ResNet 架构:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/0ac955d847fe7a787342f09232e091b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*PvE0ZALHB1LJr2A-D8jVIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5。普通残余块 vs 更深的瓶颈残余块(来源:原 ResNet 论文)</p></figure><p id="1d19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图 5 显示了普通剩余块与更深瓶颈剩余块之间的差异。在公共残差块中，有两个具有 3×3 滤波器的卷积层。在瓶颈架构中，有 3 个卷积层，而不是 2 个。这三层是 1x1、3x3 和 1x1 卷积，其中 1x1 层负责减少然后增加(恢复)维度，而 3x3 层则成为具有较小输入/输出维度的瓶颈。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ba04a8dbfd4dda6d4c77ea841654d41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Owp8io0Sv27vXa89bA0bLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6。瓶颈残余块—身份块(来源:图片由作者创建)</p></figure><p id="e66d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图 6 所示，瓶颈残差块有一个 1x1 的 Conv 层，然后是批规范化和 ReLU 激活，接着是一个 3x3 的 Conv 层，然后是批规范化和 ReLU，最后是一个 1x1 的 Conv 层和一个批规范化。这连接到跳过连接输入，然后应用最终 ReLU 激活。这是第一个瓶颈层版本(身份块)。“f”代表每个 Conv 层中滤光器的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/94d75e9f80a8d20d399e0f6934ffc365.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*uzn4ftyOtQafwSVGQ6RE2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7。瓶颈剩余块—投影版本(来源:图片由作者创建)</p></figure><p id="e374" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">瓶颈层的第二个版本(投影)与第一个版本非常相似，除了它有一个额外的 1x1 Conv 层，然后是在跳过连接上出现的批量归一化。跳过连接上的额外 Conv 层确保残差块左侧的滤波器数量与残差块右侧的滤波器数量相同。这允许将输入与残差块相加，而没有任何误差。此外，如果我们需要将<strong class="la iu">步</strong>添加到左侧 Conv 层以减小图像的大小，我们可以通过在跳过连接中在 Conv 层上添加类似数量的步来确保来自前一层的输入也具有相同的大小。</p><h1 id="e040" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">使用 Tensorflow 创建 ResNet50】</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/9369a283d56be3a0b5d18ccc106a5321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34t4V9hzKPXV2gCmQz-Q9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8。我们将创建一个 50 层的 ResNet。图像显示了网络中使用的所有块(来源:原始 ResNet 文件)</p></figure><ul class=""><li id="005b" class="mz na it la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated">在 ResNet 论文中，提到了在每个卷积块之后和激活函数之前使用批量归一化。</li><li id="6d99" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">该网络始于 Conv 层，其具有 7×7 的核形状和 64 个核(滤波器)，步长为 2。随后是批量规范化和 ReLU 激活。</li><li id="a21b" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">接下来是最大 3x3 的池层和步幅 2。</li><li id="b5c5" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">其后是 ResNet(剩余/瓶颈)块。有 4 个 ResNet 块，每个块中的内核数量和内核大小如图 8 所示。</li><li id="6609" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">最后，有一个平均池层，一个完全连接的层，然后是 Softmax 激活。</li></ul><p id="4bd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">算法:</strong></p><ol class=""><li id="6d5f" class="mz na it la b lb lc le lf lh nb ll nc lp nd lt nn nf ng nh bi translated">从 Tensorflow 导入所有必要的层</li><li id="0e31" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">为 conv-巴奇诺姆-雷卢块写一个函数</li><li id="806b" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">为身份瓶颈块编写一个函数</li><li id="4d90" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">为投影瓶颈块编写一个函数</li><li id="d177" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">为 ResNet 块编写一个函数</li><li id="553e" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">一起使用所有函数来创建模型</li></ol><p id="9a4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">导入 TensorFlow 和所有必需的库。</strong></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="893f" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#import the libraries</em><br/><br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> Input, Conv2D, BatchNormalization<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> MaxPool2D, GlobalAvgPool2D<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> Add, ReLU, Dense<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras</strong> <strong class="np iu">import</strong> Model</span></pre><p id="68b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建 conv-巴奇诺姆-雷鲁块的函数</strong></p><p id="7454" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是基本的构建模块。它包含一个卷积层，随后是 BatchNormalization，接着是 ReLU 激活功能。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="0e25" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#Conv-BatchNorm-ReLU block</em><br/><br/><strong class="np iu">def</strong> conv_batchnorm_relu(x, filters, kernel_size, strides=1):<br/>    <br/>    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    <br/>    <strong class="np iu">return</strong> x</span></pre><p id="fbe9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">功能创建身份瓶颈块</strong></p><p id="56b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们前面所看到的，一个身份块包含 3 个卷积层，每一层之后是批量标准化和 ReLU 激活，除了最后一层，它首先添加到跳过连接，然后才应用 ReLU 激活。</p><p id="cd9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，前两个卷积层的滤波器数量相同，但最后一个卷积层的滤波器数量是原来的 4 倍。</p><p id="4a20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图 8 所示，“conv2_x”的前两个卷积模块有 64 个滤波器，最后一个卷积模块有 64*4=256 个滤波器。对所有的标识块重复同样的操作。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="d39f" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#Identity block</em><br/><br/><strong class="np iu">def</strong> identity_block(tensor, filters):<br/>    <br/>    x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=1)<br/>    x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)<br/>    x = Conv2D(filters=4*filters, kernel_size=1, strides=1)(x)<br/>    x = BatchNormalization()(x)<br/>    <br/>    x = Add()([tensor,x])    <em class="ms">#skip connection</em><br/>    x = ReLU()(x)<br/>    <br/>    <strong class="np iu">return</strong> x</span></pre><p id="ed28" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建投影块的功能</strong></p><p id="bc2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">投影块的左侧与标识块相似，只是步数不同。</p><p id="8f88" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">投影块的右侧包含 1×1 卷积层，其滤波器数量是左侧第一层的 4 倍，跨距数量相同。这确保了原始输入和跳过连接具有相同数量的滤波器和相同的步距，否则矩阵加法将会出错。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="1691" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#Projection block</em><br/><br/><strong class="np iu">def</strong> projection_block(tensor, filters, strides):<br/>    <br/>    <em class="ms">#left stream</em><br/>    x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=strides)<br/>    x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)<br/>    x = Conv2D(filters=4*filters, kernel_size=1, strides=1)(x)<br/>    x = BatchNormalization()(x)<br/>    <br/>    <em class="ms">#right stream</em><br/>    shortcut = Conv2D(filters=4*filters, kernel_size=1, strides=strides)(tensor)<br/>    shortcut = BatchNormalization()(shortcut)<br/>    <br/>    x = Add()([shortcut,x])    <em class="ms">#skip connection</em><br/>    x = ReLU()(x)<br/>    <br/>    <strong class="np iu">return</strong> x</span></pre><p id="2233" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建 ResNet 块的函数</strong></p><p id="499f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图 8 所示，每个模块“conv2_x”、“conv3_x”等都重复多次。“conv2_x”重复 3 次，“conv3_x”重复 4 次，“conv4_x”重复 6 次，“conv5_x”重复 3 次。</p><p id="9d02" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每种情况下，第一个块是投影块，其余的是单位块。所以对于‘con v2 _ x’，第一个块是投影块，另外两个重复是单位块。</p><p id="0642" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一块是投影的原因是为了确保来自跳过连接的输入和该块的实际输出的步长和滤波器数量是相同的。在投影块中，第一卷积层的跨距=2。这意味着，如果输入的是一个图像，该层之后的图像的大小将会减小。但是跳过连接中的输入仍然具有先前的图像大小。添加两个不同大小的图像是不可能的。所以 skip 连接也有一个 stride=2 的卷积层。这确保了图像尺寸现在是相同的。</p><p id="bdea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下面的重复中，步幅被设置为 1，因此图像大小保持不变，并且不需要投影块。因此，除了每个 ResNet 块的第一次重复之外，所有其他重复都使用相同的块。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="71a2" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#Resnet block</em><br/><br/><strong class="np iu">def</strong> resnet_block(x, filters, reps, strides):<br/>    <br/>    x = projection_block(x, filters, strides)<br/>    <strong class="np iu">for</strong> _ <strong class="np iu">in</strong> range(reps-1):<br/>        x = identity_block(x,filters)<br/>        <br/>    <strong class="np iu">return</strong> x</span></pre><p id="74bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建模型</strong></p><p id="166b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然所有的块都准备好了，那么现在可以按照图 8 创建模型了。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="48b1" class="nt lw it np b gy nu nv l nw nx"><em class="ms">#Model</em><br/><br/>input = Input(shape=(224,224,3))<br/><br/>x = conv_batchnorm_relu(input, filters=64, kernel_size=7, strides=2)<br/>x = MaxPool2D(pool_size = 3, strides =2)(x)<br/>x = resnet_block(x, filters=64, reps =3, strides=1)<br/>x = resnet_block(x, filters=128, reps =4, strides=2)<br/>x = resnet_block(x, filters=256, reps =6, strides=2)<br/>x = resnet_block(x, filters=512, reps =3, strides=2)<br/>x = GlobalAvgPool2D()(x)<br/><br/>output = Dense(1000, activation ='softmax')(x)<br/><br/>model = Model(inputs=input, outputs=output)<br/>model.summary()</span></pre><p id="8df3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出片段:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/1274fc9898c8f69b91781a498020f019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQUD-6c_GyxbpRGRwcb45w.png"/></div></div></figure><p id="7245" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">绘制模型</strong></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="a079" class="nt lw it np b gy nu nv l nw nx"><strong class="np iu">from</strong> <strong class="np iu">tensorflow.python.keras.utils.vis_utils</strong> <strong class="np iu">import</strong> model_to_dot<br/><strong class="np iu">from</strong> <strong class="np iu">IPython.display</strong> <strong class="np iu">import</strong> SVG<br/><strong class="np iu">import</strong> <strong class="np iu">pydot</strong><br/><strong class="np iu">import</strong> <strong class="np iu">graphviz</strong><br/><br/>SVG(model_to_dot(model, show_shapes=<strong class="np iu">True</strong>, show_layer_names=<strong class="np iu">True</strong>, rankdir='TB',expand_nested=<strong class="np iu">False</strong>, dpi=60, subgraph=<strong class="np iu">False</strong>).create(prog='dot',format='svg'))</span></pre><p id="eaf7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出片段:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/55d6df47b195bcfe191af3a6a3e24ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*A1GbyPs01PeAcDjTbGWdoQ.png"/></div></figure><p id="e92a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 Tensorflow 从头开始创建 ResNet 50 的完整代码:</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="255a" class="nt lw it np b gy nu nv l nw nx"><strong class="np iu"><em class="ms">#import the libraries</em></strong><br/><br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> Input, Conv2D, BatchNormalization<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> MaxPool2D, GlobalAvgPool2D<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras.layers</strong> <strong class="np iu">import</strong> Add, ReLU, Dense<br/><strong class="np iu">from</strong> <strong class="np iu">tensorflow.keras</strong> <strong class="np iu">import</strong> Model</span><span id="70b5" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu"><em class="ms">#Conv-BatchNorm-ReLU block</em></strong></span><span id="199c" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu">def</strong> conv_batchnorm_relu(x, filters, kernel_size, strides=1):<br/>    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    <strong class="np iu">return</strong> x</span><span id="a4e2" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu"><em class="ms">#Identity block</em></strong></span><span id="c6d0" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu">def</strong> identity_block(tensor, filters):<br/>    x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=1)<br/>    x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)<br/>    x = Conv2D(filters=4*filters, kernel_size=1, strides=1)(x)<br/>    x = BatchNormalization()(x)<br/>    x = Add()([tensor,x])    <em class="ms">#skip connection</em><br/>    x = ReLU()(x)<br/>    <strong class="np iu">return</strong> x<br/><em class="ms"><br/><br/></em><strong class="np iu"><em class="ms">#Projection block</em> </strong><br/> <br/><strong class="np iu">def</strong> projection_block(tensor, filters, strides): <br/>         <br/>     <em class="ms">#left stream</em>     <br/>     x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=strides)     <br/>     x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)     <br/>     x = Conv2D(filters=4*filters, kernel_size=1, strides=1)(x)     <br/>     x = BatchNormalization()(x) <br/>         <br/>     <em class="ms">#right stream</em>     <br/>     shortcut = Conv2D(filters=4*filters, kernel_size=1, strides=strides)(tensor)     <br/>     shortcut = BatchNormalization()(shortcut)          <br/>     x = Add()([shortcut,x])    <em class="ms">#skip connection</em>     <br/>     x = ReLU()(x)          <br/>     <strong class="np iu">return</strong> x</span><span id="1571" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu"><em class="ms">#Resnet block</em></strong><br/><br/><strong class="np iu">def</strong> resnet_block(x, filters, reps, strides):<br/>    <br/>    x = projection_block(x, filters, strides)<br/>    <strong class="np iu">for</strong> _ <strong class="np iu">in</strong> range(reps-1):<br/>        x = identity_block(x,filters)<br/>    <strong class="np iu">return</strong> x</span><span id="487b" class="nt lw it np b gy oa nv l nw nx"><strong class="np iu"><em class="ms">#Model</em></strong><br/><br/>input = Input(shape=(224,224,3))<br/><br/>x = conv_batchnorm_relu(input, filters=64, kernel_size=7, strides=2)<br/>x = MaxPool2D(pool_size = 3, strides =2)(x)<br/>x = resnet_block(x, filters=64, reps =3, strides=1)<br/>x = resnet_block(x, filters=128, reps =4, strides=2)<br/>x = resnet_block(x, filters=256, reps =6, strides=2)<br/>x = resnet_block(x, filters=512, reps =3, strides=2)<br/>x = GlobalAvgPool2D()(x)<br/><br/>output = Dense(1000, activation ='softmax')(x)<br/><br/>model = Model(inputs=input, outputs=output)<br/>model.summary()</span></pre></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="85af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">参考文献:</strong></p><ol class=""><li id="ed22" class="mz na it la b lb lc le lf lh nb ll nc lp nd lt nn nf ng nh bi translated">何、、任、，深度残差学习用于图像识别，2015，<a class="ae lu" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"><br/>arXiv:1512.03385</a><strong class="la iu">【cs .简历] </strong></li></ol></div></div>    
</body>
</html>