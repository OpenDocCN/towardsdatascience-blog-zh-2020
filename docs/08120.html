<html>
<head>
<title>All you need to know about PCA technique in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的 PCA 技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-pca-technique-in-machine-learning-443b0c2be9a1?source=collection_archive---------29-----------------------#2020-06-15">https://towardsdatascience.com/all-you-need-to-know-about-pca-technique-in-machine-learning-443b0c2be9a1?source=collection_archive---------29-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d767" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最广泛使用的降维技术的详细解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c0b42ebc60d8f01a3f1529fac5125efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcr4o3ueq7E6vS37bwMzKw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">王占山在 Unsplash 上拍摄的照片</p></figure><p id="37f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">有没有人遇到过这样的情况，在建立模型时处理大量的变量，并担心准确性和效率低下？如果是这样，主成分分析(PCA)将会来救你</em> ✌️</p><p id="39e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">什么是主成分分析？</strong></p><p id="9cbd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它是最广泛使用的降维技术之一，通过识别相关性和模式将较大的数据集转换为较小的数据集，同时保留大部分有价值的信息。</p><p id="2569" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">对认证后活动的需求？</strong></p><p id="4ee8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它用于克服数据集中的要素冗余。此外，它还旨在获取有价值的信息，解释导致提供最佳准确性的高差异。它使得数据可视化易于处理。它降低了模型的复杂性，提高了计算效率。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2bf1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在选择主成分数量的概念背后有很多混乱。在本文中，我们将浏览所有步骤，并理解确定需要为数据集选择的组件数量的逻辑。</p><p id="2031" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用包含 8 个变量的披萨数据集。你可以在这里找到数据集<a class="ae mc" href="https://github.com/SushmithaPulagam/PCA" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7eaa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是披萨数据集的前几条记录。“品牌”为目标变量，其余为自变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/322682761da2aae8a8d0e4944c6760bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*V1AWMcrbiHj0mQo1V-cgIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">披萨数据集的前几条记录</p></figure><p id="c49f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">删除“品牌”列，因为我们只需要对独立变量进行 PCA。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/df5c3ae8ca327fbc5c0b28c630316de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*rjcfp_Vgf5rru7ZX7GUAGA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">去除“品牌”变量后</p></figure><p id="3246" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看这些步骤</p><p id="6cbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">第一步:数据标准化</strong></p><p id="99c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在进行 PCA 之前，我们需要对数据进行标准化。</p><p id="1102" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">执行标准化是至关重要的一步，因为原始变量可能有不同的标度。我们需要将它们带到相似的范围，以获得合理的协方差分析。</em></p><p id="5d43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从 sklearn 库中，我们可以使用下面的代码来标准化数据。</p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="0e94" class="mk ml it mg b gy mm mn l mo mp">from sklearn.preprocessing import StandardScaler<br/>df_std = StandardScaler().fit_transform(df)<br/>df_std</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/42868a2a555902386f196c64aab2c08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*Kho3wAL7xQbcazVUCO-SmA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用标准化后的输出</p></figure><p id="a4fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">步骤 2:用标准化数据计算协方差矩阵</strong></p><p id="1f4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">协方差矩阵表示两个变量之间的相关性。这有助于我们理解哪两个变量严重依赖于彼此，并捕捉数据集中的偏差和冗余。</em></p><p id="a146" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果矩阵中的条目带有负号，则意味着它们彼此成间接比例。如果符号是正的，意味着它们成正比。</p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="b1e5" class="mk ml it mg b gy mm mn l mo mp">df_cov_matrix = np.cov(df_std.T)<br/>df_cov_matrix</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8805c153815b7e51516c56f1c2fb02bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*VEfX8Dizj7HNc-iktvUuXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">协方差矩阵的输出</p></figure><p id="ab2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">步骤 3:计算协方差矩阵上的特征向量和特征值</strong></p><p id="d187" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两个代数公式总是成对计算，也称为特征分解，通过压缩数据来减少维数空间。主成分分析的核心就是建立在这些值之上的。</p><p id="01d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个特征向量将具有相应的特征值，并且所有特征值的总和表示整个数据集中的总体方差。计算特征值非常重要，因为它解释了数据集中最大方差的位置。</p><p id="8597" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要了解更多关于特征向量和特征值的信息，请访问此[ <a class="ae mc" href="https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47#:~:text=Eigenvectors%20and%20eigenvalues%20revolve%20around,to%20represent%20a%20large%20matrix." rel="noopener">链接</a></p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="07fd" class="mk ml it mg b gy mm mn l mo mp">eig_vals, eig_vecs = np.linalg.eig(df_cov_matrix)</span><span id="6709" class="mk ml it mg b gy ms mn l mo mp">print(‘Eigenvectors \n%s’ %eig_vecs)<br/>print(‘\nEigenvalues \n%s’ %eig_vals)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e5fa1b0858ddf1fd378abad49dd47c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*pPVjoqAGV43HiiMEodT9pA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征向量和特征值的输出</p></figure><p id="3199" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">步骤 4:按降序排列特征值列表</strong></p><p id="38ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成特征分解后，我们需要按降序排列特征值，其中第一个值是最重要的，从而形成我们的第一个主成分。</p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="e881" class="mk ml it mg b gy mm mn l mo mp">eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]</span><span id="4b87" class="mk ml it mg b gy ms mn l mo mp">print(‘Eigenvalues in descending order:’)<br/>for i in eig_pairs:<br/> print(i[0])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4316af62dbc4f4d4a6d178301c498302.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*NgEcauOVJwl6Pvvq1HJz7g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">排序特征值的输出</p></figure><p id="8926" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">第五步:选择主成分数</strong></p><blockquote class="mv"><p id="6eb8" class="mw mx it bd my mz na nb nc nd ne lt dk translated">第一个主成分将捕获原始变量的大部分方差，第二个主成分捕获第二高的方差，依此类推…</p></blockquote><pre class="nf ng nh ni nj mf mg mh mi aw mj bi"><span id="aa6f" class="mk ml it mg b gy mm mn l mo mp">total = sum(eig_vals)<br/>var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]<br/>cum_var_exp = np.cumsum(var_exp)<br/>print(“Variance captured by each component is \n”,var_exp)<br/>print(“Cumulative variance captured as we travel with each component \n”,cum_var_exp)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/7f2ba3351e19507620ef8c37e7a558f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bPBib2FIeMKBPJNVmaSXKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个组件捕获的差异的输出</p></figure><p id="a509" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面可以看出，第一个主成分(PC1)获得了总方差的 60%,其次是 PC2，方差为 32.7%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/4d5003494905124fbee93d8783b0e0ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1KU7plK6S-5329GbUK6pg.png"/></div></div></figure><p id="fcf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从累积差异来看，总体上 92%是由 2 个组件捕获的，98%的差异是由前 3 个组件解释的。因此，我们可以决定数据集的主成分数为 3。</p><p id="d477" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以通过下面的 scree 图用解释的方差比率的累积和来形象化同样的情况。</p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="123f" class="mk ml it mg b gy mm mn l mo mp">pca = PCA().fit(df_std)<br/>plt.plot(np.cumsum(pca.explained_variance_ratio_))<br/>plt.xlabel(‘No of components’)<br/>plt.ylabel(‘Cumulative explained variance’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/78d8acfa8c5a8c87cecc86f2c2f8459e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*tvWxr11N0Th-S5Bq7w6mdQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确定组件数量的 Scree 图</p></figure><p id="e0e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">步骤 6:创建主成分</strong></p><p id="3c6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过上面提到的所有步骤，考虑到最大方差，我们已经确定数据集所需的组件数量为 3。</p><pre class="kj kk kl km gt mf mg mh mi aw mj bi"><span id="706a" class="mk ml it mg b gy mm mn l mo mp">from sklearn.decomposition import PCA<br/>pca = PCA(n_components = 3)<br/>pcs = pca.fit_transform(df_std)<br/>df_new = pd.DataFrame(data=pcs, columns={‘PC1’,’PC2',’PC3'})<br/>df_new[‘target’] = df1[‘Brand’] <br/>df_new.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/698ad49fd44cd2df5b6d150c712c7d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*glGIezMYoMsSSRBKChRBdw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用主成分创建的新数据集的数据框架</p></figure><p id="17b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PCA 只能对数值变量进行。如果您有分类数据，那么您需要在应用 PCA 之前将其转换为数字特征。另外，请注意，这些主成分是原始数据集的线性组合。这些组件不像原始特征那样可读和可解释。</p><p id="05a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">总结</strong></p><p id="bc68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们已经经历了所有的步骤，以了解如何选择数据集所需的主成分的数量。应该记住，需要极其小心地选择组件，否则可能会丢失信息。</p><p id="6aca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您要处理数据集中的多重共线性问题，可以应用这种 PCA 技术，这将是一个很大的帮助。如果你想了解更多关于多重共线性的知识，请访问我以前的博客[ <a class="ae mc" rel="noopener" target="_blank" href="/how-to-detect-and-deal-with-multicollinearity-9e02b18695f1?source=friends_link&amp;sk=693ee8780b09f4a19a0f2d6ac33533a7">这里</a></p><p id="f290" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从我的 Github [ <a class="ae mc" href="https://github.com/SushmithaPulagam/PCA" rel="noopener ugc nofollow" target="_blank">这里</a> ]获得数据集和完整的代码</p><p id="a283" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读，快乐学习！🙂</p></div></div>    
</body>
</html>