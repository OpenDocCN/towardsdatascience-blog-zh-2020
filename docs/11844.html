<html>
<head>
<title>Principal Component Analysis explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-components-analysis-explained-33e329cec1c4?source=collection_archive---------33-----------------------#2020-08-16">https://towardsdatascience.com/principal-components-analysis-explained-33e329cec1c4?source=collection_archive---------33-----------------------#2020-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0332" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一些代数来理解 PCA 的工作原理🔧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/820ec7a5ed929ba4d3f2dc32bf450ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8k7Rr_FKkcE89WXmYYkjA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蒂姆·莫斯霍尔德在<a class="ae ky" href="https://unsplash.com/s/photos/car-engine?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f1ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> P </span>主成分分析(PCA)是机器学习(ML)中最著名的算法之一，它旨在<strong class="lb iu">降低你的数据的维度</strong>或者执行<strong class="lb iu">无监督聚类</strong>。毫无疑问，五氯苯甲醚在世界范围内广泛使用🌍，在从金融到生物学的任何操纵数据的领域。</p><p id="5045" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然有许多很好的资源给出了执行 PCA 的方法或对它所做的事情的很好的空间解释，但是很少有资源深入到它背后的数学概念的罩下。</p><p id="19cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然不需要理解数学就可以使用 PCA，但我坚信对算法的深刻理解会让你成为一个更好的用户，能够理解它在任何特定情况下的性能和缺点。此外，数学概念在 ML 中是相互联系的，理解 PCA 可以帮助你理解其他使用代数的 ML 概念(出于好奇，请查看图 3。在帖子末尾的后经文部分)。</p><p id="3e9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章试图用背后的数学概念来解释不同的步骤。我假设读者已经熟悉代数基础。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c6c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们回顾一下 PCA 方法，快速回顾一下不同的相关步骤:</p><ol class=""><li id="2387" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">规范化你的数据，我们称之为规范化数据集<strong class="lb iu"> X </strong>。<strong class="lb iu"> X </strong>有<strong class="lb iu"> N </strong>行(示例)和<strong class="lb iu"> d </strong>列(维度)，是一个(N，d)矩阵</li><li id="60da" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">计算协方差矩阵<strong class="lb iu">σ</strong>的<strong class="lb iu">X</strong><br/><strong class="lb iu">σ</strong>是一个(d，d)矩阵</li><li id="a321" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">计算<strong class="lb iu">σ</strong>的特征向量和特征值</li><li id="fbd3" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">对特征值最大的 k 个特征向量进行排序(这是 k 个主成分)，做<strong class="lb iu"> W </strong>，这是一个(d，k)矩阵</li><li id="b21d" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">将原始数据集<strong class="lb iu"> X </strong>投影到由步骤 4 排序的 k 个特征向量构成的低维空间上，即<strong class="lb iu">W</strong><br/><strong class="lb iu">X’= XW</strong>……<strong class="lb iu">X’</strong>准备就绪👨‍🍳！<strong class="lb iu">X’</strong>现在是一个(N，k)矩阵</li></ol><p id="cfe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于 k<strong class="lb iu">k&lt;d</strong>，我们摆脱了之前袭击我们🗡️的维度诅咒🐉</p><p id="3ea4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太酷了！但是，采用协方差矩阵并以某种方式将其与特征向量分解混合，如何能够在低维空间中捕获我们的数据集呢！？</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要回答这个问题，是时候把手弄脏，看看 PCA 发动机的引擎盖下到底是怎么回事了！🛠️</p><p id="e89c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主成分分析的目标是将原始数据集投影到一个低维空间，该低维空间捕获原始数据集的大部分信息。</strong>因此，我们应该问自己的第一个问题是:</p><blockquote class="mz"><p id="2d1c" class="na nb it bd nc nd ne nf ng nh ni lu dk translated">什么获取了数据集的大部分信息？</p></blockquote><p id="9d91" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">PCA 背后的基本关系是，<strong class="lb iu">方差捕获了数据集的大部分信息</strong>。方差(离差)越大，信息就越多。因此，PCA 旨在找到一个保持最大 T4 方差的低维空间。</p><p id="8028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，第一个归一化步骤旨在归一化原始数据集的每个维度的范围，使每个维度的方差具有可比性。这里没有什么太疯狂的🙏🏻</p><p id="79e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了！但是现在我们明白了<strong class="lb iu">最大方差</strong>是在低维空间中保留原始数据集的大部分信息的关键，我们可以问:</p><blockquote class="mz"><p id="60f5" class="na nb it bd nc nd ne nf ng nh ni lu dk translated">如何找到一个保持方差最大的低维空间？</p></blockquote><p id="62e0" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">为了实现这一点，我们可以简单地从寻找向量<strong class="lb iu"> u </strong>开始，当我们的数据集被投影到该向量上时，该向量捕获了最大的方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/0839ddf3dc5d01afa5435fca64a97566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBUZ9KU2eCqemr6RsRoViQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。具有最大特征值的特征向量指向最大方差的方向</p></figure><p id="5c29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们刚刚说明了协方差矩阵σ的最大特征值的<strong class="lb iu">特征向量指向最大方差方向</strong>并且这个方向的方差为<strong class="lb iu"> λ </strong>。</p><p id="f087" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以将我们的原始数据集<strong class="lb iu"> X </strong>投影到 1 轴上，即向量<strong class="lb iu"> u </strong>，这保留了其原始方差的最大值。但是，也许我们想再找一个轴，或者 k 个轴..？</p><p id="5152" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没关系！我们简单地重复图 1 所示的相同过程。以上<strong class="lb iu"> k 次</strong>，每次增加一个新的约束:先前最大的(关于它们相关的特征值)特征向量必须与新的候选向量<strong class="lb iu">正交</strong>。</p><p id="824a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们已经证明了协方差矩阵的<strong class="lb iu">特征向量指向最大方差方向</strong>，并且它们相关的特征值等于沿着这些方向的方差，正如图 2 所示。说明了它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7b43a7ee8bd98e8781fbe4b1ed43ebe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*p-vzf7W01S3JHUo1bzpm6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。协方差矩阵的特征向量指向最大方差方向[ <a class="ae ky" href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="a1ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，为什么我们选择具有最高特征值的 k 个特征向量来保留原始数据集的最大方差是显而易见的。</p><p id="8628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊！👌</p><p id="cdfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">资源:</strong></p><ul class=""><li id="5339" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu nq mr ms mt bi translated">有用的<a class="ae ky" href="https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component" rel="noopener ugc nofollow" target="_blank">主成分和特征向量的堆叠交换</a></li><li id="396e" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu nq mr ms mt bi translated">关于<a class="ae ky" href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/" rel="noopener ugc nofollow" target="_blank">协方差矩阵的几何解释</a>的帖子不错</li><li id="0287" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu nq mr ms mt bi translated">完成多伦多大学关于拉格朗日乘数和 PCA 用例的<a class="ae ky" href="http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/LagrangeMultipliers.pdf" rel="noopener ugc nofollow" target="_blank">课程</a></li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="b91c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://l" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> PS </strong> </a>:在神经网络优化中，计算损失函数的<strong class="lb iu"> Hessian 矩阵</strong>是有用的，以便帮助梯度下降步骤适应关于其方向的学习速率。简而言之，Hessian 展示了损失函数的梯度在任何方向上的变化有多快。</p><p id="182f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用我们在 PCA 中看到的相同代数概念，现在有意义的是，Hessian 矩阵的特征向量指向最大曲率的方向，并且它们相关的特征值表示函数的曲率<strong class="lb iu">在该方向。因此，在梯度下降更新期间，高曲率(高特征值)方向需要较低的学习速率，因为梯度在该方向上变化更快。<br/>反之亦然，低曲率方向的学习率更高。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c08678c3445ff525109a5b55721c6f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*bXd-nI1PfPfrzZriZkc4FA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。特征向量或 Hessian 矩阵指向最大曲率方向[ <a class="ae ky" href="https://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div></div>    
</body>
</html>