<html>
<head>
<title>Algorithms from Scratch: Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的算法:决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-decision-tree-1898d37b02e0?source=collection_archive---------21-----------------------#2020-07-24">https://towardsdatascience.com/algorithms-from-scratch-decision-tree-1898d37b02e0?source=collection_archive---------21-----------------------#2020-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/539b4a6881facf439fcd364d65ff0132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XSlkeE3lKJynfQHz"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">约翰·西门子在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="b81a" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="6f87" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">从头开始详述和构建决策树模型</h2></div><p id="97dc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">熟悉我早期作品的人会记得，我曾经写过一篇关于随机森林算法的概述。决策树的坚实基础是理解随机森林内部工作的先决条件；随机森林构建多个决策树，并输出回归问题中每棵树预测的平均值，在分类问题中，它输出每棵树预测的相对多数。</p><div class="is it gp gr iu md"><a rel="noopener follow" target="_blank" href="/random-forest-overview-746e7983316"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">随机森林概述</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">随机森林的概念概述</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="5d4e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于上面的故事，我将更加关注决策树学习算法，因为它是机器学习领域的一个基本算法。许多模型的结构基于决策树模型，如随机森林和梯度提升树。此外，我将从头开始做这个算法的 Python 实现，以进一步扩展我们对算法中发生的事情的直觉。</p><h2 id="4220" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">定义术语</h2><ul class=""><li id="0d3d" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc nr ns nt nu bi translated"><strong class="lj jt">参数模型</strong>:用于对总体参数进行推断，但是如果所有假设都不满足，这些推断就无效。</li><li id="4869" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><strong class="lj jt">非参数模型</strong>:不假设数据或总体有任何特征结构。</li></ul><h2 id="fe08" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">决策树(购物车)</h2><p id="e682" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq oa ls lt lu ob lw lx ly oc ma mb mc im bi translated">由于其可理解性和简单性而广受欢迎，决策树是最容易可视化和解释的算法之一，在向非技术观众展示结果时非常方便，这是行业中经常需要的。如果我们简单地考虑一棵处于类似流程图状态的树，从根到叶，其中从根到叶的路径定义了关于特征的决策规则，那么我们已经有了理解决策树学习所需的良好的直觉水平。</p><p id="e029" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与我们在从零开始的<a class="ae jg" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">算法</a>系列中介绍的前两种算法(<a class="ae jg" rel="noopener" target="_blank" href="/algorithms-from-scratch-linear-regression-c654353d1e7c">线性回归</a>和<a class="ae jg" rel="noopener" target="_blank" href="/algorithms-from-scratch-logistic-regression-7bacdfd9738e">逻辑回归</a>)不同，决策树算法是一种非参数算法，这意味着它不对数据或总体做出假设。这确实对我们的模型有影响，因为我们在训练期间在模型中用偏差换取方差，使得决策树更容易过度拟合。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="c6f5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在机器学习领域，有两种主要的<strong class="lj jt">决策树模型。我们使用的方法取决于我们试图预测的目标变量的类型:</strong></p><p id="7dc1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ok translated"><span class="l ol om on bm oo op oq or os di"> C </span> <strong class="lj jt">分类树</strong>:用于预测取离散值的目标变量的树模型。因此，叶节点代表一个类，而分支代表导致这些类标签的特征的合取。</p><p id="a5f4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ok translated"><span class="l ol om on bm oo op oq or os di">R</span>T14】回归树:用于预测取连续值的目标变量的树模型。与分类树相反，在回归树中，每个叶节点包含一个连续值(即房价)；分支代表导致每个连续变量的特征的合取。</p><blockquote class="ot ou ov"><p id="5e1a" class="lh li ow lj b lk ll kt lm ln lo kw lp ox lr ls lt oy lv lw lx oz lz ma mb mc im bi translated"><strong class="lj jt">注</strong>:分类和回归树(CART)是指这两个过程的总称，由 Breiman 等人于 1984 年首次提出。</p></blockquote><p id="fd06" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图 1 的<em class="ow">中，我们可以看到 CART 算法遵循的结构。尽管这种结构是为两棵树设置的，但是分类树和回归树之间还是有一些细微的差别，比如每棵树的输出；<strong class="lj jt">分类树</strong>返回叶节点的模式类别，而<strong class="lj jt">回归树</strong>返回平均值。</em></p><figure class="pb pc pd pe gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/09c9847b9a5770ae41b84cc69e730361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VrKiaotszRLgEnPjdFVmlQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 1:决策树图表。</p></figure><p id="b2e9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这两种算法之间的另一个显著区别是我们在划分特征空间时试图最小化的标准。一般来说，我们希望选择特征<strong class="lj jt"> j </strong>和分割点<strong class="lj jt"> s </strong>，它们能够最好地将特征空间分割成两个区域，但是如何在回归树和分类树中测量这一点是不同的，如图 2<em class="ow">所示。</em></p><figure class="pb pc pd pe gt iv gh gi paragraph-image"><div class="ab gu cl pf"><img src="../Images/f95412b5ed027b5be3265981dbfde70e.png" data-original-src="https://miro.medium.com/v2/format:webp/0*sQ5-X-4W2pCUIOls.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 sklearn 中使用的杂质公式。[来源:<a class="ae jg" href="https://medium.com/u/60a50d133053?source=post_page-----746e7983316----------------------" rel="noopener">Stacey Ronaghan</a>——<a class="ae jg" rel="noopener" target="_blank" href="/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3">Scikit-learn 和 Spark </a>中的决策树、随机森林和特性重要性的数学</p></figure><h2 id="f2ba" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">分块算法</h2><blockquote class="ot ou ov"><p id="50a9" class="lh li ow lj b lk ll kt lm ln lo kw lp ox lr ls lt oy lv lw lx oz lz ma mb mc im bi translated"><strong class="lj jt">注意:</strong>我们将构建一个决策树分类器，以基尼系数作为分割的标准<strong class="lj jt">。</strong></p></blockquote><ol class=""><li id="1e75" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">考虑特征 j 和分割点 s 的所有可能分割</li><li id="3e98" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">找到最佳分割后，将数据分成两个结果区域</li><li id="4577" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">重复 1 和 2，直到达到停止标准</li></ol><p id="2aaa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的伪代码演示了计算机科学中称为递归的现象:一种解决问题的方法，其中解决方案取决于同一问题的较小实例的解决方案(来源:<a class="ae jg" href="https://en.wikipedia.org/wiki/Recursion_(computer_science)#:~:text=Recursion%20in%20computer%20science%20is%20a%20method%20of,one%20of%20the%20central%20ideas%20of%20computer%20science." rel="noopener ugc nofollow" target="_blank"> Wikipedia </a>)，以及<a class="ae jg" href="https://en.wikipedia.org/wiki/Binary_splitting" rel="noopener ugc nofollow" target="_blank">二进制分裂</a>因此在一些示例中，步骤 1-2 被称为递归二进制分裂。</p><h2 id="70f9" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">履行</h2><p id="f527" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq oa ls lt lu ob lw lx ly oc ma mb mc im bi translated">对于此次实施，我们将利用以下框架:</p><ul class=""><li id="6325" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc nr ns nt nu bi translated">线性代数和数据处理</li><li id="ab94" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">熊猫(数据处理)</li><li id="56a5" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">Sci-kit 学习(机器学习)</li><li id="b655" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">图形可视化软件</li></ul><p id="22ef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">点击此处查看完整代码…</p><div class="is it gp gr iu md"><a href="https://github.com/kurtispykes/ml-from-scratch/blob/master/decision_tree.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">kurtispykes/ml-从零开始</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="pk l mo mp mq mm mr ja md"/></div></div></a></div><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="df3c" class="ms mt jj pm b gy pq pr l ps pt">import numpy as np <br/>import pandas as pd <br/>import graphviz<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.datasets import load_iris<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.tree import export_graphviz<br/>from sklearn.model_selection import train_test_split</span></pre><p id="cd75" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用的数据集是来自 Scikit learn 的虹膜数据集—参见<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html" rel="noopener ugc nofollow" target="_blank">文档</a></p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="155d" class="ms mt jj pm b gy pq pr l ps pt"># loading the data set<br/>dataset = load_iris(as_frame=True)<br/>df= pd.DataFrame(data= dataset.data)<br/><br/># adding the target and target names to dataframe<br/>target_zip= dict(zip(set(dataset.target), dataset.target_names))<br/>df["target"] = dataset.target<br/>df["target_names"] = df["target"].map(target_zip)<br/><br/>print(df.shape)<br/>df.head()</span><span id="b793" class="ms mt jj pm b gy pu pr l ps pt">(150, 6)</span></pre><figure class="pb pc pd pe gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pv"><img src="../Images/c4a5c7f0d649707f37f5184c05a6579b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBNAbOPDWz_JldkE9M5bhg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 3:虹膜数据集(前 5 行)</p></figure><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="ee95" class="ms mt jj pm b gy pq pr l ps pt"># Seperating to X and Y <br/>X = df.iloc[:, :4]<br/>y = df.iloc[:, -1]<br/><br/># splitting training and test<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=True, random_state=24)</span><span id="4985" class="ms mt jj pm b gy pu pr l ps pt">dt = DecisionTreeClassifier()<br/>dt.fit(X_train, y_train)</span><span id="eecf" class="ms mt jj pm b gy pu pr l ps pt">DecisionTreeClassifier()</span><span id="94e2" class="ms mt jj pm b gy pu pr l ps pt">dot_data = export_graphviz(dt, out_file=None, <br/>                           feature_names=X.columns,  <br/>                           class_names=dataset.target_names,  <br/>                           filled=True, rounded=True,  <br/>                           special_characters=True)  <br/>graph = graphviz.Source(dot_data)  <br/>graph</span></pre><figure class="pb pc pd pe gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pw"><img src="../Images/3d2ec698da582353c391366468c9fb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fXY_Iqb2kjB9H7wYVB7VOg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 4:经过训练的决策树可视化——上面代码单元的输出。</p></figure><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="7d6a" class="ms mt jj pm b gy pq pr l ps pt">sklearn_y_preds = dt.predict(X_test)<br/>print(f"Sklearn Implementation:\nACCURACY: {accuracy_score(y_test, sklearn_y_preds)}")</span><span id="7771" class="ms mt jj pm b gy pu pr l ps pt">Sklearn Implementation:<br/>ACCURACY: 0.9473684210526315</span></pre><h1 id="acd8" class="px mt jj bd mu py pz qa mx qb qc qd na ky qe kz nd lb qf lc ng le qg lf nj qh bi translated">我的实现</h1><p id="f5d7" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq oa ls lt lu ob lw lx ly oc ma mb mc im bi translated">我们将需要根据特定分支的决策规则，将我们的数据分为真索引和假索引。如果满足决策规则的条件，我们说该分支为真(我们将表示为<code class="fe qi qj qk pm b">left</code>)和假(表示为<code class="fe qi qj qk pm b">right</code>)。</p><h2 id="2c4e" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">划分数据</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="85bf" class="ms mt jj pm b gy pq pr l ps pt">def partition(data, column, value): <br/>    """<br/>    Partition the data into left (indicating True) and right (indicating false).<br/>    <br/>    Inputs<br/>    data: The data to partition <br/>    <br/>    Outputs<br/>    left: index of values that meet condition<br/>    right: index of values that fail to meet the condition<br/>    """<br/>    left = data[data[column] &lt;= value].index<br/>    right = data[data[column] &gt; value].index<br/><br/>    return left, right</span></pre><p id="9a23" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了检查我们的函数是否正常工作，我们将对所有数据执行拆分，并手动将最佳列和值传递给它，以查看我们的数据是否相对于上图进行了分离。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="fc84" class="ms mt jj pm b gy pq pr l ps pt"># performing a split on the root node<br/>left_idx, right_idx = partition(X_train, "petal length (cm)", 2.45)<br/><br/>print("[petal length (cm) &lt;= 2.45]")<br/><br/># print results --&gt; left_idx = 38 setosa | right index = 42 versicolor, 32 virginica <br/>print(f"left_idx: {dict(zip(np.unique(y_train.loc[left_idx], return_counts=True)[0], np.unique(y_train.loc[left_idx], return_counts=True)[1]))}\n\<br/>right_idx: {dict(zip(np.unique(y_train.loc[right_idx], return_counts=True)[0], np.unique(y_train.loc[right_idx], return_counts=True)[1]))}")</span><span id="aa80" class="ms mt jj pm b gy pu pr l ps pt">[petal length (cm) &lt;= 2.45]<br/>left_idx: {'setosa': 38}<br/>right_idx: {'versicolor': 42, 'virginica': 32}</span></pre><p id="9c77" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">完美！</p><p id="bb5a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们需要一个我们想要最小化的标准。在这个实现中，我们将使用一个名为<code class="fe qi qj qk pm b">gini_impurity</code>的函数来最小化基尼系数。在不触及技术层面的情况下，基尼系数只是简单地衡量了我们的数据在某个节点上的混合程度；为了帮助理解这个概念，想想黄金。当黄金不纯时，它指的是其中其他物质的混合物，然而当它纯时，我们可以说没有杂质(这并不完全正确，因为精炼黄金纯度高达 99.99%，所以技术上仍有一些杂质)。</p><p id="26a5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">理想情况是节点是纯的，这意味着目标标签被分成单独的节点。—要深入了解基尼系数杂质的技术细节，请参见<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">此处</a></p><h2 id="4dc7" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">基尼杂质</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="185e" class="ms mt jj pm b gy pq pr l ps pt">def gini_impurity(label, label_idx):<br/>    """<br/>    A measure of how often a randomly chosen element from the set would<br/>    be incorrectly labelled if it was randomly labelled according to the <br/>    distribution of labels in the subset (Soure: Wikipedia)<br/>    <br/>    Inputs<br/>    label: The class label available at current node<br/>    <br/>    Outputs<br/>    impurity: The gini impurity of the node <br/>    """<br/>    # the unique labels and counts in the data<br/>    unique_label, unique_label_count = np.unique(label.loc[label_idx], return_counts=True)<br/><br/>    impurity = 1.0<br/>    for i in range(len(unique_label)):<br/>        p_i = unique_label_count[i] / sum(unique_label_count)<br/>        impurity -= p_i ** 2 <br/>    return impurity</span></pre><p id="44e5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你向上滚动回到<em class="ow">图 4，</em>你会看到在根节点的杂质是 0.663。因此，为了确定我们的<code class="fe qi qj qk pm b">gini_impurity</code>函数是否正常工作，我们应该在输出中看到这个数字</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="208f" class="ms mt jj pm b gy pq pr l ps pt"># Gini impurity of the first node<br/>impurity = gini_impurity(y_train, y_train.index)<br/>impurity`</span><span id="0a04" class="ms mt jj pm b gy pu pr l ps pt">0.6626275510204082</span></pre><p id="03bc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">太好了！</p><p id="13ac" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了在一个特性(和价值)上分裂，我们需要一种量化的方法，如果我们在那个点上分裂，什么会产生最好的结果。信息增益是量化在每个节点上分割什么特征和特征值的有用方法。对于树的每个节点，信息值“表示在给定实例到达该节点的情况下，指定新实例应该被分类为是还是否所需的预期信息量”。(来源:<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p><h2 id="fba4" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">信息增益</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="3d97" class="ms mt jj pm b gy pq pr l ps pt">def information_gain(label, left_idx, right_idx, impurity): <br/>    """<br/>    For each node of the tree, the information gain "represents the<br/>    expected amount of information that would be needed to specify whether<br/>    a new instance should be classified yes or no, given that the example<br/>    reached that node. (Source: Wikipedia)<br/>    <br/>    Inputs<br/>    left: The values that met the conditions of the current node<br/>    right: The values that failed to meet the conditions of the current noode<br/>    gini_impurity: the uncertainty at the current node<br/>    <br/>    Outputs<br/>    info_gain: The information gain at the node<br/>    """<br/>    <br/>    p = float(len(left_idx)) / (len(left_idx) + len(right_idx))<br/>    info_gain = impurity - p * gini_impurity(label, left_idx) - (1 - p) * gini_impurity(label, right_idx)<br/>    return info_gain</span></pre><p id="e588" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最佳的第一分割是提供最多信息增益的分割。对每个不纯节点重复这个过程，直到树完成。(来源:<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p><p id="4f8c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于上面的陈述，我们现在可以明白为什么值为 2.45 的花瓣长度(cm)被选为第一个分割。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="49d0" class="ms mt jj pm b gy pq pr l ps pt"># testing info gain of the first split at root node<br/>info_gain = information_gain(y_train, left_idx, right_idx, impurity)<br/>info_gain</span><span id="eb35" class="ms mt jj pm b gy pu pr l ps pt">0.33830322669608387</span><span id="b4cd" class="ms mt jj pm b gy pu pr l ps pt"># testing a random feature and value to see the information gain<br/>left_idx, right_idx = partition(X_train, "petal width (cm)", 1.65)<br/><br/>impurity = gini_impurity(y_train, y_train.index)<br/><br/>info_gain = information_gain(y_train, left_idx, right_idx, impurity)<br/>info_gain</span><span id="c498" class="ms mt jj pm b gy pu pr l ps pt">0.25446843371494937</span></pre><p id="7db4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以上助手功能现在要发挥作用了。我们必须手动选择特性和值，对吗？下一个函数现在将自动搜索特征空间，并找到最佳分割数据的特征和特征值。</p><h2 id="d367" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">寻找最佳分割</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="8fb0" class="ms mt jj pm b gy pq pr l ps pt">def find_best_split(df, label, idx):<br/>    """<br/>    Splits the data on the best column and value <br/>    <br/>    Input<br/>    df: the training data<br/>    label: the target label <br/>    idx: the index of the data<br/>    <br/>    Output: <br/>    best_gain: the max information gain<br/>    best_col: the column that produced best information gain<br/>    best_val: the value of the column that produced best information gain<br/>    <br/>    """<br/>    <br/>    best_gain = 0 <br/>    best_col = None<br/>    best_value = None<br/>    <br/>    df = df.loc[idx] # converting training data to pandas dataframe<br/>    label_idx = label.loc[idx].index # getting the index of the labels<br/><br/>    impurity = gini_impurity(label, label_idx) # determining the impurity at the current node<br/>    <br/>    # go through the columns and store the unique values in each column (no point testing on the same value twice)<br/>    for col in df.columns: <br/>        unique_values = set(df[col])<br/>        # loop thorugh each value and partition the data into true (left_index) and false (right_index)<br/>        for value in unique_values: <br/><br/>            left_idx, right_idx = partition(df, col, value)<br/>            # ignore if the index is empty (meaning there was no features that met the decision rule)<br/>            if len(left_idx) == 0 or len(right_idx) == 0: <br/>                continue <br/>            # determine the info gain at the node<br/>            info_gain = information_gain(label, left_idx, right_idx, impurity)<br/>            # if the info gain is higher then our current best gain then that becomes the best gain<br/>            if info_gain &gt; best_gain:<br/>                best_gain, best_col, best_value = info_gain, col, value<br/>                <br/>    return best_gain, best_col, best_value</span><span id="593c" class="ms mt jj pm b gy pu pr l ps pt">find_best_split(X_train, y_train, y_train.index)</span><span id="8e14" class="ms mt jj pm b gy pu pr l ps pt">(0.33830322669608387, 'petal length (cm)', 1.9)</span></pre><p id="52b6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">太好了，我们有了算法运行所需的所有组件。然而，上面的函数只对我们的训练数据(树桩/树根)执行一次分割。</p><h2 id="4429" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">附加助手功能</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="aa39" class="ms mt jj pm b gy pq pr l ps pt"># helper function to count values<br/>def count(label, idx):<br/>    """<br/>    Function that counts the unique values<br/>    <br/>    Input<br/>    label: target labels<br/>    idx: index of rows <br/>    <br/>    Output<br/>    dict_label_count: Dictionary of label and counts<br/>    <br/>    """<br/>    unique_label, unique_label_counts = np.unique(label.loc[idx], return_counts=True)<br/>    dict_label_count = dict(zip(unique_label, unique_label_counts))<br/>    return dict_label_count</span><span id="774f" class="ms mt jj pm b gy pu pr l ps pt"># check counts at first node to check it aligns with sci-kit learn<br/>count(y_train, y_train.index)</span><span id="e6be" class="ms mt jj pm b gy pu pr l ps pt">{'setosa': 38, 'versicolor': 42, 'virginica': 32}</span></pre><p id="129e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有一些类，我们将使用它们来存储决策树中的特定数据并打印我们的树。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="fadd" class="ms mt jj pm b gy pq pr l ps pt"># <a class="ae jg" href="https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb</a></span><span id="d98e" class="ms mt jj pm b gy pu pr l ps pt">class Leaf:<br/>    """<br/>    A Leaf node classifies data.<br/><br/>    This holds a dictionary of class (e.g., "Apple") -&gt; number of times<br/>    it appears in the rows from the training data that reach this leaf.<br/>    """<br/><br/>    def __init__(self, label, idx):<br/>        self.predictions = count(label, idx)</span><span id="7c2a" class="ms mt jj pm b gy pu pr l ps pt"># <a class="ae jg" href="https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb</a></span><span id="882d" class="ms mt jj pm b gy pu pr l ps pt">class Decision_Node:<br/>    """<br/>    A Decision Node asks a question.<br/><br/>    This holds a reference to the question, and to the two child nodes.<br/>    """<br/><br/>    def __init__(self,<br/>                 column,<br/>                 value,<br/>                 true_branch,<br/>                 false_branch):<br/>        self.column = column<br/>        self.value = value<br/>        self.true_branch = true_branch<br/>        self.false_branch = false_branch</span><span id="c768" class="ms mt jj pm b gy pu pr l ps pt"># <a class="ae jg" href="https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb</a></span><span id="dc6f" class="ms mt jj pm b gy pu pr l ps pt">def print_tree(node, spacing=""):<br/>    """<br/>    World's most elegant tree printing function.<br/>    <br/>    Input<br/>    node: the tree node<br/>    spacing: used to space creating tree like structure<br/>    """<br/><br/>    # Base case: we've reached a leaf<br/>    if isinstance(node, Leaf):<br/>        print (spacing + "Predict", node.predictions)<br/>        return<br/><br/>    # Print the col and value at this node<br/>    print(spacing + f"[{node.column} &lt;= {node.value}]")<br/>    <br/><br/>    # Call this function recursively on the true branch<br/>    print (spacing + '--&gt; True:')<br/>    print_tree(node.true_branch, spacing + "  ")<br/><br/>    # Call this function recursively on the false branch<br/>    print (spacing + '--&gt; False:')<br/>    print_tree(node.false_branch, spacing + "  ")</span></pre><h2 id="4fd4" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">递归二进制分裂</h2><p id="96ca" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq oa ls lt lu ob lw lx ly oc ma mb mc im bi translated">为了使算法工作，我们将需要分裂递归地发生，直到我们满足停止标准——在这种情况下，直到每个叶节点都是纯的。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="1416" class="ms mt jj pm b gy pq pr l ps pt">def build_tree(df, label, idx): <br/>    """<br/>    Recursively Builds the tree until is leaf is pure. <br/>    <br/>    Input <br/>    df: the training data<br/>    label: the target labels<br/>    idx: the indexes <br/>    <br/>    Output<br/>    best_col: the best column <br/>    best_value: the value of the column that minimizes impurity <br/>    true_branch: the true branch <br/>    false_branch: the false branch <br/>    """<br/>    best_gain, best_col, best_value = find_best_split(df, label, idx)<br/>    <br/>    if best_gain == 0: <br/>        return Leaf(label, label.loc[idx].index)<br/>    <br/>    left_idx, right_idx = partition(df.loc[idx], best_col, best_value)<br/>    <br/>    true_branch = build_tree(df, label, left_idx)<br/>    <br/>    false_branch = build_tree(df, label, right_idx)<br/>    <br/>    return Decision_Node(best_col, best_value, true_branch, false_branch)</span><span id="0648" class="ms mt jj pm b gy pu pr l ps pt">my_tree = build_tree(X_train, y_train, X_train.index)</span><span id="f242" class="ms mt jj pm b gy pu pr l ps pt">print_tree(my_tree)</span><span id="d400" class="ms mt jj pm b gy pu pr l ps pt">[petal length (cm) &lt;= 1.9]<br/>--&gt; True:<br/>  Predict {'setosa': 38}<br/>--&gt; False:<br/>  [petal width (cm) &lt;= 1.6]<br/>  --&gt; True:<br/>    [petal length (cm) &lt;= 4.9]<br/>    --&gt; True:<br/>      Predict {'versicolor': 40}<br/>    --&gt; False:<br/>      [sepal length (cm) &lt;= 6.0]<br/>      --&gt; True:<br/>        [sepal width (cm) &lt;= 2.2]<br/>        --&gt; True:<br/>          Predict {'virginica': 1}<br/>        --&gt; False:<br/>          Predict {'versicolor': 1}<br/>      --&gt; False:<br/>        Predict {'virginica': 2}<br/>  --&gt; False:<br/>    [petal length (cm) &lt;= 4.8]<br/>    --&gt; True:<br/>      [sepal width (cm) &lt;= 3.0]<br/>      --&gt; True:<br/>        Predict {'virginica': 3}<br/>      --&gt; False:<br/>        Predict {'versicolor': 1}<br/>    --&gt; False:<br/>      Predict {'virginica': 26}</span></pre><p id="11fe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">超级！现在，您已经看到了如何从头开始实现决策树，并且我们已经根据我们的训练数据对其进行了训练。然而，它并没有就此停止，首先构建算法的目的是自动对新的观察结果进行分类。下一节将致力于推理…</p><h2 id="4af7" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">推理</h2><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="9838" class="ms mt jj pm b gy pq pr l ps pt">def predict(test_data, tree):<br/>    <br/>    """<br/>    Classify unseen examples<br/>    <br/>    Inputs<br/>    test_data: Unseen observation<br/>    tree: tree that has been trained on training data<br/>    <br/>    Output<br/>    The prediction of the observation.<br/>    """<br/>    <br/>    # Check if we are at a leaf node<br/>    if isinstance(tree, Leaf): <br/>        return max(tree.predictions)<br/>    <br/>    # the current feature_name and value <br/>    feature_name, feature_value = tree.column, tree.value<br/>    <br/>    # pass the observation through the nodes recursively<br/>    if test_data[feature_name] &lt;= feature_value: <br/>        return predict(test_data, tree.true_branch)<br/>    <br/>    else: <br/>        return predict(test_data, tree.false_branch)</span></pre><p id="74f1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了检查我们的函数是否正确运行，我将使用一个观察示例。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="2167" class="ms mt jj pm b gy pq pr l ps pt"># taking one instance to test function <br/>example, example_target = X_test.iloc[6], y_test.iloc[6]<br/>example, example_target</span><span id="4ef2" class="ms mt jj pm b gy pu pr l ps pt">(sepal length (cm)       5.3<br/> sepal width (cm)        3.7<br/> petal length (cm)       1.5<br/> petal width (cm)        0.2<br/> predictions          setosa<br/> Name: 48, dtype: object,<br/> 'setosa')</span></pre><p id="e65d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当我们将我们的<code class="fe qi qj qk pm b">predict</code>函数应用到示例中时，我们应该希望观察相应地遍历树并输出<code class="fe qi qj qk pm b">setosa</code>，让我们检查…</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="d30f" class="ms mt jj pm b gy pq pr l ps pt"># if working correctly should output setosa<br/>predict(example, my_tree)</span><span id="af8e" class="ms mt jj pm b gy pu pr l ps pt">'setosa'</span></pre><p id="fee6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">至高无上！！然而，这只是一个例子。如果我们想将这个函数应用到我们测试集中的每一个观察值，我们可以使用<code class="fe qi qj qk pm b">df.apply</code>——参见<a class="ae jg" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html" rel="noopener ugc nofollow" target="_blank">文档</a></strong></p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="6d62" class="ms mt jj pm b gy pq pr l ps pt"># create a new col of predictions<br/>X_test["predictions"] = X_test.apply(predict, axis=1, args=(my_tree,))</span></pre><p id="c8df" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好了，关键时刻到了。我们需要检查我们的算法是否返回与 scikit 学习模型相同的预测，作为检查我们是否正确实现了我们的算法的一种方式。我们通过简单地执行<code class="fe qi qj qk pm b">sklearn_y_preds == X_true["predictions"]</code>来做到这一点，它为每个观察返回一个布尔数组——在我们的例子中，它们都是真的。</p><pre class="pb pc pd pe gt pl pm pn po aw pp bi"><span id="1ae5" class="ms mt jj pm b gy pq pr l ps pt">print(f"Sklearn Implementation:\nACCURACY: {accuracy_score(y_test, sklearn_y_preds)}\n\n\<br/>My Implementation:\nACCURACY: {accuracy_score(y_test, X_test['predictions'])}")</span><span id="3436" class="ms mt jj pm b gy pu pr l ps pt">Sklearn Implementation:<br/>ACCURACY: 0.9736842105263158<br/><br/>My Implementation:<br/>ACCURACY: 0.9736842105263158</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="9b69" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">赞成的意见</h2><ul class=""><li id="6073" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc nr ns nt nu bi translated">简单易懂</li><li id="6fef" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">能够处理数字和分类数据</li><li id="bb68" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">几乎不需要数据准备</li><li id="7118" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">适用于大型数据集</li><li id="704a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">内置功能选择</li></ul><h2 id="ba26" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">骗局</h2><ul class=""><li id="bc98" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc nr ns nt nu bi translated">树的不稳定性(改变数据中的某些东西可以改变一切)</li><li id="7db8" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">缺乏平滑度(回归问题的特殊问题)</li><li id="ee57" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated">倾向于过度拟合</li></ul></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="8a76" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">包裹</h2><p id="eef9" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq oa ls lt lu ob lw lx ly oc ma mb mc im bi translated">建立决策树的良好基础将有助于理解许多其他重要的机器学习算法。它是一种非常强大的算法，经常被用作集成模型来赢得各种数据科学比赛。虽然很容易概念化，但决策树很难从头开始构建，因此我总是主张尽可能使用已建立的机器学习框架。</p><p id="f3d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您阅读到文章结尾！如果你想和我保持联系，你可以在 LinkedIn 上找到我。</p><div class="is it gp gr iu md"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">Kurtis Pykes -人工智能作家-走向数据科学| LinkedIn</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">在世界上最大的职业社区 LinkedIn 上查看 Kurtis Pykes 的个人资料。Kurtis 有一个工作列在他们的…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.linkedin.com</p></div></div><div class="mm l"><div class="ql l mo mp mq mm mr ja md"/></div></div></a></div></div></div>    
</body>
</html>