<html>
<head>
<title>Recommendation system for similar content creators on YouTube</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YouTube 上相似内容创建者的推荐系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recommendation-system-for-similar-content-creators-on-youtube-3fed96b3c783?source=collection_archive---------34-----------------------#2020-08-12">https://towardsdatascience.com/recommendation-system-for-similar-content-creators-on-youtube-3fed96b3c783?source=collection_archive---------34-----------------------#2020-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d46c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用无监督的机器学习算法为任何给定的 YouTube 频道推荐相似的内容创建者</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9c2bfdaecbfa6f7cec3bbdaacf649fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Fl-pArbs5J4UqRD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://unsplash.com/@konkarampelas?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">今敏</strong> </a> <strong class="bd ky">上</strong> <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">下</strong> </a></p></figure><p id="b46e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">YouTube 是任何有空闲时间的人都会去的网站之一。从观看随机视频到学习新事物，YouTube 在我的生活中发挥了非常大的作用。我喜欢 YouTube 的一个原因是它拥有种类繁多的内容创作者。从播客到 ASMR 视频，YouTube 应有尽有。</p><p id="e519" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然 YouTube 有一个很棒的推荐系统，可以根据用户的历史和类似用户观看过的内容来推荐视频，但是，假设对于一个给定的 YouTube 频道名称，如果我们想知道其他 5 个类似的 YouTube 频道(根据它们创建的内容)，那么没有任何平台可以让我找到这些(而不必观看 YouTube 上的每个视频)。例如，如果我们更喜欢一个上传旅游博客的用户，比如说我想知道其他 5 个上传旅游博客的类似内容创建者，那么我将不得不根据他们创建的内容建立一个推荐系统，而不是依赖 YouTube 在我的 feed 或“up next”部分自动推荐他们，并观看每一个视频。下面是我试图建立一个简单的推荐系统来做到这一点-</p><p id="126b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">网上没有任何数据集包含你的土豆的名字和他们创造的内容。我们可以获得任何 YouTube 频道的视频内容细节的一种方法是抓取他们在视频中提供的字幕(因为他们在视频中输入的标签可能不会准确地了解视频中的内容，并且不是每个内容创作者都为他们的视频添加“标签”)。假设一个 YouTube 频道谈论或评论汽车，那么他们视频中的字幕将包含重复的关键词，如“马力”、“发动机”等。甚至像“大众”这样的品牌名称。对于这种方法，我们首先需要所有(或受欢迎的)YouTube 频道的名称，为了获得这些名称，我们可以使用这里的数据集<a class="ae kz" href="https://www.kaggle.com/datasnaek/youtube-new" rel="noopener ugc nofollow" target="_blank"/>。这个数据集包括几个月的 YouTube 每日趋势视频数据。数据分别来自美国、英国、德国、加拿大和法国(但我只使用了美国和加拿大，即 CAvideos.csv &amp; USvideos.csv)。让我们用 python 导入这些 CSV 文件，并将它们附加到一个数据帧中</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="80b1" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">import os<br/>import pandas as pd <br/>directory = os.getcwd()<br/>df_1 = pd.read_csv(directory+"/USvideos.csv")<br/>df_2 = pd.read_csv(directory+"/CAvideos.csv")<br/>df= df_1.append(df_2)<br/>df[["video_id","channel_title"]].head()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f9de61f7fd86141f19c4855b89597c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*Qn-Z0QvbX7jd-LNPsIKa7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">我们数据集中的前 5 行(作者图片)</strong></p></figure><p id="a6a1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用这个数据集只是为了获得 YouTube 频道的名称和那些出现在 YouTube 的趋势视频标签中的视频的视频 ID。一旦我们有了视频 ID 和 YouTube 频道名称，就可以使用<a class="ae kz" href="https://pypi.org/project/youtube-transcript-api/" rel="noopener ugc nofollow" target="_blank">YouTube-抄本-api </a>，我们可以创建一个函数来遍历数据集中的所有视频 ID。csv 文件),我们可以检索数据集中提到的所有视频 ID 的字幕，然后将它们存储在. csv 文件中。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="92bd" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">!pip install youtube_transcript_api # install youtube_transcript_api<br/>import pandas as pd<br/>import os<br/>import re<br/>import csv<br/>import numpy as np<br/>directory = os.getcwd() <br/>from youtube_transcript_api import YouTubeTranscriptApi #this library that will retrieve the subtitles from a  video in the form of list of dictionaries</strong></span><span id="33e9" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">#Creating a function that will iterate through the data-set and get the subtitles from each of the video ID's available<br/>def converter(x):#function will take an input as a data-frame<br/>    array = x.to_numpy() #convert to array<br/>    for i in range(len(array)):<br/>        try:<br/>            video_id=str(array[i][0])<br/>            Channel_name = array[i][3]<br/>            subtitles_1 = YouTubeTranscriptApi.get_transcript(video_id,languages=['en']) #obtain the subtitles in the video<br/>            first=''<br/>            for g in range(len(subtitles_1)):<br/>                first += " "+str(next(iter(subtitles_1[g].values()))).replace("\n"," ")<br/> <br/>            subtitles_2= first #sanitize the subtitles<br/>            subtitles_3=' '.join([i for i in subtitles_2 if not i.isdigit()]) #remove numbers as they are not relevant<br/>            subtitles = re.sub('[^A-Za-z ]+','',subtitles_3)<br/>            print(subtitles)<br/>        except:<br/>            subtitles=''<br/>            Channel_name=''            <br/>        subtitles_part_one = subtitles[0:20000]<br/>        subtitles_part_two = subtitles[20001:40000] <br/>        subtitles_part_three = subtitles[40001:len(subtitles)] <br/>            <br/>        with open(directory+'/Subtitles.csv', 'a',encoding='utf-8') as newFile:<br/>            FileWriter = csv.writer(newFile)<br/>            FileWriter.writerow([Channel_name+"|"+subtitles_part_one+"|"+subtitles_part_two+"|"+subtitles_part_three])<br/>converter(df)#pass the data frame to the function<br/>#Note the above function takes around 10-12 hours to run completely</strong></span></pre><p id="0273" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们运行上面的代码，我们将得到。csv 文件，其中一列包含内容创建者的姓名，另外三列包含每个视频的字幕。现在让我们用 python 导入这个文件，并查看数据的前 5 行</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6b01" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">data=pd.read_csv(directory+'/Subtitles.csv',sep="|",skiprows=0,header=None)<br/>data = data.replace(np.nan, '', regex=True)</strong></span><span id="32f9" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">data["Text"] = data.apply(lambda x: f'{x[1]} {x[2]}  {x[3]}', axis=1) <em class="mj">#combining subtitles from three columns</em></strong></span><span id="b72b" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">data=data.drop([1,2,3],axis=1) </strong></span><span id="29e7" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">data =data.rename(columns={0:'Name'})</strong></span><span id="b8e3" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">data.head()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/4003780df74f5ba7bf7af74ba0a2dd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*gv15OCczITJ09IydcoDSLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">“名称”栏包含你的土豆名称，“文本”栏包含每个视频的字幕。(图片由作者提供)</strong></p></figure><p id="d60b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如您在上面的图像中看到的，文本列包含小写和大写的文本，还包含停用词。以上数据需要清理，以便我们获得一些关键的见解。</p><p id="9414" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面我们整理数据集-</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="39e3" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu"><em class="mj">#Convert text to upper case</em><br/>data["Text"]=data["Text"].str.upper()</strong></span><span id="3fd0" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu"><em class="mj">#remove words of length less than 2 (as they will not be relevant)</em><br/>data["Text"]=data["Text"].apply(lambda x: ' '.join([word for word in x.split() if len(word)&gt;2]))</strong></span></pre><p id="a6c1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由于 one You Tuber 有多个视频，我们需要将多个视频的字幕连接起来，放在一行。下面是实现这一点的代码:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="1ab2" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">grouped_df = data.groupby("Name")</strong></span><span id="c760" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">grouped_lists = grouped_df["Text"].agg(lambda column: "".join(column))</strong></span><span id="0c8c" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">grouped_lists = grouped_lists.reset_index()</strong></span></pre><p id="b86c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在下面的代码中，我们使用“TfidfVectorizer”来获得每个文档中每个单词的总权重，即我们的例子中的字幕。术语频率-逆文档频率是一种统计数据，它强调某个单词相对于文档集合中所有单词的重要性。使用“TfidfVectorizer ”,每个单词的值与计数成比例增加，但与语料库中单词的频率成反比。这种相反的文档频率将适应某些单词通常频繁出现的事实。例如，由于我们的数据基本上是每个视频的字幕，“the”、“we”和“you”等词的频率会经常出现，但是，它们并不能真正告诉我们视频中的内容。我们使用“TfidfVectorizer”来获取每个单词的权重，它将返回每个频道字幕中每个单词的 TF-IDF 值的稀疏矩阵。这将有助于我们找到那些使用类似词语如“播客”、“烹饪”等的人。(我们目前只采用单个单词，但是也可以通过改变下面代码中的“ngram_range”参数来使用单词的组合)</p><p id="9e6f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">警告——在“min_df”中提供较小的值将增加稀疏矩阵的形状，导致内存负载，因此建议相应地调整“min_df”。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="876a" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">from sklearn.feature_extraction.text import TfidfVectorizer</strong></span><span id="6647" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">from nltk.tokenize import RegexpTokenizer</strong></span><span id="acbe" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu"><em class="mj">#tokenizer to remove unwanted elements from out data like symbols and numbers</em><br/>token = RegexpTokenizer(r'[a-zA-Z0-9]+')</strong></span><span id="a140" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu"><em class="mj">#Convert the collection of subtitles to a matrix of TF-IDF features</em><br/>cv = TfidfVectorizer(analyzer='word',stop_words='english',tokenizer = token.tokenize,min_df =1,ngram_range = (1,1))</strong></span><span id="541e" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">text_counts= cv.fit_transform(grouped_lists['Text'])</strong></span><span id="0d1b" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">print('the shape of the sparse matrix is',text_counts.shape)</strong></span></pre><p id="1c6b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面是我们稀疏矩阵的形状-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/7ee08160cd23ae01c2c256baffd8f5f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*pgXHjEiaqLr9KIO090a5Ow.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">作者图片</strong></p></figure><h1 id="b69a" class="mm mc it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">k-最近邻(KNN)</h1><p id="0626" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">现在我们到了有趣的部分，即实际构建模型。为此，我们将使用 NearestNeighbors，这是一个用于实现邻居搜索的无监督学习器。下面是代码:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="f4e0" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">from sklearn.neighbors import NearestNeighbors</strong></span><span id="d9bf" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">model = NearestNeighbors(metric='cosine',algorithm='auto',n_neighbors=5)</strong></span><span id="5990" class="mb mc it lx b gy mi me l mf mg"><strong class="lx iu">model.fit(text_counts)</strong></span></pre><p id="44d4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">就这样…我们完了！！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/3d29e5558233d2e7e5d4d3e1d3932a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7L3dZW8rFrNBZyDc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://unsplash.com/@ambreenhasan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">安布琳·哈桑</strong> </a> <strong class="bd ky">上</strong> <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">下</strong> </a></p></figure><p id="5720" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，为了返回给定的 YouTube 的 5 个相似的内容创建者，我们将创建一个函数，该函数将接受一个输入(即 YouTube 频道名称)并给出 5 个相似的 YouTube 频道及其距离</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="5692" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">#this function will take input x i.e. Name of the content creator<br/>def get_recommends(x):<br/>    indices = pd.Series(grouped_lists.index, index=grouped_lists['Name'])<em class="mj">#this will allow us to map our input i.e. YouTube channel’s name for the model</em><br/>    idx = indices[x]<br/>    distances, indices = model.kneighbors(text_counts.getrow(idx), n_neighbors = 6)#here we have kept n_neighbors = 6 because 0th neighbour will be x itself<br/>    names_similar = pd.Series(indices.flatten()).map(grouped_lists.reset_index()['Name'])<br/>    for i in range(0, len(distances.flatten())):<br/>        if i==0:<br/>            print('Similar Content creators like',names_similar[0],'are :-')<br/>        else:<br/>            print(" ",i,names_similar[i],'with distance - ',distances.flatten()[i])</strong></span></pre><p id="e2a9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，我们将使用上面定义的函数，并为我们的推荐模型提供输入，以便为给定的 YouTube 频道名称提供相似的 You Tubers(邻居)。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="7100" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">get_recommends("PewDiePie")<em class="mj">#"PewDiePie" is our Input</em></strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/574e18f809288f204b6ba29f30f8e9b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*kZyy4CUWU12xskR07pCFRw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">类似的内容创建者，如“PewDiePie”以及它们的成对距离(图片由作者提供)</strong></p></figure><p id="6a9b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面的输出首先打印我们的输入，即 PewDiePie，随后是五个不同的 YouTube 频道，内容相似，如“游戏”、“反应视频”等。以及它们的成对距离。</p><p id="101e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们试试其他渠道-</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="93fd" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">get_recommends("Jimmy Kimmel Live")</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b635e7a6b0692e0e3e267ea424838382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*24ZJj9cpJFNrxGkE-YJiUA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">类似的内容创建者，如“吉米·基梅尔现场直播”以及他们的成对距离(图片由作者提供)</strong></p></figure><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0358" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">get_recommends("MLG Highlights") </strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0eb91a56f7cb173c8922d4d3510a5a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*719RgM0vQpzgECofQVxu5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">类似的内容创建者，如“MLG 集锦”(图片由作者提供)</strong></p></figure><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d7b5" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">get_recommends("NBC News")<em class="mj">#NBC News is our Input</em></strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ae21f7ed14be2fa1fb9dd14fe45ad851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*uEoC178oO4Qh51IXG6tRGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">类似的内容创建者，如“NBC 新闻”(图片由作者提供)</strong></p></figure><h1 id="ad0a" class="mm mc it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">摘要</h1><p id="0c58" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">对于那些熟悉我们上面测试的 YouTube 频道的人来说，会知道我们创建的推荐系统是有效的！！</p><p id="8782" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们在 TfidfVectorizer 函数中提供的参数可以进一步优化，以提供更精确的结果。</p><p id="1b89" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，这是一个创建简单推荐模型的尝试，在这个模型的帮助下，我们可以从 YouTube 上推荐类似的内容创作者。请分享你对这个实现和整个帖子的想法。</p></div></div>    
</body>
</html>