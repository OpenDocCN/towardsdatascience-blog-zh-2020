<html>
<head>
<title>Head Pose Estimation with Hopenet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Hopenet 的头部姿态估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/head-pose-estimation-with-hopenet-5e62ace254d5?source=collection_archive---------29-----------------------#2020-06-29">https://towardsdatascience.com/head-pose-estimation-with-hopenet-5e62ace254d5?source=collection_archive---------29-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee6e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种直接估计头部姿态的深度学习方法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0e17647e00092eb508d07f6fba4c7d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*VDLqqpAreMKtoI0ylDakWQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Hopenet 展示了艾德·希兰演唱的歌曲“<a class="ae ku" href="https://youtu.be/f6Cswdm601A" rel="noopener ugc nofollow" target="_blank">大声说出想法</a>”。</p></figure><h1 id="0ca6" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">一.动机</h1><p id="7718" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在计算机视觉领域有许多有趣的研究。我们已经看到了物体检测、人脸检测、面部识别、光学字符识别(OCR)等许多方面的惊人进步。在学习人脸检测的时候，我遇到了一个关于头部姿态估计的研究领域。在这篇文章中，我想与你分享我所学到的东西。更具体地说，我将从讨论头部姿态估计开始。然后，我将讨论一种由 Nataniel Ruiz、Eunji Chong 和 James M. Rehg 介绍的深度学习方法，称为 Hopenet。</p><blockquote class="mj mk ml"><p id="b8f4" class="ln lo mm lp b lq mn ju ls lt mo jx lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="lp iu">免责声明:</strong>本文假设你已经熟悉深度学习中的 CNN 概念。</p></blockquote><h1 id="27d4" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">二。头部姿态估计</h1><h2 id="bf26" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">什么是头部姿态估计？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/c7cb26c3e4124448b67a12ee58a1ae74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3B8ipwptA7z3HAQ5BGZWw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 1:说明三个欧拉角的图表。</p></figure><p id="4100" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">顾名思义，计算机视觉中的头部姿态估计研究侧重于预测图像中的人类头部姿态。更具体地说，它涉及人类头部欧拉角的预测。欧拉角由三个值组成:偏航、俯仰和滚动。这三个值描述了对象在 3D 空间中的旋转。通过准确预测这三个值，我们可以计算出人的头部朝向哪个方向。拥有一台能够计算出人类头部朝向哪个方向的计算机提供了许多有用的应用。例如，它可以用来绘制 3D 对象，以匹配人类头部的方向，类似于抖音，Snapchat 和 Instagram 过滤器中看到的那些。此外，它还可以用于自动驾驶汽车，以跟踪驾驶员是否专注于道路。</p><blockquote class="mj mk ml"><p id="a4e1" class="ln lo mm lp b lq mn ju ls lt mo jx lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="lp iu">注:</strong>你可以参考这个由 Udacity 提供的 Youtube <a class="ae ku" href="https://youtu.be/q0jgqeS_ACM" rel="noopener ugc nofollow" target="_blank">视频</a>进行欧拉角的互动讲解。</p></blockquote><h2 id="1d3b" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">用来估计头部姿势的不同技术有哪些？</h2><blockquote class="mj mk ml"><p id="95ce" class="ln lo mm lp b lq mn ju ls lt mo jx lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="lp iu">注意:</strong>头部姿态估计中的许多方法假设面部检测作为初步步骤。首先，检测人脸，然后才能估计头部姿态。</p></blockquote><p id="c7f7" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">有两种主要的方法用于估计头部姿态。一种方法包括估计面部标志的中间步骤。面部标志然后被映射到人类头部的 3D 模型上。将创建的 3D 界标与相机的信息(如图像中的焦距、失真和光学中心)相结合，可以使用数学公式来计算人类头部的偏航、俯仰和滚动值(Mallick，2016)。然而，这种方法有一些缺点。根据 Ruiz，Chong 和 Rehg (2018)，这种方法的性能严重依赖于面部标志预测的性能、3D 头部模型的代表性以及用于完成估计的数学模型。</p><p id="837a" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">另一种方法是直接预测偏航、俯仰和滚动值，而不必首先估计面部标志。这就是 Hopenet 的创建者所采取的方法。通过跳过面部标志估计步骤，这种方法有可能变得更鲁棒、更快速、更准确(Ruiz，Chong 和 Rehg，2018)。Ruiz，Chong &amp; Rehg (2018)的工作并不是这种方法的唯一尝试。剑桥大学的一个团队也提供了这种头部姿态估计的直接方法(杨，牟，张，，Gunes 和 Robinson，2015)。Ruiz、Chong 和 Rehg(2018)的工作与其他尝试的主要区别在于使用了一个多重损失函数，这将在下面讨论。</p><h1 id="7313" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">三。希望网</h1><blockquote class="mj mk ml"><p id="a03a" class="ln lo mm lp b lq mn ju ls lt mo jx lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="lp iu">注:</strong>Hopenet 的创建者慷慨地开源了他们在 PyTorch 中的实现，你可以在这里找到<a class="ae ku" href="https://github.com/natanielruiz/deep-head-pose" rel="noopener ugc nofollow" target="_blank"/>。本文中显示的代码片段是我自己在 Keras 中的实现。我尽我所能根据自己的理解重新创作了他们的作品。虽然我无法复制 Nataniel、Eunji 和 James 的论文中显示的完全相同的结果，但我相信这些代码片段对您创建自己的“从头”实现他们的工作是有用的。此外，我决定不解释 Hopenet 的培训过程，因为我认为这篇文章已经为你如何进行培训提供了一个清晰的指导方针。</p></blockquote><h2 id="9f8e" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">网络架构</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/9c666163de48a66efc017a23539f79b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsLS9GLi_x1uNBO8C3ej4g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 Hopenet 架构的简图。</p></figure><p id="2051" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">Hopenet 的架构由两个主要部分组成:一个主干网络和三个使用 softmax 激活的全连接层(每个欧拉角一个)。三个完全连接的层共享同一个主干网络。请务必注意，来自完全连接层的每个欧拉角的输出不是单个值。相反，它是一个大小为<em class="mm"> num_bins </em>的向量，这是您必须设置的网络的超参数。所以，如果你决定选择<em class="mm"> num_bins </em>作为<em class="mm"> n </em>，那么将会有三个输出向量，它们的大小都是<em class="mm"> n </em>。角度的输出向量中的值表示该角度落入该条柱的概率。在他们的论文中，创造者展示了两种不同主干网络的性能:ResNet50 和 AlexNet。他们已经证明，与 AlexNet 相比，使用 ResNet50 作为主干网络会产生更好的结果。即使在他们的论文中，Hopenet 的创建者仅使用 AlexNet 和 ResNet50 作为主干层，也可以使用任何其他 CNN(Ruiz，Chong，&amp; Rehg，2018)。</p><p id="66ae" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下面是用 ResNet50 作为主干层重新创建 Hopenet 网络的代码片段。值得注意的是，与 AlexNet 相比，ResNet50 的训练时间要长得多，因为它是一个更大的网络。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">以 ResNet50 为骨干网构建 Hopenet。注意，偏航、俯仰和滚转有三种不同的损失计算。</p></figure><blockquote class="mj mk ml"><p id="ecb5" class="ln lo mm lp b lq mn ju ls lt mo jx lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="lp iu">注:</strong> <a class="ae ku" href="https://github.com/OverEuro/deep-head-pose-lite" rel="noopener ugc nofollow" target="_blank">这里的</a>是 Hopenet 的开源实现，使用 ShuffleNet 作为主干网络。</p></blockquote><h2 id="5b70" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">多损失损失函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/ddcda5465679ddd8eb18881de1c761f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aa0xGjO5r_dxxw0-D1qNGg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 2: Hopenet 的多损损失函数。从我的理解来说，alpha 是用来强调回归损失值的变化。</p></figure><p id="d708" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">每个欧拉角有三个不同的全连接层，可以分别优化每个角度的损耗(Ruiz，Chong &amp; Reh，2018)。每个角度的损失由两部分组成:分类损失和回归损失。创建者使用交叉熵作为分类损失，使用均方误差作为回归损失。根据 Ruiz、Chong 和 Rehg (2018 年):</p><p id="f6a0" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><em class="mm">这种方法背后的思想是，通过执行 bin 分类，我们使用非常稳定的 softmax 层和交叉熵，因此网络学习以稳健的方式预测姿态的邻域。通过三个交叉熵损失(每个欧拉角一个),我们有三个信号反向传播到网络中以改善学习……然后我们向网络添加一个回归损失，即均方误差损失，以改善细粒度预测。</em> (Ruiz，Chong，&amp; Rehg，2018，第 3 页)</p><p id="023d" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下面的代码显示了如何为每个欧拉角定义损失函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Hopenet 的多重损失计算。</p></figure><h1 id="a940" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">动词 （verb 的缩写）结论</h1><p id="1d9e" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">最后，我希望这篇文章能帮助你理解头部姿态估计领域的一些关键概念，并让你了解 Hopenet。也就是说，我也希望这篇文章能激发你对头部姿态估计的兴趣，并促使你对这个主题进行进一步的研究。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="6eaf" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><em class="mm">喜欢这篇文章并想表达你的支持？关注我或给我买咖啡</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="http://buymeacoffee.com/socretlee"><div class="gh gi nv"><img src="../Images/69716627feab2505c60838bbd29241a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gS6Sh6i8g535gOafY4Wl1w.png"/></div></a></figure><h1 id="74d6" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">参考</h1><p id="a6ed" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">Mallick，S. (2016 年 9 月 26 日)。<em class="mm">基于 OpenCV 和 Dlib 的头部姿态估计。</em>学习 OpenCV。检索自<a class="ae ku" href="https://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/" rel="noopener ugc nofollow" target="_blank">https://www . learnopencv . com/head-pose-estimation-using-opencv-and-dlib</a></p><p id="e5b9" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">Ruiz，Chong，e . Rehg，m . j .(2018 年)。无需关键点的细粒度头部姿态估计。<em class="mm">IEEE 计算机视觉和模式识别会议(CVPR)研讨会，2018 年，第 2074–2083 页</em>。从 https://arxiv.org/abs/1710.00925<a class="ae ku" href="https://arxiv.org/abs/1710.00925" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="5282" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">杨，h，牟，w，张，y，，I，Gunes，h .，&amp; Robinson，P. (2015)。头部姿态估计辅助的人脸对齐。从 https://arxiv.org/abs/1507.03148<a class="ae ku" href="https://arxiv.org/abs/1507.03148" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="0a8e" class="pw-post-body-paragraph ln lo it lp b lq mn ju ls lt mo jx lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">魏，J. (2019)。<em class="mm"> AlexNet:挑战 CNN 的架构</em>。走向数据科学。检索自<a class="ae ku" rel="noopener" target="_blank" href="/alexnet-the-architecture-that-challenged-cnns-e406d5297951">https://towards data science . com/Alex net-the-architecture-that-challenged-CNNs-e406d 5297951</a></p></div></div>    
</body>
</html>