<html>
<head>
<title>100x faster Hyperparameter Search Framework with Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark 超参数搜索框架速度提高 100 倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6?source=collection_archive---------13-----------------------#2020-02-02">https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6?source=collection_archive---------13-----------------------#2020-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/76e92bdcefee58638f5699efa9be6f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZxYfrAaPsPtIr6SSaYY1w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">闪电力量。<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1539416" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="bebe" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated"><strong class="ak">现成的、可重用的、快速的代码</strong></h2></div><p id="c57d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近我在为一个巨大的机器学习模型调整超参数。</p><p id="1333" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">手动调整不是一个选项，因为我必须调整很多参数。<a class="ae jg" rel="noopener" target="_blank" href="/automate-hyperparameter-tuning-for-your-models-71b18f819604"> Hyperopt </a>也不是一个选项，因为它是连续工作的，也就是说，一次只能制造一个模型。所以训练每个模特要花很多时间，而我的时间很紧。</p><p id="13da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我要赶上最后期限，我必须想出一个更好更有效的方法。因此，我想到了在许多这样的场景中帮助我们数据科学家的一件事— <strong class="la jk"> <em class="lu">并行化。</em> </strong></p><p id="3fed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">我可以并行化我的模型超参数搜索过程吗？</em> </strong></p><p id="995d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可能已经猜到了，答案是肯定的。</p><p id="7e7b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">这篇文章是关于使用 scikit-learn/xgboost/lightgbm 和 pySpark 为数据科学建立一个超参数调优框架。</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d3c9" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">网格 vs 随机化？</h1><p id="1bcc" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在我们开始实现超参数搜索之前，我们有两个选项来设置超参数搜索——网格搜索或随机搜索。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3ea10f94d67c75cda1706b3dbfa5185f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*r3a9LAdXb4MH5Zht.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">从一个 3×3 的参数网格开始，我们可以看到随机搜索最终会对重要参数进行更多的搜索。</p></figure><p id="d1c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图给出了为什么随机搜索更好的明确答案。</p><p id="dcf3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们必须为我们的机器学习模型调整两个超参数。一个不重要，一个很重要。在网格搜索中，我们查看重要参数的三个设置。在随机搜索中，我们在 9 个设置中搜索重要参数。我们花的时间是一样的。</p><p id="71ea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为随机化搜索更彻底地搜索整个空间，并为我们提供更好的超参数，所以我们将在我们的示例中使用它。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="fc39" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">树立我们的榜样</h1><p id="39c3" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在我的工作场所，我可以访问一个有数百个节点的非常大的集群。这是数据科学家的梦想。但是在这篇文章中，我将使用 Databricks 社区版免费服务器和一个玩具示例。如果你想为自己设置这个小服务器进行练习，请查看我在 Spark 上的<a class="ae jg" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a">帖子</a>。</p><p id="c48f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以选择使用 Spark 加载数据，但是在这里，我首先创建我们自己的分类数据，以建立一个我们可以使用的最小示例。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="d4f3" class="nj md jj nf b gy nk nl l nm nn">X,y = datasets.make_classification(n_samples=10000, n_features=4, n_informative=2, n_classes=2, random_state=1,shuffle=True)</span><span id="2a66" class="nj md jj nf b gy no nl l nm nn">train = pd.DataFrame(X)<br/>train['target'] = y</span><span id="4aa5" class="nj md jj nf b gy no nl l nm nn"># Convert this pandas Data to spark Dataframe. <br/>train_sp = spark.createDataFrame(train)</span><span id="f2ec" class="nj md jj nf b gy no nl l nm nn"># Change the column names.<br/>train_sp = train_sp.toDF(*['c0', 'c1', 'c2', 'c3', 'target'])</span></pre><p id="a621" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe np nq nr nf b">train_sp</code> spark 数据集看起来像:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/4ecb797babdb838445e320d59bbac698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBsmRGmtt7uFLBCZM0ZDBw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cc4b" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">这个想法——复制和应用</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/03e9645c93fa418b07ee5518bedc48f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zWKAcHgxHjKV97gx"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/@frankvex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰克·维西亚</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7199" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们在 Spark 中有了我们的训练数据集。我们想在这个数据框架上运行多个模型。</p><p id="28c2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Spark 天生擅长键值对。也就是说，具有特定密钥的所有数据都可以发送到一台机器。我们可以对这些数据应用函数。</p><p id="747c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们需要每台机器上的所有数据。我们如何做到这一点？</p><p id="7c9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">我们复制我们的数据 n 次，并给我们的数据添加一个 replication_id，这样每个键都有所有的数据。</em>T11】</strong></p><p id="3a9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，现在我们可以使用 replication_id 上的 groupby 将整个数据发送到多台机器。但是我们如何使用熊猫和 scikit 来学习这些数据呢？</p><p id="9311" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">答案是:我们用 pandas_udf。这个功能是在 Spark 版本 2.3.1 中引入的。这让你可以利用 Spark 的熊猫功能。</em></strong></p><p id="2639" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您还不理解这一点，请查看代码，因为有时理解代码更容易。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1b94" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">代码</h1><p id="9ac3" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在这里，我们首先通过将<code class="fe np nq nr nf b">cross_join</code>与包含 1–100<code class="fe np nq nr nf b">replication_id</code>的列的数据帧一起使用，将我们的训练数据帧复制 100 次。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="421b" class="nj md jj nf b gy nk nl l nm nn"># replicate the spark dataframe into multiple copies</span><span id="3404" class="nj md jj nf b gy no nl l nm nn">replication_df = spark.createDataFrame(pd.DataFrame(list(range(1,100)),columns=['replication_id']))</span><span id="5b11" class="nj md jj nf b gy no nl l nm nn"><strong class="nf jk">replicated_train_df = train_sp.crossJoin(replication_df)</strong></span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/622cfcd80f4b89f878ef7a8e4c83712b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whLLrJQCwQaYUfgaz1vC8w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">每一行都用不同的 replication_id 复制 100 次</p></figure><p id="354c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还定义了一个函数，该函数将 pandas 数据帧作为输入，使用 python random 模块获取随机超参数，对数据运行模型(这里我训练的是 scikit 模型，但您也可以用任何模型(如 XGBoost 或 Lightgbm)替换它),并以 Pandas 数据帧的形式返回结果。一定要看看函数和注释。</p><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="nv nw l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">这里不需要(太多)火花。该函数将熊猫数据框作为输入。</p></figure><p id="ffbe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以将这个<code class="fe np nq nr nf b">pandas_udf</code>函数应用于我们复制的数据帧，使用:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="f6a8" class="nj md jj nf b gy nk nl l nm nn">results = replicated_train_df.groupby("replication_id").apply(run_model)</span></pre><p id="fbb9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的代码所做的是将具有相同复制 id 的所有数据发送到一台机器上，并将函数<code class="fe np nq nr nf b">run_model</code>应用到数据上。上面的调用是延迟发生的，所以在运行下面的操作调用之前，您无法看到结果。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="6b63" class="nj md jj nf b gy nk nl l nm nn">results.sort(F.desc("Accuracy")).show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/e402ee6a2cd36818b9a3eae641590f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9adA9nMegu2UOC24tjjYg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们的超参数搜索结果</p></figure><p id="dd5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这个玩具示例，精度结果可能看起来非常接近，但在嘈杂的真实世界数据集的情况下，它们会有所不同。由于这 100 个模型都是在不同的节点上并行运行的，所以我们在做随机超参数搜索的时候可以节省很多时间。</p><p id="00a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">加速因子当然取决于集群中有多少节点。对我来说，我有 100 台机器可供我使用，所以我获得了大约 100 倍的加速。</p><p id="2037" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从这个数据砖<a class="ae jg" href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7664398068420572/3438177909678058/3797400441762013/latest.html" rel="noopener ugc nofollow" target="_blank">笔记本</a>中获得完整的代码，或者从我的<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/spark_hyperparams_tuning" rel="noopener ugc nofollow" target="_blank"> GitHub </a>仓库中获得，我在那里保存了我所有帖子的代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="34ab" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">继续学习</h1><p id="c7ea" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">如果你想了解更多关于实用数据科学的知识，请看看 Coursera 的<a class="ae jg" href="https://coursera.pxf.io/yRPoZB" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">【如何赢得数据科学竞赛】</strong> </a>课程。我从卡格勒教授的这门课程中学到了很多新东西。</p><p id="b54f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我，或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p><p id="52d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——在这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>