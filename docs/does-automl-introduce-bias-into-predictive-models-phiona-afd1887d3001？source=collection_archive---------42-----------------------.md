# AutoML 会在预测模型中引入偏差吗？

> 原文：<https://towardsdatascience.com/does-automl-introduce-bias-into-predictive-models-phiona-afd1887d3001?source=collection_archive---------42----------------------->

## 保持人在循环中，以确保数据上下文

![](img/1623c62cafec3098fa288c5ff2756fb2.png)

来源: [@lensinkmitchel，](https://unsplash.com/@lensinkmitchel)经由 [Unsplash](https://unsplash.com/photos/Ismnr6WSHCU)

在过去的几年里，以数据为中心的自动化能力发展速度惊人——从机器人过程自动化，到计算机视觉，再到 AutoML。所有这些技术都通过减少人类完成任务所需的人工工作量，显著提高了多个行业的生产率。然而，随着这些技术的改进，人类离这些解决方案想要取代的实际过程越来越远。扩大这种距离有可能侵蚀自动化提供的潜在好处；一台不能推理或感觉的机器会通过在数据中强化歧视性偏见以及增加其产生的无法解释的结果的数量(“黑箱”效应)来产生意想不到的后果。构建保持“人在循环中”的技术以确保这些后果不会变得无处不在，这是非常重要的。

# 有价值的算法

自动化的当前趋势不仅仅是一个抽象问题，工具将一个更漂亮或更容易使用的界面放在通常是脚本代码的上面。软件总是随着时间的推移而发展，以简化生产软件所需的工作量。开发人员很少再构建自己的编译器了。

但是今天，许多软件工具超越了前端和后端架构的协同作用；应用程序越来越需要从某个地方(单个用户、整个用户群或另一个系统)生成的数据，作为在软件应用程序中交付价值的关键组件。算法的建立是为了理解数以百万计的数据点，并推动个人预测值，以确保信息的消费者发现它是有用的。这些算法构建起来极其复杂，但如果操作正确，在未经训练的人看来就像魔术一样。

难怪如此多的公司希望在未来几年内建立机器学习能力。在没有人为干预的情况下，自动产生针对个人消费者的个性化价值，这种能力有可能改变公司的经营方式。公司通过在正确的时间提出正确的建议而使收入增加两倍的故事，或者医生能够主动接触可能有某些临床风险的患者的故事，似乎是技术的胜利和对未来的展望。人类终于能够使用数学和工具大规模地提前预测行动，而不是一次一个地做出反应。

这些预测之所以可能，是因为我们的应用程序每天每秒都在生成大量数据集。随着更多数据成为算法逻辑的基础，算法通常会变得更具预测性，从我们的信用卡交易历史到我们的医疗记录，再到我们的智能手机点击和滚动，一切都可以用来构建越来越大的数据集来驱动这些模型。

毫无疑问，如果以正确的方式使用，这些算法的复杂性可以为社会创造难以置信的价值。以目前机器学习技术的进步水平，人们可以说，建立这些模型的数据科学家能够(在很大程度上)平衡预测的能力与使用个人数据来推动这些预测的潜在隐私和歧视影响。

# 歧视性后果

然而，也有很多组织不考虑歧视性偏见的例子。2019 年，高盛因歧视申请信用卡的女性而在[陷入困境](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwiJk7Ou-urnAhUFUa0KHYf1AAEQFjACegQIARAB&url=https%3A%2F%2Fwww.nytimes.com%2F2019%2F11%2F10%2Fbusiness%2FApple-credit-card-investigation.html&usg=AOvVaw0Ezv9NZSgG2NofQbCEbVnI)，无论是在审批过程中还是在她们获得的信用额度方面。几十年来，保险公司一直使用数据来评估风险，[最近的研究表明](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk)居住在主要是少数民族聚居区的人比居住在白人聚居区的人支付更多的汽车保险，尽管整体风险水平相同。

这两个例子都表明，当歧视被有意或无意地引入数据集时，这种可能性是存在的，数据集表面上是为了对产品或服务提供客观的推荐。这些算法模型的问题是双重的——不仅它们做出歧视性的建议，而且它们也是黑箱的定义——我们人类不一定理解他们为什么首先做出那些有缺陷的建议。

# 复合效应

顾名思义，算法使用自己掌握的数据来生成答案。“更好”的答案可以通过更广泛的数据(例如:结合智能手机使用数据的交易信息)或更深入的数据(例如:20 年的信用卡支付历史)来产生。当使用脏数据时，会出现“更糟”的答案，也就是人们常说的:垃圾进，垃圾出。让数据达到一定的清洁程度需要大量的工作——一些评估认为清洁任务要花费 [80%的时间](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&cad=rja&uact=8&ved=2ahUKEwjCg-7f-urnAhVOC6wKHQQdCnQQFjADegQIAxAB&url=https%3A%2F%2Fhbr.org%2F2018%2F08%2Fwhat-data-scientists-really-do-according-to-35-data-scientists&usg=AOvVaw2ut3nKxalzHk0wmhTVD1ZA)处理数据。

数据清理工作本质上通常是相当手动的——寻找正确的数据集来使用和组合，梳理行和列来找到要纠正的错误，并编写脚本或 Excel 公式来解决问题。持续做好这项工作需要大量熟练的人工干预，而现实是这并不是一项非常有魅力的工作，尤其是对于从事这项工作的专业人员来说。但既然是 80%的数据工作，总得有人去做。

在这个过程中，歧视性偏见的潜在问题可能会渗透进来——精通数据的分析师可以认识到，为包含性别、种族或收入(或潜在的三者)数据列的预测模型构建数据集，可能会导致算法将种族或性别视为最有可能预测高汽车保险费的因素。构建和清理数据集的人类可以理解，这种方法和结果可能会产生重大的社会问题，因为他们必须生活在这些歧视性建议产生不平等和不公平结果的世界中。

因此，人类数据分析师可以选择省略这些列，从而降低算法主要基于人口统计数据产生歧视性结果的可能性。但是，机器会知道省略这些列吗？

# 煤矿里的金丝雀

对于机器学习工作所需的复杂程度，市场的反应一直是尽可能将其自动化。尽可能简单！放入一个数据集，我们会自动清理它，并为几十种算法中的一种做好准备，这些算法可能会得到您想要的预测！看起来很不错——能够放入数据并得到 75%可能性的预测。

但是这个过程包含什么呢？这是黑盒效应导致意外结果的典型例子——我们不知道幕后发生了什么。清理流程脚本是否会删除调查数据中被调查者拒绝注明其种族的所有行(空白值可能会扭曲结果)？该脚本删除了合理的高医疗测试结果，因为其余的测试结果普遍低于平均水平吗？如果你不知道这些问题的答案，你能相信你得到的结果吗？如果算法的结果是歧视性的，你说你不知道结果中的所有因素是否足以为其辩护？

这个 AutoML 例子是过度自动化给我们带来的难题。最初，人类是建立数据集和训练算法的人，以至于自动化是足够接近的近似值。但是，当我们试图去除支撑自动化的人为步骤时，我们可能会产生意想不到的后果。虽然许多人认为自动化的危险是一般人工智能创造了下一个终结者或其他一些低可能性场景，但危险可能没有那么夸张，但也没有那么危险:数据工作已经自动化到了一个地步，甚至没有人意识到生成的建议如何包含偏见，因为我们已经变得如此依赖机器为我们思考。

# 中间立场

机器学习和更不均衡定义的术语人工智能的重点一直是取代人类工作。也许相反，在数据工作的范围内，建立增强而不是取代人的能力的解决方案更好。到目前为止，计算机还没有能力理解某些数据类型的上下文——尽管对某些信息是否是个人身份信息(PII)可能有二元理解，但计算机不识别为什么该信息被认为是 PII。人类必须做出决定。

因此，找出自动化框架中人类可以从他们的判断和理解中增加最大价值的地方可能是有利的，即使工作是由脚本和算法执行的。我们已经做了很多这种“人在回路中”的工作，即使是在机器学习的背景下。人类在回路中可能看起来更像飞机上的自动驾驶仪，而不是自动驾驶汽车，在自动驾驶汽车中，可以明确确定数据工作的某些部分不会引入偏差(例如，删除文本周围的空格)，而其他部分可能会引入偏差，这就是人类会介入的地方。

随着我们有越来越好的工具来处理信息，围绕什么是合适的以及技术将如何改进的讨论肯定会发展。与此同时，重要的是不要随意地完全改变这种或那种方式——我们不必手动地对 Excel 中的每个单元格进行更改，或者将我们的工作交给 AutoML。

Phiona 是一个增强的数据管理平台，使非技术用户能够减少数据工作:我们的技术标记了可能需要清理和标准化的领域，节省了手动梳理数据的时间，而不牺牲对数据准备过程的控制。你可以在 https://phiona.com 看到更多。

*原载于 2020 年 3 月 25 日*[*【https://phiona.com】*](https://phiona.com/blog/automl-bias)*。*