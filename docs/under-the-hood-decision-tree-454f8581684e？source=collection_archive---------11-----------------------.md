# 引擎盖下—决策树

> 原文：<https://towardsdatascience.com/under-the-hood-decision-tree-454f8581684e?source=collection_archive---------11----------------------->

## 通过观察选择正确分支的数学过程，理解决策树的工作原理，从而得到最好的树

![](img/815202f76875f12ba0a61cad8e26673d.png)

弗拉季斯拉夫·巴比延科在 [Unsplash](https://unsplash.com/s/photos/decision-tree?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

## 在后台

这是一系列文章中的第三篇，在这一系列文章中，我们将理解各种 ML 算法的“幕后”工作，使用它们的基本数学方程。

1.  [引擎盖下——线性回归](/linear-regression-under-the-hood-583003d0bf38)
2.  [幕后——逻辑回归](/under-the-hood-logistic-regression-407c0276c0b4)
3.  [幕后——决策树](/under-the-hood-decision-tree-454f8581684e)

有这么多优化的实现，我们有时太关注库和它提供的抽象，而太少关注进入模型的底层计算。理解这些计算往往是一个好模型和一个伟大模型的区别。

在本系列中，我将重点放在手工实现算法上，以理解其背后的数学原理，这将有望帮助我们训练和部署更好的模型。

# 决策图表

决策树是最受欢迎的机器学习算法之一，因为它们简单直观。它形成了许多其他算法的基础，如随机森林和梯度提升机器，这些算法是大多数数据科学竞赛的主要内容，直到几年前，仍然是最通用、最容易理解的 ML 算法之一。

决策树的工作方式非常类似于我们人类通过问一系列问题来做决定的方式。例如，我们经常预测(或者曾经预测，直到我们可以在手机上查看当天的天气预报)外出时是否要带伞，基于各种决策点，例如—

1.  天气多云吗？
2.  现在是几月？
3.  过去几天的这个时候下雨了吗？
4.  我要走多远？
5.  我有雨伞吗？(这应该是第一个问题)

基于对其中一个或多个问题的回答，我们出门时可能带伞，也可能不带伞。

决策树(用于分类)以类似的方式工作，一次询问一个变量，以找到数据中不同类别之间的最佳分割。

考虑以下数据，具有两个输入特征— **X1、X2** 和一个二元(0/1)目标特征— **Y**

![](img/0c51e0184eb0722901d304f4019d30e0.png)

有 10 行的示例数据

该算法遵循以下步骤来找到这种数据的最佳分割。

![](img/c19be9c71a12c6865d1213c6e1fa30d4.png)

0.两个类的样本数据

1.  对于每个输入变量，计算不同阈值下的数据分割。

![](img/8cde01e20c927b64a45ded05ff44abd3.png)

1.计算不同阈值下的分割

2.选择给出最佳分割的阈值。

![](img/70f0f8e8de467b04e0e3069efe01f282.png)

2.选择能产生最佳分割效果的分割

3.重复步骤 1 和 2，直到收敛。

![](img/18e7b3b2713d3d3ff1c91ea874c30363.png)

3.计算剩余数据的拆分

4.组合所有选定的阈值，形成一个用于数据分类的规则链(树)。

![](img/2be3e17358beb1e1977bff3f31bb38cd.png)

4.组合选择的阈值以形成决策树

让我们使用示例数据更深入地研究这些步骤。

## 1.对于每个输入变量，计算不同阈值下的数据分割

这是决策树算法的第一步，有两个主要的决策点

1.  我们如何选择阈值变量？
2.  我们如何选择阈值？

对于第一种，最常见的策略是考虑所有变量，并检查哪个变量+阈值组合给出了数据的最佳分割。

第二点是事情变得更有趣的地方。可以有各种方法来选择每个变量的阈值—

1.  在每个变量的范围内选择随机值。例如，对于变量 *X2* ，值的范围在 3 和 9 之间(假设最后两行被保留用于验证我们的“模型”)。我们可以选择 3、4、5、6、7、8 和 9 的整数阈值。
    很明显，当变量的范围增加到数百个时，或者在处理浮点特性时，这将不能很好地伸缩。在 3.6 和 3.7 之间，我们可以适应多少个浮点阈值？
2.  另一种方法是对变量中的唯一值进行排序，并在每个连续值之间选择一个阈值—使用最大值/最小值/平均值。这将把我们的搜索空间缩小到数据中存在的值的子集，因为阈值是基于数据中存在的值而不是整个范围的值来选择的。
    对于变量 *X2* ，阈值看起来像这样

![](img/b87599d216be95bc928086d82cc6c9c8.png)

可变 X2 的阈值策略

类似地，对于 X1 —

![](img/c8372b9fccb262301d324aa153437273.png)

有一种更好的方法来选择进一步降低搜索空间的阈值。我们将在理解“最佳分割”以及如何使用这些阈值形成决策节点之后再来看它。

让我们继续选择*阈值(平均值)*值作为我们的阈值。

## **2。选择给出最佳分割的阈值**

为了选择给出最佳分割的阈值，我们首先需要定义什么是“最佳分割”。

为了直观地理解它，我们可以看到在下面的图像中， *X1=t2* 看起来像是最佳分割，因为分割的左半部分只包含蓝色点(纯节点)。换句话说，可以说如果随机选择的点有 *X1 < t2* ，那么这个点就是蓝色的。我们不能在 *t1* 或 *t3 做出同样的推断。*

![](img/8cde01e20c927b64a45ded05ff44abd3.png)

因此，在每个阈值处，我们检查阈值两侧的点集的分布是否比分割前更好。我们选择能够提供最佳数据分布的阈值—理想情况下，该阈值将单个类(在我们的示例中为蓝色)的大多数点放在一边，同时在分割的那一边保留其他类(在我们的示例中为红色)的少数(最好是 0 个)实例。

从数学上来说，我们在拆分之前检查数据的杂质测量值(每类点的密度)与拆分的平均杂质测量值，并选择产生最大差异的阈值。对于我们的情况，让我们考虑“基尼系数”,定义为

![](img/010c8d2a4579c19154eac357d02d6220.png)

其中 *k* 为类数，*p*ᵢ*t5】为类概率 *i.**

对于我们的数据，

![](img/416036a167a1e4500a90ea5ad1c6c3c2.png)

为了计算我们在第一步中选择的基尼系数，我们首先将数据按照 X2 变量排序

![](img/cb135946641841662eac4c0d7bae7908.png)

按要素 X2 排序的数据

我们的数据在 0 类和 1 类各有四个点。因此，分割前的基尼系数可以计算为

![](img/616f5f0289d1b7520726a29a40ee424d.png)

如果我们考虑阈值为 *X2=5 的分割，*数据分布看起来像—

![](img/e8f95148a12aeed7b7867b695a0c6322.png)

我们最终在拆分的每个部分都有 4 行。我们可以计算两种分裂的基尼系数——

![](img/0b7faccd743d45cc4eb896f194983349.png)![](img/e30c07a0272ba47da86cedf9b6cb33fc.png)

因此，我们可以将杂质的总体变化(减少)计算为—

![](img/7b78f54c2a075a7043301c3acdf8f2fb.png)

这种增益变化(δG)也称为“信息增益”。
我们对 *X2* 的所有阈值进行相同的计算

![](img/95960efc213749c7f8fd61482d425e94.png)

上表显示，使用特征 *X2* 进行决策时，最佳可能增益为阈值 7。

我们需要为另一个特征重复这个过程— *X1* 。按照我们为 X2 所做的一系列步骤，我们得到—

![](img/f6371b0876226c665748a5964a603928.png)

我们注意到，如果我们在 *X1=5.5* 处分割数据，我们会得到相同的最高增益值(0.340)。

在这种情况下，我们为多个特征获得相同的最大增益，选择左边的第一个特征(如在原始数据中)。根据我们案例的规则，“最佳分割”发生在 ***X1=5.5*** 。

因此，当我们的决策节点在 *X1=5.5* 时，我们可以开始创建一个“决策树”，就像—

![](img/f1ba41f1443c34044d7fb408c7ff83fc.png)

## 3.重复步骤 1 和 2，直到收敛

我们对每个拆分数据重复步骤 1 和 2，直到我们的所有拆分都包含纯样品(杂质为零)。

根据上一步中在 *X1=5.5* 处的分割生成的数据集之一(分割 1)已经是纯的——它只有来自类 0 的样本。因此，我们可以说，这个分支已经达到了最佳点，我们不需要再分裂它了。

我们使用来自 Split 2 的数据来选择进一步的阈值—

![](img/e3dd51384f9223bd336ae21c8e5d359a.png)

来自分割 2 的数据

如果我们遵循对特征的唯一值进行排序并在连续值的平均值处选择阈值的策略，我们将最终得到 *X1* 的阈值为 6.5、7.5 和 8.5，而 *X2* 的阈值为 3.5、5 和 7.5。
快速目测显示，对于这两个变量，前三次分割都是次优的。最佳分割在 *X1=8.5。*

这使我们回到了选择最佳策略来选择阈值的问题上。
现在我们已经看到了最佳分割是如何计算的，并且考虑到我们一次只查看一个特征，我们可以说理想的阈值候选位于类别之间的过渡点*即*当特征被排序时，只有那些目标从 0 变化到 1 或相反的点才是好的决策点，因为任何其他位置都意味着从相同类别中分割点，这将导致我们两个分割中杂质的增加。

![](img/aeb05c83e8aa841259266b58fe61fe83.png)

因此，我们可以看到，当选择 *X1* 的阈值时，我们的理想候选值是 8.5，因为当从 *X1=6* 移动到 *X1=7* 或从 7 移动到 8 时，目标值(1)没有变化。

因此，我们在 *X1=8.5 —* 的增益

![](img/f2f9be86b65907dd7a0338ef724f60b5.png)

同样，对于 *X2* ，我们唯一的阈值候选是 *X2=6* ，给我们一个 0.32 的等价增益值。

这给了我们在 *X1=8.5* 的第二个决策节点，使我们的决策树呈现为—

![](img/09ec32eeda2e3672416c7445ffb4a55e.png)

因为我们所有的节点都是纯的，所以不需要进一步的分裂。

因此，我们的终端节点是具有类 0 的
Split-1、具有类 1 的
Split-3 和具有类 0 的
Split-4。

我们可以通过遍历树来预测新变量的类别，并评估它属于哪个节点。

让我们通过测试我们在训练中没有使用的两行数据来验证我们的决策树的准确性

![](img/2aaddb8bf6bda4f11e79ba37616fa4ce.png)

对于第一个数据点(3，5)，由于 X1 小于 5.5，该点以 Split-1 结束，预测类别为 0；与我们实际的目标类相同。

![](img/859429937473cc60958930b192be8ca1.png)

类似地，对于另一个端点，我们看到样本以 Split 3 结束，预测值为 1，这与实际的类相匹配。

![](img/61136a6b468c092589a0219ab3871a9e.png)

仅此而已。在其核心，这就是决策树算法所做的一切。

# 这真的是决策树所做的全部吗？

“在引擎盖下”是本系列的焦点，我们一次一个节点地查看决策树构建的基础，以适应我们的数据。

虽然这确实是决策树的核心功能，但是有一些复杂的地方我已经跳过了，比如—

1.  为什么基尼不纯？
2.  还有哪些其他杂质指标可用于选择最佳分离度？
3.  决策树过度拟合数据的趋势。
4.  特征重要性——我们可以看到，我们的两个决策点都只依赖于变量 *X1* 而不是 *X2。这是否意味着我们可以抛弃 X2？*
5.  修剪和聚集节点。
6.  预测概率是如何计算的？

我将在一个平行系列中介绍这些概念，重点是我们在这个系列中介绍的不同 ML 算法的各种复杂性。

# 下一步是什么？

在本系列的下一篇文章中，我们将继续讨论决策树，并在**决策树的引擎盖下寻找回归**，并了解如何使用决策树进行拟合，并输出连续的目标值。