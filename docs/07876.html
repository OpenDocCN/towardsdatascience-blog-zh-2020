<html>
<head>
<title>Tutoring Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">辅导强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutoring-reinforcement-learning-a52186306d6d?source=collection_archive---------37-----------------------#2020-06-11">https://towardsdatascience.com/tutoring-reinforcement-learning-a52186306d6d?source=collection_archive---------37-----------------------#2020-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6608" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习代理从零开始，什么都不知道，凭经验学习，有效但慢。我们能给他们一些提示让他们开始吗？</h2></div><blockquote class="ki kj kk"><p id="f1fb" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这个故事基于我与人合著的一篇论文，你可以在这里找到<a class="ae li" href="http://ceur-ws.org/Vol-2600/short9.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p></blockquote><p id="2b53" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">近年来，强化学习在视频游戏、机器人和推荐系统等任务中取得了令人惊讶的结果。然而，这些成功的结果是在训练 RL 代理数百万次迭代之后得到的。在那之前，经纪人的表现将远远谈不上伟大。事实上，当智能体开始时，它的行为在探索它可以采取的不同行动时是随机的，即使经过几次迭代并获得一些经验，智能体也会因为其在环境中的行动的可变性和随时可能出现的看不见的状态而经常出错。</p><p id="9c1c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">在像视频游戏这样的环境中，这可能不是问题(尽管这意味着您需要时间和一些重要的计算资源)，但在现实世界的系统中，这是一个严重的挑战。你可能会想到一个使用 RL 来学习如何移动的机器人:机器人可能需要几天来学习如何正确移动，如果移动是危险的，它可能会损坏自己。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">你不得不爱失败，虽然[视频来自 IEEE。]</p></figure><p id="3191" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">作为人类，我们不会从头开始学习一切，如果我们会，我们离穴居人就不远了。我们取得的所有进步都是通过学习他人的成就并加以改进而实现的。我们不能用 RL 做类似的事情吗？我们能不能教代理一些通用的规则，让它在从自己的经验中学习时可以应用？</p><p id="2de2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我和我的同事们一直在研究这个想法，我们开发了一种叫做<a class="ae li" href="http://ceur-ws.org/Vol-2600/short9.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> Tutor4RL </strong> </a>的方法。我们已经在今年的<a class="ae li" href="https://www.aaai-make.info" rel="noopener ugc nofollow" target="_blank"> AAAI 制造:在实践中结合机器学习和知识工程</a>上发表了它，你可以在这里看到它<a class="ae li" href="http://ceur-ws.org/Vol-2600/short9.pdf" rel="noopener ugc nofollow" target="_blank"/>(不幸的是，今年的春季会场被取消了，但论文发表了。)Tutor4RL 仍在开发中，所以我在这里介绍我们方法和目前为止的初步结果。</p><h1 id="c4c4" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">常见方法</h1><p id="6f64" class="pw-post-body-paragraph kl km it ko b kp mp ju kr ks mq jx ku lj mr kx ky lk ms lb lc ll mt lf lg lh im bi translated">正如我所说，学习速度慢是学习者面临的一个普遍挑战，许多方法已经出现来解决这个问题:</p><ul class=""><li id="99a5" class="mu mv it ko b kp kq ks kt lj mw lk mx ll my lh mz na nb nc bi translated"><strong class="ko iu"> Simulations </strong>:一种常见的方法是创建一个模拟环境，在将代理部署到真实环境之前，代理可以在其中进行实验和培训。这种方法非常有效，但是创建模拟环境需要很大的努力，并且我们通常最终会对真实环境做出假设来创建模拟，这可能并不总是正确的。这些假设会影响代理在真实环境中的表现。</li><li id="7fe4" class="mu mv it ko b kp nd ks ne lj nf lk ng ll nh lh mz na nb nc bi translated"><strong class="ko iu">基于模型的 RL </strong>:与无模型的 RL 相反，基于模型的 RL 创建其环境的模型，这允许代理更快地学习。在机器人的例子中，我们可以把我们的 RL 模型建立在机器人的物理学和力学的基础上。然而，为了制作这个模型，我们需要大量来自环境的知识，同样，我们通常会做出伤害我们的假设。此外，生成的代理是特定于其环境的，如果我们想将其用于不同的环境，我们需要修改其模型，这涉及到额外的工作。</li><li id="a914" class="mu mv it ko b kp nd ks ne lj nf lk ng ll nh lh mz na nb nc bi translated"><strong class="ko iu">从演示中学习</strong>:在 Hester et al. (2018)中，作者开发了一种方法，使智能体可以有效地从人类的演示中学习。学习速度大大加快，并且代理能够达到最先进的结果。但是，如果我们事先无法接触到环境，因此无法提供演示，会发生什么情况呢？这种方法可以与 Tutor4RL 结合使用，当代理已经部署在其环境中时，使用 Tutor 来提供演示。</li></ul><h1 id="034d" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">Tutor4RL</h1><p id="c631" class="pw-post-body-paragraph kl km it ko b kp mp ju kr ks mq jx ku lj mr kx ky lk ms lb lc ll mt lf lg lh im bi translated">我们希望我们的代理从一开始就表现得<em class="kn">好(或者至少体面)</em>，但是当我们事先无法访问环境时，这是非常困难的，如果我们对环境做出任何假设，而这些假设是错误的，那么代理的表现将会受到影响，不仅是最初，而是贯穿其整个生命周期。但是，这并不意味着我们不能提供一些有用的信息，代理一旦部署到环境中就可以使用这些信息。</p><blockquote class="ni"><p id="b892" class="nj nk it bd nl nm nn no np nq nr lh dk translated">最后，当我们学到一些东西时，我们并没有被告知我们将会遇到的每一种情况的所有细节。相反，我们可以从理论观点和其他人给我们的提示中学习。一个 RL 特工能做到这些吗？</p></blockquote><p id="41fc" class="pw-post-body-paragraph kl km it ko b kp ns ju kr ks nt jx ku lj nu kx ky lk nv lb lc ll nw lf lg lh im bi translated">为了做到这一点，我们修改了 RL 框架，如下图所示:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/dbb9152efc40662fbc08ebe68368715a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9483SlfLtCh_jDkzFZyeYQ.jpeg"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">与 Tutor4RL 框架相比的标准 RL 框架[摘自<a class="ae li" href="http://ceur-ws.org/Vol-2600/short9.pdf" rel="noopener ugc nofollow" target="_blank">原始出版物</a>。]</p></figure><p id="11b0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们添加了一个组件，我们称之为 Tutor，它包含了外部知识，是一个<strong class="ko iu">知识函数</strong>的集合。这些函数是普通的可编程函数，将状态和奖励作为输入，并输出一个向量，该向量带有每个动作的值，其方式类似于策略如何将状态映射到带有 Q 值的动作。有两种类型的知识函数:</p><ul class=""><li id="d9c6" class="mu mv it ko b kp kq ks kt lj mw lk mx ll my lh mz na nb nc bi translated"><strong class="ko iu">向导功能</strong>:这些功能向代理表达向导或提示，并被解释为建议，即使它们是错误的，代理也会根据自己的经验得知它们不好，不会遵循它们。</li><li id="cc0e" class="mu mv it ko b kp nd ks ne lj nf lk ng ll nh lh mz na nb nc bi translated"><strong class="ko iu">约束函数</strong>:这些函数限制了智能体的行为，当我们确定智能体在某个场景中不应该做某件事的时候，它们会很有用，也许是因为这可能会伤害到它自己，或者可能会导致一些危险的情况。约束函数告诉代理<strong class="ko iu">不要做什么</strong>，输出一个向量，其中 1 表示可以采取的每个动作，0 表示不可以采取的每个动作。</li></ul><p id="6a2f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">当代理不确定要做什么时，例如在最初的步骤中，它可以向导师询问它的知识。指导者将回复它的功能集合，然后代理将使用它来选择要执行的动作，并从这个经验中学习。以这种方式，指导者指导代理，但是代理总是能够发现来自指导者的建议是否是错误的，或者通过使用一些探索机制，例如ε-贪婪，还能够发现是否存在比指导者的选择更好的行动。然而，约束函数总是应用于向导函数和代理的策略，因此它提供了一个安全层来避免可能使代理处于危险中或对任务有很大负面影响的严重错误。</p><h1 id="8924" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">估价</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f317727906a50b95b923771fae87b7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*VOj3iMhHJNLkYbRi805-Dg.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">外部环境:<a class="ae li" href="https://gym.openai.com/envs/Breakout-v0/" rel="noopener ugc nofollow" target="_blank">从 OpenAI 健身房突围</a>。</p></figure><p id="116b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们已经在 Python 上使用<a class="ae li" href="https://github.com/keras-rl/keras-rl" rel="noopener ugc nofollow" target="_blank"> Keras-RL </a>实现了 Tutor4RL 的原型。我们已经将 Tutor4RL 应用到一个<a class="ae li" href="https://www.cl.uni-heidelberg.de/courses/ws17/reinforcement/MnihETAL15.pdf" rel="noopener ugc nofollow" target="_blank"> DQN 代理(Mnih et al. 2015) </a>，在 OpenAI Gym 上玩<a class="ae li" href="https://gym.openai.com/envs/Breakout-v0/" rel="noopener ugc nofollow" target="_blank">突围。我们使用了一个简单的引导函数，如果球不在横杠的正上方，它告诉代理向球所在的方向移动(不是说在这个测试中，我们没有使用约束函数)。此外，我们实现了一个非常简单的方法来控制代理何时使用导师的指南:我们定义了一个名为τ的参数，其使用方式与ε在ε-贪婪探索中的使用方式相同。当τ大于取自 U(0，1)分布的样本时，我们使用导师的输出，否则，我们使用策略的输出。我们将代理初始化为τ=1，并随时间线性减少，因此当代理启动时，导师的输出被大量使用，但随着代理积累更多经验，这种情况会减少。</a></p><p id="f877" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">下面，你可以看到 DQN 代理与 Tutor4RL 相比标准的普通 DQN 代理所获得的回报。突围中的奖励直接是游戏上达到的分数，所以你突破的格挡越多，分数越高，奖励也越高。正如你所看到的，在普通的 DQN 代理苦苦挣扎的时候，由于导师的指导，带 Tutor4RL 的 DQN 代理显示了最初的良好表现。普通 DQN 代理需要大约 130 万次迭代才能赶上，在 150 万次迭代中，导师完全停止使用。请注意，在此之前，导师是间歇性使用的，取决于每次迭代中减少的τ值。在那之后，我们可以看到两个代理有相似的表现，显示出被辅导的 DQN 代理也学会了但避免了普通 DQN 代理的粗糙开始。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi of"><img src="../Images/4f705cd70289494aa14dee48bb2f21b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kpSVCGrxOv6mg1zZVaRjDg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">与普通 DQN 代理人相比，使用 Tutor4RL 的 DQN 代理人的平均报酬【摘自<a class="ae li" href="http://ceur-ws.org/Vol-2600/short9.pdf" rel="noopener ugc nofollow" target="_blank">原始出版物</a>。]</p></figure><h1 id="e634" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结论</h1><p id="f214" class="pw-post-body-paragraph kl km it ko b kp mp ju kr ks mq jx ku lj mr kx ky lk ms lb lc ll mt lf lg lh im bi translated">Tutor4RL 已被证明能够帮助代理从其环境的一些知识开始，提高其初始步骤的性能。然而，Tutor4RL 仍在发展中，有几个方面可以改进:</p><ul class=""><li id="e441" class="mu mv it ko b kp kq ks kt lj mw lk mx ll my lh mz na nb nc bi translated">我们只测试了引导函数，所以接下来的步骤是实现约束函数的机制以及一些示例约束函数并测试它们。</li><li id="e392" class="mu mv it ko b kp nd ks ne lj nf lk ng ll nh lh mz na nb nc bi translated">用τ管理不确定性是一种幼稚的方法，可以通过使用其他方法得到很大改善，如自举法(<a class="ae li" href="https://arxiv.org/pdf/1702.01182.pdf" rel="noopener ugc nofollow" target="_blank"> Kahn 等人(2017) </a>)或贝叶斯法(Clements 等人(2020))。</li><li id="834a" class="mu mv it ko b kp nd ks ne lj nf lk ng ll nh lh mz na nb nc bi translated">可能有更好的方法来集成和使用来自知识函数的输出向量，利用函数之间的相关性，如<a class="ae li" href="https://link.springer.com/article/10.1007/s00778-019-00552-1" rel="noopener ugc nofollow" target="_blank">通气管(Ratner et al. (2019)) </a>。</li></ul><p id="08f3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">一如既往的感谢阅读！我希望你对这种方法感兴趣，并期待听到你的反馈。我相信还有更多方法可以改进 Tutor4RL。</p></div></div>    
</body>
</html>