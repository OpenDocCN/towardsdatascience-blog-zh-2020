<html>
<head>
<title>Bayesian Neural Networks with TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有张量流概率的贝叶斯神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6?source=collection_archive---------2-----------------------#2020-01-31">https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6?source=collection_archive---------2-----------------------#2020-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a7b9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率模型不确定性预测的逐步指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad80d1566dbfb4874da974c7bf3fe43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KDzmqkfpC4dIrOIAJxswaA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片via <a class="ae ky" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></figure><h1 id="c4ae" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="b866" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">机器学习</strong> </a>模型通常从数据开发为<a class="ae ky" href="https://en.wikipedia.org/wiki/Deterministic_system" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">确定性</strong> </a>机器，其使用通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">最大似然</strong> </a>方法计算的参数权重的<strong class="lt iu">点估计</strong>来映射输入到输出。然而，在后台有很多统计上的侥幸。例如，数据集本身是来自叠加了加性噪声的未知分布的任意大小的点的有限随机集合，并且对于这种特定的点集合，不同的模型(即不同的参数组合)可能是合理的。因此，对于正在进行的参数和预测，存在一些<a class="ae ky" href="https://en.wikipedia.org/wiki/Uncertainty_quantification" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">不确定性</strong> </a>。<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_statistics" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯统计</strong> </a>提供了一个框架来处理所谓的<a class="ae ky" href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2013/55997/V008T13A019/256671" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a><strong class="lt iu"/>不确定性，随着<a class="ae ky" href="https://www.tensorflow.org/probability/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">张量流概率</strong> </a>、<a class="ae ky" href="https://en.wikipedia.org/wiki/Statistical_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">概率建模</strong> </a>的发布，这已经变得容易多了，我将在这篇文章中演示。要知道不会提供任何理论背景；关于这个话题的理论，我真的可以推荐Gelman等人的书《贝叶斯数据分析》,这本书是免费的PDF文件。</p><div class="mn mo gp gr mp mq"><a href="http://www.stat.columbia.edu/~gelman/book/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">“贝叶斯数据分析”这本书的主页</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">这是本书的pdf格式，可以下载用于非商业目的。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.stat.columbia.edu</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><h1 id="fc81" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯神经网络</h1><p id="4396" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1801.07710" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯神经网络</strong> </a>的特征在于其在权重(参数)和/或输出上的分布。取决于是否考虑了听觉的、认知的或两者都考虑的不确定性，贝叶斯神经网络的代码看起来略有不同。为了演示工作原理，来自De Vito的<a class="ae ky" href="http://archive.ics.uci.edu/ml/datasets/Air+Quality" rel="noopener ugc nofollow" target="_blank">空气质量</a>数据集将作为一个例子。它包含来自不同污染物化学传感器的数据(如电压)以及作为一年时间序列的参考数据，这些数据是在一个汽车交通繁忙的意大利城市的主要街道上收集的，目标是构建从传感器响应到参考浓度的映射(图1)，即构建一个校准函数作为<a class="ae ky" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">回归</strong> </a>任务。</p><div class="kj kk kl km gt ab cb"><figure class="nf kn ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/142f158a6b357e23782c13e85e62f6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gHCYexUb6GgicwqUWclhZA.png"/></div></figure><figure class="nf kn ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c8cf92fa4fdbfa39f48469198c0dcc36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*iugiazEMVpnmunNM02z70w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nl di nm nn translated"><strong class="bd no">图1: </strong>参考和传感器数据。注意参考数据中的相关性。(图片由作者提供)</p></figure></div><h1 id="a0cb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">启动</h1><p id="ea0f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果您还没有安装TensorFlow Probability，您可以使用pip来完成，但是在此之前创建一个虚拟环境可能是一个好主意。(由于命令在以后的版本中可能会改变，您可能希望安装我用过的那些。)</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="833e" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Install libraries.</strong></span><span id="bff3" class="nu la it nq b gy nz nw l nx ny">pip install tensorflow<!-- -->==2.1.0<br/>pip install tensorflow-probability<!-- -->==0.9.0</span></pre><p id="4134" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">打开你最喜欢的编辑器或者JupyterLab。导入所有必要的库。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="86f9" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Load libriaries and functions.</strong></span><span id="879b" class="nu la it nq b gy nz nw l nx ny">import pandas as pd<br/>import numpy as np<br/>import tensorflow as tf<br/>tfk = tf.keras<br/>tf.keras.backend.set_floatx("float64")<br/>import tensorflow_probability as tfp<br/>tfd = tfp.distributions<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.ensemble import IsolationForest</span><span id="dafd" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Define helper functions.</strong></span><span id="cb4d" class="nu la it nq b gy nz nw l nx ny">scaler = StandardScaler()<br/>detector = IsolationForest(n_estimators=1000, behaviour="deprecated", contamination="auto", random_state=0)<br/>neg_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)</span></pre><p id="54c3" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">接下来，获取数据集(链接可以在上面找到)并将其作为熊猫数据框架加载。由于传感器容易因老化而漂移，最好丢弃过去六个月的数据。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="c2e3" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Load data and keep only first six months due to drift.</strong></span><span id="0f3d" class="nu la it nq b gy nz nw l nx ny">data = pd.read_excel("data.xlsx")<br/>data = data[data["Date"] &lt;= "2004-09-10"]</span></pre><h1 id="04a0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">预处理</h1><p id="bde5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">数据相当杂乱，必须先进行预处理。我们将关注大部分时间测量的输入和输出(一个传感器很早就死了)。删除缺少值的行后，对数据进行缩放。之后，使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Isolation_forest" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">隔离林</strong> </a>检测并移除异常值。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="0c98" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Select columns and remove rows with missing values.</strong></span><span id="50ef" class="nu la it nq b gy nz nw l nx ny">columns = ["PT08.S1(CO)", "PT08.S3(NOx)", "PT08.S4(NO2)", "PT08.S5(O3)", "T", "AH", "CO(GT)", "C6H6(GT)", "NOx(GT)", "NO2(GT)"]<br/>data = data[columns].dropna(axis=0)</span><span id="0a0c" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Scale data to zero mean and unit variance.</strong></span><span id="0cc7" class="nu la it nq b gy nz nw l nx ny">X_t = scaler.fit_transform(data)</span><span id="db76" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Remove outliers.</strong></span><span id="ea11" class="nu la it nq b gy nz nw l nx ny">is_inlier = detector.fit_predict(X_t)<br/>X_t = X_t[(is_inlier &gt; 0),:]</span><span id="da2b" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Restore frame.</strong></span><span id="864e" class="nu la it nq b gy nz nw l nx ny">dataset = pd.DataFrame(X_t, columns=columns)</span><span id="3d22" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Select labels for inputs and outputs.</strong></span><span id="88f7" class="nu la it nq b gy nz nw l nx ny">inputs = ["PT08.S1(CO)", "PT08.S3(NOx)", "PT08.S4(NO2)", "PT08.S5(O3)", "T", "AH"]<br/>outputs = ["CO(GT)", "C6H6(GT)", "NOx(GT)", "NO2(GT)"]</span></pre><h1 id="0cb8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据处理</h1><p id="0ad7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TensorFlow提供了一个<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">数据集类</strong> </a>来构造训练集和测试集。我们将使用70%的数据作为训练集。集合被混洗并且重复的批次被构造。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="af3e" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Define some hyperparameters.</strong></span><span id="204c" class="nu la it nq b gy nz nw l nx ny">n_epochs = 50<br/>n_samples = dataset.shape[0]<br/>n_batches = 10<br/>batch_size = np.floor(n_samples/n_batches)<br/>buffer_size = n_samples</span><span id="b3a2" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Define training and test data sizes.</strong></span><span id="0bbc" class="nu la it nq b gy nz nw l nx ny">n_train = int(0.7*dataset.shape[0])</span><span id="ff9b" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Define dataset instance.</strong></span><span id="905d" class="nu la it nq b gy nz nw l nx ny">data = tf.data.Dataset.from_tensor_slices((dataset[inputs].values, dataset[outputs].values))<br/>data = data.shuffle(n_samples, reshuffle_each_iteration=True)</span><span id="8f46" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Define train and test data instances.</strong></span><span id="888d" class="nu la it nq b gy nz nw l nx ny">data_train = data.take(n_train).batch(batch_size).repeat(n_epochs)<br/>data_test = data.skip(n_train).batch(1)</span></pre><h1 id="f9e6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">模型结构</h1><h2 id="3ce9" class="nu la it bd lb of og dn lf oh oi dp lj ma oj ok ll me ol om ln mi on oo lp op bi translated">听觉不确定性</h2><p id="3b5e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了解决由输出中的噪声引起的听觉不确定性，密集图层与概率图层相结合。更具体地，输出的均值和协方差矩阵被建模为输入和参数权重的函数。第一个隐藏层应包括10个节点，第二个隐藏层需要4个节点用于均值，加上10个节点用于最终层中的四维(有4个输出)多元高斯后验概率分布的方差和协方差。这是通过使用<a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/MultivariateNormalTriL" rel="noopener ugc nofollow" target="_blank">最后一层</a> ( <code class="fe oq or os nq b">MultivariateNormalTriL</code>)的<code class="fe oq or os nq b">params_size</code> <a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/MultivariateNormalTriL#params_size" rel="noopener ugc nofollow" target="_blank">方法</a>来实现的，这是<a class="ae ky" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">后验概率分布</strong> </a>结构的声明，在这种情况下是多元正态分布，其中仅估计了协方差矩阵的一半(由于对称性)。模型中的参数总数为224个——由<a class="ae ky" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">变分法</strong> </a>估算。这个神经网络的<a class="ae ky" href="https://github.com/gtancev/sensors/blob/master/NN_tf_keras/main_deterministic.ipynb" rel="noopener ugc nofollow" target="_blank">确定性版本由一个输入层、十个</a><a class="ae ky" href="https://en.wikipedia.org/wiki/Latent_variable" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">潜变量</strong> </a>(隐藏节点)和一个输出层(114个参数)组成，其中不包括参数权重的不确定性。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="3488" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Define prior for regularization.</strong></span><span id="0daf" class="nu la it nq b gy nz nw l nx ny">prior = tfd.Independent(tfd.Normal(loc=tf.zeros(len(outputs), dtype=tf.float64), scale=1.0), reinterpreted_batch_ndims=1)</span><span id="d5f7" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Define model instance.</strong></span><span id="2d5a" class="nu la it nq b gy nz nw l nx ny">model = tfk.Sequential([<br/>tfk.layers.InputLayer(input_shape=(len(inputs),), name="input"),<br/>tfk.layers.Dense(10, activation="relu", name="dense_1"),<br/>tfk.layers.Dense(tfp.layers.MultivariateNormalTriL.params_size(<br/>len(outputs)), activation=None, name="distribution_weights"),<br/>tfp.layers.MultivariateNormalTriL(len(outputs), activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior, weight=1/n_batches), name="output")<br/>], name="model")</span><span id="68f0" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Compile model.</strong></span><span id="5f47" class="nu la it nq b gy nz nw l nx ny">model.compile(optimizer="adam", loss=neg_log_likelihood)</span><span id="aadb" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Run training session.</strong></span><span id="7a58" class="nu la it nq b gy nz nw l nx ny">model.fit(data_train, epochs=n_epochs, validation_data=data_test, verbose=False)</span><span id="9c80" class="nu la it nq b gy nz nw l nx ny"><strong class="nq iu"># Describe model.</strong></span><span id="b566" class="nu la it nq b gy nz nw l nx ny">model.summary()</span></pre><p id="7553" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated"><code class="fe oq or os nq b">activity_regularizer</code> <strong class="lt iu"> </strong> <a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/KLDivergenceRegularizer" rel="noopener ugc nofollow" target="_blank">参数</a>作为输出层的先验(权重必须根据批次数进行调整)。根据您机器的规格，培训可能需要一段时间。该算法需要大约50个历元才能收敛(图2)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/2ac687f694b5564c8ad83869dee8521d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFeDKU0Q8h5_uHAt9lpI3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd no">图2: </strong>有训练和验证损失的学习曲线。(图片由作者提供)</p></figure><h2 id="7a9f" class="nu la it bd lb of og dn lf oh oi dp lj ma oj ok ll me ol om ln mi on oo lp op bi translated">听觉和认知的不确定性</h2><p id="07ac" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">考虑到听觉和认知的不确定性(参数权重的不确定性)，密集层必须与<a class="ae ky" href="https://arxiv.org/abs/1803.04386" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">翻转</strong> </a>层(<code class="fe oq or os nq b">DenseFlipout</code>)或<a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">变化</strong> </a>层(<code class="fe oq or os nq b">DenseVariational</code>)交换。这种模型有更多的参数，因为每个权重都由具有非共享平均值和标准偏差的正态分布来参数化，因此参数权重的数量加倍。对于不同的预测，权重将被重新采样，在这种情况下，贝叶斯神经网络将表现得像一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">集合</strong> </a>。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="11a8" class="nu la it nq b gy nv nw l nx ny">tfp.layers.DenseFlipout(10, activation="relu", name="dense_1")</span></pre><p id="e378" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">权重的默认先验分布是<code class="fe oq or os nq b">tfd.Normal(loc=0., scale=1.)</code>，可以使用<code class="fe oq or os nq b">kernel_prior_fn</code> <a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseFlipout" rel="noopener ugc nofollow" target="_blank">参数</a>进行调整。</p><h1 id="64de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">预言；预测；预告</h1><p id="8675" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因为是概率模型，所以执行<a class="ae ky" href="https://en.m.wikipedia.org/wiki/Monte_Carlo_method" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">蒙特卡罗实验</strong> </a>来提供预测。特别是，样本<em class="ou"> x </em>的每一次预测都会产生不同的输出<em class="ou"> y </em>，这就是为什么必须计算许多单独预测的期望值。此外，可以通过这种方式确定方差。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="df99" class="nu la it nq b gy nv nw l nx ny"><strong class="nq iu"># Predict.</strong></span><span id="06de" class="nu la it nq b gy nz nw l nx ny">samples = 500<br/>iterations = 10<br/>test_iterator = tf.compat.v1.data.make_one_shot_iterator(data_test)<br/>X_true, Y_true, Y_pred = np.empty(shape=(samples, len(inputs))), np.empty(shape=(samples, len(outputs))), np.empty(shape=(samples, len(outputs), iterations))<br/>for i in range(samples):<br/>    features, labels = test_iterator.get_next()<br/>    X_true[i,:] = features<br/>    Y_true[i,:] = labels.numpy()<br/>    for k in range(iterations):<br/>        Y_pred[i,:,k] = model.predict(features)<br/>        <br/><strong class="nq iu"># Calculate mean and standard deviation.</strong></span><span id="0cb1" class="nu la it nq b gy nz nw l nx ny">Y_pred_m = np.mean(Y_pred, axis=-1)<br/>Y_pred_s = np.std(Y_pred, axis=-1)</span></pre><p id="fe97" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">图3显示了所有输出的测量数据与预测值的对比。<a class="ae ky" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">决定系数</strong> </a>约为0.86，<strong class="lt iu">斜率</strong>为0.84——不算太差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/65ac53eb9ca45995b851edfd3fce1093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06I2E8rv62CUw8IujuzG2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd no">图3: </strong>测量值与预测值的一致性。(图片由作者提供)</p></figure><p id="4a7f" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">预测的不确定性可以通过绘制误差线和期望值来可视化(图4)。在这种情况下，误差棒是标准偏差的1.96倍，即占概率的95%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d2fa33f2206e7eddb9769e1b3c1be0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiUhYpiUlX3db8UIxZb8Jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd no">图4: </strong>数据与预测的比较。(图片由作者提供)</p></figure><p id="dd1a" class="pw-post-body-paragraph lr ls it lt b lu oa ju lw lx ob jx lz ma oc mc md me od mg mh mi oe mk ml mm im bi translated">理论上，由于额外的不确定性信息，贝叶斯方法优于确定性方法，但由于其高计算成本，并不总是可行的。最近的研究围绕着开发新的方法来克服这些限制。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">用贝叶斯推理量化预测的不确定性</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">贝叶斯统计最重要的特征之一。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="ox l nb nc nd mz ne ks mq"/></div></div></a></div></div></div>    
</body>
</html>