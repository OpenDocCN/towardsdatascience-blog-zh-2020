<html>
<head>
<title>Speed up training and improve performance in deep neural net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速训练并提高深度神经网络的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-training-and-improve-performance-in-deep-neural-net-5732274d51a2?source=collection_archive---------44-----------------------#2020-05-30">https://towardsdatascience.com/speed-up-training-and-improve-performance-in-deep-neural-net-5732274d51a2?source=collection_archive---------44-----------------------#2020-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="478c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为 DNN 实现足够的初始化、激活功能和批量归一化/梯度剪裁</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/13c2ea02dc7cb8a75442bafd8c793644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qIvtburn8ZtDOwW81kqI0A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="301a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di"> T </span>训练一个大而深的神经网络是一项耗时耗力的任务，也是 20-30 年前 DNN 不受欢迎的主要原因。随着几种提高训练速度的技术被发现，深度学习重新回到了人们的视线中。那么使用哪种技术，如何以及何时使用哪种技术呢？在这里讨论一下吧！</p><p id="81cf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ma">时尚 MNIST 和加州住房数据集将分别用作分类&amp;回归的示例。</em></p><h1 id="5809" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">1.应用初始化</h1><p id="34b4" class="pw-post-body-paragraph kv kw iq kx b ky mt jr la lb mu ju ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">初始化是用于加快神经元网络训练时间(以及提高性能)的第一批技术之一。在人工神经网络(ANN)中，不同神经元之间存在着大量的连接。当前层中的一个神经元连接到下一层中的几个神经元，并附着到前一层中的各个神经元。如果两个神经元比另一对更频繁地交互，它们的连接(即权重)将比另一对更强。</p><p id="6e78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，人工神经网络的一个问题是，如果在训练开始时没有指定权重，连接可能会以太小或太大的值开始，从而使它们太小或太大，无法在网络中进一步使用。换句话说，网络会陷入<strong class="kx ir">消失梯度</strong>或<strong class="kx ir">爆炸梯度</strong>的问题。</p><p id="19c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，如果从训练开始就将权重设置为合适的随机值，就可以避免这些问题。<em class="ma"> Xavier 初始化</em>或<em class="ma"> Glorot 初始化</em>技术是由<a class="ae my" href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> Glorot 和 Bengio </a>提出的，然后显著解除了这些不稳定问题。</p><p id="68a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在该策略中，神经元之间的连接权重将使用均值=0、方差σ= 2/(fan_in+fan_out)的正态分布随机初始化，其中 fan _ in 是输入神经元的数量，fan _ out 是输出神经元的数量。</p><p id="ed0c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了<strong class="kx ir"> Glorot </strong>(在 Keras 中默认使用)之外，还有另外 2 种流行的初始化技术:<strong class="kx ir"> He </strong>和<strong class="kx ir"> LeCun </strong>。</p><p id="baec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用<code class="fe mz na nb nc b">Fashion MNIST</code>数据集检查不同的初始化技术对模型性能和训练时间的影响。</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="29f9" class="nh mc iq nc b gy ni nj l nk nl"># Take a look at the dataset<br/>plt.figure(figsize=(10, 10)) <br/>for row in range(5): <br/>    for col in range(5): <br/>        index = 5 * row + col <br/>        plt.subplot(5, 5, index + 1)<br/>        plt.imshow(X_train_full[index], cmap="binary",<br/>                   interpolation="nearest") <br/>        plt.axis('off') <br/>        plt.title(y_train_full[index], fontsize=12) <br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/710fbf1e8c0db53d31fcbd669f4c81af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/0*agDFj7iJSFHX_LOz.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">以下是时尚 MNIST 的示例，其中预测值是一组代表图像的[28，28]形状的值；目标值是 10 种衣服和鞋子(用 0 到 9 表示)</p></figure><p id="1088" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，让我们从一个由 5 个隐藏层和 300，100，50，50，50 个神经元组成的网络上 Keras 的默认设置开始。</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="dd82" class="nh mc iq nc b gy ni nj l nk nl">tf.random.set_seed(50) <br/>np.random.seed(50) <br/>model_default = keras.models.Sequential() model_default.add(keras.layers.Flatten(input_shape=[28, 28])) <br/>for n_layers in (300, 100, 50, 50, 50): <br/>    model_default.add(keras.layers.Dense(n_layers, <br/>                                         activation ='relu')) <br/>model_default.add(keras.layers.Dense(10, activation='softmax'))</span><span id="2c0a" class="nh mc iq nc b gy nn nj l nk nl">model_default.compile(loss="sparse_categorical_crossentropy",<br/>                      optimizer=keras.optimizers.SGD(lr=1e-3), <br/>                      metrics=["accuracy"]) <br/>start_time = time.time() <br/>history = model_default.fit(X_train_full, y_train_full, epochs=20,<br/>                            validation_split=0.1) <br/>print("--- %s seconds ---" % (time.time() - start_time))</span></pre><p id="7104" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="be6c" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 20/20 1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518 <br/>--- 99.03307843208313 seconds ---</span></pre><p id="a72b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练集在 99.3 秒内达到 85.26%的准确率，Val 集达到 85.18%。如果没有设置<code class="fe mz na nb nc b">activation ='relu'</code>(即隐层没有激活函数)，准确率分别为 85.32%和 84.95%，训练需要 104.5 秒。</p><p id="fd10" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将其与全 0 和全 1 的权重初始化进行比较:</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="0488" class="nh mc iq nc b gy ni nj l nk nl"># Zeros initialization Epoch 20/20 1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925 <br/>--- 69.43926930427551 seconds ---</span><span id="7a48" class="nh mc iq nc b gy nn nj l nk nl"># Ones initialization Epoch 20/20 1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925 <br/>--- 67.2280786037445 seconds ---</span></pre><p id="b4fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两种情况下的性能都差得多，并且实际上，该模型从第 5 个时期开始就停止提高精度。</p><p id="baea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们试试<code class="fe mz na nb nc b">He Initialization</code>，在 Keras 中通过向隐藏层添加<code class="fe mz na nb nc b">kernel_initializer="he_normal"</code>参数来启用。</p><p id="31ab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="e5b6" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 20/20 1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637 <br/>--- 99.76096153259277 seconds ---</span></pre><p id="71db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">精确度确实提高了，但是运行时间比<strong class="kx ir"> Glorot 初始化</strong>慢了半秒。</p><p id="68c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于初始化技术中<strong class="kx ir">正态分布</strong>和<strong class="kx ir">均匀分布</strong>的性能也有讨论，但确实没有一种技术表现出比另一种更好的性能。<code class="fe mz na nb nc b">init = keras.initializers.VarianceScaling(scale=2.,mode='fan_avg',distribution='uniform')</code>的结果对该数据集没有改善(训练集准确率:87.05%，值集:86.27%，运行时间 100.82 秒)</p><h1 id="f5d9" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">2.与正确的激活功能相处</h1><p id="0ef1" class="pw-post-body-paragraph kv kw iq kx b ky mt jr la lb mu ju ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">选择不合适的激活函数是导致模型性能差的原因之一。<code class="fe mz na nb nc b">sigmoid</code>可能是个不错的选择，但我更喜欢用<strong class="kx ir"> SELU、ReLU 或 ReLU 变体</strong>来代替。</p><p id="2e1f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">先说<strong class="kx ir"> ReLU </strong>吧。简单来说，如果值大于 0，函数返回值本身；否则它返回 0。这种激活计算起来很快，但作为回报，将会出现停止输出除 0 以外的任何值的情况(即神经元死亡)。这个问题通常发生在学习率较大的情况下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/32f8420d779ae4c826b7905c39008e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FNH9Ydr5rVX3e2BE.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1020" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个问题的一些解决方案是使用 ReLU 的替代版本:<strong class="kx ir"> LeakyReLU、随机化 LeakyReLU 或缩放 ReLU (SELU) </strong>。</p><p id="3061" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">带有<strong class="kx ir">泄漏的路</strong>:</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="043a" class="nh mc iq nc b gy ni nj l nk nl">if x&gt;0: <br/>   return x <br/>else: <br/>   return ax</span></pre><p id="164b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中 a 是给定 x &lt;0. a is usually set at 0.01, serving as a small leak (that’s why this technique is called LeakyReLU). Using a helps to stop the dying problem (i.e. slope=0).</p><p id="0681" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">In the case of <strong class="kx ir">随机泄漏率</strong>时 x 的斜率，a 是在给定范围内随机选择的。这种方法可以减少过拟合问题，但是由于额外的计算需要更多的运行时间。</p><p id="01a7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">DNN 的一个出色的激活函数是<strong class="kx ir">缩放的 ReLU (SELU) </strong>。</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="aeb9" class="nh mc iq nc b gy ni nj l nk nl">if x&gt;0: <br/>   return Lambda*x <br/>else: <br/>   return Lambda*(alpha*exp(x)-alpha)</span></pre><p id="0bd9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在此函数中，每层输出的平均值为 0，标准差为 1。注意使用此激活功能时:</p><ul class=""><li id="6ffb" class="np nq iq kx b ky kz lb lc le nr li ns lm nt lq nu nv nw nx bi translated">必须和<code class="fe mz na nb nc b">kernel_initializer="lecun_normal"</code>一起使用</li><li id="776d" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">输入要素必须标准化</li><li id="df93" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">神经网络的结构必须是顺序的</li></ul><p id="b3d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们在<code class="fe mz na nb nc b">fashion MNIST</code>数据集上尝试不同的激活函数。</p><p id="bd96" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">泄漏的结果</strong></p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="6609" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 20/20 1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615 <br/>--- 101.87710905075073 seconds ---</span></pre><p id="66db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">随机化泄漏结果</strong></p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="a1a6" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 20/20 1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630 <br/>--- 113.58738899230957 seconds ---</span></pre><p id="f28c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SELU<strong class="kx ir">的结果</strong></p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="0f96" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 19/20 1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647 <br/>--- 106.25733232498169 seconds ---</span></pre><p id="6cb1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> SELU </strong>似乎比 ReLU 及其变种的性能稍好，但速度较慢(如预期)。</p><blockquote class="od oe of"><p id="3b3b" class="kv kw ma kx b ky kz jr la lb lc ju ld og lf lg lh oh lj lk ll oi ln lo lp lq ij bi translated"><strong class="kx ir">如果神经网络在低学习速率下表现相对较好，ReLU 是给定最快训练时间的最佳选择。在深度 NN 的情况下，SELU 是一个极好的尝试。</strong></p></blockquote><p id="5f62" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于这些激活功能的详细说明可以在这里找到:<a class="ae my" href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> ReLU </a>、<a class="ae my" href="https://arxiv.org/abs/1505.00853" rel="noopener ugc nofollow" target="_blank"> LeakyReLU、随机化 LeakyReLU </a>和<a class="ae my" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank"> SELU </a></p><h1 id="51f6" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">3.批量标准化</h1><p id="9400" class="pw-post-body-paragraph kv kw iq kx b ky mt jr la lb mu ju ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">为了确保消失/爆炸梯度问题不会在训练期间再次发生(因为初始化和激活功能有助于在训练开始时减少这些问题)，实施<strong class="kx ir">批量标准化</strong>。</p><p id="84ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">批量归一化零点对每个输入进行居中和归一化，然后使用 1 个参数向量进行缩放和 1 个参数向量进行平移来缩放和平移结果。此技术评估当前小批量上输入的$均值$和$标准差$并在训练集的所有小批量上重复此计算。均值和标准差是在训练期间估计的，但仅在训练后使用。</p><p id="5be3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入平均值的向量和输入标准偏差的向量将成为不可训练的参数(即反向传播不可触及的)，并用于计算训练结束时的移动平均值。随后，这些最终参数将用于归一化新数据以进行预测。</p><p id="6928" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果使用<strong class="kx ir">批量标准化</strong>，输入数据在训练前不需要标准化。</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="b173" class="nh mc iq nc b gy ni nj l nk nl">tf.random.set_seed(50) <br/>np.random.seed(50)</span><span id="6710" class="nh mc iq nc b gy nn nj l nk nl">model_default = keras.models.Sequential() model_default.add(keras.layers.Flatten(input_shape=[28, 28])) <br/>for n_layers in (300, 100, 50, 50, 50):<br/>    model_default.add(keras.layers.BatchNormalization())<br/>    model_default.add(keras.layers.Dense(n_layers, <br/>                                         activation ='relu',<br/>                                  kernel_initializer="he_normal")) <br/>model_default.add(keras.layers.Dense(10, activation='softmax'))</span><span id="2e4c" class="nh mc iq nc b gy nn nj l nk nl">model_default.compile(loss="sparse_categorical_crossentropy",                       <br/>                      optimizer=keras.optimizers.SGD(lr=1e-3),                       <br/>                      metrics=["accuracy"])  <br/>start_time = time.time() <br/>history = model_default.fit(X_train_full, y_train_full, epochs=20, <br/>                            validation_split=0.1) <br/>print("--- %s seconds ---" % (time.time() - start_time))</span></pre><p id="5ca3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果</p><pre class="kg kh ki kj gt nd nc ne nf aw ng bi"><span id="28ce" class="nh mc iq nc b gy ni nj l nk nl"># Show the highest accuracy epoch <br/>Epoch 20/20 1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685 <br/>--- 167.6186249256134 seconds ---</span></pre><p id="0fa4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">显然，在<strong class="kx ir">批量标准化</strong>中训练较慢，因为在训练过程中需要更多的计算，但相比之下，模型收敛更快，因此达到相同的性能需要更少的时期。</p><h1 id="4dcd" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">4.渐变剪辑</h1><p id="592f" class="pw-post-body-paragraph kv kw iq kx b ky mt jr la lb mu ju ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">由于<strong class="kx ir">批量归一化</strong>不建议与递归神经网络一起使用，<strong class="kx ir">梯度裁剪</strong>是 RNN 的替代选择。</p><p id="08a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于<a class="ae my" href="https://arxiv.org/abs/1211.5063" rel="noopener ugc nofollow" target="_blank">渐变裁剪</a>的细节</p><h1 id="6c91" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">时尚 MNIST 数据集分类结果汇总</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/b726ca9237b7281ff3ac3c4de8f3bd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGGfJkCVMNJs2owQuOvalA.png"/></div></div></figure><h1 id="666d" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">加州住房数据集回归任务的结果摘要</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/bd98012ee88606129ed285bf5e2ac61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4ICJ3GAgjZCt_y-cZ6PVQ.png"/></div></div></figure><p id="91e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练集和验证集的均方误差</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/aaa5c876ae480b20a45189c40a1fc3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JHdanMhA7RBaQNr4.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/377f9f7bfadbb2bddeb2fbe6b9c7c83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*5t0ng8xpVboly_GQ.png"/></div></figure><h1 id="ea6a" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">关于这一部分的最后想法🔆</h1><ul class=""><li id="6e94" class="np nq iq kx b ky mt lb mu le on li oo lm op lq nu nv nw nx bi translated">对于大多数情况，Glorot 初始化是一个很好的起点。初始化技术有时比 Glorot 执行得更好(在上述分类示例中较慢，而在回归示例中较快)。</li><li id="0914" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">如果优先考虑运行时间，ReLU 或 Leaky ReLU 是很好的选择。</li><li id="1a10" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">如果使用高学习率，应避免 ReLU。</li><li id="bbfe" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">对于复杂的数据集和深度神经网络，SELU 是一个很好的选择，但可能会在运行时间上有所取舍。然而，如果神经网络的架构不允许<em class="ma">自规范化</em>，则使用 ELU 而不是 SELU。</li><li id="cf90" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">SELU 和批处理规范化不能应用于 RNN。梯度裁剪是 RNN 批量标准化的替代策略。</li></ul><p id="eb8f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">源代码可以在<a class="ae my" href="https://github.com/geniusnhu/DNN-Improvement/blob/master/Improve_DNN_performance.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得</p></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><p id="901d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ma">参考:</em></p><ol class=""><li id="43be" class="np nq iq kx b ky kz lb lc le nr li ns lm nt lq ox nv nw nx bi translated">x .格洛特和 y .本吉奥(2010 年)。理解训练深度前馈神经网络的困难。PMLR</li><li id="0e18" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq ox nv nw nx bi translated">何刚，张，徐，任，孙(2015)。深入研究整流器:在 ImageNet 分类上超越人类水平的性能。2015 年 IEEE 计算机视觉国际会议(ICCV)论文集</li><li id="5d2d" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq ox nv nw nx bi translated">Geron，A. (2019)。使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习。奥莱利媒体公司，</li><li id="58aa" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq ox nv nw nx bi translated">徐，王，陈，李，米(2015)。卷积网络中校正激活的经验评估。2020 年 5 月 5 日从<a class="ae my" href="https://arxiv.org/abs/1505.00853" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.00853</a>取回。</li><li id="44e2" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq ox nv nw nx bi translated">Klambauer，g .，Unterthiner，t .，Mayr，a .，&amp; Hochreiter，S. (2017 年)。自标准化神经网络。神经信息处理系统进展 30 (NIPS 2017)</li></ol></div></div>    
</body>
</html>