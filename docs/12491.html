<html>
<head>
<title>One-Step Predictions with LSTM: Forecasting Hotel Revenues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM 一步预测法:预测酒店收入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/one-step-predictions-with-lstm-forecasting-hotel-revenues-c9ef0d3ef2df?source=collection_archive---------15-----------------------#2020-08-28">https://towardsdatascience.com/one-step-predictions-with-lstm-forecasting-hotel-revenues-c9ef0d3ef2df?source=collection_archive---------15-----------------------#2020-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="333a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 LSTM 生成单步预测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bfa2b80348dc947fe94dc5d2dfaff3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVZFZHSXnKDbz1tOtlMQ5g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:图片来自<a class="ae ky" href="https://pixabay.com/photos/bedroom-hotel-room-white-bedding-1285156/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/Pexels-2286921/" rel="noopener ugc nofollow" target="_blank"> Pexels </a></p></figure><p id="fba0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意:这是对我之前的文章</em> <a class="ae ky" rel="noopener" target="_blank" href="/forecasting-average-daily-rate-trends-for-hotels-using-lstm-93a31e01190a"> <em class="lv">的更新，预测使用 LSTM </em> </a> <em class="lv">的酒店的平均每日价格趋势。此后，我意识到了最初分析中的几个技术错误，并决定写一篇新文章来解决这些问题，并扩展我之前的分析。</em></p><h1 id="7ff8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">背景</h1><p id="0a8c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这种情况下，使用 LSTM 模型的目的是预测酒店的平均每日房价。</p><p id="7134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ADR 计算如下:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="4844" class="my lx it mu b gy mz na l nb nc">ADR = Revenue ÷ sold rooms</span></pre><p id="92aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本例中，计算了客户每周的平均 ADR，并将其公式化为时间序列。然后，LSTM 模型用于逐周预测这一指标。</p><p id="757f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Antonio、Almeida 和 Nunes (2019)的原始研究可在<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S2352340918315191" rel="noopener ugc nofollow" target="_blank">此处</a>找到。</p><p id="1a9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用熊猫，每周计算平均 ADR。以下是每周 ADR 趋势图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/9097e97f601f38a30b19633757b888af.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*mqJIeTG8ziTvDxeRxVRjKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter 笔记本输出</p></figure><p id="3956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，本例中的 Jupyter 笔记本可以在本文末尾找到。</p><h1 id="970c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据准备</h1><h2 id="9a93" class="my lx it bd ly ne nf dn mc ng nh dp mg li ni nj mi lm nk nl mk lq nm nn mm no bi translated">1.用 MinMaxScaler 归一化数据</h2><p id="ce21" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">与任何神经网络一样，数据需要进行缩放，以便网络进行正确的解释，这一过程称为标准化。<strong class="lb iu">最小最大缩放器</strong>用于此目的。</p><p id="af68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这伴随着一个警告。在数据被分成训练集、验证集和测试集之后，必须进行<strong class="lb iu">缩放，每个数据集都被单独缩放。第一次使用 LSTM 时的一个常见错误(这是我自己犯的错误)是在分割数据之前先对数据进行归一化。</strong></p><p id="5e79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是错误的，因为归一化技术将使用来自验证和测试集的数据作为整体缩放数据时的参考点。这将无意中影响训练数据的值，本质上导致验证和测试集的数据泄漏。</p><p id="a2ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这方面，100 个数据点被分成训练集和验证集，最后 15 个数据点作为测试数据，用于与 LSTM 预测进行比较。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="2c1d" class="my lx it mu b gy mz na l nb nc">train_size = int(len(df) * 0.8)<br/>val_size = len(df) - train_size<br/>train, val = df[0:train_size,:], df[train_size:len(df),:]</span></pre><p id="b269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形成数据集矩阵:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="10ff" class="my lx it mu b gy mz na l nb nc">def create_dataset(df, previous=1):<br/>    dataX, dataY = [], []<br/>    for i in range(len(df)-previous-1):<br/>        a = df[i:(i+previous), 0]<br/>        dataX.append(a)<br/>        dataY.append(df[i + previous, 0])<br/>    return np.array(dataX), np.array(dataY)</span></pre><p id="dcc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此时，训练数据可以按如下方式缩放:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8306" class="my lx it mu b gy mz na l nb nc">scaler = MinMaxScaler(feature_range=(0, 1))<br/>train = scaler.fit_transform(train)<br/>train</span></pre><p id="370d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是输出示例:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="602b" class="my lx it mu b gy mz na l nb nc">array([[0.35915778],<br/>       [0.42256282],<br/>       [0.53159902],<br/>...<br/>       [0.0236608 ],<br/>       [0.11987636],<br/>       [0.48651694]])</span></pre><p id="d007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，验证数据也以同样的方式进行缩放:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d95b" class="my lx it mu b gy mz na l nb nc">val = scaler.fit_transform(val)<br/>val</span></pre><h2 id="9b08" class="my lx it bd ly ne nf dn mc ng nh dp mg li ni nj mi lm nk nl mk lq nm nn mm no bi translated">2.定义回顾期</h2><p id="52f2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">“回顾周期”定义了使用多少个先前的时间步长来预测随后的时间步长。对此，我们采用的是<strong class="lb iu">一步预测</strong>模型。</p><p id="edf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，回看周期被设置为<strong class="lb iu"> 5 </strong>。这意味着我们使用<em class="lv"> t-4、t-3、t-2、t-1、</em>和<em class="lv"> t </em>的时间步长来预测时间<em class="lv"> t+1 </em>的值。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="dbec" class="my lx it mu b gy mz na l nb nc"># Lookback period<br/>lookback = 5<br/>X_train, Y_train = create_dataset(train, lookback)<br/>X_val, Y_val = create_dataset(val, lookback)</span></pre><p id="5c4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意回望周期的选择是一个相当随意的过程。在这种情况下，显示了 5 的回看窗口，以展示在测试集上的最佳预测性能。然而，另一种选择是使用 PACF 指示的滞后数量来设置回看窗口的大小，如<a class="ae ky" href="https://datascience.stackexchange.com/questions/38692/lstm-for-time-series-which-window-size-to-use" rel="noopener ugc nofollow" target="_blank">数据科学堆栈交换</a>所述。</p><p id="29ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看<strong class="lb iu"> X_train </strong>的归一化窗口。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5f57" class="my lx it mu b gy mz na l nb nc">array([[0.35915778, 0.42256282, 0.53159902, 0.6084246 , 0.63902841],<br/>       [0.42256282, 0.53159902, 0.6084246 , 0.63902841, 0.70858066],<br/>       [0.53159902, 0.6084246 , 0.63902841, 0.70858066, 0.75574219],<br/>...</span></pre><p id="5abb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是前三个条目。我们可以看到，紧接在我们试图预测的一个时间步骤之前的五个时间步骤以步进运动的方式移动。</p><p id="b52c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，第一个条目在时间<em class="lv"> t </em>显示 0.63902841。在第二个条目中，该值现在向后移动到时间<em class="lv"> t-1 </em>。</p><p id="9a57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们举一个适用于这种情况的例子。例如，对于希望预测第 26 周的 ADR 值的酒店，酒店将使用该模型，使用第 21、22、23、24 和 25 周的数据来进行前一周的预测。</p><p id="5834" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，输入被整形为<strong class="lb iu">【样本、时间步长、特征】</strong>格式。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="cd1a" class="my lx it mu b gy mz na l nb nc"># reshape input to be [samples, time steps, features]<br/>X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))<br/>X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))</span></pre><p id="3f05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，输入的形状是<strong class="lb iu">【74，1，1】</strong>。</p><p id="4b77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练数据中存在 74 个样本，模型在 1 的时间步长上运行，并且在模型中使用 1 个特征，即时间序列的滞后版本。</p><h1 id="9880" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">LSTM 模型</h1><p id="4c0a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">LSTM 模型定义如下:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1e36" class="my lx it mu b gy mz na l nb nc"># Generate LSTM network<br/>model = tf.keras.Sequential()<br/>model.add(LSTM(4, input_shape=(1, lookback)))<br/>model.add(Dense(1))<br/>model.compile(loss='mean_squared_error', optimizer='adam')<br/>history=model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=1, verbose=2)</span></pre><p id="0cc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用 4 个神经元创建 LSTM 模型。考虑到我们正在处理一个回归问题，均方误差被用作损失函数。此外，还使用了 adam 优化器，进行了 100 多个时期的训练，验证比例为 20%。</p><p id="5978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是培训和验证损失的直观概述:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ae9b" class="my lx it mu b gy mz na l nb nc"># list all data in history<br/>print(history.history.keys())<br/># summarize history for accuracy<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'val'], loc='upper left')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8c41c25f3ba441f2beae7b8469ce9529.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*r_6WsOaiXUbELg-2UT9HTA.png"/></div></figure><p id="3a87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，在验证损失的初始增加之后，损失在大约 10 个时期之后开始减少。</p><p id="b93b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，预测被转换回原始比例:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8f3c" class="my lx it mu b gy mz na l nb nc"># Convert predictions back to normal values<br/>trainpred = scaler.inverse_transform(trainpred)<br/>Y_train = scaler.inverse_transform([Y_train])<br/>valpred = scaler.inverse_transform(valpred)<br/>Y_val = scaler.inverse_transform([Y_val])<br/>predictions = valpred</span></pre><p id="4667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练和验证集上计算均方根误差:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e5f1" class="my lx it mu b gy mz na l nb nc"># calculate RMSE<br/>trainScore = math.sqrt(mean_squared_error(Y_train[0], trainpred[:,0]))<br/>print('Train Score: %.2f RMSE' % (trainScore))<br/>valScore = math.sqrt(mean_squared_error(Y_val[0], valpred[:,0]))<br/>print('Validation Score: %.2f RMSE' % (valScore))</span></pre><p id="30d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获得的 RMSE 值如下:</p><ul class=""><li id="b145" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated"><strong class="lb iu">列车误差:</strong> 3.88 RMSE</li><li id="176c" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu">验证错误:</strong> 8.78 RMSE</li></ul><p id="5e4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个验证集的平均 ADR 值为 69.99，相比之下，验证误差非常小(约为平均值的 12%)，这表明该模型在预测 ADR 值方面做得很好。</p><p id="77eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个跨训练和验证集的预测与实际 ADR 值的图表。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5d98" class="my lx it mu b gy mz na l nb nc"># Plot all predictions<br/>inversetransform, =plt.plot(scaler.inverse_transform(df))<br/>trainpred, =plt.plot(scaler.inverse_transform(trainpredPlot))<br/>valpred, =plt.plot(scaler.inverse_transform(valpredPlot))<br/>plt.xlabel('Number of weeks')<br/>plt.ylabel('Cancellations')<br/>plt.title("Predicted vs. Actual Weekly ADR")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bc6f9f735390dd38053e6929c3e778bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*SE3KUYq78iUf1CJlQ8RuBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter 笔记本输出</p></figure><p id="74cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，LSTM 模型通常捕捉时间序列的方向振荡。然而，在 ADR 的极端峰值期间，例如第 60 周，该模型的表现似乎不太好。</p><p id="6b66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，为了完全确定模型是否具有预测能力，现在将使用它来预测系列中的最后 15 个时间步长，即测试数据。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="22c3" class="my lx it mu b gy mz na l nb nc">Xnew = np.array([tseries.iloc[95:100],tseries.iloc[96:101],tseries.iloc[97:102],tseries.iloc[98:103],tseries.iloc[99:104],tseries.iloc[100:105],tseries.iloc[101:106],tseries.iloc[102:107],tseries.iloc[103:108],tseries.iloc[104:109],tseries.iloc[105:110],tseries.iloc[106:111],tseries.iloc[107:112],tseries.iloc[108:113],tseries.iloc[109:114]])</span></pre><p id="b1ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，Xnew 使用前面的五个时间步长在时间<em class="lv"> t+1 </em>进行预测。例如，第 95 至 100 周用于预测第 101 周的 ADR 值，然后第 96 至 101 周用于预测第 102 周，依此类推。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e328494a58f88a69a6d53452597b018d.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*zBhF1yI4d_BjwLSIoHb8Nw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter 笔记本输出</p></figure><p id="5490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图显示了 LSTM 预测值与测试集(系列中的最后 15 个点)中的实际 ADR 值。</p><p id="ccf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获得的 RMSE 和 MAE(平均绝对误差)值如下:</p><ul class=""><li id="e25a" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated"><strong class="lb iu">梅:</strong> -27.65</li><li id="6e14" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu"> RMSE: </strong> 31.91</li></ul><p id="b374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试集的 RMSE 误差明显高于验证集的误差——这是意料之中的，因为我们正在处理看不见的数据。</p><p id="676d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在测试集上的平均 ADR 值为<strong class="lb iu"> 160 </strong>的情况下，RMSE 误差约为平均值大小的 20%,表明 LSTM 在确定下一时间步的值时仍然具有相当强的预测能力。</p><p id="ab7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，人们希望使用大得多的数据样本来验证 LSTM 是否能保持对新数据的预测能力。此外，如这个<a class="ae ky" href="https://www.reddit.com/r/datascience/comments/79hjzn/how_good_is_lstm_for_time_series_forecasting/" rel="noopener ugc nofollow" target="_blank"> Reddit 线程</a>所示，LSTMs 可能会根据数据样本的大小而过度拟合。</p><p id="0bca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这方面，需要更大的数据样本来验证该模型在现实世界中是否可行。然而，这个案例的初步结果看起来很有希望。</p><h1 id="3c3e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="65f0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本例中，您看到了:</p><ul class=""><li id="1b5c" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated">如何正确设置数据格式以使用 LSTM 模型</li><li id="69d3" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">一步 LSTM 预测模型的建立</li><li id="7879" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">解释 RMSE 和梅值，以确定模型的准确性</li></ul><p id="fcc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢您的阅读，非常感谢您的任何反馈或问题。你可以在这里找到这个例子<a class="ae ky" href="https://github.com/MGCodesandStats/hotel-cancellations" rel="noopener ugc nofollow" target="_blank">的 Jupyter 笔记本。</a></p><p id="d604" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我还强烈推荐 Machine Learning Mastery 的本<a class="ae ky" href="https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">教程，它被用作设计本例中使用的 LSTM 模型的指南。</a></p><p id="4bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">免责声明:本文是在“原样”的基础上编写的，没有任何担保。本文旨在提供数据科学概念的概述，不应以任何方式解释为专业建议。作者与本文提及的任何第三方无任何关系。</em></p></div></div>    
</body>
</html>