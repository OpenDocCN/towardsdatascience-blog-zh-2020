<html>
<head>
<title>Temporal-Difference Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时差学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850?source=collection_archive---------20-----------------------#2020-03-01">https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850?source=collection_archive---------20-----------------------#2020-03-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7e63" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/a-journey-into-r-l" rel="noopener" target="_blank">强化学习之旅</a></h2><div class=""/><div class=""><h2 id="de14" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过经验引导优化价值函数。</h2></div><p id="a886" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">欢迎来到我的强化学习研究的下一个激动人心的章节，在这一章中，我们将讨论时差学习。和往常一样，我会在文章底部链接到那些教导和指导我的资源。</p><p id="3434" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我的前两篇帖子中，我们谈到了<em class="ln">动态规划</em> (DP)和<em class="ln">蒙特卡洛</em> (MC)方法。时差学习是这两种思想在几个方面的结合。像MC一样，TD直接从经历事件中学习，不需要环境模型。像DP一样，TD学习可以从不完整的剧集中发生，利用一种叫做<em class="ln"> bootstrapping </em>的方法来估计该剧集的剩余回报。基本上，它是对价值函数进行猜测，采取一些步骤，然后进行另一次猜测，朝着这个新的猜测更新我们的原始猜测。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/719f01ec4287d1c67b46894f291e68c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*Ms80os43GYglmakAUya89Q.png"/></div></figure><p id="f241" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中<em class="ln"> Gₜ </em>是返回，而<em class="ln"> α </em>是恒定步长参数。我们查看估计值和回报之间的误差项，并在误差方向上更新我们的价值函数。TD学习最简单的版本是TD(0)或<em class="ln">一步TD </em>，在一步之后向估计收益更新价值函数。这个估计，就像贝尔曼方程一样，由两部分组成:直接回报加上下一步的贴现值。现在，我们可以用这个估计回报代替我们的真实回报<em class="ln"> Gₜ </em>，得出一个估计的、有偏差的算法。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/aac28ad78aef9aa8149b5f6d7c8c1109.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*ayERPEN1mdu7vaE-9PX0hQ.png"/></div></figure><p id="e7d6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们用一个例子来看看TD方法是如何证明优于MC方法的。这个例子的想法是预测下班回家需要多长时间。这是状态、时间和预测的顺序。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/60f574c203504fc6d3481c278a6d3f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*exB9JFjO21PqP6CaytwsuQ.png"/></div></figure><p id="ee56" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们如何根据经验的轨迹更新我们的价值函数？下面的可视化说明了MC和TD方法之间的区别。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/767228d7e6933a6b9adb196e88c3261e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4Rn8vmBxOvUWyzqjYPUow.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">蒙特卡罗(左)与时间差分(右)方法</p></figure><p id="e111" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在左边，我们看到MC方法推荐的改变。因为我们基于实际结果更新每个预测，所以我们必须等到最后，看到总时间用了43分钟，然后返回到那个时间更新每一步。有了TD learning，如上图右侧所示，在每一步，迈出一步后，我们可以立即更新前一步。</p><p id="8134" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">TD学习的一个主要优点是，它甚至在不知道最终结果的情况下也能学习。毕竟，一个代理完全有可能接收到不完整的序列，甚至在没有最终结果的连续环境中工作。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><p id="373a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过在我们的控制循环中用TD代替MC，我们得到了强化学习中最著名的算法之一。这个想法叫做<em class="ln"> Sarsa </em>。我们从Q值开始，将Q值稍微移向我们的TD目标，即奖励加上下一个状态的贴现Q值减去我们开始时的Q值。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/fef75ba4a52f1eec0baf338edba14d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*1YT_k1iSX1nnrrxoTLNMWg.png"/></div></figure><p id="5b8f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Sarsa使用五元组中的每个元素来描述从一个状态-动作对到下一个状态-动作对的转换。开始知道它的名字是从哪里来的了吗？</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/c2b7a18619722a7cf7f41a20237f5b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*6lDckk0Xc3jrg4ID-FV9tA.png"/></div></figure><p id="0d96" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们看一下用于策略TD控制的Sarsa算法的伪代码:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mq"><img src="../Images/7b3e2fba72e689cd2a22b1697c5da6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Peb30YebQWaF6R2MP2Bv3A.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">http://incompleteideas.net/book/RLbook2018.pdf<a class="ae mr" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="adf3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了实现这一点，我们来看一个可能很熟悉的例子，Windy Gridworld。Windy Gridworld与我们在以前的帖子中了解并喜欢的Gridworld是一样的，但有一点不同:网格中间有一股侧风！</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/10100f71dbf5321cbd4ef46c4e4c46e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*_kPn3GgMWtVj7t3nIuVXvw.png"/></div></figure><p id="7693" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">风的强度因柱子而异，由每根柱子下面的数字表示。强度表示采取行动后向上移动的单元格数量。例如，如果在目标右侧的单元格中，采取向左移动的动作会将您置于目标正上方的空间中。</p><p id="cf34" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">需要注意的一件重要事情是，蒙特卡罗方法在这里可能不起作用，因为可能会找到一个策略，使代理停留在同一个空间，从而有效地使下一集永不结束。使用Sarsa让我们在插曲中了解到这样的政策是糟糕的，并将尝试其他的东西。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><p id="dea3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下一次，我将深入到非策略TD控制的<em class="ln"> Q-learning </em>中，我们将独立于遵循的策略来逼近最优状态值函数，类似于非策略MC方法。非常感谢您的阅读，我期待着在这个迷人的机器学习子领域学到更多。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="6efa" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">资源</h1><p id="e28e" class="pw-post-body-paragraph kr ks it kt b ku nt kd kw kx nu kg kz la nv lc ld le nw lg lh li nx lk ll lm im bi translated"><a class="ae mr" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">强化学习:简介</strong>作者<em class="ln">萨顿和</em> </a></p><p id="5e8a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">YouTube上大卫·西尔弗的RL课程</p><p id="5400" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae mr" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">强化学习Github </a>由<em class="ln"> dennybritz </em></p></div></div>    
</body>
</html>