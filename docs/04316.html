<html>
<head>
<title>Tutorial: Building your Own Big Data Infrastructure for Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教程:为数据科学构建您自己的大数据基础架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutorial-building-your-own-big-data-infrastructure-for-data-science-579ae46880d8?source=collection_archive---------7-----------------------#2020-04-19">https://towardsdatascience.com/tutorial-building-your-own-big-data-infrastructure-for-data-science-579ae46880d8?source=collection_archive---------7-----------------------#2020-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/60705a95da46839b20a2955636f33972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7vvkSd-ZzPBAQOGTzCNgA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Justin Jairam 拍摄的照片来自<a class="ae kf" href="https://www.instagram.com/jusspreme/?hl=en" rel="noopener ugc nofollow" target="_blank"> @jusspreme </a></p></figure><p id="f814" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从事自己的数据科学项目是学习新技能和磨练现有技能的绝佳机会，但如果您想使用 Hadoop、Spark on a distributed cluster、Hive 等行业中使用的技术，该怎么办呢？并把它们整合在一起。在构建自己的基础设施时，这就是价值的来源。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi le"><img src="../Images/181da60dc2986ade4b6b64bba4df0449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EudU6fAL7JhzntwF36Bqbg.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">大数据格局</p></figure><p id="1787" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您熟悉了这些技术，了解了它如何运行的细节，调试并体验了不同类型的错误消息，真正了解了技术的整体工作原理，而不仅仅是与它进行交互。如果您也在处理自己的私有数据或机密数据，出于隐私或安全原因，您可能不希望将其上传到外部服务来进行大数据处理。因此，在本教程中，我将介绍如何在自己的计算机、家庭实验室等设备上设置自己的大数据基础架构。我们将设置一个单节点 Hadoop &amp; Hive 实例和一个与 Jupyter 集成的“分布式”spark 集群。</p><p id="f34f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lj">编辑</em>:多亏了<a class="ae kf" href="https://medium.com/@dvillaj" rel="noopener"> @Daniel Villanueva </a>你现在可以部署一个预先配置了 Hadoop、Spark 和 Hive 的虚拟机，并准备好通过他的流浪映像。你可以在他的 Github <a class="ae kf" href="https://github.com/dvillaj/spark-box" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p><p id="b432" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">本教程不适用于工业生产安装！</strong></p><h1 id="ef58" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">先决条件</h1><ul class=""><li id="f379" class="mi mj it ki b kj mk kn ml kr mm kv mn kz mo ld mp mq mr ms bi translated">基于 Debian 的发行版——Ubuntu，Pop-os 等</li><li id="6124" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">基本的命令行知识会有所帮助，但对于安装来说不是必需的</li></ul><h1 id="7b0f" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 1 —下载 Hadoop 和 Hive</h1><p id="aaa4" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">Hadoop 无疑是当今行业中最常用的大数据仓库平台，是任何大数据工作的必备知识。简而言之，Hadoop 是一个开源软件框架，用于以分布式方式存储和处理大数据。你可以从<a class="ae kf" href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3-src.tar.gz" rel="noopener ugc nofollow" target="_blank">这里</a>下载最新版本。</p><p id="18ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Hive 通常添加在 Hadoop 之上，以类似 SQL 的方式查询 Hadoop 中的数据。Hive 使作业易于执行操作，如</p><ul class=""><li id="2c8d" class="mi mj it ki b kj kk kn ko kr nb kv nc kz nd ld mp mq mr ms bi translated">数据封装</li><li id="347a" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">即席查询</li><li id="c691" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">大型数据集的分析</li></ul><p id="8169" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Hive 速度较慢，通常仅用于批处理作业。一个更快的 Hive 版本可能类似于 Impala，但对于家庭使用来说，它可以完成任务。你可以在这里下载最新版本的 Hive<a class="ae kf" href="https://downloads.apache.org/hive/hive-3.1.2/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="c930" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确保您下载的是二进制(bin)版本，而不是源代码(src)版本！</p><h2 id="9238" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">将文件解压缩到/opt</h2><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="5cd0" class="ne ll it nr b gy nv nw l nx ny">cd ~/Downloads<br/>tar -C /opt -xzvf apache-hive-3.1.2-bin.tar.gz<br/>tar -C /opt -xzvf hadoop-3.1.3.tar.gz</span></pre><p id="b685" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将它们重命名为<code class="fe nz oa ob nr b">hive</code>和<code class="fe nz oa ob nr b">hadoop</code>。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="7fab" class="ne ll it nr b gy nv nw l nx ny">cd /opt<br/>mv hadoop-3.1.3 hadoop<br/>mv apache-hive-3.1.2-bin hive</span></pre><h1 id="c540" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 2 —设置授权(或无密码)SSH。</h1><p id="8838" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">我们为什么需要这样做？Hadoop 核心使用 Shell (SSH)在从属节点上启动服务器进程。它要求主节点和所有连接节点之间的无密码 SSH 连接。否则，您必须手动转到每个节点并启动每个 Hadoop 进程。</p><p id="5b12" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们运行的是 Hadoop 的本地实例，我们可以省去设置主机名、SSH 密钥以及将它们添加到每个机器中的麻烦。如果这是一个分布式环境，最好也创建一个<code class="fe nz oa ob nr b">hadoop</code>用户，但是对于单个节点设置和个人使用来说，这是不必要的。</p><p id="bb69" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">真正简单的，<strong class="ki iu">只适合在家里使用，不应该在其他地方使用或完成的方式是</strong>:</p><p id="fff5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p><p id="989c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在运行<code class="fe nz oa ob nr b">ssh localhost</code>，你应该可以不用密码登录了。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/df96ae39c7aef25bb9c7ab2518998ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dPO5qXhm9ovoJGm_.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">无密码 SSH 登录</p></figure><p id="c687" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了了解在分布式环境中设置网络和 SSH 配置需要做些什么，您可以阅读<a class="ae kf" href="https://www.tutorialspoint.com/hadoop/hadoop_multi_node_cluster.htm" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><h1 id="cb70" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 3 —安装 Java 8</h1><p id="8c6e" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">本教程最重要的步骤之一。</p><p id="d254" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果做得不正确，将会导致耗费大量时间调试模糊的错误消息，只是为了意识到问题和解决方案是如此简单。</p><p id="e1b5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Hadoop 有一个主要需求，这就是 Java 版本 8。有趣的是，这也是 Spark 的 Java 需求，也非常重要。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="4d95" class="ne ll it nr b gy nv nw l nx ny">sudo apt-get update<br/>sudo apt-get install openjdk-8-jdk</span></pre><p id="33df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">验证 Java 版本。</p><p id="0208" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">java -version</code></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/193143e9d0b57c6a3232d66c2b1816ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0TeW-YVFDyJeB7Lp.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Java 版本</p></figure><p id="a9ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果由于某种原因，您没有看到上面的输出，您需要更新您的默认 Java 版本。</p><p id="3f9d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">sudo update-alternatives --config java</code></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/bee482be459e787c9dd51d4df48a93c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uZNJ_rrdaslnujWR.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">更新 Java 版本</p></figure><p id="38be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择与 Java 8 相关的数字。</p><p id="ebe6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次检查版本。</p><p id="af88" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">java -version</code></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/193143e9d0b57c6a3232d66c2b1816ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0TeW-YVFDyJeB7Lp.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">正确的 Java 版本</p></figure><h1 id="04a5" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 4 —配置 Hadoop + Yarn</h1><p id="30b0" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">Apache Hadoop YARN(又一个资源协商器)是一种集群管理技术。在非常基本的层面上，它帮助 Hadoop 管理和监控其工作负载。</p><h2 id="0bd6" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">初始 Hadoop 设置</h2><p id="fdb6" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">首先让我们设置我们的环境变量。这将告诉其他组件每个组件的配置位于何处。</p><p id="5c72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">nano ~/.bashrc</code></p><p id="fb0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将此添加到您的<code class="fe nz oa ob nr b">.bashrc</code>文件的底部。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="b788" class="ne ll it nr b gy nv nw l nx ny">export HADOOP_HOME=/opt/hadoop<br/>export HADOOP_INSTALL=$HADOOP_HOME<br/>export HADOOP_MAPRED_HOME=$HADOOP_HOME<br/>export HADOOP_COMMON_HOME=$HADOOP_HOME<br/>export HADOOP_HDFS_HOME=$HADOOP_HOME<br/>export YARN_HOME=$HADOOP_HOME<br/>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native<br/>export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><span id="e668" class="ne ll it nr b gy of nw l nx ny">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH</span><span id="0e16" class="ne ll it nr b gy of nw l nx ny">export HIVE_HOME=/opt/hive<br/>export PATH=$PATH:$HIVE_HOME/bin</span></pre><p id="c458" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">保存并退出 nano <code class="fe nz oa ob nr b">CTRL + o</code>，<code class="fe nz oa ob nr b">CTRL + x</code>。</p><p id="109c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们需要通过运行<code class="fe nz oa ob nr b">source ~/.bashrc</code>来激活这些更改。您也可以关闭并重新打开您的终端来达到同样的效果。</p><p id="8181" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要制作一些目录和编辑权限。创建以下目录:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="7938" class="ne ll it nr b gy nv nw l nx ny">sudo mkdir -p /app/hadoop/tmp<br/>mkdir -p ~/hdfs/namenode<br/>mkdir ~/hdfs/datanode</span></pre><p id="7b01" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编辑<code class="fe nz oa ob nr b">/app/hadoop/tmp</code>的权限，授予其读写权限。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="4446" class="ne ll it nr b gy nv nw l nx ny">sudo chown -R $USER:$USER /app<br/>chmod a+rw -R /app</span></pre><h2 id="b018" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">配置文件</h2><p id="8742" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">所有的 Hadoop 配置文件都位于<code class="fe nz oa ob nr b">/opt/hadoop/etc/hadoop/</code>中。</p><p id="8738" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">cd /opt/hadoop/etc/hadoop</code></p><p id="1ac7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要编辑以下配置文件:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="a444" class="ne ll it nr b gy nv nw l nx ny">- core-site.xml<br/>- hadoop-env.sh<br/>- hdfs-site.xml<br/>- mapred-site.xml<br/>- yarn-site.xml</span></pre><p id="9610" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> core-site.xml </strong></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="cdfe" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br/>	&lt;value&gt;/app/hadoop/tmp&lt;/value&gt;<br/>	&lt;description&gt;Parent directory for other temporary directories.&lt;/description&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;fs.defaultFS &lt;/name&gt;<br/>	&lt;value&gt;hdfs://YOUR_IP:9000&lt;/value&gt;<br/>	&lt;description&gt;The name of the default file system. &lt;/description&gt;<br/>	&lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="e7a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">hadoop.tmp.dir</code>:不言自明，只是一个 hadoop 用来存储其他临时目录的目录<code class="fe nz oa ob nr b">fs.defaultFS</code>:你的文件系统通过网络访问的 IP 和端口。如果这是一个分布式系统，它应该是您的 IP，以便其他节点可以连接到它。</p><p id="ff17" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查找您的 ip，请在命令行中键入<code class="fe nz oa ob nr b">ip addr</code>或<code class="fe nz oa ob nr b">ifconfig</code>:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/8a8b53eb802ce07cb76dc5a0167f15f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fLsME7rqA7zpgUNH.png"/></div></div></figure><p id="f068" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> hadoop-env.sh </strong></p><ol class=""><li id="42de" class="mi mj it ki b kj kk kn ko kr nb kv nc kz nd ld oh mq mr ms bi translated">确定 Java 8 JDK 的位置，它应该与<code class="fe nz oa ob nr b">/usr/lib/jvm/java-8-openjdk-amd64/</code>相似或相同</li><li id="2fe6" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld oh mq mr ms bi translated">在<code class="fe nz oa ob nr b">hadoop-env.sh</code>中添加以下一行:<code class="fe nz oa ob nr b">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/</code></li></ol><p id="8409" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> hdfs-site.xml </strong></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="1b06" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;dfs.replication&lt;/name&gt;<br/>	&lt;value&gt;1&lt;/value&gt;<br/>	&lt;description&gt;Default block replication.&lt;/description&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;dfs.name.dir&lt;/name&gt;<br/>	&lt;value&gt;file:///home/YOUR_USER/hdfs/namenode&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;dfs.data.dir&lt;/name&gt;<br/>	&lt;value&gt;file:///home/YOUR_USER/hdfs/datanode&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="23d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">dfs.replication</code>:在多少个节点上复制数据。</p><p id="4a3c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">dfs.name.dir</code>:NameNode 块的目录</p><p id="b8ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">dfs.data.dir</code>:数据节点块的目录</p><p id="08d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> mapred-site.xml </strong></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="264c" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br/>	&lt;value&gt;yarn&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt;<br/>	&lt;value&gt;localhost:54311&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br/>        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br/>        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br/>        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>    	&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;<br/>    	&lt;value&gt;4096&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>    	&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;<br/>    	&lt;value&gt;4096&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="b71b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.framework.name</code>:执行 MapReduce 作业的运行时框架。可以是本地、经典或纱线。</p><p id="25f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.jobtracker.address</code>:MapReduce 作业跟踪器运行的主机和端口。如果是“本地”，则作业作为单个映射和简化任务在进程中运行。</p><p id="a826" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.app.mapreduce.am.env</code>:纱线图减少环境变量。</p><p id="e963" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.map.env</code>:贴图减少贴图环境变量。</p><p id="5570" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.reduce.env</code>:贴图减少减少环境变量。</p><p id="f455" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.map.memory.mb</code>:Hadoop 允许分配给映射器的内存上限，以兆字节为单位。默认值为 512。</p><p id="a738" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">mapreduce.reduce.memory.mb</code>:Hadoop 允许分配给 reducer 的内存上限，以兆字节为单位。默认值为 512。</p><p id="e80d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> yarn-site.xml </strong></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="0b9e" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>&lt;!-- Site specific YARN configuration properties --&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br/>	&lt;value&gt;localhost&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;<br/>	&lt;value&gt;16256&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;<br/>	&lt;value&gt;4096&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>	&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;<br/>	&lt;value&gt;4096&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="f677" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.resourcemanager.hostname</code>:RM 的主机名。也可能是远程 yarn 实例的 ip 地址。</p><p id="de38" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.nodemanager.aux-services</code>:选择运行 MapReduce 需要设置的随机播放服务。</p><p id="6036" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.nodemanager.resource.memory-mb</code>:可以分配给容器的物理内存量，以 MB 为单位。作为参考，我的机器有 64GB 的内存。如果这个值太低，您将无法处理大文件，得到一个<code class="fe nz oa ob nr b">FileSegmentManagedBuffer</code>错误。</p><p id="d152" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.app.mapreduce.am.resource.mb</code>:该属性指定为特定作业选择资源标准。任何具有相同或更多可用内存的节点管理器都将被选择来执行作业。</p><p id="5c2e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">yarn.scheduler.minimum-allocation-mb</code>:RM 上每个容器请求的最小分配量，单位为 MBs。低于这个值的内存请求将不会生效，指定的值将被分配为最小值。</p><h2 id="4cc6" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">启动 Hadoop</h2><p id="f91b" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">在开始 Hadoop 之前，我们必须格式化 namenode:</p><p id="5a8d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">hdfs namenode -format</code></p><p id="dd4c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以开始 Hadoop 了！运行以下命令:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="ebad" class="ne ll it nr b gy nv nw l nx ny">start-dfs.sh<br/>start-yarn.sh</span></pre><p id="3993" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要确保一切都已启动，请运行以下命令:</p><p id="e71b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">ss -ln | grep 9000</code></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/09d58453e495fea910156ffd6593d260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3su1oeUFqBb_QM5B.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">端口 9000 网络信息</p></figure><p id="3765" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行<code class="fe nz oa ob nr b">jps</code></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5edc0b24afe1a1aee92305b6fbeecf74.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*VTodFfWGnzUOg1ZF.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">运行 Java 程序</p></figure><p id="2b22" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您现在还可以在<a class="ae kf" href="http://localhost:9870" rel="noopener ugc nofollow" target="_blank"> http://localhost:9870 </a>访问 Hadoop web UI。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/748e143babaafa75312663c2b9ec587a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f7umOA-i65IhcSuA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Hadoop Web 用户界面</p></figure><p id="70df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您也可以在<a class="ae kf" href="http://localhost:8088" rel="noopener ugc nofollow" target="_blank"> localhost:8088 </a>访问 Yarn web UI。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/f2b90c26b1ce66c0d946695c751b8e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WPyAEZSXhty3Ra13.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">纱网用户界面</p></figure><h1 id="3688" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 5 —设置配置单元</h1><p id="3237" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">既然我们已经启动并运行了 Hadoop，让我们在它的基础上安装 Hive。</p><p id="a394" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们在 Hadoop 中创建一个目录，我们的 Hive 表将存储在这个目录中。</p><p id="a3ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">hdfs dfs -mkdir -p /user/hive/warehouse</code></p><p id="9e19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">配置权限。</p><p id="cb5b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">hdfs dfs -chmod -R a+rw /user/hive</code></p><h2 id="e4a2" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">设置 Metastore</h2><p id="12fa" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">配置单元 Metastore 是配置单元元数据的中央存储库。它存储了配置单元表和关系(模式和位置等)的元数据。它通过使用 metastore 服务 API 提供对此信息的客户端访问。有 3 种不同类型的元存储:</p><ul class=""><li id="71f5" class="mi mj it ki b kj kk kn ko kr nb kv nc kz nd ld mp mq mr ms bi translated">嵌入式 Metastore:一次只能打开一个配置单元会话。</li><li id="3102" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">本地 Metastore:多个配置单元会话，必须连接到外部数据库。</li><li id="c788" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">远程 metastore:多个 Hive 会话，使用 Thrift API 与 Metastore 交互，更好的安全性和可扩展性。</li></ul><p id="5bff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更详细地了解每种类型的 metastore 之间的区别，这是一个很好的<a class="ae kf" href="https://data-flair.training/blogs/apache-hive-metastore/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="8431" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本指南中，我们将使用 MySQL 数据库设置一个远程 metastore。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="540b" class="ne ll it nr b gy nv nw l nx ny">sudo apt update<br/>sudo apt install mysql-server<br/>sudo mysql_secure_installation</span></pre><p id="ef92" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行以下命令:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="bd49" class="ne ll it nr b gy nv nw l nx ny">sudo mysql</span><span id="9143" class="ne ll it nr b gy of nw l nx ny">CREATE DATABASE metastore;<br/>CREATE USER 'hive'@'%' IDENTIFIED BY 'PW_FOR_HIVE';<br/>GRANT ALL ON metastore.* TO 'hive'@'%' WITH GRANT OPTION;</span></pre><p id="2a3b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 MySQL 中将<code class="fe nz oa ob nr b">PW_FOR_HIVE</code>替换为您想要用于 hive 用户的密码。</p><p id="bf64" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下载 MySQL Java 连接器:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="0ca6" class="ne ll it nr b gy nv nw l nx ny">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19.tar.gz<br/>tar -xzvf mysql-connector-java-8.0.19.tar.gz<br/>cd mysql-connect-java-8.0.19<br/>cp mysql-connector-java-8.0.19.jar /opt/hive/lib/</span></pre><h2 id="e73b" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">编辑 hive-site.xml</h2><p id="8062" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">现在编辑<code class="fe nz oa ob nr b">/opt/hive/conf/hive-site.xml</code>:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="7727" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br/>        &lt;value&gt;jdbc:mysql://YOUR_IP:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=UTC&lt;/value&gt;<br/>        &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br/>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br/>        &lt;description&gt;MySQL JDBC driver class&lt;/description&gt;<br/>     &lt;/property&gt;<br/>     &lt;property&gt;<br/>        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br/>        &lt;value&gt;hive&lt;/value&gt;<br/>        &lt;description&gt;user name for connecting to mysql server&lt;/description&gt;<br/>     &lt;/property&gt;<br/>     &lt;property&gt;<br/>        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br/>        &lt;value&gt;PW_FOR_HIVE&lt;/value&gt;<br/>        &lt;description&gt;password for connecting to mysql server&lt;/description&gt;<br/>     &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="ec5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用本地 ip 地址替换<code class="fe nz oa ob nr b">YOUR_IP</code>。用您之前为 hive 用户初始化的密码替换<code class="fe nz oa ob nr b">PW_FOR_HIVE</code>。</p><h2 id="063c" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">初始化架构</h2><p id="cbab" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">现在让我们让 MySQL 可以从网络上的任何地方访问。</p><p id="6b87" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</code></p><p id="8e82" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将<code class="fe nz oa ob nr b">bind-address</code>改为<code class="fe nz oa ob nr b">0.0.0.0</code>。</p><p id="7dab" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重新启动服务以使更改生效:<code class="fe nz oa ob nr b">sudo systemctl restart mysql.service</code></p><p id="9775" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，运行<code class="fe nz oa ob nr b">schematool -dbType mysql -initSchema</code>来初始化 metastore 数据库中的模式。</p><h2 id="6210" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">启动配置单元 Metastore</h2><p id="6a47" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated"><code class="fe nz oa ob nr b">hive --service metastore</code></p><h2 id="bc48" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">测试蜂箱</h2><p id="bea6" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">首先通过调用<code class="fe nz oa ob nr b">hive</code>从命令行启动 Hive。</p><p id="5ed1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们创建一个测试表:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="68a5" class="ne ll it nr b gy nv nw l nx ny">CREATE TABLE IF NOT EXISTS test_table<br/> (col1 int COMMENT 'Integer Column',<br/> col2 string COMMENT 'String Column')<br/> COMMENT 'This is test table'<br/> ROW FORMAT DELIMITED<br/> FIELDS TERMINATED BY ','<br/> STORED AS TEXTFILE;</span></pre><p id="7223" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后插入一些测试数据。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="14a4" class="ne ll it nr b gy nv nw l nx ny">INSERT INTO test_table VALUES(1,'testing');</span></pre><p id="45bf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以查看表中的数据。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="7def" class="ne ll it nr b gy nv nw l nx ny">SELECT * FROM test_table;</span></pre><h1 id="4f0f" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤 6 —设置火花</h1><p id="8888" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">Spark 是一个通用的分布式数据处理引擎，适用于各种环境。在 Spark 核心数据处理引擎之上，有 SQL、机器学习、图形计算和流处理的库，它们可以在应用程序中一起使用。在本教程中，我们将使用 Docker 设置一个独立的 Spark 集群，并让它能够启动任意数量的工作线程。这背后的原因是我们想要模拟一个远程集群以及它所需的一些配置。</p><p id="02b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在生产环境中，Spark 通常被配置为使用 Yarn 和已经分配给 Hadoop 的资源。</p><p id="741a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们需要创建 Docker 文件。在本教程中，我们将使用 Spark 版本 2.4.4，但如果您想要最新版本，可以将其更改为 2.4.5，它还附带了 Hadoop 2.7 来管理节点之间的持久性和簿记。在生产设置中，spark 通常配置有 Yarn，以使用现有的 Hadoop 环境和资源，因为我们只有一个节点上的 Hadoop，我们将运行 Spark 独立集群。将 Spark 配置为与纱线一起运行只需要很少的改动，您可以在这里看到设置<a class="ae kf" href="https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/" rel="noopener ugc nofollow" target="_blank">的不同。</a></p><h2 id="b0d5" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">设置独立集群</h2><p id="2a2c" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated"><code class="fe nz oa ob nr b">nano Dockerfile</code></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="0c61" class="ne ll it nr b gy nv nw l nx ny"># Dockerfile</span><span id="ced3" class="ne ll it nr b gy of nw l nx ny">FROM python:3.7-alpine</span><span id="58c7" class="ne ll it nr b gy of nw l nx ny">ARG SPARK_VERSION=2.4.4<br/>ARG HADOOP_VERSION=2.7</span><span id="a80b" class="ne ll it nr b gy of nw l nx ny">RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \<br/> &amp;&amp; tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C / \<br/> &amp;&amp; rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \<br/> &amp;&amp; ln -s /spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark</span><span id="01b5" class="ne ll it nr b gy of nw l nx ny">RUN apk add shell coreutils procps<br/>RUN apk fetch openjdk8<br/>RUN apk add openjdk8<br/>RUN pip3 install ipython</span><span id="a36a" class="ne ll it nr b gy of nw l nx ny">ENV PYSPARK_DRIVER_PYTHON ipython</span></pre><p id="993c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们要建立一个星火大师和 N 个星火工作者。为此，我们将使用 docker-compose。</p><p id="1659" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">nano docker-compose.yml</code></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="eb1e" class="ne ll it nr b gy nv nw l nx ny">version: "3.3"<br/>networks:<br/>  spark-network:<br/>services:<br/>  spark-master:<br/>    build: .<br/>    container_name: spark-master<br/>    hostname: spark-master<br/>    command: &gt;<br/>      /bin/sh -c '<br/>      /spark/sbin/start-master.sh<br/>      &amp;&amp; tail -f /spark/logs/*'<br/>    ports:<br/>      - 8080:8080<br/>      - 7077:7077<br/>    networks:<br/>      - spark-network<br/>  spark-worker:<br/>    build: .<br/>    depends_on:<br/>      - spark-master<br/>    command: &gt;<br/>      /bin/sh -c '<br/>      /spark/sbin/start-slave.sh $$SPARK_MASTER<br/>      &amp;&amp; tail -f /spark/logs/*'<br/>    env_file:<br/>      - spark-worker.env<br/>    environment:<br/>      - SPARK_MASTER=spark://spark-master:7077<br/>      - SPARK_WORKER_WEBUI_PORT=8080<br/>    ports:<br/>      - 8080<br/>    networks:<br/>      - spark-network</span></pre><p id="5306" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于主容器，我们公开了端口 7077 供应用程序连接，以及端口 8080 供 Spark 作业 UI 连接。对于 worker，我们通过环境变量连接到我们的 Spark master。</p><p id="ccc8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于配置 spark worker 的更多选项，我们将它们添加到<code class="fe nz oa ob nr b">spark-worker.env</code>文件中。</p><p id="9eb0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">nano spark-worker.env</code></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="9d4d" class="ne ll it nr b gy nv nw l nx ny">SPARK_WORKER_CORES=3<br/>SPARK_WORKER_MEMORY=8G</span></pre><p id="c305" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此配置中，每个工作人员将使用 3 个内核和 8GB 内存。由于我的机器有 6 个内核，我们将启动 2 个工人。更改这些值，使其相对于您的机器。例如，如果您的机器只有 16GB 的 RAM，那么一个合适的内存值可能是 2gb 或 4GB。关于环境变量的完整列表和关于独立模式的更多信息，你可以在这里阅读完整的文档<a class="ae kf" href="https://spark.apache.org/docs/latest/spark-standalone.html" rel="noopener ugc nofollow" target="_blank">。如果你想知道 executor 的内存，那是在提交或启动应用程序时设置的。</a></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="07bc" class="ne ll it nr b gy nv nw l nx ny">docker-compose build<br/>docker-compose up -d --scale spark-worker=2</span></pre><p id="5912" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在 spark 已经启动并运行，您可以在<a class="ae kf" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> localhost:8080 </a>查看 web UI！</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/7baa721710adbf7327accddcda43ce39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8Yk_deYtAbym1Df-.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Spark Web 用户界面</p></figure><h2 id="5c11" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">本地安装 Spark</h2><p id="84d9" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">在您的本地机器上，或者任何将要创建或使用 Spark 的机器上，都需要安装 Spark。因为我们正在设置一个远程 Spark 集群，所以我们从源头上安装它。在本教程中，我们将使用 PySpark，因为我大部分时间在我的个人项目中使用 Python。</p><p id="44cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以从<a class="ae kf" href="https://archive.apache.org/dist/spark/" rel="noopener ugc nofollow" target="_blank">这里</a>下载 Spark。</p><p id="b47e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确保您下载的版本与您在主服务器上安装的版本相同。对于本教程，它的版本是 2.4.4</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="8a9c" class="ne ll it nr b gy nv nw l nx ny">wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz<br/>tar -C /opt -xzvf spark-2.4.4-bin-hadoop2.7.tgz</span></pre><p id="9a42" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设置 Spark 环境变量，<code class="fe nz oa ob nr b">nano ~/.bashrc</code></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="cec5" class="ne ll it nr b gy nv nw l nx ny">export SPARK_HOME=/opt/spark<br/>export PATH=$SPARK_HOME/bin:$PATH</span><span id="58c9" class="ne ll it nr b gy of nw l nx ny">export PYSPARK_DRIVER_PYTHON="jupyter"<br/>export PYSPARK_DRIVER_PYTHON_OPTS="notebook"<br/>export PYSPARK_PYTHON=python3</span></pre><p id="f60e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您更喜欢 Jupyter Lab，请将 PYSPARK_DRIVER_PYTHON_OPTS 的“笔记本”更改为“实验室”。</p><h2 id="004c" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">配置文件</h2><p id="b2d3" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">要配置 Spark 来使用我们的 Hadoop 和 Hive，我们需要在 Spark config 文件夹中有两者的配置文件。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="1434" class="ne ll it nr b gy nv nw l nx ny">cp $HADOOP_HOME/etc/hadoop/core-site.xml /opt/spark/conf/<br/>cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml /opt/spark/conf/</span></pre><p id="653b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">nano /opt/spark/conf/hive-site.xml</code></p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="cd85" class="ne ll it nr b gy nv nw l nx ny">&lt;configuration&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;hive.metastore.uris&lt;/name&gt;<br/>                &lt;value&gt;thrift://YOUR_IP:9083&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;spark.sql.warehouse.dir&lt;/name&gt;<br/>                &lt;value&gt;hdfs://YOUR_IP:9000/user/hive/warehouse&lt;/value&gt;<br/>        &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="99ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">hive.metastore.uris</code>:告诉 Spark 使用 Thrift API 与 Hive metastore 交互。<code class="fe nz oa ob nr b">spark.sql.warehouse.dir</code>:告诉 Spark 我们的蜂巢桌在 HDFS 的位置。</p><h2 id="c5cc" class="ne ll it bd lm nf ng dn lq nh ni dp lu kr nj nk ly kv nl nm mc kz nn no mg np bi translated">安装 PySpark</h2><p id="12a1" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">或者用安装在 spark master 上的任何版本替换 2.4.4。</p><p id="6417" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要运行 PySpark 连接到我们的分布式集群运行:</p><p id="bcc0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">pyspark --master spark://localhost:7077</code>，您也可以用您的 ip 或远程 ip 替换<code class="fe nz oa ob nr b">localhost</code>。</p><p id="33ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将启动一个带有预定义的 Spark 上下文的 Jupyter 笔记本。因此，我们现在有一个单一的环境来分析有或没有 Spark 的数据。</p><p id="b2f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">默认情况下，executor 内存只有大约 1GB (1024mb)，要增加内存，请使用以下命令启动 pyspark:</p><p id="f146" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nz oa ob nr b">pyspark --master spark://localhost:7077 --executor-memory 7g</code></p><p id="f365" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 Spark 中，每个执行器有 10%的开销，所以我们最多可以分配 7200mb，但是为了安全起见，我们选择 7。</p><h1 id="498a" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">测试集成</h1><p id="7f79" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">默认情况下，会自动创建一个 SparkContext，变量是<code class="fe nz oa ob nr b">sc</code>。</p><p id="e975" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从我们之前创建的配置单元表中读取。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="ff86" class="ne ll it nr b gy nv nw l nx ny">from pyspark.sql import HiveContext</span><span id="1283" class="ne ll it nr b gy of nw l nx ny">hc = HiveContext(sc)</span><span id="9d0a" class="ne ll it nr b gy of nw l nx ny">hc.sql("show tables").show()</span><span id="7f7c" class="ne ll it nr b gy of nw l nx ny">hc.sql("select * from test_table").show()</span></pre><p id="7fbe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要从 Hadoop 读取文件，命令应该是:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="9a5c" class="ne ll it nr b gy nv nw l nx ny">sparksession = SparkSession.builder.appName("example-pyspark-read-and-write").getOrCreate()<br/>df = (sparksession<br/>	.read<br/>	.format("csv")<br/>	.option("header", "true")<br/>	.load("hdfs://YOUR_IP:9000/PATH_TO_FILE")<br/>)</span></pre><h1 id="4bb4" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">实际的 Hadoop 使用案例</h1><p id="6a64" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">除了存储数据，Hadoop 还被用作一个特性库。假设你是一个团队或组织的一部分，他们有多种模型。每个模型都有一个数据管道，用于接收原始数据、计算数据并将数据转换为特征。对于一个或两个模型来说，这完全没问题，但是如果有多个模型呢？如果跨这些模型重用特性(例如，记录标准化股票价格)会怎样？</p><p id="e268" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以创建一个只计算一次要素的数据管道，并将其存储在要素库中，而不是每个数据管道都重新计算相同的要素。该模型现在可以从要素库中提取要素，而无需任何冗余计算。这减少了数据管道中冗余计算和转换的数量！</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/efb53ac15c50c851e4be4056f7c69947.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/0*f44kly9EW0qMCi-8.png"/></div></figure><p id="7957" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">功能存储还有助于解决以下问题:</p><ul class=""><li id="7163" class="mi mj it ki b kj kk kn ko kr nb kv nc kz nd ld mp mq mr ms bi translated">不会重复使用特征。数据科学家面临的一个常见障碍是花费时间重新开发功能，而不是使用以前开发的功能或其他团队开发的功能。要素存储允许数据科学家避免重复工作。</li><li id="b007" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">功能定义各不相同。任何一家公司的不同团队可能会以不同的方式定义和命名特性。此外，访问某个特定特性的文档(如果存在的话)通常是一个挑战。特征存储通过保持特征及其定义的组织性和一致性来解决这个问题。功能库的文档有助于您围绕整个公司的所有功能创建一种标准化的语言。您确切地知道每个特征是如何计算的，以及它代表什么信息。</li><li id="0c09" class="mi mj it ki b kj mt kn mu kr mv kv mw kz mx ld mp mq mr ms bi translated">培训和生产功能不一致。生产和研究环境通常使用不同的技术和编程语言。流入生产系统的数据流需要实时处理为特征，并输入到机器学习模型中。</li></ul><p id="9ec0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想看一看一个功能商店并免费开始，我推荐<a class="ae kf" href="https://streamsql.io/" rel="noopener ugc nofollow" target="_blank"> StreamSQL </a>。StreamSQL 允许您从各种来源传输数据，如 HDFS、本地文件系统、Kafka 等。并创建一个数据管道，可以养活你的模型！它能够保存在线或本地 HDFS 上的特征存储，供您训练模型。它还为您创建测试(保持)集。他们有一个很好的 API 文档，并且一直在改进它。</p><h1 id="b6fb" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">反馈</h1><p id="1a76" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">我鼓励所有关于这个帖子的反馈。如果你有任何问题或者需要任何帮助，你可以给我发邮件到 sidhuashton@gmail.com 或者在帖子上留言。</p><p id="cb60" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您也可以通过 Twitter 联系我并关注我，地址是<a class="ae kf" href="https://twitter.com/ashtonasidhu" rel="noopener ugc nofollow" target="_blank"> @ashtonasidhu </a>。</p></div></div>    
</body>
</html>