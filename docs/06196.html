<html>
<head>
<title>Basic Tweet Preprocessing in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的基本Tweet预处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e?source=collection_archive---------3-----------------------#2020-05-19">https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e?source=collection_archive---------3-----------------------#2020-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8dd3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">推文预处理！</h2><div class=""/><div class=""><h2 id="dfa1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">了解如何使用Python预处理推文</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e41f877d9ead056a5732a5aed9097a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4diTskBuSpB3fc69LWdaUQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://hdqwalls.com/astronaut-hanging-on-moon-wallpaper" rel="noopener ugc nofollow" target="_blank">https://hdqwalls.com/astronaut-hanging-on-moon-wallpaper</a></p></figure><p id="b4d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">编者注:</em> </strong> <em class="mb"> </em> <a class="ae le" href="http://towardsdatascience.com/" rel="noopener" target="_blank"> <em class="mb">走向数据科学</em> </a> <em class="mb">是一份以研究数据科学和机器学习为主的中型刊物。我们不是健康专家或流行病学家，本文的观点不应被解释为专业建议。想了解更多关于疫情冠状病毒的信息，可以点击</em> <a class="ae le" href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports" rel="noopener ugc nofollow" target="_blank"> <em class="mb">这里</em> </a> <em class="mb">。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="7fd3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">只是为了给你一点背景，为什么我要预处理推文:鉴于截至2020年5月的当前形势，我对美国州长关于正在进行的<strong class="lh ja"><em class="mb"/></strong>的政治话语感兴趣。我想分析一下两个政党——共和党&amp;民主党对给定的形势做出了怎样的反应，新冠肺炎。他们此时的主要目标是什么？谁更关注什么？他们最关心的是什么？</p><p id="eda7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在收集了从新冠肺炎案例1的第一天开始的各州州长的推文后，我们将它们合并成一个数据帧<a class="ae le" href="https://medium.com/@shahparthvi22/merge-multiple-json-files-to-a-dataframe-5e5421c40d06" rel="noopener">(如何将各种JSON文件合并成一个数据帧)</a>并进行预处理。</p><p id="1283" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们总共收到了大约3万条推文。一条推文包含了很多关于它所代表的数据的观点。未经预处理的原始推文是高度非结构化的，包含冗余信息。为了克服这些问题，通过采取多个步骤来执行tweets的预处理。</p><p id="50de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">几乎每个社交媒体网站都因其以标签形式表现的主题而闻名。特别是对我们来说，标签起了重要作用，因为我们对#Covid19、#冠状病毒、#StayHome、#InThisTogether等感兴趣。因此，第一步是基于hashtag值形成一个单独的特征，并对它们进行分段。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="c789" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated"><strong class="ak"> 1。使用正则表达式提取hashtag</strong></h1><h2 id="a00d" class="nb mk iq bd ml nc nd dn mp ne nf dp mt lo ng nh mv ls ni nj mx lw nk nl mz iw bi translated">作为新功能“hashtag”添加到新列的所有hashtag的列表</h2><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="aa12" class="nb mk iq nn b gy nr ns l nt nu">tweets[‘hashtag’] = tweets[‘tweet_text’].apply(lambda x: re.findall(r”#(\w+)”, x))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/48494255b622df3c4a33e22fec81651b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njfY1Rqc0DZQNKaS7EK5yg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">提取标签后</p></figure><p id="a607" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，超过一个单词的标签必须分段。我们使用库<strong class="lh ja"> <em class="mb"> ekphrasis将这些标签分割成n个单词。</em> </strong></p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="8681" class="nb mk iq nn b gy nr ns l nt nu">#installing ekphrasis<br/>!pip install ekphrasis</span></pre><p id="9a2b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">安装完成后，我选择了一个基于twitter语料库的分割器</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="9374" class="nb mk iq nn b gy nr ns l nt nu">from ekphrasis.classes.segmenter import Segmenter</span><span id="0589" class="nb mk iq nn b gy nw ns l nt nu">#segmenter using the word statistics from Twitter<br/>seg_tw = Segmenter(corpus=”twitter”)</span></pre><p id="55c3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我找到的最相关的tweet-preprocessor—<strong class="lh ja"><em class="mb">tweet-preprocessor</em></strong><em class="mb">，</em>这是Python中的一个tweet预处理库。</p><p id="9fa8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它处理—</p><ul class=""><li id="028f" class="nx ny iq lh b li lj ll lm lo nz ls oa lw ob ma oc od oe of bi translated">资源定位符</li><li id="74ba" class="nx ny iq lh b li og ll oh lo oi ls oj lw ok ma oc od oe of bi translated">提及</li><li id="09a5" class="nx ny iq lh b li og ll oh lo oi ls oj lw ok ma oc od oe of bi translated">保留字(RT，FAV)</li><li id="2158" class="nx ny iq lh b li og ll oh lo oi ls oj lw ok ma oc od oe of bi translated">表情符号</li><li id="7625" class="nx ny iq lh b li og ll oh lo oi ls oj lw ok ma oc od oe of bi translated">笑脸消除(游戏名)</li></ul><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="7ea2" class="nb mk iq nn b gy nr ns l nt nu">#installing tweet-preprocessor<br/>!pip install tweet-preprocessor</span></pre><h1 id="02c4" class="mj mk iq bd ml mm ol mo mp mq om ms mt kf on kg mv ki oo kj mx kl op km mz na bi translated">2 .文本清理(URL、提及等。)</h1><h2 id="6ea8" class="nb mk iq bd ml nc nd dn mp ne nf dp mt lo ng nh mv ls ni nj mx lw nk nl mz iw bi translated">将清理后的(删除网址和提及后的)推文添加到一个新列，作为新功能“文本”</h2><p id="71de" class="pw-post-body-paragraph lf lg iq lh b li oq ka lk ll or kd ln lo os lq lr ls ot lu lv lw ou ly lz ma ij bi translated">清理是使用<strong class="lh ja"><em class="mb">tweet-preprocessor</em></strong>包完成的。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="ec40" class="nb mk iq nn b gy nr ns l nt nu">import preprocessor as p</span><span id="77b6" class="nb mk iq nn b gy nw ns l nt nu">#forming a separate feature for cleaned tweets<br/>for i,v in enumerate(tweets['text']):<br/>    tweets.loc[v,’text’] = p.clean(i)</span></pre><h1 id="8574" class="mj mk iq bd ml mm ol mo mp mq om ms mt kf on kg mv ki oo kj mx kl op km mz na bi translated">3.标记化、删除数字、停用词和标点符号</h1><h2 id="62c4" class="nb mk iq bd ml nc nd dn mp ne nf dp mt lo ng nh mv ls ni nj mx lw nk nl mz iw bi translated">新特征“文本”的进一步预处理</h2><p id="e1c0" class="pw-post-body-paragraph lf lg iq lh b li oq ka lk ll or kd ln lo os lq lr ls ot lu lv lw ou ly lz ma ij bi translated">NLTK(自然语言工具包)是预处理文本数据的最佳库之一。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="5038" class="nb mk iq nn b gy nr ns l nt nu">#important libraries for preprocessing using NLTK<br/>import nltk<br/>from nltk import word_tokenize, FreqDist<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>nltk.download<br/>nltk.download('wordnet')<br/>nltk.download('stopwords')<br/>from nltk.tokenize import TweetTokenizer</span></pre><ul class=""><li id="7d93" class="nx ny iq lh b li lj ll lm lo nz ls oa lw ob ma oc od oe of bi translated">去掉数字，降低文字高度(便于处理)</li></ul><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="3541" class="nb mk iq nn b gy nr ns l nt nu">data = data.astype(str).str.replace('\d+', '')<br/>lower_text = data.str.lower()</span></pre><ul class=""><li id="d377" class="nx ny iq lh b li lj ll lm lo nz ls oa lw ob ma oc od oe of bi translated">删除标点符号</li></ul><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="a05f" class="nb mk iq nn b gy nr ns l nt nu">def remove_punctuation(words):<br/> new_words = []<br/> for word in words:<br/>    new_word = re.sub(r'[^\w\s]', '', (word))<br/>    if new_word != '':<br/>       new_words.append(new_word)<br/> return new_words</span></pre><ul class=""><li id="08ff" class="nx ny iq lh b li lj ll lm lo nz ls oa lw ob ma oc od oe of bi translated">词汇化+标记化—使用内置的TweetTokenizer()</li></ul><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="a154" class="nb mk iq nn b gy nr ns l nt nu">lemmatizer = nltk.stem.WordNetLemmatizer()<br/>w_tokenizer = TweetTokenizer()</span><span id="17ff" class="nb mk iq nn b gy nw ns l nt nu">def lemmatize_text(text):<br/> return [(lemmatizer.lemmatize(w)) for w in \<br/>                                     w_tokenizer.tokenize((text))]</span></pre><p id="3ddd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后一个预处理步骤是</p><ul class=""><li id="1050" class="nx ny iq lh b li lj ll lm lo nz ls oa lw ob ma oc od oe of bi translated">删除停用字词—有一个预定义的英文停用字词列表。但是，您可以像这样修改停用字词，只需将这些字词附加到停用字词列表中。</li></ul><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="64df" class="nb mk iq nn b gy nr ns l nt nu">stop_words = set(stopwords.words('english'))</span><span id="215a" class="nb mk iq nn b gy nw ns l nt nu">tweets['text'] = tweets['text'].apply(lambda x: [item for item in \ <br/>                         x if item not in stop_words])</span></pre><h1 id="7abe" class="mj mk iq bd ml mm ol mo mp mq om ms mt kf on kg mv ki oo kj mx kl op km mz na bi translated">4.词云</h1><h2 id="4153" class="nb mk iq bd ml nc nd dn mp ne nf dp mt lo ng nh mv ls ni nj mx lw nk nl mz iw bi translated">分段标签的频率分布</h2><p id="f276" class="pw-post-body-paragraph lf lg iq lh b li oq ka lk ll or kd ln lo os lq lr ls ot lu lv lw ou ly lz ma ij bi translated">在预处理步骤之后，我们排除了推文中的所有地名和缩写，因为它充当了泄漏变量，然后我们对最常出现的标签进行了频率分布，并创建了一个词云—</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e693e639b484f024c68275eee7c19f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*cYMHGAZhrPwPFe69Qn2dqg.png"/></div></figure><p id="f0b2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是意料之中的。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="b45e" class="nb mk iq nn b gy nr ns l nt nu">from wordcloud import WordCloud</span><span id="0ae8" class="nb mk iq nn b gy nw ns l nt nu">#Frequency of words<br/>fdist = FreqDist(tweets['Segmented#'])</span><span id="d347" class="nb mk iq nn b gy nw ns l nt nu">#WordCloud<br/>wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist)<br/>plt.figure(figsize=(12,10))<br/>plt.imshow(wc, interpolation="bilinear")<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="082b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最终的数据集—</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d8dd5ec18d8f895ca48e84e0e1ea4a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*1RdCcV8aHMjvn9L_IjiLpQ.png"/></div></figure><p id="6e5f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最终的代码—</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="f9e3" class="nb mk iq nn b gy nr ns l nt nu">import pandas as pd<br/>import numpy as np<br/>import json<br/>from collections import Counter<br/>from wordcloud import WordCloud<br/>import matplotlib.pyplot as plt<br/>import re, string, unicodedata<br/>import nltk<br/>from nltk import word_tokenize, sent_tokenize, FreqDist<br/>from nltk.corpus import stopwords<br/>from nltk.stem import LancasterStemmer, WordNetLemmatizer<br/>nltk.download<br/>nltk.download('wordnet')<br/>nltk.download('stopwords')<br/>from nltk.tokenize import TweetTokenizer</span><span id="a231" class="nb mk iq nn b gy nw ns l nt nu">!pip install ekphrasis<br/>!pip install tweet-preprocessor<br/>import preprocessor as p</span><span id="e365" class="nb mk iq nn b gy nw ns l nt nu">tweets['hashtag'] = tweets['tweet_text'].apply(lambda x: re.findall(r"#(\w+)", x))</span><span id="0e9b" class="nb mk iq nn b gy nw ns l nt nu">for i,v in enumerate(tweets['text']):<br/>    tweets.loc[v,’text’] = p.clean(i)</span><span id="142e" class="nb mk iq nn b gy nw ns l nt nu">def preprocess_data(data):<br/> #Removes Numbers<br/> data = data.astype(str).str.replace('\d+', '')<br/> lower_text = data.str.lower()<br/> lemmatizer = nltk.stem.WordNetLemmatizer()<br/> w_tokenizer =  TweetTokenizer()<br/> <br/> def lemmatize_text(text):<br/>  return [(lemmatizer.lemmatize(w)) for w \<br/>                       in w_tokenizer.tokenize((text))]</span><span id="fc4b" class="nb mk iq nn b gy nw ns l nt nu"> def remove_punctuation(words):<br/>  new_words = []<br/>   for word in words:<br/>      new_word = re.sub(r'[^\w\s]', '', (word))<br/>      if new_word != '':<br/>         new_words.append(new_word)<br/>   return new_words</span><span id="ada7" class="nb mk iq nn b gy nw ns l nt nu"> words = lower_text.apply(lemmatize_text)<br/> words = words.apply(remove_punctuation)<br/> return pd.DataFrame(words)</span><span id="e38e" class="nb mk iq nn b gy nw ns l nt nu">pre_tweets = preprocess_data(tweets['text'])<br/>tweets['text'] = pre_tweets</span><span id="fa32" class="nb mk iq nn b gy nw ns l nt nu">stop_words = set(stopwords.words('english'))<br/>tweets['text'] = tweets['text'].apply(lambda x: [item for item in \<br/>                                    x if item not in stop_words])</span><span id="32ea" class="nb mk iq nn b gy nw ns l nt nu">from ekphrasis.classes.segmenter import Segmenter</span><span id="f4b8" class="nb mk iq nn b gy nw ns l nt nu"># segmenter using the word statistics from Twitter<br/>seg_tw = Segmenter(corpus="twitter")<br/>a = []<br/>for i in range(len(tweets)):<br/> if tweets['hashtag'][i] != a<br/>  listToStr1 = ' '.join([str(elem) for elem in \<br/>                                       tweets['hashtag'][i]])<br/>  tweets.loc[i,'Segmented#'] = seg_tw.segment(listToStr1)</span><span id="edc3" class="nb mk iq nn b gy nw ns l nt nu">#Frequency of words<br/>fdist = FreqDist(tweets['Segmented#'])<br/>#WordCloud<br/>wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist)<br/>plt.figure(figsize=(12,10))<br/>plt.imshow(wc, interpolation="bilinear")<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="369c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">希望我帮到你们了。</p><p id="c164" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果文本预处理得好，文本分类通常会工作得更好。一定要给它一些额外的时间，最终一切都是值得的。</p></div></div>    
</body>
</html>