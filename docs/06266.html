<html>
<head>
<title>Model Selection in Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类中的模型选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146?source=collection_archive---------5-----------------------#2020-05-20">https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146?source=collection_archive---------5-----------------------#2020-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/aef1de737181198d314923f26c3456a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mj9_sbG3NOk0qFlsBSdgng.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</p></figure><div class=""/><blockquote class="kc kd ke"><p id="9b71" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">更新 2020–11–24:结论中增加了资源</p><p id="3520" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">更新 2020 年 8 月 21 日:管道现在可以用于二进制和多类分类问题。在处理模型的保存时，转换器仍然存在错误。</p><p id="13e4" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">更新 2020–07–16:管道现在可以保存模型，修正了主函数内部的错误。添加了 Adaboost、Catboost、LightGBM、extractree 分类器</p></blockquote><h1 id="0dbb" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">简介:</h1><p id="ccaa" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">一开始，有一个简单的问题。我的经理来问我是否可以用 NLP 方法对邮件和相关文档进行分类。</p><p id="f1f8" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">听起来并不奇怪，但我会从成千上万个样本开始。问的第一件事就是用“XGBoost”，因为:“我们可以用 XGBoost 做任何事情”。有趣的工作，如果数据科学都归结于 XGBoost…</p><p id="0064" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">在用不同的算法、指标和可视化实现了一个笔记本之后，我的脑海中仍然有一些东西。我无法一次在不同的型号中做出选择。仅仅是因为你可能对一个模型抱有侥幸心理，而不知道如何重现良好的准确度、精确度等…</p><p id="442a" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">所以此时此刻，我问自己，如何做一个模特选拔？我在网上寻找，阅读帖子，文章，等等。看到实现这种东西的不同方式是非常有趣的。但是，当考虑到神经网络时，它是模糊的。这时，我脑子里有一件事，如何比较经典方法(多项朴素贝叶斯、SVM、逻辑回归、boosting …)和神经网络(浅层、深层、lstm、rnn、cnn…)。</p><p id="838b" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">我在这里对笔记本做一个简短的解释。欢迎评论。</p><blockquote class="kc kd ke"><p id="b6bc" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">这个笔记本在 GitHub 上有:<a class="ae mk" href="https://github.com/Christophe-pere/Model-Selection" rel="noopener ugc nofollow" target="_blank">这里</a>T2【这个笔记本在 Colab 上有:<a class="ae mk" href="https://colab.research.google.com/drive/1qeDOAvtlyBTq7K2eza45b8RZ1_FRrHw_?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a></p></blockquote><h1 id="db01" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">如何开始？</h1><p id="a2e1" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">每个项目都是从<strong class="ki jg">探索性数据分析</strong>(简称<strong class="ki jg"> EDA </strong>)开始，然后直接是<strong class="ki jg">预处理</strong>(文本非常脏，邮件中的签名、url、邮件头等等……)。不同的功能将出现在 Github 库中。</p><p id="22e3" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">查看预处理是否正确的快速方法是确定最常见的<strong class="ki jg"> n 元文法</strong> (uni，bi，tri… grams)。另一篇文章将会引导你走这条路。</p><h1 id="e85c" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">数据</strong></h1><p id="e616" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">我们将对 IMDB 数据集应用<em class="kh">模型选择</em>的方法。如果您不熟悉 IMDB 数据集，它是一个包含电影评论(文本)的数据集，用于情感分析(二元——正面或负面)。<br/>更多详情可在这里找到<a class="ae mk" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">。要下载它:</a></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="ac6f" class="mu lf jf mq b gy mv mw l mx my">$ wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</span><span id="a3c3" class="mu lf jf mq b gy mz mw l mx my">$ tar -xzf aclImdb_v1.tar.gz</span></pre><h1 id="36fd" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">矢量化方法</h1><p id="3461" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated"><strong class="ki jg">One-Hot encoding(count vectorizing)</strong>:<br/>这是一种将单词替换为 0 和 1 的向量的方法，目标是获取一个语料库(重要的单词量)，并为语料库中包含的每个唯一单词制作一个向量。之后，每个单词都会被投影到这个向量中，其中<strong class="ki jg"> 0 表示不存在，1 表示存在</strong>。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="adbb" class="mu lf jf mq b gy mv mw l mx my">       | bird | cat | dog | monkey |<br/>bird   |  1   |  0  |  0  |    0   |<br/>cat    |  0   |  1  |  0  |    0   |<br/>dog    |  0   |  0  |  1  |    0   |<br/>monkey |  0   |  0  |  0  |    1   |</span></pre><p id="76f1" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">相应的 python 代码:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="4c0d" class="mu lf jf mq b gy mv mw l mx my"># create a count vectorizer object <br/>count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')<br/>count_vect.fit(df[TEXT])  # text without stopwords</span><span id="0035" class="mu lf jf mq b gy mz mw l mx my"># transform the training and validation data using count vectorizer object<br/>xtrain_count =  count_vect.transform(train_x)<br/>xvalid_count =  count_vect.transform(valid_x)</span></pre><p id="1b86" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg"> TF-IDF: </strong> <br/> <em class="kh">词频-逆文档频率</em>是信息检索和文本挖掘中经常用到的一个权重。这个权重是一个统计量，用于评估一个词对集合或语料库中的文档有多重要(来源:<a class="ae mk" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank"> tf-idf </a>)。<br/>在处理大量停用词时，这种方法非常有效(这种类型的词与信息无关→ <em class="kh">我、我的、我自己、我们、我们的、我们的、我们自己、你</em> …对于英语)。IDF 术语允许显示重要词和罕见词。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="7d23" class="mu lf jf mq b gy mv mw l mx my"># word level tf-idf<br/>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=10000)<br/>tfidf_vect.fit(df[TEXT])<br/>xtrain_tfidf =  tfidf_vect.transform(train_x_sw)<br/>xvalid_tfidf =  tfidf_vect.transform(valid_x_sw)</span></pre><p id="fc66" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg"> TF-IDF n-grams </strong> : <br/>与之前的<em class="kh"> tf-idf </em>的区别在于<strong class="ki jg">一字</strong>，<em class="kh"> tf-idf n-grams </em>兼顾了<em class="kh"> n </em>连词。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e59e" class="mu lf jf mq b gy mv mw l mx my"># ngram level tf-idf<br/>tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', ngram_range=(2,3), max_features=10000)<br/>tfidf_vect_ngram.fit(df[TEXT])<br/>xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x_sw)<br/>xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x_sw)</span></pre><p id="fc97" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">TF-IDF chars n-grams:</strong><br/>与前面的方法相同，但重点是在字符级别，该方法将重点放在 n 个连续的字符上。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="493a" class="mu lf jf mq b gy mv mw l mx my"># characters level tf-idf<br/>tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) <br/>tfidf_vect_ngram_chars.fit(df[TEXT])<br/>xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x_sw) <br/>xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x_sw)</span></pre><p id="bb9a" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">预训练模型—快速文本</strong></p><blockquote class="kc kd ke"><p id="edaf" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">FastText 是一个开源、免费、轻量级的库，允许用户学习文本表示和文本分类器。它在标准的通用硬件上工作。模型可以缩小尺寸，甚至适合移动设备。(来源:<a class="ae mk" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">此处</a>)</p></blockquote><p id="d600" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">如何加载 fastText？来自官方文件:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="eda2" class="mu lf jf mq b gy mv mw l mx my">$ git clone https://github.com/facebookresearch/fastText.git <br/>$ cd fastText <br/>$ sudo pip install . <br/>$ # or : <br/>$ sudo python setup.py install</span></pre><p id="f131" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">下载合适的型号。这里有 157 种语言的模型。要下载英文模型:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="ff68" class="mu lf jf mq b gy mv mw l mx my">$ wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip</span><span id="5b85" class="mu lf jf mq b gy mz mw l mx my">&amp; unzip crawl-300d-2M-subword.zip</span></pre><p id="27da" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">下载完成后，将其加载到 python 中:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="5361" class="mu lf jf mq b gy mv mw l mx my">pretrained = fasttext.FastText.load_model('crawl-300d-2M-subword.bin')</span></pre><p id="420e" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">单词嵌入或单词向量(WE): </strong></p><blockquote class="kc kd ke"><p id="9b35" class="kf kg kh ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld ij bi translated">将向量与单词相关联的另一种流行且强大的方法是使用密集的“单词向量”，也称为“单词嵌入”。虽然通过一位热编码获得的向量是二进制的、稀疏的(主要由零组成)和非常高维的(与词汇表中的单词数量具有相同的维数)，但是“单词嵌入”是低维浮点向量(即，与稀疏向量相反的“密集”向量)。与通过一键编码获得的单词向量不同，单词嵌入是从数据中学习的。在处理非常大的词汇表时，经常会看到 256 维、512 维或 1024 维的单词嵌入。另一方面，一键编码单词通常导致 20，000 维或更高维的向量(在这种情况下捕获 20，000 个单词的词汇)。因此，单词嵌入将更多的信息打包到更少的维度中。(来源:<a class="ae mk" href="https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb" rel="noopener ugc nofollow" target="_blank">用 Python 进行深度学习，弗朗索瓦·乔莱 2017 </a>)</p></blockquote><p id="f206" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">如何用整数映射一个句子:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="31b7" class="mu lf jf mq b gy mv mw l mx my"># create a tokenizer <br/>token = Tokenizer()<br/>token.fit_on_texts(df[TEXT])<br/>word_index = token.word_index</span><span id="7a91" class="mu lf jf mq b gy mz mw l mx my"># convert text to sequence of tokens and pad them to ensure equal length vectors <br/>train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=300)<br/>valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=300)</span><span id="0fd4" class="mu lf jf mq b gy mz mw l mx my"># create token-embedding mapping<br/>embedding_matrix = np.zeros((len(word_index) + 1, 300))<br/>words = []<br/>for word, i in tqdm(word_index.items()):<br/>    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)<br/>    words.append(word)<br/>    if embedding_vector is not None:<br/>        embedding_matrix[i] = embedding_vector</span></pre><h1 id="9471" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">型号选择</h1><p id="234f" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">计算机科学中的<strong class="ki jg">选型</strong>是什么？具体在 AI 领域？<em class="kh">模型选择</em>是在不同的机器学习方法之间进行选择的过程。所以简而言之，不同的模式。</p><p id="96d1" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">但是，我们如何比较它们呢？要做到这一点，我们需要度量标准(更多细节见此<a class="ae mk" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">链接</a>)。数据集将被分割成<strong class="ki jg"> <em class="kh">训练</em> </strong>和<strong class="ki jg"> <em class="kh">测试</em> </strong>部分(验证集将在深度学习模型中确定)。</p><p id="b688" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">在这个<strong class="ki jg">二元或多类分类</strong>模型选择中，我们使用什么样的度量标准？</p><p id="848d" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">为了分类，我们将使用以下术语:<br/> - <strong class="ki jg"> tp </strong>:真阳性预测<br/> - <strong class="ki jg"> tn </strong>:真阴性预测<br/> - <strong class="ki jg"> fp </strong>:假阳性预测<br/> - <strong class="ki jg"> fn </strong>:假阴性预测<br/>这里有一个<a class="ae mk" rel="noopener" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62">链接</a>以了解更多细节。</p><ul class=""><li id="c3a3" class="na nb jf ki b kj kk kn ko me nc mg nd mi ne ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg">准确度</strong> </a>:所有预测上的所有正预测<br/> (tp + tn) / (tp + tn + fp + fn)</li><li id="bd5c" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score" rel="noopener ugc nofollow" target="_blank"><strong class="ki jg"/></a>:定义为不平衡数据集在每个类上获得的平均召回率</li><li id="9397" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg">精度</strong> </a>:精度直观上是分类器不将 tp / (tp + fp)为阴性的样本标记为阳性的能力</li><li id="8709" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg">召回</strong> </a>(或灵敏度):召回是分类器直观地找到所有阳性样本 tp / (tp + fn)的能力</li><li id="0a78" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><strong class="ki jg">f1-得分</strong>:f1 得分可以解释为精度和召回率的加权平均值- &gt; 2 *(精度*召回率)/(精度+召回率)</li><li id="3189" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg"> Cohen kappa </strong> </a>:表示两个标注者对一个分类问题的一致程度的分数。因此，如果数值小于 0.4 就很糟糕，在 0.4 到 0.6 之间，它相当于人类，0.6 到 0.8 是很大的数值，超过 0.8 就是例外。</li><li id="952e" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg">马修斯相关系数</strong></a>:<em class="kh"/><strong class="ki jg"><em class="kh">马修斯相关系数</em> </strong> <em class="kh"> (MCC)用于</em> <a class="ae mk" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <em class="kh">机器学习</em> </a> <em class="kh">作为二进制(两类)</em> <a class="ae mk" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank"> <em class="kh">分类</em> </a> <em class="kh"> […]该系数考虑了真、假阳性和阴性，通常被视为可以使用的平衡度量 MCC 本质上是观察到的和预测的二元分类之间的相关系数；它返回 1 到+1 之间的值。系数+1 表示完美预测，0 表示不比随机预测好，1 表示预测和观察完全不一致。(来源:</em> <a class="ae mk" href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> <em class="kh">维基百科</em> </a> <em class="kh"> ) </em></li><li id="b36d" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg">受试者操作特征曲线下面积</strong> </a> <strong class="ki jg"> (ROC AUC) </strong></li><li id="cae7" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><strong class="ki jg">时间拟合</strong>:训练模型所需的时间</li><li id="9acd" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated"><strong class="ki jg">时间分数</strong>:预测结果所需的时间</li></ul><p id="a7c9" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">太好了！！我们是我们的衡量标准。下一步是什么？</p><h1 id="787b" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">交叉验证</h1><p id="8aae" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">为了能够实现模型之间的稳健比较，我们需要验证每个模型的稳健性。</p><p id="dece" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">下图显示了需要将全局数据集拆分为训练和测试数据。训练数据用于训练模型，测试数据用于测试模型。交叉验证是在<em class="kh"> k-fold </em>中分离数据集的过程。k 是我们实现数据所需的比例数。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/235a446a464ca1517d6054f109c7e16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C43AroXNLvc0aGvp.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:<a class="ae mk" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a></p></figure><p id="7844" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">一般来说，<em class="kh"> k </em>为 5 或 10 这将取决于<strong class="ki jg">数据集</strong>的大小(小数据集小<em class="kh"> k </em>，大数据集大<em class="kh"> k </em>)。</p><p id="7b80" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">目标是计算每个折叠的每个指标，并计算它们的<strong class="ki jg">平均值</strong>(平均值)和<strong class="ki jg">标准偏差</strong>(标准偏差)。</p><p id="55ec" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">在 python 中，这个过程将通过<em class="kh"> scikit-learn </em>中的<a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jg"> cross_validate </strong> </a>函数来完成。</p><p id="29cb" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">我们会对比哪些车型？</p><h1 id="4689" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">评估的型号</strong></h1><p id="da31" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">我们将测试机器学习模型、深度学习模型和 NLP 专门模型。</p><p id="c121" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">机器学习模型</strong></p><ul class=""><li id="3c48" class="na nb jf ki b kj kk kn ko me nc mg nd mi ne ld nf ng nh ni bi translated">多项式朴素贝叶斯</li><li id="5ba1" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">逻辑回归</li><li id="f226" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">SVM (SVM)</li><li id="e094" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">随机梯度下降</li><li id="7b0a" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">k 近邻</li><li id="6b96" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">随机森林</li><li id="ad23" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">梯度增强(GB)</li><li id="3eae" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">XGBoost(著名的)(XGB)</li><li id="b4c9" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">adaboost 算法</li><li id="e2a2" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">Catboost</li><li id="ed01" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">LigthGBM</li><li id="30bc" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">树外分级机</li></ul><p id="0cbe" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">深度学习模型</strong></p><ul class=""><li id="5085" class="na nb jf ki b kj kk kn ko me nc mg nd mi ne ld nf ng nh ni bi translated">浅层神经网络</li><li id="546c" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">深度神经网络(和 2 个变体)</li><li id="4cd5" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">递归神经网络(RNN)</li><li id="81f5" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">长短期记忆(LSTM)</li><li id="e13e" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">卷积神经网络(CNN)</li><li id="8e37" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">门控循环单元(GRU)</li><li id="e013" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">CNN+LSTM</li><li id="40e9" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">CNN+GRU</li><li id="778d" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">双向 RNN</li><li id="2699" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">双向 LSTM</li><li id="04a2" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">双向 GRU</li><li id="a5e7" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">递归卷积神经网络(RCNN)(和 3 个变体)</li><li id="b17c" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">变形金刚(电影名)</li></ul><p id="cfe6" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">仅此而已，30 款还不错。</p><p id="46dd" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">这可以在这里恢复:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div></figure><h1 id="f2c8" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">让我们展示一些代码</h1><p id="ffab" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated"><strong class="ki jg">机器学习</strong></p><p id="52ef" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">对于经典算法，我将使用<em class="kh">sk learn(0.23 版)</em>中的<a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate" rel="noopener ugc nofollow" target="_blank"> cross_validate() </a>函数来考虑多指标。下面的函数，<em class="kh"> report </em>，获取一个<strong class="ki jg">分类器</strong>，<strong class="ki jg"> X，y </strong>数据，以及一个<strong class="ki jg">自定义指标列表</strong>，并使用参数计算它们的<strong class="ki jg">交叉验证</strong>。它返回一个 dataframe，包含所有指标的<strong class="ki jg">值</strong>，以及每个指标的<strong class="ki jg">平均值</strong>和标准偏差(<strong class="ki jg"> std) </strong>。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用机器学习算法计算不同指标的函数</p></figure><p id="a096" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">怎么用？</strong></p><p id="c144" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">下面是多项式朴素贝叶斯的一个例子:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="dcd5" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">术语 if multinomial_naive_bayes 之所以存在，是因为此代码是笔记本的一部分，开头带有参数(布尔型)。所有代码都在<a class="ae mk" href="https://github.com/Christophe-pere/Model-Selection" rel="noopener ugc nofollow" target="_blank"> GitHub </a>和<a class="ae mk" href="https://colab.research.google.com/drive/1qeDOAvtlyBTq7K2eza45b8RZ1_FRrHw_?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab </a>中。</p><p id="4e8a" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated"><strong class="ki jg">深度学习</strong></p><p id="20c1" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">我还没有找到深度学习的 cross_validate 这样的函数，只有关于对神经网络使用<em class="kh"> k 倍</em>交叉验证的帖子。这里我将分享一个用于深度学习的自定义 cross_validate 函数，其输入和输出与<em class="kh"> report </em>函数相同。它将允许我们使用相同的指标，并将所有模型放在一起进行比较。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">利用二元和多类分类的度量进行深度学习的 k-Folds 交叉验证</p></figure><p id="1155" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">目标是将神经网络函数视为:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6427" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">并在 cross_validate_NN 函数内部调用这个函数。所有的深度学习都获得了相同的实现，并将进行比较。有关不同型号的完整实现，请访问<a class="ae mk" href="https://github.com/Christophe-pere/Model-Selection" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="1560" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结果</h1><p id="fae2" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">当所有模型计算出不同的折叠和度量时，我们可以很容易地将它们与数据框架进行比较。在 IMDB 数据集上，表现较好的模型是:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="70e5" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">这里我只展示了准确率&gt; 80%的结果，以及准确度、精确度和召回指标的结果。</p><h1 id="f534" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">如何改善？</h1><ul class=""><li id="7263" class="na nb jf ki b kj mc kn md me nr mg ns mi nt ld nf ng nh ni bi translated">在模型字典的参数中构建一个函数，并将所有工作连接成一个循环</li><li id="3439" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">使用分布式深度学习</li><li id="ef4d" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">使用 TensorNetwork 加速神经网络</li><li id="940b" class="na nb jf ki b kj nj kn nk me nl mg nm mi nn ld nf ng nh ni bi translated">使用 GridSearch 进行超调</li></ul><h1 id="09c0" class="le lf jf bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="49ea" class="pw-post-body-paragraph kf kg jf ki b kj mc kl km kn md kp kq me mf kt ku mg mh kx ky mi mj lb lc ld ij bi translated">现在，您有了一个导入管道，可以为带有大量参数的文本分类进行模型选择。所有的模型都是自动的。享受吧。</p><p id="12a6" class="pw-post-body-paragraph kf kg jf ki b kj kk kl km kn ko kp kq me ks kt ku mg kw kx ky mi la lb lc ld ij bi translated">另一个关于机器学习时代模型选择的伟大资源(更多理论文章)是由<a class="ae mk" href="https://www.linkedin.com/in/samadritaghosh/?originalSubdomain=in" rel="noopener ugc nofollow" target="_blank"> Samadrita Ghosh </a>在<a class="ae mk" href="https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning" rel="noopener ugc nofollow" target="_blank"> Neptune.ai 博客</a>上写的。</p></div></div>    
</body>
</html>