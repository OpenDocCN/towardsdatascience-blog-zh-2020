<html>
<head>
<title>Text Similarity with TensorFlow.js Universal Sentence Encoder</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow.js通用句子编码器的文本相似性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-textual-similarity-analysis-web-app-aa3139d4fb71?source=collection_archive---------10-----------------------#2020-02-16">https://towardsdatascience.com/how-to-build-a-textual-similarity-analysis-web-app-aa3139d4fb71?source=collection_archive---------10-----------------------#2020-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="478d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从学者到构建相似句子分组网络应用的旅程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cae44bcd2f3af564dc6debc0a6c4a491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KVbGhjOVGfRNi4i0SOkbAg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些布赖顿的沐浴盒比其他的更相似。“给我看看那些红色的”[照片由<a class="ae ky" href="https://medium.com/@jinglesnote" rel="noopener"> me </a>拍摄]</p></figure><p id="1d29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你想知道搜索引擎是如何理解你的查询并检索相关结果的吗？聊天机器人如何从你的问题中提取你的意图，并提供最恰当的回应？</p><p id="cf7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个故事中，我将详细介绍构建文本相似性分析web应用程序所需的每个部分，包括:</p><ul class=""><li id="bc80" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">单词嵌入</li><li id="efad" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">句子嵌入</li><li id="f9a8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">余弦相似性</li><li id="9045" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">构建文本相似性分析网络应用</li><li id="5630" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结果分析</li></ul><p id="d29f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">试试<a class="ae ky" href="https://jinglescode.github.io/demos/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank">文本相似性分析网络应用</a>，在下面的评论中让我知道它是如何为你工作的！</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="af05" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">什么是单词嵌入？</h1><p id="1c65" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">单词嵌入<strong class="lb iu">启用</strong> <strong class="lb iu">知识表示，其中向量表示单词</strong>。这提高了神经网络从文本数据集学习的能力。</p><p id="8a77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在单词嵌入成为自然语言处理的事实标准之前，处理单词的一种常见方法是使用一次性矢量化。每个单词代表向量空间中的一列，每个句子是一个由<em class="nn">个1</em>和<em class="nn">个0</em>组成的向量。<em class="nn">个</em>表示单词在句子中的存在。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4c58296db21f60e80c4b2ed30073ed55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*rUBSgHI9EYkE0KNjh2mKtg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一键矢量化[摘自<a class="ae ky" rel="noopener" target="_blank" href="/text-encoding-a-review-7c929514cccf">文本编码:综述</a></p></figure><p id="f905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果，这导致了一个巨大而稀疏的表示，因为零<em class="nn">比一</em>多得多。当一个词汇表中有很多单词时，它会创建一个很大的单词向量。这可能会成为机器学习算法的一个问题。</p><p id="148c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一键矢量化也无法捕捉单词的含义。比如“<em class="nn">饮料</em>”和“<em class="nn">饮料</em>”，虽然这是两个不同的词，但是定义却差不多。</p><p id="3caf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过单词嵌入，语义相似的单词具有相似的向量表示。因此，当出现类似“<em class="nn">我想点一杯饮料</em>”或“<em class="nn">一杯饮料</em>”的短语时，点餐系统可以以同样的方式解释该请求。</p><h2 id="cd0c" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">过去的单词嵌入</h2><p id="4129" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">早在2003年，<a class="ae ky" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> Yoshua Bengio等人</a>引入了语言模型概念。那篇论文的重点是学习单词的表示，这允许模型预测下一个单词。</p><p id="d02a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇论文是至关重要的，并导致了单词嵌入的发展和发现。约舒厄与杰弗里·辛顿和扬·勒昆一起获得了图灵奖。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/c0d357e3584247d059a9e4429af44b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wT5O0mvj-cExG3gUm8pYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将单词的特征向量序列输入到单词的条件概率分布中，以预测下一个单词[图片取自<a class="ae ky" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="4c01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2008年，<a class="ae ky" href="https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf" rel="noopener ugc nofollow" target="_blank">罗南和杰森</a>研究了一个可以学习识别相似单词的神经网络。他们的发现为自然语言处理开辟了许多可能性。下表显示了单词列表以及十个最相似的单词。</p><div class="kj kk kl km gt ab cb"><figure class="oc kn od oe of og oh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0875bbf63b53efda8686f7d87c298cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*OXRYM56lkHwF4wwg2vdaPw.png"/></div></figure><figure class="oc kn oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f0542f75fd1f2a5ad1eb496ec8b9dde0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1562/format:webp/1*TSEkbK6T5pEO0K6Cf57jrQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk oj di ok ol translated">左图:给定输入句子的神经网络结构，输出类别概率。右表:选出的5个单词和10个最相似的单词。[资料来源于<a class="ae ky" href="https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure></div><p id="7faf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2013年，<a class="ae ky" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"> Tomas Mikolov等人</a>介绍了从拥有数十亿单词的数据集学习高质量的单词向量。他们将它命名为<em class="nn"> Word2Vec，</em>它包含了词汇中的数百万个单词。</p><p id="ef1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Word2Vec从此开始流行。如今，单词嵌入层在所有流行的深度学习框架中。</p><h2 id="1a2d" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">单词嵌入示例</h2><p id="ccd8" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在谷歌预训练的Word2Vec模型上，他们从谷歌新闻数据集中训练了大约1000亿个单词。单词“<em class="nn">猫</em>”与“<em class="nn">猫</em>”、“<em class="nn">狗</em>”、“<em class="nn">老鼠</em>”、“<em class="nn">宠物</em>”的意思最接近。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/f417423792233b28dc070872eede8cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AoUkipDSCyIr4wXm580sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“<em class="on">猫</em>这个词在几何上更接近于“<em class="on">猫</em>”、“<em class="on">狗</em>”、“<em class="on">老鼠</em>”、“<em class="on">宠物</em>”。【摘自<a class="ae ky" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">嵌入式投影仪</a></p></figure><p id="975d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入还设法识别单词之间的关系。一个经典的例子是单词之间的性别角色关系。比如，“<em class="nn">男</em>之于“<em class="nn">女</em>”就好比“<em class="nn">王</em>”之于“<em class="nn">后</em>”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/436e5d71c4c681c2a4b00e43c5f3602a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFMJXXsv4fOD8zqFT_wrUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从手套无监督学习算法学习到的单词之间的有趣关系[ <a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">图像源</a></p></figure><h2 id="1cdf" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">深入挖掘单词嵌入</h2><p id="95cc" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">加利纳·奥莱尼克在描述单词嵌入的动机方面做得非常出色。从一键编码和TF-IDF到手套和庞加莱。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">单词嵌入:探索、解释和利用(带Python代码)</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">单词嵌入讨论是每个自然语言处理科学家都在谈论的话题</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div><p id="0d51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一篇由<a class="op oq ep" href="https://medium.com/u/6278d12b0682?source=post_page-----aa3139d4fb71--------------------------------" rel="noopener" target="_blank">迪潘然(DJ)萨卡尔</a>撰写的29分钟的关于各种语言模型的综合文章。他涵盖了Word2Vec、GloVe和FastText如果你打算研究单词嵌入，一定要看看这个。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">文本数据深度学习方法的直观实践方法— Word2Vec、GloVe和FastText</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">驯服非结构化文本数据的更新、高级策略</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi ks ou"/></div></div></a></div><h2 id="355e" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">项目的Word嵌入资源</h2><p id="62a1" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">TensorFlow在这个<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中提供了一个关于单词嵌入和代码的<a class="ae ky" href="https://www.tensorflow.org/tutorials/text/word_embeddings" rel="noopener ugc nofollow" target="_blank">教程</a>。您可以尝试使用这些代码，并用它来训练您在数据集上的单词嵌入。这个绝对可以帮你入门。</p><p id="5b50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于喜欢动画的人来说，在<a class="ae ky" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">嵌入投影仪</a>上有一个很酷的嵌入可视化。每个点代表一个单词，你可以在三维空间中可视化语义相似的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/a0b9d3d8e1b0795b80d3060f390cb50e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/1*ZIFALK_IJSGqo_MK5Z4GJQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个截屏<a class="ae ky" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">嵌入投影仪</a>。你看到的每个点代表一个单词。</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="2d33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有单词向量来代表单词的意思；句子怎么样？</p><h1 id="292f" class="mq mr it bd ms mt pl mv mw mx pm mz na jz pn ka nc kc po kd ne kf pp kg ng nh bi translated">什么是通用句子编码器？</h1><p id="4c21" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">像单词嵌入一样，<a class="ae ky" href="https://arxiv.org/pdf/1803.11175.pdf" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a>是一个通用的句子嵌入模型，它将文本转换成有语义意义的固定长度向量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/8735956e0f15f32bcf0ef78a6a118d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Qy3hv5iLnKnWVhpjjl2jQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通用句子编码器将文本编码成高维向量[取自<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/1" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a></p></figure><p id="dc80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由通用句子编码器产生的这些向量捕获丰富的语义信息。我们可以将它用于各种自然语言处理任务，训练分类器，如<a class="ae ky" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">分类</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Semantic_similarity" rel="noopener ugc nofollow" target="_blank">文本相似性分析</a>。</p><p id="ed34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Google有两个通用的句子编码器模型。其中一个基于<strong class="lb iu">变压器</strong>架构，另一个基于<strong class="lb iu">深度平均网络</strong>。</p><p id="5458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Transformer </strong>，句子嵌入为每个单词创建上下文感知的表示，以生成句子嵌入。它是为更高的精度而设计的，但是编码需要更多的内存和计算时间。这对于情感分类是有用的，在情感分类中，像‘not’这样的词可以改变意思，并且能够处理像‘不错’这样的双重否定。</p><p id="dfd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">深度平均网络</strong>，单词的嵌入首先一起平均，然后通过一个前馈深度神经网络产生句子嵌入。不幸的是，通过平均向量，我们在这个过程中失去了句子的上下文和句子中的单词序列。它是为了速度和效率而设计的，牺牲了一些准确性(尤其是在讽刺和双重否定上)。一个很好的主题分类模型，将长文章分类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/c6ef255c7959ec5cb7ede0ab6637c565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*khuFQ0R7LOrxeSNKSzbpIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如果句子可以得到相同的回答，那么它们在语义上是相似的。[摘自<a class="ae ky" href="https://arxiv.org/pdf/1804.07754.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="e6fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1804.07754.pdf" rel="noopener ugc nofollow" target="_blank">杨等人</a>介绍了一种利用会话数据学习句子表征的方法。</p><p id="47bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如，“<em class="nn">你多大了？</em>、<em class="nn">你多大了？</em>”，两个问题语义相似，一个聊天机器人可以回复同一个答案“<em class="nn">我20岁</em>”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/a0eaf6fa001ff16134eae4be20502c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s6BBJEE4fWv3ajeyJ9Trzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“你好吗？”以及“你多大了？”即使有相同的单词也有33%的相似度[ <a class="ae ky" href="https://jinglescode.github.io/demos/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank"> demo </a></p></figure><p id="57b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，虽然"<em class="nn">你好吗？</em>、<em class="nn">你多大了？</em>“包含相同的单词，两个句子有不同的意思。聊天机器人必须理解问题并给出适当的回答。</p><p id="bf48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一张显示三个句子相似度的热图“<em class="nn">你多大了？</em>、<em class="nn">你多大了？</em>、<em class="nn">你好吗？</em>”。</p><p id="fbc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“<em class="nn">你好吗？</em><em class="nn">你多大了？</em>“即使具有相同的单词，也具有低相似度分数。</p><p id="3574" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1803.02893" rel="noopener ugc nofollow" target="_blank"> Logeswaran等人</a>引入了一个从无标签数据中学习句子表征的框架。在本文中，现有方法中使用的解码器(<em class="nn">橙色框</em>)被从一组候选句子(<em class="nn">绿色框</em>)中选择目标句子的分类器所取代；它提高了问答系统的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/d5879ed8a79ee97d5999efcc6d03fbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JX-jV49NFDw9WhsIJ3Iz0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用从一组候选句子中选择目标句子的分类器代替先前方法中的解码器[摘自<a class="ae ky" href="https://arxiv.org/pdf/1803.02893.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="392c" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">深入了解通用句子编码器</h2><p id="8543" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="op oq ep" href="https://medium.com/u/6278d12b0682?source=post_page-----aa3139d4fb71--------------------------------" rel="noopener" target="_blank"> Dipanjan (DJ) Sarkar </a>解释了各种嵌入模型的发展。如果您热衷于构建文本分类器，他的文章详细介绍了在电影评论数据集上执行情感分析的每个步骤。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">面向自然语言处理的深度迁移学习——通用文本分类…</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">揭秘通用句子编码器指南</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pu l pf pg ph pd pi ks ou"/></div></div></a></div><p id="8d2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你好奇探索其他语言模型，<a class="op oq ep" href="https://medium.com/u/c0101388583?source=post_page-----aa3139d4fb71--------------------------------" rel="noopener" target="_blank"> Pratik Bhavsar </a>对比了BERT、ELMo、USE、Siamese、InferSent等各种语言模型的性能。学会选择正确的答案会改善你的结果。</p><div class="or os gp gr ot ou"><a href="https://medium.com/modern-nlp/on-variety-of-encoding-text-8b7623969d1e" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">NLP中的各种编码器</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">文本的主特征工程</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">medium.com</p></div></div><div class="pd l"><div class="pv l pf pg ph pd pi ks ou"/></div></div></a></div><h2 id="9f25" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">为您的项目提供通用句子编码器资源</h2><p id="39a9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">TensorFlow提供了关于通用语句编码器的<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/1" rel="noopener ugc nofollow" target="_blank">教程</a>，预训练模型和<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。如果你正在考虑构建自己的文本分类器，一定要看看这个。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="21ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了每个句子的语义向量，我们如何衡量句子之间的相似性呢？</p><h1 id="782e" class="mq mr it bd ms mt pl mv mw mx pm mz na jz pn ka nc kc po kd ne kf pp kg ng nh bi translated">余弦相似度是什么？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/8aa7e1f2f837681eb2610920914886c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-XIU-FO_LZyTBDln"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@vsmilelx?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">浮萍 闪电</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></p></figure><p id="0ec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>是通过计算<strong class="lb iu">两个向量</strong>之间的余弦角来衡量相似度。如果两个向量相似，则它们之间的角度较小，余弦相似值更接近1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/099a43a2a2c85d08526c1e16949cac3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFbltO26lsm8c0ulVV2nAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定两个向量<em class="on"> A </em>和<em class="on"> B </em>，余弦相似度cos(θ)使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener ugc nofollow" target="_blank">点积</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Magnitude_(mathematics)#Euclidean_vector_space" rel="noopener ugc nofollow" target="_blank">幅度</a>来表示【来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="398f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们将句子输入通用句子编码器，它返回给我们句子嵌入向量。</p><p id="777d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了向量，我们就可以得到向量之间的余弦相似性。对于每一个句子对，<em class="nn"> A </em>和<em class="nn"> B </em>，我们可以计算出<em class="nn"> A </em>和<em class="nn"> B </em>向量的余弦相似度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/6f37a5168b6d58018820a84ee242564f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f7Gd44VFjv5GUOZO7M3RIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语义相似度是衡量两个文本表达相同意思的程度。[摘自<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/1" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a></p></figure><p id="1f74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以确定将句子分组在一起最小阈值。当相似性得分在0到1之间时，也许我们可以选择0.5，在中间点。这意味着任何相似度大于0.5的句子都将被聚集在一起。</p><h2 id="b331" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">深入挖掘衡量相似性的方法</h2><p id="39fe" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="op oq ep" href="https://medium.com/u/5515433d5913?source=post_page-----aa3139d4fb71--------------------------------" rel="noopener" target="_blank"> Euge Inzaugarat </a>介绍了六种度量向量间相似性的方法。每种方法都适用于特定的环境，因此了解它们就像了解您的数据科学工具箱一样。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-to-measure-distances-in-machine-learning-13a396aa34ce"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">机器学习中如何测量距离</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">这完全取决于你的观点</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pz l pf pg ph pd pi ks ou"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e24a" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">文本相似性分析网络应用的构建模块</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/676befaee9cb7bf10b1f52d476111eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-DNbNVHj5TNv_bl1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@ryanquintal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ryan Quintal </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="6717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个项目中，我将使用这些库:</p><ul class=""><li id="35ae" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">TensorFlow.js</li><li id="5ff3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">通用句子编码器</li><li id="ecf5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">有角的</li></ul><h2 id="8a14" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">TensorFlow.js</h2><p id="d76d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">TensorFlow.js 是Google建立的一个框架，支持JavaScript的机器学习。我们可以<strong class="lb iu">开发机器学习模型，并将它们部署在网络浏览器和Node.js </strong>中。</p><p id="1d9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nn">因为我喜欢开发web应用程序，所以当TensorFlow.js在2018年发布时，我非常高兴。</em></p><p id="2883" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很容易上手，我们可以用<a class="ae ky" href="https://www.npmjs.com/" rel="noopener ugc nofollow" target="_blank"> npm </a>安装TensorFlow.js。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="ef4f" class="np mr it qc b gy qg qh l qi qj">$ npm install @tensorflow/tfjs</span></pre><p id="d262" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单线性回归模型的示例如下。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="afa5" class="np mr it qc b gy qg qh l qi qj">import * as tf from '@tensorflow/tfjs';</span><span id="eba7" class="np mr it qc b gy qk qh l qi qj">const model = tf.sequential();<br/>model.add(tf.layers.dense({units: 1, inputShape: [1]}));</span><span id="d3f2" class="np mr it qc b gy qk qh l qi qj">model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});</span><span id="7678" class="np mr it qc b gy qk qh l qi qj">const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);<br/>const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);</span><span id="9ef9" class="np mr it qc b gy qk qh l qi qj">model.fit(xs, ys, {epochs: 10}).then(() =&gt; {<br/>  model.predict(tf.tensor2d([5], [1, 1])).print();<br/>});</span></pre><h2 id="187b" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">通用句子编码器</h2><p id="fb2a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我将使用的是来自<a class="ae ky" href="https://www.tensorflow.org/js" rel="noopener ugc nofollow" target="_blank"> TensorFlow.js </a>的<a class="ae ky" href="https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder" rel="noopener ugc nofollow" target="_blank">通用句子编码器包</a>。我们可以使用<a class="ae ky" href="https://www.npmjs.com/" rel="noopener ugc nofollow" target="_blank"> npm </a>安装通用语句编码器。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="3685" class="np mr it qc b gy qg qh l qi qj">$ npm install @tensorflow-models/universal-sentence-encoder</span></pre><p id="593d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个例子展示了我们如何使用通用句子编码器从每个句子中提取嵌入内容。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="bc00" class="np mr it qc b gy qg qh l qi qj">import * as use from '@tensorflow-models/universal-sentence-encoder';</span><span id="531a" class="np mr it qc b gy qk qh l qi qj">use.load().then(model =&gt; {<br/>  const sentences = [<br/>    'Hello.',<br/>    'How are you?'<br/>  ];<br/>  model.embed(sentences).then(embeddings =&gt; {<br/>    embeddings.print(true /* verbose */);<br/>  });<br/>});</span></pre><h2 id="7737" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">有角的</h2><p id="0a32" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="ae ky" href="https://angular.io/" rel="noopener ugc nofollow" target="_blank"> Angular </a>是Google为创建动态单页应用而构建的web应用框架。</p><p id="5d23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个项目，我使用的是Angular 8.0。我喜欢在Angular的<a class="ae ky" href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller" rel="noopener ugc nofollow" target="_blank">模型-视图-控制器设计模式</a>的基础上构建。从Angular的第一个版本开始，我就一直在使用它，在我的大部分web开发中也是如此。但由于他们每半年推出一次主要版本，感觉我的工作会变得过时(也许？我不知道)。React是一个流行的UI框架，所以也许有一天我会改用React。谁知道呢？</p><h2 id="8762" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">余弦相似性</h2><p id="4a2b" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">创建一个函数，使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似性公式</a>计算两个向量的相似性。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="f767" class="np mr it qc b gy qg qh l qi qj">similarity(a, b) {<br/>  var magnitudeA = Math.sqrt(this.dot(a, a));<br/>  var magnitudeB = Math.sqrt(this.dot(b, b));<br/>  if (magnitudeA &amp;&amp; magnitudeB)<br/>    return this.dot(a, b) / (magnitudeA * magnitudeB);<br/>  else return false<br/>}</span></pre><p id="479a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算每个句子对的相似性得分的另一个函数如下。</p><pre class="kj kk kl km gt qb qc qd qe aw qf bi"><span id="8e01" class="np mr it qc b gy qg qh l qi qj">cosine_similarity_matrix(matrix){<br/>  let cosine_similarity_matrix = [];<br/>  for(let i=0;i&lt;matrix.length;i++){<br/>    let row = [];<br/>    for(let j=0;j&lt;i;j++){<br/>      row.push(cosine_similarity_matrix[j][i]);<br/>    }<br/>    row.push(1);<br/>    for(let j=(i+1);j&lt;matrix.length;j++){<br/>      row.push(this.similarity(matrix[i],matrix[j]));<br/>    }<br/>    cosine_similarity_matrix.push(row);<br/>  }<br/>  return cosine_similarity_matrix;<br/>}</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="ce95" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">将一切结合在一起</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/8129abde6d5dccdbea3150400f805ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J_V3FCa91IEwq2mx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@amayli?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Amélie Mourichon </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="43b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经介绍了这个项目所需的所有主要组件。现在我们只需要把它们像乐高积木一样堆叠起来，打包并部署到Github。</p><p id="97c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊。我们得到了一个用于现场演示的web应用程序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ql"><img src="../Images/16cec8d6403944bf7dbe28373d1c17f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0CkJtVzK0zoHLgrLXYrlw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入语义相似的句子列表[ <a class="ae ky" href="https://jinglescode.github.io/demos/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank">演示</a></p></figure><p id="a02d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有一个句子列表，这些将被输入到通用句子编码器中。它将输出每个句子的嵌入。然后我们计算每个句子之间的相似度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/5bddf7ea6d6f7020f6f146dd774db996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*NwKuAmPw8FQNKKDsZ74mCQ.png"/></div></figure><h2 id="4db0" class="np mr it bd ms nq nr dn mw ns nt dp na li nu nv nc lm nw nx ne lq ny nz ng oa bi translated">结果</h2><p id="5f61" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">这些是我们将测试我们的通用句子编码器的句子。目的是把意思相似的句子组合在一起。我挑了几个比较难的案例，让我们看看它的表现如何。</p><blockquote class="qn qo qp"><p id="518d" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">明天会下雪吗？<br/>最近许多飓风袭击了美国<br/>全球变暖是真的</p><p id="1aba" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">一天一个苹果，医生远离我吃草莓有益健康</p><p id="5388" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">你多大了？<br/>你多大了？你好吗？</p><p id="4eb9" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">约翰尼被狗咬了</p><p id="93fb" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">猫吃了老鼠<br/>老鼠吃了猫</p></blockquote><p id="7c5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这张热图显示了每个句子与其他句子的相似程度。绿色越亮，表示相似度越接近1，这意味着句子之间越相似。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qt"><img src="../Images/072215637bd3fe7caef9e281ccc86593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geTR_Kb42QyqBkz0R6Tq-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">12个句子对的语义相似度[ <a class="ae ky" href="https://jinglescode.github.io/demos/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank"> demo </a></p></figure><p id="2aa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以调整该值来确定一个最小相似度阈值，以便将句子组合在一起。这些是以大于0.5的相似性值分组在一起的句子。</p><blockquote class="qn qo qp"><p id="7618" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">最近许多飓风袭击了美国全球变暖是真的</p><p id="35d4" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated">每天一个苹果，医生远离我吃草莓是健康的</p><p id="e8f0" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated"><strong class="lb iu">第三组</strong> <br/>你多大了？你多大了？</p><p id="1559" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated"><strong class="lb iu">第四组</strong> <br/>狗咬了约翰尼<br/>约翰尼咬了狗</p><p id="1aa4" class="kz la nn lb b lc ld ju le lf lg jx lh qq lj lk ll qr ln lo lp qs lr ls lt lu im bi translated"><strong class="lb iu">第五组</strong> <br/>猫吃了老鼠<br/>老鼠吃了猫</p></blockquote><p id="a185" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的web应用程序出色地识别出了“<em class="nn"> Group 1 </em>”是<em class="nn">天气相关问题</em>。即使两个句子没有任何重叠的单词。</p><p id="f11a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它成功地识别出“<em class="nn">飓风</em>”和“<em class="nn">全球变暖</em>”与天气有关，但不知何故未能将“<em class="nn">雪</em>”归入这一类别。</p><p id="b6dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可惜，“<em class="nn">强尼咬了狗”和“狗咬了强尼”有着</em> 87%的相似度。可怜的约翰尼，我不知道哪个更好。</p><p id="b999" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，对于“<em class="nn">猫吃了老鼠</em>”和“<em class="nn">老鼠吃了猫</em>”，我希望这两个向量有相反的相似性。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="dcb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您到目前为止的阅读！</p><p id="c81c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再一次，试试<a class="ae ky" href="https://jinglescode.github.io/demos/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank">文本相似性分析网络应用</a>，在下面的评论中让我知道它是如何为你工作的！</p><p id="3df8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想构建类似的东西，请查看web应用程序的<a class="ae ky" href="https://github.com/jinglescode/demos/tree/master/src/app/components/nlp-sentence-encoder" rel="noopener ugc nofollow" target="_blank">代码</a>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="46b4" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">我构建的其他机器学习网络应用</h1><p id="95f3" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">因为我喜欢构建web应用程序，所以我开发了这些web应用程序来展示web上的机器学习能力。一定要跟随我的媒体(<a class="op oq ep" href="https://medium.com/u/641197e9ee36?source=post_page-----aa3139d4fb71--------------------------------" rel="noopener" target="_blank">广告歌</a>)，因为我会建立更多这样的。</p><p id="7aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用TensorFlow.js进行时间序列预测。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-with-tensorflow-js-1efd48ff2201"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">使用TensorFlow.js进行时间序列预测</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">从在线API中提取股票价格，并使用RNN和LSTM以及TensorFlow.js进行预测(包括演示和代码)</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="qu l pf pg ph pd pi ks ou"/></div></div></a></div><p id="93c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个学习玩井字游戏的强化代理。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/reinforcement-learning-value-function-57b04e911152"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">强化学习价值函数</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">代理使用价值函数学习井字游戏的强化学习算法——带网络演示</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="qv l pf pg ph pd pi ks ou"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ab cb"><figure class="oc kn qw oe of og oh paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/7820823f18c088b934fefc4fcbe5e6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="oc kn qx oe of og oh paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/ed2857d42868ce52ed8f717376bc4cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="oc kn qx oe of og oh paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/c6faf13786230940c1756ff46938c471.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="af71" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">参考</h1><p id="6d33" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">[1] Bengio，Yoshua，等.<a class="ae ky" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">一种神经概率语言模型。</a>(2003)</p><p id="14f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]科洛波特、罗南和杰森·韦斯顿。"<a class="ae ky" href="https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf" rel="noopener ugc nofollow" target="_blank">自然语言处理的统一架构:具有多任务学习的深度神经网络。</a>(2008)</p><p id="b335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Mikolov，Tomas，等.<a class="ae ky" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的高效估计。</a>(2013)</p><p id="1c4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Cer，Daniel等.<a class="ae ky" href="https://arxiv.org/pdf/1803.11175.pdf" rel="noopener ugc nofollow" target="_blank">通用语句编码器。</a>(2018)</p><p id="d9e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5]杨，，等.<a class="ae ky" href="https://arxiv.org/pdf/1804.07754" rel="noopener ugc nofollow" target="_blank">从会话中学习语义文本相似度.</a>(2018)</p><p id="a82a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] Logeswaran、Lajanugen和Honglak Lee。"<a class="ae ky" href="https://arxiv.org/pdf/1803.02893.pdf" rel="noopener ugc nofollow" target="_blank">学习句子表征的高效框架。</a>(2018)</p></div></div>    
</body>
</html>