<html>
<head>
<title>Clustering with K-means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-with-k-means-1e07a8bfb7ca?source=collection_archive---------3-----------------------#2020-01-02">https://towardsdatascience.com/clustering-with-k-means-1e07a8bfb7ca?source=collection_archive---------3-----------------------#2020-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0515" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">使用无监督的机器学习在天气数据中寻找模式</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/e4909b9e347f5d46c26ea375a005432e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4eFxhSmG6itXt4sp3CuYQ.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来源:<a class="ae kw" href="https://unsplash.com/@nicolerwilcox?utm_medium=referral&amp;utm_campaign=photographer-credit&amp;utm_content=creditBadge" rel="noopener ugc nofollow" target="_blank">unsplash-logoNicole Wilcox</a></p></figure><p id="b602" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">无监督学习最常用的技术之一是聚类。顾名思义，聚类是对具有相似特征的数据进行分组的行为。在机器学习中，当没有预先指定的数据标签可用时，即我们不知道要创建哪种分组时，会使用聚类。目标是将数据分组到相似的类中，以便:</p><p id="3ff1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="lt">类内相似度高</em></p><p id="0d1d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="lt">类间相似度低</em></p><p id="54d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有两种主要的聚类类型— <strong class="kz ir"> K均值聚类</strong>和<strong class="kz ir">层次凝聚聚类</strong>。在<em class="lt"> K均值聚类</em>的情况下，我们试图找到<em class="lt"> k </em>聚类中心作为属于这些聚类的数据点的均值。这里，预先指定了聚类的数量，并且该模型旨在为任何给定的聚类<em class="lt">，K</em>找到最优的聚类数量。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="170f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们使用的是来自Kaggle的<a class="ae kw" href="https://www.kaggle.com/julianjose/minute-weather" rel="noopener ugc nofollow" target="_blank">分钟天气数据集</a>，其中包含与天气相关的测量值，如气压、最大风速、相对湿度等。这些数据是从2011年9月到2014年9月的三年时间里在圣地亚哥采集的，包含以一分钟为间隔采集的原始传感器测量值。</p><p id="84db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第一步是导入必要的库…</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="9230" class="mg mh iq mc b gy mi mj l mk ml">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns<br/>import sklearn<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.cluster import KMeans<br/>from sklearn import metrics<br/>from sklearn.cluster import AgglomerativeClustering</span></pre><p id="67b5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">…和数据集</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="9bc6" class="mg mh iq mc b gy mi mj l mk ml">df = pd.read_csv('minute_weather.csv')<br/>df.head()</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mm"><img src="../Images/a915d74a8d4fdc45709e8c6a75ee3a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeY60hxqLHJcV4C0EHYwWw.png"/></div></div></figure><p id="7df8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">浏览我们的数据，我们发现有1，587，257行和13列！由于这个数据集非常大，我们需要随机抽取样本。此外，对于K-means方法，首先找到初始质心的位置是必要的，以便该算法可以找到收敛性。为了做到这一点，我们不使用整个数据集，而是起草一个样本，对随机初始化的质心进行短期运行，并跟踪度量的改进。这里给出了这种方法的一个很好的解释。</p><p id="73c9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从每第10行抽取一个样本，我们创建一个新的样本数据帧</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="fbae" class="mg mh iq mc b gy mi mj l mk ml">sample_df = df[(df['rowID'] % 10) == 0]<br/>sample_df.shape</span></pre><p id="2bf0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这产生了158726行和13列(好得多！)</p><p id="07ff" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">检查空值，我们发现rain_accumulation和rain_duration可以被删除</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mn"><img src="../Images/326bf80b221d09d0d88d8c02958b5fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*NR7WHoPKMEime4HDnGUkAg.png"/></div></div></figure><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="5b36" class="mg mh iq mc b gy mi mj l mk ml">df1 = sample_df.drop(columns =['rain_accumulation','rain_duration'])<br/>print(df1.columns)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mo"><img src="../Images/84dfba03ed537fc9846137089acd5162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5bqRHogSNtD7DEHgiM1Hg.png"/></div></div></figure><p id="404b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在最大风速/风向和最小风速/风向这两个值之间，出于聚类的目的，我只选择了最大值，因为我们已经有了这两个值的平均值。如果愿意，最小值也可以包括在分析中。最后，我们对聚类感兴趣的列可以排序到一个新的数据框架中，如下所示</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="f7ce" class="mg mh iq mc b gy mi mj l mk ml">cols_of_interest = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed','max_wind_direction',<br/>                    'max_wind_speed', 'relative_humidity']</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mp"><img src="../Images/b68dec32a59ff45ca7c15a1503615495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-q42B2xNR2uCKNpO3pDARA.png"/></div></div></figure><p id="25ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下一步是衡量我们的价值观，给予它们同等的重要性。从聚类的角度来看，缩放也很重要，因为点之间的距离会影响聚类的形成方式。</p><p id="4ba9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用StandardScaler，我们将数据帧转换成以下numpy数组</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="0936" class="mg mh iq mc b gy mi mj l mk ml">X = StandardScaler().fit_transform(data)<br/>X</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mq"><img src="../Images/84f363002b7497edf6c145f0ef6582cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*od_o3fMtqFMileQFkms6xw.png"/></div></div></figure><p id="9a92" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">K-均值聚类</strong></p><p id="ce36" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如前所述，在K-means的情况下，在运行模型之前已经指定了聚类的数量。我们可以为K选择一个基准水平数，并迭代找到最佳值。为了评估哪个数量的聚类更适合我们的数据集，或者找到<em class="lt">聚类适合度</em>，我们使用两种评分方法— <strong class="kz ir">剪影系数</strong>和<strong class="kz ir"> Calinski Harabasz评分。</strong>实际上，根据模型中最重要的指标，有许多不同的评分方法。通常选择一种方法作为标准，但是为了这个分析的目的，我使用了两种方法。</p><p id="3cff" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用每个样本的<strong class="kz ir">平均聚类内距离(a) </strong>和<strong class="kz ir">平均最近聚类距离(b) </strong>计算轮廓系数。样本的轮廓系数为<strong class="kz ir"> (b-a) / max(b-a) </strong></p><p id="d6d7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Calinski Harabasz得分或方差比率是<strong class="kz ir">类内离差</strong>和<strong class="kz ir">类间离差</strong>之间的比率</p><p id="f04e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们使用sci-kit learn实现K-means算法。</p><p id="5841" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">n个簇= 12</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="4df0" class="mg mh iq mc b gy mi mj l mk ml"><em class="lt">#Set number of clusters at initialisation time</em></span><span id="f3e7" class="mg mh iq mc b gy mr mj l mk ml">k_means = KMeans(n_clusters=12)</span><span id="ded2" class="mg mh iq mc b gy mr mj l mk ml"><em class="lt">#Run the clustering algorithm</em></span><span id="ffd2" class="mg mh iq mc b gy mr mj l mk ml">model = k_means.fit(X)<br/>model</span><span id="1d67" class="mg mh iq mc b gy mr mj l mk ml">#Generate cluster predictions and store in y_hat</span><span id="296e" class="mg mh iq mc b gy mr mj l mk ml">y_hat = k_means.predict(X)</span></pre><p id="e0d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">计算轮廓系数…</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="da2b" class="mg mh iq mc b gy mi mj l mk ml">from sklearn import metrics<br/>labels = k_means.labels_</span><span id="6d06" class="mg mh iq mc b gy mr mj l mk ml">metrics.silhouette_score(X, labels, metric = 'euclidean')</span></pre><p id="8614" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi">0.2405</p><p id="0f47" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">…以及CH分数</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="179c" class="mg mh iq mc b gy mi mj l mk ml">metrics.calinski_harabasz_score(X, labels)</span></pre><p id="4051" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi">39078.93</p><p id="7708" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们尝试另一个随机选择的值，即n_clusters = 8</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="f373" class="mg mh iq mc b gy mi mj l mk ml">k_means_8 = KMeans(n_clusters=8)<br/>model = k_means_8.fit(X)<br/>y_hat_8 = k_means_8.predict(X)</span></pre><p id="5833" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">再次计算轮廓系数和CV值</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="07d3" class="mg mh iq mc b gy mi mj l mk ml">labels_8 = k_means_8.labels_<br/>metrics.silhouette_score(X, labels_8, metric = 'euclidean')</span></pre><p id="ad8f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">轮廓系数= 0.244</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="f5d0" class="mg mh iq mc b gy mi mj l mk ml">metrics.calinski_harabasz_score(X, labels_8)</span></pre><p id="b378" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">CV值= 41105.01</p><p id="7945" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以看到，对于这两种类型的分数，8个聚类给出了更好的值。然而，我们必须对不同数量的集群进行多次迭代，才能找到最优的集群。相反，我们可以使用一个叫做<strong class="kz ir">肘图</strong>的东西来找到这个最佳值。</p><blockquote class="ms"><p id="63fc" class="mt mu iq bd mv mw mx my mz na nb ls dk translated">肘形图显示了k值为多少时，一个聚类的平均值与该聚类中的其他数据点之间的距离最小。</p></blockquote><p id="de35" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">这里有两个值很重要— <em class="lt">失真</em>和<em class="lt">惯性</em>。失真是离各个聚类的质心的欧几里德平方距离的平均值。惯性是样本到它们最近的聚类中心的平方距离之和。</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="48d5" class="mg mh iq mc b gy mi mj l mk ml"><em class="lt">#for each value of k, we can initialise k_means and use inertia to identify the sum of squared distances of samples to the nearest cluster centre</em></span><span id="a9b3" class="mg mh iq mc b gy mr mj l mk ml">sum_of_squared_distances = []<br/>K = range(1,15)<br/>for k in K:<br/>    k_means = KMeans(n_clusters=k)<br/>    model = k_means.fit(X)<br/>    sum_of_squared_distances.append(k_means.inertia_)</span></pre><p id="e065" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">请记住，我们关心K-means中的类内相似性，这是肘图有助于捕捉的。</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="4a06" class="mg mh iq mc b gy mi mj l mk ml">plt.plot(K, sum_of_squared_distances, 'bx-')<br/>plt.xlabel('k')<br/>plt.ylabel('sum_of_squared_distances')<br/>plt.title('elbow method for optimal k')<br/>plt.show()</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nh"><img src="../Images/7af30321076c67770a712a7bdc293db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p59HqAw8SCcfemMDDsMYeQ.png"/></div></div></figure><p id="42eb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这里我们可以看到，距离平方和的下降在k=5之后开始放缓。因此，5是我们分析的最佳聚类数。</p><p id="383e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以通过计算k=5时的轮廓系数和CH值来验证这一点。</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="b69e" class="mg mh iq mc b gy mi mj l mk ml">k_means_5 = KMeans(n_clusters=5)<br/>model = k_means_5.fit(X)<br/>y_hat_5 = k_means_5.predict(X)</span><span id="5f6e" class="mg mh iq mc b gy mr mj l mk ml">labels_5 = k_means_5.labels_<br/>metrics.silhouette_score(X, labels_5, metric = 'euclidean')</span><span id="c954" class="mg mh iq mc b gy mr mj l mk ml">metrics.calinski_harabasz_score(X, labels_5)</span></pre><p id="9893" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">轮廓系数= 0.261</p><p id="54c4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">CV值= 48068.32</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/882c3395079d9da2eafdc40dcde4b532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSJCyzHiFsqS5vRTnMRHgA.png"/></div></div></figure><p id="8bde" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这两个值都高于我们之前的星团12和8。我们可以得出结论，k=5是我们的最佳聚类数。</p><p id="012d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们可以看到每个集群中包含的值。使用<a class="ae kw" href="https://www.kaggle.com/prakharrathi25/weather-data-clustering-using-k-means/notebook" rel="noopener ugc nofollow" target="_blank">这个</a>函数，我创建了一个效用图。</p><pre class="kh ki kj kk gt mb mc md me aw mf bi"><span id="a69d" class="mg mh iq mc b gy mi mj l mk ml"><em class="lt">#function that creates a dataframe with a column for cluster number</em></span><span id="834c" class="mg mh iq mc b gy mr mj l mk ml">def pd_centers(cols_of_interest, centers):<br/>        colNames = list(cols_of_interest)<br/>        colNames.append('prediction')</span><span id="d2ec" class="mg mh iq mc b gy mr mj l mk ml"># Zip with a column called 'prediction' (index)<br/>        Z = [np.append(A, index) for index, A in enumerate(centers)]</span><span id="bda1" class="mg mh iq mc b gy mr mj l mk ml"># Convert to pandas data frame for plotting<br/>        P = pd.DataFrame(Z, columns=colNames)<br/>        P['prediction'] = P['prediction'].astype(int)<br/>        return P</span><span id="b0fb" class="mg mh iq mc b gy mr mj l mk ml">P = pd_centers(cols_of_interest, centers)<br/>P</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/3b29c3c7fe6266d3b1f381a3a03f720c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iQYCS7G2pGJf2-Dna8Rhg.png"/></div></div></figure></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="c03b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如需进一步阅读:</p><ol class=""><li id="6eb8" class="nk nl iq kz b la lb ld le lg nm lk nn lo no ls np nq nr ns bi translated"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . cluster . k means . html</a></li><li id="52e8" class="nk nl iq kz b la nt ld nu lg nv lk nw lo nx ls np nq nr ns bi translated"><a class="ae kw" href="https://www.kaggle.com/prakharrathi25/weather-data-clustering-using-k-means/notebook" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/prakharrathi 25/weather-data-clustering-using-k-means/notebook</a></li><li id="b213" class="nk nl iq kz b la nt ld nu lg nv lk nw lo nx ls np nq nr ns bi translated"><a class="ae kw" href="https://www.datasciencecentral.com/profiles/blogs/python-implementing-a-k-means-algorithm-with-sklearn" rel="noopener ugc nofollow" target="_blank">https://www . datascience central . com/profiles/blogs/python-implementing-a-k-means-algorithm-with-sk learn</a></li><li id="68f4" class="nk nl iq kz b la nt ld nu lg nv lk nw lo nx ls np nq nr ns bi translated"><a class="ae kw" href="https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f" rel="noopener ugc nofollow" target="_blank">https://blog . Cambridge spark . com/how-to-determine-the-optimal-number-of-k-means-clustering-14f 27070048 f</a></li></ol></div></div>    
</body>
</html>