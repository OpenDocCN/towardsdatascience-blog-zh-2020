<html>
<head>
<title>Feature Extraction: a mental model for search and recommendation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征提取:搜索和推荐的心理模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-extraction-a-mental-model-for-search-and-recommendation-48a2d3eeb519?source=collection_archive---------48-----------------------#2020-08-19">https://towardsdatascience.com/feature-extraction-a-mental-model-for-search-and-recommendation-48a2d3eeb519?source=collection_archive---------48-----------------------#2020-08-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0e8b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">+对神经网络的一些一般直觉</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d2327494bd7b50f44cf585a2e9df3f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-fpS_vDBChfhTV47"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Aneta Pawlik 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="6851" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">动机</h1><p id="51cd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">图像检索是一个很酷的概念。</p><p id="70aa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">点。啪。搜索。</p><p id="be57" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我想不出比这更好的用户体验了。<a class="ae kv" href="https://lens.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌镜头</a>、<a class="ae kv" href="https://affiliate-program.amazon.com/resource-center/amazon-influencers-introducing-stylesnap" rel="noopener ugc nofollow" target="_blank">亚马逊 StyleSnap </a>、<a class="ae kv" href="https://www.syte.ai/" rel="noopener ugc nofollow" target="_blank"> Syte </a>都有正确的想法。</p><p id="9658" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最近，<a class="ae kv" rel="noopener" target="_blank" href="/up-and-running-with-milvus-2466161e8b1f">写了关于向量相似度搜索引擎 Milvus </a>。我能够在几分钟内让一个快速和肮脏的图像检索应用程序工作。我意识到，建造酷的东西并不是遥不可及的，这是那些授权工程师的时刻之一。我发现自己想了解更多关于卷积神经网络(CNN)以及它们如何在图像检索中工作的信息。</p><p id="85f8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">具体来说，我在想:</p><blockquote class="mp mq mr"><p id="47f1" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated"><em class="iq">给定一些卷积神经网络，我如何选择一个层进行特征提取？</em></p></blockquote><p id="5d66" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我找到的答案大概是这样的:</p><p id="3adb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms">“因使用情形而异。”</em></p><p id="7b96" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms">“各层各有所长。”</em></p><p id="abc9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms">“需要测试。”</em></p><p id="4e2c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些回答让我不满意。他们没有方向感。他们没有留下进一步调查的线索。我在挠头。我决定更深入地研究，经过一些修补，我找到了一种测试特征向量“搜索能力”的方法。更重要的是，我对 CNN 的<strong class="lq ir"/><strong class="lq ir">架构目标</strong> <strong class="lq ir">有了更强的直觉。</strong></p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="4f9f" class="kw kx iq bd ky kz nd lb lc ld ne lf lg jw nf jx li jz ng ka lk kc nh kd lm ln bi translated">理解问题</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/be3c340b2814f1926e05b74af2ce2005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzxRI1qHXjiVXla-_NiMBA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">建筑<a class="ae kv" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG 16 </a></p></figure><h2 id="726b" class="nj kx iq bd ky nk nl dn lc nm nn dp lg lx no np li mb nq nr lk mf ns nt lm nu bi translated">像 VGG16 这样的 CNN 是如何运作的</h2><p id="d23d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">VGG16 是一种卷积神经网络架构。它在图像分类中表现非常好，并且具有相对简单的架构。为了理解它的结构，我喜欢把网络分成两部分:</p><p id="d3a6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">网络的前半部分</strong>(黑色和红色)被称为<em class="ms">特征提取器。</em>它被喂了一个图像。<em class="ms"> </em>它负责从图像中提取有用的信息。随着数据向前传递，每一层都比上一层需要更少的内存来存储有关图像的信息。</p><p id="3582" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">以下是按顺序排列的中间层的形状，请注意每一步中的信息量是如何减少的:</p><ol class=""><li id="c509" class="nv nw iq lq b lr mk lu ml lx nx mb ny mf nz mj oa ob oc od bi translated">112 x 112 x 64 = 802，816</li><li id="5da4" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">56 x 56 x 128 = 401，408</li><li id="1400" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">28 x 28 x 256 = 200，704</li><li id="d534" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">14 x 14 x 512 =100，352</li><li id="eb75" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">7x 7x 512 = 25088</li></ol><p id="4898" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为什么这很重要？一个成功的网络学会了如何接受任何输入，并以某种方式压缩信息，以有效地完成手头的任务。当<em class="ms"> </em>数据是这样的压缩形式时，它被称为<em class="ms">潜在表示</em>或<em class="ms">嵌入</em>。</p><p id="de38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">网络的后半部分</strong>(蓝色和金色)，负责取一些<em class="ms">潜在表象</em>并执行某种任务。在 VGG16 的情况下，任务通常是分类。这可能意味着输出“猫”或“狗”来分类一些图像。这部分网络学习如何从数字上理解<em class="ms">潜在表征。</em>从这里它可以分类，画一个包围盒等。</p><p id="d2cc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在图像检索的上下文中，我们不需要后半部分，所以我们将只关注前半部分。<em class="ms">特征提取器。</em></p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><blockquote class="mp mq mr"><p id="1957" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">当我把机器学习应用想成<strong class="lq ir">数据转换问题时，我的生活就轻松多了。这些是我能理解的问题类型。</strong></p></blockquote></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="0d3a" class="nj kx iq bd ky nk nl dn lc nm nn dp lg lx no np li mb nq nr lk mf ns nt lm nu bi translated">VGG16 如何用于图像检索？</h2><p id="b8cf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一旦我们有了一个模型，这个模型可以把我们的输入图像变成某种潜在的表示。结果通常以张量的形式出现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/001393ed5f4c7d20da0428ecef6ff6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YgaPusTcyZ8Mtb3G9CmdQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://miro.com/" rel="noopener ugc nofollow" target="_blank">由大卫·施维默利用米罗</a>创作。免费使用</p></figure><p id="7747" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">张量没有错，但是要使用它，我们需要把它展平成一个矢量。有不同的方法可以做到这一点(最大池，平均池，或只是扁平化)。一旦我们展平了数据，我们就有了一个由浮点数<em class="ms">(特征)</em>组成的矢量<em class="ms"> </em>，它以某种抽象的方式表示图像:</p><p id="aa2b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ok ol om on b">[0.4366, 0.7169, 0.0093, 0.1545, …]</code></p><p id="a149" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相似的图像会产生相似的<em class="ms">特征向量。</em>度量相似度的方法有很多种，但最常见的是 e <em class="ms"> uclidean distance </em>和<em class="ms">余弦相似度。这些术语可能听起来很陌生，但它们背后的数学原理没什么可怕的。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/af30c52b06a91bb2eee9e03638032ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9AE8cN4HVyMUu1NI4TjHw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://miro.com/" rel="noopener ugc nofollow" target="_blank">由大卫·施维默利用米罗</a>创作。免费使用</p></figure><p id="62b4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">给定所有这些信息，图像检索可以分为以下步骤:</p><ol class=""><li id="5eac" class="nv nw iq lq b lr mk lu ml lx nx mb ny mf nz mj oa ob oc od bi translated">使用<em class="ms">特征提取器</em>到<em class="ms"> </em>将图像处理成<em class="ms">特征向量</em>。</li><li id="23bd" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">将<em class="ms">特征向量</em>存储在像 Milvus 这样的搜索引擎中。Milvus 允许我们通过一些相似性度量来搜索向量，比如<em class="ms">欧几里德距离</em>。</li><li id="77e3" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">使用相同的特征提取器将任何查询图像处理成一个<em class="ms">特征向量</em>，然后在 Milvus 中查询相似的出现。</li></ol><p id="9b79" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">事实上，这些步骤可以应用于一般的搜索或推荐系统。它们绝不仅限于图像。音频、文本、视频都遵循相同的模式</p><p id="54c4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">回到手头的任务，我想弄清楚 CNN 的哪一层会导致最好的搜索结果。我有一个想法:<em class="ms">一个能很好地用于分类的特征提取器也能很好地用于搜索</em>。我决定测试这个想法，分享代码和我的见解。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="f907" class="kw kx iq bd ky kz nd lb lc ld ne lf lg jw nf jx li jz ng ka lk kc nh kd lm ln bi translated">实验</h1><p id="6a71" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">所用的猫/狗数据集可在<a class="ae kv" href="https://www.kaggle.com/c/dogs-vs-cats" rel="noopener ugc nofollow" target="_blank"> kaggle </a>上获得。下载完成后，我将图像提取到一个名为<code class="fe ok ol om on b">data</code>的目录中。本实验的代码可在<a class="ae kv" href="https://github.com/dshvimer/feature-extraction" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。采取的步骤如下:</p><ol class=""><li id="7565" class="nv nw iq lq b lr mk lu ml lx nx mb ny mf nz mj oa ob oc od bi translated">加载狗/猫数据集。</li><li id="bc74" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">从我们想要在模型中测试的每一层获取输出。执行 MaxPooling 操作，将每个输出平坦化为一个<em class="ms">特征向量。</em></li><li id="e303" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">将结果按图层分组:<em class="ms"> block1_pool.csv，block2_pool.csv… </em></li><li id="6953" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">对于每个数据集，训练和评估一个分类模型。选择 KNeighborsClassifier 是因为它使用欧几里得距离。</li><li id="2fd4" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">比较结果。</li></ol></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="71a7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">关于使用 VGG16 进行特征提取的说明</strong></p><p id="7f56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们需要访问网络的中间层。默认情况下，预训练的 VGG16 将只输出最终层。我们可以通过在 keras 中使用通用的<code class="fe ok ol om on b">Model</code>类来解决这个问题。在下面的代码片段中，我们声明了一个输出每个块的池层的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="op oq l"/></div></figure></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="d709" class="kw kx iq bd ky kz nd lb lc ld ne lf lg jw nf jx li jz ng ka lk kc nh kd lm ln bi translated">结果呢</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/1fbf358c2a062b06023718b335a57d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*40GePRVZEQA1gfngg64sQA.png"/></div></figure><p id="5657" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">以下是使用 10 倍交叉验证训练分类器(KNeighborsClassifier)的结果。我们看到每一层在分类上都比上一层好。我大声对自己说<em class="ms">【咄】</em><em class="ms"/>。</p><p id="2faf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对预训练的 VGG16 模型进行分类训练。它训练的<em class="ms"> imagenet </em>数据集毕竟有猫狗在里面！我的实验基本上证实了网络的前半部分已经学会了如何表示信息。被丢弃的另一半网络很容易被我们的分类模型所取代。</p><p id="4e31" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那么在搜索方面有什么结论呢？</p><p id="5e28" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“block5_pool”层将很好地表现我们的搜索图像。不会经常混淆猫狗。这将导致较高的<em class="ms">召回</em>分数，这是一种用于评估搜索相关性的衡量标准。</p><p id="7ef8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">但是，类内相似性呢？</strong></p><p id="ab6c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该模型能在多大程度上确定同一犬种的图像比不同犬种的图像<strong class="lq ir">更加</strong>相似？</p><p id="6620" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">好吧，如果网络从来没有为这个任务优化过，那就不能保证它能分辨出不同。例如，如果类内相似性取决于颜色、形状或纹理等属性，我们可能会很幸运。这是因为这些特征在最初的分类任务中可能非常重要。如果我们在寻找更细微的结果，我们将需要用更精细的数据集来测试，以获得完全的信心。更精细的可能意味着按品种、颜色、大小等给每张图片贴上标签。</p><p id="0b75" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，问题往往比这更微妙。考虑一个面部识别系统。预测一幅图像包含一张脸和识别一个特定的人的脸不是一回事。在这些情况下，人们可以考虑使用<a class="ae kv" rel="noopener" target="_blank" href="/image-similarity-using-triplet-loss-3744c0f67973">三联体网络</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/aa7a0ce08122a153fcf22aed401ebd62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-gKds4bIXuGs7TvuusY5w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://github.com/omoindrot/tensorflow-triplet-loss" rel="noopener ugc nofollow" target="_blank">此报告的图片</a></p></figure><p id="7132" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">如果数据集没有被标记怎么办？</strong></p><p id="c04d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">幸运的是，有一个工具可以解决这个问题！此处<a class="ae kv" href="https://medium.com/@franky07724_57962/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1#:~:text=Let's%20consider%20VGG%20as%20our,model%20with%2019%20weight%20layers." rel="noopener">描述的方法</a>演示了如何使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient" rel="noopener ugc nofollow" target="_blank">轮廓系数</a>。这是一个用于评估数据集在无监督学习中表现如何的指标。</p><blockquote class="mp mq mr"><p id="9ab9" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">较高的轮廓系数分数与具有更好定义的聚类的模型相关</p></blockquote><p id="18c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">不要认为这是测量未标注数据集的唯一可用方法。毕竟，有一个完整的 ML 分支关注无监督学习。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="f0db" class="kw kx iq bd ky kz nd lb lc ld ne lf lg jw nf jx li jz ng ka lk kc nh kd lm ln bi translated">最后的想法</h1><p id="8449" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它确实因用例而异。至少现在我可以解释为什么了。</p></div></div>    
</body>
</html>