<html>
<head>
<title>The Multi-Channel Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多通道神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-multi-channel-neural-network-26551bdfab6c?source=collection_archive---------14-----------------------#2020-07-04">https://towardsdatascience.com/the-multi-channel-neural-network-26551bdfab6c?source=collection_archive---------14-----------------------#2020-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b0b5efffad37362dc5e90a297d35d2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hFEDBmQEQ5Wb6I7t"/></div></div></figure><div class=""/><div class=""><h2 id="fe02" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">神经网络可以，而且<strong class="ak">应该</strong>，在多种类型的特征上被训练。本教程将介绍如何使用 Keras Functional API 来扩展您的网络。</h2></div></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><p id="3389" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">神经网络广泛应用于多个领域，如计算机视觉、音频分类、自然语言处理等。在大多数情况下，它们在这些领域中被单独考虑。然而，在现实生活中，这很少是最佳配置。更常见的是有多个通道，意味着几种不同类型的输入。类似于人类如何使用广泛的感官输入(听觉、视觉等)来提取洞察力。)，神经网络可以(也应该)在多个输入上进行训练。</p><p id="87db" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们以<strong class="lc jf">情感识别</strong>的任务为例。</p><p id="d05c" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">人类不使用单一输入来有效地分类对话者的情绪。例如，他们不仅仅使用对话者的面部表情(视觉)、声音(听觉)或词语的含义(文本)，而是混合使用。类似地，神经网络可以对多个输入进行训练，例如图像、音频和文本，并进行相应的处理(通过 CNN、NLP 等。)，以得出对目标情绪的有效预测。通过这样做，神经网络可以更好地捕捉难以从每个通道单独获得的微妙之处(例如讽刺)，类似于人类。</p><h1 id="f5b2" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">系统结构</h1><p id="ce7d" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">考虑到情绪识别的任务，为简单起见，我们将其限制为三类(积极、消极和中性)，我们可以将该系统想象为如下:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/363b252487416184c43befc6fcd2bebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ioo5Rn6B5iTENG5njBuOXQ.png"/></div></div></figure><p id="039d" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">该系统通过麦克风拾取音频，将声音的 MEL 声谱图计算为图像，并将其转录为一串文本。这两个信号然后被用作模型的输入，每个信号被馈送到它的一个分支。事实上，神经网络由两部分组成:</p><ul class=""><li id="a4af" class="my mz je lc b ld le lg lh lj na ln nb lr nc lv nd ne nf ng bi translated">左分支，通过卷积神经网络进行图像分类</li><li id="5ea4" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">右分支，使用嵌入对文本执行 NLP。</li></ul><p id="fa7e" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，每一侧的输出被馈送到一组公共的密集层，其中最后一层具有三个神经元，以分别对三个类别进行分类(阳性、中性和阴性)。</p><h1 id="c8a6" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">设置</h1><p id="0805" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">在本例中，我们将使用 MELD 数据集，该数据集由带有相关情绪标签的简短对话组成，表明陈述的情绪是积极的、中立的还是消极的。数据集可以在这里找到<a class="ae nm" href="https://github.com/declare-lab/MELD" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="4e56" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">你也可以在这里找到这篇文章<a class="ae nm" href="https://github.com/ezuccarelli/Medium/tree/master/The%20Multi-Channel%20Neural%20Network" rel="noopener ugc nofollow" target="_blank">的完整代码。</a></p><h1 id="5f11" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">图像分类处</h1><p id="7fb6" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">声音采集完成后，系统会立即计算音频信号的声谱图。音频分类的最新特征是 MEL 频谱图和 MEL 频率倒谱系数。对于这个例子，我们将使用 MEL 谱图。</p><p id="2c86" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，我们显然会从包含音频的文件夹中加载数据，在培训、验证和测试中进行拆分:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="c0e3" class="ns lx je no b gy nt nu l nv nw">import gensim.models as gm<br/>import glob as gb<br/>import keras.applications as ka<br/>import keras.layers as kl<br/>import keras.models as km<br/>import keras.optimizers as ko<br/>import keras_preprocessing.image as ki<br/>import keras_preprocessing.sequence as ks<br/>import keras_preprocessing.text as kt<br/>import numpy as np<br/>import pandas as pd<br/>import pickle as pk<br/>import tensorflow as tf<br/>import utils as ut<br/><br/><br/># Data<br/>Data_dir = np.array(gb.glob('../Data/MELD.Raw/train_splits/*'))<br/>Validation_dir = np.array(gb.glob('../Data/MELD.Raw/dev_splits_complete/*'))<br/>Test_dir = np.array(gb.glob('../Data/MELD.Raw/output_repeated_splits_test/*'))<br/><br/># Parameters<br/>BATCH = 16<br/>EMBEDDING_LENGTH = 32</span></pre><p id="d79c" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，我们可以遍历这三个文件夹中的每个音频文件，计算 MEL 频谱图，并将其作为图像保存到新文件夹中:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="018f" class="ns lx je no b gy nt nu l nv nw"># Convert Audio to Spectrograms<br/>for file in Data_dir:<br/>    filename, name = file, file.split('/')[-1].split('.')[0]<br/>    ut.create_spectrogram(filename, name)<br/><br/>for file in Validation_dir:<br/>    filename, name = file, file.split('/')[-1].split('.')[0]<br/>    ut.create_spectrogram_validation(filename, name)<br/><br/>for file in Test_dir:<br/>    filename, name = file, file.split('/')[-1].split('.')[0]<br/>    ut.create_spectrogram_test(filename, name)</span></pre><p id="4256" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为此，我们在实用程序脚本中创建了一个用于训练、验证和测试的函数。该函数使用<em class="nx"> librosa </em>包加载音频文件，对其进行处理，然后将其保存为图像:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="decb" class="ns lx je no b gy nt nu l nv nw">import librosa<br/>import librosa.display<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import path as ph<br/>import pydub as pb<br/>import speech_recognition as sr<br/>import warnings</span><span id="f10d" class="ns lx je no b gy ny nu l nv nw">def create_spectrogram(filename, name):<br/>    plt.interactive(False)<br/>    clip, sample_rate = librosa.load(filename, sr=None)</span><span id="0012" class="ns lx je no b gy ny nu l nv nw">    fig = plt.figure(figsize=[0.72, 0.72])<br/>    ax = fig.add_subplot(111)<br/>    ax.axes.get_xaxis().set_visible(False)<br/>    ax.axes.get_yaxis().set_visible(False)<br/>    ax.set_frame_on(False)</span><span id="d24d" class="ns lx je no b gy ny nu l nv nw">    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)<br/>    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))<br/>    filename = '../Images/Train/' + name + '.jpg'</span><span id="7b8f" class="ns lx je no b gy ny nu l nv nw">    plt.savefig(filename, dpi=400, bbox_inches='tight', pad_inches=0)<br/>    plt.close()<br/>    fig.clf()<br/>    plt.close(fig)<br/>    plt.close('all')<br/>    del filename, name, clip, sample_rate, fig, ax, S</span></pre><p id="2810" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们将每个音频信号转换为表示相应频谱图的图像，我们就可以加载包含每个音频标签信息的数据集。为了正确地将每个音频文件链接到它的情感，我们创建一个 ID 列，包含该样本的图像文件的名称:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="c8f8" class="ns lx je no b gy nt nu l nv nw"># Data Loading<br/>train = pd.read_csv('../Data/MELD.Raw/train_sent_emo.csv', dtype=str)</span><span id="8c2b" class="ns lx je no b gy ny nu l nv nw">validation = pd.read_csv('../Data/MELD.Raw/dev_sent_emo.csv', dtype=str)</span><span id="9da8" class="ns lx je no b gy ny nu l nv nw">test = pd.read_csv('../Data/MELD.Raw/test_sent_emo.csv', dtype=str)<br/></span><span id="4dbd" class="ns lx je no b gy ny nu l nv nw"># Create mapping to identify audio files<br/>train["ID"] = 'dia'+train["Dialogue_ID"]+'_utt'+train["Utterance_ID"]+'.jpg'</span><span id="583e" class="ns lx je no b gy ny nu l nv nw">validation["ID"] = 'dia'+validation["Dialogue_ID"]+'_utt'+validation["Utterance_ID"]+'.jpg'</span><span id="6e99" class="ns lx je no b gy ny nu l nv nw">test["ID"] = 'dia'+test["Dialogue_ID"]+'_utt'+test["Utterance_ID"]+'.jpg'</span></pre><h1 id="772c" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">自然语言处理处</h1><p id="8592" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">同时，我们还需要获取与音频信号相关的文本，并使用 NLP 技术对其进行处理，将其转换为数字向量，以便神经网络可以对其进行处理。既然我们已经有了 MELD 数据集本身的文本信息，我们就可以继续了。否则，如果信息不可用，我们可以使用文本转换库，比如谷歌云的<em class="nx">语音识别</em>:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="32cb" class="ns lx je no b gy nt nu l nv nw"># Text Features<br/>tokenizer = kt.Tokenizer(num_words=5000)<br/>tokenizer.fit_on_texts(train['Utterance'])<br/><br/>vocab_size = len(tokenizer.word_index) + 1<br/><br/>train_tokens = tokenizer.texts_to_sequences(train['Utterance'])<br/>text_features = pd.DataFrame(ks.pad_sequences(train_tokens, maxlen=200))<br/><br/>validation_tokens = tokenizer.texts_to_sequences(validation['Utterance'])<br/>validation_features = pd.DataFrame(ks.pad_sequences(validation_tokens, maxlen=200))</span></pre><p id="cc9d" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们首先标记每个音频中的句子，然后将它们转换成长度为 200 的数字向量。</p><h1 id="d1c4" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">数据管道</h1><p id="793e" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">整合多媒体输入最棘手的一个方面是创建一个定制的数据生成器。这个结构基本上是一个函数，每次被调用时，它能够迭代地返回到模型的下一批输入。使用 Keras 的预制生成器相对容易，但没有实现允许您将多个输入合并在一起，并确保两个输入并排无误地输入到模型中。</p><p id="f207" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面的代码足够通用，可以在不同的设置中使用，而不仅仅是与这个例子相关。具体来说，它需要一个文件夹位置(图像所在的位置)和一个“普通”数据集(行中有样本，列中有要素)。然后，在这种情况下，它迭代地提供图像和文本特征的下一个样本，这两个样本都是相同大小的批次:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="95f8" class="ns lx je no b gy nt nu l nv nw"># Data Pipeline<br/>def train_generator(features, batch):<br/>    # Image Generator<br/>    train_generator = ki.ImageDataGenerator(<br/>        rescale=1. / 255.)</span><span id="531e" class="ns lx je no b gy ny nu l nv nw">    train_generator = train_generator.flow_from_dataframe(<br/>        dataframe=train,<br/>        directory="../Images/Train/",<br/>        x_col="ID",<br/>        y_col="Sentiment",<br/>        batch_size=batch,<br/>        seed=0,<br/>        shuffle=False,<br/>        class_mode="categorical",<br/>        target_size=(64, 64))</span><span id="bfad" class="ns lx je no b gy ny nu l nv nw">    train_iterator = features.iterrows()<br/>    j = 0<br/>    i = 0</span><span id="23a2" class="ns lx je no b gy ny nu l nv nw">    while True:<br/>        genX2 = pd.DataFrame(columns=features.columns)</span><span id="8d5c" class="ns lx je no b gy ny nu l nv nw">        while i &lt; batch:<br/>            k,r = train_iterator.__next__()<br/>            r = pd.DataFrame([r], columns=genX2.columns)<br/>            genX2 = genX2.append(r)<br/>            j += 1<br/>            i += 1</span><span id="8aeb" class="ns lx je no b gy ny nu l nv nw">            if j == train.shape[0]:<br/>                X1i = train_generator.next()</span><span id="70e6" class="ns lx je no b gy ny nu l nv nw">                train_generator = ki.ImageDataGenerator(<br/>                    rescale=1. / 255.)</span><span id="0d26" class="ns lx je no b gy ny nu l nv nw">                train_generator = train_generator.flow_from_dataframe(<br/>                    dataframe=train,<br/>                    directory="../Images/Train/",<br/>                    x_col="ID",<br/>                    y_col="Sentiment",<br/>                    batch_size=batch,<br/>                    seed=0,<br/>                    shuffle=False,<br/>                    class_mode="categorical",<br/>                    target_size=(64, 64))</span><span id="605f" class="ns lx je no b gy ny nu l nv nw">                # Text Generator<br/>                train_iterator = features.iterrows()<br/>                i = 0<br/>                j=0<br/>                X2i = genX2<br/>                genX2 = pd.DataFrame(columns=features.columns)<br/>                yield [X1i[0], tf.convert_to_tensor(X2i.values, dtype=tf.float32)], X1i[1]</span><span id="072d" class="ns lx je no b gy ny nu l nv nw">        X1i = train_generator.next()<br/>        X2i = genX2<br/>        i = 0<br/>        yield [X1i[0], tf.convert_to_tensor(X2i.values, dtype=tf.float32)], X1i[1]</span></pre><h1 id="0cd0" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">神经网络体系结构</h1><p id="a22b" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">最后，我们可以创建输入图像(声谱图)和文本(转录)的模型，并对它们进行处理。</p><h2 id="20a5" class="ns lx je bd ly nz oa dn mc ob oc dp mg lj od oe mi ln of og mk lr oh oi mm oj bi translated">输入</h2><p id="8093" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">输入包括由 64x64 像素和 3 个通道(RGB)组成的图像，以及长度为 200 的“正常”数字特征，表示文本的编码:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="f68f" class="ns lx je no b gy nt nu l nv nw"># Model<br/># Inputs<br/>images = km.Input(shape=(64, 64, 3))<br/>features = km.Input(shape=(200, ))</span></pre><h2 id="4053" class="ns lx je bd ly nz oa dn mc ob oc dp mg lj od oe mi ln of og mk lr oh oi mm oj bi translated">图像分类(CNN)分部</h2><p id="15bf" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">影像分类分支由初始 VGG19 网络和一组自定义图层组成。通过使用 VGG19，我们可以利用迁移学习的优势。特别是，由于该模型已经在 ImageNet 数据集上进行了预训练，它已经从初始化为对图像分类任务有意义的值的系数开始。然后，输出被输入到一系列层中，这些层可以学习这类图像的具体特征，然后输出到下一个公共层:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="20f3" class="ns lx je no b gy nt nu l nv nw"># Transfer Learning Bases<br/>vgg19 = ka.VGG19(weights='imagenet', include_top=False)<br/>vgg19.trainable = False<br/><br/># Image Classification Branch<br/>x = vgg19(images)<br/>x = kl.GlobalAveragePooling2D()(x)<br/>x = kl.Dense(32, activation='relu')(x)<br/>x = kl.Dropout(rate=0.25)(x)<br/>x = km.Model(inputs=images, outputs=x)</span></pre><h2 id="eb86" class="ns lx je bd ly nz oa dn mc ob oc dp mg lj od oe mi ln of og mk lr oh oi mm oj bi translated">文本分类分支</h2><p id="3056" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">NLP 分支使用长短期记忆(LSTM)层和嵌入层来处理数据。为了避免模型过度捕捞，还添加了脱落层，类似于 CNN 分支机构所做的工作:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="a47f" class="ns lx je no b gy nt nu l nv nw"># Text Classification Branch<br/>y = kl.Embedding(vocab_size, EMBEDDING_LENGTH, input_length=200)(features)<br/>y = kl.SpatialDropout1D(0.25)(y)<br/>y = kl.LSTM(25, dropout=0.25, recurrent_dropout=0.25)(y)<br/>y = kl.Dropout(0.25)(y)<br/>y = km.Model(inputs=features, outputs=y)</span></pre><h2 id="7c7c" class="ns lx je bd ly nz oa dn mc ob oc dp mg lj od oe mi ln of og mk lr oh oi mm oj bi translated">公共层</h2><p id="c345" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">然后，我们可以将这两种输出结合起来，输入到一系列密集层中。一组密集层能够捕捉仅当音频和文本信号组合时才可用的信息，并且不能从每个单独的输入中识别。</p><p id="539f" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们还使用学习率为 0.0001 的 Adam 优化器，这通常是一个很好的组合:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="125e" class="ns lx je no b gy nt nu l nv nw">combined = kl.concatenate([x.output, y.output])<br/><br/>z = kl.Dense(32, activation="relu")(combined)<br/>z = kl.Dropout(rate=0.25)(z)<br/>z = kl.Dense(32, activation="relu")(z)<br/>z = kl.Dropout(rate=0.25)(z)<br/>z = kl.Dense(3, activation="softmax")(z)<br/><br/>model = km.Model(inputs=[x.input, y.input], outputs=z)<br/><br/>model.compile(optimizer=ko.Adam(lr=0.0001), loss='categorical_crossentropy', metrics='accuracy')<br/><br/>model.summary()</span></pre><h1 id="b66e" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">模特培训</h1><p id="2c91" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">然后，可以使用我们之前通过执行以下操作创建的训练和验证生成器来训练该模型:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="5100" class="ns lx je no b gy nt nu l nv nw"># Hyperparameters<br/>EPOCHS = 13<br/>TRAIN_STEPS = np.floor(train.shape[0]/BATCH)<br/>VALIDATION_STEPS = np.floor(validation.shape[0]/BATCH)<br/><br/># Model Training<br/>model.fit_generator(generator=train_generator(text_features, BATCH),<br/>                    steps_per_epoch=TRAIN_STEPS,<br/>                    validation_data=validation_generator(validation_features, BATCH),<br/>                    validation_steps=VALIDATION_STEPS,<br/>                    epochs=EPOCHS)</span></pre><h1 id="3084" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">模型评估</h1><p id="28a5" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">最后，在一组数据上评估模型，例如单独的验证集，然后保存为文件，以便在“实时”场景中使用时加载:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="f729" class="ns lx je no b gy nt nu l nv nw"># Performance Evaluation<br/># Validation<br/>model.evaluate_generator(generator=validation_generator(validation_features, BATCH))<br/><br/># Save the Model and Labels<br/>model.save('Model.h5')</span></pre><h1 id="1bad" class="lw lx je bd ly lz ma mb mc md me mf mg kk mh kl mi kn mj ko mk kq ml kr mm mn bi translated">摘要</h1><p id="d75d" class="pw-post-body-paragraph la lb je lc b ld mo kf lf lg mp ki li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">总的来说，我们构建了一个能够接受多种类型输入(图像、文本等)的系统。)，对它们进行预处理，然后将它们馈送到由每个输入的一个分支组成的神经网络。每个分支单独处理其输入，然后汇聚成一组公共层，预测最终输出。</p><p id="9e2b" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">具体步骤是:</p><ul class=""><li id="a52b" class="my mz je lc b ld le lg lh lj na ln nb lr nc lv nd ne nf ng bi translated">数据加载</li><li id="1225" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">分别预处理输入(频谱图、符号化)</li><li id="281c" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">创建自定义数据生成器</li><li id="2b09" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">构建模型架构</li><li id="810e" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">模特培训</li><li id="f1d8" class="my mz je lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">性能赋值</li></ul></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><p id="d3cb" class="pw-post-body-paragraph la lb je lc b ld le kf lf lg lh ki li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="nx">要阅读更多类似的文章，请关注我的</em> <a class="ae nm" href="https://twitter.com/jayzuccarelli" rel="noopener ugc nofollow" target="_blank"> <em class="nx"> Twitter </em> </a> <em class="nx">，</em><a class="ae nm" href="https://www.linkedin.com/in/ezuccarelli" rel="noopener ugc nofollow" target="_blank"><em class="nx">LinkedIn</em></a><em class="nx">或我的</em> <a class="ae nm" href="https://eugeniozuccarelli.com/" rel="noopener ugc nofollow" target="_blank"> <em class="nx">网站</em> </a> <em class="nx">。</em></p></div></div>    
</body>
</html>