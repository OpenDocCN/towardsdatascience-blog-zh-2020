<html>
<head>
<title>Language Models and Fake News: the Democratization of Propaganda</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型与假新闻:宣传的民主化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054?source=collection_archive---------55-----------------------#2020-06-07">https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054?source=collection_archive---------55-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b2fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用开放的GPT- 3模型深度伪造信息</h2></div><h1 id="3f81" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><h1 id="31f2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">“太危险了，不能释放。”</h1><p id="8702" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一个短语发布了OpenAI的<a class="ae lw" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-artificial-intelligence-dangerous-text-gpt2-elon-musk-a9192121.html" rel="noopener ugc nofollow" target="_blank">新闻声明</a>，以配合他们在2019年2月发布的GPT-2语言模型。随着上周<a class="ae lw" rel="noopener" target="_blank" href="/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc">发布更先进的GPT-3</a>，人工智能驱动的错误信息的可能性已经成为当今后事实信息景观中仍未解决的重大风险。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/d0a70d4f63de9d77f295cdf77fb490d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhKKfcKEGDgt8hS5hiE9kw.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-3生成的诗歌(布朗等人。铝)</p></figure><p id="428d" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">自2016年以来，“假新闻”一词在世界各地的政治领导层和普通民众中日益流行，作为一个不屑一顾的概念，用于不支持自己观点的报道。但是这个术语已经扩展到包括国家和非国家操作的错误信息运动。利用这种<a class="ae lw" href="https://time.com/5565991/russia-influence-2016-election/" rel="noopener ugc nofollow" target="_blank">活动影响全球事件</a>的努力已经大大加快，美国参议院两党委员会得出结论，俄罗斯在2016年美国总统选举期间的错误信息活动“通过俄罗斯控制的宣传渠道传播信息，以破坏公众对民主进程的信心”。罗伯特·穆勒也表达了类似的观点，他补充说，俄罗斯的干预是“全面和系统的”，并担心在即将到来的2020年总统选举中继续受到影响。研究人员现在认为，政治机器人和虚假信息在其他重大案件中发挥了作用，如英国退出欧盟公投和克里米亚危机。</p><p id="f868" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">这些具体的报告让人担心人工智能会被用作网络战的工具来支持错误信息活动。然而，<a class="ae lw" href="https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/" rel="noopener ugc nofollow" target="_blank">研究表明</a>大多数“人工智能支持的”解决方案只不过是机器人设计来重复特定标签下的特定短语和链接，以改变讨论的背景。这种方法更多地依赖于人类操作员理解在线话语心理的能力，而不是机器人本身的能力。最近，社交媒体平台上关于新冠肺炎危机的人工智能生成的内容<a class="ae lw" href="https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/" rel="noopener ugc nofollow" target="_blank"/>的<a class="ae lw" href="https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/" rel="noopener ugc nofollow" target="_blank">激增，导致要求改进审查的呼声日益高涨。然而，人工审查既慢又粗糙，而人工智能对大量数据进行建模以进行训练。脸书首席技术官迈克·斯科洛普夫总结了为新的不可预见的威胁构建解决方案的基本挑战:</a></p><blockquote class="ms mt mu"><p id="81a2" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated"><em class="it">“为一个能够理解前所未见内容的东西构建一个新颖的分类器需要时间和大量数据。”</em></p></blockquote><p id="3c6e" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">目前还没有关于人工智能产生的错误信息的普遍性的系统研究，很可能迄今为止大多数错误信息都是由人类行为者创造的。但是随着人工智能生成的虚假账户的扩散，加上语言模型的可用性和能力的增加，预计人工智能生成的文本错误信息将在未来遇到。如今观察到的事后、意见主导的信息格局加剧了这一问题。</p><p id="3e99" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated"><strong class="lc iu">但是为什么对这些模型忧心忡忡呢？是什么让他们变得危险？我们正在见证真理的死亡吗？</strong>要回答这个问题，我们需要深入研究它们的原理和运作方式。出于时间和范围的考虑，在本文中，我们将把注意力集中在OpenAI的GPT系列语言模型上。</p><h1 id="db37" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">变形金刚:现代自然语言处理的基石</h1><p id="bf53" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们已经在以前的文章中介绍了语言模型背后的一般理论，如递归神经网络(<a class="ae lw" href="https://medium.com/gradientcrescent/applying-sentiment-analysis-to-e-commerce-classification-using-recurrent-neural-networks-in-keras-cd89b77baa60" rel="noopener"> RNNs </a>)和长短期记忆(L <a class="ae lw" href="https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3" rel="noopener"> STM </a>)架构，因此鼓励读者参考这些来详细了解这些架构。</p><p id="7682" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">自然语言模型的GPT家族是基于转换器模型的，其特征是重复的编码器架构加上一个<em class="mv">注意力</em>机制。<a class="ae lw" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> Alammar等人已经深入解释了变压器的一般架构。但是我们将在这里提供一个高层次的总结。</a></p><p id="d3ba" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">考虑变压器模型的一般结构，如下所示:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8dd6481e5d713a0a5fafae202a80847f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*9B93a5x7Z66OneIttn7VQw.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">变压器型架构的抽象。铝)</p></figure><p id="3164" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">编码器-解码器架构的工作原理是，通过编码器将输入数据缩减为潜在维度(表示单词的含义)，由解码器组件以目标语言重建。正因为如此，它们在机器翻译方面一直表现强劲。然而，Transformer-models通过将<em class="mv">自关注</em>层整合到每个编码器和解码器模块中来构建这种架构，每个组件拥有自己的一组权重参数。<em class="mv">注意力</em>层的工作是将输入单词在句子中的位置与其含义联系起来，以便改进其编码。特别是，与传统的纯RNN和LSTM架构相比，变压器模型更能够捕捉序列中的长程相关性。</p><p id="f4bb" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">我们可以用一个例子来最好地形象化注意力对编码过程的影响。考虑下面的句子:</p><blockquote class="ms mt mu"><p id="b351" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated"><em class="it">“这只动物没有过马路，因为它太累了”</em></p></blockquote><p id="1960" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">通过检查相应层的激活，我们可以用注意力来观察句子成分之间的关系。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi na"><img src="../Images/79c21d9d469171604191d947a9a97138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hjuc1TzQFRfWSnxpA91NzA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">句子编码分析(Alammar等。铝)</p></figure><p id="3974" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">你会注意到单词<em class="mv">“它”</em>的编码表示拥有与概念<em class="mv">“动物”</em>和<em class="mv">“疲倦”</em>的强注意力链接，在这些对之间创建了一种意义关联的形式。这种关联的好处对于机器翻译应用程序特别有用，在机器翻译应用程序中，不同语言的语法可能需要完全不同的句子结构，但是这种关联也可以扩展到条件文本生成和其他应用程序。</p><h1 id="b80d" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">GPT模式</h1><p id="dfc0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">OpenAI的语义语言模型GPT家族<a class="ae lw" href="https://arxiv.org/pdf/1801.10198.pdf" rel="noopener ugc nofollow" target="_blank">基于谷歌大脑</a>的工作，完全去除了编码器组件，由一堆带有注意力层的解码器块组成。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nb"><img src="../Images/4f0d298894005aefd87ac0ed1c86fb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YV_pIHKja3w7RRJJ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-2体系结构的抽象(Alammar等人)</p></figure><p id="1f10" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">虽然该模型被训练为在给定输入文本序列的情况下预测序列中的下一个单词，但它们的高参数复杂性导致了通过元学习(或仅用新任务的几个示例进行再训练)获得的显著扩展的能力，包括:</p><ul class=""><li id="8dbd" class="nc nd it lc b ld mn lg mo lj ne ln nf lr ng lv nh ni nj nk bi translated">长期相关性分析</li><li id="c7b1" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">生词记忆</li><li id="8ebb" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">算术计算</li><li id="e500" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">机器翻译</li><li id="9015" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">摘要</li><li id="9ef0" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">语境分析</li></ul><p id="eea8" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">最初的GPT-1模型由变压器模型和预训练的ELMO语言模型组成，具有数亿个可训练参数。GPT-2扩展到超过15亿个参数，是它的10倍以上，并在10倍以上的数据量上进行训练(来自互联网和文学来源)。</p><p id="3172" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">最近的GPT-3建立了超过1750亿个参数，提高了其语义性能，特别是在一次性或零次训练应用中。实现这种应用程序的高性能对于接近语言模型的人类水平的响应和性能是必要的。相比之下，典型的人脑有超过100万亿个突触<a class="ae lw" href="https://human-memory.net/brain-neurons-synapses/#:~:text=Each%20individual%20neuron%20can%20form,as%20neural%20nets%20or%20assemblies" rel="noopener ugc nofollow" target="_blank"/>，大约比最大的GPT-3模型大三个数量级。鉴于OpenAI花了大约一年左右的时间将他们模型的参数容量增加了两个数量级，在给定时间和资源的情况下，达到这个数量似乎是可行的任务。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nb"><img src="../Images/cee9517474da797d51e53367eca85a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uX543DQmuB4mv499.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">各种GPT-2参数配置的体系结构的抽象。(阿拉玛等人。铝)</p></figure><p id="214b" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">GPT-2和GPT-3也在不同的参数配置中进行了测试，以评估其性能并防止潜在的误用。GPT-3配置的配置和基准性能如下所示:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nq"><img src="../Images/59adc4901cdc79bc652e47755a3ca62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7kkICxM2bqkxoeLe_tv1A.png"/></div></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/257638b2fb3d99a31f2028208fc56a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*D_p2upM-h0QMnsYasza7jg.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">当配置不同数量的训练参数时，GPT-3模型在零炮、一炮和少炮应用中的性能精度。</p></figure><p id="0d8a" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">我们可以通过比较GPT 3号卫星输出结果的差异来最好地展示其性能的提高。下面是GPT-2生成的文本样本，正如Brown等人在最初的OpenAI论文中所报告的。艾尔。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f1e483976e9db3913c5f36404c9157c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*1EjYXpGPNchoWpWV3Xj2iw.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">布朗等人报道的GPT-2文本输出。</p></figure><p id="3bf4" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">虽然乍一看，这篇文章似乎语法正确，但仔细检查就会发现许多事实上的不一致。例如，考虑这个句子:</p><blockquote class="ms mt mu"><p id="4984" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated">"<em class="it">这些银白色的四角独角兽此前并不为科学界所知。"</em></p></blockquote><p id="5505" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">鉴于独角兽被定义为拥有一只角，这是事实上的不一致。自然，模型不会明确知道单词的定义，也无法从训练数据中推断出它。</p><p id="5df8" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">下一节显示了类似的不一致之处。</p><blockquote class="ms mt mu"><p id="9c10" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated">虽然它们的起源仍不清楚，但有人认为这种生物可能是在人类文明之前，人类和独角兽相遇时创造出来的</p></blockquote><p id="0225" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">鉴于文章的主题是独角兽本身，这句话没有意义。然而，由于句子在语法上是正确的，这些错误需要大量的注意力和意识才能被识别出来。</p><p id="a2dd" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">相比之下，让我们举两个报道的at GPT-3输出的例子——第一个具有更高的逼真度，更能够冒充为人类生成的(88%),而第二个则明显不那么逼真(39%)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0cc1874b3a5aeb1c2278257f71a85fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*eaRqmVnFHS-UqolW-tE8tA.png"/></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/b1a88fc1309a2832f8f05495216df733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*bkbT23gfPoRbOWvjx-3lBA.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">布朗等人报道的GPT-3文本输出。</p></figure><p id="36b9" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">与GPT新协议相比，GPT新协议似乎抓住了词汇定义的微妙之处。然而，对这两个输出的分析表明，该模型仍然与扩展的对话片段作斗争。考虑摘录:</p><blockquote class="ms mt mu"><p id="e3c1" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated"><em class="it">“一年前，乔阿金·菲尼克斯出现在金球奖红毯上，他穿着燕尾服，头上套着一个纸袋，上面写着，“我是变形人”，这成为头条新闻。我不能改变世界。我只能改变自己。这是一个不改变以适应好莱坞模式的承诺:“我认为这是一件非常特别的事情，不改变自己。我觉得说“</em>真的很特别</p></blockquote><p id="957c" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">事实上，文本的真实性可以通过简单地固定每行周围的标点符号，并将它们连接在一起而得到显著提高。但是，这需要学习，而不是定义。<strong class="lc iu">平均而言，由最大的(1750亿)和最小的(1.25亿)GPT-3模型产生的检测物品的平均人类准确度为<em class="mv"> ca。</em>在200篇文章中分别为52%和76%。</strong></p><p id="a8b8" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">如前所述，GPT-3在其他各种NLP任务中表现出色，进行了少量射击、一次射击或零射击训练。这些任务的选择如下所示，以供参考:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ns"><img src="../Images/0b93b02cb1d590b938223d0e9f0c49bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZTPomaCTdxMcRgc8i008TQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-3上下文分析示例。</p></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nt"><img src="../Images/2348c9a0eaebea8f69966842bd8ff772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ax_B4lmGpc5XKraMYF48aA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-3机器翻译示例。</p></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nu"><img src="../Images/ce16c190cc9e8ad5914e68864971615b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-zLSfLOdqf1lZxtOmqnBQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-3算术计算示例。</p></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nv"><img src="../Images/acad4c96e1daf351d6c2fd2539eb5f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHlzqBzwo7yPJ7snQJllnA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">GPT-3字解扰例子。</p></figure><p id="628d" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">真正令人印象深刻的是该模型在这些新应用程序中显示的性能，因为它降低了商业应用程序(如聊天机器人或评论情感分析)的开发界限。</p><h1 id="97ba" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">真理的死亡？</h1><p id="e0ba" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，考虑到性能改善的趋势，是否有可能将模型的规模和复杂性进一步提高几个数量级，并超越随机机会愚弄人类？</p><p id="4fd6" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">也许吧。虽然条件文本生成在模型复杂性方面表现出了显著的改善，但研究人员观察到，GPT-3在分析句子之间的关系时表现不佳，并提出这里的问题与模型复杂性无关，而且方法本身可能是不正确的。他们的结论总结了这一点:</p><blockquote class="ms mt mu"><p id="ecd1" class="la lb mv lc b ld mn ju lf lg mo jx li mw mp ll lm mx mq lp lq my mr lt lu lv im bi translated"><em class="it">“对于自我监督的目标，任务规范依赖于将期望的任务强制转化为预测问题。然而最终，有用的语言系统(例如虚拟助手)可能更好地被认为是采取目标导向的行动，而不仅仅是做出预测。”</em></p></blockquote><p id="aefe" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">提到目标导向的行动是特别有趣的，因为它表明<a class="ae lw" href="https://www.google.com/search?q=reinforcement+learning+adrian+yijie+xu&amp;rlz=1C1EJFA_enSG791SG791&amp;oq=reinforcement+learning+adrian+yijie+xu&amp;aqs=chrome..69i57.6802j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">强化学习方法</a>将是一个更理想的解决方案。OpenAI此前曾探索过利用人类偏好来微调GPT-2模型，但这种方法对于现实世界的应用来说过于粗糙和劳动密集型。一个真正的强化学习模型需要为它的应用仔细定义一个奖励函数，也许通过使用前馈鉴别器模型，正如在分子设计的<a class="ae lw" href="http://OpenAI's GPT family of models" rel="noopener ugc nofollow" target="_blank">领域</a>中所做的那样。</p><p id="baee" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">那么，我们注定会有一个错误信息的未来吗？也许不是。训练这些模型是极其耗费资源的，使用单个英伟达RTX-8000 GPU单元训练最大的GPT-3模型需要相当于665年的时间。此外，HarvardNLP、<a class="ae lw" href="https://grover.allenai.org/" rel="noopener ugc nofollow" target="_blank"> AllenAI </a>和<a class="ae lw" href="https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/" rel="noopener ugc nofollow" target="_blank"> IBM Watson </a>等组织的研究人员已经利用GPT-2等生成语言模型的可用性来构建能够检测假冒输出的鉴别器。随着GPT-3和其他更复杂模型的发布，鉴别器必须类似地进化，<a class="ae lw" href="https://medium.com/gradientcrescent/ai-truth-and-society-deepfakes-at-the-front-of-the-technological-cold-war-86c3b5103ce6" rel="noopener">引发了对军备竞赛的恐惧，类似于对deepfakes </a>的观察。此外，由于鉴别器模型的训练将需要发生器的输出，总会有一个窗口，在该窗口期间检测解决方案将是不充分的。</p><p id="647e" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">但是“假新闻”的真正风险不在于语言模型的能力，而在于我们在信息素养和批判性思维方面的退化技能。无论是由于社交媒体还是其他原因，两极分化已经成为话语的一个重要组成部分，导致了从极端主义观点的在线回音室到最高层治理缺乏两党合作的症状。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nw"><img src="../Images/4b6f1ed9f46ad23d4aede73736b1867d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9ftvL7F9ftkoCcP9M5hhA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">随着时间的推移，美国民主党和共和党的意识形态观点发生了转变。(皮尤研究中心)</p></figure><p id="7b31" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated"><strong class="lc iu">除非我们能够有意识地减少两极分化，并采取一种以事实信息和尊重话语为导向的文化，否则我们将继续容易受到错误信息的影响，无论这些错误信息是由人工智能还是人类行为者产生的。</strong></p><p id="429b" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">这就结束了我们对开放GPT模型理论进展的回顾。在我们即将发表的文章中，我们将讨论如何实现GPT模式来制造一些“假新闻”和诗歌。</p><p id="0894" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">我们希望你喜欢这篇文章，并希望你查看GradientCrescent上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae lw" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库。</p><p id="011f" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated"><strong class="lc iu">参考文献</strong></p><div class="nx ny gp gr nz oa"><a href="https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">OpenAI巨大的GPT-3暗示了AI | ZDNet语言模型的局限性</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">加州研究机构OpenAI带着另一个巨大的深度学习模型GPT-3回来了。虽然它表明…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">www.zdnet.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://openai.com/blog/fine-tuning-gpt-2/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">根据人类偏好微调GPT-2</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">我们已经对774M参数GPT-2语言模型进行了微调，在各种任务中使用了人类反馈，成功地匹配了…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">openai.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://openai.com/blog/better-language-models/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">更好的语言模型及其含义</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">我们已经训练了一个大规模的无监督语言模型，它可以生成连贯的文本段落，实现…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">openai.com</p></div></div><div class="oj l"><div class="oq l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://openai.com/blog/language-unsupervised/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">用无监督学习提高语言理解</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">我们通过一个可扩展的、与任务无关的系统，在一系列不同的语言任务上获得了最先进的结果…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">openai.com</p></div></div><div class="oj l"><div class="or l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://arxiv.org/abs/2005.14165" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">语言模型是一次性学习者</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">最近的工作已经证明了在许多自然语言处理任务和基准上的巨大收益。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nx ny gp gr nz oa"><a href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">图解GPT-2(可视化变压器语言模型)</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">讨论:黑客新闻(64分，3条评论)，Reddit r/MachineLearning (219分，18条评论)翻译…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">jalammar.github.io</p></div></div><div class="oj l"><div class="os l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">图示的变压器</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">jalammar.github.io</p></div></div><div class="oj l"><div class="ot l ol om on oj oo mh oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://lambdalabs.com/blog/demystifying-gpt-3/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">揭秘GPT-3——深度学习语言模型的最新成果</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">上周四，OpenAI对语言建模的最新更新GPT-3出现在arxiv上。鉴于其影响力之大…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">lambdalabs.com</p></div></div><div class="oj l"><div class="ou l ol om on oj oo mh oa"/></div></div></a></div></div></div>    
</body>
</html>