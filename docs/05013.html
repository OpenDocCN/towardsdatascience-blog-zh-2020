<html>
<head>
<title>Make Support Vector Machine work exactly the way you want</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让支持向量机完全按照你想要的方式工作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/make-support-vector-machine-work-exactly-the-way-you-want-96a1ef69b6d6?source=collection_archive---------45-----------------------#2020-04-30">https://towardsdatascience.com/make-support-vector-machine-work-exactly-the-way-you-want-96a1ef69b6d6?source=collection_archive---------45-----------------------#2020-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c7e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">支持向量机的概念是如何工作的，以及如何用它来解决你的数据问题</h2></div><p id="5d26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在分类任务中，有几种方法可以做到这一点。可以通过用线性(直)线分隔类，或者用一棵树按照一定的阈值拆分属性直到达到期望的程度，或者计算事件属于哪个类的概率来解决。</p><p id="b222" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">支持向量机(Support Vector Machine)是一种<strong class="kh ir">非概率二元线性分类器</strong>，是一种通用的机器学习算法，可以<strong class="kh ir">执行分类和回归任务</strong>。SVM 的另一个优势是它能够<strong class="kh ir">解决线性和非线性数据集</strong>。</p><p id="96d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴于这些众多的好处，在 SVM 有许多概念和解决方案，我发现只有几篇文章/视频真正给出了容易理解的解释，特别是针对那些对 SVM 不熟悉的人。我希望这篇文章以最全面的方式到达你的手中。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="44db" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated"><strong class="ak">原始概念</strong></h1><p id="dd12" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">所有这一切都始于使用一条线(具有 2D 数据集)或一个超平面(超过 3D)将实例分成两类，并尝试<strong class="kh ir">最大化线和最近实例之间的距离</strong>。这个距离被表示为<strong class="kh ir">余量</strong>。下图说明了这一点。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/4a5f7c2610dce17e44934ccb9e479811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*5UydS1aZaxI1rBfxtCn8Bw.png"/></div></figure><p id="950d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么它需要最大化这个利润？原因是一个决策边界正好位于两个类之间比一个更靠近一个类要好得多。</p><p id="8f96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，想象有一个橙色类的<strong class="kh ir">异常值</strong>,它比它自己更接近蓝色类。如果我们严格地将上面的概念强加于这个数据集，就会产生下图。现在，差额满足要求，但结果比上面的差额小得多。这被称为<strong class="kh ir">硬边界</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/055fc4fb75e8209472d64269f162f07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*kvkgjz5oDn6JNCW0X4cS3A.png"/></div></figure><p id="cbdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果有一个新的蓝色类数据落在这个橙色实例附近，则该新数据将被误分类为橙色，换句话说，这意味着模型在新数据上的性能比在训练 1 上的性能差(我们的模型中从来不希望有训练 1)。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mf"><img src="../Images/ce205778cb9f48a44f0021ad1ca4e9fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*KqdOw-bWPTAntZp_7eWHig.png"/></div></div></figure><h1 id="a789" class="li lj iq bd lk ll mr ln lo lp ms lr ls jw mt jx lu jz mu ka lw kc mv kd ly lz bi translated"><strong class="ak">软余量</strong></h1><p id="04bc" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">有一种方法可以解决这个问题，即允许对训练集的异常值进行一些错误分类，以最大化其余训练数据的差值。这个概念被命名为<strong class="kh ir">软间隔</strong>或者换句话说，<strong class="kh ir">支持向量机。</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/710209578d3f5c89a4ed86c35b19277e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*BHHCFAv5VwxBQuc6GuZn0A.png"/></div></figure><p id="8ad9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦有新数据，就会被正确分类为蓝色。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/459776c31c505c22b4ee471de3a9304a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*TO43zN1-aQd7KebhDJaNIQ.png"/></div></figure><p id="f0e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">于是，一个问题出现了。我们如何决定软利润？(我们如何知道哪些实例在训练中会被错误分类？).</p><p id="4b23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其实这个没有完美的答案。你训练数据的几个值的差距决定使用最佳的一个为您的问题。超参数在 scikit-learn 的 VC 模型中控制这一点，表示为<em class="mw"> C. </em>如果模型过度拟合，减少<em class="mw"> C. </em></p><p id="7d92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也是因为 SVM 只使用一条直线或超平面来做分类工作，它是一个二元分类求解器。在多类问题的情况下，将采用一对所有(或一对其余)的策略。</p><blockquote class="mx my mz"><p id="c6a4" class="kf kg mw kh b ki kj jr kk kl km ju kn na kp kq kr nb kt ku kv nc kx ky kz la ij bi translated"><strong class="kh ir">因此，SVM 算法最重要的规则之一就是它试图在最大化边缘街道和限制边缘违规(误分类)之间找到一个好的平衡。</strong></p></blockquote><h1 id="a9a8" class="li lj iq bd lk ll mr ln lo lp ms lr ls jw mt jx lu jz mu ka lw kc mv kd ly lz bi translated">非线性数据集上的 SVM</h1><p id="7c6c" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">但是，对于非线性可分数据，怎么用这一招呢？请看下图，我们需要 3 行来将数据分成 2 个类，对于更复杂的数据，我们需要更多。这在计算上是低效的。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nd"><img src="../Images/d803253e9d64f884cc84b3afcdd711a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vFGR1xv8fjG5qhpfXHy2A.png"/></div></div></figure><p id="4bf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，<strong class="kh ir">内核绝招</strong>来了。<strong class="kh ir">内核技巧</strong>将添加其他特性，如多项式特性，然后 SVM 将利用超平面将数据分成两类，而不是在 2D 数据上教授模型。添加 2 次多项式特征后的上述数据将如下所示:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ne"><img src="../Images/1537bd51f8a77bb2c60117096a23116a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZG8bn1c2a6BZLxwIygbhw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">在添加了二次特征之后，实例现在被明显地分成两类</p></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="6534" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用数据来进一步理解这一点。</p><pre class="mg mh mi mj gt nj nk nl nm aw nn bi"><span id="83ac" class="no lj iq nk b gy np nq l nr ns">import random<br/>np.random.seed(42)<br/>m = 500<br/>X1 = 2 * np.random.rand(m, 1)<br/>X2 = (4 + 3 * X1**2 + np.random.randn(m, 1)).ravel()<br/>X12 = np.column_stack((X1,X2))<br/>y1 = np.zeros((500))<br/>X3 = np.random.rand(m, 1)<br/>X4 = (1 + X1**1 + 2*np.random.randn(m, 1)).ravel()<br/>X34 = np.column_stack((X3,X4))<br/>y2 = np.ones((500))<br/>X = np.concatenate((X12, X34), axis=0)<br/>y = np.concatenate((y1, y2), axis=0)</span><span id="c92c" class="no lj iq nk b gy nt nq l nr ns">def plot_dataset(X, y, axes):<br/>    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")<br/>    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")<br/>    plt.axis(axes)<br/>    plt.grid(True, which='both')<br/>    plt.xlabel("Feature 1", fontsize=20)<br/>    plt.ylabel("Feature 2", fontsize=20, rotation=0)<br/>plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])<br/>plt.show()</span></pre><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c896ec421fac33acba4a567467ed5d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*pqfVfIkc6bhQT-ctK2xbgg.png"/></div></figure><h2 id="31e7" class="no lj iq bd lk nv nw dn lo nx ny dp ls ko nz oa lu ks ob oc lw kw od oe ly of bi translated"><strong class="ak">多项式内核</strong></h2><p id="4dd1" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我在 scikit 中使用了<em class="mw"> SVC </em>类——学习 3 次多项式核，其中<em class="mw"> coef </em>超参数等于 1(它控制模型受高次和低次多项式影响的程度)。<em class="mw">linear SVC(loss = " hinge ")</em>具有先验的<em class="mw">多项式 features(degree = 3)</em>transformer 会做同样的把戏。</p><p id="1bfd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有一个非常大的数据集，继续使用<em class="mw"> LinearSVC </em>，因为它在处理大数据方面比<em class="mw"> SVC </em>更快。</p><blockquote class="mx my mz"><p id="53ce" class="kf kg mw kh b ki kj jr kk kl km ju kn na kp kq kr nb kt ku kv nc kx ky kz la ij bi translated">需要记住的一点是，在训练 SVM 之前，一定要缩放数据</p></blockquote><pre class="mg mh mi mj gt nj nk nl nm aw nn bi"><span id="8bfe" class="no lj iq nk b gy np nq l nr ns">poly_kernel_svm_clf = Pipeline([<br/>	("scaler", StandardScaler()),<br/>	("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=0.001))<br/>])<br/>poly_kernel_svm_clf.fit(X_train, y_train)<br/>poly_kernel_svm_clf10 = Pipeline([<br/>	("scaler", StandardScaler()),<br/>	("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=10))<br/>])<br/>poly_kernel_svm_clf10.fit(X_train, y_train)</span><span id="8151" class="no lj iq nk b gy nt nq l nr ns"># Plot the model overall prediction<br/>def plot_predictions(model, axes):<br/>    """<br/>    Vizualize the classification result of the model to see how it<br/>    corresponds to training data<br/>    """<br/>    x0s = np.linspace(axes[0], axes[1], 1000)<br/>    x1s = np.linspace(axes[2], axes[3], 1000)<br/>    x0, x1 = np.meshgrid(x0s, x1s)<br/>    X = np.c_[x0.ravel(), x1.ravel()]<br/>    y_pred = model.predict(X).reshape(x0.shape)<br/>    y_decision = model.decision_function(X).reshape(x0.shape)<br/>    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)<br/>    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)</span><span id="b11b" class="no lj iq nk b gy nt nq l nr ns">fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)<br/>plt.sca(axes[0])<br/>plot_predictions(poly_kernel_svm_clf, [-0.25,2.25,-5,20])<br/>plot_dataset(X_train, y_train)<br/>plt.title(r"$degree=3, C=0.001$", fontsize=18)</span><span id="5c69" class="no lj iq nk b gy nt nq l nr ns">plt.sca(axes[1])<br/>plot_predictions(poly_kernel_svm_clf10, [-0.25,2.25,-5,20])<br/>plot_dataset(X_train, y_train)<br/>plt.title(r"$degree=3, C=10$", fontsize=18)<br/>plt.show()</span></pre><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0f9bf462f5b794da9ca514204829cd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*VJqF0q2qSydgWO1r5_UIXQ.png"/></div></figure><p id="5694" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">C 值等于 10 的模型似乎很好地抓住了要点，让我们在测试集上测量它的性能。</p><pre class="mg mh mi mj gt nj nk nl nm aw nn bi"><span id="9e8f" class="no lj iq nk b gy np nq l nr ns">from sklearn.metrics import f1_score<br/>model_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]<br/>for model in model_list:<br/>    y_pred = model.predict(X_test)<br/>    print(f1_score(y_test, y_pred, average='weighted'))</span><span id="c140" class="no lj iq nk b gy nt nq l nr ns">0.6459770114942529<br/>0.8542027171311809</span></pre><h2 id="f0f2" class="no lj iq bd lk nv nw dn lo nx ny dp ls ko nz oa lu ks ob oc lw kw od oe ly of bi translated"><strong class="ak">高斯 RBF 核</strong></h2><p id="e181" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">现在，我想用这个数据尝试不同的核，我将使用<strong class="kh ir">高斯 RBF 核。这里是维基百科上对这个内核的解释<a class="ae oh" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Radial_basis_function_kernel</a></strong></p><p id="c99b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我的数据不是太大，<em class="mw">高斯 RBF 核</em>并不需要太多时间。但是，对于大型数据集，<em class="mw">高斯 RBF 核</em>会消耗你相当多的时间。</p><pre class="mg mh mi mj gt nj nk nl nm aw nn bi"><span id="18f6" class="no lj iq nk b gy np nq l nr ns">from sklearn.svm import SVC<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import train_test_split</span><span id="a711" class="no lj iq nk b gy nt nq l nr ns">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span><span id="11d4" class="no lj iq nk b gy nt nq l nr ns"># Create pipeline for training<br/>rbf_kernel_svm_clf = Pipeline([<br/>	("scaler", StandardScaler()),<br/>	("svm_clf", SVC(kernel="rbf", gamma=0.1, C=0.001))<br/>])<br/>rbf_kernel_svm_clf.fit(X_train, y_train)<br/>rbf_kernel_svm_clf10 = Pipeline([<br/>	("scaler", StandardScaler()),<br/>	("svm_clf", SVC(kernel="rbf", gamma=5, C=10))<br/>])<br/>rbf_kernel_svm_clf10.fit(X_train, y_train)</span><span id="3e89" class="no lj iq nk b gy nt nq l nr ns"># Plot the model overall prediction<br/>fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)<br/>plt.sca(axes[0])<br/>plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])<br/>plot_dataset(X_train, y_train)<br/>plt.title(r"$gamma=5, C=0.001$", fontsize=18)</span><span id="6fe3" class="no lj iq nk b gy nt nq l nr ns">plt.sca(axes[1])<br/>plot_predictions(rbf_kernel_svm_clf10, [-1.5, 2.5, -1, 1.5])<br/>plot_dataset(X_train, y_train)<br/>plt.title(r"$gamma=5, C=10$", fontsize=18)<br/>plt.show()</span></pre><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3f9671191b23319440d412ffaaaf6964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*x0-kl-YzMrkFYWxF-hNOhA.png"/></div></figure><p id="6225" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2 值的 C 似乎产生了一个类似的模型。让我们预测测试集，并用度量标准进行评估</p><pre class="mg mh mi mj gt nj nk nl nm aw nn bi"><span id="1e22" class="no lj iq nk b gy np nq l nr ns">from sklearn.metrics import f1_score<br/>model_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]<br/>for model in model_list:<br/>    y_pred = model.predict(X_test)<br/>    print(f1_score(y_test, y_pred, average='weighted'))</span><span id="a1e9" class="no lj iq nk b gy nt nq l nr ns">0.8417207792207791<br/>0.8544599213495534</span></pre><p id="847a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如所料，2 个模型的性能相当，C = 10 的值略高，也略高于上面的多项式核模型。</p><p id="324c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以通过调整超参数、交叉验证、添加另一种类型的特征转换来改善这一点(希望如此)。我们自己试试吧。</p></div></div>    
</body>
</html>