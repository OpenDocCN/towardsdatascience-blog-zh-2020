<html>
<head>
<title>PyTorch 3D in CVPR 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 3D 在 CVPR 2020</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-3d-in-cvpr-2020-ea7849dfd6e8?source=collection_archive---------46-----------------------#2020-06-17">https://towardsdatascience.com/pytorch-3d-in-cvpr-2020-ea7849dfd6e8?source=collection_archive---------46-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f4c3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">脸书如何利用 PyTorch 框架实现三维目的</h2></div><p id="32cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算机视觉和模式识别大会(<a class="ae lb" href="http://cvpr2020.thecvf.com/" rel="noopener ugc nofollow" target="_blank"> CVPR </a>)是计算机视觉和模式识别领域最新进展和趋势的顶级会议之一。它每年举行一次，由独立研究人员的高质量论文和大公司研发实验室的突破组成。由于今年新冠肺炎造成的疫情，组织者决定从 6 月 14 日至 19 日虚拟举办 CVPR 2020。尽管可能有所不同，但虚拟方面并没有在任何意义上降低会议上提交的论文和研究的绝对质量。</p><p id="9397" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在今年的 CVPR 会议上，脸书人工智能的研究人员成功地在计算机视觉的许多重要和相对较新的领域推进了<em class="lc">最先进</em>模型的边界，其中主要是推理常规 2D 图像中显示的 3D 对象的新方法。利用公平的开源机器学习框架<a class="ae lb" href="https://github.com/facebookresearch/pytorch3d" rel="noopener ugc nofollow" target="_blank"> PyTorch 3D </a>的众多功能，这项工作可以帮助解锁众多 AR/VR 增强功能，并成为在不久的将来塑造其他技术的关键。</p><h2 id="8f25" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">在复杂的真实世界场景中，从单一图像获得不同的视角</h2><p id="20ca" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">脸书人工智能研究中心(FAIR)的研究人员建造了 SynSin，这是一个最先进的模型，它从完全不同的角度拍摄一张 RGB 图像，并生成同一场景的新图像。所提出的系统通过使用 PyTorch 3D 中实现的新颖的可区分渲染器，将预测的 3D 点云投影到场景的另一个视图上来工作。基于绘制的点云输入，使用生成式对抗网络(GAN)来合成输出图像。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/35a5198a9eaf3d6658481b6161bad662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPCCQ9aZE1T2U5K2jrttbw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">由怀尔斯等人提供。<a class="ae lb" href="https://arxiv.org/pdf/1912.08804.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="979e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于 SynSin 依赖于渲染的 3D 点云作为其场景生成的基础，因此由于渲染的点云的灵活性，它能够生成不同分辨率的图像，与当代方法相比效率更高。</p><blockquote class="mr ms mt"><p id="1e58" class="kf kg lc kh b ki kj jr kk kl km ju kn mu kp kq kr mv kt ku kv mw kx ky kz la ij bi translated">我们可以生成高分辨率图像，并推广到其他输入分辨率。</p><p id="c172" class="kf kg lc kh b ki kj jr kk kl km ju kn mu kp kq kr mv kt ku kv mw kx ky kz la ij bi translated">怀尔斯等人— <a class="ae lb" href="https://arxiv.org/pdf/1912.08804.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><p id="5962" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3D 点云的投影特征也由作者提出的<em class="lc">细化网络</em>解码，以帮助填充缺失区域并生成更真实的最终输出图像。与最近经常使用密集体素网格的方法相比，本文作者的方法能够扩展到复杂现实场景中的合成场景生成。这是目前大多数方法都无法达到的相当高的精确度。</p><p id="defd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由 Wiles、Gkioxari、Szeliski 和 Johnson 撰写的完整论文可在<a class="ae lb" href="https://arxiv.org/pdf/1912.08804.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>查看，更深入地了解他们的可区分渲染器和 GAN 的细微差别。</p><h2 id="fec7" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">从单幅图像中以前所未有的细节和质量水平重建三维人体模型</h2><p id="adcf" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">脸书人工智能的研究人员基于 Saito 等人在 2019 年发布的<a class="ae lb" href="https://arxiv.org/pdf/1905.05172.pdf" rel="noopener ugc nofollow" target="_blank">像素对齐隐函数</a>(皮夫)方法，创建了一个多层神经网络，以开发一种从 2D 图像中生成人的 3D 重建的方法，这种方法能够捕捉最先进的复杂性和细节，并将其渲染为 3D 模型。高度具体的细节，如手指，面部特征和衣服褶皱，都是使用高分辨率照片作为网络的输入来捕捉的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mx"><img src="../Images/472753ab42c0025e6c07c8c3f6e6af7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_ibpW8wwD1oOV9sZ0fCJw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">斋藤等人提供。<a class="ae lb" href="https://arxiv.org/pdf/1905.05172.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ea6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据其功能，创建的网络可以大致分为两个主要部分。第一部分(或第一级网络)通过利用较低分辨率的图像来重建人类的 3D 结构。这部分类似于研究人员建立的皮夫方法。第二个网络在本质上更加轻量级，并利用更高分辨率的图像来捕捉和呈现人类更精细的方面。</p><blockquote class="mr ms mt"><p id="98b9" class="kf kg lc kh b ki kj jr kk kl km ju kn mu kp kq kr mv kt ku kv mw kx ky kz la ij bi translated">通过允许从第一级访问全局 3D 信息，我们的系统可以有效地利用局部和全局信息进行高分辨率 3D 人体重建。</p><p id="fd21" class="kf kg lc kh b ki kj jr kk kl km ju kn mu kp kq kr mv kt ku kv mw kx ky kz la ij bi translated">Saito 等人<a class="ae lb" href="https://arxiv.org/pdf/1905.05172.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><p id="4d2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">研究人员为这种精细的 3D 人类重建所采用的方法可能会被证明是 AR/VR 体验以及几个电子商务应用程序的一个重大推动。</p><p id="b184" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文全文由斋藤、黄、、森岛、金泽和在这里<a class="ae lb" href="https://arxiv.org/pdf/1905.05172.pdf" rel="noopener ugc nofollow" target="_blank">找到</a>。</p><h2 id="e765" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">融合图像中的 2D 投票和云中的 3D 投票</h2><p id="d592" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">脸书人工智能的研究人员发表了一篇更多地涉及理论方法而不是应用的论文 ImVoteNet，这是一种改进的 3D 对象检测架构，专门用于 RGB-D 场景。他们探索了来自 2D 图像的数据如何帮助基于投票的 3D 检测管道。一些最近的作品(如<a class="ae lb" href="https://arxiv.org/pdf/1904.09664.pdf" rel="noopener ugc nofollow" target="_blank"> VoteNet </a>)展示了仅利用点云的艺术表演状态。ImVoteNet 论文建立在 VoteNet 的体系结构上，并将点云提供的 3D 几何图形与图像的高分辨率和纹理相融合，以从 2D 图像中提取几何和语义特征。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi my"><img src="../Images/447ea1557c4e0a53e19e14606ded5513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlc5Db4QdPAgqTHI0NVc3w.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">由齐等人提供— <a class="ae lb" href="https://research.fb.com/wp-content/uploads/2020/04/ImVoteNet-Boosting-3D-Object-Detection-in-Point-Clouds-with-Image-Votes.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c54a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管点云在有效检测三维物体方面是有用的，但是它们的数据通常具有固有的局限性。由点云获得的数据通常是稀疏的，缺乏颜色信息，有时会受到<a class="ae lb" href="https://www.sciencedirect.com/topics/engineering/sensor-noise" rel="noopener ugc nofollow" target="_blank">传感器噪声</a>的影响。通过使用多塔训练方案融合 2D 图像和 3D 点云的特征，帮助研究人员从两种源图像中提取最佳效果。研究人员成功地从 2D 图像中提取了几何和语义特征，并利用相机参数将其提升到 3D。所建立的系统依靠一种旋转机制来有效地聚集点云中的几何信息。ImVoteNet 负责在点云本质上稀疏或分布不利的设置中，使用具有梯度混合的多模态训练来显著提高 3D 对象检测。</p><p id="4758" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">齐、陈、利塔尼和吉巴斯的论文全文可以在<a class="ae lb" href="https://research.fb.com/wp-content/uploads/2020/04/ImVoteNet-Boosting-3D-Object-Detection-in-Point-Clouds-with-Image-Votes.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="d3d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管这些论文很吸引人，但 CVPR 拥有的远不止上面展示的。竞赛、专家讲座和<a class="ae lb" href="http://cvpr2020.thecvf.com/workshops-schedule" rel="noopener ugc nofollow" target="_blank">研讨会</a>是一个人需要留意的。从<a class="ae lb" href="https://today.duke.edu/2020/06/artificial-intelligence-makes-blurry-faces-look-more-60-times-sharper" rel="noopener ugc nofollow" target="_blank"> <em class="lc">去像素化</em>图像</a>到<a class="ae lb" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">潜在自动编码器</a>，每年都会展示一些最具创新性和最令人兴奋的论文。更多关于 CVPR 的信息，接受的论文，比赛和演讲可以在它的<a class="ae lb" href="http://cvpr2020.thecvf.com/" rel="noopener ugc nofollow" target="_blank">主页</a>上找到。</p></div></div>    
</body>
</html>