<html>
<head>
<title>Data Science questions for interview prep (Machine Learning Concepts) — Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面试准备的数据科学问题(机器学习概念)——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-questions-for-interview-prep-machine-learning-concepts-part-ii-b7b58e74a1b7?source=collection_archive---------25-----------------------#2020-06-23">https://towardsdatascience.com/data-science-questions-for-interview-prep-machine-learning-concepts-part-ii-b7b58e74a1b7?source=collection_archive---------25-----------------------#2020-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0f8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这一部分包括主成分分析，聚类方法和基于树的方法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/de7b203697d53e642ecef1c2857791f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jQf2gCgWKqP_oHxVN5kvg.jpeg"/></div></div></figure><p id="b961" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在前一部分中，我们介绍了模型评估技术、正则化方法、逻辑回归和偏差方差权衡。在这一部分，我们将涉及一些最常见的关于最大似然算法的面试问题。</p><p id="103e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意:这些不是真正的面试问题，但很接近面试中问的问题。它们确实涵盖了面试者可能热衷于测试的几个方面/概念。请随意评论更多的背景，我一定会努力。</p><h1 id="d3db" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">1.PCA 是做什么的？第一主成分轴是如何选取的？</h1><p id="636a" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">PCA 代表主成分分析。这是一种降维技术，它将一大组相关变量(基本上是高维数据)总结成数量较少的代表性变量，称为主成分，它解释了原始数据集中的大多数可变性。</p><p id="f6e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">选择第一主成分轴的方式使得它解释了数据中的大部分变化，并且最接近所有 n 个观察值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0c3e4bb187f8da039f3e21a27bac6fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/0*n4umd_Ju0oiys0cX.gif"/></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae ms" href="https://www.statistixl.com/features/principal-components/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="eb5a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">2.主成分分析中的主成分代表什么？</h1><p id="95fc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">它代表数据变化最大的一条线或一个轴，也是最接近所有 n 个观察值的线。运筹学</p><p id="0cee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">观察变量的线性组合形成一个轴或一组轴，解释了数据集中的大部分可变性。</p><p id="ca9d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从数学上来说，就是第一主成分的特征向量。距离的平方和是 PC1 的特征值，特征值的平方根是 PC1 的奇异值</p><h1 id="f13d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">3.主成分的系数告诉你什么？</h1><p id="7d4d" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如果我们把所有的点都投影到主成分上，它们会告诉我们变量 2 的重要性是变量 1 的 N 倍</p><h1 id="5975" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">4.可以用 PCA 进行回归吗？什么时候使用主成分分析进行回归是可取的？</h1><p id="198e" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">是的，我们可以使用主成分进行回归设置。当前几个主成分足以捕捉预测因子<strong class="kw iu">的大部分变化以及与响应的关系时，PCA 将表现良好。</strong>这种方法的唯一缺点是，在应用主成分分析时，新的简化特征集将忽略响应变量 Y 进行建模，虽然这些特征可以很好地解释 X 的变化，但如果这些变量不能解释 Y 的变化，模型的性能将会很差。</p><h1 id="2691" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">5.我们可以用 PCA 进行特征选择吗？</h1><p id="21b5" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">不要！PCA 不是一种特征选择技术，因为如果你想一想，任何主成分轴都是所有原始特征变量集的线性组合，它定义了一组新的轴，解释了数据中的大多数可变性。因此，虽然它在许多实际设置中表现良好，但它不会导致依赖于一小组原始功能的模型的开发。</p><h1 id="3f06" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">6.五氯苯甲醚有哪些注意事项？</h1><p id="71bf" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">确保数据在相同的比例上</p><p id="ae55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">确保数据居中</p><p id="991c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">主成分的最大数量等于变量的数量。数据集中的每个变量都有一个主成分或 PC。然而，如果样本数少于变量数，那么样本数就为特征值大于 0 的 PC 数设定了一个上限</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="644f" class="lq lr it bd ls lt na lv lw lx nb lz ma jz nc ka mc kc nd kd me kf ne kg mg mh bi translated">7.什么是层次聚类？</h1><p id="6ae8" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">分层聚类顾名思义是一种无监督的机器学习算法，用于聚类。最常见的聚类技术称为自底向上聚类。它生成一个树状图，从树叶开始，根据某种相似性度量，如欧几里德距离或相关性，将它们组合成树干上的簇。与靠近树顶部融合的实例相比，在树状图底部融合/组合的实例彼此非常相似。</p><h1 id="6653" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">8.什么是 K-means 聚类？</h1><p id="33a2" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">该算法从选择 K 值开始，K 值决定了该算法形成的聚类数。然后，随机地将一个样本分配给每个聚类。此外，基于平方欧几里德距离将剩余的观察值分配给 K 个聚类之一，即，根据欧几里德距离最接近的观察值被聚类在一起。一旦所有的观察值被分配给 K 个聚类中的每一个，每个聚类的类内方差被计算和求和。这个过程被重复，通过开始一组新的观察，直到我们找到一个组内变化最小的组。</p><h1 id="4d59" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">9.K-means 和层次聚类有什么区别？</h1><p id="7995" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">k-均值聚类试图将数据聚类成用户定义数量的聚类。因此，根据 k 的选择，最终输出可能是也可能不是所期望的。而分级聚类生成自底向上的树或树状图，即 n 个聚类，其中 n 等于样本大小，这允许用户试验聚类的数量，并选择给出更好的解释/分离的数量。</p><h1 id="6fde" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">10.解释一下 K 近邻算法？</h1><p id="6286" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">K-nearest 是一种算法，它可以通过基于距离度量(如欧几里德距离)将新实例与 K 个最近邻居进行比较，并将新实例分配给 K 个中具有最大最近邻居的聚类，从而将未知实例分配给预定义的聚类。在新实例不确定的情况下，可以通过选择 K 的奇数值来打破平局。K 的低值可能会受到离群值的影响，从而导致不正确的预测。 但是如果 K 非常大，那么具有几个样本/实例但是非常接近未知实例的聚类有可能被其他类别投票否决。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="a309" class="lq lr it bd ls lt na lv lw lx nb lz ma jz nc ka mc kc nd kd me kf ne kg mg mh bi translated">11.解释分类问题的决策树？使用决策树有哪些缺点？</h1><p id="bd5b" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">分类问题的决策树从从特征空间中选择一个变量作为根节点开始。基于像基尼指数或熵这样的杂质测量来选择该节点，基尼指数或熵测量由该节点产生的分离。更清楚地分隔数据的变量被选作根节点。我们进一步重复这个过程，直到我们到达一个点，在这个点上分裂一个叶节点不能进一步改善分离。现在，当变量是分类变量时，可以使用基尼指数，但如果变量是数字，我们可以取连续数字之间的平均值，并选择给出更好分离的平均值。</p><h1 id="794a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">12.比较 adaboost 和随机森林。两者的三个主要区别是什么？</h1><p id="945c" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">Adaboost 构建树桩或弱学习者的森林，而随机森林构建完整大小的决策树。</p><p id="a1f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 AdaBoost 中，每个新树都是以一种考虑到以前的树所犯的错误或错误分类的方式来构建的，而在随机森林中，每个树都是独立构建的。</p><p id="421f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 AdaBoost 中，每棵树在最终预测中获得的权重是其产生的错误分类的函数，即，如果一棵树产生的错误分类较少，则与产生更多错误的树相比，它在最终预测中获得更大的发言权，而在随机森林中，每棵树获得相同的投票。</p><h1 id="9274" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">13.解释 Adaboost 模型的工作原理</h1><p id="8e96" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">Adaboost 从建立一个树桩或弱学习者的森林开始。只有一个节点或一个预测器和两片叶子的树称为树桩。adaboost 构建的每个树桩都有不同的权重，权重由它产生的错误数决定。森林中生成的每棵树在最终预测中都有发言权，但发言权的大小取决于每棵树获得的权重。</p><h1 id="8c9d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">14.解释回归的梯度推进</h1><p id="54cc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">梯度增强与 adaboost 相似，我们构建一组从其前辈学习的树。但是，梯度增强从 y 值的平均值叶节点开始，然后使用叶节点的值计算数据中每一行的残差平方和。在下一步中，它建立一个全尺寸的树，该树考虑了由先前的树/叶产生的误差，并试图预测残差。我们预测残差，因为最终输出是每个梯度提升树生成的预测的总和。我们可以限制这棵树的叶子数量。此外，像 adaboost 一样，梯度增强会缩放所有的树，但与 adaboost 不同，它会均等地缩放所有的树。我们继续这个过程，直到树不断提高模型的准确性，或者我们达到一个停止标准，比如要建立的树的数量。通过将学习率加权的叶节点的值相加来进行预测。我们需要学习率，这样我们就不会过度拟合我们的训练数据，所以学习率的目的是增加一些偏差。学习率通常取 0 到 1 之间的值，选择一个小的学习率是个好主意，这样我们就不会超调。这也意味着我们正在朝着正确的方向迈出更小的步伐。</p><h1 id="7425" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">15.解释用于分类的梯度增强</h1><p id="99fe" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">用于分类的梯度增强通过取对数(odds)来获得初始叶节点的值。残差是通过取观察值和预测值/概率之间的差来计算的。观察值取值(0/1)用于分类，而预测值只是每个类别的概率。剩下的过程与回归过程相似。</p><h1 id="b478" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">16.什么是梯度下降？</h1><p id="406c" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">梯度下降是一种用于优化机器学习模型参数的技术。它首先假设参数值并计算每个参数值的损失函数，直到找到使损失函数值最小的参数值。它好的原因是因为，梯度下降不是尝试一堆参数值，而是在远离最小值时以更长的步长开始，在接近最小值时以小步开始。步长由学习速率决定。在简单线性回归的情况下，我们想要估计参数 b0，b1，损失函数是残差平方和。</p><h1 id="3def" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">17.什么是随机梯度下降？</h1><p id="216e" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如果样本量很大，普通梯度下降法可能需要更长的时间来计算参数的最佳值。这就是随机梯度下降的由来。它从数据集或小批量数据中随机选择一个小样本，并在每次迭代参数估计时在这个小样本集上拟合模型。简而言之，如果 n 和要估计的参数的数量很大，梯度下降在计算上可能是昂贵的。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="c22e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请务必阅读本系列的第一部分。另外，如果你有任何问题或建议，请随时留言。</p><p id="4c4c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae ms" rel="noopener" target="_blank" href="/33-data-science-questions-for-interview-prep-machine-learning-concepts-6b1718397431">面试准备的数据科学问题(机器学习概念)—第一部分</a></p><h2 id="4fe6" class="nf lr it bd ls ng nh dn lw ni nj dp ma ld nk nl mc lh nm nn me ll no np mg nq bi translated">参考:</h2><p id="99a2" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated"><a class="ae ms" href="https://www.youtube.com/watch?v=Gv9_4yMHFhI&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=1" rel="noopener ugc nofollow" target="_blank">乔希·斯塔默的 stat quest</a></p></div></div>    
</body>
</html>