<html>
<head>
<title>Text Data Analysis and Visualization of Reuters Articles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">路透社文章的文本数据分析和可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analysis-and-visualization-of-unstructured-text-data-2de07d9adc84?source=collection_archive---------26-----------------------#2020-09-09">https://towardsdatascience.com/analysis-and-visualization-of-unstructured-text-data-2de07d9adc84?source=collection_archive---------26-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="695e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">借助 K-Means、N-gram、Tf-IDF、条形图、单词云、NER 等工具研究路透社的文章。方法</h2></div><p id="db0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">被困在付费墙后面？点击这里阅读这篇文章和我的朋友链接。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/e74c1e6755ba413ecb21bc3288dc0b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtyEQJmFS3jfDfK9rnduIw.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">资料来源:Jaredd Craig-Unsplash</p></figure><p id="1425" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我要求你解释文本数据时，你会怎么做？你将采取什么步骤来构建文本可视化的故事？在这里，我不打算解释你如何创建一个可视化的故事。</p><p id="67eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是本文将帮助您获得构建可视化故事和解释文本数据所需的信息。</p><p id="6fb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从文本数据中获得的洞察力将帮助我们发现文章之间的联系。它将检测趋势和模式。对文本数据的分析将把噪音放在一边，并揭示以前未知的信息。</p><p id="7f63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个分析过程也被称为探索性文本分析(ETA)。借助 K-means、Tf-IDF、词频等。方法，我们将分析这些文本数据。此外，ETA 在数据清理过程中也很有用。</p><p id="ee3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还使用 Matplotlib、seaborn 和 Plotly 库将结果可视化为图表、单词云和绘图。</p><p id="e6a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分析文本数据之前，请完成这些预处理任务。</p><h1 id="def7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">从数据源检索数据</h1><p id="106f" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">有大量的非结构化文本数据可供分析。您可以从以下来源获取数据。</p><p id="c368" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.来自 Kaggle 的 Twitter 文本数据集。</p><p id="31ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.使用 API 的 Reddit 和 twitter 数据集。</p><p id="7ace" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.使用 Beautifulsoup 从网站上抓取文章并请求 python 库。</p><p id="c592" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将使用 SGML 格式的<a class="ae le" href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">路透社的文章</strong> </a>。出于分析目的，我将使用 Beautifulsoup 库从数据文件中获取日期、标题和文章正文。</p><p id="4be6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用下面的代码从所有数据文件中获取数据，并将输出存储在一个 CSV 文件中。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="c8bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mu"> 1。您还可以使用 Regex 和 OS 库来组合或循环所有数据文件。</em></p><p id="2170" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mu"> 2。每篇文章的正文以&lt;路透&gt;开头，所以使用 find_all('reuters ')。</em></p><p id="99d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mu"> 3。您还可以使用 pickle 模块来保存数据，而不是 CSV。</em></p><h1 id="03a0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据清理过程</h1><p id="b45d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在这一节中，我们将删除诸如空值、标点符号、数字等干扰。从文本数据中。首先，我们删除文本列中包含空值的行。然后我们处理其他列的空值。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="09d4" class="na lw it mw b gy nb nc l nd ne">import pandas as pd import re</span><span id="ecf2" class="na lw it mw b gy nf nc l nd ne">articles_data = pd.read_csv(‘articles_data.csv’) print(articles_data.apply(lambda x: sum(x.isnull()))) articles_nonNull = articles_data.dropna(subset=[‘text’]) articles_nonNull.reset_index(inplace=True)</span><span id="16b5" class="na lw it mw b gy nf nc l nd ne">def clean_text(text):</span><span id="fd2e" class="na lw it mw b gy nf nc l nd ne">‘’’Make text lowercase, remove text in square brackets,remove \n,remove punctuation and remove words containing numbers.’’’</span><span id="e848" class="na lw it mw b gy nf nc l nd ne">    text = str(text).lower()<br/>    text = re.sub(‘&lt;.*?&gt;+’, ‘’, text)<br/>    text = re.sub(‘[%s]’ % re.escape(string.punctuation), ‘’, text)<br/>    text = re.sub(‘\n’, ‘’, text)<br/>    text = re.sub(‘\w*\d\w*’, ‘’, text)<br/>    return text</span><span id="49aa" class="na lw it mw b gy nf nc l nd ne">articles_nonNull[‘text_clean’]=articles_nonNull[‘text’]\<br/>                                  .apply(lambda x:clean_text(x))</span></pre><p id="f700" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> </strong> <em class="mu">当我们删除文本列中存在的空值时，那么其他列中的空值也会消失。</em></p><p id="a2b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mu"> </em> </strong> <em class="mu">我们已经使用 re 方法去除文本数据中的噪声。</em></p><p id="1329" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据清理过程中采取的步骤可能会根据文本数据增加或减少。因此，请仔细研究您的文本数据，并相应地构建您的 clean_text()方法。</p><p id="83f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着预处理任务的完成，我们将继续分析文本数据。</p><h2 id="2bd5" class="na lw it bd lx ng nh dn mb ni nj dp mf kr nk nl mh kv nm nn mj kz no np ml nq bi translated">先说我们的分析。</h2><h1 id="a47f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.路透社文章的长度</h1><p id="9134" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我们知道所有文章的长度是不一样的。因此，我们将考虑那些长度等于或超过一个段落的文章。根据这项研究，一个句子的平均长度是 15-20 个单词。一个段落中应该有四个句子。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="537e" class="na lw it mw b gy nb nc l nd ne">articles_nonNull[‘word_length’] = articles_nonNull[‘text’].apply(lambda x: len(str(x).split())) print(articles_nonNull.describe())</span><span id="248e" class="na lw it mw b gy nf nc l nd ne">articles_word_limit = articles_nonNull[articles_nonNull[‘word_length’] &gt; 60]</span><span id="b26a" class="na lw it mw b gy nf nc l nd ne">plt.figure(figsize=(12,6)) p1=sns.kdeplot(articles_word_limit[‘word_length’], shade=True, color=”r”).set_title(‘Kernel Distribution of Number Of words’)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nr"><img src="../Images/e98ef1c1b76cf25b29ef0740f544e21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3FK0h-rzyZj8oydhp2r4w.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><p id="a9ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经删除了那些长度小于 60 字的文章。</p><p id="2a3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mu">字长分布是右偏的。</em></p><p id="23c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mu">大部分文章都在 150 字左右。</em></p><p id="c538" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">包含事实或股票信息的路透社文章字数较少。</p><h1 id="30c4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.路透社文章中的常用词</h1><p id="61e2" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在这一部分，我们统计文章中出现的单词并分析结果。我们基于 N-gram 方法分析单词计数。N-gram 是基于其 N 值的单词的出现。</p><p id="2d80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从文本数据中删除停用词。因为停用词是噪音，在分析中没有多大用处。</p><h2 id="708a" class="na lw it bd lx ng nh dn mb ni nj dp mf kr nk nl mh kv nm nn mj kz no np ml nq bi translated">1.最常见的单字单词(N=1)</h2><p id="fa2c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">让我们在条形图和单词云中绘制单词。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ns"><img src="../Images/1780198c5091bf5ce6859d6088f462ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wagbm7hSeDdZwdHV2zS0PQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nt"><img src="../Images/0f6d1a0fc2ce8a3906a354e716f50ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mZNstJ5eyDs5tmPQYs1n6Q.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><p id="cf6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">股票，贸易和股票是一些最常见的词，基于股票市场和金融部门的文章。</p><p id="3c3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们可以说，大多数路透社的文章属于金融和股票类。</p><h2 id="c0c2" class="na lw it bd lx ng nh dn mb ni nj dp mf kr nk nl mh kv nm nn mj kz no np ml nq bi translated">2.最常见的二元词(N=2)</h2><p id="85be" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">让我们为 Bigram 单词绘制条形图和单词云。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="0883" class="na lw it mw b gy nb nc l nd ne">article_bigrams = defaultdict(int)<br/>for tweet in articles_word_limit[‘temp_list_stopw’]:<br/>    for word in generate_ngrams(tweet, n_gram=2):<br/>        article_bigrams[word] += 1</span><span id="2eab" class="na lw it mw b gy nf nc l nd ne">df_article_bigrams=pd.DataFrame(sorted(article_bigrams.items(),<br/>                                key=lambda x: x[1])[::-1])</span><span id="d7c8" class="na lw it mw b gy nf nc l nd ne">N=50</span><span id="4abc" class="na lw it mw b gy nf nc l nd ne"># bar graph of top 50 bigram words<br/>fig, axes = plt.subplots(figsize=(18, 50), dpi=100)<br/>plt.tight_layout()<br/>sns.barplot(y=df_article_bigrams[0].values[:N],<br/>            x=df_article_bigrams[1].values[:N], <br/>            color=’red’)<br/>axes.spines[‘right’].set_visible(False)<br/>axes.set_xlabel(‘’)<br/>axes.set_ylabel(‘’)<br/>axes.tick_params(axis=’x’, labelsize=13)<br/>axes.tick_params(axis=’y’, labelsize=13)<br/>axes.set_title(f’Top {N} most common Bigrams in Reuters Articles’,<br/>               fontsize=15)<br/>plt.show()</span><span id="9831" class="na lw it mw b gy nf nc l nd ne">#Word cloud<br/>wc = WordCloud(width=2000, height=1000, collocations=False,<br/>               background_color=”white”,<br/>               color_func=col_func,<br/>               max_words=200,<br/>               random_state=np.random.randint(1,8))\<br/>               .generate_from_frequencies(article_bigrams)</span><span id="ff84" class="na lw it mw b gy nf nc l nd ne">fig, ax = plt.subplots(figsize=(20,10))<br/>ax.imshow(wc, interpolation=’bilinear’)<br/>ax.axis(“off”)<br/>ax.set_title(‘Trigram Words of Reuters Articles’, pad=24,<br/>             fontdict=fd)<br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nu"><img src="../Images/163cb2b887e08f21d3269cb91e61cff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nzOiTxLWD6wi5DesLyqb4w.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nv"><img src="../Images/f7448b25f3704d3c6b895ebdca6477a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSGUj8M4-33XqNjwurIdgA.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><p id="d8d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Bigram 比 unigram 提供了更多的文本信息和上下文。比如，股票损失 bigram 显示大多数人在股票上赔钱。</p><h2 id="cb94" class="na lw it bd lx ng nh dn mb ni nj dp mf kr nk nl mh kv nm nn mj kz no np ml nq bi translated">3.最常见的三元组单词</h2><p id="6e9d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">让我们为 Trigrma 单词绘制条形图和单词云。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="6739" class="na lw it mw b gy nb nc l nd ne">article_trigrams = defaultdict(int)<br/>for tweet in articles_word_limit[‘temp_list_stopw’]:<br/>    for word in generate_ngrams(tweet, n_gram=3):<br/>        article_trigrams[word] += 1<br/>df_article_trigrams = pd.DataFrame(sorted(article_trigrams.items(),<br/>                                   key=lambda x: x[1])[::-1])</span><span id="c03b" class="na lw it mw b gy nf nc l nd ne">N=50</span><span id="4497" class="na lw it mw b gy nf nc l nd ne"># bar graph of top 50 trigram words<br/>fig, axes = plt.subplots(figsize=(18, 50), dpi=100)<br/>plt.tight_layout()<br/>sns.barplot(y=df_article_trigrams[0].values[:N],<br/>            x=df_article_trigrams[1].values[:N], <br/>            color=’red’)<br/>axes.spines[‘right’].set_visible(False)<br/>axes.set_xlabel(‘’)<br/>axes.set_ylabel(‘’)<br/>axes.tick_params(axis=’x’, labelsize=13)<br/>axes.tick_params(axis=’y’, labelsize=13)<br/>axes.set_title(f’Top {N} most common Trigrams in Reuters articles’,<br/>               fontsize=15)<br/>plt.show()</span><span id="dd9c" class="na lw it mw b gy nf nc l nd ne"># word cloud<br/>wc = WordCloud(width=2000, height=1000, collocations=False,<br/>background_color=”white”,<br/>color_func=col_func,<br/>max_words=200,<br/>random_state=np.random.randint(1,8)).generate_from_frequencies(article_trigrams)<br/>fig, ax = plt.subplots(figsize=(20,10))<br/>ax.imshow(wc, interpolation=’bilinear’)<br/>ax.axis(“off”)<br/>ax.set_title(‘Trigrams Words of Reuters Articles’, pad=24,<br/>             fontdict=fd)<br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nw"><img src="../Images/0fa5b2dd251dfec8133f83cb4a2ebdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QrxXydaagmRzSe5dFn5VRQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nt"><img src="../Images/2fc74f063b66c6554e19bd2857de9723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ2uZHAMFY0xgr2Bz4ZK7A.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><p id="302e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数三元模型类似于二元模型。三元组单词比二元组单词更能帮助我们理解文本。但是不能提供更多的信息。所以这一节到此结束。</p><h1 id="9957" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.文本数据的命名实体识别(NER)标记</h1><p id="d983" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">NER 是从文本数据中提取特定信息的过程。在 NER 的帮助下，我们从文本中提取位置、人名、日期、数量和组织实体。点击了解更多关于<a class="ae le" rel="noopener" target="_blank" href="/automate-entity-extraction-of-reddit-subgroup-using-bert-model-336f9edb176e"> NER 的信息。我们使用</a><a class="ae le" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> Spacy </a> python 库来完成这项工作。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nx"><img src="../Images/3e4409fa44f4159633b513e3a204c7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SvWrvpo6qIOIUKRkPVxwwg.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><p id="2d3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这个图表中，你可以说大多数文章包含了来自美国、日本、加拿大、伦敦和中国的新闻。</p><p id="6aa3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mu"> </em> </strong> <em class="mu">对美国的高度提及代表了路透社在美国业务的重点。</em></p><p id="f997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mu"> </em> </strong> <em class="mu">人物变量暗示 1987 年的著名人物是谁。这些信息有助于我们了解那些人。</em></p><p id="3c9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mu"> </em> </strong> <em class="mu">这个组织变量包含了全世界最多提及的组织。</em></p><h1 id="0289" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">4.文本数据中的唯一单词</h1><p id="db03" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我们将使用 TF-IDF 在文章中找到独特的单词。词频(TF)是每篇文章的字数。逆文档频率(IDF)在考虑所有提及的文章时测量单词的重要性。</p><p id="472d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于那些在一篇文章中具有高计数而在其他文章中很少或不存在的词，TF-IDF 得分很高。</p><p id="034b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们计算一下 TF-IDF 得分，找出独特的单词。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="b508" class="na lw it mw b gy nb nc l nd ne">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="95f1" class="na lw it mw b gy nf nc l nd ne">tfidf_vectorizer = TfidfVectorizer(use_idf=True)<br/>tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(articles_word_limit[‘text_clean’])<br/>tfidf = tfidf_vectorizer_vectors.todense()<br/>tfidf[tfidf == 0] = np.nan</span><span id="13c5" class="na lw it mw b gy nf nc l nd ne">#Use nanmean of numpy which will ignore nan while calculating mean<br/>means = np.nanmean(tfidf, axis=0)</span><span id="ce58" class="na lw it mw b gy nf nc l nd ne"># convert it into a dictionary for later lookup<br/>Means_words = dict(zip(tfidf_vectorizer.get_feature_names(),<br/>                       means.tolist()[0]))<br/>unique_words=sorted(means_words.items(),<br/>                    key=lambda x: x[1],<br/>                    reverse=True)<br/>print(unique_words)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/2de5b610a9b5914d82328fb5b4b95e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*670YPSo4cGu4HLgso0PniA.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">来源:作者图片</p></figure><h1 id="d9d1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">5.使用 K-Means 聚类文章</h1><p id="d481" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">K-Means 是一种无监督的机器学习算法。它帮助我们在一个组中收集相同类型的文章。我们可以通过初始化 k 值来决定组或簇的数量。了解更多关于<a class="ae le" href="https://pythonprogramminglanguage.com/kmeans-text-clustering/" rel="noopener ugc nofollow" target="_blank"> K-Means 和我们如何在这里选择 K 值</a>。作为参考，我选择 k=4 组成四组。</p><pre class="lg lh li lj gt mv mw mx my aw mz bi"><span id="7a79" class="na lw it mw b gy nb nc l nd ne">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import adjusted_rand_score</span><span id="da66" class="na lw it mw b gy nf nc l nd ne">vectorizer = TfidfVectorizer(stop_words=’english’,use_idf=True)<br/>X = vectorizer.fit_transform(articles_word_limit[‘text_clean’])<br/>k = 4<br/>model = KMeans(n_clusters=k, init=’k-means++’,<br/>               max_iter=100, n_init=1)<br/>model.fit(X)<br/>order_centroids = model.cluster_centers_.argsort()[:, ::-1]<br/>terms = vectorizer.get_feature_names()<br/>clusters = model.labels_.tolist()<br/>articles_word_limit.index = clusters<br/>for i in range(k):<br/>    print(“Cluster %d words:” % i, end=’’)</span><span id="ebd7" class="na lw it mw b gy nf nc l nd ne">for title in articles_word_limit.ix[i<br/>                    [[‘text_clean’,’index’]].values.tolist():<br/>    print(‘ %s,’ % title, end=’’)</span></pre><p id="0351" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它帮助我们将文章分类到不同的类别，如体育、货币、金融等等。K-Means 的准确率普遍较低。因此，这在低级分析中是有用的。</p><h1 id="930d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="8ffc" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">NER 和 K 均值是我最喜欢的分析方法。其他人可能喜欢 N-gram 和独特的单词方法。在本文中，我介绍了最著名的、闻所未闻的文本可视化和分析方法。本文中的所有这些方法都是独一无二的，可以帮助您进行可视化和分析。</p><p id="627d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇文章能帮助你发现文本数据中的未知。</p><p id="a837" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">作者的其他文章</strong></p><ol class=""><li id="9cc2" class="nz oa it kk b kl km ko kp kr ob kv oc kz od ld oe of og oh bi translated"><a class="ae le" href="https://medium.com/analytics-vidhya/first-step-in-eda-descriptive-statistics-analysis-f49ca309da15" rel="noopener">EDA 的第一步:描述性统计分析</a></li><li id="8fdb" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated"><a class="ae le" rel="noopener" target="_blank" href="/automate-sentiment-analysis-process-for-reddit-post-textblob-and-vader-8a79c269522f">为 Reddit Post: TextBlob 和 VADER 自动化情感分析流程</a></li><li id="c672" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated"><a class="ae le" rel="noopener" target="_blank" href="/discover-the-sentiment-of-reddit-subgroup-using-roberta-model-10ab9a8271b8">使用罗伯塔模型发现 Reddit 子群的情绪</a></li></ol></div></div>    
</body>
</html>