<html>
<head>
<title>Simple Example of Predicting with Confidence Estimates</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用置信估计进行预测的简单示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-example-of-predicting-with-confidence-estimates-cfc9e8a35c9b?source=collection_archive---------29-----------------------#2020-06-28">https://towardsdatascience.com/simple-example-of-predicting-with-confidence-estimates-cfc9e8a35c9b?source=collection_archive---------29-----------------------#2020-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/556f07dff5a9fd8eebc7f12905ac1b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VSHMG1199mII3vPd"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">西蒙·艾布拉姆斯在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e941" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在数据科学中，制作一个输出数值的模型——也称为回归——是一个古老的故事。但有时我们不仅对预测值感兴趣，还对该值周围的<em class="le">不确定性</em>感兴趣，这被称为<a class="ae kf" href="https://en.wikipedia.org/wiki/Probabilistic_forecasting" rel="noopener ugc nofollow" target="_blank">概率预测</a>。本文将带您浏览一个示意性示例，在该示例中，我们让神经网络预测一个值以及一个置信度估计值。我们将使用 Python 和 tf.keras，以及像正态分布这样的基本概率结果。此处提供有代码的笔记本<a class="ae kf" href="https://gitlab.com/Emmanuel_B/whats-up-timeseries/-/blob/master/notebooks/predict-confidence.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="e27d" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">评估模型的标准方法及其缺点</h1><p id="d854" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated"><strong class="ki iu">经典方法</strong>是评估所有测试输入的模型输出，并根据自己喜欢的度量计算<em class="le">平均错误率</em>。举个例子:假设你正在预测明天的温度，你的模型平均误差率为 5%，如果你的模型预测明天的温度为 30 度，你只需在这个数字上加上+- 5%就可以得到一个置信区间。</p><p id="9c8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这并不完美，原因有很多。这里的主要问题是我们使用一个不依赖于数据点的平均误差估计。如果模型有时对某些输入非常确定会怎样？例如，如果你正处于雨季，模型可能 99%确定明天会下雨，5%的误差率太大了。另一方面，可能有些日子模型比平均值更不确定，这 5%可能会给你带来麻烦。总之，计算平均错误率是一个平均值。它没有考虑到你所预测的实际点的特殊性。</p><p id="18f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们改进这一点。如果模型本身不仅能输出单一的预测值，还能输出其可信度会怎样？通过这种方式，您将获得一个根据您预测的实际数据点量身定制的置信度评估，从而获得上下文感知。在现实生活中，了解估计的不确定性和实际预测一样重要。</p><p id="d96c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们将制作一个非常简单的神经网络，它使用不确定性估计进行回归。为了尽可能简单明了，我将使用一个虚拟的一维数据，一个单层网络，没有测试集。</p><p id="a47b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在试图理解这篇论文时举了这个例子:<a class="ae kf" href="https://arxiv.org/abs/1704.04110" rel="noopener ugc nofollow" target="_blank"> DeepAR:用自回归递归网络进行概率预测</a>来自<em class="le"> D. Salinas 等人</em>在那里他们建立了一个用于多时间序列预测的概率预测网络。</p><p id="91a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将把工作分成三部分:</p><ol class=""><li id="c72f" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld mn mo mp mq bi translated">产生具有变化的不确定性的伪数据</li><li id="cd2e" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated">用简单的置信区间制作一个非常简单的经典回归模型</li><li id="2722" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated">改进模型，使其输出置信度估计值</li></ol><h1 id="8f0b" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">制作虚拟数据</h1><p id="cbe2" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">我们生成两个变量:<code class="fe mw mx my mz b">x</code>是我们的自变量，它取 0 到 1 之间的值，<code class="fe mw mx my mz b">y</code>是我们想要预测的值。</p><p id="b771" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们根据以下公式生成 y:</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/58216a0e8b1676d7c3450cf8d8f9aaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*x6Ew4EH6VtG43mvJPOa0mw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">数据生成方程</p></figure><p id="d686" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来分析一下。这意味着 y 相当于 x 加上一些噪声，噪声的大小取决于 x，所以，这是一个非常非常简单的预测！然而，y 上的<em class="le">不确定性</em>与 x 成线性增长，因此，如果 x 较小，则很确定 y=x，但对于较大的 x 值，y 开始围绕其期望值扩散。</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e776e3aa89292ec3064f350bbc19e0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*lNWJCfjXbcKYxJQ9Ud7Rvg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">在我们的数据图中，我们可以看到 y 值随着 x 向 1 扩展。</p></figure><h1 id="8c6b" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">经典回归模型</h1><p id="8ceb" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">让我们制作一个基于<code class="fe mw mx my mz b">x</code>预测<code class="fe mw mx my mz b">y</code>的神经网络。我们只用一个单层神经网络。为此，我们将使用 tf.keras:</p><figure class="nb nc nd ne gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a82883d2223846c6669e2e2bc92ec61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*dH3jQg5g2MKHm9u2dlj4Wg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">单层架构，没有比这更简单的了。</p></figure><p id="5e10" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在用均方误差(MSE)损失和 Adam 优化器训练该模型 200 个时期后，我们得到以下预测:</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/088622c95f5f1776a57b7b5c5448157d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*3mX2JmsdevHqwBibhWJGpQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">朴素模型的预测。</p></figure><p id="a252" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，这差不多是我们预期的预测。我们的模型正确预测 y=x，MSE 为 0.011。</p><p id="4cab" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设模型误差是高斯的，我们可以用这个 MSE 误差来计算置信区间。这些细节使用了高斯分布的一些属性，我们现在并不真正需要这些属性，所以让我们使用下面的公式，它给出了 95%的置信区间(CI):</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1e6053babb563b80524bfa3271804a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*IJJotSacaQizXHkSJIjjbQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">95%置信区间的公式</p></figure><p id="38dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的简单模型上使用这个公式，并以图形方式显示结果，给出了下图。如您所见，CI 是正确的，因为我们有大约 95%的数据点在预测区间内。但是它完全忽略了数据噪音的可变性。</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7a6947d697d727ff6ad428d52d5ff8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Z7xjY-X-7Rb7_68nuw484A.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">具有 95%置信区间的简单模型</p></figure><h1 id="a7c3" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">向模型输出添加置信度信息</h1><p id="e036" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">正如所承诺的，我们将修改我们的模型，使其输出不仅仅是一个预测，而是一个预测及其置信度。</p><p id="0cb9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是获取技术的时候了。为了在预测中引入信心，我们需要讨论我们的预测值的<em class="le">分布</em>，我们写为 P(y|x)。根据手头的数据，您期望的分布可能会有所不同。在我们的例子中，我们显然有一个正态分布，因为这是我们如何生成我们的数据，但这不是强制性的。</p><p id="85dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您如何知道哪个发行版与您的项目相关？绘制数据并查看模式的数量，围绕平均值和数据范围的对称性是一个良好的开端。根据这些参数，你应该找到一个符合这些特征的理论分布。</p><p id="77ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，正态分布是默认使用的分布，如果您正在处理实值数据，它应该是一个很好的起点。让我们继续这个例子。</p><p id="e5c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正态分布由两个参数定义:平均值<em class="le"> 𝜇 </em>和标准差<em class="le"> 𝜎 </em>。从这两个值，我们已经完全确定了分布，我们可以计算我们可能需要的所有置信区间。所以，让我们的网络输出这两个值。这很简单:</p><figure class="nb nc nd ne gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d712c6007e4568ec10bb59fe9ec2e206.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*kO6lkUqy03pcjGThDdrCsg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">具有概率输出的网络结构。</p></figure><p id="1911" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“左分支”计算标准偏差<em class="le"> 𝜎 </em>，而“右分支”计算平均值<em class="le"> 𝜇 </em>。我对<em class="le">标准偏差</em>输出使用了“relu”激活，因为标准偏差必须为正。在最后一层，两个输出被连接。</p><p id="d201" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们需要<strong class="ki iu">为模型选择一个损失函数</strong>。之前，我们使用了一个均方误差来惩罚预测误差。但是在这里，我们需要一种方法来惩罚预测和不确定性。</p><p id="4d42" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将把问题写成概率的形式。我们的模型以 x 为输入，返回一个正态概率分布 N( <em class="le"> 𝜇 </em> ( <em class="le"> 𝑥 </em>)，<em class="le"> 𝜎 </em> ( <em class="le"> 𝑥 </em>))。</p><p id="d7c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果这个分布与实际的数据分布相匹配，那么我们的训练就是成功的。我们如何对此进行量化？我们会说，如果获得我们的精确训练数据的概率很高，如果我们从我们的分布中提取数据，这是一个很好的匹配。</p><p id="4efa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据点(x，y)，y 从 N( <em class="le"> 𝜇 </em> ( <em class="le"> 𝑥 </em>)、<em class="le"> 𝜎 </em> ( <em class="le"> 𝑥 </em>))被采样的概率由下式给出:</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6fe8b8a99dedf68dc277c5925470ece4.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*MTD-cBZfp-85EXQiETlsUA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">如果我们的数据是从输出分布中抽样的，那么看到它的概率。</p></figure><p id="ea9d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这只是高斯定律的概率密度函数。现在我们要使这个概率尽可能的高，也就是我们要找到使<em class="le"/>(<em class="le">|<em class="le"/>(<em class="le">【𝑥</em>)<em class="le"/><em class="le">【𝑥</em>)最大化的函数<em class="le">【𝑃】</em>(<em class="le"/>|<em class="le">𝜇</em>(<em class="le">𝑥</em>)<em class="le"/>(</em>)</p><p id="abda" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在前面有几个有趣的计算，我打算跳过，因为它们非常简单:</p><ul class=""><li id="a588" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld nm mo mp mq bi translated">我们应用对数，因为最大化 p 与最大化 ln( <em class="le"> 𝑃 </em>)是相同的。</li><li id="a8a6" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld nm mo mp mq bi translated">我们删除常量，因为它们在优化中不起作用。</li><li id="0a14" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld nm mo mp mq bi translated">我们将否定整个表达式。这只是因为 Keras 喜欢<em class="le">最小化</em>事情，如果我们最小化负面就像最大化原始。</li></ul><p id="417a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们得到了一个 Keras 可以理解的损失:</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/057a422063a51d102196da8426a655f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*WAD12EipY1Ta5O-vLI14RQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">概率损失</p></figure><p id="40a2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是有效的损失吗？我们可以直观地检查它是否有意义。它将我们的目标值 y 与网络输出<em class="le"> 𝜎 </em>，<em class="le"> 𝜇.结合在一起</em>此外，我们可以单独理解每个术语:</p><ul class=""><li id="a51a" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld nm mo mp mq bi translated">ln( <em class="le"> 𝜎 </em>):这对于小<em class="le"> 𝜎 </em>来说会比较低，从而<em class="le">推动网络自信</em>。</li><li id="9fa7" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld nm mo mp mq bi translated">当<em class="le"> 𝜇 </em>接近 y 时，第二项将为低。这将推动网络减少预测误差。对于高<em class="le"> 𝜎 </em>值，该项也将为低。所以，当网络说它不自信时，这种损失对错误的惩罚较少(高<em class="le"> 𝜎 </em>)。</li></ul><p id="1294" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将此实现为一个 Keras 自定义损失。</p><figure class="nb nc nd ne gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="551b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练这个模型比前一个稍微复杂一点。这里有一个效果很好的迭代方法:</p><ol class=""><li id="df49" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld mn mo mp mq bi translated">使用<a class="ae kf" href="https://www.machinecurve.com/index.php/2020/02/20/finding-optimal-learning-rates-with-the-learning-rate-range-test/" rel="noopener ugc nofollow" target="_blank">学习率范围测试</a>选择学习率。</li><li id="42e5" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated">以此学习率训练 100 个纪元。</li><li id="86b1" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated">检查损失，如果低于-1.9 止损，否则返回步骤 1。</li></ol><p id="6446" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，这种方法在几百个纪元内有效。请注意，大约有一半的时间，它收敛到一个坏的局部最小值。在这种情况下，只需重新初始化模型并重新启动。</p><p id="d069" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面我们有一个样本训练曲线，我只需要重复上面的过程两次。正如你所看到的，我开始时的学习率是 0.01，然后根据范围测试结果移动到 0.1。</p><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2ee3dd1229c9fe2759b0b7786f4a058c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*Y0bc31vKIhQOIL4126GLvQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">概率模型的训练曲线。</p></figure><p id="0904" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在我们的模型已经训练好了，是时候检查预测了。我们将绘制我们的模型结果。为此:</p><ul class=""><li id="b376" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld nm mo mp mq bi translated">我们使用第一个输出<em class="le"> 𝜇 </em>作为预测。</li><li id="e648" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld nm mo mp mq bi translated">我们使用第二个输出<em class="le"> 𝜎 </em>根据公式<em class="le">𝐶𝐼</em>=<em class="le">𝜇</em>1.96×<em class="le">𝜎.建立 95% CI</em></li></ul><figure class="nb nc nd ne gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7487bc84e71aebd58775f668e3617989.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*sz_VO5n8UyG2JjwcwOTC7A.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">概率预测结果。预测的不确定性与数据中的不确定性相匹配。</p></figure><p id="a8ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样。如你所见，我们的模型对小 x 值更有信心。</p><h1 id="28f9" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="5653" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在本文中，我们已经看到了如何进行预测以及不确定性估计。我们已经研究了 Keras 中的理论和实现。</p><p id="7be3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从实践的角度来看，我们可以升级现有的网络，分两步返回置信度估计值:</p><ol class=""><li id="bd0c" class="mi mj it ki b kj kk kn ko kr mk kv ml kz mm ld mn mo mp mq bi translated">添加分支以生成标准差输出</li><li id="d474" class="mi mj it ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated">切换到概率损失</li></ol><p id="0124" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，理解理论是有好处的，这样我们就可以选择正确的分布和适当的损失。</p><p id="9b93" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，用真实数据呢？看看 DeepAR:用自回归递归网络进行概率预测，他们应用了一种更复杂的技术来获得销售预测的不确定性估计。</p><p id="8b6f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我用这个简单的例子来总结这篇文章，希望它也能对你有用。我还没有在真实数据集上尝试过。一定要让我知道你的想法！</p></div></div>    
</body>
</html>