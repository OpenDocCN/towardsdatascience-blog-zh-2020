# å¦‚ä½•å¤„ç†å¤šç±»ä¸å¹³è¡¡æ•°æ®ï¼Ÿ-å¯¹ SMOTE è¯´ä¸

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-handle-multiclass-imbalanced-data-say-no-to-smote-e9a7f393c310?source=collection_archive---------8----------------------->

## ä¸éœ€è¦å†æ‰“äº†ã€‚

![](img/59f938f0417ae46b7aac07816a74c8f6.png)

æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Œå…¶ä¸­ç›®æ ‡ç±»ä¸­å­˜åœ¨é«˜åº¦ä¸ç›¸ç§°çš„æ•°æ®ã€‚

ä½ å¥½ï¼Œä¸–ç•Œï¼Œè¿™æ˜¯æˆ‘ä¸ºæ•°æ®ç§‘å­¦ç¤¾åŒºå†™çš„ç¬¬äºŒç¯‡åšå®¢ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¤„ç†å¤šç±»ä¸å¹³è¡¡æ•°æ®é—®é¢˜ã€‚

# ä»€ä¹ˆæ˜¯å¤šç±»ä¸å¹³è¡¡æ•°æ®ï¼Ÿ

å½“åˆ†ç±»é—®é¢˜çš„ç›®æ ‡ç±»(ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Š)ä¸æ˜¯å‡åŒ€åˆ†å¸ƒæ—¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸å¹³è¡¡æ•°æ®ã€‚å¦‚æœæˆ‘ä»¬ä¸èƒ½å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å°†ä¼šå˜æˆä¸€åœºç¾éš¾ï¼Œå› ä¸ºä½¿ç”¨é˜¶çº§ä¸å¹³è¡¡æ•°æ®çš„æ¨¡å‹åå‘äºå¤§å¤šæ•°é˜¶çº§ã€‚

å¤„ç†ä¸å¹³è¡¡æ•°æ®æœ‰ä¸åŒçš„æ–¹æ³•ï¼Œæœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯è¿‡é‡‡æ ·å’Œåˆ›å»ºåˆæˆæ ·æœ¬ã€‚

# ä»€ä¹ˆæ˜¯ SMOTEï¼Ÿ

SMOTE æ˜¯ä¸€ç§è¿‡é‡‡æ ·æŠ€æœ¯ï¼Œå®ƒä»æ•°æ®é›†ç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œä»è€Œæé«˜å°‘æ•°ç±»çš„é¢„æµ‹èƒ½åŠ›ã€‚å°½ç®¡æ²¡æœ‰ä¿¡æ¯ä¸¢å¤±ï¼Œä½†å®ƒæœ‰ä¸€äº›é™åˆ¶ã€‚

![](img/51eeca46f25ea31f65f533130b4ee3b8.png)

åˆæˆæ ·å“

***å±€é™æ€§:***

1.  SMOTE å¯¹äºé«˜ç»´æ•°æ®ä¸æ˜¯å¾ˆå¥½
2.  å¯èƒ½ä¼šå‘ç”Ÿç±»çš„é‡å ï¼Œè¿™ä¼šç»™æ•°æ®å¸¦æ¥æ›´å¤šçš„å™ªå£°ã€‚

å› æ­¤ï¼Œä¸ºäº†è·³è¿‡è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨' **class_weight** 'å‚æ•°ä¸ºç±»æ‰‹åŠ¨åˆ†é…æƒé‡ã€‚

# ä¸ºä»€ä¹ˆä½¿ç”¨ç±»æƒé‡ï¼Ÿ

ç±»æƒé‡é€šè¿‡ç»™å…·æœ‰ä¸åŒæƒé‡çš„ç±»ä¸€ä¸ªæƒ©ç½šæ¥ç›´æ¥ä¿®æ”¹æŸå¤±å‡½æ•°ã€‚å®ƒæ„å‘³ç€æœ‰ç›®çš„åœ°å¢åŠ å°‘æ•°é˜¶çº§çš„æƒåŠ›ï¼Œå‡å°‘å¤šæ•°é˜¶çº§çš„æƒåŠ›ã€‚å› æ­¤ï¼Œå®ƒæ¯” SMOTE ç»™å‡ºæ›´å¥½çš„ç»“æœã€‚

# æ¦‚è¿°:

æˆ‘çš„ç›®æ ‡æ˜¯è®©è¿™ä¸ªåšå®¢éå¸¸ç®€å•ã€‚æˆ‘ä»¬æœ‰ä¸€äº›æœ€å¸¸ç”¨çš„æŠ€æœ¯æ¥è·å¾—æ•°æ®çš„æƒé‡ï¼Œè¿™äº›æŠ€æœ¯å¯¹æˆ‘ä¸å¹³è¡¡çš„å­¦ä¹ é—®é¢˜æœ‰æ•ˆã€‚

1.  Sklearn utilsã€‚
2.  è®¡æ•°åˆ°é•¿åº¦ã€‚
3.  å¹³æ»‘é‡é‡ã€‚
4.  æ ·æœ¬æƒé‡ç­–ç•¥ã€‚

# 1.Sklearn å®ç”¨ç¨‹åº:

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ sklearn è®¡ç®—ç±»æƒé‡æ¥è·å¾—ç±»æƒé‡ã€‚é€šè¿‡åœ¨è®­ç»ƒæ¨¡å‹æ—¶å°†é‚£äº›æƒé‡æ·»åŠ åˆ°å°‘æ•°ç±»ï¼Œå¯ä»¥åœ¨åˆ†ç±»ç±»æ—¶å¸®åŠ©æ€§èƒ½ã€‚

```
from sklearn.utils import class_weightclass_weight = class_weight.compute_class_weight('balanced,
                                                np.unique(target_Y),
                                                target_Y)model = LogisticRegression(class_weight = class_weight)
model.fit(X,target_Y)# ['balanced', 'calculated balanced', 'normalized'] are hyperpaameters whic we can play with.
```

ä»é€»è¾‘å›å½’åˆ° Catboostï¼Œå‡ ä¹æ‰€æœ‰çš„åˆ†ç±»ç®—æ³•éƒ½æœ‰ä¸€ä¸ª class_weight å‚æ•°ã€‚ä½†æ˜¯ XGboost æœ‰é’ˆå¯¹äºŒå…ƒåˆ†ç±»çš„ scale_pos_weight å’Œé’ˆå¯¹äºŒå…ƒå’Œå¤šç±»é—®é¢˜çš„ sample_weights(å‚è€ƒ 4)ã€‚

# 2.è®¡æ•°ä¸é•¿åº¦ä¹‹æ¯”:

éå¸¸ç®€å•ç›´ç™½ï¼å°†æ¯ç±»çš„è®¡æ•°é™¤ä»¥è¡Œæ•°ã€‚ç„¶å

```
weights = df[target_Y].value_counts()/len(df)
model = LGBMClassifier(class_weight = weights)
model.fit(X,target_Y)
```

# 3.å¹³æ»‘æƒé‡æŠ€æœ¯:

è¿™æ˜¯é€‰æ‹©æƒé‡çš„ä¼˜é€‰æ–¹æ³•ä¹‹ä¸€ã€‚

labels_dict æ˜¯åŒ…å«æ¯ä¸ªç±»çš„è®¡æ•°çš„å­—å…¸å¯¹è±¡ã€‚

log å‡½æ•°å¹³æ»‘ä¸å¹³è¡¡ç±»çš„æƒé‡ã€‚

```
def class_weight(labels_dict,mu=0.15):
    total = np.sum(labels_dict.values())
    keys = labels_dict.keys()
    weight = dict()for i in keys:
        score = np.log(mu*total/float(labels_dict[i]))
        weight[i] = score if score > 1 else 1return weight# random labels_dict
labels_dict = df[target_Y].value_counts().to_dict()weights = class_weight(labels_dict)model = RandomForestClassifier(class_weight = weights)
model.fit(X,target_Y)
```

# 4.æ ·å“é‡é‡ç­–ç•¥:

ä»¥ä¸‹å‡½æ•°ä¸åŒäº class_weight å‚æ•°ï¼Œè¯¥å‚æ•°ç”¨äºè·å– XGboost ç®—æ³•çš„æ ·æœ¬æƒé‡ã€‚å®ƒä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿”å›ä¸åŒçš„æƒé‡ã€‚

> Sample_weight æ˜¯ä¸€ä¸ªä¸æ•°æ®é•¿åº¦ç›¸åŒçš„æ•°ç»„ï¼ŒåŒ…å«åº”ç”¨äºæ¯ä¸ªæ ·æœ¬çš„æ¨¡å‹æŸå¤±çš„æƒé‡ã€‚

```
def BalancedSampleWeights(y_train,class_weight_coef):
    classes = np.unique(y_train, axis = 0)
    classes.sort()
    class_samples = np.bincount(y_train)
    total_samples = class_samples.sum()
    n_classes = len(class_samples)
    weights = total_samples / (n_classes * class_samples * 1.0)
    class_weight_dict = {key : value for (key, value) in              zip(classes, weights)}
    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * 
    class_weight_coef
    sample_weights = [class_weight_dict[i] for i in y_train]
    return sample_weights#Usage
weight=BalancedSampleWeights(target_Y,class_weight_coef)
model = XGBClassifier(sample_weight = weight)
model.fit(X, target_Y)
```

***ç±» _ æƒé‡ vs æ ·æœ¬ _ æƒé‡:***

`sample_weights`ç”¨äºç»™å‡ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚è¿™æ„å‘³ç€æ‚¨åº”è¯¥ä¼ é€’ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œå…¶ä¸­çš„å…ƒç´ æ•°é‡ä¸æ‚¨çš„è®­ç»ƒæ ·æœ¬å®Œå…¨ç›¸åŒã€‚

`class_weights`ç”¨äºä¸ºæ¯ä¸ªç›®æ ‡ç±»èµ‹äºˆæƒé‡ã€‚è¿™æ„å‘³ç€ä½ åº”è¯¥ä¸ºä½ è¦åˆ†ç±»çš„æ¯ä¸ªç±»ä¼ é€’ä¸€ä¸ªæƒé‡ã€‚

# ç»“è®º:

ä»¥ä¸Šæ˜¯ä¸ºä½ çš„åˆ†ç±»å™¨å¯»æ‰¾ç±»æƒé‡å’Œæ ·æœ¬æƒé‡çš„å‡ ç§æ–¹æ³•ã€‚æˆ‘æåˆ°äº†å‡ ä¹æ‰€æœ‰å¯¹æˆ‘çš„é¡¹ç›®æœ‰æ•ˆçš„æŠ€æœ¯ã€‚

æˆ‘è¯·æ±‚è¯»è€…å°è¯•ä¸€ä¸‹è¿™äº›å¯ä»¥å¸®åŠ©ä½ çš„æŠ€æœ¯ï¼Œå¦‚æœä¸æ˜¯æŠŠå®ƒå½“ä½œå­¦ä¹ çš„è¯ğŸ˜„ä¸‹æ¬¡å¯èƒ½ä¼šå¯¹ä½ æœ‰å¸®åŠ©ğŸ˜œ

é€šè¿‡ LinkedIn è”ç³»æˆ‘ğŸ˜