<html>
<head>
<title>What Covid-related topics are being discussed in Spotify Podcasts?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spotify 播客中正在讨论哪些与 Covid 相关的话题？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-covid-related-topics-are-being-discussed-in-spotify-podcasts-bbc7eb9a3f1?source=collection_archive---------65-----------------------#2020-06-29">https://towardsdatascience.com/what-covid-related-topics-are-being-discussed-in-spotify-podcasts-bbc7eb9a3f1?source=collection_archive---------65-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d49" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从 Spotify API 检索数据，使用 LDA 算法进行主题建模</h2></div><p id="f0be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于最近的新冠肺炎疫情，我在过去的 3 个月里一直呆在家里，一直在寻找在家自娱自乐的方式。听 Spotify 播客已经成为我周末的例行公事，同时重拾我的老爱好钩针编织。然后我就在想，Spotify 播客宇宙是什么样的？</p><p id="781c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">免责声明</em></strong><em class="le">:Spotify API 数据检索的代码大部分是根据来自</em><a class="ae lf" href="https://github.com/sam-brady/spotify-podcasts" rel="noopener ugc nofollow" target="_blank"><em class="le">Sam Brady</em></a><em class="le">的源代码完成的。LDA 建模大多是跟随</em> <a class="ae lf" href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/" rel="noopener ugc nofollow" target="_blank"> <em class="le">机器学习加</em> </a> <em class="le">的教程完成的。两者都经过一些修改后使用。</em>(谢谢！)</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="a31b" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">从 Spotify API 获取数据</h1><p id="0eec" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">令我兴奋的是，Spotify 已经有了一个<a class="ae lf" href="https://developer.spotify.com/" rel="noopener ugc nofollow" target="_blank">开发者 API </a>，我们可以用它从 Spotify 获取数据，或者为 Spotify 用户触发某些动作。我们需要做的只是注册到网站，创建一个应用程序，并获得 API 令牌。然后，我们可以使用 Python 中的<code class="fe mk ml mm mn b">spotipy</code>包从 API 中检索数据。</p><p id="cee7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我使用的是萨姆·布雷迪的源代码(谢谢！)，只需稍加修改即可获得与 Covid 相关的播客剧集列表。我使用了两种方法:</p><ul class=""><li id="cc69" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">获取包含 Covid 相关术语的播客节目，然后获取这些节目的所有剧集</li><li id="7877" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">获取包含 Covid 相关术语的所有剧集</li></ul><h2 id="a6d5" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">导入包和定义</h2><p id="0a86" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">在这里进口必需品。基本上我们将使用<code class="fe mk ml mm mn b">spotipy</code>和<code class="fe mk ml mm mn b">json requests</code>包。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="ee54" class="nc lo it mn b gy nw nx l ny nz">import pandas as pd<br/>import numpy as np</span><span id="f742" class="nc lo it mn b gy oa nx l ny nz">import json<br/>import requests<br/>import urllib.parse</span><span id="3b1d" class="nc lo it mn b gy oa nx l ny nz">import spotipy.util as util</span></pre><p id="3182" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们定义与 Spotify API 的连接。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="d1e8" class="nc lo it mn b gy nw nx l ny nz"># Get your client id, secret key, and redirect URI from your application <a class="ae lf" href="https://developer.spotify.com/dashboard/applications" rel="noopener ugc nofollow" target="_blank">https://developer.spotify.com/dashboard/applications</a></span><span id="cffc" class="nc lo it mn b gy oa nx l ny nz">client_id = 'xxxxx'  #&lt;----------------- YOUR ID HERE<br/>    <br/>client_secret = 'xxxxx' #&lt;----------------- YOUR SECRET HERE</span><span id="6e87" class="nc lo it mn b gy oa nx l ny nz">username = 'xxxxx'      #&lt;----------------- YOUR USERNAME HERE</span><span id="d7f9" class="nc lo it mn b gy oa nx l ny nz">scope = 'user-library-read' #&lt;------------- depends on your needs, you can select different scope <a class="ae lf" href="https://developer.spotify.com/documentation/general/guides/scopes/" rel="noopener ugc nofollow" target="_blank">https://developer.spotify.com/documentation/general/guides/scopes/</a></span><span id="976d" class="nc lo it mn b gy oa nx l ny nz">redirect_uri = '<a class="ae lf" href="https://developer.spotify.com/dashboard/applications/24c33183a3f54ba7893c740aac55d9ab'" rel="noopener ugc nofollow" target="_blank">https://developer.spotify.com/dashboard/applications/[application_id]'</a></span><span id="05ce" class="nc lo it mn b gy oa nx l ny nz">token = util.prompt_for_user_token(username=username, <br/>                                   scope=scope, <br/>                                   client_id=client_id,   <br/>                                   client_secret=client_secret,     <br/>                                   redirect_uri=redirect_uri)</span></pre><h2 id="186e" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">获取播客节目</h2><p id="8ada" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">在这里，我们将获得预先选择的与 covid 相关的关键字(“科罗纳”、“Covid”、“锁定”、“隔离”、“疫情”)的播客节目。结果被添加到数据框中以供进一步研究。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="2e98" class="nc lo it mn b gy nw nx l ny nz"># enter term to search here<br/>search_list = ['corona', 'covid','lockdown','quarantine','pandemic']</span><span id="f70e" class="nc lo it mn b gy oa nx l ny nz"># search endpoint<br/>endpoint_url = "<a class="ae lf" href="https://api.spotify.com/v1/search" rel="noopener ugc nofollow" target="_blank">https://api.spotify.com/v1/search</a>?"</span><span id="c005" class="nc lo it mn b gy oa nx l ny nz"># PERFORM THE QUERY</span><span id="ba43" class="nc lo it mn b gy oa nx l ny nz">id_list = []<br/>name_list = []                                         <br/>desc_list = []<br/>publisher_list = []<br/>languages_list = []</span><span id="80ce" class="nc lo it mn b gy oa nx l ny nz">type = 'show'    <br/>market  = 'US'</span><span id="dad4" class="nc lo it mn b gy oa nx l ny nz">for i in range(len(search_list)):<br/>    search = search_list[i]<br/>    print(search)<br/>    offset = 0                                   <br/>    more_runs = 1        <br/>    counter = 0  <br/>    limit = 50</span><span id="6252" class="nc lo it mn b gy oa nx l ny nz"># max offset is 2000 including limit<br/>while((offset &lt;= 1950) &amp; (counter &lt;= more_runs)):</span><span id="4cce" class="nc lo it mn b gy oa nx l ny nz">query = f'{endpoint_url}'<br/>    query += f'&amp;q={search}'<br/>    query += f'&amp;type={type}'<br/>    query += f'&amp;offset={offset}'                      <br/>    query += f'&amp;market={market}'<br/>    query += f'&amp;limit={limit}'</span><span id="4e23" class="nc lo it mn b gy oa nx l ny nz">response = requests.get(query,                                           # get request<br/>                   headers={"Content-Type":"application/json", <br/>                            "Authorization":f"Bearer {token}"})  <br/>    json_response = response.json()                                           # as a json file</span><span id="3608" class="nc lo it mn b gy oa nx l ny nz">for i in range(len(json_response['shows']['items'])):                      # loop through json<br/>        if json_response['shows']['items'][i] is not None:<br/>            id_list.append(json_response['shows']['items'][i]['id'])               # pull out info from json<br/>            name_list.append(json_response['shows']['items'][i]['name'])           # into empty lists<br/>            desc_list.append(json_response['shows']['items'][i]['description'])<br/>            publisher_list.append(json_response['shows']['items'][i]['publisher'])<br/>            languages_list.append(json_response['shows']['items'][i]['languages'])<br/>            <br/>        <br/>    more_runs = (json_response['shows']['total'] // 50 )            # how many more runs of 50 are needed?       <br/>        <br/>    counter += 1                                                    # increase conditional counter by 1<br/>    <br/>    offset = offset + 50</span><span id="5540" class="nc lo it mn b gy oa nx l ny nz"># set up a dataframe from the lists</span><span id="55eb" class="nc lo it mn b gy oa nx l ny nz">podcasts_show = pd.DataFrame()</span><span id="522c" class="nc lo it mn b gy oa nx l ny nz">podcasts_show['show_id'] = id_list<br/>podcasts_show['show_name'] = name_list<br/>podcasts_show['show_description'] = desc_list<br/>podcasts_show['show_publisher'] = publisher_list<br/>podcasts_show['show_language'] = languages_list</span></pre><p id="2388" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是上面代码的结果。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ob"><img src="../Images/a0e685213e6205d45992fac0823c99c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A76fhrgHLIGiX2sqZzsEBA.png"/></div></div></figure><p id="9fc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里我找到了 1998 年的节目，但是有些不是英文的，我无法分析。因此，在下一步中，我将只过滤英语节目。</p><h2 id="da4c" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">过滤英语播客</h2><p id="e6ea" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">有三种英语定义:通用英语(en)、美国英语(en-US)和英国英语(en-GB)</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="7835" class="nc lo it mn b gy nw nx l ny nz">en_podcasts = pd.DataFrame()<br/>en_podcasts = podcasts_show[podcasts_show.apply(lambda x: 'en' in x['show_language'], axis=1)]</span><span id="d7f2" class="nc lo it mn b gy oa nx l ny nz">en_podcasts_us = pd.DataFrame()<br/>en_podcasts_us = podcasts_show[podcasts_show.apply(lambda x: 'en-US' in x['show_language'], axis=1)]</span><span id="855d" class="nc lo it mn b gy oa nx l ny nz">en_podcasts_gb = pd.DataFrame()<br/>en_podcasts_gb = podcasts_show[podcasts_show.apply(lambda x: 'en-GB' in x['show_language'], axis=1)]</span><span id="b013" class="nc lo it mn b gy oa nx l ny nz">en_podcasts = en_podcasts.append(en_podcasts_us)<br/>en_podcasts = en_podcasts.append(en_podcasts_gb)</span><span id="afb9" class="nc lo it mn b gy oa nx l ny nz">en_podcasts</span></pre><p id="3705" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果被缩短为 995 个仅用英语显示的节目。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oj"><img src="../Images/5b557d82eae2c0d99a19ab8a5a5d8293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEQirq0cPwF1RYem8b4eVg.png"/></div></div></figure><p id="a8bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要从上面的节目中获得剧集列表，并获得所有带有 Covid 相关术语的播客剧集，只需对上面的代码进行轻微修改(在<code class="fe mk ml mm mn b">type</code>语法上)。使用的完整代码可以在找到<a class="ae lf" href="https://github.com/oliviatan29/spotify-podcast-expl/blob/master/Spotify%20Test%20-%20Data%20Retrieval.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="e968" class="ln lo it bd lp lq ok ls lt lu ol lw lx jz om ka lz kc on kd mb kf oo kg md me bi translated">初始数据探索</h1><p id="0cd2" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">从上面的数据检索方法中，我们得到了 Spotify 中与 Covid 相关的播客的一些元数据。这些元数据包括发布日期、节目名称和描述，甚至是剧集的 30 秒音频预览。</p><h2 id="ebc5" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">它什么时候开始流行？</h2><p id="4489" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">虽然 Covid 案例在 2019 年 12 月下旬就已确定，但 Spotify universe 实际上直到 2020 年 3 月 6 日左右才谈论它，我们看到发布的剧集数量从之前的每天不到 20 集激增到超过 40 集。这与 2020 年 3 月 5 日开始的美国<strong class="kk iu">发现新冠肺炎病例</strong>相一致。</p><p id="3dcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">剧集数持续增加，2020 年 5 月中旬达到<strong class="kk iu"> ~270 集/天</strong>。尽管这一趋势在最近几周略有下降，但仍高于 180 集/天。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi op"><img src="../Images/28fe803afbc673651a901084653838f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqNHYUp7u6JSt_1D1FsISA.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">按发布日期排列的总集数(忽略 3 月 7 日的高峰，一个频道在那一天回填了他们的内容)</p></figure><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ou"><img src="../Images/9772648ead447018806a27017f3baae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9OfkgyUHiXzN6l748dQQA.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">美国新冠肺炎每日新病例(来源:谷歌/维基百科)</p></figure><p id="1b14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从播客剧集图表中看到另一个有趣的模式？不知何故，这些播客在工作日发布内容(比周末高出 2-3 倍)。也许周末也是播客的节日？:D</p><h2 id="9979" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">哪个频道的内容最多？</h2><p id="c7e8" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">调查节目出版商和他们的节目，发现有一些出版商有不止一个节目讨论 Covid 相关的问题。例如，iHeartRadio 有三个与 Covid 相关的讨论节目:重开美洲(每日新闻)、妇女(与疫情战斗的女性前线)、与凯蒂和博兹一起回到商业(商业会谈)。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ov"><img src="../Images/3a780b4c1bea2d72039a8c53b3d98bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ejYr2WX-OAOPSxhTobpKHQ.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">顶级播客出版商及其与 Covid 相关的节目</p></figure><p id="df89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看播客内容类别，我们发现有几个关键内容类别的集数最高:</p><ul class=""><li id="bd73" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu">宗教内容</strong>:布道、宗教鼓励</li><li id="53fb" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">新闻与研究</strong>:每日更新关于疫情的最新案例、影响和研究</li><li id="95ff" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">娱乐</strong>:有影响力的脱口秀，隔离黑客</li><li id="dfbe" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">鼓励</strong>:来自因封锁/危机而抑郁的人的心理健康支持</li></ul><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ow"><img src="../Images/82a33bc33e4dde1e62a8fd086cddaa90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ku-8zYox8vSy5MbLsRrafg.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">集数最高的播客节目</p></figure><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ox"><img src="../Images/76607405c3af1754ecd3c34addcbe460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eUJN6a5SuUVWdKpna5-V2Q.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">月月字云</p></figure><p id="c369" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在分析的第 2 部分更深入地挖掘这个主题:)</p><h2 id="7b33" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">每集多长？</h2><p id="9c95" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">在大多数情况下，这些播客从<strong class="kk iu">15-49 分钟长</strong>。还有一些剧集的运行时间更长。观察这一持续时间与听众点击率或衰减的关系可能会很有趣(正如我们所知，一些人可能会对长时间的节目感到厌倦)。但是，Spotify API 中没有这些数据，因此无法对其进行进一步分析。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oy"><img src="../Images/e9ed9fed35e1147d159c73bfb100726c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VvKf0fLxiuqFzVnA4dPt4g.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">播客时长</p></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="8626" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">什么是 LDA？</h1><p id="84ce" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">引用<a class="ae lf" href="https://algorithmia.com/algorithms/nlp/LDA/docs" rel="noopener ugc nofollow" target="_blank"> algorithmia </a>，LDA 是一种算法，它获取一组文档(任何由 up 文本组成的东西)并返回与这些文档最相关的几个<strong class="kk iu">主题</strong>(由几个单词组成)<strong class="kk iu">。</strong></p><p id="8545" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主题被表示为单词的加权列表。基于模型的这 3 个主要参数来计算这些单词的权重:</p><ul class=""><li id="a229" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">主题的数量</li><li id="9d8b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">每个主题的字数</li><li id="d388" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">每个文档的主题数</li></ul><p id="8b07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型的输出是一个主题列表，其中包含与文档中的<strong class="kk iu">相关的单词</strong>及其<strong class="kk iu">出现次数</strong>。</p><h1 id="4ff0" class="ln lo it bd lp lq ok ls lt lu ol lw lx jz om ka lz kc on kd mb kf oo kg md me bi translated">使用 LDA 进行 Spotify 播客主题探索</h1><h2 id="2861" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">数据清理</h2><p id="f011" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">任何类型的数据分析和建模的第一步都是进行数据清理。在这种情况下，使用的数据清理技术是<strong class="kk iu">从文档中删除脱离上下文的字符</strong>(标点、数字、字母、空格)。这里我使用的是 Python 中<code class="fe mk ml mm mn b">gensim</code>包的<code class="fe mk ml mm mn b">gsp</code>模块。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="faa3" class="nc lo it mn b gy nw nx l ny nz">from gensim import utils<br/>import gensim.parsing.preprocessing as gsp</span><span id="3134" class="nc lo it mn b gy oa nx l ny nz"># remove punctuation, short words, whitespaces, numeric<br/>filters = [<br/>           gsp.strip_punctuation,<br/>           gsp.strip_multiple_whitespaces,<br/>           gsp.strip_numeric,<br/>           gsp.strip_short<br/>          ]</span><span id="249c" class="nc lo it mn b gy oa nx l ny nz">def clean_text(s):<br/>    s = s.lower()<br/>    s = utils.to_unicode(s)<br/>    for f in filters:<br/>        s = f(s)<br/>    return s</span><span id="6891" class="nc lo it mn b gy oa nx l ny nz">df2_clean = pd.DataFrame()<br/>df2_clean['notes'] = df2['ep_description'].astype(str).map(lambda x: clean_text(x))</span></pre><p id="d538" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是清洗的结果</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8781a4784b170eec92c072966a862093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*4syeCPTB419ffIgvQDzcPA.png"/></div><p class="oq or gj gh gi os ot bd b be z dk translated">播客剧集描述清理后</p></figure><h2 id="d89f" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">数据转换</h2><p id="cacc" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">LDA 算法计算文档中定义的每个标记的权重。因此，我们需要做的第一件事是创建要加权的令牌，可以按如下方式完成:</p><ul class=""><li id="7d1f" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu">分词</strong>单词</li><li id="2f22" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">从记号中创建<strong class="kk iu"> n-grams </strong>(包含短语作为主题的一部分)，推荐使用二元模型和三元模型</li><li id="bcd5" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">删除<strong class="kk iu">停用词</strong>(以及通用词，在这种情况下我会删除播客和剧集之类的词，这很常见)</li><li id="ed1b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">引理</strong>令牌，让话题进入核心形式</li></ul><p id="df79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述步骤的代码可在<a class="ae lf" href="https://github.com/oliviatan29/spotify-podcast-expl/blob/master/Spotify%20Test%20-%20Topic%20Ingestion.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pa"><img src="../Images/7c745b1a3d21301fdcd0a299532aae9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hD0ReLr7wUtij9MNV7kydg.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">之前的<strong class="bd pb">:播客剧集描述的实际文本</strong></p></figure><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pc"><img src="../Images/7716c125039522a3e312b442d9864e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQKISYJMSMvSvOXlDdtXuQ.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">之后的<strong class="bd pb">:转换后的分词</strong></p></figure><h2 id="f4ed" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">建立模型</h2><p id="3c6f" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">接下来，在预处理完成后，我们创建一个数据字典和语料库来输入到 LDA 模型中。在这个过程中，我们使用单词包来计算文档中每个语料库的频率。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="0107" class="nc lo it mn b gy nw nx l ny nz"># Create Dictionary<br/>id2word = corpora.Dictionary(data_lemmatized)</span><span id="0888" class="nc lo it mn b gy oa nx l ny nz"># Create Corpus<br/>texts = data_lemmatized</span><span id="f5de" class="nc lo it mn b gy oa nx l ny nz"># Term Document Frequency<br/>corpus = [id2word.doc2bow(text) for text in texts]</span><span id="6da7" class="nc lo it mn b gy oa nx l ny nz"># View<br/>print(corpus[:1])</span><span id="3003" class="nc lo it mn b gy oa nx l ny nz"># Human readable format of corpus (term-frequency)<br/>[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]</span></pre><figure class="no np nq nr gt oc gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d685459823b0c8a2ba33d17bc571d222.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*kH8plUVniKMG273nTl_7dw.png"/></div><p class="oq or gj gh gi os ot bd b be z dk translated">队</p></figure><p id="512c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，直接进入模型定义。这里我们在<code class="fe mk ml mm mn b">gensim</code>包中使用 LDA 算法，初始主题数为 20。请注意，这可以进一步调整。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="bc9d" class="nc lo it mn b gy nw nx l ny nz"># Build LDA model<br/>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,<br/>                                           id2word=id2word,<br/>                                           num_topics=20, <br/>                                           random_state=100,<br/>                                           update_every=1,<br/>                                           chunksize=100,<br/>                                           passes=10,<br/>                                           alpha='auto',<br/>                                           per_word_topics=True)</span><span id="d8c0" class="nc lo it mn b gy oa nx l ny nz"># Print the Keyword in the 10 topics<br/>pprint(lda_model.print_topics())<br/>doc_lda = lda_model[corpus]</span><span id="b6c6" class="nc lo it mn b gy oa nx l ny nz"># Compute Perplexity<br/>print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.</span><span id="05bb" class="nc lo it mn b gy oa nx l ny nz"># Compute Coherence Score<br/>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>coherence_lda = coherence_model_lda.get_coherence()<br/>print('\nCoherence Score: ', coherence_lda)</span></pre><p id="f469" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一个较低的<strong class="kk iu">困惑度</strong>分数和一个较高的<strong class="kk iu">连贯性</strong>分数是好的。这里我们发现模型的一致性分数很低，只有 0.28。为了改善这一结果，可以对模型进行超参数调整。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d6e9c5a38cbba1f1903180b34a6ca01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*3J2rK2gkcXXxaeO-US_MNA.png"/></div><p class="oq or gj gh gi os ot bd b be z dk translated">初始 LDA 的一致性分数</p></figure><h2 id="6037" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">超参数调谐</h2><p id="0c67" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">为了进一步改进模型，我们正在进行模拟，以获得给<strong class="kk iu">最佳一致性值</strong>的一些主题。从模拟中，我们看到<strong class="kk iu">的最佳主题数是 8 </strong>，这给出了大约 0.41 的相干值。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pf"><img src="../Images/5b4c92c2afa1c8df17763dc9da073cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zHN3iunNyoTfX74UeKy1wQ.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">主题的最佳数量</p></figure><p id="57a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据这一发现，我们进行了模型再训练(主题的更新数量为 8)，并发现了以下结果。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pg"><img src="../Images/9acbb26be0547fb106a540990645ad7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NxbBFlZ64J43GN4ZUj-3EQ.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">每个主题群的热门主题</p></figure><p id="0e80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于某些群集，更容易推断出正在讨论的主题。比如在<strong class="kk iu">题目 4 </strong>中，看关键词(商业、金钱、公司、产业)，我们可以推断这里讨论的题目与疫情时代的<strong class="kk iu">经济学</strong>有关。</p><p id="64ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进一步探索，我们可以使用“pyLDAvis”来可视化相关的主题和术语。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="2dd0" class="nc lo it mn b gy nw nx l ny nz"># Plotting tools<br/>import pyLDAvis<br/>import pyLDAvis.gensim  # don't skip this<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="73f2" class="nc lo it mn b gy oa nx l ny nz"># Visualize the topics<br/>pyLDAvis.enable_notebook()<br/>vis = pyLDAvis.gensim.prepare(lda_model_2, corpus, id2word)<br/>vis</span></pre><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ph"><img src="../Images/025611027ded19711d11febef941c303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McXE5tkdgQ8ySTeGNr7_Mg.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">主题 5 的 PyLDAvis 可视化</p></figure><h2 id="e238" class="nc lo it bd lp nd ne dn lt nf ng dp lx kr nh ni lz kv nj nk mb kz nl nm md nn bi translated">估价</h2><p id="a7cb" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">我们可以利用 LDA 可视化，进一步探索每个文档的关键主题，以及主题在文本中的分布。</p><figure class="no np nq nr gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pi"><img src="../Images/55a45b4cb36d3c8c53d2f236648c459b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*egA_MAPDpgg3HJcoYb8D9g.png"/></div></div><p class="oq or gj gh gi os ot bd b be z dk translated">主题在文档中的分布，按出现次数排序</p></figure><p id="da57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">综上所述，我们可以将 Spotify 播客中讨论的主题进一步分类如下。</p><pre class="no np nq nr gt ns mn nt nu aw nv bi"><span id="d05f" class="nc lo it mn b gy nw nx l ny nz">Topic 0 : All/Generic news introduction<br/>Topic 1 : Pandemic case updates<br/>Topic 2 : Personal impact (work, family life)<br/>Topic 3 : Encouragement and support<br/>Topic 4 : Financial impact (business and industry)<br/>Topic 5 : Personal stories (let go, life, love)<br/>Topic 6 : Social implications (crises, strike, history)<br/>Topic 7 : Entertainment (sport, game)</span></pre><p id="3954" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的东西，是吧？</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="02f7" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">结论</h1><p id="aa2c" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">相当长的探索，在这篇文章中，我们探索了使用 Spotify API 检索具有 Covid 相关主题的播客剧集。我们发现了一些有趣的见解:</p><ul class=""><li id="53a0" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">播客在三月的第一周开始上升，在五月中旬达到高峰，这与美国的 Covid 病例趋势一致</li><li id="8938" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">大部分都是在<strong class="kk iu">工作日</strong>发布！</li><li id="36ba" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">总集数最高的播客频道:<strong class="kk iu"/>(布道)<strong class="kk iu"/><strong class="kk iu">娱乐</strong>(名人脱口秀)<strong class="kk iu">鼓励</strong></li><li id="6dd2" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">热门播客剧集话题:<strong class="kk iu">新闻介绍</strong> (25%)，<strong class="kk iu">对个人生活的影响</strong> (18%)，<strong class="kk iu">疫情案件更新</strong> (17%)，<strong class="kk iu">个人故事</strong> (13%)，<strong class="kk iu">娱乐</strong> (11%)，<strong class="kk iu">财务影响</strong> (6%)</li></ul><p id="7bbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有兴趣探究并重现这一分析吗？你可以去<a class="ae lf" href="https://github.com/oliviatan29/spotify-podcast-expl" rel="noopener ugc nofollow" target="_blank"> my GitHub </a>仓库查看完整的源代码。</p></div></div>    
</body>
</html>