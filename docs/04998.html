<html>
<head>
<title>Deep Convolutional Neural Networks for Quantum Computers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于量子计算机的深度卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-convolutional-neural-networks-for-quantum-computers-98a6e96ee1d5?source=collection_archive---------30-----------------------#2020-04-30">https://towardsdatascience.com/deep-convolutional-neural-networks-for-quantum-computers-98a6e96ee1d5?source=collection_archive---------30-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a845" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这项工作发表在《2020 年学习表征国际会议论文集》(ICLR)上，详细内容可以查看相关的<a class="ae ki" href="https://openreview.net/pdf?id=Hygab1rKDS" rel="noopener ugc nofollow" target="_blank">论文</a> <strong class="ak"> <em class="kj"> </em> </strong>。</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/41c8b1685e8b6a6ca0271aa14456ab4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPL7-6TYXDmKdua_4OuK5A.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">爱因斯坦和他令人费解的自我。(来源:我自己)</p></figure><p id="96ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">量子计算机会被用来做什么？量子计算机很有希望帮助解决许多领域的难题，包括机器学习。</p><p id="f696" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这项工作中，我们提出了在<strong class="lc iu">量子计算机</strong>上<strong class="lc iu">卷积神经网络</strong> (CNN)的理论实现。我们称这种算法为<strong class="lc iu"> QCNN </strong>，我们证明它可以比 CNN 运行<strong class="lc iu">更快</strong>，并且具有良好的准确性。这是与<a class="ae ki" href="https://scholar.google.ca/citations?user=poOHhXMAAAAJ&amp;hl=en&amp;oi=ao" rel="noopener ugc nofollow" target="_blank">约尔达尼什·克里尼迪斯</a>和<a class="ae ki" href="https://arxiv.org/search/quant-ph?searchtype=author&amp;query=Prakash%2C+A" rel="noopener ugc nofollow" target="_blank">阿努帕姆·普拉卡什</a>的合作作品。</p><p id="7218" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了实现这一点，我们不得不提出卷积乘积的量子版本，找到实现非线性和合并的方法，以及对代表图像的量子态进行断层成像的新方法，这种量子态代表图像保留有意义的信息。</p><h1 id="f5e9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">CNN 和量子计算机的超短介绍</h1><p id="9ac4" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">简而言之，我们可以说<strong class="lc iu">量子物理系统可以描述为 2^n 维的某个希尔伯特空间中的向量</strong>，其中 n 是粒子数。事实上，这些向量代表了许多可能观测值的叠加。</p><p id="47d4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">另一方面，机器学习，特别是神经网络，粗略地利用向量和矩阵来理解或处理数据。<strong class="lc iu">量子机器学习(QML)旨在使用量子系统对矢量进行编码，并通过新的量子算法对其进行学习</strong>。一个关键的概念是，在许多向量上使用量子叠加，我们可以同时处理它们。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mt"><img src="../Images/efde2939cc25a77cd94f412da5d584e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BOmU0aieUzUjAoExFcsRxQ.jpeg"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">谷歌的量子计算机最近实现了“量子至上”。(来源:谷歌)</p></figure><p id="f585" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我不会深入解释量子计算或 QML。更多细节，我邀请新人阅读之前关于<strong class="lc iu">量子 k-means </strong>的帖子(发表在<em class="mu">neur IPS</em>T32】2019):</p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/quantum-machine-learning-a-faster-clustering-algorithm-on-a-quantum-computer-9a5bf5a3061c"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">量子机器学习:量子计算机上的一种快速聚类算法</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">这项工作发表在 2019 年 NeurIPS 会议录上，你可以查看相关论文了解更多细节，以及…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ku my"/></div></div></a></div><p id="8279" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">卷积神经网络(CNN)是一种用于图像分类、信号处理等的流行且有效的神经网络类型。在大多数层中，<strong class="lc iu">卷积乘积</strong>应用于输入，被视为图像或张量。随后通常是一个<strong class="lc iu">非线性和汇集层</strong>。如果你不熟悉，网上有很多教程，尤其是<a class="ae ki" href="https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf" rel="noopener ugc nofollow" target="_blank">这个技术介绍</a>。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi nn"><img src="../Images/4e5f577b1c10472cdef7b154de070c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zCrDsjya63yNJmLvV8l2Q.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">3D 张量输入 X^l (RGB 图像)和 4D 张量核 K^l.之间的卷积积(来源:我自己)</p></figure><h1 id="9c32" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">量子卷积层</h1><p id="0b53" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">在这一章中，我将通过集中在一层来解释什么是量子 CNN。</p><h2 id="b497" class="no lx it bd ly np nq dn mc nr ns dp mg lj nt nu mi ln nv nw mk lr nx ny mm nz bi translated">作为矩阵乘法的卷积乘积</h2><p id="66a4" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">这里的核心思想是，我们可以用矩阵乘法来重新表示卷积积。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oa"><img src="../Images/a67e238cbdb3c34d01ac56369bae5768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBTY9-0Hr_D65BroLzY2Uw.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">X^l 和 K^l 之间的卷积等价于它们的整形形式 A^l 和 F^l.之间的矩阵乘法。输出 Y^l+1 可以被整形为卷积结果 X^l+1.(来源:我自己)</p></figure><p id="ebd8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">该算法首先加载量子叠加矩阵的所有行和列。然后我们使用之前开发的<strong class="lc iu">量子内积估计</strong>来近似输出的每个像素。实际上，这就像只计算一个输出像素(图上的红点)，但在<strong class="lc iu">量子叠加中这样做允许同时计算所有像素</strong>！然后，我们可以同时对它们中的每一个应用非线性。</p><p id="3aed" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">不幸的是，我们<em class="mu">所拥有的</em>只是一个所有像素并行存在的量子态，这并不意味着我们<em class="mu">可以访问</em>所有像素。如果我们打开“量子盒”并查看结果(一个<em class="mu">测量</em>，我们将<strong class="lc iu">每次随机只看到一个输出像素</strong>。在我们打开盒子之前，这里有所有的“漂浮”。就像著名的活死人猫。没错。</p><h2 id="3df6" class="no lx it bd ly np nq dn mc nr ns dp mg lj nt nu mi ln nv nw mk lr nx ny mm nz bi translated">只提取有意义的信息</h2><p id="739f" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">为了处理这个问题，我们引入了一种方法<strong class="lc iu">来只检索最有意义的像素</strong>。事实上，<em class="mu">量子叠加</em>中的每个输出像素都有一个振幅，关系到<strong class="lc iu">被看到的概率</strong>如果我们测量这个系统的话。在我们的算法中，我们强制这个幅度等于像素值。<strong class="lc iu">因此，高值的输出像素更有可能被看到。</strong></p><p id="5d67" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在 CNN 中，输出中的高值像素非常重要。它们表示输入中存在特定模式的区域。通过了解不同图案出现的位置，神经网络<em class="mu">理解</em>图像。因此，这些<strong class="lc iu">高值像素携带有意义的信息</strong>，我们可以丢弃其他像素，希望 CNN 能够适应。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ob"><img src="../Images/cd332736b6a93f6cae8322a8d957dc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzZoK2zEBOsAJQiRcwBXhQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图像上量子效应(噪声、随机性、采样)的小例子。这给人一种直觉，即我们在仅采样高值像素后仍然可以“理解”图像。(来源:我自己)</p></figure><p id="8d39" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">请注意，在对这些输出像素进行采样时，我们可以在存储它们时应用任何类型的<strong class="lc iu">池</strong>(技术细节请参见论文)。我们将这些像素存储在一个传统的存储器中，这样我们可以重新加载它们作为下一层的输入。</p><h2 id="718c" class="no lx it bd ly np nq dn mc nr ns dp mg lj nt nu mi ln nv nw mk lr nx ny mm nz bi translated"><strong class="ak">更快的运行时间</strong></h2><p id="026b" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">传统上，CNN 层需要时间<strong class="lc iu">(</strong>输出大小<strong class="lc iu"> x </strong>内核大小<strong class="lc iu"> ) </strong>。这就是为什么用大量大型内核来训练这些网络会变得昂贵。我们的<strong class="lc iu">量子 CNN </strong>需要时间<strong class="lc iu">(</strong>(<strong class="lc iu">σ</strong>T22】x 输出大小)<strong class="lc iu"> x Q) </strong>其中<strong class="lc iu"> σ </strong>是我们从输出(&lt; 1)中抽取样本的比例，<strong class="lc iu"> Q </strong>代表一束量子精度参数和数据相关参数。不再依赖内核大小(数量和维度),这可以允许更深层次的 CNN。更多细节参见<a class="ae ki" href="https://openreview.net/pdf?id=Hygab1rKDS" rel="noopener ugc nofollow" target="_blank">文件</a>。</p><h1 id="9fbc" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">量子反向传播</h1><p id="3d81" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">有了这个量子 CNN 的设计，我们现在想用量子算法来训练它。训练包括按照梯度下降规则更新核参数。一个更快的量子算法也可以在这里找到，它几乎等同于通常的梯度下降，但有一些额外的误差。</p><h1 id="81af" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实验</h1><p id="6b40" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">QCNN 和量子反向传播看起来不错，但是隐含了大量的近似、噪声和随机性。尽管有这些人工制品，CNN 还能学习吗？在学习分类手写数字(MNIST 数据集)的任务上，我们比较了小型经典 CNN 的训练和 QCNN 的模拟。这表明 QCNN 可以以相似的精度学习。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oc"><img src="../Images/716fffe2e6aff1bf21afb18973e96496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ree2Fft7eOdv4aOHIVnFqg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">量子和经典 CNN 训练曲线的比较。<strong class="bd od"> σ </strong>是每层之后从输出中抽取的高值像素的比例。除了太小的<strong class="bd od"> σ </strong>之外，QCNN 可以以良好的准确度学习。注意，这个数值模拟很小，只给出直觉，不是证明。(来源:我自己)</p></figure><h1 id="af23" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="e00d" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">在这项工作中，我们设计了第一个量子算法，通过引入量子卷积积和一种新的检索有意义信息的方法，几乎精确地复制了任何经典的 CNN 结构。它可以允许一个明显更快的 CNN，有更深更大的输入或内核。我们还开发了一个量子反向传播算法，并模拟了整个训练过程。</p><p id="1c24" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">开放性问题:</strong>我们能在更大的架构上训练 QCNN 吗？用其他数据集？当然，当一台完整的量子计算机准备就绪时，量子 CNN 会有多好？</p><h1 id="99da" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><p id="ad43" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">[1] J. Allow，<strong class="lc iu"> I. Kerenidis </strong>等.<a class="ae ki" href="https://arxiv.org/abs/1812.03089" rel="noopener ugc nofollow" target="_blank">前馈神经网络的量子算法</a>(2018)</p><p id="2041" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[2] Y. LeCun，Y. Bengio 等.<a class="ae ki" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">基于梯度的学习应用于文档识别</a>。IEEE 会议录(1998)</p><p id="e694" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[3] G. Brassard，P. Hoyer 等.<a class="ae ki" href="https://arxiv.org/abs/quant-ph/0005055" rel="noopener ugc nofollow" target="_blank">量子振幅放大与估计</a><em class="mu">AMS 当代数学丛书</em>。(2000)</p><p id="ed66" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[4] <strong class="lc iu"> I. Kerenidis </strong>，<strong class="lc iu"> J. Landman </strong>等.<a class="ae ki" href="https://papers.nips.cc/paper/8667-q-means-a-quantum-algorithm-for-unsupervised-machine-learning" rel="noopener ugc nofollow" target="_blank"> q-means:一种无监督机器学习的量子算法。</a>”载于<em class="mu">神经信息处理系统进展(NeurIPS) </em> (2019)。</p><p id="1e31" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[5] S. Chakraborty，A. Gilyén 等人在 ICALP  (2019)的<em class="mu">会议录</em> <em class="mu">中的<a class="ae ki" href="http://arxiv.org/abs/1804.01973]" rel="noopener ugc nofollow" target="_blank">分组编码矩阵幂的力量:通过更快的哈密顿模拟</a>改进回归技术</em></p></div></div>    
</body>
</html>