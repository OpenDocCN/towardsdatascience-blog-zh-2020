<html>
<head>
<title>Novelty Detection with Local Outlier Factor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有局部异常因子的新奇检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/novelty-detection-with-local-outlier-factor-4867e89e4f91?source=collection_archive---------25-----------------------#2020-06-20">https://towardsdatascience.com/novelty-detection-with-local-outlier-factor-4867e89e4f91?source=collection_archive---------25-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2865" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">检测新数据是否是异常值</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48109b4e2f255a33e4937cc304b91c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A56gmB0IwojOUq80"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">兰迪·法特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="027a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与<strong class="lb iu">异常检测</strong>相比，新奇检测</strong>可能是一些人更难听到的术语。如果离群点检测旨在发现数据集中的异常或显著不同的数据，则新颖性检测旨在确定新的或未知的数据是否是离群点。</p><p id="e19e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新颖性检测是一种半监督分析，因为我们将训练未被异常值污染的训练数据，并且我们感兴趣的是通过使用训练的模型来检测<strong class="lb iu">新</strong>观察值是否是异常值。在这种情况下，离群值也称为新奇值。</p><p id="3ded" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将解释如何使用局部异常因子(LOF)进行新颖性检测。让我们开始吧。不过，如果你想跳过所有的理论，只是想接触编码部分，就跳过下一节吧。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cbd4" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">本地异常因素(LOF)</h1><p id="e678" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">局部异常因子或LOF是由Breunig等人提出的一种算法。(2000年)。概念很简单；该算法试图通过测量给定数据点相对于其邻居的局部偏差来发现异常数据点。在这个算法中，LOF会给出一个分数，告诉我们的数据是否是异常值。</p><blockquote class="na nb nc"><p id="6fc5" class="kz la mz lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">LOF(k)～1意味着<strong class="lb iu">作为邻居的相似密度。</strong></p><p id="a244" class="kz la mz lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">LOF(k) &lt; 1 means <strong class="lb iu">密度高于邻居(内/非外值)。</strong></p><p id="5d25" class="kz la mz lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">LOF(k) &gt; 1意味着<strong class="lb iu">密度低于邻居(离群值)</strong></p></blockquote><p id="c5e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上式中，我已经给你介绍了一个<em class="mz"> k </em>参数。在LOF算法中，局部性由k个最近邻给出，其距离用于估计局部密度。然后，该距离将用于测量局部偏差并查明异常。</p><p id="465a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mz"> k </em>参数是我们为获取一个<strong class="lb iu"> k距离</strong>而设置的参数。k距离是一个点到它的第k个邻居的距离。如果<em class="mz"> k </em>为4，k-距离将是一个点到第四个最近点的距离。距离本身是我们可以选择的距离计算的度量。通常它是“欧几里得”，但你可以选择其他距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/00c2d1bec96dffb6ed553c91da4605e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*U9WOFtoNsuQfpmVhCYnjjQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k = 4，该点到达第四个最近的点(由作者创建的图像)</p></figure><p id="7498" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想知道，我们的数据集中的每个数据点都将被测量k距离，以便测量下一步。</p><p id="d675" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">距离现在被用来定义我们所说的<strong class="lb iu">可达性距离</strong>。这个距离度量是两点距离的最大值或第二点的k距离。形式上，可达性距离的度量如下。</p><blockquote class="na nb nc"><p id="cc6a" class="kz la mz lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">可达性距离k(A，B)= max { k-距离(B)，d(A，B)}</p></blockquote><p id="9ce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中A和B是数据点，A和B之间的可达性距离要么是k-距离B的最大值，要么是A和B之间的距离。如果A点在B点的k-邻居内，那么可达性距离k(A，B)将是B的k-距离。否则，它将是A和B之间的距离<em class="mz">。</em></p><p id="c593" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将通过使用可达性距离来测量<strong class="lb iu">局部可达性密度</strong> (lrd)。测量lrd的公式估计如下。</p><blockquote class="na nb nc"><p id="31f8" class="kz la mz lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">lrd(A) = 1/(sum(可到达距离(A，i))/k)</p></blockquote><p id="5f46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中I是点A的邻居(从其邻居可以“到达<em class="mz">A</em><em class="mz">的点)，为了获得点A的lrd，我们计算A到其所有k个最近邻居的可达性距离之和，并取其平均值。lrd是平均值的倒数。LOF概念完全是关于密度的，因此，如果到下一个邻居的距离较长，则特定点位于较稀疏的区域。因此，密度越低——反之亦然。</em></p><p id="0a45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，lrd表示该点要移动多远才能到达下一个点或点簇。lrd越低意味着点必须走得越长，并且组越不密集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a5b2259c4c786f80a564cff00fcf7431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*j7oo9AUhiWOh6pLuYTpMkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">中心点的lrd是到其最近邻居的平均可达性距离，这些最近邻居是除(1，1)点之外的左下角的点。但是，这些邻居有其他lrd，因为它们的最近邻居不包括中心点。(图片由作者创建)</p></figure><p id="dd1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，使用以下公式将本地可达性密度与其他邻居的可达性密度进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bf05f8089e7f9cc90f64b134b21499cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*hrV1HW51W7YQtGcTUp2DmQ.png"/></div></figure><p id="5dce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将每个点的lrd与其k个邻居的lrd进行比较。LOF是A的相邻点的lrd与A的lrd的平均比率。如果该比率大于1，则点A的密度小于其相邻点的密度。这意味着，从点A到下一个点或点簇需要比从A的邻居到下一个邻居的距离长得多的距离。</p><p id="3dab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LOF表示该点相对于其相邻点的密度。如果一个点的密度小于其相邻点的密度(LOF大于1)，则该点被视为异常值，因为该点比密集区域更远。</p><p id="68f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LOF算法的优点是，即使在异常样本具有不同基础密度的数据集上，它也能很好地执行。这是因为LOF不是样本有多孤立，而是样本与周围邻居相比有多孤立。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c5d8" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">通过LOF进行新颖性检测</h1><p id="6fa8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将使用由<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>提供的LOF来帮助我们训练我们的模型。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="f8a0" class="no md it nk b gy np nq l nr ns">#Importing the module</span><span id="1238" class="no md it nk b gy nt nq l nr ns">import pandas as pd<br/>import seaborn as sns<br/>from sklearn.neighbors import LocalOutlierFactor</span><span id="f28f" class="no md it nk b gy nt nq l nr ns">#Importing the dataset. Here I use the mpg dataset as an example<br/>mpg = sns.load_dataset('mpg')</span></pre><p id="f8fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为介绍，在使用模型进行新颖性检测之前，让我们尝试使用LOF作为离群点检测模型。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="5189" class="no md it nk b gy np nq l nr ns">#Setting up the model. K is set by passing the n_neighbors parameter with integer. 20 is often considered good already to detect an outlier. By default the distance metric is Euclidean distance.</span><span id="0927" class="no md it nk b gy nt nq l nr ns">lof = LocalOutlierFactor(n_neighbors = 20)</span><span id="e8d0" class="no md it nk b gy nt nq l nr ns">#Training the model, I drop few columns that was not a continuous variable</span><span id="892e" class="no md it nk b gy nt nq l nr ns">mpg['lof'] = lof.fit_predict(mpg.drop(['cylinders', 'model_year', 'origin', 'name'], axis = 1))</span><span id="07da" class="no md it nk b gy nt nq l nr ns">#Getting the negative LOF score</span><span id="d50b" class="no md it nk b gy nt nq l nr ns">mpg['negative_outlier_factor'] = lof.negative_outlier_factor_</span><span id="3307" class="no md it nk b gy nt nq l nr ns">mpg</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/8a68dde051dae88444c82e70b7972035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R43yR0VBadi0vCsGppZC2Q.png"/></div></div></figure><p id="3016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是结果，我们得到了LOF分类和负LOF分数。如果lof等于1，那么它被认为是一个inlier如果它是-1，那么它是一个离群值。让我们试着得到所有的离群数据。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="5321" class="no md it nk b gy np nq l nr ns">mpg[mpg['lof'] == -1]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/f6a494d5628fa18edf0be09b571841d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPc6BISN1WgGYGjG_IOXaw.png"/></div></div></figure><p id="6a4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">污染或异常值取决于LOF分数。默认情况下，如果negative_outlier_score的分数小于-1.5，它将被视为异常值。</p><p id="d75d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们尝试使用模型进行新颖性检测。我们需要再次训练模型，因为我们需要传递一个额外的参数。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="52f0" class="no md it nk b gy np nq l nr ns">#For novelty detection, we need to pass novelty parameter as True</span><span id="5416" class="no md it nk b gy nt nq l nr ns">lof = LocalOutlierFactor(n_neighbors = 20, novelty = True)<br/>lof.fit(mpg.drop(['cylinders', 'model_year', 'origin', 'name'], axis = 1))</span></pre><p id="3494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只训练模型，我们根本不会使用训练数据来预测数据。在这里，我们将尝试预测新的看不见的数据。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="f536" class="no md it nk b gy np nq l nr ns">#Predict a new random unseen data, the dimension must match the training data</span><span id="e9ea" class="no md it nk b gy nt nq l nr ns">lof.predict(np.array([109, 310, 190, 3411, 15]).reshape(1,-1))</span><span id="0314" class="no md it nk b gy nt nq l nr ns">Out: array([1])</span></pre><p id="aca5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出可能是1或-1，这取决于它是否是一个异常值。如果你想得到负的LOF分数，我们也可以用下面的代码来实现。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="1f30" class="no md it nk b gy np nq l nr ns">lof.score_samples(np.array([109, 310, 190, 3411, 15]).reshape(1,-1))</span><span id="789a" class="no md it nk b gy nt nq l nr ns">Out: array([-1.34887042])</span></pre><p id="ee78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出将是数组形式的分数。就是这样，我已经向你们展示了如何用LOF做一个简单的新奇感检测。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b2df" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="a717" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">新奇性检测是检测新的看不见的数据是否是异常值的活动。局部异常因子是一种用于异常检测和新奇检测的算法。这取决于我们传递的<em class="mz"> k </em>参数。通常情况下，k = 20是可行的，但是如果您觉得数据中有大量的异常值，您可以增加这个数值。</p><p id="0df1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望有帮助。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c922" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">如果你喜欢我的内容，并想获得更多关于数据或作为数据科学家的日常生活的深入知识，请考虑在这里订阅我的<a class="ae ky" href="https://cornellius.substack.com/welcome" rel="noopener ugc nofollow" target="_blank">时事通讯。</a></h1><blockquote class="nw"><p id="c045" class="nx ny it bd nz oa ob oc od oe of lu dk translated">如果您没有订阅为中等会员，请考虑通过<a class="ae ky" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener">我的推荐</a>订阅。</p></blockquote></div></div>    
</body>
</html>