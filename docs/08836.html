<html>
<head>
<title>Blender Bot — Part 2: The Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">搅拌机机器人——第二部分:变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blender-bot-part-2-the-transformer-2e4d960b149f?source=collection_archive---------23-----------------------#2020-06-25">https://towardsdatascience.com/blender-bot-part-2-the-transformer-2e4d960b149f?source=collection_archive---------23-----------------------#2020-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b27d5469615f15b195cb9ca94964d088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ml1zhFpBFFWyfbjT99rOw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">科迪·恩格尔在Unsplash上的照片</p></figure><p id="1861" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">脸书开源聊天机器人“Blender”打破了之前由谷歌“Meena”创造的所有记录。在本帖中，我们将回顾构成Blender核心的<strong class="ki iu">多编码器</strong>转换器架构。</p><p id="0b61" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在<a class="ae kf" rel="noopener" target="_blank" href="/blender-bot-part-1-the-data-524beaedde65"> TDS </a>上阅读本系列的第一部分，在那里我们已经查看了训练聊天机器人的数据集。</p><p id="5bd1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设读者对注意力、变形金刚、伯特和生成性语言模型有预先的了解，我将继续前进。</p><h1 id="77a6" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">简介:</h1><p id="bebf" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在看到如何在Blender的上下文中使用Poly-Encoder之前，我们将首先独立地理解它们。在预训练和微调搅拌机中使用的数据集和(假)训练任务(在第1部分中有详细解释)不应该与我下面将要解释的细节混淆。这里给出的实验设置是为了在一般设置中理解一个名为“<strong class="ki iu">多句子评分</strong>的特定任务以及为该任务训练的编码器架构。然后，在为此任务训练的编码器架构中，我们将看到多编码器是如何优越的。</p><h1 id="21fa" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">任务:</h1><p id="e184" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">多句子评分在输入和输出序列之间进行成对比较。给定一个输入序列，我们对一组候选标签进行评分。</p><p id="5099" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里开始，我们将用<strong class="ki iu">【INPUT，LABEL】</strong>来表示输入输出对。</p><p id="cf5b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标是从有限的候选标签列表中找到最佳标签。所使用的编码器是BERT-Base，在前向网络中具有12个编码器块、12个注意头和768个隐藏神经元。</p><h2 id="a148" class="mh lf it bd lg mi mj dn lk mk ml dp lo kr mm mn ls kv mo mp lw kz mq mr ma ms bi translated">预培训:</h2><p id="918a" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">为此任务进行了两个版本的预培训:</p><ol class=""><li id="bfab" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld my mz na nb bi translated">像伯特一样预先训练，在多伦多图书语料库和维基百科上。这里的[输入，标签]可以认为是[句子A，句子B]。</li><li id="6c27" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">接受过Reddit上公共领域社交媒体对话的预培训。这里的[输入，标签]可以理解为[上下文，下一句]</li></ol><h2 id="fa0f" class="mh lf it bd lg mi mj dn lk mk ml dp lo kr mm mn ls kv mo mp lw kz mq mr ma ms bi translated">假训练任务:</h2><p id="8494" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">训练任务与BERT预训练中使用的任务相同。</p><ol class=""><li id="c94b" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld my mz na nb bi translated"><strong class="ki iu"> MLM:屏蔽语言模型:</strong>这里，一定百分比的输入标记被随机屏蔽(使用[MASK]标记)。接下来的任务是学习预测屏蔽的令牌。</li><li id="8eee" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="ki iu"> NSP:下一句预测:</strong>这里给出两个句子A和B，任务是说B是否跟随A？(带负采样)。负抽样是通过从数据集中随机抽取一个句子作为B来实现的，50%的情况下。</li></ol><p id="16d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有点跑题。为了记住BERT中这些预训练任务的性质，我使用的一个技巧是与生成Word2Vec嵌入时使用的假训练任务进行直接比较，即:1) CBOW 2) Skip-Gram。如果你能回忆起来，在CBOW(连续单词袋)中，给定一个上下文，任务是预测目标单词——类似于MLM任务。在Skip-Gram模型中，给定目标词，预测上下文= &gt;但是我们改变数据集而不是预测上下文/相邻词，任务变成:给定目标词和另一个词-&gt;预测另一个词是否是目标词的邻居(二进制分类问题)。由于初始数据集仅由目标单词和它们上下文中的单词形成，修改后的数据集现在仅包含正面的例子。所以我们通过负采样引入噪声。非常非常类似于伯特的NSP任务。(如果你认为在BERT和单词嵌入的训练任务之间进行这样的比较有任何不一致之处，请在评论中告诉我。谢谢！)</p><h2 id="6f20" class="mh lf it bd lg mi mj dn lk mk ml dp lo kr mm mn ls kv mo mp lw kz mq mr ma ms bi translated">微调:</h2><p id="a2e0" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">该模型在ConvAI2数据集上单独进行了微调，从而鼓励他们学习“个性”特征，并在Ubuntu聊天日志上帮助他们学习“领域知识/专业技能”。</p><h1 id="ff87" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">架构:</h1><p id="50a5" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们将看到解决“多句子评分”任务的3种编码器架构，即，</p><ol class=""><li id="b1b1" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld my mz na nb bi translated">双编码器</li><li id="7cad" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">交叉编码器</li><li id="b7f1" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">多元编码器</li></ol><p id="ef8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个架构在推理过程中的性能是通过预测质量和预测速度来衡量的。</p><p id="b462" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在继续之前，重要的是要记住这是一个<strong class="ki iu">检索而不是生成任务</strong>:我们只需要从一组固定的候选标签中检索一个正确的标签。</p><h1 id="4c60" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">双编码器:</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/199780cde2a4dc6a88ad55c798259730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vc2OFos5MAlJsdGTWJ5JrQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nm">参考文献[1]中的双编码器架构</strong></p></figure><p id="623f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在双编码器中，对输入和标签分别进行自我关注。这不过是向量空间模型的更一般的概念。这种架构的优点是在推理过程中速度更快，因为我们可以预先计算和缓存大型固定候选标签集的编码。这是可能的，因为标签是单独编码的，与输入上下文无关。</p><ul class=""><li id="2ae4" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">输入和标签都被一个特殊的标记包围。这类似于BERT中的[CLS]标记，它捕获了整个句子的特征。</li><li id="cbdf" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">输入到编码器的嵌入是标记嵌入+段嵌入+位置嵌入的组合。段嵌入一般用来说一个令牌是属于A句还是B句(在BERT的上下文中)。因为输入和标签在这里是分开编码的，所以在两种情况下，段嵌入都是‘0’。</li><li id="9c8e" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">将输入和候选标签分别映射到一个公共特征空间。在所示的公式中，T1和T2是两个独立的变压器(编码器)。</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/d84c8d1fea70ffdbe3fe6387ca1c6f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JnZ8PZ2_hUbzIIyzf1A9lw.png"/></div></div></figure><ul class=""><li id="1008" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">编码器在对输入令牌嵌入执行自我关注后，给出每个令牌的编码器表示，如:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/dd7be616ee84734113ecca64bde58ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*I59GNRD3sbT4cccwyXk6_Q.png"/></div></figure><ul class=""><li id="eac5" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">然后使用reduce函数(<strong class="ki iu"> red </strong>)将其简化为单个嵌入表示。<strong class="ki iu">减少</strong>功能可以是以下任何一种:</li></ul><p id="9816" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">-&gt;它可以采用第一个令牌的表示形式。这是对应于特殊令牌的表示</p><p id="c15a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">-&gt;或者我们可以取所有输出嵌入的平均值</p><p id="e89c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">-&gt;或者我们可以取第一个“m”的平均值(m <n where="" n="" token="" length="" output="" embeddings=""/></p><ul class=""><li id="aa69" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">Once the INPUT and LABEL are represented thus in a common vector space, measure the similarity between them using standard dot product or any other non-linear function.</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/171b6dcf4026d0d06f99766ef713eaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*kU-8h1JkHHMMg-fN7cBkjw.png"/></div></figure><ul class=""><li id="e760" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">We then minimize the Cross Entropy loss function, where the logits look like:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/197ba17f99e95345d3c9cfeb19396be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xBIqfne8BSNRo3-nOm_IA.png"/></div></div></figure><h1 id="a8b5" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Cross-Encoder:</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/b3d65711841ce6d3860a27c8155da1bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IAb1AGON_5WWWb3SQkiI5w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nm">参考文献[1] </strong>中的交叉编码器架构)</p></figure><ul class=""><li id="a33a" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">这里，输入和标签被连接起来，并且在输入和标签的整个序列之间执行完全的自我关注。也就是说，输入的每一个标记都将关注标签的每一个标记，反之亦然。这导致了输入和标签之间丰富的交互。</li><li id="93b9" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">即使在这里，输入和标签都被一个特殊的标记包围。</li><li id="d1b5" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">同样，输入到编码器的嵌入是令牌嵌入+段嵌入+位置嵌入的组合。因为输入和标签是组合的，所以段嵌入对于输入令牌是“0 ”,对于标签令牌是“1”。</li><li id="c46b" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">交叉编码器比双编码器提供更高的精度，因为输入和标签之间的完全双向关注。同时，它们在推理过程中非常慢——因为每个候选标签都应该与输入上下文连接，而不能像双编码器那样单独编码。因此，候选嵌入不能被预先计算和缓存。当候选标签的数量很大时(正如在大多数真实场景中一样)，交叉编码器不可伸缩。</li><li id="6e0f" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">在自我关注之后，转换器给出所有输入标记的编码器表示。通过采用对应于第一个标记(即特殊标记)的嵌入，我们将其简化为单个表示。然后通过线性投影将该嵌入向量转换成标量分数。这两个步骤如下所示:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/312e629df467ae41120ce98c6b2530fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsgLnxdsE8WAmVKTw_xYYg.png"/></div></div></figure><ul class=""><li id="070d" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">这里的训练目标也是最小化由逻辑给出的交叉熵损失函数:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/197ba17f99e95345d3c9cfeb19396be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xBIqfne8BSNRo3-nOm_IA.png"/></div></div></figure><ul class=""><li id="44b5" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">其中'<em class="nu"> cand1' </em>是正确的候选，其他的是从训练集中取出的否定。这里的一个问题是，在双编码器中，我们可以使用批中的其他标签作为负训练样本-这里我们不能这样做。我们使用训练集中提供的外部否定。因为它计算量大，所以交叉编码器的内存批量也很小。</li></ul><h1 id="411a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">多元编码器:</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/fd2349d80fb78b8d70dc7a0b481e8fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*80fapGuuD18__Q2KNPflIQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nm">参考文献[1]中的多编码器架构</strong></p></figure><ul class=""><li id="794d" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">多编码器结合了双编码器和交叉编码器的优点。因此，在推理过程中，它比交叉编码器更快，比双编码器更准确。</li><li id="c1f2" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">候选标签被单独编码。</li><li id="b909" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">给定输入上下文，如:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/19a0bf3a09ac6c66e07f590829bb9d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*oFEQPs0Vo4NpuCM1Vl_1Xw.png"/></div></figure><p id="d1f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们执行3种类型的注意，如下所述:</p><ul class=""><li id="d77e" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">自我关注输入上下文的标记，我们得到:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/74d7be61657cf250cb1470ba555895cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*ZjEx9xL24mchlt8MZJaqhA.png"/></div></figure><ul class=""><li id="0dbf" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">第二，我们学习“m”码(或自我关注的说法中的查询)，其中m &lt; N (N being the length of the INPUT). The number of codes to be learnt, ‘m’, is a hyperparameter. Each code Ci attends over all the outputs of the previous Self-Attention. The ‘m’ codes are randomly initialized.</li><li id="a0c7" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nn mz na nb bi translated">We first get the Attention weights (w’s) by performing a dot-product attention (or a multiplicative attention in general) between the ‘m’ codes — which serve as the “Queries”, and the previous Self-Attention outputs (Out’s)—which serve as the “Keys”. Then use these attention weights to get a weighted sum of the previous Self-Attention outputs(Out’s) — which serve as the “Values”.</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/8059bc737ce37277281f626aaf13ac22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGjKRnXebD102EZtzdatjw.png"/></div></div></figure><ul class=""><li id="47f3" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">Think about why we are doing this kind of an Attention mechanism here. In a Bi-Encoder, the candidate label does not attend over the tokens of the input context. A Cross-Encoder on the other extreme, makes the candidate label attend over every token of the input context. Somehow in the Poly-Encoder we are trying to find a middle ground, by making the candidate label embedding attend over not the entire input context, but over a subset of features learnt from the input context.</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/0f69385f38b4fc54a16adda6952e538e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oCyEkKCCKCX2B-G6xl0gZQ.png"/></div></div></figure><ul class=""><li id="9be6" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">The third kind of attention (alluded to in the previous paragraph) is between the ‘m’ global features of the Input Context and the embedding of the Candidate Label.</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/eb31a0f66b498252045f49dd1519dbbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1gO64sQtnbIABQ3Kx-sog.png"/></div></div></figure><ul class=""><li id="bbf1" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">Now we compute the Similarity score between the Input Context embedding and the Candidate Label embedding as:</li></ul><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/dcc023f71e517647306a0704356cb6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*48S-7rSi7FbsLDIp8pXN-Q.png"/></div></div></figure><ul class=""><li id="0242" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nn mz na nb bi translated">Once again, the training objective here too is to minimize the Cross-Entropy loss function given by the logits as before.</li></ul><p id="099c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">We saw three different Encoder architectures for the task of “Multi-Sentence Scoring” and saw how the Poly-Encoders were better. In the next part, we will see how the Poly-Encoders are used in the Blender and also about the different Model Architectures and training objectives. We will also touch upon the Evaluation methods used to compare the performance of Blender with that of the other Chatbots.</p><p id="5f59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">注意:</strong>上面所有的符号、公式和编码器框图都与参考文献中提到的原始论文中使用的相同。[1].</p><h1 id="fd29" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考资料:</h1><ol class=""><li id="e774" class="mt mu it ki b kj mc kn md kr ob kv oc kz od ld my mz na nb bi translated">多编码器变压器:【https://arxiv.org/abs/1905.01969 T4】</li><li id="f979" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">https://arxiv.org/abs/1810.04805</li><li id="72a3" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">变形金刚:<a class="ae kf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></li></ol></div></div>    
</body>
</html>