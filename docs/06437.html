<html>
<head>
<title>How I implemented explainable movie recommendations using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何使用Python实现可解释的电影推荐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-implemented-explainable-movie-recommendations-using-python-7aa42a0af023?source=collection_archive---------46-----------------------#2020-05-22">https://towardsdatascience.com/how-i-implemented-explainable-movie-recommendations-using-python-7aa42a0af023?source=collection_archive---------46-----------------------#2020-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="57bb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我还测试了用户是否真的喜欢它们。这是结果。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4c3b6f55d81b217efda79f77364f1dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5nsLkBYLmxxWuYzTN-jMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">如何在应用程序中向用户显示建议及其解释。图片作者:Ville Kuosmanen</p></figure><p id="78bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">这篇文章是我关于可解释建议系列文章的第二部分，基于我的</em> <a class="ae ls" href="https://github.com/villekuosmanen/SHProject/raw/master/dissertation.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> BSc论文</em> </a> <em class="lr">。</em> <a class="ae ls" rel="noopener" target="_blank" href="/explainable-recommendations-why-opening-black-boxes-matters-bd5754af63a2"> <em class="lr">第一部分</em> </a> <em class="lr">介绍了可解释建议的概念，而</em> <a class="ae ls" rel="noopener" target="_blank" href="/what-is-the-radical-content-problem-and-does-your-recommender-system-suffer-from-it-7fe017f9a8b1"> <em class="lr">第三部分</em> </a> <em class="lr">讨论了事后可解释性在数据科学中的应用。</em></p><p id="1567" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本系列的前一篇文章讨论了为什么提高推荐系统的可解释性很重要。这不是一个教程，而是我在实现一个带有可解释推荐的电影推荐服务时所采用的方法的概述。如果你想进一步探索，你可以阅读<a class="ae ls" href="https://github.com/villekuosmanen/SHProject/raw/master/dissertation.pdf" rel="noopener ugc nofollow" target="_blank">全文</a>或者深入<a class="ae ls" href="https://github.com/villekuosmanen/WebAppSHProject" rel="noopener ugc nofollow" target="_blank">前端</a>或者<a class="ae ls" href="https://github.com/villekuosmanen/SHProject/raw/master/dissertation.pdf" rel="noopener ugc nofollow" target="_blank">后端</a>代码库(它们是开源的！).我选择首先使用矩阵分解算法SVD实现一个黑盒推荐系统，然后实现两个事后解释器来生成对推荐的解释。然后通过一个模拟电影推荐服务的web应用程序来测试添加解释的效果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/75682cfc3230168326166d239e3ca75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aimi8b2aAQfeNozdcFF0kg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">描述用户研究中操作顺序的流程图。使用web应用程序，用户首先对电影进行评级。推荐系统然后为他们生成推荐，解释器为推荐生成解释，然后显示给用户。图片作者:Ville Kuosmanen</p></figure><h2 id="d77a" class="lu lv iq bd lw lx ly dn lz ma mb dp mc le md me mf li mg mh mi lm mj mk ml mm bi translated">设计和实施</h2><p id="a720" class="pw-post-body-paragraph kv kw iq kx b ky mn jr la lb mo ju ld le mp lg lh li mq lk ll lm mr lo lp lq ij bi translated">推荐系统是使用<a class="ae ls" href="http://surpriselib.com/" rel="noopener ugc nofollow" target="_blank">惊喜库</a>用Python实现的。用于该问题的数据集是公开可用的MovieLens数据集，由真实用户的电影评级组成[1]。该数据集广泛用于推荐系统研究，由于CF不是特定领域的，因此模型和算法通常会推广到电影评级之外的其他领域。当开发推荐系统时，使用包含100，000个评级的开发数据集来减少训练时间；2000万评级基准集用于评估和使用研究。在训练潜在因素模型之前，使用随机的75%-25%等级训练-测试分割，将数据分割成训练和测试数据。</p><p id="f79a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在训练之后，推荐系统可以用于预测训练集中的项目和用户的评级。这对于静态评估来说已经足够了:然而，使用系统进行的用户研究需要动态的推荐。为只有几个评分的新用户生成个性化推荐是不可能的:模型首先需要学习用户的潜在因素。这可以通过完全重新训练模型来完成，但是，这种方法在实时系统中是不可行的，因为针对数百万个评级训练模型在计算上是昂贵的，并且需要在每个新的评级时进行。因为现有的模型“几乎是正确的”,并且可以作为添加用户的良好起点，所以可以根据SVD算法的底层实现以多种方式优化该方法。例如，一些基于梯度下降的系统可以用原始模型的权重(这里是潜在因子)进行初始化，这将允许算法更快地收敛。然而，Surprise的SVD在运行其梯度下降时使用固定数量的历元，并且直到所有历元都完成时才停止。因此，为SVD模型设计了一个新的操作，它向模型添加了一个新用户。这个操作只训练新用户的潜在因子，对于物品和其他用户的因子不变。</p><p id="39ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我选择添加的解释是基于关联规则和影响，并用Python实现的。关联规则解释器的实现遵循了之前提出的方法[2]，旨在通过显示数据集中的规则来解释推荐，该规则描述了之前观看的电影是什么导致了它被推荐。影响解释旨在显示哪些先前观看的电影对推荐影响最大。使用的影响方法是新颖的，但它产生了与先前提出的快速影响分析相同的解释[3]。它通过比较在训练集中有和没有每个先前观看的电影的情况下推荐电影的预测评级来工作。这是通过在没有这些单独数据点的情况下反复重新训练模型来实现的。通常情况下，这将是不可能的昂贵，但由于为增加新用户而开发的优化方法，它可以有效地完成。</p><h2 id="19ff" class="lu lv iq bd lw lx ly dn lz ma mb dp mc le md me mf li mg mh mi lm mj mk ml mm bi translated">估价</h2><p id="72a3" class="pw-post-body-paragraph kv kw iq kx b ky mn jr la lb mo ju ld le mp lg lh li mq lk ll lm mr lo lp lq ij bi translated">进行了一项用户研究，以测试解释类型对推荐的测量可信度和说服力的影响，定义见[4]。为此，构建了一个模拟电影推荐服务的web应用程序。前端使用<a class="ae ls" href="https://reactjs.org/" rel="noopener ugc nofollow" target="_blank"> React </a>构建，后端REST API使用<a class="ae ls" href="https://palletsprojects.com/p/flask/" rel="noopener ugc nofollow" target="_blank"> Flask </a>构建。除了提供对推荐系统和解释器的访问，API还通过<a class="ae ls" href="https://www.themoviedb.org/documentation/api?language=en-US" rel="noopener ugc nofollow" target="_blank">电影DP API </a>加载关于真实电影的数据(例如标题、海报和年龄分级)。然后，web服务被部署到我的大学web服务器上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f95f3a3ee798c089ed407a27644e94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oem1Bqkq534KxmMHFPUqKA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">web应用程序的电影分级屏幕的屏幕截图，其中有几部电影已经分级(屏幕右侧)。图片作者:Ville Kuosmanen</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/a345e6d63290c1f9db1ec5e63a349216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeMyCpzHxwWQNtHzSt7K4Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">影响解释如何在web界面中呈现给用户。条形的色调显示电影的影响是积极的还是消极的，其宽度显示效果的强度。图片作者:Ville Kuosmanen</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/4419272e070c8b9e316a9e367b4ced46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0ixVPmmcNQcpwovlDENtw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">关联规则解释如何在web界面中呈现给用户。先行项显示在规则的左侧，而结果项显示在右侧。图片作者:Ville Kuosmanen</p></figure><p id="64bb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了这两个解释者，用户研究还包含了一个基线解释“推荐这部电影是因为你和喜欢它的用户相似”。这项研究首先要求用户对他们过去看过的十部电影进行评级，然后向他们展示每一类的推荐和解释。41名用户参与了这项研究，在有利于关联规则解释者的解释类型之间，在说服力(p=0.008)和信任度(p=0.001)方面观察到了统计学上的显著差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/fe25c57c7aee0f20374cfe0ee3732cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJ2otxSMba45VE10OdqLiw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">针对每种类型的解释，在用户研究中测量的用户评价的兴趣(黄色)和信任(灰色)的均值条形图。还显示了误差条(即95%置信区间)。图片作者:Ville Kuosmanen</p></figure><p id="ffae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解释生成器也在各种离线实验中进行了测试。最重要的是，关联规则解释器被发现受到低模型保真度的困扰，模型保真度是衡量可以被解释的建议的份额的指标[2]。低模型保真度导致用户研究中的解释有轻微的选择偏差:关联规则解释被添加到最受欢迎的电影的推荐中(因为它们在挖掘的关联规则中最常见)。令人惊讶的是，这将导致效果的增加——一般来说，用户应该更喜欢对他们更有针对性的推荐。研究平台的“虚假”性质可能会影响这一点——受欢迎但针对性不强的内容可能代表用户事先知道的电影，可以很容易地看出他们想看，但不会在真实系统中观看。在其他实验中，用于为单个用户重新训练推荐系统的算法被示出与完全重新训练一样准确，并且由于重新训练中的随机变化，影响计算被示出遭受适度高的方差(即，相同电影的影响的多次计算会产生非常不同的结果)。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="e2fd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用户喜欢关联规则解释，因此它是添加到实际系统中的一个很好的候选解释。我在生成解释时使用了全局关联规则:它们的优点是挖掘规则的过程只需进行一次。关联规则挖掘中使用的apriori算法[5]计算量很大，尤其是对于大型数据集，这可能会在扩大基于局部或聚类的方法[2]中造成重大问题。</p><p id="bce1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了将关联规则解释应用到实际系统中，需要提高其模型的保真度。如果解释器本身被视为机器学习模型，那么支持度和置信度的阈值(以及用于过滤关联规则集的其他参数)可以被视为其超参数。虽然阈值是根据研究人员的最佳猜测手工设置的，但也可以通过超参数优化来确定。这超出了本项目的范围，但可能是一种有趣的方式，使关联规则步骤更加科学，并允许该方法更容易地应用于更多样化的数据集。</p><p id="c199" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然用户研究没有发现添加基于影响的解释对用户评价的信任和说服力有统计学上的显著影响，但它们仍然值得进一步研究。最重要的是，可以进一步优化向用户呈现解释的方法。即使影响解释没有提高模型的可信度和说服力，但它们确实提高了透明度。它们还可以用于实现特殊的用户界面组件:例如，UI元素可以显示受特定项目影响最大的推荐。这对于实现经典的<em class="lr">“这个项目类似于下面的项目……”</em>推荐系统的更个性化版本可能是有用的，推荐系统通常是基于项目的邻居模型或基于内容的模型。</p><p id="27fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章展示了我用可解释的推荐来实现和评估电影推荐服务的方法。本系列的最后一部分将描述推荐系统中的激进内容问题，并建议如何使用关联规则解释来帮助数据科学家确定他们的推荐系统是否受到它的影响。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="0a0c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1]:哈珀，F. M .，&amp;康斯坦，J. A. (2015)。电影镜头数据集:历史和背景。<em class="lr">美国计算机学会交互式智能系统汇刊(tiis) </em>，<em class="lr"> 5 </em> (4)，1–19。</p><p id="1b4e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2]:皮克，g .，&amp;王，J. (2018年7月)。推荐系统潜在因素模型的事后可解释性。《第24届ACM SIGKDD知识发现和数据挖掘国际会议论文集》(第2060–2069页)。ACM。</p><p id="a647" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3]:程，魏，沈，杨，黄，李，朱，(2019年7月)。通过快速影响分析将可解释性纳入潜在因素模型。在<em class="lr">第25届ACM SIGKDD知识发现国际会议论文集&amp;数据挖掘</em>(第885–893页)。ACM。</p><p id="be3f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]:廷塔列夫，n .，&amp;马斯托夫，J. (2011)。设计和评估推荐系统的解释。在<em class="lr">推荐系统手册</em>(第479–510页)中。马萨诸塞州波士顿斯普林格。</p><p id="1ddb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5]阿格拉瓦尔和斯里坎特(1994年9月)。挖掘关联规则的快速算法。在<em class="lr"> Proc中。第20国际。糖膏剂超大型数据库，VLDB </em>(第1215卷，第487–499页)。</p></div></div>    
</body>
</html>