<html>
<head>
<title>Looking Beyond Feature Importance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越特性的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/looking-beyond-feature-importance-37d2807aaaa7?source=collection_archive---------22-----------------------#2020-04-30">https://towardsdatascience.com/looking-beyond-feature-importance-37d2807aaaa7?source=collection_archive---------22-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1da8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在Python中使用部分依赖图</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/64806afea9a92f16c4de7dcb9ee9efc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uXLd2JHppnk-ThWY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·塔在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0ac4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://machinelearningmastery.com/calculate-feature-importance-with-python/" rel="noopener ugc nofollow" target="_blank">许多</a> <a class="ae ky" href="https://datawhatnow.com/feature-importance/" rel="noopener ugc nofollow" target="_blank">文章</a> <a class="ae ky" rel="noopener" target="_blank" href="/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e">讨论</a>如何使用特征重要性来选择特征和分析你的机器学习模型。当您选择了您的重要特征并重新运行您的模型时会发生什么？特征重要性对于理解<strong class="lb iu">是什么</strong>在驱动我们的模型非常有用，但是它没有告诉我们<strong class="lb iu">那个特征</strong>如何与模型预测相关。本文介绍了如何超越要素重要性，并使用绘图方法来更深入地了解模型中的要素是如何驱动模型预测的。</p><h1 id="c618" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">特征重要性</h1><p id="7af0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们讨论特性重要性之前，我们需要定义特性重要性，并讨论何时使用它。</p><p id="00ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最高级别上，要素重要性是对特定预测变量(要素)对模型预测准确性的影响程度的度量。您可以使用特征重要性来修剪模型，并通过删除低性能特征来减少过度拟合。</p><p id="f423" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您使用scikit-learn (sklearn)，计算特性重要性的默认方法取决于您正在构建的模型的类型。例如，我将在本文中使用随机森林回归模型。sklearn对随机森林模型的默认特征重要性是通过<a class="ae ky" href="https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation" rel="noopener ugc nofollow" target="_blank">归一化每个特征通过分割该特征的“杂质减少”帮助预测的样本分数</a>来计算的。然而，要获得线性模型(线性回归、逻辑回归)的特征重要性，您可以查看由特征的标准偏差缩放的参数系数值。</p><p id="423a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就个人而言，我更喜欢特性重要性的模型不可知方法。默认的sklearn随机森林特性重要性对我来说很难掌握，所以我使用了一种排列重要性方法。Sklearn实现了一种排列重要性<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html" rel="noopener ugc nofollow" target="_blank">方法</a>，其中通过随机排列每个特征中的数据并计算相对于基线的MSE(或您选择的分数)的平均差异来确定特征的重要性。这种类型的特征重要性具有更大的计算负荷，但是可以跨不同类型的模型来完成，这对于标准化您的建模方法是很好的。</p><p id="c26f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，一旦我们找到了一个合理的特征集，可以准确预测我们的响应变量，我们就可以停下来。然而，有时深入了解我们的模型和系统如何工作是有用的。为此，我们需要一个单独的方法来提取特性和响应变量之间的关系。</p><p id="20a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在的问题是，我们将何去何从？</p><h1 id="85b0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">部分依赖情节:下一代</strong></h1><p id="f9b3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你可以从我之前的<a class="ae ky" rel="noopener" target="_blank" href="/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a"/><a class="ae ky" rel="noopener" target="_blank" href="/how-to-analyze-stratified-random-sampled-data-e3933199ae74">文章</a>中看出，我非常喜欢使用图表和可视化来理解数据中的关系。在大多数情况下，目视检查方法适用于各种数据分布和方法。对于机器学习，确定特征与响应变量的关系的最直接的方法之一是使用部分相关图(PDP)。</p><p id="c581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">构建PDP非常直观:</p><ol class=""><li id="a4ba" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">选择您感兴趣的功能(FOI)</li><li id="d195" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">对于连续foi:<br/>-为分类FOIs创建一个从特征<br/>的最小值到最大值的序列:<br/> -为分类特征中的每个级别(n-1)创建一个虚拟变量</li><li id="46c5" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">用序列中的值替换FOI中的每个值。</li><li id="32ad" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">从这个新的特征集中获取预测，并对所有预测进行平均。将平均值存储在向量中。</li><li id="ad43" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">对特征序列中的每个值重复步骤3和4</li><li id="4088" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">根据特征序列本身绘制平均预测</li></ol><p id="61c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个算法有<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">数学</a>描述。本质上，这个算法显示了我们的FOI对模型预测的边际效应。</p><p id="ead8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的其余部分，我将向您展示如何构建PDP以及如何解释它们。我将向你展示如何使用sklearn创建这些情节，以及如何自己直接构造这些情节。最后，我将讨论使用PDP时需要注意的潜在缺陷以及如何解决这些问题。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="f077" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated">一维部分相关图</h1><h2 id="f441" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated"><strong class="ak">读入并分割数据</strong></h2><p id="89bb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于这个分析，我将使用scikit-learn包中的波士顿住房数据集进行随机森林回归。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="b865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">波士顿住房数据集中有13个特征，你可以在<a class="ae ky" href="https://scikit-learn.org/stable/datasets/index.html#boston-dataset" rel="noopener ugc nofollow" target="_blank">这里</a>了解它们。在我们做了一些初步的特性选择后，我将分解更重要的特性代表什么。该数据集中的目标变量是“以1000美元为单位的自有住房的中值”。所以本质上，这里的任务是根据一组特征来预测房价。</p><h2 id="9075" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated"><strong class="ak">建造初始模型</strong></h2><p id="6836" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">读入数据后，我创建了一个随机森林回归。我选择了最大树深度和给出良好模型性能的估计器数量，并且没有进行任何超参数调整。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="36a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我创建了模型，我就提取了特征的重要性。上面的要点显示了如何用两种不同的方法来实现这一点，或者是默认的。来自sklearn的feature_importances_ method或使用sklearn中的permutation_importance函数。我在下面绘制了permutation_importance函数的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/fd805ffab63dd3402287344446143328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YD3XErgYW2DWhfXIekzFvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">条形的高度表示平均特征重要性，误差条形是每个特征平均值的一个标准偏差。</p></figure><p id="95c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据置换特性的重要性，RM、DIS和LSTAT特性比其他特性几乎高出一个数量级！置换特征重要性也给我们一个特征重要性方差的估计。根据对误差线的快速查看，RM和LSTAT可能对最终模型有统计上不可区分的影响，DIS显然是第二重要的。这些特征代表什么？</p><ul class=""><li id="591e" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu oh my mz na bi translated">RM:每个住宅的平均房间数</li><li id="611f" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu oh my mz na bi translated">DIS:到五个波士顿就业中心的加权距离</li><li id="3e6d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu oh my mz na bi translated">LSTAT: %人口的较低地位</li></ul><p id="b00f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，仅凭特征重要性，我们无法了解这些特征与我们的响应变量(中值住房价值)之间的关系。你可能会对这些关系的未来做出一些有根据的猜测。</p><h2 id="5118" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated"><strong class="ak">用reduce特征集构建模型</strong></h2><p id="e3d9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，让我们用一组性能最好的特性来重做这个模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="905e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的分析，简化模型预测测试集足够好，测试集上的R为0.82。我还在上面的代码片段中包含了从训练集和测试集中获取MSE的代码，以了解该模型中过度拟合有多糟糕。</p><p id="0144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您在这里检查特性的重要性，您会看到一个类似的模式，RM最高，LSTAT次之，DIS最低。同样，RM和LSTAT的特征重要性的差异看起来好像这两个特征的影响在统计上并不明显。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e8c2a47aac8b14bcee4b405e0324572b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_byvBrhHM1ZAkoOG2qCmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">条形的高度表示平均特征重要性，误差条形是每个特征平均值的一个标准偏差。</p></figure><h2 id="a151" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated"><strong class="ak">构建部分相关图</strong></h2><p id="d05c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Sklearn有一个快速的脏函数，它会为您绘制所有的特征，或者您可以执行run a函数，只获取部分依赖项而不绘制它们。在下面的代码片段中，我既有sklearn方法，也有一个quick函数，它说明了幕后发生的事情。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="1782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于sklearn函数有两点需要注意。sklearn函数中的默认网格从数据的5%到95%边界，我在绘图中使用了完整的数据范围。此外，如果您正在构建许多功能的PDP，plot _ partial _ dependence函数允许您使用“n_jobs”参数并行进行计算。</p><p id="5903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看我们的部分依赖模式在我们的模型中是什么样子的。每个图都有一条代表部分相关性的线(当所有特征值都设置为一个值时模型的平均响应)和一个沿着底部的<a class="ae ky" href="https://en.wikipedia.org/wiki/Rug_plot" rel="noopener ugc nofollow" target="_blank">凹凸图</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/90783179e3179b0fb71518846bd9ed9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHVYeikcxinD0gHccVw0fA.png"/></div></div></figure><p id="b6fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着家中房间数量的增加，预测的家庭价值会增加，直到某一点，然后开始减少。一般来说，随着房间数量的增加，房屋价值也会增加。但是，有了PDP，我们可以更进一步了解这一点。有趣的是，对房间数量的反应是相当非线性的，当超过6个房间时，房屋价值迅速增加。我们可以推测这种情况发生的一些原因，也许超过7个房间的房子有其他豪华的住宿设施(也许有一个大厨房？).我还想指出的是，我们需要小心解读这个图表的右边。观察到的中值住宅值的降低发生在非常大的值上，这些值在训练集中不经常出现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/ba1c98609f2d1a4616502aeb4a60f5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OwDaVZQMHMKG0em7hzXzow.png"/></div></div></figure><p id="2397" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到波士顿就业中心的距离只在距离很近的情况下对房屋价值有影响。再次，我们可以推测原因。对这种模式的一种可能的解释是，只有当员工可以步行、骑自行车或乘坐公共交通工具去工作场所时，靠近就业中心才是有价值的。在很短的距离之外，汽车可以让所有的距离都同样吸引人。换句话说，在更高的距离上缺乏关系可能是由于其他因素淹没了距离对价格的任何影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/de944d545dad7f362bd6f5cdaaf13a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hWyxVNIgTvE2NCyz0SRvUA.png"/></div></div></figure><p id="c86d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着较低地位百分比的增加，住房价值下降，直到达到20%左右。这种效应可能表明波士顿房地产市场已经触底，在其他因素的作用下，房地产价格不太可能下降超过某个值。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="cca7" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated"><strong class="ak">多维部分相关图</strong></h1><p id="1560" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">到目前为止，我们已经分别研究了每个特性的部分相关性。我们也可以使用上述相同的算法来构建部分相关的二维图。二维图将允许我们研究变量组合如何影响模型输出。</p><p id="a515" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一维PDP假定特征之间是独立的；因此，相关的特征可能导致PDP中的伪图案。如果两个特征相关，那么我们可以在我们的PDP算法中创建非常不可能的数据点。看看下面的情节。您可以看到，RM和LSTAT特征与-0.61的<a class="ae ky" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>呈负相关。这将如何影响我们的PDP？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/74aba9c40a90d7f2282c1be6a69c512e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhKSU3lH3Q5hZRsVN1DneQ.png"/></div></div></figure><p id="41f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们以FOI的RM特性为例。当我们构建RM PDP时，我们用min(RM)和max(RM)之间的序列中的值替换RM的每个值。在LSTAT的高值时，没有观察到RM的高值，这意味着随着我们在RM序列中的进展，我们最终将创建RM和LSTAT的组合，这些组合对于特征集来说是不符合逻辑的，从而对我们的训练数据中没有出现的值进行预测。<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/pdp.html#disadvantages-5" rel="noopener ugc nofollow" target="_blank">这本书的第</a>部分以身高和体重的相关性为例，清楚地解释了这个问题。简而言之，你不会期望一个6英尺高的人有50磅重，但是PDP算法没有做出这样的区分。</p><p id="eda5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们怎样才能解决这个问题呢？在这种情况下，您可以使用二维PDP图，只检查与相关性重叠的值。下面是使用sci kit-learn plot _ partial _ dependency()函数构建的LSTAT和RM的2D PDP图。如果我们不批判性地思考我们的数据，我们可以假设RM比LSTAT有更大的影响，如图表右侧所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/261ee34616a1a74a2f06bb02a41aaf12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8XUdImH_cMgmh5e-z62tQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等高线描绘了预测中值家庭价值分布的中断。暖色对应着较高的中值房价。</p></figure><p id="735f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果我们覆盖LSTAT和RM数据点之间的散布，我们可以看到，在我们的训练集中，图的右手侧的接近垂直的等高线没有被表示。在高RM和高LSTAT没有数据点。我们应该只考虑与数据点重叠部分的模型部分响应。从这里，我们可以确定，当房间数量增加时，当较低地位人口的百分比下降时，住房价格增加，非线性模式仍然表现良好。如果您有兴趣更正式地执行此操作，您可以解包plot _ partial _ dependence函数的输出，并且只绘制出现在二维特征分布的95%范围内的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/d52195dc436b5aa0530dab3176f82948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y4GKxz7yVSGs8NckNDTAhQ.png"/></div></div></figure></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="93be" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated"><strong class="ak">部分依赖陷阱</strong></h1><p id="b61e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">PDP很难在非常大的特征集中进行解释。通常，我只检查我的特性集中最重要的特性的PDP。一旦超过3或4个特征，立即可视化多个特征的PDP几乎是不可能的。</p><p id="98f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想重申，你的特征之间的相关性使得PDP难以解释。高度相关的特征会产生不准确的部分相关性预测，因为相关的特征可能不是独立的。通过设置多维特征分布之外的值(例如，高RM和高LSTAT)来计算预期模型响应实际上是在训练数据之外进行外推。我们可以通过构建多维部分相关图并只关注多维特征分布内的区域来解决这个问题。你也可以使用一个<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/ale.html" rel="noopener ugc nofollow" target="_blank">累积的局部效果</a>图，在<a class="ae ky" href="https://github.com/blent-ai/ALEPython" rel="noopener ugc nofollow" target="_blank">这个python库</a>中实现。</p><p id="a1b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，重要的是要记住，PDP是模型对相关特性的平均响应。在随机森林模型中，该功能可用于预测决策树中多种类型的响应。根据数据集的其余部分，特征和响应变量可能在某些情况下正相关，而在其他情况下负相关(参见<a class="ae ky" href="https://blogs.sas.com/content/subconsciousmusings/2018/06/12/interpret-model-predictions-with-partial-dependence-and-individual-conditional-expectation-plots/" rel="noopener ugc nofollow" target="_blank">此处</a>的示例)。PDP将是一条水平线，不会反映响应的异质性。如果您认为这发生在您的数据集中，您可以为每个数据点绘制单独的线，而不是这些线的平均值(这种类型的绘图称为<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/ice.html#ice" rel="noopener ugc nofollow" target="_blank">单独条件期望</a>绘图)。</p><h1 id="6941" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">就这样结束了！</h1><p id="c101" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">像许多数据科学方法一样，PDP应该小心使用，并与其他测试和数据检查结合使用。然而，我认为它们是理解黑盒模型中正在发生的事情的一种非常有用的方式，并且是一种超越特性重要性的方式。</p><p id="fd33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想要这篇文章中的任何代码，都可以在<a class="ae ky" href="https://github.com/j-sadowski/FromRtoPython/blob/master/PartialDependencePlots.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="941b" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated"><strong class="ak">延伸阅读</strong></h1><ul class=""><li id="8b82" class="ms mt it lb b lc mn lf mo li om lm on lq oo lu oh my mz na bi translated">Christoph Molnar的“可解释的机器学习”是学习如何解释你的模型的一个很好的资源，并涉及许多不同的方法。此处可在线<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">获取。我向那些希望自己的模型不仅仅是一个黑匣子的人强烈推荐它。</a></li><li id="2242" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu oh my mz na bi translated">scikit-learn文档中有一些关于如何构建不同类型的PDP的好例子。</li><li id="d0c9" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu oh my mz na bi translated">Kaggle也有一篇<a class="ae ky" href="https://www.kaggle.com/dansbecker/partial-dependence-plots/" rel="noopener ugc nofollow" target="_blank">文章</a>讨论PDP的一些来龙去脉。</li></ul></div></div>    
</body>
</html>