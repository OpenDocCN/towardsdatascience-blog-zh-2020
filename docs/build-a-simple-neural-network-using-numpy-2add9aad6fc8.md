# ä½¿ç”¨ Numpy æ„å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ

> åŸæ–‡ï¼š<https://towardsdatascience.com/build-a-simple-neural-network-using-numpy-2add9aad6fc8?source=collection_archive---------19----------------------->

## ä½¿ç”¨ NumPy æ„å»ºå•ä¸ªç¥ç»å…ƒè¿›è¡Œå›¾åƒåˆ†ç±»ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•ä½¿ç”¨ NumPy åˆ¶ä½œä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚

1.  **å¯¼å…¥åº“**

é¦–å…ˆï¼Œæˆ‘ä»¬å°†å¯¼å…¥æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„åŒ…ã€‚æˆ‘ä»¬å°†éœ€è¦ ***numpy*** ï¼Œ ***h5py*** (ç”¨äºåŠ è½½å­˜å‚¨åœ¨ H5 æ–‡ä»¶ä¸­çš„æ•°æ®é›†)ï¼Œä»¥åŠ ***matplotlib*** (ç”¨äºç»˜å›¾)ã€‚

```
**import** numpy as np
**import** matplotlib.pyplot as plt
**import** h5py
```

**2ã€‚æ•°æ®å‡†å¤‡**

æ•°æ®ä»¥(" . h5 ")æ ¼å¼æä¾›ï¼ŒåŒ…å«æ ‡è®°ä¸º cat æˆ–é cat çš„è®­ç»ƒå’Œæµ‹è¯•å›¾åƒé›†ã€‚è¯¥æ•°æ®é›†å¯ä» [github repo](https://rpaudel42.github.io/datasets/NN.zip) ä¸‹è½½ã€‚ä½¿ç”¨ä»¥ä¸‹å‡½æ•°åŠ è½½æ•°æ®é›†:

```
**def** **load_dataset**():
    train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r")
    train_x = np.array(train_dataset["train_set_x"][:]) 
    train_y = np.array(train_dataset["train_set_y"][:])test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r")
    test_x = np.array(test_dataset["test_set_x"][:]) 
    test_y = np.array(test_dataset["test_set_y"][:])classes = np.array(test_dataset["list_classes"][:]) 

    train_y = train_y.reshape((1, train_y.shape[0]))
    test_y = test_y.reshape((1, test_y.shape[0]))

    return train_x, train_y, test_x, test_y, classes
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿå®ƒä»¬çš„å½¢çŠ¶æ¥åˆ†ææ•°æ®ã€‚

```
train_x, train_y, test_x, test_y, classes = load_dataset()print ("Train X shape: " + str(train_x.shape))
print ("Train Y shape: " + str(train_y.shape))print ("Test X shape: " + str(test_x.shape))
print ("Test Y shape: " + str(test_y.shape))
```

![](img/03b8b05b1d88a832409a4194c420d1df.png)

æˆ‘ä»¬æœ‰ 209 ä¸ªè®­ç»ƒå›¾åƒï¼Œå…¶ä¸­æ¯ä¸ªå›¾åƒæ˜¯æ­£æ–¹å½¢(é«˜åº¦= 64px)å’Œ(å®½åº¦= 64px)ä¸”å…·æœ‰ 3 ä¸ªé€šé“(RGB)ã€‚ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬æœ‰ 50 ä¸ªç›¸åŒç»´åº¦çš„æµ‹è¯•å›¾åƒã€‚

è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹è¿™ä¸ªå›¾åƒã€‚æ‚¨å¯ä»¥æ›´æ”¹ç´¢å¼•æ¥æŸ¥çœ‹ä¸åŒçš„å›¾åƒã€‚

```
*# change index for another image*
index = 2
plt.imshow(train_x[index])
```

![](img/9a5e9bf2484704a6df9e994a4042e75d.png)

**æ•°æ®é¢„å¤„ç†:**å›¾åƒæ•°æ®å¸¸è§çš„æ•°æ®é¢„å¤„ç†åŒ…æ‹¬:

1.  ç®—å‡ºæ•°æ®çš„ç»´åº¦å’Œå½¢çŠ¶(m_trainï¼Œm_testï¼Œnum_pxï¼Œâ€¦)
2.  é‡å¡‘æ•°æ®é›†ï¼Œä½¿æ¯ä¸ªç¤ºä¾‹ç°åœ¨éƒ½æ˜¯ä¸€ä¸ªå¤§å°ä¸º(é«˜åº¦*å®½åº¦*é€šé“ï¼Œ1)çš„å‘é‡
3.  å°†æ•°æ®â€œæ ‡å‡†åŒ–â€

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä½¿å›¾åƒå˜å¹³ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨å½¢çŠ¶çš„ numpy æ•°ç»„(é«˜åº¦Ã·å®½åº¦Ã·é€šé“ï¼Œ1)ä¸­é‡å¡‘å½¢çŠ¶(é«˜åº¦ã€å®½åº¦ã€é€šé“)çš„å›¾åƒæ¥å®Œæˆã€‚

```
train_x = train_x.reshape(train_x.shape[0], -1).T
test_x = test_x.reshape(test_x.shape[0], -1).Tprint ("Train X shape: " + str(train_x.shape))
print ("Train Y shape: " + str(train_y.shape))print ("Test X shape: " + str(test_x.shape))
print ("Test Y shape: " + str(test_y.shape))
```

![](img/4bea498d623dbce5181088052aa61f88.png)

**æ ‡å‡†åŒ–æ•°æ®:**æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„é¢„å¤„ç†æ­¥éª¤æ˜¯å¯¹æ•°æ®é›†è¿›è¡Œå±…ä¸­å’Œæ ‡å‡†åŒ–ã€‚å¯¹äºç»™å®šçš„å›¾ç‰‡æ•°æ®é›†ï¼Œå¯ä»¥é€šè¿‡å°†æ•°æ®é›†çš„æ¯ä¸€è¡Œé™¤ä»¥ 255(åƒç´ é€šé“çš„æœ€å¤§å€¼)æ¥å®Œæˆã€‚

```
train_x = train_x/255.
test_x = test_x/255.
```

ç°åœ¨æˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ƒå¯ä»¥æ­£ç¡®åœ°å°†å›¾ç‰‡åˆ†ç±»ä¸ºçŒ«æˆ–éçŒ«ã€‚

## 3.ç¥ç»ç½‘ç»œæ¨¡å‹

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](img/45405cf2dab2efa59db225c39788e997.png)

**å…³é”®æ­¥éª¤**:æ„å»ºç¥ç»ç½‘ç»œçš„ä¸»è¦æ­¥éª¤æœ‰:

1.  å®šä¹‰æ¨¡å‹ç»“æ„(å¦‚è¾“å…¥ç‰¹å¾æ•°é‡ã€è¾“å‡ºæ•°é‡ç­‰)ã€‚)
2.  åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°(æƒé‡å’Œåå·®)
3.  å¾ªç¯:

*   è®¡ç®—ç”µæµæŸè€—(æ­£å‘ä¼ æ’­)
*   è®¡ç®—ç”µæµæ¢¯åº¦(åå‘ä¼ æ’­)
*   æ›´æ–°å‚æ•°(æ¢¯åº¦ä¸‹é™)

**3.1 æ¿€æ´»åŠŸèƒ½**

sigmoid æ¿€æ´»å‡½æ•°ç”±ä¸‹å¼ç»™å‡º

![](img/c3b5fe32ff9ab2bcf5d3cf7f90c7c9ea.png)

å¯ä»¥ä½¿ç”¨ *np.exp()è®¡ç®— sigmoid æ¿€æ´»å‡½æ•°ã€‚*

```
def **sigmoid**(z):
    return 1/(1+np.exp(-z))
```

**3.2 åˆå§‹åŒ–å‚æ•°**

æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–å‚æ•°ğ‘¤(æƒé‡)å’Œğ‘(åå·®)ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œğ‘¤ä½¿ç”¨ *np.random.randn()* åˆå§‹åŒ–ä¸ºä¸€ä¸ªéšæœºæ•°å‘é‡ï¼Œè€Œğ‘åˆå§‹åŒ–ä¸ºé›¶ã€‚

```
def **initialize_parameters**(dim):
    w = np.random.randn(dim, 1)*0.01
    b = 0
    return w, b
```

**3.3 æ­£å‘å’Œåå‘ä¼ æ’­**

ä¸€æ—¦å‚æ•°è¢«åˆå§‹åŒ–ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‰§è¡Œâ€œå‘å‰â€å’Œâ€œå‘åâ€ä¼ æ’­æ­¥éª¤æ¥å­¦ä¹ å‚æ•°ã€‚

*   ç»™å®šä¸€ç»„è¾“å…¥ç‰¹å¾(X)ã€‚
*   æˆ‘ä»¬å°†å¦‚ä¸‹è®¡ç®—æ¿€æ´»å‡½æ•°ã€‚

![](img/0134f86a9a8a41c0bccfafd1ab282bb1.png)

*   æˆ‘ä»¬å°†æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—æˆæœ¬ã€‚

![](img/bbbb961647110ba6c5429462e47300e1.png)

*   æœ€åï¼Œæˆ‘ä»¬å°†å¦‚ä¸‹è®¡ç®—æ¢¯åº¦(åå‘ä¼ æ’­)ã€‚

![](img/e6b2ef824a5a2464a9c5dbeca28ac495.png)

```
def **propagate**(w, b, X, Y):
    m = X.shape[1]

    #calculate activation function
    A = **sigmoid**(np.dot(w.T, X)+b) #find the cost
    cost = (-1/m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  
    #find gradient (back propagation)
    dw = (1/m) * np.dot(X, (A-Y).T)
    db = (1/m) * np.sum(A-Y) cost = np.squeeze(cost)
    grads = {"dw": dw,
             "db": db} 
    return grads, cost
```

**3.4 ä¼˜åŒ–**

åœ¨åˆå§‹åŒ–å‚æ•°ã€è®¡ç®—æˆæœ¬å‡½æ•°å’Œè®¡ç®—æ¢¯åº¦ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°å‚æ•°ã€‚

```
def **gradient_descent**(w, b, X, Y, iterations, learning_rate):
    costs = []
    for i in range(iterations):
        grads, cost = propagate(w, b, X, Y)

        #update parameters
        w = w - learning_rate * grads["dw"]
        b = b - learning_rate * grads["db"]
        costs.append(cost)
        if i % 500 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    params = {"w": w,
              "b": b}    
    return params, costs
```

**3.5 é¢„æµ‹**

ä½¿ç”¨å­¦ä¹ åˆ°çš„å‚æ•° *w* å’Œ *bï¼Œ*æˆ‘ä»¬å¯ä»¥é¢„æµ‹è®­ç»ƒæˆ–æµ‹è¯•ç¤ºä¾‹çš„æ ‡ç­¾ã€‚ä¸ºäº†é¢„æµ‹ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®¡ç®—å¦‚ä¸‹ç»™å‡ºçš„æ¿€æ´»å‡½æ•°ã€‚

![](img/5594cf25a8a64962768252f347860047.png)

ç„¶åå°†è¾“å‡º(é¢„æµ‹)è½¬æ¢æˆ 0(if***A***<= 0.5)æˆ– 1(if***A***>0.5)å­˜å‚¨åœ¨ *y_pred* ä¸­ã€‚

```
def **predict**(w, b, X):    
    # number of example
    m = X.shape[1]
    y_pred = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X)+b)

    for i in range(A.shape[1]):
        y_pred[0,i] = 1 if A[0,i] >0.5 else 0 
        pass
    return y_pred
```

3.6 æœ€ç»ˆæ¨¡å‹

æˆ‘ä»¬å¯ä»¥æŒ‰æ­£ç¡®çš„é¡ºåºæŠŠæ‰€æœ‰çš„æ„ä»¶æ”¾åœ¨ä¸€èµ·ï¼Œåˆ¶ä½œä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

```
def **model**(train_x, train_y, test_x, test_y, iterations, learning_rate):
    w, b = **initialize_parameters**(train_x.shape[0])
    parameters, costs = **gradient_descent**(w, b, train_x, train_y, iterations, learning_rate)

    w = parameters["w"]
    b = parameters["b"]

    # predict 
    train_pred_y = **predict**(w, b, train_x)
    test_pred_y = **predict**(w, b, test_x) print("Train Acc: {} %".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))
    print("Test Acc: {} %".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))

    return costs
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼Œä½¿ç”¨ä¸Šé¢æ„å»ºçš„æ¨¡å‹å¯¹å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ 0.005 çš„ *learning_rate* ï¼Œå¹¶ä¸º 2000 æ¬¡*è¿­ä»£*è®­ç»ƒæ¨¡å‹ã€‚

```
costs = **model**(train_x, train_y, test_x, test_y, iterations = 2000, learning_rate = 0.005)
```

![](img/f1c79ea3ef241242c81733fcefebeea6.png)

è®­ç»ƒå‡†ç¡®ç‡åœ¨ 99%å·¦å³ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹æ­£åœ¨å·¥ä½œï¼Œå¹¶ä¸”ä»¥é«˜æ¦‚ç‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚æµ‹è¯•å‡†ç¡®ç‡åœ¨ 70%å·¦å³ã€‚ç»™å®šç®€å•çš„æ¨¡å‹å’Œå°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªå¥½çš„æ¨¡å‹ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ç”»å‡ºæˆæœ¬å›¾ï¼Œçœ‹çœ‹æ¨¡å‹æ˜¯å¦‚ä½•å­¦ä¹ å‚æ•°çš„ã€‚

```
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations')
plt.title("Learning rate =" + str(d["learning_rate"]))
plt.show()
```

![](img/41db1217eca39dda58ab0c22b110f70e.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿­ä»£çš„æˆæœ¬éƒ½åœ¨ä¸‹é™ï¼Œè¿™è¡¨æ˜å‚æ•°æ­£åœ¨è¢«å­¦ä¹ ã€‚

åœ¨[ä¸‹ç¯‡](/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf)ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åˆ¶ä½œä¸€ä¸ªå¸¦éšè—å±‚çš„ç¥ç»ã€‚

> åœ¨è¿™é‡Œæˆä¸ºåª’ä½“ä¼šå‘˜[å¹¶æ”¯æŒç‹¬ç«‹å†™ä½œï¼Œæ¯æœˆ 5 ç¾å…ƒï¼Œå¯ä»¥å®Œå…¨è®¿é—®åª’ä½“ä¸Šçš„æ¯ä¸ªæ•…äº‹ã€‚](https://medium.com/@rmesfrmpkr/membership)