<html>
<head>
<title>Geometric Interpretation of Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的几何解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/geometric-interpretation-of-linear-regression-dd10601a85b1?source=collection_archive---------32-----------------------#2020-06-06">https://towardsdatascience.com/geometric-interpretation-of-linear-regression-dd10601a85b1?source=collection_archive---------32-----------------------#2020-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4790" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用几何解释推导线性回归的成本函数，并从头开始实现算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f332e99529129a86499eee8186904aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9y8xTiVdF7oYN01O"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾萨克·史密斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d816" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是一种建模标量响应(或因变量)和一个或多个解释变量(或自变量)之间关系的线性方法。在几何解释术语中，线性回归算法试图找到尽可能最适合数据点的平面或直线。线性回归是一种预测真实值的回归技术。</p><blockquote class="lv lw lx"><p id="ed75" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">术语</em>“寻找最符合数据点的平面”<em class="it">是什么意思</em>？</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/da84ba7affad6e5b4a6a49062303aab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*GcONSrs73uhL4V4Gy3XwVw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:示例二维数据集的表示</p></figure><p id="bfc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于上面给出的二维数据集样本(图 1)，覆盖尽可能多的点的直线的一般方程是<strong class="lb iu"> y = m*x+c，</strong>其中 m 是直线的斜率，c 是截距项。线性回归算法试图找到成本函数最小化的线/平面。在本文的后面，你会知道一个成本函数是如何推导出来的。</p><p id="7fb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将上述平面方程表示为<strong class="lb iu"> y = w1*x + b </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/7b037f62411c7f2ff750ba073ae1bfe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*t6x32kxLKeaFaZe3N8LyQA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:样本三维数据集的表示</p></figure><p id="a786" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，对于样本 3 维数据集(图 3)，最适合尽可能多的点的平面方程是<strong class="lb iu">y = w1 * x1+w2 * x2+b。</strong></p><p id="5824" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的等式可以扩展到 d 维数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/5ca891aa4d6ba3c0cb70ba6ff8aa497f.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*XRVmvrV3DKvopS5Zms0e0A.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/c648a6e914c93630ff4f68121a4d2220.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*pbNmvQGWTzwz7JPeNX9NHg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/9c7064c4c4d3c52ec32bf6141fe82a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*fQswUUyqpByyEHC0rpRxjg.png"/></div></figure><p id="c832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们需要找到上面方程的一个平面<strong class="lb iu"> (W，b) </strong>来最好地拟合大多数数据点。</p><blockquote class="lv lw lx"><p id="746c" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">深究推导几何解释的算法:</em> </strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/a7d8e2fdae3f1f1a6f35e8e94310960f.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*_C3ohI-fLENOxaZFoYpzMQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3</p></figure><p id="6fa4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于任意点<strong class="lb iu"> P </strong>(图 3)，<strong class="lb iu"> y_iAct </strong>为该点的实际输出值，而<strong class="lb iu"> y_iPre </strong>为预测值。因此，误差可以计算为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/863c7b4b8d7f193d71827241a26f9f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*odMvRKSQfDdZJuCy1wvwOw.png"/></div></figure><p id="e873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于误差可以是正的，也可以是负的，因为 y_iPred 可以在平面/直线之上或之下，所以为了保持正误差，我们对每个 x_i 的误差求平方</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/47f3d02e2c8b960522970a43f0e88bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*zpb-EEjkRsARsU_WPBT9SQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/cbf3e5275d4f3c76f9d9164d5c684158.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*CSl5jYj1qJal48Cjdz7oSw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:谷歌地图，y=x 的地图</p></figure><p id="b793" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">误差函数遵循抛物线，这意味着误差(Y 轴)将始终为正。</p><p id="d188" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要最小化所有点集的误差，这被称为 MSE(均方误差)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/b9a4c3d392286c6bbd606f2fc11f13dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*TUNGxGGDqPyS0pe3ZMhOAA.png"/></div></figure><p id="9557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而线性回归的成本函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/cdac33d91e3727e4a0f352db1b39fb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*6FntxqmwSGYW2EQyRCDhUQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/22bb27d592445b1fb4299d5018a088a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*aegEW4Fk3izkQP6studUcA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/01943c190b504733d9ff713955ca401f.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*4g0kqI1AHe4zUtHPqoKd6g.png"/></div></figure><p id="5369" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成本函数定义了我们需要找到一个具有给定的<strong class="lb iu"> W，b </strong>的平面，使得所有点集的误差最小。用平面方程代替<strong class="lb iu"> y_iPred </strong>新的成本函数变成:</p><p id="806b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用优化器来计算使上述成本函数最小化的<strong class="lb iu"> W，b </strong>的最佳值。梯度下降优化器可用于找到均方误差最小的平面的最佳值。</p><blockquote class="lv lw lx"><p id="1014" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">对查询点的预测:</em> </strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/038ff080c6c236ad4c24d3898ffc78fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*ouobFNhRs1KiIbjzx5MdzQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/36149cc1f477620f7b8759d33b990e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*yylmkbcBEyctYao3I99FUg.png"/></div></figure><p id="1295" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于查询点'<em class="ly"> Qx </em>'(图 5)，对应的预测值是<em class="ly"> Ypred </em>，其可以使用平面(W，w0)的方程使用上述方程来计算。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><blockquote class="lv lw lx"><p id="f531" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">使用梯度下降优化器实现线性回归模型:</strong></p></blockquote><p id="f28e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了找到平面(W，b)，我们希望误差尽可能小。梯度下降是一种迭代方法，以达到最小误差。为了找到最小误差，我们找到函数的梯度。</p><p id="9d11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降法应遵循的步骤:</p><ol class=""><li id="264b" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">初始化权重向量和偏差项。</li><li id="bbf3" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">求函数关于权重和偏差的导数。</li><li id="dbc5" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">更新权重和偏差</li></ol><p id="35d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复这三个步骤，直到我们达到最小误差。随着每次迭代，权重向量和偏置项被更新以达到最小值。</p><blockquote class="lv lw lx"><p id="7f28" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">样本数据集 LR 算法的实现:</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/368e64eb3f79fe74218f6b744d35a67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*YdZ28-vf7D9C2QuU4ZrbhA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:样本数据集点</p></figure><p id="b755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们取 300 个数据点的二维样本数据集(第 2 行)。将数据集分为训练和测试数据集，其中 210 个点(70%)属于训练数据，其余 90 个点(30%)用于测试(第 5 行)。上图(图 6)表示样本数据集，下面是代码实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码实现</p></figure><p id="55a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#1:初始化权重向量和偏置项:</strong></p><p id="dfd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用随机值初始化权重向量和偏差项(第 8-10 行)。权重向量的维数将等于数据集的维数。</p><p id="c053" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步:求函数关于权重和偏差的导数:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/23e371c1cf7c6b2fe02657932663fdf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*5N_MHk5-DhK0J_fp-4sByQ.png"/></div></figure><p id="e984" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是文章上面推导的 MSE 的方程。我们需要找到函数 f(W，b)关于 W 的导数(第 21 行)，以及函数 f(W，b)关于 b 的导数(第 22 行)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8094ed57403590d57a1fe1b9403fbfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*fvItLVxyZius2_6aih56ZA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a89871d11e8e8056e5439164cab015a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*b3KH5v8FKB-rvCP6UWBaow.png"/></div></figure><p id="0805" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述两个方程分别表示函数 f 关于权向量和偏差项的导数。</p><p id="7ab1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#1:更新权重和偏差:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5daa1172a20f6079f9ba291dd6f21b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*2nfuu1Oj1U8gL3v01dPx-w.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8211ede186d700dbefc34657191b9cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*4AbJl3gJ_5gKVp9RFLxcqA.png"/></div></figure><p id="32db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">权重向量和偏置项的更新方程分别如上所述。根据上述等式，为每次迭代更新权重向量和偏差项(第 24-25 行)。<em class="ly">lr’</em>代表学习率，其定义了更新应该发生的速度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ca5ed596aa468b2e527b32b0b171eacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*0Pz6bmWWM8eeIt8P6Ja8OQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左图:大单反，右图:小单反</p></figure><p id="783b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果学习率很大，则永远无法达到最小值，如果学习率很小，则通过一定次数的迭代可以达到最小值。</p><blockquote class="lv lw lx"><p id="d8d6" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">上述 3 个步骤重复一定次数的迭代(第 16 行),直到 W(I+1)变得非常接近或等于 W_i。</p><p id="d2ad" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">观察:</strong></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="0a35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算出的权重向量和偏差项分别为[50.265]，-0.131(第 2-3 行)。计算所有测试数据集的预测值(第 6–9 行)。绘制直线和所有数据点(第 12-19 行),观察结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d86495a00557f163f65a49fca59a9385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*UYqPYIsTLGk637x7vrnwNg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:线的观察</p></figure><p id="a747" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">观察:</strong>可以观察到线图(图 8)与数据点尽可能吻合。</p><div class="nu nv gp gr nw nx"><a href="https://github.com/krsatyam1996/Linear_Regression_from_scratch" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">krsatyam 1996/Linear _ Regression _ 从头开始</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">在 GitHub 上创建一个帐户，为 krsatyam 1996/Linear _ Regression _ from _ scratch 开发做贡献。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">github.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ks nx"/></div></div></a></div><blockquote class="om"><p id="0e23" class="on oo it bd op oq or os ot ou ov lu dk translated">感谢您的阅读！</p></blockquote></div></div>    
</body>
</html>