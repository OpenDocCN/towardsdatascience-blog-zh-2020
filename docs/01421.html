<html>
<head>
<title>Neural Networks: From Zero to Hero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:从零到英雄</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-from-hero-to-zero-afc30205df05?source=collection_archive---------13-----------------------#2020-02-08">https://towardsdatascience.com/neural-networks-from-hero-to-zero-afc30205df05?source=collection_archive---------13-----------------------#2020-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a517" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解神经网络最重要的参数！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5fa08d5446251c08eb7a0d6ea5cba93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*QmM2B6LNmhW9hdgMZfboVw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来自<a class="ae ku" href="https://www.vectorstock.com/royalty-free-vector/super-hero-brain-cartoon-vector-17919284" rel="noopener ugc nofollow" target="_blank"> vectorstock </a></p></figure><h1 id="b604" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="3beb" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">本文将涵盖以下主题:</p><ul class=""><li id="0162" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">最优化理论:梯度下降及其变化</li><li id="70ea" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">学习率和批量大小</li><li id="bd08" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">损失和激活功能</li><li id="a218" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">重量初始化</li></ul><h1 id="57f6" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">最优化理论:梯度下降及其变化</h1><p id="8ee0" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">如果你想要梯度下降的详细解释，我建议你看看<a class="ae ku" rel="noopener" target="_blank" href="/neural-networks-everything-you-wanted-to-know-327b78b730ab">这篇文章</a>，其中有对神经网络所基于的数学的深入研究。</p><p id="a9a4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">总之，梯度下降计算训练集中每个样本的误差，然后在指向梯度的方向上更新权重。</p><p id="a6cf" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">换句话说，对于每个时代，我们都需要:</p><ol class=""><li id="0f06" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi nc mr ms mt bi translated">计算每个预测(向前传递)。</li><li id="474f" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">计算每一个误差。</li><li id="b2dd" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">向后传播误差，以评估每个权重在该误差中的重要性。</li><li id="8e7d" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">最后，相应地更新权重。</li></ol><p id="28a1" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">假设我们有:</p><ul class=""><li id="0c11" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">包含100，000个样本的数据集</li><li id="763b" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">每次向前传递需要2毫秒</li><li id="5c25" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">每次误差计算需要1毫秒</li><li id="78e0" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">每次反向传播需要3毫秒</li></ul><p id="61e4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">如果我们计算一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/38ef0be7f9f5a7a00c8f0de524c95b36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8iUclOR8LkcFHyuvAve1Q.png"/></div></div></figure><p id="4ef4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">一个常规的神经网络可能需要数百甚至数千个时期来适当地收敛。假设我们需要100个历元，这是一个很低的数字。</p><p id="f50b" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们的神经网络需要多少才能被训练好？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/cbc357b27ea6c84882989d9586e7b14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sx3irMjkMvZY3d4LZz6MQQ.png"/></div></div></figure><p id="6cc0" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">时间太长了。我们假设只有100，000个样本，这很好。例如，ImageNet由120万幅图像组成，每个时期需要2小时，换句话说，需要8.3天。一个多星期来看一个网络的行为。</p><p id="012e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">大幅减少训练神经网络所需时间的一种方法是，每次我们想要更新权重时，使用随机选择的单个样本。</p><p id="d38a" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这种方法被称为随机梯度下降(SDG)。使用SDG，我们只需计算一个样本的预测、误差和反向传播来更新权重。</p><p id="ca77" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这将把总培训时间减少到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/7d37632649e403909b2fd76e9df87004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rn2KzC7ptWZNMj92bsSnlA.png"/></div></div></figure><p id="4f5e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这是一个巨大的进步。但是这种方法有一个非常重要的缺点。</p><p id="3d39" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在这两条路径中，你认为哪一条是梯度下降路径？哪一个是随机梯度下降的？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/a705b9e8f40a39406fc2abc54198d9dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eU6liacD4IfNmkT0PB6N5Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来自<a class="ae ku" href="https://www.fromthegenesis.com/gradient-descent/" rel="noopener ugc nofollow" target="_blank">这里</a></p></figure><p id="9ca4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">红色路径是沿着梯度下降的路径。它使用数据集的所有样本计算梯度(下降路径),并总是在最小化误差的方向上获得一致的更新。</p><p id="7b09" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">紫色路径是SGD遵循的路径。这是怎么回事？每次权重更新都是为了通过仅考虑一个样本来最小化误差，因此我们最小化的是该特定样本的误差。</p><p id="d7d6" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这就是为什么它有一个更混乱的行为，它的收敛成本更高，虽然，作为回报，它运行得更快，所以在GD需要运行一个纪元的时间里，SGD可以运行数千个。</p><p id="8471" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">看起来最好的选择是平衡这两种方法。如果我们看一下前面的图片，最佳路径是绿线。</p><p id="5a27" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">为了计算这条路径，让我们回顾一下到目前为止讨论过的方法:</p><ul class=""><li id="cf07" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">计算我们的训练集的所有元素的预测和误差的方法:(香草)梯度下降</li><li id="38ae" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">计算从我们的训练集中随机选择的1个元素的预测和误差的方法:随机梯度下降</li></ul><p id="b7e3" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">如果我们选择的不是1个元素，而是K个元素呢？这样一来:</p><ul class=""><li id="4018" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">我们增加了算法的稳定性，因为我们不仅查看一个元素，而且查看<strong class="lp iu"> K </strong>(也就是说，我们减少了洋红色线方向的突然和混乱的变化)</li><li id="2ffc" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">我们减少了关于传统下降梯度的执行时间，因为我们从训练集的N个样本减少到<strong class="lp iu"> K </strong>，其中<strong class="lp iu"> K </strong> &lt; &lt; <strong class="lp iu"> N </strong></li></ul><p id="21fd" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这种方法被称为小批量随机梯度下降法，是实践中最常用的方法。</p><p id="fdc0" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">K<strong class="lp iu">通常被选择为2的幂，因为这允许你利用一些针对这些情况的GPU实现的优化。一个典型的<strong class="lp iu"> K </strong>可能是<strong class="lp iu"> K </strong> =32，但最终，这还是受到了GPU内存的限制。</strong></p><p id="8f26" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">K 越低，它就越像纯SGD，它需要收敛的纪元就越多，尽管它会更快地计算它们也是事实。</p><p id="d8ee" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">另一方面，<strong class="lp iu"> K </strong>越高，就越像纯GD，每个历元的计算就越麻烦，但收敛所需的时间就越少。</p><h1 id="a6e3" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">学习率和批量大小</h1><p id="c5e5" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">学习速率和批量大小是与下降梯度算法直接相关的两个参数。</p><h2 id="bedc" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">学习率</h2><p id="12c8" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">你可能知道，(或者如果你不知道，<a class="ae ku" rel="noopener" target="_blank" href="/neural-networks-everything-you-wanted-to-know-327b78b730ab">你可以在这里查看</a>)更新神经权重的方法是通过这些公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nx"><img src="../Images/007f4db65fe7167c4a2bc0654fce145c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6atVwOVQjfzm_8KWHKfJg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="32ad" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">乘以∂Etotal/∂wn的叫做<strong class="lp iu"> η </strong>，就是学习率。学习率表示我们给予误差的重要性，以更新每个权重。也就是说，权重的变化有多快或多突然。</p><p id="091f" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，一个非常高的<strong class="lp iu"> η </strong>，将在从一个迭代到另一个迭代的巨大步骤中改变权重，这可能导致跳过最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ny"><img src="../Images/3b2ed8ba6d3ffafc2e6de4ff964ca0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cV9B6J1gMI652VxT1QHTfw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.quora.com/In-neural-networks-why-would-one-use-many-learning-rates-in-decreasing-steps-rather-than-one-smooth-learning-rate-decay" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/In-neural-networks-why-one-use-many-learning-rates-In-decay-steps-than-one-smooth-learning-rates-decay</a></p></figure><p id="64d7" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">另一种可能性是建立非常低的<strong class="lp iu"> η </strong>，这将使我们的网络需要太多的历元来达到可接受的最小值。此外，我们可能会陷入比更高的<strong class="lp iu">η</strong>所能达到的最佳最小值更糟糕的最小值中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b915517389e051f887dafa9cd55b34fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*dHpvoNaP0qmmN4hQAdafoA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.researchgate.net/publication/226939869_An_Improved_EMD_Online_Learning-Based_Model_for_Gold_Market_Forecasting/figures?lo=1" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/publication/226939869 _ An _ Improved _ EMD _ Online _ Learning-Based _ Model _ for _ Gold _ Market _ Forecasting/figures？lo=1 </a></p></figure><p id="d48d" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">让我们谈谈最小值:我们用神经网络实现的，通常不是我们函数的全局最小值，而是足以正确执行我们正在开发的任务的局部最小值。</p><p id="ad02" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们想要的是一个最优的学习速率，它允许我们随着时间的推移减少误差，直到我们达到最小值。在图表中，这个学习率就是红线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d8d3c0d86b3174449e23bcebcaa8fdea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*KvpOyCVlRO7i_haPlj5m_g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/useful-plots-to-diagnose-your-neural-network-521907fa2f45">https://towards data science . com/useful-plots-to-diagnostic-your-neural-network-521907 fa2f 45</a></p></figure><p id="f9e2" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">为了让我们的学习率达到最优，我们可以对我们的学习率进行衰减。这种衰减会随着时间的推移降低学习速度，所以当我们达到最小值时，它会小到足以避免跳过它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ob"><img src="../Images/831c2e26f8409d029a7517cc6fc8a3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdhTaEC-zKkkDQqwzAgDCw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.pinterest.es/pin/825918019139095502/" rel="noopener ugc nofollow" target="_blank">https://www.pinterest.es/pin/825918019139095502/</a></p></figure><p id="0692" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，我们通过选择一个非常低的学习率和跳过最小值来避免长时间等待收敛，因为我们越接近它，我们向它迈出的步伐就越小。</p><h2 id="ae93" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">批量</h2><p id="0b4f" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">回想一下上一节，SGD是一个小批量SGD，其中K=1。</p><p id="f5c3" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">并且小批量SGD的K表示每次用于更新权重的样本数量。这不是一个关键参数，它通常被设置为我们的GPU可以容纳的最大样本数。</p><blockquote class="oc od oe"><p id="edd3" class="ln lo of lp b lq ml ju ls lt mm jx lv og mz ly lz oh na mc md oi nb mg mh mi im bi translated">我们有一个8GB内存的GPU，如果每个图像占用1MB，我们可以容纳多少个样本？</p></blockquote><p id="37b0" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">嗯，没那么简单！这取决于网络的架构。密集或完全连接的层(在传统层中，所有神经元都与下一层中的所有神经元相互连接)具有更多参数，因此占据更多内存。</p><p id="c8ea" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们还有卷积层、汇集层、分离层和许多其他类型。因此在实践中，很难手工计算出我们可以使用的最大样本数。</p><p id="d610" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们所做的是尝试将批量大小设置为2的倍数，如果出现内存错误，就减少批量大小。例如，我们将从512开始，如果我们有一个错误，我们将下降到256、128、64、32、16、8、4、2甚至1。</p><p id="3d9e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">根据您的网络架构，您可能必须使用K=1，因此SGD。尽管通常优选的是将图像尺寸从512×512减小到256×256或128×128像素，并使用较大的k</p><h2 id="7f9c" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">学习率和批量关系</h2><p id="cda8" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">记住学习速度与批量大小有关，这一点非常重要。</p><p id="be0b" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">如果我们接近K=1，我们必须降低学习率，使得权重的更新不那么重要，因为它更接近SGD，换句话说，更接近用单个随机样本计算梯度。</p><p id="52f3" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，总之，如果我们使用较低的批量大小，建议使用较低的学习速率，但我们也会增加历元的数量，因为后面的条件会使我们的神经网络需要更多的时间来收敛。</p><h1 id="57fe" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">损失和激活功能</h1><h2 id="7402" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">损失函数</h2><p id="053d" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">损失函数告诉我们我们的预测有多错误。</p><p id="0218" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">想象一下，我们只需要看一张照片就能猜出一栋房子的价格。我们的神经网络将以照片的像素作为输入，以显示价格的数字作为输出。</p><p id="5f46" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">例如，假设我们想要预测一栋房子的价格，所以我们正在训练网络，而这栋房子就在我们的训练集中。当图片经过时，计算出一个预测，那就是它值323567美元。事实是房子价值600，000美元，因此很明显，一个合适的损失函数可以是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5487b31f2de8fed94198d60f8307be99.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-mWRVGK8PYzh68PgrcaoCQ.png"/></div></figure><p id="7cf4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">考虑到这一点，最常见的损失函数是:</p><p id="8392" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">对于回归问题</strong></p><ul class=""><li id="81c4" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">均方误差</li><li id="f787" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">绝对平均误差</li></ul><p id="7508" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">分类问题</strong></p><ul class=""><li id="49f6" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">二元交叉熵</li><li id="7abe" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">范畴交叉熵</li></ul><p id="d5e5" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">正如我在<a class="ae ku" rel="noopener" target="_blank" href="/supervised-learning-basics-of-linear-regression-1cbab48d0eba">的上一篇文章</a>中所写的那样，只关注回归问题，让我们来看看其中的每一个问题:</p><ul class=""><li id="8f7d" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">均方差:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8e881a9dddb264bcf5100f0b6d521a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/0*tVFvIbpKsibgNwOB.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/supervised-learning-basics-of-linear-regression-1cbab48d0eba">https://towards data science . com/supervised-learning-basics-of-linear-regression-1 cbab 48 d0 EBA</a></p></figure><p id="3138" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">均方差或MSE是真实数据点和预测结果之间的平方差的平均值。这种方法惩罚越大的距离，这是标准的回归问题。</p><ul class=""><li id="ccc9" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">平均误差</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/59a7070290391701d0bec855cedbe3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*I5MnjpneGCXrB7io.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/supervised-learning-basics-of-linear-regression-1cbab48d0eba">https://towards data science . com/supervised-learning-basics-of-linear-regression-1 cbab 48 d0 EBA</a></p></figure><p id="237a" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">平均绝对误差或MAE是真实数据点和预测结果之间的绝对差值的平均值。如果我们将此作为遵循的策略，梯度下降的每一步都将减少MAE。</p><h2 id="ae4f" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">什么是交叉熵？</h2><p id="1cc4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我们首先需要理解什么是熵。让我们用几个例子来说明这一点:</p><p id="766c" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">例一</strong></p><p id="a33a" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">想象一下，我们正在玩一个游戏:我们有一个装有不同颜色球的袋子，游戏的目标是猜一个志愿者用最少的问题数画出哪种颜色。</p><p id="f22e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在这种情况下，我们有一个蓝色球、一个红色球、一个绿色球和一个橙色球:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1f78326951783c52e14d745ef5b4708a.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*MXW097JyKxei9wH-Mk7cbA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Whats-an-intuitive-way-to-think-of-cross-entropy</a></p></figure><p id="5cab" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">也就是说每个球都有1/4的机会出去。</p><p id="94d0" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">最好的策略之一是先问你发的球是蓝色的还是红色的。如果是，我们会问球是不是蓝色的。如果不是，我们会问它是否是绿色的。所以我们需要两个问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi on"><img src="../Images/af5c546afda06b256f30384835b0f347.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*0lu7gHCLo5Ca2zIdbP8Hag.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Whats-an-intuitive-way-to-think-of-cross-entropy</a></p></figure><p id="1b05" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">例2 </strong></p><p id="c0e0" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这次我们有一个装有球的包，其中1/2是蓝色的，1/4是红色的，1/8是绿色的，1/8是红色的。现在，最佳策略是先问它是不是蓝色的，因为蓝色的可能性更大。如果是的话，我们就完了。如果不是，我们可以问它是否是红色的，这是下一个最有可能的类。如果是的话，我们就完了。如果不是，我们可以问它是绿色的(还是橙色的)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/aa426b1334a2d6b783b18d93c976e28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*sPjNd728xoCoXbbaYNBKug.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Whats-an-intuitive-way-to-think-of-cross-entropy</a></p></figure><p id="1af6" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">现在发生的情况是，一半的时间(1/2)是蓝色的，我们要花1个问题来猜测。1/4的时间是红色的，它花了我们2个问题。1/8是绿色的，要花我们3个问题，如果是橙色的也一样。</p><p id="c54b" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">所以猜一个球的期望题数是:1/2⋅1(蓝色)+1/4⋅2(红色)+1/8⋅3(绿色)+1/8⋅3(橙色)=1.75</p><p id="f1bd" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">例3 </strong></p><p id="21e1" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">想象一下，现在我们有一个装满蓝色球的袋子。我们需要问多少题才能找出他们拿出的是什么颜色的球？无，0。</p><p id="2c8b" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">从这些例子中，我们可以得出一个表达式，它允许我们根据球的概率来计算问题的数量。因此，概率为p的球需要log2(1/p)个问题。</p><p id="b650" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">例如，对于p=1/8的球，我们需要<strong class="lp iu">n _ quest</strong>= log2(1/(1/8)=<strong class="lp iu">3</strong></p><p id="07b6" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，预期的问题总数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c65c08abb91bb0048ab08ba36ddea70f.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*HonWklrHEvC9wbjLaiomdA.png"/></div></figure><p id="cbc6" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">最后，理解熵的一种方法如下:</p><blockquote class="oc od oe"><p id="f311" class="ln lo of lp b lq ml ju ls lt mm jx lv og mz ly lz oh na mc md oi nb mg mh mi im bi translated">如果我们遵循最优策略，让我们猜测球的颜色的预期问题数量是多少？</p></blockquote><p id="3642" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">所以，<strong class="lp iu">游戏越复杂，熵</strong>越高。在这种情况下，例1 &gt;例2 &gt;例3。</p><p id="e37e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">好，现在我们知道了熵是什么，让我们看看交叉熵是什么。</p><h2 id="0dec" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">交叉熵</h2><p id="fde2" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">假设我们对例2采用了例1的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/9318a278306c2b76142e22fae95ef519.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*hLlYJYhnXDPAuCoe-6HDFQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Whats-an-intuitive-way-to-think-of-cross-entropy</a></p></figure><p id="f496" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">所以，我们必须问两个问题来确定它是不是任何颜色。如果我们计算所需问题的总数，考虑到这一次每个球都有一个概率，我们得出:</p><p id="d00e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">n_total_quest=1/2⋅2(blue)+1/4⋅2(red)+1/8⋅2(green)+1/8⋅2(orange)=2</p><p id="38f4" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">所以这个策略比例1中的策略更差。</p><blockquote class="oc od oe"><p id="6d97" class="ln lo of lp b lq ml ju ls lt mm jx lv og mz ly lz oh na mc md oi nb mg mh mi im bi translated">最后，直觉上，熵是使用最佳策略时预期的问题数量，交叉熵是不使用最佳策略时预期的问题数量。</p></blockquote><p id="6502" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">为此，我们试图做的是最小化交叉熵。</p><p id="9e32" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在形式上，它被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5da872d3e28b2cd7f63596fe975ebd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*oFn_zSflqdmAhcmpMg4-GQ.png"/></div></figure><p id="69ba" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">其中:</p><ul class=""><li id="1e2d" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated"><strong class="lp iu"> pi </strong>是球的实际概率(在我们的例子中，蓝色是1/2，红色是1/4，绿色和橙色是1/8)</li><li id="ccc0" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated"><strong class="lp iu"> pi^ </strong>是我们使用策略时假设的概率，在这种情况下，所有球的概率相等(所有颜色的概率为1/4)</li></ul><p id="4de3" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">记住公式中什么在哪里的一个方法是，记住我们想做的是找出按照我们的策略需要多少个问题，这是pi^，所以在log2里面是pi^.</p><h2 id="210c" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">激活功能</h2><p id="d94a" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">如果我们没有激活功能，我们会有以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi os"><img src="../Images/d94e83737b3880aa90aea66b7557c8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5roxMa8aEe6GMUBaK8i6hA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/neural-representation-of-logic-gates-df044ec922bc">https://towards data science . com/neural-representation-of-logic-gates-df 044 EC 922 BC</a></p></figure><p id="6e8e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们必须y(x)=Wx+b，这是一个线性组合，甚至不能解决像XOR这样的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e7104a9b7ce91e167ce7387464dda381.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*i14iXxexdzJMjRNcJjsZ6g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html" rel="noopener ugc nofollow" target="_blank">http://www . ECE . utep . edu/research/web fuzzy/docs/kk-thesis/kk-thesis-html/node 19 . html</a></p></figure><p id="8cae" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，我们需要一种方法来引入非线性，这就是激活函数的作用。在下图中，您可以看到一些最典型的网络，以及它们在网络中的位置:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/f6c6828d2e6a4bc645e4ebe959011816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*qZRJ6kfvN5Kugb8va3OknQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/activation-functions-and-its-types-which-is-better-a9a5310cc8f">https://towards data science . com/activation-functions-and-its-types-哪个更好-a9a5310cc8f </a></p></figure><p id="b494" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这里你可以看到最常用的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ov"><img src="../Images/efa8480553a2e7fb95b85923f3c7cd26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16piSd4N6I7A-Umc61bcOw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" rel="noopener" target="_blank" href="/complete-guide-of-activation-functions-34076e95d044">https://towards data science . com/complete-guide-of-activation-functions-34076 e95d 044</a></p></figure><p id="7ae5" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">很难知道使用哪一种方法我们的网络会表现得更好，但是有一种方法通常几乎总能得到好的结果:ReLU。</p><p id="a819" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，每当我们开始时，我们将使用ReLU，一旦我们得到一些我们认为好的结果，我们可以尝试使用泄漏的ReLU或任何其他您想要的。每天都有新的出现，简单的谷歌搜索就能让你找到一些有趣的:比如SELU(<a class="ae ku" rel="noopener" target="_blank" href="/selu-make-fnns-great-again-snn-8d61526802a9">https://towards data science . com/卢瑟-make-fnns-great-again-snn-8d 61526802 a9</a>)。</p><p id="f00f" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">这些激活函数中的许多需要权重初始化的特定方法，以便位于特定范围的值之间，并且梯度下降正常工作。</p><p id="6541" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在输出图层的情况下，softmax激活函数是使用最多的一个，因为它能够为每个类提供一个概率，使所有类的总和为1。</p><p id="769c" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">由于这可能看起来有点复杂，请在下面找到推荐的食谱，作为以上所有内容的总结:</p><p id="1986" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><strong class="lp iu">配方</strong></p><ol class=""><li id="c43b" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi nc mr ms mt bi translated">开始使用学习率为0.01或0.001的ReLU，观察会发生什么。</li><li id="9362" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">如果网络训练(收敛)但是很慢，可以试着提高一点学习速率</li><li id="6ede" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">如果网络不收敛且表现混乱，则降低学习速率</li><li id="0117" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">一旦你的网络启动并运行，尝试一下漏热、最大或eLU</li><li id="1cdf" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi nc mr ms mt bi translated">不要使用乙状结肠，实际上它通常不会给出好的结果</li></ol><h1 id="992c" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">重量初始化</h1><p id="869b" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">正如您之前看到的，权重和偏差初始化对于实现我们的网络收敛到适当的最小值非常重要。让我们来看看初始化权重的一些方法。</p><p id="3294" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">如果我们遵循MNIST数据集(<a class="ae ku" rel="noopener" target="_blank" href="/deep-learning-solving-problems-with-tensorflow-3722b8eeccb1">正如我们在本文</a>中所做的)，我们的权重矩阵将是768(输入)x 10(输出)。</p><h2 id="ede0" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">常数初始化</h2><p id="9e28" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我们可以预先设定我们的重量</p><ul class=""><li id="fd99" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated"><strong class="lp iu">零:</strong></li></ul><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="11c5" class="nl kw it ox b gy pb pc l pd pe">W = np.zeros((768, 10))</span></pre><ul class=""><li id="85c6" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated"><strong class="lp iu">一:</strong></li></ul><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="cf0c" class="nl kw it ox b gy pb pc l pd pe">W = np.ones((768, 10))</span></pre><ul class=""><li id="e20e" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated"><strong class="lp iu">到常数C : </strong></li></ul><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="c4b3" class="nl kw it ox b gy pb pc l pd pe">W = np.ones((768, 10)) * C</span></pre><h2 id="d999" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">正态和均匀分布</h2><p id="9d2a" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我们还可以使用均匀分布来初始化权重，其中定义了[上界，下界]，并且该范围内的所有数字具有相同的被选中概率。</p><p id="a622" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">例如，对于[-0.2，0.2][-0.2，0.2]之间的分布:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="567a" class="nl kw it ox b gy pb pc l pd pe">W = np.random.uniform(low=-0.2, high=0.2, size=(768, 10))</span></pre><p id="6748" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">利用该指令，我们将使用从[-0.2，0.2][-0.2，0.2]之间的范围中提取的值来初始化WW权重矩阵，在该范围中，它们都具有相同的被提取概率。</p><p id="0a8f" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">我们也可以用正态或高斯分布来计算，其定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/524971c7db13ab92083c5aa5e2c9cb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*r2WRM9I6PD9s5bRaxRgi6A.png"/></div></figure><p id="2ca2" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">如你所知:</p><ul class=""><li id="582e" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">μ是平均值</li><li id="1bec" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">σ是标准差，σ2σ2是方差</li></ul><p id="534e" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">因此，我们可以用正态分布初始化我们的权重，其中<strong class="lp iu"> μ=0 </strong>和<strong class="lp iu"> σ=0.2 </strong>，例如:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="d438" class="nl kw it ox b gy pb pc l pd pe">W = np.random.normal(0.0, 0.2, size=(768, 10))</span></pre><h2 id="1409" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">初始化:正常和统一的LeCun</h2><p id="da67" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">另一种更先进的方法是LeCun方法，也称为“高效反向投影”。</p><p id="6354" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">该方法定义了3个参数:</p><ul class=""><li id="7697" class="mj mk it lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">Fin:该层的条目数(在我们的例子中是768)</li><li id="2d86" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">Fout:该层的出口数量(在我们的示例中为10)</li><li id="3b7d" class="mj mk it lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">极限:根据Fin和Fout定义为:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/66f327c7227ca17cf34747d8c60d16c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*VXOzG2Wzd-834zmBRZvVBg.png"/></div></figure><p id="dbdb" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">通过这种使用均匀分布的方法来初始化<strong class="lp iu"> W </strong>的代码将是:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="9832" class="nl kw it ox b gy pb pc l pd pe">W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))</span></pre><p id="413c" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">对于一个普通人来说:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="2674" class="nl kw it ox b gy pb pc l pd pe">W = np.random.normal(low=-limit, high=limit, size=(F_in, F_out))</span></pre><h2 id="67a6" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">初始化:Glorot/Xavier正常和统一</h2><p id="04b6" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">这可能是最广泛使用的初始化权重和偏差的方法。这是使用Keras时的默认设置。</p><p id="af96" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在这种情况下，也定义了与LeCun相同的参数，但是极限的计算有所不同:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/6deed8d0d8137801fa2089197f978081.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*iOj6JXmFLp0w_r_BKHDZAw.png"/></div></figure><p id="1636" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">使用这个方法初始化WW的代码与使用LeCun的代码相同。</p><p id="4b88" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">对于均匀分布，它将是:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="a531" class="nl kw it ox b gy pb pc l pd pe">W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))</span></pre><p id="07d2" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">对于一个普通人来说:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="0539" class="nl kw it ox b gy pb pc l pd pe">W = np.random.normal(low=-limit, high=limit, size=(F_in, F_out))</span></pre><h2 id="c7bc" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">初始化:何等人/和正常而统一</h2><p id="b7d7" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">这种方法是以何的名字命名的，他是《深入研究整流器:在ImageNet分类上超越人类水平的性能》一书的第一作者。</p><p id="e572" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">通常，当我们训练使用特定类型的ReLU作为激活(如参数ReLU)的非常深的神经网络时，使用这种方法。</p><p id="30b7" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">制服的代码是这样的:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="d674" class="nl kw it ox b gy pb pc l pd pe">limit = np.sqrt(6 / float(F_ini))</span><span id="47a6" class="nl kw it ox b gy pi pc l pd pe">W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))</span></pre><p id="d006" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">在正常情况下，这个:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="978a" class="nl kw it ox b gy pb pc l pd pe">limit = np.sqrt(2 / float(F_ini))</span><span id="a925" class="nl kw it ox b gy pi pc l pd pe">W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))</span></pre><h2 id="8c42" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">食谱:</h2><p id="f927" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">权重的初始化通常不是网络训练中的决定因素，但有时它会导致网络无法训练，因为它无法收敛。</p><p id="f25a" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated">所以推荐的建议是用Glorot的，如果那天你觉得运气好，想看看自己在准确率上能不能提高，那就和别人一起试试。</p><h2 id="c832" class="nl kw it bd kx nm nn dn lb no np dp lf lw nq nr lh ma ns nt lj me nu nv ll nw bi translated">最后的话</h2><p id="3bba" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">一如既往，我希望你<strong class="lp iu"> </strong>喜欢这个帖子，你现在是神经网络的专家了！</p><p id="6368" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><em class="of">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae ku" href="https://medium.com/@rromanss23" rel="noopener"> <em class="of">这里</em> </a> <em class="of">。</em></p><p id="7e0a" class="pw-post-body-paragraph ln lo it lp b lq ml ju ls lt mm jx lv lw mz ly lz ma na mc md me nb mg mh mi im bi translated"><em class="of">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="lp iu"> <em class="of">请在Medium </em> </strong> <em class="of">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>