<html>
<head>
<title>Introduction to NLP - Part 1: Preprocessing text in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP介绍——第1部分:用Python预处理文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96?source=collection_archive---------19-----------------------#2020-06-07">https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96?source=collection_archive---------19-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2500" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">欢迎来到NLP入门！这是5篇系列文章的第一部分。这篇文章将展示一种预处理文本的方法，使用一种叫做单词包的方法，其中每个文本都用它的单词来表示，而不管它们出现的顺序或嵌入的语法。预处理时，我们将完成以下步骤:</p><ol class=""><li id="3333" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">象征化</li><li id="39fd" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">正常化</li><li id="8094" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">删除停用词</li><li id="f7de" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">计数矢量</li><li id="533f" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">转换到tf-idf表示</li></ol><p id="e8d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">💤这些术语对你来说像是胡言乱语吗？不要担心，当你读完这篇文章的时候，他们已经不在了！🎓</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/7450b6d78637a26f5fe5f0d32854ecc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vtvUzAIvTdegyLJB"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">萨法尔·萨法罗夫在<a class="ae ls" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="d0da" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">0.Python设置🔧</h1><p id="a0a0" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">我假设读者(👀是的，你！)可以访问并熟悉Python，包括安装包、定义函数和其他基本任务。如果你是Python的新手，<a class="ae ls" href="https://www.python.org/about/gettingstarted/" rel="noopener ugc nofollow" target="_blank">这个</a>是一个入门的好地方。</p><p id="bd0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我已经使用并测试了Python 3.7.1中的脚本。在我们开始之前，让我们确保你有合适的工具。</p><h2 id="afee" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">⬜️确保安装了所需的软件包:pandas，<em class="np"> nltk </em> &amp; sklearn</h2><p id="29d4" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">我们将使用以下强大的第三方软件包:</p><ul class=""><li id="251a" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn nq ku kv kw bi translated"><em class="nr">熊猫</em>:数据分析库，</li><li id="9bbc" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn nq ku kv kw bi translated"><em class="nr"> nltk: </em>自然语言工具包库和</li><li id="01f3" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn nq ku kv kw bi translated"><em class="nr"> sklearn: </em>机器学习库。</li></ul><h2 id="e569" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">⬜️ <strong class="ak">从nltk下载“停用词”和“wordnet”语料库</strong></h2><p id="dedf" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">下面的脚本可以帮助你下载这些语料库。如果您已经下载了，运行此程序将通知您它们是最新的:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="76be" class="nd mb it nt b gy nx ny l nz oa">import nltk<br/>nltk.download('stopwords') <br/>nltk.download('wordnet')</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="827a" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">1.数据📦</h1><p id="9406" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">为了使事情易于管理，我们将使用微小的文本数据，这将允许我们监视每一步的输入和输出。对于这个数据，我选择了情景喜剧<a class="ae ls" href="https://www.imdb.com/title/tt0108778/" rel="noopener ugc nofollow" target="_blank">老友记</a>中乔伊为钱德勒和莫妮卡的婚礼准备的演讲稿。</p><p id="9880" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">他的演讲是这样的:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="851d" class="nd mb it nt b gy nx ny l nz oa">part1 = """We are gathered here today on this joyous occasion to celebrate the special love that Monica and Chandler share. It is a love based on giving and receiving as well as having and sharing. And the love that they give and have is shared and received. And<br/>through this having and giving and sharing and receiving, we too can share and love and have... and receive."""</span><span id="d107" class="nd mb it nt b gy ob ny l nz oa">part2 = """When I think of the love these two givers and receivers share I cannot help but envy the lifetime ahead of having and loving and giving and receiving."""</span></pre><p id="ae45" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你没有看到我提到的部分，YouTube上有一些短视频(关键词:乔伊的婚礼致辞)。我认为乔伊的表演和莫妮卡和钱德勒的反应绝对让这个演讲比单纯的文字有趣多了。写这个帖子给了我一个很好的借口来反复观看这个场景，我无法满足它。🙈</p><h1 id="b2fd" class="ma mb it bd mc md oc mf mg mh od mj mk ml oe mn mo mp of mr ms mt og mv mw mx bi translated">2.最终代码📃</h1><p id="8dfa" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">首先，让我们用包和数据准备环境:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="4f08" class="nd mb it nt b gy nx ny l nz oa"># Import packages and modules<br/>import pandas as pd<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer<br/>from nltk.corpus import stopwords<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="ad1b" class="nd mb it nt b gy ob ny l nz oa"># Create a dataframe<br/>X_train = pd.DataFrame([part1, part2], columns=['speech'])</span></pre><p id="bdfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其次，让我们定义一个文本预处理函数，将其传递给<em class="nr"> TfidfVectorizer </em>:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="8d35" class="nd mb it nt b gy nx ny l nz oa">def preprocess_text(text):<br/>    # Tokenise words while ignoring punctuation<br/>    tokeniser = RegexpTokenizer(r'\w+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]<br/>    <br/>    # Remove stopwords<br/>    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]<br/>    return keywords</span></pre><p id="dfc9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们利用前面定义的函数对文本数据进行预处理:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="ab90" class="nd mb it nt b gy nx ny l nz oa"># Create an instance of TfidfVectorizer<br/>vectoriser = TfidfVectorizer(analyzer=preprocess_text)</span><span id="8f4e" class="nd mb it nt b gy ob ny l nz oa"># Fit to the data and transform to feature matrix<br/>X_train = vectoriser.fit_transform(X_train['speech'])</span><span id="c6bb" class="nd mb it nt b gy ob ny l nz oa"># Convert sparse matrix to dataframe<br/>X_train = pd.DataFrame.sparse.from_spmatrix(X_train)</span><span id="c52b" class="nd mb it nt b gy ob ny l nz oa"># Save mapping on which index refers to which words<br/>col_map = {v:k for k, v in vectoriser.vocabulary_.items()}</span><span id="e13e" class="nd mb it nt b gy ob ny l nz oa"># Rename each column using the mapping<br/>for col in X_train.columns:<br/>    X_train.rename(columns={col: col_map[col]}, inplace=True)<br/>X_train</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oh"><img src="../Images/b0feb48c4d3af68149eb1173cca8d28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cr_HcGorvD8mQLJv0ifehQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">tf-idf矩阵(未显示所有列)</p></figure><p id="2fa5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Ta-da❕我们将文本预处理成特征矩阵。这些脚本在没有任何解释的情况下有意义吗？让我们在下一节通过例子来分解和理解开头提到的5个步骤。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="0c7c" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">3.最终代码分解和解释🔍</h1><h2 id="ad34" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated"><strong class="ak">第一步:标记化</strong></h2><blockquote class="oi oj ok"><p id="cda5" class="jq jr nr js b jt ju jv jw jx jy jz ka ol kc kd ke om kg kh ki on kk kl km kn im bi translated"><em class="it">💡</em> " <a class="ae ls" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">给定一个字符序列和一个定义好的文档单元，标记化就是把它分割成小块的任务，叫做标记，也许同时扔掉某些字符，比如标点符号。</a></p></blockquote><p id="9564" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一步，我们将把一个字符串<em class="nr"> part1 </em>转换成一个记号列表，同时去掉标点符号。我们有许多方法可以完成这项任务。我将通过使用<em class="nr"> nltk: </em>中的<em class="nr"> RegexpTokenizer </em>向您展示一种方法</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="0b87" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from nltk.tokenize import RegexpTokenizer</span><span id="83d6" class="nd mb it nt b gy ob ny l nz oa"># Create an instance of RegexpTokenizer for alphanumeric tokens<br/>tokeniser = RegexpTokenizer(r'\w+')</span><span id="88bb" class="nd mb it nt b gy ob ny l nz oa"># Tokenise 'part1' string<br/>tokens = tokeniser.tokenize(part1)<br/>print(tokens)</span></pre><p id="42c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们来看看<em class="nr">令牌</em>是什么样子的:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oo"><img src="../Images/709c1842fa95995bc604b42def5775cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0_p3u5LHBYNNmptsgc2ZQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代币</p></figure><p id="d77a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们看到每个单词现在是一个单独的字符串。你注意到同一个词有不同的用法了吗？例如:单词的大小写可以不同:“and”和“and”或它们的后缀:“share”、“shared”和“sharing”。这就是标准化的由来。</p><h2 id="321d" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">第二步。正常化</h2><blockquote class="oi oj ok"><p id="d6d5" class="jq jr nr js b jt ju jv jw jx jy jz ka ol kc kd ke om kg kh ki on kk kl km kn im bi translated"><em class="it">💡</em> <em class="it">把一个词规格化就是把它转换成它的词根形式。</em></p></blockquote><p id="76aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">词干分析和词条解释是规范化文本的常用方法。在这一步中，我们将使用lemmatisation将单词转换为它们的字典形式，并通过将所有单词转换为小写来消除大小写差异。</p><p id="cd21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">🔗如果你想了解更多关于词干和词尾的知识，你可能想看看这个系列的第二部分。</p><p id="eba3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用<em class="nr"> nltk </em>中的<em class="nr"> WordNetLemmatizer </em>来对我们的<em class="nr">令牌</em>进行符号化:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="30c3" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from nltk.stem import WordNetLemmatizer</span><span id="458d" class="nd mb it nt b gy ob ny l nz oa"># Create an instance of WordNetLemmatizer<br/>lemmatiser = WordNetLemmatizer()</span><span id="67e6" class="nd mb it nt b gy ob ny l nz oa"># Lowercase and lemmatise tokens<br/>lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]<br/>print(lemmas)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi op"><img src="../Images/9d6003152a3521db85f603df0d2ac265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfHrx3nk4tflZlU3rkrv3Q.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">前题</p></figure><p id="05c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些单词现在被转换成它的字典形式。例如，“分享”、“分享”和“共享”现在都只是“分享”。</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="c8fe" class="nd mb it nt b gy nx ny l nz oa"># Check how many words we have<br/>len(lemmas)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/bef057f2f174be4902e69f9d90a280e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:58/format:webp/1*cSxDK_6N4TTdkcdXUgmgtw.png"/></div></figure><p id="d760" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们有66个单词，但不是所有的单词对文本意义的贡献都是相同的。换句话说，有些词对关键信息不是特别有用。这就是停用词出现的地方。</p><h2 id="45e6" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">第三步。删除停用词</h2><blockquote class="oi oj ok"><p id="58a6" class="jq jr nr js b jt ju jv jw jx jy jz ka ol kc kd ke om kg kh ki on kk kl km kn im bi translated"><em class="it">💡停用词是常见的词，对文本的意义没有什么价值。</em></p></blockquote><p id="d84b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">想一想:如果你必须用三个词尽可能详细地描述你自己，你会包括“我”还是“我”？如果我让你在乔伊演讲中的关键词下面划线，你会划‘a’还是‘the’？大概不会。I、am、a和The都是停用词的例子。我想你明白了。</p><p id="2dc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据文本涉及的领域，可能需要不同的停用词集。在这一步，我们将利用<em class="nr"> nltk的停用词</em>语料库。您可以定义自己的停用字词集，或者通过添加适合文本领域的常用术语来丰富标准停用字词。</p><p id="d55d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们先稍微熟悉一下<em class="nr">的常用词</em>:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="b01d" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from nltk.corpus import stopwords</span><span id="d53a" class="nd mb it nt b gy ob ny l nz oa"># Check out how many stop words there are <br/>print(len(stopwords.words('english')))</span><span id="3bab" class="nd mb it nt b gy ob ny l nz oa"># See first 5 stop words<br/>stopwords.words('english')[:5]</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi or"><img src="../Images/b13599db8e0e889f4fbd95551b841f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*_PJRMzOQnxVWuDX2sbCvQQ.png"/></div></figure><p id="1da6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在写这篇文章的时候，nltk的停用词语料库中有179个英语停用词。一些例子包括:“我”、“我”、“我的”、“我自己”、“我们”。如果您很想看到完整的列表，只需从最后一行代码中删除<code class="fe os ot ou nt b">[:5]</code>。</p><p id="177a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意这些停用词是如何小写的？为了有效地删除停用词，我们必须确保所有单词都是小写的。在这里，我们已经在第二步中这样做了。</p><p id="acf3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用列表理解，让我们从列表中删除所有停用词<em class="nr"> </em>:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="8b7a" class="nd mb it nt b gy nx ny l nz oa">keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]<br/>print(keywords)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ov"><img src="../Images/d2ed7094735272a5a290b61d0fdb318b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SaN1oE8QSqsvE0Mb6FJrHA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">关键词</p></figure><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="61bd" class="nd mb it nt b gy nx ny l nz oa"># Check how many words we have<br/>len(keywords)</span></pre><p id="6ef2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">去掉停用词后，我们只有26个词，而不是66个，但要点仍然保留。</p><p id="8475" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，如果您向上滚动到第2部分(最终代码)并快速查看一下<code class="fe os ot ou nt b">preprocess_text</code>函数，您将会看到这个函数捕获了步骤1到3中所示的转换过程。</p><h2 id="09f2" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">第四步。计数矢量</h2><blockquote class="oi oj ok"><p id="55b8" class="jq jr nr js b jt ju jv jw jx jy jz ka ol kc kd ke om kg kh ki on kk kl km kn im bi translated"><em class="it">💡</em> <a class="ae ls" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Count vectorise是将一组文本文档转换成一个token counts的矩阵</em> </a> <em class="it">。</em></p></blockquote><p id="7aa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们来看看步骤3中<em class="nr">关键词</em>中每个单词的计数:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="d488" class="nd mb it nt b gy nx ny l nz oa">{word: keywords.count(word) for word in set(keywords)}</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c5d232d86c24e97a1619dad6daf4937c.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*_JgJdHq0_XUE9a50rZcGNA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">关键词计数</p></figure><p id="f26a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“给予”这个词出现了三次，而“快乐”只出现了一次。</p><p id="ab98" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是计数矢量器对所有记录所做的事情。<em class="nr">计数矢量器</em>通过<em class="nr"> n </em>将文本转换成一个<em class="nr"> m </em>的矩阵，其中m是文本记录的数量，n是所有记录中唯一的<em class="nr">标记</em>的数量，矩阵的元素指的是给定记录的<em class="nr">标记</em>的计数。</p><p id="8769" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一步，我们将把文本数据帧转换成计数矩阵。我们将把我们的自定义预处理器函数传递给<em class="nr">计数矢量器:</em></p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="ab19" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="9ffb" class="nd mb it nt b gy ob ny l nz oa"># Create an instance of CountfVectorizer<br/>vectoriser = CountVectorizer(analyzer=preprocess_text)</span><span id="afe9" class="nd mb it nt b gy ob ny l nz oa"># Fit to the data and transform to feature matrix<br/>X_train = vectoriser.fit_transform(X_train['speech'])</span></pre><p id="5ea4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出特征矩阵将是稀疏矩阵形式。让我们将它转换成具有适当列名的dataframe，使它更易于阅读:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="9f54" class="nd mb it nt b gy nx ny l nz oa"># Convert sparse matrix to dataframe<br/>X_train = pd.DataFrame.sparse.from_spmatrix(X_train)</span><span id="5a08" class="nd mb it nt b gy ob ny l nz oa"># Save mapping on which index refers to which terms<br/>col_map = {v:k for k, v in vectoriser.vocabulary_.items()}</span><span id="6176" class="nd mb it nt b gy ob ny l nz oa"># Rename each column using the mapping<br/>for col in X_train.columns:<br/>    X_train.rename(columns={col: col_map[col]}, inplace=True)<br/>X_train</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ox"><img src="../Images/0106c90d53621016a0c243bf3b4715aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LIRhTthp2XFgxsZ-ATN3Q.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">计数矩阵(未显示所有列)</p></figure><p id="4f86" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们将其转换为数据帧，列将只是索引(即从0到n-1的数字)，而不是实际的单词。因此，我们需要重命名这些列，以便于解释。</p><p id="2bbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当向量器适合数据时，我们可以从<code class="fe os ot ou nt b">vectoriser.vocabulary_</code>中找到单词的索引映射。此索引映射的格式为{word:index}。要重命名列，我们必须将键值对切换到{index:word}。这在第二行代码中完成，并保存在<code class="fe os ot ou nt b">col_map</code>中。</p><p id="80a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在代码末尾使用for循环，我们使用映射重命名每一列，输出应该类似于上表中的内容(由于空间限制，只显示了部分输出)。</p><p id="7108" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从这个矩阵中，我们可以看到“give”在<em class="nr"> part1(行索引=0) </em>中被提及3次，在<em class="nr"> part2(行索引=1) </em>中被提及1次。</p><p id="6a62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的例子中，我们只有2条记录，每条记录只包含少量的句子，所以计数矩阵非常小，稀疏性也不高。稀疏性是指矩阵中所有元素中零元素的比例。当您处理包含数百、数千甚至数百万条记录(每条记录都由富文本表示)的真实数据时，计数矩阵可能会非常大，并且大部分包含0。在这些情况下，使用稀疏格式可以节省存储内存，并加快进一步的处理。因此，在现实生活中预处理文本时，您可能并不总是像我们这里举例说明的那样将稀疏矩阵转换成数据帧。</p><h2 id="c7b1" class="nd mb it bd mc ne nf dn mg ng nh dp mk kb ni nj mo kf nk nl ms kj nm nn mw no bi translated">第五步。转换到TF-IDF表示</h2><blockquote class="oi oj ok"><p id="3c9c" class="jq jr nr js b jt ju jv jw jx jy jz ka ol kc kd ke om kg kh ki on kk kl km kn im bi translated"><em class="it">💡</em> tf-idf <em class="it">代表词频逆文档频率。</em></p></blockquote><p id="ad48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当转换为<em class="nr"> tf-idf </em>表示时，我们将计数转换为加权频率，其中我们通过使用一个名为<em class="nr">逆文档频率</em>的权重，对不太频繁的词赋予更高的重要性，对较频繁的词赋予较低的重要性。</p><p id="a080" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">🔗我已经在<a class="ae ls" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">系列的第三部分</a>中专门写了一个单独的帖子来详细解释这一点，因为我认为它应该有自己的一节。</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="7d49" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from sklearn.feature_extraction.text import TfidfTransformer</span><span id="aade" class="nd mb it nt b gy ob ny l nz oa"># Create an instance of TfidfTransformer<br/>transformer = TfidfTransformer()</span><span id="e341" class="nd mb it nt b gy ob ny l nz oa"># Fit to the data and transform to tf-idf<br/>X_train = pd.DataFrame(transformer.fit_transform(X_train).toarray(), columns=X_train.columns)<br/>X_train</span></pre><p id="3965" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在最后一步中，我们确保输出仍然是正确命名的数据帧:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oh"><img src="../Images/b0feb48c4d3af68149eb1173cca8d28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cr_HcGorvD8mQLJv0ifehQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">tf-idf矩阵(未显示所有列)</p></figure><p id="c709" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然我们已经分别理解了第4步和第5步，我想指出，使用<em class="nr"> TfidfVectorizer有一种更有效的方法来完成第4步和第5步。</em>这是使用以下代码完成的:</p><pre class="ld le lf lg gt ns nt nu nv aw nw bi"><span id="d158" class="nd mb it nt b gy nx ny l nz oa"># Import module<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="af0d" class="nd mb it nt b gy ob ny l nz oa"># Create an instance of TfidfVectorizer<br/>vectoriser = TfidfVectorizer(analyzer=preprocess_text)</span><span id="6dde" class="nd mb it nt b gy ob ny l nz oa"># Fit to the data and transform to tf-idf<br/>X_train = vectoriser.fit_transform(X_train['speech'])</span></pre><p id="f049" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要将这个稀疏矩阵输出到具有相关列名的dataframe中，您知道该怎么做(提示:参见我们在步骤4中所做的)。</p><p id="a5c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">介绍完所有步骤后，如果您再次回到第2部分(最终代码)中的脚本，是否会比您第一次看到时更熟悉？👀</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oy"><img src="../Images/4e5926e65fad0fcc2c2fcda060cf19da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CdcVw1-qwNjPC4Lk"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">Gabriel Beaudry 在<a class="ae ls" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5272" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="nr">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae ls" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="nr">我的推荐链接</em></a><em class="nr">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="70a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您花时间阅读这篇文章。我希望你从阅读它中学到一些东西。其余帖子的链接整理如下:<br/> ◼️ <strong class="js iu">第一部分:Python中的文本预处理</strong> <br/> ◼️ <a class="ae ls" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">第二部分:引理和词干化的区别</a> <br/> ◼️ <a class="ae ls" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">第三部分:TF-IDF解释</a> <br/> ◼️ <a class="ae ls" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener">第四部分:Python中的监督文本分类模型</a> <br/> ◼️ <strong class="js iu"> </strong> <a class="ae ls" rel="noopener" target="_blank" href="/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d">第五部分:Python中的无监督主题模型(sklearn) </a> 【T29</p><p id="447b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">快乐预处理！再见🏃💨</p><h1 id="7712" class="ma mb it bd mc md oc mf mg mh od mj mk ml oe mn mo mp of mr ms mt og mv mw mx bi translated">4.参考📁</h1><ul class=""><li id="0c02" class="ko kp it js b jt my jx mz kb oz kf pa kj pb kn nq ku kv kw bi translated"><a class="ae ls" href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener ugc nofollow" target="_blank"> Christopher D. Manning，Prabhakar Raghavan和Hinrich Schütze，<em class="nr">信息检索导论</em>，剑桥大学出版社，2008年</a></li><li id="ed32" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn nq ku kv kw bi translated"><a class="ae ls" href="http://www.nltk.org/book/" rel="noopener ugc nofollow" target="_blank">伯德、史蒂文、爱德华·洛珀和伊万·克莱恩，<em class="nr">用Python进行自然语言处理</em>。奥莱利媒体公司，2009年</a></li><li id="b36d" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn nq ku kv kw bi translated"><a class="ae ls" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank"> <em class="nr">特征提取</em>，sklearn文档</a></li></ul></div></div>    
</body>
</html>