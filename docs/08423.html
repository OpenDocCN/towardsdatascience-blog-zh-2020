<html>
<head>
<title>Finally, an intuitive explanation of why ReLU works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最后，对ReLU工作原理的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792?source=collection_archive---------8-----------------------#2020-06-19">https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792?source=collection_archive---------8-----------------------#2020-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cc8990f935b41708f1a652075bd2a372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IUy4nuPERpDaVlqR.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/surface-pattern-texture-design-945444/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="acd5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">分段线性函数如何增加非线性</h2></div><p id="c4fe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">几乎所有使用过神经网络的人都用过ReLU函数。这是激活功能的现代标准，但是关于它是如何工作的还有一些问题。神经网络需要非线性来处理非线性问题，通常唯一可以注入非线性的地方是激活函数。然而，尽管严格地说，ReLU在数学上不是线性函数，但它是由两个线性函数(分段线性)组成的。那么，直观地说，ReLU如何保持并超越真正非线性的曲线函数的性能呢？</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="0b29" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，重要的是要充分理解为什么需要在神经网络中加入非线性。在标准的神经网络中，每一层都将前一层乘以一组权重，并添加一个偏差。为了将长度为<em class="mb"> m </em>的向量转换为长度为<em class="mb"> n </em>的向量，权重采用了一个<em class="mb"> n </em>乘以<em class="mb"> m </em>矩阵(乘以一个<em class="mb"> m </em>乘以一个矩阵，或者一个向量)的形式。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mc"><img src="../Images/2b472b1f7932e99a8d15c562325476d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWQ-thLuthQctmT7oDmNDg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由作者创建。</p></figure><p id="ed04" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">即使在输入通过网络的几层之后，所应用的变换也是纯线性的；也就是说，只通过乘以一个系数并加上一个偏差来允许自由移动。因此，网络将只能解决不能被严格线性函数的有限运动所分离的问题。由于支配神经网络应用的许多复杂问题，线性神经网络的有限能力根本没有用武之地。</p><p id="719c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为一个更严格的必要性的证明，普适近似定理需要非线性激活函数才能有效。本质上，该定理指出，在一些条件下，任何连续函数<em class="mb"> f </em>都可以用具有一个隐藏层和足够数量单元的神经网络来建模。要求之一是激活函数必须具有某种形式的非线性。概括地说，为了处理非线性问题，网络中必须存在某种非线性。这是足够直观的理解。</p><p id="058b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ReLU已经成为神经网络世界的宠儿激活函数。整流线性单元的简称，它是一个分段线性函数，对于<em class="mb"> x </em>的所有负值定义为0，否则等于<em class="mb"> a </em> × <em class="mb"> x </em>，其中<em class="mb"> a </em>为可学习参数。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mh"><img src="../Images/d13cce027bff282ae0fc74354bb98fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hj_c6GoitO6kUaSHy8_i-w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用德斯莫斯画的。</p></figure><p id="6806" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种分段线性函数如何给网络增加非线性，这可能看起来不直观。毕竟还是线性的。虽然它显然不是完全线性的，但非线性的数学定义并不令人满意或直观。与激活函数宝座的其他竞争者相比——soft max、sigmoid和tanh——ReLU缺乏干净的曲线，相反似乎只是线性函数的兄弟。然而，ReLU成功的关键在于它如何解决环空问题:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mi"><img src="../Images/079bf5b5f35cc39bef8e0eb6fc96c8fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKjPz4KaEERCpvI04D6Bng.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">每个神经网络有三个隐藏层，每层有三个单元。唯一的区别是激活功能。学习率:0.03，正规化:L2。由作者创建。</p></figure><p id="71b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">tanh是一个平滑的曲线函数，它在圆的周围画出一个干净的包络线(而linear完全失败)，而ReLU画出一个六边形，有几个尖角。其实这就是ReLU的优势所在:可以在某一点弯曲线性函数，到一定程度。结合前一层的偏差和权重，ReLU可以在任何位置以任何角度弯曲。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/f173e96a10472e4b27c6a4bb87695872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-wvog2yVpHHFvCQJj05EQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用德斯莫斯画的。</p></figure><p id="ca71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些小弯曲构成了近似法的基础。任何关系或函数都可以通过将许多ReLU函数聚合在一起来粗略估计，这发生在神经元在下一层中被折叠和组合的时候。这已经在数学上得到证明，例如，用正弦波或指数函数，很像低阶泰勒级数如何拟合函数。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/d36e5fe1d46dd7aa97128225ac80103b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5E_SBTA2gLBrGZX7qj1now.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">酷似乙状结肠。用德斯莫斯画的。</p></figure><p id="babd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们可能倾向于指出，ReLUs不能外推；也就是说，拟合为类似于-4&lt; <em class="mb">x</em>T6】4的正弦波的一系列ReLUs将不能在这些界限之外继续正弦波的<em class="mb"> x </em>的值。然而，重要的是要记住，神经网络的目标不是外推，而是概括。例如，考虑一个基于浴室数量和卧室数量来预测房价的模型。如果模型很难将该模式应用到浴室数量的负值或卧室数量超过500的负值，这并不重要，因为这不是模型的目标。(你可以在这里阅读更多关于一般化vs外推<a class="ae jg" href="https://medium.com/swlh/real-artificial-intelligence-understanding-extrapolation-vs-generalization-b8e8dcf5fd4b" rel="noopener">。)</a></p><p id="efa2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ReLU功能的强大不在于它本身，而在于一整支ReLU大军。这就是为什么在神经网络中使用几个ReLUs不会产生令人满意的结果；相反，必须有大量的ReLU激活，以允许网络构建一个完整的点地图。在多维空间中，校正的线性单元沿着类边界组合形成复杂的多面体。</p><p id="b544" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ReLU工作得如此好的原因在于:当有足够多的ReLU时，它们可以近似任何函数，就像其他激活函数一样，如sigmoid或tanh，就像堆叠数百个乐高积木一样，而没有缺点。平滑曲线函数有几个问题是ReLU不会出现的，其中之一是计算导数或变化率，即梯度下降背后的驱动力，ReLU比任何其他平滑曲线函数都要便宜得多。</p><p id="d6e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个是sigmoid和其他曲线有一个消失梯度的问题；因为对于更大的绝对值<em class="mb"> x </em>，sigmoid函数的导数逐渐变缓。因为输入的分布可能在训练期间较早地偏离0，所以导数将非常小，以至于没有有用的信息可以反向传播来更新权重。这通常是神经网络训练中的一个主要问题。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ml"><img src="../Images/e366cec1f718843ac73091198bac42a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cHMu8hdRtXNHklzdy99Qw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用德斯莫斯画的。</p></figure><p id="0155" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，ReLU函数的导数很简单；它是输入所在直线的斜率。它将可靠地返回有用的梯度，尽管<em class="mb">x</em>= 0 {<em class="mb">x</em>T6】0 }有时可能会导致“死神经元问题”，但总体而言，ReLU仍然表现出不仅比曲线函数(sigmoid，tanh)更强大，而且比试图解决死神经元问题的ReLU变体(如Leaky ReLU)更强大。</p><p id="30cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ReLU设计用于大量工作；在大容量的情况下，它近似得很好，并且在近似得很好的情况下，它的表现与任何其他激活功能一样好，没有缺点。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="9989" class="mm mn jj bd mo mp mq mr ms mt mu mv mw kp mx kq my ks mz kt na kv nb kw nc nd bi translated">如果你喜欢这个，</h1><p id="7cf8" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">你可能也会喜欢我关于深度学习的一些其他文章。</p><div class="is it gp gr iu nj"><a rel="noopener follow" target="_blank" href="/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd jk gy z fp no fr fs np fu fw ji bi translated">批量规范化:深度学习的最大突破</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">它是如何工作的——又是如何如此有效的？</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ja nj"/></div></div></a></div><div class="is it gp gr iu nj"><a href="https://medium.com/analytics-vidhya/gans-for-everyone-an-intuitive-explanation-of-the-revolutionary-concept-2f962c858b95" rel="noopener follow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd jk gy z fp no fr fs np fu fw ji bi translated">每个人的GANs</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">革命性人工智能概念的直观解释</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">medium.com</p></div></div><div class="ns l"><div class="ny l nu nv nw ns nx ja nj"/></div></div></a></div></div></div>    
</body>
</html>