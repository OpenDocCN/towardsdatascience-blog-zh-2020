<html>
<head>
<title>The Good Old Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">传统的梯度推进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-good-old-gradient-boosting-f4614b0e62b0?source=collection_archive---------35-----------------------#2020-02-02">https://towardsdatascience.com/the-good-old-gradient-boosting-f4614b0e62b0?source=collection_archive---------35-----------------------#2020-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="49f2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="http://towardsdatascience.com/tagged/the-gradient-boosters" rel="noopener" target="_blank">梯度助推器</a></h2><div class=""/><div class=""><h2 id="a86b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">梯度推进的数学入门</h2></div><p id="501c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2001年，杰罗姆·h·弗里德曼写了一篇开创性的论文——贪婪函数逼近:梯度推进机。他一点也不知道这将演变成一类方法，威胁到表格世界中沃伯特的<a class="ae ln" href="https://chemicalstatistician.wordpress.com/2014/01/24/machine-learning-lesson-of-the-day-the-no-free-lunch-theorem/" rel="noopener ugc nofollow" target="_blank">没有免费的午餐定理</a>。Gradient Boosting及其同类产品(XGBoost和LightGBM)在分类和表格数据领域的回归问题上表现出色，已经征服了世界。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/8372e7a5722fd4aff1b5a093d01cca7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*rXiBqy5H0c3naMPj"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">不尽然！(来源:<a class="ae ln" href="https://memecreator.org/meme/xgboost-all-the-things5" rel="noopener ugc nofollow" target="_blank">我自己古怪的大脑</a>)</p></figure><p id="fed0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们从理解弗里德曼提出的经典梯度推进方法开始。尽管这是一个数学难题，但也没那么难。只要有可能，我都试图对正在发生的事情提供直觉。</p><h1 id="334f" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">问题设置</h1><p id="5293" class="pw-post-body-paragraph kr ks it kt b ku ms kd kw kx mt kg kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">假设有一个数据集<em class="mx"> D </em>有<em class="mx"> n </em>个样本。每个样本在向量<em class="mx"> x </em>和实值目标<em class="mx"> y </em>中具有<em class="mx"> m </em>组特征。形式上，它被写成</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/cbd5b6d06f6bff51fb7923e4e03d318b.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/0*d4yGd0z5fwiOGvZe"/></div></figure><p id="1851" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，梯度推进算法是一种采用加法形式的集成方法。直觉告诉我们，我们试图估算的复杂函数可以由更小更简单的函数相加而成。</p><p id="9170" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">假设我们试图逼近的函数是</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e46a7b027bbbe099a8f2291357835870.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/0*8rVfFyDtKjnygimf"/></div></figure><p id="0079" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以把这个函数分解为:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/4808ae92cbf0c61fc8a50aeae19e2810.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/0*Yzp6tkNr3rLTn7-e"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a977edc9921f59275efd1089762d1b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/0*IV_0vO6BFHOO_Mqz"/></div></figure><p id="83b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是我们在选择加性集成模型时采用的假设，并且我们在讨论梯度增强时通常谈到的树集成可以写成如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8d218bda468cf16a07233ecc6ea16005.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/0*bWoGLT-UENE8nknw"/></div></figure><p id="b247" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中M是基础学习器的数量，F是回归树的空间。</p><h1 id="73b0" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">损失函数</h1><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6a059f7270675305120de01b116a6036.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/0*_hye0z_WVFOINkQ_"/></div></figure><p id="f3eb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中<em class="mx"> l </em>为可微凸损失函数<em class="mx"> f(x)。</em></p><p id="0dbb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因为我们在寻找f(x)的加法函数形式，我们可以用以下等式代替yᵢ</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9b73d537bbf91644e4a81f5914327ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/0*Vot_Stl3N9tuianD"/></div></figure><p id="de04" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以，损失函数将变成:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/eeb4b69f49a7f85eb924245a6a29702d.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/0*RWo6CAd2pWtWVX_T"/></div></figure><h1 id="bd05" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">算法</h1><ol class=""><li id="642a" class="nk nl it kt b ku ms kx mt la nm le nn li no lm np nq nr ns bi translated">通过最小化损失函数，用常数值初始化模型</li></ol><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/468cc6265cca3df0c4e1faf3a56857d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/0*0GI_iqXlmA5VC32V"/></div></figure><ul class=""><li id="2209" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated">b₀是在第0次迭代时最小化损失函数的模型的预测</li><li id="d82d" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">对于平方误差损失，它是所有训练样本的平均值</li><li id="7a5d" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">对于最小绝对偏差损失，它是所有训练样本的中间值</li></ul><p id="eeb5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.对于m=1至M:</p><p id="496e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.1计算</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6a160a40c4cd365ab6697eb2ab0681e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/0*lO9DgAbZ-Hqh4GeG"/></div></figure><ul class=""><li id="3bf1" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated"><em class="mx"> rᵢₘ </em>只不过是损失函数(在真实值和来自最后一次迭代的输出之间)的导数w.r.t. <em class="mx"> F(x) </em>来自最后一次迭代</li><li id="650b" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">对于平方误差损失，这就是残差<em class="mx">(观察值—预测值)</em></li><li id="920c" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">它也被称为伪残差，因为它的行为类似于残差，并且它是平方误差损失函数的残差</li><li id="ea25" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">我们为所有的<em class="mx"> n </em>个样本计算<em class="mx"> rᵢₘ </em></li></ul><p id="e5a2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.2使用基尼系数或熵(通常的方法)将回归树拟合到<em class="mx"> rᵢₘ </em>值</p><ul class=""><li id="2738" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated">对于j = 1……Jₘ，树的每片叶子由Rⱼₘ表示，其中jₘ是在迭代m中创建的树的叶子数量</li></ul><p id="b208" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.3对于j = 1 … Jₘ，计算</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f0a7db5bd4d954e6214562458e9ea302.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/0*T_kV8H4jxwUtT_RM"/></div></figure><ul class=""><li id="d669" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated">bⱼₘ是基函数或最小平方系数。这可以方便地计算出任何叶中所有样本的平均误差损失和最小绝对偏差损失的中值</li><li id="1354" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">ρₘ是叶重的比例因子。</li><li id="5420" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">由于回归树的不相交性质，可以忽略<em class="mx"> b </em>上的内部求和。一个特殊的样本只会出现在其中一片叶子上。因此，等式简化为:</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6378c08dc2f918ff9537a0e32848ba5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/0*KeL_xi5_7s5K-58R"/></div></figure><ul class=""><li id="051e" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated">因此，对于每片叶子，Rⱼₘ，我们计算最佳值ρ，当加入到最后一次迭代的预测中时，最小化驻留在叶子中的样本的损失</li><li id="227d" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">对于已知的损失函数，如平方误差损失和最小绝对偏差损失，比例因子为1。正因为如此，标准GBM实施忽略了比例因子。</li><li id="696d" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">对于一些损耗，如Huber损耗，使用线搜索找到最小损耗来估算ρ。</li></ul><p id="ddf3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.4更新</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c2b6473331d3e48ae0e6dd0367fcaafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/0*nb6rs0k_unpq2pgg"/></div></figure><ul class=""><li id="3f7c" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated">现在，我们将最新的优化树添加到上一次迭代的结果中。</li><li id="82f5" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">η是收缩率或学习率</li><li id="ba3e" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">等式中的求和仅在特定样本出现在多个节点的极小概率情况下有用。否则只是优化后的回归树得分<em class="mx"> b </em>。</li></ul><h1 id="8d7f" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">正规化</h1><p id="9922" class="pw-post-body-paragraph kr ks it kt b ku ms kd kw kx mt kg kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在标准实现(Sci-kit Learn)中，没有实现目标函数中的正则化项。在那里实施的唯一正规化如下:</p><ul class=""><li id="3e25" class="nk nl it kt b ku kv kx ky la nu le nv li nw lm nx nq nr ns bi translated"><strong class="kt jd">收缩正则化</strong> —在加法公式中，每个新的弱学习器被“收缩”一个因子η。在某些实现中，这种收缩也被称为学习速率，因为它类似于神经网络中的学习速率。</li><li id="0e16" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated"><strong class="kt jd">行子采样</strong> —集合中的每个候选树使用样本子集。这具有规则化的效果。</li><li id="93e1" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated"><strong class="kt jd">列子采样</strong> —集合中的每个候选树使用一个特征子集。这也有一个正则化的效果，通常更有效。这也有助于并行化。</li></ul><h1 id="174b" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">梯度推进和梯度下降</h1><h1 id="59e1" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">相似之处</h1><p id="5bf8" class="pw-post-body-paragraph kr ks it kt b ku ms kd kw kx mt kg kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">我们知道梯度推进是一个加法模型，可以表示如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0c10dbdaebb2f1d134d433d36f0faa61.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/0*yndl_-u2MKXVhq-N"/></div></figure><p id="9ea9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中<em class="mx"> F </em>是集成模型，<em class="mx"> f </em>是弱学习器，<em class="mx"> η </em>是学习率，<em class="mx"> X </em>是输入向量。</p><p id="ff57" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">用y^代替f，我们得到熟悉的等式，</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f2a461cc5f2589417a5032d7c1ecda3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/0*sMNVkETF-B9EFWdS"/></div></figure><p id="5de5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，由于<em class="mx"> fₘ(X) </em>是通过最小化损失函数在每次迭代中获得的，损失函数是一阶和二阶梯度(导数)的函数，我们可以直观地将其视为指向最陡下降的方向向量。让我们称这个方向向量为rₘ₋₁.下标是m-1，因为向量已经在迭代的阶段m-1被训练。或者，直觉上的剩余</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/16b7250de4912c151c6cfd6dc38ba019.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*uzxNhaqaOGffz38P"/></div></figure><p id="dc98" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">。所以等式现在变成了:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e41a0e09e68f11b8b54c51bedbb3353d.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/0*O-4P77fB7kAz5fS6"/></div></figure><p id="7bf8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">翻转标志，我们得到:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d799609ae496a499f452cd70eea99871.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/0*ubw99gAECEcHApaM"/></div></figure><p id="480a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们看看标准的梯度下降方程:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/ecd896acd9bbbc140800d86eda24bf09.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/0*cPUBX0C0RgG-8ON8"/></div></figure><p id="ee92" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以清楚地看到相似之处。这个结果使我们能够使用任何可微损失函数。</p><h1 id="c104" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">差别</h1><p id="e00b" class="pw-post-body-paragraph kr ks it kt b ku ms kd kw kx mt kg kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">当我们使用梯度下降来训练神经网络时，它试图找到最佳参数(权重和偏差)，<strong class="kt jd"><em class="mx"/></strong>，这使得损失函数最小化。这是利用损耗相对于参数的梯度来完成的。</p><p id="d06a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是在梯度提升中，梯度仅调整创建集成的方式，而不调整底层基本学习器的参数。</p><p id="444a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">而在神经网络中，梯度直接给出了损失函数的方向向量，在Boosting中，我们只能从弱学习器中得到该方向向量的近似。因此，GBM的损耗只可能单调减少。随着迭代的进行，损耗完全有可能跳跃一点。</p><h1 id="41f6" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">履行</h1><p id="879a" class="pw-post-body-paragraph kr ks it kt b ku ms kd kw kx mt kg kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">Sci-kit Learn中的<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" rel="noopener ugc nofollow" target="_blank">GradientBoostingClassifier</a>和<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" rel="noopener ugc nofollow" target="_blank">GradientBoostingRegressor</a>是python生态系统中最早的实现之一。这是一个直截了当的实现，忠实于原始文档。我很清楚我们之前的讨论。它已经实现了多种损失函数，其中<em class="mx">贪婪函数近似:Friedman的梯度推进机</em>【1】已经导出了算法。</p><h2 id="a358" class="on mb it bd mc oo op dn mg oq or dp mk la os ot mm le ou ov mo li ow ox mq iz bi translated">回归损失</h2><ul class=""><li id="ddcd" class="nk nl it kt b ku ms kx mt la nm le nn li no lm nx nq nr ns bi translated">ls’→最小二乘法</li><li id="4807" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">lad’→最小绝对偏差</li><li id="595d" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">“胡伯”→胡伯损失</li><li id="539e" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">分位数→分位数损失</li></ul><h2 id="a115" class="on mb it bd mc oo op dn mg oq or dp mk la os ot mm le ou ov mo li ow ox mq iz bi translated">分类损失</h2><ul class=""><li id="ec6f" class="nk nl it kt b ku ms kx mt la nm le nn li no lm nx nq nr ns bi translated">“偏差”→逻辑回归损失</li><li id="8f95" class="nk nl it kt b ku ny kx nz la oa le ob li oc lm nx nq nr ns bi translated">“指数”→指数损失</li></ul><h1 id="e597" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">参考</h1><ol class=""><li id="0d8c" class="nk nl it kt b ku ms kx mt la nm le nn li no lm np nq nr ns bi translated">贪婪函数近似:一种梯度推进机器。安。统计学家。29 (2001年)，第5号，1189-1232。</li></ol></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="b3e7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="mx">原载于2020年2月2日http://deep-and-shallow.com</em><a class="ae ln" href="https://deep-and-shallow.com/2020/02/02/the-gradient-boosters-i-the-math-heavy-primer-to-gradient-boosting-algorithm/" rel="noopener ugc nofollow" target="_blank"><em class="mx"/></a><em class="mx">。</em></p></div></div>    
</body>
</html>