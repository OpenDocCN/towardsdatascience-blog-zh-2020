# 使用 Unity ML-agent 的课程学习

> 原文：<https://towardsdatascience.com/curriculum-learning-with-unity-ml-agents-c8e7a1aa5415?source=collection_archive---------25----------------------->

## 教一队猎人捕捉猎物。

![](img/13437211bcb74827a30af77d72416493.png)

要查看我为本文编写的代码，请点击这里查看 GitHub repo:[https://github.com/adamprice97/UnityPursitEvasionGame](https://github.com/adamprice97/UnityPursitEvasionGame)

## 介绍

当一个人面临一个新问题时，他们需要足够的背景知识来解决它。你不能指望有人不先学几何就去学微积分。我们的强化学习代理也是如此！

在本文中，我将展示一个具有挑战性的游戏环境，并向您展示如何使用 Unity ML-Agents 和课程学习来解决这个问题。

# 追逐-逃避游戏

追逐-逃避游戏是一系列问题，要求一组代理人追踪另一组成员。在我们的实现中，我们将训练一组 3 个猎人来捕捉一个猎物。每一集，猎人们在森林空地的边缘产卵，他们的目标是在猎物逃进树林之前抓住它！

## 奖励信号

我们可以通过奖励代理在环境中完成任务来影响他们的学习。为了让代理在培训中取得成功，我们需要定义一套奖励，引导他们找到游戏的解决方案。

![](img/f2c6ec24dece4678689ea3b604d14e0c.png)

猎人捕获了猎物。

![](img/f5e02f67bb78b883df313d94028a988b.png)

猎物逃脱了。

*   **捕获猎物(+5)。**这三名特工必须都足够靠近猎物才能将其捕获。
*   **猎物逃跑(-3)。如果猎物逃进了树里，猎人将会受到惩罚。**
*   代理离开清算(-1)。这将鼓励代理在训练中远离空地的边缘，但惩罚足够小，不会阻止代理将猎物追逐到边缘。
*   **每个时间步长(-0.001)。这鼓励了代理人迅速捕捉猎物。**

## 观察和行动

为了在环境中导航，代理需要采取行动。这些操作让代理直接与环境交互，并由代理的策略选择。该策略是代理观察的函数，它输出代理要执行的动作。

我们的猎人进行并采取以下观察和行动:

![](img/fd0e5960a0c5e82e960b60a845749652.png)

猎人特工的观察和行动向量。

这种环境是完全可以观察到的，因为每个智能体都知道自己、其他智能体和猎物的确切位置。您还会注意到观察向量是堆叠的，前两个观察向量通知当前要采取的行动。这样做可以让代理推断每个实体的速度信息。

我们的代理人的行动让他们在 Z 和 X 平面上前后推动自己，让他们在固定的平面上完全移动。

## 猎物启发式

最后，环境的最后一个要素是猎物。它是由一个简单的启发式算法控制的，这个算法将直接远离猎人代理的平均位置。启发法很简单，但是代理必须作为一个团队工作，因为需要一个交叉点来捕捉快速的猎物。

# 培养

代理人需要找到最大化奖励信号的行动。为此，我们使用神经网络作为代理的策略。然后，在训练期间，奖励信号用于更新神经网络和改进代理的策略。

![](img/f7fd554cca536bc61841c2d3bfe40239.png)

训练配置。

我们需要一种算法来处理这些神经网络的训练。幸运的是，Unity ML-Agents 提供了 Open-AI 的近似策略优化(PPO)算法的实现。

我们为 Unity 提供了一个配置文件来开始训练。有很多变量需要考虑，每一个都会对训练产生巨大的影响。有关所有这些的信息，请查阅 Unity 的文档。

PPO 是一个强大的算法，但是我们的环境太复杂了，一个代理不可能盲入。我们需要为我们的代理创造一个更好的学习环境。

## 课程学习

复杂的环境可能要求代理在开始利用环境的奖励信号之前掌握多个概念。这些概念可能如此先进，以至于随机探索将无法可靠地强化为获得奖励而采取的行动。我们的追逃游戏就是一个例子。猎物很容易逃脱天真的特工，所以猎人永远不会发现他们捕获它会得到的奖励。

![](img/9d2e62a6351d0e9b364f22402b238378.png)

未经训练的特工不太可能随机捕捉猎物。

**这就是课程学习的用武之地！**

我们没有让代理在困难的环境中放松，而是首先向他们介绍一个更简单的游戏版本。从那里开始，我们可以慢慢增加难度，逐步增加难度。

我们的追逃游戏的难度由两个参数控制:

*   **猎物速度***——*施加在猎物身上使其移动的标量力。越高，它跑得越快。
*   **捕捉范围—** 猎人特工能从猎物处捕捉到的最大距离。

我们的第一课需要非常简单；新特工需要先学会走路，然后才能打猎。我们从一个固定的目标(猎物速度= 0)和一个非常宽的捕捉范围开始。课程结束后，我们会缩小捕捉范围。

![](img/36864607a551a831d4ee446f44f5e470.png)

猎人与猎物之间的距离在两堂课之间减少。

一旦代理完全掌握了导航到猎物的方法，我们就可以开始让猎物以逐渐增加的速度移动:

![](img/d0ba88597d384c10fa5657702497aae3.png)

有了我们的课程计划，我们只需要定义通过一课的标准。幸运的是，这很容易。当代理在一定数量的剧集中累积的奖励值超过阈值时，我们认为每一课都“通过”。在我们的培训中，当代理在 200 集内每集平均获得超过 4 个奖励时，课程就通过了。在实践中，代理人需要在 9 次中捕获 8 次猎物(超过 200 集)才能通过一堂课。

## 运行时间

我们在一个加速的 unity 环境中训练代理，以减少实时训练时间。我以 2.5 倍的速度训练了 300 万个时间步。如果您想观看代理培训，您可以观看下面的视频:

Unity ML-Agents 制作了许多 tensorboard 图表来告知我们培训的进展情况，但这两个图表对我们来说是最重要的:

![](img/165f6548ee579ea59f54a92ecfca8d30.png)

超过 300 万步的奖励累积和课程进度。

你可能已经注意到奖励积累的异常学习曲线。通常情况下，我们希望奖励积累在整个培训过程中不断增加。然而，当使用课程学习时，我们环境的回报变得更难获得，因此获得它变得更困难。我们仍然可以看到每一课的个人学习曲线。第 14 课大约 60 万就是一个很好的例子。

## 结果

我们可以通过代理通过的课程数量来衡量他们的成功。前 300 万步的训练让他们通过了前 37 课。不幸的是，这意味着代理仍然比猎物快，这在一定程度上降低了团队合作的需要。所以，我又训练了 300 万个时间步。代理人达到第 59 课，猎物比猎人更快，所以团队行为出现了。

![](img/e6b28c42e8c1807eb1e77f7d94470d5f.png)![](img/2ba5400cc42c09b88b673273f27c7383.png)

600 万集后的表现。

左边的 gif 显示黄色特工移动进行拦截，而另外两个猎人在追击。

你会注意到代理人面对的方向是他们最后用力的方向。黄色特工也有效地学会了突破。如果猎物调整路线，这将是拦截猎物的关键。

我希望我已经阐明了学习课程的力量。即使我们的学习算法在过去几年里有了很大的改进，我们仍然不能指望它们自己解决所有问题。