<html>
<head>
<title>Slow and Arbitrary Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">缓慢而随意的风格转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/slow-and-arbitrary-style-transfer-3860870c8f0e?source=collection_archive---------45-----------------------#2020-08-20">https://towardsdatascience.com/slow-and-arbitrary-style-transfer-3860870c8f0e?source=collection_archive---------45-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="70ea" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">神经类型转移，进化</h2><div class=""/><div class=""><h2 id="e7bd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用卷积神经网络的图像风格转换</h2></div><h1 id="2d3b" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">介绍</h1><p id="7fd8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">风格转移是组合两个图像的技术，一个<strong class="ll jd"> <em class="mf">内容</em> </strong>图像和一个<strong class="ll jd"> <em class="mf">风格</em> </strong>图像，使得<strong class="ll jd"> <em class="mf">生成的</em> </strong>图像显示其两个成分的属性。<strong class="ll jd"> <em class="mf">目标</em> </strong>是生成在风格(例如，颜色组合、笔触)上与风格图像相似并且在结构上与内容图像相似(例如，边缘、形状)的图像。</p><p id="e881" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在这篇文章中，我们描述了一种基于<strong class="ll jd"> <em class="mf">优化的</em> </strong>方法，该方法由 Gatys <em class="mf">等人</em>在其开创性的工作“使用卷积神经网络 进行<strong class="ll jd"> <em class="mf">图像风格转换”中提出。但是，让我们先来看看导致最终解决方案的一些构件。</em></strong></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ml"><img src="../Images/5267a122d09d6d32524afc96c6fc9412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CFIa1aLeOvl6Eqnc9zqDQ.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">图一。<strong class="bd nb">演示</strong>，图像取自“[R2]实时风格转换和超分辨率的感知损失”</p></figure><h1 id="7544" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">CNN 在学什么？</h1><p id="5445" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">一开始，你可以把<strong class="ll jd"> <em class="mf">低级</em> </strong>特征想象成在一个<strong class="ll jd"> <em class="mf">放大的</em> </strong> <em class="mf"> </em>图像中可见的特征。相比之下，<strong class="ll jd"> <em class="mf">高级</em> </strong> <em class="mf"> </em>特征可以在图像<strong class="ll jd"> <em class="mf">缩小</em> </strong>时最佳观看。现在，计算机如何知道如何区分图像的这些细节？CNN，来救援。</p><p id="78d9" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">预训练卷积神经网络的学习滤波器是优秀的通用图像特征提取器。CNN 的不同层提取不同尺度的特征。<strong class="ll jd"> <em class="mf">浅层</em> </strong>中的隐藏单元只看到输入图像相对较小的一部分，它提取<strong class="ll jd"> <em class="mf">低级</em> </strong> <em class="mf"> </em>特征，如边缘、颜色和简单纹理。<strong class="ll jd"><em class="mf"/></strong>更深的层次，然而，具有更广感受野的人倾向于提取<strong class="ll jd"><em class="mf"/></strong><em class="mf"/>等高级特征，如形状、图案、错综复杂的纹理，甚至是物体。</p><p id="e3cf" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">那么，我们如何利用这些特征提取器进行风格转换呢？T71】</p><h1 id="0b92" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">深层图像表示</h1><p id="b476" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在卷积神经网络中，具有<em class="mf"> N </em>个不同滤波器(或，<em class="mf"> C </em>通道)<em class="mf">T5】的层具有<em class="mf"> N </em>(或，<em class="mf"> C </em> ) <em class="mf"> </em>个大小为<em class="mf"> H </em> x <em class="mf"> W、</em>的特征映射，其中<em class="mf"> H </em>和<em class="mf"> W </em>分别是特征激活映射的高度和宽度。该层的特征激活是形状为<em class="mf">N</em>x<em class="mf">H</em>x<em class="mf">W</em>的体积(或者，<em class="mf">C</em>x<em class="mf">H</em>x<em class="mf">W</em>)<em class="mf">。</em>让我们看看如何使用这些激活从单个图像中分离出<strong class="ll jd"> <em class="mf">内容</em> </strong>和<strong class="ll jd"> <em class="mf">样式</em> </strong>信息。</em></p><h2 id="4d3b" class="nc ks it bd kt nd ne dn kx nf ng dp lb ls nh ni ld lw nj nk lf ma nl nm lh iz bi translated">内容表示</h2><p id="9c30" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">传统上，两幅图像之间的<strong class="ll jd"> <em class="mf">相似度</em> </strong>是使用像素空间中的<strong class="ll jd"> <em class="mf"> L1/L2 </em> </strong>损失函数来测量的。虽然这些损失很好地衡量了<strong class="ll jd"><em class="mf"/></strong><em class="mf"/>的低级相似性，但它们没有捕捉到<strong class="ll jd"><em class="mf"/></strong><em class="mf"/>图像之间的差别。例如，彼此偏移一个像素的两个相同的图像，尽管在感觉上相似，但是将具有高的每像素损失。</p><p id="8c0b" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">直观上，如果两幅图像的卷积特征激活是相似的，那么它们在感知上应该是<em class="mf"> </em>相似的。因此，我们将网络的特征响应称为<strong class="ll jd"> <em class="mf">内容表示</em> </strong>，两幅图像的特征响应之差称为<strong class="ll jd"> <em class="mf">感知损失。</em> </strong>为了找到原始<strong class="ll jd"> <em class="mf">内容</em> </strong>图像的<strong class="ll jd"> <em class="mf">内容重构</em> </strong>，我们可以对触发相似特征响应的白噪声图像执行梯度下降。</p><p id="19ed" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">通过仅匹配来自特定层的特征响应来生成<strong class="ll jd"><em class="mf"/></strong>内容重构，可以可视化由网络的不同层捕获的特征的规模(参见图 2)。较低层的重建几乎是完美的(a，b，c)。在网络的较高层中，详细的像素信息丢失，而高级内容被保留(d，e)。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nn"><img src="../Images/5d635ff0c7237e22fdc647ccaf84d022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySDhHvF_AH6S8h48Ofq0-w.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">图二。<strong class="bd nb">内容和风格表示</strong>，图像取自“[R1]使用卷积神经网络的图像风格转换”</p></figure><h2 id="6453" class="nc ks it bd kt nd ne dn kx nf ng dp lb ls nh ni ld lw nj nk lf ma nl nm lh iz bi translated">风格表现</h2><p id="13c4" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了获得输入图像风格的表示，在网络的每一层中的滤波器响应之上建立特征空间。它由特征图的空间范围内不同滤波器响应之间的<strong class="ll jd"> <em class="mf">相关性</em> </strong>组成。数学上，不同滤波器响应之间的相关性可以计算为两个激活图的<strong class="ll jd"> <em class="mf">点积</em> </strong>。形式上，图像的<strong class="ll jd"> <em class="mf">风格表示</em> </strong>可以由<strong class="ll jd"> <em class="mf"> Gram 矩阵</em> </strong>(参见图 3)捕获，该矩阵捕获所有特征激活对的相关性。对于一层中的<em class="mf"> N </em>个滤镜，克矩阵是一个<em class="mf"> N </em> x <em class="mf"> N </em>维矩阵。</p><ol class=""><li id="4ff3" class="no np it ll b lm mg lp mh ls nq lw nr ma ns me nt nu nv nw bi translated">在 Gram 矩阵的位置<em class="mf"> (i，i) </em>处的对角元素测量过滤器<strong class="ll jd"><em class="mf"/></strong><em class="mf">I</em>如何活动。假设滤波器<em class="mf"> i </em>正在检测图像中的垂直边缘，那么<em class="mf"> (i，</em> <em class="mf"> i) </em>处的高值意味着图像具有许多垂直边缘。</li><li id="3706" class="no np it ll b lm nx lp ny ls nz lw oa ma ob me nt nu nv nw bi translated">Gram 矩阵的位置<em class="mf"> (i，j) </em>处的值测量两个不同滤波器<em class="mf"> i </em>和<em class="mf"> j </em>的激活的<strong class="ll jd"> <em class="mf">相似度</em> </strong>。换句话说，这意味着由两个过滤器<em class="mf"> i </em>和<em class="mf"> j. </em>捕获的特征的<strong class="ll jd"> <em class="mf">共存</em> </strong></li></ol><p id="5859" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">通过捕捉不同类型的特征<em class="mf"> (i，i) </em>的流行程度，以及有多少不同的特征一起出现<em class="mf"> (i，j) </em>，Gram Matrix 测量图像的风格。本质上，通过丢弃存储在功能激活图中每个位置的空间信息，我们可以成功地提取样式信息。</p><div class="mm mn mo mp gt ab cb"><figure class="oc mq od oe of og oh paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/74d21debfc91b25ffe2006d95cda9013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*mbeEuJLyw_2atq3pi1PgAA.png"/></div></figure><figure class="oc mq oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/064aa06bc8d3ed4cdf4b5b2f2af56c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*NDSV5qAAkprp71pIewABcw.png"/></div><p class="mx my gj gh gi mz na bd b be z dk oj di ok ol translated">图三。<strong class="bd nb"> Gram Matrix </strong>，图片取自吴恩达的“卷积神经网络”课程</p></figure></div><p id="3c45" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">类似于内容重建，<strong class="ll jd">风格重建</strong>可以通过最小化随机白色图像和参考风格图像的 Gram 矩阵之间的差异来生成(参见图 2)。这创建了在增加的比例上匹配给定图像的风格的图像，同时丢弃了场景的全局排列的信息。</p><p id="7a69" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">现在我们已经有了定义损失函数的所有关键要素，让我们直接进入它。</p><h1 id="698d" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">损失函数</h1><p id="634f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">设<strong class="ll jd"> <em class="mf"> C，s，</em> </strong>和<strong class="ll jd"> <em class="mf"> G </em> </strong>为原始内容图像，原始风格图像和生成的图像，<strong class="ll jd"> <em class="mf"> aᶜ，</em> </strong>和<strong class="ll jd"> <em class="mf"> aᴳ </em> </strong>从一个预先训练好的 CNN 的层<strong class="ll jd"> <em class="mf"> l </em> </strong>中激活它们各自的特征。</p><h2 id="cd66" class="nc ks it bd kt nd ne dn kx nf ng dp lb ls nh ni ld lw nj nk lf ma nl nm lh iz bi translated">内容损失</h2><p id="8444" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">如图 4 所示，内容损失可以定义为<strong class="ll jd"> <em class="mf">内容</em> </strong>和<strong class="ll jd"> <em class="mf">生成的</em> </strong>图像的特征表示之间的平方误差损失。沿着 CNN 的处理层级，输入图像被转换成对图像的实际<strong class="ll jd"> <em class="mf">内容</em> </strong>越来越敏感但对其精确外观相对不变的表示。在实践中，我们可以通过选择网络中间某处的层<strong class="ll jd"> <em class="mf"> l </em> </strong>来最好地捕捉图像的内容。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi om"><img src="../Images/67c5b682b663657eefd5de5dbaeaa4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*xTZeYzB9XLQpszxqH0ZZwA.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">图 4。<strong class="bd nb">内容丢失</strong></p></figure><h2 id="e7c6" class="nc ks it bd kt nd ne dn kx nf ng dp lb ls nh ni ld lw nj nk lf ma nl nm lh iz bi translated">风格丧失</h2><p id="a31b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">如图 5 所示，风格损失可以定义为<strong class="ll jd"><em class="mf"/></strong>风格和<strong class="ll jd"> <em class="mf">生成的</em> </strong>图像的 Gram 矩阵之间的平方误差损失。我们通常在预先训练的网络的多个层上采用风格损失的加权贡献。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7e1612cd45f4a6249e5ddb209e2e6c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*3YQtoRluLgL7d59irFVpfQ.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">图五。<strong class="bd nb">款式丢失</strong></p></figure><h1 id="65f1" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">将这些点连接起来</h1><p id="1462" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">结合单独的内容和风格损失，最终损失公式在图 6 中定义。我们从随机图像<em class="mf"> G、</em>开始，迭代地<strong class="ll jd"> <em class="mf">优化</em> </strong>该图像，以匹配图像的内容<em class="mf"> C </em>和图像的风格<em class="mf"> S、</em>，同时保持预先训练的特征提取器网络的权重固定。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/5a8fe7a5b9012672f901a68af36908f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*42nH5EbcnGkGdWnIq9F4oQ.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">图六。<strong class="bd nb">款式转移损耗</strong></p></figure><p id="7aef" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">总之，值得注意的是，尽管优化过程是<strong class="ll jd"> <em class="mf">缓慢的，</em> </strong>这种方法允许在任意<strong class="ll jd"><em class="mf"/></strong>对内容和样式图像之间进行样式转换。</p><h1 id="d28f" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">参考</h1><ol class=""><li id="3ab0" class="no np it ll b lm ln lp lq ls op lw oq ma or me nt nu nv nw bi translated">利昂·A·加蒂斯、亚历山大·S·埃克和马蒂亚斯·贝奇。<a class="ae os" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用卷积神经网络的图像风格转移</a>。在<em class="mf"> CVPR </em>，2016。</li><li id="587d" class="no np it ll b lm nx lp ny ls nz lw oa ma ob me nt nu nv nw bi translated">贾斯廷·约翰逊，亚历山大·阿拉希和李菲菲。<a class="ae os" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>。在<em class="mf"> ECCV </em>，2016。</li><li id="7b61" class="no np it ll b lm nx lp ny ls nz lw oa ma ob me nt nu nv nw bi translated"><a class="ae os" href="https://www.coursera.org/learn/convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/convolutionary-neural-networks/</a></li></ol></div></div>    
</body>
</html>