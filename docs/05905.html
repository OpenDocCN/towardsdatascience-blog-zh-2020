<html>
<head>
<title>Supervised Machine Learning Models: Decisions you have to make in practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督机器学习模型:你在实践中必须做出的决定</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b?source=collection_archive---------67-----------------------#2020-05-14">https://towardsdatascience.com/supervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b?source=collection_archive---------67-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5993" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">聚在一起—选择哪种模式？要调整哪些超参数？如何评价？</h2></div><p id="ab62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我提供了机器学习模型背后的一些直觉，但本文的重点不是解释机器学习模型背后的数学，而是它们的实际应用，即我将尝试回答使用什么、如何调优、如何评估等问题。本文讨论了数据科学家在尝试将模型与给定数据相匹配时需要考虑的任务、挑战、假设、利弊和注意事项。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/82f29f2fb57de59e96d6b11f66cbe8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvZ9YHLuIou7psAaQag1lQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">通过<a class="ae lv" href="https://monkeylearn.com/word-cloud/" rel="noopener ugc nofollow" target="_blank"> monkeylearn </a>生成的Wordcloud</p></figure><h2 id="99db" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">偏差-方差权衡</h2><p id="913c" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">所有的模型都遭受这种权衡。理解这一点对于调整机器学习模型的性能来说是微不足道的。假设我们有一个数据集，我们已经拟合了一个ML模型。</p><p id="b540" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个好的模型应该是一致的，并且能够处理训练数据集中的任何变化，而本身没有太大的变化，也就是说，它应该已经对看不见的数据进行了推广(这是我们首先要达到的目标)。模型由于训练数据集中的变化而变化的量被解释为模型的方差。</p><p id="4954" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">偏差可以解释为模型倾向于较少的预测变量。具有非常高的偏差会使模型对数据的变化(较低的方差)不敏感，并导致对情况的不良近似。另一方面，较低的偏差会使模型利用所有不必要的信息，并允许较高的方差。</p><p id="d7d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，考虑到模型和数据的需求和使用，这是一个数据科学家必须明智选择的权衡。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="a011" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">简单线性回归</h2><p id="c288" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">让我们从简单开始，假设我们有一个斜率为m、Y轴截距为c的直线方程，给定这条直线，我们可以计算任意特定X值的Y值。简单LR旨在估计直线(称为回归线)的斜率和截距，该直线在某种程度上最适合所有数据点，即最小化残差平方和(称为最小二乘法)。</p><p id="c382" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可能的情况是，我们将有一个零斜率(m)。这在某种程度上传达了Y对X值的独立性。我们使用零假设检验来检测这种行为，该检验确定m是否足够远离零。在实践中，我们使用p值，它在某种程度上告诉我们数据独立于预测变量的概率。我们从零假设假设的t分布中得到这个概率。最后，我们只需查看p值，如果它小于0.05(一般情况下)，我们可以拒绝零假设。</p><p id="0099" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们拒绝零假设，我们想知道我们的模型有多好地符合数据。为此，我们使用R平方度量(我们也可以使用RSE，但是RSE依赖于Y的单位，不能推广到任何数据)。R-square接近1意味着我们的LR模型解释了Y的可变性，是一个很好的拟合。如果R平方接近于0，那么可能有两个原因。(参考下面的可解释差异部分)</p><h2 id="15d6" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">多元线性回归</h2><p id="fb2e" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">如果我们有不止一个预测变量呢？我们可以将简单的LR从直线扩展到多维超平面来拟合数据。想象一下，假设我们有两个预测变量。现在，我们在3D空间中拟合一个平面，其中响应是这两个预测变量的函数，具有一些截距。这可以进一步扩展到更高维度。</p><p id="832d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用f统计量来检验零假设。尽管我们有单个预测因子的p值，为什么我们要看总体的F统计量？当有大量预测因子(比如100个)时，那么只是偶然地，我们保证观察到至少一个p值低于0.05的预测因子，即使在预测因子和反应之间没有真正的联系。但是，f-statistic不会受此影响，因为它会根据预测值的数量进行调整。</p><p id="4558" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们拒绝零假设，我们可能不得不选择只有重要的预测，以减少计算需求。这是使用子集选择方法完成的。</p><p id="2219" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用R-square来评估模型的拟合度。然而，需要注意的一点是，当我们包含更多的预测因子时，R-square会得到改善。由我们来决定计算需求和模型拟合之间的权衡。此外，以更高的R平方为目标可能会导致过度拟合问题。</p><h2 id="ab48" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">多项式回归</h2><p id="7efb" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><em class="le">它被认为是线性回归，因为它在预测系数中是线性的</em></p><p id="31b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">观察多重LR，我们看到两个主要假设。首先，特定预测变量的变化对响应Y的影响独立于任何其他预测变量。(加法假设)。第二，由于预测变量的单位变化引起的响应Y的变化是恒定的。(线性假设)。</p><p id="61e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，我们的数据可能并不总是如此。我们可能不得不考虑更高程度的预测变量来恰当地拟合数据(注意:这就是过度拟合和偏差-方差概念出现的地方。此外，了解协同效应(即预测变量之间的交互效应)也是值得的</p><p id="992d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们必须选择什么程度的预测变量？这是我们作为数据科学家试图解决的问题，选择这个程度和交互术语(称为特征工程)取决于我们。没有直接的答案或解决方案来处理这个问题。但是，可以使用残差图采用试错法。</p><p id="2b4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">残差图中，残差(<em class="le">真实响应</em> — <em class="le">估计响应</em>)相对于估计响应值绘制，这将揭示关于假设模型的一些见解。如果残差图没有显示模式，那么假设的模型与数据吻合得很好。否则，假设的模型会有一些问题，我们可能需要调整多项式的次数。</p><p id="18fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们最终确定了我们的模型，我们使用R-square评估拟合。我们必须注意，拥有更高程度的预测变量可能会过度拟合数据(看看偏差-方差权衡)。</p><h2 id="caee" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">KNN回归</h2><p id="5e1c" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">到目前为止，我们一定已经理解了在线性回归设置中假设一个合适的模型来很好地拟合数据是非常重要的。KNN回归消除了这种必要性，更加灵活。因此，如果预测准确性是我们的目标，给定足够的数据，KNN优于线性回归。但是，这里有一个陷阱。“给定足够的数据”让我们陷入维度诅咒。KNN回归是数据饥渴的，在更高的维度，实际上，永远不会有足够的数据，因此KNN比LR回归表现更差。此外，LR更容易解释，因为它估计了每个预测因子对反应的影响。另一方面，KNN是不可解释的，因为它只遵循数据，不关心预测。</p><p id="fdb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">当每个预测值的观测值较少时，参数方法往往优于非参数方法</em></p><h2 id="05e9" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">逻辑回归</h2><p id="5a57" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">它特别用于两个类的分类设置中(可以调整为两个以上，但效率不高，因此这里不讨论)。它不是模拟响应Y，而是利用逻辑函数来模拟Y属于特定类别的概率。这种分析可以用来观察特定预测变量的变化如何影响反应的几率。</p><p id="6405" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过最大化似然函数找到逻辑回归系数的估计。z统计量类似于t统计量，用于执行零假设检验。较大的z统计量表示反对零假设的证据。使用分类错误率对逻辑回归模型进行评估。</p><p id="fe9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当类被很好地分开时，逻辑回归不能在看不见的数据点上产生有希望的结果，即，即使对于数据中的小变化，它也是高度不稳定的。此外，在数据遭受“维数灾难”的更高维度中，由于缺乏足够的数据导致非常高的不稳定性，逻辑回归不能很好地执行。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="f101" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">贝叶斯分类器</h2><p id="23ee" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">它是一种理想的分类器，利用贝叶斯定理找出属于每一类的数据点的概率。数据点X的后验概率或类别Y的概率与类别Y的先验概率和X属于Y的类别条件概率的乘积成比例</p><p id="004a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它以最小的误差拟合数据。但不幸的是，我们不知道数据的确切类别条件概率和先验概率，我们试图在LDA和QDA的一些假设下进行估计。</p><p id="bd5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">线性判别分析:</strong></p><p id="164b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA可用于多类(&gt; 2)分类设置。首先，我们分别对每一类中预测因子的分布进行建模。然后，我们利用贝叶斯定理找出数据点属于特定类别的概率。</p><p id="0b47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以使用属于相应类别的数据点的比例来找到每个类别的先验概率。然而，对于类别条件概率，我们必须假设从中提取数据点的分布。</p><p id="16da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们假设数据点X是从多元高斯分布中提取的，该分布具有特定类别的均值向量和公共协方差矩阵。现在，我们的问题简化为估计每个类别和公共协方差的平均向量。判别方程在x上是线性的。</p><p id="ae84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">二次判别分析:</strong></p><p id="9a2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这类似于LDA，除了关于所有类的预测变量的公共协方差的假设。这里，每个类都有自己的协方差矩阵。这导致判别方程在x上是二次的。</p><p id="aea9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当决策边界是线性时，LDA和逻辑回归方法往往表现更好。对于中度非线性边界，QDA可能表现得更好。对于更复杂的决策边界，在有足够数据的情况下，选择好平滑参数的KNN表现得更好。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="6173" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">套索和岭回归</h2><p id="7cf3" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">一般来说，我们最小化RSS(称为最小二乘法)来估计回归设置中的预测系数。然而，由于拟合的灵活性，有时这些估计值可能会有很大的差异。</p><p id="ad65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，我们通过向零收缩(正则化)一些预测系数估计值(在山脊的情况下)或通过使其正好为零(在套索的情况下)来惩罚预测系数估计值。岭和套索回归模型背后的直觉是，为了方差的更大减少，在模型偏差的轻微上升上进行妥协。</p><p id="7cce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">收缩量取决于一个参数(λ),这个参数必须由作为数据科学家的我们来决定。对于我们选择的每个λ值，产生一组不同的预测系数估计值，即对于每个λ，我们得到一个适合数据的不同模型。</p><p id="e83d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从岭回归方程或套索回归方程中，我们可以观察到预测系数依赖于预测变量的尺度。因此，在对数据应用岭回归或套索回归之前，将数据中的每个预测值归一化至标准差为1是非常重要的。</p><p id="71cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用训练数据的R平方值或测试数据的均方差(MSE)来比较每个模型。交叉验证方法可以用来决定λ的值。需要注意的是，在高维设置中，由于缺乏足够的数据，训练R-square具有很大的可变性，因此不应用于评估模型性能。</p><p id="2875" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果响应是许多系数大小大致相似的预测值的函数，则岭回归表现更好。如果少数预测因素主导了反应，那么套索回归表现更好。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="ad0b" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">决策树</h2><p id="68b5" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">基于树的方法将预测空间分割成多个框(树中的叶子),并对落入该框中的所有数据点进行相同的预测。在回归设置中，该预测通常是该框中所有训练观察的平均值，对于分类设置，多数投票被认为是预测对新数据点的响应。</p><p id="23bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在树中进行切割时，我们以具有最低可能RSS(回归树)或分类误差(分类树)的方式决定预测变量及其切割点。对于分类设置，也可以使用像基尼指数和交叉熵这样的度量，因为它们对叶子(或盒子)的纯度更敏感。</p><p id="90a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为一名数据科学家，我需要回答一个问题——什么时候我应该停止拆分树，或者我应该制作多少个盒子？</p><p id="2044" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">成本复杂度剪枝或者最薄弱环节剪枝</strong>:直观上，我们种一棵大树，然后剪枝，选择一棵导致误差最低的树。我们不是考虑所有可能的子树，而是使用一个调整参数α将树修剪到期望的水平。</p><p id="8578" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个阿尔法值，我们将有一个相应的树。随着alpha的增加，树被修剪的越来越多，导致分裂越来越少。分割越小，模型的方差越小，但偏差会有所增加。我们可以利用交叉验证来决定alpha的值，从而获得数据的最佳拟合。</p><p id="7c97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">Bagging(bootstrapped Aggregation)</strong>:我们利用bootstrap方法生成多个数据集，并为每个Bootstrapped数据集生成一个深度Bootstrapped树。尽管每个自举树都有很高的方差，但是对所有生长的树进行平均将会减少方差。我们利用出包测试误差来评估树的性能。</p><p id="e28e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当预测准确性是我们的目标时，我们使用Bagging。然而，打包损害了模型的可解释性。另一方面，成本复杂度修剪方法生成可解释树。</p><p id="cfbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">随机森林:</strong>这是一个有趣而聪明的打包方法，可以去相关生成的树。在Bagging中，在构建决策树时，我们允许所有预测变量参与分裂。如果几乎没有主要的预测因素，这个过程可能会产生高度相关的树。</p><p id="bad4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在随机森林中，在每次分裂时，我们允许m个预测者的随机样本(大约sqrt(预测者的数量))参与，从而允许所有预测者参与并进而产生去相关的树。这种方法的优势是显而易见的，因为对不相关的树进行平均会大大降低方差。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="2f5b" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">支持向量分类器</h2><p id="be87" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在进入支持向量分类器之前，我们必须了解最大间隔分类器。最大间隔分类器生成分离两个类的超平面，使得它垂直于每个属于不同类的最近的数据点。这些指导最大间隔分类器的数据点被称为“支持向量”。有趣的是，我们发现分类器只依赖于这些支持向量，而不依赖于所有其他数据点。</p><p id="999d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最大间隔分类器对不可分数据集的推广是支持向量分类器。由于我们无法找到完美分类数据集的硬边界，我们不得不允许一些数据点被错误分类。问题—我们应该允许多少个数据点被错误分类？</p><p id="ff45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们引入一个调整参数或成本“C ”,它允许模型对点进行错误的分类。可以使用交叉验证来选择它。c控制支持向量分类器的偏差-方差权衡。位于其类的边缘的错误侧的数据点和位于边缘的点是该分类器中的支持向量，因为只有这些考虑了支持向量分类器的形成。</p><p id="d658" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">较小的C值说明了数据的硬拟合，并且非常不稳定，即，即使支持向量有很小的变化，分类器也会变化。这导致高方差但低偏差。另一方面，较高的C值允许较宽的余量，并产生具有较高偏差但较低方差的灵活分类器。</p><p id="aaf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过在预测器空间中引入更高阶特征以及交互项来拟合非线性决策边界。核和支持向量机的概念是随着这个概念的复杂初始化而出现的，但是中心思想是相同的。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="5f19" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">可解释的方差(RSS、TSS、R平方)</h2><p id="d7f3" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><strong class="kk iu">总平方和:</strong>是数据反应中固有的方差总量。</p><p id="5c31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">残差平方和:</strong> RSS测量数据响应中的方差，这种方差不能用我们用来拟合的模型来解释。</p><p id="12cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> R-square: </strong>可以用我们过去拟合的模型解释的数据响应中方差的比例。R-square接近1表示模型很好。无法解释的差异可能是由两个原因造成的:a .考虑的模型不能准确表示数据(直线不能符合二次型-数据)b .数据中的固有误差(由于无法建模的噪声)。</p><p id="7d5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当在训练数据上测量R-square时，试图通过增加模型复杂性来实现更接近1的R-square会导致数据的过度拟合，因为它利用了数据中存在的固有噪声。因此，总是建议报告我们使用的数据集以及评估指标。</p><p id="2dfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:<em class="le">加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼。《统计学习导论:在r .纽约的应用》: Springer，2013年。</em></p></div></div>    
</body>
</html>