<html>
<head>
<title>Evolution of YOLO — YOLO version 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO-YOLO版本1的演变</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolution-of-yolo-yolo-version-1-afb8af302bd2?source=collection_archive---------16-----------------------#2020-05-20">https://towardsdatascience.com/evolution-of-yolo-yolo-version-1-afb8af302bd2?source=collection_archive---------16-----------------------#2020-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3a07" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">YOLO的起源——“你只看一次”物体探测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/d7f8fb561a87c3ffcef7dc2e899be2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*2fkwaruwVUxLvV3o2Tk9RQ.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a></p></figure><p id="81b9" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">YOLO(<strong class="kv ir">Y</strong>ou<strong class="kv ir">O</strong>only<strong class="kv ir">L</strong>ook<strong class="kv ir">O</strong>nce)是最流行的对象检测卷积神经网络(CNN)之一。Joseph Redmon等人在2015年发表了他们的第一篇YOLO <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>之后，随后的版本分别由他们在<a class="ae ks" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank"> 2016 </a>、<a class="ae ks" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank"> 2017 </a>和Alexey Bochkovskiy在<a class="ae ks" href="https://arxiv.org/pdf/2004.10934.pdf" rel="noopener ugc nofollow" target="_blank"> 2020 </a>发表。本文是一系列文章中的第一篇，概述了YOLO CNN是如何从第一个版本发展到最新版本的。</p><h1 id="accb" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">1.YOLO v1 —动机:</h1><p id="bf01" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">在YOLO发明之前，诸如<a class="ae ks" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank"> R-CNN </a>的对象检测器CNN首先使用区域提议网络(RPNs)来在输入图像上生成边界框提议，然后在边界框上运行分类器，最后应用后处理来消除重复检测以及细化边界框。R-CNN网络的各个阶段必须单独训练。R-CNN网络很难优化，而且速度很慢。</p><p id="856d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">YOLO的创造者被激励去设计一个单级CNN，它可以被端到端地训练，易于优化并且是实时的。</p><h1 id="c763" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">2.YOLO v1 —概念设计:</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/cde25ca724f2a1a26b75af040326bac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*15uBgdR3_rNZzx665Leang.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图1: </strong> YOLO版本1概念设计(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="fe3e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">如图1左图所示，YOLO将输入图像分成S×S网格单元。如图1中上图所示，每个网格单元预测B个边界框和一个“对象性”得分P(Object ),表示网格单元是否包含对象。如图1中下部图像所示，每个网格单元还预测网格单元包含的对象所属类的条件概率P(Class | Object)。</p><p id="d5cd" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于每个边界框，YOLO预测了五个参数——x，y，w，h和一个置信度得分。边界框相对于网格单元的中心由坐标<em class="mn"> (x，y) </em>表示。<em class="mn"> x </em>和<em class="mn"> y </em>的值介于0和1之间。边界框的宽度<em class="mn"> w </em>和高度<em class="mn"> h </em>被预测为整个图像的宽度和高度的一部分。所以它们的值在0和1之间。置信度得分指示边界框是否具有对象以及边界框有多精确。如果边界框没有对象，则置信度得分为零。如果边界框具有对象，则置信度得分等于预测边界框和基础真值的并集上的交集(IoU)。因此，对于每个网格单元，YOLO预测了B×5个参数。</p><p id="a773" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于每个网格单元，YOLO预测C类概率。这些类别概率是基于网格单元中存在的对象的条件。即使格网单元具有B边界框，YOLO也只预测每个格网单元的一组C类概率。因此，对于每个网格单元，YOLO预测C+B×5个参数。</p><p id="0f28" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图像的总预测张量= S x S x (C + B x 5)。对于PASCAL VOC数据集，YOLO使用S = 7、B = 2和C = 20。因此，帕斯卡VOC的最终YOLO预测是7×7 ×( 20+5×2)= 7×7×30张量。</p><p id="59f8" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">最后，YOLO版本1应用非最大值抑制(NMS)和阈值来报告最终预测，如图1右图所示。</p><h1 id="249b" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">3.YOLO v1 — CNN设计:</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/177b7a8f82869ef3183f1e4476e1a96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQW39sVTms7W_ZBLnzKp5A.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图2: </strong> YOLO第一版CNN ( <strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="682c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">YOLO版本1的CNN如图2所示。它有24个卷积层，作为特征提取器。接着是两个完全连接的层，负责对象的分类和边界框的回归。最终输出是一个7 x 7 x 30的张量。YOLO CNN是一个简单的单路CNN，类似于<a class="ae ks" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG19 </a>。YOLO使用1x1卷积，然后是3x3卷积，灵感来自谷歌的<a class="ae ks" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">盗梦空间版本1 </a> CNN。泄漏ReLU激活用于除最后一层之外的所有层。最后一层使用线性激活函数。</p><h1 id="6842" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">4.YOLO v1 —损失设计:</h1><p id="5c6b" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">平方和误差是YOLO损失设计的基础。因为多个网格单元不包含任何对象，并且它们的置信度得分为零。它们压倒了包含对象的几个单元的梯度。为了避免这种导致训练发散和模型不稳定的过度控制，YOLO增加了来自包含对象的边界框的预测的权重(<strong class="kv ir"> λ </strong> <em class="mn"> coord = </em> 5)，并减少了权重(<strong class="kv ir"> λ </strong> <em class="mn"> noobj = 0。</em> 5)用于不包含任何对象的边界框的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f74a30696c3e41ba2b512d65a5c8129e.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*4f64cxy07yhIy-qpOMvV5w.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图3: </strong> YOLO v1丢失部分1 —包围盒中心坐标(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="da27" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图3示出了YOLO损失的第一部分，其计算边界框中心坐标预测中的误差。损失函数仅惩罚边界框中心坐标的误差，如果该预测器负责地面真实框的话。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mu"><img src="../Images/f06081c7f4cc6f08f5a1e4c204dcab79.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*YTWn-hQz6I7UFJJglzkuNg.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图4: </strong> YOLO v1丢失部分2——包围盒宽度和高度(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="aaf7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图4示出了YOLO损失的第二部分，其计算边界框宽度和高度的预测误差。如果小边界框与大边界框的预测误差大小相同，则它们会产生相同的损失。但是相同大小的误差对于小边界框比大边界框更“错误”。因此，这些值的平方根用于计算损失。因为宽度和高度都在0和1之间，所以它们的平方根对于较小的值比较大的值增加更多的差异。损失函数仅惩罚边界框的宽度和高度误差，如果该预测值负责地面真实框的话。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/19bb19b470b3bc8a4a9356915ef62df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*oG9rPsXaRDMbMX0XFcWvmA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图5: </strong> YOLO v1损失部分3-物体置信度得分(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:约瑟夫·雷德蒙等人的统一实时物体检测</a>)</p></figure><p id="f078" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图5示出了YOLO损失的第三部分，其计算具有对象的边界框的对象置信度得分的预测误差。损失函数仅惩罚对象置信度误差，如果该预测器负责基本事实框的话。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mw"><img src="../Images/ce5b3d66b091a9b4c3a3a1a6411684ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*osoeTpYsMGN1gFPReg7_1g.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图6: </strong> YOLO v1损失第4部分——无对象置信度得分。(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="19f2" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图6示出了YOLO损失的第四部分，其计算没有对象的边界框的对象置信度得分的预测误差。损失函数仅惩罚对象置信度误差，如果该预测器负责基本事实框的话。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b20c088e37588e3d1aa21fc7c5d8eaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*4PUOz1Ul9Ah2sdefxsUdcQ.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">图7: </strong> YOLO v1丢失部分5类概率(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时物体检测</a>)</p></figure><p id="9380" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图7示出了YOLO损失的第五部分，其计算具有对象的网格单元的类概率预测中的误差。如果网格单元中存在对象，损失函数只惩罚类概率错误。</p><h1 id="89a9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">5.YOLO第一版—结果:</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/492eab0e66f73af480f635bb10df5424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*hbpu18IgJC71d_RDuOEywA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图8: YOLO v1 —结果(<strong class="bd kr">来源:</strong> <a class="ae ks" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:Joseph Redmon等人的统一实时对象检测</a>)</p></figure><p id="65bb" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">YOLO v1在PASCAL VOC 2007数据集上的结果如图8所示。YOLO实现了45 FPS和63.4 %的mAP，与另一种实时对象检测器DPM相比显著提高。尽管速度更快的R-CNN VGG-16的mAP更高，为73.2%，但其速度相当慢，为7 FPS。</p><h1 id="ec9a" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">6.YOLO第一版—限制:</h1><ol class=""><li id="8f59" class="mz na iq kv b kw mh kz mi lc nb lg nc lk nd lo ne nf ng nh bi translated">YOLO很难发现成群出现的小物体。</li><li id="a33a" class="mz na iq kv b kw ni kz nj lc nk lg nl lk nm lo ne nf ng nh bi translated">YOLO在探测具有不寻常长宽比的物体时有困难。</li><li id="babc" class="mz na iq kv b kw ni kz nj lc nk lg nl lk nm lo ne nf ng nh bi translated">与快速的R-CNN相比，YOLO犯了更多的定位错误。</li></ol><h1 id="e897" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">7.参考资料:</h1><p id="5e01" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">[1] J. Redmon，S. Divvala，R. Girshick和a .法尔哈迪，<a class="ae ks" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的实时对象检测</a> (2015)，arxiv.org</p><p id="5808" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">[2] R. Girshick，J. Donahue，T.Darrell和J. Malik，<a class="ae ks" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank">用于精确对象检测和语义分割的丰富特征层次</a> (2013)，arxiv.org</p><p id="5e68" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">[3] K. Simnoyan和A. Zisserman，<a class="ae ks" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的甚深卷积网络</a> (2014)，arxiv.org</p><p id="93e4" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">[4] C .塞格迪，w .刘，y .贾，p .塞尔马内，s .里德，d .安盖洛夫，d .埃汉，v .万霍克和a .拉宾诺维奇，<a class="ae ks" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">用卷积深化</a> (2014)</p></div></div>    
</body>
</html>