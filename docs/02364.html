<html>
<head>
<title>Setting up text preprocessing pipeline using scikit-learn and spaCy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用scikit-learn和spaCy设置文本预处理管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-e09b9b76758f?source=collection_archive---------5-----------------------#2020-03-06">https://towardsdatascience.com/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-e09b9b76758f?source=collection_archive---------5-----------------------#2020-03-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ecb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用sklearn管道进行标记化、词条化、删除停用词和标点符号</h2></div><p id="feb4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">文本预处理是将原始文本转换成可<a class="ae le" href="http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/" rel="noopener ugc nofollow" target="_blank">矢量化</a>的形式，随后由机器学习算法用于自然语言处理(NLP)任务，如文本分类、主题建模、命名实体识别等。</p><p id="c1fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原始文本由所有文本分析API进行广泛预处理，如<a class="ae le" href="https://azure.microsoft.com/en-in/services/cognitive-services/text-analytics/" rel="noopener ugc nofollow" target="_blank"> Azure的文本分析API</a>或我们在<a class="ae le" href="http://www.specrom.com/products-services/" rel="noopener ugc nofollow" target="_blank"> Specrom Analytics </a>开发的API，尽管预处理的程度和类型取决于输入文本的类型。例如，对于我们的历史新闻API，输入由抓取的HTML页面组成，因此，在将文本输入到NLP算法之前，从文本中去除不需要的HTML标签是很重要的。然而，对于一些新闻媒体，我们从他们的官方REST APIs获得JSON格式的数据。在这种情况下，根本没有HTML标记，对这样一个干净的文本运行基于正则表达式的预处理程序是浪费CPU时间。因此，根据数据源对文本进行不同的预处理是有意义的。</p><p id="e159" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您想要创建如下所示的词云，那么通常建议您删除停用词。但是在诸如名称实体识别(NER)的情况下，这并不是真正必需的，您可以安全地将语法完整的句子添加到您选择的NER中。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/b86187efb6c3c63872379faf97726f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Pcg9EGBOGsC6uEpTNMgfew.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">从搜集的新闻语料库中生成的词云(2016-2020)。杰伊·帕特尔</p></figure><p id="2f46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多好的<a class="ae le" rel="noopener" target="_blank" href="/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79">博客文章</a>开发了文本预处理步骤，但是为了完整起见，让我们在这里浏览一下。</p><h1 id="3906" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">1.标记化</h1><p id="cde6" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">将段落或句子中包含的文本转换为单个单词(称为标记)的过程称为标记化。在我们将文本转换成充满数字的向量之前，这通常是文本预处理中非常重要的一步。</p><p id="4aa8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直观而又相当天真的是，对文本进行标记的一种方法是简单地在空格处断开字符串，python已经提供了非常好的字符串方法，可以轻松地完成这一任务，让我们将这种标记方法称为“空白标记化”。</p><p id="b4fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，空白标记化无法理解单词缩写，例如当我们将两个单词“can”和“not”组合成“can”、don(do+not)和I ve(I+have)时。这些都不是无关紧要的问题，如果我们不把“不能”分成“能”和“不能”，那么一旦我们去掉标点符号，我们将只剩下一个单词“cant”，它实际上不是一个字典单词。</p><p id="4aae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python中用于文本处理的经典库NLTK附带了其他标记化器，如WordPunctTokenizer和TreebankWordTokenizer，它们都按照不同的约定运行，试图解决单词缩写问题。对于高级标记化策略，还有一个RegexpTokenizer，它可以根据正则表达式拆分字符串。</p><p id="cbeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这些方法基本上都是基于规则的，因为没有真正的“学习”发生，所以作为用户，您必须处理所有可能因令牌化策略而出现的特殊情况。</p><p id="78a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Spacy和Apache Spark NLP等下一代NLP库在很大程度上解决了这个问题，并在语言模型中使用标记化方法处理常见缩写。</p><h1 id="8ec5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">1.1 NLTK标记化示例</h1><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="467c" class="mt ls it mp b gy mu mv l mw mx"># Create a string input <br/>sample_text = "Gemini Man review: Double Will Smith can't save hackneyed spy flick U.S.A"</span><span id="b1d8" class="mt ls it mp b gy my mv l mw mx">from nltk.tokenize import WhitespaceTokenizer<br/>tokenizer_w = WhitespaceTokenizer()</span><span id="b885" class="mt ls it mp b gy my mv l mw mx"># Use tokenize method <br/>tokenized_list = tokenizer_w.tokenize(sample_text) <br/>tokenized_list</span><span id="82a0" class="mt ls it mp b gy my mv l mw mx"># output</span><span id="eb49" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review:',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> "can't",<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A']</span></pre><p id="45c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">WordPunct Tokenizer将在标点符号上拆分，如下所示。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="7e6c" class="mt ls it mp b gy mu mv l mw mx"><br/>from nltk.tokenize import WordPunctTokenizer <br/>tokenizer = WordPunctTokenizer()<br/>tokenized_list= tokenizer.tokenize(sample_text)<br/>tokenized_list</span><span id="ac0c" class="mt ls it mp b gy my mv l mw mx"># Output</span><span id="bbf7" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> 'can',<br/> "'",<br/> 't',<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U',<br/> '.',<br/> 'S',<br/> '.',<br/> 'A']</span></pre><p id="7926" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">NLTK的treebanktokenizer将单词缩写拆分成两个标记，如下所示。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="ddae" class="mt ls it mp b gy mu mv l mw mx">from nltk.tokenize import TreebankWordTokenizer<br/>tokenizer = TreebankWordTokenizer()<br/>tokenized_list= tokenizer.tokenize(sample_text)<br/>tokenized_list<br/>#Output</span><span id="eeba" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> 'ca',<br/> "n't",<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A']</span></pre><h1 id="2d94" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">1.2空间标记化示例</h1><p id="5ded" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在SpaCy中执行标记化也很简单，在后面关于词条化的部分，您会注意到为什么标记化作为语言模型的一部分可以解决单词缩写的问题。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="a91f" class="mt ls it mp b gy mu mv l mw mx"># Spacy Tokenization example<br/>sample_text = "Gemini Man review: Double Will Smith can't save hackneyed spy flick U.S.A"</span><span id="9093" class="mt ls it mp b gy my mv l mw mx">from spacy.lang.en import English<br/>nlp = English()<br/>tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>tokens = tokenizer(sample_text)<br/>token_list = []<br/>for token in tokens:<br/>    token_list.append(token.text)<br/>token_list<br/>#output<br/>['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> 'ca',<br/> "n't",<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A']</span></pre><h1 id="f668" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">2.词干化和词汇化</h1><p id="d4e2" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">词干化和词汇化试图为不同的词形变化(raining，rained等)获得词根(例如rain)。Lemma algos给你的是真正的词典单词，而词干法只是简单地去掉了单词的最后部分，所以速度更快，但不太准确。词干分析返回的单词并不是真正的字典单词，因此你将无法在Glove、Word2Vec等中找到它的预训练向量，这是取决于应用程序的一个主要缺点。</p><p id="2c04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，使用像波特和更高级的雪球词干分析器这样的词干算法是非常流行的。Spacy没有附带任何词干算法，所以我们将使用NLTK来执行词干；这里我们将展示两种词干提取算法的输出。为了便于使用，我们将把空白符号化器封装到一个函数中。如你所见，两位词干学家都将动词形式(raining)简化为rain。</p><h1 id="4d4b" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">2.1 NLTK的词干示例</h1><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="dd88" class="mt ls it mp b gy mu mv l mw mx">sample_text = '''Gemini Man review: Double Will Smith can't save hackneyed spy flick U.S.A raining rained ran'''</span><span id="2531" class="mt ls it mp b gy my mv l mw mx">from nltk.tokenize import WhitespaceTokenizer</span><span id="5d10" class="mt ls it mp b gy my mv l mw mx">def w_tokenizer(text):<br/>    <br/>    tokenizer = WhitespaceTokenizer()   <br/># Use tokenize method <br/>    tokenized_list = tokenizer.tokenize(text) <br/>    return(tokenized_list)</span><span id="01ba" class="mt ls it mp b gy my mv l mw mx">from nltk.stem.snowball import SnowballStemmer</span><span id="7e4a" class="mt ls it mp b gy my mv l mw mx">def stemmer_snowball(text_list):<br/>    snowball = SnowballStemmer(language='english')<br/>    return_list = []<br/>    for i in range(len(text_list)):<br/>        return_list.append(snowball.stem(text_list[i]))<br/>    return(return_list)<br/>stemmer_snowball(w_tokenizer(sample_text))</span><span id="6933" class="mt ls it mp b gy my mv l mw mx">#Output</span><span id="41a9" class="mt ls it mp b gy my mv l mw mx">['gemini',<br/> 'man',<br/> 'review:',<br/> 'doubl',<br/> 'will',<br/> 'smith',<br/> "can't",<br/> 'save',<br/> 'hackney',<br/> 'spi',<br/> 'flick',<br/> 'u.s.a',<br/> 'rain',<br/> 'rain',<br/> 'ran']</span></pre><p id="d5fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以从NLTK的波特·斯特梅尔那里得到同样的结果，这个也可以把单词转换成非字典形式，比如spy -&gt; spi和double -&gt; doubl</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="3032" class="mt ls it mp b gy mu mv l mw mx">from nltk.stem.porter import PorterStemmer</span><span id="9920" class="mt ls it mp b gy my mv l mw mx">def stemmer_porter(text_list):<br/>    porter = PorterStemmer()<br/>    return_list = []<br/>    for i in range(len(text_list)):<br/>        return_list.append(porter.stem(text_list[i]))<br/>    return(return_list)<br/>stemmer_porter(w_tokenizer(sample_text))</span><span id="63ce" class="mt ls it mp b gy my mv l mw mx">#Output</span><span id="5cdd" class="mt ls it mp b gy my mv l mw mx">['gemini',<br/> 'man',<br/> 'review:',<br/> 'doubl',<br/> 'will',<br/> 'smith',<br/> "can't",<br/> 'save',<br/> 'hackney',<br/> 'spi',<br/> 'flick',<br/> 'u.s.a',<br/> 'rain',<br/> 'rain',<br/> 'ran']</span></pre><h1 id="b0b3" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">2.2 SpaCy的引理化示例</h1><p id="15de" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">如果您使用SpaCy进行标记化，那么它已经为每个标记存储了一个名为<code class="fe mz na nb mp b">.lemma_</code>的属性，您可以简单地调用它来获得每个单词的词汇化形式。请注意，它不像词干分析器那样具有攻击性，它将单词缩写(如“不可以”)转换为“可以”和“不可以”。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="2c35" class="mt ls it mp b gy mu mv l mw mx"># <a class="ae le" href="https://spacy.io/api/tokenizer" rel="noopener ugc nofollow" target="_blank">https://spacy.io/api/tokenizer</a><br/>from spacy.lang.en import English<br/>nlp = English()</span><span id="4008" class="mt ls it mp b gy my mv l mw mx">tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>tokens = tokenizer(sample_text)<br/>#token_list = []<br/>lemma_list = []<br/>for token in tokens:<br/>    #token_list.append(token.text)<br/>    lemma_list.append(token.lemma_)<br/>#token_list<br/>lemma_list<br/>#Output</span><span id="b35b" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> 'can',<br/> 'not',<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A',<br/> 'rain',<br/> 'rain',<br/> 'run']</span></pre><h1 id="ced5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">3.停止单词删除</h1><p id="55df" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">上面有一些特定的词，如“它”、“是”、“那个”、“这个”等。这对基本句子的意思没有多大帮助，实际上在所有英语文档中都很常见；这些词被称为停用词。在通过计数矢量化器对记号进行矢量化之前，通常需要移除这些“常用”单词，以便我们可以减少向量的总维数，并减轻所谓的“维数灾难”。</p><p id="3e10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本上有三种方法可以删除停用字词:</p><ul class=""><li id="4ac7" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">第一种方法是最简单的，您可以创建一个列表或一组单词来从令牌中排除；比如list已经是sklearn的countvectorizer、NLTK以及SpaCy的一部分。这种去除停用词的方法已经被接受了很长时间，然而，在研究人员和工作专业人员中有一种意识，即这种一刀切的方法实际上对学习文本的整体含义是非常有害的；有<a class="ae le" href="https://www.aclweb.org/anthology/W18-2502/" rel="noopener ugc nofollow" target="_blank">篇论文指出</a>反对这种方法。</li></ul><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="0e40" class="mt ls it mp b gy mu mv l mw mx"># using hard coded stop word list</span><span id="b9a7" class="mt ls it mp b gy my mv l mw mx">from spacy.lang.en import English<br/>import spacy<br/>spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS<br/># spacy_stopwords is a hardcoded set<br/>nlp = English()<br/>tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>tokens = tokenizer(sample_text)<br/>#token_list = []<br/>lemma_list = []<br/>for token in tokens:<br/>    if token.lemma_.lower() not in spacy_stopwords:<br/>    #token_list.append(token.text)<br/>        lemma_list.append(token.lemma_)<br/>#token_list<br/>lemma_list<br/>#Output</span><span id="0291" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Smith',<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A',<br/> 'rain',<br/> 'rain',<br/> 'run']</span></pre><p id="8f6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不出所料，单词“will”和“can”等被删除了，因为它们出现在SpaCy中可用的硬编码停用词集中。让我们将它封装到一个名为remove_stop_words的函数中，这样我们就可以在第5节中将它用作sklearn管道的一部分。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="4698" class="mt ls it mp b gy mu mv l mw mx">import spacy<br/>def remove_stopwords(text_list):<br/>    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS</span><span id="0404" class="mt ls it mp b gy my mv l mw mx">return_list = []<br/>    for i in range(len(text_list)):<br/>        if text_list[i] not in spacy_stopwords:<br/>            return_list.append(text_list[i])<br/>    return(return_list)</span></pre><ul class=""><li id="ccd7" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">第二种方法是让语言模型判断给定的标记是否是停用词。Spacy的标记化已经为此提供了一个名为is <code class="fe mz na nb mp b">.is_stop</code>的属性。现在，有些时候，spacy的标志不会排除常见的停用词，但这仍然比要排除的硬编码单词列表要好。仅供参考，在一些SpaCy模型[ <a class="ae le" href="https://stackoverflow.com/questions/52263757/spacy-is-stop-doesnt-identify-stop-words" rel="noopener ugc nofollow" target="_blank"> 1 </a> ][ <a class="ae le" href="https://github.com/explosion/spaCy/issues/1574" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]中有一个记录良好的错误，它避免了在第一个字母大写的情况下检测停用词，因此如果它不能正确检测停用词，您需要应用变通方法。</li></ul><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="0b4d" class="mt ls it mp b gy mu mv l mw mx"># using the .is_stop flag</span><span id="0eb3" class="mt ls it mp b gy my mv l mw mx">from spacy.lang.en import English</span><span id="5ac0" class="mt ls it mp b gy my mv l mw mx">nlp = English()<br/>tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>tokens = tokenizer(sample_text)<br/><br/>lemma_list = []<br/>for token in tokens:<br/>    if token.is_stop is False:<br/>    <br/>        lemma_list.append(token.lemma_)<br/><br/>lemma_list</span><span id="b117" class="mt ls it mp b gy my mv l mw mx">#Output</span><span id="0dd4" class="mt ls it mp b gy my mv l mw mx">['Gemini',<br/> 'Man',<br/> 'review',<br/> ':',<br/> 'Double',<br/> 'Will',<br/> 'Smith',<br/> 'not',<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'U.S.A',<br/> 'rain',<br/> 'rain',<br/> 'run']</span></pre><p id="a6f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这显然做得更好，因为它检测到这里的“Will”是一个人的名字，只是从示例文本中去掉了“can”。让我们将它封装在一个函数中，以便在最后一节中使用。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="47d9" class="mt ls it mp b gy mu mv l mw mx"># <br/>from spacy.lang.en import English</span><span id="3753" class="mt ls it mp b gy my mv l mw mx">def spacy_tokenizer_lemmatizer(text):<br/>    <br/>    nlp = English()<br/>    tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>    tokens = tokenizer(text)<br/>    <br/>    lemma_list = []<br/>    for token in tokens:<br/>        if token.is_stop is False:<br/>            lemma_list.append(token.lemma_)<br/>    <br/>    return(lemma_list)</span></pre><ul class=""><li id="b3d3" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">对抗停用词的第三种方法是排除在给定语料库中出现太频繁的词；sklearn的countvectoriser和tfidfvectorizer方法有一个名为“max_df”的参数，它让您可以忽略文档频率严格高于给定阈值的标记。您也可以通过“max_features”参数指定标记总数来排除单词。如果您打算在count vectorizer之后使用tf-idf，那么与对句子的整体含义有贡献的单词相比，它会自动为停用词分配一个低得多的权重。</li></ul><h1 id="83e8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">4.删除标点符号</h1><p id="aa21" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">一旦我们标记了文本，转换了单词的缩写，文本中的标点符号和特殊字符就不再有用了。当然，当我们处理可能有twitter句柄、电子邮件地址等的文本时，这是不正确的。在这些情况下，我们改变我们的文本处理管道，只从令牌中去除空白，或者完全跳过这一步。我们可以使用regex '  ]* &gt; '清除所有的HTML标签；所有非单词字符都可以用“[\W]+”删除。不过，在单词缩写被lemmatizer处理之前，您应该小心不要去掉标点符号。在下面的代码块中，我们将修改我们的空间代码，以考虑停用词，并从令牌中删除任何标点符号。如下面的例子所示，我们已经成功地删除了特殊字符标记，例如“:”，它们在单词矢量化的包中实际上没有任何语义贡献。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="89ac" class="mt ls it mp b gy mu mv l mw mx">import re<br/>def preprocessor(text):<br/>    if type(text) == string:</span><span id="39dd" class="mt ls it mp b gy my mv l mw mx">    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    text = re.sub('[\W]+', '', text.lower())<br/>    return text</span><span id="665c" class="mt ls it mp b gy my mv l mw mx">from spacy.lang.en import English<br/>nlp = English()<br/>tokenizer = nlp.Defaults.create_tokenizer(nlp)<br/>tokens = tokenizer(sample_text)</span><span id="73d4" class="mt ls it mp b gy my mv l mw mx">lemma_list = []<br/>for token in tokens:<br/>    if token.is_stop is False:<br/>        token_preprocessed = preprocessor(token.lemma_)<br/>        if token_preprocessed != '':</span><span id="366e" class="mt ls it mp b gy my mv l mw mx">             lemma_list.append(token_preprocessed)<br/>lemma_list<br/>#Output:</span><span id="c2f6" class="mt ls it mp b gy my mv l mw mx">['gemini',<br/> 'man',<br/> 'review',<br/> 'double',<br/> 'will',<br/> 'smith',<br/> 'not',<br/> 'save',<br/> 'hackneyed',<br/> 'spy',<br/> 'flick',<br/> 'usa',<br/> 'rain',<br/> 'rain',<br/> 'run']</span><span id="3b75" class="mt ls it mp b gy my mv l mw mx">#</span><span id="ad05" class="mt ls it mp b gy my mv l mw mx">A more appropriate preprocessor function is below which can take both a list and a string as input</span><span id="5e41" class="mt ls it mp b gy my mv l mw mx">def preprocessor_final(text):<br/>    if isinstance((text), (str)):<br/>        text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>        text = re.sub('[\W]+', '', text.lower())<br/>        return text<br/>    if isinstance((text), (list)):<br/>        return_list = []<br/>        for i in range(len(text)):<br/>            temp_text = re.sub('&lt;[^&gt;]*&gt;', '', text[i])<br/>            temp_text = re.sub('[\W]+', '', temp_text.lower())<br/>            return_list.append(temp_text)<br/>        return(return_list)<br/>    else:<br/>        pass</span></pre><p id="87c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个常见的文本处理用例是当我们试图从社交媒体评论、推文等web数据中执行文档级情感分析时。所有这些都广泛使用表情符号，如果我们简单地去掉所有特殊字符，我们可能会错过一些非常有用的符号，这些符号对文本的语义和情感有很大的贡献。如果我们计划使用一包单词类型的文本矢量化，那么我们可以简单地找到所有这些表情符号，并将它们添加到标记化列表的末尾。在这种情况下，作为标记化之前的第一步，您可能必须运行预处理器。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="64ba" class="mt ls it mp b gy mu mv l mw mx"># find emoticons function</span><span id="e02a" class="mt ls it mp b gy my mv l mw mx">import re<br/>def find_emo(text):<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',text)<br/>    return emoticons<br/>sample_text = " I loved this movie :) but it was rather sad :( "<br/>find_emo(sample_text)<br/># output<br/>[':)', ':(']</span></pre><h1 id="a5c8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">5.Sklearn管道</h1><p id="83ea" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">正如您在上面看到的，文本预处理很少是一刀切的，大多数真实世界的应用程序要求我们根据文本源和我们计划做的进一步分析使用不同的预处理模块。</p><p id="df75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多方法可以创建这样的自定义管道，但一个简单的选择是使用sklearn管道，它允许我们顺序组装几个不同的步骤，唯一的要求是中间步骤应该已经实现了fit和transform方法，并且最终的估计器至少有一个fit方法。</p><p id="07d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，这对于许多小函数来说可能是一个过于繁重的要求，比如预处理文本的函数；但幸运的是，sklearn还附带了一个functionTransformer，它允许我们将任意函数包装成一个sklearn兼容的函数。不过有一个问题:函数不应该直接对对象进行操作，而是将它们包装成列表、pandas系列或Numpy数组。不过这并不是主要的障碍，您可以创建一个助手函数，将输出打包成一个列表理解。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="dac0" class="mt ls it mp b gy mu mv l mw mx"># Adapted from <a class="ae le" href="https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-3/" rel="noopener ugc nofollow" target="_blank">https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-3/</a><br/></span><span id="2676" class="mt ls it mp b gy my mv l mw mx">from sklearn.preprocessing import FunctionTransformer</span><span id="2642" class="mt ls it mp b gy my mv l mw mx">def pipelinize(function, active=True):<br/>    def list_comprehend_a_function(list_or_series, active=True):<br/>        if active:<br/>            return [function(i) for i in list_or_series]<br/>        else: # if it's not active, just pass it right back<br/>            return list_or_series<br/>    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})</span></pre><p id="831b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一步，让我们构建一个sklearn管道，它使用NLTK的w_tokenizer函数和2.1节中的stemmer_snowball，并使用第4节中的预处理函数。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="a699" class="mt ls it mp b gy mu mv l mw mx">from sklearn.pipeline import Pipeline</span><span id="c39b" class="mt ls it mp b gy my mv l mw mx">estimators = [('tokenizer', pipelinize(w_tokenizer)), ('stemmer', pipelinize(stemmer_snowball)),('stopwordremoval', pipelinize(remove_stopwords)), ('preprocessor', pipelinize(preprocessor_final))]</span><span id="46c4" class="mt ls it mp b gy my mv l mw mx">pipe = Pipeline(estimators)<br/><br/>pipe.transform([sample_text])</span><span id="8d94" class="mt ls it mp b gy my mv l mw mx">Output:</span><span id="037f" class="mt ls it mp b gy my mv l mw mx">[['gemini',<br/>  'man',<br/>  'review',<br/>  'doubl',<br/>  'smith',<br/>  'cant',<br/>  'save',<br/>  'hackney',<br/>  'spi',<br/>  'flick',<br/>  'usa',<br/>  'rain',<br/>  'rain',<br/>  'ran']]</span></pre><p id="bede" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以很容易地更改上面的管道来使用如下所示的SpaCy函数。请注意，第3节中介绍的标记化函数(spacy_tokenizer_lemmatizer)返回不带任何停用词的标记化标记，因此这些步骤在我们的管道中是不必要的，我们可以直接运行预处理器。</p><pre class="lg lh li lj gt mo mp mq mr aw ms bi"><span id="727d" class="mt ls it mp b gy mu mv l mw mx">spacy_estimators = [('tokenizer', pipelinize(spacy_tokenizer_lemmatizer)), ('preprocessor', pipelinize(preprocessor_final))]<br/>spacy_pipe = Pipeline(spacy_estimators)<br/>spacy_pipe.transform([sample_text])</span><span id="63e7" class="mt ls it mp b gy my mv l mw mx"># Output:</span><span id="c4bb" class="mt ls it mp b gy my mv l mw mx">[['gemini',<br/>  'man',<br/>  'review',<br/>  'double',<br/>  'will',<br/>  'smith',<br/>  'not',<br/>  'save',<br/>  'hackneyed',<br/>  'spy',<br/>  'flick',<br/>  'usa',<br/>  'rain',<br/>  'rain',<br/>  'run']]</span></pre><p id="6bf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望我已经展示了使用Sklearn管道和基于空间的预处理工作流来有效地执行几乎所有NLP任务的预处理的巨大优势。</p></div></div>    
</body>
</html>