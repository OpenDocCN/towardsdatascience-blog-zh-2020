<html>
<head>
<title>Turbo-charge your spaCy NLP pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速您的空间NLP管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad?source=collection_archive---------30-----------------------#2020-05-02">https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad?source=collection_archive---------30-----------------------#2020-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b1fe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用自定义管道和joblib显著加快spaCy中文本预处理的技巧和诀窍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f1a1f4d86a8e6e6c17a5bcd013fe9501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cRvMDq5Tbzpq-M7n.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://pixabay.com/illustrations/space-rocket-night-cartoon-3262811/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="4106" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您有一个大型文本数据集，您希望对其应用一些非平凡的NLP转换，例如去除停用词，然后对文本中的词进行词汇化(即，将它们简化为词根形式)。spaCy 是一个工业级的NLP库，就是为这样的任务而设计的。</p><p id="9e5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，<a class="ae kv" href="https://www.kaggle.com/nzalake52/new-york-times-articles" rel="noopener ugc nofollow" target="_blank">纽约时报数据集</a>用于展示如何显著加快spaCy NLP管道。目标是接受一篇文章的文本，并快速返回一个词条列表以及不必要的单词，即删除的<em class="ls">停用词</em>。</p><p id="d850" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Pandas数据框架提供了一个方便的界面来处理这种性质的表格数据spaCy NLP方法可以方便地直接应用于数据框架的相关列。首先通过运行<a class="ae kv" href="https://github.com/prrao87/blog/tree/master/_notebooks/data/spacy_multiprocess" rel="noopener ugc nofollow" target="_blank">预处理笔记本</a> ( <code class="fe lt lu lv lw b">./data/preprocessing.ipynb</code>)获取新闻数据，该笔记本处理从Kaggle下载的raw文本文件，并对其进行一些基本的清理。该步骤生成一个包含表格数据的文件(存储为<code class="fe lt lu lv lw b">nytimes.tsv</code>)。在同一目录中<a class="ae kv" href="https://github.com/prrao87/blog/tree/master/_notebooks/data/spacy_multiprocess" rel="noopener ugc nofollow" target="_blank">还提供了一个精选的停用词文件。</a></p><h1 id="04af" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">负载空间模型</h1><p id="5ad0" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">由于在本练习中我们不会执行任何专门的任务，比如依赖关系解析和命名实体识别，所以在加载spaCy模型时这些组件是禁用的。</p><blockquote class="mu mv mw"><p id="21d9" class="kw kx ls ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">提示:</strong> spaCy有一个<code class="fe lt lu lv lw b">sentencizer</code>组件，可以插入空白管道。</p></blockquote><p id="ca9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">句子分析器管道简单地执行标记化和句子边界检测，随后可以将词条提取为标记属性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="8b24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">定义了一个方法，从文本文件中读入停用词，并将其转换为Python中的集合(为了高效查找)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="315f" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">在纽约时报数据集中阅读</h1><p id="c543" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">NYT新闻数据集的预处理版本作为熊猫数据帧读入。这些列被命名为<code class="fe lt lu lv lw b">date</code>、<code class="fe lt lu lv lw b">headline</code>和<code class="fe lt lu lv lw b">content</code>——内容列中显示的文本将被预处理以删除停用词并生成标记词条。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/41f183d67b60315cfd6aea92c799ad77.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*Jkt27i1d0A0EYzC1VxwHkQ.png"/></div></figure><h1 id="a959" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">定义文本清理器</h1><p id="21f8" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">由于新闻文章数据来自原始HTML转储，因此非常混乱，包含大量不必要的符号、社交媒体句柄、URL和其他工件。清理它的一个简单方法是使用正则表达式，它只解析给定长度(3到50)之间的字母数字字符串和连字符(以便包含带连字符的单词)。这会将每个文档过滤成对lemmatizer有意义的文本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/cab74a147ab8ebd1d6f59ec605d87c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*IzSS_Rqknc1TYeQrRbj-vA.png"/></div></figure><h1 id="aba7" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">选项1:顺序处理数据帧列</h1><p id="1039" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">现在我们只剩下了干净的字母数字标记，在继续词汇化之前，可以通过删除停用词来进一步清理这些标记。</p><p id="439b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">处理该文本的直接方法是使用一个现有的方法，在本例中是下面显示的<code class="fe lt lu lv lw b">lemmatize</code>方法，并使用<code class="fe lt lu lv lw b">pandas.Series.apply</code>将其应用到DataFrame的<code class="fe lt lu lv lw b"><a class="ae kv" href="https://prrao87.github.io/blog/images/copied_from_nb/clean" rel="noopener ugc nofollow" target="_blank">clean</a></code>列。使用每个令牌的底层<code class="fe lt lu lv lw b"><a class="ae kv" href="https://spacy.io/usage/spacy-101#annotations" rel="noopener ugc nofollow" target="_blank">Doc</a></code> <a class="ae kv" href="https://spacy.io/usage/spacy-101#annotations" rel="noopener ugc nofollow" target="_blank">表示</a>来完成词汇化，其中包含一个<code class="fe lt lu lv lw b">lemma_</code>属性。停用词在词汇化过程中同时被删除，因为这些步骤中的每一步都涉及到遍历相同的标记列表。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="b49f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">得到的词条作为一个列表存储在一个单独的列<code class="fe lt lu lv lw b">preproc</code>中，如下所示。</p><pre class="kg kh ki kj gt ne lw nf ng aw nh bi"><span id="a5e5" class="ni ly iq lw b gy nj nk l nl nm">%%time<br/>df_preproc['preproc'] = df_preproc['clean'].apply(lemmatize)<br/>df_preproc[['date', 'content', 'preproc']].head(3)</span><span id="ae1f" class="ni ly iq lw b gy nn nk l nl nm">CPU times: user 48.5 s, sys: 146 ms, total: 48.6 s Wall time: 48.6 s</span></pre><p id="e97e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将这种方法应用到DataFrame的<code class="fe lt lu lv lw b"><a class="ae kv" href="https://prrao87.github.io/blog/images/copied_from_nb/clean" rel="noopener ugc nofollow" target="_blank">clean</a></code>列并计时，它显示在8，800篇新闻文章上运行几乎需要一分钟。</p><h1 id="dc0c" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">选项2:使用<code class="fe lt lu lv lw b">nlp.pipe</code></h1><p id="04d1" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">我们能做得更好吗？在<a class="ae kv" href="https://spacy.io/api/language#pipe" rel="noopener ugc nofollow" target="_blank"> spaCy文档</a>中，指出“将文本作为流处理通常比逐个处理更有效”。这是通过调用语言管道来实现的，语言管道在内部将数据分成几批，以减少纯Python函数调用的数量。这意味着数据越大，<code class="fe lt lu lv lw b">nlp.pipe</code>所能获得的性能增益就越好。</p><p id="598a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使用语言管道来传输文本，定义了一个新的lemmatizer方法，它直接作用于spaCy <code class="fe lt lu lv lw b">Doc</code>对象。然后，该方法被批量调用，以处理通过管道传输的<code class="fe lt lu lv lw b">Doc</code>对象的<em class="ls">序列</em>，如下所示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="b7c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和以前一样，通过传递来自现有DataFrame的<code class="fe lt lu lv lw b"><a class="ae kv" href="https://prrao87.github.io/blog/images/copied_from_nb/clean" rel="noopener ugc nofollow" target="_blank">clean</a></code>列的数据来创建一个新列。注意，与上面的工作流#1不同，我们在这里不使用<code class="fe lt lu lv lw b">apply</code>方法——相反，数据列(一个iterable)作为参数直接传递给预处理管道方法。</p><pre class="kg kh ki kj gt ne lw nf ng aw nh bi"><span id="c79c" class="ni ly iq lw b gy nj nk l nl nm">%%time<br/>df_preproc['preproc_pipe'] = preprocess_pipe(df_preproc['clean'])<br/>df_preproc[['date', 'content', 'preproc_pipe']].head(3)</span><span id="5839" class="ni ly iq lw b gy nn nk l nl nm">CPU times: user 51.6 s, sys: 144 ms, total: 51.8 s Wall time: 51.8 s</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d17da5f9dbe1431521302e89cab3da1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*sWWQG-de-DEQcgcIKto9nA.png"/></div></figure><p id="f2b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对这个工作流进行计时似乎并没有显示出比之前的工作流有所改进，但是根据spaCy文档，随着我们处理越来越大的数据集，这种方法应该会显示出一些计时改进(平均而言)。</p><h1 id="3e7d" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">选项3:使用joblib并行化工作</h1><p id="f796" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">我们可以做得更好！以前的工作流程依次处理每个新闻文档，生成词条列表，然后作为新列附加到数据帧。因为每一行的输出都完全独立于另一行，这是一个令人尴尬的并行<em class="ls">问题，非常适合使用多个内核。</em></p><p id="56d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">spaCy推荐使用<code class="fe lt lu lv lw b">joblib</code>库来并行处理NLP流水线的块。确保您在运行以下部分之前<code class="fe lt lu lv lw b">pip install joblib</code>。</p><p id="f8b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了并行化工作流，必须定义更多的助手方法。</p><ul class=""><li id="f80b" class="np nq iq ky b kz la lc ld lf nr lj ns ln nt lr nu nv nw nx bi translated"><strong class="ky ir">分块:</strong>新闻文章内容是一个(长)字符串列表，其中每个文档代表一篇文章的文本。这些数据必须以“块”的形式提供给由<code class="fe lt lu lv lw b">joblib</code>启动的每个工作进程。每次调用<code class="fe lt lu lv lw b">chunker</code>方法都会返回一个生成器，该生成器只包含特定块的文本作为字符串列表。在词汇化过程中，基于迭代器索引检索每个新块(前面的块被“遗忘”)。</li><li id="0d56" class="np nq iq ky b kz ny lc nz lf oa lj ob ln oc lr nu nv nw nx bi translated"><strong class="ky ir">扁平化:</strong>一旦joblib创建了一组在每个块上工作的worker进程，每个worker返回一个包含每个文档的lemmas的“列表列表”。然后，这些列表由执行者组合，以提供一个3级嵌套的最终“列表列表列表”。为了确保executor输出的长度与文章的实际数量相同，定义了一个“flatten”方法来将结果合并到一个包含词条的列表中。例如，两个并行的执行器将返回一个最终的嵌套列表:<code class="fe lt lu lv lw b">[[[a, b, c], [d, e, f]], [[g, h, i], [j, k, l]]]</code>，其中<code class="fe lt lu lv lw b">[[a, b, c], [d, e, f]]</code>和<code class="fe lt lu lv lw b">[[g, h, i], [j, k, l]]</code>是指每个执行器的输出(最终的输出由joblib连接成一个列表)。这个结果的展平版本是<code class="fe lt lu lv lw b">[[a, b, c], [d, e, f], [g, h, i], [j, k, l]]</code>，即去掉了一层嵌套。</li></ul><p id="c91d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了上面的方法，一个类似的<code class="fe lt lu lv lw b">nlp.pipe</code>方法被用在工作流#2中，在每个文本块上。这些方法中的每一个都被封装到一个<code class="fe lt lu lv lw b">preprocess_parallel</code>方法中，该方法定义了要使用的工作进程的数量(在本例中为7个)，将输入数据分成块并返回一个扁平的结果，然后可以将该结果附加到DataFrame中。对于具有更多物理内核的机器，工作进程的数量可以进一步增加。</p><p id="2003" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用joblib的并行工作流如下所示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><pre class="kg kh ki kj gt ne lw nf ng aw nh bi"><span id="17d6" class="ni ly iq lw b gy nj nk l nl nm">%%time<br/>df_preproc['preproc_parallel'] = preprocess_parallel(df_preproc['clean'], chunksize=1000)</span><span id="682f" class="ni ly iq lw b gy nn nk l nl nm">CPU times: user 683 ms, sys: 248 ms, total: 932 ms Wall time: 17.2 s</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c40bfe4fa9dced63fe08a1bb9f90ca88.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*p2Mu31iMUdHGYt3OvChrEA.png"/></div></figure><p id="236d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对这种并行工作流进行计时显示出显著的性能提升(运行时间几乎减少了<strong class="ky ir">3倍</strong>)！随着文档数量的增加，使用joblib启动多个工作进程的额外开销很快就会得到补偿，并且这种方法可以显著优于顺序方法。</p><h1 id="801f" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">块大小和批量大小的影响</h1><p id="3add" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">请注意，在并行化的工作流中，需要指定两个参数—最佳数量可能因数据集而异。<code class="fe lt lu lv lw b">chunksize</code>控制每个进程处理的每个块的大小。在本例中，对于8，800个文档，使用的块大小为1000。太小的块大小意味着会产生大量的工作线程来处理大量的块，这会降低执行速度。通常，几百到几千个文档的块大小是一个很好的起点(当然，这取决于数据中每个文档的大小，以便块可以放入内存)。</p><p id="318e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">批量大小是特定于<code class="fe lt lu lv lw b">nlp.pipe</code>的参数，同样，一个好的值取决于正在处理的数据。对于相当长的文本，比如新闻文章，保持批量适当小是有意义的(这样每一批就不会包含真正的<em class="ls">长文本</em>，所以在这种情况下选择20作为批量大小。对于其他情况(例如Tweets ),如果每个文档的长度非常短，可以使用更大的批量。</p><p id="a86f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">建议试验任一参数，看看哪种组合能产生最佳性能</strong>。</p><h1 id="187b" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">集合与列表</h1><blockquote class="mu mv mw"><p id="4cd6" class="kw kx ls ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">重要提示:</strong>尽可能使用集合而不是列表进行查找。</p></blockquote><p id="68eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，在前面定义的<code class="fe lt lu lv lw b">get_stopwords()</code>方法中，从停用词文件中读入的停用词列表在lemmatizer方法中通过查找移除停用词之前被转换为一个集合。一般来说，这是一个非常有用的技巧，但是特别是对于停用词移除，集合的使用变得更加重要。为什么？</p><p id="3151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在任何现实的停用词表中，比如这个新闻数据集的停用词表，有理由期待<em class="ls">几百个</em>停用词。这是因为对于主题建模或情感分析等下游任务，有许多特定于领域的单词需要删除(非常常见的动词，无用的缩写，如时区、星期几等。).每一个文档中的每个单词都需要与停用词表中的每个单词进行比较，这对于成千上万的文档来说是一项昂贵的操作。</p><p id="3185" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，集合的查找时间为O(1)(即常数)，而列表的查找时间为O(n)。在<code class="fe lt lu lv lw b">lemmatize()</code>方法中，由于我们在停用词集中检查每个词的成员资格，我们期望集合比列表好得多。为了测试这一点，我们可以重新运行工作流#1，但是这一次，使用一个停用词列表<em class="ls">来代替。</em></p><pre class="kg kh ki kj gt ne lw nf ng aw nh bi"><span id="30cd" class="ni ly iq lw b gy nj nk l nl nm">stopwords = list(stopwords)</span><span id="016d" class="ni ly iq lw b gy nn nk l nl nm">%%time<br/>df_preproc['preproc_stopword_list'] = df_preproc['clean'].apply(lemmatize)<br/>df_preproc[['date', 'content', 'preproc_stopword_list']].head(3)</span><span id="d7fb" class="ni ly iq lw b gy nn nk l nl nm">CPU times: user 1min 17s, sys: 108 ms, total: 1min 18s Wall time: 1min 18s</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e60f9f18a8d74cad453d1d698ac1f154.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*ST6ABVaF4XGimMauLkWweQ.png"/></div></figure><p id="49a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了停用词表，现在产生相同的结果比以前(有了集合)多花了50%的时间，也就是说运行时间增加了1.5倍！这是有意义的，因为在这种情况下，停用词列表大约有500个单词长，并且需要检查语料库中的每个单词是否属于这个合理大小的列表。</p><h1 id="f897" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">结论</h1><p id="4935" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">在本练习中，使用spaCy管道处理了一个新闻文章数据集(NY Times ),以输出表示每篇文章内容中有用标记的词条列表。因为真实世界的新闻数据集几乎肯定比这个大，并且大小可以是无限的，所以需要快速、高效的NLP管道来对数据执行任何有意义的分析。以下步骤对于加速空间管道非常有用。</p><p id="7b6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">禁用spaCy模型中不必要的组件:</strong>标准spaCy模型的流水线包含tagger(分配词性标签)、parser(生成依赖解析)和命名实体识别组件。如果需要这些动作中的任何一个或者都不需要，这些组件<em class="ls">必须在加载模型后立即</em>被禁用(如上所示)。</p><p id="d8ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用集合而不是列表进行查找:</strong>当执行查找以将一组标记与另一组标记进行比较时，总是使用集合来执行成员资格检查——列表的查找速度要慢得多！停用词的列表/集合越大，使用集合时看到的性能增益就越大。</p><p id="2c77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">尽可能使用定制语言管道:</strong>使用<code class="fe lt lu lv lw b">nlp.pipe</code>设置语言管道是处理大块文本的一种非常灵活有效的方式。更好的是，spaCy允许您单独禁用每个特定子任务的组件，例如，当您需要单独执行词性标记和命名实体识别(NER)时。<a class="ae kv" href="https://spacy.io/usage/processing-pipelines#disabling" rel="noopener ugc nofollow" target="_blank">有关如何在模型加载、处理或处理定制块期间禁用管道组件的示例，请参见spaCy文档</a>。</p><p id="e1ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">尽可能使用多个内核:</strong>当处理彼此完全独立的单个文档时，请考虑通过在多个内核之间分配计算来并行化工作流。随着文档数量变得越来越大，性能提升可能是巨大的。人们只需要确保文档被分成块，所有这些块在任何给定的时间都必须适合内存。</p><p id="208c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这是有用的——祝你在下一个NLP项目中测试它们时愉快！</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="e8ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">原载于2020年5月2日</em><a class="ae kv" href="https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html" rel="noopener ugc nofollow" target="_blank"><em class="ls">https://prrao 87 . github . io</em></a><em class="ls">。</em></p></div></div>    
</body>
</html>