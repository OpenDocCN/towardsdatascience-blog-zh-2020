<html>
<head>
<title>How to Colab with TPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何与 TPU 合作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-colab-with-tpu-98e0b4230d9c?source=collection_archive---------8-----------------------#2020-08-29">https://towardsdatascience.com/how-to-colab-with-tpu-98e0b4230d9c?source=collection_archive---------8-----------------------#2020-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="daf6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在谷歌实验室 TPU 上训练一个拥抱脸伯特</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/6adf6a441d27d6d4d79bd213b6445470.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/0*nl2PNOKDLtz2mcuy.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">TPU 演示通过<a class="ae ku" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank">谷歌云平台博客</a></p></figure><p id="27e7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">TPU(张量处理单元)是专门为处理矩阵而优化的专用集成电路(ASICs)。</p><blockquote class="lr ls lt"><p id="6d8d" class="kv kw lu kx b ky kz ju la lb lc jx ld lv lf lg lh lw lj lk ll lx ln lo lp lq im bi translated">云 TPU 资源加速了线性代数计算的性能，线性代数计算在机器学习应用中被大量使用</p><p id="2f2e" class="kv kw lu kx b ky kz ju la lb lc jx ld lv lf lg lh lw lj lk ll lx ln lo lp lq im bi translated">— <a class="ae ku" href="https://cloud.google.com/tpu/docs/tpus#us" rel="noopener ugc nofollow" target="_blank">云 TPU 文档</a></p></blockquote><p id="8949" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">Google Colab 免费为 TPUs 提供实验支持！在本文中，我们将讨论如何在 Colab 上使用 TPU 训练模型。具体来说，我们将<strong class="kx iu">训练</strong> <a class="ae ku" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener"> <strong class="kx iu">伯特</strong> </a> <strong class="kx iu">进行</strong> <strong class="kx iu">文本分类</strong>使用<a class="ae ku" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚包通过 huggingface </a>上一个 TPU。</p><h1 id="762a" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">什么时候该用什么时候不该用 TPU</h1><p id="78ba" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">重要的事情先来。由于 TPU 针对一些特定的操作进行了优化，我们需要检查我们的模型是否实际使用了它们；即<strong class="kx iu">我们需要检查 TPU 是否真的帮助我们的模型</strong>训练得更快。以下是我们可能希望使用<a class="ae ku" href="https://cloud.google.com/tpu/docs/tpus#us" rel="noopener ugc nofollow" target="_blank">云 TPU 文档</a>中提到的 TPU 的一些用例:</p><ul class=""><li id="16e6" class="mv mw it kx b ky kz lb lc le mx li my lm mz lq na nb nc nd bi translated">由矩阵计算主导的模型</li><li id="2080" class="mv mw it kx b ky ne lb nf le ng li nh lm ni lq na nb nc nd bi translated">主训练循环中没有自定义张量流运算的模型</li><li id="90d5" class="mv mw it kx b ky ne lb nf le ng li nh lm ni lq na nb nc nd bi translated">训练数周或数月的模特</li><li id="aa3c" class="mv mw it kx b ky ne lb nf le ng li nh lm ni lq na nb nc nd bi translated">有效批量非常大的较大和非常大的模型</li></ul><blockquote class="lr ls lt"><p id="9adf" class="kv kw lu kx b ky kz ju la lb lc jx ld lv lf lg lh lw lj lk ll lx ln lo lp lq im bi translated"><strong class="kx iu">如果你的模型使用了<a class="ae ku" href="https://cloud.google.com/tpu/docs/tensorflow-ops" rel="noopener ugc nofollow" target="_blank">云 TPU 支持的张量流运算</a>中没有的自定义张量流运算</strong>，你可能宁愿<strong class="kx iu">使用 GPU 加速器</strong>来代替。</p></blockquote><h1 id="0939" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">初始化</h1><p id="9d75" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">TPU 在云上工作，不像 GPU 或 CPU 在本地工作。因此，我们需要在开始之前进行一些初始化:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">TPU 用途的设置</p></figure><p id="01a0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果您观察上面代码片段的输出，我们的 TPU 集群有<strong class="kx iu"> 8 个能够并行处理的逻辑 TPU 设备(0–7)</strong>。因此，我们定义了一个<strong class="kx iu">分布策略</strong>，用于在这 8 个设备上进行分布式培训:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="b3ef" class="nq lz it nm b gy nr ns l nt nu">strategy = tf.distribute.TPUStrategy(resolver)</span></pre><p id="734f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有关分布式培训的更多信息，请参考:<a class="ae ku" href="https://www.tensorflow.org/guide/distributed_training" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/distributed_training</a></p><h1 id="cd69" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">训练模型</h1><p id="5dd7" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">在本节中，我们将实际了解如何在 TPU 上训练 BERT。我们将通过两种方式做到这一点:</p><ol class=""><li id="0310" class="mv mw it kx b ky kz lb lc le mx li my lm mz lq nv nb nc nd bi translated">使用 model.fit()</li><li id="a317" class="mv mw it kx b ky ne lb nf le ng li nh lm ni lq nv nb nc nd bi translated">使用自定义训练循环。</li></ol><h2 id="a80b" class="nq lz it bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">使用 model.fit()</h2><p id="5cc4" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">由于我们使用的是分布策略，因此必须在每个设备上创建模型以共享参数。因此，需要在战略范围内创建和构建<strong class="kx iu">模型:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在分销战略的范围内创建和构建模型</p></figure><p id="e94d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后，我们简单地根据数据拟合模型，就像在常规训练设置中所做的那样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">训练模型</p></figure><p id="cfdc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">要保存模型权重:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="8c2f" class="nq lz it nm b gy nr ns l nt nu">model.save_weights("checkpoint/tpu-model.h5")</span></pre><p id="0f2a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在下一小节中，我们将讨论如何使用自定义训练循环来做同样的事情。</p><h2 id="a8ba" class="nq lz it bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">使用自定义训练循环</h2><p id="53dc" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">这里，我们只需要手动调整 TensorFlow 在前面的方法中在后端为我们做的一些事情。</p><p id="849e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，我们使用<strong class="kx iu"> tf.data </strong> API 创建一个数据管道:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">构建 tf.data 管道</p></figure><p id="4919" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，我们在前面的部分中不必担心这个的原因是 TensorFlow 自己处理了这些事情；也就是我们调用 model.fit()的时候。同样，这次，<strong class="kx iu">我们需要在 TPU 设备之间手动分发数据集</strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在 TPU 之间分配数据</p></figure><p id="4553" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们<strong class="kx iu">以与前面方法完全相同的方式</strong>创建和构建模型。或者，我们可以在策略范围中添加一些指标，用于手动损失和准确性监控:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在战略范围内定义指标</p></figure><p id="364e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在是最重要的部分，即训练步骤功能。但是首先，让我们为分布式数据集创建一个迭代器:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="56c4" class="nq lz it nm b gy nr ns l nt nu">train_iterator = iter(train_dataset)</span></pre><p id="6fc5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后我们编写 train_step 函数，并像平常一样用@tf.function 修饰它。我们使用 strategy.run()来执行训练步骤:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">训练阶跃函数</p></figure><p id="a71e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们在训练循环中运行这个 train_step 函数几次:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">训练循环</p></figure><p id="41b9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这一次，让我们尝试使用<strong class="kx iu">检查点</strong>保存模型。现在这里有一个问题。我们不能就这样拯救这个模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/d63fb6c851281ae9b55ef8a399e3787c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Em5pGrcn6mKcLAdIRg5jbg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">本地文件系统访问错误</p></figure><p id="97c7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">错误非常明显，它说<strong class="kx iu">当<a class="ae ku" href="https://www.tensorflow.org/guide/eager" rel="noopener ugc nofollow" target="_blank">急切执行</a>时，您不能访问本地文件系统</strong>，因为执行被带到云中，以便 TPU 执行其操作。</p><p id="8f61" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，为了克服这一点，我们需要<strong class="kx iu">将检查点保存在 GCS 桶</strong>中。您可以在此创建一个自由级 GCP 账户<a class="ae ku" href="https://cloud.google.com/free" rel="noopener ugc nofollow" target="_blank">。首先，我们需要创建一个云存储桶。</a><a class="ae ku" href="https://cloud.google.com/storage/docs/creating-buckets" rel="noopener ugc nofollow" target="_blank">这里有一个来自官方文档的关于创建 GCS bucket 的教程</a>。</p><p id="1026" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们需要使用我们的 GCP 凭据登录，并将 GCP 项目设置为活动配置:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="74a3" class="nq lz it nm b gy nr ns l nt nu">from google.colab import auth</span><span id="166d" class="nq lz it nm b gy om ns l nt nu">auth.authenticate_user()</span><span id="7905" class="nq lz it nm b gy om ns l nt nu">!gcloud config set project &lt;project-id&gt;</span></pre><blockquote class="lr ls lt"><p id="5812" class="kv kw lu kx b ky kz ju la lb lc jx ld lv lf lg lh lw lj lk ll lx ln lo lp lq im bi translated"><strong class="kx iu"> gcloud config set </strong>仅在您的活动配置中设置指定的属性。</p><p id="f0ae" class="kv kw lu kx b ky kz ju la lb lc jx ld lv lf lg lh lw lj lk ll lx ln lo lp lq im bi translated">— <a class="ae ku" href="https://cloud.google.com/sdk/gcloud/reference/config/set" rel="noopener ugc nofollow" target="_blank">谷歌云文档</a></p></blockquote><p id="86ab" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">完成后，我们只需使用以下命令即可访问我们的存储桶:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="2e03" class="nq lz it nm b gy nr ns l nt nu">gs://&lt;bucket-name&gt;/&lt;file-path&gt;</span></pre><p id="51ca" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，检查点看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">保存检查点</p></figure><p id="7249" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">而这一次，它会成功地将模型检查点保存到您的桶中！</p><h1 id="74f0" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">结论</h1><p id="3358" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">在本文中，我们看到了为什么以及如何调整为训练模型而编写的原始代码，使其与 TPU 兼容。我们还讨论了什么时候使用 TPU，什么时候不使用。</p><h1 id="47fe" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">参考</h1><div class="on oo gp gr op oq"><a href="https://www.tensorflow.org/guide/tpu" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">使用 TPUs | TensorFlow 核心</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">目前 Keras 和 Google Colab 提供了对云 TPU 的实验性支持。在你运行这个 Colab 之前…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.tensorflow.org</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ko oq"/></div></div></a></div><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/10-tensorflow-tricks-every-ml-practitioner-must-know-96b860e53c1"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">每个 ML 从业者必须知道的 10 个张量流技巧</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">为什么 TensorFlow 是完整的 ML 包</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe ko oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">伯特:语言理解变形金刚的前期训练</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe ko oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">BERT -变压器 3.0.2 文档</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">BERT 模型是在 BERT:用于语言理解的深度双向转换器的预训练中提出的…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">huggingface.co</p></div></div></div></a></div></div></div>    
</body>
</html>