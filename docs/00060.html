<html>
<head>
<title>Bagging, Boosting, and Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">装袋、增压和梯度增压</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bagging-boosting-and-gradient-boosting-1a8f135a5f4e?source=collection_archive---------31-----------------------#2020-01-02">https://towardsdatascience.com/bagging-boosting-and-gradient-boosting-1a8f135a5f4e?source=collection_archive---------31-----------------------#2020-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ebe7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">易于理解的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e11cc892b989c6234349e4b0a7b2eae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2iPDffxxXPD65xmT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·穆尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="86ef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">制袋材料</h1><p id="1b54" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Bagging 是在 bootstrap 样本上训练的机器学习模型的聚合(Bootstrap AGGregatING)。什么是引导样本？这些是<em class="mn">几乎</em>独立同分布(iid)样本——因为样本之间的相关性很低，而且它们来自同一个分布。引导过程用于创建这些样本。这包括从数据集中抽取样本<em class="mn">并替换掉</em>，记住如果这些样本足够大，它们将代表抽取样本的数据集，假设抽取样本的数据集也足够大以代表总体。在我们的数据集中有大量的观察值也意味着 bootstrap 样本更有可能在样本之间具有低相关性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/4fa165e41c8413b0a2c10b8961fd1e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5B-4-4VYFnekKD63RjRt5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。样本取自数据集，数据集也是取自总体的样本。</p></figure><p id="b139" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">为什么我们不简单地从总体中抽取更多的样本，而不是从我们的数据集中抽取样本？我们可能没有资源或能力从人群中获取更多样本——通常，在研究和工业中，我们必须完全依赖我们已经获得的或以前收集的数据。在这种情况下，bootstrapping 已经被证明是一种准确且计算效率高的估计人口分布的方法。</p><p id="d16b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">例如，如果我们有关于一个人是否拖欠贷款的数据，带有伯努利分布响应变量(0 或 1 表示此人是否拖欠)，我们可以计算响应变量的平均值，以快速估计我们的数据集中类的表现程度。为了找到响应变量均值的分布(了解其可变性以及我们的数据集对总体的代表性)，我们必须从总体中重新取样几次，并计算每个样本的均值，以便更好地了解响应变量的真实分布。然而，这并不总是可能的，我们只限于一个样本(我们的数据集)。经验表明，通过 bootstrapping，仅使用我们通过从数据集本身中抽取替换样本<em class="mn">而获得的数据，有可能获得我们采样统计(响应变量的平均值)的真实分布的相当准确的估计值。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/f337cb225e967374a02f5f34bad3cd94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSKfqGyEs-UVuI4Q3laJmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。左图:样本统计量α估计值的直方图，该估计值是根据总体中的 1，000 个样本(本例中为数据集)计算得出的。中心:从单个数据集的 1000 个 bootstrap 样本中获得的α估计值的直方图。右图:显示在左图和中间图中的α估计值显示为箱线图。在每个面板中，粉色线表示α的真实值。(詹姆斯等人，未注明)</p></figure><p id="8230" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">bagging 的想法自然来自 bootstrapping，因为独立、同质的机器学习模型是在几乎独立的 bootstrap 样本上训练的，并且它们的结果是通过加权平均值聚合的。在这些 bootstrap 样本上进行训练的原因是，样本具有低相关性，因此我们可以更好地代表总体。流行的 bagging 算法，random forest，在为每个引导样本拟合决策树时，也对一小部分特征进行子采样，从而进一步降低样本之间的相关性。Bagging 提供了真实总体的良好表示，因此最常用于具有高方差的模型(如基于树的模型)。</p><h1 id="df80" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">助推</h1><p id="73c0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在高层次上，所有提升算法都以相似的方式工作:</p><ol class=""><li id="2032" class="mv mw it lt b lu mp lx mq ma mx me my mi mz mm na nb nc nd bi translated">数据集中的所有观察值最初被赋予相同的权重。</li><li id="fc22" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">在这个数据集上训练机器学习模型。</li><li id="531e" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">来自步骤 2 的模型被添加到带有与其相关联的误差的集合中，该误差是误分类观测值的权重之和。</li><li id="7f44" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">错误分类的观察值被赋予更大的权重。</li><li id="26ef" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">步骤 2-4 会重复许多个时期。</li></ol><p id="3f4b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">这里的想法是使错误分类的观察在随后的学习迭代中显得更加重要。在这一点上应该注意的是，boosting 是一种非常强大的集合技术，但是由于其关注困难观测的性质，它容易过拟合。因此，它通常用于相对有偏差的基础模型，例如浅树。</p><p id="4949" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">最容易理解的升压算法是 AdaBoost，其工作原理如下:</p><ul class=""><li id="db33" class="mv mw it lt b lu mp lx mq ma mx me my mi mz mm nj nb nc nd bi translated">初始化变量。</li></ul><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="3a0a" class="np la it nl b gy nq nr l ns nt">M = Number of models to fit</span><span id="82aa" class="np la it nl b gy nu nr l ns nt">N = Number of observations in dataset</span><span id="a81e" class="np la it nl b gy nu nr l ns nt">m = 1 # Iteration counter, m &lt;= M.</span><span id="e1ce" class="np la it nl b gy nu nr l ns nt">w_i = 1/N # For i = 1, 2, 3, ..., N.</span><span id="d236" class="np la it nl b gy nu nr l ns nt">F_0 = 0 # Ensemble model.</span></pre><ul class=""><li id="53b9" class="mv mw it lt b lu mp lx mq ma mx me my mi mz mm nj nb nc nd bi translated">使用最小化加权误差<code class="fe nv nw nx nl b">e_m = sum(w_misclassified)</code>的观察权重<code class="fe nv nw nx nl b">w_i</code>训练模型<code class="fe nv nw nx nl b">f_m</code>。</li><li id="3dd6" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm nj nb nc nd bi translated">最新合奏模型<code class="fe nv nw nx nl b">F_m = F_m_minus_1 + (alpha_m * f_m)</code>哪里<code class="fe nv nw nx nl b">alpha_m = log(1 — e_m) / e_m</code>。</li><li id="8366" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm nj nb nc nd bi translated">以与<code class="fe nv nw nx nl b">alpha_m</code>成比例的增量更新错误分类观测值的权重。</li><li id="557a" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm nj nb nc nd bi translated">递增迭代计数器。</li></ul><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0f65" class="np la it nl b gy nq nr l ns nt">m += 1</span><span id="4c64" class="np la it nl b gy nu nr l ns nt">if m &lt;= M:<br/>    go to step 2</span></pre><ul class=""><li id="ac55" class="mv mw it lt b lu mp lx mq ma mx me my mi mz mm nj nb nc nd bi translated">最终模型是各个模型的预测总和乘以各自的 alpha 系数。</li></ul><p id="d20b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated"><code class="fe nv nw nx nl b">F = (alpha_1 * f_1) + (alpha_2 * f_2) + ... + (alpha_m * f_m)</code></p><h1 id="13f4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度推进</h1><p id="0690" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这种方法被称为梯度推进，因为它使用梯度下降算法来最小化向集合添加模型时的损失。后续模型被拟合到<em class="mn">伪残差</em>而不是调整权重。<em class="mn">随机梯度下降</em>(如 XGBoost 所用)通过对每个阶段的观察值和特征进行采样来增加随机性，这类似于随机森林算法，只是采样是在没有替换的情况下进行的。</p><p id="5d18" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">梯度推进算法通过构建新的模型来改进每次迭代，该新的模型添加了估计器<em class="mn"> h </em>以提供更好的模型。完美的<em class="mn"> h </em>意味着:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dad3dee92eed2d56b8b2a381d68a7ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*XTDCt40iMn1egmulwSJD1w.png"/></div></figure><p id="81f2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">其中 y 是目标。或者简单地说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/048028c8cd2384c67671b899cdefa785.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*sEaK5DnKFkscqB5F6sJbUA.png"/></div></figure><p id="ccdb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">这是目标和前一次迭代的模型预测之间的残差。为了近似这个残差，我们使用前面提到的伪残差，这在下面的步骤中解释:</p><ol class=""><li id="d1e0" class="mv mw it lt b lu mp lx mq ma mx me my mi mz mm na nb nc nd bi translated">最初，像往常一样在数据集上训练，并获得每个观察的预测。</li><li id="9f14" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">根据步骤 1 中的预测，将伪残差计算为损失函数偏导数的负值。</li><li id="46b1" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">使用伪残差作为下一个模型的新目标，并像以前一样获得每个观测值的预测。</li><li id="1737" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">重复步骤 2-3，进行<em class="mn"> M </em>次迭代。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0f4d8a66293530b61fced9d60b114795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*C1aZRW2IZv4chOwuqr0nNw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图三。按照步骤 2 的伪残差。该值用作在每个训练时期预测 F(x)和目标 y 之间的“距离”的度量。m 是每个基础模型，N 是之前数据集中的观察次数。</p></figure><p id="e46a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">在进行上述操作时，我们有效地预测了每次迭代中残差的近似值，因此，我们得到了以下集合模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0d855fa6aba4330142a0ae91fefef48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*hvnQy_FeZKCvzsv7kIM5XQ.png"/></div></figure><p id="1a5d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">带<em class="mn"> h </em>，如中所述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dad3dee92eed2d56b8b2a381d68a7ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*XTDCt40iMn1egmulwSJD1w.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/048028c8cd2384c67671b899cdefa785.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*sEaK5DnKFkscqB5F6sJbUA.png"/></div></figure><p id="3525" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">取而代之的是我们的伪残值。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="e90f" class="kz la it bd lb lc oj le lf lg ok li lj jz ol ka ll kc om kd ln kf on kg lp lq bi translated">参考</h1><p id="670f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">詹姆斯·g·威滕·d·哈斯蒂·t·蒂布希拉尼·r(未注明)。<em class="mn">统计学习入门</em>。</p></div></div>    
</body>
</html>