<html>
<head>
<title>Regularization Techniques And Their Implementation In TensorFlow(Keras)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化技术及其在TensorFlow(Keras)中的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-techniques-and-their-implementation-in-tensorflow-keras-c06e7551e709?source=collection_archive---------6-----------------------#2020-05-06">https://towardsdatascience.com/regularization-techniques-and-their-implementation-in-tensorflow-keras-c06e7551e709?source=collection_archive---------6-----------------------#2020-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1c6b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">技术(包括代码)</h2><div class=""/><div class=""><h2 id="9d40" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解用于减轻深度神经网络中过拟合问题的传统技术。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/351bf71319201fd906d6ac2bc083e214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uiDhrbUiq-y5vVlUFeVIFg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lh" href="https://unsplash.com/@herfrenchness?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Clarisse Croset </a>拍摄的照片</p></figure><h1 id="2028" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">正规化</h1><p id="1a82" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">深度神经网络(DNN)具有大量的架构内部权重参数，可以学习一系列值。这些数值范围是使神经网络能够解决巨大复杂函数的关键。</p><p id="9498" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">神经网络越深，其具有的代表能力就越强，但是，随着权参数数量的增加，会出现一个缺点。这个缺点是神经网络更容易过拟合训练数据集。</p><blockquote class="nb nc nd"><p id="aaf8" class="ma mb ne mc b md mw kd mf mg mx kg mi nf my ml mm ng mz mp mq nh na mt mu mv im bi translated"><strong class="mc jd">过拟合:</strong>这个问题涉及到算法预测呈现给它的模式的新实例，基于<strong class="mc jd">太接近于</strong>它在训练期间观察和学习的模式的实例。这可能导致机器学习算法无法准确地推广到看不见的数据。如果训练数据没有准确地表示测试数据的分布，则会发生过度拟合。可以通过减少训练数据中的特征数量以及通过各种技术降低网络的复杂性来修复过度拟合</p></blockquote><p id="87f9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正则化技术通过限制网络中权重值的范围来降低神经网络过拟合的可能性(稍后<em class="ne">将详细介绍</em>)。</p><p id="bf23" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">本文介绍了两种正则化策略，对损失函数的结果施加约束条件。</p><blockquote class="nb nc nd"><p id="7418" class="ma mb ne mc b md mw kd mf mg mx kg mi nf my ml mm ng mz mp mq nh na mt mu mv im bi translated"><strong class="mc jd">损失函数</strong>是一种量化机器学习模型表现<em class="it">有多好</em>的方法。量化是基于一组输入的输出(成本)，这些输入被称为参数值。参数值用于估计预测，而“损失”是预测值和实际值之间的差异。</p></blockquote><p id="61a3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">本文不会关注正则化的数学。相反，本文介绍了一些标准的正则化方法，以及如何使用TensorFlow(Keras)在神经网络中实现它们。</p><p id="e662" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">关于数学的更多细节，这些由<a class="ni nj ep" href="https://medium.com/u/c2958659896a?source=post_page-----c06e7551e709--------------------------------" rel="noopener" target="_blank">莱米·卡里姆</a>和<a class="ni nj ep" href="https://medium.com/u/31b07253bc35?source=post_page-----c06e7551e709--------------------------------" rel="noopener" target="_blank">雷努·汉德尔瓦尔</a>撰写的文章合理地展示了L1和L2的正则化数学。</p><div class="nk nl gp gr nm nn"><a href="https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">L1 L2正则化</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">在这篇文章中，我们将了解为什么我们需要正规化，什么是正规化，什么是不同类型的…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob lb nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/intuitions-on-l1-and-l2-regularisation-235f2db4c261"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">对L1和L2正则化的直觉</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">他们如何防止过度拟合？</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob lb nn"/></div></div></a></div></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="d278" class="li lj it bd lk ll ok ln lo lp ol lr ls ki om kj lu kl on km lw ko oo kp ly lz bi translated">正规化的类型</h1><p id="a2f1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">从上一节中，我们可以理解正则化技术作用于神经网络中的权重参数。更具体地说，它修改结果损失函数，进而修改产生的权重值。</p><h2 id="7cda" class="op lj it bd lk oq or dn lo os ot dp ls mj ou ov lu mn ow ox lw mr oy oz ly iz bi translated">l1正则化</h2><p id="1f73" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">L1正则化对神经网络权重值的影响在于，它通过使权重值等于0来惩罚接近0的权重值。负权重值也取值为0；所以如果一个权值是-2，在L1正则化的作用下，它变成0。</p><p id="ea30" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">L1正则化的一般直觉是，如果权重值接近0或非常小，那么它对于模型的整体性能来说是可以忽略的，因此使其为0不会影响模型的性能，并且可以减少模型的内存容量。</p><ul class=""><li id="13f9" class="pa pb it mc b md mw mg mx mj pc mn pd mr pe mv pf pg ph pi bi translated">L1对权重的绝对值之和(|weight|)进行惩罚</li></ul><p id="ebf0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我知道我说过我不会涉足数学，但是下面的数学符号应该相对容易理解。</p><p id="e2e0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们有损失函数，在这种情况下，均方差。</p><p id="9cc8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然后我们加上权重绝对值之和与<strong class="mc jd">正则化超参数值</strong>的乘积，用λ符号()表示。</p><p id="79bb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">数学符号中的<em class="ne"> i </em>表示当前权值的索引，<em class="ne"> n </em>表示层内权值的总数。<em class="ne"> W </em>代表重量值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/b8a1b8154fdeb91cd2b6f72d3990e19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*ER6_uFGh_JWFq_F1mxrJbA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">L1正则化</p></figure><h2 id="d99d" class="op lj it bd lk oq or dn lo os ot dp ls mj ou ov lu mn ow ox lw mr oy oz ly iz bi translated">l2正则化</h2><p id="6869" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">L2正则化也惩罚权重值。对于较小的权重值和相对较大的权重值，L2正则化将这些值转换为接近0但不完全0的数字。</p><ul class=""><li id="30a8" class="pa pb it mc b md mw mg mx mj pc mn pd mr pe mv pf pg ph pi bi translated">l2惩罚权重的平方和(权重)</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/c763c4b4b487e9d176f004919a6f0d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*8yuCvZMoYld71BrhW0Mcyg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">L2正则化</p></figure><p id="987b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你要结合L1和L2正则化技术的效果，那么你将得到<a class="ae lh" href="https://en.wikipedia.org/wiki/Elastic_net_regularization" rel="noopener ugc nofollow" target="_blank">‘弹性网正则化</a>’。</p><p id="d7fa" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正则化技术在训练过程中对神经网络产生影响，而不是进行推理。</p><p id="eee6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在我们对正则化有了一些基本的理解(<em class="ne">随意探索两种方法的数学</em>)和一些例子，让我们看看它们是如何实现的。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="c15e" class="li lj it bd lk ll ok ln lo lp ol lr ls ki om kj lu kl on km lw ko oo kp ly lz bi translated">实施正规化</h1><p id="4bf1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">第一步是导入用于实现或支持神经网络实现的工具和库。</p><ul class=""><li id="6bfd" class="pa pb it mc b md mw mg mx mj pc mn pd mr pe mv pf pg ph pi bi translated"><a class="ae lh" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> TensorFlow </strong> </a>:机器学习模型实现、训练、部署的开源平台。</li><li id="cd14" class="pa pb it mc b md pl mg pm mj pn mn po mr pp mv pf pg ph pi bi translated"><a class="ae lh" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> Keras </strong> </a>:一个开源库，用于实现运行在CPU和GPU上的神经网络架构。</li></ul><pre class="ks kt ku kv gt pq pr ps pt aw pu bi"><span id="4a74" class="op lj it pr b gy pv pw l px py">import tensorflow as tf<br/>from tensorflow import keras</span></pre><p id="029f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们将利用的数据集是普通的<a class="ae lh" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">时尚-MNIST数据集</a>。</p><p id="e85a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">时尚-MNIST数据集包含70，000幅服装图像。更具体地说，它包括60，000个训练样本和10，000个测试样本，这些样本都是尺寸为28×28的灰度图像，分为十类。</p><p id="475b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">数据集的准备包括通过将每个像素值除以255.0来归一化训练图像和测试图像。这将像素值置于范围0和1之间。</p><p id="86fb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">数据集的验证部分也在此阶段创建。这组数据集在训练期间被用来评估网络在各种迭代中的性能。</p><pre class="ks kt ku kv gt pq pr ps pt aw pu bi"><span id="1547" class="op lj it pr b gy pv pw l px py">(train_images, train_labels),(test_images, test_labels) = keras.datasets.fashion_mnist.load_data()<br/>train_images = train_images / 255.0<br/>test_images = test_images / 255.0</span><span id="3a62" class="op lj it pr b gy pz pw l px py">validation_images = train_images[:5000]<br/>validation_labels = train_labels[:5000]</span></pre><p id="9219" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">接下来，我们使用Keras Sequential API实现一个简单的模型。我们模型中的隐藏层使用了多种正则化技术。</p><p id="c5a9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">要添加一个正则化到一个层，你只需要把首选的正则化技术传递给层的关键字参数' kernel _ regularizer '。</p><p id="1e00" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Keras正则化实现方法可以提供代表<strong class="mc jd">正则化超参数值</strong>的参数。这显示在下面的一些层中。</p><p id="522b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Keras提供了l1和l2正则化的实现，我们将在下面的代码片段中的一些隐藏层中使用它。此外，我们包括一个利用l1和l2正则化的层。</p><p id="769c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这就是在神经网络中实现各种正则化技术的全部内容。不太难。</p><pre class="ks kt ku kv gt pq pr ps pt aw pu bi"><span id="b6e0" class="op lj it pr b gy pv pw l px py">model = keras.models.Sequential([<br/>    keras.layers.Flatten(input_shape=[28,28]),<br/>    keras.layers.Dense(200, activation='relu', kernel_regularizer=keras.regularizers.l1()),<br/>    keras.layers.Dense(100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),<br/>    keras.layers.Dense(50, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.01)),<br/>    keras.layers.Dense(10, activation='softmax')<br/>])</span></pre><p id="2b07" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在下一段代码中，我们设置并指定用于训练已实现的神经网络的优化算法；以及损失函数和超参数，例如学习速率和时期数。</p><pre class="ks kt ku kv gt pq pr ps pt aw pu bi"><span id="ca60" class="op lj it pr b gy pv pw l px py">sgd = keras.optimizers.SGD(lr=0.01)<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer=sgd, metrics=["accuracy"])</span><span id="6255" class="op lj it pr b gy pz pw l px py">model.fit(train_images, train_labels, epochs=60, validation_data=(validation_images, validation_labels))</span></pre><p id="19cd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用先前搁置的测试数据进行模型性能的评估。</p><p id="15cc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过评估结果，您可以在观察测试数据集评估的准确性后，决定微调网络超参数或继续生产。</p><pre class="ks kt ku kv gt pq pr ps pt aw pu bi"><span id="aff9" class="op lj it pr b gy pv pw l px py">model.evaluate(test_images, test_labels)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="af14" class="li lj it bd lk ll ok ln lo lp ol lr ls ki om kj lu kl on km lw ko oo kp ly lz bi translated">结论</h1><p id="8588" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如果损失函数上的正则化项被从层中排除并被训练相同数量的历元，则实现的模型可能在测试数据集上具有更好的性能。</p><p id="5a77" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正则化更常用于具有数百万个参数和更多特征的更深层次的神经网络中。</p><p id="2fe2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我希望读者现在对各种各样的正则化技术以及如何实现它们有一个直觉。</p><p id="ad7d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">探索神经网络实现和训练中的其他正则化方法可能是令人感兴趣的，例如<a class="ae lh" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">退出</a>或<a class="ae lh" href="https://en.wikipedia.org/wiki/Early_stopping" rel="noopener ugc nofollow" target="_blank">提前停止</a>。</p><p id="6f0b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">下面是本文包含的代码的GitHub存储库。</p><div class="nk nl gp gr nm nn"><a href="https://github.com/RichmondAlake/tensorflow_2_tutorials/blob/master/09_regularization.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">Richmond alake/tensor flow _ 2 _教程</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">github.com</p></div></div><div class="nw l"><div class="qa l ny nz oa nw ob lb nn"/></div></div></a></div></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><div class="ks kt ku kv gt nn"><a rel="noopener follow" target="_blank" href="/batch-normalization-in-neural-networks-code-d7c9b88da9f5"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">神经网络中的批量标准化(代码)</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">通过TensorFlow (Keras)实施</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="qb l ny nz oa nw ob lb nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/5-soft-skills-you-need-as-a-machine-learning-engineer-and-why-41ef6854cef6"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">作为机器学习工程师你需要的5个软技能(以及为什么)</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">包括成为任何劳动力的有用组成部分的提示</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="qc l ny nz oa nw ob lb nn"/></div></div></a></div></div></div>    
</body>
</html>