<html>
<head>
<title>Hidden Markov Model — Implemented from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐马尔可夫模型—从头开始实施</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e?source=collection_archive---------0-----------------------#2020-03-28">https://towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e?source=collection_archive---------0-----------------------#2020-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d399085aa44118395390504d552b7a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9sOLluzCXbiePjjRzeJ4w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">酷暑中的短暂停顿。葡萄牙，<a class="ae kf" href="https://private.zerowithdot.com/travelling/" rel="noopener ugc nofollow" target="_blank"> 2019 </a>。</p></figure><p id="b410" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想把这项工作扩展成一系列教程视频。如果你感兴趣，请<strong class="ki iu">订阅</strong>我的<a class="ae kf" href="https://landing.mailerlite.com/webforms/landing/j5y2q1" rel="noopener ugc nofollow" target="_blank">简讯</a>保持联系。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="2abc" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">介绍</h1><p id="5423" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">互联网上充满了很好地解释隐马尔可夫模型(HMM)背后理论的好文章(例如<a class="ae kf" href="https://medium.com/@jonathan_hui/machine-learning-hidden-markov-model-hmm-31660d217a61" rel="noopener"> 1 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/probability-learning-vi-hidden-markov-models-fab5c1f0a31d"> 2 </a>、<a class="ae kf" href="https://medium.com/@Ayra_Lux/hidden-markov-models-part-1-the-likelihood-problem-8dd1066a784e" rel="noopener"> 3 </a>和<a class="ae kf" href="https://medium.com/@Ayra_Lux/hidden-markov-models-part-2-the-decoding-problem-c628ba474e69" rel="noopener"> 4 </a>)。然而，许多这些作品包含了相当数量的相当先进的数学方程。如果想要解释理论，方程是必要的，我们决定将它带到下一个层次，创建一个<strong class="ki iu">温和的一步一步的实际实现</strong>来补充其他人的良好工作。</p><p id="cdb5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个简短的系列文章<em class="mo">的两篇文章</em>中，我们将专注于将所有复杂的数学转换成代码。我们的起点是马克·斯坦普写的<a class="ae kf" href="https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf" rel="noopener ugc nofollow" target="_blank">文件</a>。我们将使用这篇文章来定义我们的代码(本文)，然后使用一个有点特殊的例子“<a class="ae kf" rel="noopener" target="_blank" href="/hidden-markov-model-a-story-of-the-morning-insanity-718b34318120">晨昏</a>”来展示它在实践中的表现。</p><h2 id="c5c1" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">注释</h2><p id="88ba" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在我们开始之前，让我们回顾一下我们将使用的符号。顺便说一句，如果有些内容你不清楚，也不要担心。我们会握住你的手。</p><ul class=""><li id="9fc5" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld ng nh ni nj bi translated"><em class="mo"> T </em> -观察序列的长度。</li><li id="67e0" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo">N</em>——潜在(隐藏)状态的数量。</li><li id="19c3" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo">M</em>——可观察到的数量。</li><li id="2646" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo">问</em> = {q₀，q₁，…} -潜州。</li><li id="5ebb" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo"> V = </em> {0，1，…，M — 1} -一组可能的观察值。</li><li id="9d2e" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><strong class="ki iu">一个</strong> -状态转移矩阵。</li><li id="1c40" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><strong class="ki iu"> B </strong> -排放概率矩阵。</li><li id="8053" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">π-初始状态概率分布。</li><li id="a520" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo"> O </em> -观察顺序。</li><li id="f644" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="mo"> X = </em> (x₀，x₁，…)，x_t ∈ Q -隐态序列。</li></ul><p id="1d27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义了该集合后，我们可以使用矩阵计算任何状态和观察值的概率:</p><ul class=""><li id="4e4e" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld ng nh ni nj bi translated"><strong class="ki iu"> A </strong> = {a_ij} —开始一个转换矩阵。</li><li id="5053" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated"><strong class="ki iu"> B </strong> = {b_j(k)} —为发射矩阵。</li></ul><p id="da8a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与跃迁和观察(发射)相关的概率为:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bef7f99656dd5e8b1c38cecb82fc1a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*T0Zhx76AK8Wbxi9E0sIvPQ.png"/></div></figure><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e94fccba6651be5e2db02d240439d815.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*lZqmmKNImg_M9DEqbrcFgQ.png"/></div></figure><p id="6fda" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，模型被定义为一个集合:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9d8f427003ce897bb6ae1a894498b7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*ZpWRw7mtNyISVlcJw-QzRg.png"/></div></figure><h1 id="994c" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">基本定义</h1><p id="c23a" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">因为 HMM 是基于概率向量和矩阵的，所以让我们首先定义表示基本概念的对象。为了有用，对象必须反映某些属性。例如，概率向量的所有元素必须是数字<em class="mo"> 0 </em> ≤ x ≤ 1，并且它们的总和必须为 1。因此，让我们设计对象的方式，它们将固有地保护数学属性。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="841a" class="mp lm it oc b gy og oh l oi oj">import numpy as np<br/>import pandas as pd<br/><br/><br/>class ProbabilityVector:<br/>    def __init__(self, probabilities: dict):<br/>        states = probabilities.keys()<br/>        probs  = probabilities.values()<br/>        <br/>        assert len(states) == len(probs), <br/>            "The probabilities must match the states."<br/>        assert len(states) == len(set(states)), <br/>            "The states must be unique."<br/>        assert abs(sum(probs) - 1.0) &lt; 1e-12, <br/>            "Probabilities must sum up to 1."<br/>        assert len(list(filter(lambda x: 0 &lt;= x &lt;= 1, probs))) == len(probs), \<br/>            "Probabilities must be numbers from [0, 1] interval."<br/>        <br/>        self.states = sorted(probabilities)<br/>        self.values = np.array(list(map(lambda x: <br/>            probabilities[x], self.states))).reshape(1, -1)<br/>        <br/>    @classmethod<br/>    def initialize(cls, states: list):<br/>        size = len(states)<br/>        rand = np.random.rand(size) / (size**2) + 1 / size<br/>        rand /= rand.sum(axis=0)<br/>        return cls(dict(zip(states, rand)))<br/>    <br/>    @classmethod<br/>    def from_numpy(cls, array: np.ndarray, state: list):<br/>        return cls(dict(zip(states, list(array))))<br/><br/>    @property<br/>    def dict(self):<br/>        return {k:v for k, v in zip(self.states, list(self.values.flatten()))}<br/><br/>    @property<br/>    def df(self):<br/>        return pd.DataFrame(self.values, columns=self.states, index=['probability'])<br/><br/>    def __repr__(self):<br/>        return "P({}) = {}.".format(self.states, self.values)<br/><br/>    def __eq__(self, other):<br/>        if not isinstance(other, ProbabilityVector):<br/>            raise NotImplementedError<br/>        if (self.states == other.states) and (self.values == other.values).all():<br/>            return True<br/>        return False<br/><br/>    def __getitem__(self, state: str) -&gt; float:<br/>        if state not in self.states:<br/>            raise ValueError("Requesting unknown probability state from vector.")<br/>        index = self.states.index(state)<br/>        return float(self.values[0, index])<br/><br/>    def __mul__(self, other) -&gt; np.ndarray:<br/>        if isinstance(other, ProbabilityVector):<br/>            return self.values * other.values<br/>        elif isinstance(other, (int, float)):<br/>            return self.values * other<br/>        else:<br/>            NotImplementedError<br/><br/>    def __rmul__(self, other) -&gt; np.ndarray:<br/>        return self.__mul__(other)<br/><br/>    def __matmul__(self, other) -&gt; np.ndarray:<br/>        if isinstance(other, ProbabilityMatrix):<br/>            return self.values @ other.values<br/><br/>    def __truediv__(self, number) -&gt; np.ndarray:<br/>        if not isinstance(number, (int, float)):<br/>            raise NotImplementedError<br/>        x = self.values<br/>        return x / number if number != 0 else x / (number + 1e-12)<br/><br/>    def argmax(self):<br/>        index = self.values.argmax()<br/>        return self.states[index]</span></pre><p id="66a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">初始化这个对象最自然的方法是使用一个<em class="mo">字典</em>，因为它将值与唯一的键相关联。不幸的是，字典不提供任何断言机制来约束值。因此，我们构建自定义的<em class="mo"> ProbabilityVector </em>对象来确保我们的值行为正确。最重要的是，我们实施以下措施:</p><ul class=""><li id="b075" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld ng nh ni nj bi translated">值的数量必须等于键的数量(我们各州的名称)。虽然这在从字典初始化对象时不是问题，但我们稍后将使用其他方法。</li><li id="1b32" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">所有州的名称必须是唯一的(同样的参数也适用)。</li><li id="acc7" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">概率总和必须为 1(达到一定的容差)。</li><li id="91ac" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">所有概率必须是 0 ≤ p ≤ 1。</li></ul><p id="a9fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确保了这一点，我们还提供了两种可选的方法来实例化<code class="fe ok ol om oc b">ProbabilityVector</code>对象(用<code class="fe ok ol om oc b">@classmethod</code>修饰)。</p><ol class=""><li id="edf4" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld on nh ni nj bi translated">我们随机实例化对象——这在训练时会很有用。</li><li id="4501" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">我们使用现成的 numpy 数组并在其中使用值，并且只提供状态的名称。</li></ol><p id="7704" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了方便和调试，我们提供了另外两种请求值的方法。用修饰，它们将 PV 对象的内容作为字典或熊猫数据帧返回。</p><p id="24ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PV 对象需要满足以下数学运算(为了构建 HMM):</p><ol class=""><li id="ab01" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld on nh ni nj bi translated">比较(<code class="fe ok ol om oc b">__eq__</code>)——要知道任何两个 PV 是否相等，</li><li id="487a" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">两个 PV 的逐元素乘法或标量乘法(<code class="fe ok ol om oc b">__mul__</code>和<code class="fe ok ol om oc b">__rmul__</code>)。</li><li id="4bdb" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">点积(<code class="fe ok ol om oc b">__matmul__</code> ) -执行向量矩阵乘法</li><li id="9c8c" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">按数字划分(<code class="fe ok ol om oc b">__truediv__</code>)，</li><li id="06b1" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated"><code class="fe ok ol om oc b">argmax</code>找出哪个州的概率最高。</li><li id="2344" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated"><code class="fe ok ol om oc b">__getitem__</code>通过按键选择数值。</li></ol><p id="96de" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，当一个 PV 乘以一个标量时，返回的结构是一个 numpy 数组，而不是另一个 PV。这是因为乘以 1 以外的任何值都会破坏 PV 本身的完整性。</p><p id="3d52" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在内部，这些值存储为大小为(1 × N)的 numpy 数组。</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f2dcdf531e5445849b727952d5e5229b.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*3kJ1kVSOYyCpHNp2cpeq-g.png"/></div></figure><h2 id="e000" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="88fa" class="mp lm it oc b gy og oh l oi oj">a1 = ProbabilityVector({'rain': 0.7, 'sun': 0.3})<br/>a2 = ProbabilityVector({'sun': 0.1, 'rain': 0.9})<br/>print(a1.df)<br/>print(a2.df)<br/><br/>print("Comparison:", a1 == a2)<br/>print("Element-wise multiplication:", a1 * a2)<br/>print("Argmax:", a1.argmax())<br/>print("Getitem:", a1['rain'])<br/><br/># OUTPUT<br/>&gt;&gt;&gt;              rain  sun<br/>    probability   0.7  0.3<br/>                 rain  sun<br/>    probability   0.9  0.1<br/><br/>&gt;&gt;&gt; Comparison: False<br/>&gt;&gt;&gt; Element-wise multiplication: [[0.63 0.03]]<br/>&gt;&gt;&gt; Argmax: rain<br/>&gt;&gt;&gt; Getitem: 0.7</span></pre><h2 id="72d6" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">随机阵</h2><p id="0ba5" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">另一个对象是一个<code class="fe ok ol om oc b">Probability Matrix</code>，它是 HMM 定义的核心部分。形式上，<strong class="ki iu"> A </strong>和<strong class="ki iu"> B </strong>矩阵必须是<em class="mo">行随机的</em>，这意味着每一行的值总和必须为 1。因此，我们可以通过堆叠几个 PV 来定义我们的 PM，我们以保证这种约束的方式构建了这些 PV。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="b839" class="mp lm it oc b gy og oh l oi oj">class ProbabilityMatrix:<br/>    def __init__(self, prob_vec_dict: dict):<br/>        <br/>        assert len(prob_vec_dict) &gt; 1, \<br/>            "The numebr of input probability vector must be greater than one."<br/>        assert len(set([str(x.states) for x in prob_vec_dict.values()])) == 1, \<br/>            "All internal states of all the vectors must be indentical."<br/>        assert len(prob_vec_dict.keys()) == len(set(prob_vec_dict.keys())), \<br/>            "All observables must be unique."<br/><br/>        self.states      = sorted(prob_vec_dict)<br/>        self.observables = prob_vec_dict[self.states[0]].states<br/>        self.values      = np.stack([prob_vec_dict[x].values \<br/>                           for x in self.states]).squeeze() <br/><br/>    @classmethod<br/>    def initialize(cls, states: list, observables: list):<br/>        size = len(states)<br/>        rand = np.random.rand(size, len(observables)) \<br/>             / (size**2) + 1 / size<br/>        rand /= rand.sum(axis=1).reshape(-1, 1)<br/>        aggr = [dict(zip(observables, rand[i, :])) for i in range(len(states))]<br/>        pvec = [ProbabilityVector(x) for x in aggr]<br/>        return cls(dict(zip(states, pvec)))<br/><br/>    @classmethod<br/>    def from_numpy(cls, array: <br/>                  np.ndarray, <br/>                  states: list, <br/>                  observables: list):<br/>        p_vecs = [ProbabilityVector(dict(zip(observables, x))) \<br/>                  for x in array]<br/>        return cls(dict(zip(states, p_vecs)))<br/><br/>    @property<br/>    def dict(self):<br/>        return self.df.to_dict()<br/><br/>    @property<br/>    def df(self):<br/>        return pd.DataFrame(self.values, <br/>               columns=self.observables, index=self.states)<br/><br/>    def __repr__(self):<br/>        return "PM {} states: {} -&gt; obs: {}.".format(<br/>            self.values.shape, self.states, self.observables)<br/><br/>    def __getitem__(self, observable: str) -&gt; np.ndarray:<br/>        if observable not in self.observables:<br/>            raise ValueError("Requesting unknown probability observable from the matrix.")<br/>        index = self.observables.index(observable)<br/>        return self.values[:, index].reshape(-1, 1)</span></pre><p id="7be7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们实例化 PM 的方法是向类的构造函数提供一个 PV 的字典。通过这样做，我们不仅确保 PM 的每一行都是随机的，而且还提供了每个<strong class="ki iu">可观察</strong>的名称。</p><p id="26ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们的 PM 可以给出任何可观测值的系数数组。数学上，PM 是一个矩阵:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/209612cb7a86239a606c633ff2d60f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*YywRl3rWWpWJrlMdyKTlFw.png"/></div></figure><p id="291d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其他方法的实现方式与 PV 类似。</p><h2 id="9af8" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="bc65" class="mp lm it oc b gy og oh l oi oj">a1 = ProbabilityVector({'rain': 0.7, 'sun': 0.3})<br/>a2 = ProbabilityVector({'rain': 0.6, 'sun': 0.4})<br/>A  = ProbabilityMatrix({'hot': a1, 'cold': a2})<br/><br/>print(A)<br/>print(A.df)<br/>&gt;&gt;&gt; PM (2, 2) states: ['cold', 'hot'] -&gt; obs: ['rain', 'sun'].<br/>&gt;&gt;&gt;      rain  sun<br/>   cold   0.6  0.4<br/>   hot    0.7  0.3<br/><br/>b1 = ProbabilityVector({'0S': 0.1, '1M': 0.4, '2L': 0.5})<br/>b2 = ProbabilityVector({'0S': 0.7, '1M': 0.2, '2L': 0.1})<br/>B =  ProbabilityMatrix({'0H': b1, '1C': b2})<br/><br/>print(B)<br/>print(B.df)<br/>&gt;&gt;&gt; PM (2, 3) states: ['0H', '1C'] -&gt; obs: ['0S', '1M', '2L'].<br/>&gt;&gt;&gt;       0S   1M   2L<br/>     0H  0.1  0.4  0.5<br/>     1C  0.7  0.2  0.1<br/><br/>P = ProbabilityMatrix.initialize(list('abcd'), list('xyz'))<br/>print('Dot product:', a1 @ A)<br/>print('Initialization:', P)<br/>print(P.df)<br/>&gt;&gt;&gt; Dot product: [[0.63 0.37]]<br/>&gt;&gt;&gt; Initialization: PM (4, 3) <br/>    states: ['a', 'b', 'c', 'd'] -&gt; obs: ['x', 'y', 'z'].<br/>&gt;&gt;&gt;          x         y         z<br/>   a  0.323803  0.327106  0.349091<br/>   b  0.318166  0.326356  0.355478<br/>   c  0.311833  0.347983  0.340185<br/>   d  0.337223  0.316850  0.345927</span></pre><h1 id="9d8e" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">实现隐马尔可夫链</h1><p id="6d6b" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在我们继续计算分数之前，让我们使用 PV 和 PM 定义来实现隐马尔可夫链。</p><p id="dbbd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们将这样做作为一个类，称之为<code class="fe ok ol om oc b">HiddenMarkovChain</code>。它将在<strong class="ki iu"> A </strong>、<strong class="ki iu"> B </strong>和π处进行分页。稍后，我们将实现更多适用于这个类的方法。</p><h2 id="5ddd" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">计算分数</h2><p id="778b" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">计算分数意味着在给定我们的(已知)模型λ = ( <strong class="ki iu"> A </strong>，<strong class="ki iu"> B </strong>，π)的情况下，找出特定观察链<em class="mo"> O </em>的概率是多少。换句话说，我们对寻找 p(O|λ) 感兴趣。</p><p id="8959" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过<em class="mo">边缘化</em>隐藏变量<em class="mo"> X </em>的所有可能链，我们可以找到<em class="mo"> p(O|λ) </em>，其中<em class="mo"> X = { </em> x₀，<em class="mo"> …} </em>:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/8c57b9a5fe1bed3333212e433687dd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*iwv-JeejXCGA83nXsjQMbQ.png"/></div></figure><p id="19e9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<em class="mo"> p(O|X，λ) = ∏ b(O) </em>(与可观测值相关的所有概率的乘积)和<em class="mo"> p(X|λ)=π ∏ a </em>(从在<em class="mo"> t </em>的<em class="mo"> x </em>到在<em class="mo"> t + 1 </em>的<em class="mo"> x </em>跃迁的所有概率的乘积)，我们要寻找的概率(<strong class="ki iu">分数【T45</strong></p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/0574c4b6380eb41fe0454307fb44957d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64rWhxvZgBI_aa6poPm0gw.png"/></div></div></figure><p id="3585" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一种简单的计算分数的方法，因为我们需要计算每个可能的链<em class="mo"> X </em>的概率。不管怎样，让我们用 python 来实现它:</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="1c27" class="mp lm it oc b gy og oh l oi oj">from itertools import product<br/>from functools import reduce<br/><br/><br/>class HiddenMarkovChain:<br/>    def __init__(self, T, E, pi):<br/>        self.T = T  # transmission matrix A<br/>        self.E = E  # emission matrix B<br/>        self.pi = pi<br/>        self.states = pi.states<br/>        self.observables = E.observables<br/>    <br/>    def __repr__(self):<br/>        return "HML states: {} -&gt; observables: {}.".format(<br/>            len(self.states), len(self.observables))<br/>    <br/>    @classmethod<br/>    def initialize(cls, states: list, observables: list):<br/>        T = ProbabilityMatrix.initialize(states, states)<br/>        E = ProbabilityMatrix.initialize(states, observables)<br/>        pi = ProbabilityVector.initialize(states)<br/>        return cls(T, E, pi)<br/>    <br/>    def _create_all_chains(self, chain_length):<br/>        return list(product(*(self.states,) * chain_length))<br/>    <br/>    def score(self, observations: list) -&gt; float:<br/>        def mul(x, y): return x * y<br/>        <br/>        score = 0<br/>        all_chains = self._create_all_chains(len(observations))<br/>        for idx, chain in enumerate(all_chains):<br/>            expanded_chain = list(zip(chain, [self.T.states[0]] + list(chain)))<br/>            expanded_obser = list(zip(observations, chain))<br/>            <br/>            p_observations = list(map(lambda x: self.E.df.loc[x[1], x[0]], expanded_obser))<br/>            p_hidden_state = list(map(lambda x: self.T.df.loc[x[1], x[0]], expanded_chain))<br/>            p_hidden_state[0] = self.pi[chain[0]]<br/>            <br/>            score += reduce(mul, p_observations) * reduce(mul, p_hidden_state)<br/>        return score</span></pre><h2 id="0189" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="c28f" class="mp lm it oc b gy og oh l oi oj">a1 = ProbabilityVector({'1H': 0.7, '2C': 0.3})<br/>a2 = ProbabilityVector({'1H': 0.4, '2C': 0.6})<br/><br/>b1 = ProbabilityVector({'1S': 0.1, '2M': 0.4, '3L': 0.5})<br/>b2 = ProbabilityVector({'1S': 0.7, '2M': 0.2, '3L': 0.1})<br/><br/>A = ProbabilityMatrix({'1H': a1, '2C': a2})<br/>B = ProbabilityMatrix({'1H': b1, '2C': b2})<br/>pi = ProbabilityVector({'1H': 0.6, '2C': 0.4})<br/><br/>hmc = HiddenMarkovChain(A, B, pi)<br/>observations = ['1S', '2M', '3L', '2M', '1S']<br/><br/>print("Score for {} is {:f}.".format(observations, hmc.score(observations)))<br/>&gt;&gt;&gt; Score for ['1S', '2M', '3L', '2M', '1S'] is 0.003482.</span></pre><p id="7b6b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们的实现是正确的，那么对于一个给定的模型，所有可能的观察链的所有分值应该加起来是 1。即:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi os"><img src="../Images/27970b26aa5faac5d6424ade0947ae10.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*_hQL9-7vqRtmakk0epwH7Q.png"/></div></figure><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="d4bf" class="mp lm it oc b gy og oh l oi oj">all_possible_observations = {'1S', '2M', '3L'}<br/>chain_length = 3  # any int &gt; 0<br/>all_observation_chains = list(product(*(all_possible_observations,) * chain_length))<br/>all_possible_scores = list(map(lambda obs: hmc.score(obs), all_observation_chains))<br/>print("All possible scores added: {}.".format(sum(all_possible_scores)))<br/>&gt;&gt;&gt; All possible scores added: 1.0.</span></pre><p id="6550" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确实如此。</p><h1 id="747d" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">向前传球得分</h1><p id="541d" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">用我们上面的方法计算分数有点天真。为了找到特定观察链<em class="mo">或</em>的数字，我们必须计算所有可能的潜在变量序列<em class="mo"> X </em>的分数。这需要 2TN^T 乘法，即使是小数字也需要时间。</p><p id="4c93" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种方法是计算到时间 t 为止的序列的<em class="mo">部分观测值。</em></p><p id="af68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">For 和<em class="mo"> i ∈ {0，1，…，N-1} </em>和<em class="mo"> t ∈ {0，1，…，T-1} </em>:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/683a1c23e77f27ce37012d28c117e8a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*mXqvaUwFkNfqpjorU27NDA.png"/></div></figure><p id="9ea1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7296a7aa68e76e6724a63a89c9f9d051.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*JSGl-T_Z8bDkrmtyJwVAww.png"/></div></figure><p id="5a14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/64dd85afe528cc0a6fdc4593c1159713.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*iMouiFNUBISWwqnNbbvmVg.png"/></div></figure><p id="110a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4f150f3f00a157b6bacc09a51de1dc4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*jIMu6YqeNDqNeEAnkB-pjw.png"/></div></figure><p id="e414" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意<em class="mo"> α_t </em>是一个长度为<em class="mo"> N </em>的向量。乘积之和<em class="mo"> α a </em>实际上可以写成点积。因此:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5749f2aad69aaf10c3476ca98c276bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*KwVaGXQwHbcqipX-CV3l8w.png"/></div></figure><p id="84de" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中星号表示元素间的乘法。</p><p id="614d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这种实现，我们将乘法次数减少到 N T，并且可以利用向量化。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="254f" class="mp lm it oc b gy og oh l oi oj">class HiddenMarkovChain_FP(HiddenMarkovChain):<br/>    def _alphas(self, observations: list) -&gt; np.ndarray:<br/>        alphas = np.zeros((len(observations), len(self.states)))<br/>        alphas[0, :] = self.pi.values * self.E[observations[0]].T<br/>        for t in range(1, len(observations)):<br/>            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1) <br/>                         @ self.T.values) * self.E[observations[t]].T<br/>        return alphas<br/>    <br/>    def score(self, observations: list) -&gt; float:<br/>        alphas = self._alphas(observations)<br/>        return float(alphas[-1].sum())</span></pre><h2 id="bd62" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="2a31" class="mp lm it oc b gy og oh l oi oj">hmc_fp = HiddenMarkovChain_FP(A, B, pi)<br/><br/>observations = ['1S', '2M', '3L', '2M', '1S']<br/>print("Score for {} is {:f}.".format(observations, hmc_fp.score(observations)))<br/>&gt;&gt;&gt; All possible scores added: 1.0.</span></pre><p id="408b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">…是的。</p><h1 id="d5ea" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">模拟和收敛</h1><p id="a981" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们再测试一个东西。基本上，我们就拿我们的<em class="mo">λ=(</em><strong class="ki iu"><em class="mo">A</em></strong><em class="mo">，</em> <strong class="ki iu"> <em class="mo"> B </em> </strong> <em class="mo">，【π】</em>来说，用它来生成一个随机可观测量的序列，从某个初始状态概率<em class="mo"> π </em>开始。</p><p id="aa18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果期望的长度<em class="mo"> T </em>足够大，我们将期望系统收敛到一个序列上，该序列平均起来给出与我们直接从<strong class="ki iu"> A </strong>和<strong class="ki iu"> B </strong>矩阵中期望的相同数量的事件。换句话说，对于每一步，跃迁矩阵和发射矩阵以一定的概率分别“决定”下一个状态是什么，以及我们将得到什么样的观察结果。因此，最初看起来像随机事件的东西，平均起来应该反映矩阵本身的系数。让我们也检查一下。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="b685" class="mp lm it oc b gy og oh l oi oj">class HiddenMarkovChain_Simulation(HiddenMarkovChain):<br/>    def run(self, length: int) -&gt; (list, list):<br/>        assert length &gt;= 0, "The chain needs to be a non-negative number."<br/>        s_history = [0] * (length + 1)<br/>        o_history = [0] * (length + 1)<br/>        <br/>        prb = self.pi.values<br/>        obs = prb @ self.E.values<br/>        s_history[0] = np.random.choice(self.states, p=prb.flatten())<br/>        o_history[0] = np.random.choice(self.observables, p=obs.flatten())<br/>        <br/>        for t in range(1, length + 1):<br/>            prb = prb @ self.T.values<br/>            obs = prb @ self.E.values<br/>            s_history[t] = np.random.choice(self.states, p=prb.flatten())<br/>            o_history[t] = np.random.choice(self.observables, p=obs.flatten())<br/>        <br/>        return o_history, s_history</span></pre><h2 id="e6b0" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="ebe8" class="mp lm it oc b gy og oh l oi oj">hmc_s = HiddenMarkovChain_Simulation(A, B, pi)<br/>observation_hist, states_hist = hmc_s.run(100)  # length = 100<br/>stats = pd.DataFrame({<br/>    'observations': observation_hist,<br/>    'states': states_hist}).applymap(lambda x: int(x[0])).plot()</span></pre><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/138df35dc3c98f17ff20a1e97ddc4379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lUKNRqnxALUv-5LT.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图一。马尔可夫过程的一个例子。显示了状态和可观察序列。</p></figure><h1 id="f2e5" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">潜在状态</h1><p id="d045" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">状态矩阵<strong class="ki iu"> A </strong>由以下系数给出:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/fe43aac3ca72ed6d871209939d7e38bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*Km-BqZiWjKNAZYKa79I3mQ.png"/></div></figure><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/148ea68fcd3d54475ba1a6908dde2a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*H80JBfAN1E0rF1pzNVei9g.png"/></div></figure><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/83ad4f121a5cb37cec844003824cd983.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*pAq0blp3_g2THZ3cspEGUw.png"/></div></figure><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1ac34c99cf905a356de38f1c08dcf51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*V__J0lbI7d_Xvqpk6ZOEKA.png"/></div></figure><p id="ff0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，在<em class="mo"> t </em> +1 时处于状态“1H”的概率等于:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d65cbe33cffa621e67680e78a8fd2ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*x5-UHueLGK-_NjaITlPMjw.png"/></div></figure><p id="1a04" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们假设<em class="mo">在</em>之前处于某个状态 at 的概率是完全随机的，那么<em class="mo"> p(1H) = 1 </em>和 p(2C) = 0.9，在重正化之后分别给出 0.55 和 0.45。</p><p id="be14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们计算每个状态出现的次数，并除以序列中元素的数量，随着序列长度的增长，我们会越来越接近这些数字。</p><h2 id="3709" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="4376" class="mp lm it oc b gy og oh l oi oj">hmc_s = HiddenMarkovChain_Simulation(A, B, pi)<br/><br/>stats = {}<br/>for length in np.logspace(1, 5, 40).astype(int):<br/>    observation_hist, states_hist = hmc_s.run(length)<br/>    stats[length] = pd.DataFrame({<br/>        'observations': observation_hist,<br/>        'states': states_hist}).applymap(lambda x: int(x[0]))<br/><br/>S = np.array(list(map(lambda x: <br/>        x['states'].value_counts().to_numpy() / len(x), stats.values())))<br/><br/>plt.semilogx(np.logspace(1, 5, 40).astype(int), S)<br/>plt.xlabel('Chain length T')<br/>plt.ylabel('Probability')<br/>plt.title('Converging probabilities.')<br/>plt.legend(['1H', '2C'])<br/>plt.show()</span></pre><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pd"><img src="../Images/cab6d5e70966e3ddee0fe4888c3f51d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KhO5MgDJ7SWClJ_f.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图二。概率相对于链长的收敛性。</p></figure><p id="ccc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们的<code class="fe ok ol om oc b">HiddenMarkovChain</code>类更上一层楼，补充更多的方法。这些方法将帮助我们发现观察序列背后最可能的隐藏变量序列。</p><h1 id="71ed" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">扩展类</h1><p id="97d7" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们已经把α定义为到目前为止部分观察到序列的概率。</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/cd01fab147b4eb94ea90ea15d94b7648.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*iK3qr3oB96cpDrkAZEjZJw.png"/></div></figure><p id="210d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们来定义“相反”的概率。即观察到从<em class="mo"> T - 1 </em>到<em class="mo"> t </em>序列的概率。</p><p id="7050" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="mo"> t= 0，1，…，T-1 </em>和<em class="mo"> i=0，1，…，N-1 </em>，我们定义:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/8ddf0f1d0fa653b394aa13d3410962ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*ZZjiWEflYiIy-yGtfJUlwQ.png"/></div></figure><p id="8e12" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">c ` 1 如前，我们可以<em class="mo"> β(i) </em>递归计算:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ea3f41648f08693bcfc1e502313a8113.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*-NirISd6itC4Wy_NB0LmDg.png"/></div></figure><p id="b0dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后对于<em class="mo"> t ≠ T-1 </em>:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/9175b2ac4a7c053ca869bd9573b0dbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*T_s2sGP8q_quWP-zJXeHjg.png"/></div></figure><p id="34a1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其矢量化形式为:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/51d123f8e508a30c9ad29e64c58244ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*k04Zy1_oAKq-6byfNXsLXQ.png"/></div></figure><p id="cafb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们还定义了一个新的量<em class="mo"> γ </em>来表示在时间<em class="mo"> t </em>的状态<em class="mo"> q_i </em>，对于该状态，概率(向前和向后计算)是最大的:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/926dfe283a447c20ea518a4455d48cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*4GqFamyukrXgC5NHNmYg6g.png"/></div></figure><p id="7c73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对于任何步骤<em class="mo"> t = 0，1，…，T-1 </em>，最大似然的状态可以使用下式找到:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/a67c9377baa6f5c7d80a9e1f1d71775e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*IocLUrCOE0HiqgeBk62hHg.png"/></div></figure><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="c66d" class="mp lm it oc b gy og oh l oi oj">class HiddenMarkovChain_Uncover(HiddenMarkovChain_Simulation):<br/>    def _alphas(self, observations: list) -&gt; np.ndarray:<br/>        alphas = np.zeros((len(observations), len(self.states)))<br/>        alphas[0, :] = self.pi.values * self.E[observations[0]].T<br/>        for t in range(1, len(observations)):<br/>            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1) @ self.T.values) \<br/>                         * self.E[observations[t]].T<br/>        return alphas<br/>    <br/>    def _betas(self, observations: list) -&gt; np.ndarray:<br/>        betas = np.zeros((len(observations), len(self.states)))<br/>        betas[-1, :] = 1<br/>        for t in range(len(observations) - 2, -1, -1):<br/>            betas[t, :] = (self.T.values @ (self.E[observations[t + 1]] \<br/>                        * betas[t + 1, :].reshape(-1, 1))).reshape(1, -1)<br/>        return betas<br/>    <br/>    def uncover(self, observations: list) -&gt; list:<br/>        alphas = self._alphas(observations)<br/>        betas = self._betas(observations)<br/>        maxargs = (alphas * betas).argmax(axis=1)<br/>        return list(map(lambda x: self.states[x], maxargs))</span></pre><h1 id="08d6" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">确认</h1><p id="f76d" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">为了验证，让我们生成一些可观察的序列<em class="mo">或</em>。为此，我们可以使用我们模型的<code class="fe ok ol om oc b">.run</code>方法。然后，我们将使用<code class="fe ok ol om oc b">.uncover</code>方法找到最有可能的潜在变量序列。</p><h2 id="1e47" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="8887" class="mp lm it oc b gy og oh l oi oj">np.random.seed(42)<br/><br/>a1 = ProbabilityVector({'1H': 0.7, '2C': 0.3})<br/>a2 = ProbabilityVector({'1H': 0.4, '2C': 0.6})<br/>b1 = ProbabilityVector({'1S': 0.1, '2M': 0.4, '3L': 0.5}) <br/>b2 = ProbabilityVector({'1S': 0.7, '2M': 0.2, '3L': 0.1})<br/>A  = ProbabilityMatrix({'1H': a1, '2C': a2})<br/>B  = ProbabilityMatrix({'1H': b1, '2C': b2})<br/>pi = ProbabilityVector({'1H': 0.6, '2C': 0.4})<br/><br/>hmc = HiddenMarkovChain_Uncover(A, B, pi)<br/><br/>observed_sequence, latent_sequence = hmc.run(5)<br/>uncovered_sequence = hmc.uncover(observed_sequence)</span><span id="a926" class="mp lm it oc b gy pl oh l oi oj">|                    | 0   | 1   | 2   | 3   | 4   | 5   |<br/>|:------------------:|:----|:----|:----|:----|:----|:----|<br/>| observed sequence  | 3L  | 3M  | 1S  | 3L  | 3L  | 3L  |<br/>| latent sequence    | 1H  | 2C  | 1H  | 1H  | 2C  | 1H  |<br/>| uncovered sequence | 1H  | 1H  | 2C  | 1H  | 1H  | 1H  |</span></pre><p id="a0f2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，最有可能的潜在状态链(根据算法)与实际引起观察的潜在状态链并不相同。这是可以预料的。毕竟每个观察序列只能以一定的概率显现，依赖于潜序列。</p><p id="4037" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码评估了产生我们的观察序列的不同潜在序列的可能性。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="11db" class="mp lm it oc b gy og oh l oi oj">all_possible_states = {'1H', '2C'}<br/>chain_length = 6  # any int &gt; 0<br/>all_states_chains = list(product(*(all_possible_states,) * chain_length))<br/><br/>df = pd.DataFrame(all_states_chains)<br/>dfp = pd.DataFrame()<br/><br/>for i in range(chain_length):<br/>    dfp['p' + str(i)] = df.apply(lambda x: <br/>        hmc.E.df.loc[x[i], observed_sequence[i]], axis=1)<br/><br/>scores = dfp.sum(axis=1).sort_values(ascending=False)<br/>df = df.iloc[scores.index]<br/>df['score'] = scores<br/>df.head(10).reset_index()</span><span id="873d" class="mp lm it oc b gy pl oh l oi oj">|    index | 0   | 1   | 2   | 3   | 4   | 5   |   score |<br/>|:--------:|:----|:----|:----|:----|:----|:----|--------:|<br/>|        8 | 1H  | 1H  | 2C  | 1H  | 1H  | 1H  |     3.1 |<br/>|       24 | 1H  | 2C  | 2C  | 1H  | 1H  | 1H  |     2.9 |<br/>|       40 | 2C  | 1H  | 2C  | 1H  | 1H  | 1H  |     2.7 |<br/>|       12 | 1H  | 1H  | 2C  | 2C  | 1H  | 1H  |     2.7 |<br/>|       10 | 1H  | 1H  | 2C  | 1H  | 2C  | 1H  |     2.7 |<br/>|        9 | 1H  | 1H  | 2C  | 1H  | 1H  | 2C  |     2.7 |<br/>|       25 | 1H  | 2C  | 2C  | 1H  | 1H  | 2C  |     2.5 |<br/>|        0 | 1H  | 1H  | 1H  | 1H  | 1H  | 1H  |     2.5 |<br/>|       26 | 1H  | 2C  | 2C  | 1H  | 2C  | 1H  |     2.5 |<br/>|       28 | 1H  | 2C  | 2C  | 2C  | 1H  | 1H  |     2.5 |</span></pre><p id="e840" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的结果显示了潜在序列的排序表，给出了观察序列。实际的潜在序列(引起观察的序列)位于第 35 位(我们从零开始计算索引)。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="abbe" class="mp lm it oc b gy og oh l oi oj">dfc = df.copy().reset_index()<br/>for i in range(chain_length):<br/>    dfc = dfc[dfc[i] == latent_sequence[i]]<br/>    <br/>dfc<br/>|   index | 0   | 1   | 2   | 3   | 4   | 5   |   score |<br/>|:-------:|:----|:----|:----|:----|:----|:----|--------:|<br/>|      18 | 1H  | 2C  | 1H  | 1H  | 2C  | 1H  |     1.9 |</span></pre><h1 id="48db" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">训练模型</h1><p id="3507" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">是时候展示训练程序了。形式上，我们感兴趣的是找到<em class="mo">λ=(</em><strong class="ki iu"><em class="mo">A</em></strong><em class="mo">，</em> <strong class="ki iu"> <em class="mo"> B </em> </strong> <em class="mo">，π) </em>，使得给定期望的观察序列<em class="mo"> O </em>，我们的模型λ将给出最佳拟合。</p><h1 id="6bde" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">扩展类</h1><p id="ddc3" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">这里，我们的起点是我们之前定义的<code class="fe ok ol om oc b">HiddenMarkovModel_Uncover</code>。我们将增加新的方法来训练它。</p><p id="cb8b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">知道了我们的潜在状态<em class="mo"> Q </em>和可能的观察状态<em class="mo"> O </em>，我们就自动知道了矩阵<strong class="ki iu"> A </strong>和<strong class="ki iu"> B </strong>的大小，由此得到<em class="mo"> N </em>和<em class="mo"> M </em>。但是，我们需要确定<em class="mo"> a </em>和<em class="mo"> b </em>和<em class="mo"> π </em>。</p><p id="3908" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="mo"> t = 0，1，…，T-2 </em>和<em class="mo"> i，j =0，1，…，N -1 </em>，我们定义“双γ”:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pm"><img src="../Images/78d0726ea64ce2a4df7be141d5773b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*TteOdsgD8yOAosXJNepdZg.png"/></div></div></figure><p id="008b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mo"> γ(i，j) </em>是<em class="mo"> q </em>在<em class="mo"> t </em>到<em class="mo"> t </em> + 1 的转移概率。用<em class="mo"> α、</em>β、<strong class="ki iu"> A、</strong>、<strong class="ki iu"> B、</strong>来写，我们有:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/3de0cc176daeb0a4302d177d01971699.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*QaSTMHFVLfMZXiwMtuMhgg.png"/></div></figure><p id="39e9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，从实现的角度考虑，我们希望避免同时循环遍历<em class="mo"> i、j </em>和<em class="mo"> t </em>，因为这会非常慢。幸运的是，我们可以对等式进行矢量化:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi po"><img src="../Images/aa53f1bedd2e9874df7021f3810916e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*CAsQqrG0qraQYKQXanF06Q.png"/></div></figure><p id="fd93" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了<em class="mo"> γ(i，j) </em>的等式，我们可以计算</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/1c8d403744c3d5b917c0444a39779065.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*xsfpOsxdOoTLgU7ji_MVTQ.png"/></div></figure><p id="f5df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要找到<em class="mo">λ=(</em><strong class="ki iu"><em class="mo">)A</em></strong><em class="mo">，</em> <strong class="ki iu"> <em class="mo"> B </em> </strong> <em class="mo">，【π】</em>，我们做</p><p id="491c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="mo"> i = 0，1，…，N-1 </em>:</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/2d5e1ed2f171cee285cf8f5c7d4a6917.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*K0j0--FQysUHgOIfG_XSqw.png"/></div></figure><p id="b150" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者</p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/76dc814ec54a3d2db0962c691ced6c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*Q_1_NefGd77m8rkGiIq3gA.png"/></div></figure><p id="6d64" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="mo"> i，j = 0，1，…，N-1: </em></p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/99631652fcfb722c4797a82ef997eb62.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*zkEQbIs1ItiFw5ZUO9kKcQ.png"/></div></figure><p id="a65e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="mo"> j = 0，1，…，N-1 </em>和<em class="mo"> k = 0，1，…，M-1: </em></p><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/1be2555023b1b8517ff6a6678ff95635.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*AHpjdfCk7dSXn9-NNUEyJA.png"/></div></figure><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="7440" class="mp lm it oc b gy og oh l oi oj">class HiddenMarkovLayer(HiddenMarkovChain_Uncover):<br/>    def _digammas(self, observations: list) -&gt; np.ndarray:<br/>        L, N = len(observations), len(self.states)<br/>        digammas = np.zeros((L - 1, N, N))<br/><br/>        alphas = self._alphas(observations)<br/>        betas = self._betas(observations)<br/>        score = self.score(observations)<br/>        for t in range(L - 1):<br/>            P1 = (alphas[t, :].reshape(-1, 1) * self.T.values)<br/>            P2 = self.E[observations[t + 1]].T * betas[t + 1].reshape(1, -1)<br/>            digammas[t, :, :] = P1 * P2 / score<br/>        return digammas</span></pre><p id="d468" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了补充了<code class="fe ok ol om oc b">._difammas</code>方法的“层”,我们应该能够执行所有必要的计算。然而，将该层的“管理”委托给另一个类是有意义的。事实上，模型训练可以概括如下:</p><ol class=""><li id="7a57" class="nb nc it ki b kj kk kn ko kr nd kv ne kz nf ld on nh ni nj bi translated">初始化<strong class="ki iu"> A </strong>、<strong class="ki iu"> B </strong>和π。</li><li id="fa53" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">计算<em class="mo"> γ(i，j) </em>。</li><li id="2266" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">更新模型的<strong class="ki iu"> A </strong>、<strong class="ki iu"> B </strong>和π。</li><li id="8507" class="nb nc it ki b kj nk kn nl kr nm kv nn kz no ld on nh ni nj bi translated">我们重复 2。第三。直到分数<em class="mo"> p </em> (O|λ)不再增加。</li></ol><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="8f84" class="mp lm it oc b gy og oh l oi oj">class HiddenMarkovModel:<br/>    def __init__(self, hml: HiddenMarkovLayer):<br/>        self.layer = hml<br/>        self._score_init = 0<br/>        self.score_history = []<br/><br/>    @classmethod<br/>    def initialize(cls, states: list, observables: list):<br/>        layer = HiddenMarkovLayer.initialize(states, observables)<br/>        return cls(layer)<br/><br/>    def update(self, observations: list) -&gt; float:<br/>        alpha = self.layer._alphas(observations)<br/>        beta = self.layer._betas(observations)<br/>        digamma = self.layer._digammas(observations)<br/>        score = alpha[-1].sum()<br/>        gamma = alpha * beta / score <br/><br/>        L = len(alpha)<br/>        obs_idx = [self.layer.observables.index(x) \<br/>                  for x in observations]<br/>        capture = np.zeros((L, len(self.layer.states), len(self.layer.observables)))<br/>        for t in range(L):<br/>            capture[t, :, obs_idx[t]] = 1.0<br/><br/>        pi = gamma[0]<br/>        T = digamma.sum(axis=0) / gamma[:-1].sum(axis=0).reshape(-1, 1)<br/>        E = (capture * gamma[:, :, np.newaxis]).sum(axis=0) / gamma.sum(axis=0).reshape(-1, 1)<br/><br/>        self.layer.pi = ProbabilityVector.from_numpy(pi, self.layer.states)<br/>        self.layer.T = ProbabilityMatrix.from_numpy(T, self.layer.states, self.layer.states)<br/>        self.layer.E = ProbabilityMatrix.from_numpy(E, self.layer.states, self.layer.observables)<br/>            <br/>        return score<br/><br/>    def train(self, observations: list, epochs: int, tol=None):<br/>        self._score_init = 0<br/>        self.score_history = (epochs + 1) * [0]<br/>        early_stopping = isinstance(tol, (int, float))<br/><br/>        for epoch in range(1, epochs + 1):<br/>            score = self.update(observations)<br/>            print("Training... epoch = {} out of {}, score = {}.".format(epoch, epochs, score))<br/>            if early_stopping and abs(self._score_init - score) / score &lt; tol:<br/>                print("Early stopping.")<br/>                break<br/>            self._score_init = score<br/>            self.score_history[epoch] = score</span></pre><h2 id="7b82" class="mp lm it bd ln mq mr dn lr ms mt dp lv kr mu mv lz kv mw mx md kz my mz mh na bi translated">例子</h2><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="d50c" class="mp lm it oc b gy og oh l oi oj">np.random.seed(42)<br/><br/>observations = ['3L', '2M', '1S', '3L', '3L', '3L']<br/><br/>states = ['1H', '2C']<br/>observables = ['1S', '2M', '3L']<br/><br/>hml = HiddenMarkovLayer.initialize(states, observables)<br/>hmm = HiddenMarkovModel(hml)<br/><br/>hmm.train(observations, 25)</span></pre><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/47dab694ef4bde47e9a92e6e4238c0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/0*UXkAcwTqcRzy3bo_.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图 3。训练过程中的得分函数示例。</p></figure><h1 id="bbf6" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">确认</h1><p id="b4a2" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">让我们看看生成的序列。“需求”顺序是:</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="f200" class="mp lm it oc b gy og oh l oi oj">|    | 0   | 1   | 2   | 3   | 4   | 5   |<br/>|---:|:----|:----|:----|:----|:----|:----|<br/>|  0 | 3L  | 2M  | 1S  | 3L  | 3L  | 3L  |</span><span id="cd5a" class="mp lm it oc b gy pl oh l oi oj">RUNS = 100000<br/>T = 5<br/><br/>chains = RUNS * [0]<br/>for i in range(len(chains)):<br/>    chain = hmm.layer.run(T)[0]<br/>    chains[i] = '-'.join(chain)</span></pre><p id="dbaf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下表总结了基于 100000 次尝试(见上文)的模拟运行，以及出现的频率和匹配观察的数量。</p><p id="3c78" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">底线是，如果我们真的训练了模型，我们应该会看到它产生类似我们需要的序列的强烈趋势。让我们看看它是否会发生。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="94b0" class="mp lm it oc b gy og oh l oi oj">df = pd.DataFrame(pd.Series(chains).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})<br/>df = pd.merge(df, df['chain'].str.split('-', expand=True), left_index=True, right_index=True)<br/><br/>s = []<br/>for i in range(T + 1):<br/>    s.append(df.apply(lambda x: x[i] == observations[i], axis=1))<br/><br/>df['matched'] = pd.concat(s, axis=1).sum(axis=1)<br/>df['counts'] = df['counts'] / RUNS * 100<br/>df = df.drop(columns=['chain'])<br/>df.head(30)<br/>---<br/>|---:|---------:|:----|:----|:----|:----|:----|:----|----------:|<br/>|  0 |    8.907 | 3L  | 3L  | 3L  | 3L  | 3L  | 3L  |         4 |<br/>|  1 |    4.422 | 3L  | 2M  | 3L  | 3L  | 3L  | 3L  |         5 |<br/>|  2 |    4.286 | 1S  | 3L  | 3L  | 3L  | 3L  | 3L  |         3 |<br/>|  3 |    4.284 | 3L  | 3L  | 3L  | 3L  | 3L  | 2M  |         3 |<br/>|  4 |    4.278 | 3L  | 3L  | 3L  | 2M  | 3L  | 3L  |         3 |<br/>|  5 |    4.227 | 3L  | 3L  | 1S  | 3L  | 3L  | 3L  |         5 |<br/>|  6 |    4.179 | 3L  | 3L  | 3L  | 3L  | 1S  | 3L  |         3 |<br/>|  7 |    2.179 | 3L  | 2M  | 3L  | 2M  | 3L  | 3L  |         4 |<br/>|  8 |    2.173 | 3L  | 2M  | 3L  | 3L  | 1S  | 3L  |         4 |<br/>|  9 |    2.165 | 1S  | 3L  | 1S  | 3L  | 3L  | 3L  |         4 |<br/>| 10 |    2.147 | 3L  | 2M  | 3L  | 3L  | 3L  | 2M  |         4 |<br/>| 11 |    2.136 | 3L  | 3L  | 3L  | 2M  | 3L  | 2M  |         2 |<br/>| 12 |    2.121 | 3L  | 2M  | 1S  | 3L  | 3L  | 3L  |         6 |<br/>| 13 |    2.111 | 1S  | 3L  | 3L  | 2M  | 3L  | 3L  |         2 |<br/>| 14 |    2.1   | 1S  | 2M  | 3L  | 3L  | 3L  | 3L  |         4 |<br/>| 15 |    2.075 | 3L  | 3L  | 3L  | 2M  | 1S  | 3L  |         2 |<br/>| 16 |    2.05  | 1S  | 3L  | 3L  | 3L  | 3L  | 2M  |         2 |<br/>| 17 |    2.04  | 3L  | 3L  | 1S  | 3L  | 3L  | 2M  |         4 |<br/>| 18 |    2.038 | 3L  | 3L  | 1S  | 2M  | 3L  | 3L  |         4 |<br/>| 19 |    2.022 | 3L  | 3L  | 1S  | 3L  | 1S  | 3L  |         4 |<br/>| 20 |    2.008 | 1S  | 3L  | 3L  | 3L  | 1S  | 3L  |         2 |<br/>| 21 |    1.955 | 3L  | 3L  | 3L  | 3L  | 1S  | 2M  |         2 |<br/>| 22 |    1.079 | 1S  | 2M  | 3L  | 2M  | 3L  | 3L  |         3 |<br/>| 23 |    1.077 | 1S  | 2M  | 3L  | 3L  | 3L  | 2M  |         3 |<br/>| 24 |    1.075 | 3L  | 2M  | 1S  | 2M  | 3L  | 3L  |         5 |<br/>| 25 |    1.064 | 1S  | 2M  | 1S  | 3L  | 3L  | 3L  |         5 |<br/>| 26 |    1.052 | 1S  | 2M  | 3L  | 3L  | 1S  | 3L  |         3 |<br/>| 27 |    1.048 | 3L  | 2M  | 3L  | 2M  | 1S  | 3L  |         3 |<br/>| 28 |    1.032 | 1S  | 3L  | 1S  | 2M  | 3L  | 3L  |         3 |<br/>| 29 |    1.024 | 1S  | 3L  | 1S  | 3L  | 1S  | 3L  |         3 |</span></pre><p id="1ec2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们不想让模型创建的序列。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="a450" class="mp lm it oc b gy og oh l oi oj">|     |   counts | 0   | 1   | 2   | 3   | 4   | 5   |   matched |<br/>|----:|---------:|:----|:----|:----|:----|:----|:----|----------:|<br/>| 266 |    0.001 | 1S  | 1S  | 3L  | 3L  | 2M  | 2M  |         1 |<br/>| 267 |    0.001 | 1S  | 2M  | 2M  | 3L  | 2M  | 2M  |         2 |<br/>| 268 |    0.001 | 3L  | 1S  | 1S  | 3L  | 1S  | 1S  |         3 |<br/>| 269 |    0.001 | 3L  | 3L  | 3L  | 1S  | 2M  | 2M  |         1 |<br/>| 270 |    0.001 | 3L  | 1S  | 3L  | 1S  | 1S  | 3L  |         2 |<br/>| 271 |    0.001 | 1S  | 3L  | 2M  | 1S  | 1S  | 3L  |         1 |<br/>| 272 |    0.001 | 3L  | 2M  | 2M  | 3L  | 3L  | 1S  |         4 |<br/>| 273 |    0.001 | 1S  | 3L  | 3L  | 1S  | 1S  | 1S  |         0 |<br/>| 274 |    0.001 | 3L  | 1S  | 2M  | 2M  | 1S  | 2M  |         1 |<br/>| 275 |    0.001 | 3L  | 3L  | 2M  | 1S  | 3L  | 2M  |         2 |</span></pre><p id="a457" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，我们的模型倾向于生成与我们需要的序列相似的序列，尽管精确的序列(与 6/6 匹配的序列)已经位于第 10 位！另一方面，根据该表，前 10 个序列仍然与我们请求的序列有些相似。</p><p id="fd15" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了最终验证我们的模型的质量，让我们将结果与<em class="mo">出现频率</em>一起绘制出来，并将其与新初始化的模型进行比较，该模型应该给我们完全随机的序列——只是为了进行比较。</p><pre class="nq nr ns nt gt ob oc od oe aw of bi"><span id="1fe5" class="mp lm it oc b gy og oh l oi oj">hml_rand = HiddenMarkovLayer.initialize(states, observables)<br/>hmm_rand = HiddenMarkovModel(hml_rand)<br/><br/>RUNS = 100000<br/>T = 5<br/><br/>chains_rand = RUNS * [0]<br/>for i in range(len(chains_rand)):<br/>    chain_rand = hmm_rand.layer.run(T)[0]<br/>    chains_rand[i] = '-'.join(chain_rand)<br/><br/>df2 = pd.DataFrame(pd.Series(chains_rand).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})<br/>df2 = pd.merge(df2, df2['chain'].str.split('-', expand=True), left_index=True, right_index=True)<br/><br/>s = []<br/>for i in range(T + 1):<br/>    s.append(df2.apply(lambda x: x[i] == observations[i], axis=1))<br/><br/>df2['matched'] = pd.concat(s, axis=1).sum(axis=1)<br/>df2['counts'] = df2['counts'] / RUNS * 100<br/>df2 = df2.drop(columns=['chain'])<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(14, 6))<br/><br/>ax.plot(df['matched'], 'g:')<br/>ax.plot(df2['matched'], 'k:')<br/><br/>ax.set_xlabel('Ordered index')<br/>ax.set_ylabel('Matching observations')<br/>ax.set_title('Verification on a 6-observation chain.')<br/><br/>ax2 = ax.twinx()<br/>ax2.plot(df['counts'], 'r', lw=3)<br/>ax2.plot(df2['counts'], 'k', lw=3)<br/>ax2.set_ylabel('Frequency of occurrence [%]')<br/><br/>ax.legend(['trained', 'initialized'])<br/>ax2.legend(['trained', 'initialized'])<br/><br/>plt.grid()<br/>plt.show()</span></pre><figure class="nq nr ns nt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pv"><img src="../Images/ec463764e6b619bef1767780e49ec9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xAdlWm6TemJ-jrcl.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图 4。模型训练后的结果。虚线代表匹配的序列。线条代表特定序列的出现频率:已训练的模型(红色)和刚初始化的模型(黑色)。初始化导致序列的几乎完美的均匀分布，而训练的模型对可观察的序列给出强烈的偏好。</p></figure><p id="3129" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看来我们已经成功实施了训练程序。如果我们观察这些曲线，那么<em class="mo">初始化的仅</em>模型以几乎相等的概率生成观察序列。完全是随机的。然而，经过训练的模型给出的序列与我们期望的序列高度相似，而且频率要高得多。尽管真正的序列仅在总运行的 2%中被创建，但是其他相似的序列被生成的频率大致相同。</p><h1 id="b0f3" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">结论</h1><p id="cf07" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在本文中，我们提出了一个逐步实现的隐马尔可夫模型。我们通过采用第一原则方法创建了代码。更具体地说，我们已经展示了通过方程表达的概率概念是如何作为对象和方法实现的。最后，我们通过发现分数、揭示潜在变量链和应用训练程序来演示模型的使用。</p><p id="a6d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PS。我为这里糟糕的方程式表达道歉。基本上，我需要全部手动完成。然而，请随意阅读我的博客上的这篇文章。在那里，我处理了它；)</p><h1 id="48e0" class="ll lm it bd ln lo nw lq lr ls nx lu lv lw ny ly lz ma nz mc md me oa mg mh mi bi translated">还会有更多…</h1><p id="f1b4" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我计划把文章带到下一个层次，并提供简短的视频教程。</p><p id="b613" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您想了解关于视频和未来文章的更新，<strong class="ki iu">订阅我的</strong> <a class="ae kf" href="https://landing.mailerlite.com/webforms/landing/j5y2q1" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">简讯</strong> </a> <strong class="ki iu">。你也可以通过填写<a class="ae kf" href="https://forms.gle/bNpf9aqZJGLgaU589" rel="noopener ugc nofollow" target="_blank">表格</a>让我知道你的期望。回头见！</strong></p></div></div>    
</body>
</html>