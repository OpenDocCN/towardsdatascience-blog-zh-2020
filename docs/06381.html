<html>
<head>
<title>Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow通用语句编码器和Spark EMR嵌入数十亿个文本文档</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-and-spark-emr-422407eecf60?source=collection_archive---------56-----------------------#2020-05-21">https://towardsdatascience.com/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-and-spark-emr-422407eecf60?source=collection_archive---------56-----------------------#2020-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1aad" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/dd115f366e5ffdc044bf416d1157fb7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dsTXtVnMTjyvviZhoIDjQ.jpeg"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">照片由Enoc Valenzuela拍摄(图片来源:<a class="ae kl" href="https://unsplash.com/photos/WJolaNbXt90" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="fdfe" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Tensorflow HUB提供了各种预训练模型，可随时用于推理。一个非常强大的模型是<a class="ae kl" href="https://tfhub.dev/google/universal-sentence-encoder-multilingual/3" rel="noopener ugc nofollow" target="_blank">(多语言)通用句子编码器</a>，它允许将以任何语言编写的文本嵌入到一个通用的数字向量表示中。</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/776d5f6374a7955896a7265113beda50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*cRHJogYZdVgk1GLe"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来自<a class="ae kl" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>的通用语句编码器概述</p></figure><p id="c04d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">嵌入文本是一种非常强大的自然语言处理(NLP)技术，用于从文本字段中提取特征。这些特征可用于训练其他模型或用于数据分析，例如基于单词语义的聚类文档或搜索引擎。</p><p id="a543" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">不幸的是，如果我们有数十亿的文本数据要编码，在一台机器上运行可能需要几天时间。在本教程中，我将展示如何利用Spark。特别是，我们将使用AWS管理的Elastic MapReduce (EMR)服务将句子编码器应用到大型数据集，并在几个小时内完成。</p><h1 id="b4f4" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">配置</h1><h1 id="b279" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">EMR集群</h1><p id="0514" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">在本例中，我们假设一个集群有一个主节点(r4.4xlarge)和50个核心节点(r4.2xlarge spot实例)。该集群总共有400个内核和大约3TB的理论内存。在实践中，每个执行器的最大内存会被YARN限制在52GB左右。</p><p id="1141" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果运行一个包含大量节点的集群是不经济的，那么集群的总内存大小不应该成为瓶颈，因为Spark惰性执行模式不需要将整个数据集同时加载到内存中。</p><p id="ed63" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了充分利用EMR集群资源，我们可以方便地使用属性“maximizeResourceAllocation”。此外，我们还需要配置livy，使我们的会话不超时(spark-submit作业不需要)。</p><p id="0ded" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以通过指定以下配置来实现这两个目标:</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="2b1c" class="mx lq iq mt b gy my mz l na nb">[{"classification":"spark","properties":{"maximizeResourceAllocation":"true"}},{"classification":"livy-conf","properties":{"livy.server.session.timeout-check":"false"}}]</span></pre><p id="c18c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我还建议选择最近发布的emr-5。x并且至少包括以下软件包:Hadoop 2.8.5、Ganglia 3.7.2、Spark 2.4.4、Livy 0.6.0。</p><p id="163b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为主节点和核心节点添加对外界开放的安全组(如果集群部署在VPC中，这将是访问Spark UI和Ganglia所必需的)。</p><h1 id="bbac" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">火花会议</h1><p id="f33b" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">创建一个EMR笔记本，并将其连接到之前创建的集群。在创建会话之前，我们需要调整一些内存配置。</p><p id="db7c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于大部分计算和内存将由python进程使用，我们需要改变JVM和python进程之间的内存平衡:</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9c4c7ea4f42f58dd7203180323e7302d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*kaRaIRVAscEm8yjx"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来自<a class="ae kl" href="https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals" rel="noopener ugc nofollow" target="_blank"> Apache Spark wiki </a>的PySpark内部消息</p></figure><p id="3ebe" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">指定的执行器内存将只考虑JVM，而不考虑外部进程所需的内存，在我们的例子中是TensorFlow。</p><p id="c844" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们需要调整spark . yarn . executor . memory overhead，使其大于spark.executor.memory的10%,并分配spark.python.worker.memory，以避免不必要的磁盘溢出。</p><p id="553c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在启动Livy会话之前，我们将首先配置这些纱线参数，在笔记本单元中运行:</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="94dc" class="mx lq iq mt b gy my mz l na nb">%%configure -f <br/>{ "conf":{ "spark.pyspark.python": "python3", "spark.pyspark.virtualenv.enabled": "true", "spark.pyspark.virtualenv.type":"native", "spark.pyspark.virtualenv.bin.path":"/usr/bin/virtualenv", "spark.executor.memory": "50g", "spark.yarn.executor.memoryOverhead": "12000", "spark.python.worker.memory": "10g" }}</span></pre><p id="d94f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在61GB的可用空间中，我们为python workers分配了10GB，为JVM分配了50GB，其中12GB是开销。</p><p id="7c77" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在亚马逊EMR 上成功管理Apache Spark应用程序内存的<a class="ae kl" href="https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/" rel="noopener ugc nofollow" target="_blank">最佳实践中有更多关于配置调优的细节。</a></p><p id="865d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在可以创建一个会话来执行包含spark上下文对象的单元:</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/8a3c8fc285318b2b4195dedffe1791cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*eqYCcXVZwEtUSiEr"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">Livy信息小部件</p></figure><h1 id="8b57" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">依赖性管理</h1><p id="da60" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">AWS做得很好，使得在运行时安装库更容易，而不必编写定制的引导动作或ami。我们可以使用<a class="ae kl" href="https://aws.amazon.com/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/" rel="noopener ugc nofollow" target="_blank"> install_pypi_package API </a>在主节点和核心节点安装软件包:</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="4443" class="mx lq iq mt b gy my mz l na nb">for package in ["pandas==0.25", "tensorflow==2.1.0", "tensorflow_text==2.1.1", "tensorflow-hub==0.7.0"]: <br/>    sc.install_pypi_package(package) </span><span id="dec0" class="mx lq iq mt b gy nd mz l na nb">sc.list_packages()</span></pre><p id="67f8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它将安装所提供的包，并打印出python 3.6虚拟环境中已安装包的列表。</p><p id="538e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意:在Hadoop 3.0 (EMR 6.x)中，应该可以在Docker容器中部署Spark集群，但是我还没有尝试过。</p><h1 id="f7c3" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">嵌入作业</h1><p id="c326" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">我们需要将数据加载为Spark DataFrame，其中包含一个键列和一个文本列。</p><p id="3948" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">嵌入作业在概念上将执行以下操作:</p><ol class=""><li id="1c2c" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj nj nk nl nm bi translated">下载TensorFlow多语言通用句子编码器模型</li><li id="4756" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">将数据分区分割成文本文档块</li><li id="fa50" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">将每个块嵌入一个NumPy矩阵中</li><li id="a9df" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">将矩阵转换成spark.sql.Row对象列表</li></ol><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="80fc" class="mx lq iq mt b gy my mz l na nb">muse_columns = [f"muse_{(format(x, '03'))}" for x in range(512)]<br/><br/>def get_embedding_batch(batch, model, id_col, text_col, muse_columns):<br/>    <br/>    rows = [row for row in batch if row[text_col] is not None and len(row[text_col].split(" ")) &gt;=3]<br/>    if len(rows) == 0:<br/>        return []<br/>    <br/>    from pyspark.sql import Row<br/>    <br/>    EmbeddingRow = Row(id_col, *muse_columns)<br/>    <br/>    keys = [x[id_col] for x in rows]<br/>    text = [x[text_col] for x in rows]<br/>    <br/>    embedding_mat = model(text).numpy()<br/>    return [EmbeddingRow(keys[i], *embedding_mat[i, :].reshape(-1).tolist()) for i in range(len(keys))]<br/><br/><br/>def chunks(iterable, n=10):<br/>    from itertools import chain, islice<br/>    iterator = iter(iterable)<br/>    for first in iterator:<br/>        yield chain([first], islice(iterator, n - 1))<br/><br/><br/>def get_embedding_batch_gen(batch, <br/>                            id_col, <br/>                            text_col, <br/>                            muse_columns=muse_columns,<br/>                            chunk_size=1000):<br/>    import tensorflow_hub as hub<br/>    import tensorflow_text<br/>    <br/>    model = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual/3")<br/>    chunk_iter = chunks(batch, n=chunk_size)<br/>    <br/>    for chunk in chunk_iter:<br/>        for row in get_embedding_batch(batch=chunk, model=model, id_col=id_col, <br/>                                       text_col=text_col, muse_columns=muse_columns):<br/>            yield row</span></pre><p id="64b2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">几个问题:</p><ul class=""><li id="5445" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj ns nk nl nm bi translated">模型仅被下载和实例化一次；或者，我们可以使用Spark本地广播变量。</li><li id="ebfd" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ns nk nl nm bi translated">为了让模型在运行时工作，我们首先必须在每个执行器中导入tensorflow_text</li><li id="cc58" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ns nk nl nm bi translated">我们通过每次仅具体化1000行中的一个块，将行对象的可迭代转换为行对象的可迭代。</li><li id="15f2" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ns nk nl nm bi translated">我们丢弃了少于3个标记的句子。</li><li id="1b90" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ns nk nl nm bi translated">numpy float32型与Spark DoubleType不兼容；因此，必须首先将其转换为浮点型。</li></ul><h1 id="e7f2" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">玩具示例</h1><p id="047a" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">让我们用一个小的数据样本来试试这段代码:</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="15e6" class="mx lq iq mt b gy my mz l na nb">english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]<br/>italian_sentences = ["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."]<br/>japanese_sentences = ["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"]<br/><br/>sentences = english_sentences + italian_sentences + japanese_sentences</span></pre><p id="d77e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们可以使用mapPartitions API批量运行推断，然后将结果转换成包含键列和512 muse嵌入列的Spark数据帧。</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="8e93" class="mx lq iq mt b gy my mz l na nb">from pyspark.sql.types import StructType<br/>from pyspark.sql.types import StructField<br/>from pyspark.sql.types import StringType, FloatType<br/>from pyspark.sql import Row<br/>from functools import partial<br/>sentences = [Row(id=i, text=sentence) for i, sentence in enumerate(sentences)]<br/><br/>sentence_embeddings_rdd = sc.parallelize(sentences).mapPartitions(partial(get_embedding_batch_gen, id_col='id', text_col='text'))<br/><br/>schema = StructType([StructField('id', StringType(), False)] + [StructField(col, FloatType(), False) <br/>                                                                for col in muse_columns])<br/>sentence_embeddings_df = sqlContext.createDataFrame(sentence_embeddings_rdd, schema)</span></pre><p id="366d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的例子中，我们手动指定模式以避免动态模式推理的减速。</p><h1 id="ec3a" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">分割</h1><p id="b946" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">玩具示例应该马上就能工作，因为数据样本非常小。如果我们必须为非常大的数据集进行缩放，我们既不想遇到内存错误，也不想将输出存储在成千上万个小部分中。在映射句子RDD之前，我们可能需要调整它们的分区大小(大约几万个分区),并在写入存储层之前将嵌入数据帧合并到一个合理的小范围(几百个分区),并减少输出文件的数量。</p><p id="5d65" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请注意，块大小对分区进行切片，以使张量在进行推理时不会太大，但它们不能保证执行器不会将整个分区保存在内存中。</p><p id="f9d1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，10亿个文本文档的数据集可以被分成5k个分区，每个分区200k个文档，这意味着每个分区将包含大约200个连续的组块。输出应该保存为400个部分的拼花文件。</p><pre class="ll lm ln lo gt ms mt mu mv aw mw bi"><span id="f3d3" class="mx lq iq mt b gy my mz l na nb">sentence_embeddings_rdd = large_text_corpus_rdd.repartition(5000).mapPartitions(embedding_generator_function) large_text_corpus_df = sqlContext.createDataFrame(large_text_corpus_rdd, schema) large_text_corpus_df.coalesce(400).write.option("compression", "snappy").parquet(output_path)</span></pre><p id="fd94" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就这样，您可以监视Spark作业，并最终访问以parquet格式划分为400个大小几乎相同的部分并用snappy压缩的嵌入。</p><h1 id="e0eb" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">监视</h1><p id="b52f" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">监控Spark作业的主要工具是它的UI和Ganglia。</p><h1 id="460e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">Spark UI</h1><p id="cffd" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">如果我们在Jupyter笔记本中执行%info来获得当前和过去的Livy会话列表。从这个小部件中，您还可以获得到Spark UI和主主机名的链接(从这里您可以通过<a class="ae kl" href="http://master_hostname/ganglia/)." rel="noopener ugc nofollow" target="_blank">http://master _ hostname/Ganglia/)访问Ganglia)。</a>如果集群部署在私有网络中，我们将需要通过代理访问这些服务器。</p><p id="47b1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从Spark UI中，我们可以看到如下的计算图:</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/bbaa17c6ab3d948c30383a6bedf305ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/0*ccsqVfNjHgCNKH7R"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">Spark作业的DAG可视化</p></figure><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/7c4ac982da5df561f4793175f43016eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*GQSgcuOzJxYK-BYY"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">Spark工作的各个阶段</p></figure><p id="a602" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以观察到两个级别的重新分区:阶段32在模型推断之前对数据进行重新分区，阶段33在写操作之前进行重新分区。</p><h1 id="74a6" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">神经中枢</h1><p id="0169" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">如果您打开Ganglia UI并正确完成了所有操作，您应该会看到如下内容:</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/a5a5726e109d8ace5bcd9ad0d4149191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*iyf_dzGgxf7KB-NQ"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">集群的Ganglia概述</p></figure><p id="648c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果您遇到内存使用和CPU使用之间的严重不平衡，您可能希望将实例类型更改为计算优化程度更高的系列，而不是r4。</p><p id="76c9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于每个80k文本句子，每个任务的执行时间大约为20分钟，并且考虑到8个任务将在同一个执行器中并发执行。</p><h1 id="a018" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结论</h1><p id="653c" class="pw-post-body-paragraph km kn iq ko b kp mn kr ks kt mo kv kw kx mp kz la lb mq ld le lf mr lh li lj ij bi translated">这种方法可以适用于用任何规模的任何机器学习库运行模型推理。与spot实例一起使用EMR将使它变得快速和便宜。为了方便起见，我们使用了EMR笔记本，但是您可以将相同的逻辑封装到spark-submit作业中，并使用引导操作来安装包。</p><p id="2714" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了以数据框的形式存储嵌入之外，您还可以扩展用于存储每个分区的原始张量的代码，并将它们加载到TensorBoard中，以实现高效的三维可视化。</p><p id="a396" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">相反，如果您正在寻找在Spark上以分布式模式运行TensorFlow的方法，您必须使用不同的架构，如文章<a class="ae kl" rel="noopener" target="_blank" href="/scaling-up-with-distributed-tensorflow-on-spark-afc3655d8f95">在Spark上扩展分布式tensor flow</a>中所述。</p><p id="e5ff" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请留下您的评论并订阅下一期教程。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="ce33" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ob">原载于2020年5月21日</em><a class="ae kl" href="https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/" rel="noopener ugc nofollow" target="_blank"><em class="ob"/></a><em class="ob">。</em></p></div></div>    
</body>
</html>