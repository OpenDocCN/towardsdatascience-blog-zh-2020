<html>
<head>
<title>Univariate Linear Regression-Theory and Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一元线性回归-理论与实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/univariate-linear-regression-theory-and-practice-99329845e85d?source=collection_archive---------26-----------------------#2020-07-01">https://towardsdatascience.com/univariate-linear-regression-theory-and-practice-99329845e85d?source=collection_archive---------26-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e4fc25ea658c01b9c5e315dddca7de1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rTS8PQrapEpLYRkC"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">卢克·切瑟在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="20e7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">简介:</strong>这篇文章解释了一元线性回归的数学和执行。这将包括成本函数背后的数学、梯度下降和成本函数的收敛。</p><p id="e7c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习主要分为三种类型</p><ol class=""><li id="1974" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">监督机器学习</li><li id="1125" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">无监督机器学习</li><li id="fc47" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">强化学习</li></ol><p id="8839" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在监督机器学习中，使用一组具有预期输出的训练示例来训练模型。模型在对这些示例进行训练后，会尝试预测另一组示例的输出值。有两种类型的监督机器学习:</p><ol class=""><li id="9eea" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">回归-预测连续值输出。所有类型的回归都属于这一部分。</li><li id="8902" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">分类-预测离散值输出。SVM、KNN 和兰登森林属于这一部分。</li></ol><p id="b65d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有几种类型的回归算法，如线性回归，逻辑回归，多项式回归，逐步回归，岭回归，套索回归，弹性网回归。</p><p id="89e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑一元线性回归的情况。我们将同时处理数学和执行。所用数据集的链接如下:</p><p id="9c79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://www.kaggle.com/c/home-data-for-ml-course/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/home-data-for-ml-course/data</a></p><p id="e2a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该数据集包含波士顿不同房屋的销售价值信息及其对其他要素的依赖性。</p><p id="10ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们导入所需的库:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="65db" class="mb mc it lx b gy md me l mf mg">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from matplotlib import style</span></pre><p id="5853" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并将数据集加载到熊猫数据框架中</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="5072" class="mb mc it lx b gy md me l mf mg">read_df = pd.read_csv(‘train.csv’)<br/>df = read_df.copy()<br/>df.head()<br/>df.info()</span></pre><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/e45cb760c7fdb68d1e95d805acfe18e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djzLI7SvUU98ZbWa5v-iNQ.png"/></div></div></figure><p id="727c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些特征的详细描述与数据集一起提供。可以获得简要信息:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="1e1a" class="mb mc it lx b gy md me l mf mg">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 1460 entries, 0 to 1459<br/>Data columns (total 81 columns):<br/> #   Column         Non-Null Count  Dtype  <br/>---  ------         --------------  -----  <br/> 0   Id             1460 non-null   int64  <br/> 1   MSSubClass     1460 non-null   int64  <br/> 2   MSZoning       1460 non-null   object <br/> 3   LotFrontage    1201 non-null   float64<br/> 4   LotArea        1460 non-null   int64  <br/> 5   Street         1460 non-null   object <br/> 6   Alley          91 non-null     object <br/> 7   LotShape       1460 non-null   object <br/> 8   LandContour    1460 non-null   object <br/> 9   Utilities      1460 non-null   object <br/> 10  LotConfig      1460 non-null   object <br/> 11  LandSlope      1460 non-null   object <br/> 12  Neighborhood   1460 non-null   object <br/> 13  Condition1     1460 non-null   object <br/> 14  Condition2     1460 non-null   object <br/> 15  BldgType       1460 non-null   object <br/> 16  HouseStyle     1460 non-null   object <br/> 17  OverallQual    1460 non-null   int64  <br/> 18  OverallCond    1460 non-null   int64  <br/> 19  YearBuilt      1460 non-null   int64  <br/> 20  YearRemodAdd   1460 non-null   int64  <br/> 21  RoofStyle      1460 non-null   object <br/> 22  RoofMatl       1460 non-null   object <br/> 23  Exterior1st    1460 non-null   object <br/> 24  Exterior2nd    1460 non-null   object <br/> 25  MasVnrType     1452 non-null   object <br/> 26  MasVnrArea     1452 non-null   float64<br/> 27  ExterQual      1460 non-null   object <br/> 28  ExterCond      1460 non-null   object <br/> 29  Foundation     1460 non-null   object <br/> 30  BsmtQual       1423 non-null   object <br/> 31  BsmtCond       1423 non-null   object <br/> 32  BsmtExposure   1422 non-null   object <br/> 33  BsmtFinType1   1423 non-null   object <br/> 34  BsmtFinSF1     1460 non-null   int64  <br/> 35  BsmtFinType2   1422 non-null   object <br/> 36  BsmtFinSF2     1460 non-null   int64  <br/> 37  BsmtUnfSF      1460 non-null   int64  <br/> 38  TotalBsmtSF    1460 non-null   int64  <br/> 39  Heating        1460 non-null   object <br/> 40  HeatingQC      1460 non-null   object <br/> 41  CentralAir     1460 non-null   object <br/> 42  Electrical     1459 non-null   object <br/> 43  1stFlrSF       1460 non-null   int64  <br/> 44  2ndFlrSF       1460 non-null   int64  <br/> 45  LowQualFinSF   1460 non-null   int64  <br/> 46  GrLivArea      1460 non-null   int64  <br/> 47  BsmtFullBath   1460 non-null   int64  <br/> 48  BsmtHalfBath   1460 non-null   int64  <br/> 49  FullBath       1460 non-null   int64  <br/> 50  HalfBath       1460 non-null   int64  <br/> 51  BedroomAbvGr   1460 non-null   int64  <br/> 52  KitchenAbvGr   1460 non-null   int64  <br/> 53  KitchenQual    1460 non-null   object <br/> 54  TotRmsAbvGrd   1460 non-null   int64  <br/> 55  Functional     1460 non-null   object <br/> 56  Fireplaces     1460 non-null   int64  <br/> 57  FireplaceQu    770 non-null    object <br/> 58  GarageType     1379 non-null   object <br/> 59  GarageYrBlt    1379 non-null   float64<br/> 60  GarageFinish   1379 non-null   object <br/> 61  GarageCars     1460 non-null   int64  <br/> 62  GarageArea     1460 non-null   int64  <br/> 63  GarageQual     1379 non-null   object <br/> 64  GarageCond     1379 non-null   object <br/> 65  PavedDrive     1460 non-null   object <br/> 66  WoodDeckSF     1460 non-null   int64  <br/> 67  OpenPorchSF    1460 non-null   int64  <br/> 68  EnclosedPorch  1460 non-null   int64  <br/> 69  3SsnPorch      1460 non-null   int64  <br/> 70  ScreenPorch    1460 non-null   int64  <br/> 71  PoolArea       1460 non-null   int64  <br/> 72  PoolQC         7 non-null      object <br/> 73  Fence          281 non-null    object <br/> 74  MiscFeature    54 non-null     object <br/> 75  MiscVal        1460 non-null   int64  <br/> 76  MoSold         1460 non-null   int64  <br/> 77  YrSold         1460 non-null   int64  <br/> 78  SaleType       1460 non-null   object <br/> 79  SaleCondition  1460 non-null   object <br/> 80  SalePrice      1460 non-null   int64  <br/>dtypes: float64(3), int64(35), object(43)<br/>memory usage: 924.0+ KB</span></pre><p id="a1e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">包括 ID 在内共有 81 个特征。参赛作品总数为 1460 件。我们现在不会做任何数据清理，因为我们的目的只是了解线性回归。让我们将特性 LotArea 作为输入特性，将 SalePrice 作为输出特性。策划他们两个:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="895b" class="mb mc it lx b gy md me l mf mg">fig = plt.figure(figsize = (10,10))<br/>style.use(‘ggplot’)<br/>plt.scatter(df.LotArea, df.SalePrice, s = 5, c = ‘k’)</span></pre><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/9fa66e2b0ffabf548940efc6e5eaa3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cS-7cLvz7CB4zAIx61IpWg.png"/></div></div></figure><p id="08ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，大多数房子的土地面积不到 50000 平方英尺，售价不到 500000 美元。所有其他情况都可以被认为是例外情况或异常值(取决于其他特征)。</p><p id="8a55" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性回归的基本思想是找到一条符合数据点集的直线。我将在本文中使用以下符号:</p><p id="dbbb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，m =训练样本数=&gt; 1460</p><p id="d482" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">x's = 'Input '变量/特征=&gt; LotArea</p><p id="e931" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">y's = '输出'变量/标签= &gt;销售价格</p><p id="f1d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(x，y)包括一个训练示例。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/92d7bb8cc9d4480eb09af9eb3dfde080.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*TorJmuY3JYMK_XTVPDt2xA.png"/></div></figure><p id="8303" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表示第 I 个训练示例。</p><p id="30f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，回归过程由下面的流程图解释:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/8c807cee52f3fb74a821008fe3158887.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*uio_hQYRLYxoYiWTANRl2A.png"/></div></figure><p id="e328" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练集被植入学习算法以产生假设。假设是根据输入值预测输出值的函数。在我们的例子中，LotArea 作为假设的输入，得到估计的销售价格。h 是从 x 到 y 的映射函数。</p><p id="e4bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在线性回归的情况下，假设由以下等式表示:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/07cb96a025cc359b37ae216a58c2916a.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*yIe522p0zoPJ34hTYhTuPw.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/c095e8e0baf08dad1b13322648e97519.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*GSCqw-5qkN9oZ9qnLxgDMQ.png"/></div></figure><p id="b4a4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是假设的参数。在引用参数时，我将使用‘theta _ 0’和‘theta _ 1’。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/f50c2cd5f4dbe60e6c07d3589de17176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lRrofr6B1PvRXID29aWwg.png"/></div></div></figure><p id="0c07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图展示了一个假设的图示例子。它是符合给定数据点的回归线。这里的假设类似于直线方程，因为我们使用的是线性回归。我们也可以使用任何类型的函数作为假设来相应地拟合数据。这里使用的假设是单变量线性回归或单变量线性回归。</p><p id="c9d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看如何选择假设的参数。这里的想法是选择参数，使其最适合 y，即选择 theta_0 和 theta_1，使 h(x)接近每个 x 的 y 值。这种情况可以用数学方法表示如下:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/7004989a4d6cc9bd3f91a22d056d9d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*SOs5PcGsAbOajumI6HCLgA.png"/></div></div></figure><p id="016f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 m 是训练样本的数量。</p><p id="54c6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述等式相当于:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/bd6cf32ce571bfb3d80d6d2d08885c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*jisUR1Elns7uAinrMRH7DA.png"/></div></figure><p id="529e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里 h(x)是上面提到的假设，y 是相应 x 值的输出值。上述等式简化为寻找θ_ 0 和θ_ 1 的值，该值最小化估计输出 h(x)和实际输出 y 之间的差。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/53f8b3e571dd29a3546584e2609b89e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*_q8r1HBBVa_nvpWJYa1uhg.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e3040b53801f83fcece300541776c7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*LAms4yv-bO8N--D0RJe9-w.png"/></div></figure><p id="7064" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">J(theta_0，theta_1)称为<strong class="ki iu">代价函数</strong>。有许多类型的成本函数可用。均方差(MSE)成本函数是回归问题中通常使用的函数，如上所示。</p><p id="9f5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们进一步分析成本函数。为简化起见，请考虑:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e7186ac6a354cf374e14619cd8617b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/1*2JyrbC92IcQj4_OV1m0-vw.png"/></div></figure><p id="8e6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">于是假设变成了:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f8760b2096136e30032c885af893c362.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*o3TR2P2xGZ74FwKR5e9qIQ.png"/></div></figure><p id="caf4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和成本函数</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e0ac36891e64e637a1dd05e1f8d65894.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*SRbMo4aGTv7T-IVmZdtd8Q.png"/></div></figure><p id="5399" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着所有假设函数都经过原点，即(0，0)。</p><p id="bb49" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑以下数据点:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9f0438091fb948cc042119da60471b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*SYaoXKn0Wy-5HMXRAj0RYg.png"/></div></figure><p id="6462" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c290f4247bbd7d1960ed656edcb0ec68.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*Pv9R8UAWbSYTuy_q8PL0Rg.png"/></div></figure><p id="2e8d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将获得以下回归线:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c08aff61969188a63ddb8894e0acf27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*b6N9EGx2oicaS-rHAI1-Jw.png"/></div></figure><p id="162b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">成本函数的值是:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a9b4bfaba10182527ad9700bfea3d76e.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*Rka2FXdLBUGdDDFsvEtRYQ.png"/></div></figure><p id="93e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">认为</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3a2f36f0fde208dc2f8ce382410c1423.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*IC36woNv54fvipQuhFW4Ag.png"/></div></figure><p id="13ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">得到的回归线将是:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a70766421a2d1ccf0e9c3d0a333e451c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*UvQIVeaCLGoQ3wFrx7vF9g.png"/></div></figure><p id="18fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从数据点到回归线的直线是实际值和估计值之间的误差。计算成本 J</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/a20f129a54f21ef25c6e3c6609a78b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*D8Gpln0rsKqg81NynyekCQ.png"/></div></div></figure><p id="eab4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">认为</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7d7fbd9231dbe51ec1b2494476352d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*tZu-Kj1W6xC6A9XNMQyb6A.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/dc181cfb96dd2562ed22768cde4dd3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*xyHcnB72KUYbI5emSWU0dA.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/b0562c5ee1356ab0aec91115c539f111.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*pKW6O_RT9iTK8iR3vH7Wfg.png"/></div></figure><p id="5f3c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">绘制不同θ_ 0 值的成本，我们得到:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ed53899394fa85f24cd3fc0a0efd1c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*S_5MojUjBRo_oQFogaB59w.png"/></div></figure><p id="499b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从图中，我们可以看到成本函数值在</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c290f4247bbd7d1960ed656edcb0ec68.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*Pv9R8UAWbSYTuy_q8PL0Rg.png"/></div></figure><p id="6030" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其对应于穿过所有三个数据点的回归线。由此，我们可以暗示，给出成本函数值最小的参数值对应于最佳拟合回归线。</p><p id="2c1c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们回到最初的成本函数。由于我们在上一节中考虑了θ_ 0 = 0，我们能够为成本函数绘制一个二维图。但实际上，由于成本函数取决于θ_ 0 和θ_ 1，我们将得到一个弓形的三维图形(该形状取决于训练集)。考虑房价数据集中的 LotArea 和 SalePrice 要素。让我们试着画出这些特征的成本函数。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="c312" class="mb mc it lx b gy md me l mf mg">from mpl_toolkits.mplot3d.axes3d import Axes3D<br/>import numpy as np<br/>m = len(df.LotArea)<br/>theta_0 = np.linspace(10000, 100000, num = 1000)<br/>theta_1 = np.linspace(5, 10, num = 1000)<br/>Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)<br/>def cost(theta_0, theta_1, LotArea, SalePrice):<br/>    h = 0<br/>    for i in range(len(LotArea)):<br/>        h += (((theta_0 + (theta_1 * LotArea[i])) - SalePrice[i]) **            2)<br/>    J = (1/2*len(LotArea)) * h<br/>    return J<br/>fig = plt.figure(figsize = (12,12))<br/>style.use('ggplot')<br/>ax = fig.add_subplot(111, projection = '3d')<br/>ax.plot_surface(Theta_0, Theta_1, cost(Theta_0, Theta_1, df.LotArea, df.SalePrice), cmap = 'viridis')<br/>ax.set_xlabel('θ0')<br/>ax.set_ylabel('θ1')<br/>ax.set_zlabel('J')</span></pre><p id="1dd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">得到了如下的图:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/d17dc97ed7bad25e3da776df37068df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5pRlUmaQ_U3TZL2ljbV1g.png"/></div></div></figure><p id="1ce4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XY 平面表示θ_ 0 和θ_ 1 的不同值，并且从 XY 平面上的任意点开始的表面图的高度给出了对应于该点的θ_ 0 和θ_ 1 值的 J 值。我们也可以画一个等高线图来表示 J(theta_0，theta_1)。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ab01" class="mb mc it lx b gy md me l mf mg">fig = plt.figure(figsize = (12,12))<br/>style.use(‘ggplot’)<br/>ax = fig.add_subplot(111)<br/>cs = ax.contourf(Theta_0, Theta_1, cost(Theta_0, Theta_1, df.LotArea, df.SalePrice))<br/>cbar = fig.colorbar(cs)<br/>ax.set_xlabel('θ0')<br/>ax.set_ylabel('θ1')<br/>plt.show()</span></pre><p id="45a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得以下等高线图:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/0e37bf10fc4cf65b00e70abbfe0a4161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVMM1kaEbiggI7OU3pJYug.png"/></div></div></figure><p id="301b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">侧栏显示了 J 值随着图中颜色的变化而变化。对于不同的θ_ 0 和θ_ 1 值，等高线图中特定环中的所有点具有相同的 J 值。</p><p id="b44d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们为上面考虑的θ_ 0 和θ_ 1 的值绘制回归线:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="0be3" class="mb mc it lx b gy md me l mf mg">minj = np.min(cost(Theta_0, Theta_1, df.LotArea, df.SalePrice))<br/>point = np.array(cost(Theta_0, Theta_1, df.LotArea, df.SalePrice)) == minj<br/>position = np.where(point)<br/>position</span><span id="334a" class="mb mc it lx b gy ng me l mf mg">output&gt;&gt;(array([9]), array([999]))</span></pre><p id="0ef5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出显示了给出最小成本值的θ_ 0 和θ_ 1 的值的位置。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="a2f7" class="mb mc it lx b gy md me l mf mg">theta_1_min = Theta_1[9][999]<br/>theta_0_min = Theta_0[9][999]<br/>def fitline(theta_0, theta_1, LotArea):<br/>    x = []<br/>    for i in range(m):<br/>        x.append(theta_0 + (theta_1 * LotArea[i]))<br/>    return x<br/>fig = plt.figure(figsize = (10,10))<br/>style.use('ggplot')<br/>count = 0<br/>for i in range(len(theta_0)):<br/>    plt.plot(df.LotArea,fitline(theta_0[i], theta_1[i], df.LotArea), color = 'k', alpha = 0.1, linewidth = 0.1)<br/>    count += 1<br/>print(count)<br/>plt.scatter(df.LotArea, df.SalePrice, s = 5, c = 'r')<br/>plt.plot(df.LotArea, (theta_0_min + (theta_1_min * df.LotArea)), color = 'k')<br/>plt.show()</span></pre><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7ef9553598b207bd52772b68e713ed77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*F0vwq9npjpcb4z7Q98D3yw.png"/></div></figure><p id="5264" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">阴影区域由对应于θ_ 0 和θ_ 1 值的 1000 条线组成，突出显示的线是对应于我们获得最小成本的参数值的线。这不是实际的最小成本，而只是所考虑的参数范围内的最小值。</p><p id="6d3b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于找到给出实际最小成本值的参数值的算法是<strong class="ki iu">梯度下降</strong>。它是一种通用算法，不仅用于最小化成本函数 J，还用于最小化其他函数。该算法的大致轮廓将是:</p><ol class=""><li id="adb8" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">从θ_ 0 和θ_ 1 的某个值开始(通常两者都被设置为零。)</li><li id="9341" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">不断改变数值，直到我们达到 J(theta_0，theta_1)的最小值。</li></ol><p id="8f23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">算法如下:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/15a641f10254260d3d028f40145bbdde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*x2_qsrFl-g92BlcUmiR-NA.png"/></div></figure><p id="41cf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/54219f8d53c71cf0036f048f58f53624.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*bQ71_U7XgDIIIebaEjRC7A.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f4ba6208b7429229a3d0d2aba32629aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*a_bQixj8FRp-ELd--vtKeQ.png"/></div></figure><p id="feb9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Alpha 或学习率决定了算法达到最小成本值(即，给出最小成本值的参数值)所采取的步长。我们必须注意的非常重要的细节是，我们必须<strong class="ki iu">同时更新 theta_1 和 theta _ 0</strong>。我们不应该更新θ_ 0，然后升级成本函数，然后θ_ 1，这不是我们想要的方式。</p><p id="9cb4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过研究算法的偏导数部分，考虑我们之前绘制的二维成本函数图。假设我们正在更新θ_ 1 的值，因为我们在那种情况下忽略了θ_ 0。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/84edfb60b5b405d241f4fc0b78f1b0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*gzMI7436pviiecVKe81miA.png"/></div></figure><p id="082a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">偏导数项给出了图上每个θ_ 1 点切线的斜率。算法将是:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/39ce363886e490589b34bb484c0b41b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*Jo5se5psBebnZdQY7XDbdg.png"/></div></figure><p id="150c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑上图中的点及其切线。正如我们所见，切线具有正斜率，因此等式为:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7986a057204bcf1752b092530a28b70b.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*gFXD9pX-uzLjh2uTHDbRMg.png"/></div></figure><p id="63f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们可以看到，θ_ 1 的值在减小，这就是我们要做的，逐步达到最小值。如果我们考虑曲线左侧的θ_ 1 值，斜率将是负数，从而增加θ_ 1 并最终达到最小值。</p><p id="eb1d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习率(α)也会影响收敛路径。如果α太小，则算法需要很长时间才能达到最小值，从而延迟收敛。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/87458ca76b68452cf9578ae0691c85d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*oIP8htGc5cHs0cn2H1liSA.png"/></div></figure><p id="b0af" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果 alpha 太大，则算法可能会超过最小值，从而不会收敛，甚至可能发散，如上图所示。</p><p id="593d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们达到局部最小值时，考虑应用该算法。该算法的偏导数部分将为零，因为切线在该点将是水平的，因此θ_ 0 的值不会改变。</p><p id="d11f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们接近一个局部最小值时，切线的斜率继续下降到零。因此，当我们接近最小值时，参数减少或增加的值也减少。由此我们可以看出，即使我们保持α值不变，算法也是收敛的。</p><p id="3abf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这种梯度下降算法应用于一元线性回归问题。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5747e8b44a5d3cd5d3a2b2470a18b20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*TCYHUvDUE9A1aiqXO5g_Mw.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1c0f90fa2a16c8d1a407009d629ef257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*rRdP5vSYDglsf3Kd8I78Yg.png"/></div></figure><p id="08cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 j = 0</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4f46230d36b67bd8fdc3da9bf4b3c564.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*RloLV7nDuMp_BTASmRZn4g.png"/></div></figure><p id="c31d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 j = 1</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/776dc77b54e4458b96a15adc2a2ad93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ugkcXGHt3vuk2FzUE6Orgw.png"/></div></figure><p id="36be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这为我们提供了梯度下降算法</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/de560ae4ac8e3adc13f05fa9e1884ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*YbY-Zd08IOQXS3g7LA8bCg.png"/></div></figure><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ec9505f000c885c962ab1a2ad21cb34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*_yMqafMvMcLeMZ7Zk5rpfA.png"/></div></figure><p id="6aa7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次考虑房价数据集。让我们对特征 LotArea 和输出 SalePrice 执行梯度下降。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="7fa9" class="mb mc it lx b gy md me l mf mg">from sklearn import preprocessing<br/>x = df.LotArea<br/>y = df.SalePrice<br/>x = preprocessing.scale(x)<br/>theta_0_gd = 0<br/>theta_1_gd = 0<br/>alpha = 0.01<br/>h_theta_0_gd = 1<br/>h_theta_1_gd = 1<br/>epoch = 0<br/>fig = plt.figure(figsize = (10,10))<br/>style.use('ggplot')<br/>plt.scatter(x, y, s = 5, c = 'r')<br/>while h_theta_0_gd != 0 or h_theta_0_gd != 0:<br/>    if epoch &gt; 1000:<br/>        break<br/>    h_theta_0_gd = 0<br/>    h_theta_1_gd = 0<br/>    for i in range(m):<br/>        h_theta_0_gd += (theta_0_gd + (theta_1_gd * x[i]) - y[i])<br/>        h_theta_1_gd += ((theta_0_gd + (theta_1_gd * x[i]) - y[i]) * x[i])<br/>    h_theta_0_gd = (1/m) * h_theta_0_gd<br/>    h_theta_1_gd = (1/m) * h_theta_1_gd<br/>    theta_0_gd -= (alpha * h_theta_0_gd)<br/>    theta_1_gd -= (alpha * h_theta_1_gd)<br/>    plt.plot(x,(theta_0_gd + (theta_1_gd * x)), color = 'k', alpha = 0.1, linewidth = 1)<br/>    epoch += 1<br/>plt.plot(x,(theta_0_gd + (theta_1_gd * x)), color = 'r', linewidth = 3)<br/>plt.show()</span></pre><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3d980832ab8a224e46bff769983d2d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*Vwmts0X7lTOkmZhWitMKJA.png"/></div></figure><p id="f2da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参数的初始值被设置为零。学习率设置为 0.01。最多允许 1000 个重复或时期。图中所有阴影部分由获得每个时期的参数后绘制的回归线组成。突出显示的红线是 1000 个时期后的最终回归线。正如我们所见，线的密度随着接近红线而增加。这是因为当算法接近最小值时所采取的步骤很小，从而减少了每个时期中相应回归线之间的间隔。为了确认我们是否正确地应用了该算法，我们可以用 sklearn 的线性回归模型进行交叉验证。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="0375" class="mb mc it lx b gy md me l mf mg">from sklearn import model_selection<br/>from sklearn.linear_model import LinearRegression<br/>x_model = np.array(df.LotArea).reshape(-1,1)<br/>y_model = np.array(df.SalePrice).reshape(-1,1)<br/>x_model = preprocessing.scale(x_model)<br/>x_train, x_test, y_train, y_test = model_selection.train_test_split(x_model, y_model, test_size = 0.33)<br/>clf = LinearRegression()<br/>clf.fit(x_train, y_train)<br/>theta_1_model = clf.coef_<br/>theta_0_model = clf.intercept_<br/>fig = plt.figure(figsize = (10,10))<br/>style.use('ggplot')<br/>plt.scatter(x, y, s = 5, c = 'r')<br/>plt.plot(x_model,(theta_0_model + (theta_1_model * x_model)), color = 'k')</span></pre><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5328bd638a0f6264d7f1bc79fff0d2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*pLuP9Vc7_16nXLETitf44w.png"/></div></figure><p id="eb2b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们可以看到的，sklearn 的回归模型也产生了相同的图形。</p><p id="53ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">结论:</strong>我们开始了解不同类型的机器学习以及监督学习的类型。我们看到了假设、成本函数和单变量线性回归的梯度下降背后的概念，并从头开始使用上述概念在房价数据集的要素上构建了一条回归线。然后将其与使用 sklearn 的线性回归模型构建的模型进行比较。</p><p id="505f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">参考文献:</strong></p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="9956" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我正在从这个 Youtube 播放列表中学习大多数关于机器学习的概念。这很有帮助，也很容易理解。</p></div></div>    
</body>
</html>