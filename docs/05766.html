<html>
<head>
<title>Visualization of Word Embedding Vectors using Gensim and PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Gensim和PCA的单词嵌入向量可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354?source=collection_archive---------13-----------------------#2020-05-13">https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354?source=collection_archive---------13-----------------------#2020-05-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d0b6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将学习如何使用单词嵌入来训练文本数据，并使用主成分分析来进一步可视化这些向量。</h2></div><h1 id="8295" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介:</strong></h1><p id="afff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">有某些方法可以从任何文本数据中提取特征，然后输入到机器学习模型中。最基本的技术是—计数矢量器、Tf-Idf。这些技术的主要缺点是它们不能捕捉文本数据的语义，并且每个单词被分配到一个新的维度，就像一个热编码向量一样。因此，如果语料库规模相当大，那么维度可能高达数百万个单词，这是不可行的，并且可能导致模型不佳。我们可以使用简单的余弦相似度计算来验证这一点。假设我们有两个句子，每个句子包含一个单词“good”和“nice”。在这里，我们知道这两个词彼此有一些相似性，但是当我们使用计数矢量器向量计算余弦相似性时，结果是零。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/ac2462008758de6eec26f3fb10933c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*cXtjND1RlKWtEFd_l5IB1A.png"/></div></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/8cedf4aae108e60c2f73fcbf811f3f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*4Yert_9-t2fCVflyOuTk7g.png"/></div></figure><p id="a8ab" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">其中U和V是两个句子-[1，0]和[0，1]的向量表示</p><h1 id="04d4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">什么是文字嵌入？</strong></h1><p id="7882" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是一种将单词映射到实数向量的技术，同时捕捉关于文本含义的一些信息。它说，如果两个单词有相似的意思，它们将在密集的空间中彼此靠近。就像这两个词一样，我们前面用过的“好”和“漂亮”会在嵌入空间中彼此靠近。在本文中，我们将使用gensim库中的Word2Vec算法在密集空间中可视化这类单词。Word2Vec包含两个模型，分别用于训练跳格模型和连续词袋(CBOW)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/b8d39a3a30f69318ce79bb63fb404118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgcCZqoyLrSNp0-hH_QGcQ.png"/></div></div></figure><p id="058b" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">对于想要熟悉单词嵌入的基本概念的人来说，他们应该首先回顾下面给出的文章。在本文中，我们将主要关注python的实现和可视化。</p><div class="mp mq gp gr mr ms"><a rel="noopener follow" target="_blank" href="/introduction-to-word-embeddings-4cf857b12edc"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd iu gy z fp mx fr fs my fu fw is bi translated">单词嵌入简介</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">什么是单词嵌入？</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng mc ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd iu gy z fp mx fr fs my fu fw is bi translated">Word2Vec的数学优先解释</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">介绍</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nh l nd ne nf nb ng mc ms"/></div></div></a></div><h1 id="0376" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak"> Python实现:</strong></h1><h2 id="57c9" class="ni kj it bd kk nj nk dn ko nl nm dp ks lj nn no ku ln np nq kw lr nr ns ky nt bi translated">数据集:</h2><p id="8656" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我从维基百科对单词嵌入的定义中摘录了一小段文字。我们将在密集的空间中呈现这篇课文的单词。</p><pre class="lx ly lz ma gt nu nv nw nx aw ny bi"><span id="e338" class="ni kj it nv b gy nz oa l ob oc">text="Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.The use of multi-sense embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation identification, and semantic relatedness. However, tasks involving named entity recognition and sentiment analysis seem not to benefit from a multiple vector representation."</span></pre><p id="9706" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">导入相关库:</strong></p><p id="27c0" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">我们将安装gensim库并从中导入Word2Vec模块。此外，我们将导入NLTK并将其用于句子标记化。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="e81f" class="ni kj it bd kk nj nk dn ko nl nm dp ks lj nn no ku ln np nq kw lr nr ns ky nt bi translated"><strong class="ak">标记化:</strong></h2><p id="be90" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将在NLTK分词器的帮助下对句子进行分词。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="fa78" class="ni kj it bd kk nj nk dn ko nl nm dp ks lj nn no ku ln np nq kw lr nr ns ky nt bi translated">Word2Vec:</h2><p id="7612" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们在上一部分中获得的标记化格式的句子中采用的模型将被直接输入其中。除此之外，这个类还有各种参数——大小、窗口、最小计数、sg</p><p id="0f89" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">大小</strong>=这决定了我们想要的单词表示的维数。(默认值=100)</p><p id="d23f" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">Window</strong>=这是一个中心单词与其周围单词之间的最大距离。</p><p id="c334" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">min _ count</strong>=这是训练模型时要考虑的最小字数；出现次数少于此数的单词将被忽略。</p><p id="ff7f" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> sg </strong> =这指定了训练算法CBOW (0)，Skip-Gram (1)</p><p id="1073" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">对于我们的设置，由于文本较少，我们将使用min_count=1来考虑所有的单词。我们将在跳格模型中使用窗口=50，因此sg=1。</p><pre class="lx ly lz ma gt nu nv nw nx aw ny bi"><span id="ff13" class="ni kj it nv b gy nz oa l ob oc">model = Word2Vec(tokens,size=50,sg=1,min_count=1)<br/>model["the"]</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8bea0a809b6a8ccc1427db5acf916807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*XPlZmgzs0l-4Flh9fYoZsg.png"/></div></figure><p id="40e3" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">我们可以看到单词“the”的矢量表示是通过使用模型[“the”]获得的。我们可以看到单词“the”的向量表示是在50维空间中。</p><p id="3e34" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">我们可以把学过的词汇打印出来如下:</p><pre class="lx ly lz ma gt nu nv nw nx aw ny bi"><span id="beb9" class="ni kj it nv b gy nz oa l ob oc">words=list(model.wv.vocab)<br/>print(words)</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi og"><img src="../Images/61a64a4c7ba547a3a327fb40798a499d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_o5eXgH3QKbuj14ai9w8Lw.png"/></div></div><p class="oh oi gj gh gi oj ok bd b be z dk translated">词汇</p></figure><p id="a7a6" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">此外，我们将在具有50维的数据帧中存储所有的词向量，并将该数据帧用于PCA。</p><pre class="lx ly lz ma gt nu nv nw nx aw ny bi"><span id="9d8c" class="ni kj it nv b gy nz oa l ob oc">X=model[model.wv.vocab]<br/>df=pd.DataFrame(df)<br/>df.shape<br/>df.head()</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/6e14b5b7263112b7c3582a24c2e44e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/format:webp/1*24V1xkDsluJbBatqDbIGIg.png"/></div><p class="oh oi gj gh gi oj ok bd b be z dk translated">数据框的形状</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi om"><img src="../Images/dff29cac6dccd94eb4a4d3417c7a3e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXg0hWIla5cYiNX95e_zCg.png"/></div></div><p class="oh oi gj gh gi oj ok bd b be z dk translated">数据帧</p></figure><h1 id="c854" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">PCA:</h1><p id="7b94" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将使用numpy库实现PCA。PCA涉及的步骤如下-</p><p id="1707" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">1-标准化数据集并计算相关矩阵。</strong></p><p id="b1f7" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">2-使用特征分解计算特征值和特征向量。</strong></p><p id="be08" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">3-对特征值及其对应的特征向量进行排序。</strong></p><p id="8492" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">4-挑选两个最大的特征值，创建一个特征向量矩阵。</strong></p><p id="3376" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu">5-使用这些新特征向量的点积变换原始数据。</strong></p><p id="be49" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">在这种情况下，我们可以忽略标准化步骤，因为数据使用相同的单位。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="7a19" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">获得的新数据集如下所示:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi on"><img src="../Images/717e47e21b8a10cf7ed4265de055e23f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*Y6dzQmA1XtW-OatxqYSIIw.png"/></div><p class="oh oi gj gh gi oj ok bd b be z dk translated">维度=(78，2)</p></figure><h1 id="f706" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">可视化:</strong></h1><p id="cd89" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将使用matplotlib来可视化密集空间中的单词。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi oo"><img src="../Images/bf615212ad0c54fc323a1d555698da82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z15sdOMhavIN75ltU8XQUQ.png"/></div></div></figure><h1 id="afe6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">结论:</strong></h1><p id="280e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文中，我们学习了如何使用Word2Vec技术将文本数据转换为特征向量。此外，我们还学习了如何使用matplotlib和PCA在二维空间中表示这些单词向量。在我的下一篇文章中，我们将讨论单词嵌入技术背后的数学原理。</p><h1 id="fb81" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">参考文献:</strong></h1><p id="eaeb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae op" href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html" rel="noopener ugc nofollow" target="_blank">https://d2l . ai/chapter _ natural-language-processing-pre training/word 2 vec . html</a></p><h1 id="7eda" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">感谢您的阅读！！！！</h1><p id="dbef" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果你喜欢我的工作，想支持我。</p><p id="e31e" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">1-支持我的最好方式是在<a class="ae op" href="https://medium.com/@saketthavananilindan" rel="noopener"> <strong class="lc iu">中</strong> </a> <strong class="lc iu">上关注我。</strong></p><p id="14f9" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">2-关注我<a class="ae op" href="https://www.linkedin.com/in/saket-thavanani-b1a149a0/" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">LinkedIn</strong></a><strong class="lc iu">。</strong></p></div></div>    
</body>
</html>