<html>
<head>
<title>Dimensionality Reduction for Data Visualization: PCA vs TSNE vs UMAP vs LDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据可视化的降维:主成分分析与 TSNE、UMAP 和线性判别分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29?source=collection_archive---------1-----------------------#2020-05-31">https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29?source=collection_archive---------1-----------------------#2020-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="27f1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 PCA、TSNE、UMAP 和 LDA 在 2D 和 3D 中可视化高维数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0d67cb3db56e511cd37fd637737c4b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RNm-F0k16igyhlpN5loiQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/s/photos/wall?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@hinbong?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Hin Bong Yeung </a>拍摄</p></figure><p id="2281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个故事中，我们将介绍三种专门用于<em class="lv">数据可视化</em> : <strong class="lb iu"> PCA、t-SNE、LDA 和 UMAP 的降维技术。</strong>我们将使用<em class="lv">手语 MNIST 数据集</em>来详细探索它们，而不是深入研究算法背后的数学。</p><h1 id="8623" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">什么是降维？</strong></h1><p id="bc96" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">许多机器学习问题涉及成千上万个特征，拥有如此大量的特征会带来许多问题，最重要的是:</p><ul class=""><li id="b52d" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><em class="lv">使训练极其缓慢</em></li><li id="5a7c" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><em class="lv">很难找到好的解决方案</em></li></ul><p id="1ae6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是所谓的<strong class="lb iu"> <em class="lv">维数灾难</em> </strong> <em class="lv"> </em>简而言之，降维就是将特征的数量减少到最相关的数量的过程。</p><p id="1999" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降低维数确实会丢失一些信息，但是大多数压缩过程都会有一些缺点，即使我们训练得更快，我们也会使系统性能稍差，但这没关系！“有时降低维度可以过滤掉一些存在的噪音和一些不必要的细节”。</p><p id="2145" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数降维应用用于:</p><ul class=""><li id="3137" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">数据压缩</strong></li><li id="4965" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">降噪</strong></li><li id="0bd3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">数据分类</strong></li><li id="5b11" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">数据可视化</strong></li></ul><p id="4846" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维最重要的一个方面，就是<strong class="lb iu"> <em class="lv">数据可视化。</em></strong></p><h2 id="f12d" class="nh lx it bd ly ni nj dn mc nk nl dp mg li nm nn mi lm no np mk lq nq nr mm ns bi translated">降维的主要方法</h2><p id="cd59" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">降维的两种主要方法:<strong class="lb iu">投影</strong>和<strong class="lb iu">流形学习。</strong></p><ul class=""><li id="8e94" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu"/></li><li id="481d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">流形学习:</strong>许多降维算法通过对训练实例所在的流形进行建模来工作；这叫做<em class="lv">流形学习</em>。它依赖于流形假设或假设，<em class="lv">认为大多数真实世界的高维数据集都接近一个低得多的维流形，</em>这种假设在大多数情况下是基于观察或经验，而不是理论或纯逻辑。[4]</li></ul><p id="9ab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在开始解决用例之前，让我们简要地解释一下这三种技术。</p><h1 id="e81e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">主成分分析</h1><p id="e5fb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最著名的降维“无监督”算法之一是 PCA(主成分分析)。</p><blockquote class="nt"><p id="056a" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated">其工作原理是识别最接近数据的超平面，然后将数据投影到该超平面上，同时保留数据集中的大部分变化。</p></blockquote><h2 id="f175" class="nh lx it bd ly ni od dn mc nk oe dp mg li of nn mi lm og np mk lq oh nr mm ns bi translated">主成分</h2><p id="4123" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">解释训练集中最大方差的轴称为<strong class="lb iu"> <em class="lv">主成分</em> </strong>。</p><p id="5d0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与该轴正交的轴称为<strong class="lb iu">第二主分量</strong>。当我们追求更高的维度时，主成分分析会找到与其他两个分量正交的第三个分量，以此类推，为了可视化的目的，我们总是坚持 2 个或最多 3 个主分量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/50ac747a7158698070d5da655bbe84ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gjxmv1XYf6udQX9d6uUxiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Packt_Pub，via Hackernoon</p></figure><p id="7b9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择正确的超平面非常重要，这样当数据投影到超平面上时，可以获得关于原始数据如何分布的最大信息量。</p><h1 id="6c25" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">T-SNE(T-分布式随机邻居嵌入)</h1><p id="8c9c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"> (t-SNE) </strong>或<strong class="lb iu">T-分布式随机邻居嵌入</strong>于<strong class="lb iu"> </strong> 2008 <strong class="lb iu"> </strong>由<strong class="lb iu"> ( </strong>劳伦斯·范·德·马腾和<strong class="lb iu"> </strong>杰弗里·辛顿)创建，用于降维，特别适合于高维数据集的可视化。</p><blockquote class="nt"><p id="79d9" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated"><strong class="ak"> (t-SNE) </strong>取一个高维数据集，把它还原成一个低维图，保留了很多原始信息。它通过在二维或三维地图中给每个数据点一个位置来做到这一点。这种技术发现数据中的聚类，从而确保嵌入保留了数据中的含义。t-SNE 减少维数，同时试图保持相似的实例接近，不相似的实例分开。[2]</p></blockquote><p id="f0a7" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">要快速了解这种技术，请参考下面的动画(它摘自西里尔·罗桑特的精彩教程，我强烈推荐看看他的精彩教程。</p><p id="a989" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">链接:<a class="ae ky" href="https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/content/an-illustrated-introduction-to-the-t-SNE-algorithm/</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f6b5ef42f7283b88eddd4b69bebf3678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*lKLB_1aghhnxQjMQziEyGQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://www.oreilly.com/people/cyrille-rossant/" rel="noopener ugc nofollow" target="_blank">西里尔·罗桑特</a>，转自奥雷利</p></figure><h1 id="be41" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> LDA ( </strong>线性判别分析)</h1><p id="9c9a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在<strong class="lb iu">模式分类的预处理步骤中，线性判别分析(LDA)是最常用的降维技术。</strong></p><blockquote class="nt"><p id="6317" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated">目标是将数据集投影到具有良好类别可分性的低维空间上，以避免过拟合并降低计算成本。</p></blockquote><p id="4e62" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">一般的方法非常类似于 PCA，而不是找到使我们的数据方差最大化的分量轴，<strong class="lb iu"> <em class="lv">我们另外感兴趣的是使多个类之间的间隔最大化的轴</em></strong>(LDA)【5】。</p><p id="34c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA 是“受监督的”并计算方向(“线性判别式”)，这些方向将代表使多个类之间的间隔最大化的轴。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/2853dfde864a614d7a9d6e3b42306030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXQ5sgMF0XmiY4Jc6gJVwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka.com/Articles/2014_python_lda.html</a></p></figure><h1 id="c65f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> UMAP(均匀流形近似和投影)</strong></h1><p id="4647" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">由<strong class="lb iu"> ( </strong> <a class="ae ky" href="https://arxiv.org/search/stat?searchtype=author&amp;query=McInnes%2C+L" rel="noopener ugc nofollow" target="_blank">利兰·麦金尼斯</a>、<a class="ae ky" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Healy%2C+J" rel="noopener ugc nofollow" target="_blank">约翰·希利</a>、<a class="ae ky" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Melville%2C+J" rel="noopener ugc nofollow" target="_blank">詹姆斯·梅尔维尔</a>)于 2018 年创建的均匀流形逼近与投影<strong class="lb iu">、T15，是一种通用的流形学习与降维算法。</strong></p><blockquote class="nt"><p id="04a5" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated">UMAP 是一种<strong class="ak"> <em class="oq">非线性</em> </strong>降维方法，对于可视化聚类或<strong class="ak">组数据点及其相对邻近度</strong>非常有效。</p></blockquote><p id="a519" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">与 TSNE 的显著区别是<strong class="lb iu">可伸缩性</strong>，它可以直接应用于稀疏矩阵，从而无需应用任何维数缩减，如<em class="lv"> s </em> PCA 或截断 SVD(奇异值分解)<em class="lv"> </em>作为之前的预处理步骤<em class="lv">。</em>【1】</p><p id="66c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，它类似于 t-SNE，但可能具有更高的处理速度，因此，更快，可能更好的可视化。(让我们在下面的教程中找出答案)</p><h1 id="18b6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">用例</h1><p id="eecb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，我们将浏览上述所有三种技术都将被应用的用例:具体来说，我们将尝试使用这些技术来可视化一个高维数据集:T <strong class="lb iu"> <em class="lv">何手语-MNIST </em> </strong>数据集:<a class="ae ky" href="https://www.kaggle.com/datamunge/sign-language-mnist" rel="noopener ugc nofollow" target="_blank"/></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl or"><img src="../Images/0142b511dbae1c9ea85a978f7f09ef1e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*37rFqXBGZYPfKfVTJISK1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(手语-MNIST 数据集)，截图自 kaggle.com</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h1 id="fddd" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/1c815ba20ecc907b0362bcbd1269a4bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djZ0CLFYYTldLWhyuyFneg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/5925ea0ad9a5b5a32e7de7cb5efce6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8CAwfSdQeyLSKmObKcMvNA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练数据的大小</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/3f1773296a56f0b1065abe31c95646ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nAkbwrMEJsc8cpDXuX9vA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">唯一标签的数量</p></figure><p id="f2d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意:有 25 个独特的标签代表不同手语的数量。现在为了更好的可视化(“在单个可视化中观察所有 24 个类是非常困难的”)和更快的计算。我只保留前 10 个标签，省略其余的。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="b26f" class="nh lx it bd ly ni nj dn mc nk nl dp mg li nm nn mi lm no np mk lq nq nr mm ns bi translated">实现绘图功能</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="f54b" class="nh lx it bd ly ni nj dn mc nk nl dp mg li nm nn mi lm no np mk lq nq nr mm ns bi translated"><strong class="ak">数据标准化</strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="99c7" class="nh lx it bd ly ni nj dn mc nk nl dp mg li nm nn mi lm no np mk lq nq nr mm ns bi translated"><strong class="ak">实施 PCA </strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="1851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在应用 PCA 之后，与 x 数据的 784 个特征相比，数据的新维度现在只有 3 个特征。</p><p id="47d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维度的数量已经大幅减少，同时试图尽可能多地保留信息中的“变化”。</p><h1 id="05da" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— PCA — 2D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="96f7" class="nh lx it oy b gy pc pd l pe pf">plot_2d(principalComponents[:, 0],principalComponents[:, 1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/82b4fcfc8b992dbfee0064f95fb4c1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7jZpK-Cxrk8Zxktsf_G8Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从 2D 图中，我们可以看到这两个部分肯定包含了一些信息，尤其是特定的数字，但显然不足以将它们区分开来。</p><h1 id="4da3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— PCA— 3D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="0ca9" class="nh lx it oy b gy pc pd l pe pf">plot_3d(principalComponents[:, 0],principalComponents[:, 1],principalComponents[:, 2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0f18cb4af4057ab36326c0b0be6a7110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*rTBp3ZsQTvo1QRRBy60rVQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="47ae" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实施 t-SNE</h1><p id="c3c2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">需要注意的一点是，<strong class="lb iu"> t-SNE </strong>的计算开销非常大，因此在其文档中提到:</p><p id="7fb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">“如果特征的数量非常多，强烈建议使用另一种降维方法(例如，针对密集数据的 PCA 或针对稀疏数据的 TruncatedSVD)将维数减少到合理的数量(例如，50)。这将抑制一些噪声，并加快样本之间成对距离的计算。”[2] </em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="0788" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我应用了 PCA，选择保留原始数据中的 50 个主成分，以减少对更多处理能力的需求，并且如果我们考虑了原始数据，将需要时间来计算维度缩减。</p><p id="69ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">这三种技术的速度将在以下章节中进一步详细分析和比较。</em></p><h1 id="b065" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— t-SNE — 2D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="1dba" class="nh lx it oy b gy pc pd l pe pf">plot_2d(tsne[:, 0],tsne[:, 1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/f6bdeb4512c75ebd83124823bd8f8be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6fr21RUP4ICe5GOz41p6cA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9c5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 PCA 2d 结果相比，我们可以清楚地看到不同聚类的存在以及它们是如何定位的。</p><h1 id="20bb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— t-SNE — 3D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="b7fd" class="nh lx it oy b gy pc pd l pe pf">plot_3d(tsne[:, 0],tsne[:, 1],tsne[:, 2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ab9c1d804790ab6eae9085d990eaf3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Ql3BINTwPj6PNRx99t-_MA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="c9da" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实施 UMAP</h1><p id="dccc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">UMAP 具有不同的超参数，这些参数会对生成的嵌入产生影响:</p><ul class=""><li id="b4d2" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><code class="fe ph pi pj oy b">n_neighbors</code></li></ul><p id="6345" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该参数控制<strong class="lb iu"> UMAP </strong>如何平衡数据中的本地和全局结构。n _ neighbours 的低值迫使<strong class="lb iu"> UMAP </strong>关注非常局部的结构，而较高的值将使<strong class="lb iu"> UMAP </strong>关注较大的街区。</p><ul class=""><li id="28e1" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><code class="fe ph pi pj oy b">min_dist</code></li></ul><p id="8e49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该参数控制允许<strong class="lb iu"> UMAP </strong>将点打包在一起的紧密程度。较低的值意味着这些点将紧密地聚集在一起，反之亦然。</p><ul class=""><li id="a9ae" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><code class="fe ph pi pj oy b">n_components</code></li></ul><p id="3d8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该参数允许用户确定降维空间的维度。</p><ul class=""><li id="a7b4" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><code class="fe ph pi pj oy b">metric</code></li></ul><p id="fcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此参数控制如何在输入数据的环境空间中计算距离。</p><p id="a114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细的信息，我建议查阅<strong class="lb iu"> UMAP </strong>文档<a class="ae ky" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">://umap-learn . readthedocs . io/en/latest/</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl or"><img src="../Images/e1729099cc775e85dd9a5c562b42d12f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*GkLoLjgTzhzBpOGGRONM9g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">UMAP(默认设置)</p></figure><p id="8cea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本教程，我选择保留默认设置<strong class="lb iu"> <em class="lv"> n_components </em> </strong>，我为 3d 空间图设置为 3。最好试验不同的超参数设置，以获得算法的最佳效果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h1 id="9932" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— UMAP— 2D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="3d98" class="nh lx it oy b gy pc pd l pe pf">plot_2d(reducer.embedding_[:, 0],reducer.embedding_[:, 1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/d11b3d8c826c04b197c91245327a0348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nsg3cf-kjMs4ru0hVH2RJg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以清楚地看到，与 t-SNE 和 PCA 相比，UMAP 在分离数据点方面做得非常好。然而，从 2d 的角度来看，没有大的聚类充分地分隔符号，在其他部分也有相似的数据点聚集在一起。</p><h1 id="f834" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— UMAP— 3D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="b9b2" class="nh lx it oy b gy pc pd l pe pf">plot_3d(reducer.embedding_[:, 0],reducer.embedding_[:, 1],reducer.embedding_[:, 2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/2105574d1ad7e362c7e25824d3332d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*PzEAM3EnbZWM-SUpkkpcdQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="dcd0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实施 LDA</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h1 id="0033" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— LDA— 2D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="d3e5" class="nh lx it oy b gy pc pd l pe pf">plot_2d(X_LDA[:, 0],X_LDA[:, 1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/ef140606f53d01ea1c7c92da92abfa72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sa_phhAbPSEhZLinn3Q7eg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用 LDA，我们可以清楚地识别出这九个具有显著分离的集群的存在。如果使用 UMAP 和 SNE 霸王龙，我们几乎看不到聚类的主干，而使用 LDA，我们可以看到聚集在相同聚类区域的数据点的整个聚类。</p><h1 id="8f9a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">— LDA— 3D —</h1><pre class="kj kk kl km gt ox oy oz pa aw pb bi"><span id="5147" class="nh lx it oy b gy pc pd l pe pf">plot_3d(X_LDA[:, 0],X_LDA[:, 1],X_LDA[:, 2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4e5c4042b9c8499d976a5daa50256aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*X6Ex3mJhulnUZ9a_sx779w.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="f16a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">降维技术的比较:主成分分析 vs t-SNE vs UMAP vs LDA</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/d0b665b37050859671c8b3c443031674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pi5lKwCFkLrl7EcqvxeJ8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/cff9a7b993c029ea130921f968294894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRoH-5_xo85OM92927yjUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">速度比较</p></figure><p id="f531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意:上表是考虑到 Kaggle 上使用 GPU 的内核的计算时间而构建的。</em></p><ul class=""><li id="67d5" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">PCA 不能很好地区分这些信号。PCA 的主要缺点是受数据中异常值的影响很大。PCA 是一种<strong class="lb iu">线性投影</strong>，这意味着它不能捕捉非线性依赖关系，它的目标是找到使数据集中的方差最大化的方向(所谓的主成分)。</li><li id="ea5d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">与<strong class="lb iu"> PCA </strong>相比，t-SNE 在可视化集群的不同模式方面做得更好(它试图保持拓扑邻域结构)。相似的标签被聚集在一起，即使有大量的数据点在彼此之上，当然也不足以期望聚类算法表现良好。</li><li id="e240" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> UMAP </strong>的表现优于<strong class="lb iu"> t-SNE </strong>和<strong class="lb iu"> PCA </strong>，如果我们观察 2d 和 3d 图，我们可以看到被很好分离的迷你星团。这对于可视化集群或<strong class="lb iu">组数据点及其相对接近度</strong>非常有效。然而，对于这个用例来说，肯定没有好到期望一个聚类算法来区分模式。<strong class="lb iu"> UMAP </strong>比<strong class="lb iu"> t-SNE </strong>快得多，后者面临的另一个问题是需要<em class="lv">另一种降维方法优先，否则计算时间会更长。</em></li><li id="5250" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">最后，<strong class="lb iu"> LDA </strong>在各方面都超过了之前所有的技术。出色的计算时间(第二快)以及证明我们所期望的分离良好的集群。</li></ul><blockquote class="pm pn po"><p id="4b80" class="kz la lv lb b lc ld ju le lf lg jx lh pp lj lk ll pq ln lo lp pr lr ls lt lu im bi translated">注意:LDA 比其他技术表现得更好并不奇怪，这正是我们所期望的。与 LDA 不同，PCA、TSNE 和 UMAP 是在不知道真实类别标签的情况下执行的。</p></blockquote><h1 id="7a97" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">总结</strong></h1><p id="aa80" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们探索了四种用于数据可视化的降维技术:(PCA、t-SNE、UMAP、LDA)并尝试使用它们在 2d 和 3d 图中可视化高维数据集。</p><blockquote class="nt"><p id="9ba2" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated">注意:很容易陷入认为一种技术比另一种技术更好的陷阱，最终没有办法将高维数据映射到低维，同时保留整个结构，总有一种技术与另一种技术相比的质量权衡。</p></blockquote><p id="d496" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">再次感谢到达这里，希望这是一个信息丰富的职位！值得你花时间。还有许多其他变体和其他用例，我强烈鼓励您探索这一令人惊叹且发展良好的科学领域。</p><h1 id="8eb0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><p id="b1f4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1]麦金尼斯和希利(2018 年)。UMAP:一致流形逼近和降维投影。ArXiv 电子印花。</p><p id="25c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]范德马滕，l . j . p . t-分布式随机邻居嵌入</p><div class="ps pt gp gr pu pv"><a href="https://lvdmaaten.github.io/tsne/" rel="noopener  ugc nofollow" target="_blank"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">t-SNE</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">t 分布随机近邻嵌入(t-SNE)是一种降维技术，特别适用于</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">lvd maten . github . io</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj ks pv"/></div></div></a></div><p id="9a61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]Kaggle.com. 2020。与 T-SNE 一起观想卡纳达语 MNIST。可在:<a class="ae ky" href="https://www.kaggle.com/parulpandey/visualizing-kannada-mnist-with-t-sne" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/parulpandey/visualizing-kannada-Mn ist-with-t-SNE</a>获取</p><p id="82b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]通过 Scikit-Learn、Keras &amp; Tensorflow 实践机器学习，作者 Aurelien Geron</p><p id="5e69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5]https://sebastianraschka.com/Articles/2014_python_lda.html<a class="ae ky" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>