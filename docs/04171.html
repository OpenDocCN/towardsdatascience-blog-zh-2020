<html>
<head>
<title>Deep Reinforcement Learning and Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习和超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-and-hyperparameter-tuning-df9bf48e4bd2?source=collection_archive---------14-----------------------#2020-04-16">https://towardsdatascience.com/deep-reinforcement-learning-and-hyperparameter-tuning-df9bf48e4bd2?source=collection_archive---------14-----------------------#2020-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b3a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用雷的曲子优化你的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9fcb97abf8d125930eab28c52c730150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bdnd8Y6fv8xeFqNuXvSeNA.png"/></div></div></figure><p id="8cff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度强化学习最困难和最耗时的部分之一是超参数的优化。这些值(如折扣系数[latex]\gamma[/latex]或学习率)会对代理的性能产生重大影响。</p><p id="c5f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代理需要接受培训，以了解超参数如何影响绩效——没有先验的<em class="lq">方法来知道给定参数的更高或更低的值是否会提高总报酬。除了跟踪实验、数据和与训练模型相关的一切之外，这转化为多次昂贵的训练运行以获得好的代理。</em></p><p id="c233" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lr" href="https://www.datahubbs.com/ray-and-rllib-fast-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"> Ray </a>提供了一种使用Tune library处理所有这些的方法，它可以自动处理您的各种模型，保存数据，调整您的超参数，并总结结果以便快速轻松地参考。</p><h1 id="d0b7" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="1a28" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们通过一个简单的例子来说明如何使用Tune的网格搜索特性来优化我们的超参数。</p><h1 id="4f23" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">安装Tune</h1><p id="e7dd" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">Tune是Ray项目的一部分，但是需要单独安装，所以如果您还没有安装它，您需要运行下面的命令来让Tune工作。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="e337" class="mu lt it mq b gy mv mw l mx my">pip install ray[tune]</span></pre><p id="6a2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从这里，我们可以导入我们的包来训练我们的模型。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="b270" class="mu lt it mq b gy mv mw l mx my">import ray<br/>from ray import tune</span></pre><h1 id="9ca0" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">调整您的第一个模型</h1><p id="bdcf" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">从基础开始，让我们用Tune训练一个agent来解决<code class="fe mz na nb mq b">CartPole-v0</code>。Tune需要一些具有不同设置和标准的字典来训练。它必须具有的两个参数是<code class="fe mz na nb mq b">config</code>和<code class="fe mz na nb mq b">stop</code>。</p><p id="1283" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mz na nb mq b">config</code>字典将为调优提供它需要运行的环境，以及您可能想要指定的任何特定于环境的配置。这也是你的大部分超参数将要驻留的地方，但是我们一会儿会讲到。</p><p id="1873" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mz na nb mq b">stop</code>字典告诉Tune什么时候结束训练或者什么时候完全停止训练。它可以根据奖励标准、经过的时间、完成的步数等进行定制。当我第一次开始使用Tune时，我忽略了设置任何停止标准，最终让算法训练了几个小时才意识到这一点。所以，你可以不用这个来运行它，但是如果你不小心的话，你可能会得到一个不错的AWS账单！</p><p id="ee50" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尝试使用以下代码在<code class="fe mz na nb mq b">CartPole-v0</code>上运行PPO算法10，000个时间步长。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="8903" class="mu lt it mq b gy mv mw l mx my">ray.init(ignore_reinit_error=True)<br/>config = {<br/>    'env': 'CartPole-v0'<br/>}<br/>stop = {<br/>    'timesteps_total': 10000<br/>}<br/>results = tune.run(<br/>    'PPO', # Specify the algorithm to train<br/>    config=config,<br/>    stop=stop<br/>)</span></pre><p id="c632" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过这些设置，您应该可以看到您的工作人员、内存以及<code class="fe mz na nb mq b">logdir</code>状态的打印输出，所有数据都存储在这里以供以后分析。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/31fe5c23cd3331a16bb20c8773b9914b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*BAAWkNBPPdVtcpnEb0TuEw.png"/></div></figure><p id="9e75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">控制台将在每次迭代中打印这些值，除非<code class="fe mz na nb mq b">tune.run()</code>中的<code class="fe mz na nb mq b">verbose</code>参数被设置为0(无声)。</p><p id="143d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当训练完成时，你会得到一个输出，显示状态已经终止，经过的时间，以及过去100集的平均奖励和其他数据。</p><h1 id="f81e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">使用网格搜索调整超参数</h1><p id="9d7c" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">当我们利用它来调整我们的超参数时，Tune的力量就真正发挥出来了。为此，我们将求助于<code class="fe mz na nb mq b">grid_search</code>函数，它允许用户为要测试的模型指定一组超参数。</p><p id="e0a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为此，我们只需要在<code class="fe mz na nb mq b">tune.grid_search()</code>函数中包装一个值列表，并将其放入我们的配置字典中。让我们回到上面的<code class="fe mz na nb mq b">CartPole</code>例子。我们可能想看看学习速度是否有什么不同，双头网络是否有什么好处。我们可以使用<code class="fe mz na nb mq b">grid_search()</code>来实现这些的不同组合，如下所示:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="e832" class="mu lt it mq b gy mv mw l mx my">config = {<br/>    "env": 'CartPole-v0',<br/>    "num_workers": 2,<br/>    "vf_share_layers": tune.grid_search([True, False]),<br/>    "lr": tune.grid_search([1e-4, 1e-5, 1e-6]),<br/>    }<br/>results = tune.run(<br/>    'PPO', <br/>    stop={<br/>        'timesteps_total': 100000<br/>    },<br/>    config=config)</span></pre><p id="b036" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们看到一个展开的状态打印输出，其中包含我们想要运行的各种试验:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/282701e9fbf344f3738768dd703cb30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XLkHqAZg66KtXb_nc1FL2w.png"/></div></div></figure><p id="cbda" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当Ray启动其中的每一个时，它将显示我们想要探索的超参数的组合，以及每个超参数的回报、迭代和运行时间。当它完成时，我们应该看到每个的状态为终止，以表明它正常工作(否则它将读取错误)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7dc4b010c3a799e680dab5e420888d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*f-4fdo3GxMRgwFedAo8j_Q.png"/></div></figure><h1 id="e042" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">分析调节结果</h1><p id="db0e" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们的<code class="fe mz na nb mq b">tune.run()</code>函数的输出是一个被我们标记为<code class="fe mz na nb mq b">results</code>的<code class="fe mz na nb mq b">analysis</code>对象。我们可以利用这一点来进一步了解我们实验的细节。可以通过<code class="fe mz na nb mq b">results.dataframe()</code>访问相关数据，这将返回一个Pandas数据帧，其中包含平均奖励、迭代次数、KL散度、配置设置等等。数据框还包含保存您的实验的特定目录(<code class="fe mz na nb mq b">logdir</code>)，因此您可以了解您的特定运行的详细信息。</p><p id="370d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您查看<code class="fe mz na nb mq b">logdir</code>目录，您会发现许多包含您的训练运行中保存的数据的文件。出于我们的目的，主文件将是<code class="fe mz na nb mq b">progress.csv</code>——它包含来自每个迭代的训练数据，允许您深入细节。</p><p id="58c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，如果我们想要查看不同设置的训练和损耗曲线，我们可以循环遍历数据框中的<code class="fe mz na nb mq b">logdir</code>列，加载每个<code class="fe mz na nb mq b">progress.csv</code>文件并绘制结果。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="5af9" class="mu lt it mq b gy mv mw l mx my"># Plot training results<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="4e67" class="mu lt it mq b gy nf mw l mx my">colors = plt.rcParams['axes.prop_cycle'].by_key()['color']<br/>df = results.dataframe()</span><span id="cc7c" class="mu lt it mq b gy nf mw l mx my"># Get column for total loss, policy loss, and value loss<br/>tl_col = [i for i, j in enumerate(df.columns)<br/>          if 'total_loss' in j][0]<br/>pl_col = [i for i, j in enumerate(df.columns)<br/>          if 'policy_loss' in j][0]<br/>vl_col = [i for i, j in enumerate(df.columns)<br/>          if 'vf_loss' in j][0]<br/>labels = []<br/>fig, ax = plt.subplots(2, 2, figsize=(15, 15), sharex=True)<br/>for i, path in df['logdir'].iteritems():<br/>    data = pd.read_csv(path + '/progress.csv')<br/>    # Get labels for legend<br/>    lr = data['experiment_tag'][0].split('=')[1].split(',')[0]<br/>    layers = data['experiment_tag'][0].split('=')[-1]<br/>    labels.append('LR={}; Shared Layers={}'.format(lr, layers))<br/>    <br/>    ax[0, 0].plot(data['timesteps_total'], <br/>            data['episode_reward_mean'], c=colors[i],<br/>            label=labels[-1])<br/>    <br/>    ax[0, 1].plot(data['timesteps_total'], <br/>           data.iloc[:, tl_col], c=colors[i],<br/>           label=labels[-1])<br/>    <br/>    ax[1, 0].plot(data['timesteps_total'], <br/>               data.iloc[:, pl_col], c=colors[i],<br/>               label=labels[-1])<br/>    <br/>    <br/>    ax[1, 1].plot(data['timesteps_total'], <br/>               data.iloc[:, vl_col], c=colors[i],<br/>               label=labels[-1])</span><span id="9e88" class="mu lt it mq b gy nf mw l mx my">ax[0, 0].set_ylabel('Mean Rewards')<br/>ax[0, 0].set_title('Training Rewards by Time Step')<br/>ax[0, 0].legend(labels=labels, loc='upper center',<br/>        ncol=3, bbox_to_anchor=[0.75, 1.2])<br/></span><span id="743b" class="mu lt it mq b gy nf mw l mx my">ax[0, 1].set_title('Total Loss by Time Step')<br/>ax[0, 1].set_ylabel('Total Loss')<br/>ax[0, 1].set_xlabel('Training Episodes')</span><span id="e14f" class="mu lt it mq b gy nf mw l mx my">ax[1, 0].set_title('Policy Loss by Time Step')<br/>ax[1, 0].set_ylabel('Policy Loss')<br/>ax[1, 0].set_xlabel('Time Step')</span><span id="3c12" class="mu lt it mq b gy nf mw l mx my">ax[1, 1].set_title('Value Loss by Time Step')<br/>ax[1, 1].set_ylabel('Value Loss')<br/>ax[1, 1].set_xlabel('Time Step')</span><span id="b184" class="mu lt it mq b gy nf mw l mx my">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/7b8264fd48f71761352c9a085b795586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5FfAImjyL2La4JkkaCu9g.png"/></div></div></figure><h1 id="6f19" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">超越网格搜索</h1><p id="1d46" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">Tune中提供了更多的调优选项。如果你想看看你能调整什么，看看你的特定算法的<a class="ae lr" href="https://ray.readthedocs.io/en/latest/rllib-algorithms.html" rel="noopener ugc nofollow" target="_blank">文档。此外，Tune支持不同的超参数优化方法。网格搜索可能会很慢，所以只需改变几个选项，就可以使用贝叶斯优化、HyperOpt等。最后，Tune使得<strong class="kw iu">基于人口的培训</strong> (PBT)变得容易，允许多个代理跨不同的机器伸缩。所有这些都将在以后的帖子中介绍！</a></p></div></div>    
</body>
</html>