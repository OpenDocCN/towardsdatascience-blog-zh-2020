<html>
<head>
<title>Understand the machine learning Blackbox with ML-interpreter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用ML解释器理解机器学习黑盒</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f?source=collection_archive---------28-----------------------#2020-03-23">https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f?source=collection_archive---------28-----------------------#2020-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="14be" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个自动解释XGBoost等算法决策的web应用程序</h2></div><p id="9ee3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让模型来管理这个世界并做出从雇佣到刑事司法的决策是有危险的。虽然拥有既可解释又准确的模型是理想的，但许多流行且强大的算法仍然是黑盒。</p><p id="ff1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中有高性能的树集成模型，如lightGBM，XGBoost，random forest。了解他们的内部运作会带来很多好处，包括透明、信任、合规和公平。否则，人们可能不得不求助于更易解释的原始模型。</p><p id="58b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了让解释更容易，我构建了一个web应用<a class="ae lb" href="http://ml-interpret.herokuapp.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> ML-interpreter </strong> </a>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/4be8c5eb787161895eaee821a755c786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7XJcYznW1pEpIXv1vuqsgg.gif"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd ls">查看此处</strong><a class="ae lb" href="http://ml-interpret.herokuapp.com" rel="noopener ugc nofollow" target="_blank"><strong class="bd ls"/></a><strong class="bd ls">。</strong></p></figure><p id="d7b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在深入详细的功能之前，这里有一个关于可解释性的初级读本。</p><h1 id="b14c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">解释框架的简要概述</h1><p id="797b" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">现在有很多可解释性框架，包括Python包<a class="ae lb" href="https://lime-ml.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LIME </a>、<a class="ae lb" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a>、<a class="ae lb" href="https://github.com/TeamHG-Memex/eli5" rel="noopener ugc nofollow" target="_blank"> ELI5 </a>、<a class="ae lb" href="https://pypi.org/project/skater/" rel="noopener ugc nofollow" target="_blank"> Skater </a>，以及不太知名的如<a class="ae lb" href="https://pypi.org/project/alibi/" rel="noopener ugc nofollow" target="_blank"> ALIBI </a>、R包<a class="ae lb" href="http://uc-r.github.io/dalex" rel="noopener ugc nofollow" target="_blank"> Dalex </a>以及SHAP和LIME的R包装器。一开始，处理许多不同框架的机制、语法和用法可能会令人不知所措，这促使我开发了这个应用程序。以下是一些关键概念。</p><p id="343d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">冲击与方向</strong></p><p id="b490" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解释一个模型，我们通常想知道(1)特性对结果的影响和(2)影响的方向。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mq"><img src="../Images/02aaf6a6bf41b407579484f1de93b9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VAjPgYuvDtyogFBI"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图片来源:<a class="ae lb" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a></p></figure><p id="b5ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">全球对本地</strong></p><p id="b4e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还想知道:</p><ul class=""><li id="e100" class="mr ms iq kh b ki kj kl km ko mt ks mu kw mv la mw mx my mz bi translated">哪些功能总体影响最大(全局解释)</li><li id="eb39" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated">如何批准每个决策点(当地解释)</li></ul><p id="06ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数框架既可以进行全局解释，也可以进行局部解释，有些侧重于局部层次，比如LIME。</p><p id="fe75" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">与型号无关与与型号相关</strong></p><p id="dedc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像SHAP这样的框架是模型不可知的。其他人专门研究特定的模型，如ELI5，它主要涵盖基于树的模型，但也可以用于解释sklearn线性模型和文本/图像用例。</p><h1 id="2996" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">这些框架是如何工作的</h1><p id="3d41" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><strong class="kh ir">排列重要性</strong></p><p id="9098" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于全局重要性，一种常用的方法是排列重要性。通过改变特定特性的值并观察它对模型性能的影响程度，可以推断出该特性的重要性。这种方法被认为比某些包中默认的特征重要性度量更一致。你可以在这里阅读更多关于为什么<a class="ae lb" href="https://explained.ai/rf-importance/" rel="noopener ugc nofollow" target="_blank">的内容</a>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nf"><img src="../Images/29477366f86fdfb9da05b326dda1cadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mmNaTy-KuCIIo3Zj"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图片来源:<a class="ae lb" href="https://www.kaggle.com/dansbecker/permutation-importance" rel="noopener ugc nofollow" target="_blank">ML Kaggle的可解释性</a></p></figure><p id="354d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">代理模型</strong></p><p id="a6fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于局部解释，可以训练简单的可解释模型(如决策树)作为代理，然后使用该代理模型将原始数据与作为解释目标的黑盒模型的预测进行拟合，并使用该简单模型进行解释。</p><p id="ac2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">博弈论</strong></p><p id="526f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种方法是基于博弈论的沙普利值。单个特征的影响可以加起来解释为什么预测与基线不同。你可以在这篇博客<a class="ae lb" rel="noopener" target="_blank" href="/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d">帖子</a>或这篇arXiv <a class="ae lb" href="https://arxiv.org/abs/1802.03888" rel="noopener ugc nofollow" target="_blank">论文</a>中了解更多关于SHAP的树解释器是如何工作的。</p><p id="86fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将使用app进行一些解读。</p><h1 id="3dba" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">ML解释程序</h1><p id="ea5e" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">该应用程序由4个主要部分组成:用户选择、分类报告、每个决策点的全局解释和局部解释。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ng"><img src="../Images/eb0111123c37da87c4d2f7f21c93c634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Jy9u7aeqkZ0SP0_m"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae lb" href="http://ml-interpret.herokuapp.com" rel="noopener ugc nofollow" target="_blank"> ML解释器应用</a></p></figure><p id="eee4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人们可以选择一个演示数据或更新一个小的表格csv，运行一个分类器(randomforest，XGBoost，lightGBM)，并在可解释性框架之间进行选择。treeSHAP和ELI5，因为它们覆盖了全局和局部解释，并且很好地处理了树集合模型。</p><p id="4f40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">示例</strong></p><p id="b117" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将在UCI的成人收入数据集(包含在应用程序的演示数据中)上测试它，该数据集根据人口普查预测个人年收入超过5万美元的概率。首先我们可以选择一个内置模型(比如lightGBM)，选择一个框架(SHAP)。</p><p id="d0d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">视察模特表演</strong></p><p id="971a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以从查看<strong class="kh ir">分类报告</strong>和混淆矩阵开始。请注意，这是没有任何数据清理或特征工程的输出。因此，如果预先对数据进行预处理，比如删除零方差或id列(如果有的话)，结果可能会更好。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f90e05bcddedb80e77211262dbf1636c.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/0*alYDoQdKLBUF68rF"/></div></figure><p id="cfc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">理解全局解释</strong></p><p id="0b4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们对模型的结果满意后，我们可以检查<strong class="kh ir">最重要的特征</strong>并发现关系、教育和年龄对收入的影响最大。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ni"><img src="../Images/40aed68ce13fea52bed2b9a23fbd836c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TLkaocblbh6ZhrNv"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">全局解释</p></figure><p id="e235" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">放大局部解释</strong></p><p id="74f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，为了检查这些特征如何影响收入，我们可以使用滑块放大到<strong class="kh ir">单个数据点</strong>。你可以在应用中阅读更多关于如何阅读这个情节的信息，或者点击<a class="ae lb" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nj"><img src="../Images/f172fc730731f5d259555db37544e8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xS6NP3E8CEgBRJIT"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">示例1</p></figure><p id="2673" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个例子1中，这个人是一个21岁的人，每周只工作8个小时，他被正确地归入低收入类别。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nk"><img src="../Images/442dd0725cc02ce30c7f4669abe9fedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c5jZMmbbua7LJ0dU"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">示例2</p></figure><p id="4ad7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在另一个例子2中，被归类为高收入预算的人工作了相当长的时间，已婚并具有高学历，但是位置似乎降低了机会。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nj"><img src="../Images/e3c6ea20983bf7367e13151fd09d99ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K1qaW1tmt0bhO2T2"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">示例3</p></figure><p id="ad86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后一个例子中，这个人工作了相当长的时间，但是许多因素都有负面影响。人们可以从最长的箭头看到最重要的因素。如果这个因素不合理，就应该重新处理数据，找到混淆因素或者进行调试。</p><p id="f4f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们转而使用ELI5框架，我们可以以表格形式查看不同的视图，总结出对个人决策影响最大的因素。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f468f6d0fadf9813a2bc29d455b489cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*w4C0KXtPJ475WKzT6R2NPA.png"/></div></figure><p id="249b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个自动解释的应用程序，它运行一个选择的算法，显示模型的性能，并指出是什么使模型进行预测。</p><p id="a63d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对如何改进它有任何想法/建议，这里有一个<a class="ae lb" href="https://docs.google.com/forms/d/e/1FAIpQLSdTXKpMPC0-TmWf2ngU9A0sokH5Z0m-QazSPBIZyZ2AbXIBug/viewform" rel="noopener ugc nofollow" target="_blank">反馈</a>表格和到演示<a class="ae lb" href="http://ml-interpret.herokuapp.com" rel="noopener ugc nofollow" target="_blank">应用</a>和<a class="ae lb" href="https://github.com/yanhann10/ml_interpret" rel="noopener ugc nofollow" target="_blank"> Github </a>的链接。</p><h2 id="c4fe" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mf ks nt nu mh kw nv nw mj nx bi translated">一路上的挑战</h2><ol class=""><li id="d305" class="mr ms iq kh b ki ml kl mm ko ny ks nz kw oa la ob mx my mz bi translated">速度</li></ol><p id="f8a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建这一框架的主要挑战是可解释性框架会变得计算密集型。像SHAP这样的软件包有C++加速，而其他的需要更长的计算时间。一些分类器也比其他分类器花费更长的时间，数据大小加剧了这种情况，尽管人们可以首先尝试数据的样本。</p><p id="315d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最初，我将PDPbox中的PDP/ICE图作为图之一，显示每个特性如何随结果变化，但在其他计算之上呈现之前计算20秒的图表并不理想，所以最终我将它作为可选图表移动，可以通过选择复选框按需查看。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oc"><img src="../Images/829310ed23f14e60a7c63a9b506e3355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IID9Mz4Ndl2Sa-ECR5ATJQ.png"/></div></div></figure><p id="d5da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个选择是<a class="ae lb" href="https://christophm.github.io/interpretable-ml-book/ale.html" rel="noopener ugc nofollow" target="_blank"> ALEplot </a>(最初是一个R包)，它应该更快更好(可以处理PDP不能处理的相关特性)，当其羽翼未丰的python <a class="ae lb" href="https://github.com/blent-ai/ALEPython" rel="noopener ugc nofollow" target="_blank">版本</a>稳定下来时，这可能会很方便。SHAP也有自己版本的部分相关图，显示散点图中的每个数据点，但有时数据点只是相互遮挡(尽管其局部解释图超级棒)。</p><p id="1ab9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.翻译口译员</p><p id="d3eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个挑战是这些框架没有一个是完全不言自明的。感谢几位ML/DS人员的反馈，他们友好地帮助了初始测试，我在每个部分添加了<em class="od">操作</em>按钮和链接来详细解释输出。</p><h2 id="a526" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mf ks nt nu mh kw nv nw mj nx bi translated">学习</h2><ul class=""><li id="a34e" class="mr ms iq kh b ki ml kl mm ko ny ks nz kw oa la mw mx my mz bi translated">我开始理解不同可解释性框架的复杂性。虽然重点是用树集成模型进行分类，但这些框架中的许多可以应用于更广泛的用例，包括回归、文本和图像。拥有黑盒中的可解释性可以为我们提供许多见解，但它可能不是万灵药，仍然需要谨慎使用。</li><li id="1702" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated">通过这个项目，我使用了Atair进行绘图，并发现它的声明性语法非常简洁，这让我想起了ggplot2中的图形语法。现在为了保持功能简单，我没有用图表过载它。</li><li id="7859" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated">使用<a class="ae lb" href="https://www.streamlit.io" rel="noopener ugc nofollow" target="_blank">构建和部署ML应用程序简化了</a>。我最近在这里和这里写了详细的操作方法<a class="ae lb" href="https://dev.to/hannahyan/getting-started-in-building-and-deploying-interactive-data-science-apps-with-streamlit-6ab" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><h1 id="a9ac" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">后续步骤</h1><p id="3004" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><strong class="kh ir">口译组</strong></p><p id="4778" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与重要特性的俯视图相比，理解单个决策点是相当大的进步。然而，更有趣和实用的是了解模型对特定群体的预测，基于输入要素或输出类别进行聚类，而交互式应用程序是实现这一点的好方法。我还没有找到一个适合解释集群的框架。这对于评估模型的公平性也有积极的意义。</p><h1 id="5c91" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">反光</h1><p id="c24a" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在学习这些框架时，我注意到它们在架构和实现上有多么不同。例如，Eli5以(1)表格数据框格式和(2)图表两种形式提供结果，尽管它们的图表也是彩色的HTML表格。SHAP的方法相当不同——它直接生成高度紧凑的可视化。但是，您可以运行一些脚本来获得数据框格式的输出。</p><p id="9303" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些框架都可以归结为两个部分——解释算法(T0)和视觉表现(T2)两部分。他们的大多数图表都有相同的目标，比如特性重要性或部分相关性，但他们都使用不同的图表来呈现相同的见解。对于新用户来说，在解释释义时转换齿轮并不那么容易。我开始想，将解释算法和它们的视觉表示分离开来，并拥有一种<strong class="kh ir">通用设计语言</strong>用于机器学习解释是否会更好，这种语言与框架无关，并且可以使交流更容易。</p><h1 id="1938" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">资源</strong></h1><p id="b732" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我参考了这些文章/书籍，发现它们信息量很大:</p><p id="9eb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://www.kaggle.com/learn/machine-learning-explainability" rel="noopener ugc nofollow" target="_blank">机器学习的可解释性</a>由Kaggle</p><p id="a506" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://www.oreilly.com/radar/ideas-on-interpreting-machine-learning/" rel="noopener ugc nofollow" target="_blank">关于解释机器学习的想法</a>奥赖利</p><p id="de76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">克里斯托弗·莫尔纳尔<em class="od">撰写的可解释的ML书</em></a></p><p id="43f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf" rel="noopener ugc nofollow" target="_blank">机器学习可解释性介绍</a>作者Patrick Hall和Navdeep Gill在H2o</p><p id="b69d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，请分享。建议或反馈总是受欢迎的。</p></div></div>    
</body>
</html>