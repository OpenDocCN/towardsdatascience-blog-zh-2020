<html>
<head>
<title>YOLO — You Only Look Once</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO——你只能看一次</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yolo-you-only-look-once-3dbdbb608ec4?source=collection_archive---------2-----------------------#2020-05-30">https://towardsdatascience.com/yolo-you-only-look-once-3dbdbb608ec4?source=collection_archive---------2-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0369" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种实时目标检测系统的算法</h2></div><h1 id="3029" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="277f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们借助眼睛看到一切，它捕捉框架中的信息，并将其发送到我们的大脑，以解码并从中得出有意义的推论。听起来很简单，对吧？我们只是看着，就能理解我们正在看的所有物体是什么，它们是如何放置的，以及关于它们的大量其他信息。但是大脑的处理能力是无与伦比的。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/f8142e053e9ddb34fdb4d9e574a74e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxUACIk1b_5kSXrdby2Zpg.jpeg"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">图片来源:<a class="ae mm" href="https://unsplash.com/@victorfreitas" rel="noopener ugc nofollow" target="_blank">维克多·弗雷塔斯</a>，Unsplash.com</p></figure><p id="46be" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">大脑这种有趣的能力让研究人员想到，如果我们能把这种能力赋予一台机器，会怎么样呢？有了这个，机器的任务将会变得简单得多，一旦它能够识别它周围的物体，它就能更好地与它们互动，这就是改进机器的全部目的，使它们对人类更友好，使它们更像人类。</p><p id="655d" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">在这个过程中，有一个很大的障碍。我们如何让机器识别一个物体？这就是计算机视觉领域的起源，我们称之为<strong class="lc iu">“物体检测”</strong>。对象检测是计算机视觉和图像处理的一个领域，它处理各种类型的对象(如人、书、椅子、汽车、公共汽车等)的检测。)在数字捕获的图像或视频中。</p><p id="4b3d" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">这个领域进一步划分为子领域，如人脸检测、活动识别、图像注释等等。目标检测在许多重要领域都有应用，如自动驾驶汽车、机器人、视频监控、目标跟踪等。</p><h1 id="d484" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">挑战</h1><h2 id="fd57" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">1.可变数量的对象</h2><p id="f6d4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">目标检测是对图像中数量可变的目标进行定位和分类的问题。重要的是<strong class="lc iu">【变量】</strong>部分。要检测的对象的数量可能因图像而异。因此，与此相关的主要问题是，在机器学习模型中，我们通常需要用固定大小的向量来表示数据。由于我们事先不知道图像中对象的数量，因此我们不知道输出的正确数量，并且我们可能需要一些后处理，这增加了复杂性。</p><h2 id="c297" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">2.多种空间比例和纵横比</h2><p id="2781" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">图像中的对象具有多种空间比例和长宽比，可能有一些对象覆盖了图像的大部分，但也有一些我们可能想要找到，但只有十几个像素(或图像的很小一部分)。即使是相同的物体，在不同的图像中也会有不同的比例。这些不同尺寸的物体给追踪它们带来了困难。一些算法为此使用了滑动窗口的概念，但是这是非常低效的。</p><h2 id="9a75" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">3.建模</h2><p id="48c7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">进行目标检测需要同时解决两种方法-目标检测和目标定位。我们不仅想对物体进行分类，还想在图像中找到它的位置。为了解决这些问题，大多数研究使用多任务损失函数来惩罚误分类错误和定位错误。由于损失函数的这种双重性，很多时候它在两方面都表现不佳。</p><h2 id="57a4" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">4.有限的数据</h2><p id="fda3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当前可用于对象检测的有限数量的注释数据是该过程中的另一个障碍。对象检测数据集通常包含大约十几到一百个类别的注释示例，而图像分类数据集可以包括多达100，000个类别。为每个类收集基本事实标签和边界框仍然是一项非常繁琐的任务。</p><h2 id="6ddb" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">5.实时检测的速度</h2><p id="a39a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对象检测算法不仅需要准确地预测对象的类别及其位置，还需要以令人难以置信的速度完成所有这些工作，以满足视频处理的实时需求。通常，一个视频以大约24帧/秒的速度拍摄，建立一个可以达到这种帧率的算法是一项非常困难的任务。</p><h1 id="2409" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">各种方法</h1><p id="f3a4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们考虑了对象检测领域的一些主要挑战，它们意味着什么以及它们如何影响过程。现在，我们先来看看一些试图解决这些挑战的模型，然后再揭晓其中的最佳模型——<strong class="lc iu">YOLO算法。</strong></p><h2 id="92cd" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">1.快速R-CNN</h2><p id="2985" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">快速R-CNN是R-CNN的改进版本，但它存在多级流水线环境、空间和时间开销大、对象检测速度慢等缺点。为了消除它们，快速R-CNN引入了一种新的结构。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b3e01379a0ddc86edc215eca2c6176a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*28yyoJU7aTGRHGOEIdB5WQ.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">快速R-CNN架构，来源:<a class="ae mm" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a></p></figure><p id="f266" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">它将整个图像与对象建议一起作为输入。最初，该算法在输入图像上运行CNN，并通过使用各种conv和最大汇集层来形成特征地图。之后，对于每个对象提议，感兴趣区域(RoI)汇集层提取固定长度的特征向量，并将其输入到全连接(FC)层。这一层进一步分支成两个输出层:一层为每个类以及“背景”类产生SoftMax概率，另一层为每个类输出四个实数，这四个实数定义了该特定类的边界框。</p><h2 id="6033" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">2.单触发多盒探测器(SSD)</h2><p id="ba7a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">SSD工作在自由向前卷积层的方法上，该方法输出固定大小的边界框集合以及要出现在这些边界框中的对象类实例的分数。它还使用非最大抑制来产生最终决策。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e24d6abbf04b0443400a508c8fbfa106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*ndKxS2xFmkv_wwN0HYr12w.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">SSD架构，来源:<a class="ae mm" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a></p></figure><p id="a8c2" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">固态硬盘的架构非常简单。模型中的初始层是用于图像分类的标准ConvNet层，用他们的术语来说就是基本网络，在这个基本网络的基础上，他们然后添加一些辅助层来产生检测，同时牢记多尺度特征地图、默认框和纵横比。</p><h2 id="1652" class="ms kj it bd kk mt mu dn ko mv mw dp ks lj mx my ku ln mz na kw lr nb nc ky nd bi translated">3.视网膜网</h2><p id="f417" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Retina-Net是一个单一的统一网络，由一个主干网络和两个特定任务的子网组成。主干负责计算整个输入图像上的conv特征图，并且是脱离自卷积网络。第一子网对骨干网输出进行分类；第二个子网执行卷积包围盒回归。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ng"><img src="../Images/03e14c9de952d7c5c21d7e2f20f4b809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWFiSybIV4KyaZ7UbY7lXg.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">视网膜网络架构，来源:<a class="ae mm" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体探测的焦点损失</a></p></figure><p id="4117" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">它在前馈ResNet架构之上使用特征金字塔网络(FPN)主干来生成丰富的多尺度卷积特征金字塔，然后将其馈送到两个子网，其中一个对锚盒进行分类，另一个执行从锚盒到地面真实锚盒的回归。</p><h1 id="073b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">YOLO算法</h1><p id="30d1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">到目前为止，我们已经看到了一些非常著名且性能良好的对象检测架构。所有这些算法都解决了本文开头提到的一些问题，但是没有解决最重要的一个问题——实时目标检测的速度。</p><p id="472e" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">YOLO算法在我们讨论的所有参数上都给出了更好的性能，并且具有实时使用的高fps。YOLO算法是一种基于回归的算法，它不是选择图像中感兴趣的部分，而是在<strong class="lc iu">算法的一次运行中预测整个图像的类别和包围盒。</strong></p><p id="29d8" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">为了理解YOLO算法，首先我们需要理解实际预测的是什么。最终，我们的目标是预测对象的类别和指定对象位置的包围盒。每个边界框可以用四个描述符来描述:</p><ol class=""><li id="8747" class="nh ni it lc b ld mn lg mo lj nj ln nk lr nl lv nm nn no np bi translated">盒子中心(<strong class="lc iu"> bx，by </strong>)</li><li id="b89b" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">宽度(<strong class="lc iu"> bw </strong>)</li><li id="72ec" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">高度(<strong class="lc iu"> bh </strong>)</li><li id="2509" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">对应于对象类别的值<strong class="lc iu"> c </strong></li></ol><p id="d724" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">与此同时，我们预测一个实数<strong class="lc iu"> pc </strong>，它是在边界框中有一个对象的概率。</p><p id="27ea" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">YOLO不会在输入图像中搜索可能包含对象的感兴趣区域，而是将图像分割成单元，通常是19x19的网格。然后，每个单元负责预测K个边界框。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nv"><img src="../Images/0a98f66a53a0de4b0e0d2ed342bf6ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qT8BC1tSNs-jXKJl5efOWg.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">这里我们取K=5，预测80类的可能性</p></figure><p id="d32c" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">只有当锚定框的中心坐标位于特定单元中时，才认为对象位于该单元中。由于这一特性，中心坐标总是相对于单元计算，而高度和宽度是相对于整个图像尺寸计算的。</p><p id="c1c3" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">在前向传播的一个过程中，YOLO确定单元包含某个类的概率。同样的等式是:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/93c177336cb6bca51b8052ccf3b77663.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*wzfMExgN-2yWXKexqLEmgg.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">存在某个“c”类对象的概率</p></figure><p id="2f60" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">具有最大概率的类被选择并分配给特定的格网单元。对于图像中存在的所有网格单元，发生类似的过程。</p><p id="32fb" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">计算上述类别概率后，图像可能如下所示:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nx"><img src="../Images/2311a57e20036b0ec30b4f6708d6b34f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZkfC29ck9IqmZ7poyESVxQ.png"/></div></div></figure><p id="0aa7" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">这显示了预测每个网格单元的类概率之前和之后的情况。预测类别概率后，下一步是非最大值抑制，它有助于算法去除不必要的锚框，就像你在下图中看到的，有许多基于类别概率计算的锚框。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ad6189b11f8251d502c7bbdf3623ffa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*dVwUbY2BZa1STZyGgVtNTQ.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">锚箱</p></figure><p id="fa15" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">为了解决这个问题，非最大值抑制通过执行IoU(并集上的交集)来消除非常接近的边界框，其中IoU具有最高的类概率。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fe86607ad8721b35012b65b58e0d70d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*SQTZAlSXrXHFE_JwkGVP5w.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">IoU操作</p></figure><p id="4e71" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">它计算所有边界框的IoU值，分别对应于具有最高分类概率的边界框，然后它拒绝IoU值大于阈值的边界框。它表示这两个边界框覆盖了相同的对象，但是另一个覆盖相同对象的概率很低，因此被排除。</p><p id="25ef" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">一旦完成，算法找到具有下一个最高类概率的包围盒，并进行相同的过程，直到我们剩下所有不同的包围盒。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b1a37c69c648d64e04d809140c1cc57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*1aeze3VVzGYRrXwfEHWwJQ.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">非最大抑制之前和之后</p></figure><p id="d4fe" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">在这之后，我们几乎所有的工作都完成了，算法最终输出所需的向量，显示各个类的边界框的细节。该算法的整体架构如下所示:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ob"><img src="../Images/8d190d076f2b16ad35d2d96c7db81602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfrxbwxAGPvHnHVzFMb5Vw.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">YOLO架构，来源:<a class="ae mm" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测</a></p></figure><p id="bf69" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">此外，该算法最重要的参数，其损失函数如下所示。YOLO同时获知它预测的所有四个参数(如上所述)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/98f817fa5c2f063e519e44d269d9bd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*xQVVamSDs4XM4A7l5tJJ2Q.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">YOLO损失函数，来源:<a class="ae mm" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测</a></p></figure><p id="5be5" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">所以这都是关于YOLO算法的。我们讨论了对象检测的所有方面以及我们在该领域面临的挑战。然后，我们看到了一些试图解决这些挑战的算法，但在最关键的实时检测(fps速度)方面失败了。然后，我们研究了YOLO算法，它在面临的挑战方面优于所有其他模型，它的快速性可以在实时对象检测中很好地工作，遵循回归方法。</p><p id="1124" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">算法仍在改进中。我们目前有四代YOLO算法，从v1到v4，还有一个稍微小一点的版本YOLO-tiny，它是专门设计来实现令人难以置信的220fps的高速度。</p><p id="15c3" class="pw-post-body-paragraph la lb it lc b ld mn ju lf lg mo jx li lj mp ll lm ln mq lp lq lr mr lt lu lv im bi translated">我希望我能够清楚你对算法和物体检测相关概念的理解。如果你觉得这篇文章信息丰富，你可以在未来关注我更多这样的文章。快乐学习！</p></div></div>    
</body>
</html>