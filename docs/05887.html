<html>
<head>
<title>Word-Vectors and Semantics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">词向量和语义</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-vectors-and-semantics-2863e7e55417?source=collection_archive---------49-----------------------#2020-05-14">https://towardsdatascience.com/word-vectors-and-semantics-2863e7e55417?source=collection_archive---------49-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b00" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">观点挖掘还是情感人工智能</h2></div><p id="e1ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2vec是一个处理文本的双层神经网络。它的输入是文本语料库，输出是一组向量，这些向量本质上是该语料库中单词的特征向量。Word2vec的目的和用途是在向量空间中对相似词的向量进行分组。它用数学方法检测相似性。它创建向量，这些向量是单词特征的分布式数字表示，例如单个单词的上下文，并且它在没有人工干预的情况下完成。</p><p id="3c5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定足够的数据、用法和上下文，Word2vec可以根据单词过去的出现对其含义做出高度准确的猜测。这些猜测可以用来建立一个单词与其他单词的联系，就像男人对于男孩就像女人对于女孩一样。</p><p id="d591" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2vec根据输入语料库中与单词相邻的其他单词来训练单词。它通过两种方式实现，或者使用上下文来预测目标单词，或者使用单词来预测目标上下文，这被称为skip-gram。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/2c1ff82bef39c910f760eb2eccfc8209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0KT2249gDvrGVvtAKHWorw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">它们都是彼此相反的。</p></figure><p id="f9ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CBOW方法我们有几个输入单词，然后我们的投影基本上是试图预测在给定这些周围单词的上下文的情况下，出现的概率最高的单词是什么。另一方面，Skip-gram方法需要更长的时间来训练和开发，因为它本质上是在做相反的事情。给定使用自动编码器神经网络投影的单个单词的输入，尝试输出将在该输入单词的上下文周围出现的其他单词的加权概率</p><p id="06e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还必须记住，每个单词都由一个向量来表示。这意味着我们可以使用余弦相似度来衡量单词向量之间的相似程度</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lu"><img src="../Images/7c8db512dd58142a251824b386e85d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dmjuwu_pEb7mVWypM2dmMA.png"/></div></div></figure><h1 id="3f05" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">向量值</h1><p id="639c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">那么单词向量是什么样子的呢？因为spaCy采用300维，所以单词向量被存储为300个元素的数组。</p><p id="7336" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，我们会看到与<strong class="kk iu"> en_core_web_md </strong>和<strong class="kk iu"> en_core_web_lg </strong>相同的一组值，</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="f24b" class="mx lw it mt b gy my mz l na nb"><em class="nc"># Import spaCy and load the language library</em><br/><strong class="mt iu">import</strong> <strong class="mt iu">spacy</strong></span><span id="acee" class="mx lw it mt b gy nd mz l na nb">nlp = spacy.load('en_core_web_lg')  <br/><em class="nc"># make sure to use a larger model!</em></span><span id="f4be" class="mx lw it mt b gy nd mz l na nb">#or</span><span id="4802" class="mx lw it mt b gy nd mz l na nb"><em class="nc"># Import spaCy and load the language library</em><br/><strong class="mt iu">import</strong> <strong class="mt iu">spacy</strong></span><span id="c3a5" class="mx lw it mt b gy nd mz l na nb"> nlp = spacy.load('en_core_web_md') <br/> <em class="nc"># make sure to use a larger model!</em></span><span id="e7be" class="mx lw it mt b gy nd mz l na nb">doc = nlp(u'The quick brown fox jumped over the lazy dogs.')<br/><br/>doc.vector</span></pre><p id="7092" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将返回一个大数组。(参见GitHub链接中的输出)</p><h1 id="a4ab" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">识别相似向量</h1><p id="fc6e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">公开向量关系的最佳方式是通过。文档标记的similarity()方法。</p><p id="3ff4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们知道狮子和猫有一点相似之处，因为它们都是一个大家庭的成员。此外，猫和宠物之间也有关系，因为猫在世界各地大多作为宠物饲养</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="c6d1" class="mx lw it mt b gy my mz l na nb"><em class="nc"># Create a three-token Doc object:</em><br/>tokens = nlp(u'lion cat pet')<br/></span><span id="d9c5" class="mx lw it mt b gy nd mz l na nb"><em class="nc"># Iterate through token combinations:</em><br/><strong class="mt iu">for</strong> token1 <strong class="mt iu">in</strong> tokens:<br/>    <strong class="mt iu">for</strong> token2 <strong class="mt iu">in</strong> tokens:<br/>        print(token1.text, token2.text, token1.similarity(token2))<br/></span><span id="53c5" class="mx lw it mt b gy nd mz l na nb">Output-</span><span id="d1f9" class="mx lw it mt b gy nd mz l na nb">lion lion 1.0<br/>lion cat 0.526544<br/>lion pet 0.399238<br/>cat lion 0.526544<br/>cat cat 1.0<br/>cat pet 0.750546<br/>pet lion 0.399238<br/>pet cat 0.750546<br/>pet pet 1.0</span></pre><p id="5b5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到我们得到了一些正确的和可理解的结果。</p><h1 id="6623" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">对立面不一定是不同的</h1><p id="118d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">意思相反但经常出现在相同<em class="nc">上下文</em>中的单词可能有相似的向量。</p><p id="03ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以认为喜欢、爱和恨有完全不同的含义，但是如果我们在一个句子中一起使用它们，它是有意义的，并且模型可以识别它。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="5ca4" class="mx lw it mt b gy my mz l na nb"><em class="nc"># Create a three-token Doc object:</em><br/>tokens = nlp(u'like love hate')<br/></span><span id="21d2" class="mx lw it mt b gy nd mz l na nb"><em class="nc"># Iterate through token combinations:</em><br/><strong class="mt iu">for</strong> token1 <strong class="mt iu">in</strong> tokens:<br/>    <strong class="mt iu">for</strong> token2 <strong class="mt iu">in</strong> tokens:<br/>        print(token1.text, token2.text, token1.similarity(token2))</span><span id="a382" class="mx lw it mt b gy nd mz l na nb"><br/>Output-</span><span id="07a4" class="mx lw it mt b gy nd mz l na nb">like like 1.0<br/>like love 0.657904<br/>like hate 0.657465<br/>love like 0.657904<br/>love love 1.0<br/>love hate 0.63931<br/>hate like 0.657465<br/>hate love 0.63931<br/>hate hate 1.0</span></pre><h1 id="4fa8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">向量范数</h1><p id="1229" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">有时将300个维度聚合成一个<a class="ae ne" href="https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm" rel="noopener ugc nofollow" target="_blank">欧几里德(L2)范数</a>会很有帮助，这个范数被计算为矢量平方和的平方根。这可作为。vector_norm令牌属性。其他有用的属性包括。has_vector和is_oov或<em class="nc">出词汇</em>。</p><p id="e1b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比如我们的685k矢量库可能没有“<a class="ae ne" href="https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Magic/Nargle" rel="noopener ugc nofollow" target="_blank"> nargle </a>这个词。要测试这一点:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="19a2" class="mx lw it mt b gy my mz l na nb">tokens = nlp(u'dog cat nargle')</span><span id="80f3" class="mx lw it mt b gy nd mz l na nb"><strong class="mt iu">for</strong> token <strong class="mt iu">in</strong> tokens:<br/>    print(token.text, token.has_vector, token.vector_norm, token.is_oov)</span><span id="6690" class="mx lw it mt b gy nd mz l na nb"><br/>Output-</span><span id="5898" class="mx lw it mt b gy nd mz l na nb">dog True 7.03367 False<br/>cat True 6.68082 False<br/>nargle False 0.0 True</span></pre><p id="d726" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，我们看到“nargle”没有向量，所以vector_norm值为零，它被标识为词汇表之外的<em class="nc">。</em></p><h1 id="f443" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">向量运算</h1><p id="37cd" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">信不信由你，我们可以通过加减相关向量来计算新的向量。一个著名的例子表明</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="2618" class="mx lw it mt b gy my mz l na nb">"queen" - "woman" + "man" = "king"</span></pre><p id="5e3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来试试吧！</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="8d73" class="mx lw it mt b gy my mz l na nb"><strong class="mt iu">from</strong> <strong class="mt iu">scipy</strong> <strong class="mt iu">import</strong> spatial<br/><br/>cosine_similarity = <strong class="mt iu">lambda</strong> x, y: 1 - spatial.distance.cosine(x, y)<br/><br/>queen = nlp.vocab['queen'].vector<br/>woman = nlp.vocab['woman'].vector<br/>man = nlp.vocab['man'].vector</span><span id="7aac" class="mx lw it mt b gy nd mz l na nb"><br/><em class="nc"># Now we find the closest vector in the vocabulary to the result of "man" - "woman" + "queen"</em><br/>new_vector = queen - woman + man<br/>computed_similarities = []</span><span id="ae0b" class="mx lw it mt b gy nd mz l na nb"><br/><strong class="mt iu">for</strong> word <strong class="mt iu">in</strong> nlp.vocab:<br/>    <em class="nc"># Ignore words without vectors and mixed-case words:</em><br/>    <strong class="mt iu">if</strong> word.has_vector:<br/>        <strong class="mt iu">if</strong> word.is_lower:<br/>            <strong class="mt iu">if</strong> word.is_alpha:<br/>                similarity = cosine_similarity(new_vector, word.vector)<br/>                computed_similarities.append((word, similarity))<br/><br/>computed_similarities = sorted(computed_similarities, key=<strong class="mt iu">lambda</strong> item: -item[1])<br/><br/>print([w[0].text <strong class="mt iu">for</strong> w <strong class="mt iu">in</strong> computed_similarities[:10]])</span><span id="d7d3" class="mx lw it mt b gy nd mz l na nb">['queen', 'king', 'kings', 'queens', 'prince', 'lord', 'throne', 'royal', 'god', 'monarch']</span></pre><p id="f5ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在下面提供的我的Github库中查看完整的代码。</p><div class="nf ng gp gr nh ni"><a href="https://github.com/aditya-beri/Word-Vectors-and-Semantics.git" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">aditya-beri/词向量和语义</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">通过在GitHub上创建一个帐户，为aditya-beri/词向量和语义开发做出贡献。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw lo ni"/></div></div></a></div><h1 id="7578" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">结论</strong></h1><p id="970a" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在这篇博客中，我们学习了如何进一步进行机器学习，并试图从复杂的短语中提取出想要表达的意思。一些简单的例子包括:</p><ul class=""><li id="1d37" class="nx ny it kk b kl km ko kp kr nz kv oa kz ob ld oc od oe of bi translated">数据科学相对容易学。</li><li id="610d" class="nx ny it kk b kl og ko oh kr oi kv oj kz ok ld oc od oe of bi translated">那是我看过的最好的电影。</li></ul><p id="6b4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，像这样的短语会让事情变得更难:</p><ul class=""><li id="030c" class="nx ny it kk b kl km ko kp kr nz kv oa kz ob ld oc od oe of bi translated">我不讨厌绿鸡蛋和火腿。(需要否定处理)</li></ul><p id="837d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现的方式是通过复杂的机器学习算法，如<a class="ae ne" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>。目的是为大型语料库中的每个单词创建数字数组或<em class="nc">单词嵌入</em>。每个单词都被赋予了自己的向量，在这种方式下，在同一上下文中频繁出现的单词被赋予了相互靠近的向量。结果是一个模型可能不知道“狮子”是一种动物，但知道“狮子”在上下文中比“蒲公英”更接近“猫”。</p><p id="3372" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要注意的是<em class="nc">构建</em>有用的模型需要很长时间——训练一个大的语料库需要几个小时或几天——为了我们的目的，最好是导入一个现有的模型，而不是花时间训练我们自己的模型。</p><h1 id="d128" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">这只是对什么是语义和词向量以及它们如何工作的一个小小的窥探。<br/>如有任何疑问和澄清，请随时回复本博客。</h1></div></div>    
</body>
</html>