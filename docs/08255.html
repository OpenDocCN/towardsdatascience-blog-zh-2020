<html>
<head>
<title>Ultimate Guide to Input shape and Model Complexity in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中输入形状和模型复杂性的最终指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b?source=collection_archive---------3-----------------------#2020-06-17">https://towardsdatascience.com/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b?source=collection_archive---------3-----------------------#2020-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="720a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><span class="l ki kj kk bm kl km kn ko kp di"/>计算神经网络输入形状和复杂性的直观指南</h2></div><p id="c4ec" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在构建神经网络时，许多初学者和非初学者似乎都陷入了计算需要输入到神经网络中的输入形状的困境。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="b08d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="lt">但是我们为什么要知道输入的形状，又为什么要喂它呢？神经网络不能自己算出来吗？<br/> </em> </strong> <em class="lt">这个问题的答案在于矩阵乘法的基础知识。</em></p><p id="9ec4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">假设我们有两个矩阵A和B，设B的维数为m行x n列。现在，为了使两个矩阵在乘法上兼容，A的列维度应该与b的行维度相同，这意味着A的维数应该是k x m，其中k可以是任何数字。</p><p id="e950" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，图片A作为输入张量(一组图像、一组输入特征样本、特定词汇大小的文本数据等)。)和B作为神经网络中的第一个隐藏层。k是输入样本的数量，m是每个输入样本的维数。m的形状取决于输入的类型和隐藏层的类型。</p><h2 id="9538" class="lu lv it bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">让我们来看看最常用的神经网络类型，并弄清楚输入形状应该是什么:</h2><ol class=""><li id="5235" class="mn mo it ks b kt mp kw mq kz mr ld ms lh mt ll mu mv mw mx bi translated"><strong class="ks iu"><em class="lt">(DNN)深度神经网络:</em> </strong></li></ol><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/64534f325b554173a50a018a80cb4b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAMNusemlDZOvwTN1WKKhQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">一个4层神经网络(承蒙:<a class="ae nj" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">http://alexlenail.me/NN-SVG/index.html</a>)</p></figure><p id="ed2c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这些是用于分类和回归任务的完全连接的神经网络。这些有时也会附加到某些更高级架构的末端(<a class="ae nj" href="https://iq.opengenus.org/resnet50-architecture/" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>、<a class="ae nj" href="https://neurohive.io/en/popular-networks/vgg16/" rel="noopener ugc nofollow" target="_blank"> VGG16 </a>、<a class="ae nj" href="https://www.learnopencv.com/understanding-alexnet/#:~:text=AlexNet%20Architecture,two%20GTX%20580%203GB%20GPUs.&amp;text=AlexNet%20consists%20of%205%20Convolutional%20Layers%20and%203%20Fully%20Connected%20Layers." rel="noopener ugc nofollow" target="_blank"> AlexNet </a>等)。)</p><p id="8c65" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们看一个这样的神经网络:</p><pre class="lm ln lo lp gt nk nl nm nn aw no bi"><span id="5adb" class="lu lv it nl b gy np nq l nr ns">model = Sequential()<br/>model.add(Dense(units=12, activation='relu', input_shape=(32,)))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(units=8, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(units=6, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(units=1, activation='softmax'))</span></pre><p id="9723" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该模型由三个隐藏层和一个输入层组成。在每对致密层之间添加脱落层，以实现规整化。Dropout层采用参数“rate ”,该参数指定前一密集层中应取零值的神经元的比例。在这个模型中，比率设置为0.5，这意味着隐藏层中50%的神经元的权重为0。</p><p id="2019" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在Keras中，需要给出输入维度，不包括批量大小(样本数)。在这个神经网络中，输入形状被给定为(32，)。32是指每个输入样本中的特征数量。不要提批量大小，甚至可以给出一个占位符。另一种给出上述模型中输入维度的方法是(None，32，)。</p><p id="614f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果数据是多维的，比如图像数据，那么输入数据必须以(m，n)的形式给出，其中m是高度维度，n是宽度维度。</p><p id="7f39" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因为32是特征大小，所以它是输入矩阵的列维数。这意味着隐藏层的行尺寸也是32。现在我们已经对输入游戏进行了分类，让我们看看这个模型并理解它的复杂性。这里，复杂性是指可训练参数的数量(权重和偏差参数)。可训练参数的数量越多，模型越复杂。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nt"><img src="../Images/d3a0cb617b3821d1a90c8388db81a072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95FxP19n2Vw57IZocHQzRw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">深度神经网络模型综述</p></figure><blockquote class="nu nv nw"><p id="247c" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">由于每个输入特征都连接到隐藏层中的每个神经元，因此连接总数是输入特征大小(m)和隐藏层大小(n)的乘积。由于每个连接与一个权重参数相关联，权重参数的数量为m×n。每个输出神经元与一个偏置参数相关联，因此偏置参数的数量为n。<strong class="ks iu">可训练参数的总数= m×n+n</strong></p></blockquote><p id="e2e5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">第一密/隐层有12个神经元，这是它的输出维度。这在模型摘要中作为第二个输出参数出现，与第一个隐藏层相对。后续的脱落层不会改变输出的尺寸。它只是改变了神经元的重量。第二个隐层有8个输出神经元，下一层有6个。最终输出层有1个神经元。让我们验证这个模型的全部可训练参数。</p><blockquote class="nu nv nw"><p id="236b" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">第一隐藏层(m = 32，n = 12):32 x 12+12 =<strong class="ks iu">396<br/>396</strong>第二隐藏层(m = 12，n = 8) : 12 x 8 + 8 = <strong class="ks iu"> 104 <br/> </strong>第三隐藏层(m = 8，n = 6) : 8 x 6 + 6 = <strong class="ks iu"> 54 <br/> </strong>输出层(m = 6，n = 1) : 6 x 1 + 1 = <strong class="ks iu"> 7 </strong></p><p id="7ebe" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">总可训练参数= 396 + 104 + 54 + 7 = <strong class="ks iu"> 561 </strong></p></blockquote><p id="ff0a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">2.<strong class="ks iu"> <em class="lt">卷积神经网络(CNN): </em> </strong>这些大多用于处理各种计算机视觉应用的图像数据，如图像检测、图像分类、语义分割等。由于图像数据是多维数据，因此需要不同类型的处理层来检测图像的最重要特征。这就是卷积神经网络的作用。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oa"><img src="../Images/bf381f59bf7864f958eea3e903e79303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYuDHYnqqSodF_NC43fcEw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">一个示例性的CNN模型(承蒙:<a class="ae nj" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">http://alexlenail.me/NN-SVG/index.html</a>)</p></figure><p id="7a7b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图像被表示为一个三维元组——水平维度(宽度)、垂直维度(高度)和通道数量。如果图像是灰度的，那么通道参数取值为1，如果是彩色的，那么取值为3，红色、绿色和蓝色通道各一个。</p><pre class="lm ln lo lp gt nk nl nm nn aw no bi"><span id="5ef9" class="lu lv it nl b gy np nq l nr ns">Input_shape=(None, 284, 284, 3)<br/>cnn=Sequential()<br/>cnn.add(Conv2D(16, kernel_size=(2,2), padding="same", activation='relu', input_shape=Input_shape[1:]))</span><span id="2076" class="lu lv it nl b gy ob nq l nr ns">cnn.add(MaxPooling2D(4))<br/>cnn.add(Conv2D(32, kernel_size=(2,2), padding="same", activation='relu', input_shape=Input_shape[1:]))</span><span id="e8bc" class="lu lv it nl b gy ob nq l nr ns">cnn.add(MaxPooling2D(2))<br/>cnn.add(Flatten())</span><span id="6657" class="lu lv it nl b gy ob nq l nr ns">cnn.add(Dense(10, activation='softmax'))</span></pre><p id="93d4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">卷积神经网络有两种特殊类型的层。卷积层(模型中的Conv2D)和池层(MaxPooling2D)。维度为k的2-D卷积层由一个k x k过滤器组成，该过滤器通过图像中的每个像素。因为k×k滤波器覆盖k×k个像素，所以当它经过一个像素时，它的k -1个邻居也被覆盖。执行滤波器矩阵和覆盖图像矩阵的逐元素乘法。这些值相加并填充到相应的输出像素中。</p><p id="ba1d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">例如，如果2×2维的卷积滤波器通过位置(1，1)处的图像像素，则它也覆盖(0，0)、(0，1)和(1，0)。滤镜的(0，0)值乘以图像的(0，0)值，依此类推。我们得到四个值，它们相加并填充到输出的(1，1)位置。</p><blockquote class="nu nv nw"><p id="52e4" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">请注意，这会缩小图像的大小。如果滤波器要通过一个边界像素，比如(0，1)，那么就没有输出，因为这个像素没有相邻像素可供卷积通过。这将在宽度为k/2的图像周围显示为黑色边框。</p><p id="e75a" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">当大小为k x k的卷积滤波器通过大小为n x n的图像时，输出大小变为n-k+1。</p></blockquote><p id="e309" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了防止缩水，边框周围加了衬垫。参数padding设置为“same”，这意味着以不改变原始图像大小的方式在图像周围添加填充。</p><p id="4d92" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">池层就是这样做的；它汇集图像中一定数量的像素，并捕获最显著的特征(最大汇集)或像素的集合(平均汇集)作为输出。该模型包含一个大小为2 x 2的最大池层，它捕获每个4像素聚类的最大像素值。这将输出的大小减少到其原始大小的1/4(或者对于大小为k x k的池层，减少1/k)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oa"><img src="../Images/956d51d20025d9aa2a5ae514eb13134c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMCglhNnY3PoeyyS2NwWEA.png"/></div></div></figure><p id="0891" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们仔细看看这个模型。输入尺寸为284 x 284 x 3。它通过16个2×2滤波器的第一卷积层，带有填充。因此，该层的输出尺寸为284 x 284 x 16。随后的层是尺寸为4 x 4的最大池层，它将图像缩小16倍，高度方向缩小4倍，宽度方向缩小4倍。所以输出尺寸是71 x 71 x 16。下一个卷积层也有填充和32个滤波器，输出为71 x 71 x 32。维度为2 x 2的下一个池层将输入缩小到维度35 x 35 x 32。</p><p id="9685" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该输出被馈送到输出层(全连接/密集层)。但是，在将数据提供给密集层之前，需要将数据重新整形为一维。这是通过展平层实现的。</p><blockquote class="nu nv nw"><p id="5b88" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">对于m个输入通道和n个滤波器/内核大小为k×k的滤波器(输出通道)的卷积层，内核单独检查图像的每个通道，并产生每个输出通道的输出。因此，对于输入-输出通道的每个组合，我们需要分配k×k个权重。因此，权重参数的数量为m×k×k×n，偏差的数量等于通道的数量n。参数的总数为<strong class="ks iu">m×k×k×n+n</strong>。</p><p id="90df" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">池层只不过是聚合像素值。因此这里没有可训练的参数。</p></blockquote><p id="1ce7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们验证卷积神经网络的模型参数的吻合。</p><blockquote class="nu nv nw"><p id="f15d" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">第一Conv层(m = 3，k = 2，n = 16):3×2×2×16+16 =<strong class="ks iu">208<br/></strong>第二Conv层(m = 16，k = 2，n = 32):16×2×2×32+32 =<strong class="ks iu">2080<br/></strong>输出层(m = 39200，n = 10):39200×10+10 =<strong class="ks iu">392010</strong></p><p id="4b57" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">总可训练参数= 208+2080+392010 =<strong class="ks iu">394298</strong></p></blockquote><p id="b3ee" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="lt"> 3。递归神经网络(RNN): </em> </strong>这些神经网络用于处理顺序数据，或者当前输出不仅取决于当前输入，还取决于先前输入的数据。它用于时间序列预测、自然语言处理等。<br/>RNNs的一个独特特征是它包含门，允许或省略来自先前隐藏状态的输入添加到当前输入，完全可由用户配置。</p><p id="9e67" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有三种主要的RNN。最基本的是“香草”RNN，其中包含一个门。另外两个是长短期记忆单元(LSTM)和门控循环单元(GRU)。LSTM有4个门，而GRU有3个，这使得它在计算上比LSTM单位更快。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oc"><img src="../Images/726db6e0301446edf3b8504fdb1e39a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eYN2pk76YvGd7R2H.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">由Michael Phi提供:<a class="ae nj" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">https://towardsdatascience . com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-a-step-explain-44e 9 EB 85 BF 21</a></p></figure><p id="60a1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">上面是一幅区分LSTM和GRU建筑的插图，由Michael Phi的博客提供，你可以在这里参考<a class="ae nj" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"/>。</p><pre class="lm ln lo lp gt nk nl nm nn aw no bi"><span id="3114" class="lu lv it nl b gy np nq l nr ns">model=Sequential()<br/>model.add(LSTM(32, input_shape=(100,2)))<br/>model.add(Dense(1))</span></pre><p id="8665" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们这里有一个简单的LSTM模型(4门)，它馈入一个密集输出层。该模型接受三维输入:批量大小、时间戳和特征。与所有Keras层的情况一样，批量大小不是一个强制参数，但需要给出其他两个参数。在上述示例中，输入包含100个时间步长和2个要素。每一个时间步都是一个观察序列(例如一个单词序列)。特征类似于卷积神经网络中的通道。在该模型中，有2个输入通道。</p><blockquote class="nu nv nw"><p id="fb70" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">在具有g个门、m个输入特征和n个输出单元的递归神经网络中，每个门与当前输入以及前一个单元的隐藏状态(输出)有联系。因此，对于每个门，权重参数的数量为n x n+ m x n。每个输出单元都有一个偏置参数，因此偏置参数的数量为n。单个门的总参数为n x n + n x m + n。对于g个门，总参数为g x (n x n + n x m + n)。</p></blockquote><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi od"><img src="../Images/96ee5b52b66a347e73f158aca6b4007b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLacpKVbBHA8jWeNCWoo9g.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">LSTM模式概述</p></figure><p id="1d23" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于上述模型，让我们验证参数吻合。</p><blockquote class="nu nv nw"><p id="0879" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">LSTM层(g = 3，m = 2，n = 32):3 x(32 x 32+32 x 2+32)=<strong class="ks iu">4480<br/></strong>输出层(m = 32，n = 1) : 32 x 1 + 1 = <strong class="ks iu"> 33 </strong></p><p id="3a00" class="kq kr lt ks b kt ku ju kv kw kx jx ky nx la lb lc ny le lf lg nz li lj lk ll im bi translated">总可训练参数= 4480 + 33 = <strong class="ks iu"> 4513 </strong></p></blockquote><p id="edf6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">知道输入形状对于建立神经网络非常重要，因为所有的线性代数计算都是基于矩阵维数的。然而，这篇文章试图解决围绕它的谜团。此外，Keras对每个模型和输入形状都有非常全面的文档，这使得处理这个挑战变得稍微容易一些。</p><h2 id="6214" class="lu lv it bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated"><strong class="ak">参考:</strong></h2><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">LSTM和GRU的图解指南:一步一步的解释</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">嗨，欢迎来到长短期记忆(LSTM)和门控循环单位(GRU)的图解指南。我是迈克尔…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov nd oh"/></div></div></a></div><p id="1c7f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">https://keras . io/API/Layers/#:~:text = Layers % 20 are % 20 basic % 20 building，variables%20(层的%20weights)。</p></div></div>    
</body>
</html>