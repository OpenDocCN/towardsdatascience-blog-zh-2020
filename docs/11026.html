<html>
<head>
<title>Classification Model from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的分类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-model-from-scratch-49f24bdd0636?source=collection_archive---------37-----------------------#2020-07-31">https://towardsdatascience.com/classification-model-from-scratch-49f24bdd0636?source=collection_archive---------37-----------------------#2020-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d9a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 从头构建朴素贝叶斯分类模型(简单分类模型)的初学者指南。</h2></div><h2 id="64bc" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><a class="ae le" href="https://github.com/amindazad/Naive_Bayes_Classifier" rel="noopener ugc nofollow" target="_blank"> Github 库</a></h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/74c35bef3064a1b444c2634d03a558c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dn8o5YtS4QjYMRam4BQ9fg.gif"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://dribbble.com/shots/2194312-Hello-World" rel="noopener ugc nofollow" target="_blank">卡梅隆·福克斯里《旧电脑的基本编程》</a></p></figure><p id="f6f6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">在机器学习中，我们可以使用概率来进行预测。也许最广泛使用的例子是所谓的朴素贝叶斯算法。它不仅简单易懂，而且在一系列问题上取得了令人惊讶的好结果。</p><p id="4242" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">朴素贝叶斯算法是一种基于贝叶斯定理的分类技术。它假设一个类中的一个特性与任何其他特性的存在无关。如以下公式所示，该算法依赖于给定预测值的类的后验概率:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/bb7a0a67ac3e76e3e6dc2d708bbed6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*_sBK3LWI7jp3zoqE4PS97g.png"/></div></figure><p id="3be9" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">其中:</p><ul class=""><li id="c7fc" class="mp mq it lx b ly lz mb mc kr mr kv ms kz mt mn mu mv mw mx bi translated"><strong class="lx iu"> P(c|x) </strong>是给定一个预测值的类的概率</li><li id="a653" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">P(x|c) 是给定 la 类的预测值的概率。也称为可能性</li><li id="ef16" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">P(c) 是类的先验概率</li><li id="5ec9" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated"><strong class="lx iu"> P(x) </strong>是预测值的先验概率。</li></ul><p id="d319" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">或者用简单的英语来说，朴素贝叶斯分类器等式可以写成:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e3ea86ccb65d43d282b5531ce036bc31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*3BCMTbQ-i7AKd_cFxAQbkA.png"/></div></figure><p id="b6fc" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">好消息是朴素贝叶斯分类器易于实现并且表现良好，即使训练数据集很小。在预测数据类别时，这是最好的快速解决方案之一。<a class="ae le" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>为各种类型的问题提供不同的算法。其中之一就是<strong class="lx iu">高斯朴素贝叶斯</strong>。当要素为连续变量时使用，并假设要素遵循高斯分布。让我们深入挖掘，使用 Python 从头开始构建我们自己的朴素贝叶斯分类器。</p><h1 id="3469" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">1.加载所需的库</h1><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="4ca2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">构建自己的朴素贝叶斯分类器唯一需要的库是<a class="ae le" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>。NumPy 是一个开源项目，旨在用 Python 实现数值计算，我们将使用它进行算术运算。</p><h1 id="1a40" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">2.实例化该类</h1><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="0b5a" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">下一步是实例化我们的朴素贝叶斯分类器类。一个类就像一个对象构造器，或者一个创建对象的“蓝图”。在面向对象的编程语言中，几乎所有东西都是对象，包括它的属性和方法。</p><p id="2e92" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">“__init__”是 python 类中的保留方法。在面向对象的概念中，它被称为构造函数。当从类创建对象时调用此方法，它允许类初始化类的属性。</p><h1 id="74c0" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">3.分开上课</h1><p id="c766" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">根据贝叶斯定理，在试图预测某一特定类别之前，我们需要知道该类别的先验概率。为了计算这一点，我们必须将特征值分配给特定的类。我们可以通过分离类并将它们保存到字典中来做到这一点。</p><p id="63bb" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">字典是 Python 对一种数据结构的实现，这种数据结构通常被称为关联数组。字典由一组键值对组成。每个键-值对都将键映射到其关联的值。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><h1 id="b001" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">4.功能摘要(统计信息)</h1><p id="707f" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">给定类别的可能性或预测值的概率假定为正态分布(<a class="ae le" href="https://en.wikipedia.org/wiki/Gaussian_function" rel="noopener ugc nofollow" target="_blank">高斯</a>)，并基于平均值和标准差<em class="nv">(见公式)</em>进行计算。我们将为数据集中的每个特征创建一个摘要，这样做可以使我们在将来更容易地访问特征的均值和标准差。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><ul class=""><li id="b9b9" class="mp mq it lx b ly lz mb mc kr mr kv ms kz mt mn mu mv mw mx bi translated">这里的<code class="fe nw nx ny nz b">zip()</code>函数是元组的迭代器，其中每个特性的值被配对在一起。在 Python 中，元组是不可变的复合数据类型。</li><li id="776c" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">我们选择<code class="fe nw nx ny nz b">yield</code>,因为我们想要生成一个值序列，稍后我们将对其进行迭代，而不需要显式地将该序列保存在内存中。</li></ul><h1 id="5237" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">5.高斯分布函数</h1><p id="e079" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">使用高斯分布函数计算遵循正态分布的要素的可能性:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oa"><img src="../Images/6f41bc0adf9168b530f0ba4330d4fcc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bm3Ga_syxriFXwTLB8EsVg.png"/></div></div></figure><p id="c218" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">为了使用该公式进行进一步的计算，我们定义了一个分配方法，并完全按照上面的公式嵌入了该公式。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><h1 id="2060" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">6.训练模型</h1><p id="0472" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">训练模型意味着将模型应用于数据集，以便它可以遍历数据集并学习数据集的模式。在朴素贝叶斯分类器中，训练包括计算每个类的每个特征的平均值和标准差。这将允许我们计算用于预测的可能性。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8845" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">如果我们仔细看看上面的代码片段，我们可以看到我们已经在训练数据集中分离了类。然后，计算每类的均值和标准差，然后使用<code class="fe nw nx ny nz b">len(feature_values)/len(X)</code>计算该类的先验概率。</p><h1 id="6371" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">7.预测</h1><p id="e7f7" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">为了预测一个类，我们必须首先计算每个类的后验概率。具有最高后验概率的类别将是预测的类别。</p><p id="385f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">后验概率是联合概率除以边际概率。边际概率或分母是所有类的总联合概率，并且在所有类中都是相同的。我们需要具有最高后验概率的类，这意味着它将是最大的联合概率。</p><h2 id="ccfc" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">联合概率</strong></h2><p id="6d60" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">联合概率是用于计算后验概率的分数的分子。对于多个特征，联合概率公式为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5c516961f1456aeeeba02a4505a97b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*duSMQO8fhSpyiWzD8-K4jg.png"/></div></figure><p id="4f0b" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">使用 Python 应用相同的公式会产生以下代码片段:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="97a9" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">仔细看看上面的片段，我们对每个类都遵循了以下步骤:</p><ul class=""><li id="8da5" class="mp mq it lx b ly lz mb mc kr mr kv ms kz mt mn mu mv mw mx bi translated">获得摘要(平均值、标准偏差和先验概率)</li><li id="3c1e" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">计算每个特征的正态概率</li><li id="29ba" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">获得总可能性(所有正态概率的乘积)</li><li id="bfbe" class="mp mq it lx b ly my mb mz kr na kv nb kz nc mn mu mv mw mx bi translated">将先验概率乘以总似然得到联合概率。</li></ul><h2 id="db7e" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">预测班级</strong></h2><p id="3dfb" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">有了每个类的联合概率后，我们可以选择联合概率值最大的类:</p><p id="8ae5" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated"><code class="fe nw nx ny nz b">max(joint_proba, key=joint_proba.get)</code></p><h2 id="1106" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">将所有这些放在一起</strong></h2><p id="22ef" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">如果我们将联合概率步骤和预测类步骤放在一起，我们可以用下面的代码片段预测测试数据集中每一行的类。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><h1 id="08d6" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">8.准确度分数</h1><p id="4b51" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">计算准确度分数是测试任何机器学习模型的基本部分。为了测试我们的朴素贝叶斯分类器的性能，我们将正确预测的数量除以预测的总数，得到一个从 0 到 1 的数。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><h1 id="78ba" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">我们的 naive Bayes classifier vs . sk learn gaussianb</h1><p id="6a5e" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated">现在我们已经建立了分类模型，让我们使用<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/Wine" rel="noopener ugc nofollow" target="_blank"> UCI 葡萄酒数据集</a>来比较我们的模型与 scikit-learn 的 GaussianNB 模型的性能。</p><h2 id="4905" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">朴素贝叶斯分类器</h2><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8761" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">naive Bayes 分类器精度:<strong class="lx iu"> 0.972 </strong></p><h2 id="c030" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">Sklearn 高斯 NB(朴素贝叶斯)</h2><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="2224" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">sci kit-学习高斯精度:<strong class="lx iu"> 0.972 </strong></p><p id="c176" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">如您所见，模型的准确性是相同的，这意味着我们从头开始实现了一个成功的高斯朴素贝叶斯模型。</p><p id="9ce6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated">请随意使用这里的<a class="ae le" href="https://github.com/amindazad/Naive_Bayes_Classifier" rel="noopener ugc nofollow" target="_blank"> github 资源库</a>来查找整个 python 文件和用于创建本文的笔记本。</p><h1 id="3ce7" class="nd kj it bd kk ne nf ng kn nh ni nj kq jz nk ka ku kc nl kd ky kf nm kg lc nn bi translated">参考</h1><p id="0ec8" class="pw-post-body-paragraph lv lw it lx b ly nq ju ma mb nr jx md kr ns mf mg kv nt mi mj kz nu ml mm mn im bi translated"><a class="ae le" href="https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器如何在机器学习中工作</a></p><p id="4aac" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated"><a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/Wine" rel="noopener ugc nofollow" target="_blank"> UCI 葡萄酒数据集</a></p><p id="99fb" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md kr me mf mg kv mh mi mj kz mk ml mm mn im bi translated"><a class="ae le" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">Scikit-学习朴素贝叶斯</a></p></div></div>    
</body>
</html>