<html>
<head>
<title>Generative Adversarial Networks in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的生成对抗网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generative-adversarial-networks-in-python-73d3972823d3?source=collection_archive---------17-----------------------#2020-06-11">https://towardsdatascience.com/generative-adversarial-networks-in-python-73d3972823d3?source=collection_archive---------17-----------------------#2020-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dffb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中的GANs简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/19503ee1594489f569c1c7f13420f435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4X1UhSNrMovIHLu7ov3Ew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.pexels.com/photo/battle-board-game-castle-challenge-277124/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2ac0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成对抗网络(GANs)是一组用于产生合成数据的深度神经网络模型。该方法由伊恩·古德费勒于2014年开发，并在论文<a class="ae ky" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a>中进行了概述。GAN的目标是训练鉴别器能够区分真实和虚假数据，同时训练生成器产生可以可靠地欺骗鉴别器的数据的合成实例。</p><p id="8a5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GANs的一个流行应用是在“GANgogh”项目中，在这个项目中，经过wikiart.org绘画训练的GANs生成合成画。独立研究人员Kenny Jones和Derrick Bonafilia能够生成合成的宗教、风景、花卉和肖像图像，表现令人印象深刻。文章<a class="ae ky" rel="noopener" target="_blank" href="/gangogh-creating-art-with-gans-8d087d8f74a1">甘戈:用甘斯来创造艺术</a>详细介绍了这种方法。在本帖中，我们将介绍用python构建基本GAN的过程，我们将使用它来生成手写数字的合成图像。这篇文章中使用的大部分代码可以在GANs Tensorflow教程页面上找到，可以在<a class="ae ky" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="c4df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><p id="78d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们导入必要的包。让我们从导入‘matplotlib’，‘tensor flow . keras’层和‘tensor flow’库开始。让我们也定义一个变量，我们可以使用它来存储和清除我们的会话:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b72a" class="ma mb it lw b gy mc md l me mf">import matplotlib.pyplot as plt<br/>from tensorflow.keras import layers<br/>import tensorflow as tf<br/>from tensorflow.python.keras import backend as K<br/>K.clear_session()<br/>config = tf.ConfigProto()<br/>config.gpu_options.allow_growth = True</span></pre><p id="4ddc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们加载“MNIST”数据集，它在“张量流”库中可用。数据包含手写数字的图像和对应于数字的标签:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9b79" class="ma mb it lw b gy mc md l me mf">(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()</span></pre><p id="5cf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看训练数据中的第一幅图像:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7755" class="ma mb it lw b gy mc md l me mf">plt.imshow(train_images[0], cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0b78f94d7fd8a6a9e88a0dae8a67b363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*aNUvE__-3WcfmnslusUdkQ.png"/></div></figure><p id="55ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到这是手写的‘5’。接下来，让我们重塑数据，将图像像素转换为浮点值，并将像素值归一化为-1到1之间的值:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5a11" class="ma mb it lw b gy mc md l me mf">train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')<br/>train_images = (train_images - 127.5) / 127.5</span></pre><p id="7d54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们定义我们的发电机模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4d4d" class="ma mb it lw b gy mc md l me mf">def generator_model():<br/>    model = tf.keras.Sequential()<br/>    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))<br/>    model.add(layers.BatchNormalization())<br/>    model.add(layers.LeakyReLU())</span><span id="1c83" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Reshape((7, 7, 256)))<br/>    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size</span><span id="17e5" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))<br/>    assert model.output_shape == (None, 7, 7, 128)<br/>    model.add(layers.BatchNormalization())<br/>    model.add(layers.LeakyReLU())</span><span id="415a" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))<br/>    assert model.output_shape == (None, 14, 14, 64)<br/>    model.add(layers.BatchNormalization())<br/>    model.add(layers.LeakyReLU())</span><span id="8c8e" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))<br/>    assert model.output_shape == (None, 28, 28, 1)</span><span id="d7cd" class="ma mb it lw b gy mh md l me mf">    return model</span></pre><p id="8c40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先初始化一个顺序模型对象。然后我们添加第一层，这是一个普通的密集神经网络层。还有一系列转置卷积层，是带填充的卷积层。对于那些不熟悉的，卷积层学习矩阵(核心)的权利，然后结合起来，形成过滤器用于特征提取。通过学习滤波器权重，卷积层学习表示关于图像的高级信息的卷积特征。通过学习的过滤器，这些层可以执行像边缘检测、图像锐化和图像模糊这样的操作。这些是计算机视觉中核心矩阵的一些例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/d4db2b0850bf2f417eda2b4a1acbe1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpi0zeh20c1i0JhAc-0IGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Kernel_(image_processing)" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7051" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣，可以在这里了解更多关于卷积神经网络<a class="ae ky" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">的知识。还有一系列泄漏的“ReLu”层:</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/3ef621d36f7859def34c306c55cea4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1mGeUgg-0MtXxQ9D0pSLQ.png"/></div></div></figure><p id="e093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是经过修改的“ReLu”激活，通过增加“ReLu”功能的范围，有助于缓解神经元死亡问题。还有批量归一化图层，用于固定各图层输入的均值和方差。这有助于提高神经网络的速度、性能和稳定性。</p><p id="36fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发生器和鉴别器网络以类似于普通神经网络的方式被训练。即，随机初始化权重，评估损失函数及其相对于权重的梯度，并且通过反向传播迭代更新权重。</p><p id="6db7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练过程将帮助生成器模型从噪声中产生看起来真实的图像，并且鉴别器在检测看起来真实的假图像方面做得更好。让我们看一个生成器模型的输入示例。首先，让我们定义我们的生成器并初始化一些噪声“像素”数据:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="29c7" class="ma mb it lw b gy mc md l me mf">generator = generator_model()<br/>noise = tf.random.normal([1, 100])</span></pre><p id="07d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将噪声数据传入“generator_model”函数，并使用“matplotlib”绘制图像:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5efd" class="ma mb it lw b gy mc md l me mf">your_session = K.get_session()<br/>generated_image = generator(noise, training=False)<br/>array = generated_image[0, :, :, 0].eval(session=your_session)<br/>plt.imshow(array, cmap='gray')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/8bc2d582d26a38d6553cd6da66d21909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*4nRTyjql_OzMqD5zMCtBeg.png"/></div></figure><p id="0d22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到这只是一个嘈杂的黑白图像。我们的目标是让我们的生成器学习如何通过对这些嘈杂的数据进行迭代训练来生成看起来真实的数字图像，就像我们之前绘制的图像一样。经过充分的训练，我们的生成器应该能够从如上所示的噪声输入中生成真实的手写数字。</p><p id="f7b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们定义我们的鉴别器函数。这将是一个用于分类的普通卷积神经网络:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="baca" class="ma mb it lw b gy mc md l me mf">def discriminator_model():<br/>    model = tf.keras.Sequential()<br/>    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',<br/>                                     input_shape=[28, 28, 1]))<br/>    model.add(layers.LeakyReLU())<br/>    model.add(layers.Dropout(0.3))</span><span id="3790" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))<br/>    model.add(layers.LeakyReLU())<br/>    model.add(layers.Dropout(0.3))</span><span id="9387" class="ma mb it lw b gy mh md l me mf">    model.add(layers.Flatten())<br/>    model.add(layers.Dense(1))</span><span id="df8b" class="ma mb it lw b gy mh md l me mf">    return model</span></pre><p id="a5c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们定义损失函数和鉴别器对象:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0ffb" class="ma mb it lw b gy mc md l me mf">discriminator = discriminator_model()<br/>cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)</span></pre><p id="c625" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义特定于鉴别器的损失函数。该函数测量鉴别器区分真实图像和虚假图像的能力。它将鉴别器的二进制预测与真实图像和伪图像上的标签进行比较，其中“1”对应于真实图像，“0”对应于伪图像:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6c6d" class="ma mb it lw b gy mc md l me mf">def discriminator_loss(real_output, fake_output):<br/>    real_loss = cross_entropy(tf.ones_like(real_output), real_output)<br/>    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)<br/>    total_loss = real_loss + fake_loss<br/>    return total_loss</span></pre><p id="89fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发生器损耗函数衡量发生器欺骗鉴别器的能力:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7b7b" class="ma mb it lw b gy mc md l me mf">def generator_loss(fake_output):<br/>    return cross_entropy(tf.ones_like(fake_output), fake_output)</span></pre><p id="7997" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于生成器和鉴别器是独立的神经网络，它们各自都有自己的优化器。我们将使用“Adam”优化器来训练我们的鉴别器和生成器:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b03a" class="ma mb it lw b gy mc md l me mf">generator_optimizer = tf.keras.optimizers.Adam(1e-4)<br/>discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)</span></pre><p id="dc7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们定义历元数(训练数据的完整遍数)、噪声数据的维度大小以及要生成的样本数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="fada" class="ma mb it lw b gy mc md l me mf">EPOCHS = 50<br/>noise_dim = 100<br/>num_examples_to_generate = 16</span></pre><p id="109c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们为训练循环定义我们的函数。“@tf.function”装饰器编译该函数。“train_step()”函数从随机噪声生成图像开始:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="aced" class="ma mb it lw b gy mc md l me mf">@tf.function<br/>def train_step(images):<br/>    noise = tf.random.normal([BATCH_SIZE, noise_dim])<br/>    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:<br/>        generated_images = generator(noise, training=True) #random seed images</span></pre><p id="663f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后使用鉴别器对真实和伪造的图像进行分类:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ff3f" class="ma mb it lw b gy mc md l me mf">@tf.function<br/>def train_step(images):<br/>      ...<br/>          real_output = discriminator(images, training=True)<br/>          fake_output = discriminator(generated_images, training=True)</span></pre><p id="5b69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们计算发生器和鉴频器损耗:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="83ce" class="ma mb it lw b gy mc md l me mf">@tf.function<br/>def train_step(images):<br/>    ...</span><span id="4a5c" class="ma mb it lw b gy mh md l me mf">gen_loss = generator_loss(fake_output)<br/>        disc_loss = discriminator_loss(real_output, fake_output)</span></pre><p id="c1ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们计算损失函数的梯度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e1e5" class="ma mb it lw b gy mc md l me mf">@tf.function<br/>def train_step(images):<br/>    ...<br/>    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)<br/>    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)</span></pre><p id="2ae9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们应用优化器来找到使损失最小化的权重，并更新生成器和鉴别器:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="aa2d" class="ma mb it lw b gy mc md l me mf">@tf.function<br/>def train_step(images):<br/>    ...<br/>    generator_optimizer.apply_gradients(zip(gradients_of_generator,        generator.trainable_variables))<br/>    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))</span></pre><p id="6ed1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义一种方法，允许我们在训练完成后生成假图像，并保存它们:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1be1" class="ma mb it lw b gy mc md l me mf">def generate_and_save_images(model, epoch, test_input):</span><span id="a4c0" class="ma mb it lw b gy mh md l me mf">  predictions = model(test_input, training=False)</span><span id="b355" class="ma mb it lw b gy mh md l me mf">  fig = plt.figure(figsize=(4,4))</span><span id="0b38" class="ma mb it lw b gy mh md l me mf">  for i in range(predictions.shape[0]):<br/>      plt.subplot(4, 4, i+1)<br/>      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')<br/>      plt.axis('off')</span><span id="7e33" class="ma mb it lw b gy mh md l me mf">  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))<br/>  plt.show()</span></pre><p id="ac42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义允许我们同时训练生成器和鉴别器的训练方法。接下来，让我们导入“时间”和“操作系统”模块。让我们也定义一个检查点对象，它将允许我们保存和恢复模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="be50" class="ma mb it lw b gy mc md l me mf">import time<br/>import os<br/>checkpoint_dir = './training_checkpoints'<br/>checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")<br/>checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,                        discriminator_optimizer=discriminator_optimizer,<br/>                                 generator=generator,<br/>                                 discriminator=discriminator)</span></pre><p id="05a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义我们的函数，该函数从迭代历元数开始:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0824" class="ma mb it lw b gy mc md l me mf">def train(dataset, epochs):<br/>    for epoch in range(epochs):</span></pre><p id="cda0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在历元循环中，我们从每个训练步骤中生成图像:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9574" class="ma mb it lw b gy mc md l me mf">def train(dataset, epochs):<br/>        ...<br/>        display.clear_output(wait=True)<br/>        generate_and_save_images(generator,<br/>                             epoch + 1,<br/>                             seed)<br/>        if (epoch + 1) % 5 == 0:<br/>            checkpoint.save(file_prefix = checkpoint_prefix)</span><span id="d843" class="ma mb it lw b gy mh md l me mf">print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))</span></pre><p id="795e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们从最后一个时期生成图像。让我们每隔5个时期保存我们的模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6878" class="ma mb it lw b gy mc md l me mf">def train(dataset, epochs):<br/>        ...</span><span id="9fcd" class="ma mb it lw b gy mh md l me mf">    display.clear_output(wait=True)<br/>    generate_and_save_images(generator,<br/>                           epochs,<br/>                           seed)</span></pre><p id="8140" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以使用epochs参数对训练数据调用“train()”方法:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3faa" class="ma mb it lw b gy mc md l me mf">train(train_dataset, EPOCHS)</span></pre><p id="964d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们用两个时期运行我们的代码，我们应该得到假图像的如下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a1ebb06acd40ef19532eab115bbeb32d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*oFrEZMvOuzE_xOdCnh0aIA.png"/></div></figure><p id="0322" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到输出噪声仍然很大。经过50个时期后，我们应该生成以下图(注意，这需要在16 G内存的MacBook Pro上运行几个小时):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/f7651e8853c8cccfaf227bd2ab5919d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*mH3ZeZ4cObGC5aPqPHe7Ug.png"/></div></figure><p id="68b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，一些数字是可识别的，而另一些需要更多的训练来提高。可以推测，随着时代的增加，数字看起来会更可信。我就讲到这里，但是您可以随意使用数据并自己编码。还有许多其他数据集可以用来训练GANs，包括<a class="ae ky" href="https://www.kaggle.com/puneet6060/intel-image-classification" rel="noopener ugc nofollow" target="_blank">英特尔图像分类</a>数据集、<a class="ae ky" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR </a>数据集和<a class="ae ky" href="https://www.kaggle.com/tongpython/cat-and-dog#dog.4010.jpg" rel="noopener ugc nofollow" target="_blank">猫&amp;狗</a>数据集。其他有趣的应用包括深度假视频和深度假音频。</p><p id="0a86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要开始在视频上训练GAN，您可以查看论文<a class="ae ky" href="https://arxiv.org/pdf/1907.06571.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mn">复杂数据集的对抗性视频生成</em> </a> <em class="mn">。</em>在本文中，作者在<a class="ae ky" href="https://www.crcv.ucf.edu/data/UCF101.php" rel="noopener ugc nofollow" target="_blank"> UCF-101动作识别数据集</a>上训练GAN，该数据集包含来自YouTube的101个动作类别的视频。要开始在音频上训练GAN，请查看论文<a class="ae ky" href="https://arxiv.org/pdf/1802.04208.pdf" rel="noopener ugc nofollow" target="_blank">对抗性音频合成</a>。在本文中，作者对GAN进行了第一个到第九个语音命令的训练，这些命令包含鼓的声音、鸟的叫声等等。</p><h1 id="0d75" class="mo mb it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论</h1><p id="400a" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">总之，在这篇文章中，我们讨论了生成对抗网络(GAN)以及如何用python实现它。我们发现GANs同时训练两个神经网络，一个用于数据生成，另一个用于数据识别。鉴别器和生成器的层最显著地分别包含转置卷积和普通卷积层，它们学习图像的高级特征表示。我鼓励你尝试在一些其他有趣的数据上训练GAN，比如我上面提到的语音或视频数据集。同样，这篇文章中使用的代码可以在GANs Tensorflow教程页面上找到，可以在这里找到<a class="ae ky" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"/>。我希望你觉得这篇文章有用/有趣。这篇文章的代码也可以在GitHub 上找到。感谢您的阅读！</p></div></div>    
</body>
</html>