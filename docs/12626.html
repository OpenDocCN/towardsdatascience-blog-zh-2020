<html>
<head>
<title>Exploratory text analysis in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的探索性文本分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploratory-text-analysis-in-python-8cf42b758d9e?source=collection_archive---------22-----------------------#2020-08-31">https://towardsdatascience.com/exploratory-text-analysis-in-python-8cf42b758d9e?source=collection_archive---------22-----------------------#2020-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="34ac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">建立情感分类器的一步</em></h2></div><p id="0ca6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">为什么我们在建立模型之前要做探索性的数据分析？我会说“<em class="lf">为了更好地理解数据，以便我们以合适的方式预处理数据，并选择合适的建模技术”</em>。这种理解数据的必要性在处理文本数据时仍然适用。这篇文章是构建情感分类器的三篇系列文章中的第一篇。在这篇文章中，我们将看看一种对文本进行<em class="lf">探索性数据分析的方法，或者为了简洁起见，将<em class="lf">探索性文本分析</em>。</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/d64f98b063aac1ca2a61c3f7752d9bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PN-qSZm-DwC-GrDD"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">照片由<a class="ae lw" href="https://unsplash.com/@andrewtneel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安德鲁·尼尔</a>在<a class="ae lw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a3bd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在我们深入研究之前，让我们先退一步，看看更大的图景。CRISP-DM 方法概述了成功的数据科学项目的流程。下图显示了数据科学项目的第 2-4 阶段。在<strong class="kl iu">数据理解</strong>阶段，探索性数据分析是关键任务之一。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*qvgzhs_qixhx0SRCDN5-jg.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">CRISP-DM 工艺流程摘录</p></figure><p id="5708" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在从事数据科学项目时，在不同阶段之间来回切换而不是线性前进并不罕见。这是因为想法和问题会在随后的阶段出现，你想回到一两个阶段去尝试这个想法或找到问题的答案。官方的 CRISP-DM 中没有粉色箭头，但是，我认为这些经常是必要的。事实上，为了探索性的文本分析，我们将在这篇文章中做一些数据准备。对于那些有兴趣了解 CRISP-DM 更多信息的人来说，<a class="ae lw" href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的简短介绍，<a class="ae lw" href="https://www.sv-europe.com/crisp-dm-methodology/" rel="noopener ugc nofollow" target="_blank">这个资源</a>提供了更详细的解释。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="6f28" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">0.Python 设置🔧</h1><p id="8710" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">这篇文章假设读者(👀是的，你！)可以访问并熟悉 Python，包括安装包、定义函数和其他基本任务。如果你是 Python 的新手，<a class="ae lw" href="https://www.python.org/about/gettingstarted/" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的起点。</p><p id="fa10" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我在 Jupyter 笔记本里测试过 Python 3.7.1 的脚本。</p><p id="a567" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们在开始之前确保您已经安装了以下库:<br/> ◼️ <strong class="kl iu">数据操作/分析:</strong> <em class="lf"> numpy，pandas <br/> </em> ◼️ <strong class="kl iu">数据分区:</strong> <em class="lf"> sklearn <br/> </em> ◼️ <strong class="kl iu">文本预处理/分析:</strong> nltk <em class="lf"> <br/> </em> ◼️ <strong class="kl iu">可视化:</strong> <em class="lf"> matplotlib，seaborn </em></p><p id="3704" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一旦你安装了<em class="lf"> nltk </em>，请确保你已经从<em class="lf"> nltk </em>下载了<em class="lf">【punkt】</em><em class="lf">【停用词】</em>和<em class="lf">【wordnet】</em>，脚本如下:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="926f" class="nh mg it nd b gy ni nj l nk nl">import nltk<br/>nltk.download('punkt') # for sent_tokenize<br/>nltk.download('stopwords') <br/>nltk.download('wordnet') # for WordNetLemmatizer</span></pre><p id="fe3b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果你已经下载了，运行这个会通知你。</p><p id="ece1" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们准备导入所有的包:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="ee2a" class="nh mg it nd b gy ni nj l nk nl"># Setting random seed<br/>seed = 123</span><span id="0f29" class="nh mg it nd b gy nm nj l nk nl"># Data manipulation/analysis<br/>import numpy as np<br/>import pandas as pd</span><span id="3ec3" class="nh mg it nd b gy nm nj l nk nl"># Data partitioning<br/>from sklearn.model_selection import train_test_split</span><span id="46bd" class="nh mg it nd b gy nm nj l nk nl"># Text preprocessing/analysis<br/>import re<br/>from nltk import word_tokenize, sent_tokenize, FreqDist<br/>from nltk.util import ngrams<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer</span><span id="520c" class="nh mg it nd b gy nm nj l nk nl"># Visualisation<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style="whitegrid", context='talk', <br/>        palette=['#D44D5C', '#43AA8B'])</span></pre><p id="9c9d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="lf">外卖或笔记将附带</em>🍀<em class="lf">探索时</em>。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="4dc7" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">1.数据📦</h1><p id="ea4f" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">我们将使用 IMDB 电影评论数据集。您可以在这里下载数据集<a class="ae lw" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">，并将其保存在您的工作目录中。保存后，让我们将其导入 Python:</a></p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="9c2e" class="nh mg it nd b gy ni nj l nk nl">sample = pd.read_csv('IMDB Dataset.csv')<br/>print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")<br/>sample.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4cd7b0ca77e89ffb684e7d846279c031.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*p0QhKcYA9b6Q4dVCXGnZVA.png"/></div></figure><p id="973d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">从查看数据的头部，我们可以立即看到第二条记录中有 html 标记。</p><p id="4bcf" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀进一步检查 html 标签，看看它们有多普遍。</p><p id="bd69" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们来看看情绪之间的分歧:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="8a02" class="nh mg it nd b gy ni nj l nk nl">sample['sentiment'].value_counts()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5e5c06fc380f7ac8acfe9701d65dedac.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*e3f7OlQu1JOKN3FCw-HBMA.png"/></div></figure><p id="87c2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在样本数据中，情感是平均分配的。在开始探索性文本分析之前，我们先把数据分成两组:<em class="lf">训练</em>和<em class="lf">测试</em>。我们将留出 5000 箱进行测试:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="1dc4" class="nh mg it nd b gy ni nj l nk nl"># Split data into train &amp; test<br/>X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, <br/>                                                    stratify=sample['sentiment'])</span><span id="21aa" class="nh mg it nd b gy nm nj l nk nl"># Append sentiment back using indices<br/>train = pd.concat([X_train, y_train], axis=1)<br/>test = pd.concat([X_test, y_test], axis=1)</span><span id="62d8" class="nh mg it nd b gy nm nj l nk nl"># Check dimensions<br/>print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")<br/>print(f"{train['sentiment'].value_counts()}\n")</span><span id="2e91" class="nh mg it nd b gy nm nj l nk nl">print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")<br/>print(test['sentiment'].value_counts())</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi np"><img src="../Images/dd7a92b9923d4142d655cce22d3d6f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*aBoCBXmLQy8-P8WF2awz8w.png"/></div></figure><p id="a412" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">通过指定<code class="fe nq nr ns nd b">stratify</code>参数，我们确保了情感在两组中平均分配。</p><p id="2544" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在本帖中，我们将使用<em class="lf">训练</em>进行探索性文本分析。一旦我们探索了训练数据集，单独检查<em class="lf">测试</em>集的关键特征可能是有用的。理想情况下，这两组都应该代表潜在人群。让我们检查一下训练数据集的头部:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="318d" class="nh mg it nd b gy ni nj l nk nl">train.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ff942e16a2a52ba3489fa71cb2246a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*OUD0YC45nPYbfoxA6LgSXQ.png"/></div></figure><p id="7d2d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">好了，我们都准备好去探索了！✨</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="ef39" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak"> 2。探索性文本分析🔎</strong></h1><p id="011c" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">引导探索性数据分析的一个方法就是写下你感兴趣的问题，用数据来回答。找到问题的答案通常会引出你可能想探究的其他问题。以下是我们可以回答的一些问题示例:</p><p id="9293" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">📋<strong class="kl iu"> 2.1。热身:<br/> </strong> *一共有几根弦？<br/> *最常见的字符串有哪些？<br/> *最短的字符串是什么样子的？<br/> *最长的字符串是什么样子的？</p><p id="7251" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">📋<strong class="kl iu"> 2.2。代币:</strong></p><blockquote class="nu nv nw"><p id="e386" class="kj kk lf kl b km kn ju ko kp kq jx kr nx kt ku kv ny kx ky kz nz lb lc ld le im bi translated"><em class="it">💡</em> <a class="ae lw" href="https://www.nltk.org/book/ch01.html" rel="noopener ugc nofollow" target="_blank">令牌是一个字符序列。</a> <a class="ae lw" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">令牌通常被笼统地称为文字。</a> <br/> <em class="it">💡</em> <a class="ae lw" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">记号化是将文档拆分成记号的过程，有时还会丢弃某些字符，如标点符号。</a>示例:标记化将“这部电影棒极了”变成 4 个标记:[“这部电影”、“电影”、“曾经是”、“棒极了”]</p></blockquote><p id="21ca" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">*有多少代币？<br/> *有多少个唯一令牌？<br/> *每个令牌的平均字符数是多少？</p><p id="1dd5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">📋<strong class="kl iu"> 2.3。停字:</strong></p><blockquote class="nu nv nw"><p id="59f6" class="kj kk lf kl b km kn ju ko kp kq jx kr nx kt ku kv ny kx ky kz nz lb lc ld le im bi translated"><strong class="kl iu"> <em class="it">💡</em> </strong>停用词是对文本意义几乎没有价值的常用词。示例:和。</p></blockquote><p id="9e56" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">*最常见的停用词是什么？<br/> *还有哪些词经常出现，可以添加到停用词中？</p><p id="cf71" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">📋<strong class="kl iu"> 2.4。常见单词 n-grams: </strong></p><blockquote class="nu nv nw"><p id="b9de" class="kj kk lf kl b km kn ju ko kp kq jx kr nx kt ku kv ny kx ky kz nz lb lc ld le im bi translated"><strong class="kl iu"> <em class="it">💡</em> </strong> <a class="ae lw" href="https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram" rel="noopener ugc nofollow" target="_blank">单词 n-grams 是文本数据中相邻 n 个单词的所有组合</a>。例如:“这部电影棒极了”中的二元结构是[“这部电影”、“电影曾经是”、“曾经棒极了”]</p></blockquote><p id="143b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">*最常见的令牌是什么？最常见的二元模型是什么？最常见的三元模型是什么？最常见的四字格是什么？</p><p id="3d67" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">📋<strong class="kl iu"> 2.5。文件:</strong></p><blockquote class="nu nv nw"><p id="50d7" class="kj kk lf kl b km kn ju ko kp kq jx kr nx kt ku kv ny kx ky kz nz lb lc ld le im bi translated"><strong class="kl iu"> <em class="it">💡</em> </strong>文档是文本记录。例如:每个电影评论都是一个文档。<br/> <em class="it">💡</em>语料库是文档的集合。简单来说，文本数据就是一个语料库。示例:我们可以将训练数据称为训练语料库。</p></blockquote><p id="f265" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">*每个文档的平均句子数量是多少？<br/> *每个文档的平均令牌数是多少？<br/> *每个文档的平均字符数是多少？<br/> *每个文档的平均停用词数量是多少？<br/> *答案如何因情绪而异？</p><p id="93af" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，是时候找到这些问题的答案了！😃</p><h2 id="21a1" class="nh mg it bd mh oa ob dn ml oc od dp mp ks oe of mr kw og oh mt la oi oj mv ok bi translated">📋 2.1.W <strong class="ak">手臂抬起</strong></h2><p id="c2df" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">让我们将所有的评论合并成一个字符串，然后在空格处将其拆分成子字符串(以下简称为字符串)。这确保了对于这种预热探索，语料库被最小程度地改变(例如，保持标点不变):</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="2fab" class="nh mg it nd b gy ni nj l nk nl"># Prepare training corpus into one giant string<br/>train_string = " ".join(X_train.values)<br/>print(f"***** Extract of train_string ***** \n{train_string[:101]}", "\n")</span><span id="79f6" class="nh mg it nd b gy nm nj l nk nl"># Split train_corpus by white space<br/>splits = train_string.split()  <br/>print(f"***** Extract of splits ***** \n{splits[:18]}\n")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ol"><img src="../Images/7bf90154b7d470700fb80d5b071d42bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jsl7e_SuHzEkT3TKy-DrFw.png"/></div></div></figure><p id="8074" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️ <strong class="kl iu"> 2.1.1。有多少根弦？</strong></p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="3e9f" class="nh mg it nd b gy ni nj l nk nl">print(f"Number of strings: {len(splits)}")<br/>print(f"Number of unique strings: {len(set(splits))}")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi om"><img src="../Images/87f19d349019ffc33e3929485cd5874e.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*2tNq7d8-MRA1zvgfbxF1Rg.png"/></div></figure><p id="9da3" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在训练语料库中有超过 1000 万个字符串，其中有大约 41 万个独特的字符串。这给了我们初步的大概数字。在我们正确标记后，我们将看到这些数字是如何寻找标记的。</p><p id="46f6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️。最常见的字符串有哪些？</p><p id="a333" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们为每个字符串准备频率分布来回答这个问题:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="def0" class="nh mg it nd b gy ni nj l nk nl">freq_splits = FreqDist(splits)<br/>print(f"***** 10 most common strings ***** \n{freq_splits.most_common(10)}", "\n")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi on"><img src="../Images/54b595632e63aff2b6734398713b3e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5uGtpKjHSrFP5TQp8_4bw.png"/></div></div></figure><p id="3239" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">看到最常见的字符串是停用词并不奇怪。我们将在第 2.3 节<em class="lf">中进一步探讨停用词。停止言语</em>。</p><p id="dfdd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">在查看普通令牌和 n 元语法之前，删除停用词。</em></p><p id="8556" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu"> ✏️。最短的字符串是什么样子的？</strong></p><p id="4b0f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们将短字符串定义为长度小于 4 个字符的字符串，并检查它们的频率:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="87ed" class="nh mg it nd b gy ni nj l nk nl">short = set(s for s in splits if len(s)&lt;4)<br/>short = [(s, freq_splits[s]) for s in short]<br/>short.sort(key=lambda x:x[1], reverse=True)<br/>short</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0849eea47688e8e2df335bcb1fbd5041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*TAsKvkgUGIwnjFKsOyRLSQ.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><em class="ki">摘录自</em> <strong class="bd op"> <em class="ki">短</em> </strong> <em class="ki">，未显示所有输出</em></p></figure><p id="999c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">许多短字符串似乎是停用词，但也有数字和其他短词。</p><p id="8d11" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有不同形式的数字:3，2，70，90% —我们需要决定是放弃还是保留它们。</em></p><p id="2fe0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有大小写变化:“the”、“The”、“The”-这些需要规范化。</em></p><p id="3016" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因为我们还没有标记化，一些字符串目前包含附加在单词上的标点符号。因此，在其他情况下，相同的单词被认为是不同的，如下例所示:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/75039ed8f4f322bea65fbda5514c8ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*gN_7ap3JRBAizBTzaja35w.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><em class="ki">摘自</em> <strong class="bd op"> <em class="ki">短</em> </strong> <em class="ki">，并非所有输出都显示</em></p></figure><p id="c55c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">舍弃标点符号将有助于进一步规范文字。</em></p><p id="7a33" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️ 2.1.4。最长的字符串是什么样子的？</p><p id="4fe8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们将长字符串定义为 16+字符长，并重复这个过程。</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="3ee8" class="nh mg it nd b gy ni nj l nk nl">long = set(s for s in splits if len(s)&gt;15)<br/>long = [(s, freq_splits[s]) for s in long]<br/>long.sort(key=lambda x:x[1], reverse=True)<br/>long</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi or"><img src="../Images/9a749293465333e00373bd4fe44b0552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOcJuvFSDXUUmvbLOMWCwg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><em class="ki">摘录自</em> <strong class="bd op"> <em class="ki">长</em> </strong> <em class="ki">，并非所有输出都显示</em></p></figure><p id="155a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">长弦的频率看起来比短弦低得多，这并不奇怪。长字符串看起来很有趣，有几个要点:</p><p id="e9d0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀同一个词有美国和英国的拼法:“charactering”和“charactering”。使用下面的脚本快速检查后，这个单词的美式拼写看起来更占优势。量化这两种拼写在整个训练语料库中的流行程度有点棘手。</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="77bc" class="nh mg it nd b gy ni nj l nk nl">print(f"characterisation: {sum([c for s, c in long if re.match(r'characterisation*', s.lower())])} strings")<br/>print(f"characterization: {sum([c for s, c in long if re.match(r'characterization*', s.lower())])} strings")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi os"><img src="../Images/3649c07d59bb4a3bd983b940ada03f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Bf1UrClo-BrJtZINgQSoEg.png"/></div></div></figure><p id="dc37" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有带连字符的词:“发人深省”、“后启示录”、“不被欣赏”和“电影特别”(这个有双连字符)。如果我们在空格或标点符号上做记号，这些字符串将被分割成单独的单词。在大多数情况下，这将保留句子的要点。如果我们保留用连字符连接的单词，它们就不会像生僻字一样常见，因此会被删除。</em></p><p id="cd9b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有文字结合其他标点符号(有些由于篇幅不够)</em> <strong class="kl iu"> <em class="lf"> : ' </em> </strong> <em class="lf">演员/女演员'，《碟中谍:不可能'，《演员(哈！哈！哈！)…他们是“，‘不同:其实，布洛克’。在标记时，最好将这些情况分成单独的单词。因此，基于空格或标点符号的标记可能是一个好主意。</em></p><p id="314c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有网址和邮箱:</em><strong class="kl iu"><em class="lf"/></strong><em class="lf">'/&gt;www。ResidentHazard.com '，</em><strong class="kl iu"><em class="lf">'</em></strong><em class="lf">http://www.PetitionOnline.com/gh1215/petition.html', ' iamaseal 2 @ YAHOO。COM“”。但是，好像并不多。</em></p><p id="0589" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">有把同一个字重复两次以上的不法词语</em><strong class="kl iu"><em class="lf">:</em></strong><em class="lf">“Booooring……不要，不要！),'.如果你知道这些拉长的单词的正确术语，我很想知道。在此之前，我们将把它们称为“非法词汇”。这些违法案件似乎很少出现。</em></p><p id="fb87" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">还有其他有趣的发现，我们可以添加到这些，但这些是一个很好的开端。</p><p id="f602" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">理解我们刚刚探索的这些情况是否普遍到足以证明额外的预处理步骤(即更长的运行时间)是正确的，这一点很重要。进行实验以了解添加额外的预处理步骤是否会提高模型性能是很有用的。我们将在第三篇文章中做一些介绍。</p><p id="35cc" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️ 2.1.5。到目前为止出现的后续问题</p><p id="612f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们已经回答了所有 4 个热身问题！在寻找答案的同时，我们收集了更多的问题。在我们跳到下一组预定义的问题之前，让我们快速跟进一些移动中出现的问题。</p><p id="ef67" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">◼️<strong class="kl iu">html 标签出现的频率如何？</strong> <br/>这个问题是我们在查看<em class="lf">第 1 节的样本数据头时出现的。数据</em>。当我们根据空白分割数据时，示例 html 标记:'&lt; br / &gt; &lt; br / &gt;'将被分割成三个字符串:'&lt; br '，'/ &gt; &lt; br '和'/ &gt;'。顺便说一下，<a class="ae lw" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/br#:~:text=The%20HTML%20element%20produces,division%20of%20lines%20is%20significant." rel="noopener ugc nofollow" target="_blank"><em class="lf">&lt;br&gt;</em>标签</a>似乎是用来换行的。让我们粗略估计一下 html 标签有多普遍。</p><p id="4f03" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="lf">所有跟进问题都是相似的，因为我们要评估特定类型字符串的出现频率。为了避免重复我们自己，我们来做一个函数。</em></p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="1eef" class="nh mg it nd b gy ni nj l nk nl">def summarise(pattern, strings, freq):<br/>    """Summarise strings matching a pattern."""<br/>    # Find matches<br/>    compiled_pattern = re.compile(pattern)<br/>    matches = [s for s in strings if compiled_pattern.search(s)]<br/>    <br/>    # Print volume and proportion of matches<br/>    print("{} strings, that is {:.2%} of total".format(len(matches), len(matches)/ len(strings)))<br/>    <br/>    # Create list of tuples containing matches and their frequency<br/>    output = [(s, freq[s]) for s in set(matches)]<br/>    output.sort(key=lambda x:x[1], reverse=True)<br/>    <br/>    return output</span><span id="1496" class="nh mg it nd b gy nm nj l nk nl"># Find strings possibly containing html tag<br/>summarise(r"/?&gt;?w*&lt;|/&gt;", splits, freq_splits)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ot"><img src="../Images/a84b446bac24d7c4818cbb92e290cb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbEZ-dMpyUZrdQSPXoQI-w.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="b0dc" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果我们滚动输出，除了<em class="lf"> &lt; br &gt; </em>标签和少数<em class="lf"> &lt; i &gt; </em>标签之外，没有多少 html 标签。</p><p id="60c5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">如果我们在分词时去掉标点符号，那么“/ &gt; &lt; br”和“&lt; br”将变成“br ”,我们可以添加“br”来停止单词。</em></p><p id="f033" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu"> ◼ ️How 经常出现的数字是多少？<br/> </strong>我们从 2.1.3 节的<em class="lf">中找到了一些数字的实例。短弦</em>。让我们通过下面的脚本来看看它们出现的频率:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="b1f9" class="nh mg it nd b gy ni nj l nk nl">summarise(r"\d", splits, freq_splits)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/843f4e6b4b2b7fbc32a866e3b0f3b8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*eczGLYNj9kh-UY67YPXogQ.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="373e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">包含数字的字符串很少出现。在电影评论的背景下，很难直观地理解数字是如何有用的。10/10 可能是积极情绪的标志，但我们能从 4、80 和 20 这样的数字中推断出什么呢？我们将在标记时丢弃数字。</p><p id="6411" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">根据项目的时间表，你可能没有足够的时间去尝试所有有趣的想法。在这种情况下，保留一份可以尝试的项目清单<strong class="kl iu"> <em class="lf">是很方便的，你可以在有时间的时候尝试一下。我们将在该列表中添加以下任务:<br/> 1)保存数字并将其转换为文本<br/> 2)创建一个特征来指示评论是否包含数字</em></strong></p><p id="fb64" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu"> ◼ ️How 常用的是连字符的单词吗？</strong> <br/>我们在<em class="lf">第 2.1.4 节</em>中检查长字符串时看到了连字符。让我们看看它们出现的频率:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="4165" class="nh mg it nd b gy ni nj l nk nl">summarise(r"\w+-+\w+", splits, freq_splits)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ov"><img src="../Images/b99187d09445178fa729b6ffc867b15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NmXFOLXPKPhdABj2S01tJA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="6af2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">大约不到 1%的字符串包含带连字符的单词。浏览用连字符连接的单词，将它们分开以保持数据简单更有意义。例如:我们应该将“camera-work”标记为 2 个标记:['camera '，' work']而不是 1 个标记:['camera-work']。我们可以在<em class="lf">列表中添加“保持连字符单词的原样”。</em></p><p id="8022" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu"> ◼ ️How 常用的词是由其他标点符号组合而成的吗？<br/> </strong>很像上一个问题，我们在长串探索中看到了这些案例。让我们看看它们出现的频率:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="08bd" class="nh mg it nd b gy ni nj l nk nl">summarise(r"\w+[_!&amp;/)(&lt;\|}{\[\]]\w+", splits, freq_splits)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ov"><img src="../Images/941b0b224b350da458a58f772cb7d636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ztmYsahAFgYeR0t02KHZQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="76b5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">不要太频繁，这些肯定是要分开的。</p><p id="25e8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu"> ◼ ️How 频繁出现的都是绿林好汉的话吗？我们看到了像 NOOOOOOIIIISE！)，‘早先。让我们看看它们有多普遍:</strong></p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="a147" class="nh mg it nd b gy ni nj l nk nl">def find_outlaw(word):<br/>    """Find words that contain a same character 3+ times in a row."""<br/>    is_outlaw = False<br/>    for i, letter in enumerate(word):<br/>        if i &gt; 1:<br/>            if word[i] == word[i-1] == word[i-2] and word[i].isalpha():<br/>                is_outlaw = True<br/>                break<br/>    return is_outlaw</span><span id="3efd" class="nh mg it nd b gy nm nj l nk nl">outlaws = [s for s in splits if find_outlaw(s)]<br/>print("{} strings, that is {:.2%} of total".format(len(outlaws), len(outlaws)/ len(splits)))<br/>outlaw_freq = [(s, freq_splits[s]) for s in set(outlaws)]<br/>outlaw_freq.sort(key=lambda x:x[1], reverse=True)<br/>outlaw_freq</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ow"><img src="../Images/c656669fb94d93837832f50cef23783a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MIIFc68OZFr3FKqCtgr-Vg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="585b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这些不值得纠正，因为案例太少。</p><p id="9820" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这是最后一个跟进问题！我们已经了解了一些数据。希望你感觉暖和了。💦你可能已经注意到，我们可以很容易地不断扩展我们的问题，不断探索？出于时间的考虑，我们将在这里停止这一部分，并尽量使接下来的部分尽可能简洁。否则，这篇文章会超过几个小时。💤</p><h2 id="f94a" class="nh mg it bd mh oa ob dn ml oc od dp mp ks oe of mr kw og oh mt la oi oj mv ok bi translated">📋 2.2.代币</h2><p id="14af" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">让我们一口气回答这两个问题:</p><p id="6a48" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️2 . 2 . 1。有多少代币？<br/>  ✏️ <strong class="kl iu"> 2.2.2。有多少独特的代币？</strong></p><p id="abb0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们必须先将数据符号化。回想一下之前的探索，似乎最好在标记时去掉标点和数字。记住这一点，让我们将文本标记为字母标记:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="4010" class="nh mg it nd b gy ni nj l nk nl">tokeniser = RegexpTokenizer("[A-Za-z]+")<br/>tokens = tokeniser.tokenize(train_string)<br/>print(tokens[:20], "\n")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ox"><img src="../Images/0775c5021c29976a5fd66cccefdb773e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*PtDfrxcrvCzm4c42M4kkgw.png"/></div></div></figure><p id="1cfc" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在我们已经标记化了，我们可以回答前两个问题:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="cfb6" class="nh mg it nd b gy ni nj l nk nl">print(f"Number of tokens: {len(tokens)}")<br/>print(f"Number of unique tokens: {len(set(tokens))}")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/64fcb6dbfcd3a29c1f0063f121eb7675.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*RAF1bdQuH82F7E8PUdLPSA.png"/></div></figure><p id="4352" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">训练数据中有超过 1000 万个令牌，其中大约有 12.2 万个唯一令牌。目前，“手表”、“手表”和“观看”被视为不同的令牌。嗯，如果我们能把它们正常化为“手表”,并把它们算作一个唯一的令牌，那不是很好吗？如果我们进行标准化，唯一令牌的数量将会减少。让我们快速地做两件事:将所有的记号转换成小写，并对它们进行 lemmatise:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="6a72" class="nh mg it nd b gy ni nj l nk nl">lemmatiser = WordNetLemmatizer()<br/>tokens_norm = [lemmatiser.lemmatize(t.lower(), "v") for t in tokens]<br/>print(f"Number of unique tokens: {len(set(tokens_norm))}")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/66c3430c031ad15a391ab4aaa7c21d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*wQFx2oZUPcYRSojRKgGENg.png"/></div></figure><p id="ec28" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">太好了，独特令牌的数量下降了约 30%。</p><p id="77e2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">📌练习:</strong>如果你感兴趣并且有时间，不要像上面那样把两个步骤结合起来，试着分离出来，看看每个步骤中独特代币的数量是如何变化的。例如，您可以首先将记号转换成小写，并检查数字，然后使用 lemmatise 并再次检查数字。如果改变这两个操作的顺序，唯一令牌的最终数量是否不同于 82562？为什么会这样？</p><p id="fa1e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">👂<em class="lf">嘶，我会在</em> <a class="ae lw" rel="noopener" target="_blank" href="/preprocessing-text-in-python-923828c4114f"> <em class="lf">下一篇</em> </a> <em class="lf">为模型预处理文本时，展示 lemmatise 的另一种方式。</em></p><p id="7854" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️ <strong class="kl iu"> 2.2.3。每个令牌的平均字符数是多少？</strong></p><p id="5299" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们找出平均令牌长度并检查其分布情况:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="cc54" class="nh mg it nd b gy ni nj l nk nl"># Create list of token lengths for each token<br/>token_length = [len(t) for t in tokens]</span><span id="82fa" class="nh mg it nd b gy nm nj l nk nl"># Average number of characters per token<br/>print(f"Average number of characters per token: {round(np.mean(token_length),4)}")</span><span id="d07f" class="nh mg it nd b gy nm nj l nk nl"># Plot distribution<br/>plt.figure(figsize=(12, 12))<br/>sns.countplot(y=token_length)<br/>plt.title("Counts of token length", size=20);</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pa"><img src="../Images/c364a9172bb6f42badcfde0542a0a5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqo1aZldIhGRrcYIrf4fFQ.png"/></div></div></figure><p id="d59f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">有几个令牌很长，但也很罕见。让我们来看看超过 10 个字符的确切数量:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="60a6" class="nh mg it nd b gy ni nj l nk nl">pd.DataFrame(data=token_length, columns=['length']).query("length&gt;10").value_counts()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/e8df79cb7715c1dcc93f9d4ec4ed52a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*j-t29Qgo8Pz8BT0EL_v7jA.png"/></div></figure><p id="d7d2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">超过 17 个字符的长单词并不常见。让我们来看看其中的一些:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="01cb" class="nh mg it nd b gy ni nj l nk nl">[t for t in tokens if len(t)&gt;=20]</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pc"><img src="../Images/ec7d6102ad0a7839772fd623b4ea4fde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*IXRi-WTFyju8HGGRxCdlGQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">没有显示所有输出</p></figure><p id="351d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">有趣的是，有些是有效的长单词，而有些长是因为它们缺少空格或非法单词(即拉长)。</p><p id="ff54" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">🍀<em class="lf">在预处理时，我们应该确保像这样非常罕见的记号被丢弃，这样它们就不会在将记号矢量化成矩阵时创建单独的列。</em></p><h2 id="41ba" class="nh mg it bd mh oa ob dn ml oc od dp mp ks oe of mr kw og oh mt la oi oj mv ok bi translated">📋 2.3.停止言语</h2><p id="aa74" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">✏️2 . 3 . 1。最常用的停用词是什么？</p><p id="f1d3" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们首先检查所有停用词:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="8cae" class="nh mg it nd b gy ni nj l nk nl">stop_words = stopwords.words("english")<br/>print(f"There are {len(stop_words)} stopwords.\n")<br/>print(stop_words)</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pd"><img src="../Images/734593cb3ce4b2d7ef59b933da21750d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LvwbnaehEYSWCk2_ofvVkA.png"/></div></div></figure><p id="342c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在写这篇文章的时候，有 179 个停用词。停用词的清单将来还会增加。看起来我们可以扩展停用词来包含更多的停用词。事实上，我已经在 Github 上<a class="ae lw" href="https://github.com/nltk/nltk/issues/2588" rel="noopener ugc nofollow" target="_blank">提议</a>将下面的通用停用词添加到<em class="lf"> nltk </em>的英文停用词列表中。我们还要确保在列表中添加一个自定义停用词“br”<em class="lf"/>:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="8da8" class="nh mg it nd b gy ni nj l nk nl">stop_words.extend(["cannot", "could", "done", "let", "may" "mayn",  "might", "must", "need", "ought", "oughtn", "shall", "would", "br"])<br/>print(f"There are {len(stop_words)} stopwords.\n")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/fd0538eb1cf136ef0a65c324ca93475c.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*xwjLcOpZmdNfKZEYxn5GgQ.png"/></div></figure><p id="67e7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，让我们来看看最常见的停用词是什么:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="dba6" class="nh mg it nd b gy ni nj l nk nl">freq_stopwords = [(sw, tokens_norm.count(sw)) for sw in stop_words]<br/>freq_stopwords.sort(key=lambda x: x[1], reverse=True)<br/>freq_stopwords[:10]</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cc60b4aa9d15d626582ca785312970c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*uwTs1pUxDj2LHnZWxlty0Q.png"/></div></figure><p id="249c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">频率真的很高(咄，我的意思是它们是停用词，当然会频繁😈)，特别是对于‘be’和‘the’。找出停用词在标记中所占的比例不是很有趣吗？让我们快速检查一下:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="38e3" class="nh mg it nd b gy ni nj l nk nl">n_stopwords = len([t for t in tokens_norm if t in stop_words])<br/>print(f"{n_stopwords} tokens are stop words.")<br/>print(f"That is {round(100*n_stopwords/len(tokens_norm),2)}%.")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/122515318663adb4725c1cfe2576fa7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*Tx8UX-CslFFSqCl6Z81QuQ.png"/></div></figure><p id="4e42" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">大约一半的标记是停用词。💭</p><p id="0bef" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️。还有哪些词经常出现，可以添加到停用词中？</p><p id="e33f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们很快会在查看常见令牌时回答这个问题。</p><h2 id="de63" class="nh mg it bd mh oa ob dn ml oc od dp mp ks oe of mr kw og oh mt la oi oj mv ok bi translated">📋 2.4.常见 n 元语法</h2><p id="fa8a" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">是时候找出常见的 n-gram 了。让我们一起回答这四个问题:</p><p id="9f42" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️<strong class="kl iu">2 . 4 . 1–4。什么是最常见的令牌，二元，三元和四元？</strong></p><p id="de6d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">首先，让我们删除停用词:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="201b" class="nh mg it nd b gy ni nj l nk nl">tokens_clean = [t for t in tokens_norm if t not in stop_words]<br/>print(f"Number of tokens: {len(tokens_clean)}")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/01c43b1f786e216d90403222e5174704.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*pPPHZn290gKxq0FLeASclQ.png"/></div></figure><p id="fb04" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这是剩下的 49%的代币。现在，我们可以检查常见的记号(即，一元词)、二元词、三元词和四元词:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="caa3" class="nh mg it nd b gy ni nj l nk nl">def preprocess_text(text):<br/>    """Preprocess text into normalised tokens."""<br/>    # Tokenise words into alphabetic tokens<br/>    tokeniser = RegexpTokenizer(r'[A-Za-z]{2,}')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]<br/>    <br/>    # Remove stopwords<br/>    keywords= [lemma for lemma in lemmas if lemma not in stop_words]<br/>    return keywords</span><span id="6aa6" class="nh mg it nd b gy nm nj l nk nl">def get_frequent_ngram(corpus, ngram, n=20):<br/>    """Find most common n n-grams tokens."""<br/>    # Preprocess each document<br/>    documents = [preprocess_text(document) for document in corpus]<br/>    <br/>    # Find ngrams per document<br/>    n_grams = [list(ngrams(document, ngram)) for document in documents]<br/>    <br/>    # Find frequency of ngrams<br/>    n_grams_flattened = [item for sublist in n_grams for item in sublist]<br/>    freq_dist = FreqDist(n_grams_flattened)<br/>    top_freq = freq_dist.most_common(n)<br/>    return pd.DataFrame(top_freq, columns=["ngram", "count"])</span><span id="d8c9" class="nh mg it nd b gy nm nj l nk nl"># Get frequent ngrams for all 4<br/>for i in range(1,5):<br/>    mapping = {1:"uni", 2:"bi", 3:"tri", 4:"four"}<br/>    plt.figure(figsize=(12,10))<br/>    sns.barplot(x="count", y="ngram", data=get_frequent_ngram(train['review'], i))<br/>    plt.title(f"Most common {mapping[i]}grams");</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pi"><img src="../Images/e7b2a7ba1b28621b027a16e726e58bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yivM8f07DYXr3K9xWxLIfg.png"/></div></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pj"><img src="../Images/3a3680e88b7ce3869280419c6f8ea4c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwbdDo6CK2OPAXGGkjty8Q.png"/></div></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pk"><img src="../Images/98163ffe99f931e4ed5e887d814b7d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieZjiZ0s90Zoj_Uen4gxkQ.png"/></div></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pl"><img src="../Images/354f553665945339ea693e4c724e67a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gm91nSIxZ3Bc11Usorn6Dw.png"/></div></div></figure><p id="5f5c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">与其他常用词相比，单词“film”和“movie”看起来相当频繁。问题 2.3.2 的答案。就是潜在的加上‘电影’，和‘电影’来停止用词。有趣的是经常看到二元、三元和四元。随着 n 的增加，频率如预期的那样下降。二元模型可能是潜在有用的，但是三元模型和四元模型相对于标记频率来说不够频繁。</p><h2 id="522d" class="nh mg it bd mh oa ob dn ml oc od dp mp ks oe of mr kw og oh mt la oi oj mv ok bi translated">📋 2.5.文档</h2><p id="97f0" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">让我们一起来回答这些问题:</p><p id="d258" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">✏️2 . 5 . 1。每个文档的平均句子数是多少？ <br/> <strong class="kl iu"> ✏️ 2.5.2。每个文档的平均令牌数是多少？<br/> ✏️。每个文档的平均字符数是多少？<br/> ✏️。每个文档的平均停用词数量是多少？<br/> ✏️。这些问题的答案如何因情绪而异？</strong></p><p id="db54" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">首先，我们必须准备数据:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="6f1f" class="nh mg it nd b gy ni nj l nk nl"># tokeniser = RegexpTokenizer("[A-Za-z]+")<br/>train["n_sentences"] = train["review"].apply(sent_tokenize).apply(len)<br/>train["tokens"] = train["review"].apply(tokeniser.tokenize)<br/>train["n_tokens"] = train["tokens"].apply(len)<br/>train["n_characters"] = train["review"].apply(len)<br/>train["n_stopwords"] = train["tokens"].apply(lambda tokens: len([t for t in tokens if t in stop_words]))<br/>train["p_stopwords"] = train["n_stopwords"]/train["n_tokens"]</span><span id="f32a" class="nh mg it nd b gy nm nj l nk nl"># Inspect head<br/>columns = ['sentiment', 'n_sentences', 'n_tokens', 'n_characters', 'n_stopwords', 'p_stopwords']<br/>train[columns].head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/9bfe626210a7e19b4735ac653a159c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*g9MQUZGDCevmig3xBAJbjQ.png"/></div></figure><p id="fc5a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">让我们检查感兴趣的变量的描述性统计数据:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="ffd8" class="nh mg it nd b gy ni nj l nk nl">train.describe()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/ff1265bc98cee8b42295f573b773f986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*otcPRjVIRGudrhWkeZ8k2g.png"/></div></figure><p id="4b3b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在这个表格中，我们有前四个问题的答案。现在，让我们看看它是否因情绪而不同。如果它们显著不同，我们可以使用变量作为模型的特征:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="6bc0" class="nh mg it nd b gy ni nj l nk nl">num_vars = train.select_dtypes(np.number).columns<br/>train.groupby("sentiment")[num_vars].agg(["mean", "median"])</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi po"><img src="../Images/62679be7466e9371602e38825eed395c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geoGt3IyXvTZ24u9uP-LMA.png"/></div></div></figure><p id="3d15" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">从集中趋势来看，情绪似乎没有实质性的不同。为了确保万无一失，我们来看看分布情况:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="03ad" class="nh mg it nd b gy ni nj l nk nl">def plot_distribution(df, var, hue):<br/>    """Plot overlayed histogram and density plot per sentiment."""<br/>    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[16,4])<br/>    <br/>    # Histogram<br/>    sns.histplot(data=df, x=var, hue=hue, bins=30, kde=False, ax=ax[0])<br/>    ax[0].set_title(f"Histogram for {var}")<br/>    <br/>    # Density plot<br/>    sns.kdeplot(data=df, x=var, hue=hue, shade=True, ax=ax[1])<br/>    ax[1].set_title(f"Density plot for {var}");<br/>    <br/># Plot for all numerical variables<br/>for var in num_vars:<br/>    plot_distribution(train, var, 'sentiment')</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pp"><img src="../Images/ef4787d0050a08099484c00beee3e686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zqu5lTH6vl6bVLkpyawFRA.png"/></div></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pq"><img src="../Images/bcab9836b444a8e3862d5726c0ecc302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EL9c8tLm36_sg6QNXYC4LA.png"/></div></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pr"><img src="../Images/6a924a05811f753eb19748c2d082f29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2la5b_QbupmULbM48MnJaQ.png"/></div></div></figure><p id="30fa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">情感之间变量的分布似乎非常相似。它们不太可能作为有用的特性，但是我们总是可以尝试。也许我们可以把它添加到<em class="lf">一个值得尝试的项目列表中</em>？</p><p id="9991" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在我们结束之前，让我们看看最后一件事——常用词是否因情感不同而不同。让我们为每种情绪准备数据:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="1c83" class="nh mg it nd b gy ni nj l nk nl">pos_documents = [preprocess_text(document) for document in train.loc[train['sentiment']=='positive', 'review']]<br/>pos_tokens = [item for sublist in pos_documents for item in sublist]<br/>pos_freq = FreqDist(pos_tokens)<br/>pos_common = [word for word, frequency in pos_freq.most_common(20)]<br/>print(f"***** 20 frequent tokens in positive reviews: *****\n{pos_common}\n")</span><span id="2fb3" class="nh mg it nd b gy nm nj l nk nl">neg_documents = [preprocess_text(document) for document in train.loc[train['sentiment']=='negative', 'review']]<br/>neg_tokens = [item for sublist in neg_documents for item in sublist]<br/>neg_freq = FreqDist(neg_tokens)<br/>neg_common = [word for word, frequency in neg_freq.most_common(20)]<br/>print(f"***** 20 frequent tokens in negative reviews: *****\n{neg_common}\n")</span><span id="0187" class="nh mg it nd b gy nm nj l nk nl">common = set(neg_common).union(pos_common)<br/>print(f"***** Their union: *****\n{common}\n")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ps"><img src="../Images/f051bab6efa6584ce96707310d28efd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsAA2fzghrzXUgUnuXXdRA.png"/></div></div></figure><p id="5d18" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这两种情绪中最常见的三个符号是“电影”、“电影”和“一”。让我们看看它们的频率:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="400a" class="nh mg it nd b gy ni nj l nk nl"># Create a dataframe containing the common tokens and their frequency<br/>common_freq = pd.DataFrame(index=common, columns=["neg", "pos"])<br/>for token in common:<br/>    common_freq.loc[token, "pos"] = pos_freq[token]<br/>    common_freq.loc[token, "neg"] = neg_freq[token]<br/>common_freq.sort_values(by="pos", inplace=True)</span><span id="6ec2" class="nh mg it nd b gy nm nj l nk nl"># Add ranks and rank difference<br/>common_freq['pos_rank'] = common_freq['pos'].rank()<br/>common_freq['neg_rank'] = common_freq['neg'].rank()<br/>common_freq['rank_diff'] = common_freq['neg_rank'] - common_freq['pos_rank']<br/>common_freq.sort_values(by='rank_diff', inplace=True)<br/>common_freq.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/f41bf816e25a161cba563674036e8a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Oz7F4OudYPONogRApWJQxQ.png"/></div></figure><p id="7ddd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，是时候想象了:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="e880" class="nh mg it nd b gy ni nj l nk nl">fig, ax =plt.subplots(1, 2, figsize=(16, 10))<br/>sns.barplot(x="pos", y=common_freq.index, data = common_freq, ax=ax[0])<br/>sns.barplot(x="neg", y=common_freq.index, data = common_freq, ax=ax[1])<br/>fig.suptitle('Top tokens and their frequency by sentiment');</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pu"><img src="../Images/b50f6c2ff03c7e203896b539809a041b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A7SXQ6CZoMErSEFXXN4WnA.png"/></div></div></figure><p id="0be5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">嗯，有趣的是，在正面评论中,“film”比“movie”出现得更频繁。在负面评论中，它被翻转了。也许它们不应该被添加到停用词中，尽管它们出现的频率很高。我们再来看一下图表，但是排除这两个常用词:</p><pre class="lh li lj lk gt nc nd ne nf aw ng bi"><span id="e9ec" class="nh mg it nd b gy ni nj l nk nl">rest = common_freq.index.drop(['film', 'movie'])<br/>fig, ax =plt.subplots(1, 2, figsize=(16, 10))<br/>sns.barplot(x="pos", y=rest, data = common_freq.loc[rest], ax=ax[0])<br/>sns.barplot(x="neg", y=rest, data = common_freq.loc[rest], ax=ax[1])<br/>fig.suptitle('Top tokens and their frequency by sentiment');</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pv"><img src="../Images/a1d187aed98c4729a4efb4164297ce43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNOfuvTdWugSZtXxEygPjw.png"/></div></div></figure><p id="22f8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">很直观地看到，单词“棒”、“好”和“爱”在正面评价中更频繁出现，而“甚至”和“不好”在负面评价中更频繁出现。</p><p id="a1a8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">还有很多东西要探索，但是是时候总结了！🕛</p><h1 id="ac42" class="mf mg it bd mh mi pw mk ml mm px mo mp jz py ka mr kc pz kd mt kf qa kg mv mw bi translated">3.结束语💭</h1><p id="78e0" class="pw-post-body-paragraph kj kk it kl b km mx ju ko kp my jx kr ks mz ku kv kw na ky kz la nb lc ld le im bi translated">干得好，你走了这么远！😎让我们总结一下要点:<br/> ◼️在标记时删除标点和数字<br/> ◼️规范化文本(小写、字母等)<br/> ◼️用“br”和其他缺失的辅助动词丰富停用词<br/> ◼️删除罕见词</p><p id="733b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一个不错的尝试列表:<br/> ◼️将英式拼写转换为美式拼写(反之亦然)<br/> ◼️保留数字并将其转换为单词<br/> ◼️在标记时保留连字符<br/> ◼️包含二元模型<br/> ◼️添加数字特征，如句子、标记、字符和停用词的数量</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi qb"><img src="../Images/be71aa5d3b12a6dde646e3e1a18c3812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ash1110JcwwH9d6w"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图片由<a class="ae lw" href="https://unsplash.com/@andreaschu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Andreas Chu </a>在<a class="ae lw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4a05" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="lf">您想要访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae lw" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="lf">我的推荐链接</em></a><em class="lf">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="55f8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">谢谢你看我的帖子。探索性数据分析是一项开放式的主观任务。您可能已经注意到，在探索和预处理时，我们不得不做出许多小的选择。我希望这篇文章能让你体会到如何构建分析，并展示一些你可以在这个过程中思考的问题。做了一些探索性分析后，我们离构建模型更近了一步。在下一篇文章中，我们将为模型准备数据。以下是该系列另外两篇帖子的链接:<em class="lf"> <br/> </em> ◼️ <a class="ae lw" rel="noopener" target="_blank" href="/preprocessing-text-in-python-923828c4114f">用 Python 预处理文本</a> <br/> ◼️ <a class="ae lw" rel="noopener" target="_blank" href="/sentiment-classification-in-python-da31833da01b">用 Python 进行情感分类</a></p><p id="6643" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">以下是我的其他 NLP 相关帖子的链接:<br/>◼️<a class="ae lw" rel="noopener" target="_blank" href="/simple-wordcloud-in-python-2ae54a9f58e5">Python 中的简单 word cloud</a><br/><em class="lf">(下面列出了一系列关于 NLP 介绍的帖子)</em> <br/> ◼️ <a class="ae lw" rel="noopener" target="_blank" href="/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96">第一部分:Python 中的预处理文本</a> <br/> ◼️ <a class="ae lw" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">第二部分:词条满足和词干的区别</a> <br/> ◼️ <a class="ae lw" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">第三部分:TF-IDF 解释</a> <br/> ◼️ <a class="ae lw" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener">第四部分:python 中的监督文本分类模型</a><a class="ae lw" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener"/></p><p id="7874" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">再见🏃💨</p></div></div>    
</body>
</html>