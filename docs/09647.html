<html>
<head>
<title>Decoding Strategies that You Need to Know for Response Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成响应时需要了解的解码策略</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc?source=collection_archive---------7-----------------------#2020-07-09">https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc?source=collection_archive---------7-----------------------#2020-07-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="00f0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">区分波束搜索、随机采样、Top-K 和 Nucleus</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/54d24527295058383705cacec69dd5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qC-KFHQLToPASxawmUPWLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同的采样技术。注意，我们在这个例子中没有使用强生成模型。</p></figure><h1 id="5825" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="4101" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">深度学习已经被部署在 NLP 的许多任务中，例如翻译、图像字幕和对话系统。在机器翻译中，它用于读取源语言(输入)并生成所需的语言(输出)。类似地，在对话系统中，它用于在给定上下文的情况下生成响应。这也被称为自然语言生成(NLG)。</p><p id="de12" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">该模型分为两部分:编码器和解码器。编码器读取输入文本，并返回一个表示该输入的向量。然后，解码器获取该向量并生成相应的文本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/90f0d940b44b0b4ac47c7a5dfd83e2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yef6QgRpT1ktP6BpHelPmA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 1:编码器-解码器架构</p></figure><p id="f77d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">要生成文本，通常是通过一次生成一个标记来完成的。如果没有适当的技术，生成的响应可能非常普通和乏味。在本文中，我们将探讨以下策略:</p><ul class=""><li id="7b46" class="mp mq iq lp b lq mj lt mk lw mr ma ms me mt mi mu mv mw mx bi translated">贪婪的</li><li id="2cdb" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">波束搜索</li><li id="0c48" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">随意采样</li><li id="3531" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">温度</li><li id="4080" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">Top-K 抽样</li><li id="9491" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">细胞核取样</li></ul><h1 id="f971" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">解码策略</h1><p id="e7b2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在解码过程中的每个时间步长，我们取向量(保存从一个步骤到另一个步骤的信息),并用 softmax 函数将其转换为每个单词的概率数组。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/3e239f3bf5ff95576968441dbd76a314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*an6EilLiOG0G4KwoE0vKYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 1: Softmax 函数。x 是时间步长 I 处的令牌。u 是包含词汇表中每个令牌的值的向量。</p></figure><h2 id="905e" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">贪婪的方法</h2><p id="ef91" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这种方法是最简单的。在每个时间步，它只选择最有可能的记号。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="f049" class="ne kw iq nr b gy nv nw l nx ny">Context:            Try this cake. I baked it myself.<br/>Optimal Response  : This cake tastes great.<br/>Generated Response: This is okay.</span></pre><p id="ed50" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是，这种方法可能会产生次优的响应，如上例所示。生成的响应可能不是最佳响应。这是由于训练数据通常具有类似“这是[…]”的例子。因此，如果我们一次生成最可能的令牌，它可能输出“是”而不是“蛋糕”。</p><h2 id="fb48" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">波束搜索</h2><p id="aef9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">穷举搜索可以解决前面的问题，因为它将搜索整个空间。然而，这在计算上将是昂贵的。假设有 10，000 个词汇，要生成一个长度为 10 个单词的句子，将需要(10，000)个⁰.</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/415f9f0bddf3f65ca2c48f8480cc2a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nx7Slea9lBDs6uEx7VQQRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 2:波束宽度=2 的波束搜索(<a class="ae oa" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html" rel="noopener ugc nofollow" target="_blank">源</a></p></figure><p id="b5e5" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">波束搜索可以应对这个问题。在每个时间步，它生成词汇表中所有可能的标记；然后，它将选择概率最大的前 B 名候选人。那些 B 候选人将移动到下一个时间步，并且重复该过程。最后只会有 B 人选。搜索空间只有(10000)* b。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="4887" class="ne kw iq nr b gy nv nw l nx ny">Context:    Try this cake. I baked it myself.<br/>Response A: That cake tastes great.<br/>Response B: Thank you.</span></pre><p id="a87b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但有时，它会选择一个更优的方案(对策 B)。在这种情况下，这是完全合理的。但是想象一下，这个模型喜欢玩安全的游戏，并且在大多数情况下不断地产生“我不知道”或者“谢谢你”，这是一个非常糟糕的机器人。</p><h2 id="8034" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">随意采样</h2><p id="f413" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">或者，我们可以研究随机方法，以避免反应是通用的。我们可以利用来自 softmax 函数的每个令牌的概率来生成下一个令牌。</p><p id="eecb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">假设我们正在生成上下文的第一个单词“我喜欢看电影”，下图显示了第一个单词应该是什么的概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/2ad7de75feb3fd3c66a7419b39012f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2KSr9AnmQP6VRnmbE6fbQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 3:每个单词的概率。x 轴是令牌索引。即，索引 37 对应于单词“耶”</p></figure><p id="1718" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果我们使用贪婪的方法，将选择一个令牌“I”。然而，通过随机抽样，令牌<em class="oc"> i </em> <strong class="lp ir"> <em class="oc"> </em> </strong>有 0.2 左右的概率发生。同时，任何概率为 0.0001 的令牌也可能出现。只是可能性很小。</p><h2 id="930d" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">温度随机抽样</h2><p id="e278" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">随机抽样本身可能会偶然产生一个非常随机的单词。温度用于增加可能令牌的概率，同时减少不是可能令牌的概率。通常情况下，范围是 0 &lt; temp ≤ 1. Note that when temp=1, there is no effect.</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/23b6c46c24ef07154ce748928ab66f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41TqaBXrhIGU2V1JCEzU5Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Equation 2: Random sampling with temperature. Temperature t is used to scale the value of each token before going into a softmax function</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/b62dcd64d7e51b2b25d03040981c0668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9cXkz-TWG7-BS6CycahuQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Figure 4: Random sampling vs. random sampling with temperature</p></figure><p id="ac39" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">In Figure 4, with temp=0.5, the most probable words like <em class="oc"> i </em>，<em class="oc"> yeah </em>，<em class="oc"> me </em>，产生的几率较大。同时，这也降低了不太可能发生的事件的概率，尽管这并不能阻止它们的发生。</p><h2 id="3ae0" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">Top-K 抽样</h2><p id="d4e2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Top-K 采样用于确保不太可能的单词根本不会有任何机会。对于一个世代，应该只考虑前 K 个可能的记号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/127cbb481531856773dfc223402348cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xszHiw9cXHZfXZkAzf2ixw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 5:3 个随机抽样的分布，带 temp 的随机抽样和 top-k</p></figure><p id="13e1" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果我们使用温度=0.5 或 1.0 的随机抽样，50 到 80 之间的令牌指数有一些小概率。对于 top-k 采样(K=10)，这些令牌没有机会被生成。请注意，我们也可以将 Top-K 采样与温度相结合，但是您可能已经了解了这个概念，所以我们选择不在这里讨论它。</p><p id="4ea2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这种采样技术已在许多新一代任务中采用。它的性能相当不错。这种方法的一个限制是需要在开始时定义前 K 个单词的数量。假设我们选择 K = 300 然而，在解码时间步长，模型确定应该有 10 个极有可能的单词。如果我们使用 Top-K，这意味着我们也将考虑其他 290 个可能性较小的单词。</p><h2 id="1263" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lw nj nk lh ma nl nm lj me nn no ll np bi translated">细胞核取样</h2><p id="6556" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">细胞核采样类似于 Top-K 采样。nucleus sampling 不是关注 Top-K 单词，而是关注 Top-V 单词的最小可能集合，使得它们的概率之和≥ p。然后，不在 V^(p 中的记号被设置为 0；其余的被重新缩放，以确保它们的总和为 1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/74e9cb59fb54e9dc215eb64292506f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIM24H7IclafNGWmIaTZZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式 3:细胞核取样。V^(p)是最小的代币。P(x|…)是给定先前生成的从 1 到 i-1 的记号 x，生成记号 x 的概率</p></figure><p id="3ddf" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">直觉是，当模型对一些记号非常确定时，潜在候选记号的集合很小，否则，将会有更多的潜在候选记号。</p><blockquote class="oh oi oj"><p id="d8fc" class="ln lo oc lp b lq mj jr ls lt mk ju lv ok ml ly lz ol mm mc md om mn mg mh mi ij bi translated">C <em class="iq">确定→那几个记号概率大=几个记号之和足以超过 p. <br/>不确定→很多记号概率小=需要很多记号之和才能超过 p. </em></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/4704dbd1378e9c5ff37f06f1b066469a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjCJSAxvMa3HhisMl7D8Ug.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 6:Top-K 和 Nucleus 采样的分布</p></figure><p id="4d3d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">将 nucleus 采样(p=0.5)与 top-K 采样(K = 10)进行比较，我们可以看到 nucleus 不认为令牌“you”是候选。这说明它可以适应不同的情况，选择不同数量的令牌，不像 Top-K 采样。</p><h1 id="cce4" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">摘要</h1><ul class=""><li id="cfef" class="mp mq iq lp b lq lr lt lu lw oo ma op me oq mi mu mv mw mx bi translated">贪婪:一次选择最可能的令牌</li><li id="7185" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">波束搜索:选择最可能的响应</li><li id="f9ab" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">随机抽样:基于概率的随机抽样</li><li id="42fd" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">温度:缩小还是放大概率</li><li id="4408" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">前 K 个采样:选择前 K 个可能的令牌</li><li id="d009" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">细胞核取样:动态选择 K 的数量(排序)</li></ul><p id="27c2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">通常，研究人员的首选是波束搜索、top-K 采样(带温度)和原子核采样。</p><h1 id="3ee0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="597b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们已经讨论了一系列解码响应的不同方法。这些技术可以应用于不同的生成任务，即图像字幕、翻译和故事生成。使用具有坏解码策略的好模型或者具有好解码策略的坏模型是不够的。两者之间的良好平衡会让这一代人更有趣。</p><h1 id="c7b6" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><p id="fba1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">霍尔茨曼，a .，布茨，j .，杜，l .，福布斯，m .，，崔，Y. (2020)。神经文本退化的奇特案例。在<em class="oc">国际学术交流会议</em>。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/word-sequence-decoding-in-seq2seq-architectures-d102000344ad"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">Seq2Seq 架构中的字序列解码</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">自然语言生成(NLG)是一项生成文本的任务。像机器这样的自然语言生成任务…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi kp ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/an-intuitive-explanation-of-beam-search-9b1d744e7a0f"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">波束搜索的直观解释</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">波束搜索的简单易懂的解释</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi kp ou"/></div></div></a></div></div></div>    
</body>
</html>