<html>
<head>
<title>Feature Selection? You Are Probably Doing It Wrong</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择？你可能做错了</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-you-are-probably-doing-it-wrong-985679b41456?source=collection_archive---------13-----------------------#2020-04-09">https://towardsdatascience.com/feature-selection-you-are-probably-doing-it-wrong-985679b41456?source=collection_archive---------13-----------------------#2020-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9caf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">…除非您正在使用这个Python包</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6c4ab3b2b14ee6ac5726bcf5c32920aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MybZMEAuwmNwUCLH4K1A5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一张关于樱桃采摘的精美照片(版权所有<a class="ae ky" href="https://www.shutterstock.com/image-photo/female-hand-picking-red-cherry-isolated-533345503" rel="noopener ugc nofollow" target="_blank"> Shutterstock </a></p></figure><p id="0190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我敢打赌，在那里，99%的特征选择是在“SelectFromModel”模式下进行的(这个名字来自Scikit-learn的<code class="fe lv lw lx ly b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html" rel="noopener ugc nofollow" target="_blank">SelectFromModel</a></code>)，它是这样工作的:</p><ol class=""><li id="494b" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">你选择一个预测模型(姑且称之为<code class="fe lv lw lx ly b">WhatevBoost</code>)；</li><li id="c808" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">你符合<code class="fe lv lw lx ly b">WhatevBoost</code>的所有特征；</li><li id="5470" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">你从<code class="fe lv lw lx ly b">WhatevBoost</code> <em class="mn">中提取特征重要性；</em></li><li id="d330" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">您删除了所有重要性低于某个阈值的特性，保留了所有其他特性。</li></ol><p id="1aec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这似乎是一个合理的方法，对不对？答案可能会让你大吃一惊。</p><p id="3448" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将测试这个过程是否真的如预期的那样工作。最后，我将展示一个出色的——但仍然鲜为人知的——用于特性选择的Python包，它恰好优于所有其他测试过的算法。</p><h1 id="6b91" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">给我数据</h1><p id="e207" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在本帖中，我们将使用模拟数据。</p><p id="d4db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道数据科学家对模拟数据嗤之以鼻，但这是测试事物的有效方法。在数据科学中，这是我们能够真正了解真相的唯一情况。</p><p id="1949" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模拟数据集会是什么样子？我们希望生成一些独立的特征:其中一些将与目标变量(称为<code class="fe lv lw lx ly b">y</code>)相关，而其他特征将只是噪声。目的是找到一种能够确定哪些特征属于第一组和哪些特征属于第二组的特征选择方法。</p><p id="70c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的特征矩阵由16个独立特征组成，我们将其命名如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="a67f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能已经从它们的名字猜到了，前6个特性与<code class="fe lv lw lx ly b">y</code>有某种关系，而后10个只是噪音。</p><p id="a856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">y</code> <em class="mn"> </em>的函数形式应该足够复杂，能够包含一些自然界中存在的变量之间的非平凡效应。特别是，我们希望考虑以下类型的关系:</p><ul class=""><li id="31a1" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu nn mf mg mh bi translated">线性的</li><li id="c09e" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu nn mf mg mh bi translated">非线性的</li><li id="91a7" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu nn mf mg mh bi translated">相互作用</li></ul><p id="27a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而且<code class="fe lv lw lx ly b">X</code> <em class="mn"> </em>和<code class="fe lv lw lx ly b">y</code> <em class="mn"> </em>的关系应该是非确定性的，所以增加了一个误差因子，叫做<em class="mn"> ε </em>。为了说明这一切，我想出了下面的函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/a06f8d67368050b37cb2c3c968d1d2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7SyNuRwh_dyTMVpe-aBuw.png"/></div></div></figure><p id="acc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这转化为以下Python代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="4f38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后我们可以生成<em class="mn"> X </em>和<em class="mn"> y </em>，然后在一个训练集和一个测试集中拆分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="b756" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">尝试“SelectFromModel”模式</h1><p id="a64f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">现在我们有了数据，是时候挑选一些预测模型，并在“SelectFromModel”模式下进行尝试了。我们来看8个模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="6384" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很有可能你听过所有的，但最后一个。你可能会挠头问“这个<code class="fe lv lw lx ly b">UnbeatableRegressor</code>到底是什么？”。好吧，因为我们的数据是用我们确实知道的函数模拟的，所以我们可以得到每个数据科学家的圣杯:完美的模型。它是y的功能形式，不用说，它是不可战胜的。</p><p id="c451" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得与scikit兼容的回归器，我们只需将我们的函数包装在一个类中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="a4a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们根据训练数据拟合我们的模型，看看它们在平均绝对误差方面的表现如何。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/baa5fb18f25de47e0420b5832c849a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkXqe_MYf0Z0y_QIxKgnNA.png"/></div></div></figure><p id="9d68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，这并不奇怪:更酷的算法，如<code class="fe lv lw lx ly b">XGBoost</code>或<code class="fe lv lw lx ly b">LightGBM</code>比其他算法表现得更好(即使它们离理论最小值还很远)。</p><p id="5d0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们关注特性重要性，这是“SelectFromModel”模式的核心。</p><p id="1545" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征重要性在机器学习中是一个相当难以捉摸的概念，这意味着没有一种明确的方法来计算它。无论如何，这个想法非常直观:这是一种量化任何单一特征对预测模型准确性的贡献的方式。</p><p id="ab4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值为0%的特性不会以任何方式影响模型的准确性(因此可以将其删除)。值为100%的特征是影响预测的唯一特征。在实际应用中，特性从来不是0%或100%重要，它们总是介于两者之间。</p><p id="cb23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，由于我们只有16个特征，并且其中的后10个特征(从<code class="fe lv lw lx ly b">noise_1</code> <em class="mn"> </em>到<code class="fe lv lw lx ly b">noise_10</code>)与<code class="fe lv lw lx ly b">y</code>没有任何关系，我们期望这些特征的重要性为0%，其余6个特征(<code class="fe lv lw lx ly b">linear</code>、<code class="fe lv lw lx ly b">nonlinear_square</code>、<code class="fe lv lw lx ly b">nonlinear_sin</code>、<code class="fe lv lw lx ly b">interaction_1</code>、<code class="fe lv lw lx ly b">interaction_2</code>和<code class="fe lv lw lx ly b">interaction_3</code>)在它们之间共享所有的重要性。</p><p id="11c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是让我们更保守一点:让我们取1%的安全缓冲:我们将丢弃任何重要性低于1%的特性。这个阈值是相当武断的，但是，考虑到我们只处理16个特征，似乎有理由相信低于1%重要性的特征实际上是无用的。</p><p id="46f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，显示了所有8个模型的特征重要性。此外，重要性低于1%的特性在其名称旁边标有“<em class="mn">【DROP】”</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/befef0dd97d6f2ef2750cc9b96eb565d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ch9cFgn2a-Bk2Sm_mKXTmQ.png"/></div></div></figure><p id="da53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">令人惊讶的是，最差的模型之一——T0——是唯一一个正确猜测所有相关特征的模型(但是，应该注意的是,<code class="fe lv lw lx ly b">nonlinear_sin</code>由一根头发保持，因为它的重要性是1.17%:稍微不同的阈值就会改变事情)。</p><p id="4eeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">UnbeatableRegressor</code>提供的更接近特征重要性的“实际”形状的模型是<code class="fe lv lw lx ly b">LGBMRegressor</code>，然而，许多嘈杂的特征被高估了，在这个意义上它们超过了1%。</p><p id="64f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会惊讶于这样一个事实，性能更好的模型不一定特性选择更好:<strong class="lb iu">拥有一个优秀的模型并不能保证特性选择的成功</strong>。此外，这种进行方式受到重要性阈值选择的严重影响:不同的阈值会给出完全不同的结果。</p><p id="4f14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于这些原因，我开始寻找另一种选择。直到我在波鲁塔绊倒。</p><h1 id="868b" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">使用Boruta进行特征选择</h1><p id="c080" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">Boruta是一种可以追溯到2010年的特征选择算法。它是作为R的一个包而诞生的(<a class="ae ky" href="https://www.jstatsoft.org/article/view/v036i11" rel="noopener ugc nofollow" target="_blank">)这个</a>是米隆·库尔萨(Miron Kursa)和维托尔德·鲁德尼克(Witold Rudnickii)的原创文章，丹尼尔·霍莫拉(Daniel)的《蟒蛇》改编本:你可以在这里找到相关的项目<a class="ae ky" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="875c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boruta背后的直觉非常聪明，值得在一篇专门的文章中解释。在本文中，我们将限制自己使用Python的Boruta库，它可以通过<code class="fe lv lw lx ly b">pip install Boruta</code>安装。</p><p id="fa44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">波鲁塔是一个包装纸。建议用<code class="fe lv lw lx ly b">max_depth = 5</code>包裹一片随机森林。不用考虑参数调整确实是一个很大的好处。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="1690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会问自己:“我们为什么使用<code class="fe lv lw lx ly b">RandomForestRegressor</code>？在之前的实验中，它是表现最差的模型之一”。好吧，这正是Boruta的美妙之处:尽管它是随机森林之类的弱模型的包装，但它的设计是为了克服它的缺点。</p><p id="5163" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boruta的输出是一个特征排序，它可以将特征细分为3个类别:</p><ul class=""><li id="7cf9" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu nn mf mg mh bi translated"><strong class="lb iu">排序1:确认特征</strong>(这些特征对目标变量有一定的信号性，应当保留)；</li><li id="ee8a" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu nn mf mg mh bi translated"><strong class="lb iu">排名第2:暂定特征</strong>(博鲁塔对这些特征举棋不定，由你来选择，我是个保守的人，我的建议是保留它们)；</li><li id="2037" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu nn mf mg mh bi translated"><strong class="lb iu">排名3或更高:被拒绝的特性</strong>(这些只是噪音，所以应该被丢弃)。</li></ul><p id="f98b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以绘制排名并可视化3个区域:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/cb8da1af22ba653c7730649e82a73f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-y04Fli1oYrQTw4ISm-eIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过Boruta获得的特性排名</p></figure><p id="e0f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boruta做了一项惊人的工作:它从剩余的10个噪声特征中分离出了6个有意义的特征，正如我们所预期的那样。</p><h1 id="5c01" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">总结</h1><p id="2c40" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在本文中，我们使用模拟数据来证明，即使是Xgboost或LightGBM这样的复杂模型，在选择特性时也可能不是一个好的选择。</p><p id="6474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为替代，我们提出了一个名为Boruta的相当未知但功能强大的Python包，它已经交付了预期的结果。</p><p id="e546" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对Boruta的功能感兴趣(我相信理解算法是有效使用它们的唯一方法)，你可以阅读我写的关于这个主题的文章:“<a class="ae ky" rel="noopener" target="_blank" href="/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a"> Boruta准确地解释了你希望有人如何向你解释</a>”。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="80a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的Python代码可以在这里找到:<a class="ae ky" href="https://github.com/smazzanti/tds_feature_selection_you_are_probably_doing_it_wrong" rel="noopener ugc nofollow" target="_blank">https://github . com/smazzanti/TDS _ feature _ selection _ you _ are _ possible _ doing _ it _ error</a></p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="b954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读！希望这篇帖子有用。我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，你可以在这里给我发短信或在<a class="ae ky" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>。</p></div></div>    
</body>
</html>