<html>
<head>
<title>5 Spark Best Practices For Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5 Spark 数据科学最佳实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-spark-best-practices-61587a35ac15?source=collection_archive---------11-----------------------#2020-07-26">https://towardsdatascience.com/5-spark-best-practices-61587a35ac15?source=collection_archive---------11-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c179" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我希望在开始我的项目之前就知道</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad47aa4b2ecc20ac609e502dd24de3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RK0KlfS7xoYkD-ks"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> chuttersnap </a>拍摄</p></figure><h1 id="8e9a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么要搬到 spark？</h1><p id="f52d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">虽然我们都<a class="ae ky" href="https://www.facebook.com/dan.ariely/posts/904383595868" rel="noopener ugc nofollow" target="_blank">谈论大数据</a>，但通常在你的职业生涯中需要一些时间，直到你遇到它。对我在 Wix.com 的工作来说，这比我想象的要快，超过 1 . 6 亿的用户产生了大量的数据——随之而来的是对扩展我们数据处理的需求。</p><p id="acec" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然有其他选择(例如<a class="ae ky" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask </a>)，但我们决定使用 Spark，主要有两个原因:(1)它是当前的技术水平，广泛用于大数据。(2)我们有了 Spark inplace 所需的基础设施。</p><h1 id="71c2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">如何用 PySpark 为熊猫们写作</h1><p id="04b7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可能很熟悉熊猫，我说的熟悉是指流利，你的母语:)</p><p id="d72a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面这个演讲的标题说明了一切— <a class="ae ky" href="https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas" rel="noopener ugc nofollow" target="_blank">了解熊猫的数据科学家与 PySpark 的数据争论</a>这是一个伟大的争论。</p><p id="d393" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这将是一个很好的时机来说明，简单地获得正确的语法可能是一个很好的起点，但是对于一个成功的 PySpark 项目来说，你需要更多的东西，你需要理解 Spark 是如何工作的。</p><blockquote class="ms"><p id="fef6" class="mt mu it bd mv mw mx my mz na nb mm dk translated">让 Spark 正常工作很难，但当它工作时——它工作得非常好！</p></blockquote><h1 id="aa09" class="kz la it bd lb lc ld le lf lg lh li lj jz nc ka ll kc nd kd ln kf ne kg lp lq bi translated">一言以蔽之</h1><p id="5d23" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我在这里只会深入膝盖，但我建议访问以下文章并阅读 MapReduce 解释，以获得更广泛的解释— <a class="ae ky" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a">使用 Spark 处理大数据的搭便车指南</a>。</p><p id="cf37" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们要理解的概念是<strong class="lt iu">水平缩放。</strong></p><p id="0264" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从<strong class="lt iu">垂直缩放开始更容易。如果我们有一个运行良好的 pandas 代码，但是数据对它来说变得太大了，我们可以转移到一个更强大的有更多内存的机器上，并希望它能管理。这意味着我们仍然有一台机器同时处理全部数据——我们纵向扩展了<strong class="lt iu">。</strong></strong></p><p id="067c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">相反，如果我们决定使用 MapReduce，将数据分割成块，让不同的机器处理每个块——我们就在横向扩展<strong class="lt iu"/>。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="13eb" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">5 Spark 最佳实践</h1><p id="3176" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这些是 5 spark 最佳实践，帮助我将运行时间减少了 10 倍，并扩展了我们的项目。</p><h2 id="09b4" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">1 -从小处着手—对数据进行采样</h2><p id="f2c3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们想让大数据发挥作用，我们首先希望看到我们使用一小块数据的方向是正确的。在我的项目中，我对 10%的数据进行了采样，并确保管道正常工作，这允许我使用 Spark UI 中的 SQL 部分，并看到数字在整个流程中增长，同时不会等待流程运行太长时间。</p><p id="8404" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从我的经验来看，如果你用小样本达到了你想要的运行时，你通常可以很容易地扩大规模。</p><h2 id="6e68" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">2 -了解基础知识—任务、分区、内核</h2><p id="64c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这可能是使用 spark 时需要理解的最重要的事情:</p><blockquote class="ms"><p id="3384" class="mt mu it bd mv mw mx my mz na nb mm dk translated"><strong class="ak"> 1 个分区支持在 1 个内核上运行 1 个任务</strong></p></blockquote><p id="78cc" class="pw-post-body-paragraph lr ls it lt b lu od ju lw lx oe jx lz ma of mc md me og mg mh mi oh mk ml mm im bi translated">您必须时刻注意您拥有的分区数量——跟踪每个阶段的任务数量，并将它们与您的 spark 连接中正确的内核数量相匹配。一些提示和经验法则可以帮助你做到这一点(所有这些都需要在你的案例中进行测试):</p><ul class=""><li id="cc4f" class="oi oj it lt b lu mn lx mo ma ok me ol mi om mm on oo op oq bi translated">任务与内核之间的比率应该是每个内核大约 2-4 个任务。</li><li id="1961" class="oi oj it lt b lu or lx os ma ot me ou mi ov mm on oo op oq bi translated">每个分区的大小应该在 200-400 MB 之间，这取决于每个工作者的内存，根据你的需要进行调整。</li></ul><h2 id="b971" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">3 -调试火花</h2><p id="4a49" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Spark 使用惰性求值，这意味着它在执行计算指令图之前会一直等到一个动作被调用。动作的例子有<code class="fe ow ox oy oz b">show(), count(),...</code></p><p id="4237" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这使得我们很难理解代码中哪里有需要优化的错误/地方。我发现很有帮助的一个实践是使用<code class="fe ow ox oy oz b">df.cache()</code>将代码分成几个部分，然后使用<code class="fe ow ox oy oz b">df.count()</code>强制 spark 计算每个部分的 df。</p><p id="4dc1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，使用 spark UI，您可以查看每个部分的计算并找出问题。重要的是要注意，如果不使用我们在(1)中提到的采样，使用这种实践可能会创建一个很长的运行时间，这将很难调试。</p><h2 id="5d39" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">4 -寻找和解决偏斜</h2><p id="31a5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们从定义偏斜度开始。正如我们提到的，我们的数据被划分为分区，随着转换的进行，每个分区的大小可能会发生变化。这可能会在分区之间产生很大的大小差异，这意味着我们的数据存在偏斜。</p><p id="a37f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过查看 spark UI 中的阶段细节，并寻找最大值和中值之间的显著差异，可以找到偏斜度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/3738d36886a192f1e2e6541e77ad6d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DzgF5k4S5d-NYC2fRPfx1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">较大的方差(中位数= 3 秒，最大值= 7.5 分钟)可能表明数据存在偏斜</p></figure><p id="d890" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这意味着我们有一些任务比其他任务慢得多。</p><p id="b94b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为什么这不好——这可能会导致其他阶段等待这几个任务，并让内核等待而不做任何事情。</p><p id="6600" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最好是，如果你知道偏斜来自哪里，你可以直接解决它，并改变划分。如果您不知道/没有直接解决它的选项，请尝试以下方法:</p><p id="e782" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">调整任务与内核的比例</strong></p><p id="fc34" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们提到的，通过拥有比内核更多的任务，我们希望当较长的任务运行时，其他内核将继续忙于其他任务。虽然这是真的，但前面提到的比率(2-4:1)并不能真正解决任务持续时间之间如此大的差异。我们可以尝试将比例提高到 10:1，看看是否有帮助，但这种方法可能有其他缺点。</p><p id="56bf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">加盐数据</strong></p><p id="8331" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Salting 是用一个随机键对数据进行重新分区，这样新的分区就会平衡。下面是 pyspark 的代码示例(使用 groupby，这通常是导致偏斜的原因):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="fa9a" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">spark 中迭代代码的问题</h2><p id="f359" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这一次真的很难。正如我们提到的，spark 使用惰性评估，所以当运行代码时，它只构建一个计算图，一个 DAG。但是当你有一个迭代过程时，这种方法可能是非常有问题的，因为 DAG 重新打开先前的迭代，并且变得非常大，我的意思是非常非常大。对于驱动程序来说，这可能太大而无法保存在内存中。这个问题很难定位，因为应用程序被卡住了，但它出现在 spark UI 中，就好像很长时间没有作业运行(这是真的)——直到驱动程序最终崩溃。</p><p id="d24f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是目前 spark 的一个固有问题，对我有效的解决方法是每 5-6 次迭代使用<code class="fe ow ox oy oz b">df.checkpoint() / df.localCheckpoint()</code>(通过一点试验找到你的数字)。这样做的原因是<code class="fe ow ox oy oz b">checkpoint()</code>正在打破血统和 DAG(不像<code class="fe ow ox oy oz b">cache()</code>)，保存结果并从新的检查点开始。缺点是，如果发生了不好的事情，您没有完整的 DAG 来重新创建 df。</p><h1 id="0ea8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="92a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如我之前所说的，学习如何让 spark 发挥其魔力需要时间，但这 5 个实践确实推动了我的项目向前发展，并在我的代码上洒下了一些 spark 的魔力。</p><p id="781a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，这是我开始我的项目时一直在寻找(但没有找到)的帖子——我希望你及时找到了它。</p><h2 id="f6f6" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">参考</h2><ul class=""><li id="2f5c" class="oi oj it lt b lu lv lx ly ma pd me pe mi pf mm on oo op oq bi translated"><a class="ae ky" href="https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas" rel="noopener ugc nofollow" target="_blank">与 PySpark 争夺了解熊猫的数据科学家</a></li><li id="9ce6" class="oi oj it lt b lu or lx os ma ot me ou mi ov mm on oo op oq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a">使用 Spark 处理大数据的搭便车指南</a></li><li id="5302" class="oi oj it lt b lu or lx os ma ot me ou mi ov mm on oo op oq bi translated"><a class="ae ky" href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a> —关于监控和调试的第 18 章令人惊叹。</li></ul></div></div>    
</body>
</html>