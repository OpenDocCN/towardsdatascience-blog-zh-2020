# 简化机器学习中的决策树

> 原文：<https://towardsdatascience.com/simplifying-the-decision-tree-in-machine-learning-66305f439643?source=collection_archive---------40----------------------->

## 最流行和最常用的 ML 算法之一

![](img/5df1f016a066bbb3f72fd8a8573ad2dc.png)

资料来源:联合国人类住区规划署

这是机器学习最简单和最基本的模型之一，可以用于分类和回归。让我们以我们熟悉的问答方式深入了解决策树。我们在这里还有一个理论[的视频讲座](https://youtu.be/JZm152gQUIU)，在这里还有 python [的实际操作](https://youtu.be/xm-462KADOs)。

**决策树有哪些组成部分？**

基本成分是

一个**)根节点:**这通常会在节点中提供完整的训练数据。来自所有类的数据将混合在一起。

b) **决策节点:**通常情况下，它的格式类似于属性“A”和条件“k”。例如年龄> 30，这创建了另外两个节点。

c) **内部节点和终端节点:**内部节点是决策节点的结果，如果它们足够纯，它们可以是终端节点，否则它们是内部节点，这将需要再次经过决策节点。*决策节点可以清楚的告诉我们，一个未知的观测如果落在那个节点会被贴上什么标签。*

让我们用一个非常简单的例子来说明，假设我们有 10 个学生的数据，他们的语言能力和数学成绩。根据学生的安置成功率，将学生分为三类。这三个类分别被立即放置(PI)、一段时间后放置(PA)、未放置(U)。

![](img/2205a5f49048c7839f63f1e518764fe4.png)

图 1:样本学生安置数据(来源:作者)

当一个决策树，它将有一个结构如下，为上述数据。该树具有一个根节点、两个决策节点(数学=高，英语=高)和对应于三个类别的三个终端节点。

![](img/d2438fe191afee3767d7a39e20204d80.png)

图 2:学生安置数据的决策树(来源:作者)

**观察结果**:

*   就类而言，根节点是非常混合的，当我们移动到终端节点时，节点具有更同质的群体(最多 2 个类，1 个类是大多数)。
*   现在，如果我们有一个学生萨姆，我们知道他数学和英语成绩都很高，按照决策树，我们知道，他被立即安排的机会非常高。

因此，一个有争议的问题是，我们希望向更同质的终端节点发展。这需要定义一个同质性或缺乏同质性的衡量标准。

**我们如何正式度量一个节点的同质性或异质性？**

这需要引入一个叫做熵的概念。

**熵:**

它是对随机变量中存在的不确定性的一种度量。根节点是所有类的混合，所以如果我们随机选择一个，不确定性在根节点是最高的，并且我们向终端节点移动，这种不确定性逐渐降低。我们称之为*信息增益*。

我们现在明白了，熵将是概率的函数。

![](img/436008b1ed0f57b92a329328bc4c16d2.png)

图 3:熵方程(来源:作者)

> X 是随机变量，H(X)是随机变量的熵。s 是随机变量或样本空间的一组值。s 是采样点。我想你会同意抛硬币是任何随机实验的终极 hello world。

当它是一个公平的硬币时，它是完全随机或不确定的，逐渐地我们可以偏向它，使它甚至为 1。让我们用例子再次理解这一点。我们来看下头部概率的不同场景。

![](img/d08850cb1f7d765c8bb1eee1c10bfd80.png)

图 4:熵值与头部概率(来源:作者)

**观察**:

*   当头部和尾部具有相等的概率时，熵最高
*   当压头完全确定时，它是最低的
*   在这两者之间，随着获得优势的确定性增加，熵值下降了

所以，我们知道我们应该从高熵的节点转移到低熵的节点，但是如果有两个属性‘a’和‘b’，我们如何使用熵来做出这个决定。

> 信息增益进入的时间。

**什么是信息增益？**

这由下面的等式正式定义。这个想法很简单。假设根节点具有熵 E，并且当我们在某种条件下(比如数学分数=高)通过属性“a”进行分割时，我们得到另外两个节点 N1 和 N2，它们具有相应的熵 E1 和 E2。

第一个切割信息增益将是 E -( E1 + E2)/2。下一步，我们只是做了一个改进，用 N1 和 N2 分别出现的元素数量来衡量熵。这就是下面等式中的形式。

H(T) —分裂前的熵 H(T/a) —在某种条件下被属性‘a’分裂后的熵

![](img/db2ced1313b26ae97926da7e2e09fc5a.png)

图 5:信息增益方程(来源作者)

**如何应用信息增益来选择要拆分的属性？**

让我们用必要的数学知识进一步理解这一点，让我们比较两种可能的拆分选项，如“A”和“B”

![](img/6985f200e70b4cbfb94591cdcf40bd6d.png)

图 6:分割选项(图片:作者)

信息增益的计算如下所示

![](img/29c07ce5b75aa36c4d88de6c258ad642.png)

图 7:通过属性 A 或 B 分割的选项(来源:作者)

**观察**

*   场景 A 的熵为 0.81，场景 B 的熵为 0.69
*   根节点的熵为 1.0(与投掷公平硬币的场景相同)
*   因此，信息增益在第一种情况下为 0.19，在第二种情况下为 0.31 **。因此，将选择属性 B。**

> 还有另一个衡量信息增益的等效指标，叫做基尼指数。根据参考文献[1]，只有 2%的情况下使用基尼指数或信息增益才重要。所以，我们不要再提这件事了。

这些树有哪些不同的变种？

*   这更具有学术意义，我们在这里讨论初步的树，如 ID3、C4.5、C5.0 和 CART。
*   **ID3** 迭代二分器 3 使用信息增益。由罗斯·昆兰于 1986 年发明
*   它首先找到要分割的最佳分类属性。
*   然后递归选择下一个属性(未选择)
*   直到不可能再进行拆分，即到达叶节点或没有属性留下来进行拆分
*   这是一个贪婪的算法，不回溯。
*   **C4.5** 取消了分类属性的限制，但允许将数值属性转换为分类属性(J48)
*   C 4.5 有能力处理丢失的数据 C4.5 有能力回溯它在数据挖掘的十大算法中排名第一之后变得非常流行
*   C 5.0 在生成更小的树、有效使用内存、加速
*   **CART** 是另一个非常著名的算法，它可以处理数值和分类属性，可以处理分类和回归，并且具有处理异常值的能力。它的分裂是双向分裂。

**我们如何处理决策树中的过度拟合？**

像任何机器学习问题一样，这些模型也容易过度拟合。如果一棵树被过度种植，那么它通常会有更多的深度。更多数量的终端节点。所以，一棵光滑的树会更少的过度生长。我们在这里讨论一些策略。

**a)叶或末端节点中的最小观察数:**

理想情况下，为了拟合数据，我们最终可以在所有叶节点中进行一次观察。会很好地拟合训练数据，测试数据失败

**b)分割一个节点所需的最小观察次数:**

这将确保我们不会为了更高的训练集精度而继续分裂甚至具有少量观察值的节点

最大允许深度:

基本上，随着最大允许深度的增加(基本上是连续决策节点的数量)，树往往会过度拟合。

![](img/0b97fcb454d6e5526885b992411cc329.png)

图 8:最大深度对测试集准确性的影响(来源:作者)

上图是我们使用决策树([https://www.kaggle.com/saptarsi/decision-tree-sg/](https://www.kaggle.com/saptarsi/decision-tree-sg/))进行实验的结果，还有一些有趣的实验，你可以了解一下。

它清楚地显示了 12 的深度是好的，在此之后，测试精度没有太大的增加。

**d)正则化损失函数:**

下面的等式是在回归树的上下文中，其中 T 是叶节点的数量，基本上，决策树将特征空间分割成矩形区域。如果有 T 个节点，则在特征空间中将有 T 个矩形判定区域。随后的目标函数的第一部分是正则平方误差，第二部分针对决策区域的数量进行惩罚或正则化。

> 所以，如果我们有两棵树，都有相同的平方误差。树 1 有 5 个叶节点，树 2 有 4 个叶节点。这个等式会更喜欢第二个。(转载自 ISLR)

![](img/04b1c4dd5384a05e5841f64e9767f067.png)

图 9:决策树目标函数(来源:作者)

**结论**:

*   决策树提供了极好的可视化，分类的原因对业务用户来说是清楚的，因此是一个白盒模型。
*   非常简单易懂
*   但是，可能无法提供最佳性能(这可以使用随机森林、增强等解决。)

========================================

谢谢你读到这里。这些是我们在加尔各答大学数据科学实验室创建的一些附加资源

一)用 Python 分类([https://www.youtube.com/playlist?list = plts 7 rwcd 0 do 2 zoo 4 sad 3 jrxnvfyxhd 6 _ S](https://www.youtube.com/playlist?list=PLTS7rWcD0Do2ZoO4Sad3jRxnVFyxHd6_S)

b)用 Python([https://www.youtube.com/playlist?)进行聚类 list = plts 7 rwcd 0 do 3ts 44 xgwgvgxmeohyfi 3pm](https://www.youtube.com/playlist?list=PLTS7rWcD0Do3ts44xgWGVgxMeoHYFi3pM))

**参考:**

[1] Raileanu LE，Stoffel K .基尼指数与信息增益标准的理论比较。数学与人工智能年鉴。2004 年 5 月 1 日；41(1):77–93.

[2][https://towards data science . com/the-complete-guide-to-decision-trees-28 a4 E3 C7 be 14](/the-complete-guide-to-decision-trees-28a4e3c7be14)

[3][https://towards data science . com/decision-trees-in-machine-learning-641 B9 c4e 8052](/decision-trees-in-machine-learning-641b9c4e8052)