<html>
<head>
<title>K-Means vs. DBSCAN Clustering — For Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means 与 DBSCAN 聚类—适用于初学者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-vs-dbscan-clustering-49f8e627de27?source=collection_archive---------3-----------------------#2020-05-27">https://towardsdatascience.com/k-means-vs-dbscan-clustering-49f8e627de27?source=collection_archive---------3-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d0bb6a6b58c9cefed235b3a9fca2e35e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOZnjFUV8WKaEfIAvW8nTQ.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源——un splash</p></figure><h1 id="26f2" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">聚类——一种无监督的机器学习技术</h1><p id="a772" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">聚类是将未标记的数据点以这样的方式分组:<br/> <strong class="lf iu"> <em class="mb">同一组内的数据点彼此相似，<br/>不同组内的数据点彼此不相似。</em> </strong> <br/>目标是创建具有高的类内相似性和低的类间相似性的类。</p><h1 id="8f98" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">k 均值聚类</h1><p id="b769" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="mb"> K-Means 聚类是最常用的无监督机器学习聚类技术之一。这是一种基于质心的聚类技术，需要您决定聚类(质心)的数量，并随机放置聚类质心来开始聚类过程。目标是重复地将 N 个观察值分成 K 个簇，直到不能形成更多的组。</em></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/b9ee6297374b9e021f99298d837eca47.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*V4iOCW8MDwtIKg0z5iEmaQ.png"/></div></figure><h2 id="bc74" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">K 均值的优势</h2><ul class=""><li id="2bbd" class="mt mu it lf b lg lh lk ll lo mv ls mw lw mx ma my mz na nb bi translated">易于理解和实施。</li><li id="67bb" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">能很好的处理大型数据集。</li></ul><h2 id="5fba" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">K 均值的缺点</h2><ul class=""><li id="bde1" class="mt mu it lf b lg lh lk ll lo mv ls mw lw mx ma my mz na nb bi translated">对选择的簇/质心的数量敏感。即使使用了像 Elbow 方法这样的技术，有时也很难生成好的集群。</li><li id="17b6" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">不适用于异常值。质心可能会被离群值拖住，导致聚类偏斜。</li><li id="7b99" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">在高维空间中变得困难，因为点之间的距离增加，欧几里德距离发散(收敛到恒定值)。</li><li id="737a" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">随着维数的增加而变慢。</li></ul><h1 id="568d" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">k-均值算法</h1><p id="8957" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">1.决定集群的数量。这个数叫做 K，聚类数等于质心数。基于 K 的值，生成 K 个随机质心的坐标。</p><p id="52c8" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">2.对于每个点，计算该点和每个质心之间的欧几里德距离。</p><p id="8c82" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">3.将该点指定给其最近的质心。分配给同一个质心的点形成一个簇。</p><p id="f3aa" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">4.一旦聚类形成，通过取聚类平均值来计算每个聚类的新质心。聚类平均值是属于该聚类的所有点的 x 和 y 坐标的平均值。</p><p id="a20a" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">5.重复步骤 2、3 和 4，直到质心不能再移动。换句话说，重复这些步骤直到收敛。</p><h1 id="9605" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">使用 Scikit Learn 的虹膜数据集的 k 均值</h1><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="807b" class="mh kg it nn b gy nr ns l nt nu">import pandas as pd<br/>from sklearn import metrics<br/>from sklearn.cluster import KMeans<br/>import matplotlib.pyplot as plt</span><span id="d708" class="mh kg it nn b gy nv ns l nt nu"># reading the classic iris dataset into a df<br/>iris_df = pd.read_csv(“iris_dataset.csv”)</span><span id="2f4d" class="mh kg it nn b gy nv ns l nt nu"># Setting the independent features (input)<br/>X = iris_df.drop(“species”, axis=1).values</span><span id="279c" class="mh kg it nn b gy nv ns l nt nu"># Creating the KMeans object and fitting it to the Iris data<br/>iris_kmeans = KMeans(n_clusters=3)</span><span id="c27a" class="mh kg it nn b gy nv ns l nt nu">iris_kmeans.fit(X)</span><span id="0de8" class="mh kg it nn b gy nv ns l nt nu"># Predicting the cluster labels<br/>labels = iris_kmeans.predict(X)<br/>print(labels)</span><span id="0609" class="mh kg it nn b gy nv ns l nt nu"># Finding the final centroids<br/>centroids = iris_kmeans.cluster_centers_</span><span id="0a11" class="mh kg it nn b gy nv ns l nt nu"># Evaluating the quality of clusters<br/>s = metrics.silhouette_score(X, labels, metric=’euclidean’)<br/>print(f”Silhouette Coefficient for the Iris Dataset Clusters: {s:.2f}”)</span><span id="7c3c" class="mh kg it nn b gy nv ns l nt nu"># plotting the clusters using sepal_length and sepal_width<br/>plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=”rainbow”)<br/>plt.show()</span></pre><h2 id="1ab5" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">输出</h2><p id="b63d" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">标签值代表聚类数。</p><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="db9c" class="mh kg it nn b gy nr ns l nt nu">[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1<br/>1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2<br/>2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0<br/>0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0<br/>0 2]</span><span id="bb70" class="mh kg it nn b gy nv ns l nt nu">Silhouette Coefficient for the Iris Dataset Clusters: 0.55</span></pre><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3c8f17eb085a4b3069bfbff709fc63cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*zg9ZakKSuCLxUD-bANCgQQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">三个 K 均值聚类</p></figure><h1 id="a10c" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">寻找 K-Means 的最佳聚类数(肘方法)</h1><p id="8aa0" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">使用 K-Means 形成的聚类的质量很大程度上取决于 K 的选定值。K 的错误选择会导致较差的聚类。那么如何挑选 K 呢？我们来看看常用的手法叫做“<strong class="lf iu">肘法</strong>”。目标是选择形成弯头的 K 点。</p><p id="7ee4" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu">步骤:</strong></p><p id="9fb8" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">1.对于不同的 K 值，执行以下步骤:</p><p id="606d" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">2.对于每个聚类，计算每个点到其质心的距离的平方和。</p><p id="0435" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">3.将每个聚类的距离平方和相加，得到 k 值的距离平方和。</p><p id="c1ed" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">4.不断将每个 K 的距离平方和添加到列表中。</p><p id="ff19" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">5.绘制距离平方和(使用上一步创建的列表)及其 K 值。</p><p id="0ba9" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">6.选择发生急剧变化的 K(看起来像曲线的弯头)。</p><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="5142" class="mh kg it nn b gy nr ns l nt nu"># Arbitrarily selecting a range of values for K<br/>K = range(1,10)</span><span id="bfe4" class="mh kg it nn b gy nv ns l nt nu">sum_of_squared_distances = []</span><span id="720c" class="mh kg it nn b gy nv ns l nt nu"># Using Scikit Learn’s KMeans Algorithm to find sum of squared distances</span><span id="92e3" class="mh kg it nn b gy nv ns l nt nu">for k in K:<br/>    model = KMeans(n_clusters=k).fit(X)<br/>    sum_of_squared_distances.append(model.inertia_)</span><span id="2eb3" class="mh kg it nn b gy nv ns l nt nu">plt.plot(K, sum_of_squared_distances, “bx-”)<br/>plt.xlabel(“K values”)<br/>plt.ylabel(“Sum of Squared Distances”)<br/>plt.title(“Elbow Method”)<br/>plt.show()</span></pre><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4c314c3479313f63a2241bb3d67fa863.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*pO-csTIFVEC1kZdJ75t9OA.png"/></div></figure><p id="e36b" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">查看该图，我们可以看到 K=3 处的拐点，因此这是该数据集的最佳聚类数。</p><p id="7217" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">有时，我们可能会以多个值显示一个肘形结束。在这种情况下，为了找到最佳 K，可以使用像轮廓系数这样的评估度量。应该选择将返回轮廓系数的最高正值的 K。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="bce2" class="kf kg it bd kh ki of kk kl km og ko kp kq oh ks kt ku oi kw kx ky oj la lb lc bi translated">基于密度的含噪声应用空间聚类(DBSCAN)</h1><p id="5606" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="mb"> DBSCAN 是一种基于密度的聚类算法，它形成数据点密集区域的聚类，忽略低密度区域(将其视为噪声)。</em></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/52eb3f047e8a4211f21d4e72d65397ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*ScEZlfsQ89Bs3V1dMNRcAA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">维基百科图片</p></figure><h2 id="487b" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">DBSCAN 的优势</h2><ul class=""><li id="eb43" class="mt mu it lf b lg lh lk ll lo mv ls mw lw mx ma my mz na nb bi translated">适用于噪声数据集。</li><li id="b818" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">可以轻松识别异常值。</li><li id="9d22" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">聚类可以采取任何不规则的形状，不像 K-Means 中的聚类或多或少是球形的。</li></ul><h2 id="4d29" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">DBSCAN 的缺点</h2><ul class=""><li id="e12d" class="mt mu it lf b lg lh lk ll lo mv ls mw lw mx ma my mz na nb bi translated">不适用于稀疏数据集或密度变化的数据集。</li><li id="4e1c" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">对 eps 和 minPts 参数敏感。</li><li id="8429" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">对于多处理器系统不可分区。</li></ul><h1 id="668f" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">DBSCAN 参数</h1><p id="920c" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">DBSCAN 使用以下两个用户定义的参数进行聚类:</p><p id="8b58" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu">ε(EPS)</strong>:定义为被认为是相邻点(属于同一个聚类)的两点之间的最大距离。</p><p id="c8e3" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu">最小点(min_samples 或 minPts) </strong>:这定义了一个给定点需要被视为核心数据点的相邻点的最小数量。这包括点本身。例如，如果最小点数设置为 4，则给定点需要有 3 个或更多的相邻数据点才能被视为核心数据点。</p><p id="2126" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">如果最小数量的点满足ε距离要求，则它们被视为一个聚类。</p><h1 id="d34f" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">DBS can 中的重要术语</strong></h1><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="dbcf" class="mh kg it nn b gy nr ns l nt nu"><strong class="nn iu">Core Point: </strong>A data point is considered to be a core point if it has minimum number of neighboring data points (min_samples) at an epsilon distance from it. This minimum number of data points includes the original data point.</span><span id="d997" class="mh kg it nn b gy nv ns l nt nu"><strong class="nn iu">Border Point: </strong>A data point that has less than minimum number of data points needed but has at least one core point in the neighborhood.</span><span id="1ed8" class="mh kg it nn b gy nv ns l nt nu"><strong class="nn iu">Noise: </strong>A data point that is not a core point or a border point is considered noise or an outlier.</span></pre><h1 id="2fce" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> DBSCAN 算法</strong></h1><p id="53f9" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lf iu"> 1 </strong>。决定 eps 和 minPts 的值。</p><p id="bde4" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu"> 2 </strong>。对于每个点:</p><ul class=""><li id="d4eb" class="mt mu it lf b lg nh lk ni lo ol ls om lw on ma my mz na nb bi translated">计算它与所有其他点的距离。如果距离小于或等于 eps，则将该点标记为 x 的邻居。</li><li id="9631" class="mt mu it lf b lg nc lk nd lo ne ls nf lw ng ma my mz na nb bi translated">如果该点的相邻点计数大于或等于 minPts，则将其标记为核心点或已访问点。</li></ul><p id="cd3b" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu"> 3 </strong>。对于每个核心点，如果它还没有被分配给一个集群，则创建一个新的集群。递归地找到它的所有邻近点，并把它们分配到与核心点相同的簇中。</p><p id="9078" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><strong class="lf iu"> 4 </strong>。继续这些步骤，直到覆盖所有未访问的点。</p><h1 id="a4b0" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">使用 Scikit Learn 对虹膜数据集进行 DBSCAN 聚类</strong></h1><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="07b6" class="mh kg it nn b gy nr ns l nt nu">import pandas as pd<br/>from sklearn import metrics<br/>from sklearn.cluster import DBSCAN<br/>import matplotlib.pyplot as plt</span><span id="232d" class="mh kg it nn b gy nv ns l nt nu"># reading the classic iris dataset into a df<br/>iris_df = pd.read_csv(“iris_dataset.csv”)</span><span id="628d" class="mh kg it nn b gy nv ns l nt nu">X = iris_df.drop(“species”, axis=1).values</span><span id="5f98" class="mh kg it nn b gy nv ns l nt nu">iris_dbscan = DBSCAN(eps=0.5, min_samples=5)<br/>iris_dbscan.fit(X)</span><span id="e637" class="mh kg it nn b gy nv ns l nt nu">labels = iris_dbscan.labels_</span><span id="2586" class="mh kg it nn b gy nv ns l nt nu"># label=-1 means the point is an outlier. Rest of the values represent the label/cluster number starting from 0<br/>print(labels)</span><span id="edd5" class="mh kg it nn b gy nv ns l nt nu"># Creating a numpy array with all values set to false by default<br/>core_samples_mask = np.zeros_like(labels, dtype=bool)</span><span id="bf8e" class="mh kg it nn b gy nv ns l nt nu"># Setting core and border points (all points that are not -1) to True<br/>core_samples_mask[iris_dbscan.core_sample_indices_] = True</span><span id="42d7" class="mh kg it nn b gy nv ns l nt nu"># Finding the number of clusters in labels (ignoring noise if present)<br/>n_clusters_ = len(set(labels)) — (1 if -1 in labels else 0)<br/>n_noise_ = list(labels).count(-1)</span><span id="5a66" class="mh kg it nn b gy nv ns l nt nu"># Printing the number of clusters and number of noise points (outliers)<br/>print(“Estimated number of clusters: %d” % n_clusters_)<br/>print(“Estimated number of noise points: %d” % n_noise_)</span><span id="8167" class="mh kg it nn b gy nv ns l nt nu"># Evaluating the quality of clusters<br/>s = metrics.silhouette_score(X, iris_dbscan.labels_)<br/>print(f”Silhouette Coefficient for the Iris Dataset Clusters: {s:.2f}”)</span></pre><h2 id="53ed" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">输出</h2><p id="ece5" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">Label = -1 表示它是一个噪声点(异常值)。<br/> Label = 0 或以上，表示集群编号。</p><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="df09" class="mh kg it nn b gy nr ns l nt nu">[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br/>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 0 0 0 0 0<br/>0 0 1 1 1 1 1 1 1 -1 1 1 -1 1 1 1 1 1 1 1 -1 1 1 1<br/>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 -1 1 1 1 1 1 -1 1 1<br/>1 1 -1 1 1 1 1 1 1 -1 -1 1 -1 -1 1 1 1 1 1 1 1 -1 -1 1<br/>1 1 -1 1 1 1 1 1 1 1 1 -1 1 1 -1 -1 1 1 1 1 1 1 1 1<br/>1 1 1 1 1 1]</span><span id="7106" class="mh kg it nn b gy nv ns l nt nu">Estimated number of clusters: 2</span><span id="f501" class="mh kg it nn b gy nv ns l nt nu">Estimated number of noise points: 17</span><span id="ec73" class="mh kg it nn b gy nv ns l nt nu">Silhouette Coefficient for the Iris Dataset Clusters: 0.49</span></pre><h2 id="5c39" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">绘制聚类图</h2><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="6f34" class="mh kg it nn b gy nr ns l nt nu">unique_labels = set(labels)</span><span id="4815" class="mh kg it nn b gy nv ns l nt nu">colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))</span><span id="16db" class="mh kg it nn b gy nv ns l nt nu">for k, col in zip(unique_labels, colors):<br/>    if k == -1:<br/>        # Black used for noise<br/>        col = “k”<br/>    <br/>    class_member_mask = (labels == k)<br/>    xy = X[class_member_mask &amp; core_samples_mask]<br/>    plt.plot(xy[:, 0], xy[:, 1], “o”, markerfacecolor=col,<br/>    markeredgecolor=”k”, markersize=10)</span><span id="95ee" class="mh kg it nn b gy nv ns l nt nu">    xy = X[class_member_mask &amp; ~core_samples_mask]<br/>    plt.plot(xy[:, 0], xy[:, 1], “o”, markerfacecolor=col,<br/>    markeredgecolor=”k”, markersize=5)</span><span id="66b4" class="mh kg it nn b gy nv ns l nt nu">plt.title(“Estimated number of clusters: %d” % n_clusters_)<br/>plt.show()</span></pre><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/16ca416e712b37ad26088a8f04b94238.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*YOxJ3HLhhjzAD4ctafLMqw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">黑色代表异常值，彩色点代表两个 DBSCAN 集群</p></figure><h1 id="8d7b" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">寻找每股收益的最佳值(拐点法)</h1><p id="bbc9" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">DBSCAN 聚类算法对我们选择的 eps 值很敏感。那么如何才能知道我们选择了最优的 eps 值呢？这里有一个常用的手法叫做“<strong class="lf iu">膝法</strong>”。目标是找出每个点到其 K 个最近邻点的平均距离，并选择曲率最大或发生急剧变化的距离。K 的值被设置为等于 minPoints。</p><p id="045b" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">以下是一个使用 Scikit Learn 的 NearestNeighbors 模块显示最佳 eps 值选择的示例。</p><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="4126" class="mh kg it nn b gy nr ns l nt nu">from sklearn.neighbors import NearestNeighbors<br/>nearest_neighbors = NearestNeighbors(n_neighbors=5)</span><span id="c2ad" class="mh kg it nn b gy nv ns l nt nu">nearest_neighbors.fit(X)<br/>distances, indices = nearest_neighbors.kneighbors(X)<br/>distances = np.sort(distances, axis=0)[:, 1]<br/>print(distances)</span><span id="4180" class="mh kg it nn b gy nv ns l nt nu">plt.plot(distances)<br/>plt.show()</span></pre><h2 id="8409" class="mh kg it bd kh mi mj dn kl mk ml dp kp lo mm mn kt ls mo mp kx lw mq mr lb ms bi translated">输出</h2><pre class="md me mf mg gt nm nn no np aw nq bi"><span id="9b67" class="mh kg it nn b gy nr ns l nt nu">[0. 0. 0. 0. 0. 0.1<br/>0.1 0.1 0.1 0.1 0.1 0.1<br/>0.1 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.14142136 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.14142136 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.14142136 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.14142136 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.14142136 0.14142136 0.14142136 0.14142136 0.14142136 0.14142136<br/>0.17320508 0.17320508 0.17320508 0.17320508 0.17320508 0.17320508<br/>0.17320508 0.2 0.2 0.2 0.2 0.2<br/>0.2 0.2 0.2 0.2 0.2 0.2<br/>0.2236068 0.2236068 0.2236068 0.2236068 0.2236068 0.2236068<br/>0.2236068 0.2236068 0.2236068 0.2236068 0.24494897 0.24494897<br/>0.24494897 0.24494897 0.24494897 0.24494897 0.24494897 0.24494897<br/>0.24494897 0.24494897 0.24494897 0.24494897 0.26457513 0.26457513<br/>0.26457513 0.26457513 0.26457513 0.26457513 0.26457513 0.26457513<br/>0.26457513 0.26457513 0.26457513 0.26457513 0.28284271 0.28284271<br/>0.28284271 0.28284271 0.3 0.3 0.3 0.3<br/>0.3 0.3 0.3 0.31622777 0.31622777 0.31622777<br/>0.33166248 0.33166248 0.33166248 0.33166248 0.33166248 0.34641016<br/>0.34641016 0.34641016 0.34641016 0.34641016 0.34641016 0.34641016<br/>0.36055513 0.36055513 0.36055513 0.36055513 0.37416574 0.38729833<br/>0.38729833 0.38729833 0.41231056 0.41231056 0.41231056 0.41231056<br/>0.42426407 0.42426407 0.43588989 0.45825757 0.48989795 0.48989795<br/>0.53851648 0.53851648 0.55677644 0.6244998 0.63245553 0.73484692]</span></pre><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/79afbba483943c8e742ebc5a836f2d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*6CRoMHqFSD69KXWAZZIBYA.png"/></div></figure><p id="9930" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">最佳值应该是我们看到最大曲率的值，在这种情况下，该值似乎接近 0.5。</p><p id="41fd" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated">有时，我们可能会以多个值显示急剧变化而告终。在这种情况下，为了找到最佳 K，可以使用像轮廓系数这样的评估度量。应该选择将返回轮廓系数的最高正值的 K。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="034e" class="pw-post-body-paragraph ld le it lf b lg nh li lj lk ni lm ln lo nj lq lr ls nk lu lv lw nl ly lz ma im bi translated"><em class="mb">何时使用这两种聚类技术中的哪一种，取决于问题。尽管 K-Means 是最流行的聚类技术，但也有使用 DBSCAN 得到更好的聚类的用例。</em></p></div></div>    
</body>
</html>