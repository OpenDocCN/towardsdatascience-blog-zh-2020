<html>
<head>
<title>SWAP: Softmax-Weighted Average Pooling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SWAP:soft max-加权平均池</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/swap-softmax-weighted-average-pooling-70977a69791b?source=collection_archive---------30-----------------------#2020-07-25">https://towardsdatascience.com/swap-softmax-weighted-average-pooling-70977a69791b?source=collection_archive---------30-----------------------#2020-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="62eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">提高梯度精度的探讨</h2></div><p id="b107" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="http://blakeelias.name" rel="noopener ugc nofollow" target="_blank"> <em class="lf">布莱克伊利亚</em> </a> <em class="lf">是新英格兰复杂系统研究所</em> <a class="ae le" href="https://necsi.edu/" rel="noopener ugc nofollow" target="_blank"> <em class="lf">的研究员。</em></a><em class="lf"><br/></em><a class="ae le" href="http://github.com/darkmatter08" rel="noopener ugc nofollow" target="_blank"><em class="lf">肖恩·贾恩</em> </a> <em class="lf">是微软研究院</em>  <em class="lf">的一名 AI 常驻者。</em></p><blockquote class="lg"><p id="1624" class="lh li it bd lj lk ll lm ln lo lp ld dk translated">我们的方法 softmax-加权平均池(SWAP)应用平均池，但是通过每个窗口的 soft max 对输入重新加权。</p></blockquote><p id="58a2" class="pw-post-body-paragraph ki kj it kk b kl lq ju kn ko lr jx kq kr ls kt ku kv lt kx ky kz lu lb lc ld im bi translated">我们提出了一种卷积神经网络的池化方法，作为最大池化或平均池化的替代方法。我们的方法 softmax-加权平均池(SWAP)应用平均池，但是通过每个窗口的 soft max 对输入重新加权。虽然向前传递的值与最大池化的值几乎相同，但 SWAP 的向后传递具有这样的属性，即窗口中的所有元素都接收渐变更新，而不仅仅是最大值。我们假设这些更丰富、更精确的梯度可以改善学习动力。在这里，我们实例化这个想法，并在 CIFAR-10 数据集上研究学习行为。我们发现交换既不能让我们提高学习速度，也不能提高模型性能。</p><h1 id="8403" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">起源</strong></h1><p id="72fd" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在观看来自 DeepMind / UCL 深度学习课程的詹姆斯·马腾斯关于优化的讲座时，我们注意到他的观点，即随着学习的进行，你必须降低学习速率或增加批量以确保收敛。这些技术中的任何一种都会导致对梯度的更精确的估计。这让我们开始思考精确梯度的必要性。另外，我们一直在深入研究反向传播如何为所有类型的层计算梯度。在进行卷积和池化的练习时，我们注意到最大池化仅计算相对于窗口中最大值的梯度。这丢弃了信息——我们如何才能做得更好？通过使用所有的信息，我们能得到一个更精确的梯度估计吗？</p><blockquote class="lg"><p id="c940" class="lh li it bd lj lk ll lm ln lo lp ld dk translated">Max-pooling 丢弃了梯度信息——我们如何能做得更好？</p></blockquote><h1 id="6cd8" class="lv lw it bd lx ly lz ma mb mc md me mf jz ms ka mh kc mt kd mj kf mu kg ml mm bi translated"><strong class="ak">进一步背景</strong></h1><p id="1e5e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">最大池通常在视觉任务的 CNN 中用作下采样方法。例如，AlexNet 使用 3x3 Max-Pooling。【<a class="ae le" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">引用</a></p><p id="1e71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在视觉应用中，max-pooling 将一个特征图作为输入，并输出一个较小的特征图。如果输入影像为 4x4，则步幅为 2(无重叠)的 2x2 最大池操作符将输出 2x2 要素地图。max-pooling 运算符的 2x2 内核在输入要素地图上有 2x2 个不重叠的“位置”。对于每个位置，选择 2x2 窗口中的最大值作为输出特征图中的值。其他值将被丢弃。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7ae9a546c8cb1264e3aa320a11cd1e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*x_h9pb7hw1pfM_SZ"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图 1:最大池化的结果。来源:斯坦福 CS231n(通过<a class="ae le" href="https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank"> quora </a>)。</p></figure><p id="b52e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">隐含的假设是“值越大越好”，即，值越大对最终输出越重要。这种建模决策是由我们的直觉推动的，尽管可能不是绝对正确的。[艾德。:也许其他价值观也很重要！在接近平局的情况下，将梯度传播到第二大值可能会使其成为最大值。这可能会改变模型学习的轨迹。也更新第二大值，可能是更好的学习轨迹。]</p><p id="e2f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会想，这是可微的吗？毕竟，深度学习要求模型中的所有操作都是可微分的，以便计算梯度。在纯数学意义上，这不是一个可微的运算。实际上，在向后传递中，对应于最大值的所有位置简单地复制入站梯度；所有非最大值位置简单地将其梯度设置为零。PyTorch 将其实现为一个定制的 CUDA 内核(<a class="ae le" href="https://github.com/pytorch/pytorch/blob/0257f5d19f0585f9a82bc06e0c4987e2136332c9/aten/src/THCUNN/generic/SpatialMaxPooling.cu" rel="noopener ugc nofollow" target="_blank">这个函数</a>调用<a class="ae le" href="https://github.com/pytorch/pytorch/blob/502aaf39cf4a878f9e4f849e5f409573aa598aa9/aten/src/THNN/generic/SpatialDilatedMaxPooling.c" rel="noopener ugc nofollow" target="_blank">这个函数</a>)。</p><p id="e728" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，最大池生成稀疏梯度。而且很管用！从 AlexNet [ <a class="ae le" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> cite </a>到 ResNet [ <a class="ae le" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> cite </a>再到强化学习[<a class="ae le" href="https://arxiv.org/pdf/1312.5602v1.pdf" rel="noopener ugc nofollow" target="_blank">cite</a>T6】cite]，应用广泛。</p><p id="55ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">已经开发了许多变体；Average-Pooling 输出窗口中的平均值，而不是最大值。扩大的最大池使窗口不连续；相反，它使用类似棋盘的图案。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/884981d50c9d12d368f0b03096c2a731.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*je6eVxm3q6Qm1eVu"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图 2:扩大的最大池。来源:<a class="ae le" href="https://arxiv.org/pdf/1603.07285.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv </a>(通过<a class="ae le" href="https://stackoverflow.com/questions/45809486/why-do-dilated-convolutions-preserve-resolution" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>)。</p></figure><p id="efca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有争议的是，Geoff Hinton 不喜欢 Max-Pooling:</p><blockquote class="ni nj nk"><p id="7032" class="ki kj lf kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">卷积神经网络中使用的池操作是一个很大的错误，它如此有效的事实是一场灾难。</p><p id="0acf" class="ki kj lf kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">如果池不重叠，池将丢失关于事物位置的有价值的信息。我们需要这些信息来检测物体各部分之间的精确关系。的确，如果这些池有足够的重叠，那么特征的位置将通过“粗略编码”得到精确的保留(参见我在 1986 年关于“分布式表示”的论文中对这种效果的解释)。但是我不再相信粗略编码是表示物体相对于观察者的姿态的最佳方式(我所说的姿态是指位置、方向和比例)。</p><p id="5ede" class="ki kj lf kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">【来源:Reddit 上的 Geoff Hinton。]</p></blockquote><h1 id="560a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">动机</strong></h1><blockquote class="lg"><p id="349d" class="lh li it bd lj lk ll lm ln lo lp ld dk translated">最大池生成稀疏梯度。有了更好的梯度估计，我们能否通过提高学习速率来迈出更大的步伐，从而更快地收敛？</p></blockquote><p id="664a" class="pw-post-body-paragraph ki kj it kk b kl lq ju kn ko lr jx kq kr ls kt ku kv lt kx ky kz lu lb lc ld im bi translated">稀疏渐变丢弃了太多信息。有了更好的梯度估计，我们能否通过提高学习速率来迈出更大的步伐，从而更快地收敛？</p><p id="ec70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然由最大池生成的出站梯度是稀疏的，但是该操作通常用于 Conv →最大池操作链中。注意，可训练参数(即滤波器值，<strong class="kk iu"><em class="lf"/></strong>)都在 Conv 算子中。还要注意的是:</p><p id="ad0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf"> dL/dF = Conv(X，dL/dO) </em> </strong>，其中:</p><ul class=""><li id="15ed" class="no np it kk b kl km ko kp kr nq kv nr kz ns ld nt nu nv nw bi translated"><strong class="kk iu"> <em class="lf"> dL/dF </em> </strong>是相对于卷积滤波器的梯度</li><li id="5cc1" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu"> <em class="lf"> dL/dO </em> </strong>是来自 Max-Pool 的出站梯度，并且</li><li id="fba8" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu"> <em class="lf"> X </em> </strong>是对 Conv 的输入(正向)。</li></ul><p id="f0e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，卷积滤波器<strong class="kk iu"> <em class="lf"> F </em> </strong>中的所有位置都获得梯度。然而，这些梯度是从稀疏矩阵<strong class="kk iu"> <em class="lf"> dL/dO </em> </strong>而不是密集矩阵计算的。(稀疏程度取决于最大池窗口大小。)</p><p id="374f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">向前:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oc"><img src="../Images/ea7733c16526854ae80c06dac42d3f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/0*N2Kux81JQ7EUyS4V"/></div></div></figure><p id="3794" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">向后:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/39263c7fc9425a524c4a5b44cf7c9e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/0*eJzR59SXWCVXZNhp"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图 3:最大池生成稀疏梯度。(作者图片)</p></figure><p id="d145" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还要注意的是，<strong class="kk iu"> <em class="lf"> dL/dF </em> </strong>是<em class="lf">而不是</em>稀疏，因为<strong class="kk iu"> <em class="lf"> dL/dO </em> </strong>的每个稀疏条目都将一个渐变值发送回所有<em class="lf">条目</em><strong class="kk iu"><em class="lf">dL/dF</em></strong>。</p><p id="3161" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但这就提出了一个问题。虽然<strong class="kk iu"> <em class="lf"> dL/dF </em> </strong>本身不是稀疏的<em class="lf"/>，但是它的条目是基于稀疏输入的平均来计算的。如果它的输入(<strong class="kk iu"><em class="lf">dL/dO</em></strong>—Max-Pool 的出站梯度)是密集的，那么<strong class="kk iu"> <em class="lf"> dL/dF </em> </strong>会是真实梯度的更好估计吗？我们如何使<strong class="kk iu"> <em class="lf"> dL/dO </em> </strong>密集，同时仍然保留 Max-Pool 的“值越大越好”假设？</p><p id="1519" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种解决办法是平均分摊。在那里，<em class="lf">所有的</em>激活都向后传递一个梯度，而不仅仅是每个窗口中的最大值。但是，它违反了 MaxPool 的假设“值越大越好”。</p><p id="d070" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输入 soft max-加权平均池(SWAP)。正向传递最好解释为伪代码:</p><blockquote class="lg"><p id="e43b" class="lh li it bd lj lk ll lm ln lo lp ld dk translated"><strong class="ak"> <em class="oi"> average_pool(O，weights = soft max _ per _ window(O))</em></strong></p></blockquote><figure class="ok ol om on oo na gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f24c0a7660b952b7bb96cf7699d897ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*mXqThRnEoaAMp5MW"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图 4: SWAP 产生一个与 max-pooling 几乎相同的值——但是将渐变传递回窗口中的所有条目。(作者图片)</p></figure><p id="0117" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">softmax 运算符将值归一化为概率分布，但是，它更倾向于大值。这给了它一个类似最大池的效果。</p><p id="19b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在向后传递中，<strong class="kk iu"> <em class="lf"> dL/dO </em> </strong>是密集的，因为<strong class="kk iu"><em class="lf"/></strong>中的每个出站激活都依赖于其窗口中的所有激活—而不仅仅是最大值。<strong class="kk iu"> <em class="lf"> O </em> </strong>中的非最大值现在接收相对较小但非零的渐变。答对了！</p><h1 id="f639" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">实验设置</strong></h1><p id="7008" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我们在<a class="ae le" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR10 </a>上进行实验。我们的代码可在<a class="ae le" href="https://colab.research.google.com/drive/1zUjtUF9nuMH-tHolXemPBturYX10LdxX#scrollTo=r7u9xW7naK9l" rel="noopener ugc nofollow" target="_blank">这里</a>获得。我们将网络架构固定为:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi op"><img src="../Images/b7106e25ada2691705eeb3aaf546c2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDNUJJQy427wtS9seI8ibA.png"/></div></div></figure><p id="2423" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们测试了“池”层的三种不同变体:两个基线(最大池和平均池)，以及交换。使用 SGD，LR=1e-3 对模型进行 100 个时期的训练(除非另有说明)。</p><p id="10b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还训练了 LR 增加{25，50，400}%的 SWAP。这是为了测试这样一个想法，使用更精确的梯度，我们可以采取更大的步骤，而使用更大的步骤，模型会收敛得更快。</p><h1 id="4d70" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">结果</strong></h1><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oq"><img src="../Images/f5ac20ae5b6e8422aff3730029785102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbUZuHktOd3OUs6kIoEwug.png"/></div></div></figure><h1 id="e3f8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">讨论</strong></h1><p id="33a9" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">与两个基准相比，SWAP 的性能更差。我们不明白为什么会这样。LR 的增加没有带来任何好处；一般来说，随着 LR 的增加，观察到的性能比基线更差。我们将 LR 性能提高 400%比提高 50%归因于随机性；我们只测试了一个随机种子，并且只报道了一个试验。对于 400%的增长表现更好的另一个可能的解释是，用更高的 LR“覆盖更多的地面”的能力。</p><blockquote class="lg"><p id="ceaf" class="lh li it bd lj lk ll lm ln lo lp ld dk translated">LR 的增加没有带来任何好处；一般来说，随着 LR 的增加，观察到的性能比基线更差。</p></blockquote><h1 id="1245" class="lv lw it bd lx ly lz ma mb mc md me mf jz ms ka mh kc mt kd mj kf mu kg ml mm bi translated"><strong class="ak">未来工作及结论</strong></h1><p id="4b31" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">虽然 SWAP 没有显示出改进，但我们仍然想尝试几个实验:</p><ul class=""><li id="82eb" class="no np it kk b kl km ko kp kr nq kv nr kz ns ld nt nu nv nw bi translated"><strong class="kk iu">重叠池窗口。</strong>一种可能性是使用重叠的池窗口(即步距= 1)，而不是我们这里使用的不相交窗口(步距= 2)。现代卷积架构，如<a class="ae le" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>和<a class="ae le" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet </a>都使用重叠池窗口。因此，为了公平的比较，明智的做法是与更接近艺术水平的东西进行比较，而不是我们在这里为了简单而使用的架构。事实上，Hinton 对最大池的批评在非重叠池窗口的情况下是最严格的，理由是这丢弃了空间信息。</li><li id="12a8" class="no np it kk b kl nx ko ny kr nz kv oa kz ob ld nt nu nv nw bi translated"><strong class="kk iu">激活直方图。</strong>我们希望使用完全相同的初始化来尝试 Max-Pool &amp; SWAP，训练两者，并比较梯度的分布。调查梯度的差异可以更好地理解训练行为中的鲜明对比。</li></ul><blockquote class="lg"><p id="96bc" class="lh li it bd lj lk or os ot ou ov ld dk translated">提高梯度精度仍然是一个令人兴奋的领域。我们还能如何修改模型或梯度计算来提高梯度精度？</p></blockquote></div></div>    
</body>
</html>