<html>
<head>
<title>Serve Your Machine Learning Models With A Simple Python Server</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用简单的 Python 服务器为您的机器学习模型提供服务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serve-your-machine-learning-models-with-a-simple-python-server-5a72d005e0ae?source=collection_archive---------28-----------------------#2020-09-12">https://towardsdatascience.com/serve-your-machine-learning-models-with-a-simple-python-server-5a72d005e0ae?source=collection_archive---------28-----------------------#2020-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a241" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何轻松服务于机器学习模型</h2></div><p id="cf30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多数据科学项目中最大的挑战之一是模型部署。在某些情况下，您可以批量离线进行预测，但在其他情况下，您必须实时调用您的模型。</p><p id="c5f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们看到越来越多的模型服务器像<code class="fe lb lc ld le b"><a class="ae lf" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank">TensorFlow Serving</a></code>和<code class="fe lb lc ld le b"><a class="ae lf" href="https://pytorch.org/serve/" rel="noopener ugc nofollow" target="_blank">Torch Serve</a></code>被发布。尽管有很多很好的理由在生产中使用这些解决方案，因为它们快速高效，但是它们不能封装模型技术和实现细节。</p><p id="6957" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是为什么我创造了<code class="fe lb lc ld le b"><a class="ae lf" href="https://pypi.org/project/mlserving/" rel="noopener ugc nofollow" target="_blank">mlserving</a></code>。一个 python 包，帮助数据科学家将更多的火力集中在机器学习逻辑上，而不是服务器技术上。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/e7fe47e7cb9e70548144b3baa70e98c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ib84q6PRHfv9mtxLWqew-g.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">作者图片</p></figure><p id="8dcc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">mlserving</code>尝试一般化预测流，而不是您自己实现预测，或者为此使用另一个服务应用程序。</p><p id="8286" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过实现一个简单的接口，您可以轻松地为您的模型设置一个端点。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lw"><img src="../Images/e26bff00ef130f781d0fc970c293e551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1b7POSHvaK7CA1adxkj4Q.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">作者图片</p></figure><p id="b22b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个接口表示一个通用的预测流。</p><p id="ee63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看一个例子(假设我们已经有了一个训练好的模型)</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="f437" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将训练好的模型加载到构造函数中，并实现接口。</p><p id="fccd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">稍后，我们创建一个<code class="fe lb lc ld le b">ServingApp</code>，并添加我们刚刚实现的推理处理器。</p><p id="ab4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个例子中，我们没有对输入进行任何处理，但是正如您所看到的，为您的模型设置一个端点是非常容易的。</p><p id="a678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过发送 POST 请求来调用我们的模型。</p><pre class="lh li lj lk gt lz le ma mb aw mc bi"><span id="6a80" class="md me iq le b gy mf mg l mh mi">curl -X POST <a class="ae lf" href="http://localhost:1234/api/v1/predict" rel="noopener ugc nofollow" target="_blank">http://localhost:1234/api/v1/predict</a> \<br/>-H 'Content-Type: application/json' \<br/>  -d '{<br/>    "features": [...]<br/>}'</span></pre></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="0fd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 TensorFlow 服务也不例外，用我们的定制处理来包装 TensorFlow 服务非常容易，但首先，我们需要设置我们的 TensorFlow 服务。在本例中，我们将使用 docker:</p><pre class="lh li lj lk gt lz le ma mb aw mc bi"><span id="a25e" class="md me iq le b gy mf mg l mh mi">docker run -p 8501:8501 \<br/>  --mount type=bind,source=/path/to/my_model/,target=/models/model \<br/>  -e MODEL_NAME=model -t tensorflow/serving</span></pre><p id="0411" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里阅读更多:<a class="ae lf" href="https://www.tensorflow.org/tfx/serving/docker" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tfx/serving/docker</a></p><p id="99b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经成功运行了 TensorFlow 服务，让我们围绕它创建一个 python 处理层:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="5568" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一次，我们没有在构造函数中加载我们的模型，因为 TensorFlow Serving 已经加载了它。</p><p id="268b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">def predict</code>方法由<code class="fe lb lc ld le b">TFServingPrediction </code>实现，基本上，它发送从<code class="fe lb lc ld le b">def pre_process </code>返回的有效载荷，并处理 TensorFlow 服务的推理。所以我们剩下要做的就是实现预处理和后处理。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="25ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！我们已经设法用一个易于使用的 API 来包装我们的模型。</p><p id="b116" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我很乐意听到任何反馈或建议，随时联系！</p><p id="c230" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢阅读！</p><p id="61a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">附加资源</strong></p><ul class=""><li id="b0fe" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">在这里阅读更多关于<code class="fe lb lc ld le b">mlserving</code>的内容:<a class="ae lf" href="https://mlserving.readthedocs.io" rel="noopener ugc nofollow" target="_blank">https://ml serving . readthedocs . io</a></li><li id="ef46" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">带有<em class="ne">逻辑回归</em>模型的<code class="fe lb lc ld le b">mlserving</code>的代码示例(sci kit learn):<a class="ae lf" href="https://github.com/orlevii/mlserving-example" rel="noopener ugc nofollow" target="_blank">https://github.com/orlevii/mlserving-example</a></li></ul></div></div>    
</body>
</html>