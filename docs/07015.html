<html>
<head>
<title>PCA clearly explained —When, Why, How to use it and feature importance: A guide in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA 清楚地解释了——何时、为什么、如何使用它以及特性的重要性:Python 指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=collection_archive---------0-----------------------#2020-05-30">https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=collection_archive---------0-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4a43" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我解释了 PCA 是什么，<strong class="ak">何时</strong>为什么使用它，以及如何使用 scikit-learn 在 Python 中实现它。此外，我解释了如何在 PCA 分析后得到特征的重要性。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab677f86b284ccd513591b51364f617f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ba0XpZtJrgh7UpzWcIgZ1Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图<strong class="bd ky">作者制作。</strong></p></figure><h1 id="7170" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.简介和背景</h1><p id="72b6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">主成分分析</strong> (PCA)是一种众所周知的<strong class="lt iu">无监督</strong> <strong class="lt iu">维数</strong> <strong class="lt iu">约简</strong>技术，通过原始变量(特征)的线性(线性 PCA)或非线性(核 PCA) <strong class="lt iu">组合</strong>构造<strong class="lt iu">相关</strong>特征/变量。在本帖中，我们将只关注著名且广泛使用的<strong class="lt iu">线性 PCA </strong>方法。</p><p id="b5c4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">相关特征的构建是通过<strong class="lt iu">将相关变量</strong>线性转换成较少数量的<strong class="lt iu">不相关变量</strong>来实现的。这是通过<strong class="lt iu">使用协方差/相关矩阵的特征向量(也称为主分量(PCs))将</strong>(点积)原始数据投影到<strong class="lt iu">缩减的 PCA 空间</strong>来完成的。</p><p id="2686" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">结果</strong> <strong class="lt iu">预计</strong> <strong class="lt iu">数据</strong>实质上是<strong class="lt iu">原始</strong>数据<strong class="lt iu">的<strong class="lt iu">线性</strong> <strong class="lt iu">组合</strong>捕捉</strong> <strong class="lt iu">数据</strong> ( <a class="ae ms" href="https://www.springer.com/gp/book/9780387954424" rel="noopener ugc nofollow" target="_blank"> Jolliffe 2002 </a>)。</p><p id="e037" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">总之，<strong class="lt iu"> PCA </strong>是将数据的<strong class="lt iu">正交</strong> <strong class="lt iu">变换</strong>成一系列<strong class="lt iu">不相关的</strong>数据，这些数据存在于缩减的 PCA 空间中，使得第一个分量解释数据中的最大差异，而每个后续分量解释的较少。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="8197" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">——我的邮件列表只需 5 秒:<a class="ae ms" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></h2><h2 id="d38d" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">-成为会员支持我:<a class="ae ms" href="https://seralouk.medium.com/membership" rel="noopener">https://seralouk.medium.com/membership</a></h2></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="262b" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">2.何时/为何使用 PCA</h1><ul class=""><li id="261a" class="nr ns it lt b lu lv lx ly ma nt me nu mi nv mm nw nx ny nz bi translated">PCA 技术在处理<strong class="lt iu">多个</strong> - <strong class="lt iu">共线性</strong>存在于<strong class="lt iu">特征</strong> / <strong class="lt iu">变量</strong>之间的数据时特别有用。</li><li id="919b" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">当<strong class="lt iu">输入特征的尺寸较大</strong>(如变量较多)时，可使用 PCA。</li><li id="c007" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">PCA 还可以用于<strong class="lt iu">去噪</strong>和<strong class="lt iu">数据</strong> <strong class="lt iu">压缩</strong>。</li></ul></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae ms" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="4d64" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">3.主成分分析方法的核心</h1><p id="c299" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">设<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>是包含形状为<code class="fe of og oh oi b">[n_samples, n_features]</code>的原始数据的矩阵。</p><p id="6c75" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">简而言之，<strong class="lt iu"> PCA </strong>分析由以下<strong class="lt iu">步骤</strong>组成:</p><ul class=""><li id="a2a0" class="nr ns it lt b lu mn lx mo ma oj me ok mi ol mm nw nx ny nz bi translated">首先，<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>中存储的原始输入变量是<strong class="lt iu"> z 值</strong>，这样每个原始变量(<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>列)的平均值和单位标准差为零。</li><li id="78d6" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">下一步涉及<strong class="lt iu">协方差</strong>矩阵<code class="fe of og oh oi b"><strong class="lt iu">Cx= (1/n)X'X</strong></code>的构造和<a class="ae ms" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">特征分解</strong> </a>(在 z 得分数据的情况下，协方差等于相关矩阵，因为所有特征的标准偏差为 1)。</li><li id="6141" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated"><strong class="lt iu">特征值</strong>然后按照代表数据中递减方差的<strong class="lt iu">递减</strong>顺序<strong class="lt iu">排序</strong>(特征值等于方差——我将在下面第 6 段使用 Python 来证明这一点)。</li><li id="86a8" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">最后，<strong class="lt iu">原始</strong> <strong class="lt iu">归一化</strong> <strong class="lt iu">数据</strong>到<strong class="lt iu">约简 PCA 空间</strong>的<strong class="lt iu">投影</strong>(变换)通过<strong class="lt iu">乘以</strong>(点积)<strong class="lt iu">原始归一化数据</strong>与协方差矩阵即 PCs 的<strong class="lt iu">前导</strong> <strong class="lt iu">特征向量</strong>得到。</li><li id="1263" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">新的<strong class="lt iu">减少的</strong> PCA 空间<strong class="lt iu">最大化</strong>原<strong class="lt iu">数据的<strong class="lt iu">方差</strong>。为了<strong class="lt iu">可视化</strong>投影数据以及原始变量的贡献，在联合绘图中，我们可以使用<strong class="lt iu">双绘图</strong>。</strong></li></ul><h1 id="8282" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.有意义组件的最大数量</h1><p id="ac49" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">有意义的</strong> <strong class="lt iu">成分</strong>有一个<strong class="lt iu">上</strong> <strong class="lt iu">界</strong>，可以用<strong class="lt iu"> PCA </strong>提取。这与<strong class="lt iu">协方差/相关性</strong>矩阵(<code class="fe of og oh oi b"><strong class="lt iu">Cx</strong></code>)的<a class="ae ms" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">秩</strong> </a>有关。具有形状为<code class="fe of og oh oi b">[n_samples, n_features/n_variables]</code>的数据矩阵<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>，则<strong class="lt iu">协方差</strong> / <strong class="lt iu">相关性</strong>矩阵将为<code class="fe of og oh oi b">[n_features, n_features]</code>，其中<strong class="lt iu">最大秩</strong>等于<code class="fe of og oh oi b">min(n_samples, n_features)</code>。</p><p id="45f5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我们可以在<strong class="lt iu">处拥有最</strong> <code class="fe of og oh oi b">min(n_samples, n_features)</code> <strong class="lt iu">有意义的</strong> PC <strong class="lt iu">组件/维度</strong>归因于协方差/相关矩阵的<strong class="lt iu">最大值</strong> <a class="ae ms" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">秩</strong> </a>。</p><h1 id="8402" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.使用 scikit-learn 和 Iris 数据集的 Python 示例</h1><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="d398" class="na la it oi b gy oq or l os ot">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/>from sklearn.decomposition import PCA<br/>import pandas as pd<br/>from sklearn.preprocessing import StandardScaler<br/>plt.style.use('ggplot')</span><span id="6c20" class="na la it oi b gy ou or l os ot"># Load the data<br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target</span><span id="8b59" class="na la it oi b gy ou or l os ot"># Z-score the features<br/>scaler = StandardScaler()<br/>scaler.fit(X)<br/>X = scaler.transform(X)</span><span id="3af2" class="na la it oi b gy ou or l os ot"># The PCA model<br/>pca = PCA(n_components=2) # estimate only 2 PCs<br/>X_new = pca.fit_transform(X) # project the original data into the PCA space</span></pre><p id="fa8f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们在 PCA 变换之前和<strong class="lt iu">变换之后绘制数据，并使用花</strong> <code class="fe of og oh oi b">(y)</code>的相应<strong class="lt iu">类对每个点(样本)进行<strong class="lt iu">颜色</strong>编码。</strong></p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="37c9" class="na la it oi b gy oq or l os ot">fig, axes = plt.subplots(1,2)</span><span id="2d46" class="na la it oi b gy ou or l os ot">axes[0].scatter(X[:,0], X[:,1], c=y)<br/>axes[0].set_xlabel('x1')<br/>axes[0].set_ylabel('x2')<br/>axes[0].set_title('Before PCA')</span><span id="3e6c" class="na la it oi b gy ou or l os ot">axes[1].scatter(X_new[:,0], X_new[:,1], c=y)<br/>axes[1].set_xlabel('PC1')<br/>axes[1].set_ylabel('PC2')<br/>axes[1].set_title('After PCA')</span><span id="3b90" class="na la it oi b gy ou or l os ot">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/231b65c6df061b1af8a5b7caf7f4b638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RsKGvLlewFt5kktQhbaO7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">上面的 PCA 输出代码。</p></figure><p id="311e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到在 PCA 空间中，<strong class="lt iu">方差</strong>是沿着<strong class="lt iu">PC1</strong><strong class="lt iu"> PC2 </strong><strong class="lt iu">最大化</strong>(解释了 73%的方差)和<strong class="lt iu">PC2</strong>(解释了 22%的方差)。两者加起来解释了 95%。</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="1a8a" class="na la it oi b gy oq or l os ot">print(pca.explained_variance_ratio_)<br/># array([0.72962445, 0.22850762])</span></pre><h1 id="b7a8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">6.原协方差矩阵特征值等于约简空间方差的证明</h1><h2 id="66ad" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">数学公式和证明</h2><p id="c1d5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">假设存储在<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>中的原始输入变量是<strong class="lt iu"> z 得分的</strong>，这样每个原始变量(<code class="fe of og oh oi b"><strong class="lt iu">X</strong></code>列)具有零均值和单位标准偏差，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/72c3d956556b5c2bf349b81bcd9766ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fDA0M4piVQGaWujmbiHSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者写的 Latex 代码。</p></figure><p id="8e45" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上面的<code class="fe of og oh oi b"><strong class="lt iu">Λ</strong></code>矩阵存储了原始<strong class="lt iu">空间/数据集的<strong class="lt iu">协方差</strong>矩阵的<strong class="lt iu">特征值</strong>。</strong></p><h2 id="59cf" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">使用 Python 验证</h2><p id="2a1b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">最大方差证明</strong>也可以通过估计<strong class="lt iu">约简</strong> <strong class="lt iu">空间</strong>的<strong class="lt iu">协方差</strong>矩阵来看:</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="18e1" class="na la it oi b gy oq or l os ot">np.cov(X_new.T)</span><span id="7d1d" class="na la it oi b gy ou or l os ot">array([[<strong class="oi iu">2.93808505e+00</strong>, 4.83198016e-16],<br/>       [4.83198016e-16, <strong class="oi iu">9.20164904e-01</strong>]])</span></pre><p id="82d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们观察到这些值(对角线上有方差)等于存储在<code class="fe of og oh oi b">pca.explained_variance_</code>中的协方差的<strong class="lt iu">实际</strong> <strong class="lt iu">特征值</strong>:</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="a917" class="na la it oi b gy oq or l os ot">pca.explained_variance_<br/>array([2.93808505, 0.9201649 ])</span></pre><h1 id="2221" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">7.特征重要性</h1><p id="ba5f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每个<strong class="lt iu">特征</strong>的<strong class="lt iu">重要性</strong>由特征向量中<strong class="lt iu">对应值的<strong class="lt iu">量级</strong>来反映(量级越高，重要性越高)。</strong></p><p id="cd52" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们找到<strong class="lt iu">最重要的特征:</strong></p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="5b16" class="na la it oi b gy oq or l os ot">print(abs( pca.components_ ))</span><span id="005f" class="na la it oi b gy ou or l os ot">[[0.52106591 0.26934744 0.5804131 0.56485654]<br/> [0.37741762 0.92329566 0.02449161 0.06694199]]</span></pre><p id="1d75" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里，<code class="fe of og oh oi b">pca.components_</code>具有形状<code class="fe of og oh oi b">[n_components, n_features]</code>因此，通过查看<strong class="lt iu"> PC1 </strong>(第一主成分)即<strong class="lt iu">第一</strong>行<strong class="lt iu">行</strong></p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="2f88" class="na la it oi b gy oq or l os ot">[[0.52106591 0.26934744 0.5804131 0.56485654]</span></pre><p id="2f3e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以得出结论，对于<strong class="lt iu"> PC1 来说，<strong class="lt iu">特性 1、3 和 4 </strong>是<strong class="lt iu">最重要的</strong>。</strong>同样，我们可以说<strong class="lt iu">特征 2 </strong>和<strong class="lt iu"> </strong>那么<strong class="lt iu"> 1 </strong>就是<strong class="lt iu"/><strong class="lt iu">对于<strong class="lt iu"> PC2 来说最重要的</strong>。</strong></p><p id="649b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ox">综上所述，我们看特征向量的分量的绝对值对应于</em> <strong class="lt iu"> <em class="ox"> k </em> </strong> <em class="ox">最大特征值。在 sklearn 中，组件按解释的方差排序。</em> <strong class="lt iu"> <em class="ox">这些绝对值越大，特定特征对该主成分的贡献越大。</em> </strong></p><h1 id="20fa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">8.双地块</h1><p id="e010" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">双标图</strong>是在<strong class="lt iu"> PCA </strong>分析后可视化<strong class="lt iu">一体化</strong>的最佳方式。</p><p id="381a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在<a class="ae ms" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/biplot" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> R </strong> </a>中有一个实现，但是在<strong class="lt iu"> python </strong>中没有标准实现，所以我决定为此编写自己的<strong class="lt iu">函数</strong>:</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="93aa" class="na la it oi b gy oq or l os ot">def biplot(score, coeff , y):<br/>    '''<br/>    Author: Serafeim Loukas, serafeim.loukas@epfl.ch<br/>    Inputs:<br/>       score: the projected data<br/>       coeff: the eigenvectors (PCs)<br/>       y: the class labels<br/>   '''</span><span id="01e3" class="na la it oi b gy ou or l os ot">    xs = score[:,0] # projection on PC1<br/>    ys = score[:,1] # projection on PC2<br/>    n = coeff.shape[0] # number of variables<br/>    plt.figure(figsize=(10,8), dpi=100)<br/>    classes = np.unique(y)<br/>    colors = ['g','r','y']<br/>    markers=['o','^','x']<br/>    for s,l in enumerate(classes):<br/>        plt.scatter(xs[y==l],ys[y==l], c = colors[s], marker=markers[s]) # color based on group<br/>    for i in range(n):<br/>        #plot as arrows the variable scores (each variable has a score for PC1 and one for PC2)<br/>        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', alpha = 0.9,linestyle = '-',linewidth = 1.5, overhang=0.2)<br/>        plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color = 'k', ha = 'center', va = 'center',fontsize=10)<br/><br/>    plt.xlabel("PC{}".format(1), size=14)<br/>    plt.ylabel("PC{}".format(2), size=14)<br/>    limx= int(xs.max()) + 1<br/>    limy= int(ys.max()) + 1<br/>    plt.xlim([-limx,limx])<br/>    plt.ylim([-limy,limy])<br/>    plt.grid()<br/>    plt.tick_params(axis='both', which='both', labelsize=14)</span></pre><p id="7e50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">调用函数(确保首先运行加载虹膜数据和执行 PCA 分析的初始代码块):</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="3f14" class="na la it oi b gy oq or l os ot">import matplotlib as mpl<br/>mpl.rcParams.update(mpl.rcParamsDefault) # reset ggplot style</span><span id="cec5" class="na la it oi b gy ou or l os ot"># Call the biplot function for only the first 2 PCs<br/>biplot(X_new[:,0:2], np.transpose(pca.components_[0:2, :]), y)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/5c02f96c187c47a61dd0b90f761948c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VxrEZWEr4k7ZljKp-OUEw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用我的自定义函数的 PCA 双图。</p></figure><p id="dd03" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以再次从视觉上验证<strong class="lt iu"/><strong class="lt iu">a)</strong>方差最大化<strong class="lt iu"> b) </strong>特征 1、3 和 4 是<strong class="lt iu"> PC1 最重要的<strong class="lt iu">。</strong>同样，<strong class="lt iu">功能 2 </strong>和<strong class="lt iu">功能 1</strong>和<strong class="lt iu">功能 1 </strong>都是<strong class="lt iu">功能</strong>功能<strong class="lt iu">对于<strong class="lt iu"> PC2 最重要的</strong>。</strong></strong></p><p id="3654" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，指向<strong class="lt iu">相同</strong> <strong class="lt iu">方向</strong>的<strong class="lt iu">箭头</strong>(变量/特征)表示它们所代表的变量之间的<strong class="lt iu">相关性</strong>，而指向与 <strong class="lt iu">方向</strong>相反<strong class="lt iu">的箭头表示它们所代表的变量之间的<strong class="lt iu">对比</strong>。</strong></p><p id="98b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">使用<strong class="lt iu">代码</strong>验证</strong>上述内容:</p><pre class="kj kk kl km gt om oi on oo aw op bi"><span id="620d" class="na la it oi b gy oq or l os ot"># <strong class="oi iu">Var 3 and Var 4</strong> are extremely <strong class="oi iu">positively</strong> correlated<br/>np.corrcoef(X[:,2], X[:,3])[1,0]<br/>0.9628654314027957</span><span id="9ba5" class="na la it oi b gy ou or l os ot"># <strong class="oi iu">Var 2and Var 3</strong> are <strong class="oi iu">negatively</strong> correlated<br/>np.corrcoef(X[:,1], X[:,2])[1,0]<br/>-0.42844010433054014</span></pre><p id="fc35" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">那都是乡亲们！希望你喜欢这篇文章！</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="9d89" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">最新帖子</h1><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">时间序列预测:用脸书的先知模型预测股票价格</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">使用可从《先知脸书》公开获得的预测模型预测股票价格</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq ks pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">用新冠肺炎假设的例子解释 ROC 曲线:二分类和多分类…</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">在这篇文章中，我清楚地解释了什么是 ROC 曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq ks pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的 python 教程…</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="ps l pn po pp pl pq ks pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">关于 Python 中的最小-最大规范化，您需要知道的一切</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用 scikit 在 Python 中实现它</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pt l pn po pp pl pq ks pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">Scikit-Learn 的标准定标器如何工作</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用 scikit-learn 应用标准化</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pu l pn po pp pl pq ks pc"/></div></div></a></div><h1 id="6b86" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">敬请关注并支持这一努力</h1><p id="7a46" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你喜欢并发现这篇文章有用，<strong class="lt iu">关注我吧！</strong></p><p id="0d60" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p><h1 id="2722" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="8b87" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] Jolliffe，I. T. <em class="ox">主成分分析。纽约州纽约市:斯普林格出版社，2002 年。</em></p><p id="eb67" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ms" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><p id="dd3a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ms" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank">https://stattrek.com/matrix-algebra/matrix-rank.aspx</a></p><h1 id="d7ce" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系</h1><ul class=""><li id="54cc" class="nr ns it lt b lu lv lx ly ma nt me nu mi nv mm nw nx ny nz bi translated"><strong class="lt iu">LinkedIn</strong>:【https://www.linkedin.com/in/serafeim-loukas/ T2】</li><li id="4c1e" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated">https://www.researchgate.net/profile/Serafeim_Loukas<strong class="lt iu">研究之门</strong>:<a class="ae ms" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">T7】</a></li><li id="48b0" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated"><strong class="lt iu">https://people.epfl.ch/serafeim.loukas</strong>EPFL<strong class="lt iu">简介</strong>:<a class="ae ms" href="https://people.epfl.ch/serafeim.loukas" rel="noopener ugc nofollow" target="_blank"/></li><li id="db30" class="nr ns it lt b lu oa lx ob ma oc me od mi oe mm nw nx ny nz bi translated"><strong class="lt iu">堆栈</strong> <strong class="lt iu">溢出</strong>:<a class="ae ms" href="https://stackoverflow.com/users/5025009/seralouk" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/users/5025009/seralouk</a></li></ul></div></div>    
</body>
</html>