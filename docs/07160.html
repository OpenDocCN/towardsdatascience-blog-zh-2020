<html>
<head>
<title>An introduction to Deep Similarity Learning for sequences</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列深度相似性学习导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-deep-similarity-learning-for-sequences-89d9c26f8392?source=collection_archive---------23-----------------------#2020-06-01">https://towardsdatascience.com/introduction-to-deep-similarity-learning-for-sequences-89d9c26f8392?source=collection_archive---------23-----------------------#2020-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e2c7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对用于相似性分类任务的深度学习技术的深入回顾。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3994163e03d08ceee3b00927f0250e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQAiNI3ukHDB34ipsOMNlg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗曼·维涅斯在<a class="ae ky" href="https://unsplash.com/s/photos/text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="906b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将阐述相似性学习的一般概念，它涉及哪些过程，以及如何对其进行总结。然后，我将把这些概述的概念应用到带有<strong class="lb iu">问题相似性</strong>的序列相似性检测的上下文中。</p><h1 id="b8b1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目录</h1><ol class=""><li id="3dbb" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated">相似性学习概述</li><li id="de38" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">文本相似性学习</li><li id="679b" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">源代码(PyTorch实现)</li></ol><h1 id="fa3d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.深度相似性学习综述</h1><p id="334d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">当一个人在进行相似性学习时，总是执行相同的过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/2177a5256612e6d2a3f2e42f0cf350b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qj3Blwpyv-Zme4TbivP0pw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有相似性学习的数据处理流水线</p></figure><p id="aa26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如本信息图中所解释的，任何涉及相似性学习的过程都围绕着3个主要概念:</p><ol class=""><li id="3c16" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu mu mv mw mx bi translated"><strong class="lb iu">特征向量中数据的变换</strong></li><li id="2019" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">使用距离度量对矢量进行比较</strong></li><li id="2bca" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">距离的分类</strong>为<em class="nk">相似</em>或<em class="nk">不相似</em></li></ol><h2 id="cdc7" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">1 .通过编码器的转换</h2><p id="0f9c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在大多数深度学习任务中，模型的第一层代表有时被称为“<em class="nk">编码阶段</em>”:它的作用是从输入数据中提取相关特征。</p><p id="eaf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本文的其余部分，我们将编写如下编码函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a80b1e3783ce01492b0c6669056189fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*dDbgm0SLQ8ozjtGLvvb68g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">编码函数符号的表示</p></figure><p id="bedc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据输入，该编码器可以采用不同的形式，其中我们发现:</p><ul class=""><li id="631a" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated"><strong class="lb iu"> RNN层</strong>用于<strong class="lb iu">序列</strong>的编码和对比；</li><li id="672a" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated"><strong class="lb iu"> CNN层</strong>用于<strong class="lb iu">时间</strong> / <strong class="lb iu">空间</strong>数据(1D卷积也可用于<strong class="lb iu">序列</strong>)；</li></ul><p id="9166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，<strong class="lb iu">在</strong>输入数据被这些<strong class="lb iu">编码器</strong>还原成一个矢量后，我们将多层<strong class="lb iu">全连接神经元</strong>堆叠起来<strong class="lb iu">对这些提取的<strong class="lb iu">特征</strong>进行分类。在我们的例子中，我们使用这个<strong class="lb iu">向量</strong>作为我们数据的降维版本来<strong class="lb iu">计算与其他数据的距离</strong>。用数字来表示两个向量的不同，比用两个句子来表示要容易得多。</strong></p><p id="7590" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，编码器将使用任何类型的层的组合，这些层将充分地针对其输入数据，生成数据的<strong class="lb iu">潜在表示</strong>，这是一种压缩的、非人类可解释的信息向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/e95b8c9d417687b6423f940220b44ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjDLJLjy_B4DUOp1jYy6HA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器的图示</p></figure><p id="4ee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">纵观深度学习的历史，已经创建了多种类型的架构来生成潜在向量。其中一些是:</p><ul class=""><li id="16ac" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated">暹罗神经网络<em class="nk">(科赫、泽梅尔和萨拉胡特迪诺夫，2015) </em></li><li id="dbc9" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated">多模态自动编码器<em class="nk">(西尔伯勒和拉帕塔，2015年)</em></li></ul><p id="85c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将进一步探索连体神经网络。</p><h2 id="95d7" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">1.b距离计算</h2><p id="f953" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">一旦我们有了矢量化的输入数据，我们就可以使用距离函数来比较这两个向量。最受欢迎的距离是:</p><ul class=""><li id="a307" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated"><strong class="lb iu">曼哈顿</strong>距离</li><li id="225f" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated"><strong class="lb iu">欧几里德</strong>距离</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/d58f54a235f0171d98c9b07057db04cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnngH4BqVLu-069cOa89YQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两种最常用距离的比较</p></figure><p id="b914" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦计算出距离，我们可以设置一个<strong class="lb iu">阈值</strong>，超过该阈值，我们认为两个数据<em class="nk">不相似</em>，反之亦然，认为它们<em class="nk">相似</em>。</p><h2 id="0e4a" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">1.c距离分类</h2><p id="6e07" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">但是，根据输入数据的不同，设置此阈值可能会很复杂或耗时。为简单起见，我们可以使用另一个分类器，该分类器将<strong class="lb iu">给定一个输入距离，如果该距离是相似或不相似物体之一<em class="nk">则进行分类。</em> </strong>我的选择是使用逻辑回归分类器:在我们对应的数据中找到一个线性间隔，以学习对我们的距离进行分类的阈值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/a3872a869bdc1c0ce6f8888c2494ec55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0tjgZeWqjHoTf793yOS_kA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">距离分类器的描述(这里是逻辑回归)</p></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="db54" class="lv lw it bd lx ly oi ma mb mc oj me mf jz ok ka mh kc ol kd mj kf om kg ml mm bi translated">2.文本相似性学习</h1><h2 id="acb5" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">2 .背景</h2><p id="2cb7" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">文本是一种极其难以处理的数据结构:虽然人们常说图像是通用的，但文本是文化的。不管是用的语言还是作者特有的词汇，文本都很难理解，即使对我们来说也是如此。</p><p id="2148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在某些情况下，我们希望能够测量文本之间的相似性。例如，我们可能想知道:</p><ul class=""><li id="0db3" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated">如果根据描述，<strong class="lb iu">两种产品是相同的</strong>；</li><li id="534c" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated">如果<strong class="lb iu">两个问题是</strong>问<strong class="lb iu">同一个</strong>事情。</li></ul><p id="e059" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这最后一个问题是我将在本文中用来谈论文本相似性问题的上下文:它源于Kaggle <em class="nk"> (Quora问题对)</em>，由Quora团队作为标记为重复或不重复的问题列表发布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/e1f32dba7a42b3be79e934a0075783fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gq9WEAGH0sSleZ0H96Q_-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从Quora数据集中提取的问题的比较</p></figure><p id="ef94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如<strong class="lb iu"> 1.a </strong>所述，已经为相似性学习任务创建了多种架构。对于这个任务的上下文，我们将重点关注<strong class="lb iu">连体递归神经网络</strong> <em class="nk"> (Thyagarajan，2015) </em>。</p><h2 id="0f54" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">2.b连体递归神经网络架构</h2><p id="eb02" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">连体递归神经网络是一种使用RNN堆栈来计算输入数据的固定大小的向量表示的神经网络。</p><p id="7b69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的暹罗网络的全局视图如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/3976efaf0b872d026bdd7fb888d34fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKCLCg7mKEdMlA4oF3NJjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我的连体身材</p></figure><p id="b86b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在Quora数据集上使用的架构代码是一个BiLSTMs(双向LSTM)堆栈，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="d438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，下面的编码器类使用BiLSTM的堆栈:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="d087" class="nl lw it bd lx nm nn dn mb no np dp mf li nq nr mh lm ns nt mj lq nu nv ml nw bi translated">2.c对比损失</h2><p id="efac" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在该模型中，需要注意两个主要组件:</p><ul class=""><li id="e7cc" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated">它的架构；</li><li id="99e0" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated">它的损失。</li></ul><p id="e65c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="nk"> (Hadsell，Chopra和LeCun，2006) </em>中描述，对比损失的目标是训练模型将<strong class="lb iu">相似的数据放在一起</strong>(即最小化它们的距离)和<strong class="lb iu">不相似的数据彼此远离</strong>(即最大化它们的距离)。其公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/4b3070eaef45289b85869cf5498aebe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCDjKx3PgWS-1DR5BJEDMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对比损失公式</p></figure><p id="a934" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个公式非常容易理解:</p><ul class=""><li id="ca75" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated">当两个输入相似(<em class="nk"> Y </em> =1)时，只保留左边的项，距离为平方。因此，最小化损失意味着最小化输入之间的距离，迫使模型学习相似物体的相似表示。</li><li id="8c4b" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated">当两个输入不同(<em class="nk"> Y </em> =0)时，仅保留正确的项，使用max函数。该max函数教导模型将两个输入的距离推至大于或等于余量超参数<em class="nk"> m </em>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/d1145ede250c1f0c2f202d7de04854be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AY3E-X02iswHvdtLWkwElg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失函数图(y=0时为橙色，y=1时为蓝色)</p></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="d992" class="lv lw it bd lx ly oi ma mb mc oj me mf jz ok ka mh kc ol kd mj kf om kg ml mm bi translated">3.源代码</h1><p id="475d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">该项目的源代码可以在Github的以下链接中找到:<a class="ae ky" href="https://github.com/dimartinot/Text-Semantic-Similarity/" rel="noopener ugc nofollow" target="_blank">https://github.com/dimartinot/Text-Semantic-Similarity/</a></p><p id="6d13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在所有3组数据(train、val和test)上实现了76%到79%的准确率以及0.83的AUC分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/09922565d3de3c5c9c245b1f420b24a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*-k5xT3-h0HPLmVyVFdTJsA.png"/></div></figure><p id="7254" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最重要的文件是:</p><ul class=""><li id="55b7" class="mn mo it lb b lc ld lf lg li nh lm ni lq nj lu ny mv mw mx bi translated"><a class="ae ky" href="https://github.com/dimartinot/Text-Semantic-Similarity/blob/master/notebook/EDA.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/dimartinot/Text-Semantic-Similarity/blob/master/notebook/EDA . ipynb</a>探索性数据分析笔记本:用于清理和分析数据集。生成带有预先计算的句子嵌入的数据集的删节版本</li><li id="d135" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated"><a class="ae ky" href="https://github.com/dimartinot/Text-Semantic-Similarity/blob/master/notebook/Training.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/dimartinot/Text-Semantic-Similarity/blob/master/notebook/training . ipynb</a>主训练管道:加载使用EDA.ipynb笔记本生成的酸洗数据集</li><li id="d432" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu ny mv mw mx bi translated"><a class="ae ky" href="https://github.com/dimartinot/Text-Semantic-Similarity/tree/master/src/model" rel="noopener ugc nofollow" target="_blank">https://github . com/dimartinot/Text-Semantic-Similarity/tree/master/src/model</a>:主模型文件夹。模型暂时保存在一个文件中，因为它们有很多相似之处。</li></ul><h1 id="dc47" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">参考文献</strong></h1><p id="a42c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">Koch g .，Zemel r .和Salakhutdinov r .，2015年。用于一次性图像识别的连体神经网络。<em class="nk"> ICML深度学习工场。</em></p><p id="54f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">西尔贝雷和拉帕塔，2015年。用自动编码器学习基础意义表征。计算语言学协会第52届年会会议录，第721–732页。</p><p id="146e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Thyagarajan，2015年。用于学习句子相似性的连体递归结构。</p><p id="9e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Quora。2017.<em class="nk"> Quora问题对</em>，<a class="ae ky" href="https://www.kaggle.com/c/quora-question-pairs/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p><p id="62da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哈德塞尔，拉伊亚&amp;乔普拉，苏米特&amp;勒昆，扬。2006.通过学习不变映射进行降维。</p></div></div>    
</body>
</html>