<html>
<head>
<title>Surprisingly Effective Way To Name Matching In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 命名匹配的惊人有效的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/surprisingly-effective-way-to-name-matching-in-python-1a67328e670e?source=collection_archive---------1-----------------------#2020-06-30">https://towardsdatascience.com/surprisingly-effective-way-to-name-matching-in-python-1a67328e670e?source=collection_archive---------1-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0460" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数据匹配、模糊匹配、重复数据删除</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e2677290a45fc1b511a731ff029ff106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRrnoWq4h9cRh1N5N5z0aA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">姓名匹配问题先睹为快，<em class="ky">图片作者</em>。</p></figure><p id="539d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> R </span>最近我偶然发现了这个数据集，在这里我需要分析数码产品的销售记录。我得到的数据集差不多有 572000 行和 12 列。我非常兴奋能够处理如此大的数据。我兴致勃勃地快速查看数据，发现同一个名字重复走不同的行。(啊！数据清理时间太长了！).</p><p id="5f37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一些产品栏上，它包含 iphone，而在另一个栏上，它写着 Iphone，或 iphone 7 +和 iphone 7 Plus，对于一些常规客户列表，有些有 Pushpa Yadav，而在其他 Pushpa Yadav(名称是伪的)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/9b337f75ad37266c150609e9bd522acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*9N8PA6uWcOvlclKfP8PX5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">先睹为快的问题视图(不是来自原始数据集)，作者的<em class="ky">图片。</em></p></figure><p id="d00c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是相同的产品名称和客户名称，但采取了不同的形式，即<strong class="lb iu">处理相同名称</strong>的不同版本。这类问题是数据科学家在数据分析过程中要处理的常见场景。这个场景有一个名字叫做<strong class="lb iu">数据匹配</strong>或<strong class="lb iu">模糊匹配</strong>(概率数据匹配)或简称为<strong class="lb iu">重复数据删除</strong>或<strong class="lb iu">字符串/名称匹配</strong>。</p><h2 id="dadb" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">为什么会有“不同但相似的数据”？</h2><p id="d39e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">常见的原因可能是:</p><ul class=""><li id="b208" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">数据输入时出现打字错误。</li><li id="b7f5" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">缩写用法。</li><li id="3914" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">系统数据输入没有被很好地验证以检查这种错误。</li><li id="a801" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">其他人。</li></ul><p id="4558" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，作为数据科学家或分析师，我们有责任匹配这些数据，以创建主记录供进一步分析。</p><p id="df20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我草草记下了解决问题的行动:</p><p id="c543" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.手动检查并解决。</p><p id="5667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.寻找有用的库/资源，这些库/资源是由社区的精英们分享的。</p><p id="b39a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这样一个扩展的数据集(572000* 12)来说，第一个选择确实很麻烦，所以我开始查看不同的库，并用模糊匹配器(conda 现在没有)尝试了这些方法，fuzzywuzzy 在核心上使用了<a class="ae nr" href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener ugc nofollow" target="_blank"> Levenshtein distance </a>和<a class="ae nr" href="https://docs.python.org/3/library/difflib.html" rel="noopener ugc nofollow" target="_blank"> difflib </a>。</p><blockquote class="ns nt nu"><p id="ca4f" class="kz la nv lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated">然而，在使用它们的过程中，我发现对于这样的大数据，这是非常耗时的。因此，我需要寻找一种更快更有效的方法。</p></blockquote><p id="9ee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经历了许多天的挫折后，我终于知道了克里斯·范登贝格分享的解决方案。</p><h2 id="4283" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated"><strong class="ak">我们将介绍</strong></h2><ul class=""><li id="ed1d" class="nd ne it lb b lc my lf mz li nz lm oa lq ob lu ni nj nk nl bi translated">ngram</li><li id="bdc6" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">…向量化…</li><li id="b1d1" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">TF-IDF</li><li id="7e08" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">余弦相似度与稀疏点 topn:领先果汁</li><li id="56a3" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">工作演示(代码工作)</li></ul><p id="853c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们的目标不仅仅是匹配字符串，而且是以更快的方式匹配。因此，ngram 的概念，具有余弦相似性的 TF-IDF 开始发挥作用。</p><p id="b102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在进入工作演示(代码工作)之前，让我们了解一下基础知识。</p><h2 id="a4aa" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">N-grams</h2><p id="f75e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">n 元语法广泛用于文本挖掘和自然语言处理，它是给定句子或(单词文件)中一组共现的单词。为了找到 n-gram，我们向前移动一个单词(可以根据需要移动任何一步)。</p><p id="5cd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，对于房型<em class="nv">“标准房海景威基基塔”</em></p><p id="10f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果 N=3(称为三元组)，那么 N 元组(三元组)将是:</p><ul class=""><li id="ab1e" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">标准客房海洋</li><li id="05dd" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">海景客房</li><li id="1622" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">怀基基海滩海景</li><li id="c1b2" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">观看怀基基塔</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c45230feeb3c07e355c313ad49277610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyteC41mmxINPbYYVlNjlQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">公式为 Ngrams，<em class="ky">图片作者</em>。</p></figure><p id="a409" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<a class="ae nr" href="https://ieeexplore.ieee.org/abstract/document/4470313/" rel="noopener ugc nofollow" target="_blank">研究论文</a>中探索更多关于 ngrams 的信息</p><p id="e759" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">为什么 n=3？</strong></p><p id="3ce0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能会产生一个问题，为什么 n = 3 可以被看作 n= 1(一元)或 n=2(二元)。</p><p id="28cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的直觉是，与单个词相比，双词和三词可以捕捉上下文信息。例如，当独立观察时，“房间海景”比仅仅“房间”、“海洋”和“景色”有更多的含义。</p><h2 id="51e6" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">TF-IDF</h2><p id="b65d" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">TF-IDF 代表术语频率-逆文档频率，而 TF-IDF 权重是信息检索和文本挖掘中经常使用的两个主要关注的权重。</p><p id="8c26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.用于评估一个单词对集合中的文档有多重要</p><p id="6c00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.重要性随着单词出现的次数成比例增加</p><p id="3290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">TF-IDF 背后的两个想法</strong></p><p id="fe2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词频(TF)，又名。一个单词在文档中出现的次数除以该文档中的总单词数，即它衡量一个术语在文档中出现的频率。</p><p id="8963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逆文档频率(IDF)，计算为语料库中文档数量的对数除以特定术语出现的文档数量，即，它衡量术语的重要性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c00f018798f1a44fa448658e9a3428fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*9EYNrtHzMywroHokf9cx8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF 的公式。</p></figure><p id="19ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算 TF 时，所有项都被认为是同等重要的。然而，众所周知，特定的词，如“是”、“的”和“那个”，可能会出现很多次，但没有什么重要性。</p><h2 id="76e0" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">…向量化…</h2><p id="ea47" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">将每个单词拆分后，需要将这些单词(词条)转换为 SciKit 学习算法模型可以使用的向量，因为这些算法只理解数字特征的概念，而不考虑其底层类型(文本、图像和数字等)。)允许我们对不同类型的数据执行复杂的机器学习任务。</p><p id="28f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Scikit-learn 的 tfidf 矢量器</strong>旨在将一组原始文档转换为 TF-IDF 特征矩阵，它可以一次性计算字数、IDF 和 TF-IDF 值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/685cc615069d04278db7f1a1d6bc10c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jEL_sW7GlW2WUnUn2tkyQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">图片作者。</em></p></figure><p id="2d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，我们就生成了 tf_idf_matrix，这是一个稀疏矩阵。与 Tfidfvectorizer 一样，我们将原始文本转换为单词和 n 元语法的数字向量表示。正如已经说过的，这使得直接使用变得容易；算法只理解数字特征的概念，而不管其基本类型(文本、图像、数字等)。).</p><h2 id="9399" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">余弦相似性</h2><p id="f6e4" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">两个文本文档在它们的上下文(表面接近度)和意义(即，词汇相似度和语义相似度)方面如何彼此接近被称为文本相似度，并且有各种方法来计算文本相似度，例如<strong class="lb iu">余弦相似度、欧几里德距离、雅克卡系数和骰子。</strong></p><p id="2aba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">余弦相似性度量两个文档之间的文本相似性，而不考虑它们的大小。数学上，余弦相似性度量测量在多维空间中投影的两个 n 维向量之间的角度的余弦，并且取值范围从 0 到 1，</p><p id="c91d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里，</p><ul class=""><li id="2f8e" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">1 表示更相似</li><li id="bd37" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">0 表示相似度较低。</li></ul><p id="ee25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/2e73ecf1b31f77e27f5f5fb5c683fbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCXUlvhSjvcIpHLNsrMl_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">余弦相似性公式。</p></figure><p id="5bd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，<a class="ae nr" href="https://medium.com/wbaa/what-does-ing-wb-advanced-analytics-do-707a09175530" rel="noopener"> ING 批发银行高级分析</a>团队的数据科学家发现<strong class="lb iu"> </strong>余弦相似度有一些缺点:</p><ul class=""><li id="339c" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">sklearn 版本做了大量的类型检查和错误处理。</li><li id="e33f" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">sklearn 版本一次性计算并存储所有相似度，而我们只对最相似的感兴趣。</li></ul><p id="b6e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此两者都会导致<strong class="lb iu">使用更多的内存消耗和时间。</strong></p><blockquote class="og"><p id="f6bc" class="oh oi it bd oj ok ol om on oo op lu dk translated">为了优化这些缺点，他们创建了他们的库，该库仅存储每行中前 N 个最高匹配，并且仅存储阈值以上的相似性。</p></blockquote><p id="ba79" class="pw-post-body-paragraph kz la it lb b lc oq ju le lf or jx lh li os lk ll lm ot lo lp lq ou ls lt lu im bi translated">获取更多关于库:<strong class="lb iu">s</strong><a class="ae nr" href="https://github.com/ing-bank/sparse_dot_topn" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">parse _ dot _ topn</strong></a></p><blockquote class="og"><p id="8d8e" class="oh oi it bd oj ok ol om on oo op lu dk translated">他们自豪地声称，这种方法提高了大约 40%的速度，并减少了内存消耗。</p></blockquote><p id="842b" class="pw-post-body-paragraph kz la it lb b lc oq ju le lf or jx lh li os lk ll lm ot lo lp lq ou ls lt lu im bi translated">有了这些基本的基础知识，我们开始我们的代码工作。</p><p id="3b11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我假设你熟悉 Jupyter 笔记本和下载库。如果没有，请查看我之前的<a class="ae nr" href="https://medium.com/analytics-vidhya/exploratory-data-analysis-for-beginner-7488d587f1ec" rel="noopener">帖子</a>，在那里我已经分享了探索性数据分析的初学者指南。</p><p id="25c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">工作环境:</strong> <a class="ae nr" href="https://docs.anaconda.com/ae-notebooks/user-guide/basic-tasks/apps/jupyter/" rel="noopener ugc nofollow" target="_blank">来自 Anaconda 的 Jupyter 笔记本</a></p><p id="ed91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据集:</strong> <a class="ae nr" href="https://github.com/maladeep/Name-Matching-In-Python/blob/master/room_type.csv" rel="noopener ugc nofollow" target="_blank">房间类型</a></p><p id="4afb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据集快照</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b06ce42805d66d829038b0e4e8fa48f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6_tWZUXpIXiNJu8fivLhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的快照。</p></figure><p id="1d4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们导入所需的库/方法，读取我们的数据集并将其保存到“df”中，只要快速查看数据集，就可以将“df”命名为任何名称。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="853a" class="mf mg it ox b gy pb pc l pd pe">#  Importing libraries and module and some setting for notebook</span><span id="cd0d" class="mf mg it ox b gy pf pc l pd pe">import pandas as pd <br/>import re<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import numpy as np<br/>from scipy.sparse import csr_matrix<br/>import sparse_dot_topn.sparse_dot_topn as ct  # Leading Juice for us<br/>import time<br/>pd.set_option('display.max_colwidth', -1)</span><span id="6d70" class="mf mg it ox b gy pf pc l pd pe"># reading dataset as df</span><span id="0024" class="mf mg it ox b gy pf pc l pd pe">df =  pd.read_csv('room_type.csv')</span><span id="7502" class="mf mg it ox b gy pf pc l pd pe"># printing first five rows</span><span id="02ae" class="mf mg it ox b gy pf pc l pd pe">df.head(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c9b18d1d915fa34a221efe166c1124ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*E9t5iuvrfQr2dfTwgkY-Iw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">快速查看 5 行数据集。</p></figure><h2 id="89fc" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">ngrams</h2><p id="fa8d" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">这里，我们取 n = 3，因此 3-gram 或三元组，因为大多数房间类型包含两个或三个单词，并且我们将句子分成标记，去除所有特殊字符、标点符号和单个字符(，-。/).收集那 3 克。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="02e5" class="mf mg it ox b gy pb pc l pd pe">def ngrams(string, n=3):</span><span id="54f6" class="mf mg it ox b gy pf pc l pd pe">string = re.sub(r'[,-./]|\sBD',r'', string)<br/>    ngrams = zip(*[string[i:] for i in range(n)])<br/>    return [''.join(ngram) for ngram in ngrams]<br/></span><span id="34ec" class="mf mg it ox b gy pf pc l pd pe"># Testing ngrams work for verification</span><span id="3345" class="mf mg it ox b gy pf pc l pd pe">print('All 3-grams in "Deluxroom":')<br/>ngrams('Deluxroom')</span></pre><p id="07f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了验证，我们测试了“Deluxroom”的 3-gram 结果，输出如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/d473b7024b0dea56501ed2b495e460fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwhXiRodosQ4HiwsIA6GEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“豪华客房”的 3 克结果。</p></figure><p id="09e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来不错！</p><p id="df41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在拆分每个单词(标记或词条(n-gram 生成的项目) )之后，我们需要对使用 Scikit-Learn 库提供的<a class="ae nr" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> Tfidfvectorizer </a>的每个文档中出现的单词进行计数，并将一组原始材料转换为 TF-IDF 特征矩阵。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="fd27" class="mf mg it ox b gy pb pc l pd pe">room_types = df['RoomTypes']<br/>vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)<br/>tf_idf_matrix = vectorizer.fit_transform(room_types)</span></pre><p id="dc2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们查看生成的稀疏 CSR 矩阵。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="f450" class="mf mg it ox b gy pb pc l pd pe">print(tf_idf_matrix[0])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/880e01767437a40b1e53fbd332ad3334.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*_PTT8-Dc6mg6wLVU3r5_Uw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">稀疏 CSR 矩阵。</p></figure><p id="9453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来不错！</p><h2 id="5916" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">余弦相似度与稀疏点 topn:领先果汁</h2><p id="8e78" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">我们正在用<strong class="lb iu"/>sparse _ dot _ topn<strong class="lb iu"/>库处理一个 CSR 矩阵。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="2417" class="mf mg it ox b gy pb pc l pd pe">def awesome_cossim_top(A, B, ntop, lower_bound=0):<br/>    # force A and B as a CSR matrix.<br/>    # If they have already been CSR, there is no overhead<br/>    A = A.tocsr()<br/>    B = B.tocsr()<br/>    M, _ = A.shape<br/>    _, N = B.shape<br/> <br/>    idx_dtype = np.int32<br/> <br/>    nnz_max = M*ntop<br/> <br/>    indptr = np.zeros(M+1, dtype=idx_dtype)<br/>    indices = np.zeros(nnz_max, dtype=idx_dtype)<br/>    data = np.zeros(nnz_max, dtype=A.dtype)</span><span id="4d66" class="mf mg it ox b gy pf pc l pd pe">ct.sparse_dot_topn(<br/>        M, N, np.asarray(A.indptr, dtype=idx_dtype),<br/>        np.asarray(A.indices, dtype=idx_dtype),<br/>        A.data,<br/>        np.asarray(B.indptr, dtype=idx_dtype),<br/>        np.asarray(B.indices, dtype=idx_dtype),<br/>        B.data,<br/>        ntop,<br/>        lower_bound,<br/>        indptr, indices, data)</span><span id="b783" class="mf mg it ox b gy pf pc l pd pe">return csr_matrix((data,indices,indptr),shape=(M,N))</span></pre><p id="935d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们存储前 10 个最相似的项目，并且只存储相似度高于 0.8 的项目，并显示模型所用的时间。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="602c" class="mf mg it ox b gy pb pc l pd pe">#  Top 10 with similarity above 0.8</span><span id="0b8a" class="mf mg it ox b gy pf pc l pd pe">t1 = time.time()<br/>matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)<br/>t = time.time()-t1<br/>print("SELFTIMED:", t)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/87bcf9f01c3ad6145413a364feed63ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*L3AWNP1jY7bgwMPJVkXo9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Selftime 0.0019，快！</p></figure><p id="66db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们解包得到的稀疏矩阵；</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="011f" class="mf mg it ox b gy pb pc l pd pe"># unpacks the resulting sparse matrix</span><span id="deec" class="mf mg it ox b gy pf pc l pd pe">def get_matches_df(sparse_matrix, name_vector, top=100):<br/>    non_zeros = sparse_matrix.nonzero()<br/>    <br/>    sparserows = non_zeros[0]<br/>    sparsecols = non_zeros[1]<br/>    <br/>    if top:<br/>        nr_matches = top<br/>    else:<br/>        nr_matches = sparsecols.size<br/>    <br/>    left_side = np.empty([nr_matches], dtype=object)<br/>    right_side = np.empty([nr_matches], dtype=object)<br/>    similairity = np.zeros(nr_matches)<br/>    <br/>    for index in range(0, nr_matches):<br/>        left_side[index] = name_vector[sparserows[index]]<br/>        right_side[index] = name_vector[sparsecols[index]]<br/>        similairity[index] = sparse_matrix.data[index]<br/>    <br/>    return pd.DataFrame({'left_side': left_side,<br/>                          'right_side': right_side,<br/>                           'similairity': similairity})</span></pre><p id="7230" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观看比赛。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="1dcd" class="mf mg it ox b gy pb pc l pd pe"># store the  matches into new dataframe called matched_df and <br/># printing 10 samples</span><span id="6215" class="mf mg it ox b gy pf pc l pd pe">matches_df = get_matches_df(matches, room_types, top=200)<br/>matches_df = matches_df[matches_df['similairity'] &lt; 0.99999] # For removing all exact matches<br/>matches_df.sample(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/f33b9cb8adf65b3d2314eda9e9445ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyVkaFVi1F-L7_Iw11G8sQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">火柴看起来相当令人满意！</p></figure><p id="71e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">套房</strong>、<strong class="lb iu"> 1 居室</strong>和<strong class="lb iu">豪华套房，1 居室</strong>很可能不是同一个房型，我们得到的相似度为 0.81。看起来不错！</p><p id="f864" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">按照排序的顺序，我们查看我们的匹配。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="9eaa" class="mf mg it ox b gy pb pc l pd pe"># printing the matches in sorted order</span><span id="20c1" class="mf mg it ox b gy pf pc l pd pe">matches_df.sort_values(['similairity'], ascending=False).head(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/774cf21f6659a04575c58930adcaf029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cX5I2FBIHW6KhhdXT092gA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">排序匹配。</p></figure><p id="3c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比赛看起来相当令人满意！余弦相似性给出了两种房间类型之间相似性的良好指示。<strong class="lb iu">大转角大床房 1 特大床</strong>和<strong class="lb iu">大转角大床房</strong>大概是同一个房型，我们得到的<strong class="lb iu">相似度为 0.93。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pm pn l"/></div></figure><blockquote class="ns nt nu"><p id="1346" class="kz la nv lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated">因此，正确的视觉评估和用这种策略进行的匹配是非常令人满意的。</p></blockquote><p id="cbbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，通过使用 ngram 进行标记化，使用 TF-IDF 进行向量矩阵，使用 TfidfVectorizer 对每个文档中出现的单词进行计数，并使用与 sparse_dot_topn 的余弦相似性，我们甚至可以最快地匹配大型数据集的字符串(使用 572000*12 获得了良好的结果)。</p><p id="9a3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的<a class="ae nr" href="https://github.com/maladeep/Name-Matching-In-Python" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中获取整个工作 Jupyter 笔记本。</p><blockquote class="ns nt nu"><p id="d3ad" class="kz la nv lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated">喜欢这篇文章吗？成为一个中等会员继续学习，没有限制。如果您使用 <a class="ae nr" href="/@maladeep.upadhaya/membership" rel="noopener ugc nofollow" target="_blank"> <em class="it">以下链接</em> </a> <em class="it">，我将收取您一部分会员费，无需您支付额外费用。</em></p><p id="b90e" class="kz la nv lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated">如果你对这篇文章有任何疑问，或者想在你的下一个数据科学项目中合作，请在 LinkedIn 上 ping 我。</p></blockquote><p id="ca95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">敬请关注下一篇与数据科学相关的帖子。</em></p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h2 id="798d" class="mf mg it bd mh mi mj dn mk ml mm dp mn li mo mp mq lm mr ms mt lq mu mv mw mx bi translated">推荐读物</h2><ul class=""><li id="19d0" class="nd ne it lb b lc my lf mz li nz lm oa lq ob lu ni nj nk nl bi translated"><a class="ae nr" href="https://mungingdata.wordpress.com/2017/11/25/episode-1-using-tf-idf-to-identify-the-signal-from-the-noise/" rel="noopener ugc nofollow" target="_blank">使用 TF-IDF 从噪声中识别信号</a></li><li id="9cc7" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><a class="ae nr" href="https://medium.com/wbaa/https-medium-com-ingwbaa-boosting-selection-of-the-most-similar-entities-in-large-scale-datasets-450b3242e618" rel="noopener">促进大规模数据集中最相似实体的选择</a></li><li id="95b6" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><a class="ae nr" href="https://studymachinelearning.com/cosine-similarity-text-similarity-metric/" rel="noopener ugc nofollow" target="_blank">余弦相似度——文本相似度度量</a></li><li id="bb45" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">N-gram 简介:它们是什么，我们为什么需要它们？</li><li id="9e69" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><a class="ae nr" href="https://medium.com/analytics-vidhya/quick-exploratory-data-analysis-pandas-profiling-421cd3ec5a50" rel="noopener">快速探索性数据分析:熊猫概况</a></li></ul></div></div>    
</body>
</html>