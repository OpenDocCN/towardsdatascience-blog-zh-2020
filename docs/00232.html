<html>
<head>
<title>Making A Model Is Like Baking A Cake</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">做模型就像烤蛋糕</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/making-a-model-is-like-baking-a-cake-5f2443894c5f?source=collection_archive---------32-----------------------#2020-01-07">https://towardsdatascience.com/making-a-model-is-like-baking-a-cake-5f2443894c5f?source=collection_archive---------32-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0627c6905c1c73623b1d7acce1324587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YTr1_3vJjH3OQTqf"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@henry_be?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亨利·贝</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="ee50" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">可供选择的蛋糕种类</h2><p id="1cb2" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">随着我们在当今时代的进一步发展，数据科学和技术的进步继续在研究和实践的各个领域取得惊人的进展。由于数据科学和技术的广泛适用性，已经构建了各种不同类型的模型。</p><p id="df61" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">举几个例子:广义线性模型、支持向量机、K-最近邻算法、梯度推进决策树、随机森林和神经网络。</p><p id="bab2" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">鉴于数据量和数据内交互的复杂性，已经用几种不同的语言开发了各种特定于数据科学的包。在python中，我们有sklearn、xgboost、lightgbm、pyspark和H2O。在R中，我们有，但不限于，Caret、Prophet、SparkR和xgboost。</p><p id="4a25" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">上述每个软件包都试图解决非常具体的数据科学问题。然而，考虑到初始问题的性质，我们可能需要一种更加定制的方法来导出解决方案。这就是每个数据科学家努力解决的问题。</p><blockquote class="mc md me"><p id="1db3" class="lc ld mf le b lf lx lh li lj ly ll lm mg lz lo lp mh ma lr ls mi mb lu lv lw im bi translated">通常，由于该领域中存在各种各样的问题，数据科学家使用模型和预定义的模型架构来开发解决方案，并寻求提高预定义模型无法解决的某些KPI指标性能。</p></blockquote><p id="4e6b" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">幸运的是，有一种方法可以开发一个建模架构来专门解决您的问题。通过理解这些预定义模型架构中每一个所使用的数学和统计过程，就有可能针对您的具体问题对模型进行逆向工程。</p><p id="0214" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">首先，让我们检查一系列模型，然后继续开发我们自己的模型。</p><h2 id="cae2" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">我们的预制蛋糕</h2><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/7ecb83185b9ba41c023a82663d0dcb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Uk3zUs46s8MCL35V"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@camitalpone?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡米·塔尔朋</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="caed" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">如前所述，有不同类型的模型，其中大多数是预定义的。唯一的例外是神经网络，在神经网络中，可以指定您自己的模型架构，并定义您自己的性能来直接优化。然而，考虑到参数的整体复杂性，我们将不讨论如何做到这一点。</p><p id="b5b5" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">为了便于讨论，我们将讨论一个相当基础的模型架构，它很容易进一步扩展。我们将考虑广义线性模型族。</p><blockquote class="mo"><p id="f43a" class="mp mq it bd mr ms mt mu mv mw mx lw dk translated"><strong class="ak">警告。</strong>这部分文章本质上是非常数学化的，需要一定程度的数学成熟度和理解力。如果您不想阅读这一部分，请跳到下一节，因为这一节有更多的python代码表示。</p></blockquote><p id="a3cd" class="pw-post-body-paragraph lc ld it le b lf my lh li lj mz ll lm kp na lo lp kt nb lr ls kx nc lu lv lw im bi translated">广义线性模型族由不同的模型架构组成。它可以是普通的最小二乘线性回归、休伯回归、逻辑回归和泊松回归，具有<em class="mf"> L2 </em>或<em class="mf"> L1、</em>或两者的混合、<em class="mf">T7】正则化。从解决数据科学问题的角度来看，这些不同的模型都是相似的</em></p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c11bba8e0f097b10ef483aa20be66d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*oWx5kwqtpyavonlcVS5r1A.png"/></div></figure><p id="91ed" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">在这个问题陈述中，我们有一个任意函数，<strong class="le iu"> <em class="mf"> f </em> </strong>，作用于我们提出的定义为<strong class="le iu"> <em class="mf"> Ax </em> </strong>的预测，其中<strong class="le iu"> <em class="mf"> A </em> </strong>是我们的训练数据集，<strong class="le iu"> <em class="mf"> x </em> </strong>是我们的系数列表，以及我们的实际训练标签，<strong class="le iu"> <em class="mf"> y </em> </strong>。我们的罚函数，可以作为我们的正则子，是<strong class="le iu"> <em class="mf"> p </em> </strong>。此外，双括号指的是一个规范。</p><p id="4f03" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">我们的预定义模型架构如何使用的一个示例是我们的普通线性平方问题，其中</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2d381e04fca0b2d70ab5c1e58f6d8322.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*g7o8yNZpFBWR0MdHJmW_5w.png"/></div></figure><p id="562c" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">而我们的范数叫做<a class="ae kf" href="https://en.wikipedia.org/wiki/Norm_(mathematics)" rel="noopener ugc nofollow" target="_blank">2-范数</a>。</p><p id="664d" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">我们的罚函数，<strong class="le iu"><em class="mf">【p】</em></strong><em class="mf"/><strong class="le iu"><em class="mf"/></strong>取决于我们是否要使用正则化。目前，让我们假设<strong class="le iu"> <em class="mf"> p(x) = 0 </em> </strong>。</p><p id="b6ad" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">在下一节中，我们将介绍一个例子，说明如何为我们自己的指标创建我们自己的GLM。</p><h2 id="468e" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">我们需要的原料</h2><p id="b299" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">现在让我们定义我们想要直接优化的指标！</p><p id="a677" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">对于我们的例子，让我们优化平均绝对误差。</p><p id="4725" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">对于这个任务，我们将使用两个特定的python包。一个是scipy，另一个是亲笔签名的。下面是如何利用这两者来创建我们选择的模型的代码片段，请注意这是如何遵循sklearn API的。</p><pre class="mk ml mm mn gt nf ng nh ni aw nj bi"><span id="bcc6" class="kg kh it ng b gy nk nl l nm nn"># this is for our optimization scheme<br/>from scipy.optimize import minimize</span><span id="1caf" class="kg kh it ng b gy no nl l nm nn"># this is to allow autograd to perform auto-differentiation<br/>from autograd import grad<br/>import autograd.numpy as np</span><span id="9dea" class="kg kh it ng b gy no nl l nm nn"># this function calculates our metric<br/>def mean_absolute_error(y_pred, y_true):<br/>    return np.mean(np.abs(y_pred - y_true))</span><span id="f7c2" class="kg kh it ng b gy no nl l nm nn"># the loss function we want to use, where data is our training data,</span><span id="169b" class="kg kh it ng b gy no nl l nm nn"># y is our training labels, and x is our linear coefficients<br/>def loss(data, x, y):<br/>    return mean_absolute_error(np.dot(data, x), y)</span><span id="d671" class="kg kh it ng b gy no nl l nm nn"># this class is fully equipped with all of the needed functions to </span><span id="9607" class="kg kh it ng b gy no nl l nm nn"># follow the sklearn API<br/>class MAERegressor:</span><span id="b0e7" class="kg kh it ng b gy no nl l nm nn">    def __init__(self, fit_intercept=True):<br/>        self.fi = fit_intercept<br/>        self.coef_ = None</span><span id="1656" class="kg kh it ng b gy no nl l nm nn">    def fit(self, data, y):<br/>        # we try to determine whether we want to fit the intercept<br/>        # or not</span><span id="35fe" class="kg kh it ng b gy no nl l nm nn">        fi = self.fi</span><span id="32dd" class="kg kh it ng b gy no nl l nm nn">        if fi:<br/>            _data = np.hstack((np.ones((data.shape[0], 1)), data))<br/>        else:<br/>            _data = data</span><span id="ac82" class="kg kh it ng b gy no nl l nm nn">        # below we define the optimization function and the gradient</span><span id="f984" class="kg kh it ng b gy no nl l nm nn">        def opt(x):<br/>            return loss(_data, x, y)</span><span id="c811" class="kg kh it ng b gy no nl l nm nn">        def f_grad(x):<br/>            return grad(opt)(x)</span><span id="adc5" class="kg kh it ng b gy no nl l nm nn">        # we assume our initial coefficients follow a normal<br/>        # distribution</span><span id="1684" class="kg kh it ng b gy no nl l nm nn">        res = minimize(opt, x0=np.random.normal(0, 1, (_data.shape[1], 1)),<br/>                       jac=f_grad)</span><span id="0273" class="kg kh it ng b gy no nl l nm nn">        # the coefficient is saved here<br/>        self.coef_ = res.get("x")<br/>        <br/>        return self</span><span id="c9d8" class="kg kh it ng b gy no nl l nm nn">    def predict(self, data):</span><span id="1f3a" class="kg kh it ng b gy no nl l nm nn">        fi = self.fi</span><span id="b28e" class="kg kh it ng b gy no nl l nm nn">        if fi:<br/>            _data = np.hstack((np.ones((data.shape[0], 1)), data))<br/>        else:<br/>            _data = data</span><span id="0db2" class="kg kh it ng b gy no nl l nm nn">        coef_ = self.coef_</span><span id="cef7" class="kg kh it ng b gy no nl l nm nn">        if coef_ is None:<br/>            raise RunTimeError("The model has not been trained yet")</span><span id="2f5d" class="kg kh it ng b gy no nl l nm nn">        return np.dot(_data, coef_)</span><span id="c13a" class="kg kh it ng b gy no nl l nm nn">    def score(self, val_data, val_y):</span><span id="a1a8" class="kg kh it ng b gy no nl l nm nn">        return mean_absolute_error(self.predict(val_data), val_y)</span></pre><p id="0a41" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">从上面可以看出，我们能够创建我们自己的模型架构，并优化我们自己的度量函数！</p><h2 id="a253" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">别忘了糖霜！</h2><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/eb3c8f3c0c0aec083ce693796e2a5d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SslPp__IH8lAYsRs"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Jordane Mathieu 在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4ffc" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">作为演示，在将上一节中的代码复制并粘贴到Python内核中之后，尝试运行以下代码片段</p><pre class="mk ml mm mn gt nf ng nh ni aw nj bi"><span id="8cd5" class="kg kh it ng b gy nk nl l nm nn">n = 10000<br/>m = 5</span><span id="ebd8" class="kg kh it ng b gy no nl l nm nn">num_labels = 1</span><span id="5660" class="kg kh it ng b gy no nl l nm nn">data = np.random.normal(0, 1, (n, m))<br/>w_ = np.random.random((m + 1, num_labels))<br/>_data = np.hstack((np.ones((n, 1)), data))<br/>labels = np.dot(_data, w_)</span><span id="a9b4" class="kg kh it ng b gy no nl l nm nn">model = MAERegressor(True)</span><span id="98f7" class="kg kh it ng b gy no nl l nm nn">model.fit(data, labels)</span><span id="dc0c" class="kg kh it ng b gy no nl l nm nn">model.score(data, labels)</span></pre><p id="17fc" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">你有它！自定义模型，它将确定线性模型的系数，该模型专门设置为最小化平均绝对误差。</p><p id="0c8f" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">为了创造一个类似的模型，你所要做的就是按照食谱做，你应该很棒！</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="e110" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">如果这引起了你的共鸣，请到我的主页订阅我的时事通讯</p><div class="nx ny gp gr nz oa"><a href="https://ed-turner.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">主页</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">在这里，您将找到有关Edward Turner所做工作的信息，以及…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">ed-特纳. github.io</p></div></div></div></a></div><p id="1adb" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">或者，在LinkedIn上关注我</p><div class="nx ny gp gr nz oa"><a href="https://www.linkedin.com/in/edward-turner-polygot/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">爱德华·特纳-数据科学家- Paylocity | LinkedIn</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">爱德华·特纳(Edward Turner)是一名多语言开发人员，懂Python、R和Scala，懂Java和C/C++的语法。他…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">www.linkedin.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo jz oa"/></div></div></a></div></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="8317" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">如果你有兴趣了解更多关于我是如何在这个领域变得熟练的，你可能也会对这篇文章感兴趣。</p><div class="nx ny gp gr nz oa"><a href="https://medium.com/@edward.turnerr/the-journey-into-data-science-d702f8b810a9" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">数据科学之旅</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">通常，所有的旅程都有开始和结束。幸运的是，或者至少我现在知道，我的…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div></div></a></div></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="7620" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">一如既往，#快乐编码</p></div></div>    
</body>
</html>