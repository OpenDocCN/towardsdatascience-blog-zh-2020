<html>
<head>
<title>Optimization — Descent Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化—下降算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimization-descent-algorithms-bf595f069788?source=collection_archive---------6-----------------------#2020-04-01">https://towardsdatascience.com/optimization-descent-algorithms-bf595f069788?source=collection_archive---------6-----------------------#2020-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e87e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本帖中，我们将看到几种基本的优化算法，您可以在各种数据科学问题中使用它们。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ded6c08c5db7e0ceaa0b941abddfa0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNjI4S1T7jNBDi3oaJEGbg.png"/></div></div></figure><p id="2ae8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习中使用的许多算法都是基于基本的数学优化方法。由于所有的先决条件，在机器学习的背景下直接发现这些算法可能会令人困惑。因此，我认为，为了更好地理解这些技术，将这些算法脱离任何上下文可能是一个好主意。</p><h1 id="4d1b" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">下降算法</h1><p id="7bb6" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">下降算法意味着<strong class="kw iu">最小化</strong>一个给定的函数，就是这样。真的。这些算法不断地进行<strong class="kw iu">迭代</strong>，这意味着它们不断地改进当前的解决方案。你可能会想:</p><blockquote class="mn mo mp"><p id="ee41" class="ku kv mq kw b kx ky ju kz la lb jx lc mr le lf lg ms li lj lk mt lm ln lo lp im bi translated">如果我想求一个函数的最大值呢？</p></blockquote><p id="8688" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">简单来说，在你的函数前面加一个<em class="mq">减</em>号，就成了一个“min”问题！</p><p id="4e6f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们开始吧。这是我们的问题定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/89abdcb6461858d33868980e2a29375b.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/0*X61V0WDN5Cx6vxhz.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">我们的问题包括寻找一个能使函数 f 最小的向量 x*</p></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="87b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你必须知道的一个先决条件是，如果一个点是最小值、最大值或鞍点(意味着两者同时存在)，那么函数的<strong class="kw iu">梯度</strong>在该点为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6f079f113f74732bb00f856b7cf0b943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*1og7x2jLFgEWn8mjAqQiqA.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">1D 案件</p></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="e231" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下降算法包括构建一个序列<strong class="kw iu"><em class="mq">【x }</em></strong>，该序列将向<strong class="kw iu"><em class="mq">【x *】</em></strong>(<em class="mq">arg min f(x)</em>)收敛。该序列按以下方式构建:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/60558f98af6eae02f7d330d36230a13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/0*9OIjEOTWgZxIsa4n.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">为了到达 x*我们试图建立的序列</p></figure><p id="3dc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="mq"> k </em>为迭代，<strong class="kw iu"> <em class="mq"> d </em> </strong>为向量，大小与<em class="mq"> x </em>相同，称为<strong class="kw iu">下降向量</strong>。那么，这就是算法的样子:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="7d72" class="nn lr it nj b gy no np l nq nr">x = x_init<br/>while ||gradient(f(x))|| &gt; epsilon:<br/>    x = x + d</span></pre><p id="c458" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就是这样！我们一直进行更新，直到梯度的范数足够小(因为它应该在某个极值达到零值)。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="2e0a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将看到 3 个不同的下降/方向向量:牛顿方向、梯度方向和梯度+最佳步长方向。首先，我们需要定义一个函数，在我们的实验中，我们会尽量使它最小化。</p><h1 id="3cee" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">罗森布罗克函数</h1><p id="8cb8" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我选择了 Rosenbrock 函数，但你可能会发现许多其他函数，比如这里的<a class="ae ns" href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" rel="noopener ugc nofollow" target="_blank"/>。另一个很好的例子是 Himmelblau 函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d454277903bc9800b426e08c62e4f9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*SQpYo3KNVPXP-bAr.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">罗森布罗克函数—维基百科</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/282ebc738e190073c0f188fa5d87ccd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*tquPJO6RiFd82jxQ.png"/></div></figure><p id="0a8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它在<em class="mq"> (x，y)=(a，a ) </em>处有全局最小值，其中<em class="mq"> f(a，a ) = 0 </em>。我会用<em class="mq"> a=1，b=100 </em>这些都是常用值。</p><p id="6596" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还需要另外两条信息，函数的<a class="ae ns" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"/></a>，以及<a class="ae ns" href="https://en.wikipedia.org/wiki/Hessian_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">黑森</strong> </a>矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/90000a9a952b9742f05c9fb72253bb5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*awI4Z4pf1SHyMsb2.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">罗森布罗克函数的梯度</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/530910674bbd24b336e5b442a8de36b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*td5QTu3mj6wee1Ev.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">罗森布罗克函数的海森</p></figure><p id="202d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们打开一个文件并启动一个 Python 脚本。我将在 Google Colab 中这样做，本文中使用的所有代码都可以在这里找到:</p><div class="nx ny gp gr nz oa"><a href="https://colab.research.google.com/drive/1jOK_C_pSDV6J98ggV1-uEaWUrPuVm5tu" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">下降算法</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">奥马尔·阿夫拉克</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">colab.research.google.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><p id="2453" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是我们的第一段代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="8f4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从现在开始，我将函数输入向量称为<strong class="kw iu"> <em class="mq"> x </em> </strong>，类似于前面的问题定义。现在我们准备好了，让我们看看第一个下降向量！</p><h1 id="130c" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">牛顿方向</h1><p id="f846" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">牛顿的方向如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f95ffe35ff2ceb0a2efbd89884b0d074.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*Ym2KTv-3Ne3qQLti.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">牛顿方向</p></figure><p id="99bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以更新是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/57afdb8d6daee3e54847f971370336be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*SRjUsiT-8MZFy1ku.png"/></div></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="4b08" class="nn lr it bd ls ot ou dn lw ov ow dp ma ld ox oy mc lh oz pa me ll pb pc mg pd bi translated">快速注释 1</h2><p id="1de6" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">你可以通过做<em class="mq">f(</em><strong class="kw iu"><em class="mq">x+d</em></strong><em class="mq">)</em>的二阶<a class="ae ns" href="https://en.wikipedia.org/wiki/Taylor_series" rel="noopener ugc nofollow" target="_blank">泰勒展开</a>找到这个更新的公式，因为我们正在执行的更新是<em class="mq">x _ new =</em><strong class="kw iu"><em class="mq">x+d</em></strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/af6229398225dac8ddeab6e4db72e134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*hHi_rPu2OTG6qwqs.png"/></div></figure><p id="f01c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们希望找到<em class="mq"> d </em>使得<em class="mq"> f(x + d) </em>尽可能低。假设<em class="mq">f″(x)</em>为正，这个方程是一条有最小值的抛物线。当<em class="mq"> f(x + d) </em>的导数为零时，达到该最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7497b61c9b138d236563b5c5dcf7aa43.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/0*lIaTOSfJZ8H5w_-D.png"/></div></figure><p id="1747" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 n 维中，<em class="mq">f″(x)</em>成为 hessian 矩阵，<em class="mq">1/f″(x)</em>表现为 hessian 矩阵的逆。最后，<em class="mq"> f'(x) </em>将是渐变。</p><h2 id="fed1" class="nn lr it bd ls ot ou dn lw ov ow dp ma ld ox oy mc lh oz pa me ll pb pc mg pd bi translated">快速注释 n 2</h2><p id="7a2e" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我们需要计算海森矩阵的逆矩阵。对于大型矩阵，这是一项计算量非常大的任务。因此，在实践中，我们解决这一点有点不同，但在一个完全等同的方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/11cf55d5b451996779bdd78c29e1be83.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/0*sFe5mu9XfCiJCQni.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">线性方程</p></figure><p id="db83" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代替计算 hessian 矩阵的逆矩阵，我们为<em class="mq"> g </em>求解该方程，并使更新规则如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/09fa24314005a041e0960f9ff321db45.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/0*c1lIIlI1ooMXD0Sl.png"/></div></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="db5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们编写算法代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="abc6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您会注意到与我在开始时介绍的算法有一点不同。我加了一个<em class="mq"> max_iteration </em>参数，这样算法不收敛就不会无限期运行。让我们试一试。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="e972" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们得到这样的结果:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="91d6" class="nn lr it nj b gy no np l nq nr">x* = [1. 1.]<br/>Rosenbrock(x*) = 0.0<br/>Grad Rosenbrock(x*) = [0. 0.]<br/>Iterations = 2</span></pre><p id="ab4a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该算法仅在<strong class="kw iu"> 2 </strong>次迭代中收敛！那真的很快。你可能会想:</p><blockquote class="mn mo mp"><p id="25de" class="ku kv mq kw b kx ky ju kz la lb jx lc mr le lf lg ms li lj lk mt lm ln lo lp im bi translated">嘿，初始的<em class="it"> x </em>非常接近目标<em class="it"> x*，这使得任务很容易！</em></p></blockquote><p id="81dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你说得对。尝试使用一些其他值，例如<strong class="kw iu"><em class="mq">x _ init =【50，-30】</em></strong>，算法终止于<strong class="kw iu"> 5 </strong>次迭代。</p><p id="1e29" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个算法叫做<a class="ae ns" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank">牛顿法</a>所有的下降算法都是这个方法的修改！这是一种母亲配方。它真正快速的原因是它使用了<strong class="kw iu">二阶信息</strong>(黑森矩阵)。</p><p id="ce15" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用 hessian 矩阵，即使它是 dope，也是有代价的:效率。计算逆矩阵是一项计算量很大的任务，因此数学家们想出了解决这个问题的方法。主要有:<em class="mq">拟牛顿</em>方法，和<em class="mq">梯度</em>方法。拟牛顿法试图用各种技术逼近 hessian 矩阵的逆矩阵，而梯度法只是简单地坚持一阶信息。</p><h1 id="ea0a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">渐变的方向</h1><p id="7830" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如果你做过机器学习，你可能已经看过这个了。渐变方向:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/64fce515e087853f5c3ff9189508fef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/0*GQuSlYLQehPMwVcV.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">渐变的方向</p></figure><p id="7efe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="mq"> α </em>称为步长(或以 ML 为单位的学习速率)，是一个在<em class="mq"> [0，1]范围内的实数。</em></p><p id="45a8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你一直在做一些机器学习，现在你知道这个公式实际上是一个更大的公式的一部分:牛顿方向，只是我们用一个常数代替了逆 hessian！不管怎样，现在的更新规则是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/dfe27f271fb7958c790eb5481d10b0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*NtDW3s6liFdSMtiW.png"/></div></figure><p id="d9b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该算法变成:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="ad9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们试一试:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="a73b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以调整<em class="mq"> alpha </em>、<em class="mq"> epsilon </em>和<em class="mq"> max_iterations </em>的值。为了得到类似牛顿法的结果，我想出了这些方法。这是结果:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="16bf" class="nn lr it nj b gy no np l nq nr">x* = [0.99440769 0.98882419]<br/>Rosenbrock(x*) = 3.132439308613923e-05<br/>Grad Rosenbrock(x*) = [-0.00225342 -0.00449072]<br/>Iterations = 5000</span></pre><p id="1132" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">哇！梯度下降法需要 5000 次迭代，而牛顿法只需要 2 次！而且算法没有完全达到最小点<em class="mq"> (1，1) </em>。</p><p id="96ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与牛顿相比，该算法收敛如此缓慢的主要原因是，我们不仅不再拥有由<em class="mq"> f、</em>的二阶导数给出的信息，而且我们使用了一个<strong class="kw iu">常数</strong>来代替逆 hessian。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="ba2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">想想吧。函数的导数是该函数的变化率。所以黑森给出了梯度变化率的信息。由于寻找最小值必然意味着零梯度，hessian 变得非常有用，因为它告诉你梯度何时上升或下降。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="6852" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ML 中的许多论文只是关于为这个特定的步骤找到一个更好的方法。Momentum、Adagrad 或 Adadelta 就是一些例子。</p><h1 id="391a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">梯度的方向+最佳步长</h1><p id="5728" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">对经典梯度下降的一个改进是在每次迭代中使用可变步长，而不是常数。这不仅是一个可变的步长，而且也是<strong class="kw iu">可能的最佳步长。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b769b03b923565c453af8a71ccf38d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*xSXYsQ5i-1mBF15w.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">αk 是迭代 k 时的步长</p></figure><p id="1495" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">更新是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/72ab8632d75d9b5989f8b56e5d2db3d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*GbfRrDMpqgLMuZsH.png"/></div></figure><p id="7d0f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们如何找到<strong class="kw iu"> α </strong>？由于我们希望此次更新尽可能高效，即尽可能减少<em class="mq"> f </em>，我们正在寻找<strong class="kw iu"> α </strong>，以便:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ee2b914fca0af373341b47636446fa8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*nDn9PmkevXiEzciP.png"/></div></figure><p id="5645" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，在这一步，<strong class="kw iu"> <em class="mq"> x </em> </strong>和<strong class="kw iu"> <em class="mq"> grad(x) </em> </strong>是常量。因此，我们可以定义一个新的函数<strong class="kw iu"> <em class="mq"> q </em> </strong> <em class="mq"> : </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/e64e76324ace4ebaa2a972268b2cb08c.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*Aon4JwDNW75ZG3X6.png"/></div></figure><p id="1ba1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> <em class="mq"> q </em> </strong>实际上是一个变量的函数。我们想找到最小化这个函数的<strong class="kw iu"> α </strong>。嗯……梯度下降？我们可以，但是既然这样，让我们学习一个新方法:<a class="ae ns" href="https://en.wikipedia.org/wiki/Golden-section_search" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">黄金分割搜索</strong> </a>。</p><p id="d6cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="mq">黄金分割搜索</em>旨在寻找一个函数在指定区间内的极值(最小值或最大值)。由于我们在[0，1] 范围内使用<strong class="kw iu"> α，这是使用该算法的绝佳机会。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/98ee2a06d05e1e8e8afbeae96b99fe7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/1*gDw31rWNTTm9X5Dnuk11xg.gif"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">黄金分割搜索—前 5 次迭代</p></figure><p id="2126" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为这篇文章开始变得很长，所以我不打算深入细节。希望在我花了很长时间制作的精美 GIF 和下面的代码的帮助下，你能够理解这里发生了什么。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">维基百科实现</p></figure><p id="a224" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">既然我们能够找到最佳的<strong class="kw iu"> α </strong>，让我们用最佳步长编码梯度下降！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="dd8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们可以运行这段代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="2f75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们得到以下结果:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="58c9" class="nn lr it nj b gy no np l nq nr">x* = [0.99438271 0.98879563]<br/>Rosenbrock(x*) = 3.155407544747055e-05<br/>Grad Rosenbrock(x*) = [-0.01069628 -0.00027067]<br/>Iterations = 3000</span></pre><p id="b689" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">即使在这种情况下，结果没有明显好于纯梯度下降，通常最佳步长执行得更好。例如，我尝试用 Himmelblau 的函数进行同样的比较，最优步长的梯度下降比纯梯度下降快两倍多。</p><h1 id="c628" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">结论</h1><p id="7dbc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">本帖到此结束。希望你学到了一些新的东西，触发了你对数学优化的好奇心！还有很多其他有趣的方法。去找他们！不要忘了检查一下<a class="ae ns" href="https://colab.research.google.com/drive/1jOK_C_pSDV6J98ggV1-uEaWUrPuVm5tu" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>文件，你会发现所有使用的代码和我们在这里对 Himmelblau 函数做的相同测试。不要犹豫，留下评论，直到下次，和平！:)</p></div></div>    
</body>
</html>