<html>
<head>
<title>Databricks Delta Lake — Database on top of a Data Lake — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据湖之上的数据库第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/databricks-delta-lake-database-on-top-of-a-data-lake-part-2-f44e44f51a63?source=collection_archive---------32-----------------------#2020-09-03">https://towardsdatascience.com/databricks-delta-lake-database-on-top-of-a-data-lake-part-2-f44e44f51a63?source=collection_archive---------32-----------------------#2020-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0610" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第 2 部分，共 2 部分——了解 Databricks Delta Lake 的基础知识——分区、模式演变、数据沿袭和真空</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b22706c1e1dd1d8441ef55b003a3be07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqM67Jv2ke910lunJA_bUg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2899901" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的 Gerd Altmann 提供</p></figure><p id="8d69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第 1 部分中，我们探讨了 Delta Lake 特性(如 ACID 事务、检查点、事务日志和时间旅行)如何对变更数据捕获、处理和管理产生积极影响。在本文中，我们将继续推进我们对一些高级特性的理解，比如分区、模式演化、数据沿袭和真空</p><h2 id="4f13" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据起源/数据血统</h2><p id="25a5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">从摄取到管理和转换数据经历了一个旅程。这个旅程被称为 ad<strong class="lb iu">T5 数据血统。如今，监管机构对跟踪和审计数据实施了非常严格的指导方针。因此，验证数据沿袭是满足法规遵从性和治理要求的关键活动。</strong></p><p id="b62a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Delta Lake 将每次写入表的数据沿袭信息存储 30 天。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="3b4f" class="lv lw it mv b gy mz na l nb nc">$ git clone <a class="ae ky" href="https://github.com/mkukreja1/blogs.git" rel="noopener ugc nofollow" target="_blank">https://github.com/mkukreja1/blogs.git</a></span></pre><p id="408c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的笔记本可从<strong class="lb iu">/delta _ lake/delta _ lake-demo-2 . ipynb 获得。</strong>让我解释一下下面的每个步骤:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="e47f" class="lv lw it mv b gy mz na l nb nc">deltaTable = DeltaTable.forPath(spark, "hdfs:///delta_lake/products")</span><span id="3748" class="lv lw it mv b gy nd na l nb nc">df_history = deltaTable.history()<br/>df_history.show(20, False)</span><span id="4a41" class="lv lw it mv b gy nd na l nb nc">+-------+-----------------------+------+--------+----------------------+-------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+<br/>|version|timestamp              |userId|userName|operation             |operationParameters                                                      |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                           |userMetadata|<br/>+-------+-----------------------+------+--------+----------------------+-------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+<br/>|12     |2020-09-01 16:48:45.792|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 270)]                                     |null|null    |null     |11         |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|11     |2020-09-01 16:48:43.906|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 280)]                                     |null|null    |null     |10         |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|10     |2020-09-01 16:48:41.15 |null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 260)]                                     |null|null    |null     |9          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|9      |2020-09-01 16:48:39.497|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 200)]                                     |null|null    |null     |8          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|8      |2020-09-01 16:48:37.695|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 240)]                                     |null|null    |null     |7          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|7      |2020-09-01 16:48:35.437|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 220)]                                     |null|null    |null     |6          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|6      |2020-09-01 16:48:33.499|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 250)]                                     |null|null    |null     |5          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|5      |2020-09-01 16:48:31.559|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 210)]                                     |null|null    |null     |4          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|4      |2020-09-01 16:48:29.492|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 230)]                                     |null|null    |null     |3          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 0]                                                                                                                        |null        |<br/>|3      |2020-09-01 16:48:26.544|null  |null    |MERGE                 |[predicate -&gt; (products.`ProductID` = products_new.`ProductID`)]         |null|null    |null     |2          |null          |false        |[numTargetRowsCopied -&gt; 0, numTargetRowsDeleted -&gt; 0, numTargetFilesAdded -&gt; 10, numTargetRowsInserted -&gt; 5, numTargetRowsUpdated -&gt; 4, numOutputRows -&gt; 9, numSourceRows -&gt; 9, numTargetFilesRemoved -&gt; 1]|null        |<br/>|2      |2020-09-01 16:48:19.493|null  |null    |DELETE                |[predicate -&gt; ["(`ProductID` = 210)"]]                                   |null|null    |null     |1          |null          |false        |[numRemovedFiles -&gt; 1, numDeletedRows -&gt; 1, numAddedFiles -&gt; 1, numCopiedRows -&gt; 4]                                                                                                                        |null        |<br/>|1      |2020-09-01 16:48:12.635|null  |null    |UPDATE                |[predicate -&gt; (ProductID#529 = 200)]                                     |null|null    |null     |0          |null          |false        |[numRemovedFiles -&gt; 1, numAddedFiles -&gt; 1, numUpdatedRows -&gt; 1, numCopiedRows -&gt; 4]                                                                                                                        |null        |<br/>|0      |2020-09-01 16:47:31.819|null  |null    |CREATE TABLE AS SELECT|[isManaged -&gt; false, description -&gt;, partitionBy -&gt; [], properties -&gt; {}]|null|null    |null     |null       |null          |true         |[numFiles -&gt; 1, numOutputBytes -&gt; 1027, numOutputRows -&gt; 5]                                                                                                                                                |null        |<br/>+-------+-----------------------+------+--------+----------------------+-------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+<br/></span></pre><p id="1a4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不需要担心数据审计，因为对数据的每一次更改都有完整的更改审计跟踪作为备份。</p><p id="4c1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单看一下表上的最后一个操作如下:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="2796" class="lv lw it mv b gy mz na l nb nc">df_lastOperation = deltaTable.history(1)<br/>df_lastOperation.show()</span><span id="5452" class="lv lw it mv b gy nd na l nb nc">+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+<br/>|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|<br/>+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+<br/>|     12|2020-09-01 16:48:...|  null|    null|   UPDATE|[predicate -&gt; (Pr...|null|    null|     null|         11|          null|        false|[numRemovedFiles ...|        null|<br/>+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+</span></pre><h2 id="a101" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模式演变——检测和适应模式变化</h2><p id="5b7d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一直在创建数据管道的人肯定会对这个问题感兴趣。我们使用通常在项目开始时提供给我们的给定模式来创建和部署管道。在一段时间内，一切都很好，直到有一天管道出错。原来传入文件的模式已经改变。就我个人而言，我已经多次被同一个问题所困扰。在我的第一次攻击中，数据遭到了大范围的破坏，因为我们没有在破坏数据之前<strong class="lb iu">发现它。我仍然记得我们不得不花费无数的时间来处理损坏——修复代码和回溯数据。</strong></p><p id="5f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那天我学到了重要的一课— <strong class="lb iu">在接收数据之前验证模式</strong>。这是以后另一篇文章的主题。现在，让我们把注意力集中在三角洲湖如何能有所帮助。</p><p id="d6c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Delta Lake 可以通过在写入时执行模式验证来保护数据。那是什么意思？</p><ol class=""><li id="d360" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">这意味着在写入时，新输入数据的模式会与过去的数据进行比较。</li><li id="ad83" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">如果发现差异，则取消事务—没有数据写入存储—为用户引发异常。现在让我们来看看它的运行情况。这是我们之前摄取的最后一个文件。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e47841afcb66287ab11a318a9d7057d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*1iNFV0rzI1EcnI7C.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">产品 _aug21.csv</p></figure><p id="38ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是第二天的新文件。请注意，文件的模式已经更改，添加了新的列数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f7e40189c3ae8d7a248bc6246b9ca7a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*0Gt3ItWiVHeaOhkV7T02YA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">产品 _aug22.csv</p></figure><p id="6e64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回顾一下 pypark 代码:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a089" class="lv lw it mv b gy mz na l nb nc">df_productsaug22 = spark.read.csv('hdfs:///delta_lake/raw/products_aug22.csv', header=True, inferSchema=True)<br/>df_productsaug22.show()</span><span id="1b8e" class="lv lw it mv b gy nd na l nb nc">deltaTable = DeltaTable.forPath(spark, "hdfs:///delta_lake/products")</span><span id="7b05" class="lv lw it mv b gy nd na l nb nc">df_productsaug22.write.format("delta").mode("append").save("hdfs:///delta_lake/products")</span><span id="7f61" class="lv lw it mv b gy nd na l nb nc">AnalysisException                         Traceback (most recent call last)<br/>&lt;ipython-input-15-85affcb142df&gt; in &lt;module&gt;<br/>----&gt; 1 df_productsaug22.write.format("delta").mode("append").save("hdfs:///delta_lake/products")<br/><br/>/opt/spark/python/pyspark/sql/readwriter.py in save(self, path, format, mode, partitionBy, **options)<br/>    825             self._jwrite.save()<br/>    826         else:<br/>--&gt; 827             self._jwrite.save(path)<br/>    828 <br/>    829     @since(1.4)<br/><br/>~/.local/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)<br/>   1303         answer = self.gateway_client.send_command(command)<br/>   1304         return_value = get_return_value(<br/>-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)<br/>   1306 <br/>   1307         for temp_arg in temp_args:<br/><br/>/opt/spark/python/pyspark/sql/utils.py in deco(*a, **kw)<br/>    135                 # Hide where the exception came from that shows a non-Pythonic<br/>    136                 # JVM exception message.<br/>--&gt; 137                 raise_from(converted)<br/>    138             else:<br/>    139                 raise<br/><br/>/opt/spark/python/pyspark/sql/utils.py in raise_from(e)<br/><br/><strong class="mv iu">AnalysisException: A schema mismatch detected when writing to the Delta table </strong>(Table ID: 320f5591-72dd-4f4c-bdac-38f560e90dba).<br/>To enable schema migration using DataFrameWriter or DataStreamWriter, please set:<br/>'.option("mergeSchema", "true")'.<br/>For other operations, set the session configuration<br/>spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation<br/>specific to the operation for details.<br/><br/>Table schema:<br/>root<br/>-- ProductID: integer (nullable = true)<br/>-- Date: string (nullable = true)<br/>-- Price: double (nullable = true)<br/><br/><br/>Data schema:<br/>root<br/>-- ProductID: integer (nullable = true)<br/>-- Date: string (nullable = true)<br/>-- Price: double (nullable = true)<br/>-- Quantity: integer (nullable = true)<br/><br/>         ;</span></pre><p id="4fce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Delta Lake 立即拒绝新文件，因为模式不匹配。相当酷。</p><p id="e549" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对新列没有意见，也就是说，即使存在模式不匹配，您也希望接收数据，那会怎么样呢？使用 option("mergeSchema "，" true ")进行如下操作。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="f263" class="lv lw it mv b gy mz na l nb nc">df_productsaug22.write.format("delta").mode("append").option("mergeSchema", "true").save("hdfs:///delta_lake/products")</span><span id="e1b9" class="lv lw it mv b gy nd na l nb nc">spark.table("products").show()</span><span id="b3ec" class="lv lw it mv b gy nd na l nb nc">+---------+----------+------+--------+<br/>|ProductID|      Date| Price|Quantity|<br/>+---------+----------+------+--------+<br/>|      200|2020-08-22|  25.5|       2|<br/>|      210|2020-08-22|  46.0|       5|<br/>|      220|2020-08-22| 34.56|       6|<br/>|      230|2020-08-22| 23.67|      11|<br/>|      200|2020-08-20|  25.5|    null|<br/>|      250|2020-08-21| 99.76|    null|<br/>|      230|2020-08-20| 23.67|    null|<br/>|      210|2020-08-21|  46.0|    null|<br/>|      220|2020-08-20| 34.56|    null|<br/>|      260|2020-08-21| 64.55|    null|<br/>|      280|2020-08-21| 54.78|    null|<br/>|      270|2020-08-21|106.32|    null|<br/>|      240|2020-08-20|100.82|    null|<br/>+---------+----------+------+--------+</span></pre><p id="d275" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新列现在是三角洲湖元数据的一部分。注意，数量数据为以前的数据(&lt; 2020–08–22) has been set to <strong class="lb iu"><em class="mt">)null。</em>T3】</strong></p><h2 id="96a3" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">分割</h2><p id="6997" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了显著提高 Delta Lake 中的查询性能，您应该考虑创建一个按列分区的表。选择正确的分区柱非常重要。</p><ol class=""><li id="76aa" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">选择具有低基数的列，如 date，绝对不要选择顺序 ID 列</li><li id="3b17" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">选择大小均匀且数据量大(希望大于 1 GB)列</li></ol><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="a25e" class="lv lw it mv b gy mz na l nb nc">df_productsaug_partition = spark.read.csv('hdfs:///delta_lake/raw/*.csv', header=True, inferSchema=True)</span><span id="9b5e" class="lv lw it mv b gy nd na l nb nc">df_productsaug_partition.write.format("delta").partitionBy("Date").option("path", "hdfs:///delta_lake/products_p").saveAsTable("products_p")</span><span id="8ceb" class="lv lw it mv b gy nd na l nb nc">$ hadoop fs -ls /delta_lake/products_p<br/>Found 4 items<br/>drwxr-xr-x   - mkukreja supergroup          0 2020-09-01 17:19 /delta_lake/products_p/<strong class="mv iu">Date=2020-08-20</strong><br/>drwxr-xr-x   - mkukreja supergroup          0 2020-09-01 17:19 /delta_lake/products_p/<strong class="mv iu">Date=2020-08-21</strong><br/>drwxr-xr-x   - mkukreja supergroup          0 2020-09-01 17:19 /delta_lake/products_p/<strong class="mv iu">Date=2020-08-22</strong><br/>drwxr-xr-x   - mkukreja supergroup          0 2020-09-01 17:19 /delta_lake/products_p/_delta_log</span></pre><p id="77f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，Delta Lake 已经基于日期列创建了一个分区文件夹结构。</p><h2 id="3740" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">真空</h2><p id="61fe" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Delta Lake 实现了数据版本化，因此它可以按需提供旧版本的数据。随着时间的推移，存储同一数据的多个版本可能会变得非常昂贵。因此，Delta Lake 包含了一个叫做<em class="mt">真空</em>的清理机制，可以删除旧版本的数据。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="2d60" class="lv lw it mv b gy mz na l nb nc">deltaTable.vacuum()</span></pre><p id="d4e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章是有帮助的。<strong class="lb iu">三角洲湖</strong>作为大数据 Hadoop、Spark &amp; Kafka 课程的一部分，由<a class="ae ky" href="http://www.datafence.com" rel="noopener ugc nofollow" target="_blank"> Datafence 云学院</a>提供。课程是周末自己在网上教的。</p></div></div>    
</body>
</html>