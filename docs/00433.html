<html>
<head>
<title>Logistic regression: the basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归:基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-the-basics-b1716661c71b?source=collection_archive---------19-----------------------#2020-01-13">https://towardsdatascience.com/logistic-regression-the-basics-b1716661c71b?source=collection_archive---------19-----------------------#2020-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/462c0599f773eb4f844793f5a2aea421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LXMKaL2lKBGQyjR_"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">尼克·霍克斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="b357" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">理解逻辑回归技术的基础</h2></div><h2 id="198d" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">介绍</h2><p id="0c44" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">逻辑回归是一种模拟事件概率的技术。就像<a class="ae jg" rel="noopener" target="_blank" href="/linear-regression-the-basics-4daad1aeb845">线性回归</a>一样，它帮助你理解一个或多个变量与一个目标变量之间的关系，只不过，在这种情况下，我们的目标变量是二元的:它的值不是0就是1。例如，它可以允许我们说“吸烟会使你患肺癌的风险增加20%”，因为患肺癌是一个二元变量:你要么有，要么没有(希望没有)。由此，我们可以推断出分类问题的答案。例如，它可以帮助我们做出有根据的猜测，如果某人不吸烟，住在污染的城市，并且有肺癌家族史，他/她是否会患肺癌。</p><p id="e85a" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">也就是说，逻辑回归的结构也类似于线性回归模型的结构:你有一组解释变量(X1，X2…)和我们的目标二元变量(Y)。然而，其背后的功能有点复杂:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/237a9e2fbded9add4dcbb2a353b00a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMhEmHqlieWQC1ryMl_mxg@2x.png"/></div></figure><p id="19f6" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">P(Y=1)表示Y等于1的概率，而b0是与X无关的参数，B是系数向量，表示Y与X1、X2等中的每一个之间的关系。</p><p id="14fc" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">然后，逻辑回归将估计更适合您的数据的b参数值，通常使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然法</a>。一旦我们有了这些估计量，我们就可以计算新数据点的P(Y=1 ),或者坚持这个概率，或者使用它根据一个阈值(例如:如果某人患肺癌的概率大于50%，我们可以有根据地猜测他们会得肺癌)。</p><p id="2e63" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">为了更好地理解线性回归和逻辑回归之间的差异，假设我们在Y轴绘制了肺癌变量(如果患者患有肺癌，Y = 1，否则为0 ),在X轴绘制了患者的年龄。下面是每次回归的结果线。哪一个似乎更符合我们的数据？</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/7ab105cab47e2365d219cf6a4f224d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DalqI0RtzyMjIQFpMhVeiA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://bit.ly/35MhQwg" rel="noopener ugc nofollow" target="_blank">https://bit.ly/35MhQwg</a></p></figure><h2 id="05a2" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">开始弄脏我们的手</h2><p id="0369" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">处理分类问题的最著名的数据集之一是<a class="ae jg" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>，其中有泰坦尼克号乘客的名单，他们的一些特征，如年龄、性别和姓名，以及他们是否在灾难中幸存(出于某种原因，许多分类问题往往与坏事有关，如肺癌和在灾难中死亡)。我们将在R中工作，但是如果你愿意，你可以在Python中做同样的事情。</p><p id="9038" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">让我们先来看看可用的变量:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="a3b0" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:<br/></strong>path = 'insert you file path here'<br/>data = read.csv(paste(path,'train.csv', sep = ""))<br/>colnames(data)</span><span id="0996" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/> [1] "PassengerId" "Survived"    "Pclass"      "Name"        "Sex"         "Age"         "SibSp"      <br/> [8] "Parch"       "Ticket"      "Fare"        "Cabin"       "Embarked"</span></pre><p id="1f41" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">所以，除了Id，我们还有一些可能有用的信息，比如他们在船上的级别(第一、第二或第三)和他们的性别。</p><h2 id="3a13" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据清理</h2><p id="db1c" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在我们开始建模之前，我们必须清理我们的数据。但是，请记住，本文的目标是介绍逻辑回归，而不是数据清理，所以我们在这里不做太深入的讨论。</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="7564" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:<br/></strong>data$Age[is.na(data$Age)] = median(data$Age,na.rm=T)<br/>data$Pclass = as.factor(data$Pclass)</span></pre><p id="bf15" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">我们必须首先用年龄的中位数来代替缺失的年龄。然后，我们将乘客类特性转化为一个因子:这意味着，R将把它作为一个类别读取，而不是作为整数读取，这在这种情况下更有意义。</p><h2 id="b916" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">训练/测试分割</h2><p id="c710" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">下一步是将我们的数据集分为训练和测试，这样我们就可以构建我们的模型，然后在另一个数据集中计算我们的模型尚未使用的一些准确性指标。我们为训练集选择了一个任意的大小，但它通常是原始数据集的70%到80%左右。</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="c613" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>train = data[1:700,]<br/>test = data[701:889,]</span></pre><h2 id="4c62" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">系统模型化</h2><p id="1783" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">对于我们的第一个模型，让我们选择一些我们凭直觉认为可能与泰坦尼克号灾难幸存概率有某种联系的变量。我个人认为，乘客的阶级、年龄和性别可以帮助我们预测他们是否幸存:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="be5a" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>model = glm(Survived ~ Pclass + Sex + Age,<br/>             family = binomial(link = 'logit'), data = train)<br/>summary(model)</span><span id="ad70" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/>Call:<br/>glm(formula = Survived ~ Pclass + Sex + Age, family = binomial(link = "logit"), <br/>    data = train)</span><span id="3c2e" class="ky kz jj mz b gy nh ne l nf ng">Deviance Residuals: <br/>    Min       1Q   Median       3Q      Max  <br/>-2.5352  -0.7055  -0.4390   0.6186   2.3728</span><span id="680e" class="ky kz jj mz b gy nh ne l nf ng">Coefficients:<br/>             Estimate Std. Error z value Pr(&gt;|z|)    <br/>(Intercept)  3.226908   0.398636   8.095 5.73e-16 ***<br/>Pclass2     -0.908714   0.288174  -3.153 0.001614 ** <br/>Pclass3     -2.153203   0.268262  -8.026 1.00e-15 ***<br/>Sexmale     -2.603025   0.209018 -12.454  &lt; 2e-16 ***<br/>Age         -0.027199   0.008157  -3.334 0.000855 ***<br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="6332" class="ky kz jj mz b gy nh ne l nf ng">(Dispersion parameter for binomial family taken to be 1)</span><span id="114b" class="ky kz jj mz b gy nh ne l nf ng">Null deviance: 934.43  on 699  degrees of freedom<br/>Residual deviance: 645.20  on 695  degrees of freedom<br/>AIC: 655.2</span><span id="30ac" class="ky kz jj mz b gy nh ne l nf ng">Number of Fisher Scoring iterations: 4</span></pre><p id="3239" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">哇，一下子这么多信息，对吧？但是现在让我们把注意力集中在基础上，从我们如何构建模型开始:</p><blockquote class="ni nj nk"><p id="c1a9" class="lu lv nl lw b lx mn kk lz ma mo kn mc nm mp me mf nn mq mh mi no mr mk ml mm im bi translated">glm(存活~ Pclass +性别+年龄，家庭=二项式(link = 'logit ')，数据=训练)</p></blockquote><p id="e846" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">我们首先调用一个名为<em class="nl"> glm </em>的函数，用于拟合广义线性模型。为了使它像逻辑回归一样工作，我们设置<em class="nl">家庭=二项式</em>和<em class="nl">链接=‘logit’。</em>对于我们的问题，我们也可以将link设置为'<em class="nl"> probit' </em>或'<em class="nl"> cochit' </em>，但是我们将坚持使用logit函数。它们之间的差异主要是理论上的，它们的结果通常是相当相似的。</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="1ec2" class="ky kz jj mz b gy nd ne l nf ng">Coefficients:<br/>             Estimate Std. Error z value Pr(&gt;|z|)    <br/>(Intercept)  3.226908   0.398636   8.095 5.73e-16 ***<br/>Pclass2     -0.908714   0.288174  -3.153 0.001614 ** <br/>Pclass3     -2.153203   0.268262  -8.026 1.00e-15 ***<br/>Sexmale     -2.603025   0.209018 -12.454  &lt; 2e-16 ***<br/>Age         -0.027199   0.008157  -3.334 0.000855 ***</span></pre><p id="a892" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">现在，继续讨论系数，我们可以看到它们都是负的(看一下<em class="nl">估计值</em>栏)，这意味着所有这些变量都与生存概率负相关。那就是:作为一名男性或处于2类或3类(而不是作为一名女性或处于1类)使你在泰坦尼克号灾难中幸存的可能性更小。年龄系数也是负的，所以，年龄越大，存活的可能性越小。为了解释系数的精确值，让我们回到概率函数:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/237a9e2fbded9add4dcbb2a353b00a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMhEmHqlieWQC1ryMl_mxg@2x.png"/></div></figure><p id="3e22" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">这里的<em class="nl">截距</em>系数是b0，其他系数是向量b。我们的模型看起来像这样(为了更好的可读性，我将系数四舍五入):</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/3878c959d9675ce5f69178979f8f798c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8GMHquUkVn5FpGOS65EJQ@2x.png"/></div></div></figure><p id="1e44" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">其中，如果乘客在2等舱，Pclass2 = 1，否则为0(其他变量类似，除了年龄，年龄等于乘客的实际年龄)。除了是我们概率方程的一部分之外，它们还帮助我们解释几率:性别男性的系数为-2.6意味着当你是男性时存活的几率是女性时存活几率的0.07倍。</p><p id="3b81" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">表中另一个重要的列是Pr(&gt;|z|)，我们称之为p值。它显示了我们对估计系数显著性的确信程度(越接近零，我们越有信心)。如果我们有一些高p值的系数，我们可能不应该在我们的模型中包括相关的变量。</p><p id="59d8" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">最后，我们将讨论的最后一项是Akaike信息标准(AIC)，显示在模型总结的末尾。简而言之，AIC是对我们将模型应用于测试样本时的误差的估计，它有助于我们比较模型(AIC越小越好)。</p><p id="4715" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">现在，让我们尝试第二个模型，添加票价变量(乘客为机票支付了多少钱):</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="7f10" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>model2 = glm(Survived ~ Pclass + Sex + Age + Fare,<br/> family = binomial(link = ‘logit’), data = train)<br/>summary(model2)</span><span id="210a" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/>Call:<br/>glm(formula = Survived ~ Pclass + Sex + Age + Fare, family = binomial(link = "logit"), <br/>    data = train)</span><span id="6580" class="ky kz jj mz b gy nh ne l nf ng">Deviance Residuals: <br/>    Min       1Q   Median       3Q      Max  <br/>-2.5225  -0.7060  -0.4382   0.6187   2.3749</span><span id="2f00" class="ky kz jj mz b gy nh ne l nf ng">Coefficients:<br/>              Estimate Std. Error z value Pr(&gt;|z|)    <br/>(Intercept)  3.2957397  0.4738515   6.955 3.52e-12 ***<br/>Pclass2     -0.9500899  0.3265572  -2.909 0.003621 ** <br/>Pclass3     -2.2016346  0.3229743  -6.817 9.31e-12 ***<br/>Sexmale     -2.6085804  0.2100784 -12.417  &lt; 2e-16 ***<br/>Age         -0.0275303  0.0082521  -3.336 0.000849 ***<br/>Fare        -0.0006707  0.0024848  -0.270 0.787211    <br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="4e5d" class="ky kz jj mz b gy nh ne l nf ng">(Dispersion parameter for binomial family taken to be 1)</span><span id="8ec6" class="ky kz jj mz b gy nh ne l nf ng">Null deviance: 934.43  on 699  degrees of freedom<br/>Residual deviance: 645.13  on 694  degrees of freedom<br/>AIC: 657.13</span><span id="f01b" class="ky kz jj mz b gy nh ne l nf ng">Number of Fisher Scoring iterations: 4</span></pre><p id="4d1a" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">请注意，票价的p值很高，这意味着它不是一个重要的变量，AIC增加了，这意味着模型稍微差一些。一种可能是，由于我们已经考虑了乘客的等级，机票票价并没有增加太多新的信息。为了测试这一点，让我们运行第三个模型，使用fare但删除Pclass:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="75f3" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>model3 = glm(Survived ~  Sex + Age + Fare,<br/>             family = binomial(link = 'logit'), data = train)<br/>summary(model3)</span><span id="da71" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/>Call:<br/>glm(formula = Survived ~ Sex + Age + Fare, family = binomial(link = “logit”), <br/> data = train)</span><span id="1880" class="ky kz jj mz b gy nh ne l nf ng">Deviance Residuals: <br/> Min 1Q Median 3Q Max <br/>-2.2015 -0.6174 -0.5889 0.8093 1.9786</span><span id="5167" class="ky kz jj mz b gy nh ne l nf ng">Coefficients:<br/> Estimate Std. Error z value Pr(&gt;|z|) <br/>(Intercept) 0.835415 0.250798 3.331 0.000865 ***<br/>Sexmale -2.429141 0.192294 -12.632 &lt; 2e-16 ***<br/>Age -0.005092 0.007294 -0.698 0.485142 <br/>Fare 0.009933 0.002412 4.119 3.81e-05 ***<br/> — -<br/>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="c490" class="ky kz jj mz b gy nh ne l nf ng">(Dispersion parameter for binomial family taken to be 1)</span><span id="5033" class="ky kz jj mz b gy nh ne l nf ng">Null deviance: 934.43 on 699 degrees of freedom<br/>Residual deviance: 701.47 on 696 degrees of freedom<br/>AIC: 709.47</span><span id="bcf0" class="ky kz jj mz b gy nh ne l nf ng">Number of Fisher Scoring iterations: 4</span></pre><p id="d61e" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">这一次，我们的AIC显著恶化，票价有一个重要的系数，但年龄不再重要。知道为什么吗？在这里评论你的假设:)</p><p id="aec3" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">现在，让我们将我们的第一个模型应用于测试样本，看看效果如何，该模型的性能优于以下两个模型:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="944c" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>predictions = ifelse(predict(model, newdata = test) &gt; 0.5, 1, 0)</span><span id="7ea1" class="ky kz jj mz b gy nh ne l nf ng">accuracy = mean(predictions == test$Survived)<br/>print(paste(‘Accuracy :’, accuracy))</span><span id="3d46" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/>"Accuracy : 0.825396825396825"</span></pre><p id="f497" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">我们首先将我们的模型应用于测试集，并声明如果计算的概率大于0.5，则乘客幸存。我们计算的第一个指标是准确性，它代表我们正确预测的比率。0.82的准确度意味着我们82%的预测是正确的。不错吧？嗯，看情况。想象一下99%的乘客都死了。然后，我们可以预测所有乘客死亡，我们的准确率将达到99%,而不需要为此建立模型。因此，我们应该在我们的度量中考虑幸存者的比率。这就是ROC曲线和AUC出现的原因。</p><p id="cbbc" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">ROC代表接收器操作特性，它是真阳性率(当实际值为1时预测为1的概率)与假阳性率(当实际值为0时预测为1的概率)的曲线图。当我们绘制曲线并计算其下方的面积时，我们得到AUC，代表曲线下的面积。该区域始终在0.5和1之间，考虑到1和0的样本分布，这为我们提供了衡量模型性能的良好尺度。</p><p id="cb6e" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">为了在R中进行这些计算，我们需要ROCR包:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="8cd2" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>library(ROCR)</span><span id="7ba2" class="ky kz jj mz b gy nh ne l nf ng">probabilities = predict(model, newdata = test)<br/>prediction_object = prediction(probabilities, test$Survived)<br/>roc = performance(prediction_object, measure = "tpr", x.measure = "fpr")<br/>plot(roc)</span><span id="593b" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong></span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/41a20f49472a23475fe71c67e03b0e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*rYtbROK8BdJzPD8B0698Dg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们第一个模型的ROC曲线</p></figure><p id="f3bd" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">曲线下的面积越大，我们的模型就越好，所以我们希望曲线尽可能靠近图的左上角。请注意，在我们的代码中，我们是如何使用performance()函数创建它的，并对x.measure使用“fpr ”,对measure使用“tpr”。FPR代表假阳性率，TPR代表真阳性率。为了计算auc，我们再次使用performance()函数，但是这次我们输入“AUC”作为测量值:</p><pre class="mt mu mv mw gt my mz na nb aw nc bi"><span id="a4ca" class="ky kz jj mz b gy nd ne l nf ng"><strong class="mz jk">IN:</strong><br/>perf_auc = performance(prediction_object, measure = “auc”)<br/>auc = perf_auc@y.values[[1]]<br/>print(paste(‘AUC :’, auc))</span><span id="bd7d" class="ky kz jj mz b gy nh ne l nf ng"><strong class="mz jk">OUT:</strong><br/>"AUC : 0.863385354141657"</span></pre><p id="b3ea" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">我们有一个0.86的AUC，这对于一个分类问题来说是相当不错的。</p><h2 id="45d9" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论</h2><p id="e4bf" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">逻辑模型用于分类问题，与更复杂的替代方案相比，它们的优势之一是可解释性:它们的结果很容易用外行人的话来解释。我们已经看到了如何在R中运行逻辑回归，理解它的结果，如何比较不同的模型并评估它们的性能。正如标题所示，这是一篇介绍性文章，我鼓励您深入挖掘由此产生的所有可能性。您可以通过为glm()函数设置不同的链接或者添加/删除变量来尝试改进这个模型。也许有一种自动化的方法，比如线性回归？</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="b4ef" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">你可以在这里访问完整的R脚本<a class="ae jg" href="https://github.com/arthurmello/statistics/tree/master/2.%20Logistic%20regression" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>