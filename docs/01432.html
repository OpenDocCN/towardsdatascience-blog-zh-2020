<html>
<head>
<title>The Many Uses of Input Gradient Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">输入梯度正则化的许多用途</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950?source=collection_archive---------24-----------------------#2020-02-08">https://towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950?source=collection_archive---------24-----------------------#2020-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1e73" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将这一通用的概念添加到您的深度学习工具带中，以实现对抗鲁棒性、输入稀疏性、更好的泛化能力等</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/87adcd535f1dd6e658d96ed11a9f7556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lbGdjP7hWIROCpaH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">菲利普·斯温伯恩在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b07a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">正则化</em>是机器学习的基本技术之一。这个想法是，我们对理想模型的结构做出假设，并修改我们的训练过程，以鼓励产生满足这些期望的模型。通常，正则项是损失函数的一部分。也许机器学习中最常见的正则化是模型权重上的 L1 范数(假设:稀疏权重)和 L2 范数(假设:小权重)，分别导致套索和岭回归。相对较新的是<a class="ae ky" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> dropout </a>的概念，我们喜欢网络中的冗余。</p><p id="789a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我讨论正则化的输入梯度。具体来说，这意味着对输入的梯度如何表征<em class="lv"> x </em> wrt 施加一些结构约束。我们的损失函数表现为。自然，这种技术只能应用于训练完全可微分的模型，如神经网络。</p><p id="c88f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对输入梯度施加约束的想法确实很古老，但似乎只是在最近几年才受到重视。为了理解它的有用性，我们需要理解输入梯度的<em class="lv">的含义</em>。直观地说，输入梯度告诉我们，在非常小的输入扰动下，模型损耗如何变化。输入梯度是输入空间中导致局部损耗变化最大的方向。</p><p id="566f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个特性使它成为许多可解释的 ML 技术的核心，试图用输入来解释输出。(例如，参见<a class="ae ky" href="https://arxiv.org/abs/1703.01365" rel="noopener ugc nofollow" target="_blank">综合梯度</a>、<a class="ae ky" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">显著图</a>、<a class="ae ky" href="https://arxiv.org/abs/1704.03296v3" rel="noopener ugc nofollow" target="_blank">迭代梯度</a>。)这个想法是，知道输入的哪一部分在小扰动下剧烈地改变输出，告诉我们什么输入对当前输出负责。</p><p id="5454" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从广义上来说，输入梯度告诉我们网络<em class="lv">关注的是</em>。因此，调整输入梯度给了我们改变模型看世界方式的能力。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="6474" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">引导网络注意力</h1><p id="f143" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">限制输入梯度的最自然的方法是告诉模型哪些输入区域是重要的，哪些是要忽略的。<a class="ae ky" href="https://arxiv.org/abs/1703.03717" rel="noopener ugc nofollow" target="_blank"> Ross 等人</a>实现了一个简单的惩罚，阻止神经网络查看输入的某些部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/c46f0751fe88f1c7be9eb10e0db55ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAT8KQa4gTMiwoLuZBfBcA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">要添加到损失中的惩罚条款。a 是要惩罚的掩码。它适用于损耗 wrt 的梯度。<em class="nb"> x </em>。存在 D 维的 N 个数据点，导致 K 维输出<em class="nb"> y </em>。</p></figure><p id="5475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图像处理应用中，通过训练网络仅关注前景对象并在背景元素上具有零梯度，可以潜在地避免噪声或虚假相关。</p><p id="eb2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果成功应用，这种正则化可能会提高泛化能力(忽略虚假的相关性)并加快学习速度(仅使用输入的重要部分)。</p><h1 id="c3a7" class="md me it bd mf mg nc mi mj mk nd mm mn jz ne ka mp kc nf kd mr kf ng kg mt mu bi translated">对抗鲁棒性</h1><p id="3a31" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">另一个有趣的应用是使用输入正则化实现对抗鲁棒性。<a class="ae ky" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337/15866" rel="noopener ugc nofollow" target="_blank">已经证明</a>输入梯度的 L2 正则化可以避免输入上的大梯度——这正是使网络容易受到敌对攻击的原因。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/eb3a34a18720dc0ab670a8dd8be21a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5o5BM6II1JPTYcsfDEpKnA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这个正则化项与前一个相似，但没有应用掩膜，交叉熵损失项的表达方式也不同。</p></figure><p id="8f3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们可以认为这个正则化器改变了网络对输入的感知，使得它必须稍微注意许多特征，而不是大量依赖少数特征。从这个意义上说，这种正规化与辍学有着相似的动机。</p><p id="1363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，为输入梯度正则化网络创建的对抗性攻击可以成功地转移到在相同数据集上训练的其他网络。这使得这个简单的正则化器不仅是一个防御工具，还可能在不知道目标网络的情况下制造攻击。</p><h1 id="93ec" class="md me it bd mf mg nc mi mj mk nd mm mn jz ne ka mp kc nf kd mr kf ng kg mt mu bi translated">输入稀疏度</h1><p id="90d2" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在查看了 L2 正则化在输入梯度上的用途之后，探索 L1 正则化的效用是很自然的。幸运的是，<a class="ae ky" href="https://asross.github.io/publications/RossLageDoshiVelez2017.pdf" rel="noopener ugc nofollow" target="_blank">这已经为我们完成了</a>。事实上，使用 L1 规范改变了网络对数据的感知，使得它在任何时候都只认为很少的输入是相关的。</p><p id="7941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当获取特征的成本很高，并且人们更喜欢仅使用少量特征的模型时，这可能是有用的。或许，它也可以用作分析特征相关性的分析工具，类似于传统的神经注意机制。</p><p id="4797" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则化方法的优点是简单。您可以轻松地将输入梯度正则化应用于任何网络(但需要一些计算成本)。</p><p id="6175" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你学到了有用的东西！</p></div></div>    
</body>
</html>