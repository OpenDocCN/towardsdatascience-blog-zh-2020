<html>
<head>
<title>Deploying BERT using Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Kubernetes部署BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-bert-using-kubernetes-6ddca23caec5?source=collection_archive---------24-----------------------#2020-01-27">https://towardsdatascience.com/deploying-bert-using-kubernetes-6ddca23caec5?source=collection_archive---------24-----------------------#2020-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8e80" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌云平台，Docker，微服务，NLP</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/946af1c97cb12a62983f52aefd34cf56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QyPsIZzxdqkfDpsl.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伯特:<a class="ae ky" href="https://www.propellermediaworks.com/blog/seo-bert-google-update/" rel="noopener ugc nofollow" target="_blank">形象信用</a></p></figure><p id="037c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">T3】简介T5】</strong></p><p id="f236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT是Google广泛使用的NLP模型。2019年被认为是BERT年，取代了大多数生产系统。BERT使用基于自我关注的方法，该模型本身由变压器块组成，变压器块是一系列自我关注层。与基于RNN的序列对序列模型相比，BERT证明了自我注意的使用导致了更优越的性能，特别是在Stanford问答数据集(SQUAD)上。我们已经跳过了所有的理论细节，如RNN，LSTM模型，自我关注，因为它们在吴恩达课程和<a class="ae ky" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">博客文章</a>s here<a class="ae ky" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">here</a>(作者Jay Alammar)的课程中会讲到。另一个很棒的<a class="ae ky" rel="noopener" target="_blank" href="/2019-year-of-bert-and-transformer-f200b53d05b9">帖子</a>提供了基于BERT的主要模型。请注意，注意力机制最初被应用于改善基于RNN的序列对序列模型的性能。Lilian Weng 也发表了一篇关于注意力机制的概述，她在其中解释了transformer架构。特别是键、值和查询概念，其中键和值相当于编码器输出，即存储器，而查询是解码器输出，然而，作为原始论文，它们是位抽象的，并且作用于所有字。ukasz Kaiser 解释了为什么它们更有效和高效的原因</p><p id="18e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从实现的角度来看，BERT是一个拥有1.1亿或更多参数的大型模型，从头开始训练非常具有挑战性，正如Google的博客中所强调的。它需要有效的计算能力，模型结构必须仔细编程以避免错误。因此，为了使BERT模型易于使用，谷歌开源了它的<a class="ae ky" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">实现</a>，它可以在大约30分钟内微调到各种任务。一个例子是情绪分析任务，正如这里的<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="noopener ugc nofollow" target="_blank">所解释的</a>。限制参数数量的研究工作正在进行中。然而，微调BERT也是广泛使用的方法。</p><p id="2bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">目标</em> </strong></p><p id="1203" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的目标是部署一个经过训练的BERT模型，作为一个微服务，对流媒体tweets进行情感分析。这个简单的例子揭示了为ML开发微服务的关键挑战。目标是涵盖强调训练和部署ML模型的高级工具需求的关键概念。这篇文章中涉及的主要任务是</p><ol class=""><li id="d9f8" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">流数据(tweets)的数据预处理:就ETL而言，真实世界的系统要复杂得多</li><li id="fd00" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">使用经过训练(微调)的BERT模型进行推理</li></ol><p id="cffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练模型需要额外的考虑，以供审查和保留。这些将在另一篇文章中讨论。</p><p id="bce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的架构如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/a0d12612fdbd929fbf8f3ddf404e2016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKm463y60_C2-3WfgFVitw.png"/></div></div></figure><p id="389c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">采用的技术有(通过相关的python API)</p><ol class=""><li id="81dc" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">伯特，张量流，张量流服务，雷迪斯，</li><li id="08ec" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">容器:Docker容器和用于编排的kubernetes</li></ol><p id="0204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">Docker和kubernetes的背景信息</em></p><p id="8506" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将开发微服务，使用容器部署它们，并使用K8S来管理这些容器。简而言之，容器是将代码和依赖项打包在一起独立运行的抽象。如果你不熟悉容器，请查看docker <a class="ae ky" href="https://www.docker.com/resources/what-container" rel="noopener ugc nofollow" target="_blank">网站</a>获取更多信息和教程。Kubernetes (K8S)由Google开发，用于管理多个容器，它可以在本地计算集群或云上运行。所有主要的云提供商都支持Kubernetes。这篇文章需要的主要Kubernetes概念如下</p><ol class=""><li id="d53d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">基本kubernetes组件和it <a class="ae ky" href="https://kubernetes.io/docs/concepts/" rel="noopener ugc nofollow" target="_blank">架构</a>。视频<a class="ae ky" href="https://www.youtube.com/watch?v=8C_SCDbUJTg" rel="noopener ugc nofollow" target="_blank">这里</a></li><li id="7a40" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">pod和部署</li><li id="2c11" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">持久性卷和持久性卷声明</li><li id="ea78" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">Kubenetes服务公司</li><li id="e382" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://kubernetes.io/docs/reference/kubectl/overview/" rel="noopener ugc nofollow" target="_blank"> Kubectl </a>:用来创建资源的命令。我们通过yaml <a class="ae ky" href="https://www.youtube.com/watch?v=o9pT9cWzbnI" rel="noopener ugc nofollow" target="_blank">文件</a>创建各种资源</li></ol><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="6b91" class="mq mr it mm b gy ms mt l mu mv">#Create deployment/pods<br/>kubectl create -f tensorflowServing.yaml </span><span id="1ec2" class="mq mr it mm b gy mw mt l mu mv">#Delete deployment/pods<br/>kubectl delete -f tensorflowServing.yaml</span><span id="cfe9" class="mq mr it mm b gy mw mt l mu mv">#Check pods and service<br/>kubectl get pods<br/>kubectl get svc</span><span id="ace7" class="mq mr it mm b gy mw mt l mu mv">#Check persistance volume and persistance volume cliam<br/>kubectl get pv<br/>kubectl get pvc</span></pre><p id="6bf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<a class="ae ky" href="https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16" rel="noopener">和这里的</a><a class="ae ky" href="https://medium.com/google-cloud/kubernetes-110-your-first-deployment-bf123c1d3f8?" rel="noopener">提供了对这些概念的快速概述，而youtube的</a><a class="ae ky" href="https://www.youtube.com/playlist?list=PLF3s2WICJlqOiymMaTLjwwHz-MSVbtJPQ" rel="noopener ugc nofollow" target="_blank">播放列表</a>提供了对这些概念的更详细的阐述。微软也开发了好的资源来开始K8S。谷歌提供的资源是广泛的，最初可能有点令人不知所措。一旦你对K8S的概念有了概念，并尝试“你好世界”这里。它涵盖了流程，我们需要遵循相同的流程，几次！！！</p><p id="e77a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文档中提到的过程如下</p><ol class=""><li id="e1c2" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">将您的应用程序打包成Docker映像</li><li id="e95c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">在您的机器上本地运行容器(可选)</li><li id="a80b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">将图像上传到注册表</li><li id="5e8f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">创建一个容器集群</li><li id="72c6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">将您的应用部署到集群</li><li id="343f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">通过服务向集群外部公开您的应用</li><li id="784a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">扩展您的部署</li></ol><p id="c7df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">回到我们的微服务</strong></p><ol class=""><li id="3b09" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">我们的第一个微服务使用tensorflow-serving和经过训练的保存模型运行BERT，允许我们使用客户端执行推理</li><li id="a591" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">第二个微服务使用<a class="ae ky" href="https://grpc.io/docs/tutorials/basic/python/" rel="noopener ugc nofollow" target="_blank"> gRPC </a>客户端通过“请求”BERT来执行情感分析，并将其发布到Redis发布/订阅频道</li></ol><p id="3183" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你正在使用GCP，我们假设你已经在你的机器上安装了<em class="lv"> gcloud，kubectl </em>并连接到名为“kubernetes”的项目，更多信息请参见<a class="ae ky" href="https://cloud.google.com/deployment-manager/docs/step-by-step-guide/installation-and-setup" rel="noopener ugc nofollow" target="_blank">这里</a>。对于其他设置，我们假设存在等效性。</p><p id="5aae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先通过命令行创建并连接到GCP的kubernetes集群</p><p id="e647" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">创建</em> </strong></p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="1adf" class="mq mr it mm b gy ms mt l mu mv"><strong class="mm iu">gcloud</strong> beta container — project “kubernetes” clusters create “standard-cluster-1” — zone “us-central1-a” — no-enable-basic-auth — cluster-version “1.13.11-gke.14” — machine-type “n1-standard-1” — image-type “COS” — disk-type “pd-standard” — disk-size “100” — scopes “https://www.googleapis.com/auth/devstorage.read_only","https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/servicecontrol","https://www.googleapis.com/auth/service.management.readonly","https://www.googleapis.com/auth/trace.append" — num-nodes “3” — enable-cloud-logging — enable-cloud-monitoring — enable-ip-alias — network “projects/kubernetes/global/networks/default” — subnetwork “projects/kubernetes/regions/us-central1/subnetworks/default” — default-max-pods-per-node “110” — addons HorizontalPodAutoscaling,HttpLoadBalancing — enable-autoupgrade — enable-autorepair</span></pre><p id="e781" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">连接</em> </strong></p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="d000" class="mq mr it mm b gy ms mt l mu mv"><strong class="mm iu">gcloud </strong>container clusters get-credentials standard-cluster-1 — zone us-central1-a — project kubernetes</span></pre><p id="b8db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于数据存储，我们使用谷歌云<a class="ae ky" href="https://cloud.google.com/sdk/gcloud/reference/compute/disks/create" rel="noopener ugc nofollow" target="_blank">磁盘</a>，模型是存储在一个单独的位置</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="4bd9" class="mq mr it mm b gy ms mt l mu mv"><strong class="mm iu">gcloud</strong> beta compute disks create redis-disk-gce — project=kubernetes — type=pd-standard — size=100GB — zone=us-central1-a — physical-block-size=4096</span></pre><p id="f9e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"><em class="lv"/></strong></p><p id="991e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这个设置中使用redis在微服务之间进行通信。Redis被用作发布/订阅消息传递。有必要在集群上运行健壮的redis微服务。我们可能面临的一个问题是万一redis崩溃。Kubernetes能够检测崩溃并创建一个新的容器，即redis实例来维护服务。然而，当重新创建容器时，新容器可能具有不同的状态。因此，为了避免这种情况，我们使用了持久性磁盘(一个专用的存储磁盘)，它通过使用持久性卷声明将这个存储链接到我们的Redis容器。这是一个非常简单的设置，在企业级别，要实现扩展，我们需要使用有状态集合。关于它的详细讨论可以在<a class="ae ky" href="https://www.youtube.com/watch?v=FserPvxKvTA&amp;t=3050s" rel="noopener ugc nofollow" target="_blank">这里</a>找到，而示例redis配置可以在<a class="ae ky" href="https://estl.tech/deploying-redis-with-persistence-on-google-kubernetes-engine-c1d60f70a043" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="16ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的设置中，有一个redis实例，我们使用google云盘通过yaml文件创建一个持久性卷和持久性卷声明。我们通过redis容器中的持久性卷声明的方式在make use storage中使用这些，如ymal文件中所概述的。我们还通过kubernetes服务向外部世界公开redis。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="a3b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">服务张量流模型</strong></p><p id="b132" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用tensorflow服务API，在kubernetes中服务tensorflow模型非常简单。这些步骤如下</p><ol class=""><li id="1389" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">从张量流图中收集输入(“sinputs”)和输出(“soutput”)变量的列表</li><li id="396c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">在保存模型之前创建预测签名</li><li id="379c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">使用保存模型<a class="ae ky" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank">格式</a>保存模型</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="9910" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将训练好的模型复制到tensorflow服务docker映像，使用我们的模型创建新的docker映像，并上传到容器注册表，使用此处列出的<a class="ae ky" href="https://www.tensorflow.org/tfx/serving/serving_kubernetes" rel="noopener ugc nofollow" target="_blank">流程。我们的docker映像是gcr.io/kubernetes-258108/bert.的<strong class="lb iu"/>我们通过kubenetes部署来部署这个映像。我们还创建了kubernetes服务。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="ce4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:使用通过“kubectl”获得的服务IP地址，让我们的集群之外的客户端发出请求。为了避免使用IP地址，需要使用kube-dns发现服务，这是有据可查的，这里有一个python示例<a class="ae ky" href="https://stackoverflow.com/questions/51447374/kubernetes-automatically-determine-ip-address-of-another-pod-container?answertab=active#tab-top" rel="noopener ugc nofollow" target="_blank"/>。因为我们已经为bert推断客户端配置了服务，并且可以使用服务名“bertservice:8500 ”,所以8500是端口，因为我们是在集群中发出请求的。</p><p id="a7d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在客户端发出请求的关键代码组件如下，它们基于google client <a class="ae ky" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/resnet_client_grpc.py" rel="noopener ugc nofollow" target="_blank">示例</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="600a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">推理客户端可用的完整示例如下。它由<a class="ae ky" href="https://tweepy.readthedocs.io/en/latest/streaming_how_to.html#step-1-creating-a-streamlistener" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Tweepy </strong> </a> <strong class="lb iu">流监听器</strong>组成，用于订阅tweets主题和redis sub/pub消息服务</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="9918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv"> Bert推理客户端使用gRPC请求客户端</em> </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="46c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用一个<a class="ae ky" href="https://docs.docker.com/get-started/part2/" rel="noopener ugc nofollow" target="_blank">docker文件</a>将这个客户端打包在docker容器中，并部署在我们的kubernetes集群上</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">要为python容器创建的docker文件</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="9203" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以通过连接到redis pods或使用python订阅者，通过kubectl来观察带有情感的流推文</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="cd39" class="mq mr it mm b gy ms mt l mu mv">kubectl exec -it redis-pod-name redis-cli<br/>127.0.0.1:6379&gt; SUBSCRIBE STREAM</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="171b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:我们不使用GPU进行推理，因此标准N1计算资源上的gRPC请求需要大约12秒(对于20个批处理大小)</p><p id="b5e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">关闭思绪</em> </strong></p><p id="2657" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能够在GCP集群上部署此设置3天，没有发生任何事件。这展示了ML的容器和简单应用程序的威力，并且可以很快部署。然而，在现实世界中，情况完全不同。例如，ETL可能是一项非常复杂的任务，可能需要rabbit-MQ、Kafka或Apache beam等高级技术来集成来自其他来源的数据。我们可能需要重新培训和重新部署ML模型。这些在另一篇博文中有所涉及。结合起来，这些应该提供开发微服务的介绍，并强调对其他技术的需求，如Kubeflow、Apache Airflow、Seldon、helm charts，用于使用kubernetes基础设施开发ML应用程序。我们还没有讨论如何使用Kubernetes扩展我们的微服务。一种方法是利用复制集，但根据我们需要的任务和规模，可能会有一些考虑。例如，OpenAI在最近的博客<a class="ae ky" href="https://openai.com/blog/scaling-kubernetes-to-2500-nodes/" rel="noopener ugc nofollow" target="_blank">帖子</a>中记录了将kubernetes扩展到2500个节点的挑战。</p><p id="f620" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">git回购:这项工作可以在<a class="ae ky" href="https://github.com/JP-MRPhys/BERT-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></div></div>    
</body>
</html>