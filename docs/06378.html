<html>
<head>
<title>Python and world literature: elementary web scraping with BeautifulSoup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 与世界文学:使用 BeautifulSoup 的初级网络抓取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/python-and-world-literature-elementary-web-scraping-with-beautifulsoup-a43daaf46ab?source=collection_archive---------53-----------------------#2020-05-21">https://towardsdatascience.com/python-and-world-literature-elementary-web-scraping-with-beautifulsoup-a43daaf46ab?source=collection_archive---------53-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1b7d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学提示</h2><div class=""/><div class=""><h2 id="0ffe" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数据挖掘课程 101</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c3bd587617988cf23995f5f20ff96f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QQ3WoZ2IxkvzSP_B"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lh" href="https://unsplash.com/@daiga_ellaby?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Daiga Ellaby </a>拍摄的照片</p></figure><p id="edda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每一位新的数据科学家都希望创建壮观的可视化效果，构建渐进的预测模型，从数据中获得惊人的洞察力。好吧，这些东西很吸引人，也很漂亮。但有时人们会在每次分析过程开始时忘记很多“脏”工作。而且这还不是数据清理阶段。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c2e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在你开始处理数据之前，你需要…数据！</p><p id="4ca6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是的，认为数据分析从数据集清理开始是一个常见的错误，因为它指的是您已经拥有数据的事实。有人可能会相信你总是会神奇地从空气中获得数据。但是数据搜集、收集、挖掘和收集是你需要不断提高的技能。您必须能够处理不同的源代码、格式，有时甚至是语言，而这个事实假设您有足够的技能。基本技能之一是网络抓取——从网站和网络应用程序中加载数据。</p><p id="6efc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当然，蜘蛛和爬虫的创作有很多技巧和方法。今天我想从最原始但非常有效的工具开始— <a class="ae lh" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#" rel="noopener ugc nofollow" target="_blank"> BeautifulSoap parser </a>。这是一个非常强大的 HTML 解析器，结合适当的请求逻辑，可以自动完成数据收集过程。所以，不再多言——让我们开始编码吧。</p><h2 id="1044" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">准备会话</h2><p id="e31f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">我将重复文章的第一个想法:使用您需要的数据…数据！所以，我们需要加载一些内容。我想创建自己的数据集用于进一步分析，它是献给诺贝尔奖的。起点是获奖者的名单，我选择了文学类别来工作。我想保持简单，所以我将使用<a class="ae lh" href="https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Literature" rel="noopener ugc nofollow" target="_blank">适当的维基百科页面</a>。是的，它并不完美，但仍然是一个很好的起点，因为它有很多交叉链接，我将使用这些链接来获得额外的信息。</p><p id="1be8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是首先我们应该为请求准备引擎。我创建了一个简单的会话(因为我想保持与同一台主机的稳定连接)和一个简单的适配器(用于超时或网络切换的情况)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="b955" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以在<a class="ae lh" href="https://requests.readthedocs.io/en/master/user/advanced/" rel="noopener ugc nofollow" target="_blank">请求库页面</a>上了解其他设置。这很简单，让我们继续。</p><h2 id="eae2" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">获取内容</h2><p id="8c4a" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这里仍然没有什么有趣的东西:只需加载页面并创建适当的<code class="fe nk nl nm nn b">BeautifulSoup parser</code>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="f561" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们为下一步做准备，这包括一些探索。</p><h2 id="577e" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">找到信息</h2><p id="5a52" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这听起来可能很奇怪，但是在我们强制脚本自动搜索信息之前，我们应该手动找到它。而且不，这和文章的目的并不矛盾！我们必须研究数据，探索其内部结构，并将这些知识应用于 scrapper 的创建。换句话说，我们应该为 scrapper 找到一些定向器来解析想要的 HTML-tag。我们可以在页面上看到一个包含数据的大表，但是我们需要挖掘 HTML 代码来查看该表的实际位置。我们不能引用位置，因为更多的表可能会被插入到所需的表之前。出于同样的原因，我们不能从本章开始就查阅行数。但是我们可能依赖于章节名称和不同的标签属性。因此，让我们“找到”带有“桂冠诗人”姓名和<code class="fe nk nl nm nn b">id</code>属性的标签，并在它下面找到所需的表。每个 BeautifulSoup 标签都可以使用<code class="fe nk nl nm nn b">find_all()</code>方法来遍历其内容(所有内部标签):</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="4818" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是了。根据我们知道的标准维基页面结构，它是标题的子标签。幸运的是，每个被解析的标签都有一个指向<code class="fe nk nl nm nn b">parent</code>标签的指针:</p><pre class="ks kt ku kv gt no nn np nq aw nr bi"><span id="444b" class="ml mm it nn b gy ns nt l nu nv">tag = laureates_tag.parent</span></pre><p id="dedd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用该信息来查找所需的表。每个 BeautifulSoup 标签不仅有<code class="fe nk nl nm nn b">parent</code>指针，还有<code class="fe nk nl nm nn b">next_sibling</code>和<code class="fe nk nl nm nn b">previous_sibling</code>指针。所以，我们可以走遍“邻居”标签。正如我们在页面代码中看到的，我们的表格就在 found 标题下面。</p><pre class="ks kt ku kv gt no nn np nq aw nr bi"><span id="c5fa" class="ml mm it nn b gy ns nt l nu nv">while tag.name != ‘table’:<br/>    tag = tag.next_sibling<br/>tag.name # &lt;table&gt;</span></pre><p id="cf9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们准备好解析了！</p><h2 id="9005" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">解析数据</h2><p id="676d" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这是棘手的部分。我们看到，该表有许多行，有些行的所有单元格都已填充，但有些行有常见的字符串“<em class="nw">Not award”</em>。但这还不是最困难的部分:有些年份有两位获奖者。但是我们配备了一些美丽的乐器:</p><ul class=""><li id="0bb1" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">访问内部标签:<code class="fe nk nl nm nn b">tag.internal_tag</code>；</li><li id="b969" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">检查标签是否有属性:<code class="fe nk nl nm nn b">tag.has_attr(‘attr’)</code>；</li><li id="1210" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">访问属性:<code class="fe nk nl nm nn b">tag[‘attr’]</code>；</li><li id="55e7" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">从标签中获取文本:<code class="fe nk nl nm nn b">tag.text</code>；</li></ul><p id="70c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有很多其他的，你可以在文档中找到<a class="ae lh" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1e5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">主要思想是遍历每一行，用<code class="fe nk nl nm nn b">&lt;tr&gt;</code>标记表示，并获取所有单元格，用<code class="fe nk nl nm nn b">&lt;td&gt;</code>标记表示。由于表可能非常大，我们可能不想一次复制和遍历它的所有行。不过，BeautifulSoup 为我们提供了所选标签的所有内部标签的<code class="fe nk nl nm nn b">tag.children</code>访问器生成器:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="caf5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在我的 GitHub 上的完整代码<a class="ae lh" href="https://github.com/Midvel/medium_jupyter_notes/blob/master/bs_scrapping/bs-scrapping.ipynb" rel="noopener ugc nofollow" target="_blank">中查看解析函数本身。其实是纯解析逻辑，应该包括一些异常情况。所以，经过一些尝试，我们收集了数据。但对我们来说还不够！</a></p><h2 id="0917" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">附加数据</h2><p id="901a" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">除了我们需要的数据，我们还取消了每个作者个人页面的链接。我们将使用这些信息来构建刮板的下一部分。实际上，这是爬行器的最基本层次，因为我们将重用废弃的链接到新页面。</p><p id="639d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们有很多行，当然，我们不会检查每一行的每一页。因此，我们将依赖于这样的假设，即每个作者的页面都有传记元素。该元素是我们的目标，因为它包含大量附加数据:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="96ad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">再来一个循环，再来一个解析函数:数据集就准备好了！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/5464255797d1f1e078f52668244fb4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5o2A9w752LK8ElYLNJX1fw.png"/></div></div></figure><p id="27d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，这只是长期数据分析过程的第一步。数据是原始的，需要大量清理。但是，这是一个非常不同的故事。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="6406" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一如既往(老读者已经知道了，新读者——你的乐趣)，这里是我的 GitHub 工作示例的链接。</p><div class="om on gp gr oo op"><a href="https://github.com/Midvel/medium_jupyter_notes/blob/master/bs_scrapping/bs-scrapping.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">中级/中等 _jupyter_notes</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">github.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd lb op"/></div></div></a></div><p id="0d28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，由于下一个合乎逻辑的步骤是数据清理，请确保您没有错过我关于丢失数据处理的文章:</p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/7-idioms-to-acquire-missing-values-every-data-scientist-should-know-2edf4224360c"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">每个数据科学家都应该知道的获取缺失值的 7 个习惯用法</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">你应该把命令自动化</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pe l pa pb pc oy pd lb op"/></div></div></a></div><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/handle-missing-data-with-r-10-daily-used-idioms-13d849d01690"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">用 R: 10 日常习惯用法处理缺失数据</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">你应该把命令自动化</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pf l pa pb pc oy pd lb op"/></div></div></a></div></div></div>    
</body>
</html>