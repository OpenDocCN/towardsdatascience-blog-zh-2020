<html>
<head>
<title>LSTMs In PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 的 LSTMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstms-in-pytorch-528b0440244?source=collection_archive---------4-----------------------#2020-07-30">https://towardsdatascience.com/lstms-in-pytorch-528b0440244?source=collection_archive---------4-----------------------#2020-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="88ea" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">简单的解释</h2><div class=""/><div class=""><h2 id="5b0f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解 LSTM 架构和数据流</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c94d913c5e35eb1fdb45aff30a7f9638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kjof2hyp00Xb0yPG"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">大象永远不会忘记。LSTMs 呢？</p></figure><p id="b131" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> <em class="md">让我猜猜… </em> </strong> <em class="md">你已经和 MLPs 和 CNN 完成了几个小项目，对吗？MLPs 让你开始理解梯度下降和激活函数。CNN 让你看到了计算机视觉的世界。它们的所有层次都有点难，但你最终会找到窍门的。</em></p><p id="559c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，你碰壁了。你在用头撞它。那座血淋淋、泪痕斑斑的建筑叫做伊斯特姆斯。</p><p id="a275" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我感受到了你的痛苦，朋友。我是来帮忙的。</p><p id="f746" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">让我们从概念上熟悉 LSTMs，然后深入研究您的问题的具体难点:即定义网络架构，以及在数据流经网络的每一层时控制数据的形状。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="ae35" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">RNNs 背后的理念</h1><p id="89a7" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">递归神经网络通常保持关于先前通过网络传递的数据的状态信息。普通 rnn 和 LSTMs 都是如此。这种所谓的“隐藏状态”随着数据点序列的每个新元素被传回网络。因此，网络的每个输出不仅是输入变量的函数，也是隐藏状态的函数，隐藏状态是网络过去所见的“记忆”。</p><h1 id="9655" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">普通 rnn 失败的地方</h1><p id="bfa1" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这有助于理解 LSTMs 填补传统 rnn 能力的空白。香草 RNN 遭受快速<strong class="lj jd">渐变消失</strong>或<strong class="lj jd">渐变爆炸</strong>。粗略地说，当链式法则应用于支配网络内“记忆”的方程时，会产生一个指数项。如果满足某些条件，该指数项可能会变得很大或很快消失。</p><p id="2246" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">LSTMs 不会(严重地)遭受消失梯度的问题，因此能够保持更长的“记忆”，使它们成为学习时态数据的理想选择。</p><h1 id="f58d" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">PyTorch 中 LSTMs 的难点</h1><p id="331d" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">现在，你可能已经知道了 LSTMs 背后的故事。你在这里是因为你很难将你的概念性知识转化为工作代码。</p><p id="8901" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">快速搜索<a class="ae nn" href="https://discuss.pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch 用户论坛</a>会产生几十个问题，关于如何定义 LSTM 的架构，如何在数据从一层移动到另一层时塑造数据，以及当数据从另一端出来时如何处理。这些问题中有许多没有答案，而更多的问题的答案对于提问的初学者来说是难以理解的。</p><p id="9438" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以说，理解通过 LSTM 的数据流是我在实践中遇到的头号难题。似乎我并不孤单。</p><h1 id="1c2b" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">理解数据流:LSTM 层</h1><p id="1311" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在学习其他类型的网络之后，学习 LSTMs 时最难理解的一个概念可能是数据如何在模型的各层中流动。</p><p id="f7d1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这不是魔法，但看起来可能是。图片可能有所帮助:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/5102f0fbe196c79575ec3e7139900f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRRY7SAGhar79MyehSwyqg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者配图。</p></figure><ol class=""><li id="d185" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">一个 LSTM 层由一组<strong class="lj jd"> <em class="md"> M </em> </strong>隐藏节点组成。该值<strong class="lj jd"> <em class="md"> M </em> </strong>由用户在模型对象实例化时分配。很像传统的神经网络，虽然有指导方针，但这是一个有点武断的选择。</li><li id="c899" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">当长度为<strong class="lj jd"><em class="md"/></strong>的单个序列<strong class="lj jd"> <em class="md"> S </em> </strong>被传递到网络中时，序列<strong class="lj jd"> 𝑆 </strong>的每个单独元素<strong class="lj jd"> <em class="md"> s_i </em> </strong>通过每个隐藏节点。</li><li id="f073" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">每个隐藏节点为它看到的每个输入提供一个输出。这导致了形状<strong class="lj jd"> ( <em class="md"> N </em>，<em class="md"> M </em> ) </strong>的隐藏层的整体输出</li><li id="8e68" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">如果将小批量的<strong class="lj jd"> 𝐵 </strong>序列输入到网络中，则会增加一个额外的维度，从而产生形状为<strong class="lj jd"> (𝐵、<em class="md"> N </em>、<em class="md"> M </em> ) </strong>的输出</li></ol><h1 id="f069" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">了解数据流:全连接层</h1><p id="3197" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在 LSTM 层(或一组 LSTM 层)之后，我们通常会通过<code class="fe od oe of og b">nn.Linear()</code>类向网络添加一个完全连接的层，用于最终输出。</p><ol class=""><li id="a1f6" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">最后一个<code class="fe od oe of og b">nn.Linear()</code>层的输入大小将总是等于它之前的 LSTM 层中隐藏节点的数量。</li><li id="73ca" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">这个最终完全连接层的输出将取决于目标的形式和/或您使用的损失函数。</li></ol><h1 id="efcd" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">了解数据流:示例</h1><p id="e30d" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们将讨论两个定义网络架构和通过网络传递输入的示例:</p><ol class=""><li id="f45e" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">回归</li><li id="32fb" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">分类</li></ol><h1 id="f1a4" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">示例 1a:回归网络架构</h1><p id="72c0" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">考虑一些时间序列数据，也许是股票价格。给定特定产品过去 7 天的股票价格，我们希望预测第 8 天的价格。在这种情况下，我们希望我们的输出是单个值。我们将使用 MSE 来评估这个单值的准确性，因此对于预测和性能评估，我们需要来自七天输入的单值输出。因此，我们将我们的网络架构定义为:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="f64f" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">示例 1b:对层间数据进行整形<a class="ae nn" href="http://localhost:8888/lab#Example-1b:-Shaping-Data-Between-Layers" rel="noopener ugc nofollow" target="_blank"> </a></h1><p id="2be0" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我告诉你一个小秘密，我的一个朋友曾经告诉过我:</p><blockquote class="oj ok ol"><p id="079e" class="lh li md lj b lk ll kd lm ln lo kg lp om lr ls lt on lv lw lx oo lz ma mb mc im bi translated">这些天来，我对它有了一种理解<em class="it">【LSTM 数据流】</em>如果我在做的时候看向别处，它就会起作用。</p><p id="c1bb" class="lh li md lj b lk ll kd lm ln lo kg lp om lr ls lt on lv lw lx oo lz ma mb mc im bi translated"><em class="it"> —亚历克</em></p></blockquote><p id="e5f8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然他说的在某种意义上是对的，但我认为我们可以确定这台机器如何工作的一些细节。</p><ol class=""><li id="ca40" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated"><strong class="lj jd">LSTM 层的输入</strong>必须是<code class="fe od oe of og b">(batch_size, sequence_length, number_features)</code>的形状，其中<code class="fe od oe of og b">batch_size</code>是指每批序列的数量，<code class="fe od oe of og b">number_features</code>是时间序列中变量的数量。</li><li id="a80d" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated"><strong class="lj jd">你的 LSTM 层的输出</strong>将会是<code class="fe od oe of og b">(batch_size, sequence_length, hidden_size)</code>的形状。再看一下我在上面创建的流程图。</li><li id="8fba" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated"><strong class="lj jd">我们的全连接<code class="fe od oe of og b">nn.Linear()</code>层的输入</strong>需要对应于前一 LSTM 层中隐藏节点数量的输入大小。因此，我们必须将我们的数据重塑为<code class="fe od oe of og b">(batches, n_hidden)</code>形式。</li></ol><p id="c04e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">重要提示:</strong> <code class="fe od oe of og b">batches</code>与<code class="fe od oe of og b">batch_size</code>不同，因为它们不是同一个数字。然而，想法是相同的，因为我们将 LSTM 层的输出分成<code class="fe od oe of og b">batches</code>个片段，其中每个片段的大小为<code class="fe od oe of og b">n_hidden</code>，即隐藏的 LSTM 节点的数量。</p><p id="1613" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是一些模拟通过整个网络传递输入数据<code class="fe od oe of og b">x</code>的代码，遵循上面的协议:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="1828" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回想一下<code class="fe od oe of og b">out_size = 1</code>,因为我们只希望知道一个值，这个值将使用 MSE 作为度量进行评估。</p><h1 id="ebbb" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">示例 2a:分类网络架构</h1><p id="b75d" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在这个例子中，我们想要生成一些文本。一个模型在一大堆文本上被训练，也许是一本书，然后输入一系列字符。该模型将查看每个字符，并预测下一个字符应该出现。这一次我们的问题是分类而不是回归，我们必须相应地改变我们的架构。我制作了这张图表来勾画总体思路:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/ce73dc939f93faaa1b561df1feda9f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXloo2VFHuARiskUttV2Ag.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者配图。</p></figure><p id="a213" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">也许我们的模型训练的是由 50 个独特字符组成的数百万字的文本。这意味着当我们的网络得到一个字符时，我们希望知道接下来是 50 个字符中的哪一个。因此，对于单个字符，我们的网络输出将是 50 个概率，对应于 50 个可能的下一个字符中的每一个。</p><p id="30fb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，我们将对文本字符串中的每个字符进行一次性编码，这意味着变量(<code class="fe od oe of og b">input_size = 50</code>)的数量不再像以前那样是一个，而是一次性编码的字符向量的大小。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="27e0" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">示例 2b:对层间数据进行整形</h1><p id="b2ad" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">至于在各层之间形成数据，没有太大的区别。逻辑是相同的:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="aae1" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">示例 2c:培训挑战</h1><p id="4b5a" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">然而，这种情况提出了一个独特的挑战。因为我们正在处理分类预测，所以我们可能想要使用<strong class="lj jd">交叉熵损失</strong>来训练我们的模型。在这种情况下，<strong class="lj jd"> <em class="md">知道你的损失函数的要求是非常重要的。例如，看看 PyTorch 的<code class="fe od oe of og b">nn.CrossEntropyLoss()</code>输入要求(强调我的，因为老实说有些文档需要帮助):</em></strong></p><blockquote class="oj ok ol"><p id="038b" class="lh li md lj b lk ll kd lm ln lo kg lp om lr ls lt on lv lw lx oo lz ma mb mc im bi translated"><strong class="lj jd">输入</strong> <em class="it">应该包含每个类的原始的、未标准化的分数。</em> <strong class="lj jd">输入</strong> <em class="it">必须是一个大小为(minibatch，C)… </em>的张量</p><p id="be17" class="lh li md lj b lk ll kd lm ln lo kg lp om lr ls lt on lv lw lx oo lz ma mb mc im bi translated"><em class="it">这个准则</em> <strong class="lj jd"> <em class="it">【交叉熵损失】</em> </strong> <em class="it">期望一个类索引在[0，C-1]范围内作为</em> <strong class="lj jd">的目标</strong> <em class="it">为一个</em> <strong class="lj jd"> <em class="it">的 1D 张量</em> </strong> <em class="it">的大小 minibatch 的每一个值。</em></p></blockquote><p id="ae1a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好吧，无意冒犯 PyTorch，但那是狗屎。我不确定这是不是英语。我来翻译一下:</p><ol class=""><li id="2f5d" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">预测(上面称为<strong class="lj jd">输入</strong>，尽管有两个输入)应该是<strong class="lj jd">(迷你批处理，C) </strong>的形式，其中<strong class="lj jd"> C </strong>是可能的类的数量。在我们的例子<code class="fe od oe of og b"><strong class="lj jd">C = 50</strong></code>中。</li><li id="76c2" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">作为第二个输入的目标的大小应该是<strong class="lj jd"> (minibatch，1) </strong>。换句话说，目标<strong class="lj jd">不应该</strong>被一键编码。但是，它<strong class="lj jd">应该是</strong>标签编码的。</li></ol><p id="3d31" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这对你来说意味着你必须以两种不同的方式来塑造你的训练数据。输入<code class="fe od oe of og b">x</code>将被一键编码，但是你的目标<code class="fe od oe of og b">y</code>必须被标签编码。此外，<code class="fe od oe of og b">x</code>的独热列应按照<code class="fe od oe of og b">y</code>的标签编码进行索引。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="c82a" class="ml mm it bd mn mo ni mq mr ms nj mu mv ki nk kj mx kl nl km mz ko nm kp nb nc bi translated">离别的思绪</h1><p id="f478" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">LSTMs 的实现可能很复杂。通过了解您试图解决的问题的个人需求，然后相应地调整您的数据，可以消除这种复杂性。</p><p id="307b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面所有的代码都是未经测试的伪代码。如果你想看看上面两个例子的完整的 Jupyter 笔记本，请访问我的 GitHub:</p><ol class=""><li id="ffb9" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated"><a class="ae nn" href="https://github.com/wcneill/jn-ml-textbook/blob/master/Deep%20Learning/04%20Recurrent%20Networks/pytorch13b_LSTM.ipynb" rel="noopener ugc nofollow" target="_blank">回归示例</a></li><li id="0cb0" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated"><a class="ae nn" href="https://github.com/wcneill/jn-ml-textbook/blob/master/Deep%20Learning/04%20Recurrent%20Networks/pytorch13_char_RNN.ipynb" rel="noopener ugc nofollow" target="_blank">分类示例</a></li></ol><p id="34f6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望这篇文章有助于您理解通过 LSTM 的数据流！</p><p id="e9f8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">其他资源:</strong></p><ol class=""><li id="4bf3" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">拉兹万·帕斯卡努等<em class="md">论训练递归神经网络的难度，2013</em><a class="ae nn" href="https://arxiv.org/pdf/1211.5063.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1211.5063.pdf</a></li><li id="47dd" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">PyTorch 用户论坛:https://discuss.pytorch.org</li></ol><p id="958f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">照片致谢:</strong></p><ol class=""><li id="817b" class="np nq it lj b lk ll ln lo lq nr lu ns ly nt mc nu nv nw nx bi translated">大象照片由<a class="ae nn" href="https://unsplash.com/@huchenme?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">陈虎</a>在<a class="ae nn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</li><li id="39f2" class="np nq it lj b lk ny ln nz lq oa lu ob ly oc mc nu nv nw nx bi translated">所有图表均由作者提供</li></ol></div></div>    
</body>
</html>