<html>
<head>
<title>Cross-entropy for classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分类的交叉熵</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451?source=collection_archive---------0-----------------------#2020-05-22">https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451?source=collection_archive---------0-----------------------#2020-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d8dae1a9c8f04aed57c302336579a888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W9glFGnazR1YcGqH"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@lijiangang?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">李建刚</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><div class=""><h2 id="2085" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">二元、多类和多标签分类</h2></div><p id="74ad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">TL；博士在最后</strong></p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="b12f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">交叉熵</strong>是分类任务常用的损失函数。让我们看看为什么要使用它，在哪里使用它。我们将从一个典型的多类分类任务开始。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="f71e" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">多类分类</h2><p id="ee0e" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">图片上是哪一类——狗、猫还是熊猫？只能是其中之一。让我们想象一只狗。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/e361783bcc6cf55ee30681e0deb1b820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2FQ8uu8HvjBJkN_Ja9fHA.png"/></div></div></figure><p id="dd02" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测是一个<strong class="kx jh">概率向量</strong>，这意味着它表示所有类别的预测概率，总计为1。</p><p id="5ffa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在神经网络中，通常通过softmax函数激活最后一层来实现这种预测，但任何事情都可以，它只是必须是一个概率向量。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/1df89a4aa7e70634677ad46738ac1d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ht8z2rlwrHwFc_ErRdpzg.png"/></div></div></figure><p id="9eaf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们计算这张图片的交叉熵损失。</p><blockquote class="nc nd ne"><p id="224c" class="kv kw nf kx b ky kz kh la lb lc kk ld ng lf lg lh nh lj lk ll ni ln lo lp lq ij bi translated">损失是对模型性能的一种度量。越低越好。学习时，模型的目标是获得尽可能低的损失。</p></blockquote><p id="a58e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">目标表示所有类别的概率—狗、猫和熊猫。</p><blockquote class="nc nd ne"><p id="047a" class="kv kw nf kx b ky kz kh la lb lc kk ld ng lf lg lh nh lj lk ll ni ln lo lp lq ij bi translated">多类分类的目标是一个热点向量，这意味着它在单个位置上为1，在其他位置上为0。</p></blockquote><p id="bb8f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于dog类，我们希望概率为1。对于其他类，我们希望它是0。</p><p id="06f2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将开始分别计算每个类别的<strong class="kx jh">损失，然后将它们相加。每个单独类别的损耗计算如下:</strong></p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/bf5a74241b614c52995b1f3d2c25425f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28r0P7DRmcjZi6spYUauzw.png"/></div></div></figure><p id="611b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不要太担心公式，我们马上会谈到它。请注意，如果目标的职业概率是0，那么它的损失也是0。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/cf30096d7fe93b002bc394c84e9811e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDSUoE-G2OQXiPZat9uyGw.png"/></div></div></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/8418a4f46f8f99fdfec9da51c9c4bb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIkQjmY49OcJ8M3-7cx1mg.png"/></div></div></figure><p id="2245" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后——狗类的损失:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/146c25df09243d98811910b2692f830b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8J9F2kl1lBc6-0-QoUG_WQ.png"/></div></div></figure><p id="4bbb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那个数字是什么意思？</p><p id="0a9d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看，如果预测的概率不同，损失会如何表现:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7c73b7ed5ce05ebb563b0b8d1c2c2db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*lSIBf81FMNkkcC_QrlZ3iQ.png"/></div></figure><ul class=""><li id="b2d3" class="no np jg kx b ky kz lb lc le nq li nr lm ns lq nt nu nv nw bi translated">预测为1时损失为0(与目标相同)。</li><li id="8806" class="no np jg kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">如果预测值为0(与我们的目标完全相反)，损失就是无穷大。</li><li id="8a55" class="no np jg kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">我们永远不会预测小于0或大于1的东西，所以我们不用担心这个。</li></ul><p id="199f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们预测中间的东西呢？</p><p id="919a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们离目标越远，损失越大。<br/>你可以把它想成一个类似于平方误差的概念——我们离目标越远，误差增长越快。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="b6f0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">为什么猫和熊猫类的损失为0？</strong></p><p id="ea90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看起来我们是在奖励低损失的模型，即使它预测了图像中不存在的类别的高概率。</p><p id="4829" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们不介意模型预测有一只猫有80%的概率，如果没有，因为它只剩下20%的概率来预测正确的类别。在那里，损失会大得多。换句话说，我们不关心模型在哪些类上浪费了预测的概率，只关心它如何正确地识别唯一的当前类。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="3a0c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该图像的总损失是每类损失的总和。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/1cca725506647438680f7275d3cdd1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdyUaCqyINtyaf7ifn6EoA.png"/></div></div></figure><p id="84d8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它可以被公式化为所有类的总和。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/d469b9d702130c9cb11d26910500bedc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDgFCR7SxMcGRzInj0AC9g.png"/></div></div></figure><p id="36d7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是<strong class="kx jh">交叉熵</strong>公式，可以作为任意两个概率向量的损失函数。这是我们损失了一张图片——我们一开始展示的一只狗的图片。如果我们想要我们的批次或整个数据集的损失，我们只需合计单个图像的损失。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="1155" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们有两个不同的模型给出以下预测:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/60fb46a8cc42ab58bc246e043fc03cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2PA1Or-4Pk_SMPdqUGGzw.png"/></div></div></figure><p id="1708" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在交叉熵的眼中，模型B更好——它的交叉熵损失更低。如果你能明白为什么——干得好！有一只熊猫。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/6c297d9c00c37daa52c1106ca0b3adb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vMH19usLtFSSLRL-"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@zuoanyixi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰瑞米C </a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7a9d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过惩罚大错误比惩罚小错误多得多来训练模型，在机器学习中被证明是一个好主意。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="fb26" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果大多数类别的损失为0，为什么要对所有类别求和？ <br/>如果我们的目标是一个热点向量，我们确实可以忽略所有其他类别的目标和预测，只计算热点类别的损失。这是我们预测的负自然对数。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/de5da70f9ac7799feca0878a387f8bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngGdMBNkF1U0-DJImomQvg.png"/></div></div></figure><p id="bb22" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这被称为<strong class="kx jh">分类交叉熵</strong>——交叉熵的一个特例，其中我们的目标是一个热点向量。</p><p id="fd1d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事情是这样的——交叉熵损失甚至适用于非热点向量的分布。<br/>即使对于这个任务，损失也会起作用:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/f46c4f689aa1e670cd768e8aa2b8b7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNhpbKkGCnM6OHZjAJ_Rsg.png"/></div></div></figure><p id="b2c5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有了交叉熵，我们仍然能够计算损失，如果所有的类都是正确的，损失将是最小的，并且仍然具有惩罚更大错误的属性。</p><p id="7d76" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的单热目标例子中，熵是0，所以最小损失是0。如果你的目标是一个<strong class="kx jh">而不是</strong>的概率向量，熵(最小损失)将大于0，但是你仍然可以使用交叉熵损失。</p><p id="c020" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你更好奇熵是什么，我推荐你看这个<a class="ae jd" href="https://www.youtube.com/watch?v=ErfnhcEV1O8" rel="noopener ugc nofollow" target="_blank">视频</a>。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="6d6d" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">二元分类</h2><p id="498a" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated"><strong class="kx jh">二进制交叉熵</strong>是交叉熵的另一个特例——如果我们的目标是0或1，就使用它。在神经网络中，通常通过激活sigmoid来实现这种预测。</p><p id="e836" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">目标是<strong class="kx jh">而不是</strong>一个概率向量。我们仍然可以用一点小技巧来使用交叉熵。</p><p id="3f92" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们希望预测图像中是否包含熊猫。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/9b7a5e2216c73552b5239963ee43b579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bp1j1BQ1lGCDTKFgOusM-A.png"/></div></div></figure><p id="80de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就好像我们将目标转换为一个热点向量，将我们的预测转换为一个概率向量——熊猫的概率将与预测相同，而非熊猫的概率将是1-预测。换句话说，如果我们预测0.6，这意味着我们说60%是熊猫，40%不是熊猫。</p><p id="8ccd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种损失可以用交叉熵函数来计算，因为我们现在只比较两个概率向量，或者甚至用分类交叉熵来计算，因为我们的目标是一个热点向量。它也可以在没有使用<strong class="kx jh">二进制交叉熵</strong>进行转换的情况下进行计算。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/df614a0fa8b40d5cdb4447c0abfc245a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wz59MnXkXdWxrq_fkA2wew.png"/></div></div></figure><p id="bc5e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们只是将自然对数应用于我们的预测和目标之间的差异。</p><p id="3600" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是二元交叉熵的全部内容。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="7348" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">多标签分类</h2><p id="3b75" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">交叉熵也可以用作多标签问题的损失函数，方法很简单:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/7603fd7be3cb32ed911c0af11ce0826f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UwPD3hNltoBbxYxS517MDA.png"/></div></div></figure><p id="1315" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意我们的目标和预测是<strong class="kx jh">而不是</strong>一个概率向量。有可能图像中有所有的类，也有可能没有。在神经网络中，通常通过激活乙状结肠来实现这一点。</p><p id="b33d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以把这个问题看作多个二元分类子任务。假设我们只想预测有没有狗。</p><p id="bf6b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们知道我们的目标是1，而我们预测的是0.6</p><p id="a3f3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将计算这个子任务的二元交叉熵:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/b06132f1e8989629f0688b2c8059213f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSYKMqjWflxHypVKOFcE-g.png"/></div></div></figure><p id="14d9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对其他班级也这样做。</p><p id="9dd3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于猫，我们的目标是0，所以二进制交叉熵的另一部分抵消了:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/1f4c3cd76eb1d27d361a9f1219640aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdrG94CJqZM_rdNHk93jzQ.png"/></div></div></figure><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/14b1e4278bec4c18745bdd163505f498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJ0NjFByr8cHPB2gWTGYEg.png"/></div></div></figure><p id="6002" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">并合计每个子任务的损失:</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/9728ee5eb430cb41cbf45951fdce1cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nvn0bqXyK4Ph2jlwacnLYw.png"/></div></div></figure><p id="496e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是多标签分类的交叉熵损失。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="3f57" class="op lz jg bd ma oq or os md ot ou ov mg km ow kn mj kp ox kq mm ks oy kt mp oz bi translated">结论/TL；速度三角形定位法(dead reckoning)</h1><p id="ff7a" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated"><strong class="kx jh">交叉熵</strong> —通用公式，用于计算两个概率向量之间的损失。我们离目标越远，误差就越大——类似于平方误差。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/d469b9d702130c9cb11d26910500bedc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDgFCR7SxMcGRzInj0AC9g.png"/></div></div></figure><p id="16ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">多类分类</strong> —我们使用多类交叉熵<strong class="kx jh"> </strong> —交叉熵的一种特殊情况，其中目标是一个热点编码向量。它可以用交叉熵公式计算，但可以简化。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/de5da70f9ac7799feca0878a387f8bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngGdMBNkF1U0-DJImomQvg.png"/></div></div></figure><p id="6af0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">二进制分类</strong> —我们使用二进制交叉熵——交叉熵的一种特殊情况，我们的目标是0或1。如果我们将目标分别转换为像[0，1]或[1，0]这样的单热点向量和预测，则可以使用交叉熵公式来计算。即使没有这种转换，我们也可以用简化的公式来计算。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/df614a0fa8b40d5cdb4447c0abfc245a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wz59MnXkXdWxrq_fkA2wew.png"/></div></div></figure><p id="f0cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">多标签分类</strong> —我们的目标可以一次表示多个(甚至零个)类。我们分别计算每一类的二元交叉熵，然后将它们相加得到完全损失。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/4fe0d7164e449e227a2d71119a7382f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVvNdds2J1owKYGD8nhJsw.png"/></div></div></figure></div></div>    
</body>
</html>