<html>
<head>
<title>Text Cleaning Methods for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的文本清洗方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7?source=collection_archive---------6-----------------------#2020-02-28">https://towardsdatascience.com/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7?source=collection_archive---------6-----------------------#2020-02-28</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><figure class="it iu gq gs iv iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj is"><img src="../Images/fa1e2295d3d7c9233e51d17175a9a1f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYEuZyHqws_Bg-kltjP6Jw.jpeg"/></div></div><p class="jd je gk gi gj jf jg bd b be z dk translated">保罗·花冈在<a class="ae jh" href="https://unsplash.com/s/photos/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="8c3a" class="pw-subtitle-paragraph kh jj jk bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">为 NLP 准备数据的 5 种 python 方法</h2></div><p id="0530" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">自然语言处理被定义为“将计算技术应用于自然语言和语音的分析和合成”。为了执行这些计算任务，我们首先需要将文本语言转换为机器可以理解的语言。</p><p id="4965" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在下面的帖子中，我将描述为自然语言处理准备文本数据的最常见步骤。我将讨论一些可用于执行这些步骤的工具，并提供一些示例 python 代码来执行它们。</p></div><div class="ab cl lv lw hy lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="in io ip iq ir"><p id="57ff" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在整篇文章中，我将使用一个取自 Kaggle 的数据集，该数据集可以从<a class="ae jh" href="https://www.kaggle.com/c/nlp-getting-started/data" rel="noopener ugc nofollow" target="_blank">这里</a>下载。这个数据集由来自推文的文本和一个目标变量组成，该变量将推文分类为真正的灾难和非灾难。下面的代码使用 pandas 将数据读入数据框。为了简单起见，我删除了除文本和目标变量之外的所有列。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="8e16" class="ml mm jk mh b gz mn mo l mp mq">import pandas as pd<br/>pd.set_option('display.max_colwidth', -1)</span><span id="89e9" class="ml mm jk mh b gz mr mo l mp mq">train_data = pd.read_csv('train.csv')<br/>cols_to_drop = ['id', 'keyword', 'location']<br/>train_data = train_data.drop(cols_to_drop, axis=1)<br/>train_data.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj ms"><img src="../Images/7cd425827d9daf8daa016aa2bcd63e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKZbu_2ZetujvDjcoWoB1w.png"/></div></div></figure><h2 id="7833" class="ml mm jk bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">1)标准化</h2><p id="8073" class="pw-post-body-paragraph kz la jk lb b lc nk kl le lf nl ko lh li nm lk ll lm nn lo lp lq no ls lt lu in bi translated">处理语言数据的关键步骤之一是去除噪声，以便机器可以更容易地检测数据中的模式。文本数据包含大量干扰，这些干扰以特殊字符的形式出现，如标签、标点符号和数字。所有这些如果出现在数据中，计算机都很难理解。因此，我们需要处理数据以去除这些元素。</p><p id="e966" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">此外，注意单词的大小写也很重要。如果我们包含相同单词的大写和小写版本，那么计算机会将它们视为不同的实体，即使它们可能是相同的。</p><p id="912e" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">下面的代码执行这些步骤。为了跟踪我们对文本所做的更改，我将干净的文本放在了一个新的列中。输出显示在代码下方。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="6088" class="ml mm jk mh b gz mn mo l mp mq">import re</span><span id="fb68" class="ml mm jk mh b gz mr mo l mp mq">def  clean_text(df, text_field, new_text_field_name):<br/>    df[new_text_field_name] = df[text_field].str.lower()<br/>    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))  <br/>    # remove numbers<br/>    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))<br/>    <br/>    return df</span><span id="03f7" class="ml mm jk mh b gz mr mo l mp mq">data_clean = clean_text(train_data, 'text', 'text_clean')</span><span id="4606" class="ml mm jk mh b gz mr mo l mp mq">data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj np"><img src="../Images/44349508f4d2edf4f6f4ed8481a66b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ed7peMcT_8-7WxWBh-G1nA.png"/></div></div></figure><h2 id="ff3c" class="ml mm jk bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">2)停用词</h2><p id="8a0c" class="pw-post-body-paragraph kz la jk lb b lc nk kl le lf nl ko lh li nm lk ll lm nn lo lp lq no ls lt lu in bi translated">停用字词是常见的字词，对于某些计算过程来说，这些字词提供的信息很少，或者在某些情况下会引入不必要的噪声，因此需要将其移除。对于文本分类任务来说尤其如此。</p><p id="faa2" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在其他情况下，不建议删除停用词，或者需要更仔细地考虑。这包括由于删除停用词而失去一段文本意义的任何情况。例如，如果我们正在构建一个聊天机器人，并从短语“<strong class="lb jl">我不高兴</strong>”中删除单词“<strong class="lb jl">而不是</strong>，那么算法实际上可能会解释相反的意思。这对于聊天机器人或情感分析等用例尤为重要。</p><p id="d355" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><a class="ae jh" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包</a> (NLTK) python 库内置了移除停用词的方法。下面的代码使用它来删除 tweets 中的停用词。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="63ee" class="ml mm jk mh b gz mn mo l mp mq">import nltk.corpus<br/>nltk.download('stopwords')</span><span id="1d8a" class="ml mm jk mh b gz mr mo l mp mq">from nltk.corpus import stopwords<br/>stop = stopwords.words('english')</span><span id="0ec5" class="ml mm jk mh b gz mr mo l mp mq">data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))<br/>data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nq"><img src="../Images/df7c767ceba0ea861985dc73d475fb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*It_iuiPsnKjeftOSmwwlkw.png"/></div></div></figure><h2 id="7a94" class="ml mm jk bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">3)词干</h2><p id="dbc1" class="pw-post-body-paragraph kz la jk lb b lc nk kl le lf nl ko lh li nm lk ll lm nn lo lp lq no ls lt lu in bi translated">词干化是将单词还原为其词根形式的过程。比如“<strong class="lb jl"> rain </strong>”、“<strong class="lb jl"> raining </strong>”和“<strong class="lb jl"> rained </strong>”这几个词就非常相似，在很多情况下，意思是一样的。堵塞的过程会将这些还原为“雨”的根形式。这也是一种减少噪音和数据维数的方法。</p><p id="d5ae" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">NLTK 库也有执行词干提取任务的方法。下面的代码使用 PorterStemmer 来阻止我上面的例子中的单词。从输出中可以看到，所有的单词现在都变成了“rain”。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="c9d1" class="ml mm jk mh b gz mn mo l mp mq">from nltk.stem import PorterStemmer <br/>from nltk.tokenize import word_tokenize</span><span id="5ea2" class="ml mm jk mh b gz mr mo l mp mq">word_list = ['rains', 'raining', 'rain', 'rained']</span><span id="9739" class="ml mm jk mh b gz mr mo l mp mq">ps = PorterStemmer()<br/>for w in word_list:<br/>    print(ps.stem(w))</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div class="gi gj nr"><img src="../Images/daa80e86aecabe5de658d157cd48ee6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/format:webp/1*NrFnPT9JWuYNKK1cjqT19w.png"/></div></figure><p id="f19d" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在我们对数据进行词干提取之前，我们需要对推文进行标记。这是一种用于将文本拆分成其组成部分(通常是单词)的方法。下面的代码使用 NLTK 来完成这项工作。我已经将输出放到了一个名为“text_tokens”的新列中。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="1ce1" class="ml mm jk mh b gz mn mo l mp mq">import nltk <br/>nltk.download('punkt')</span><span id="a1c7" class="ml mm jk mh b gz mr mo l mp mq">from nltk.tokenize import sent_tokenize, word_tokenize</span><span id="8e8f" class="ml mm jk mh b gz mr mo l mp mq">data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))</span><span id="99d9" class="ml mm jk mh b gz mr mo l mp mq">data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj ns"><img src="../Images/5efd084d5c56b3c513030baadb551229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_loIp5ZcNfnjZBjHQ20HA.png"/></div></div></figure><p id="72ce" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">下面的代码使用 NLTK 中的 PorterStemmer 方法对 text_tokens 应用词干，并将处理后的文本输出到新列。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="aca1" class="ml mm jk mh b gz mn mo l mp mq">def word_stemmer(text):<br/>    stem_text = [PorterStemmer().stem(i) for i in text]<br/>    return stem_text</span><span id="a0b6" class="ml mm jk mh b gz mr mo l mp mq">data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))<br/>data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nt"><img src="../Images/2e19c18bf63e2170ba5514a518900e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSxgfRMtBwOC5m8kOlGfCg.png"/></div></div></figure><h2 id="a5be" class="ml mm jk bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">4)词汇化</h2><p id="d4e2" class="pw-post-body-paragraph kz la jk lb b lc nk kl le lf nl ko lh li nm lk ll lm nn lo lp lq no ls lt lu in bi translated">词汇化的目标与词干化的目标是一样的，因为它旨在将单词简化为它们的根形式。然而，众所周知，词干处理是一种相当粗糙的方法。另一方面，词汇化是一种工具，它执行完整的词法分析，以更准确地找到单词的词根或“词汇”。</p><p id="4cc1" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">同样，NLTK 可以用来执行这项任务。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="e551" class="ml mm jk mh b gz mn mo l mp mq">nltk.download('wordnet')<br/>from nltk.stem import WordNetLemmatizer</span><span id="5d17" class="ml mm jk mh b gz mr mo l mp mq">def word_lemmatizer(text):<br/>    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]<br/>    return lem_text</span><span id="04cb" class="ml mm jk mh b gz mr mo l mp mq">data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))<br/>data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nu"><img src="../Images/257eafdd3779445822da03880046cb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6raeXih5NtTNe1utXDwZQ.png"/></div></div></figure><h2 id="4209" class="ml mm jk bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">5)词性标注和组块</h2><p id="8011" class="pw-post-body-paragraph kz la jk lb b lc nk kl le lf nl ko lh li nm lk ll lm nn lo lp lq no ls lt lu in bi translated">词性标注是一种对单词进行分类的方法，它提供了一些与单词在语音中的使用方式有关的信息。</p><p id="cb69" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">有八个主要的词类，它们都有相应的标签。这些显示在下表中。</p><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nv"><img src="../Images/c69a224eaea2e25203254c1f2252b338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRrCL3rpkS7p7453EI9tZw.jpeg"/></div></div></figure><p id="1ddc" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">NLTK 库有一个执行词性标注的方法。下面的代码对我们数据集中的 tweets 执行 POS 标记，并返回一个新列。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="f4c8" class="ml mm jk mh b gz mn mo l mp mq">def word_pos_tagger(text):<br/>    pos_tagged_text = nltk.pos_tag(text)<br/>    return pos_tagged_text</span><span id="e29c" class="ml mm jk mh b gz mr mo l mp mq">nltk.download('averaged_perceptron_tagger')</span><span id="5461" class="ml mm jk mh b gz mr mo l mp mq">data_clean['text_tokens_pos_tagged'] = data_clean['text_tokens'].apply(lambda x: word_pos_tagger(x))<br/>data_clean.head()</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nw"><img src="../Images/656ddaf765ebe41af1f15c739bbd327c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cof66CpBCFl0zOGaStRi5w.png"/></div></div></figure><p id="3348" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">组块建立在词性标注的基础上，它使用词性标注中的信息从文本中提取有意义的短语。在许多类型的文本中，如果我们把一切都简化为单个单词，我们可能会失去很多意义。例如，在我们的推文中，我们有很多位置名称和其他短语，这些都是很重要的。</p><p id="b340" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">如果我们拿这句话“<strong class="lb jl">la ronge sask 附近的森林大火</strong>”这个地名“<strong class="lb jl"> la ronge </strong>”和“<strong class="lb jl">森林大火</strong>”这几个字就传达了一个我们可能不想失去的重要意义。</p><p id="6633" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">spaCy python 库对此有一个方法。如果我们把这种方法应用到上面的句子中，我们可以看到它分离出了合适的短语。</p><pre class="mc md me mf gu mg mh mi mj aw mk bi"><span id="02f2" class="ml mm jk mh b gz mn mo l mp mq">import spacy<br/>nlp = spacy.load('en')</span><span id="7515" class="ml mm jk mh b gz mr mo l mp mq">text = nlp("forest fire near la ronge sask canada")<br/>for chunk in text.noun_chunks:<br/>    print(chunk.text, chunk.label_, chunk.root.text)</span></pre><figure class="mc md me mf gu iw gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/86dbba5503558330f4fee7121840f9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*CB2SOLaEOwyzoTuSMgmBkw.png"/></div></figure></div><div class="ab cl lv lw hy lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="in io ip iq ir"><p id="4f80" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">本文给出了一些最流行的方法的简要概述，这些方法用于清理准备用于自然语言处理的文本数据。为了更深入地了解这些概念和工具，这篇博客<a class="ae jh" href="https://nlpforhackers.io/complete-guide-to-spacy/" rel="noopener ugc nofollow" target="_blank"> nlpforhackers.io </a>是我的最爱之一。</p><p id="6b53" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">如果你对将机器学习技术应用于这些数据感兴趣，我上周写了一篇文章，介绍如何让你的<a class="ae jh" rel="noopener" target="_blank" href="/how-to-enter-your-first-kaggle-competition-4717e7b232db">第一次提交给这个数据集来自的 Kaggle 竞赛</a>。</p><p id="3c31" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">感谢阅读！</p></div></div>    
</body>
</html>