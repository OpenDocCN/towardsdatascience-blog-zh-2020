<html>
<head>
<title>Forecasting Food Demand</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测食物需求</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forecasting-food-demand-applying-neural-networks-to-the-meal-kit-industry-6f1e3b2207aa?source=collection_archive---------46-----------------------#2020-06-28">https://towardsdatascience.com/forecasting-food-demand-applying-neural-networks-to-the-meal-kit-industry-6f1e3b2207aa?source=collection_archive---------46-----------------------#2020-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e620" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络在餐包行业中的应用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/15b341001c0465b5b488cda41a209c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SU6-9-yO1Ggg7LnlCGXCwQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@lvnatikk" rel="noopener ugc nofollow" target="_blank">百合网</a>在<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cb78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">所以这将是过度拟合。</strong></p><p id="0286" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">时间序列问题通常与过度拟合作斗争。这整个练习变得更具挑战性，看我如何能防止时间序列预测中的过度拟合。</p><p id="aead" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我添加了<a class="ae kv" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank">重量衰减</a>和<a class="ae kv" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">脱落</a>。这将有助于防止过度拟合。网络有分类变量的嵌入层(我改变了大小)，接着是丢弃和<a class="ae kv" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量标准化</a>(对于连续变量)。</p><p id="e60b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据这篇文章<a class="ae kv" href="https://medium.com/@lankinen/fast-ai-lesson-6-notes-part-1-v3-646edf916c04" rel="noopener">的说法</a>理想情况下，你想要更低的脱落量和更大的体重衰减量。</p><h1 id="0ff2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">资料组</h1><p id="47ed" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据由一家<a class="ae kv" href="https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/#ProblemStatement" rel="noopener ugc nofollow" target="_blank">餐包公司</a>给出。由于食物容易腐烂，计划和需求预测极其重要。</p><p id="b0d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">弄错了这一点可能会给一家餐盒公司带来灾难。补充通常每周进行一次。我们需要预测未来 10 周的需求。</p><h1 id="7b4a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">预处理</strong></h1><p id="657d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">多亏了 Fastai，标准化、填充缺失值和编码分类变量现在是一个相对简单的过程。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="937e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将为每个分类变量创建嵌入。问题是，每个分类嵌入应该有多大？Fastai 有一个很好的经验法则，其中分类嵌入由以下公式给出:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="2b01" class="mw lt iq ms b gy mx my l mz na">cardinality = len(df[cat_vars[0]].value_counts().index)<br/>emb_size = min(50, cardinality//2)</span></pre><p id="2e6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着嵌入的大小将是分类变量中唯一值的数量除以 2 并向下舍入。也可能是 50。哪个最小。</p><p id="b51d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们最终的嵌入大小字典如下所示</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="202f" class="mw lt iq ms b gy mx my l mz na"><br/>emb_szs = {cat_vars[i]:min(len(df[cat_vars[i]].value_counts().index)//2, min_size) for i in range(1, len(cat_vars))}<br/><br/>{‘Day’: 15,<br/> ‘Dayofweek’: 0,<br/> ‘Dayofyear’: 50,<br/> ‘Is_month_end’: 1,<br/> ‘Is_month_start’: 1,<br/> ‘Is_quarter_end’: 1,<br/> ‘Is_quarter_start’: 1,<br/> ‘Is_year_end’: 1,<br/> ‘Is_year_start’: 1,<br/> ‘Month’: 6,<br/> ‘Week’: 26,<br/> ‘Year’: 1,<br/> ‘category’: 7,<br/> ‘center_id’: 38,<br/> ‘center_type’: 1,<br/> ‘city_code’: 25,<br/> ‘cuisine’: 2,<br/> ‘email_plus_homepage’: 1,<br/> ‘emailer_for_promotion’: 1,<br/> ‘homepage_featured’: 1,<br/> ‘meal_id’: 25,<br/> ‘op_area’: 15,<br/> ‘region_code’: 4}</span></pre><p id="0b9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要调整我们的模型架构。这总是最难的部分。几篇文章强调了获得正确的模型架构的重要性。在许多方面，这可以被视为<a class="ae kv" href="https://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html" rel="noopener ugc nofollow" target="_blank">“新”特征工程</a>。</p><p id="f59c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank">由<a class="ae kv" href="https://scholar.google.com/citations?user=pwh7Pw4AAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Leslie Smith </a>撰写的这篇论文</a>提出了一种有趣的方法，以一种更加规范的方式来选择超参数。我将从<a class="ae kv" href="https://www.kaggle.com/qitvision/a-complete-ml-pipeline-fast-ai" rel="noopener ugc nofollow" target="_blank">这个 kaggle 内核</a>建模实现。</p><p id="0f3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要找到最优的学习速率、权重衰减和嵌入丢失。根据 Leslie Smith 的说法，为了选择最佳的超参数，我们需要针对一些不同的权重衰减和下降值运行学习率查找器。然后，我们选择具有最低损失、最高学习速率(在快速增加之前)和最高重量衰减的最大组合。</p><p id="8789" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那要考虑很多。此外，还有几个我们没有考虑的超参数。为此，我将借用具有相对较大的层的模型架构。这款车型曾在罗斯曼·卡格尔比赛中获得第三名。</p><p id="cf0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我需要考虑批量大小。根据 Leslie 的说法，这应该设置得尽可能高，以适应所有可用的内存。我非常乐意做那件事。这大大减少了我的训练时间。</p><p id="d382" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用 Fastai 中的学习率查找器来可视化我们改变模型架构时的损失。从那里，我创建了一个基本的网格搜索。我不想实现疯狂的深度网格搜索——那将是计算开销很大的。我认为最好的方法是手动操作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="b6b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们描绘出模型架构的所有不同组合，事情就会变得更加清晰。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5804a3514aa4166d834c97e173048e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*VStrYsFZhc8J-4Z8YD9JDw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><p id="2f22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们选择重量衰减为零的模型架构，损耗峰值会更早出现。从选项中,“0.6”的权重衰减允许我们以最低的损失训练合理的高学习率。</p><p id="f6de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然与最低损失水平相对应的学习率在 1e-1 区域，但我不会使用该学习率。相反，我将为学习率选择 1e-2 值。这是爆炸的安全值。这已经<a class="ae kv" href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html" rel="noopener ugc nofollow" target="_blank">显示</a>有助于训练。</p><p id="f138" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们的最终模型:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="8918" class="mw lt iq ms b gy mx my l mz na">learn = get_learner(emb_szs=emb_szs, layers = [1000,500], ps = [0.2,0.4], emb_drop = 0.04)<br/><br/><br/>TabularModel(<br/> (embeds): ModuleList(<br/> (0): Embedding(146, 26)<br/> (1): Embedding(78, 38)<br/> (2): Embedding(52, 25)<br/> (3): Embedding(3, 1)<br/> (4): Embedding(3, 1)<br/> (5): Embedding(15, 7)<br/> (6): Embedding(5, 2)<br/> (7): Embedding(52, 25)<br/> (8): Embedding(9, 4)<br/> (9): Embedding(4, 1)<br/> (10): Embedding(31, 15)<br/> (11): Embedding(4, 1)<br/> (12): Embedding(13, 6)<br/> (13): Embedding(53, 26)<br/> (14): Embedding(32, 15)<br/> (15): Embedding(2, 0)<br/> (16): Embedding(146, 50)<br/> (17): Embedding(3, 1)<br/> (18): Embedding(3, 1)<br/> (19): Embedding(3, 1)<br/> (20): Embedding(3, 1)<br/> (21): Embedding(3, 1)<br/> (22): Embedding(3, 1)<br/> (23): Embedding(4, 1)<br/> )<br/> (emb_drop): Dropout(p=0.04, inplace=False)<br/> (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/> (layers): Sequential(<br/> (0): Linear(in_features=256, out_features=1000, bias=True)<br/> (1): ReLU(inplace=True)<br/> (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/> (3): Dropout(p=0.2, inplace=False)<br/> (4): Linear(in_features=1000, out_features=500, bias=True)<br/> (5): ReLU(inplace=True)<br/> (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/> (7): Dropout(p=0.4, inplace=False)<br/> (8): Linear(in_features=500, out_features=1, bias=True)<br/> )<br/>)</span></pre><p id="d76d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将在这里使用学习率退火。这是<a class="ae kv" href="https://sgugger.github.io/the-1cycle-policy.html" rel="noopener ugc nofollow" target="_blank">显示工作良好</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/4beb85f80cb759bc13879d9d4cb0adb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qwujp_tm58ZsfOFliB9J1A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/cf69cbfb1a47c6fade88db2b3524f6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTAPxcPlniMpVF5EGNI-lw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><p id="1753" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将继续拟合更多的周期，直到验证开始增加。我将在拟合几个纪元后保存模型。这样我就可以在以后的推理中使用最好的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/cf69cbfb1a47c6fade88db2b3524f6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTAPxcPlniMpVF5EGNI-lw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9a0cd029ef54fb2584894197a5b3e8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*mI_F6y_6l_YIyVazi3XZCw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><p id="9e9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我能得到的最好结果是大约 0.29 的确认损失(四舍五入)。</p><p id="6f15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与马丁·阿拉克伦的<a class="ae kv" href="https://www.martinalarcon.org/2018-12-31-b-water-pumps/" rel="noopener ugc nofollow" target="_blank">文章</a>类似。我想将神经网络的性能与更传统的方法进行比较。</p><h1 id="a2c6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">其他方法</h1><p id="3533" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">XGBoost、随机森林回归器和 LightGBM。相对于神经网络，它们的表现如何？</p><p id="1bed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将使用或多或少与神经网络相同的数据。Fastai 已经内置了优秀的预处理方法。</p><p id="7225" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，Fastai 的分类编码有点奇怪。Fastai 从分类值到它们的编码值创建一个字典。在推断时间，分类值<a class="ae kv" href="https://forums.fast.ai/t/fastai-v2-code-walk-thru-8/55068" rel="noopener ugc nofollow" target="_blank">被编码值交换</a>。</p><p id="2648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很聪明，也很有用。但这使得在 Fastai 生态系统之外的模型中使用 Fastai 预处理数据有点困难。</p><p id="f71c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解决这个问题，我创建了一个简单的脚本，将 Fastai 表格数据束转换成可以提供给另一个模型的数据。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="0ba2" class="mw lt iq ms b gy mx my l mz na"># inspired by <a class="ae kv" href="https://www.martinalarcon.org/2018-12-31-b-water-pumps/" rel="noopener ugc nofollow" target="_blank">https://www.martinalarcon.org/2018-12-31-b-water-pumps/</a><br/>class convert_tabular_learner_to_df():</span><span id="43ed" class="mw lt iq ms b gy ng my l mz na">def __init__(self, cat_names, tabular_data_bunch):<br/> self.cat_names = cat_names<br/> self.tabular_data_bunch = tabular_data_bunch</span><span id="4f97" class="mw lt iq ms b gy ng my l mz na">def driver(self):</span><span id="6d76" class="mw lt iq ms b gy ng my l mz na"># convert tabular data to dataframe<br/> X_train, y_train = self.list_to_df(self.tabular_data_bunch.train_ds)<br/> X_valid, y_valid = self.list_to_df(self.tabular_data_bunch.valid_ds)</span><span id="e1a2" class="mw lt iq ms b gy ng my l mz na"># label encode data<br/> encoder = BinaryEncoder(cols = self.cat_names)<br/> X_train = encoder.fit_transform(X_train)<br/> X_valid = encoder.transform(X_valid)</span><span id="33bf" class="mw lt iq ms b gy ng my l mz na">return X_train, X_valid, y_train, y_valid</span><span id="d358" class="mw lt iq ms b gy ng my l mz na">def list_to_df(self, tabular_learner):</span><span id="7c68" class="mw lt iq ms b gy ng my l mz na"># create X df<br/> x_vals = np.concatenate([tabular_learner.x.codes, tabular_learner.x.conts], axis=1)<br/> cols = tabular_learner.x.cat_names + tabular_learner.x.cont_names<br/> x_df = pd.DataFrame(data=x_vals, columns=cols)</span><span id="ab8f" class="mw lt iq ms b gy ng my l mz na"># reorder cols<br/> x_df = x_df[[c for c in tabular_learner.inner_df.columns if c in cols]]</span><span id="9aaa" class="mw lt iq ms b gy ng my l mz na"># create y labels<br/> cols = [i.obj for i in tabular_learner.y]<br/> y_vals = np.array(cols, dtype=”float64")</span><span id="35cc" class="mw lt iq ms b gy ng my l mz na">return x_df, y_vals</span></pre><p id="c94c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将对数据进行一系列回归分析。每个都使用默认值。我想知道神经网络相对于回归的标准方法表现如何。好点了吗？更糟？</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="be21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在看结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/36c57796830d7310c0ca3f3745021703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*xR9dID_RJrsNLAfnml8HiA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nc">作者图片</em></p></figure><p id="ac61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，似乎其他模型优于神经网络。尽管如此，具有类别嵌入的深度学习模型在<a class="ae kv" href="https://www.kaggle.com/c/rossmann-store-sales/discussion/17974" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上非常受欢迎。我可能需要改变我使用的下降量和体重衰减量。但目前，就 RMSPE 而言，RandomForestRegressor 是最好的模型。</p><h1 id="9c24" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">丰富</h1><p id="feb5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">1.模型架构。我想改变更多的超参数，同时避免昂贵的网格搜索。这可能是进一步改进模型的唯一最有用的事情。</p><p id="a2e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Fastai 明确警告您不要减少参数，以免过度拟合。相反，使用辍学和体重衰减宽松。</p><p id="96bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经尝试过了。但最终还是稍微超配了一下。改变超参数可能有助于进一步减少过度拟合。</p><p id="3da1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具体来说，我可能会受益于不同的辍学。我想改变嵌入层的漏失，更重要的是漏失的概率。</p><p id="e5f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://scholarworks.uark.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&amp;httpsredir=1&amp;article=1028&amp;context=csceuht" rel="noopener ugc nofollow" target="_blank">本文</a>讲述了大型*深度*神经网络中的辍学效应。也许加深网络和更自由地应用 dropout 可以提高性能？</p><p id="e424" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">网格搜索可以随机实现。这叫做随机搜索。我可以潜在地使用<a class="ae kv" href="https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search" rel="noopener ugc nofollow" target="_blank"> skorch </a>来做这件事。</p><p id="ecf0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.我也想试试脸书的先知。这是一个用于时间序列预测的开源工具。我想看看它相对于这个神经网络的表现。</p><p id="39ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.混合。在<a class="ae kv" href="https://www.kaggle.com/fl2ooo/nn-wo-pseudo-1-fold-seed" rel="noopener ugc nofollow" target="_blank"> kaggle </a>上的第一名解决方案使用了混合了 lightGBM 模型的神经网络。这可能是未来研究的希望。</p><p id="b998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的<a class="ae kv" href="https://github.com/spiyer99/spiyer99.github.io/blob/master/nbs/medium_food_demand_prediction_mealkit.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上有完整的代码</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="1ceb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原帖来自<a class="ae kv" href="https://spiyer99.github.io/Mealkit-Food-Demand-Forecasting/" rel="noopener ugc nofollow" target="_blank"> spiyer99.github.io </a></p></div></div>    
</body>
</html>