<html>
<head>
<title>What is a Markov Decision Process Anyways?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">到底什么是马尔可夫决策过程？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-markov-decision-process-anyways-bdab65fd310c?source=collection_archive---------14-----------------------#2020-03-07">https://towardsdatascience.com/what-is-a-markov-decision-process-anyways-bdab65fd310c?source=collection_archive---------14-----------------------#2020-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="282b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解大多数强化学习问题中使用的模型。</h2></div><h1 id="9a6e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">我为什么要关心马尔可夫决策过程？</h1><p id="7176" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">任何对强化学习的发展感兴趣的人都应该知道他们建立的模型——马尔可夫决策过程。他们建立了一个具有不确定性的世界结构，行动将带你去哪里，代理人需要学习如何行动。</p><h2 id="7eed" class="lw kj it bd kk lx ly dn ko lz ma dp ks lj mb mc ku ln md me kw lr mf mg ky mh bi translated">非确定性搜索</h2><p id="3a58" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">搜索是人工智能和智能代理的中心问题。通过规划未来，搜索允许代理解决游戏和后勤问题——但它们依赖于知道某个动作会把你带到哪里。在传统的基于<strong class="lc iu">树的方法</strong>中，一个动作把你带到下一个状态，没有下一个状态的分布。这意味着，如果你有足够的存储空间，你可以规划<strong class="lc iu">集，确定性轨迹</strong>到未来。马尔可夫决策过程使得这种规划<strong class="lc iu">具有随机性、</strong>或不确定性。与本文相关的搜索主题列表很长——<a class="ae mi" href="https://en.wikipedia.org/wiki/Graph_traversal" rel="noopener ugc nofollow" target="_blank">图搜索</a>、<a class="ae mi" href="https://en.wikipedia.org/wiki/Game_tree" rel="noopener ugc nofollow" target="_blank">博弈树</a>、<a class="ae mi" href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning" rel="noopener ugc nofollow" target="_blank"> alpha-beta 剪枝</a>、<a class="ae mi" href="https://en.wikipedia.org/wiki/Minimax" rel="noopener ugc nofollow" target="_blank"> minimax 搜索</a>、<a class="ae mi" href="https://en.wikipedia.org/wiki/Expectiminimax" rel="noopener ugc nofollow" target="_blank"> expectimax 搜索</a>等。</p><p id="55ca" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">在现实世界中，这是一个更好的代理行为模型。我们采取的每一个简单的行动——倒咖啡、寄信、移动关节——都有预期的结果，但是生活中有一种随机性。马尔可夫决策过程是让计划捕捉这种不确定性的工具。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/1efabb51daca6b084d6447e5a6656fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10oIc8pHFC2lBzH3PKtA9g.jpeg"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">日常决策。照片由<a class="ae mi" href="https://www.pexels.com/@mhtoori?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> mhtoori 拍摄。com </a> from <a class="ae mi" href="https://www.pexels.com/photo/aerial-photography-of-concrete-road-1646164/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>。</p></figure><h1 id="7845" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">马尔可夫决策过程的马尔可夫性是什么？</h1><p id="521c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Markov 是关于 Andrey Markov 的，Andrey Markov 是一位著名的俄罗斯数学家，以他在随机过程方面的工作而闻名。</p><blockquote class="ne"><p id="0851" class="nf ng it bd nh ni nj nk nl nm nn lv dk translated">“马尔可夫”一般是指给定现在的状态，未来和过去是独立的。</p></blockquote><figure class="np nq nr ns nt mt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8e3689bbea58913eaae1d9d72a8afadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*VrprbG6zLNUKk3gg4mvvoQ.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">安德烈·马尔科夫(1856-1922)。</p></figure><p id="8b97" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">制造<em class="nu">马尔可夫</em>系统的关键思想是<strong class="lc iu">无记忆</strong>。无记忆是指系统的历史不会影响当前状态。在概率符号中，<strong class="lc iu">无记忆性</strong>翻译成这个。考虑一系列的行动产生一个轨迹，我们正在看当前的行动会把我们带到哪里。长条件概率可能看起来像:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6142837e30793dc296a7ef91355dd5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*Alwi8C8HZoyuc8YjpBUJ-w.png"/></div></figure><p id="56bf" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">现在——如果系统是马尔可夫的，历史是<strong class="lc iu"> <em class="nu">全部包含在当前状态</em> </strong>中。所以，我们的一步分布要简单得多。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nw"><img src="../Images/c152d549c908ff1377bb07d2f6041ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*Sms2jDwBY2HAQSyeCThoSQ.png"/></div></div></figure><p id="d681" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">这一步改变了计算效率的游戏规则。<em class="nu">马尔可夫属性支撑了所有现代强化学习算法的存在和成功。</em></p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="ff20" class="ki kj it bd kk kl oe kn ko kp of kr ks jz og ka ku kc oh kd kw kf oi kg ky kz bi translated">马尔可夫决策过程</h1><p id="04a2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">MDP 由以下量定义:</p><ul class=""><li id="b388" class="oj ok it lc b ld mj lg mk lj ol ln om lr on lv oo op oq or bi translated">一组状态<strong class="lc iu"> s ∈ S </strong>。这些状态代表了世界上所有可能的构型。在下面的示例中，是机器人位置。</li><li id="4cbe" class="oj ok it lc b ld os lg ot lj ou ln ov lr ow lv oo op oq or bi translated">一组动作<strong class="lc iu"> a ∈ A </strong>。动作是代理可以采取的所有可能动作的集合。动作下面是{北，东，南，西}。</li><li id="bd35" class="oj ok it lc b ld os lg ot lj ou ln ov lr ow lv oo op oq or bi translated">一个转移函数<strong class="lc iu"> T(s，a，s’)</strong>。T(s，a，s’)持有 MDP 的<strong class="lc iu">不确定性</strong>。给定当前位置和提供的动作，T 决定下一个状态跟随的频率。在下面的例子中，转移函数可以是下一个状态在 80%的时间里是在动作的方向上，但是在另外 20%的时间里偏离 90 度。这对规划有什么影响？在下面的例子中，机器人选择了北方，但有 10%的可能性是向东或向西。</li><li id="81fc" class="oj ok it lc b ld os lg ot lj ou ln ov lr ow lv oo op oq or bi translated">一个奖励函数<strong class="lc iu"> R(s，a，s’)。任何代理人的目标都是回报总和最大化。</strong>这个函数说的是每一步获得多少奖励。一般来说，在每一步都会有一个小的负奖励(成本)来鼓励快速解决问题，在最终状态会有大的正(目标)或负(失败的任务)奖励。下面，宝石和火坑是终端状态。</li><li id="eca7" class="oj ok it lc b ld os lg ot lj ou ln ov lr ow lv oo op oq or bi translated">起始状态<strong class="lc iu"> s0 </strong>，也可能是终止状态。</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ox"><img src="../Images/f27a4118b47f3616b93d7982e29aebdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-oiL7isNsMmktFCNalhTqQ.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">MDP 就是一个例子。来源——我在 CS188 做的一次讲座。</p></figure><h2 id="5132" class="lw kj it bd kk lx ly dn ko lz ma dp ks lj mb mc ku ln md me kw lr mf mg ky mh bi translated">这给了我们什么？</h2><p id="b9c4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个定义给了我们一个有限的世界，我们一套向前的动力学模型。我们知道每个转变的确切概率，以及每个行动有多好。最终，这个模型是一个<strong class="lc iu">场景</strong>——在这个场景中，我们将计划如何行动，知道我们的行动可能会有点偏差。</p><p id="4d0f" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">如果机器人在火坑旁边，机器人应该总是选择北方吗？知道北方有机会把它送到东方吗？</p><p id="467b" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">不，最佳政策是向西。撞墙最终会(20%几率)北上，将机器人送上通往目标的轨道。</p><h1 id="94ec" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">政策</h1><p id="bfac" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">学习如何在未知的环境中行动是了解环境的最终目标。在 MDP 中，这被称为<strong class="lc iu">策略</strong>。</p><blockquote class="ne"><p id="76ad" class="nf ng it bd nh ni nj nk nl nm nn lv dk translated">策略是一种功能，它从一个状态给你一个动作。π*: S → A。</p></blockquote><p id="28a2" class="pw-post-body-paragraph la lb it lc b ld oy ju lf lg oz jx li lj pa ll lm ln pb lp lq lr pc lt lu lv im bi translated">获得策略的方法有很多，但核心思想是价值和策略迭代。这两种方法<strong class="lc iu">迭代地建立一个状态的总效用</strong>的估计，也许是一个动作。</p><blockquote class="ne"><p id="701d" class="nf ng it bd nh ni nj nk nl nm nn lv dk translated">一个国家的效用是(贴现)奖励的总和。</p></blockquote><p id="dddb" class="pw-post-body-paragraph la lb it lc b ld oy ju lf lg oz jx li lj pa ll lm ln pb lp lq lr pc lt lu lv im bi translated">一旦每个州都有一个效用，高层次的规划和政策制定<strong class="lc iu">就会遵循效用最大化的路线</strong>。</p><p id="b3d8" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">在 MDP 和其他学习方法中，模型增加了一个<strong class="lc iu">折扣因子</strong> r γ，以区分短期和长期奖励的优先级。贴现因子直观上是有意义的——人类和生物现在比以后更能创造手中的货币(或食物)价值。折扣因子还通过将奖励的总和变成几何级数，带来了巨大的计算收敛帮助。(<em class="nu">如果你有兴趣，</em> <a class="ae mi" href="https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec12.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nu">这里有一个关于 MDP 解的收敛性的讲座</em> </a>)。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pd"><img src="../Images/f9e426ae3d5c0b2ec9978fff13e709e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rFLIpkGtvfXcKy1ARVogsg.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">来源——我在 CS188 做的一个<a class="ae mi" href="https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec10.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p></figure><p id="2de4" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">我把它作为一个练习留给读者，让他们为这个例子找出最优策略——想想最终状态会是什么。我们能避免它吗？学习如何获得这些策略留待另一篇文章来讨论。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="3e21" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">更多关于强化学习的介绍——你也可以在这里或者这里找到课程材料<a class="ae mi" href="https://inst.eecs.berkeley.edu/~cs188/sp20/" rel="noopener ugc nofollow" target="_blank">。下面是我的几篇文章。</a></p><div class="pe pf gp gr pg ph"><a rel="noopener follow" target="_blank" href="/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd iu gy z fp pm fr fs pn fu fw is bi translated">强化学习的隐藏线性代数</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">线性代数的基础如何支持深度强化学习的顶点？</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pq l"><div class="pr l ps pt pu pq pv my ph"/></div></div></a></div><div class="pe pf gp gr pg ph"><a rel="noopener follow" target="_blank" href="/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd iu gy z fp pm fr fs pn fu fw is bi translated">强化学习算法的收敛性</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">有什么简单的收敛界限吗？</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pq l"><div class="pw l ps pt pu pq pv my ph"/></div></div></a></div><div class="pe pf gp gr pg ph"><a rel="noopener follow" target="_blank" href="/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd iu gy z fp pm fr fs pn fu fw is bi translated">强化学习的基本迭代方法</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">学习价值和策略迭代能掌握多少强化学习？很多。</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pq l"><div class="px l ps pt pu pq pv my ph"/></div></div></a></div><p id="13be" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">更多？订阅我关于机器人、人工智能和社会的时事通讯！</p><div class="pe pf gp gr pg ph"><a href="https://robotic.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd iu gy z fp pm fr fs pn fu fw is bi translated">自动化大众化</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">一个关于机器人和人工智能的博客，让它们对每个人都有益，以及即将到来的自动化浪潮…</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">robotic.substack.com</p></div></div><div class="pq l"><div class="py l ps pt pu pq pv my ph"/></div></div></a></div></div></div>    
</body>
</html>