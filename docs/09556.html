<html>
<head>
<title>Does CNN represent Convolutional Neural Networks or Correlational Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN 代表卷积神经网络还是相关神经网络？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/does-cnn-represent-convolutional-neural-networks-or-correlational-neural-networks-76c1625c14bd?source=collection_archive---------53-----------------------#2020-07-07">https://towardsdatascience.com/does-cnn-represent-convolutional-neural-networks-or-correlational-neural-networks-76c1625c14bd?source=collection_archive---------53-----------------------#2020-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="771d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">试图解开为什么我们在 CNN 中提到使用卷积，而实际上我们使用的是互相关…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/291f55405eee5643ee2bdbb981ba43c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvbMwfy6ozqDXvNUYndT3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://pixabay.com/vectors/squirrel-reading-books-surprise-304021/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><blockquote class="kz la lb"><p id="9ab1" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">CNN 或俗称的卷积神经网络(甚至 CovNets)是深度学习中使用的高效深度神经网络的关键成分之一，尤其是在处理非结构化数据时。但是我们在使用 CNN 的时候真的进行卷积运算吗？我知道数据科学和人工智能社区在有效使用 CNN 方面已经获得了很多专业知识，并且有像 Tensorflow，Keras 和 PyTorch 这样的框架，可以非常快速有效地使用 CNN 来构建深度学习模型？但是每当我们讲授 CNN 的概念时，为什么我们提到使用卷积的数学运算，而实际上我们做的是执行互相关！等等，你不相信我说的话？那么就让我们来深入探究一下这篇文章吧！</p></blockquote><p id="53c2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">对于一些正在阅读这篇文章的人来说，我在接下来的部分将要讨论的内容可能会让你大吃一惊！但是最近，我和我的一个朋友进行了一次大讨论，他试图让我相信卷积和相关运算实际上是相同的，这就是为什么我们仍然说 CNN 使用卷积，而不是相关运算，我强烈反对，因此我想表达我的想法和我的观点，CNN 实际上是相关的神经网络！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/cc6501e4828d6abad19b7e48f64c9649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Retc-7AQX6gnKo7OlCgqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:(<a class="ae ky" href="https://pixabay.com/vectors/co-workers-argument-argue-worker-294266/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>)</p></figure><h1 id="8cb9" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">卷积与相关滤波</h1><p id="32e8" class="pw-post-body-paragraph lc ld it lf b lg mv ju li lj mw jx ll lz mx lo lp ma my ls lt mb mz lw lx ly im bi translated">对于我们中的一些人来说，我们可能没有足够的时间重温我们很久以前学过的数学概念，所以我想重放数学术语卷积和相关性之间的区别，以便更好地论证:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/d75b6b0caafd08eaaee882b3c3890040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOwNKWwiYDx16DfhvrhklQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://medium.com/@aybukeyalcinerr/correlation-vs-convolution-filtering-2711d8bb3666" rel="noopener">相关 vs 卷积</a></p></figure><p id="6066" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">所以基本上当我们在计算机视觉或者图像处理相关的工作中做相关滤波的时候，我们通常会在图像上滑动相关滤波器的中心，然后将相关滤波器中的每个值乘以图像中的像素值，最后将这些乘积求和。那么这一系列的操作你听起来是不是很熟悉呢？什么事？是的，你是对的，这正是我们在对图像等非结构化数据应用 CNN 时所想做的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/1f2c59bd1cfac67dd583428896ed93cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*asNSGHjlDLqOW_pX.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://medium.com/@aybukeyalcinerr/correlation-vs-convolution-filtering-2711d8bb3666" rel="noopener">相关滤波步骤</a></p></figure><p id="f882" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我让你困惑了吗？那么让我来解释一下当我们尝试应用卷积滤波时会发生什么。像相关滤波一样，卷积滤波也是一种线性滤波，但是相关和卷积之间有很小的区别。在卷积滤波中，我们在两个维度(从下到上，从右到左)翻转内核或滤波器，然后执行与相关滤波相同的步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/cdc4cc98c2ff2bd27cf6384fffcd9d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/0*iaxA8Z-Eg6PO15EK.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://medium.com/@aybukeyalcinerr/correlation-vs-convolution-filtering-2711d8bb3666" rel="noopener">在卷积滤波中在两个维度上反转滤波器</a></p></figure><p id="3dd8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">产量有什么区别？</strong></p><p id="f8d7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">你们中的一些人可能会想，<em class="le">好吧，没关系，这可能只是术语上的缺陷，所以这有什么大不了的</em>？你们中的一些人可能会想，<em class="le">如果我们应用实际的卷积运算而不是伪装的相关操作符，会有什么变化呢</em>？</p><p id="38e2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了回答这些问题，让我给你看一些通过卷积和相关运算在图像上使用高斯模糊滤镜的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/b5f2d0eec1b5e8a9a4368ab913a984c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXucGNk9wO25WIaHlTR8_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在左边的基础图像上使用相关性应用高斯模糊滤镜(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/43af41a022a92344c700e1ca25db36b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2N9y05jx_aqtZehp4NufA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用卷积在左边的基础图像上应用高斯模糊滤镜(图片由作者提供)</p></figure><p id="195c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">从上面两幅图像中，我们可以清楚地看到，当我们使用卷积和相关对同一幅图像应用相同的滤波器时，输出有所不同。所以，现在问题来了，对于神经网络学习权重和偏差来说，哪一个是真正有用的？我在这里的答案是相关滤波，因为卷积滤波在某些情况下实际上可能会引入噪声，这可能会导致深度学习模型的误导性结果。</p><p id="b0e3" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">命名法中为什么会有这种谬误？</strong></p><p id="f3a8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">你可能想知道，即使我们在 CNN 中使用互相关，为什么我们仍然称它为卷积神经网络？是因为卷积听起来更复杂吗？还是 CovNet 看起来比 CorrNet 更时髦？嗯，这些原因可能是真的，但当我做我的研究时，我发现 CNN 的最开始被认为是源于论文<a class="ae ky" href="https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Neocognitron </strong> </a>，已知它使用卷积滤波，但后来的 CNN 和大多数 ML/DL 包的实现都使用相关滤波而不是卷积滤波，但我们仍然继续使用卷积神经网络的名称。虽然数学或算法的复杂性实际上几乎保持不变，但结果却有所不同。</p><p id="6feb" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，看完这篇文章，你会支持我把 CovNets 重命名为 CorrNets 的观点吗？或者至少支持我，当有人争论说 CovNets 和 CorrNets 实际上是一样的，当有明显的区别时？</p><p id="a36b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">你喜欢这篇文章吗？还想多看几篇同一主题的文章？那么请看看这个:<a class="ae ky" rel="noopener" target="_blank" href="/convolution-vs-correlation-af868b6b4fb5">卷积 Vs 相关</a>，<a class="ae ky" href="https://medium.com/@zhang_yang/convolutional-neural-networks-conv-cnn-correlation-or-convolution-5840b91c46a6#:~:text=The%20name%20Convolutional%20Neural%20Networks,is%20correlation%20that%20it's%20using." rel="noopener">卷积神经网络 Conv (CNN):相关还是卷积？</a>我们的媒体作者同事在强调差异方面做得很好，因此这些文章可能对你真的有帮助。</p><p id="4d41" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><em class="le">因此，这就把我们带到了本文的结尾。在</em> <a class="ae ky" href="https://aditya-bhattacharya.net/" rel="noopener ugc nofollow" target="_blank"> <em class="le">我的个人网站</em> </a> <em class="le">中，我正在尝试编写一个</em> <a class="ae ky" href="https://www.aditya-bhattacharya.com/?p=170" rel="noopener ugc nofollow" target="_blank"> <em class="le">深度学习词汇表</em> </a> <em class="le">，里面会有更多深度学习中用到的有趣概念的细节，我会在以后挑选一些更有趣的概念。在那之前，请鼓掌并激励我进行更多的讨论，并与社区分享我的发现。希望我能帮上忙！继续关注:</em><a class="ae ky" href="https://medium.com/@adib0073" rel="noopener"><em class="le"/></a><em class="le">和我的网站:</em><a class="ae ky" href="https://aditya-bhattacharya.net/" rel="noopener ugc nofollow" target="_blank">https://www.aditya-bhattacharya.net/</a></p><p id="45b0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">* *更新** </strong> : <strong class="lf iu"> <em class="le">我正在用 python 写一本关于应用计算机视觉的书，正在寻找可以在写作或审查内容和代码方面做出贡献的投稿人。我最感兴趣的是接收研究生，通过写一本书和维护一个开源项目来帮助他们成为应用计算机视觉专家！请看看我的这篇文章了解更多:</em> </strong> <a class="ae ky" href="https://medium.com/analytics-vidhya/become-an-applied-computer-vision-expert-in-a-fun-and-practical-way-b6c79335d4b1" rel="noopener"> <strong class="lf iu"> <em class="le">链接</em> </strong> </a> <strong class="lf iu"> <em class="le">。如果你想看看该倡议的早期努力，请看看我在</em></strong><a class="ae ky" href="https://www.youtube.com/watch?v=jHMNL5KOmjE" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu"><em class="le">YouTube</em></strong></a><strong class="lf iu"><em class="le">中关于应用计算机视觉的详细实践研讨会。如果你有兴趣成为其中的一员，请随时通过这里提到的任何交流方式联系我们:</em></strong><a class="ae ky" href="https://aditya-bhattacharya.net/contact-me/" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu"><em class="le">https://aditya-bhattacharya.net/contact-me/</em></strong></a><strong class="lf iu"><em class="le">带着你更新的简历。</em> </strong></p></div></div>    
</body>
</html>