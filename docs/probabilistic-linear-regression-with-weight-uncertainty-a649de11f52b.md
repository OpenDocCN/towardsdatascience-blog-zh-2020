# 权重不确定的概率线性回归

> 原文：<https://towardsdatascience.com/probabilistic-linear-regression-with-weight-uncertainty-a649de11f52b?source=collection_archive---------18----------------------->

## 用张量流概率实现贝叶斯线性回归预测汽车的 MPG

![](img/7696b002b2e7ad1c7ee11aafca472ec5.png)

[韦斯利·廷吉](https://unsplash.com/@wesleyphotography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/unsymmetry-line?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

线性回归可能是你在学习数据科学和机器学习时遇到的第一种统计方法。所以，我会抓住机会猜测这不是你第一次处理线性回归。因此，在本文中，我想讨论概率线性回归，而不是典型/确定性线性回归。

但是在此之前，让我们简单地讨论一下确定性线性回归的概念，让我们快速了解本文的主要观点。

线性回归是一种基本的统计方法，用于模拟一个或多个输入变量(或自变量)与一个或多个输出变量(或因变量)之间的线性关系。

![](img/66be8081964c4cf88761ead1f10b5a76.png)

上式中，`a`称为截距，`b`称为斜率。`x`是我们的自变量，`y`是我们的因变量，也就是我们试图预测的值。

需要使用梯度下降算法来优化`a` 和`b`的值。然后，我们得到一条回归线，显示自变量和因变量之间的最佳拟合。有了回归线，我们可以用任何给定的`x`输入来预测`y`的值。这些是典型或确定性线性回归算法通常是如何构建的步骤。

![](img/7ba922e3cba468fe2dd680b690417527.png)

确定性线性回归方法中最佳拟合线的典型图

然而，这种确定性的线性回归算法并不能真正说明数据和模型的全部情况。这是为什么呢？

实际上，当我们进行线性回归分析时，会出现两种类型的不确定性:

*   随机不确定性，即由数据产生的不确定性。
*   认知不确定性，这是由回归模型产生的不确定性。

在我们阅读这篇文章的时候，我将详细阐述这些不确定性。为了考虑这些不确定性，应使用概率线性回归代替确定性线性回归。

在本文中，我们将讨论概率线性回归以及它与确定性线性回归的区别。我们将首先看到如何在 TensorFlow 中构建确定性线性回归，然后我们将继续构建具有 TensorFlow 概率的概率性线性回归模型。

首先，让我们从加载我们将在本文中使用的数据集开始。

# 加载和预处理数据

本文将使用的数据集是[汽车的 MPG 数据集](https://www.kaggle.com/uciml/autompg-dataset)。像往常一样，我们可以用熊猫加载数据。

![](img/2a77cdd2eb232fc4e49e3eac3cb4da62.png)

下面是数据的统计汇总。

![](img/de7269bbe3caea567ef75eccc4ce2755.png)

接下来，我们可以用下面的代码来看看数据集中变量之间的相关性。

![](img/a85795ad9837b393efc76026f1f436e2.png)

现在，如果我们看看相关性，汽车的每加仑英里数(MPG)和汽车的重量有很强的负相关性。

在本文中，我将做一个简单的线性回归分析，以实现可视化。自变量将是汽车的重量，因变量将是汽车的英里数。

现在，让我们用 Scikit-learn 将数据拆分为训练数据和测试数据。拆分数据后，我们现在可以缩放因变量和自变量。这是为了确保这两个变量将在相同的规模，这也将提高我们的线性回归模型的收敛速度。

现在，如果我们将训练数据可视化，我们会得到以下可视化结果:

![](img/319f4b8b230b1b8f90ad563e97865b5d.png)

厉害！接下来，让我们继续用 TensorFlow 构建确定性线性回归模型。

# 张量流的确定性线性回归

用 TensorFlow 建立一个简单的线性回归模型是非常容易的。我们所要做的就是建立一个没有任何激活函数的单一致密层。对于成本函数，通常使用均方误差。在这个例子中，我将使用 RMSprop 作为优化器，模型将在 100 个时期内被训练。我们可以用下面几行代码来构建和训练模型。

在我们训练了模型之后，让我们看看模型的损失历史来检查损失收敛。

![](img/5f3eb992143bc592de8f57c9bcd4d2a2.png)

损失 v 历元数

损失似乎已经收敛了。现在，如果我们使用训练好的模型来预测测试集，我们可以看到下面的回归线。

![](img/7356e9015f09431631042c2337d191c0.png)

仅此而已。我们完了！

正如我前面提到的，用 TensorFlow 建立一个简单的线性回归模型是非常容易的。有了回归线，我们现在可以在任何给定的汽车重量输入下近似汽车的 MPG。举个例子，假设汽车在特征缩放后的重量是 0.64。我们可以通过将此值传递给训练好的模型来获得汽车的 MPG 的相应值，如下所示。

![](img/a7880702b2a7fea6482586b5a783adb4.png)

现在你可以看到，模型预测汽车的 MPG 将是 0.21。简单地说，对于任何给定的汽车重量，我们得到一个单一的确定性的汽车的 MPG 值

然而，这个产值并不能真正说明全部情况。这里有两点需要注意。首先，我们只有有限的数据点。第二，从线性回归图可以看出，大部分数据点并没有真正位于回归线上。

虽然我们得到的输出值是 0.21，但我们知道实际汽车的 MPG 并不精确地是 0.21。可能略低于这个数字，也可能略高于这个数字。换句话说，有一种不确定性需要考虑进去。这种不确定性被称为任意不确定性。

确定性线性回归无法捕捉数据的这种任意不确定性。为了捕捉这种随机的不确定性，可以改为应用概率线性回归。

# **具有张量流概率的概率线性回归**

得益于 TensorFlow Probability，建立概率线性回归模型也非常容易。但是，你需要先安装`tensorflow_probability`库。您可以使用 pip 命令进行安装，如下所示:

```
pip install tensorflow_probability
```

安装这个库的先决条件是需要有 TensorFlow 版本 2.3.0。因此，请确保在安装 TensorFlow Probability 之前升级您的 TensorFlow 版本。

## 建立随机不确定性的概率线性回归模型

在这一节中，我们将建立一个概率线性回归模型，将随机不确定性考虑在内。

该模型非常类似于确定性线性回归。然而，不是像以前一样只使用一个单一的致密层，我们需要增加一层作为最终层。最后一层将最终输出值从确定性分布转换为概率分布。

在本例中，我们将创建一个最终图层，将输出值转换为正态分布的概率值。下面是它的实现。

注意，我们在张量流概率层的最后增加了一层。该层将把先前密集层的两个输出(一个用于平均值，一个用于标准偏差)转换成概率值，该概率值正态分布有可训练的平均值( *loc* )和标准偏差(*标度*)。

我们可以使用 RMSprop 作为优化器，但是如果您愿意，也可以使用其他优化器。对于损失函数，我们需要使用负对数似然。

但是为什么我们使用负对数似然作为损失函数呢？

## 负对数似然作为成本函数

为了使分布符合某些数据，我们需要使用似然函数。利用似然函数，我们尝试根据我们在数据中看到的模式来估计未知参数 *θ* (例如，正态分布数据的平均值和标准差)。

![](img/6e6030532e9875ac65a817452a70e6cd.png)

在我们的概率回归模型中，优化器的工作是找到未知参数的最大似然估计。换句话说，模型被训练以从我们的数据中找到给定模式的最可能的参数值。

![](img/c5d4dd59f005c9c6d175abb1f9e4fd89.png)

最大化似然估计与最小化负对数似然是一样的。在优化领域，目标通常是最小化成本，而不是最大化成本。这就是为什么我们使用负对数似然作为我们的成本函数。

下面是负对数似然作为我们的自定义损失函数的实现。

## 任意不确定性概率线性回归模型的训练和预测结果

既然我们已经构建了模型并定义了优化器和损失函数，让我们编译和训练模型。

现在我们可以从训练好的模型中抽取样本。我们可以用下面的代码来可视化测试集和从模型中生成的样本之间的比较。

![](img/3a4697a701184746ad8803c37251fa3e.png)

测试数据 v 从概率线性回归模型生成的样本

从上面的可视化中可以看出，对于任何给定的输入值，模型都不会返回确定性的值。相反，它将返回一个分布，并根据该分布绘制一个样本。

如果将测试集的数据点(蓝点)与训练模型预测的数据点(绿点)进行比较，您可能会认为绿点与蓝点来自相同的分布。

接下来，在给定训练集中的数据的情况下，我们还可以可视化由训练模型生成的分布的均值和标准差。我们可以通过应用下面的代码来做到这一点。

![](img/fec803f4fe5d5db8e5f5750020e9b0d2.png)

我们可以看到，概率线性回归模型给我们的不仅仅是回归线。它还给出数据标准偏差的近似值。可以看出，测试集的大约 95%的数据点位于两个标准偏差内。

## 建立随机和认知不确定性的概率线性回归模型

到目前为止，我们已经建立了一个概率回归模型，它考虑了来自数据的不确定性，或者我们称之为随机不确定性。

然而，在现实中，我们还需要处理来自回归模型本身的不确定性。由于数据的不完善，回归参数的权重或斜率也存在不确定性。这种不确定性被称为认知不确定性。

到目前为止，我们建立的概率模型只考虑了一个确定性权重。正如您从可视化中看到的，该模型仅生成一条回归线，通常这并不完全准确。

在这一节中，我们将改进我们的概率回归模型，将任意的和认知的不确定性都考虑在内。我们可以使用贝叶斯观点来引入回归权重的不确定性。

首先，在我们看到数据之前，我们需要定义我们对体重分布的先验信念。通常，我们不知道会发生什么，对吗？为了简单起见，让我们假设权重的分布是正态分布，平均值等于 0，标准差等于 1。

因为我们硬编码了均值和标准差，这种先验信念是不可训练的。

接下来，我们需要定义回归权重的后验分布。后验分布显示了在看到数据中的模式后，我们的信念是如何改变的。因此，这个后验分布中的参数是可训练的。下面是定义后验分布的代码实现。

现在的问题是，上面后验函数中定义的这个`VariableLayers`是什么？这个可变层背后的想法是，我们试图近似真实的后验分布。通常，不可能得到真实的后验分布，因此我们需要近似它。

在定义了先验和后验函数之后，现在我们可以建立权重不确定的概率线性回归模型。下面是它的代码实现。

你可能注意到了，这个模型和之前的概率回归模型唯一的区别只是第一层。我们使用`DenseVariational`层，而不是普通的密集层。在这一层，我们将先验和后验函数作为自变量传递。第二层和前面的模型一模一样。

## 随机和认知不确定性的概率线性回归模型的训练和预测结果

现在是我们编译和训练模型的时候了。

优化器和成本函数仍然与之前的模型相同。我们使用 RMSprop 作为优化器，使用负对数似然作为成本函数。我们来编译训练或者建模。

现在是时候让我们可视化回归模型的权重或斜率不确定性了。下面是可视化结果的代码实现。

![](img/f20f2326d2c2095086f94e40da5322b8.png)

在上面的可视化中，您可以看到线性线(平均值)以及由训练模型的后验分布生成的标准偏差在每次迭代中都是不同的。所有这些线都是拟合测试集中数据点的合理解决方案。然而，由于认知的不确定性，我们不知道哪条线是最好的。

通常，我们拥有的数据点越多，我们将看到的回归线的不确定性就越小。

# 最后的想法

就是这样！现在，您已经看到了概率线性回归与确定性线性回归的不同之处。使用概率线性回归，可以考虑由数据(任意的)和回归模型(认知的)产生的两种不确定性。

如果我们想要建立一个深度学习模型，其中不准确的预测会导致非常严重的负面后果，例如在自动驾驶和医疗诊断领域，考虑这些不确定性是非常重要的。

通常，当我们有更多的数据点时，模型的认知不确定性会降低。