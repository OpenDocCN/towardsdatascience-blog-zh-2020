<html>
<head>
<title>The Exploration-Exploitation Trade-off</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">勘探-开发权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-exploration-exploitation-trade-off-7bc369027ba1?source=collection_archive---------35-----------------------#2020-01-28">https://towardsdatascience.com/the-exploration-exploitation-trade-off-7bc369027ba1?source=collection_archive---------35-----------------------#2020-01-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2fa0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习导论</h2></div><p id="19e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">探索和开发的思想是设计一个<strong class="kk iu">便利的</strong>强化学习系统的核心。“权宜”一词是一个从学习自动机理论改编的术语，指的是一个系统，其中代理(或自动机)学习随机环境的动力学。换句话说，代理人学习了一个在随机环境中制定行动的策略，这个策略比纯粹的机会要好。</p><p id="cbb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练代理人在随机环境中学习时，探索和开发的挑战会立即出现。当代理在反馈框架中与环境交互时，它接收奖励。为了最大化其回报，代理人通常会重复过去尝试过的产生“有利”回报的行为。但是，为了找到这些导致奖励的行为，代理必须从一组行为中进行抽样，并尝试以前没有选择的不同行为。请注意这个想法是如何从行为心理学的“效果法则”中很好地发展而来的，在行为心理学中，一个代理人加强了对产生回报的行为的心理联系。这样做时，代理还必须尝试以前未选择的动作；否则，它将无法发现更好的行动。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/0b76032a2e11ffc169546718002ddc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tokM84zupGqMNMM3.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">强化学习反馈框架。</strong>一个智能体反复地与环境交互，并学习一个从环境中获取最大长期回报的策略。</p></figure><p id="6e0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">探索是指代理人为了获得更好的回报，不得不从一组行为中抽取行为样本。另一方面，剥削是指一个代理人利用他已经知道的东西重复行动，从而获得“有利的”长期回报。设计强化学习系统的关键挑战是平衡探索和利用之间的平衡。在一个随机的环境中，行为必须被足够好地采样以获得一个期望的回报估计。一个专门从事勘探或开发的代理人注定不会是权宜之计。它变得比纯粹的偶然性(即随机化的代理)更糟糕。</p><h1 id="e542" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">多武装匪徒</h1><p id="0c31" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在多武装匪徒问题(MAB)(或n武装匪徒)中，代理从一组动作中做出选择。这种选择导致基于所选动作的来自环境的数字奖励。在这种特定情况下，环境的性质是一种平稳的概率分布。所谓稳定，我们是指概率分布在环境的所有状态下都是恒定的(或独立的)。换句话说，概率分布不会随着环境状态的改变而改变。在MAB问题中，代理人的目标是在特定时期内从环境中获得最大的回报。</p><p id="a0d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MAB问题是“独臂强盗”问题的延伸，它被表示为赌场中的老虎机。在MAB的设置中，我们有多杆的，而不是单杆的老虎机。每个杠杆对应于代理可以执行的一个动作。代理的目标是从机器中进行最大化其赢得物(即奖励)的游戏。代理人必须找出最佳杠杆(探索)，然后专注于能使其回报(即回报总和)最大化的杠杆(开发)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ms"><img src="../Images/1bed86d3b0d1f3671dc43f5e0201c22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DIuaF_oP5YlZMufB.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">左:独臂强盗。</strong>吃角子老虎机有一个控制杆，玩的时候会返回一个数字奖励。<br/> <strong class="bd lu">右:多臂土匪。</strong>吃角子老虎机有多个(n)分支，每个分支在玩时返回一个数字奖励。在MAB问题中，强化代理人必须平衡勘探和开采，以使回报最大化。</p></figure><p id="1997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器上的每一个动作(即杠杆)，都有一个预期的回报。如果代理人知道这个期望报酬，那么问题就变成了一个无关紧要的问题，它仅仅涉及到选择具有最高期望报酬的行动。但是，由于杠杆的预期回报是未知的，我们必须核对估计，以了解每个行动的可取性。为此，代理将不得不<strong class="kk iu">探索</strong>以获得每个动作的平均奖励。之后，它可以<strong class="kk iu">利用</strong>它的知识，选择一个预期回报最高的行动(这也称为选择贪婪行动)。正如我们所看到的，代理人必须平衡探索和开发行为，以最大化整体长期回报。</p><h1 id="3965" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文献学</h1><ul class=""><li id="9f78" class="mt mu it kk b kl mn ko mo kr mv kv mw kz mx ld my mz na nb bi translated">纳伦德拉，K. S .，&amp; Thathachar，硕士(2012)。学习自动机:导论。快递公司。</li><li id="2d9b" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">萨顿和巴尔托(1998年)。强化学习:导论。麻省理工出版社。</li></ul></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="3a73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="no">最初发表于</em><a class="ae np" href="https://ekababisong.org/the-exploration-exploitation-trade-off/" rel="noopener ugc nofollow" target="_blank"><em class="no">https://ekababisong.org</em></a><em class="no">。</em></p></div></div>    
</body>
</html>