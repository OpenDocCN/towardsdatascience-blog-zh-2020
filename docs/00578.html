<html>
<head>
<title>Understanding HDBSCAN and Density-Based Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解HDBSCAN和基于密度的聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-hdbscan-and-density-based-clustering-121dbee1320e?source=collection_archive---------2-----------------------#2020-01-17">https://towardsdatascience.com/understanding-hdbscan-and-density-based-clustering-121dbee1320e?source=collection_archive---------2-----------------------#2020-01-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5326a12d7f96fd82c5f12c676f548afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JD17IVNk-h_Y1V66hGaXGA.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="4012" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">自上而下全面介绍HDBSCAN聚类算法的内部工作原理以及基于密度的聚类的关键概念</h2></div><p id="ae16" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">HDBSCAN是由<a class="ae lm" href="http://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14" rel="noopener ugc nofollow" target="_blank"> Campello、Moulavi和Sander </a> [8]开发的一种聚类算法。它代表“<em class="ln">带噪声的应用的基于层次密度的空间聚类”</em></p><p id="b516" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这篇博文中，我将尝试以自上而下的方式介绍一些关键概念，以帮助理解HDBSCAN的工作方式和原因。这是为了补充现有的文档，如sklearn的“【HDBSCAN如何工作”[1]，以及麦金尼斯和希利的其他作品和演示文稿[2]，[3]。</p><h2 id="e114" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">除了一些噪音，没有(很少)假设</h2><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/51fb5a233b367cac46b53d480f724ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCs1-C6TMWSjCL8kcvx4Ww.jpeg"/></div></div></figure><p id="dcf6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们从最上面开始。在我们描述我们的聚类算法之前，我们应该问，<em class="ln">“我们要聚类什么类型的数据？”</em></p><p id="2ecd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们希望对我们的数据有尽可能少的假设。也许我们唯一可以安全做出的假设是:</p><ul class=""><li id="5911" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">我们的数据中有噪音</li><li id="9944" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">在我们的数据中有我们希望发现的聚类</li></ul><h2 id="47ed" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">聚类数据集</h2><p id="9c12" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">为了激发我们的讨论，我们从[1]和[3]中使用的<a class="ae lm" href="https://github.com/lmcinnes/hdbscan/blob/master/notebooks/clusterable_data.npy" rel="noopener ugc nofollow" target="_blank">数据集</a>开始。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/cfd393932dd1d267a9fa883aee8730e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36x7yPCJGVrKnogBpE2n4w.png"/></div></div></figure><p id="7478" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">只有二维，我们可以绘制数据，并在我们的数据集中识别6个“自然”集群。我们希望通过一些聚类算法来自动识别这些。</p><h2 id="c9b8" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">k均值与HDBSCAN</h2><p id="f849" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">知道了预期的聚类数，我们运行经典的K-means算法，并将结果标签与使用HDBSCAN得到的标签进行比较。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/cd95f0281bea12cb2daac75fe1dc0083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-hr07E_ygPJEqDXgaoGQA.png"/></div></div></figure><p id="42d9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">即使提供了正确数量的聚类，K-means也明显无法将数据分组到有用的聚类中。另一方面，HDBSCAN为我们提供了预期的聚类。</p><h2 id="5d22" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">K-means为什么会失败？</h2><p id="2496" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">简而言之，<strong class="ks jc"> K-means表现不佳，因为不满足对聚类形状的基本假设</strong>；这是一个参数算法，由<em class="ln"> K个簇形心、</em>高斯球的中心来参数化。当聚类满足以下条件时，K-means表现最佳:</p><ul class=""><li id="c157" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">“圆形”或球形</li><li id="f5d9" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">大小相等</li><li id="ded1" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">同等密集</li><li id="36d7" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">球体中心密度最大</li><li id="46ef" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">未被噪音/异常值污染</li></ul><p id="f6a0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们从ESLR [4]借用一个简单的例子来说明K-means如何对簇的形状敏感。下面是来自相同数据的两个聚类。在左边，数据在聚类前被标准化。没有标准化，我们会得到一个“错误的”聚类。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/8edf68c2c0373e5a5fdb889cb8cda41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzRRGby6vq6buR1SlzGaJg.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">图14.5摘自《ESLR》第十四章[4]。标准化数据的聚类(左)与原始数据的聚类(右)。</p></figure><h2 id="bd32" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">我们的数据有什么特点？</h2><p id="6aec" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">我们回到我们的原始数据集，通过简单地描述它，很明显为什么K-means有困难。该数据集具有:</p><ul class=""><li id="b76b" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">任意形状的簇</li><li id="2419" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">不同大小的集群</li><li id="b362" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">不同密度的集群</li><li id="3084" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">一些噪音和一些异常值</li></ul><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/e79525c1e2df69f5c0ef3fc7930993ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHCw-IeJvNWSm4iq0UkZNg.png"/></div></div></figure><h2 id="99cc" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">需要稳健的数据探索</h2><p id="f511" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">虽然每个要点都可以从真实世界的数据集中合理地得到，但每个要点对于参数算法(如K-means)来说都是有问题的。在信任算法的输出之前，我们可能需要检查算法的假设是否成立。但是，当对数据知之甚少时，检查这些假设可能是困难的。这是不幸的，因为聚类算法的主要用途之一是数据探索，我们仍在理解数据的过程中</p><p id="0ac4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">因此，将用于数据探索的聚类算法需要尽可能少的假设，以便我们获得的初始洞察是“有用的”；更少的假设使其更加稳健，适用于更广泛的真实世界数据。</p><h1 id="f963" class="nn lp jb bd lq no np nq lt nr ns nt lw kh nu ki lz kk nv kl mc kn nw ko mf nx bi translated">稠密区和多元模式</h1><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/bf314a51a4c6d5c0d11c8f41e6fc5c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Edn3kjSI6G9u5bceAzWQ2w.jpeg"/></div></div></figure><p id="a838" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在，我们知道了我们要处理的是什么类型的数据，让我们来探讨一下HDBSCAN的核心思想，以及它在数据具有以下特征时如何表现出色:</p><ul class=""><li id="653e" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">任意形状的簇</li><li id="4a23" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">不同大小和密度的集群</li><li id="c060" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">噪音</li></ul><p id="8264" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">HDBSCAN使用基于密度的方法，该方法很少对聚类进行隐含假设。它是一种非参数方法，寻找由基础分布的多元模式形成的聚类层次。它不是寻找具有特定形状的簇，而是寻找比周围空间更密集的数据区域。你可以使用的心理图像是试图将岛屿从海洋中分离出来，或者将山脉从山谷中分离出来。</p><h2 id="8cac" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">什么是集群？</h2><p id="3d99" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">我们如何定义“集群”？我们直觉认为的集群的特征可能很难定义，并且通常是特定于上下文的。(概述见克里斯蒂安·亨宁的演讲[5])</p><p id="2f9b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果我们回到原始数据集，我们识别簇的原因是我们看到被稀疏和嘈杂空间包围的6个密集区域。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/78a6a6ded2ffa94b94a8254eba949373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eStGcmNGVN3-WC2IcEDY4A.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">被包围的区域密度很高</p></figure><p id="f61e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">一种通常与我们对集群的直观概念一致的定义集群的方式是:<em class="ln">由稀疏区域分隔的高密度区域。</em></p><p id="a03b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请看一维模拟数据的曲线图。我们可以看到3个集群。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/08becc54f0817543e932ef0b10edc51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyD-oZmG6tGcAAXyxrn72g.png"/></div></div></figure><h2 id="bac6" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">看看下面的分布</h2><p id="6515" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated"><em class="ln"> X </em>是来自混合正态分布的模拟数据，我们可以画出X的精确概率分布</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/38c8cf4275497f93d8f2ca1cc1dcc9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*naEKid6E2eO43jgLsIhGmA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">峰值=密集区域。波谷=稀疏区域</p></figure><p id="30b1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">波峰对应于最密集的区域，波谷对应于稀疏的区域。这给了我们另一种解决问题的方法，假设我们知道潜在的分布，<em class="ln">簇是被不可能的区域分开的高度可能的区域。</em>想象更高维的概率分布形成了<em class="ln">山脉和山谷的景观，</em>其中<em class="ln">山脉是你的集群</em>。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/df7a91f2be5734ad0838381ce1af1d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3kun_Pxbmgn6S_-ZzHZAA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">给三座山峰/山脉/集群着色</p></figure><p id="3d40" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于不太熟悉的人来说，这两种说法实际上是一样的:</p><ul class=""><li id="6f5c" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated"><em class="ln">由稀疏区域分隔的高密度区域</em></li><li id="dceb" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated"><em class="ln">被不太可能的区域分开的高度可能的区域</em></li></ul><p id="1ece" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">一种是通过概率分布来描述数据，另一种是通过分布中的随机样本。</p><p id="8670" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">PDF图和上面的带状图是等效的。PDF，<em class="ln">概率密度函数，</em>解释为位于某点周围小区域内的概率，从<em class="ln"> X </em>看样本时，也可以解释为该点周围的期望密度。</p><p id="939b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">给定基础分布，我们预计在随机样本中，更有可能的区域往往会有更多的点(更密集)。同样，给定一个随机样本，你可以根据经验密度推断出一个区域的概率。</p><p id="8c59" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc">随机样本中更密集的区域对应于基础分布中更可能的区域。</strong></p><p id="cbc4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">事实上，如果我们观察X的随机样本的直方图，我们会发现它看起来与X的真实分布完全一样。直方图有时被称为<em class="ln">经验概率分布，</em>有了足够的数据，我们希望直方图收敛到真实的基本分布。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/76888f5fb5d3c516801701c89085cb94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W75kS8rV_3sVmfOMojPDFA.png"/></div></div></figure><p id="6919" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">还是那句话，密度=概率。密度越大=可能性越大。</p><h2 id="22f4" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">但是……什么是集群？</h2><p id="3e91" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">可悲的是，即使有了我们对集群的定义，也很难知道某个东西是否是一个单独的集群。看看下面的例子，我们把X的一个模式向右移动了。虽然我们还有3个峰值，但是我们有3个集群吗？在某些情况下，我们可以考虑3个集群。“直觉上”我们说只有两个集群。我们如何决定？</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/b7a2778c5672ee563ef2bba3a193c732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ryn4FCawiYe966f9sCpW0A.png"/></div></div></figure><p id="4773" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">通过观察<em class="ln">X’，</em>的带状图，我们可以更加确定只有两个星团。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/f0ceb328e87285fae504e852387ba990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8l_UQbLhcCS5zF17YNG9A.png"/></div></div></figure><p id="98d8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="ln"> X </em>有3个簇，<em class="ln">X’</em>有2个簇。集群的数量在什么时候会发生变化？</p><p id="7eb5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">定义它的一种方法是为底层分布的PDF设置一些全局阈值。所得水平集的连通分量就是你的聚类[3]。这就是DBSCAN算法所做的，在多个层次上做将导致DeBaCl [7]。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/925ab623194605606185114c957c6ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VPRd0PAv382y8rb0wBJmA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">基于两个不同水平集的两个不同聚类</p></figure><p id="6d2b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这可能因为它的简单而吸引人，但是不要被愚弄了！我们最终得到一个额外的超参数，<em class="ln">阈值𝜆，</em>，我们可能需要对其进行微调。此外，这对于具有不同密度的集群来说不太适用。</p><p id="fb60" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了帮助我们选择，我们给我们的聚类选择涂上颜色，如下图所示。我们应该只考虑<em class="ln">蓝色</em>和<em class="ln">黄色、</em>或<em class="ln">绿色吗？</em></p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/3c03fe692ded65520b145f5d300a1a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rylw8CoEoujh61f4D8v-RQ.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">左侧有3个集群，右侧有2个集群</p></figure><p id="ac03" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">要选择，我们看哪一个更“坚持”。我们看到他们在一起多还是分开多？我们可以用彩色区域的面积来量化。</p><p id="cdcb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在左侧，我们看到蓝色区域<em class="ln">和黄色区域</em>和黄色区域<em class="ln">的面积之和大于绿色区域<em class="ln">和绿色区域</em>的面积之和<em class="ln">。</em>这意味着两个峰值更加突出，因此我们决定它们是两个独立的集群。</em></p><p id="c79b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在右边，我们看到绿色的面积要大得多。这意味着它们只是“凸起”而不是峰值。所以我们说它们只是一个集群。</p><p id="01f9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在文献[2]中，这些区域的面积是<em class="ln">持续时间、</em>的度量，这种方法称为<code class="fe oe of og oh b">eom</code>或<em class="ln">质量过剩</em>。更正式的说法是<em class="ln">，我们在所选聚类不重叠的约束下，最大化聚类的持久性总和。</em></p><h2 id="5fa3" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">构建层次结构</h2><p id="4ce0" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">通过在<em class="ln"> 𝜆 </em>的不同值处得到多个水平集，我们得到一个层次结构。对于多维设置，想象星团是海洋中间的岛屿。随着你降低海平面，岛屿将开始“生长”,最终岛屿将开始彼此连接。</p><p id="2e68" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了能够捕捉和表示集群(岛)之间的这些关系，我们将其表示为<em class="ln">一个层次树</em>。这种表示推广到更高的维度，是一种自然的抽象，更容易表示为我们可以遍历和操作的数据结构。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/28c7f24fb8fd2b39cac2967bbbaccb9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Afnvx3A3hPOM3eoT8l9dDg.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">将集群层次结构可视化为树</p></figure><p id="641c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">按照惯例，树是自上而下绘制的，其中根(所有东西都是一个集群的节点)在顶部，树向下生长。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/1e928e3d321271fbe5b980ae6d390d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3QwinXi8M0hD7uBRklWfw.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">自上而下可视化树</p></figure><p id="95e3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果您正在使用HDBSCAN库，您可能会使用<code class="fe oe of og oh b">clusterer.condensed_tree_.plot()</code> API。下面显示的结果与上面显示的结果相同。圈出的节点对应于所选择的簇，分别是<em class="ln">黄色、蓝色</em>和<em class="ln">红色</em>区域。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/fe226fca737084cc22b5f761895fb566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkOGMAIcE7MbJ94tNVvk1w.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">来自HDBSCAN的压缩树图</p></figure><p id="a713" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">使用HDBSCAN时，这个特定的图可能有助于评估集群的质量，并有助于微调超参数，我们将在<em class="ln">“参数选择”</em>部分讨论。</p><h1 id="c62c" class="nn lp jb bd lq no np nq lt nr ns nt lw kh nu ki lz kk nv kl mc kn nw ko mf nx bi translated">局部近似密度</h1><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/0364b5b153e05f763dffd9b7afff9845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4CGv35I1gEGxouqwYT-erQ.jpeg"/></div></div></figure><p id="1640" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在上一节中，我们访问了底层发行版的真实PDF。然而，<strong class="ks jc">对于真实世界的数据，基本分布几乎总是未知的。</strong></p><p id="0529" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">因此，我们必须使用经验密度来估计PDF。我们已经讨论了一种方法，使用直方图。然而，这只对一维数据有用，并且随着维数的增加，计算变得困难。</p><p id="c459" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们需要其他方法来得到经验概率密度函数。这里有两种方法:</p><ul class=""><li id="460d" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">计算𝜀-radius内特定点的邻居数量</li><li id="68da" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">寻找到第K个最近邻居的距离(这是HDBSCAN使用的)</li></ul><h2 id="5f1b" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">计算𝜀-radius境内的邻居</h2><p id="ad98" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">对于每个点，我们围绕该点画一个𝜀-radius超球，并计算其中的点数。这是我们对空间中该点密度的局部近似。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/d3c273172068abd96208a3fdbfceac26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUj8YbgbJQ4Y9WN5qaslGQ.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">使用邻居计数估计概率密度函数</p></figure><p id="4886" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们对每个点都这样做，并将估计的PDF与PDF的真实值进行比较(我们现在才这样做，因为我们模拟了数据，它的分布是我们定义的)。</p><p id="598a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于我们的一维模拟数据，邻居计数与PDF的真实值高度相关。邻居的数量越多，估计的PDF就越高。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/d50ca2bbd23693da0060728b26a8a709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4N1thiiBJQMcXifU5YOJkw.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">使用邻居计数eps = 0.1估计X的PDF</p></figure><p id="dc80" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们看到，这种方法可以很好地估计模拟数据x的PDF。请注意，这可能对数据规模和样本大小很敏感。您可能需要迭代𝜀的几个值才能获得好的结果。</p><h2 id="f42b" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">到第K个最近邻居的距离</h2><p id="02cf" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">在这个例子中，我们得到了前面方法的补充。我们不是设置𝜀然后计算邻居，而是确定我们想要的邻居数量，并找到包含这k个邻居的𝜀的最小值。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/827d9dfdfbd82e8c248db429302c2f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8G4xh5x3W5Pit2Yke22YZw.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">K = 7时的核心距离</p></figure><p id="6d6f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">结果就是我们在HDBSCAN中所说的<em class="ln">核心距离</em>。核心距离较小的点位于密度较大的区域，因此PDF的估计值较高。具有更大核心距离的点位于更稀疏的区域，因为我们必须行进更大的距离来包含足够多的邻居。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/3baf0a484ef97dd96639a813404cbec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdP5uOogCgYfBZR4jeRvUg.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">使用核心距离估算X的PDF，其中K = 100</p></figure><p id="89f0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们尝试在模拟数据x上估计PDF。在上面的图中，我们使用<code class="fe oe of og oh b">1/core_distance</code>作为PDF的估计值。正如所料，估计值与真实的PDF高度相关。</p><p id="5ef3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">虽然以前的方法对数据的规模和数据集的大小都很敏感，但这种方法主要对数据集的大小敏感。如果您均等地缩放每个维度，那么所有核心距离将成比例地增加。</p><p id="e89c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里的关键要点是:</p><ul class=""><li id="dd8e" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">核心距离=密度估计值</li><li id="645f" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">(回想一下)密度=概率</li><li id="b47d" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated"><strong class="ks jc">核心距离PDF的一些估计值</strong></li></ul><p id="1d58" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">所以当我们提到一个点的<em class="ln">核心距离</em>时，你可以想到隐含地提到<em class="ln"> PDF。</em>基于核距离过滤点类似于从底层分布获得水平集。</p><p id="7d2e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">每当我们有<code class="fe oe of og oh b">core_distance ≤ 𝜀</code>，就有一个隐含的<code class="fe oe of og oh b">pdf(x) ≥ 𝜆</code>在发生。𝜀和𝜆之间总是有一个映射，为了简单起见，我们将只使用符号𝜆来表示核心距离和PDF。</p><h2 id="5d7e" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">找到水平集并给区域着色</h2><p id="950a" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">回想一下，在前面的例子中，我们从PDF中得到一个水平集，得到的区域就是我们的聚类。这很容易，因为一个区域被表示为某种形状。但是当我们处理点的时候，我们怎么知道不同的区域是什么呢？</p><p id="4241" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">左边是一个小数据集，右边是相应的PDF。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/d8df21fbcee032b02ff17a870bb4370a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOzW8Z2GTsgpr4VqhaBL7Q.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">PDF不“准确”</p></figure><p id="7538" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">第一步是找到某个𝜆的水平集。我们用<code class="fe oe of og oh b">core_distance ≤ 𝜆</code>过滤区域<code class="fe oe of og oh b">pdf(x) ≥ 𝜆</code>或者过滤点。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/1198b5461071127b6b8dd680a6fd777c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_xjGOqczwUNWcF5ZjUTaeg.png"/></div></div></figure><p id="ae87" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在我们需要找到不同的区域。这是通过将“附近”的点相互连接来实现的。“附近”是由𝜆定义的当前密度水平决定的，如果两个点的欧几里德距离小于𝜆.，我们说这两个点足够近</p><p id="d111" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们围绕每个点画一个半径为𝜆的球体。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/bf619e40a6fa97ab9383f1bc15ee6148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSFhV6nn1lWVU7xEyDXXsw.png"/></div></div></figure><p id="7968" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们将该点连接到其𝜆-球内的所有点。如果两个点相连，则它们属于同一区域，并且应该具有相同的颜色。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/624070aff0f5009bbf078c9c9a404544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjZXxMyzso5nHAkK51KmWQ.png"/></div></div></figure><p id="3e69" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对每个点都这样做，我们剩下的是几个相连的部分。这些是我们的集群。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/ec3da1bd3524d843681ff03759b43cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wa8Eb7rXSsXmaiLchqptbA.png"/></div></div></figure><p id="a8d6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这是你在某个水平集上得到的聚类。我们继续“降低海平面”,并跟踪新的星团出现，一些星团增长，最终一些合并在一起。</p><h2 id="2c99" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">降低海平面</h2><p id="2d30" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">这里有四个可视化，我们在4个不同的水平集显示4个集群。我们跟踪不同的集群，以便能够构建我们之前讨论过的层次树。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/f2fcd0f08215cccbf37f288e011f68d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UT2qWJOD3nKf4N-IQPulUw.png"/></div></div></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/e3856a59e21bcc93c0b72707a91dea00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXzGZAETjpOwtj5Lbb2o3Q.png"/></div></div></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/63539823779008f8563402ce60082a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y07eE6ByvPBekB5NreD-Yw.png"/></div></div></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/d164ef9c128b20f0d9272e5e69169fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcZ77cv00XTA590C_jJOfQ.png"/></div></div></figure><h2 id="c633" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">定义新的距离度量</h2><p id="77bd" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">我想强调的是，点可以在𝜆-球内，但它们仍然不会连接。它们必须首先包含在水平集内，因此对于要考虑的点来说，<em class="ln"> 𝜆 </em>应该大于其核心距离。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/0f4c868cfa87a6ef62dab1d057b4bdf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAgbLxwU5Rvia9RbvN804A.png"/></div></div></figure><p id="7e29" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">两点最终连接的<em class="ln"> 𝜆 </em>的值可以解释为某个新的距离。对于要连接的两点，它们必须是:</p><ul class=""><li id="e72e" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll mr ms mt mu bi translated">在足够密集的区域</li><li id="2ea5" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll mr ms mt mu bi translated">彼此足够接近</li></ul><p id="a15a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于<em class="ln"> a </em>和<em class="ln"> b </em>，我们根据<em class="ln"> 𝜆 </em>得到如下不等式:</p><ol class=""><li id="4a70" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll os ms mt mu bi translated">核心_距离(a) ≤ <em class="ln"> 𝜆 </em></li><li id="16d9" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">核心_距离(b) ≤ <em class="ln"> 𝜆 </em></li><li id="fd97" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">距离(a，b) ≤ <em class="ln"> 𝜆 </em></li></ol><p id="cf21" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">(1)和(2)是针对<em class="ln">“在足够密集的区域”</em>。③是为<em class="ln">【彼此足够接近】</em></p><p id="68f1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">结合这些不等式，能够直接连接a和b所需的𝜆的最小值为</p><pre class="mi mj mk ml gt ot oh ou ov aw ow bi"><span id="fb12" class="lo lp jb oh b gy ox oy l oz pa">mutual_reachability_distance(a, b)<em class="ln"> = </em>max(<br/>    core_distance(a), <br/>    core_distance(b), <br/>    distance(a, b)<br/>)</span></pre><p id="59c5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这在HDBSCAN文献中称为<em class="ln">相互可达性距离</em>。</p><h2 id="a0cb" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">投影到<em class="pb"> 𝜆-space </em></h2><p id="05c9" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated"><em class="ln">注意:这个“λ空间”是一个在文献中找不到的术语。这只是为了这个博客。</em></p><p id="4cf9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们现在可以使用相互可达性距离作为新的度量，而不是使用欧几里德距离作为我们的度量。使用它作为度量相当于将点嵌入一些新的度量空间，我们将简单地称之为<em class="ln"> 𝜆-space*.</em></p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/a7f80a8e860d78623222814bd0472b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPkkB8-x63EUA_XZ6LPy1Q.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">排斥效应。圆圈代表每个点的核心距离。</p></figure><p id="eebb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这具有在稀疏区域中分散接近点的效果。</p><p id="89ca" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">由于随机样本的随机性，两个点可以在非常稀疏的区域中彼此接近。然而，我们期望稀疏区域中的点彼此相距很远。通过使用相互可达性距离，稀疏区域中的点如果过于靠近就会“排斥其他点”，而非常密集区域中的点则不受影响。</p><p id="6ff3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">下面是使用<em class="ln">多维缩放</em>在𝜆-space投影的点的图，以更具体地显示其效果。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/b61045e3729ffc2da57a0c71a2ddf7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uic098xuuv6_XsShmT7sQ.png"/></div></div></figure><p id="296f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以在左边和上面看到这种排斥效应。左边的四个点是最分散的，因为它们在一个非常稀疏的空间中。</p><h2 id="e06a" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">使用𝜆-space构建层次结构树</h2><p id="b647" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">回想一下，为了构建层次结构树，我们有以下步骤:</p><ol class=""><li id="39b5" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll os ms mt mu bi translated">将<em class="ln"> 𝜆 </em>设置为核心距离的最小值</li><li id="3903" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">过滤水平集中的点</li><li id="d177" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">连接最多相距<em class="ln"> 𝜆 </em>单位的点</li><li id="84b6" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">创建新集群、扩展新集群和合并集群</li><li id="2373" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">将<em class="ln"> 𝜆 </em>设置为核心距离的下一个最小值，并转到步骤(2)</li></ol><p id="b44f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">注意，在执行步骤(3)时，连接已经属于同一个连通分量的两个点是没有用的。真正重要的是集群之间的联系。将连接两个聚类的连接对应于来自两个不同聚类的具有最小相互可达性距离的点对。如果我们忽略这些“无用”的连接，只注意相关的连接，我们剩下的是一个有序的边列表，这些边总是合并两个簇(连接的组件)。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/820c91a642ba8f357970a856759c98fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaw-8_A94C7W4oNLq2pF-Q.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">丢弃“无用”边的连接…这是最小生成树的形成吗？</p></figure><p id="53e1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这听起来可能很复杂，但如果我们将<em class="ln">相互可达性距离</em>视为我们的新指标<em class="ln"> : </em>，这就可以简化</p><ol class=""><li id="a2be" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll os ms mt mu bi translated">将点嵌入𝜆-space，并将每个点视为一个独立的聚类</li><li id="4a75" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">找出两个不同聚类中两点之间的最短距离</li><li id="a97f" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">合并两个集群</li><li id="2f96" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">回到步骤(2 ),直到只有一个集群</li></ol><p id="aab3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果这听起来很熟悉，这就是经典的凝聚聚类。这只是𝜆-space的单一连锁集群！</p><p id="cfd5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在欧几里得空间中进行单链聚类可能对噪声敏感，因为噪声点可能会形成跨越岛的伪桥。通过在𝜆-space中嵌入点,“排斥效应”使得聚类对噪声更加鲁棒。</p><p id="ec18" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">单一链接集群相当于构建一棵最小生成树！因此，我们可以利用图论中所有有效的方法来构造MST。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/75bd4af219663c3c039363faa04aff17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuEV86Dvxh--cwUgFQWqHg.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">HDBSCAN的最小生成树</p></figure><h1 id="912a" class="nn lp jb bd lq no np nq lt nr ns nt lw kh nu ki lz kk nv kl mc kn nw ko mf nx bi translated">参数选择和其他注意事项</h1><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/d60899937994387531560961f985ce08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2kuSogPpTME_CKhYKLFEQ.jpeg"/></div></div></figure><p id="fd8b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在，我们来了解一下关于HDBSCAN、<code class="fe oe of og oh b">min_samples</code>和<code class="fe oe of og oh b">min_cluster_size</code>以及HDBSCAN的主要参数的说明。</p><h2 id="41fb" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">最小样本数</h2><p id="5f16" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">回想一下我们的模拟数据X，在这里我们试图估计真实的PDF。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/7af1311e1afc266492f5df8c70cea8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwlZ7DMW-_nrkgZnwOPn2w.png"/></div></div></figure><p id="c412" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们尝试使用核心距离来估计这一点，核心距离是到第K个最近邻居的距离。超参数K在HDBSCAN API中被称为<code class="fe oe of og oh b">min_samples</code>。</p><p id="61b5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这些只是来自模拟数据的经验观察。我们将上面的图与基于不同<code class="fe oe of og oh b">min_samples</code>值的估计PDF进行比较。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pf"><img src="../Images/ff28d2a30e77817163bb31ea22ace378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D7eWMyPN9iZhnZTLs8sswA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">基于10000样本量的估计PDF</p></figure><p id="1162" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">正如你所看到的，设置<code class="fe oe of og oh b">min_samples</code>太低会导致PDF非常嘈杂的估计，因为核心距离变得对密度的局部变化敏感。这可能导致虚假集群，或者一些大集群可能最终分裂成许多小集群。</p><p id="9ceb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">设置<code class="fe oe of og oh b">min_samples</code>过高会使PDF过于平滑。PDF的更精细的细节丢失了，但是至少你能够捕捉到更大更全局的底层分布结构。在上面的例子中，两个小集群被“模糊”成一个集群。</p><p id="26ce" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">确定<code class="fe oe of og oh b">min_samples</code>的最佳值可能很困难，并且最终取决于数据。不要被我们这里使用的<code class="fe oe of og oh b">min_samples</code>的高值所误导。我们使用1-d模拟数据，该数据在整个域和仅3个集群中具有平滑的密度变化。典型的真实世界数据是完全不同的特性，较小的<code class="fe oe of og oh b">min_samples</code>值就足够了。</p><p id="0709" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">关于平滑效果的见解肯定适用于其他数据集。增加<code class="fe oe of og oh b">min_samples</code>的值可以平滑估计的分布，从而使小峰变平，我们可以只关注更密集的区域。</p><blockquote class="pg ph pi"><p id="6894" class="kq kr ln ks b kt ku kc kv kw kx kf ky pj la lb lc pk le lf lg pl li lj lk ll ij bi translated">对于<code class="fe oe of og oh b">min_samples</code>所做的事情，最简单的直觉是提供一个你希望你的聚类有多保守的度量。您提供的<code class="fe oe of og oh b">min_samples</code>值越大，聚类就越保守——更多的点将被声明为噪声，聚类将被限制在越来越密集的区域。[7]</p></blockquote><p id="4877" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请小心，这样做的一个可能的副作用是，它可能需要更长的运行时间，因为您必须为每个点找到更多的“最近邻居”，并且可能需要更多的内存。</p><h2 id="aafc" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">最小聚类大小</h2><p id="a38c" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">请注意，我们试图估计的潜在PDF非常平滑，但因为我们试图用样本进行估计，所以我们预计估计值会有一些变化。</p><p id="424c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这导致一个“颠簸”的估计PDF。让我们关注PDF的一小部分来说明这一点。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/cb0fc1ce642000fc10bbe719c329696a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkQh6R4wgzcKVJzHaQJZoA.png"/></div></div></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/c6f86a7b54ec072ff5d663346245590d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wc2He4lt8SmLIY-gUo1RrQ.png"/></div></div></figure><p id="7116" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这种凹凸在层次树中有什么影响？这影响了集群的持久性度量。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/3736d925da4aba91195f3e5c1bdbd460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LsYVnmh71hXRFu7ib77ZWw.png"/></div></div></figure><p id="8b10" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">因为小突起被解释为迷你聚类，所以真实聚类的持久性度量被划分为小段。如果不移除凸起，通过<em class="ln">质量过剩</em>方法可能看不到主星团。它看到的不是一座光滑的大山，而是无数迷你山峰的集合。</p><p id="6859" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了解决这个问题，我们把这些小突起弄平。这是通过“修剪”层次树中不够大的集群来实现的。这样做的效果是<em class="ln">质量过剩</em>方法不再被小突起分散注意力，现在可以看到主星团。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/d34f42789d127e29f58292e82632a368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojCsB2GUf7lEJQ-8qirBSQ.png"/></div></div></figure><p id="4fb4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe oe of og oh b">min_cluster_size</code>规定了在被认为是峰之前“凸起”的最大尺寸。通过增加<code class="fe oe of og oh b">min_cluster_size</code>的值，你在某种程度上平滑了估计的PDF，使得分布的真正峰值变得突出。</p><p id="5138" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">因为我们可以访问X的真实PDF，我们知道一个好的<code class="fe oe of og oh b">min_samples</code>值，它将产生一个平滑的估计PDF。如果估计不错，那么<code class="fe oe of og oh b">min_cluster_size</code>就不那么重要了。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pn"><img src="../Images/d5218548f9ab492e30a4bc046f016d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ialUxaTXy1CGV0pVjEEqjA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">理想凝聚树</p></figure><p id="c8dd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">假设我们对<code class="fe oe of og oh b">min_samples</code>使用了一个较小的值，并将其设置为100。如果你看PDF图，它有PDF的一般形状，但有明显的差异。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi po"><img src="../Images/c9ee7c32fbae7603c89423cf104b2549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-LvD3iYaMIzBdBrKMzxETg.png"/></div></div></figure><p id="0316" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">尽管我们知道应该只有3个峰值，但我们看到了许多小峰。</p><p id="9a29" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果你看到一个更极端的版本，也许你甚至看不到条形的颜色了，那么这意味着层次结构树是复杂的。也许是因为估计的方差，也许这就是数据的结构。解决这个问题的一个方法是增加<code class="fe oe of og oh b">min_cluster_size</code>，这有助于hdb简化树，专注于更大更全局的结构。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/da3eab9bdcfb88a9be090a54e222ae52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JbgChfpowsqEdKeppzfFQ.png"/></div></div></figure><h2 id="e55b" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">数据转换</h2><p id="917a" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">尽管我们已经确定HDBSCAN可以找到任意形状的簇，但这并不意味着不需要任何数据转换。这真的取决于你的用例。</p><p id="19e6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">缩放某些特征可以增加或减少该特征的影响。此外，一些变换如<em class="ln">对数</em>和平方根<em class="ln">变换</em>可以完全改变底层分布的形状。</p><h2 id="ba9e" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">评估集群质量</h2><p id="8daa" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">另一个需要注意的观点是，在使用HDSCAN时，评估和总结集群的传统方法可能没有意义。当聚类为圆形时，一些度量标准(如<em class="ln">轮廓得分</em>)工作得最好。</p><p id="b0b1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于<em class="ln"> sklearn中的“月亮”数据集，K-means </em>比HDBSCAN的结果具有更好的轮廓得分，即使我们看到HDBSCAN中的聚类更好。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pp"><img src="../Images/83e6fd1de22a4f130f75a729063d8dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tpYmcrUsXXlDBsKbs1RTQ.png"/></div></div></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pq"><img src="../Images/91d45c13a27d80c0a514b4346f26e798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSTdsy65yjIgd4ioAPRc7g.png"/></div></div></figure><p id="362d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这也适用于通过获取聚类所有点的平均值来汇总聚类。这对于K-means非常有用，是一个很好的集群原型。但是对于HDBSCAN来说，可能会有问题，因为星团不是圆的。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pr"><img src="../Images/54d5fe1a7e255726e772078cf174aa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*feexk8y-6gGj53fU5e1xyA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">空心圆是星团的“质心”。</p></figure><p id="0245" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">均值点可以远离实际集群！这可能会非常误导人，并导致错误的见解。您可能想要使用类似于<em class="ln"> medoid </em>的东西，它是最接近所有其他点的集群的一部分。但是要小心，试图用空间中的一个点来概括一个复杂的形状可能会丢失太多的信息。</p><p id="fd34" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这完全取决于您喜欢哪种类型的集群以及您正在处理的底层数据。请参见<a class="ae lm" href="https://www.youtube.com/watch?v=Mf6MqIS2ql4" rel="noopener ugc nofollow" target="_blank"> Henning's talk [5] </a>了解集群评估的概述。</p><h1 id="21cf" class="nn lp jb bd lq no np nq lt nr ns nt lw kh nu ki lz kk nv kl mc kn nw ko mf nx bi translated">HDBSCAN摘要</h1><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/05ba4300cce8ddb27b481058ac92f48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWoyrTZx_q__wnBP4bVuYg.jpeg"/></div></div></figure><p id="c455" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们完了！我们已经讨论了HDBSCAN的核心思想！我们将简要介绍一些具体的实现细节。</p><p id="9d9d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">HDBSCAN的实施大致如下:</p><ol class=""><li id="7101" class="mm mn jb ks b kt ku kw kx kz mo ld mp lh mq ll os ms mt mu bi translated">计算每个点的核心距离</li><li id="2b1f" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">使用<code class="fe oe of og oh b">mutual_reachability(a, b)</code>作为每个a、b的距离度量</li><li id="8d81" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">构建一棵最小生成树</li><li id="d9f0" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">修剪这棵树</li><li id="5cb5" class="mm mn jb ks b kt mv kw mw kz mx ld my lh mz ll os ms mt mu bi translated">使用<em class="ln">过剩质量</em>选择集群</li></ol><h2 id="323b" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">计算每个点的核心距离</h2><p id="1195" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">这基本上是我们“估计潜在pdf”的方式</p><h2 id="bf6e" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">使用相互可达性距离的最小生成树</h2><p id="b168" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">相互可达距离是在什么层次的<em class="ln"> 𝜆 </em>两个在一起的点将连接的总结。这就是我们使用的新度量标准。</p><p id="9f96" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">构建最小生成树相当于𝜆-space中的单链接聚类，这相当于遍历每个可能的水平集并跟踪聚类。</p><h2 id="ffa1" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">修剪生成的树</h2><p id="0015" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">简而言之，因为我们得到的只是一个估计的PDF，我们期望有一些差异。因此，即使底层分布非常平滑，估计的PDF也可能非常不平坦，因此导致非常复杂的层次树。</p><p id="89ad" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们使用参数<code class="fe oe of og oh b">min_cluster_size</code>来平滑估计分布的曲线，因此，将树简化为<code class="fe oe of og oh b">condensed_tree_</code></p><h2 id="221e" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kz lx ly lz ld ma mb mc lh md me mf mg bi translated">用“质量过剩”来选择星团</h2><p id="2f8d" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">使用<em class="ln">浓缩树</em>，我们可以估计每个集群的持久性，然后计算出最佳集群，如前一节所述。</p></div><div class="ab cl ps pt hu pu" role="separator"><span class="pv bw bk pw px py"/><span class="pv bw bk pw px py"/><span class="pv bw bk pw px"/></div><div class="ij ik il im in"><h1 id="f333" class="nn lp jb bd lq no pz nq lt nr qa nt lw kh qb ki lz kk qc kl mc kn qd ko mf nx bi translated">参考</h1><p id="7867" class="pw-post-body-paragraph kq kr jb ks b kt na kc kv kw nb kf ky kz nc lb lc ld nd lf lg lh ne lj lk ll ij bi translated">[1]<a class="ae lm" href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html" rel="noopener ugc nofollow" target="_blank">https://hdb scan . readthedocs . io/en/latest/how _ hdb scan _ works . html</a></p><p id="dc33" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[2]麦金尼斯，利兰和约翰希利。<a class="ae lm" href="https://arxiv.org/abs/1705.07321" rel="noopener ugc nofollow" target="_blank">加速分层密度聚类</a>。<em class="ln"> arXiv预印本arXiv:1705.07321 </em> (2017)。</p><p id="3902" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[3]约翰·希利。<a class="ae lm" href="https://www.youtube.com/watch?v=dGsxd67IFiU" rel="noopener ugc nofollow" target="_blank"> HDBSCAN，基于快速密度的聚类，方法和原因。</a> PyData纽约。2018</p><p id="5bfe" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[4]哈斯蒂、特雷弗、罗伯特·蒂布拉尼和杰罗姆·弗里德曼。<em class="ln">统计学习的要素:数据挖掘、推理和预测</em>。斯普林格科学&amp;商业媒体，2009年。</p><p id="07e2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[5]克里斯蒂安·亨宁。<a class="ae lm" href="https://www.youtube.com/watch?v=Mf6MqIS2ql4" rel="noopener ugc nofollow" target="_blank">评估聚类质量</a>。PyData纽约。2018.</p><p id="10d9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[6]亚历山德罗·里纳尔多。<a class="ae lm" href="http://www.stat.cmu.edu/topstat/topstat_old/resources/AleDebacl.pdf" rel="noopener ugc nofollow" target="_blank"> DeBaCl:一种基于密度的聚类算法及其性质</a>。</p><p id="8bfd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[7]<a class="ae lm" href="https://hdbscan.readthedocs.io/en/latest/parameter_selection.html" rel="noopener ugc nofollow" target="_blank">https://hdb scan . readthedocs . io/en/latest/parameter _ selection . html</a></p><p id="7c08" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">[8]坎佩洛、里卡多·JGB、达武德·穆拉维和约尔格·桑德。"基于层次密度估计的密度聚类."亚太知识发现和数据挖掘会议。施普林格，柏林，海德堡，2013。</p><p id="a433" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="ln">照片由</em><a class="ae lm" href="https://unsplash.com/@danotis?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="ln">【Dan Otis】</em></a><em class="ln">【on】</em><a class="ae lm" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="ln">【不连续】</em></a> <em class="ln">，</em>  <a class="ae lm" href="https://unsplash.com/@mischievous_penguins?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">【泰51】【凯西荷纳】 </a> <em class="ln">【对</em> <a class="ae lm" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="ln">【非致命】</em> </a> <em class="ln">，</em> <a class="ae lm" href="https://unsplash.com/@keisuke_h?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">】</a></p></div></div>    
</body>
</html>